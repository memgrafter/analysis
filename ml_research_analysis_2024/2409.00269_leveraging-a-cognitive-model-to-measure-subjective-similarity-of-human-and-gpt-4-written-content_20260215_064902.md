---
ver: rpa2
title: Leveraging a Cognitive Model to Measure Subjective Similarity of Human and
  GPT-4 Written Content
arxiv_id: '2409.00269'
source_url: https://arxiv.org/abs/2409.00269
tags:
- similarity
- human
- emails
- these
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces Instance-Based Individualized Similarity (IBIS),
  a method for measuring subjective document similarity that incorporates individual
  cognitive biases and constraints using Instance-Based Learning (IBL) cognitive models.
  The approach addresses limitations of traditional cosine similarity and semantic
  similarity metrics in educational and recommendation contexts where personalization
  is critical.
---

# Leveraging a Cognitive Model to Measure Subjective Similarity of Human and GPT-4 Written Content

## Quick Facts
- arXiv ID: 2409.00269
- Source URL: https://arxiv.org/abs/2409.00269
- Authors: Tailia Malloy; Maria José Ferreira; Fei Fang; Cleotilde Gonzalez
- Reference count: 15
- Primary result: IBIS achieves KDE log probability score of -717.45 and 0.93 ± 0.04 accuracy in predicting individual similarity judgments

## Executive Summary
This study introduces Instance-Based Individualized Similarity (IBIS), a method for measuring subjective document similarity that incorporates individual cognitive biases using Instance-Based Learning (IBL) cognitive models. The approach addresses limitations of traditional cosine similarity and semantic similarity metrics by creating a digital twin of individual participants that predicts their subjective similarity judgments based on decision history. IBIS is evaluated using a novel dataset of 433 participants categorizing 1440 emails, demonstrating superior performance with a KDE log probability score of -717.45 compared to -812.23 for ensemble methods.

## Method Summary
The method integrates an Instance-Based Learning cognitive model with LLM embeddings to develop IBIS. The IBL model stores each participant's decision history with utilities and feature vectors, calculating activation values for memory instances using similarity to current stimuli. IBIS then calculates the ratio of the IBL model's value for predicting specific categories to the sum of values for all categories, creating a probability distribution that reflects individual biases. The approach is evaluated against baseline methods (cosine similarity, weighted cosine, embedding pruning, ensemble) using KDE log probability scores and logistic regression accuracy metrics.

## Key Results
- IBIS achieves KDE log probability score of -717.45, outperforming ensemble methods (-812.23) and embedding pruning (-846.20)
- IBIS demonstrates superior accuracy (0.93 ± 0.04) in predicting individual participant annotations versus other methods
- IBIS successfully predicts similarity judgments for previously unseen documents, making it valuable for educational feedback and recommendation systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The IBL model serves as a digital twin that predicts individual participant behavior by storing and retrieving instances weighted by similarity, activation, and cognitive constraints
- Mechanism: The IBL model stores each participant's decision history with utilities and feature vectors, then calculates activation values for memory instances using similarity to current stimuli. Higher activation instances are more likely to be retrieved and influence future decisions through weighted soft-max probability and blended value calculations
- Core assumption: Individual decision patterns are sufficiently captured by stored instances of past decisions and their outcomes, allowing the model to predict future behavior on unseen documents
- Evidence anchors:
  - [abstract] "The method is evaluated using a novel dataset of 433 participants categorizing 1440 emails... IBIS also demonstrates superior accuracy (0.93 ± 0.04) in predicting individual participant annotations"
  - [section] "The probability of retrieval represents the probability that a single instance in memory will be retrieved when estimating the value associated with an option... Pi(t) = exp Ai(t)/τP i′∈Mk exp Ai′(t)/τ"
  - [corpus] Weak evidence - corpus shows related work on cognitive models and similarity measures but lacks direct evidence for this specific IBL-digital twin approach
- Break condition: If individual decision patterns are too complex or influenced by factors not captured in the instance features, the model's predictions would degrade significantly

### Mechanism 2
- Claim: The IBIS metric creates personalized similarity scores by comparing how an individual's cognitive model values different document categories
- Mechanism: IBIS calculates the ratio of the IBL model's value for predicting a specific category to the sum of values for all categories, creating a probability distribution that reflects individual biases and constraints in document similarity judgments
- Core assumption: The relative value an individual's cognitive model assigns to different categories accurately represents their subjective similarity judgments
- Evidence anchors:
  - [abstract] "This similarity metric is beneficial in that it takes into account individual biases and constraints in a manner that is grounded in the cognitive mechanisms of decision making"
  - [section] "Using this we determine the value that the IBL model assigns to predicting a category c as Vk(c|x)... This results in the IBIS metric which can be calculated after each decision is made by a participant"
  - [corpus] Weak evidence - corpus contains related work on similarity measures but limited evidence specifically for this ratio-based IBIS approach
- Break condition: If the category value ratios do not capture the full complexity of individual similarity judgments, the metric would fail to accurately represent subjective similarity

### Mechanism 3
- Claim: The ensemble approach combining multiple similarity metrics achieves better performance than any single method by capturing different aspects of document similarity
- Mechanism: The ensemble method weights multiple similarity metrics (cosine, semantic, weighted, pruned) to maximize correlation with human subjective similarity, leveraging the strengths of each individual approach
- Core assumption: Different similarity metrics capture complementary aspects of document similarity, and their weighted combination better approximates human judgment than any single metric
- Evidence anchors:
  - [section] "The final comparison method is based on using an ensemble of each of the previous similarity metrics, weighted to maximize the similarity to the average of the human subjective similarity metrics... This ensemble approach has the highest KDE log probability score of any individual method by itself, at a value of -812.23"
  - [abstract] "IBIS also demonstrates superior accuracy (0.93 ± 0.04) in predicting individual participant annotations versus other methods"
  - [corpus] Weak evidence - corpus shows ensemble methods exist in related domains but limited evidence for this specific application to document similarity
- Break condition: If the metrics are too correlated or if weighting optimization overfits to the training data, the ensemble would not provide meaningful improvement

## Foundational Learning

- Concept: Instance-Based Learning Theory
  - Why needed here: The IBL model forms the core mechanism for predicting individual behavior based on stored decision instances and their activation values
  - Quick check question: How does the IBL model calculate the probability of retrieving a specific instance from memory when evaluating a new document?

- Concept: Cosine similarity and embedding representations
  - Why needed here: The baseline similarity measures use document embeddings to calculate cosine similarity, which IBIS must outperform to demonstrate its value
  - Quick check question: What are the limitations of using raw cosine similarity between document embeddings for measuring subjective similarity?

- Concept: Kernel Density Estimate (KDE) for distribution comparison
  - Why needed here: KDE log probability scores are used to evaluate how well different similarity metrics match human subjective similarity distributions
  - Quick check question: Why does a higher KDE log probability score indicate better alignment between a similarity metric and human subjective similarity?

## Architecture Onboarding

- Component map: Document embedding generation -> IBL instance storage and retrieval -> IBIS metric calculation -> Evaluation via KDE and accuracy metrics
- Critical path: Document embedding → IBL instance storage → IBIS metric calculation → Evaluation via KDE and accuracy metrics
- Design tradeoffs: IBIS trades computational complexity for personalization accuracy; ensemble methods trade interpretability for performance; embedding pruning trades information retention for noise reduction
- Failure signatures: Poor KDE scores indicate similarity metric-human alignment issues; low logistic regression accuracy indicates poor predictive power; inconsistent individual vs. average performance suggests model overfitting
- First 3 experiments:
  1. Generate embeddings for all documents and calculate baseline cosine similarity to establish reference performance
  2. Implement IBL model with instance storage and retrieval, test on individual participant data to verify behavioral prediction capability
  3. Calculate IBIS metrics for all documents and participants, compare KDE scores and accuracy against baseline methods to validate improvements

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in a dedicated section. However, based on the discussion and methodology presented, several implicit open questions emerge regarding the generalizability of IBIS to different document types, the optimal balance between cognitive modeling complexity and computational efficiency, and the method's performance on non-textual similarity judgments.

## Limitations
- The study relies on a specific email categorization dataset, limiting generalizability to other document types and similarity judgment contexts
- The IBL model's effectiveness depends heavily on having sufficient historical decision data for each participant, raising concerns about performance with sparse or noisy training data
- The computational complexity of running individual IBL models for each participant may limit scalability in large-scale applications

## Confidence
- High confidence in IBIS metric's superiority over baseline methods for the specific email categorization task, given statistically significant performance improvements
- Medium confidence in generalizability across different document types and similarity judgment contexts, as the study focuses exclusively on email phishing detection
- Medium confidence in the ensemble approach's superiority, though this comes at the cost of reduced interpretability and increased computational overhead

## Next Checks
1. Test IBIS performance on a different document type (e.g., academic papers, social media posts) to validate generalizability beyond email categorization tasks
2. Conduct ablation studies to determine which IBL model parameters most significantly impact IBIS performance and identify potential optimization opportunities
3. Compare IBIS predictions against human similarity judgments in a real-time recommendation system to evaluate practical utility in educational feedback scenarios