---
ver: rpa2
title: A Universal Prompting Strategy for Extracting Process Model Information from
  Natural Language Text using Large Language Models
arxiv_id: '2407.18540'
source_url: https://arxiv.org/abs/2407.18540
tags:
- process
- information
- extraction
- llms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a prompting strategy for using large language
  models (LLMs) to extract process model information from natural language text. The
  authors present a modular prompt structure with three key components: Context, Task
  Description, and Restrictions.'
---

# A Universal Prompting Strategy for Extracting Process Model Information from Natural Language Text using Large Language Models

## Quick Facts
- arXiv ID: 2407.18540
- Source URL: https://arxiv.org/abs/2407.18540
- Reference count: 35
- Primary result: LLM-based process information extraction outperforms state-of-the-art ML methods by up to 8% F1 score

## Executive Summary
This paper introduces a modular prompting strategy for using large language models (LLMs) to extract process model information from natural language text. The authors propose a three-component prompt structure (Context, Task Description, Restrictions) that consistently improves extraction quality across diverse datasets and LLMs. Through systematic evaluation on three datasets and eight different LLMs, the study demonstrates that LLMs can effectively handle Mention Detection, Entity Resolution, and Relation Extraction tasks, outperforming traditional machine learning approaches while requiring less labeled data.

## Method Summary
The method employs a modular prompting strategy with three key components: Context (defining the persona and setting), Task Description (specifying the extraction task and creating meta-language), and Restrictions (imposing output format requirements and constraints). The approach uses few-shot prompting with process description examples and implements a chain-of-thought methodology that breaks tasks into sequential steps. The prompt structure incorporates general design patterns including persona assignment, context management, meta-language creation, chain-of-thought reasoning, reflection, and format instructions. The system processes text through the prompt generator, LLM inference, output parsing, and finally generates structured process model information.

## Key Results
- LLMs outperform state-of-the-art machine learning approaches by up to 8% F1 score across three different datasets
- The modular prompt structure with Context, Task Description, and Restrictions consistently improves extraction quality
- The number of examples, specificity of definitions, and rigor of format instructions are identified as key factors for improving accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The modular prompt structure with Context, Task Description, and Restrictions consistently improves extraction quality across diverse datasets and LLMs.
- Mechanism: By explicitly defining task parameters through persona assignment, meta-language creation, and formalized output formats, the prompt reduces ambiguity in the LLM's understanding of what constitutes process-relevant information.
- Core assumption: LLMs can interpret and apply structured instructions when they are clearly separated into logical modules that address different aspects of the task.
- Evidence anchors:
  - [abstract] "Based on a novel prompting strategy, we show that LLMs are able to outperform state-of-the-art machine learning approaches with absolute performance improvements of up to 8% F1 score across three different datasets"
  - [section] "To design potentially relevant components for all three modules we rely on general design patterns [11, 22, 31, 33]"
  - [corpus] Weak - the corpus provides general information extraction papers but lacks specific evidence about modular prompt structure benefits
- Break condition: If the modular structure introduces excessive token usage that exceeds model limits, or if the separation creates conflicting instructions that the LLM cannot reconcile.

### Mechanism 2
- Claim: Few-shot prompting with process description examples significantly improves extraction performance compared to zero-shot approaches.
- Mechanism: Providing concrete examples of input text paired with expected output formats teaches the LLM the specific schema and terminology used in process modeling, overcoming data unawareness challenges.
- Core assumption: LLMs can generalize from a small number of carefully selected examples to handle novel process descriptions within the same domain.
- Evidence anchors:
  - [abstract] "The number of example texts, the specificity of definitions, and the rigour of format instructions are identified as key for improving the accuracy of extracted information"
  - [section] "Finally, few-shot prompting [22, 31] dynamically adds examples for input and corresponding output"
  - [corpus] Weak - corpus mentions few-shot prompting generally but lacks specific evidence about process modeling applications
- Break condition: If the examples provided are too domain-specific or contain biases that prevent the LLM from generalizing to new process types.

### Mechanism 3
- Claim: The chain-of-thought approach, breaking tasks into sequential steps, improves relation extraction accuracy by separating mention detection from relation classification.
- Mechanism: By structuring the extraction process into discrete phases (extract mentions, then extract relations, then justify), the LLM can focus on simpler subtasks rather than attempting complex multi-step reasoning in a single pass.
- Core assumption: LLMs perform better on complex information extraction tasks when the cognitive load is distributed across multiple, simpler reasoning steps.
- Evidence anchors:
  - [section] "Module (B) is mainly concerned with defining the specifics of the process information extraction task. Its backbone is the creation of a meta language [33], which defines the types of elements to extract"
  - [section] "Thus, our prompt divides the relation extraction task into two steps that separate the extraction of mentions from the prediction of their relations"
  - [corpus] Weak - corpus discusses chain-of-thought generally but lacks specific evidence about its application to relation extraction
- Break condition: If the intermediate steps introduce errors that compound, or if the LLM fails to maintain consistency between the separate extraction phases.

## Foundational Learning

- Concept: Information extraction task decomposition
  - Why needed here: Understanding how mention detection, entity resolution, and relation extraction work together is crucial for designing effective prompts
  - Quick check question: What are the three main subtasks of process information extraction described in the paper?

- Concept: Prompt engineering best practices
  - Why needed here: Knowledge of design patterns like persona assignment, meta-language creation, and chain-of-thought is essential for implementing the modular prompt structure
  - Quick check question: Which three design patterns are mentioned as being incorporated into the prompt modules?

- Concept: Evaluation metrics for information extraction
  - Why needed here: Understanding precision, recall, and F1 score is necessary to interpret the experimental results and compare against baselines
  - Quick check question: How is the F1 score calculated from precision and recall in the context of this paper?

## Architecture Onboarding

- Component map: Text input → Prompt generator creates structured prompt → LLM processes prompt and generates output → Output parser validates and structures results → Process model generation algorithm creates BPMN model
- Critical path: Text input → Prompt generator creates structured prompt → LLM processes prompt and generates output → Output parser validates and structures results → Process model generation algorithm creates BPMN model
- Design tradeoffs: The modular prompt structure improves accuracy but increases token usage, requiring careful balance between prompt complexity and model token limits. Few-shot prompting improves performance but adds data requirements and processing time.
- Failure signatures: Parsing errors indicate prompt format issues, low precision suggests overly broad definitions, low recall indicates insufficient examples or constraints, and hallucination of entity types points to inadequate disambiguation hints.
- First 3 experiments:
  1. Test zero-shot extraction on a small subset of the PET dataset to establish baseline performance
  2. Implement the full modular prompt structure with three examples and evaluate on the same subset
  3. Conduct ablation study by removing each prompt component individually to measure impact on F1 score

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well can LLM-based process information extraction generalize to new domains beyond municipalities and small service providers?
- Basis in paper: [explicit] The paper mentions that current datasets are limited in origin and plans to analyze the ability of LLMs to generalize beyond these domains.
- Why unresolved: The paper only tests on existing datasets and acknowledges this limitation without providing empirical evidence for cross-domain generalization.
- What evidence would resolve it: Experiments applying the same LLM prompts to process descriptions from diverse domains (manufacturing, healthcare, finance, etc.) with varying linguistic styles and process complexity.

### Open Question 2
- Question: What is the optimal balance between prompt specificity and brevity for different types of process information extraction tasks?
- Basis in paper: [inferred] The ablation study shows that removing certain components affects performance differently, and the paper mentions challenges with input presentation dependencies.
- Why unresolved: The paper identifies some useful prompt components but doesn't systematically explore the trade-offs between comprehensive versus concise prompts across different extraction tasks.
- What evidence would resolve it: Comparative experiments testing multiple prompt variants with varying levels of detail across different datasets and task types.

### Open Question 3
- Question: How can LLM-based extraction be integrated into human-in-the-loop systems for process model creation while maintaining user trust?
- Basis in paper: [explicit] The paper discusses using reflective explanations from LLMs for human-in-the-loop systems but doesn't provide implementation details.
- Why unresolved: While the paper acknowledges this potential application, it doesn't explore how to design interfaces, validation workflows, or trust mechanisms for practitioners using these tools.
- What evidence would resolve it: User studies with process modelers using LLM-extracted information in real-world scenarios, measuring both extraction accuracy and user confidence/trust in the system.

## Limitations
- The exact prompt templates and examples used for few-shot prompting are not fully detailed in the paper
- The specific parsing rules for converting LLM outputs into structured format are not explicitly defined
- The study's evaluation across only three datasets may not generalize to all process modeling scenarios

## Confidence
- **High confidence**: The modular prompt structure with Context, Task Description, and Restrictions modules consistently improves extraction quality across datasets.
- **Medium confidence**: Few-shot prompting significantly improves extraction performance compared to zero-shot approaches.
- **Medium confidence**: The chain-of-thought approach improves relation extraction accuracy by separating mention detection from relation classification.

## Next Checks
1. Cross-dataset generalization test: Apply the prompt structure to an additional, unseen process modeling dataset to validate whether the 8% performance improvement generalizes beyond the three tested datasets.

2. Model-specific hallucination analysis: Conduct controlled experiments to quantify hallucination rates across different LLMs and identify prompt modifications that specifically reduce entity type hallucinations in problematic models.

3. Token efficiency evaluation: Measure the relationship between prompt complexity (token usage) and extraction accuracy to identify the optimal balance between prompt detail and model token limits, particularly for longer process descriptions.