---
ver: rpa2
title: 'Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases Generation
  with Small Language Models'
arxiv_id: '2407.16565'
source_url: https://arxiv.org/abs/2407.16565
tags:
- medical
- prage
- language
- arxiv
- french
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces pRAGe, a pipeline for retrieval-augmented
  generation and evaluation of medical paraphrases using small language models (SLMs).
  The method combines a RAG architecture with small, open-source language models to
  generate concise, patient-friendly explanations of medical terms in French.
---

# Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases Generation with Small Language Models

## Quick Facts
- arXiv ID: 2407.16565
- Source URL: https://arxiv.org/abs/2407.16565
- Reference count: 25
- Small language models with retrieval augmentation achieve 90% strict correctness in 25-token medical paraphrases

## Executive Summary
This paper presents pRAGe, a pipeline for retrieval-augmented generation of medical paraphrases using small language models (SLMs). The approach addresses the limitations of large language models (hallucinations and resource intensity) by combining retrieval from external medical knowledge bases with smaller, more efficient models. The system generates concise, patient-friendly explanations of medical terms in French, demonstrating that fine-tuning a 7B medical model on a paraphrase dataset significantly improves accuracy. The best configuration achieves 90% strict correctness for 25-token paraphrases, outperforming base models and validating the effectiveness of fine-tuning and retrieval augmentation for this task.

## Method Summary
The pRAGe pipeline combines retrieval-augmented generation with small language models to generate medical paraphrases. The method uses a RAG architecture with an encoder-retriever-decoder framework, retrieving relevant documents from a medical knowledge base (RefoMed-KB) before generating paraphrases. Two small models are evaluated: BARThez (general domain) and BioMistral-7B-SLERP-GPTQ (medical domain). The system is fine-tuned on the RefoMed dataset containing 6,297 pairs of medical terms and sub-sentential paraphrases in French. The pipeline tests two generation lengths (25 and 50 tokens) and compares performance with and without fine-tuning, using both automatic metrics (BLEU, ROUGE, BLEURT, BERTScore) and human evaluation.

## Key Results
- Fine-tuning BioMistral on RefoMed improves generation of accurate medical paraphrases
- pRAGe pipeline with fine-tuned BioMistral achieves 90% strict correctness for 25-token paraphrases
- Retrieval augmentation reduces hallucinations compared to base model inference alone

## Why This Works (Mechanism)

### Mechanism 1
- Fine-tuning BioMistral on a medical paraphrase dataset improves its ability to generate short, accurate medical paraphrases in French by adapting the model's parameters to the specific task of generating concise paraphrases, improving its understanding of medical terminology and the expected output format. The core assumption is that the fine-tuning dataset contains sufficient examples to effectively adapt the model. Evidence shows that fine-tuning improves generation accuracy. Break condition: If the fine-tuning dataset is too small or not representative of the target domain.

### Mechanism 2
- Retrieval augmentation reduces hallucinations in medical text generation by grounding the model's output in external knowledge. The pRAGe pipeline retrieves relevant documents from a medical knowledge base based on the input query, providing factual information to support generation. The core assumption is that the knowledge base contains accurate and relevant medical information. Evidence shows RAG helps reduce hallucinations. Break condition: If the knowledge base is incomplete or contains inaccurate information.

### Mechanism 3
- Using small language models instead of large language models reduces computational resource requirements while maintaining acceptable performance. SLMs have fewer parameters than LLMs, making them more efficient to train and deploy, especially on limited hardware. The core assumption is that SLM performance is sufficient for medical paraphrase generation. Evidence shows SLM + RAG can achieve high accuracy. Break condition: If the task requires complexity that exceeds SLM capabilities.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG is used to reduce hallucinations in medical text generation by grounding the model's output in external knowledge.
  - Quick check question: How does RAG help prevent hallucinations in language models?

- Concept: Fine-tuning
  - Why needed here: Fine-tuning adapts the model's parameters to the specific task of generating concise paraphrases, improving its understanding of medical terminology and the expected output format.
  - Quick check question: What is the purpose of fine-tuning a language model on a specific dataset?

- Concept: Tokenization
  - Why needed here: Tokenization is the process of splitting text into smaller units (tokens) that can be processed by the language model.
  - Quick check question: What is tokenization, and why is it important in natural language processing?

## Architecture Onboarding

- Component map: Input query -> Encoder (sent-CamemBERT/DrBERT) -> Retriever (FAISS/Annoy) -> Knowledge base (RefoMed-KB) -> Decoder (BARThez/BioMistral) -> Output paraphrase

- Critical path: Input query -> Encoder -> Retriever -> Knowledge base -> Decoder -> Output paraphrase

- Design tradeoffs: Using a general encoder vs. domain-specific encoder; smaller vs. larger knowledge base; smaller vs. larger decoder model

- Failure signatures: Hallucinations in output paraphrase; inaccurate or irrelevant information; poor performance on medical paraphrase generation

- First 3 experiments:
  1. Test pRAGe pipeline with general encoder and smaller decoder on sample input query
  2. Evaluate impact of larger knowledge base on quality of output paraphrases
  3. Compare performance of pRAGe with domain-specific encoder and larger decoder model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of knowledge base relevance on the performance of RAG systems for medical paraphrase generation?
- Basis in paper: The paper hypothesizes that the relevance of retrieved information from the knowledge base might explain lower performance of RAG system compared to standalone model.
- Why unresolved: This hypothesis is not tested or validated in the study.
- What evidence would resolve it: Experiments with different knowledge bases varying in size, relevance, and coverage, measuring RAG system performance.

### Open Question 2
- Question: How does the choice of knowledge base affect the generation of simplified medical paraphrases?
- Basis in paper: The paper mentions plans to explore how generation is affected by choice of knowledge base used.
- Why unresolved: The paper does not investigate how different knowledge bases might influence quality and simplification level of generated paraphrases.
- What evidence would resolve it: Comparing RAG system performance using different knowledge bases (general Wikipedia, medical-specific databases, curated datasets) on simplified medical paraphrase generation.

### Open Question 3
- Question: What is the optimal balance between parametric and non-parametric knowledge for medical paraphrase generation?
- Basis in paper: The paper references work on impact of parametric and non-parametric knowledge in GPT models.
- Why unresolved: The paper does not explore optimal balance between model's learned knowledge and external knowledge base for this task.
- What evidence would resolve it: Experiments with varying amounts of parametric and non-parametric knowledge, measuring performance on medical paraphrase generation tasks.

## Limitations

- Knowledge base coverage and quality may be insufficient for comprehensive medical information retrieval
- Human evaluation methodology lacks detailed qualification criteria for annotators
- Generalizability to real-world clinical practice with diverse patient populations remains untested

## Confidence

- High Confidence: Small language models can effectively generate medical paraphrases when combined with retrieval augmentation; fine-tuning improves performance on medical paraphrase generation; pRAGe pipeline outperforms base model inference
- Medium Confidence: 90% strict correctness rate represents clinically useful performance; retrieval augmentation significantly reduces hallucinations; computational efficiency gains justify SLM use over LLMs
- Low Confidence: Model performance generalizes to all medical terminology and patient populations; human evaluation results are robust across different annotator pools; knowledge base provides sufficient coverage for comprehensive generation

## Next Checks

- Validation Check 1: Expand RefoMed-KB with additional medical knowledge sources beyond Wikipedia and evaluate impact on retrieval precision and paraphrase accuracy
- Validation Check 2: Conduct formal annotator qualification study requiring medical domain expertise and calculate inter-annotator agreement
- Validation Check 3: Extend evaluation to complete clinical scenarios and patient questions, testing pRAGe's ability to generate paraphrases in context-rich scenarios with both medical professionals and patient representatives