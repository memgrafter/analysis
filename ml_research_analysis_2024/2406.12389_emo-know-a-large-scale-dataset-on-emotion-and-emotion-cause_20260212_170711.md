---
ver: rpa2
title: 'EMO-KNOW: A Large Scale Dataset on Emotion and Emotion-cause'
arxiv_id: '2406.12389'
source_url: https://arxiv.org/abs/2406.12389
tags:
- emotion
- dataset
- tweets
- cause
- emotions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EMO-KNOW, a large-scale dataset of emotion-cause
  pairs derived from 9.8 million tweets over 15 years. The dataset contains over 700,000
  tweets with corresponding emotion-cause pairs spanning 48 emotion classes, validated
  by human evaluators.
---

# EMO-KNOW: A Large Scale Dataset on Emotion and Emotion-cause

## Quick Facts
- arXiv ID: 2406.12389
- Source URL: https://arxiv.org/abs/2406.12389
- Reference count: 13
- Contains over 700,000 tweets with emotion-cause pairs across 48 emotion classes

## Executive Summary
This paper introduces EMO-KNOW, a large-scale dataset of emotion-cause pairs derived from 9.8 million tweets over 15 years. The dataset contains over 700,000 tweets with corresponding emotion-cause pairs spanning 48 emotion classes, validated by human evaluators. The core method involves curating tweets that follow specific patterns expressing emotions and their causes, then using large language models to generate abstractive summaries of the emotion causes. The dataset's novelty lies in its broad spectrum of emotion classes and abstractive emotion causes, which facilitate the development of an emotion-cause knowledge graph for nuanced reasoning. Human evaluation shows high quality of the generated emotion causes, with average scores of 4.50, 4.47, and 4.54 for relevance, fluency, and consistency respectively.

## Method Summary
The dataset was created by collecting tweets from 2008-2022 using search patterns like "I feel [emotion] because" and "I am feeling [emotion] because" across 48 emotion classes. After cleaning and filtering, emotion labels were directly extracted from user text following these patterns. Large language models (specifically InstructGPT) were then used to generate abstractive emotion-cause summaries in response to the question "Why do I feel e?" The outputs were filtered using a grammar checker and validated by human evaluators on Amazon Mechanical Turk for relevance, fluency, and consistency.

## Key Results
- Dataset contains over 700,000 tweets with emotion-cause pairs spanning 48 emotion classes
- Human evaluation shows high quality with average scores of 4.50 (relevance), 4.47 (fluency), and 4.54 (consistency)
- The abstractive approach enables development of emotion-cause knowledge graphs for nuanced reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using user self-reported emotion words as labels improves authenticity over external annotation.
- Mechanism: The dataset directly extracts emotion labels from the user's own tweet text following "I am/feel/am feeling X because" patterns, ensuring the label reflects the user's actual expressed emotion.
- Core assumption: Users accurately express their emotions in these structured patterns without sarcasm or negation.
- Evidence anchors:
  - [abstract]: "Unlike other datasets that rely on outside labeling, ours uses the users' own words, making it more genuine."
  - [section 3.2.1]: "we simply extract the emotion label from the word that fits into our identified pattern. This method ensures that the emotion label accurately reflects the individual's true feelings"

### Mechanism 2
- Claim: Abstractive summarization of emotion causes provides more generalizable and nuanced understanding than extractive methods.
- Mechanism: Instead of extracting specific clauses that contain emotion causes, the approach generates abstractive summaries that capture the root cause of emotions, enabling deeper reasoning.
- Core assumption: Abstractive summaries can effectively capture the generalized root cause while maintaining relevance to the specific tweet.
- Evidence anchors:
  - [abstract]: "The novelty of our dataset stems from its broad spectrum of emotion classes and the abstractive emotion cause that facilitates the development of an emotion-cause knowledge graph for nuanced reasoning."
  - [section 3.2.2]: "We approach this task by treating it as a question-answer challenge... we train a language model to answer the question: Why do I feel e?"

### Mechanism 3
- Claim: Large-scale dataset with 48 emotion classes enables better representation of human emotional diversity.
- Mechanism: By expanding beyond the typical 6-8 emotion categories to 48 classes, the dataset captures more nuanced emotional states and their diverse causes.
- Core assumption: More emotion categories lead to better representation of emotional diversity and enable more precise emotion-cause reasoning.
- Evidence anchors:
  - [abstract]: "The final dataset comprises over 700,000 tweets with corresponding emotion-cause pairs spanning 48 emotion classes"
  - [section 1]: "Modern theories of emotion suggest that emotions are not simply reducible to a basic set of 6-8 emotion categories... a finer granularity of emotion categories would better reflect the true diversity and nuance of human emotions."

## Foundational Learning

- Concept: Understanding the difference between extractive and abstractive text summarization
  - Why needed here: The paper contrasts extractive methods (common in emotion-cause analysis) with abstractive methods that generate generalized causes
  - Quick check question: What is the key difference between extractive and abstractive summarization approaches?

- Concept: Knowledge of emotion classification systems and their limitations
  - Why needed here: The paper discusses why traditional 6-8 emotion categories are insufficient and introduces 48 categories
  - Quick check question: Why might traditional basic emotion sets (like Ekman's 6 emotions) be insufficient for comprehensive emotion analysis?

- Concept: Understanding of question-answering models and their adaptation for text generation
  - Why needed here: The paper frames emotion-cause extraction as a Q&A task and uses language models for this purpose
  - Quick check question: How can question-answering models be adapted for generative tasks beyond extractive answering?

## Architecture Onboarding

- Component map: Data collection -> Preprocessing -> Emotion labeling -> Cause generation -> Validation
- Critical path: Data collection → Preprocessing → Emotion labeling → Cause generation → Validation
- Design tradeoffs:
  - Scale vs. quality: Large dataset (700K+ tweets) vs. potential noise from automated labeling
  - Granularity vs. practicality: 48 emotion classes vs. manageability and model complexity
  - Abstractiveness vs. specificity: Abstractive causes vs. potential loss of specific contextual details
- Failure signatures:
  - Poor grammar detection causing quality issues
  - Sarcasm or negation not being properly handled
  - Emotion-cause pairs not aligning correctly
  - Model-generated causes being too generic or irrelevant
- First 3 experiments:
  1. Validate emotion extraction accuracy by sampling tweets and checking if extracted emotion matches expressed emotion
  2. Test cause generation quality by comparing human-written causes vs. model-generated causes on a subset
  3. Evaluate the impact of using different numbers of emotion classes (e.g., 6, 12, 24, 48) on downstream task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of emotion labeling using self-reported emotions compare to traditional human annotation methods?
- Basis in paper: [explicit] The paper mentions that their method uses users' own words for emotion labeling, which is more genuine than traditional methods where human annotators determine emotion labels, potentially introducing biases.
- Why unresolved: The paper does not provide a direct comparison between the accuracy of their self-reported emotion labeling method and traditional human annotation methods.
- What evidence would resolve it: A study comparing the accuracy of emotion labels obtained through self-reported emotions versus human annotations on the same dataset.

### Open Question 2
- Question: What is the impact of using different large language models on the quality of abstractive emotion cause generation?
- Basis in paper: [explicit] The paper experimented with various language models (BART-large, T5-base, T5-flan) and selected T5-flan based on BLEU scores and BERT-score. However, the impact of using different models on the quality of abstractive emotion cause generation is not fully explored.
- Why unresolved: The paper only provides a comparison of three models and does not explore the performance of other potential models or the effect of model choice on the quality of generated emotion causes.
- What evidence would resolve it: A comprehensive study evaluating the performance of various large language models on abstractive emotion cause generation tasks, including newer models not mentioned in the paper.

### Open Question 3
- Question: How does the EMO-KNOW dataset perform in real-world applications compared to existing datasets?
- Basis in paper: [inferred] The paper introduces EMO-KNOW as a novel, comprehensive dataset for emotion-cause analysis but does not provide direct comparisons of its performance in real-world applications versus existing datasets.
- Why unresolved: The paper focuses on the creation and evaluation of the dataset but does not explore its practical applications or compare its performance to other datasets in real-world scenarios.
- What evidence would resolve it: Empirical studies applying EMO-KNOW and other existing datasets to real-world emotion-aware systems and comparing their performance in terms of accuracy, efficiency, and user satisfaction.

## Limitations
- The dataset's reliance on Twitter data introduces platform-specific biases that may not generalize to other contexts
- The assumption that users accurately express emotions without sarcasm or negation remains unverified
- Abstractive cause generation may lose specific contextual details important for understanding nuanced emotional triggers

## Confidence
- Dataset Quality: High (supported by strong human evaluation scores)
- Method Innovation: Medium (novel approach but effectiveness in downstream tasks not demonstrated)
- Generalizability: Low (Twitter-specific patterns may not transfer to other domains)

## Next Checks
1. Conduct a systematic analysis of sarcasm and negation detection in the dataset to quantify how often these phenomena affect the accuracy of emotion extraction.
2. Compare the performance of abstractive versus extractive cause generation methods on a shared downstream task to empirically validate the claimed benefits of abstractiveness.
3. Test the dataset's cross-cultural validity by evaluating whether the same emotion-cause patterns and classifications apply to non-English social media data from different cultural contexts.