---
ver: rpa2
title: 'Creativity Has Left the Chat: The Price of Debiasing Language Models'
arxiv_id: '2406.05587'
source_url: https://arxiv.org/abs/2406.05587
tags:
- aligned
- base
- diversity
- outputs
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how Reinforcement Learning from Human Feedback
  (RLHF) affects creativity in Large Language Models (LLMs). The study focuses on
  comparing Llama-2 base and aligned models using three experiments.
---

# Creativity Has Left the Chat: The Price of Debiasing Language Models

## Quick Facts
- **arXiv ID**: 2406.05587
- **Source URL**: https://arxiv.org/abs/2406.05587
- **Reference count**: 0
- **Primary result**: RLHF alignment reduces creativity and diversity in LLM outputs through mode collapse and attractor states

## Executive Summary
This paper investigates how Reinforcement Learning from Human Feedback (RLHF) affects creativity in Large Language Models, specifically comparing Llama-2 base and aligned models. Through three experiments measuring entropy, clustering, and attractor states, the study demonstrates that aligned models exhibit significantly reduced output diversity across multiple dimensions. While RLHF improves consistency and safety, it comes at the cost of creativity and diversity in model outputs, suggesting base models may be more suitable for tasks requiring creative output.

## Method Summary
The study compares Llama-2-7B-text (base model) and Llama-2-7B-chat (aligned model) across three experiments. Models are run at temperature=1.0 for maximum variation. Experiment 1 generates 100 customer personas and product reviews, analyzing demographic diversity and sentiment. Experiment 2 uses a "Grace Hopper was" prompt to generate 200 outputs, analyzing semantic diversity through Sentence-BERT embeddings and k-means clustering. Experiment 3 examines syntactic diversity by calculating token entropy using top-5 predicted tokens at each step for startup story generation.

## Key Results
- Aligned models show significantly lower entropy in token predictions, indicating reduced output variety
- Embedding space analysis reveals distinct clusters in aligned model outputs, suggesting convergence to attractor states
- Aligned models produce more homogeneous demographics and sentiment patterns in generated content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLHF alignment leads to mode collapse, reducing diversity in LLM outputs.
- Mechanism: During RLHF, the model optimizes for human preference rewards, which causes it to overfit to certain high-reward responses, effectively "blocking" many token trajectories and reducing the entropy of predictions.
- Core assumption: The reward model accurately captures human preferences and the RL algorithm (PPO) can effectively optimize for these rewards without collapsing to a narrow set of behaviors.
- Evidence anchors: [abstract] "Results show that aligned models exhibit lower entropy in token predictions, form distinct clusters in the embedding space, and gravitate towards 'attractor states', indicating reduced output diversity."

### Mechanism 2
- Claim: Aligned models form attractor states, leading to repetitive and limited output patterns.
- Mechanism: The RLHF process causes the model to converge to a small set of "attractor states" in the output space. When perturbed, the model returns to these states, indicating a loss of flexibility and creativity.
- Core assumption: The model's outputs can be modeled as a dynamical system with attractor states, and RLHF causes the model to converge to these states.
- Evidence anchors: [abstract] "aligned models...gravitate towards 'attractor states', indicating limited output diversity."

### Mechanism 3
- Claim: Reduced token-level entropy in aligned models leads to lower semantic diversity.
- Mechanism: Aligned models have lower entropy in their token predictions, meaning they assign higher probabilities to fewer tokens. This reduces the variety of token trajectories the model can generate, leading to lower semantic diversity in the outputs.
- Core assumption: Token-level entropy is a necessary condition for semantic diversity, and reducing token-level entropy will lead to a reduction in semantic diversity.
- Evidence anchors: [abstract] "aligned models exhibit lower entropy in token predictions...indicating limited output diversity."

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding how RLHF works is crucial to understanding why it leads to reduced creativity and diversity in LLM outputs.
  - Quick check question: What are the main steps in the RLHF process, and how does each step contribute to the final aligned model?

- Concept: Entropy and its relationship to diversity
  - Why needed here: Entropy is used as a measure of diversity in the paper, and understanding its relationship to diversity is essential for interpreting the results.
  - Quick check question: How does entropy relate to the variety of token trajectories a model can generate, and why is this important for creativity?

- Concept: Dynamical systems and attractor states
  - Why needed here: The concept of attractor states is used to explain the repetitive behavior of aligned models, and understanding this concept is necessary to grasp the paper's findings.
  - Quick check question: What are attractor states in dynamical systems, and how can they be used to model the behavior of aligned LLMs?

## Architecture Onboarding

- Component map: Base LLM (Llama-2) -> Human preference data collection -> Reward model training -> PPO fine-tuning -> Aligned LLM

- Critical path: Pretrained base LLM → Human preference data collection → Reward model training → PPO fine-tuning → Aligned LLM

- Design tradeoffs: Balancing alignment with human preferences and preserving creativity and diversity; choosing the right hyperparameters for the reward model and RL algorithm to prevent mode collapse

- Failure signatures: Mode collapse (narrow repetitive outputs); Reduced entropy (fewer high-probability tokens); Attractor states (convergence to limited output patterns)

- First 3 experiments:
  1. Compare diversity of customer personas and product reviews generated by base and aligned models
  2. Analyze semantic diversity of "Grace Hopper was" outputs using Sentence-BERT embeddings and clustering
  3. Investigate syntactic diversity by analyzing token entropy and probability distributions for startup story generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the RLHF process specifically affect the distribution of demographic attributes (age, gender, ethnicity) in generated personas, and what mechanisms cause these changes?
- Basis in paper: [explicit] The paper shows aligned models generate more homogeneous demographics (e.g., mostly female, narrow age range) compared to base models, but doesn't explain why RLHF causes this shift.
- Why unresolved: The study demonstrates the effect but doesn't investigate whether RLHF directly rewards or penalizes certain demographics, or if the effect is an indirect consequence of other alignment objectives.
- What evidence would resolve it: Analysis of the reward model's scoring patterns across demographic variations, or ablation studies showing which RLHF components (safety vs. helpfulness) drive demographic convergence.

### Open Question 2
- Question: What is the relationship between token-level entropy reduction and the emergence of attractor states in aligned models?
- Basis in paper: [explicit] The paper shows aligned models have lower entropy and form attractor states, but doesn't establish a causal link between these phenomena.
- Why unresolved: The study observes both phenomena but doesn't test whether entropy reduction directly causes attractor states, or if they're independent consequences of RLHF.
- What evidence would resolve it: Experiments manipulating entropy levels while measuring attractor state formation, or mathematical modeling of how probability distributions evolve under RLHF training.

### Open Question 3
- Question: How would alternative alignment methods (like supervised fine-tuning alone) compare to RLHF in terms of preserving creativity and diversity?
- Basis in paper: [inferred] The paper suggests RLHF fundamentally constrains creativity through mode collapse, but doesn't test whether other alignment approaches would produce similar effects.
- Why unresolved: The study only examines RLHF but suggests the problem might be inherent to alignment, not specific to RLHF's mechanism.
- What evidence would resolve it: Direct comparison of SFT-only models versus RLHF models on the same diversity metrics, or analysis of intermediate RLHF training stages to pinpoint when diversity loss occurs.

## Limitations

- Findings are based on synthetic generation tasks that may not generalize to real-world creative applications
- The temperature setting of 1.0 represents maximum variation, but practical applications often use lower temperatures
- Results focus specifically on Llama-2-7B, leaving uncertainty about whether findings extend to larger models or different architectures

## Confidence

- **High Confidence**: The finding that aligned models show lower token-level entropy is well-supported by direct measurement across multiple experiments.
- **Medium Confidence**: The clustering and attractor state analyses, while methodologically sound, depend on subjective interpretation of embedding space visualizations.
- **Medium Confidence**: The generalization from synthetic task outputs to "creativity" as a broader concept involves interpretive leaps.

## Next Checks

1. Test base vs aligned models on established creative benchmarks (story completion, poetry generation, code generation) to verify synthetic task findings generalize to practical creative applications.

2. Systematically vary temperature from 0.1 to 1.0 and measure how entropy and diversity differences between base and aligned models change, identifying at which temperatures differences become most pronounced.

3. Repeat experiments with other base/aligned model pairs (e.g., GPT-3.5 vs InstructGPT, or other RLHF-tuned models) to determine if creativity loss pattern is consistent across different architectures and alignment approaches.