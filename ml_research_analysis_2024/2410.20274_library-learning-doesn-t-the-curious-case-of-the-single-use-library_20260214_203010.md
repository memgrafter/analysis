---
ver: rpa2
title: 'Library Learning Doesn''t: The Curious Case of the Single-Use "Library"'
arxiv_id: '2410.20274'
source_url: https://arxiv.org/abs/2410.20274
tags:
- sqrt
- have
- simp
- library
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether library learning systems for mathematical
  reasoning, such as LEGO-Prover and TroVE, actually learn reusable tools or achieve
  performance gains through other mechanisms. Analysis of publicly released logs reveals
  that direct reuse of learned functions or lemmas is extremely infrequent, with only
  2 direct reuses found across thousands of test problems.
---

# Library Learning Doesn't: The Curious Case of the Single-Use "Library"

## Quick Facts
- arXiv ID: 2410.20274
- Source URL: https://arxiv.org/abs/2410.20274
- Reference count: 40
- Single-use "library": Direct reuse of learned functions is extremely rare (2 instances across thousands of problems)

## Executive Summary
This study investigates whether library learning systems for mathematical reasoning, such as LEGO-Prover and TroVE, actually learn reusable tools or achieve performance gains through other mechanisms. Analysis of publicly released logs reveals that direct reuse of learned functions or lemmas is extremely infrequent, with only 2 direct reuses found across thousands of test problems. Follow-up ablation experiments, where cross-problem sharing is disabled, show that performance remains similar to baseline models. These findings suggest that self-correction and self-consistency, rather than direct reuse, are the primary drivers of the observed performance gains in these systems.

## Method Summary
The researchers analyzed publicly released logs from library learning systems to track function reuse patterns. They examined thousands of test problems across systems like LEGO-Prover and TroVE, searching for instances where learned functions were directly reused. To further investigate the source of performance gains, they conducted ablation experiments where cross-problem sharing of learned components was disabled, comparing results to baseline models. The study also included a survey of the library learning literature to contextualize their findings.

## Key Results
- Only 2 direct reuses of learned functions found across thousands of test problems
- Ablation experiments show disabling cross-problem sharing has minimal impact on performance
- Self-correction and self-consistency likely drive performance gains rather than direct reuse

## Why This Works (Mechanism)
The mechanism behind library learning systems' performance appears to stem from improved self-correction and self-consistency rather than direct reuse of learned components. When these systems generate and test proofs, they benefit from having a library of previously successful proof steps, but this benefit comes primarily from the ability to reconsider and refine their own reasoning rather than from directly applying learned functions to new problems. The library acts more as a memory aid for the system's own past work rather than as a collection of reusable tools.

## Foundational Learning
- Mathematical proof construction: Why needed - Library learning systems operate in mathematical domains requiring formal proof; Quick check - Verify understanding of proof steps and logical dependencies
- Automated theorem proving: Why needed - These systems use ATP techniques to explore proof spaces; Quick check - Understand how proof search works in systems like E-prover
- Neural-guided search: Why needed - Library learning combines neural networks with search algorithms; Quick check - Know how neural guidance influences proof search

## Architecture Onboarding

Component map:
Neural Model -> Proof Search -> Library Management -> Self-Correction Loop

Critical path:
Problem input → Neural guidance → Proof search with library access → Proof construction → Library update → Self-correction analysis

Design tradeoffs:
- Direct reuse vs. indirect influence of learned components
- Library size and management overhead vs. search efficiency
- Neural guidance quality vs. computational cost

Failure signatures:
- No improvement over baseline when library is disabled
- Very low direct reuse rates across problems
- Performance gains persist without cross-problem sharing

Three first experiments:
1. Measure direct reuse frequency in library learning systems across multiple domains
2. Conduct ablation study disabling library sharing to isolate performance sources
3. Analyze proof traces to identify indirect reuse patterns and self-correction mechanisms

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis based only on publicly released logs, potentially missing undetected reuse patterns
- Findings may not generalize beyond mathematical reasoning domains
- Small sample size of direct reuses (2 instances) limits statistical confidence

## Confidence

Major claim cluster: "Library learning systems primarily achieve performance gains through self-correction and self-consistency rather than direct reuse" - Medium confidence

Major claim cluster: "Direct reuse of learned functions/lemmas is extremely rare in library learning systems" - Low confidence

Major claim cluster: "Ablation experiments show disabling cross-problem sharing has minimal impact on performance" - High confidence

## Next Checks

1. Conduct a systematic review of library learning literature to identify all reported instances of direct reuse across different systems and domains, comparing frequencies and patterns.

2. Design and implement enhanced logging mechanisms to capture more granular information about function usage, including indirect reuse patterns and the influence of learned components on reasoning processes.

3. Extend the ablation study to include a broader range of library learning systems and problem domains, testing whether the findings generalize beyond mathematical reasoning tasks.