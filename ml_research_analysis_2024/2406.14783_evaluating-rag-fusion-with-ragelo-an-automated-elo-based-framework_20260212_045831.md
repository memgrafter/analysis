---
ver: rpa2
title: 'Evaluating RAG-Fusion with RAGElo: an Automated Elo-based Framework'
arxiv_id: '2406.14783'
source_url: https://arxiv.org/abs/2406.14783
tags:
- answer
- answers
- ragf
- queries
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates retrieval-augmented generation (RAG) systems
  using RAGElo, a framework combining LLM-generated synthetic queries, LLM-as-a-judge
  evaluation, and Elo-based ranking. The authors compare traditional RAG against RAG-Fusion
  (RAGF) for answering product questions in Infineon's technical documentation.
---

# Evaluating RAG-Fusion with RAGElo: an Automated Elo-based Framework

## Quick Facts
- **arXiv ID:** 2406.14783
- **Source URL:** https://arxiv.org/abs/2406.14783
- **Reference count:** 40
- **Primary result:** RAG-Fusion achieves higher Elo scores than traditional RAG by generating more complete answers at the cost of reduced precision

## Executive Summary
This paper introduces RAGElo, an automated framework for evaluating retrieval-augmented generation (RAG) systems using LLM-generated synthetic queries, LLM-as-a-judge evaluation, and Elo-based ranking. The authors apply RAGElo to compare traditional RAG against RAG-Fusion (RAGF) for answering product questions in Infineon's technical documentation. RAGElo successfully ranks systems without requiring gold standard answers, showing moderate positive correlation (Kendall's τ ≈ 0.56) with human expert annotations. RAGF demonstrates significantly better completeness than RAG but sacrifices precision, while BM25 retrieval consistently outperforms KNN embeddings in this domain-specific application.

## Method Summary
RAGElo combines synthetic query generation, LLM-as-a-judge evaluation, and Elo-based ranking to automate RAG system evaluation. The framework generates synthetic queries from document passages using LLMs with few-shot examples from real user queries. These queries are processed through RAG and RAG-Fusion pipelines with different retrieval methods (BM25, KNN, hybrid). RAGElo evaluates document relevance and answer quality using LLM judges, then conducts pairwise comparisons in an Elo tournament to produce system rankings. The approach eliminates the need for human-annotated gold standard answers while maintaining correlation with expert judgments.

## Key Results
- RAG-Fusion achieved higher Elo scores than traditional RAG, demonstrating superior overall system performance
- RAG-Fusion significantly outperformed RAG in completeness (p ≈ 0.01) but showed reduced precision
- BM25 retrieval consistently outperformed KNN embeddings for document relevance in this technical domain
- RAGElo's LLM-as-a-judge showed moderate positive correlation (τ ≈ 0.56) with human expert annotations across four evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAGElo's Elo-based ranking successfully ranks RAG systems without requiring gold standard answers
- Mechanism: RAGElo uses pairwise LLM-as-a-judge comparisons between two RAG agents' answers, with each comparison treated as a game in an Elo tournament. The LLM evaluates answers based on retrieved document relevance and answer quality, then updates Elo scores accordingly
- Core assumption: LLM judgments correlate sufficiently with human expert preferences to produce meaningful rankings
- Evidence anchors:
  - [abstract] "RAGElo's LLM-as-a-judge showed moderate positive correlation (Kendall's τ ≈ 0.56) with human expert annotations"
  - [section 7.1] "This test returned τ ≈ 0.56, indicating a moderate, positive correlation [32] with a p-value against a null hypothesis of no association of p < 0.01 (99.99% confidence level)"
  - [corpus] Weak - no direct evidence in corpus papers about Elo-based ranking correlation with human preferences
- Break condition: If LLM-as-a-judge correlation with human experts falls below ~0.4 or becomes statistically insignificant

### Mechanism 2
- Claim: RAG-Fusion produces more complete answers than traditional RAG by generating query variations
- Mechanism: RAG-Fusion generates multiple query variations from the original user question, retrieves documents for each variation, and combines results using reciprocal rank fusion (RRF). This increases the diversity and quantity of relevant documents fed to the LLM
- Core assumption: Multiple query variations will retrieve different but complementary relevant documents
- Evidence anchors:
  - [section 3] "The intuition behind RAGF is that submitting variations of the same query and combining the final rankings increases the likelihood of relevant passages being injected into the LLM prompt"
  - [abstract] "RAGF achieved higher Elo scores than RAG but demonstrated significantly better completeness at the cost of reduced precision"
  - [section 7.2.2] "RAGF significantly outperforms RAG in completeness at the 95% confidence level with p ≈ 0.01"
- Break condition: If query variations generate redundant or irrelevant queries that don't improve document diversity

### Mechanism 3
- Claim: Synthetic queries generated from document passages with few-shot examples provide valid evaluation data
- Mechanism: The system samples passages from domain documents and prompts LLMs to generate questions about them, using real user queries as few-shot examples to guide question style and domain specificity
- Core assumption: LLMs can generate realistic user questions when given document context and examples of real questions
- Evidence anchors:
  - [section 4] "To keep the questions generated as diverse as possible, we prompt four different LLMs to generate up to ten questions based on the same documents"
  - [section 4] "We compiled a list of 23 of these queries to use as a base for experimentation and used them as few-shot examples in the query generation prompt"
  - [corpus] Weak - no direct evidence about synthetic query quality from corpus papers
- Break condition: If generated questions are too generic, hallucinate non-existent product features, or don't match actual user query patterns

## Foundational Learning

- Concept: Elo rating systems and pairwise comparison
  - Why needed here: Understanding how RAGElo's tournament mechanism converts pairwise judgments into system rankings
  - Quick check question: If Agent A beats Agent B 60% of the time and Agent B beats Agent C 70% of the time, can we determine with certainty whether Agent A would beat Agent C?

- Concept: Retrieval metrics (MRR@5, BM25, KNN)
  - Why needed here: Evaluating document retrieval quality is critical for understanding RAG system performance before answer generation
  - Quick check question: If a system retrieves 3 relevant documents at ranks 1, 3, and 5, what is its MRR@5 score?

- Concept: LLM-as-a-judge methodology
  - Why needed here: RAGElo relies on LLMs to evaluate both document relevance and answer quality without human annotations
  - Quick check question: What are the key risks when using an LLM to judge the quality of answers generated by another LLM?

## Architecture Onboarding

- Component map: Query generation module -> Retrieval component (BM25/KNN/hybrid) -> RAG pipeline or RAG-Fusion pipeline -> RAGElo evaluation system (document relevance evaluator + pairwise answer judge + Elo tournament) -> LLM judges (GPT-4 Turbo, Claude 3 models)

- Critical path: User query → Retrieval (documents) → Answer generation → RAGElo evaluation → Elo ranking update

- Design tradeoffs:
  - RAG vs RAG-Fusion: RAG is simpler and more precise but may miss relevant documents; RAG-Fusion is more comprehensive but can sacrifice precision
  - BM25 vs KNN vs Hybrid: BM25 excels with keyword matching and outperformed embeddings in this domain; KNN captures semantic similarity; hybrid attempts to combine benefits
  - Synthetic queries vs real user queries: Synthetic queries enable scalable evaluation but may not capture all real user query patterns

- Failure signatures:
  - Low correlation between LLM judgments and human experts indicates RAGElo evaluations are unreliable
  - BM25 consistently outperforming embeddings suggests domain-specific retrieval challenges
  - RAG-Fusion significantly underperforming in precision indicates query variation generation is too broad

- First 3 experiments:
  1. Compare BM25 vs KNN retrieval on a small sample of queries to validate the paper's finding that BM25 outperforms KNN in this domain
  2. Run RAGElo pairwise evaluation with only one judge model (e.g., GPT-4 Turbo) to isolate judge model effects
  3. Test RAG-Fusion with different numbers of query variations (2, 4, 6) to find the optimal balance between completeness and precision

## Open Questions the Paper Calls Out
None

## Limitations
- The moderate correlation (τ ≈ 0.56) between LLM-as-a-judge and human expert annotations indicates substantial disagreement in 44% of pairwise comparisons
- Synthetic query generation relies heavily on LLM judgment without validation against real user query distributions
- Domain-specific nature of Infineon's technical documentation may limit generalizability to other domains

## Confidence
- **High confidence**: BM25 outperforms KNN for document retrieval in this domain, and RAG-Fusion achieves higher completeness than traditional RAG
- **Medium confidence**: The correlation between LLM judgments and human experts is sufficient for meaningful rankings, though with notable disagreement
- **Medium confidence**: RAG-Fusion produces more comprehensive but less precise answers compared to RAG

## Next Checks
1. Test RAGElo's correlation with human judgments across multiple domains beyond technical documentation to assess generalizability
2. Conduct ablation studies varying the number of query variations in RAG-Fusion to find the optimal balance between completeness and precision
3. Compare RAGElo's Elo rankings with alternative evaluation methods like static scoring or human pairwise judgments to validate the ranking mechanism