---
ver: rpa2
title: Improving Actor-Critic Training with Steerable Action-Value Approximation Errors
arxiv_id: '2406.03890'
source_url: https://arxiv.org/abs/2406.03890
tags:
- critic
- actor
- learning
- optimism
- pessimism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a utility-theoretic framework to balance
  optimism and pessimism in actor-critic reinforcement learning. The authors propose
  Utility Soft Actor-Critic (USAC), which uses parametric distributions over action-value
  functions and interpretable hyperparameters to control the degree of optimism or
  pessimism for both the actor and critic.
---

# Improving Actor-Critic Training with Steerable Action-Value Approximation Errors

## Quick Facts
- arXiv ID: 2406.03890
- Source URL: https://arxiv.org/abs/2406.03890
- Reference count: 40
- Five MuJoCo tasks, USAC outperforms SAC/TD3 with appropriate κ tuning

## Executive Summary
This paper introduces Utility Soft Actor-Critic (USAC), a framework that uses parametric distributions over action-value functions to balance optimism and pessimism in actor-critic reinforcement learning. By modeling critic pairs as samples from a distribution (e.g., Laplace) and controlling the degree of optimism/pessimism through interpretable hyperparameters κcritic and κactor, USAC provides task-specific control over exploration-exploitation trade-offs. The framework successfully explains prior findings about the optimism-pessimism trade-off and demonstrates improved performance and stability across five MuJoCo continuous control tasks when hyperparameters are appropriately tuned.

## Method Summary
USAC extends SAC by modeling Q-value estimates as following a parametric distribution (Laplace by default) and using a utility function to combine the mean and scaled standard deviation based on κ parameters. The framework decouples optimism/pessimism control for the actor and critic independently, allowing pessimistic critics for stable policy evaluation and optimistic actors for effective exploration. The utility function U^Q_κ(s,a) = µQ(s,a) + g(κ)σQ(s,a) provides bounded, interpretable parameterization of optimism-pessimism with κ ∈ (-1, 1), where g(κ) maps to different risk measures. Implementation requires estimating mean and standard deviation from twin critics, computing target utilities for TD updates, and maximizing expected utility for policy improvement.

## Key Results
- USAC outperforms SAC, TD3, OAC, and TOP on HalfCheetah-v4 with return of 11,736 ± 297 vs SAC's 10,763 ± 891
- Performance improvements depend on task-specific κ values: HalfCheetah benefits from κactor > 0 while Ant benefits from κactor < 0
- The framework successfully explains prior findings about optimism-pessimism trade-offs using two interpretable scalars
- SAC performance can be recovered using κ ≈ -0.83, which corresponds to min-clipping

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The utility function formulation allows for a continuous, steerable control of optimism and pessimism via two interpretable scalars.
- Mechanism: By modeling the critic pair as samples from a parametric distribution (e.g., Laplace) and using the utility function U^Q_λ(s,a) = (1/λ) log E[exp(λQ(s,a))], the degree of optimism/pessimism can be controlled through λ (or κ in the Laplace case), which maps directly to a spectrum between -1 (full pessimism) and +1 (full optimism).
- Core assumption: The action-value function Q(s,a) can be modeled as following a parametric distribution whose parameters can be estimated from the twin critics.
- Evidence anchors:
  - [abstract] "USAC dynamically adapts its exploration strategy based on the uncertainty of critics using a utility function, enabling a task-specific balance between optimism and pessimism."
  - [section 4] "We use (8) as a measure of pessimism or optimism of the Q distribution: a positive value of λ implies an optimistic view, while a negative value indicates a pessimistic view."
- Break condition: If the assumed parametric distribution does not accurately model the true Q-value uncertainty, the utility function may not provide meaningful control over optimism/pessimism.

### Mechanism 2
- Claim: Decoupling optimism/pessimism control for the actor and critic independently improves both policy evaluation and exploration.
- Mechanism: The framework allows separate utility parameters κcritic and κactor for the critic and actor respectively. A pessimistic critic (κcritic < 0) reduces overestimation bias during policy evaluation, while an optimistic actor (κactor > 0) encourages exploration during policy improvement.
- Core assumption: The critic and actor have complementary roles that benefit from different degrees of optimism/pessimism.
- Evidence anchors:
  - [section 4.2] "This decoupling allows both investigating the relationship between the optimism and pessimism demands of the actor and the critic and tuning them separately, thereby benefiting from their complementary roles in online training."
- Break condition: If the environment requires uniform optimism/pessimism across both actor and critic, the decoupling may not provide additional benefits.

### Mechanism 3
- Claim: The Laplace distribution implementation provides a bounded, interpretable parameterization of optimism/pessimism.
- Mechanism: When Q follows a Laplace distribution, the utility function simplifies to U^Q_κ(s,a) = µQ(s,a) + g(κ)σQ(s,a) where κ ∈ (-1, 1) maps directly to a spectrum of optimism/pessimism, with κ ≈ -0.83 corresponding to min-clipping.
- Core assumption: The Laplace distribution accurately captures the uncertainty in Q-value estimates and has a tractable utility function.
- Evidence anchors:
  - [section 4.3] "Proposition 2. Suppose Q is a Laplace distribution. Then, for any (s, a) ∈ S × A , the utility function of Q can be expressed as U^Q_κ(s,a) = µQ(s,a) + g(κ)σQ(s,a) for κ ∈ (-1, 1)"
- Break condition: If the true Q-value uncertainty is better modeled by a different distribution (e.g., Gaussian), the Laplace parameterization may not provide optimal control.

## Foundational Learning

- Concept: Utility function and its relationship to risk measures
  - Why needed here: Understanding the utility function U^Q_λ(s,a) = (1/λ) log E[exp(λQ(s,a))] is crucial for grasping how optimism/pessimism is controlled in USAC
  - Quick check question: What does a positive λ value in the utility function represent in terms of optimism/pessimism?

- Concept: Distributional reinforcement learning and parametric distributions
  - Why needed here: The framework assumes the Q-function follows a parametric distribution, which is fundamental to how the utility function operates
  - Quick check question: How does modeling Q-values as following a Laplace distribution differ from modeling them as Gaussian in terms of the resulting utility function?

- Concept: Actor-critic architecture and temporal difference learning
  - Why needed here: Understanding how the actor and critic interact, and how TD learning works, is essential for implementing USAC
  - Quick check question: What is the purpose of using twin critics in TD3/SAC, and how does USAC modify this approach?

## Architecture Onboarding

- Component map:
  Twin critics (Q1, Q2) -> Estimate mean and standard deviation of Q-value distribution -> Target critics (Q̄1, Q̄2) -> Delayed versions for stable training
  Actor network -> Outputs parameters for stochastic policy (SquashedGaussian)
  Utility function -> Combines mean and scaled standard deviation based on κ values
  Replay buffer -> Stores transitions for off-policy learning

- Critical path:
  1. Sample transition (s,a,r,s') from replay buffer
  2. Compute target utility using target critics and κcritic
  3. Update twin critics to minimize TD error with target utility
  4. Compute actor utility using twin critics and κactor
  5. Update actor to maximize expected utility
  6. Update target critics via Polyak averaging

- Design tradeoffs:
  - Choice of distribution (Laplace vs Gaussian): Laplace provides bounded κ ∈ (-1,1), while Gaussian allows κ ∈ (-∞,∞)
  - Decoupling actor/critic parameters: Offers flexibility but increases hyperparameter search space
  - Computational overhead: Additional computation for mean and std deviation, but negligible compared to network forward passes

- Failure signatures:
  - Poor performance despite hyperparameter tuning: May indicate distribution assumption is incorrect for the task
  - High variance across seeds: Could suggest instability in the utility function implementation or inappropriate κ values
  - Convergence to suboptimal policies: May indicate insufficient pessimism in critic or excessive optimism in actor

- First 3 experiments:
  1. Implement USAC with κcritic = κactor = -0.83 (equivalent to SAC) and verify performance matches baseline
  2. Test κcritic = -0.83 with varying κactor ∈ {-0.99, 0, 0.99} to observe impact on exploration
  3. Perform coarse grid search on HalfCheetah-v4 with κcritic ∈ {-0.83, -0.5, -0.33} and κactor ∈ {-0.99, 0, 0.99} to find optimal combination

## Open Questions the Paper Calls Out

- How can the utility-theoretic hyperparameters (κcritic and κactor) be automatically tuned during training rather than through a fixed grid search?
- Does the USAC framework generalize beyond the five MuJoCo continuous control environments to other domains such as robotics manipulation, Atari games, or real-world applications?
- How does incorporating higher moments (skewness, kurtosis) of critic distributions beyond mean and variance affect the performance and stability of the USAC framework?

## Limitations
- The assumption that Q-value uncertainty follows a Laplace distribution may not hold for all environments
- The framework lacks theoretical analysis of why the utility function specifically improves learning dynamics
- Hyperparameter sensitivity across tasks suggests extensive tuning may be required for new domains

## Confidence
- **High** for empirical demonstration on tested MuJoCo tasks
- **Medium** for theoretical framework's general applicability
- **Low** for claims about optimal κ values being task-specific without mechanistic explanation

## Next Checks
1. **Distribution Validation**: Conduct experiments comparing USAC with Gaussian vs Laplace distribution assumptions on environments where Q-value uncertainty is known to be non-Laplacian, to verify the distribution choice is critical to performance gains.

2. **Generalization Test**: Evaluate USAC on non-MuJoCo continuous control tasks (e.g., PyBullet, DeepMind Control Suite) with the same κ tuning procedure to assess whether the task-specific hyperparameter sensitivity observed in the paper generalizes.

3. **Sample Efficiency Analysis**: Measure and compare sample efficiency (learning curves) between USAC and baselines during training rather than just final performance, to determine if the utility function provides benefits throughout learning or primarily in final convergence.