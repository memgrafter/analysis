---
ver: rpa2
title: Non-stationary and Sparsely-correlated Multi-output Gaussian Process with Spike-and-Slab
  Prior
arxiv_id: '2409.03149'
source_url: https://arxiv.org/abs/2409.03149
tags:
- data
- non-stationary
- prior
- target
- sources
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a non-stationary multi-output Gaussian process
  (MGP) model with a spike-and-slab prior to handle dynamic and sparse correlations
  among outputs in time-series data. The model uses convolution of time-varying kernel
  functions to capture non-stationarity and applies a dynamic spike-and-slab prior
  to automatically select informative sources while mitigating negative transfer.
---

# Non-stationary and Sparsely-correlated Multi-output Gaussian Process with Spike-and-Slab Prior

## Quick Facts
- arXiv ID: 2409.03149
- Source URL: https://arxiv.org/abs/2409.03149
- Authors: Wang Xinming; Li Yongxiang; Yue Xiaowei; Wu Jianguo
- Reference count: 8
- Introduces a non-stationary MGP model with spike-and-slab prior for dynamic sparse correlations

## Executive Summary
This paper addresses the challenge of modeling non-stationary and sparsely-correlated multi-output time series data. The authors propose a convolution process-based MGP that uses time-varying Gaussian kernels to construct non-stationary covariance functions without requiring intractable numerical integration. A dynamic spike-and-slab prior is applied to correlation parameters to automatically identify informative sources while mitigating negative transfer. The model is trained using an efficient expectation-maximization algorithm and demonstrates superior performance compared to existing methods on both synthetic and real-world datasets.

## Method Summary
The proposed method constructs multi-output Gaussian processes using convolution of time-varying kernel functions, where each output is modeled as a weighted sum of convolutions between independent latent processes and Gaussian kernels with time-varying parameters. The model incorporates a dynamic spike-and-slab prior on correlation parameters to enable automatic source selection and prevent negative transfer. An expectation-maximization algorithm is developed to efficiently estimate all parameters, alternating between computing posterior expectations of binary indicators in the E-step and performing gradient ascent on a regularized likelihood in the M-step.

## Key Results
- Convolution of Gaussian kernels enables closed-form non-stationary covariance functions without numerical integration
- Dynamic spike-and-slab prior automatically selects informative sources while mitigating negative transfer
- EM algorithm provides efficient parameter estimation for time-varying models
- Outperforms existing methods in capturing dynamic sparse correlations and improving prediction accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Convolution of time-varying Gaussian kernels enables non-stationary modeling without intractable integration.
- Mechanism: By using Gaussian kernels and their convolution properties, closed-form covariance functions are derived, avoiding numerical integration while capturing time-varying smoothness and length-scale.
- Core assumption: The latent processes are independent white Gaussian noise; kernel parameters vary smoothly over time.
- Evidence anchors:
  - [section]: "we utilize the Gaussian kernel in Eq. (8) and derive closed-form covariance functions in Equations (10a- 10c)."
  - [abstract]: "covariance functions of MGP are constructed using convolutions of time-varying kernel functions."
  - [corpus]: No direct match, but related literature shows Gaussian convolution processes as standard in non-stationary GP work.
- Break condition: If latent processes are correlated, the independence assumption fails and the covariance matrix loses block-diagonal structure, breaking computational efficiency.

### Mechanism 2
- Claim: Spike-and-slab prior dynamically selects informative sources and prevents negative transfer.
- Mechanism: The binary indicator γi,t controls whether the amplitude parameter αim,t follows a spike prior (pushing toward zero) or a slab prior (allowing smooth variation). The posterior expectation Eγγi,t is updated during EM to adaptively weight shrinkage.
- Core assumption: The slab prior captures smooth correlation changes; the spike prior enforces sparsity.
- Evidence anchors:
  - [section]: "we apply a spike-and-slab prior to the parameters that are related to the sparse correlation between the sources and the target."
  - [abstract]: "dynamic spike-and-slab prior is placed on correlation parameters to automatically decide which sources are informative."
  - [corpus]: Related works (e.g., Wang et al. 2022) use L1 regularization for static selection, but no corpus neighbor explicitly matches dynamic spike-and-slab for MGP.
- Break condition: If the slab variance ν1 is too small, the slab and spike priors become indistinguishable, causing over-shrinkage of non-zero parameters.

### Mechanism 3
- Claim: EM algorithm efficiently estimates a large number of time-varying parameters while avoiding MCMC inefficiency.
- Mechanism: E-step computes posterior expectations of binary indicators γi,t; M-step performs gradient ascent on a regularized Gaussian process likelihood, with regularization weights modulated by Eγγi,t.
- Core assumption: The complete data log posterior is tractable and convex enough for gradient methods to converge.
- Evidence anchors:
  - [section]: "we develop an efficient EM algorithm...instead of directly maximizing the posterior p(Φ|y) = p(Φ(s),Φm|y(s), ym), we proceed iteratively in terms of the complete log posterior log p(Φ, γ|y)."
  - [abstract]: "An expectation-maximization algorithm is developed for efficient parameter estimation."
  - [corpus]: No direct neighbor cites EM for non-stationary MGP, but EM is standard in spike-and-slab regression literature.
- Break condition: If the parameter space is too large or highly multimodal, the EM may converge to poor local optima.

## Foundational Learning

- Concept: Gaussian Process covariance structure and block-partitioning for multi-output data.
  - Why needed here: The MGP model relies on constructing a block-partitioned covariance matrix K(ss), K(sm), Kmm to capture inter-output correlations and enable efficient inversion.
  - Quick check question: Why is K(ss) block-diagonal in this model, and how does that reduce computational complexity compared to a full dense covariance matrix?

- Concept: Convolution processes and their closed-form covariance properties.
  - Why needed here: The model constructs each output as a convolution of latent processes and time-varying kernels, requiring knowledge of how convolution of Gaussian kernels yields analytic covariance forms.
  - Quick check question: Given a Gaussian kernel g(x) = exp(-x²/(2θ)), what is the covariance function of its convolution with another Gaussian kernel?

- Concept: Spike-and-slab priors and EM inference for variable selection.
  - Why needed here: The prior on αim,t uses a spike (Laplace) and slab (Gaussian) mixture, and the EM algorithm iteratively updates the posterior of the binary indicator γi,t.
  - Quick check question: In the E-step, how is Eγγi,t computed from the current estimate of αim,t and the spike/slab densities?

## Architecture Onboarding

- Component map:
  - Latent processes {zj(x)}h j=1 (independent white noise) -> Time-varying kernel functions gij,t(x) (Gaussian form) -> Amplitude parameters αij,t (controlled by spike-and-slab prior) -> Length-scale parameters θij,t (modeled by slab prior only) -> EM optimization loop (E-step γi,t posterior, M-step gradient ascent on Φ) -> Covariance construction (block-diagonal K(ss) + cross-covariance K(sm) + target covariance Kmm)

- Critical path:
  1. Initialize Φ(s) by maximizing sum of source marginal likelihoods.
  2. Iterate E-step: update Eγγi,t via Eq. (21).
  3. Iterate M-step: compute K(ss), K(sm), Kmm, evaluate objective, apply ADAM update.
  4. Repeat until convergence.
  5. Forecast/recover at new time points using estimated αm,t*, θm,t*.

- Design tradeoffs:
  - Convolution kernels vs. direct covariance specification: Convolution yields closed forms but assumes specific kernel types.
  - Spike-and-slab vs. L1 regularization: Spike-and-slab provides adaptive shrinkage and uncertainty quantification but requires EM inference; L1 is simpler but static.
  - EM vs. MCMC: EM is faster but point-estimate only; MCMC captures full posterior but is computationally heavier.

- Failure signatures:
  - If Eγγi,t stays near 0.5 for many parameters, the spike/slab separation is weak (ν0 too large or ν1 too small).
  - If covariance matrix inversion fails, likely due to numerical instability (ill-conditioned θij,t).
  - If prediction accuracy does not improve over static MGP, check if γi,t actually drives sparsity or if slab prior is too restrictive.

- First 3 experiments:
  1. Simulate Case 1 (piecewise constant correlation) with k=1 source per type; compare DMGP-SS vs. GP and MGP-L1 on MAE.
  2. Vary k=4 to test scalability; measure runtime and accuracy trade-off.
  3. Implement missing data recovery at random t* and evaluate CRPS vs. baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed non-stationary MGP model perform on extremely large datasets with millions of data points, and what computational optimizations could be developed to handle such scale efficiently?
- Basis in paper: [explicit] The paper mentions computational challenges in handling large datasets and suggests potential solutions like sparse approximations or more efficient optimization algorithms.
- Why unresolved: The current implementation and experiments focus on moderate-sized datasets, leaving the model's scalability to truly large-scale problems unexplored.
- What evidence would resolve it: Benchmarking the model on datasets with millions of points, comparing its performance and computational efficiency against other large-scale GP methods, and demonstrating the effectiveness of proposed optimizations.

### Open Question 2
- Question: How does the model's performance change when using different types of spike-and-slab priors (e.g., point mass, Laplace, or Gaussian with varying variances) for the sparse correlation parameters, and what is the optimal choice for different types of data?
- Basis in paper: [explicit] The paper uses a Laplace distribution for the spike prior and explores both hard and soft slab priors, but does not systematically compare different prior choices.
- Why unresolved: The impact of different prior choices on model performance and interpretability is not fully explored, leaving open the question of which prior is most suitable for various data characteristics.
- What evidence would resolve it: Conducting experiments comparing different spike-and-slab prior combinations on diverse datasets and analyzing their effects on model accuracy, sparsity, and computational efficiency.

### Open Question 3
- Question: How does the proposed model handle online learning scenarios where new data arrives sequentially, and what modifications to the EM algorithm would be necessary for efficient incremental updates?
- Basis in paper: [explicit] The paper mentions the potential extension to online reinforcement learning tasks but does not provide a concrete online learning framework.
- Why unresolved: The current EM algorithm is designed for batch learning, and its adaptation to online settings with streaming data is not addressed.
- What evidence would resolve it: Developing and testing an online version of the EM algorithm that can efficiently update model parameters as new data arrives, and evaluating its performance on online learning benchmarks.

## Limitations
- Independence assumption of latent processes is critical for computational efficiency
- Spike-and-slab separation depends heavily on hyperparameter choice (ν0, ν1)
- EM convergence to global optimum not guaranteed in highly multimodal settings

## Confidence

**High**: Convolution of Gaussian kernels yields closed-form covariances (well-established in GP literature).
**Medium**: Spike-and-slab EM converges and selects sparse sources (depends on tuning and initialization).
**Low**: Negative transfer is consistently avoided in all scenarios (only one real-data example provided).

## Next Checks

1. Test sensitivity of correlation recovery to ν0/ν1 values by sweeping across orders of magnitude.
2. Compare EM vs. full Bayesian MCMC on a small dataset to quantify approximation error.
3. Introduce correlated latent processes and measure the degradation in computational efficiency and accuracy.