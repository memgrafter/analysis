---
ver: rpa2
title: 'Checkmating One, by Using Many: Combining Mixture of Experts with MCTS to
  Improve in Chess'
arxiv_id: '2401.16852'
source_url: https://arxiv.org/abs/2401.16852
tags:
- learning
- test
- phase
- training
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of adapting chess engines to\
  \ the varying strategic demands of different game phases\u2014opening, middlegame,\
  \ and endgame. Current engines rely on a single neural network for all phases, which\
  \ can lead to data imbalance and suboptimal performance, especially in underrepresented\
  \ phases."
---

# Checkmating One, by Using Many: Combining Mixture of Experts with MCTS to Improve in Chess

## Quick Facts
- arXiv ID: 2401.16852
- Source URL: https://arxiv.org/abs/2401.16852
- Reference count: 40
- Primary result: M2CTS achieves up to +122 Elo points over standard single-model baselines in chess.

## Executive Summary
This paper introduces M2CTS, a modular framework that combines Mixture of Experts (MoE) with Monte Carlo Tree Search (MCTS) to improve chess engine performance by handling different game phases—opening, middlegame, and endgame—more effectively. By training specialized neural networks for each phase and dynamically selecting them during search using a rule-based gating mechanism, M2CTS addresses the limitations of single-model chess engines, which often struggle with data imbalance across phases. Experimental results show significant Elo gains, particularly when using separated learning strategies under sufficient data conditions.

## Method Summary
The authors propose M2CTS, a framework that integrates Mixture of Experts with MCTS to specialize neural networks for different chess game phases. Three training strategies are explored: Separated Learning (each expert trained only on its phase data), Staged Learning (sequential training across phases), and Weighted Learning (joint training with phase-specific weights). A rule-based gating mechanism based on Lichess phase definitions determines which expert to use during search. The method is evaluated on the KingBase Lite 2019 dataset and tested in both supervised and multi-agent settings.

## Key Results
- M2CTS with separated learning achieves up to +122 Elo points over single-model baselines in chess.
- Separated learning outperforms staged and weighted learning when sufficient data is available.
- M2CTS generalizes to multi-agent domains like Pommerman, yielding up to 11% improvement in win rate.
- Simple dataset splitting outperforms complex training strategies in supervised settings when data per phase is sufficient.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Phase-specific expert networks in M2CTS reduce task interference and improve generalization compared to a single monolithic model.
- Mechanism: By training separate neural networks for each game phase (opening, middlegame, endgame), the model avoids conflating distinct strategic patterns. During MCTS search, a gating mechanism selects the appropriate expert based on real-time board state analysis, ensuring that each phase is evaluated with its dedicated knowledge.
- Core assumption: Each chess phase has sufficiently distinct strategic characteristics that specialized training yields better performance than joint training.
- Evidence anchors:
  - [abstract] "By routing decisions through specialized neural networks trained for each phase, M2CTS improves both computational efficiency and playing strength."
  - [section II-C] "This method’s simplicity allows parallelized training across experts, leading to significant reductions in training time."
  - [corpus] Weak; no direct neighbor citations support phase-specialization in chess engines.
- Break condition: If phase transitions are ambiguous or strategic characteristics overlap significantly, expert switching may not improve performance.

### Mechanism 2
- Claim: Separated learning outperforms more complex training strategies (staged and weighted learning) when sufficient data is available.
- Mechanism: Training each expert only on data from its corresponding phase eliminates cross-phase interference and simplifies optimization. This results in faster convergence and better phase-specific accuracy than strategies that attempt to balance or sequence training across phases.
- Core assumption: Sufficient data per phase exists to train effective specialists without catastrophic forgetting or overfitting.
- Evidence anchors:
  - [section III-C] "In the supervised setting, separated learning — where each expert is trained only on data from its respective phase — consistently achieves the highest Elo."
  - [section III-D] "M2CTS with separated learning requires sufficient data to outperform MCTS in separated learning."
  - [corpus] Weak; no direct neighbor citations discuss dataset sufficiency for phase-specific learning.
- Break condition: If data per phase is limited, experts may overfit and performance degrades relative to a single general model.

### Mechanism 3
- Claim: The choice of gating mechanism significantly impacts M2CTS performance; domain-informed phase definitions outperform naive temporal splits.
- Mechanism: The gating mechanism uses Lichess-defined criteria (piece counts, backrank sparseness, mixedness score) to assign phases. This semantic segmentation aligns expert selection with meaningful strategic transitions, whereas simple move-count splits ignore game dynamics and reduce expert utility.
- Core assumption: Chess phases can be defined in a way that meaningfully reflects strategic shifts in the game.
- Evidence anchors:
  - [section II-A] "To classify the game phase, we utilize the well-defined phase definitions from Lichess... These definitions consider factors such as piece count, board complexity, and back-rank vulnerability."
  - [section III-C] "Our analysis shows that simplistic approaches, such as relying only on move counts, lead to suboptimal phase categorization."
  - [corpus] Weak; no direct neighbor citations validate Lichess-style gating over simpler heuristics.
- Break condition: If phase definitions are too rigid or fail to capture real game dynamics, gating errors propagate through the search tree.

## Foundational Learning

- Concept: Supervised learning of phase-specific policies and values
  - Why needed here: M2CTS relies on expert networks trained on labeled chess positions to evaluate board states during MCTS search.
  - Quick check question: What is the loss function used to train each expert network in M2CTS?

- Concept: Monte Carlo Tree Search (MCTS) with neural network guidance
  - Why needed here: M2CTS integrates MoE into AlphaZero-style MCTS, using expert networks for policy and value prediction at each node.
  - Quick check question: How does M2CTS adapt the standard MCTS simulation phase to accommodate batch processing with MoE?

- Concept: Phase-aware gating mechanisms
  - Why needed here: The gating system must accurately classify game states into phases to activate the correct expert network.
  - Quick check question: Which three criteria from Lichess are used to determine the endgame phase?

## Architecture Onboarding

- Component map: Expert networks (opening, middlegame, endgame) → Gating mechanism (Lichess phase definitions) → MCTS search with PUCT → Neural network heads (policy/value)
- Critical path: Board state → Gating → Expert selection → Forward pass → PUCT update → Tree traversal → Backpropagation
- Design tradeoffs: Modular specialization vs. memory overhead; simple gating vs. learned gating; batch processing efficiency vs. per-phase accuracy
- Failure signatures: Low phase accuracy in gating → Wrong expert selection; data imbalance → Poor expert performance in underrepresented phases; insufficient data → overfitting or underperformance
- First 3 experiments:
  1. Train a single monolithic network and baseline M2CTS with separated learning on full KingBase Lite 2019 dataset; measure Elo difference.
  2. Train M2CTS with each learning strategy (separated, staged, weighted) and compare performance across batch sizes.
  3. Evaluate expert impact by running ablation tournaments where only one expert is active per phase while others use baseline model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the performance gap between M2CTS and Stockfish be closed by integrating M2CTS into Stockfish’s architecture?
- Basis in paper: [explicit] The paper compares M2CTS against Stockfish but notes that M2CTS does not yet surpass Stockfish’s performance.
- Why unresolved: The paper evaluates M2CTS as a standalone system but does not explore hybrid approaches combining M2CTS with traditional engine components like Stockfish’s handcrafted heuristics.
- What evidence would resolve it: Experimental results showing Elo gains when integrating M2CTS’s phase-specific experts into Stockfish’s evaluation function or search framework.

### Open Question 2
- Question: Does the effectiveness of separated learning depend on the richness of phase-specific data, or could it fail under more extreme data imbalance?
- Basis in paper: [inferred] Separated learning performs best under sufficient data conditions, but the paper does not test scenarios with extreme phase data scarcity.
- Why unresolved: The paper demonstrates separated learning’s superiority with the current dataset but does not explore performance degradation under severe data imbalance (e.g., opening data reduced to 1% of total).
- What evidence would resolve it: Controlled experiments varying the proportion of training data per phase and measuring Elo impact on M2CTS variants.

### Open Question 3
- Question: How does the choice of gating mechanism affect generalization to domains without clear phase boundaries?
- Basis in paper: [explicit] The paper uses rule-based gating based on chess theory but acknowledges limitations for domains lacking similar structures.
- Why unresolved: The paper only tests rule-based gating in chess and Pommerman, without exploring learned gating mechanisms or domains with ambiguous or overlapping phases.
- What evidence would resolve it: Comparative experiments using learned gating networks versus rule-based gating across diverse domains with varying phase clarity.

## Limitations
- The gating mechanism's effectiveness depends on the accuracy of Lichess phase definitions, which are not empirically validated within the paper.
- Data imbalance across phases is noted but not quantitatively addressed; this could lead to overfitting in underrepresented phases.
- Memory usage and computational costs for training multiple expert networks are acknowledged but not thoroughly analyzed.

## Confidence
- **High**: The core concept of using MoE for phase-specific specialization is supported by clear theoretical reasoning and experimental evidence.
- **Medium**: The experimental results and Elo gains are plausible but depend on factors not fully disclosed (e.g., exact architecture, hyperparameters).
- **Low**: The claim that separated learning outperforms other strategies in all scenarios is qualified and depends on data availability, making it less robust.

## Next Checks
1. **Phase Accuracy Validation**: Measure the accuracy of the Lichess-based gating mechanism on a held-out dataset to ensure reliable expert selection.
2. **Data Sufficiency Analysis**: Analyze the impact of varying dataset sizes per phase to quantify the risk of overfitting or underperformance in data-limited scenarios.
3. **Generalization to Other Games**: Test the M2CTS framework on a simpler game (e.g., Tic-Tac-Toe or Connect Four) to verify the robustness of the phase-specialization approach.