---
ver: rpa2
title: Planning In Natural Language Improves LLM Search For Code Generation
arxiv_id: '2409.03733'
source_url: https://arxiv.org/abs/2409.03733
tags:
- pass
- search
- sampling
- repeated
- idea
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving large language
  model (LLM) performance in code generation through better search strategies at inference
  time. The key insight is that traditional search methods suffer from low diversity
  in model outputs, leading to inefficient exploration of potential solutions.
---

# Planning In Natural Language Improves LLM Search For Code Generation

## Quick Facts
- arXiv ID: 2409.03733
- Source URL: https://arxiv.org/abs/2409.03733
- Reference count: 40
- Primary result: PlanSearch achieves 77.0% pass@200 on LiveCodeBench using Claude 3.5 Sonnet, outperforming standard repeated sampling (60.6%) and best score without search (41.4%)

## Executive Summary
This paper addresses the challenge of improving large language model (LLM) performance in code generation through better search strategies at inference time. The key insight is that traditional search methods suffer from low diversity in model outputs, leading to inefficient exploration of potential solutions. The authors propose PlanSearch, a novel approach that searches over natural language plans rather than directly over code, generating diverse observations about problems, combining these into candidate plans, and then converting these plans into executable code.

The method demonstrates strong empirical results, achieving a pass@200 score of 77.0% on LiveCodeBench when applied to Claude 3.5 Sonnet, outperforming both the best score without search (41.4%) and standard repeated sampling (60.6%). Similar improvements are observed across multiple benchmarks including HumanEval+ and MBPP+. The paper also establishes a correlation between idea diversity and performance gains, showing that diversity in the natural language idea space is predictive of search effectiveness.

## Method Summary
PlanSearch is a novel search algorithm for code generation that operates in natural language idea space rather than directly over code. The method works by first generating diverse observations about a problem, then combining these observations into candidate plans using subset combinations, converting these plans into natural language ideas, and finally translating these ideas into executable code through pseudocode and code generation steps. The approach uses temperature sampling (0.9 for most models) to encourage diversity and employs an LLM-based pairwise comparison method to measure diversity of generated solutions. The algorithm explores a significantly more diverse range of potential solutions compared to baseline search methods that search over tokens, lines of code, or entire programs.

## Key Results
- PlanSearch achieves 77.0% pass@200 on LiveCodeBench with Claude 3.5 Sonnet, outperforming best score without search (41.4%) and repeated sampling (60.6%)
- The method shows consistent improvements across multiple benchmarks: HumanEval+ and MBPP+
- Diversity in the idea space is highly correlated with performance gains, with higher diversity scores predicting better search effectiveness
- PlanSearch demonstrates that searching over natural language plans provides more diverse exploration than searching over code directly

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Searching over natural language plans rather than directly over code solutions increases diversity of explored solutions.
- **Mechanism**: PlanSearch generates diverse observations about a problem, combines these observations into candidate plans, and then converts these plans into executable code. This approach explores a significantly more diverse range of potential solutions compared to baseline search methods that search over tokens, lines of code, or entire programs.
- **Core assumption**: The correct direction of search is to explore through idea space to maximize the chance of arriving at a correct idea, as having a correct sketch is sufficient to produce the correct final solution with relatively high accuracy.
- **Evidence anchors**:
  - [abstract]: "PlanSearch generates a diverse set of observations about the problem and then uses these observations to construct plans for solving the problem. By searching over plans in natural language rather than directly over code solutions, PlanSearch explores a significantly more diverse range of potential solutions compared to baseline search methods."
  - [section 3.2]: "We observe that prompting a model with a backtranslated idea significantly improves accuracy, increasing with the length of the translated idea... This suggests that having a correct sketch is sufficient to produce the correct final solution with relatively high accuracy, even only after 10 tokens of backtranslated solution."
  - [corpus]: The corpus contains several papers on planning and search with LLMs, supporting the general approach of using higher-level abstractions for search.

### Mechanism 2
- **Claim**: The lack of diversity in model outputs leads to inefficient search due to models repeatedly sampling highly similar, yet incorrect generations.
- **Mechanism**: By searching over natural language plans, PlanSearch introduces high-level diversity that breaks the pattern of generating similar incorrect solutions. This is achieved by first generating observations, then combining these observations into plans, which are then translated into code.
- **Core assumption**: Models optimized for producing single correct answers (as chatbots) have reduced diversity when generating multiple samples, leading to repeated similar incorrect outputs.
- **Evidence anchors**:
  - [abstract]: "We hypothesize that a core missing component is a lack of diverse LLM outputs, leading to inefficient search due to models repeatedly sampling highly similar, yet incorrect generations."
  - [section 3.1]: "We hypothesize that the key factor is obtaining the correct solution sketch... Prior work has observed strong positive effects from being allowed to conduct such reasoning in natural language, making it a natural place to search over."
  - [section 6.1]: "We find that diversity as measured in idea space is highly predictive of search performance, as measured by the relative improvement between a model/method's pass@1 and its pass@200."

### Mechanism 3
- **Claim**: The diversity of output code over the idea space is highly correlated with the performance gains generated by that search method.
- **Mechanism**: PlanSearch increases diversity in the idea space, which translates to more diverse code outputs. This diversity is measured by comparing generated programs pairwise and calculating a diversity score. Higher diversity scores correlate with better performance gains.
- **Core assumption**: Diversity in the idea space directly translates to diversity in the code space, and this diversity is what enables more effective search.
- **Evidence anchors**:
  - [abstract]: "Finally, we measure the diversity of output code over the idea space of all search methods via an LLM-as-a-judge procedure... and show that the resulting diversity score is highly correlated with the performance gains generated by that search method."
  - [section 6.1]: "Our results suggest that both PlanSearch and IdeaSearch outperform basic sampling by a wide margin... Investigating the differences in specific models, we notice that trends exhibited by the pass@k curves are not uniform across all models; in fact, each curve seems unique. We hypothesize that these differences are in part due to changes in idea diversity."
  - [corpus]: The corpus contains papers on diversity in language generation, supporting the general principle that diversity is important for search effectiveness.

## Foundational Learning

- **Concept**: Backtranslation of code solutions to natural language descriptions
  - **Why needed here**: Understanding backtranslation is crucial for grasping how PlanSearch verifies that having correct sketches leads to correct solutions. The paper uses backtranslation to show that providing correct natural language descriptions significantly improves code generation accuracy.
  - **Quick check question**: If a model can correctly implement code when given a correct natural language description, what does this suggest about the relationship between idea space and code space?

- **Concept**: Diversity measurement in idea space
  - **Why needed here**: The diversity score calculation is central to understanding why PlanSearch works. It measures how different the ideas behind generated programs are from each other, which is key to the paper's claim that diversity drives performance.
  - **Quick check question**: How does the diversity score differ from simple entropy measures, and why is this distinction important for evaluating search methods?

- **Concept**: Search algorithms and their relationship to diversity
  - **Why needed here**: Understanding different search approaches (repeated sampling, beam search, plan-based search) helps contextualize PlanSearch's innovation. The paper contrasts PlanSearch with existing methods to highlight its diversity advantage.
  - **Quick check question**: Why might searching over natural language plans be more effective than searching over individual tokens or lines of code?

## Architecture Onboarding

- **Component map**: Observation Generation -> Observation Combination -> Idea Generation -> Code Translation
- **Critical path**: Observation Generation → Observation Combination → Idea Generation → Code Translation. Each step builds on the previous one to create diverse, executable solutions.
- **Design tradeoffs**:
  - Depth vs. Breadth: The paper uses L=2 layers of observations to balance computational cost with diversity gains
  - Subset Size (S): Set to 2 to maximize diversity while keeping the number of combinations manageable
  - Temperature: Set to 0.9/0.95 to encourage diversity without excessive randomness
- **Failure signatures**:
  - Low diversity scores despite multiple generations indicate the observation generation or combination steps aren't creating meaningful differences
  - Pass@k curves that plateau early suggest the diversity isn't translating to better solutions
  - High refusal rates from models (especially noted with o1-mini) indicate the prompts may be triggering safety filters
- **First 3 experiments**:
  1. **Backtranslation validation**: Test if providing correct natural language descriptions improves code generation accuracy across different description lengths
  2. **Diversity correlation**: Measure diversity scores for different search methods and correlate with performance gains on a small benchmark
  3. **Ablation study**: Test PlanSearch with different values of S (subset size) and L (observation layers) to find optimal parameters for a specific model-dataset combination

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- The paper relies on LLM-as-a-judge for diversity measurement, which introduces uncertainty about the validity of claimed diversity-performance correlations
- Ablation studies for optimal parameters (S and L) are limited to specific model-dataset combinations, raising questions about generalizability
- The mechanism explaining why plan-based search inherently produces more diversity than alternative approaches lacks rigorous theoretical grounding

## Confidence
**High Confidence**: The empirical results demonstrating PlanSearch's superiority over baseline methods on multiple benchmarks are robust and clearly presented.

**Medium Confidence**: The claim that diversity in idea space predicts search performance is supported by correlation analysis, but the causal relationship remains unclear.

**Low Confidence**: The mechanism explaining why searching over natural language plans specifically creates more diversity than other search approaches lacks rigorous theoretical grounding.

## Next Checks
1. **Diversity Measurement Validation**: Conduct a human evaluation study comparing the LLM-as-a-judge diversity scores against human judgments of program similarity. Have 3-5 human annotators independently rate pairs of programs for similarity, then compute the correlation between human scores and LLM scores to validate the measurement methodology.

2. **Causal Diversity Experiment**: Design an experiment that artificially constrains diversity at different stages of PlanSearch (e.g., by forcing observation generation to produce similar observations) while keeping other components constant. If diversity is truly causal, these constraints should reduce performance even when the final output appears similar to unconstrained runs.

3. **Cross-Domain Generalization Test**: Apply PlanSearch to a non-code generation domain (such as mathematical problem solving or creative writing) using the same parameter settings (S=2, L=2) as the main experiments. Compare the performance and diversity gains to baseline methods to assess whether the approach generalizes beyond code generation tasks.