---
ver: rpa2
title: Efficient LLM-Jailbreaking via Multimodal-LLM Jailbreak
arxiv_id: '2405.20015'
source_url: https://arxiv.org/abs/2405.20015
tags:
- jailbreak
- arxiv
- jailbreaking
- mllm
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an efficient multimodal LLM jailbreak method
  by first constructing a multimodal LLM built upon the target LLM and then performing
  jailbreak on it. Compared to direct LLM jailbreak, the proposed method is more efficient
  because MLLMs are more vulnerable to jailbreak.
---

# Efficient LLM-Jailbreaking via Multimodal-LLM Jailbreak

## Quick Facts
- arXiv ID: 2405.20015
- Source URL: https://arxiv.org/abs/2405.20015
- Authors: Haoxuan Ji; Zheng Lin; Zhenxing Niu; Xinbo Gao; Gang Hua
- Reference count: 8
- One-line primary result: Outperforms current state-of-the-art jailbreak methods in both efficiency and effectiveness, requiring only ~3% of GCG's runtime

## Executive Summary
This paper proposes an innovative approach to LLM jailbreaking that leverages multimodal models to achieve greater efficiency and effectiveness compared to direct text-based methods. The key insight is that by first constructing a multimodal LLM (MLLM) that incorporates a visual module, and then performing jailbreak on this MLLM, the attack becomes more successful because MLLMs are inherently more vulnerable to jailbreak attacks than pure LLMs. The method converts the jailbreaking embedding back to text using a de-embedding and de-tokenization process, achieving superior cross-class generalization ability while requiring significantly less computational resources.

## Method Summary
The proposed method constructs an MLLM by integrating a visual module (typically CLIP) into the target LLM, then performs jailbreak optimization on this MLLM using projected gradient descent with a maximum likelihood objective. The jailbreaking process operates in the continuous pixel space, making it more efficient than discrete token optimization. After obtaining the jailbreaking embedding (JBemb), the method converts it back to textual form through de-embedding and de-tokenization operations. An image-text semantic matching scheme is used to find appropriate initial inputs that maximize CLIP similarity with harmful query embeddings, improving attack success rates.

## Key Results
- Achieves higher attack success rates than state-of-the-art methods across nine harmful behavior classes
- Requires only approximately 3% of the runtime compared to GCG, demonstrating significant efficiency gains
- Exhibits superior cross-class generalization ability, with jailbreaking suffixes trained on one class successfully transferring to other classes
- Shows strong black-box jailbreaking performance against models like Mistral-v0.3, Gemma2, and Deepseek-R1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multimodal approach increases jailbreak vulnerability by exploiting the continuous optimization space of images rather than discrete token spaces.
- Mechanism: By constructing an MLLM that includes a visual module, the jailbreak operates in the continuous pixel space where gradient-based optimization is more flexible and efficient than discrete text token optimization.
- Core assumption: MLLMs are inherently more vulnerable to jailbreak attacks than pure LLMs due to the nature of continuous optimization spaces.
- Evidence anchors:
  - [abstract] "Compared to the direct LLM-jailbreak methods, our indirect jailbreaking approach is more efficient, as MLLMs are more vulnerable to jailbreak than pure LLMs."
  - [section] "finding jailbreaking images within continuous pixel spaces is significantly easier and more flexible than identifying jailbreaking text within discrete token spaces"
- Break condition: If the visual module's output embeddings do not map well to valid textual tokens, the conversion process fails and the jailbreak becomes ineffective.

### Mechanism 2
- Claim: The visual module acts as a regularizer that ensures jailbreaking embeddings correspond to valid tokens.
- Mechanism: The visual module (e.g., CLIP encoder) is trained with image-text alignment objectives, which constrains the jailbreaking embeddings to remain within a distribution that can be converted back to natural language text.
- Core assumption: Visual modules trained on image-text pairs produce embeddings that maintain semantic coherence with textual representations.
- Evidence anchors:
  - [abstract] "Compared to token-based methods such as GCG, these approaches are more efficient, as they rely on continuous optimization. However, previous embedding-based jailbreaks have been shown to be largely ineffective, primarily because the obtained embeddings often lack corresponding tokens"
  - [section] "Since the visual module (e.g.,a CLIP encoder) is trained with an image-text alignment objective, it increases the likelihood that our JBemb corresponds to valid tokens"
- Break condition: If the visual module's architecture changes or the training data distribution shifts significantly, the embedding-to-token mapping may break.

### Mechanism 3
- Claim: Image-text semantic matching improves attack success by aligning jailbreak inputs with harmful query semantics.
- Mechanism: The proposed matching scheme optimizes the initial jailbreak image to maximize CLIP similarity with harmful query embeddings, ensuring the input strongly influences the model's cross-attention mechanism to produce affirmative responses.
- Core assumption: Higher semantic alignment between visual and textual inputs increases the likelihood of successful jailbreak through enhanced cross-attention influence.
- Evidence anchors:
  - [abstract] "to improve the attack success rate of jailbreak, we propose an image-text semantic matching scheme to identify a suitable initial input"
  - [section] "our image-text semantic matching scheme significantly outperforms both baselines, yielding substantially higher CLIP scores"
- Break condition: If the CLIP model's embedding space changes or the semantic relationships between harmful queries and images become less predictable, the matching scheme's effectiveness diminishes.

## Foundational Learning

- Concept: Continuous vs discrete optimization spaces
  - Why needed here: Understanding why image-based jailbreak is more efficient than text-based approaches requires grasping the fundamental difference between optimizing in continuous pixel spaces versus discrete token spaces.
  - Quick check question: What makes gradient-based optimization easier in continuous spaces compared to discrete token spaces?

- Concept: Cross-modal alignment and attention mechanisms
  - Why needed here: The jailbreak success depends on the visual input influencing the LLM's text generation through cross-attention mechanisms, requiring understanding of how multimodal models process and align different input modalities.
  - Quick check question: How does cross-attention between visual and textual inputs enable the jailbreak image to influence the LLM's response?

- Concept: Embedding space regularization and token mapping
  - Why needed here: The effectiveness of converting jailbreaking embeddings back to text depends on understanding how visual modules constrain embeddings to remain within token-distributable spaces.
  - Quick check question: Why do visual modules trained on image-text pairs produce embeddings that map more reliably to valid tokens than directly optimized text embeddings?

## Architecture Onboarding

- Component map: Target LLM → Visual module integration → MLLM construction → MLLM jailbreak optimization → JBemb extraction → De-embedding/De-tokenization → JBtxt generation → LLM jailbreak execution
- Critical path: MLLM construction → JBinit optimization → MLLM jailbreak → JBemb → JBtxt → LLM jailbreak
- Design tradeoffs: Using pre-trained MLLM pairs provides speed but limits customization; building custom MLLMs offers more control but requires significant computational resources
- Failure signatures: Low ASR indicates issues with either JBinit selection, embedding-to-token conversion quality, or insufficient alignment between MLLM and target LLM
- First 3 experiments:
  1. Test different JBinit initialization schemes (random, ranking-based, semantic matching) on a single harmful class to measure impact on ASR
  2. Compare conversion quality by measuring cosine similarity between re-embedded JBtxt and original JBemb across different visual modules
  3. Evaluate cross-class generalization by training JBtxt on one class and testing on all other classes to identify transferability patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed indirect jailbreaking approach compare in terms of computational efficiency and effectiveness when targeting proprietary LLMs with unknown architectures and training data?
- Basis in paper: [explicit] The paper mentions that the approach exhibits strong black-box jailbreaking performance against Mistral-v0.3, Gemma2, and Deepseek-R1, but only moderate efficacy against proprietary models like Grok-3 and DouBao1.5.
- Why unresolved: The paper does not provide a detailed analysis of the computational efficiency and effectiveness of the approach when targeting proprietary LLMs with unknown architectures and training data. It also does not explore the potential challenges and limitations of applying the approach to such models.
- What evidence would resolve it: A comprehensive study comparing the computational efficiency and effectiveness of the approach when targeting both open-source and proprietary LLMs with varying architectures and training data would provide insights into its generalizability and limitations.

### Open Question 2
- Question: What are the potential risks and ethical implications of developing and deploying jailbreaking techniques, and how can they be mitigated?
- Basis in paper: [explicit] The paper focuses on developing an efficient and effective jailbreaking technique but does not discuss the potential risks and ethical implications of its use.
- Why unresolved: The paper does not address the potential risks and ethical implications of developing and deploying jailbreaking techniques, such as the potential for misuse or the impact on the safety and reliability of LLMs.
- What evidence would resolve it: A thorough analysis of the potential risks and ethical implications of jailbreaking techniques, along with proposed mitigation strategies, would help to ensure their responsible development and deployment.

### Open Question 3
- Question: How does the proposed approach compare to other jailbreaking techniques in terms of its ability to generalize across different types of harmful behaviors and LLMs?
- Basis in paper: [explicit] The paper mentions that the approach exhibits superior cross-class generalization ability and has been evaluated on various harmful behavior classes and LLMs.
- Why unresolved: The paper does not provide a comprehensive comparison of the approach's generalization ability with other jailbreaking techniques, such as token-based or embedding-based methods, across different types of harmful behaviors and LLMs.
- What evidence would resolve it: A systematic comparison of the approach's generalization ability with other jailbreaking techniques across a wide range of harmful behavior classes and LLMs would provide insights into its strengths and limitations.

## Limitations

- The method's effectiveness depends on finding compatible MLLM pairs where the visual module can successfully bridge the embedding-to-token conversion gap
- Scalability to different LLM architectures beyond LLaMA2 remains uncertain and may degrade due to embedding space mismatches
- The cross-model semantic alignment between CLIP embeddings and LLM's understanding of harmful content is not guaranteed and may fail for certain queries

## Confidence

**High Confidence:**
- The efficiency claim (3% runtime of GCG) is well-supported by experimental results
- The cross-class generalization ability is demonstrated through systematic evaluation

**Medium Confidence:**
- The claim that MLLMs are inherently more vulnerable to jailbreak than pure LLMs is supported by experimental evidence but lacks theoretical guarantees
- The effectiveness of the visual module as a regularizer for valid token mapping is demonstrated but not extensively validated across different architectures

**Low Confidence:**
- The generalizability to LLM architectures beyond LLaMA2 remains uncertain without additional testing
- The robustness against potential defenses or model fine-tuning is not evaluated

## Next Checks

1. **Cross-Architecture Validation**: Test the proposed jailbreak method against multiple LLM architectures (GPT, Claude, Mistral) to evaluate whether the MLLM vulnerability advantage holds across different embedding spaces and tokenization schemes.

2. **Visual Module Ablation Study**: Systematically evaluate the impact of different visual modules (CLIP, BLIP, SigLIP) on jailbreak success rates to determine whether the vulnerability advantage is specific to certain visual architectures or a general property of MLLMs.

3. **Embedding Space Alignment Analysis**: Measure and analyze the alignment between MLLM and target LLM embedding spaces using quantitative metrics (cosine similarity, Procrustes distance) to identify when the jailbreak pipeline is likely to fail due to embedding space mismatches.