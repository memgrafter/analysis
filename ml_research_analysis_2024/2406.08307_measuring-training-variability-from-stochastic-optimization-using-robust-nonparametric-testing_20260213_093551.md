---
ver: rpa2
title: Measuring training variability from stochastic optimization using robust nonparametric
  testing
arxiv_id: '2406.08307'
source_url: https://arxiv.org/abs/2406.08307
tags:
- ensemble
- accuracy
- test
- validation
- candidate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a framework to measure the variability of\
  \ deep neural network training using robust nonparametric testing. The key idea\
  \ is to use the logit gap distribution from trained models and apply a novel robust\
  \ Kolmogorov-Smirnov test with \u03B1-trimming to compare models."
---

# Measuring training variability from stochastic optimization using robust nonparametric testing

## Quick Facts
- arXiv ID: 2406.08307
- Source URL: https://arxiv.org/abs/2406.08307
- Reference count: 37
- This paper proposes a framework to measure DNN training variability using robust nonparametric testing with logit gap distributions and α-trimming.

## Executive Summary
This paper introduces a novel framework for quantifying variability in deep neural network training caused by stochastic optimization. The key innovation is using robust Kolmogorov-Smirnov testing with α-trimming to compare the logit gap distributions of trained models. The framework measures how representative a model is of the training process through the α-trimming level, which quantifies the minimum fraction of samples that must be trimmed to accept the model as close to a reference ensemble. Experiments on CIFAR-10 demonstrate that this metric provides more nuanced information about model quality than validation accuracy alone, particularly for identifying models that are closer to the expected distribution of the training process.

## Method Summary
The framework trains multiple models with different random seeds and computes their logit gap distributions on a test set. A reference function is formed by averaging the empirical cumulative distribution functions (eCDFs) of these models. For each candidate model, a robust Kolmogorov-Smirnov test with α-trimming is applied to estimate how close it is to the reference. The method trims a fraction α of the candidate's eCDF samples to minimize the L∞ distance to the reference, accepting models where this distance falls below a threshold. The estimated α-trimming level quantifies the model's representativeness, with lower values indicating closer proximity to the expected distribution. The framework also determines the minimum number of models needed for a reliable ensemble.

## Key Results
- The α-trimming metric successfully identifies model representativeness beyond validation accuracy alone
- For CNNs on CIFAR-10, accuracy range for models with α ≤ 0.05 was [90.42, 91.78], while for ViTs it was [99.2, 99.3]
- At least 30 models are needed to form a reliable ensemble that approximates the expected distribution well
- The framework shows that models with similar validation accuracy can have widely varying α values, indicating different levels of representativeness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The robust Kolmogorov-Smirnov test with α-trimming quantifies how representative a trained model is of the stochastic training process.
- Mechanism: Instead of using classical two-sample KS testing, which is overly sensitive in large-sample regimes, the method trims a small fraction α of the candidate model's eCDF samples to find the closest L∞ approximation to the reference ensemble eCDF. Lower α indicates the candidate is closer to the expected distribution of models from the training process.
- Core assumption: The distribution of logit gaps under different random seeds is sufficiently similar that trimming can meaningfully isolate "outlier" samples caused by stochastic variability rather than fundamental differences in learned functions.
- Evidence anchors:
  - [abstract]: "The proposed metric, the α-trimming level, quantifies how representative a model is of the training process. Lower α indicates the model is closer to the expected distribution."
  - [section IV-A]: Discusses how the trimming-based test accepts models that are close to the reference function, allowing controlled false alarm rates via contamination neighborhoods.
  - [corpus]: Weak match. Corpus papers focus on hypothesis testing but do not specifically address DNN training variability via trimming.
- Break condition: If the underlying logit gap distributions from different seeds are too dissimilar (e.g., due to fundamental differences in architecture or optimization landscape), the α-trimming may not meaningfully reduce L∞ distance and the metric will fail to distinguish representative models.

### Mechanism 2
- Claim: The reference function formed by averaging eCDFs of multiple trained models approximates the expected CDF of the training process.
- Mechanism: By training M models with different random seeds and averaging their eCDFs, the reference function captures the consensus behavior of the model family, reducing the impact of individual stochastic fluctuations. This reference serves as a proxy for the unknown true expected CDF under the data and parameter distributions.
- Core assumption: The average eCDF over many independently trained models converges to the expected CDF, and the variance of individual eCDFs is small enough relative to the difference between models to make the average informative.
- Evidence anchors:
  - [section II-B]: Defines the reference function as an average of eCDFs and relates it to the expected CDF under the parameter distribution.
  - [section IV-A1]: Empirically shows that increasing the number of ensemble models reduces L∞ distance to the reference, indicating convergence.
  - [corpus]: Weak match. Corpus papers discuss hypothesis testing and distributions but not specifically ensemble CDF averaging for DNNs.
- Break condition: If the number of models M is too small, or if the training process is highly unstable (large variance across seeds), the reference function may poorly approximate the expected CDF, leading to unreliable α estimates.

### Mechanism 3
- Claim: Validation accuracy alone is insufficient to assess model quality under stochastic training; the α-trimming metric provides complementary information.
- Mechanism: Models with similar validation accuracy can have very different logit gap distributions, reflected in their α values. Low α models are closer to the expected distribution and thus more reliable representatives of the training process, even if their accuracy is not the highest.
- Core assumption: The logit gap distribution captures meaningful differences in learned functions beyond what accuracy measures, and these differences correlate with model reliability or generalization.
- Evidence anchors:
  - [abstract]: "demonstrate experimentally that it is more expressive than performance metrics like validation accuracy, churn, or expected calibration error when taken alone."
  - [section V-B3]: Shows that models with similar validation accuracy can have widely varying α, indicating that α captures variability not reflected in accuracy.
  - [corpus]: Weak match. Corpus papers focus on robustness and fairness metrics but do not directly compare accuracy to trimming-based measures.
- Break condition: If the logit gap distributions of models with similar accuracy are nearly identical, α will not provide additional discriminatory power, and accuracy may suffice.

## Foundational Learning

- Concept: Non-parametric hypothesis testing (Kolmogorov-Smirnov test)
  - Why needed here: The method compares distributions of logit gaps without assuming a parametric form, which is essential given the complexity and variability of DNN outputs.
  - Quick check question: What is the null hypothesis in a two-sample KS test, and how is the test statistic computed?

- Concept: Robust statistics and contamination neighborhoods
  - Why needed here: Classical KS tests are too sensitive to small differences in large samples; robust trimming allows the test to focus on the bulk of the distribution and ignore outliers caused by stochastic variability.
  - Quick check question: How does an α-trimming of a distribution differ from a contamination neighborhood, and why is this distinction important for the test?

- Concept: Empirical cumulative distribution functions (eCDFs)
  - Why needed here: eCDFs are used to empirically estimate the distribution of logit gaps from finite test samples, forming the basis for both the reference function and candidate model comparison.
  - Quick check question: How is an eCDF constructed from a finite sample, and what is its relationship to the true CDF?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> Logit gap computation -> eCDF calculation -> Reference function formation -> Robust KS test with α-trimming -> Model evaluation

- Critical path:
  1. Train M models with different seeds
  2. Compute eCDF for each model on test set
  3. Form reference function by averaging eCDFs
  4. For each candidate, compute eCDF and apply robust KS test
  5. Estimate ˆα and compare to other metrics

- Design tradeoffs:
  - Number of models M vs. computational cost: More models give better reference approximation but increase training time
  - Choice of logit gap vs. other functions: Logit gaps are interpretable and related to confidence, but other probes (e.g., Jacobians) might capture different aspects of variability
  - Threshold γ for KS test: Balances false alarm rate and sensitivity; too low requires many models, too high may accept dissimilar models

- Failure signatures:
  - High variance in ˆα across candidates with similar accuracy: Indicates the metric is sensitive to stochastic differences
  - Reference function poorly approximated (high L∞ distance): Suggests M is too small or training is unstable
  - ˆα close to 1 for all candidates: Implies models are very dissimilar or the test is too strict

- First 3 experiments:
  1. Train a small CNN on CIFAR-10 binary task with 10 different seeds; compute reference function and ˆα for each model; verify ˆα decreases as models become more similar
  2. Fix one seed, vary batch size; compare ˆα and accuracy to see if batch effects are captured beyond accuracy
  3. Create ensemble models with varying sizes; measure L∞ distance to reference and ˆα to find minimum size for reliable ensemble

## Open Questions the Paper Calls Out
None

## Limitations
- The framework assumes logit gap distributions are stable enough across random seeds for meaningful comparison, which may not hold for highly unstable training processes
- The minimum ensemble size of 30 models is empirically determined and may not generalize to other architectures or datasets
- The framework focuses on internal variability within a single architecture and may not capture fundamental differences between different model families

## Confidence

| Claim | Confidence |
|-------|------------|
| α-trimming captures meaningful differences beyond accuracy | Medium |
| Methodological framework is sound | High |
| Minimum ensemble size claims | Low |

## Next Checks

1. Verify whether α-trimming distinguishes models with similar accuracy but different generalization behavior on out-of-distribution data
2. Test sensitivity of ˆα to number of test samples and bootstrap iterations B
3. Compare alternative reference functions (e.g., median eCDF instead of mean) to assess impact on model representativeness conclusions