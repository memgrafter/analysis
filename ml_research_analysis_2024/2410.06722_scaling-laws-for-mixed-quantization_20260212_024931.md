---
ver: rpa2
title: Scaling Laws For Mixed Quantization
arxiv_id: '2410.06722'
source_url: https://arxiv.org/abs/2410.06722
tags:
- loss
- quantization
- size
- degeneration
- actual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a unified scaling law for post-training quantization
  of large language models that captures how quantization ratio and block size affect
  performance loss as model size scales. It shows that larger models can tolerate
  higher quantization ratios without performance degradation, and that very fine-grained
  block sizes (e.g., 16 or 32) provide diminishing returns for large models.
---

# Scaling Laws For Mixed Quantization

## Quick Facts
- arXiv ID: 2410.06722
- Source URL: https://arxiv.org/abs/2410.06722
- Authors: Zeyu Cao; Boyang Gu; Cheng Zhang; Pedro Gimenes; Jianqiao Lu; Jianyi Cheng; Xitong Gao; Yiren Zhao
- Reference count: 40
- Primary result: Unified scaling law shows larger LLMs tolerate higher quantization ratios, making mixed-precision inference viable

## Executive Summary
This paper introduces a unified scaling law framework that captures how quantization ratio, block size, and model size interact to affect performance loss during post-training quantization of large language models. The framework extends previous precision scaling laws to mixed quantization scenarios, showing that larger models can tolerate higher quantization ratios without performance degradation. Through experiments across 17 models (60M-14B parameters) and four quantization methods, the authors demonstrate that their scaling laws achieve R² > 0.95 fit quality and provide actionable insights for both quantization method design and hardware architecture decisions.

## Method Summary
The authors develop a unified scaling law framework that predicts loss degeneration during post-training quantization as a function of model size (N), quantization ratio (Qr), and block size (Qb). They use layer-wise and matmul-wise mixed-precision quantization with random search over 100 trials per configuration, evaluating loss via perplexity on the SlimPajama dataset. The framework is validated through experiments on 17 models ranging from 60M to 14B parameters using MXINT and HQQ quantization methods, with scaling law parameters fitted using least-squares regression and achieving R² > 0.95 for most fits.

## Key Results
- Larger models (1B-14B parameters) can tolerate higher quantization ratios without performance loss
- Very fine-grained block sizes (16 or 32) provide diminishing returns for large models
- Unified scaling law achieves R² > 0.95 fit quality across 17 models and 4 quantization methods
- The framework extends Kumar et al.'s precision scaling law to mixed quantization scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models tolerate higher quantization ratios without performance loss due to a multiplicative relationship between parameter scaling and ratio scaling.
- Mechanism: The unified scaling law combines model size (N) and quantization ratio (Qr) such that their effects are independent but multiplicative. Since e^(A·Qr) grows faster than N^(-γ_N), increasing Qr dominates the loss. However, Qr is bounded by 1 while N can grow indefinitely, so larger models can handle higher Qr.
- Core assumption: The loss degeneration is separable into ratio scaling (exponential in Qr) and parameter scaling (polynomial in N) terms that act independently.
- Evidence anchors:
  - [abstract]: "Our scaling law implies that parameter scaling and ratio scaling have a multiplicative relationship."
  - [section]: "For Qr, our scaling law implies that parameter scaling and ratio scaling have a multiplicative relationship."
  - [corpus]: Weak evidence - related work discusses scaling laws but doesn't specifically address this multiplicative relationship.

### Mechanism 2
- Claim: Fine-grained block sizes (Qb) provide diminishing returns for large models due to the quasi-polynomial term in the scaling law.
- Mechanism: The strong scaling law includes a granularity term (Qb + d)^γ_c where γ_c is between 0 and 1. This means that as model size increases, the impact of block size on loss degeneration becomes relatively smaller compared to the ratio scaling effect.
- Core assumption: The granularity scaling effect is independent from parameter scaling and ratio scaling effects.
- Evidence anchors:
  - [abstract]: "Regarding Qb, our findings indicate that a small block size, similar to that used in Blackwell, is not essential for large models."
  - [section]: "Since γc is within 0 and 1 with our scaling law... lowering Qr is more effective than lowering Qb related terms."
  - [corpus]: Moderate evidence - related work discusses block sizes but doesn't establish this specific diminishing returns relationship.

### Mechanism 3
- Claim: The unified scaling law agrees asymptotically with previously proposed Precision Scaling Laws when Qr = 1.
- Mechanism: When Qr = 1, there is only one possible pair (Wl, Wh) = (∅, M), causing the random variable ∆ to degenerate to its only realization. The quantization scaling effect is absorbed into the constant C, aligning with the Precision Scaling Law from Kumar et al.
- Core assumption: The random variable ∆ properly captures the combinatorial space of low- and high-precision parameter pairs.
- Evidence anchors:
  - [abstract]: "Our scaling laws integrate model sizes, mixed quantization ratios, and quantization granularities, thereby extending the scope beyond existing scaling laws that focus solely on precision levels."
  - [section]: "Takeaway 1: Across different Qr values, our unified scaling law agrees asymptotically with previously proposed Precision Scaling Laws proposed by Kumar et al."
  - [corpus]: Strong evidence - the Kumar et al. paper is directly referenced and the asymptotic agreement is explicitly stated.

## Foundational Learning

- Concept: Post-training quantization (PTQ)
  - Why needed here: The paper focuses on scaling laws specifically for post-training quantization, which is a method to reduce model size after training without retraining.
  - Quick check question: What is the key difference between post-training quantization and quantization-aware training?

- Concept: Scaling laws for language models
  - Why needed here: The paper builds on existing scaling laws for language models to extend them to quantization scenarios, requiring understanding of how model performance scales with parameters and data.
  - Quick check question: According to the Chinchilla scaling law, how should parameter count relate to training tokens for optimal performance?

- Concept: Mixed-precision arithmetic
  - Why needed here: The paper introduces metrics for mixed-precision quantization, requiring understanding of how different precision levels can be combined in hardware.
  - Quick check question: What is the key difference between mixed-precision and uniform precision quantization?

## Architecture Onboarding

- Component map: Pretrained models (60M-14B) -> Quantization methods (MXINT/HQQ) -> Random search -> Loss evaluation -> Scaling law fitting -> Validation
- Critical path: Pretrain models → Apply quantization with random search → Evaluate loss → Fit scaling law parameters → Validate with additional models and methods
- Design tradeoffs: The paper trades computational cost of exhaustive search for statistical estimation using random sampling, and prioritizes fitting empirical data over theoretical guarantees
- Failure signatures: Poor R² values (<0.95), inconsistent loss patterns across similar model sizes, failure to generalize to new quantization methods, or outliers that deviate significantly from fitted contours
- First 3 experiments:
  1. Verify the weak law by plotting loss degeneration vs. quantization ratio for a fixed model size and checking for exponential growth pattern
  2. Test the strong law by examining how block size affects loss degeneration at different quantization ratios for a fixed model size
  3. Validate generalization by applying the scaling law to a new quantization method (e.g., HQQ) and checking fit quality

## Open Questions the Paper Calls Out

- Question: Does the scaling law for mixed quantization hold for models significantly larger than 14B parameters, such as 70B or 400B parameter models?
  - Basis in paper: [inferred] The authors note this as a clear direction for future research and mention that one challenge is the computational cost of evaluating larger models with their current emulation approach
  - Why unresolved: The paper only tested models up to 14B parameters due to computational constraints. Evaluating the scaling law on much larger models would require actual hardware support for MXINT4/MXFP4 quantization rather than emulation
  - What evidence would resolve it: Experimental validation of the scaling law on models larger than 14B parameters using actual MXINT4/MXFP4 hardware quantization, demonstrating that the observed trends continue to hold

- Question: How do the scaling laws change when applied to mixture-of-experts (MoE) architectures like DeepSeek or Mixtral?
  - Basis in paper: [inferred] The authors explicitly suggest this as a future research direction, noting that MoE models represent a different architectural paradigm than the dense models studied
  - Why unresolved: The paper focused on dense transformer architectures (LLaMA, Qwen) and did not investigate how the quantization ratio and block size relationships might differ in MoE models with their sparse activation patterns
  - What evidence would resolve it: Experimental results showing how the quantization ratio (Qr) and block size (Qb) scaling relationships vary in MoE architectures compared to dense models

- Question: Is there a unified scaling law governing the ratio of approximate to exact compute across different efficient AI methods beyond quantization?
  - Basis in paper: [explicit] The authors hypothesize the existence of a broader scaling law governing how the ratio of approximate compute to exact compute scales with model sizes and the granularity at which approximate compute is applied
  - Why unresolved: This is presented as a hypothesis for future research. The paper only examined quantization, not other approximation methods like sparsity or low-rank approximations
  - What evidence would resolve it: Experimental validation showing similar scaling relationships between approximation ratio, model size, and granularity for multiple efficient AI methods (e.g., quantization, pruning, low-rank factorization) following a unified mathematical form

## Limitations
- The multiplicative separability assumption between parameter scaling, ratio scaling, and granularity scaling may not hold for all model architectures or quantization schemes
- Random search methodology may miss optimal configurations, particularly for larger models with combinatorial search spaces
- Scaling laws are derived from limited quantization methods (MXINT and HQQ variants) and may not generalize to other schemes
- Empirical validation relies on perplexity-based loss metrics which may not capture all aspects of model quality degradation

## Confidence
- **High Confidence**: The unified scaling law form itself, given strong empirical fits (R² > 0.95) across 17 models and multiple quantization methods
- **Medium Confidence**: The mechanistic interpretation of why larger models tolerate higher quantization ratios, though underlying reasons are not fully established
- **Low Confidence**: Practical implications for hardware design, particularly the claim that very fine-grained block sizes are unnecessary for large models

## Next Checks
1. Apply the unified scaling law to at least two additional quantization methods (e.g., GPTQ, AWQ) across the same model sizes to test generalizability beyond MXINT and HQQ
2. Test whether the scaling law holds for different model architectures (e.g., decoder-only vs. encoder-decoder, different attention mechanisms) to validate the independence assumption from architectural details
3. Evaluate whether loss degeneration predictions based on perplexity correlate with performance degradation on specific downstream tasks (e.g., reasoning, coding, summarization) to ensure the scaling law captures practically relevant quality metrics