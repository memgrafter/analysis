---
ver: rpa2
title: 'The application of GPT-4 in grading design university students'' assignment
  and providing feedback: An exploratory study'
arxiv_id: '2409.17698'
source_url: https://arxiv.org/abs/2409.17698
tags:
- reliability
- design
- students
- iteration
- instructor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored whether GPT-4 could effectively grade design
  university assignments and provide useful feedback. Using an iterative design-based
  research approach, researchers built and tested a Custom GPT model to assess students'
  design projects.
---

# The application of GPT-4 in grading design university students' assignment and providing feedback: An exploratory study

## Quick Facts
- arXiv ID: 2409.17698
- Source URL: https://arxiv.org/abs/2409.17698
- Reference count: 0
- Primary result: Custom GPT achieved inter-rater reliability with human instructors (ICC > 0.72) and provided personalized feedback for design assignments

## Executive Summary
This study explored the application of GPT-4 in grading design university assignments and providing feedback through an iterative Design-Based Research approach. The researchers built a Custom GPT model that incorporated instructor rubrics and exemplar projects to assess 20 student design projects. After six iterations of refinement, the system achieved inter-rater reliability between GPT and human raters reaching 0.7652, while maintaining intra-rater reliability between 0.65 and 0.78. The study demonstrated that GPT-4 could provide personalized feedback from multiple perspectives and compare student work to established designers, suggesting its potential as a reliable grading assistant to complement human raters in design education.

## Method Summary
The researchers employed an iterative Design-Based Research approach with six iterations to develop and refine a Custom GPT model for grading design assignments. They used 20 student design project screenshots (10 students, 2 poster designs each) and instructor rubrics as primary inputs. The method involved building Custom GPT with embedded rubrics and exemplars, having it grade student projects, calculating inter-rater and intra-rater reliability using ICC scores, and iterating based on results. The study progressed from basic rubric implementation to incorporating domain-specific perspectives (architect/engineer) and exemplar projects as benchmarks, with reliability metrics improving across iterations from below 0.5 to over 0.72.

## Key Results
- Inter-rater reliability between GPT and human instructors improved from below 0.5 to 0.7652 after iterative refinement with rubrics and exemplars
- Intra-rater reliability of GPT's scoring remained consistent at 0.65-0.78 across different grading sessions
- GPT provided personalized feedback from multiple perspectives and could compare student work to established designers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative refinement with rubrics and exemplar examples progressively increases inter-rater reliability between GPT and human raters.
- Mechanism: Providing structured rubrics and exemplar projects allows GPT to align its scoring criteria with human expectations, reducing subjective variance in grading.
- Core assumption: Rubrics and examples sufficiently capture the nuanced evaluation criteria used by instructors.
- Evidence anchors:
  - [abstract] "through several rounds of iterations the inter-reliability between GPT and human raters reached a level that is generally accepted by educators."
  - [section] "The inter-reliability between GPT and the two instructors has improved...reaching 0.5803."
  - [corpus] No direct corpus evidence found; weak external validation.
- Break condition: If rubrics are ambiguous or examples are not representative, GPT's scoring will not converge with human judgment.

### Mechanism 2
- Claim: Role-playing as domain-specific instructors improves GPT's alignment with discipline-specific grading perspectives.
- Mechanism: Instructing GPT to adopt the viewpoint of a specific instructor type (e.g., architecture vs. engineering) enables it to apply relevant domain knowledge in assessment.
- Core assumption: GPT can meaningfully simulate different instructor perspectives when prompted appropriately.
- Evidence anchors:
  - [section] "we let General GPT grade the students' assignments from the perspectives of both the architect instructor and the engineer instructor."
  - [section] "The inter-reliability between Architect GPT and the Architect instructor is very low...while the inter-reliability between Engineer GPT and the Engineer instructor is 0.5423."
  - [corpus] No direct corpus evidence found; weak external validation.
- Break condition: If role-play prompts do not sufficiently differentiate instructor perspectives, reliability improvements will plateau.

### Mechanism 3
- Claim: Providing an exemplar project as a benchmark enables GPT to consistently evaluate assignments against a known standard.
- Mechanism: GPT uses the exemplar to calibrate its scoring rubric, ensuring consistency in grading similar assignments.
- Core assumption: A single exemplar can represent the full range of acceptable student work.
- Evidence anchors:
  - [section] "giving it an example of what would be considered a good assignment."
  - [section] "the inter-reliability between GPT and the two instructors continued to improve...exceeded 0.7 (0.7652)."
  - [corpus] No direct corpus evidence found; weak external validation.
- Break condition: If the exemplar is too narrow or too broad, GPT's consistency will suffer.

## Foundational Learning

- Concept: Intraclass Correlation Coefficient (ICC)
  - Why needed here: ICC quantifies the reliability of ratings between multiple raters or across time, critical for validating GPT's grading consistency.
  - Quick check question: What ICC value threshold is generally accepted for high-stakes assessments?

- Concept: Design-Based Research (DBR)
  - Why needed here: DBR allows iterative refinement of GPT grading models based on empirical feedback, ensuring practical applicability.
  - Quick check question: What are the two main phases of DBR as applied in this study?

- Concept: Rubric Development
  - Why needed here: Clear rubrics ensure GPT applies consistent evaluation criteria aligned with instructor expectations.
  - Quick check question: Why is it important to align GPT rubrics with human instructor rubrics?

## Architecture Onboarding

- Component map:
  - Custom GPT model (with embedded rubrics and exemplars) -> Input pipeline (student project images) -> Scoring engine (rubric-based evaluation) -> Feedback generator (personalized suggestions) -> Rater comparison module (ICC calculation)

- Critical path:
  1. Input project images
  2. Apply Custom GPT with rubrics and exemplars
  3. Generate scores and feedback
  4. Calculate ICC against human raters
  5. Iterate based on results

- Design tradeoffs:
  - Rubric specificity vs. flexibility for creative design work
  - Exemplar representativeness vs. simplicity of benchmark
  - GPT model complexity vs. inference speed

- Failure signatures:
  - ICC values consistently below 0.5 indicate misalignment
  - Large variance in scores across iterations suggests instability
  - Feedback not actionable for students signals poor rubric definition

- First 3 experiments:
  1. Run Custom GPT without exemplars; measure baseline ICC.
  2. Add domain-specific rubrics; measure ICC improvement.
  3. Incorporate exemplar project; measure final ICC convergence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the reliability of Custom GPT grading compare to traditional peer grading methods in design education?
- Basis in paper: [inferred] The paper discusses inter-rater reliability between GPT and human raters, but doesn't compare to peer grading reliability
- Why unresolved: The study focuses on comparing GPT to instructor grading rather than student-to-student grading
- What evidence would resolve it: A study comparing ICC values between GPT-human rater pairs versus student-student rater pairs for the same assignments

### Open Question 2
- Question: What is the impact of Custom GPT grading on student learning outcomes compared to traditional grading methods?
- Basis in paper: [inferred] The paper discusses feedback provision but doesn't measure learning outcomes
- Why unresolved: The study focuses on grading reliability and feedback quality rather than actual learning impact
- What evidence would resolve it: A longitudinal study tracking student performance and skill development under different grading methods

### Open Question 3
- Question: How does the cost-effectiveness of Custom GPT grading compare to traditional grading methods when considering implementation and maintenance?
- Basis in paper: [inferred] The paper discusses grading reliability but doesn't address cost factors
- Why unresolved: The study focuses on technical reliability rather than economic viability
- What evidence would resolve it: A comprehensive cost analysis including initial setup, training, maintenance, and time savings compared to traditional methods

## Limitations
- Small sample size with only 20 student projects from a single course limits generalizability
- ICC values, while acceptable, still fall short of ideal reliability thresholds (>0.8) for high-stakes assessments
- Lack of external validation and comparison to alternative grading methods (e.g., peer grading)

## Confidence
- Confidence in core claims is Medium. The iterative methodology and use of established reliability metrics (ICC) provide reasonable empirical support, but the small sample size, single-course context, and lack of external validation weaken confidence.

## Next Checks
1. **External Validation Study**: Replicate the study with a different instructor, course, and student population to test generalizability of GPT-4's grading reliability across institutional contexts.

2. **Longitudinal Consistency Test**: Conduct a multi-semester study tracking GPT-4's grading consistency across different assignment iterations and evolving rubrics to assess stability over time.

3. **Student Outcome Analysis**: Compare student learning outcomes between GPT-assisted and human-only grading conditions to determine if the feedback quality translates to measurable improvements in subsequent work.