---
ver: rpa2
title: Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative
  In-context Learning
arxiv_id: '2402.02388'
source_url: https://arxiv.org/abs/2402.02388
tags:
- llms
- solutions
- self
- generation
- abms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAGE, a framework that leverages large language
  models (LLMs) for automatic generation of solution-oriented agent-based models (ABMs).
  SAGE addresses the challenge of LLMs' limitations in handling complex ABM interactions
  and their lack of self-evaluation.
---

# Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning

## Quick Facts
- arXiv ID: 2402.02388
- Source URL: https://arxiv.org/abs/2402.02388
- Authors: Tong Niu; Weihao Zhang; Rong Zhao
- Reference count: 40
- Primary result: SAGE achieves 18.7% improvement in modeling quality and 38.1% in solution generation effectiveness compared to non-SAGE approaches

## Executive Summary
This paper introduces SAGE, a framework that leverages large language models (LLMs) for automatic generation of solution-oriented agent-based models (ABMs). SAGE addresses the challenge of LLMs' limitations in handling complex ABM interactions and their lack of self-evaluation. The framework employs semi-structured conceptual and objective representations to guide LLMs in modeling scenarios and proposing solutions. A two-level verifier with chain-of-thought prompting drives iterative generation optimization, ensuring executability and feasibility. Evaluations on a constructed dataset demonstrate SAGE's effectiveness, achieving an average improvement of 18.7% in modeling quality and 38.1% in solution generation effectiveness compared to non-SAGE approaches.

## Method Summary
SAGE is a framework that uses semi-structured conceptual representations to guide LLMs in generating executable ABMs, and semi-structured objective representations to guide solution generation for ABM problems. The framework employs a two-level verifier system - verifier-level1 checks for compilation errors and missing details to ensure executability, while verifier-level2 validates solutions against objective criteria. Chain-of-thought prompting enhances the solution generation process through systematic analysis of variable relationships, failure causes, and targeted modifications. The framework operates in two stages: "Modeling" generates executable ABMs from conceptual representations, and "Solving" generates and refines solutions using objective representations and iterative verification.

## Key Results
- SAGE achieves 18.7% average improvement in modeling quality compared to non-SAGE approaches
- Solution generation effectiveness improves by 38.1% with SAGE
- The framework successfully handles complex ABM interactions and non-linear dynamics through iterative verification
- Verifier-assisted generation ensures both executability and solution feasibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-level verifier system compensates for LLMs' lack of self-evaluation by identifying both compilation errors and missing details.
- Mechanism: Verifier-level1 uses compiler/runtime feedback to catch syntactic and runtime errors, while semantic analysis flags incomplete method implementations. These defects are fed back to the LLM for iterative correction.
- Core assumption: Compilation errors and "lacking details" are the primary failure modes in LLM-generated ABMs.
- Evidence anchors:
  - [abstract] "devises a two-level verifier with chain-of-thought prompting tailored to the complex interactions and non-linear dynamics of ABMs, driving the iterative generation optimization"
  - [section 3.1] "Verifier-level1 is designed to identify defects in generated programs, ensuring their executability and integrity"
- Break condition: If defect types expand beyond these two categories or if the LLM cannot correct identified issues within a reasonable number of iterations.

### Mechanism 2
- Claim: Semi-structured conceptual and objective representations bridge the gap between natural language expressiveness and program executability.
- Mechanism: Conceptual representations provide structured object definitions and scheduling primitives while maintaining NL flexibility. Objective representations encode problem statements and criteria in a semi-structured format that LLMs can parse into verification functions.
- Core assumption: LLMs can effectively learn to translate semi-structured representations into executable code through few-shot prompting.
- Evidence anchors:
  - [abstract] "introduces an semi-structured conceptual representation expliciting the intricate structures of ABMs and an objective representation to guide LLMs in modeling scenarios"
  - [section 3.1] "we design a semi-structured, semi-natural language conceptual representation that serves as a user-friendly guide"
- Break condition: If LLMs fail to generalize from few-shot examples or if the semi-structured format becomes too restrictive for complex scenarios.

### Mechanism 3
- Claim: Chain-of-thought prompting with relation extraction, cause analysis, and solution proposal enables LLMs to reason about ABM dynamics and propose effective modifications.
- Mechanism: The CoT process guides LLMs through systematic analysis of variable relationships, identification of failure causes, and generation of targeted solutions based on domain knowledge.
- Core assumption: Breaking down the solution generation process into these three steps improves LLM reasoning quality for complex ABM modifications.
- Evidence anchors:
  - [abstract] "SAGE devises a two-level verifier with chain-of-thought prompting tailored to the complex interactions and non-linear dynamics of ABMs"
  - [section 3.2] "we apply the CoT prompt technique to enhance the solution generation of LLMs"
- Break condition: If the CoT steps do not lead to improved solution quality or if the LLM struggles with the analysis steps.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: SAGE relies on ICL to enable LLMs to generate ABMs and solutions without additional training
  - Quick check question: How does ICL differ from traditional fine-tuning in terms of data requirements and task adaptation?

- Concept: Agent-based modeling fundamentals
  - Why needed here: Understanding ABM concepts like agents, environments, interactions, and scheduling is crucial for working with SAGE
  - Quick check question: What are the key differences between ABM and other modeling paradigms like system dynamics or discrete event simulation?

- Concept: Chain-of-thought reasoning
  - Why needed here: CoT prompting is central to SAGE's solution generation process
  - Quick check question: What are the three main steps in SAGE's CoT prompting approach and how do they contribute to solution quality?

## Architecture Onboarding

- Component map: Modeling stage (conceptual representation → LLM → verifier-level1 → rectified ABM) and Solving stage (objective representation → verifier-level2 → CoT prompting → LLM → verifier-level1 → enhanced ABM)
- Critical path: The critical path for generating executable ABMs involves creating the conceptual representation, LLM generation, verifier-level1 validation, and iterative correction until an executable model is produced.
- Design tradeoffs: SAGE trades the flexibility of pure natural language input for the executability benefits of semi-structured representations. The two-level verification system adds computational overhead but significantly improves output quality.
- Failure signatures: Common failure modes include LLMs getting stuck in local optima during iterative correction, conceptual representations being too vague for effective code generation, and verification functions failing to capture all problem criteria.
- First 3 experiments:
  1. Generate an ABM from a simple conceptual representation (e.g., epidemic spread with basic parameters) and verify executability
  2. Apply the Solving stage to a criteria-defined problem using a pre-existing ABM and evaluate solution effectiveness
  3. Test the iterative correction process with a deliberately flawed ABM to assess verifier-level1's error detection capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the semi-structured conceptual and objective representations balance expressiveness and formalizability for diverse domain scenarios?
- Basis in paper: [explicit] The paper describes these representations as "semi-structured" and "semi-natural language" to maintain expressiveness while enabling formal analysis.
- Why unresolved: The paper doesn't empirically demonstrate how well this balance works across extremely diverse or novel domains, nor does it compare against purely natural language or fully structured approaches.
- What evidence would resolve it: Comparative studies showing SAGE's performance across domains with varying levels of structural formality, and analysis of edge cases where the semi-structured approach succeeds or fails.

### Open Question 2
- Question: What are the limitations of the two-level verifier approach when dealing with emergent behaviors or non-linear dynamics that weren't explicitly programmed?
- Basis in paper: [explicit] The paper states the verifier identifies "compilation errors" and "lacking details" but doesn't address how it handles unexpected emergent behaviors.
- Why unresolved: The verifier is designed for known defect types, but complex ABMs often produce unexpected emergent properties that might not fit these categories.
- What evidence would resolve it: Case studies of ABMs where unexpected emergent behaviors occur, and analysis of whether the verifier can detect and handle these situations appropriately.

### Open Question 3
- Question: How does the performance of SAGE scale with increasing model complexity and number of interacting agents?
- Basis in paper: [inferred] The paper demonstrates effectiveness on a constructed dataset but doesn't discuss scalability limitations or performance degradation with larger, more complex models.
- Why unresolved: LLMs have context window limitations and computational constraints that could impact SAGE's effectiveness on very large or complex ABMs.
- What evidence would resolve it: Systematic experiments varying ABM size and complexity, measuring iteration counts, success rates, and computational requirements across different scales.

## Limitations
- Evaluation is limited to a constructed dataset of 15 scenarios without external validation
- The claimed improvements lack detail on baseline comparison and real-world usability assessment
- Framework's scalability to complex ABMs with thousands of agents remains unverified
- Semi-structured representations' expressiveness limitations for capturing all ABM features are not thoroughly explored

## Confidence

**High Confidence**: The core architectural approach of using semi-structured representations with verifier-assisted iterative generation is well-founded and technically sound.

**Medium Confidence**: The claimed improvements in modeling quality (18.7%) and solution effectiveness (38.1%) are based on internal evaluations using CodeBLEU and custom objective verification functions.

**Low Confidence**: The framework's generalizability to diverse ABM domains beyond the constructed dataset is uncertain, and its ability to handle complex emergent behaviors remains unverified.

## Next Checks

1. **External Dataset Validation**: Test SAGE on established ABM benchmarks like NetLogo's model library or Swarm to verify performance across diverse domains and complexity levels.

2. **Human Expert Evaluation**: Have domain experts assess the quality, realism, and usefulness of SAGE-generated ABMs and solutions beyond automated metrics like CodeBLEU and objective verification.

3. **Performance Benchmarking**: Compare SAGE's execution speed and resource usage against traditional ABM development tools when generating models of comparable complexity, particularly for scenarios with large agent populations.