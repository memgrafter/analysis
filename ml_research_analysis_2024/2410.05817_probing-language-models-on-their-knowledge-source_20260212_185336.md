---
ver: rpa2
title: Probing Language Models on Their Knowledge Source
arxiv_id: '2410.05817'
source_url: https://arxiv.org/abs/2410.05817
tags:
- knowledge
- relation
- llms
- source
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) resolve
  conflicts between their learned parametric knowledge (PK) and external contextual
  knowledge (CK) during inference. To probe this, the authors construct prompts that
  contradict the model's PK, and train classifiers on model activations to detect
  which knowledge source is being used.
---

# Probing Language Models on Their Knowledge Source

## Quick Facts
- arXiv ID: 2410.05817
- Source URL: https://arxiv.org/abs/2410.05817
- Reference count: 7
- This paper investigates how large language models resolve conflicts between learned parametric knowledge and external contextual knowledge during inference.

## Executive Summary
This paper introduces a framework for probing how large language models resolve conflicts between their learned parametric knowledge (PK) and external contextual knowledge (CK). The authors construct prompts that contradict the model's PK and train classifiers on model activations to detect which knowledge source is being used. Experiments on multiple LLMs show that specific activations related to relation tokens are highly predictive of knowledge source selection, with success rates reaching 87% for Pythia-1.4B. The study also finds that higher subject frequency in training data correlates with greater likelihood of PK usage, revealing that knowledge source selection is detectable early in the inference process.

## Method Summary
The framework constructs controlled prompts that contradict the model's parametric knowledge using the ParaRel dataset, generating counter-knowledge by replacing objects with semantically similar ones not learned by the model. Model activations are extracted from MLP-L1, MLP-L2, and MHSA layers, focusing on relation token activations. Linear classifiers are trained on these activations to predict whether the model uses PK or CK, with balanced training and test sets across relation groups. The approach measures the correlation between specific activation patterns and knowledge source selection while leaving the underlying mechanisms for future research.

## Key Results
- Classifiers trained on model activations successfully predict knowledge source selection with up to 87% accuracy for Pythia-1.4B
- MLPs play a key role in storing and retrieving factual associations, with knowledge source patterns transferable across relation groups
- Higher subject frequency in training data correlates with greater likelihood of PK usage (Mann-Whitney U test shows significant difference)
- Knowledge source selection is detectable early in inference through specific activation patterns in relation tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLPs in transformer layers store factual associations as key-value memories, retrieved by MHSA when processing relation tokens
- Mechanism: During inference, relation token's MHSA attention focuses on subject's MLP activations to extract associated object from parametric memory
- Core assumption: Factual knowledge is primarily encoded in MLP weights, not attention heads
- Evidence anchors:
  - [abstract] "MLPs play a key role in storing and retrieving factual associations"
  - [section] "Our framework successfully transfer the learned knowledge source patterns from one relation group to another"
  - [corpus] Weak evidence - neighboring papers discuss residual streams but not MLP-MHSA retrieval mechanism specifically
- Break condition: If MLP weights are overwritten during fine-tuning or if model uses retrieval-augmented generation instead of parametric memory

### Mechanism 2
- Claim: Knowledge source selection is detectable early through specific activation patterns in relation tokens
- Mechanism: When model encounters counter-knowledge, activation pattern in relation token's MLP and MHSA layers diverges predictably from parametric knowledge patterns
- Core assumption: Choice between PK and CK manifests as measurable changes in neural activations capturable by linear classifiers
- Evidence anchors:
  - [abstract] "specific activations—particularly those related to relation tokens—are highly predictive of knowledge source selection"
  - [section] "This finding is consistent with prior research, which indicates that LLMs primarily store knowledge in the MLPs"
  - [corpus] Weak evidence - neighboring papers discuss knowledge conflicts but not activation-based detection methods
- Break condition: If model uses non-linear reasoning beyond what linear classifiers can capture, or if knowledge selection depends on factors outside probed activations

### Mechanism 3
- Claim: Subject frequency in training data correlates with PK usage probability
- Mechanism: During training, frequent subjects form stable parametric associations, making model more likely to rely on these when subject appears
- Core assumption: Training data frequency directly influences strength of parametric memory associations
- Evidence anchors:
  - [abstract] "higher subject frequency in training data correlates with greater likelihood of PK usage"
  - [section] "Mann-Whitney U test reveals that the subject frequency distribution for PK outputs is significantly higher than for CK and ND outputs"
  - [corpus] Weak evidence - neighboring papers don't discuss frequency effects on knowledge source selection
- Break condition: If model uses adaptive strategies that override frequency-based preferences, or if context strength consistently dominates parametric knowledge regardless of subject frequency

## Foundational Learning

- Concept: Transformer architecture - specifically interaction between MLP and MHSA modules in decoder layers
  - Why needed here: Understanding how MLPs store knowledge and how MHSAs retrieve it is crucial for interpreting activation patterns indicating knowledge source selection
  - Quick check question: In a transformer layer, what are the two main sub-modules that process the hidden state, and what are their respective roles?

- Concept: Probing techniques using linear classifiers on model activations
  - Why needed here: Methodology relies on training classifiers to detect knowledge source from activation patterns, requiring understanding of how probing works
  - Quick check question: What is the difference between probing and causal tracing methods for understanding model behavior?

- Concept: Knowledge conflicts in LLMs - parametric vs contextual knowledge
  - Why needed here: Entire framework built around understanding how models handle conflicts between learned knowledge and new information
  - Quick check question: What happens when a model encounters information that contradicts its parametric knowledge during inference?

## Architecture Onboarding

- Component map: Prompt generation → Model inference → Activation extraction → Classifier training → Knowledge source prediction
- Critical path: Prompt generation → Model inference → Activation extraction → Classifier training → Knowledge source prediction
- Design tradeoffs: Using linear classifiers enables fast detection but may miss non-linear patterns; focusing on relation tokens captures retrieval mechanisms but may miss other knowledge sources
- Failure signatures: Random classifier performance (50% accuracy) indicates failure to detect knowledge source patterns; consistently low PK usage might indicate model bias toward context
- First 3 experiments:
  1. Test classifier performance on a single relation group to establish baseline detection capability
  2. Compare MLP-L1, MLP-L2, and MHSA activation effectiveness for knowledge source detection
  3. Vary counter-knowledge selection strategy (k parameter) to find optimal balance between difficulty and reliability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the probing framework be extended to handle cases where both parametric and contextual knowledge are consistent or unrelated?
- Basis in paper: [inferred] The paper mentions that the current framework is designed to probe LLMs by introducing contradictions to their learned knowledge, but this controlled experimental setting does not account for many other situations where the knowledge remains unperturbed.
- Why unresolved: The paper explicitly states that the framework needs to be extended to handle cases where both PK and CK are consistent or not related, providing a more comprehensive understanding of LLM behavior.
- What evidence would resolve it: Experimental results showing the effectiveness of an extended framework in handling various knowledge consistency scenarios, including where PK and CK are both consistent or unrelated.

### Open Question 2
- Question: What are the underlying mechanisms that govern knowledge source selection in LLMs?
- Basis in paper: [explicit] The paper states that while the framework measures the correlation between specific activations and the use of PK or CK, it does not establish an explanation of the underlying process. Further research is needed to uncover the underlying mechanisms.
- Why unresolved: The paper acknowledges that the current approach provides valuable insights but does not explain the underlying mechanisms governing knowledge source selection.
- What evidence would resolve it: Experimental designs that manipulate specific model parameters or activations to observe resulting behavioral changes, potentially revealing the mechanisms at play.

### Open Question 3
- Question: How do different prompt structures affect knowledge source selection in LLMs?
- Basis in paper: [inferred] The paper suggests that it might be interesting to employ a variety of prompt structures to mitigate biases associated with the conventional subject-relation-object format, exploring alternative combinations such as relation-subject-object.
- Why unresolved: The paper does not explore how different prompt structures might influence knowledge source selection, focusing instead on a specific format.
- What evidence would resolve it: Comparative studies using various prompt structures to determine their impact on knowledge source selection, potentially revealing biases or preferences in LLM behavior.

## Limitations
- Framework generalizability across different knowledge types remains uncertain, with experiments focusing primarily on factual knowledge triplets
- Reliance on linear classifiers may miss complex, non-linear decision processes in larger models
- Controlled prompt generation methodology may not fully capture real-world knowledge conflict scenarios where context and prompt phrasing significantly influence model behavior

## Confidence

- **High Confidence**: The detection of knowledge source selection through relation token activations (87% success rate for Pythia-1.4B) - supported by multiple experiments and consistent with MLP knowledge storage literature
- **Medium Confidence**: The correlation between subject frequency and PK usage probability - statistically significant but may be influenced by dataset biases and model-specific training patterns
- **Low Confidence**: The generalizability of MLP-MHSA retrieval mechanisms across different model architectures and knowledge types - based primarily on analysis of factual knowledge triplets in decoder-only transformers

## Next Checks
1. Test classifier performance on a held-out relation group not seen during training to verify the framework's ability to generalize knowledge source detection patterns across different semantic domains
2. Apply the framework to models with different architectural designs (e.g., encoder-decoder models, models with retrieval augmentation) to assess whether activation patterns remain predictive of knowledge source selection
3. Conduct ablation studies on the MLP vs. MHSA components to determine their relative contributions to knowledge retrieval and whether the MLP-MHSA interaction is truly the primary mechanism for parametric knowledge access