---
ver: rpa2
title: Promoting cross-modal representations to improve multimodal foundation models
  for physiological signals
arxiv_id: '2410.16424'
source_url: https://arxiv.org/abs/2410.16424
tags:
- data
- multimodal
- pretraining
- modality
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of pretraining multimodal
  foundation models for physiological signal analysis using the PhysioNet 2018 dataset.
  The authors explore various pretraining strategies, including masked autoencoding
  (MAE) with modality dropout and contrastive learning approaches.
---

# Promoting cross-modal representations to improve multimodal foundation models for physiological signals

## Quick Facts
- arXiv ID: 2410.16424
- Source URL: https://arxiv.org/abs/2410.16424
- Authors: Ching Fang; Christopher Sandino; Behrooz Mahasseni; Juri Minxha; Hadi Pouransari; Erdrin Azemi; Ali Moin; Ellen Zippi
- Reference count: 23
- Key outcome: MAE with input modality dropout outperforms other pretraining strategies for physiological signal analysis across sleep staging, age classification, and arousal detection tasks

## Executive Summary
This paper investigates multimodal foundation models for physiological signal analysis, specifically addressing the challenge of effective pretraining strategies for heterogeneous biomedical data. The authors propose a masked autoencoding approach with modality dropout that forces the model to reconstruct missing modalities using information from available ones. Through systematic evaluation on the PhysioNet 2018 dataset, they demonstrate that this cross-modal reconstruction objective significantly outperforms unimodal baselines and other multimodal pretraining approaches across multiple downstream tasks.

## Method Summary
The authors develop a multimodal transformer architecture that uses early fusion of modality-specific tokens followed by an 8-layer transformer encoder. The model is pretrained using masked autoencoding with 70% token masking, augmented with modality dropout where one modality is randomly removed per batch. After pretraining, a linear probe approach is used where the frozen encoder serves as feature extractor for downstream classification tasks including sleep staging (5-way), age classification (binary), and arousal detection (binary). The model is trained on 989 unlabeled patients and evaluated on 219 test patients from the PhysioNet 2018 dataset.

## Key Results
- MAE with input modality dropout achieves the highest aggregate score across all downstream tasks compared to unimodal baselines and other multimodal pretraining strategies
- Attention weights become more temporally aligned and cross-modal when using the proposed pretraining approach
- Individual embedding units become more diversely tuned to different modalities, improving representation quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Cross-modal reconstruction during pretraining improves multimodal representation quality for physiological signals.
- **Mechanism**: When modality dropout is applied in the input space, the model is forced to use information from other modalities to reconstruct the missing one. This creates stronger cross-modal dependencies in the learned representations, making them more useful for downstream tasks that may rely on different modality combinations.
- **Core assumption**: The PhysioNet 2018 dataset contains sufficient cross-modal correlations that can be exploited during reconstruction to improve generalization.
- **Evidence anchors**:
  - [abstract] "We demonstrate that modality dropout in the input space improves performance across downstream tasks."
  - [section 4.2] "Removing input modality drop causes a performance drop in downstream tasks (compare 'MAE Only' to 'MAE + Input Mod. Drop' in Table 2)."
  - [corpus] Weak - corpus neighbors focus on different pretraining strategies but don't specifically address modality dropout effects.
- **Break condition**: If the physiological signals are largely independent across modalities with minimal correlation, forcing cross-modal reconstruction would add noise rather than signal.

### Mechanism 2
- **Claim**: Early fusion architecture with masked autoencoding enables better temporal alignment of attention weights.
- **Mechanism**: The early fusion structure concatenates tokens across modalities before encoding, allowing self-attention layers to learn cross-modal relationships. MAE pretraining with modality dropout further encourages the model to align attention temporally by requiring reconstruction across time windows using information from all modalities.
- **Core assumption**: The physiological signals, while heterogeneous, contain temporally aligned patterns that can be jointly learned through cross-modal attention.
- **Evidence anchors**:
  - [section 4.4] "We also observe an additional effect from both MultiMAE models where attention weights become more temporally aligned."
  - [section 4.4] "The attention matrix develops strong vertical bands... In this case, EEG and EOG tokens are most dominant."
  - [corpus] Weak - corpus neighbors mention temporal modeling but not specifically the temporal alignment mechanism through early fusion.
- **Break condition**: If physiological signals have asynchronous patterns or the temporal alignment is task-specific rather than generalizable, the learned attention alignment would be brittle.

### Mechanism 3
- **Claim**: Diverse tuning of individual embedding units to different modalities emerges from multimodal MAE with input dropout.
- **Mechanism**: The pretraining objective with modality dropout creates pressure for embedding units to specialize in reconstructing specific modalities when they are missing. This leads to a more distributed representation where different units encode different modality combinations, improving robustness.
- **Core assumption**: Having distributed modality-specific representations in embedding units provides better generalization than having all units encode all modalities equally.
- **Evidence anchors**:
  - [section 4.4] "In contrast, the model trained with MAE and modality drop is equally tuned to all modalities across all layers (Figure 2F)."
  - [section 4.4] "We find that representations become more tuned for EEG in the later layers of the model" (scratch model).
  - [corpus] Weak - corpus neighbors don't discuss unit-level modality specialization in representations.
- **Break condition**: If downstream tasks require modality-specific specialization rather than distributed encoding, the equally tuned units might perform worse than specialized ones.

## Foundational Learning

- **Concept**: Masked Autoencoding (MAE)
  - Why needed here: MAE is the core pretraining objective that allows the model to learn representations by reconstructing missing input tokens, which is essential for self-supervised learning with unlabeled physiological data.
  - Quick check question: Why does masking 70% of tokens work better than masking fewer tokens in this context?

- **Concept**: Modality Dropout
  - Why needed here: Modality dropout is used to explicitly encourage cross-modal learning by forcing the model to reconstruct missing modalities using information from others, addressing the challenge of heterogeneous modality informativeness.
  - Quick check question: How does modality dropout differ from token masking in terms of the cross-modal relationships it creates?

- **Concept**: Attention Rollout Analysis
  - Why needed here: Attention rollout is used to quantify how cross-modal and temporally aligned the learned representations are by tracing attention flows through the transformer layers.
  - Quick check question: What does it mean when attention weights form "vertical bands" versus being "evenly spread" across the attention matrix?

## Architecture Onboarding

- **Component map**: Modality-specific tokenizers (1D conv + linear layers) -> Early fusion layer (concatenation) -> Transformer encoder (8 layers, 8 heads each) -> Modality-specific decoders (cross-attention + transformer + linear) -> Classification head (linear layer on top of frozen encoder)

- **Critical path**: Input → Modality tokenizers → Early fusion → Transformer encoder → Decoder(s) → Reconstruction loss (during pretraining) or Classification head (during finetuning)

- **Design tradeoffs**: Early fusion enables cross-modal attention but may lose modality-specific features early; modality-specific decoders preserve reconstruction ability but add complexity; input modality dropout improves cross-modal learning but may hurt reconstruction of truly independent modalities.

- **Failure signatures**: If downstream performance is worse than unimodal baselines, check if modality dropout is too aggressive; if training is unstable, verify token masking ratio; if representations don't transfer well, examine attention rollout patterns.

- **First 3 experiments**:
  1. Train unimodal MAE models for each modality to establish baseline performance and identify which modality is most informative for each task.
  2. Train multimodal MAE without input modality dropout to verify if early fusion alone provides cross-modal benefits.
  3. Vary the modality dropout probability to find the optimal balance between cross-modal learning and reconstruction quality.

## Open Questions the Paper Calls Out

- How does the effectiveness of multimodal foundation models vary across different physiological signal datasets with varying levels of inter-subject variability?
- What is the optimal balance between early fusion and late fusion architectures for multimodal physiological signal processing, and how does this balance depend on the specific downstream tasks?
- How do different modalities contribute to cross-modal reconstruction in masked autoencoding, and can this contribution be optimized for specific downstream tasks?

## Limitations
- Study relies on a single dataset (PhysioNet 2018) limiting generalizability to other physiological signal domains
- Focus on sleep-related signals may not transfer to broader healthcare applications
- Attention rollout analysis is qualitative rather than quantitative, making rigorous comparison difficult

## Confidence
- **High confidence**: The core finding that MAE with input modality dropout outperforms other pretraining strategies is well-supported by quantitative results across multiple downstream tasks (Table 2).
- **Medium confidence**: The qualitative observations about attention alignment and embedding unit tuning provide useful insights but would benefit from more rigorous statistical analysis.
- **Low confidence**: The generalizability of these findings to other physiological signal domains and datasets remains untested.

## Next Checks
1. Test the pretrained model on a different physiological signal dataset (e.g., MIT-BIH arrhythmia database) to assess generalizability beyond sleep signals.
2. Systematically vary both token masking rate (currently 70%) and modality dropout probability to identify optimal hyperparameters and their interaction effects.
3. Develop quantitative metrics for measuring temporal alignment of attention weights and correlate these with downstream task performance.