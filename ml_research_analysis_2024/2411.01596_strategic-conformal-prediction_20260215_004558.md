---
ver: rpa2
title: Strategic Conformal Prediction
arxiv_id: '2411.01596'
source_url: https://arxiv.org/abs/2411.01596
tags:
- strategic
- conformal
- prediction
- alterations
- xgboost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Strategic Conformal Prediction, the first
  method for uncertainty quantification in the strategic setting where individuals'
  covariates change in response to deployed machine learning models. The approach
  modifies standard conformal prediction by calibrating prediction sets to be robust
  against strategic alterations using supremum over a set of alteration functions.
---

# Strategic Conformal Prediction

## Quick Facts
- arXiv ID: 2411.01596
- Source URL: https://arxiv.org/abs/2411.01596
- Reference count: 30
- Primary result: Introduces Strategic Conformal Prediction for uncertainty quantification in strategic settings where individuals modify covariates to avoid predicted outcomes

## Executive Summary
Strategic Conformal Prediction is the first method for uncertainty quantification in the strategic setting where individuals' covariates change in response to deployed machine learning models. The approach modifies standard conformal prediction by calibrating prediction sets to be robust against strategic alterations using supremum over a set of alteration functions. Experiments on 9 datasets show Strategic Conformal Prediction consistently achieves valid coverage (e.g., 90% coverage when targeting 90%) under various strategic alterations, while standard conformal prediction fails substantially (e.g., 15-57% coverage).

## Method Summary
The method calibrates prediction sets by computing a threshold t_strat_α using the supremum of conformity scores over a set of strategic alteration functions D. This ensures coverage even when individuals strategically modify their covariates to avoid the predicted outcome set. The approach maintains valid marginal coverage, training-conditional coverage, and group-conditional extensions while being robust to model misspecification and strategic alteration misspecification.

## Key Results
- Strategic Conformal Prediction achieves valid coverage (90%) on 9 datasets when targeting 90% coverage
- Standard conformal prediction fails substantially under strategic alterations (15-57% coverage)
- The method incurs some increase in interval size but this can be mitigated using strategically-trained base models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The supremum over strategic alteration functions in the calibration step ensures coverage even when individuals strategically modify their covariates to avoid the predicted outcome set.
- Mechanism: By computing t_strat_α using sup_∆∈D s(∆(Xi), Yi) instead of just s(Xi, Yi), the calibration threshold accounts for the worst-case strategic alteration within the specified set D. This forces the prediction set to be large enough to cover the outcome even after the most effective strategic manipulation.
- Core assumption: The strategic alterations ∆ do not depend on the calibrated threshold t, only on the conformity score s.
- Evidence anchors:
  - [abstract]: "The approach modifies standard conformal prediction by calibrating prediction sets to be robust against strategic alterations using supremum over a set of alteration functions."
  - [section]: "t_strat_α = inf (t ∈ R : 1/(n+1) Σ 1{sup_∆∈D s(∆(Xi), Yi) > t} + 1/(n+1) ≤ α)"

### Mechanism 2
- Claim: Strategic Conformal Prediction maintains valid coverage even when the base model is vulnerable to strategic perturbations.
- Mechanism: The calibration process inherently accounts for model vulnerability by considering strategic alterations during threshold computation. Even if the base model can be fooled by strategic changes, the prediction set size increases accordingly to maintain coverage guarantees.
- Core assumption: The conformity score s captures the vulnerability of the base model to strategic alterations.
- Evidence anchors:
  - [abstract]: "The method provides marginal coverage, training-conditional coverage, and group-conditional extensions while maintaining theoretical guarantees including robustness to model misspecification and interval tightness under stability assumptions."
  - [section]: "if the underlying model is itself vulnerable to strategic perturbations, then these sets will necessarily have to increase accordingly"

### Mechanism 3
- Claim: The method is robust to misspecification of the strategic alteration functions as long as they approximately cover the true strategic behavior.
- Mechanism: Proposition 2.2 shows that coverage under the true strategic alteration ∆* is bounded by the infimum of total variation distances between each ∆ in D and ∆*. As long as D contains approximations to the true strategic behavior, coverage remains close to the nominal level.
- Core assumption: The set D contains strategic alterations that are close in total variation distance to the true strategic alterations.
- Evidence anchors:
  - [section]: "Proposition 2.2 bounds the coverage with ∆* in terms of the infimum of the average deviation terms from Proposition 2.1."
  - [section]: "if there is randomness, then even under significant misspecification the total variation distances will be well below 1"

## Foundational Learning

- Concept: Exchangeability of random variables
  - Why needed here: The coverage guarantees rely on the exchangeability of the calibration data points to apply the quantile lemma and establish valid coverage probabilities.
  - Quick check question: Why can we treat the calibration data and test point as exchangeable in the proof of Theorem 2.1?

- Concept: Total variation distance as a measure of distribution closeness
  - Why needed here: The robustness results use total variation distance to quantify how misspecification of strategic alterations affects coverage guarantees.
  - Quick check question: How does the total variation distance between two strategic alterations relate to the difference in coverage probabilities they produce?

- Concept: Split conformal prediction framework
  - Why needed here: Strategic Conformal Prediction builds directly on split conformal prediction, using separate calibration and test sets to compute prediction sets with guaranteed coverage.
  - Quick check question: What are the key differences between standard split conformal prediction and Strategic Conformal Prediction in terms of the calibration step?

## Architecture Onboarding

- Component map: Base model -> Conformity score computation -> Strategic alteration function generator -> Calibration module -> Prediction set generator -> Evaluation module

- Critical path: Training base model → Computing conformity scores → Generating strategic alterations → Calibrating threshold → Making predictions with prediction sets

- Design tradeoffs:
  - Computational cost vs. coverage guarantees: More strategic alterations in D increase computation but improve robustness
  - Interval size vs. coverage: Conservative strategic alterations lead to larger prediction sets
  - Model complexity vs. vulnerability: More complex base models may be more vulnerable to strategic alterations

- Failure signatures:
  - Coverage significantly below nominal level indicates insufficient strategic alterations in D
  - Extremely large prediction sets suggest overly conservative strategic alterations
  - Inconsistent coverage across datasets may indicate base model vulnerability issues

- First 3 experiments:
  1. Implement basic Strategic Conformal Prediction on a simple regression dataset with a single strategic alteration function
  2. Compare coverage and interval sizes against standard conformal prediction on a classification dataset
  3. Test robustness by varying the strategic alteration functions and measuring coverage degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is Strategic Conformal Prediction when the true strategic alterations deviate significantly from the assumed alteration functions in D?
- Basis in paper: [inferred] from Proposition 2.1 and Proposition 2.2 which show bounds on coverage deviation based on total variation distance between assumed and true alterations, but don't quantify performance under large misspecification
- Why unresolved: The paper provides theoretical bounds showing robustness exists but doesn't empirically test how large deviations can be before coverage guarantees break down completely
- What evidence would resolve it: Experiments testing coverage as a function of increasing misspecification in the alteration functions, perhaps by systematically modifying the utility/cost functions or sampling distributions used to generate strategic alterations

### Open Question 2
- Question: Can Strategic Conformal Prediction be extended to settings where the outcome Y also changes in response to strategic alterations?
- Basis in paper: [explicit] The authors acknowledge this as a limitation in footnote 1: "Though restrictive, this is a typical assumption in the strategic ML literature... Our setup can also be extended to the case where the ∆ alters the outcomes Y, but this has additional subtleties and we leave to future work"
- Why unresolved: The authors explicitly state this extension is left for future work and mention it has "additional subtleties"
- What evidence would resolve it: Development and theoretical analysis of a framework that handles both X and Y alterations, including modified coverage guarantees and experimental validation

### Open Question 3
- Question: How does the computational complexity of Strategic Conformal Prediction scale with the number and complexity of strategic alteration functions in D?
- Basis in paper: [inferred] from the need to compute suprema over D in the calibration procedure and the discussion of computational budget in experiments, but no formal complexity analysis is provided
- Why unresolved: The paper focuses on coverage guarantees but doesn't analyze computational complexity or provide runtime comparisons between different D constructions
- What evidence would resolve it: Formal analysis of time/space complexity as a function of |D|, dimensionality of X, and complexity of individual ∆ functions, plus empirical runtime benchmarks across different D constructions

## Limitations

- The method assumes strategic alterations can be modeled as functions independent of the threshold, which may not hold in practice
- Computational cost increases significantly with the complexity of the strategic alteration set D
- The approach focuses on marginal and training-conditional coverage but doesn't extensively explore group-conditional coverage in experiments

## Confidence

- High confidence in the theoretical framework and coverage guarantees under the stated assumptions
- Medium confidence in the empirical results, given the synthetic nature of strategic alterations and limited real-world validation
- Medium confidence in the robustness claims, as the approximation quality depends heavily on the choice of D

## Next Checks

1. Test the method's performance when strategic alterations depend on the threshold, violating the core assumption.
2. Evaluate the sensitivity of coverage guarantees to misspecification of the strategic alteration set D using real-world strategic behavior data.
3. Assess the computational scalability by testing on high-dimensional datasets with complex strategic alteration functions.