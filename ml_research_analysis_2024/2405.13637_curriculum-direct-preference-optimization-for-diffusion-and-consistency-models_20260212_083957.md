---
ver: rpa2
title: Curriculum Direct Preference Optimization for Diffusion and Consistency Models
arxiv_id: '2405.13637'
source_url: https://arxiv.org/abs/2405.13637
tags:
- curriculum
- diffusion
- training
- reward
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Curriculum Direct Preference Optimization (Curriculum
  DPO), a novel training strategy that enhances Direct Preference Optimization (DPO)
  through curriculum learning for text-to-image generation. The method first ranks
  generated images using a reward model, then creates pairs of examples with varying
  difficulty levels based on their rank differences.
---

# Curriculum Direct Preference Optimization for Diffusion and Consistency Models

## Quick Facts
- **arXiv ID**: 2405.13637
- **Source URL**: https://arxiv.org/abs/2405.13637
- **Reference count**: 40
- **Primary result**: Curriculum DPO outperforms state-of-the-art fine-tuning methods while using 10× fewer training images.

## Executive Summary
This paper introduces Curriculum Direct Preference Optimization (Curriculum DPO), a novel training strategy that enhances Direct Preference Optimization (DPO) through curriculum learning for text-to-image generation. The method ranks generated images using a reward model, creates pairs of examples with varying difficulty levels based on their rank differences, and organizes these pairs into batches from easy to hard for progressive training. The approach was evaluated on nine benchmarks across three tasks using two generative models and three reward models, consistently outperforming state-of-the-art fine-tuning methods while requiring significantly fewer training images.

## Method Summary
Curriculum DPO enhances standard DPO by incorporating curriculum learning principles. The method generates multiple images per prompt using a pre-trained generative model, ranks these images using a reward model, and creates preference pairs with varying difficulty based on rank differences. Easy pairs (images far apart in ranking) are presented first, followed by progressively harder pairs. The approach uses LoRA fine-tuning with a curriculum-structured training process, organizing pairs into batches that progress from easy to hard. The method was tested with Stable Diffusion v1.5 and Latent Consistency Model on datasets for text alignment, aesthetics, and human preference tasks.

## Key Results
- Curriculum DPO achieved 0.3237±0.0012 on human preference task with LCM vs 0.2990±0.0010 for DPO and 0.2952±0.0011 for DDPO
- The method required 10× fewer training images than baseline methods while achieving similar or better performance
- Human evaluation studies confirmed superior performance over state-of-the-art fine-tuning methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Curriculum DPO improves learning efficiency by progressively training on increasingly difficult preference pairs.
- Mechanism: The method ranks generated images using a reward model, then creates pairs with varying difficulty based on rank differences. Easy pairs (far apart in ranking) are presented first, allowing the model to learn coarse preference patterns before refining with harder pairs (close in ranking).
- Core assumption: Preference differences between images correlate with learning difficulty - obvious preferences are easier to learn than subtle ones.
- Evidence anchors:
  - [abstract]: "Generated samples that are far apart in the ranking are considered to form easy pairs, while those that are close in the ranking form hard pairs."
  - [section 3.3]: "Samples that are far apart in the preference ranking form easy pairs, under the assumption that the preferred sample is more easily identifiable thanks to obvious factors."
  - [corpus]: Weak - no direct corpus evidence for this specific curriculum learning mechanism in preference optimization.
- Break condition: If the reward model ranking doesn't correlate with actual human preference difficulty, or if subtle differences are more important than obvious ones for the task.

### Mechanism 2
- Claim: Curriculum DPO achieves comparable performance with 10× fewer training images than baseline methods.
- Mechanism: By organizing training pairs by difficulty and progressively introducing harder examples, the model learns more efficiently, extracting more information from each training sample.
- Core assumption: Structured exposure to preference patterns enables more effective learning than random sampling.
- Evidence anchors:
  - [section 4]: "Curriculum DPO achieves similar scores to DPO and DDPO, while using 10× less training images."
  - [section 4]: "Our ablation study on the number of training samples shows that Curriculum DPO achieves similar performance to Diffusion-DPO and DDPO, while using 10× less images."
  - [corpus]: Moderate - related work shows curriculum learning can improve efficiency in other domains.
- Break condition: If the efficiency gain is primarily due to other factors like better hyperparameter tuning or if the baseline methods are already near-optimal efficiency.

### Mechanism 3
- Claim: The curriculum approach prevents overfitting to reward model biases by setting minimum preference difference thresholds.
- Mechanism: By requiring a minimum rank difference between samples in each pair, the method avoids training on pairs where preference is too subtle to distinguish reliably.
- Core assumption: Very small preference differences are unreliable signals that can cause the model to learn noise or reward model biases.
- Evidence anchors:
  - [section 3.3]: "However, we set a minimum difference threshold in terms of preference, to make sure that the generative model does not learn preference patterns that are indistinguishable."
  - [section 3.3]: "This prevents the generative model from overfitting to the biases of the reward model or the noise in the data."
  - [corpus]: Weak - no direct corpus evidence for this specific regularization mechanism.
- Break condition: If the minimum threshold is set too high and excludes valuable subtle preference information, or if the reward model is perfectly reliable.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Curriculum DPO builds directly on DPO by adding curriculum learning to the training process.
  - Quick check question: What is the key difference between DPO and traditional RLHF approaches?

- Concept: Curriculum Learning
  - Why needed here: The core innovation is applying curriculum learning to preference optimization, organizing training pairs by difficulty.
  - Quick check question: How does organizing training data by difficulty potentially improve learning efficiency?

- Concept: Reward Models for Image Generation
  - Why needed here: The method relies on reward models to rank generated images and create preference pairs.
  - Quick check question: What are the three types of reward models used in this work and what aspects do they evaluate?

## Architecture Onboarding

- Component map:
  Pre-trained generative model -> Reward model for ranking -> Curriculum pair sampler -> DPO training loop with LoRA -> Evaluation pipeline

- Critical path:
  1. Generate M images per prompt using pre-trained model
  2. Rank images using reward model
  3. Create preference pairs with varying difficulty levels
  4. Split pairs into B curriculum batches
  5. Train model progressively through batches using DPO loss
  6. Evaluate on three tasks using three different reward models

- Design tradeoffs:
  - Number of batches (B) vs. training efficiency: More batches provide smoother curriculum but increase complexity
  - Minimum preference difference threshold: Too high loses subtle information, too low risks overfitting
  - M (images per prompt) vs. performance: More images enable better ranking but increase computational cost

- Failure signatures:
  - Performance degrades if reward model ranking is poor or inconsistent
  - Curriculum learning provides no benefit if difficulty ordering doesn't correlate with learning value
  - Training instability if batches are not properly sized or if minimum difference threshold is inappropriate

- First 3 experiments:
  1. Baseline: Run standard DPO with same hyperparameters but random pair sampling
  2. Curriculum ablation: Test different numbers of curriculum batches (B) to find optimal value
  3. Efficiency test: Compare performance using M=5 images per prompt vs. M=50 to verify 10× efficiency claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Curriculum DPO perform when applied to other generative models beyond diffusion and consistency models, such as GANs or autoregressive models?
- Basis in paper: [inferred] The paper demonstrates Curriculum DPO's effectiveness on diffusion and consistency models but does not explore its application to other generative architectures.
- Why unresolved: The paper focuses specifically on diffusion and consistency models, leaving the generalizability to other architectures untested.
- What evidence would resolve it: Experiments applying Curriculum DPO to GANs or autoregressive models with comparable benchmark results.

### Open Question 2
- Question: What is the optimal number of curriculum batches (B) for different tasks and datasets, and how does this hyperparameter affect training efficiency?
- Basis in paper: [explicit] The paper performs ablation studies on B but does not provide a theoretical framework for determining optimal values across different scenarios.
- Why unresolved: The optimal value appears dataset and task-dependent, and the paper only tests a limited range of values.
- What evidence would resolve it: A comprehensive study mapping B values to task complexity and dataset characteristics with theoretical justification.

### Open Question 3
- Question: How does Curriculum DPO's performance scale with increasingly complex prompts that involve multiple objects, attributes, and spatial relationships?
- Basis in paper: [inferred] The experiments use relatively simple prompts (SVO patterns, single objects), but real-world applications often involve more complex instructions.
- Why unresolved: The paper does not test Curriculum DPO on prompts requiring compositional understanding or spatial reasoning.
- What evidence would resolve it: Experiments using prompts with multiple objects, attributes, and spatial relationships, measuring performance degradation or improvement.

### Open Question 4
- Question: What is the relationship between the reward model's bias and the final performance of Curriculum DPO, and how can we detect or mitigate reward model bias?
- Basis in paper: [explicit] The paper mentions avoiding overfitting to reward model biases but does not systematically study this relationship.
- Why unresolved: The paper assumes reward models are fixed and does not explore how their biases propagate through Curriculum DPO training.
- What evidence would resolve it: Analysis showing how different reward model architectures or training data affect Curriculum DPO's outputs, and methods to detect/remediate bias propagation.

## Limitations

- The method's performance heavily depends on the quality of the reward model's ranking, which may not accurately reflect human preference difficulty
- The approach requires generating multiple images per prompt (M) for ranking, increasing computational cost during training
- The study only evaluates three specific reward models, leaving open questions about robustness to different ranking qualities

## Confidence

**High Confidence Claims:**
- Curriculum DPO consistently outperforms baseline methods across all nine benchmarks
- The method achieves better human preference scores than DPO and DDPO
- The training efficiency improvement (10× fewer images) is demonstrated across multiple experiments

**Medium Confidence Claims:**
- The specific mechanisms by which curriculum learning improves preference optimization
- The optimal configuration parameters (number of batches, minimum preference difference threshold)
- The generalization of results to other generative model architectures beyond Stable Diffusion and LCM

**Low Confidence Claims:**
- The assertion that the method prevents overfitting to reward model biases
- The assumption that rank differences directly correlate with learning difficulty
- The scalability of the approach to larger, more diverse datasets

## Next Checks

1. **Reward Model Robustness Test**: Evaluate Curriculum DPO performance using different reward models or combinations of reward models to verify that improvements aren't specific to the three models tested.

2. **Curriculum Structure Sensitivity**: Conduct systematic ablation studies varying the number of curriculum batches (B) and minimum preference difference thresholds to identify optimal configurations and test the sensitivity of results to these parameters.

3. **Cross-Architecture Generalization**: Test Curriculum DPO on additional generative model architectures beyond Stable Diffusion and Latent Consistency Model to verify the approach's generalizability to different diffusion-based generation methods.