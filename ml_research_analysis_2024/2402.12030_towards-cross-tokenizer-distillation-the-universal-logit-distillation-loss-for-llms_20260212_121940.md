---
ver: rpa2
title: 'Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss
  for LLMs'
arxiv_id: '2402.12030'
source_url: https://arxiv.org/abs/2402.12030
tags:
- loss
- teacher
- text
- student
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Universal Logit Distillation (ULD) loss,
  a novel method for distilling knowledge from large language models (LLMs) to smaller
  ones, even when they use different tokenizers. ULD leverages the Wasserstein distance
  with a closed-form solution, overcoming the limitations of traditional logit-based
  distillation methods that require shared vocabularies.
---

# Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs

## Quick Facts
- arXiv ID: 2402.12030
- Source URL: https://arxiv.org/abs/2402.12030
- Authors: Nicolas Boizard; Kevin El Haddad; Céline Hudelot; Pierre Colombo
- Reference count: 40
- Key outcome: Introduces Universal Logit Distillation (ULD) loss enabling knowledge distillation across models with different tokenizers, achieving up to 2.3 point improvements in F1 score and BERTScore while requiring only half the dataset size.

## Executive Summary
This paper addresses the challenge of knowledge distillation when teacher and student models use different tokenizers by introducing the Universal Logit Distillation (ULD) loss. ULD leverages the Wasserstein distance with a closed-form solution to align output probability distributions without requiring shared vocabularies, overcoming limitations of traditional KL divergence-based methods. Experimental results across extractive QA, generative QA, and summarization tasks demonstrate that ULD consistently outperforms standard teacher-generated text distillation, enabling effective distillation with smaller models and reduced dataset sizes while mitigating overfitting issues.

## Method Summary
The ULD loss replaces KL divergence with Wasserstein distance to compute the minimal cost of transporting probability mass between teacher and student output distributions. By assuming uniform transport costs and equal support, the method achieves a computationally tractable closed-form solution that operates purely on output logits without requiring shared vocabularies. The training process involves freezing a large teacher model to generate synthetic answers, then training a smaller student model using the ULD loss computed from teacher logits. This black-box approach works across different model architectures since it only requires output probability distributions rather than internal features or shared token vocabularies.

## Key Results
- ULD outperforms teacher-generated text distillation by up to 2.3 points in F1 score and BERTScore across multiple tasks
- Enables effective distillation with only half the dataset size (50% of SQuAD) while maintaining performance
- Successfully transfers knowledge from decoder-only models (LLama 2) to encoder-decoder models (MT0-580m), with student slightly outperforming teacher on DIALOGSum
- Consistent improvements across extractive QA (SQuAD), generative QA (QED, FairytaleQA, PubMedQA), and summarization (DIALOGSum) tasks

## Why This Works (Mechanism)

### Mechanism 1
ULD enables distillation across models with different tokenizers by replacing KL divergence with Wasserstein distance, which does not require identical support or absolute continuity. The Wasserstein distance computes the minimal cost to transport probability mass between teacher and student output distributions, treating them as probability measures over their respective vocabularies. Under uniform cost and equal support assumptions, it achieves a closed-form solution that becomes computationally tractable for large vocabularies.

### Mechanism 2
ULD stabilizes training and mitigates overfitting compared to raw text distillation by aligning the full output distribution rather than just the gold token. The student learns the teacher's confidence calibration across all tokens, not just the target, which regularizes learning and prevents overfitting to the exact gold token sequence. This distributional alignment helps the student capture the teacher's relative confidences that improve generalization.

### Mechanism 3
ULD transfers knowledge effectively even when teacher and student have different architectures (e.g., decoder to encoder-decoder) because it operates purely on output logits and does not rely on internal model features or shared vocabularies. Since ULD only requires probability distributions over tokens, it can distill between any pair of generative models regardless of architecture, as long as they produce probability distributions over tokens.

## Foundational Learning

- Concept: Optimal transport theory and Wasserstein distance
  - Why needed here: ULD loss is built on the Wasserstein distance, which requires understanding how to measure and minimize the cost of transforming one probability distribution into another.
  - Quick check question: What is the main advantage of the Wasserstein distance over KL divergence when comparing probability distributions with different supports?

- Concept: Knowledge distillation framework and loss design
  - Why needed here: The paper builds on standard KD formulations (cross-entropy + distillation loss) but modifies the distillation loss to work across tokenizers.
  - Quick check question: In the standard KD loss formula L = LCE + λ × LKD, what role does the λ parameter play?

- Concept: Computational complexity and closed-form solutions
  - Why needed here: The Wasserstein distance is normally O(n³ log n), but ULD uses a closed-form O(n log n) solution under specific assumptions.
  - Quick check question: What assumptions about the transport cost and support length enable the closed-form solution for the Wasserstein distance used in ULD?

## Architecture Onboarding

- Component map: Teacher model (frozen) → Generate synthetic answers → Compute teacher logits → Student model training loop: (1) Forward pass with student on teacher-generated data, (2) Compute ULD loss using teacher logits, (3) Backpropagate to update student only
- Critical path: Teacher inference → Logit extraction → Student training (with ULD loss). The bottleneck is teacher inference and logit computation, which must be done once per batch.
- Design tradeoffs: Uniform transport cost assumption simplifies computation but may lose token-level semantic nuance; closed-form solution enables scalability but requires padding distributions to equal length.
- Failure signatures: If student performance degrades compared to raw text distillation, check whether the teacher's probability distribution is well-calibrated; if training is unstable, verify that the Wasserstein distance implementation is numerically stable for small probabilities.
- First 3 experiments:
  1. Verify ULD loss reduces to standard cross-entropy when teacher and student share identical tokenizers and the teacher assigns probability 1 to the gold token.
  2. Compare ULD loss vs raw text distillation on a small dataset (e.g., 25% of SQuAD) to confirm the 50% dataset size claim.
  3. Test ULD loss with non-uniform transport costs (e.g., Levenshtein distance) on a small vocabulary pair to assess the impact of the uniform cost assumption.

## Open Questions the Paper Calls Out

### Open Question 1
How does the ULD loss perform when the teacher and student models have vastly different architectures, such as distilling a large decoder-only model to a much smaller encoder-decoder model? The paper mentions distilling a Llama teacher to an MT0-580m student but only on three datasets. A more comprehensive study across diverse architectures and tasks would be needed to fully understand the performance of ULD loss in cross-architecture distillation.

### Open Question 2
What is the impact of using different transport cost matrices, such as the Levenshtein distance or embedding L2 norm, on the performance of ULD loss? The paper conducted limited experiments comparing uniform transport costs with non-uniform costs, showing no consistent improvement but with significant computational cost. A more comprehensive study with a wider range of models and tasks would be needed to fully understand the impact of different transport cost matrices.

### Open Question 3
How does the performance of ULD loss compare to other knowledge distillation methods, such as attention-based or feature-based distillation, when the teacher and student models have different architectures? The paper does not provide a direct comparison between ULD loss and other knowledge distillation methods in such scenarios. A comparative study would be needed to understand the relative strengths and weaknesses of different approaches.

## Limitations
- The uniform transport cost and equal support assumptions may not hold across different tokenizer pairs, potentially limiting performance when semantic similarity strongly affects token relationships
- Evaluation is limited to specific datasets and model pairs, with the claim about 50% dataset size reduction demonstrated only on SQuAD
- ULD transfers the teacher's probability distribution, potentially amplifying any calibration issues or biases present in the teacher's outputs

## Confidence

**High Confidence**: The core mechanism of using Wasserstein distance instead of KL divergence for cross-tokenizer distillation is well-founded theoretically and demonstrates consistent improvements over the teacher-generated text baseline across multiple tasks and model architectures.

**Medium Confidence**: The claims about ULD enabling effective distillation with smaller models (160M parameters) and reduced dataset sizes (50%) are supported by experiments but would benefit from broader validation across more diverse model sizes and dataset fractions.

**Low Confidence**: The claim that ULD "slightly outperforms the teacher" when distilling from decoder to encoder-decoder models is based on a single comparison and lacks statistical significance testing.

## Next Checks

1. **Assumption Sensitivity Analysis**: Systematically vary the transport cost function (uniform vs. Levenshtein distance vs. learned semantic similarity) on a controlled set of small vocabularies to quantify the impact of the uniform cost assumption on distillation performance.

2. **Teacher Calibration Impact Study**: Compare ULD performance when using calibrated vs. uncalibrated teacher models on the same tasks to determine whether ULD amplifies or mitigates teacher miscalibration issues.

3. **Cross-Domain Generalization Test**: Evaluate ULD on out-of-domain datasets (e.g., cross-lingual tasks or specialized domains like legal or medical text) to assess whether the improvements generalize beyond the standard benchmarks used in the paper.