---
ver: rpa2
title: Multi-Agent Deep Q-Network with Layer-based Communication Channel for Autonomous
  Internal Logistics Vehicle Scheduling in Smart Manufacturing
arxiv_id: '2411.00728'
source_url: https://arxiv.org/abs/2411.00728
tags:
- scheduling
- each
- jobs
- learning
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses autonomous internal logistics vehicle scheduling
  in smart manufacturing, focusing on minimizing total job tardiness, the number of
  tardy jobs, and vehicle energy consumption. The core method is a multi-agent deep
  Q-network (MADQN) with a layer-based communication channel (LBCC), where each job
  is an agent that makes decisions about workstation and AIV selection.
---

# Multi-Agent Deep Q-Network with Layer-based Communication Channel for Autonomous Internal Logistics Vehicle Scheduling in Smart Manufacturing

## Quick Facts
- arXiv ID: 2411.00728
- Source URL: https://arxiv.org/abs/2411.00728
- Authors: Mohammad Feizabadi; Arman Hosseini; Zakaria Yahouni
- Reference count: 26
- One-line primary result: MADQN with LBCC outperforms nine heuristics in minimizing job tardiness, tardy jobs, and AIV energy consumption

## Executive Summary
This paper presents a multi-agent deep Q-network (MADQN) with layer-based communication channel (LBCC) for autonomous internal logistics vehicle scheduling in smart manufacturing. The approach treats each job as an independent agent that simultaneously selects workstations and Automated Guided Vehicles (AIVs) to minimize total job tardiness, number of tardy jobs, and vehicle energy consumption. Evaluated against nine heuristic methods across varying job counts and dynamic conditions, MADQN demonstrates superior performance with stable results as job counts increase, reducing total tardiness by approximately 15,000 time units and energy consumption by about 70% compared to the best heuristic methods.

## Method Summary
The method employs a decentralized multi-agent DQN architecture where each job operates as an independent agent with two DQNs: one for workstation selection and another for AIV selection. The layer-based communication channel enables coordination by sharing hidden-layer outputs across agents, mitigating non-stationarity issues common in multi-agent environments. The approach handles flexible job shop routing through masking in the workstation-selection DQN, ensuring agents only select valid actions. Training involves normalization of inputs, SGD optimization with learning rate 0.01 and discount factor 0.9, and evaluation across 100 simulations for job counts ranging from 20 to 100.

## Key Results
- MADQN outperforms nine heuristic methods in all three objectives: total tardiness, number of tardy jobs, and energy consumption
- Performance remains stable as job counts increase, with MADQN reducing total tardiness by approximately 15,000 time units compared to the best heuristic with 40 jobs
- Energy consumption is reduced by about 70%, decreasing recharging needs and improving operational efficiency
- MADQN achieves lower total tardiness than all heuristic combinations across all tested scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decentralized multi-agent decision-making improves scalability and adaptability in dynamic job shop scheduling
- Mechanism: Each job is treated as an independent agent that simultaneously selects workstations and AIVs using two DQNs. The layer-based communication channel (LBCC) allows agents to share hidden-layer outputs, enabling coordination without a central controller
- Core assumption: Decentralized agents can effectively approximate global scheduling performance when coordinated via LBCC
- Evidence anchors:
  - [abstract] "multi-agent deep Q-network (MADQN) with a layer-based communication channel (LBCC) to dynamically allocate vehicles"
  - [section 4.2] "A key advantage of multi-agent systems lies in their capacity for inter-agent communication, which substantially enhances system performance"
- Break condition: If agent observations become too sparse or conflicting, coordination may break down leading to suboptimal decisions

### Mechanism 2
- Claim: LBCC mitigates non-stationarity in multi-agent environments by sharing information across agents' hidden layers
- Mechanism: Each agent's hidden layer output is concatenated with outputs from other agents at the same layer, creating a communication channel that provides global context for local decisions
- Core assumption: Hidden-layer communication provides sufficient information for agents to adapt to others' changing policies without direct policy sharing
- Evidence anchors:
  - [section 4.2] "within each hidden layer of a job agent's network, the input to that hidden layer... is combined with the output of the same hidden layer from all other job agents"
  - [section 2.4] "A key advantage of multi-agent systems lies in their capacity for inter-agent communication"
- Break condition: If the number of agents grows too large, the communication channel may become noisy and degrade performance

### Mechanism 3
- Claim: Masking in DQN output layers handles variable action space sizes for workstation selection
- Mechanism: When an operation can only be processed on certain workstations, unavailable options are masked with -∞ values, ensuring the agent only selects valid actions
- Core assumption: The DQN can effectively learn optimal policies even with dynamic masking of action space
- Evidence anchors:
  - [section 4.1] "This mask delineates the feasible actions (workstations) available... the network's output is adjusted to −∞ where no viable action exists"
  - [section 3] describes flexible job shop routing where operations can be processed on multiple workstations
- Break condition: If masking is too restrictive or too permissive, the agent may fail to learn optimal policies

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The scheduling problem is modeled as an MDP where states represent system configuration, actions represent scheduling decisions, and rewards reflect scheduling performance
  - Quick check question: What are the state, action, and reward components in the job scheduling MDP?

- Concept: Deep Q-Learning
  - Why needed here: DQN approximates the Q-function for complex scheduling decisions that cannot be solved analytically due to problem size
  - Quick check question: How does the DQN approximate Q-values differently from traditional Q-learning?

- Concept: Multi-Agent Reinforcement Learning
  - Why needed here: Multiple jobs require simultaneous scheduling decisions, necessitating a framework where agents can learn to coordinate without centralized control
  - Quick check question: What challenges arise in multi-agent RL that don't exist in single-agent RL?

## Architecture Onboarding

- Component map: Job arrival → Agent observation → Workstation selection → AIV selection → State transition → Reward calculation → Agent update
- Critical path: Job arrival → Agent observation → Workstation selection → AIV selection → State transition → Reward calculation → Agent update
- Design tradeoffs:
  - Decentralized vs. centralized control (chosen: decentralized for scalability)
  - Full vs. partial observability (chosen: partial with LBCC for efficiency)
  - Fixed vs. variable action space (chosen: variable with masking for flexibility)
- Failure signatures:
  - Increasing tardiness despite learning iterations
  - AIVs consistently low on charge while jobs wait
  - Workstation queues growing despite available AIVs
- First 3 experiments:
  1. Single job, single workstation, single AIV - verify basic DQN learning
  2. Multiple jobs, fixed routing, single AIV - test AIV selection logic
  3. Multiple jobs, flexible routing, multiple AIVs - validate full MADQN with LBCC

## Open Questions the Paper Calls Out

- Question: How would the performance of MADQN with LBCC change if higher-capacity AIVs were considered?
  - Basis in paper: [explicit] The paper mentions that while they focused on AIVs with a capacity of two jobs, exploring higher-capacity AIVs could introduce further complexities in determining the most efficient policies for loading and unloading
  - Why unresolved: The paper did not test the approach with higher-capacity AIVs, leaving the impact on performance unknown
  - What evidence would resolve it: Testing MADQN with LBCC using AIVs with capacities greater than two and comparing the results to the current findings

- Question: How would different recharging policies for AIVs affect the production objectives?
  - Basis in paper: [explicit] The paper assumed a recharging policy where AIVs recharge when their battery level drops below 40%, and suggests that exploring different charging strategies could provide insights into their effects on production objectives
  - Why unresolved: The paper only tested one recharging policy, and the impact of alternative policies remains unexplored
  - What evidence would resolve it: Implementing and comparing various recharging policies within the MADQN framework to assess their influence on job tardiness and energy consumption

- Question: How generalizable is the proposed approach across different shop configurations?
  - Basis in paper: [explicit] The paper tested the approach under varying numbers of job arrivals but did not test it under differing shop configurations, which could provide valuable proof of the method's ability to generalize to various circumstances
  - Why unresolved: The study focused on a specific shop configuration, and its adaptability to other configurations is not demonstrated
  - What evidence would resolve it: Applying MADQN with LBCC to shop configurations with different numbers of job types, machines, and AIVs to evaluate its effectiveness and generalizability

## Limitations

- The comparison with only nine heuristic baselines may not capture the full landscape of scheduling algorithms
- The evaluation focuses on a specific manufacturing configuration (4 workstations, 2 AIVs) that may not generalize to larger or differently structured systems
- Computational requirements for training multiple DQN agents are not discussed, which could limit practical deployment

## Confidence

**High Confidence**: The technical implementation of the MADQN architecture, including the layer-based communication channel and masking mechanisms, appears sound and well-documented. The reported performance improvements over heuristics are supported by specific numerical comparisons.

**Medium Confidence**: The generalization claims to varying job counts are reasonable given the experimental results, but the performance trends with system size could be more thoroughly characterized. The energy consumption metrics are specific but depend on the assumed energy model.

**Low Confidence**: The scalability claims beyond the tested scenarios (up to 100 jobs) are speculative without additional experiments. The robustness to different manufacturing layouts or vehicle types is not explored.

## Next Checks

1. **Sensitivity Analysis**: Test the MADQN performance across different manufacturing layouts (varying numbers of workstations and AIVs) to assess scalability claims

2. **Algorithm Comparison**: Benchmark against state-of-the-art single-agent RL approaches and traditional optimization methods to contextualize the multi-agent advantage

3. **Training Stability**: Conduct multiple training runs with different random seeds to quantify variance in performance and assess the reliability of reported results