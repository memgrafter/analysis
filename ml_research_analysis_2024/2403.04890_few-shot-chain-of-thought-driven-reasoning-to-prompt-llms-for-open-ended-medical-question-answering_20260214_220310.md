---
ver: rpa2
title: Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical
  question answering
arxiv_id: '2403.04890'
source_url: https://arxiv.org/abs/2403.04890
tags:
- medical
- reasoning
- dataset
- prompts
- fewshot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MEDQA-OPEN, a modified open-ended version
  of the MedQA-USMLE dataset to better reflect real clinical scenarios. The authors
  develop CLINICR, a Chain-of-Thought (CoT) reasoning prompt that incrementally builds
  medical context to mimic human diagnostic reasoning, outperforming existing 5-shot
  CoT methods.
---

# Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering

## Quick Facts
- arXiv ID: 2403.04890
- Source URL: https://arxiv.org/abs/2403.04890
- Authors: Saeel Sandeep Nachane; Ojas Gramopadhye; Prateek Chanda; Ganesh Ramakrishnan; Kshitij Sharad Jadhav; Yatin Nandwani; Dinesh Raghu; Sachindra Joshi
- Reference count: 12
- Key outcome: CLINICR Chain-of-Thought prompt achieves 82-89% expert agreement on open-ended medical questions by incrementally building medical context

## Executive Summary
This paper addresses the challenge of open-ended medical question answering by introducing MEDQA-OPEN, a modified version of the MedQA-USMLE dataset that better reflects real clinical scenarios. The authors develop CLINICR, a Chain-of-Thought reasoning prompt that incrementally builds medical context to mimic human diagnostic reasoning, outperforming existing 5-shot CoT methods. They also propose an approach using differential diagnosis generation via MCQ-CLINICR followed by MCQ-ELIMINATIVE selection, and a reward model-based verifier to ensure response accuracy. Human evaluation across 100 open-ended questions shows CLINICR achieves 82–89% expert agreement, with the verifier-driven selection improving correctness and reasoning quality, particularly for smaller Llama2-7B models.

## Method Summary
The method involves converting MedQA-USMLE multiple-choice questions to open-ended format (MEDQA-OPEN), then applying CLINICR prompt to generate incremental reasoning responses using Llama2 models. The approach generates differential diagnoses through MEDCODEX (prospective reasoning) and selects final answers using CODEX (eliminative selection) or a reward model verifier trained on expert-annotated question-reasoning-answer triplets. The verifier learns to distinguish high-quality from low-quality reasoning through pairwise "chosen-rejected" comparisons.

## Key Results
- CLINICR Chain-of-Thought prompt achieves 82–89% expert agreement on open-ended medical questions
- Incremental reasoning approach outperforms elimination-based prompting for open-ended questions
- Verifier-driven selection improves accuracy and reasoning quality, especially for Llama2-7B models
- Differential diagnosis generation followed by eliminative selection mirrors real clinical practice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incremental reasoning-driven prompting (MEDCODEX) outperforms elimination-based prompting (CODEX) for open-ended medical questions.
- Mechanism: MEDCODEX builds medical context step-by-step, mimicking clinical diagnostic reasoning, which is more natural for open-ended questions without predefined options.
- Core assumption: Human diagnostic reasoning in medicine is additive and contextual rather than eliminative.
- Evidence anchors:
  - [abstract]: "CLINICR, a Chain-of-Thought (CoT) reasoning prompt that incrementally builds medical context to mimic human diagnostic reasoning, outperforming existing 5-shot CoT methods."
  - [section]: "Due to its inherent eliminative approach, the CODEX FEWSHOT PROMPTS has a smaller search space and results in higher accuracy." (Table 2)
  - [corpus]: Weak - related work focuses on structured data extraction and close-ended QA, not incremental reasoning.

### Mechanism 2
- Claim: Generating multiple differential diagnoses followed by eliminative selection improves accuracy on open-ended questions.
- Mechanism: MEDCODEX first generates a diverse set of possible diagnoses (forward reasoning), then CODEX selects the most appropriate one (backward reasoning), combining strengths of both approaches.
- Core assumption: Forward reasoning can generate plausible options even without predefined choices, and eliminative reasoning can effectively select from these options.
- Evidence anchors:
  - [abstract]: "We also present an approach that mirrors real-life clinical practice by first exploring multiple differential diagnoses through MCQ-CLINICR and subsequently narrowing down to a final diagnosis using MCQ-ELIMINATIVE."
  - [section]: "As indicated in Figures 6 and 7, for 80% and 89.5% of the questions, the expert medical personnel agreed with the reasoning and the evaluation provided by the Differential diagnosis generation... followed by CODEX FEWSHOT PROMPTS for Selection..."
  - [corpus]: Missing - no direct evidence in corpus about differential diagnosis generation approaches.

### Mechanism 3
- Claim: A reward model-based verifier trained on medical expert evaluations improves answer selection quality.
- Mechanism: The verifier learns to distinguish between high-quality and low-quality reasoning by training on pairs of correct and incorrect responses annotated by medical experts.
- Core assumption: Medical experts can reliably distinguish between good and poor reasoning, and this distinction generalizes to the verifier's selection task.
- Evidence anchors:
  - [abstract]: "Finally, emphasizing the importance of response verification in medical settings, we utilize a reward model mechanism, replacing the elimination process performed by MCQ-ELIMINATIVE."
  - [section]: "We use pairwise 'chosen-rejected' pairs of strings consisting of ⟨qi; Ri; Ai⟩<Question-Reasoning-Answer> triplets to train the reward model LMrw ϕ."
  - [corpus]: Weak - related work focuses on structured data extraction and Arabic clinical QA, not reward model verification.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: Medical diagnosis requires multi-step reasoning that builds context progressively, which CoT can facilitate.
  - Quick check question: Can you explain how step-by-step reasoning differs from direct answer generation in complex problem-solving?

- Concept: Few-shot prompting
  - Why needed here: The approach uses 5-shot examples to demonstrate the reasoning pattern to the LLM, which is essential for guiding the model's behavior.
  - Quick check question: What is the purpose of providing multiple examples in few-shot prompting, and how does it affect model performance?

- Concept: Differential diagnosis in medicine
  - Why needed here: The approach explicitly mimics this clinical practice by generating multiple possible diagnoses before selecting the most appropriate one.
  - Quick check question: How does the process of differential diagnosis in clinical practice relate to the forward-then-eliminative reasoning approach used in this paper?

## Architecture Onboarding

- Component map:
  MEDQA-OPEN dataset -> CLINICR prompt -> MEDCODEX generation -> CODEX selection -> Reward model verifier -> Human evaluation

- Critical path:
  1. Convert MedQA-USMLE questions to open-ended format (MEDQA-OPEN)
  2. Apply CLINICR prompt to generate incremental reasoning responses
  3. Use MEDCODEX to generate differential diagnoses
  4. Apply CODEX to select final answer from generated options
  5. (Optional) Use verifier to validate/replace CODEX selection
  6. Evaluate with medical experts

- Design tradeoffs:
  - Few-shot examples (5) vs. single-shot: More examples provide better guidance but increase prompt length and cost
  - Model size (7B vs 70B): Smaller models are more resource-efficient but may lack reasoning depth
  - Verifier vs. CODEX selection: Verifier potentially more accurate but requires additional training data and computation

- Failure signatures:
  - Poor performance on questions requiring factual recall rather than reasoning
  - Failure to generate diverse differential diagnoses (forward reasoning collapse)
  - Verifier overfitting to training data patterns
  - Human evaluation showing disagreement on reasoning quality

- First 3 experiments:
  1. Compare MEDCODEX vs CODEX on a small subset of MEDQA-OPEN with human evaluation to validate the incremental reasoning hypothesis
  2. Test differential diagnosis generation with MEDCODEX followed by CODEX selection on 20 questions to assess the two-stage approach
  3. Train a minimal verifier on 50 annotated examples and evaluate its selection accuracy against CODEX on a held-out set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the incremental reasoning approach compare to the CODEX approach when using larger Llama2 models (e.g., Llama2-70B) on open-ended medical questions?
- Basis in paper: [explicit] The paper states that the incremental reasoning approach performs better on smaller Llama2-7B models, but the CODEX approach is better able to leverage larger models like Llama2-70B.
- Why unresolved: The paper only reports results for Llama2-7B and Llama2-70B on the MEDQA-N O-OPT dataset, but does not directly compare the performance of the incremental reasoning approach to the CODEX approach on the larger Llama2-70B model.
- What evidence would resolve it: Running the incremental reasoning approach on the Llama2-70B model and comparing its performance to the CODEX approach on the same model and dataset.

### Open Question 2
- Question: How generalizable is the incremental reasoning approach to other open-source language models beyond Llama2?
- Basis in paper: [explicit] The paper only demonstrates the approach on Llama2 models and mentions that future work will focus on testing the approach on other open-source LLMs.
- Why unresolved: The paper does not provide any results or analysis on the performance of the incremental reasoning approach on language models other than Llama2.
- What evidence would resolve it: Testing the incremental reasoning approach on other open-source language models (e.g., GPT-3, PaLM) and comparing its performance to Llama2.

### Open Question 3
- Question: How does the performance of the verifier-driven selection approach compare to other selection methods (e.g., self-consistency, majority voting) when using the incremental reasoning approach?
- Basis in paper: [explicit] The paper introduces a verifier-driven selection approach and shows that it improves performance compared to the CODEX approach, but does not compare it to other selection methods.
- Why unresolved: The paper does not provide any results or analysis on the performance of the verifier-driven selection approach compared to other selection methods.
- What evidence would resolve it: Comparing the performance of the verifier-driven selection approach to other selection methods (e.g., self-consistency, majority voting) when using the incremental reasoning approach on the same dataset and model.

## Limitations

- The paper's human evaluation scale is unclear, with methodology describing only 10 questions while abstract claims 100-question evaluation
- Limited comparison to baseline 5-shot CoT methods, with unclear specification of what these baselines are
- Potential internal inconsistency between claims about CLINICR superiority and statements about CODEX having "higher accuracy"

## Confidence

**High Confidence**: The paper successfully demonstrates that incremental CoT reasoning (CLINICR) can be implemented and evaluated on medical question answering tasks. The methodology for creating MEDQA-OPEN from MedQA-USMLE is clearly specified, and the basic framework for differential diagnosis generation followed by selection is technically sound.

**Medium Confidence**: The claim that CLINICR outperforms 5-shot CoT methods is supported by human evaluation data, but the limited scale of this evaluation (potentially only 10 questions) reduces confidence in the generalizability of these results. The verifier mechanism appears technically feasible, but without more detail on training data and validation, confidence in its practical effectiveness is moderate.

**Low Confidence**: The assertion that CLINICR achieves 82-89% expert agreement across "100 open-ended questions" is questionable given the methodology only explicitly describes 10-question expert evaluation. The paper's internal consistency is also low confidence due to apparent contradictions between different sections regarding which approach performs better.

## Next Checks

1. **Scale Verification Check**: Replicate the human evaluation process on the full set of 100 MEDQA-OPEN questions with medical experts to verify whether the 82-89% agreement rate holds across the complete dataset, not just the initial 10-question sample.

2. **Verifier Generalization Test**: Conduct a systematic evaluation of the reward model verifier's performance on held-out test sets with varying characteristics (different question types, difficulty levels, and reasoning patterns) to assess whether it generalizes beyond the training data distribution.

3. **Baseline Comparison Audit**: Implement and test at least three specific baseline 5-shot CoT methods mentioned in related literature on the MEDQA-OPEN dataset, providing direct quantitative comparisons with CLINICR to validate the claimed performance improvements.