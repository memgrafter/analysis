---
ver: rpa2
title: Non-transferable Pruning
arxiv_id: '2410.08015'
source_url: https://arxiv.org/abs/2410.08015
tags:
- learning
- pruning
- target
- domain
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses intellectual property protection for pre-trained
  deep neural networks by preventing unauthorized transfer learning. It proposes Non-Transferable
  Pruning (NTP), which combines ADMM-based model pruning with Fisher space regularization
  to selectively reduce model transferability to unauthorized target domains.
---

# Non-transferable Pruning

## Quick Facts
- arXiv ID: 2410.08015
- Source URL: https://arxiv.org/abs/2410.08015
- Authors: Ruyi Ding; Lili Su; Aidong Adam Ding; Yunsi Fei
- Reference count: 40
- Key outcome: NTP achieves average SLC-AUC of -0.54 across diverse dataset pairs, preventing unauthorized transfer learning

## Executive Summary
This paper addresses the critical challenge of protecting pre-trained deep neural networks from unauthorized transfer learning. The proposed Non-Transferable Pruning (NTP) framework combines ADMM-based model pruning with Fisher space regularization to selectively degrade model transferability to unauthorized target domains while preserving source domain performance. NTP introduces a novel evaluation metric, Area Under Sample-wise Learning Curve (SLC-AUC), which comprehensively measures transferability across various fine-tuning data scales. The method demonstrates effectiveness across diverse dataset pairs and model architectures, including both supervised and self-supervised models.

## Method Summary
NTP employs ADMM optimization to solve a constrained problem balancing sparsity and non-transferability. The framework integrates Fisher space discriminative regularization with the target domain to collapse inter-class feature separation, making fine-tuning ineffective. By pruning weights important for target domain performance while preserving source domain capabilities, NTP creates models that maintain functionality in authorized domains but resist unauthorized transfer learning. The method operates through iterative optimization of weight updates, sparsity enforcement, and regularization terms.

## Key Results
- Achieves average SLC-AUC of -0.54 across diverse dataset pairs, indicating successful prevention of unauthorized transfer learning
- Outperforms state-of-the-art approaches in preventing unauthorized transfer while maintaining source domain accuracy
- Works effectively for both supervised models (VGG-11, ResNet-18, ResNet-50) and self-supervised models (SimCLR, MoCo, MoCo-v2)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model pruning combined with Fisher space regularization selectively degrades transferability to unauthorized domains while preserving source domain performance.
- Mechanism: The pruning removes weights important for target domain performance while Fisher regularization collapses inter-class feature separation in the target domain, making fine-tuning ineffective.
- Core assumption: Certain weights in a DNN are specifically important for target domain generalization but not for source domain performance.
- Evidence anchors:
  - [abstract]: "Selective pruning can deliberately diminish a model's suitability on unauthorized domains, even with full fine-tuning."
  - [section]: "Our framework employs the alternating direction method of multipliers (ADMM) for optimizing both the model sparsity and an innovative non-transferable learning loss, augmented with Fisher space discriminative regularization"
  - [corpus]: Weak evidence - corpus focuses on related NTL attacks but doesn't directly address the pruning+Fisher combination mechanism
- Break condition: If attackers have sufficient data and compute resources to overcome collapsed feature space through extensive fine-tuning, or if pruning removes weights critical for source domain performance.

### Mechanism 2
- Claim: The SLC-AUC metric provides a comprehensive evaluation of transferability across different fine-tuning data scales.
- Mechanism: By measuring the area between transfer learning and training-from-scratch learning curves across various sample sizes, SLC-AUC captures how much pre-trained knowledge helps at different data scales.
- Core assumption: Transfer learning benefits vary significantly with available training data, and a single-point accuracy measurement is insufficient.
- Evidence anchors:
  - [abstract]: "We also propose a novel effective metric to measure the model non-transferability: Area Under the Sample-wise Learning Curve (SLC-AUC)"
  - [section]: "SLC-AUC evaluates the volume of fine-tuning data required by an attacker to redirect the model for the target domain"
  - [corpus]: Weak evidence - corpus doesn't provide specific evidence about SLC-AUC metric effectiveness
- Break condition: If attackers can access sufficient training data to make the SLC-AUC positive regardless of pruning strategy.

### Mechanism 3
- Claim: ADMM-based optimization effectively balances sparsity and non-transferability objectives.
- Mechanism: ADMM reformulates the constrained pruning problem into separate subproblems for weight optimization and sparsity enforcement, allowing efficient joint optimization.
- Core assumption: ADMM can find a good solution to the non-convex optimization problem that balances source domain performance with target domain non-transferability.
- Evidence anchors:
  - [abstract]: "Our framework employs the alternating direction method of multipliers (ADMM) for optimizing both the model sparsity and an innovative non-transferable learning loss"
  - [section]: "We use the ADMM-based model pruning strategy... to obtain a NTP pre-trained model, we solve the following constrained optimization problem"
  - [corpus]: Weak evidence - corpus doesn't provide specific evidence about ADMM effectiveness in this context
- Break condition: If ADMM optimization gets stuck in poor local minima or if hyperparameters are poorly tuned, leading to either insufficient pruning or excessive source domain degradation.

## Foundational Learning

- Concept: Alternating Direction Method of Multipliers (ADMM)
  - Why needed here: ADMM provides an efficient way to solve the constrained optimization problem of achieving both sparsity and non-transferability simultaneously
  - Quick check question: What are the three main steps in each ADMM iteration for this pruning problem?

- Concept: Fisher Discriminant Analysis
  - Why needed here: Fisher analysis measures class separability in feature space, and collapsing this separability reduces transferability to unauthorized domains
  - Quick check question: How does maximizing Fisher ratio differ from minimizing it in terms of class separability?

- Concept: Sample-wise Learning Curve (SLC)
  - Why needed here: SLC captures how transfer learning benefits change with available training data, providing a more complete picture than single-point accuracy
  - Quick check question: What would a negative SLC-AUC indicate about a model's transferability?

## Architecture Onboarding

- Component map: ADMM optimizer -> Weight update subproblem -> Sparsity subproblem -> Fisher regularization -> Source domain loss + target domain non-transferability loss
- Critical path: ADMM weight update -> sparsity enforcement -> Fisher space regularization -> loss computation -> next iteration
- Design tradeoffs: Higher sparsity provides better non-transferability but risks source domain performance; Fisher regularization strength affects transferability but may impact source domain too
- Failure signatures: Positive SLC-AUC scores (indicating successful transfer learning), significant source domain accuracy drop, ADMM convergence issues
- First 3 experiments:
  1. Verify basic NTP functionality: Run NTP on a simple source-target pair (e.g., MNIST to MNIST-M) and check if SLC-AUC becomes negative
  2. Test sparsity sensitivity: Vary the target sparsity level and observe the trade-off between SLC-AUC and source domain accuracy
  3. Validate Fisher regularization: Compare NTP with and without Fisher regularization on the same task to confirm its contribution to non-transferability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of NTP scale with increasingly complex model architectures beyond ResNet-50 and VGG-11, particularly for modern transformer-based architectures?
- Basis in paper: [inferred] The paper demonstrates NTP effectiveness on VGG-11, ResNet-18, ResNet-50, and SimCLR/MoCo self-supervised models, but does not explore transformer architectures or more recent model families that dominate modern computer vision.
- Why unresolved: The paper focuses on traditional CNN architectures and does not provide evidence about how NTP performs on more complex or fundamentally different architectures like vision transformers, which have different parameter distributions and transfer learning dynamics.
- What evidence would resolve it: Systematic experiments applying NTP to vision transformer architectures (e.g., ViT, Swin) and comparing transferability reduction metrics against CNN baselines would demonstrate whether NTP's pruning and regularization approach generalizes across architectural paradigms.

### Open Question 2
- Question: What is the relationship between the choice of source and target domain similarity and the effectiveness of NTP's non-transferability guarantees, particularly for domains with overlapping but semantically distinct classes?
- Basis in paper: [explicit] The paper notes that transferability depends on dataset characteristics, citing that SN (SVHN) and SD (SYN) datasets show less non-transferability due to their similarity as RGB digit datasets, suggesting a relationship between domain similarity and NTP effectiveness.
- Why unresolved: While the paper observes that NTP effectiveness varies with domain similarity, it does not provide a systematic analysis of how the semantic and feature space similarity between source and target domains quantitatively affects the non-transferability guarantees, nor does it establish thresholds for when NTP becomes ineffective.
- What evidence would resolve it: A comprehensive study measuring NTP effectiveness across controlled pairs of domains with varying degrees of feature space overlap and semantic similarity, potentially using domain adaptation metrics like A-distance or MMD, would establish the boundaries of NTP's effectiveness.

### Open Question 3
- Question: How does NTP's performance degrade or adapt when applied to multi-task or multi-domain source models where the model must maintain non-transferability across multiple target domains simultaneously?
- Basis in paper: [inferred] The paper evaluates NTP in single-source single-target scenarios but does not address the realistic scenario where a model vendor might need to protect against multiple unauthorized target domains or where source models are trained on multi-domain/multi-task datasets.
- Why unresolved: The current formulation of NTP optimizes for non-transferability to a single target domain, and extending this to multiple targets introduces potential conflicts between different non-transferability objectives, which the paper does not address or provide evidence about.
- What evidence would resolve it: Experiments applying NTP to source models trained on multi-domain datasets (like DomainNet or WILDS) and measuring non-transferability across multiple distinct target domains simultaneously would reveal whether NTP can be extended to multi-domain protection scenarios.

## Limitations

- Effectiveness not validated across diverse model architectures beyond traditional CNNs and self-supervised models
- Limited exploration of robustness against advanced transfer learning attacks and fine-tuning strategies
- No systematic analysis of how domain similarity affects non-transferability guarantees

## Confidence

**High Confidence**: The effectiveness of ADMM for model pruning and the conceptual framework of combining pruning with regularization are well-established in literature. The SLC-AUC metric provides a reasonable way to measure transferability across different data scales.

**Medium Confidence**: The specific combination of pruning with Fisher space regularization for non-transferability is novel, but the mechanism's robustness against determined attackers is not fully explored. The claim that NTP achieves average SLC-AUC of -0.54 needs validation across more diverse scenarios.

**Low Confidence**: The paper doesn't adequately address potential failure modes or provide sufficient evidence that NTP can withstand advanced transfer learning attacks. The comparison with state-of-the-art approaches lacks comprehensive benchmarking against all relevant methods.

## Next Checks

1. **Robustness Testing**: Apply advanced fine-tuning techniques like meta-learning, adversarial training, or domain adaptation to NTP-pruned models to test if attackers can overcome the non-transferable barrier.

2. **Cross-Domain Generalization**: Test NTP on non-vision tasks (NLP, speech, or multimodal models) to verify whether the pruning+Fisher regularization approach generalizes beyond image classification.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary ADMM parameters (ρ, λ) and Fisher regularization strength (γ) to understand their impact on the trade-off between source performance and non-transferability, and identify potential failure thresholds.