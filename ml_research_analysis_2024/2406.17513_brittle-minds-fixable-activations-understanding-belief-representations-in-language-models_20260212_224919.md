---
ver: rpa2
title: 'Brittle Minds, Fixable Activations: Understanding Belief Representations in
  Language Models'
arxiv_id: '2406.17513'
source_url: https://arxiv.org/abs/2406.17513
tags:
- milk
- belief
- noor
- accuracy
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically investigates how language models internally\
  \ represent beliefs of self and others across 12 models of varying size, training\
  \ regimes, and fine-tuning. Using probing experiments with control tasks, the authors\
  \ find that belief representations improve with model size and fine-tuning, are\
  \ structured rather than spurious (confirmed via PCA dimensionality reduction and\
  \ permuted-label controls), but are brittle to prompt variations\u2014particularly\
  \ for others' beliefs."
---

# Brittle Minds, Fixable Activations: Understanding Belief Representations in Language Models

## Quick Facts
- arXiv ID: 2406.17513
- Source URL: https://arxiv.org/abs/2406.17513
- Reference count: 40
- Models improve belief representations with size, fine-tuning, and activation editing

## Executive Summary
This study systematically investigates how language models internally represent beliefs of self and others across 12 models of varying size, training regimes, and fine-tuning. Using probing experiments with control tasks, the authors find that belief representations improve with model size and fine-tuning, are structured rather than spurious (confirmed via PCA dimensionality reduction and permuted-label controls), but are brittle to prompt variations—particularly for others' beliefs. They also demonstrate that targeted activation editing using contrastive activation addition (CAA) can strengthen these representations and significantly improve Theory of Mind performance (up to +56% accuracy), outperforming inference-time intervention while being computationally cheaper.

## Method Summary
The study uses linear probing on residual stream activations to decode belief states from 12 language models (Pythia and Llama-2 families) across three Theory of Mind tasks from the BigToM dataset. Control experiments with permuted labels and PCA-reduced activations validate probe selectivity and structure. Sensitivity analysis tests prompt robustness through variations like random tokens and misleading statements. CAA strengthens belief representations by injecting steering vectors computed from activation differences between true and false belief examples.

## Key Results
- Probing accuracy increases with model size and fine-tuning, confirming better internal belief representations
- Belief representations are structured (low-dimensional) rather than spurious correlations
- Representations are brittle to prompt variations, especially for others' beliefs
- CAA significantly improves ToM performance (up to +56% accuracy) while being computationally cheaper than inference-time intervention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probing reveals structured belief representations that improve with model size and fine-tuning
- Mechanism: Linear probes trained on residual stream activations can decode belief states (self vs others) with increasing accuracy as model capacity and training regime improve
- Core assumption: Higher probing accuracy reflects genuine internal belief representation rather than spurious correlations
- Evidence anchors:
  - [abstract] "experiments provide evidence that both model size and fine-tuning substantially improve LMs' internal representations of others' beliefs"
  - [section] "probing accuracy increases with model size and, more crucially for smaller models, with fine-tuning"
  - [corpus] Weak - neighboring papers discuss belief representations but don't validate probe selectivity
- Break condition: If control experiments (random labels, PCA-reduced activations) show probe performance at chance, the mechanism fails

### Mechanism 2
- Claim: Belief representations are embedded in a low-dimensional subspace
- Mechanism: Principal component analysis shows that belief representations can be recovered using only top-k principal components, indicating structure rather than high-dimensional noise
- Core assumption: Low-dimensional recovery implies meaningful, structured representations
- Evidence anchors:
  - [abstract] "probes trained on top-k principal components still recover most accuracy for k ≪ dmodel"
  - [section] "it is generally possible to recover most of the original accuracy by training probes on a smaller number k of principal components"
  - [corpus] Missing - no direct corpus evidence supporting low-dimensional structure
- Break condition: If probes trained on top-k components show no accuracy recovery, the mechanism fails

### Mechanism 3
- Claim: Contrastive activation addition (CAA) can strengthen brittle belief representations
- Mechanism: CAA computes steering vectors by averaging activation differences between positive and negative belief examples, then injects these vectors to improve ToM performance
- Core assumption: Steering vectors capture the direction in activation space corresponding to correct belief attribution
- Evidence anchors:
  - [abstract] "targeted edits to model activations can correct wrong ToM inferences"
  - [section] "CAA consistently delivers the most substantial accuracy improvements across all models and tasks, up to +56"
  - [corpus] Weak - neighboring papers discuss belief representation but don't validate CAA effectiveness
- Break condition: If CAA fails to improve performance on held-out tasks, the mechanism fails

## Foundational Learning

- Concept: Linear probing and probe selectivity
  - Why needed here: Understanding whether high probe accuracy reflects genuine representations or spurious correlations
  - Quick check question: What would it mean if a probe trained on random labels achieves high accuracy?

- Concept: Principal component analysis and dimensionality reduction
  - Why needed here: Determining whether belief representations are structured or high-dimensional noise
  - Quick check question: If belief representations are structured, what should happen to probe accuracy when using only top-k principal components?

- Concept: Activation editing and steering vectors
  - Why needed here: Understanding how to strengthen brittle belief representations without retraining
  - Quick check question: How does CAA differ from inference-time intervention in terms of computational requirements?

## Architecture Onboarding

- Component map: 12 models (Pythia and Llama-2 families) → probing experiments → control tasks → robustness tests → activation editing → evaluation on ToM tasks
- Critical path: Model selection → data preparation (BigToM) → probe training → control experiments → prompt variation testing → CAA implementation → performance evaluation
- Design tradeoffs: Using residual stream vs attention heads for probing (richer integration vs finer-grained control), CAA vs ITI (computational efficiency vs targeted intervention)
- Failure signatures: Probe accuracy at chance on control tasks, no improvement from CAA on held-out tasks, sensitivity to prompt variations
- First 3 experiments:
  1. Train probes on Pythia-70M residual activations for protagonist beliefs, evaluate on validation set
  2. Run control experiment with random labels, compare probe accuracy to original
  3. Apply CAA to Llama-2-7B-chat, evaluate on Forward Belief task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of fine-tuning (e.g., human feedback vs. synthetic data) influence the emergence of internal belief representations in smaller language models?
- Basis in paper: [explicit] The paper states "Especially for smaller models, future work could explore how different types of fine-tuning (e.g., human feedback vs. synthetic data) influence the emergence of internal belief representations."
- Why unresolved: The study only examined fine-tuning broadly without distinguishing between fine-tuning methods, and only used open-source instruction datasets for Pythia models.
- What evidence would resolve it: Systematic comparison of belief representation emergence across different fine-tuning approaches (SFT, RLHF, DPO, synthetic data) for the same base model family.

### Open Question 2
- Question: What are the architectural differences between language models that lead to the observed differences in belief representation emergence patterns?
- Basis in paper: [explicit] The paper notes "Llama-2 offers 'chat' versions first trained with SFT and then RLHF, Pythia's open-source training set ensures that there is no data leakage" but doesn't explore architectural factors.
- Why unresolved: The study compared models with different architectures, sizes, and training regimes together, making it difficult to isolate architectural effects from other variables.
- What evidence would resolve it: Controlled experiments varying only architectural parameters (attention mechanisms, layer normalization, etc.) while keeping training data and objectives constant.

### Open Question 3
- Question: How do belief representations generalize to mental states beyond beliefs, such as desires, emotions, and intentions?
- Basis in paper: [explicit] The paper states "While in this work we focused on beliefs, our experimental approach can be adapted to investigate how LMs represent desires, emotions, intentions, or preferences."
- Why unresolved: The study was limited to belief representations only, leaving open whether the observed emergence patterns, structure, and brittleness apply to other mental state types.
- What evidence would resolve it: Systematic probing experiments using datasets designed to test multiple mental state types (beliefs, desires, emotions, intentions) across the same model families.

## Limitations
- Control experiments could be strengthened with additional baselines like shuffled examples
- PCA analysis lacks comparison with alternative dimensionality reduction methods
- CAA assumes steering vectors transfer across tasks, but this assumption is only partially validated

## Confidence
- High: Probe selectivity (control experiments confirm non-spurious representations), CAA effectiveness (consistent performance gains across models and tasks)
- Medium: Low-dimensional structure (PCA recovery suggests structure but alternative explanations not fully ruled out)
- Low: Generalizability of CAA to naturalistic prompts beyond controlled variations

## Next Checks
1. Implement attention-head probing as an alternative to residual-stream probing to verify that belief representations are not localized to specific architectural components
2. Test CAA with steering vectors trained on one task (e.g., Forward Belief) and applied to completely different narrative domains to assess transfer capability
3. Conduct ablation studies varying the coefficient α across multiple orders of magnitude to identify optimal intervention strength and potential saturation effects