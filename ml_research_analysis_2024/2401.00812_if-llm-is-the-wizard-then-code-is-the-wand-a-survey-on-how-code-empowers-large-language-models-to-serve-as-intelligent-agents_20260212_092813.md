---
ver: rpa2
title: 'If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers
  Large Language Models to Serve as Intelligent Agents'
arxiv_id: '2401.00812'
source_url: https://arxiv.org/abs/2401.00812
tags:
- code
- llms
- language
- arxiv
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews how integrating code into large
  language model (LLM) training enhances their capabilities, enabling them to serve
  as intelligent agents (IAs). Code's structured, machine-executable nature improves
  LLMs' reasoning, programming skills, and ability to capture structured knowledge.
---

# If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents

## Quick Facts
- arXiv ID: 2401.00812
- Source URL: https://arxiv.org/abs/2401.00812
- Reference count: 39
- This survey comprehensively reviews how integrating code into large language model (LLM) training enhances their capabilities, enabling them to serve as intelligent agents (IAs).

## Executive Summary
This survey examines how code serves as a powerful tool for enhancing large language models (LLMs) to function as intelligent agents (IAs). By integrating code into LLM training and operation, models gain improved reasoning capabilities, better programming skills, and the ability to capture structured knowledge. Code's structured, machine-executable nature enables LLMs to connect with diverse tools and APIs, facilitating complex task completion across multiple domains. The survey identifies key mechanisms including enhanced reasoning through code's logical structure, improved tool integration via code's unambiguous syntax, and automated self-improvement through code execution feedback.

## Method Summary
The survey employs a comprehensive literature review methodology, examining 39 references across multiple domains including machine learning, programming, and AI. The authors synthesize existing research to create a taxonomy of code-LLM applications and identify key mechanisms through which code enhances LLM capabilities. The methodology involves analyzing how code functions as both a training signal and an operational framework for intelligent agents, with particular focus on the code-centric paradigm for tool integration and feedback collection.

## Key Results
- Code training significantly improves LLM performance on reasoning tasks, particularly those requiring step-by-step logical decomposition
- The code-centric paradigm enables flexible and scalable tool integration compared to hard-coded approaches
- Code execution environments provide deterministic feedback that enables automated self-improvement through fine-tuning and in-context learning

## Why This Works (Mechanism)

## Mechanism 1: Code's Structured Expressiveness Enables Enhanced Reasoning in LLMs
- Claim: Training LLMs on code improves their complex reasoning capabilities, especially in tasks requiring step-by-step logical decomposition
- Mechanism: Code's inherent structure, with explicit logical operators (if/else), loops, and nesting, teaches LLMs to generate more organized and verifiable reasoning steps
- Core assumption: The structured nature of code directly transfers to improved reasoning abilities in LLMs, beyond just learning syntax
- Evidence anchors:
  - [abstract]: "code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity"
  - [section 3.2]: "pre-training on code improves LLM performance in both standard and CoT prompting scenarios across downstream tasks"
  - [corpus]: Weak evidence; the corpus neighbors don't directly address this mechanism, suggesting a gap in the literature
- Break condition: If the LLM's reasoning improvements are not transferable to non-code tasks or if the benefits are solely due to increased model size rather than code's structural properties

## Mechanism 2: Code Provides Unambiguous Interface for Tool Integration
- Claim: The code-centric paradigm allows LLMs to dynamically invoke diverse tools and APIs, enhancing their functionality across modalities and domains
- Mechanism: Code's explicitness and unambiguous syntax enable LLMs to generate precise function calls with adaptable parameters, connecting them to external tools like calculators, databases, and multimodal models
- Core assumption: The clarity and precision of code syntax enable more effective communication between LLMs and external tools compared to natural language
- Evidence anchors:
  - [abstract]: "code translates high-level goals into executable steps"
  - [section 4.1]: "The code-centric framework facilitates the invocation of a diverse range of external text modules... These include calculators, calendars, machine translation systems, web navigation tools"
  - [corpus]: Weak evidence; the corpus neighbors don't directly address this mechanism, indicating a potential research gap
- Break condition: If the overhead of generating and parsing code outweighs the benefits of tool integration or if the LLM struggles to generate correct function calls in complex scenarios

## Mechanism 3: Code Execution Environment Enables Automated Feedback for Self-Improvement
- Claim: Embedding LLMs in a code execution environment allows them to receive deterministic feedback, enabling self-correction and performance improvement
- Mechanism: Code execution provides faithful, automatic, and customizable feedback, such as correctness checks, error messages, and reward values
- Core assumption: Feedback from code execution is more reliable and actionable than feedback from natural language tasks, facilitating more effective self-improvement
- Evidence anchors:
  - [abstract]: "code compilation and execution environment, which also provides diverse feedback for model improvement"
  - [section 5.1]: "The code execution process enables assessing LLM-generated content with more comprehensive evaluation metrics derived from deterministic execution results"
  - [corpus]: Weak evidence; the corpus neighbors don't directly address this mechanism, highlighting a potential area for further research
- Break condition: If the feedback from code execution is too limited or if the LLM cannot effectively utilize the feedback to improve its performance

## Foundational Learning

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: Understanding CoT is crucial for grasping how code training enhances LLM reasoning, as it leverages structured step-by-step thinking
  - Quick check question: What is the primary benefit of using CoT prompting with LLMs?
- Concept: Reinforcement Learning (RL)
  - Why needed here: RL is a key method for utilizing feedback from code execution to improve LLM performance, especially in complex task completion
  - Quick check question: How does RL differ from supervised learning in the context of LLM training?
- Concept: Formal Languages
  - Why needed here: Understanding formal languages, including code, is essential for comprehending the code-centric paradigm and its advantages over natural language
  - Quick check question: What distinguishes a formal language from a natural language?

## Architecture Onboarding

- Component map:
  - LLM Core -> Code Execution Environment -> Tool Integration Layer -> Memory Organization -> Feedback Processing
- Critical path:
  1. LLM receives input and generates code-based plan
  2. Plan is executed in the code environment, invoking tools as needed
  3. Execution results are analyzed to provide feedback
  4. Feedback is used to update the LLM's parameters or guide in-context learning
  5. Improved LLM generates refined plans for future tasks
- Design tradeoffs:
  - Flexibility vs. Efficiency: Code-centric approach offers flexibility but may have higher overhead than hardcoded solutions
  - Precision vs. Expressiveness: Code is precise but may lack the expressiveness of natural language for certain tasks
  - Automation vs. Control: Automated feedback enables self-improvement but may reduce human control over the LLM's behavior
- Failure signatures:
  - Incorrect function calls: LLM generates code that doesn't match the intended tool's API
  - Feedback misinterpretation: LLM misinterprets feedback from code execution, leading to incorrect updates
  - Overfitting to code: LLM performs well on code-related tasks but poorly on natural language tasks
- First 3 experiments:
  1. Evaluate LLM's performance on a simple coding task with and without code training
  2. Integrate a calculator tool and test LLM's ability to use it for mathematical reasoning
  3. Implement a basic feedback loop using code execution results and observe its impact on LLM's performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does reinforcing specific code properties during LLM training directly enhance reasoning abilities beyond the general improvements seen from code pre-training?
- Basis in paper: [explicit] The paper identifies a gap in providing explicit experimental evidence linking specific code properties to reasoning enhancement
- Why unresolved: Current research shows code pre-training improves reasoning but doesn't isolate which specific code properties (e.g., executability, logical structure) drive this improvement
- What evidence would resolve it: Controlled experiments comparing LLMs trained on code with specific properties isolated against those trained on general code corpora, measuring reasoning performance on standardized benchmarks

### Open Question 2
- Question: What are the limitations of using code-centric paradigms for complex tasks in data-intensive domains like chemistry and biology?
- Basis in paper: [explicit] The paper identifies challenges in applying code-centric paradigms to domains requiring domain-specific Python libraries with intricate function calls
- Why unresolved: While the code-centric paradigm shows promise, its effectiveness for expert-level tasks in fine-grained domains is untested
- What evidence would resolve it: Empirical studies applying code-centric approaches to real-world tasks in chemistry, biology, and astronomy, comparing performance against traditional methods and measuring error rates and task completion times

### Open Question 3
- Question: How can reinforcement learning be optimally integrated with LLMs for complex task completion using feedback from code execution environments?
- Basis in paper: [explicit] The paper hypothesizes that reinforcement learning could be a more effective approach for utilizing feedback than current selection-based or prompting-based methods
- Why unresolved: Current methods for using code execution feedback (selection, prompting, fine-tuning) have limitations
- What evidence would resolve it: Comparative studies of RL-based approaches against existing methods for utilizing code execution feedback, measuring task completion rates, learning efficiency, and robustness across diverse tasks

## Limitations

- The causal relationship between code training and improved reasoning capabilities is primarily inferred rather than experimentally demonstrated
- The scalability of code-centric paradigms to complex, non-deterministic domains remains largely theoretical
- The trade-offs between code-based and natural language approaches in real-world IA deployments are not fully characterized

## Confidence

- High confidence: Code's role as an unambiguous interface for tool integration
- Medium confidence: Code's contribution to structured knowledge capture and reasoning enhancement
- Low confidence: The extent to which automated feedback loops enable meaningful self-improvement in practical applications

## Next Checks

1. Design a controlled experiment comparing LLM performance on reasoning tasks with and without code training, isolating the effect of code structure from other factors like model size
2. Implement a code-centric IA prototype and evaluate its performance across multiple tool integration scenarios, measuring both flexibility and overhead
3. Develop a benchmark suite specifically designed to test the self-improvement capabilities of code-embedded LLMs, with measurable metrics for feedback utilization efficiency