---
ver: rpa2
title: NTU-NPU System for Voice Privacy 2024 Challenge
arxiv_id: '2410.02371'
source_url: https://arxiv.org/abs/2410.02371
tags:
- speaker
- speech
- anonymization
- embedding
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper presents systems for the Voice Privacy Challenge 2024,\
  \ focusing on enhancing existing baseline approaches rather than proposing novel\
  \ methods. The authors improve upon two baseline systems (B3 and B5) by incorporating\
  \ emotion embeddings, experimenting with different speaker embedders (WavLM, ECAPA2),\
  \ exploring disentanglement models (\u03B2-VAE, NaturalSpeech3 FACodec), and introducing\
  \ Mean Reversion F0 for B5."
---

# NTU-NPU System for Voice Privacy 2024 Challenge

## Quick Facts
- arXiv ID: 2410.02371
- Source URL: https://arxiv.org/abs/2410.02371
- Reference count: 0
- Enhanced baseline systems (B3, B5) with emotion embeddings, improved speaker embedders, disentanglement models, and F0 manipulation

## Executive Summary
This paper presents the NTU-NPU system for the Voice Privacy Challenge 2024, focusing on enhancing existing baseline approaches rather than proposing novel methods. The authors improve upon two baseline systems (B3 and B5) by incorporating emotion embeddings, experimenting with different speaker embedders (WavLM, ECAPA2), exploring disentanglement models (β-VAE, NaturalSpeech3 FACodec), and introducing Mean Reversion F0 for B5. The modified systems demonstrate effective privacy enhancement while maintaining reasonable utility metrics across multiple evaluation conditions.

## Method Summary
The approach enhances baseline voice conversion systems by adding emotion embeddings extracted from a fine-tuned Wav2Vec2 model on MSP-Podcast data, which are integrated into the FastSpeech2 model alongside speaker embeddings. Speaker embedders are upgraded from GST to WavLM and ECAPA2 for improved privacy-utility tradeoffs. For the B5 system, Mean Reversion F0 transformation reduces F0 dynamic range for privacy enhancement. The systems are trained on LibriTTS-clean-100 data and evaluated on multiple conditions with different EER thresholds using UAR for emotion recognition and WER for ASR utility metrics.

## Key Results
- Modified B3 system with emotion embeddings and cross-gender conversion achieved EER1=12.09%, UAR=49.20%, WER=4.97%
- B5 system with Mean Reversion F0 and AWGN achieved EER4=42.46%, UAR=39.41%, WER=4.63%
- Cross-gender conversion improves both privacy and utility metrics
- Emotion embeddings preserve emotional expressiveness while enabling speaker anonymization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Emotion embeddings preserve emotional expressiveness while enabling speaker anonymization
- Mechanism: Emotion embeddings extracted from Wav2Vec2 are added to FastSpeech2 model alongside speaker embeddings, allowing maintenance of emotional prosody without leaking speaker identity
- Core assumption: Emotion information is separable from speaker identity in embedding space
- Evidence anchors: [abstract] mentions emotion embedding implementation; [section 3.1] describes adding linear projection and concatenating with Conformer output
- Break condition: If emotion embeddings contain speaker identity information, they could worsen privacy metrics instead of preserving emotional content

### Mechanism 2
- Claim: Mean Reversion F0 transformation enhances privacy by reducing F0 dynamic range while maintaining utility
- Mechanism: F0 values are weighted toward their moving average (α=0.75), reducing distinctive pitch characteristics that identify speakers
- Core assumption: Speaker identity can be partially encoded in F0 dynamic range and patterns
- Evidence anchors: [section 3.3] explains mean reversion motivation; [section 4.3] shows EER increases with α
- Break condition: If F0 is not a significant speaker identifier, or if transformation distorts prosody too much, utility metrics may degrade

### Mechanism 3
- Claim: Cross-gender speaker conversion improves privacy without significantly degrading utility
- Mechanism: Selecting target speakers of opposite gender from source speaker creates larger differences in voice characteristics
- Core assumption: Gender differences in voice characteristics provide substantial privacy improvement over same-gender conversion
- Evidence anchors: [section 3.1] describes cross-gender anonymization technique; [section 4.4] shows improvements in EER and WER
- Break condition: If gender is not a strong speaker identifier or dataset lacks gender diversity, cross-gender conversion may not provide expected privacy benefits

## Foundational Learning

- Concept: Speaker embedding extraction and manipulation
  - Why needed here: System relies on replacing source speaker embeddings with anonymized versions to obscure identity
  - Quick check question: What are the key differences between GST, WavLM, and ECAPA2 speaker embedding approaches, and when would each be preferred?

- Concept: Voice conversion pipeline architecture
  - Why needed here: Understanding how phonetic transcriptions, speaker embeddings, and prosody components flow through system is critical for effective modifications
  - Quick check question: How do B3 and B5 baseline systems differ in their approach to voice conversion, and what are the trade-offs of each?

- Concept: Evaluation metrics for voice privacy
  - Why needed here: System must balance EER (privacy), UAR (emotion recognition), and WER (ASR) metrics, requiring understanding of their relationships
  - Quick check question: How do changes that improve EER typically affect UAR and WER, and what strategies can balance all three metrics?

## Architecture Onboarding

- Component map: ASR → phonetic transcriptions → speaker embedding extraction → anonymization → prosody manipulation → synthesis
- Critical path: Phonetic transcription → speaker embedding extraction → anonymization → prosody manipulation → synthesis
- Design tradeoffs: Privacy vs utility (higher EER typically means lower UAR/WER), complexity vs performance (more sophisticated models like WavLM provide better embeddings but increase computational cost), gender-specific vs general anonymization strategies
- Failure signatures: High WER indicates poor linguistic preservation, low UAR suggests emotional content loss, insufficient EER means speaker identity remains detectable
- First 3 experiments:
  1. Compare EER, UAR, and WER across different speaker embedders (GST vs WavLM vs ECAPA2) with identical prosody and emotion settings
  2. Test cross-gender vs random speaker selection on privacy-utility tradeoff curve
  3. Evaluate impact of different Mean Reversion F0 α values on three metrics to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of emotion embeddings change when using different emotion recognition models beyond Wav2Vec2 Large Robust model fine-tuned on MSP-Podcast?
- Basis in paper: [explicit] Authors mention using fine-tuned Wav2Vec2 Large Robust model on MSP-Podcast for emotion embeddings and observe improvements in Emotion Recognition performance while maintaining ASR performance, but also cause some degradation in privacy
- Why unresolved: Paper only explores one emotion embedding model, and different models may have varying impacts on privacy-utility tradeoffs
- What evidence would resolve it: Experimental results comparing emotion embedding performance using multiple emotion recognition models (e.g., different architectures, training datasets) on same VPC2024 metrics

### Open Question 2
- Question: What is the optimal balance between Mean Reversion F0 strength (α value) and AWGN noise level that maximizes EER without excessive degradation in UAR and WER?
- Basis in paper: [explicit] Authors explore Mean Reversion F0 with different α values and find EER increases with α while UAR and WER fluctuate. They also test AWGN with different magnitudes and observe privacy gains at cost of utility
- Why unresolved: Paper provides results for individual parameter changes but doesn't systematically explore joint optimization space of α and AWGN parameters
- What evidence would resolve it: Grid search or optimization study exploring various combinations of α values and AWGN noise levels, measuring resulting EER, UAR, and WER to identify optimal parameter combinations

### Open Question 3
- Question: How do disentanglement models like ß-VAE and NaturalSpeech3 FACodec compare when applied to other speech anonymization tasks beyond VPC2024 conditions, such as different languages or recording conditions?
- Basis in paper: [explicit] Authors compare ß-VAE and NaturalSpeech3 FACodec for VPC2024 and find NaturalSpeech3 shows better utility performance, while ß-VAE performs poorly in utility-based tasks
- Why unresolved: Evaluation limited to specific VPC2024 conditions and datasets, and generalizability of findings to other scenarios remains unknown
- What evidence would resolve it: Comparative experiments applying these disentanglement models to speech anonymization tasks in different languages, recording environments, or with different speaker demographics, measuring same privacy-utility metrics

## Limitations

- The effectiveness of emotion embeddings for privacy preservation remains uncertain due to lack of corpus evidence demonstrating separability of emotional expressiveness from speaker identity
- Cross-gender anonymization approach may not generalize to gender-nonconforming speakers or languages with different gender expression patterns
- Mean Reversion F0 method's impact on emotional expressiveness is unclear, potentially degrading UAR metrics in ways not fully explored

## Confidence

**High Confidence:** The implementation of Mean Reversion F0 technique is straightforward and the mathematical formula is clearly specified, making this modification reproducible with predictable effects on F0 distributions.

**Medium Confidence:** The privacy-utility tradeoff observations (cross-gender conversion improving both EER and WER) are supported by empirical results, though underlying mechanisms require further investigation across different speaker demographics.

**Low Confidence:** The claim that emotion embeddings can preserve emotional expressiveness while enabling speaker anonymization lacks sufficient validation. The separation between emotional and speaker identity information in embedding space needs more rigorous testing.

## Next Checks

1. **Emotion Embedding Isolation Test:** Conduct systematic ablation study to verify whether emotion embeddings extracted from Wav2Vec2 contain speaker identity information, including speaker verification tests on emotion-only embeddings and comparison with speaker embeddings to quantify information overlap.

2. **Gender Diversity Validation:** Evaluate cross-gender anonymization effectiveness on dataset with balanced gender representation and include non-binary speakers if available, validating whether privacy improvements generalize beyond binary gender assumptions.

3. **F0 Dynamic Range Analysis:** Perform detailed analysis of how Mean Reversion F0 affects emotional prosodic features by comparing distribution of F0 contours before and after transformation, specifically measuring changes in emotional expressiveness markers like pitch range and contour shape.