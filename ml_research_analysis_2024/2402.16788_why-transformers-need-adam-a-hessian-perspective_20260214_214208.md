---
ver: rpa2
title: 'Why Transformers Need Adam: A Hessian Perspective'
arxiv_id: '2402.16788'
source_url: https://arxiv.org/abs/2402.16788
tags:
- adam
- hessian
- blocks
- arxiv
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates why Stochastic Gradient Descent (SGD)
  performs significantly worse than Adam on Transformer models. The authors analyze
  the Hessian matrix of various models and discover a key difference: Transformers
  exhibit "block heterogeneity," meaning the Hessian spectra vary dramatically across
  parameter blocks, while Convolutional Neural Networks (CNNs) show "block homogeneity"
  with similar spectra across blocks.'
---

# Why Transformers Need Adam: A Hessian Perspective

## Quick Facts
- arXiv ID: 2402.16788
- Source URL: https://arxiv.org/abs/2402.16788
- Reference count: 40
- Key outcome: Transformers exhibit "block heterogeneity" in Hessian spectra while CNNs show "block homogeneity"

## Executive Summary
This paper investigates why Adam outperforms SGD for training Transformer models by analyzing the Hessian matrix structure. The authors discover that Transformers exhibit significant variation in Hessian spectra across different parameter blocks ("block heterogeneity"), while CNNs show relatively uniform spectra ("block homogeneity"). This structural difference explains why SGD, with its single learning rate per block, struggles with Transformers but works well for CNNs. The findings provide both theoretical insights and a practical metric for predicting whether SGD will be effective for a given architecture based on initial Hessian spectra.

## Method Summary
The authors conduct a comprehensive analysis of Hessian spectra across multiple architectures including Transformers, CNNs, MLPs, and quadratic problems. They decompose the Hessian matrix into parameter blocks and compute spectral statistics for each block. Through controlled experiments, they compare optimization performance of SGD versus Adam on problems with varying degrees of block heterogeneity. The theoretical analysis connects the spectral properties to optimization dynamics, showing how Adam's coordinate-wise learning rates can adapt to heterogeneous landscapes while SGD cannot. A practical metric is proposed based on the ratio of maximum to minimum eigenvalue across blocks to predict SGD's effectiveness.

## Key Results
- Transformers exhibit significant block heterogeneity in Hessian spectra (ratio of max to min eigenvalues ~100-1000)
- CNNs show block homogeneity with much smaller spectral variation (ratio ~1-10)
- SGD performs poorly on heterogeneous problems but comparably to Adam on homogeneous ones
- The proposed Hessian-based metric accurately predicts SGD's optimization difficulty before training begins

## Why This Works (Mechanism)
The paper demonstrates that block heterogeneity in the Hessian spectrum creates optimization landscapes where a single learning rate cannot effectively handle all parameter blocks. Transformers, with their attention mechanisms and feed-forward networks, create parameter blocks with vastly different curvature properties. Adam's adaptive learning rates can adjust per-coordinate, effectively handling this heterogeneity. The theoretical analysis shows that SGD's convergence depends critically on the condition number of each block, and when these vary dramatically, no single learning rate can optimize all blocks effectively.

## Foundational Learning
1. **Hessian Spectrum Analysis** - Understanding eigenvalue distributions of the second-order derivative matrix is crucial for analyzing optimization landscapes. Quick check: Compute spectral norm of Hessian for a simple neural network layer.

2. **Block Decomposition** - Partitioning parameters into logical blocks (e.g., attention vs. FFN in Transformers) allows analysis of heterogeneity. Quick check: Verify block decomposition matches architectural components.

3. **Condition Number Impact** - The ratio of largest to smallest eigenvalue determines optimization difficulty for gradient methods. Quick check: Simulate gradient descent on quadratic problems with varying condition numbers.

## Architecture Onboarding

**Component Map**: Input -> Embedding -> Multi-Head Attention -> Feed-Forward Network -> Output

**Critical Path**: The attention mechanism and feed-forward network create the most heterogeneous parameter blocks, with attention weights showing dramatically different spectral properties than position-wise feed-forward weights.

**Design Tradeoffs**: Block heterogeneity enables Transformers' superior representational power but creates optimization challenges. CNNs sacrifice some representational flexibility for optimization-friendly homogeneous landscapes.

**Failure Signatures**: SGD with a single learning rate will show stalled training or poor convergence on blocks with extreme Hessian spectra, while other blocks may show oscillatory behavior.

**First Experiments**:
1. Compute Hessian spectra for each parameter block in a small Transformer
2. Compare SGD vs Adam training curves on homogeneous vs heterogeneous synthetic problems
3. Apply the proposed metric to predict SGD's effectiveness on different architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses on static Hessian properties rather than dynamic optimization behavior during training
- Block decomposition may oversimplify complex parameter interactions within architectures
- Theoretical analysis provides supporting evidence but lacks complete mathematical rigor
- Proposed metric based on initial spectra may not account for optimization dynamics that emerge during training

## Confidence

**High**: Empirical demonstration of block heterogeneity in Transformers is well-supported by data
**Medium**: Theoretical explanation connecting heterogeneity to optimization difficulties shows promise but lacks complete rigor
**Medium**: Practical metric for predicting SGD effectiveness is based on sound principles but requires more validation

## Next Checks

1. **Dynamic Analysis**: Track how block heterogeneity evolves during training and whether this correlates with optimization difficulties for SGD across multiple epochs.

2. **Cross-Architecture Validation**: Test the block heterogeneity metric on a wider range of architectures including modern variants like Vision Transformers, Perceiver, and state-space models to verify generalizability.

3. **Transfer Learning Scenarios**: Evaluate whether block heterogeneity patterns and optimization behavior transfer when using pre-trained models or in multi-task learning settings where parameter reuse is common.