---
ver: rpa2
title: 'Design2Code: Benchmarking Multimodal Code Generation for Automated Front-End
  Engineering'
arxiv_id: '2403.03163'
source_url: https://arxiv.org/abs/2403.03163
tags:
- code
- webpages
- text
- webpage
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Design2Code, the first real-world benchmark
  for multimodal code generation from webpage screenshots. The authors manually curate
  484 diverse webpages and develop comprehensive automatic metrics (CLIP similarity,
  block-match, text, position, color) to evaluate how well models generate functional
  HTML/CSS that visually matches the input design.
---

# Design2Code: Benchmarking Multimodal Code Generation for Automated Front-End Engineering

## Quick Facts
- arXiv ID: 2403.03163
- Source URL: https://arxiv.org/abs/2403.03163
- Reference count: 34
- Primary result: GPT-4o achieves highest performance in multimodal code generation from webpage screenshots, with text-augmented prompting improving weaker models' text recall

## Executive Summary
This paper introduces Design2Code, the first real-world benchmark for multimodal code generation from webpage screenshots. The authors manually curate 484 diverse webpages and develop comprehensive automatic metrics (CLIP similarity, block-match, text, position, color) to evaluate how well models generate functional HTML/CSS that visually matches the input design. They test prompting strategies—text-augmented (providing extracted text) and self-revision (iteratively improving code)—across commercial and open-source models. Results show GPT-4o performs best, with text-augmentation improving weaker models, especially in text recall. Human evaluations confirm rankings, though discrepancies exist between automatic and human metrics, suggesting humans prioritize layout over exact text.

## Method Summary
The authors create a benchmark using 484 manually curated real-world webpages from the C4 validation set, with corresponding screenshots and HTML/CSS implementations. They develop automatic evaluation metrics including CLIP embedding similarity, element block-matching, text similarity, position similarity, and color similarity. Three prompting methods are tested: direct prompting, text-augmented prompting (providing extracted text elements), and self-revision prompting (iteratively improving code by comparing generated vs reference screenshots). The approach is evaluated across commercial models (GPT-4o, GPT-4V, Claude 3, Gemini) and open-source models (LLaVA, DeepSeek-VL, Idefics2), including finetuned models (WebSight VLM-8B, Design2Code-18B).

## Key Results
- GPT-4o achieves highest performance with CLIP similarity of 0.68 and block-match score of 0.77
- Text-augmented prompting improves block-match and text similarity scores for most models, especially weaker ones
- Self-revision prompting shows minimal improvement across models, suggesting limited effectiveness
- Human evaluations confirm automatic metric rankings but reveal humans prioritize layout over exact text content
- Design2Code-HARD subset (80 examples) shows 30-40% drop in visual element matching compared to full dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP-based visual similarity effectively captures overall design alignment between generated and reference webpages.
- Mechanism: CLIP embedding similarity measures high-level visual similarity without relying on exact text matching, focusing on perceptual layout and style.
- Core assumption: CLIP embeddings are robust to minor layout variations and effectively capture visual semantics of webpages.
- Evidence anchors: [abstract] "Our metrics consider a comprehensive set of dimensions, including bounding box matches, text content, position, and color of all matched visual elements on the webpages, which we later show highly correlate with human judgment." [section] "To evaluate the visual similarity of IR and IG, we use the similarity of their CLIP (Radford et al., 2021) embedding, denoted as CLIP(IR, IG)."
- Break condition: When CLIP embeddings fail to capture subtle layout differences that humans notice, or when model generations have different content but similar visual structure.

### Mechanism 2
- Claim: Text-augmented prompting improves model performance by reducing OCR burden and allowing focus on layout design.
- Mechanism: Providing extracted text elements from the reference webpage allows models to focus computational resources on layout generation rather than text recognition.
- Core assumption: Multimodal LLMs have limited computational resources for simultaneous text recognition and layout design.
- Evidence anchors: [abstract] "To reflect such a setting, we also explore a text-augmented prompting method, where we extract all text elements from the original webpage first and append these texts after the instruction prompt along with the screenshot input." [section] "Text-augmented prompting successfully increases the block-match score and text similarity score on most tested models, especially those that are suboptimal in terms of text recognition."
- Break condition: When models already have strong OCR capabilities or when text elements are minimal compared to layout complexity.

### Mechanism 3
- Claim: Self-revision prompting can improve generated code by allowing models to visually contrast and refine their outputs.
- Mechanism: Models compare their generated webpage screenshot with the reference and revise code to improve alignment.
- Core assumption: Models can effectively identify differences between two images and translate those differences into meaningful code modifications.
- Evidence anchors: [abstract] "we develop a self-revision prompt where we provide the following as input: (1) the screenshot of the input webpage, (2) the screenshot of the generated webpage from text-augmented prompting, (3) the generated code from text-augmented prompting as the initial solution; then we ask the model to improve the generated implementation code." [section] "Self-revision has some minor improvement on block-match and position similarity for GPT-4V and Claude 3, but brings no improvement on Gemini Pro Vision and all other open-source models."
- Break condition: When models lack sufficient visual comparison capabilities or when code differences require more than visual inspection to identify.

## Foundational Learning

- Concept: Multimodal understanding - ability to process and integrate visual and textual information
  - Why needed here: The task requires understanding webpage screenshots (visual) and generating corresponding HTML/CSS code (text)
  - Quick check question: Can the model correctly identify and locate text elements within a webpage screenshot?

- Concept: Layout design principles - understanding spatial relationships and visual hierarchy
  - Why needed here: Generated webpages must match the reference layout, requiring understanding of element positioning and sizing
  - Quick check question: Can the model correctly position elements relative to each other to match the reference layout?

- Concept: Code generation from specifications - translating design requirements into functional code
  - Why needed here: The core task is converting visual designs into working HTML/CSS implementations
  - Quick check question: Can the model generate syntactically correct HTML/CSS that renders to match a given design?

## Architecture Onboarding

- Component map: Input webpage screenshot → Text extraction (optional) → Model generation → HTML/CSS output → Rendering and evaluation → Iterative refinement (optional)
- Critical path: 1. Input webpage screenshot → 2. Text extraction (optional) → 3. Model generation → 4. HTML/CSS output → 5. Rendering and evaluation
- Design tradeoffs: Text vs. layout focus: Text-augmented prompting trades OCR capability for layout design quality; Model size vs. performance: Larger models show better performance but increase computational cost; Evaluation metrics: Automatic metrics provide fine-grained analysis but may not fully align with human preferences
- Failure signatures: Low block-match scores indicate missing or hallucinated elements; Poor text similarity suggests OCR or text placement issues; Low position similarity indicates layout misalignment; CLIP similarity drops may signal major visual design discrepancies
- First 3 experiments: 1. Test direct prompting vs. text-augmented prompting on a small subset to measure OCR impact; 2. Evaluate self-revision prompting effectiveness on models that show moderate performance with text-augmented prompting; 3. Compare automatic metrics vs. human evaluation on 20 examples to identify metric-human alignment gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multimodal LLMs be trained to reliably self-revise their code generations when given visual feedback of their rendered output versus the reference?
- Basis in paper: [explicit] The paper shows that self-revision prompting provides minimal improvement for most models, with the authors suggesting this may be due to "limited capabilities of LLMs to do self-revision" and the difficulty of understanding image differences and translating them into code modifications.
- Why unresolved: The paper only tested self-revision on a small set of commercial models with limited prompting strategies. The underlying reasons for failure (model capability, prompt design, or both) remain unclear.
- What evidence would resolve it: Systematic experiments varying self-revision prompt structure, providing explicit visual difference highlighting, and testing on models with different training data could determine whether the limitation is fundamental or architectural.

### Open Question 2
- Question: What specific HTML/CSS patterns or webpage characteristics make generation most challenging for current multimodal LLMs?
- Basis in paper: [explicit] The authors identify that webpages with more HTML tags are significantly harder to generate and create Design2Code-HARD with 80 examples containing over 500 tags and non-English content. They note 30-40% of visual elements are missing on these hard examples.
- Why unresolved: The analysis only correlates tag count with difficulty but doesn't identify which specific patterns (nested layouts, complex CSS, dynamic content, etc.) drive the performance drop or how models fail on these cases.
- What evidence would resolve it: Detailed error analysis categorizing failures by HTML pattern type, measuring model performance on progressively complex layout structures, and identifying which visual or code features correlate most strongly with generation errors.

### Open Question 3
- Question: Can automatic metrics be designed that better align with human preferences for webpage similarity, given the observed discrepancy between CLIP-based metrics and human judgments?
- Basis in paper: [explicit] The authors find that humans prioritize layout over exact text content, with text similarity having a negative association with human judgment in their logistic regression analysis. They note this discrepancy is "a feature rather than a bug" but don't propose better metrics.
- Why unresolved: While the authors acknowledge the limitation of current metrics, they don't explore alternative evaluation approaches that could bridge the gap between automatic and human assessment.
- What evidence would resolve it: Development and validation of hybrid metrics that combine visual similarity with layout-aware text matching, or metrics that weight high-level structural elements more heavily than pixel-level matching, tested against human preferences.

## Limitations
- Evaluation reveals significant gaps between automatic metrics and human preferences, particularly in text content prioritization
- Self-revision prompting strategy shows inconsistent improvements across models with limited effectiveness
- Performance drops significantly on Design2Code-HARD subset (30-40% reduction in visual element matching)

## Confidence
- High confidence in core benchmark design and methodology
- Medium confidence in model performance rankings
- Low confidence in self-revision effectiveness

## Next Checks
1. Conduct systematic comparison of automatic metrics vs. human preferences across 100+ examples to identify which metrics best predict human judgment
2. Design controlled experiments to separate OCR accuracy from layout generation quality by providing ground-truth text elements
3. Test model performance on Design2Code-HARD subset across different domain categories to identify specific failure modes and domain-specific challenges