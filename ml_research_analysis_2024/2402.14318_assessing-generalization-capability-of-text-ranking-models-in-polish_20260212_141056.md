---
ver: rpa2
title: Assessing generalization capability of text ranking models in Polish
arxiv_id: '2402.14318'
source_url: https://arxiv.org/abs/2402.14318
tags:
- retrieval
- polish
- rerankers
- language
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates reranking models for Polish text retrieval,
  comparing them with retrieval models on 41 diverse information retrieval tasks.
  The authors assess publicly available Polish and multilingual rerankers, as well
  as their own models trained with various methods, including knowledge distillation.
---

# Assessing generalization capability of text ranking models in Polish

## Quick Facts
- arXiv ID: 2402.14318
- Source URL: https://arxiv.org/abs/2402.14318
- Authors: Sławomir Dadas; Małgorzata Grębowiec
- Reference count: 40
- Key outcome: Reranking models for Polish text retrieval evaluated on 41 diverse tasks show most struggle with out-of-domain generalization, except large models; knowledge distillation achieves new SOTA with 30x fewer parameters

## Executive Summary
This paper evaluates reranking models for Polish text retrieval, comparing them with retrieval models on 41 diverse information retrieval tasks. The authors assess publicly available Polish and multilingual rerankers, as well as their own models trained with various methods, including knowledge distillation. Results show that most rerankers struggle with out-of-domain generalization, except for large models with billions of parameters. The authors successfully train compact rerankers using knowledge distillation, outperforming the teacher model with 30 times fewer parameters and achieving a new state-of-the-art for Polish reranking tasks.

## Method Summary
The authors evaluate reranking models on the Polish Information Retrieval Benchmark (PIRB) consisting of 41 diverse IR tasks. They compare publicly available Polish and multilingual rerankers with their own models trained using different approaches: fine-tuning with BCE loss, MSE knowledge distillation, and RankNet permutation distillation. The knowledge distillation uses a 13B MT5 model to score 1.4M query-document pairs from Polish MS MARCO, ELI54, and ZnanyLekarz datasets. Models are evaluated using NDCG@10 metric against retrieval baselines using mmlw-retrieval-roberta-large as the first-stage retriever.

## Key Results
- Most rerankers struggle with out-of-domain generalization, performing worse than retrieval baselines on many tasks
- Large models with billions of parameters maintain strong generalization capabilities across diverse tasks
- Knowledge distillation enables training compact rerankers that outperform teacher models with 30x fewer parameters
- RankNet distillation proves more effective than MSE, achieving higher performance than the teacher model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reranking models can improve retrieval quality when the initial dense retriever is not optimal.
- Mechanism: The reranker uses a more powerful neural model (e.g., transformer-based) to refine the ranking of documents already pre-selected by the retriever. By scoring query-document pairs more accurately, it corrects the retriever's ranking errors.
- Core assumption: The retriever produces a ranked list of candidates that contains the most relevant documents, and the reranker can improve the order without missing the best documents.
- Evidence anchors:
  - [abstract] "the results of our experiments show that most models struggle with out-of-domain generalization" suggests that rerankers *can* help in-domain but not always in zero-shot settings.
  - [section] "the idea of employing additional ranking models is based on the assumption that these models have the ability to better assess the relevance of a document to the query."

### Mechanism 2
- Claim: Knowledge distillation allows training smaller rerankers that match the performance of much larger teacher models.
- Mechanism: A large teacher reranker (e.g., MT5-XXL with 13B parameters) generates scores for a large training dataset. Smaller student models (e.g., polish-roberta-large-v2) are trained to mimic these scores using MSE or RankNet loss, capturing the teacher's ranking behavior in fewer parameters.
- Core assumption: The teacher model has superior ranking capability, and its predictions are accurate enough to serve as training labels for the student.
- Evidence anchors:
  - [abstract] "the best of our models establishes a new state-of-the-art for reranking in the Polish language, outperforming existing models with up to 30 times more parameters."
  - [section] "using the 13B model to score all query-document pairs. For the MSE method, the scores were used directly for training."

### Mechanism 3
- Claim: Permutation-based distillation (RankNet) is more effective than regression-based (MSE) distillation for learning ranking behavior.
- Mechanism: Instead of mimicking raw relevance scores, RankNet trains the student to reproduce the order of documents for each query by comparing pairs and optimizing pairwise ranking loss. This focuses learning on the relative order rather than absolute score accuracy.
- Core assumption: The relative order of documents is more important for ranking performance than exact score values, and pairwise ranking captures this better than regression.
- Evidence anchors:
  - [abstract] "the best of our models outperforms the teacher model on the PIRB benchmark while being 30 times smaller in terms of the number of parameters."
  - [section] "RankNet distillation proved to be even more effective. The large model trained with this method achieved performance higher than the teacher model."

## Foundational Learning

- Concept: Dense retrieval and its limitations
  - Why needed here: The reranker operates on the output of a dense retriever; understanding its behavior (e.g., tendency to over-retrieve or misrank) explains when reranking helps.
  - Quick check question: What is the main difference between sparse (BM25) and dense retrieval in terms of input representation?

- Concept: Knowledge distillation in ranking tasks
  - Why needed here: The paper uses distillation to compress a large teacher reranker into a smaller student; knowing how distillation loss functions (MSE vs. RankNet) affect ranking quality is key.
  - Quick check question: In distillation for ranking, why might pairwise loss (RankNet) outperform pointwise regression (MSE)?

- Concept: Zero-shot vs. in-domain generalization
  - Why needed here: The evaluation highlights that most rerankers fail on out-of-domain tasks; understanding the distinction guides expectations for model deployment.
  - Quick check question: How does the performance of a reranker on in-domain data typically compare to its zero-shot performance on out-of-domain data?

## Architecture Onboarding

- Component map: Query → Retriever (mmlw-retrieval-roberta-large) → k candidate documents → Reranker → reordered top-k → Reader → Answer
- Critical path: Query → Retriever → 100 candidate documents → Reranker → reordered top-10 → Reader → Answer
- Design tradeoffs:
  - Reranker size vs. latency: Larger models give better ranking but slower inference; distillation helps balance this.
  - Generalization vs. specialization: Fine-tuning on domain-specific data improves in-domain performance but hurts zero-shot ability.
  - Training data diversity: Larger, more diverse datasets improve generalization but increase cost.
- Failure signatures:
  - Reranker consistently underperforms retriever → likely poor zero-shot generalization or training data mismatch.
  - Reranker improves on some tasks but degrades on others → dataset bias or overfitting to training domain.
  - Training instability or slow convergence → inappropriate loss function or learning rate.
- First 3 experiments:
  1. Run retriever-only pipeline on PIRB and record NDCG@10 per dataset to establish baseline.
  2. Apply a small reranker (e.g., polish-roberta-base-v2 with BCE) to same data and compare per-dataset NDCG@10 to baseline.
  3. Apply the distilled large reranker (polish-roberta-large-v2 with RankNet) and measure speed/accuracy tradeoff vs. baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do rerankers perform when evaluated on retrieval tasks that are structurally different from question-answering tasks, such as finding counterarguments or identifying duplicate questions?
- Basis in paper: [explicit] The paper shows that rerankers perform poorly on arguana (counterarguments) and quora (duplicate questions) datasets compared to the retriever.
- Why unresolved: While the paper identifies this performance gap, it doesn't investigate whether this is due to the reranker's inability to understand the task structure or simply lack of training data for these specific tasks.
- What evidence would resolve it: Experiments training rerankers specifically on counterargument and duplicate question data, or adapting existing rerankers to these tasks through prompt engineering or task-specific fine-tuning.

### Open Question 2
- Question: Does increasing the size of the training dataset beyond 1.4 million queries continue to improve reranker performance, particularly for out-of-domain generalization?
- Basis in paper: [explicit] The paper demonstrates significant performance gains when increasing the training dataset from 800k to 1.4 million queries using knowledge distillation, but doesn't explore whether even larger datasets would yield further improvements.
- Why unresolved: The paper uses a fixed training dataset size and doesn't systematically vary the dataset size to measure its impact on performance.
- What evidence would resolve it: Controlled experiments training rerankers on progressively larger datasets (e.g., 1.4M, 2.8M, 5.6M queries) and measuring their performance on both in-domain and out-of-domain tasks.

### Open Question 3
- Question: Are there specific dataset characteristics or task properties that make reranking more beneficial than retrieval alone?
- Basis in paper: [inferred] The paper observes that some datasets (PolEval, MAUPQA) show better reranker performance than others (Web Datasets, BEIR-PL), suggesting that dataset characteristics influence reranker effectiveness.
- Why unresolved: The paper identifies performance differences across dataset groups but doesn't analyze which specific properties (e.g., query length, document length, domain specificity) drive these differences.
- What evidence would resolve it: Statistical analysis correlating reranker performance gains with dataset features such as query complexity, document diversity, domain specificity, or the similarity between training and evaluation data distributions.

## Limitations

- Evaluation limited to Polish language tasks, raising questions about generalizability to other languages or multilingual settings
- Knowledge distillation approach relies heavily on a single large teacher model without exploring ensemble distillation approaches
- State-of-the-art claim difficult to verify given limited availability of comparable benchmarks for Polish language

## Confidence

**High Confidence**: Most rerankers struggle with out-of-domain generalization (well-supported by comprehensive evaluation across 41 diverse Polish IR tasks)

**Medium Confidence**: RankNet distillation outperforms MSE and achieves better results than teacher model (based on single experimental setup, needs more ablation studies)

**Low Confidence**: New state-of-the-art claim for Polish reranking tasks (difficult to verify given limited comparable benchmarks)

## Next Checks

1. **Cross-linguistic generalization test**: Evaluate the distilled rerankers on retrieval tasks in other languages (e.g., English, German) using multilingual datasets to determine if strong performance generalizes beyond Polish.

2. **Teacher model ablation study**: Replace the 13B MT5 teacher with smaller teacher models (e.g., 1B, 3B parameters) to quantify the relationship between teacher quality and student performance.

3. **Long-tail domain performance analysis**: Conduct detailed error analysis on the 41 PIRB tasks to identify specific domains where rerankers fail, and test whether targeted fine-tuning on underrepresented domains improves zero-shot performance.