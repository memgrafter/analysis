---
ver: rpa2
title: Control Large Language Models via Divide and Conquer
arxiv_id: '2410.04628'
source_url: https://arxiv.org/abs/2410.04628
tags:
- generation
- llms
- constraints
- text
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the performance of large language models
  (LLMs) on lexically constrained generation (LCG) tasks using prompt-based control.
  Through extensive experiments, it identifies three key limitations of LLMs for LCG:
  position bias in keyword placement, low responsiveness to decoding parameters (temperature,
  top-k, top-p), and struggles with compound words due to subword tokenization.'
---

# Control Large Language Models via Divide and Conquer

## Quick Facts
- **arXiv ID**: 2410.04628
- **Source URL**: https://arxiv.org/abs/2410.04628
- **Reference count**: 15
- **Primary result**: Achieves over 90% improvement in instance success rate for lexically constrained generation tasks using Divide and Conquer strategy

## Executive Summary
This paper investigates the performance of large language models (LLMs) on lexically constrained generation (LCG) tasks, where generated text must include specific pre-specified keywords. Through extensive experiments, the authors identify three key limitations of LLMs for LCG: position bias in keyword placement, low responsiveness to decoding parameters, and struggles with compound words due to subword tokenization. To address these issues, they propose a Divide and Conquer Generation strategy that iteratively generates and merges sentences to satisfy all constraints. The approach significantly improves performance, achieving near-perfect results (approaching 100% success rate) in real-world applications including recipe generation, table-to-text summarization, and profile writing.

## Method Summary
The paper proposes a Divide and Conquer Generation (DnC) strategy to improve LLM performance on LCG tasks. The method works by iteratively generating text with subsets of constraints and merging the results until all constraints are satisfied. The process involves: (1) identifying unsatisfied keywords from previous generations, (2) generating new text with the remaining constraints, and (3) merging the generated sentences. The authors test this approach across multiple models (LLaMA2, LLaMA3, GPT-3.5, GPT-4) and datasets, comparing performance against baseline greedy decoding and rejection sampling methods. They systematically vary constraint complexity (3-20 keywords) and evaluate using instance success rate and keyword coverage metrics.

## Key Results
- DnC strategy achieves over 90% improvement in instance success rate for most challenging LCG tasks
- Performance approaches 100% success rate in real-world applications (recipe generation, table-to-text, profile writing)
- LLMs exhibit position bias in keyword placement, with early and late positions being more effective for constraint inclusion
- Decoding parameters (temperature, top-k, top-p) show minimal impact on constraint satisfaction, with less than 4% variance in performance

## Why This Works (Mechanism)

### Mechanism 1: Position Bias Exploitation
- Claim: LLMs are more likely to include constraints placed at certain positions in the prompt, especially early or late positions.
- Mechanism: The Divide and Conquer strategy exploits this bias by iteratively generating smaller sentences that include subsets of the constraints, ensuring each subset's constraints are placed optimally for inclusion.
- Core assumption: LLMs exhibit consistent position bias that can be leveraged to improve constraint satisfaction.
- Evidence anchors:
  - [abstract] Identifies position bias as a key limitation where LLMs tend to satisfy constraints in specific positions.
  - [section 3.1] Confirms all LLMs exhibit position bias, with some models influenced by primacy effect and others by recency effect.
  - [corpus] Weak evidence - no direct support for position bias exploitation in the corpus neighbors.
- Break condition: If the position bias pattern changes significantly across different model architectures or prompt styles, the strategy may become less effective.

### Mechanism 2: Iterative Constraint Satisfaction
- Claim: Breaking down complex constraint satisfaction tasks into smaller, simpler subtasks improves overall success rates.
- Mechanism: The Divide and Conquer Generation strategy iteratively generates sentences with subsets of constraints, then merges them, reducing cognitive load on the model compared to handling all constraints at once.
- Core assumption: LLMs perform better on simpler tasks with fewer constraints per generation.
- Evidence anchors:
  - [abstract] Proposes Divide and Conquer Generation to enhance LLM performance in LCG tasks.
  - [section 2.3] Shows performance declines dramatically as constraint complexity (number of keywords) increases.
  - [section 5.2] Demonstrates LLaMA2-7b-chat error rate decreases from ~96% to 3% using DnC strategy.
- Break condition: If the merging process introduces coherence issues that outweigh the benefits of improved constraint satisfaction, or if the number of iterations becomes prohibitively large.

### Mechanism 3: Decoding Parameter Insensitivity Workaround
- Claim: Since LLMs show low responsiveness to decoding parameters for LCG tasks, alternative strategies like DnC are needed to improve performance.
- Mechanism: The Divide and Conquer approach circumvents the need for sensitive decoding parameter tuning by focusing on iterative constraint satisfaction rather than relying on parameter adjustments.
- Core assumption: Decoding parameters have minimal impact on constraint satisfaction in LCG tasks.
- Evidence anchors:
  - [abstract] Identifies low responsiveness to decoding parameters as a key limitation.
  - [section 3.3] Shows minimal variance in performance across different temperature, top-k, and top-p settings (within 4%).
  - [corpus] No direct evidence in corpus neighbors about decoding parameter insensitivity.
- Break condition: If future LLM architectures show improved responsiveness to decoding parameters for constraint satisfaction, this mechanism may become less critical.

## Foundational Learning

- **Concept: Lexical Constrained Generation (LCG)**
  - Why needed here: Understanding LCG is fundamental to grasping the problem the paper addresses and the solution it proposes.
  - Quick check question: What is the primary goal of Lexically Constrained Generation in natural language processing?

- **Concept: Position Bias in Language Models**
  - Why needed here: Position bias is one of the key limitations identified that the Divide and Conquer strategy exploits.
  - Quick check question: How does position bias affect the likelihood of constraint inclusion in generated text?

- **Concept: Subword Tokenization and Compound Words**
  - Why needed here: The paper identifies that LLMs struggle with compound words due to subword tokenization, which is relevant to understanding constraint complexity.
  - Quick check question: Why do compound words pose challenges for LLMs in lexical constraint generation?

## Architecture Onboarding

- **Component map**: Input keywords → LLM Generator → Constraint Evaluator → Merger → Output text
- **Critical path**: Input keywords → Divide constraints → Generate with subsets → Check constraints → Merge → Output
- **Design tradeoffs**:
  - More iterations improve constraint satisfaction but increase API calls and cost
  - Smaller constraint subsets improve success rates but may require more complex merging
  - Choice of which constraints to group together affects both generation quality and merging coherence
- **Failure signatures**:
  - Persistent constraint omission despite multiple iterations
  - Severe coherence degradation in merged text
  - Excessive iteration count without improvement
- **First 3 experiments**:
  1. Test basic DnC with 2-keyword subsets on a simple LCG task
  2. Compare performance with varying maximum iterations (K=1,2,3)
  3. Evaluate merged text quality using both human and automatic metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the position bias in keyword placement vary across different types of constraints (e.g., simple vs. compound words)?
- Basis in paper: [explicit] The paper identifies position bias as a key limitation, where LLMs tend to satisfy constraints appearing in specific positions within the input.
- Why unresolved: The paper mentions that keywords placed earlier in the input sequence are more likely to be covered, but does not delve into how this bias varies with different types of constraints.
- What evidence would resolve it: Conducting experiments to analyze the position bias specifically for compound words versus simple words, and comparing the results to see if there is a significant difference in how position affects coverage rates.

### Open Question 2
- Question: What is the impact of varying the number of iterations in the Divide and Conquer Generation strategy on the quality and coherence of the generated text?
- Basis in paper: [explicit] The paper introduces the Divide and Conquer Generation strategy and evaluates its performance in improving LLM success rates.
- Why unresolved: While the paper demonstrates that the strategy improves success rates, it does not provide detailed analysis on how the number of iterations affects the quality and coherence of the text.
- What evidence would