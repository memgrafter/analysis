---
ver: rpa2
title: 'Large Language Models for Human-like Autonomous Driving: A Survey'
arxiv_id: '2407.19280'
source_url: https://arxiv.org/abs/2407.19280
tags:
- llms
- driving
- autonomous
- available
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively reviews the integration of Large Language
  Models (LLMs) into autonomous driving (AD) systems, categorizing the evolution of
  AD from rule-based to knowledge-based approaches powered by LLMs. The paper highlights
  two primary applications: modular AD pipelines and end-to-end AD systems.'
---

# Large Language Models for Human-like Autonomous Driving: A Survey

## Quick Facts
- **arXiv ID**: 2407.19280
- **Source URL**: https://arxiv.org/abs/2407.19280
- **Reference count**: 40
- **Primary result**: Comprehensive survey reviewing integration of LLMs into autonomous driving systems, categorizing evolution from rule-based to knowledge-based approaches.

## Executive Summary
This survey provides a comprehensive review of integrating Large Language Models (LLMs) into autonomous driving (AD) systems, examining the transition from traditional rule-based approaches to knowledge-based systems powered by LLMs. The paper focuses on two primary applications: modular AD pipelines where LLMs enhance decision-making through multimodal reasoning, and end-to-end systems that leverage LLMs for holistic reasoning across perception, planning, and control. Key advancements include improved adaptability to complex scenarios, enhanced interpretability, and better generalization to novel environments, though significant challenges remain in real-time inference, computational costs, and safety assurance.

## Method Summary
The survey synthesizes existing literature on LLM applications in autonomous driving, analyzing current approaches and identifying future research directions. It examines fine-tuning techniques like PEFT and RLHF for adapting large pre-trained LLMs to driving-specific tasks, while proposing frameworks for integrating multimodal information. The methodology involves categorizing existing work into modular and end-to-end approaches, analyzing their strengths and limitations, and identifying key challenges such as computational overhead, safety constraints, and ethical considerations. The survey proposes experimental validation through closed-loop testing in realistic simulators and development of lightweight LLM variants suitable for real-time AD applications.

## Key Results
- LLMs serve as knowledge bases to provide rich commonsense knowledge for AD, compensating for limitations of pure data-driven methods
- Multimodal LLMs can integrate perception, planning, and control into unified frameworks for end-to-end autonomous driving
- Fine-tuning techniques like PEFT and RLHF enable efficient adaptation of large LLMs to specific autonomous driving tasks while maintaining safety alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs enhance autonomous driving by serving as knowledge bases that compensate for the limitations of pure data-driven methods.
- Mechanism: LLMs encode vast amounts of structured and unstructured knowledge from their training corpora, enabling them to provide commonsense reasoning and contextual understanding in novel driving scenarios that traditional deep learning models cannot generalize to.
- Core assumption: The pre-training process on diverse text data captures generalizable patterns and knowledge that are applicable to real-world driving situations.
- Evidence anchors:
  - [abstract] "LLMs can serve as knowledge bases to provide rich commonsense knowledge for AD, compensating for the shortcomings of pure data-driven methods."
  - [section II.A] "Pre-training plays a pivotal role in the development of LLMs and MLLMs by enabling them to acquire a broad and nuanced understanding of natural language and multimodal data."
- Break condition: If the LLM's training corpus lacks relevant driving-related knowledge or contains biases that conflict with safe driving practices.

### Mechanism 2
- Claim: Multimodal LLMs integrate perception, planning, and control into a unified framework for end-to-end autonomous driving.
- Mechanism: By processing multiple input modalities (images, text, point clouds) through modality encoders and a fusion module, MLLMs can directly map sensory inputs to driving actions without requiring separate perception-planning-control pipelines.
- Core assumption: The fusion of multimodal information enables richer contextual understanding that leads to more accurate and human-like driving decisions.
- Evidence anchors:
  - [abstract] "End-to-end systems leverage LLMs for holistic reasoning, combining perception, planning, and control in a unified framework."
  - [section II] "Several influential MLLMs, such as BLIP-2 [19], LLaV A [20], and Flamingo [21], have laid the foundation for the development of this field. These models have demonstrated impressive performance in tasks like Visual Question Answering (VQA), image captioning, and multimodal reasoning."
- Break condition: If the multimodal fusion fails to preserve critical information from individual modalities or if the computational overhead prevents real-time operation.

### Mechanism 3
- Claim: Fine-tuning techniques like PEFT and RLHF enable efficient adaptation of large LLMs to specific autonomous driving tasks while maintaining safety alignment.
- Mechanism: Parameter-efficient fine-tuning methods allow large pre-trained LLMs to be adapted to driving-specific tasks with minimal additional parameters, while RLHF uses human feedback to align the model's outputs with human driving preferences and safety standards.
- Core assumption: The pre-trained LLM contains general reasoning capabilities that can be effectively specialized to driving tasks through targeted fine-tuning.
- Evidence anchors:
  - [section II.A.2] "Fine-tuning techniques like PEFT and RLHF are particularly beneficial for AD applications. PEFT methods allow for efficient adaptation of large models to specific driving tasks, such as pedestrian behavior prediction or traffic sign recognition, without the need for complete model retraining. RLHF, on the other hand, can be used to align AD systems with human driving preferences and safety standards, potentially leading to more natural and acceptable driving behaviors."
  - [section V] "Design multi-modal training frameworks that can fuse textual, visual and geographical information to ground LLMs' reasoning in physical AD contexts."
- Break condition: If the fine-tuning process overfits to specific scenarios or if the human feedback used in RLHF is not representative of diverse driving conditions.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how LLMs process and generate sequences is fundamental to grasping how they can be applied to driving tasks that require reasoning over time and context
  - Quick check question: What is the key difference between self-attention and cross-attention in the context of multimodal LLMs?

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: RLHF is a critical technique for aligning LLM outputs with human values and preferences, which is essential for ensuring safe and acceptable autonomous driving behavior
  - Quick check question: How does the Boltzmann distribution model human choices in the RLHF framework?

- Concept: Multimodal fusion techniques
  - Why needed here: Autonomous driving requires integrating information from multiple sensors and data sources, making multimodal fusion a key capability for LLM-based systems
  - Quick check question: What are the trade-offs between early, late, and hybrid fusion approaches in multimodal systems?

## Architecture Onboarding

- Component map: Sensor data → Modality encoders → Multimodal fusion → LLM reasoning → Safety constraint checking → Control command generation → Vehicle actuation
- Critical path: Sensor data → Modality encoders → Multimodal fusion → LLM reasoning → Safety constraint checking → Control command generation → Vehicle actuation. The LLM reasoning step is typically the most computationally intensive and latency-sensitive.
- Design tradeoffs: Larger LLMs provide better reasoning capabilities but increase computational cost and latency; end-to-end approaches offer simplicity but may lack interpretability compared to modular pipelines; fine-tuning on driving data improves task-specific performance but may reduce general reasoning capabilities.
- Failure signatures: Unexpected vehicle behavior in rare scenarios, increased inference latency beyond real-time requirements, safety constraint violations, degraded performance on tasks not seen during fine-tuning, or biased decision-making based on training data imbalances.
- First 3 experiments:
  1. Implement a simple modular pipeline where an LLM processes traffic scene descriptions and generates high-level driving decisions, comparing its performance against a rule-based baseline in a driving simulator.
  2. Fine-tune a pre-trained LLM on a driving instruction dataset using PEFT techniques, then evaluate its ability to follow natural language driving commands in various scenarios.
  3. Integrate a multimodal LLM with a perception module to create an end-to-end system that maps raw sensor data to driving actions, benchmarking its performance against a traditional perception-planning-control pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can lightweight LLMs be effectively developed for real-time autonomous driving applications without compromising safety and reasoning capabilities?
- Basis in paper: [explicit] The paper identifies the need for lightweight LLMs tailored for AD applications with limited computational budgets and suggests knowledge distillation from large LLMs to smaller models as a potential direction.
- Why unresolved: Current LLMs require significant computational resources that may not be feasible for real-time AD systems, yet simply reducing model size risks losing critical reasoning capabilities needed for safe autonomous driving.
- What evidence would resolve it: Demonstration of a lightweight LLM that achieves comparable reasoning performance to larger models on AD-specific benchmarks while meeting real-time inference requirements (<100ms latency) and maintaining safety standards in closed-loop simulation tests.

### Open Question 2
- Question: What training frameworks can effectively integrate multimodal information (text, vision, geographical data) to ground LLM reasoning in physical autonomous driving contexts?
- Basis in paper: [explicit] The survey proposes designing multi-modal training frameworks that can fuse textual, visual and geographical information to ground LLMs' reasoning in physical AD contexts.
- Why unresolved: While multimodal LLMs show promise, current approaches struggle to properly integrate diverse data sources (sensor inputs, maps, traffic rules) in a way that produces coherent, physically-grounded driving decisions.
- What evidence would resolve it: Development of a training framework that successfully combines multiple modalities and demonstrates superior performance on complex driving scenarios compared to unimodal approaches, validated through both simulation and real-world testing.

### Open Question 3
- Question: How can safety constraints and ethical guidelines be effectively incorporated into LLM training objectives and inference processes to ensure reliable and value-aligned outputs for autonomous driving?
- Basis in paper: [explicit] The paper identifies the need to introduce safety constraints and ethical guidelines into LLM's training objectives and inference processes to improve reliability and value alignment of their outputs.
- Why unresolved: LLMs trained on large-scale online corpora may inherit social biases and generate outputs misaligned with human values, yet there is no established methodology for embedding safety and ethical considerations into the training and deployment of LLMs for safety-critical applications.
- What evidence would resolve it: A framework that successfully incorporates safety constraints and ethical guidelines into LLM training, demonstrated by consistent adherence to safety standards and ethical principles across diverse driving scenarios in extensive testing.

## Limitations
- The survey lacks empirical validation of proposed LLM integration methods, relying primarily on theoretical frameworks and existing literature without providing quantitative performance comparisons
- Limited discussion of computational requirements and real-time constraints for deploying large LLMs in safety-critical driving scenarios
- Unclear guidance on how to address the safety assurance gap between simulation environments and real-world deployment

## Confidence
- **High**: The categorization of AD evolution from rule-based to knowledge-based approaches and the identification of key LLM applications in modular and end-to-end systems
- **Medium**: Claims about LLM capabilities for commonsense reasoning and contextual understanding, as these depend heavily on the quality and relevance of training data
- **Low**: Assertions about achieving human-like interactions and interpretability without concrete metrics or benchmark results

## Next Checks
1. Conduct closed-loop testing of an LLM-enhanced AD system in CARLA simulator across diverse driving scenarios to measure safety metrics and decision-making quality
2. Perform computational profiling to quantify the real-time inference overhead of different LLM architectures in AD pipelines
3. Design a human-in-the-loop evaluation protocol to assess the interpretability and user acceptance of LLM-generated driving decisions