---
ver: rpa2
title: 'MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward
  Models'
arxiv_id: '2410.17578'
source_url: https://arxiv.org/abs/2410.17578
tags:
- language
- languages
- arxiv
- preprint
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MM-Eval addresses the lack of multilingual meta-evaluation benchmarks
  for assessing the performance of LLM-based evaluators across languages. It introduces
  five core subsets covering 18 languages and a Language Consistency subset spanning
  122 languages, using controlled translations and language-specific challenges rather
  than direct translations.
---

# MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models

## Quick Facts
- **arXiv ID:** 2410.17578
- **Source URL:** https://arxiv.org/abs/2410.17578
- **Reference count:** 40
- **Primary result:** 12 LLM evaluators achieve only 68.9% average accuracy on multilingual meta-evaluation, with significant drops for low-resource languages and LCI scores below 0.6 for open models.

## Executive Summary
MM-Eval addresses the critical gap in multilingual evaluation of LLM-as-a-judge and reward models by introducing a comprehensive benchmark spanning 18 core languages and 122 languages for consistency testing. Unlike existing benchmarks that simply translate English data, MM-Eval uses controlled translations and language-specific prompt construction to avoid artifacts while testing true multilingual capabilities. The benchmark introduces the Language Consistency Index (LCI) to measure fairness across languages, revealing that existing evaluators show significant bias against low-resource languages despite strong English performance.

## Method Summary
MM-Eval constructs five core subsets (Reasoning, Chat, Linguistics, Language Hallucination, Safety) and one Language Consistency subset using controlled translation rather than direct machine translation to avoid artifacts. For each language, prompt-response pairs are specifically tailored rather than translated, ensuring labels reflect true preference. The benchmark evaluates pairwise accuracy and introduces LCI to measure consistency of absolute scores across languages. All evaluator feedback is generated in English regardless of target language. The dataset is validated against Best-of-N rankings, showing strong correlation (R²=0.848) with practical performance.

## Key Results
- Average accuracy across 12 evaluators is only 68.9% (random baseline: 50%), with 9 models scoring below 70%
- Low-resource languages show 12.8-18.4% performance drops compared to English
- Open models achieve LCI scores below 0.6, indicating unfair and inconsistent scoring
- Strong correlation with Best-of-N rankings (R²=0.848, p=0.0265) validates practical utility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Controlled translation of preference datasets avoids translation artifacts that can invalidate labels and introduce language-specific biases.
- Mechanism: Instead of machine-translating entire English benchmarks, the authors construct new preference data using prompt-response pairs specifically tailored to each language, ensuring labels reflect true preference rather than translation-induced artifacts.
- Core assumption: Translation artifacts (e.g., mistranslations, unnatural phrasing) can mislead evaluators and distort their ability to judge true quality across languages.
- Evidence anchors:
  - [abstract]: "A core attribute of MM-E VAL is that, instead of merely translating existing English meta-evaluation benchmarks, it is designed with multilingual-specific challenges in mind."
  - [section 2]: "Machine Translation of entire preference datasets should be avoided, as artifacts can introduce subtle errors and invalidate original labels."

### Mechanism 2
- Claim: The Language Consistency Index (LCI) quantifies fairness by measuring how uniformly evaluators score equivalent content across languages, revealing hidden bias.
- Mechanism: LCI compares the difference in scores between chosen and rejected responses for each language, normalizing by the maximum difference; lower LCI values indicate inconsistent or unfair scoring across languages.
- Core assumption: A perfectly fair evaluator should assign identical score differences to parallel inputs in different languages, regardless of language resource level.
- Evidence anchors:
  - [abstract]: "unlike existing meta-evaluation benchmarks that focus solely on ranking accuracy over pairwise data, MM-E VAL also evaluates the consistency and fairness of absolute score values across a wide range of languages."
  - [section 3.1]: "LCI is defined as... A perfectly fair evaluator LLM would achieve an LCI score of 1.0, as ΔSi would equal ΔSnorm for all i."

### Mechanism 3
- Claim: Strong English evaluation performance does not guarantee strong multilingual evaluation performance, as evidenced by significant accuracy drops for low-resource languages.
- Mechanism: Experimental results show average accuracy of only 68.9% (random: 50%) and substantial performance degradation (12.8-18.4%) for low-resource languages, with open models scoring LCI < 0.6.
- Core assumption: Evaluators that perform well in English will generalize to other languages without additional multilingual-specific training or tuning.
- Evidence anchors:
  - [abstract]: "Our results show that existing evaluator LLMs that excel in English contexts have considerable room for improvement when assessing non-English outputs."
  - [section 4.2]: "average performance of the models is 68.9%, with nine models scoring below 70%, indicating considerable room for improvement."

## Foundational Learning

- **Multilingual text evaluation**: Evaluators must judge text quality in languages they were not primarily trained on, requiring sensitivity to linguistic features beyond English.
  - Quick check question: Can an evaluator reliably distinguish a fluent vs. disfluent sentence in a low-resource language without training data in that language?

- **Preference learning and reward modeling**: Evaluators are trained to predict human preferences over response pairs; understanding this framework is critical for interpreting benchmark results.
  - Quick check question: In a pairwise evaluation, if an evaluator assigns higher score to the rejected response, what does that imply about its alignment with human preference?

- **Translationese and its effects on NLP evaluation**: Translation artifacts can introduce unnatural phrasing that affects both the content and the evaluator's perception, invalidating labels.
  - Quick check question: Why might a directly translated prompt-response pair be harder for an evaluator to judge fairly compared to a natively generated one?

## Architecture Onboarding

- **Component map**: Dataset construction → controlled translation/generation → quality checks → pairwise labeling → evaluation → statistical analysis (accuracy, LCI) → correlation validation
- **Critical path**: Data construction → controlled translation / generation → quality checks → pairwise labeling → evaluation → statistical analysis (accuracy, LCI) → correlation validation against Best-of-N rankings
- **Design tradeoffs**: Avoiding full MT preserves label quality but increases construction effort; focusing on parallel consistency isolates fairness but may not capture all real-world scenarios; excluding very low-quality translations keeps data clean but reduces language coverage
- **Failure signatures**: (a) Accuracy close to 50% across all languages → evaluator lacks discrimination; (b) LCI << 1.0 → evaluator biased toward high-resource languages; (c) High variance in scores for same content across languages → instability or hallucination
- **First 3 experiments**:
  1. Run evaluators on the Language Consistency subset and compute LCI to detect fairness issues
  2. Evaluate accuracy on Reasoning vs. Safety subsets to observe language-resource impact
  3. Compare correlation of MM-Eval rankings with Best-of-32 scores to validate practical utility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Language Consistency Index (LCI) behave when applied to different types of parallel datasets beyond the BELEBELE benchmark?
- Basis in paper: [explicit] The authors introduce LCI as a metric to quantify fairness and consistency of evaluator LLMs across languages, but only validate it on the BELEBELE dataset.
- Why unresolved: The paper does not explore whether LCI maintains its effectiveness when applied to other parallel datasets with different linguistic properties or task types.
- What evidence would resolve it: Testing LCI on multiple parallel datasets (e.g., different reading comprehension benchmarks, translationese data, or code-switching datasets) and comparing the results to validate its generalizability.

### Open Question 2
- Question: What specific factors contribute to the negative and positive hallucinations observed in Prometheus 2 8x7B's feedback across different languages?
- Basis in paper: [explicit] The authors observe that Prometheus 2 8x7B frequently fails to ground its feedback in the given response and hallucinates, with examples of both negative and positive hallucinations.
- Why unresolved: The paper identifies the phenomenon but does not investigate the underlying causes, such as model architecture limitations, training data bias, or language-specific factors.
- What evidence would resolve it: Analyzing the model's attention patterns, conducting ablation studies on training data composition, or comparing hallucination rates across different model architectures and training regimes.

### Open Question 3
- Question: How do evaluator LLMs perform when tasked with evaluating code-switched outputs versus outputs in a single language?
- Basis in paper: [inferred] The Language Hallucination subset tests for unintentional code-switching, but does not evaluate the model's ability to properly assess intentional code-switching or mixed-language outputs.
- Why unresolved: The paper focuses on detecting language hallucinations but does not explore whether evaluators can fairly assess outputs that intentionally mix languages, which is common in multilingual communication.
- What evidence would resolve it: Creating a benchmark with controlled code-switching patterns (varying switching frequency, language pairs, and switching positions) and measuring evaluator performance across these variations.

## Limitations

- Controlled translation methodology may introduce new biases through authors' prompt construction choices, lacking systematic validation of linguistic diversity preservation
- LCI assumes parallel datasets should receive identical scores across languages, but subtle linguistic differences may legitimately affect evaluator judgments
- Dataset construction quality control procedures are described as "thorough" but lack specific details about validation methods, making it difficult to assess potential artifacts

## Confidence

**High confidence** - The core finding that English-proficient evaluators perform significantly worse on low-resource languages (68.9% average accuracy, 12.8-18.4% drops) is well-supported by experimental results across multiple models and subsets.

**Medium confidence** - The correlation with Best-of-N rankings (R²=0.848, p=0.0265) suggests MM-Eval captures meaningful evaluation quality, though the sample size of 12 evaluators may limit generalizability.

**Low confidence** - The claim that avoiding MT is essential for valid labels is plausible but not directly tested against MT-based benchmarks in this work.

## Next Checks

1. **Replication of LCI computation**: Implement the exact Language Consistency Index calculation from the paper and verify that open models consistently score below 0.6 while proprietary models score higher, confirming the fairness gap finding.

2. **Cross-validation with alternative evaluation methods**: Test whether MM-Eval rankings correlate with LLM-as-a-judge performance on task-specific benchmarks (not just Best-of-N) to assess broader validity.

3. **Controlled bias analysis**: Systematically vary prompt construction parameters (e.g., complexity, formality) across languages to determine whether controlled translation itself introduces language-specific biases that affect evaluator performance.