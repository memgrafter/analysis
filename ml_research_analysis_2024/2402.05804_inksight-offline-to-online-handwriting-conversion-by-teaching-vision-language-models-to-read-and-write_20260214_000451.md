---
ver: rpa2
title: 'InkSight: Offline-to-Online Handwriting Conversion by Teaching Vision-Language
  Models to Read and Write'
arxiv_id: '2402.05804'
source_url: https://arxiv.org/abs/2402.05804
tags:
- input
- recognized
- training
- figure
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "InkSight converts photos of offline handwriting into digital ink\
  \ using a vision-language model trained with a multi-task setup combining recognition\
  \ and derendering tasks. It employs standard components\u2014a ViT encoder and mT5\
  \ encoder-decoder\u2014to produce semantically and geometrically accurate digital\
  \ ink representations without requiring paired training data."
---

# InkSight: Offline-to-Online Handwriting Conversion by Teaching Vision-Language Models to Read and Write

## Quick Facts
- arXiv ID: 2402.05804
- Source URL: https://arxiv.org/abs/2402.05804
- Reference count: 40
- One-line primary result: Converts photos of offline handwriting to digital ink with 87% valid tracing rate and 67% human-like appearance in human evaluation

## Executive Summary
InkSight is a novel approach for converting photos of offline handwriting into digital ink (online handwriting) without requiring paired training data. The system uses a vision-language model trained with a multi-task setup combining recognition and derendering tasks, employing standard components—a ViT encoder and mT5 encoder-decoder—to produce semantically and geometrically accurate digital ink representations. The model achieves strong performance on diverse handwriting styles, backgrounds, and simple sketches, with human evaluation showing 87% of outputs are valid tracings and 67% appear human-traced.

## Method Summary
InkSight converts photos of offline handwriting into digital ink using a vision-language model trained with multi-task learning that combines recognition and derendering tasks. The approach uses a frozen ViT B/16 encoder (ImageNet-21k pretrained) and mT5 encoder-decoder, with synthetic rendered ink images augmented to mimic real photos during training. The system tokenizes normalized ink coordinates into discrete tokens (x, y values and stroke start markers) for autoregressive generation, enabling the model to learn both where text is and how to write it without requiring large amounts of paired image-ink data.

## Key Results
- Human evaluation shows 87% of outputs are valid tracings of the input image and 67% appear human-traced
- Automated metrics confirm strong similarity to both the original image and real digital ink
- The model generalizes across diverse handwriting styles, backgrounds, and simple sketches
- System is released publicly with training code and expert-traced data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task training with recognition tasks injects semantic priors that improve derendering accuracy on real photos.
- Mechanism: Recognition tasks teach the model to locate and extract text content from images, while derendering tasks teach it to convert that text into strokes. Combined, the model learns both where text is and how to write it, enabling generalization to real photos with occlusions and diverse backgrounds.
- Core assumption: The semantic understanding gained from recognition tasks transfers to the derendering task, improving geometric accuracy.
- Evidence anchors:
  - [abstract] "Our approach combines reading and writing priors, allowing training a model in the absence of large amounts of paired samples"
  - [section] "The learnedreading prior enhances the model's capability to precisely locate and extract textual elements from images"
  - [corpus] Weak - corpus papers focus on different aspects of handwriting processing, not multi-task learning for derendering
- Break condition: If recognition tasks are removed from training mixture, F1 and accuracy scores drop significantly (Table 3, rows 6-8).

### Mechanism 2
- Claim: Ink tokenization using coordinate-based representation enables autoregressive generation of digital ink strokes.
- Mechanism: The model tokenizes normalized ink coordinates into discrete tokens (x, y values and stroke start markers), allowing mT5 to generate stroke sequences autoregressively. This bridges vision-language models with digital ink representation.
- Core assumption: The discretized coordinate space preserves sufficient geometric detail for accurate stroke reconstruction.
- Evidence anchors:
  - [abstract] "It employs standard components—a ViT encoder and mT5 encoder-decoder—to produce semantically and geometrically accurate digital ink representations"
  - [section] "To enable sequence models like mT5 to generate ink strokes, we first normalize the ink and then tokenize it into a set of discrete tokens"
  - [corpus] Weak - corpus papers don't discuss ink tokenization approaches
- Break condition: If N (canvas size) is too small, rounding errors dominate and stroke geometry degrades.

### Mechanism 3
- Claim: Frozen ViT encoder provides stable feature extraction while mT5 decoder learns task-specific transformations.
- Mechanism: Freezing ViT weights prevents training instability during multi-task learning, while mT5 learns to map visual features to ink/text sequences. This separation of concerns enables effective learning without catastrophic forgetting.
- Core assumption: Pre-trained ViT features contain sufficient information for both recognition and derendering tasks.
- Evidence anchors:
  - [section] "In our training, we freeze the weights of the ViT encoder – we justify this choice empirically in Section 4.6"
  - [section] "Unfreezing the Vision Transformer (ViT) in multi-task training typically incurs training instability"
  - [corpus] Weak - corpus papers don't discuss frozen vs unfrozen vision encoders in this context
- Break condition: If ViT is unfrozen, training variance increases and background noise interpretation degrades (Figure 43, Table 3 last row).

## Foundational Learning

- Concept: Vision-Language Model Integration
  - Why needed here: InkSight needs to combine image understanding (ViT) with sequence generation (mT5) to convert photos to digital ink
  - Quick check question: What architectural component bridges the vision encoder and text decoder in InkSight?

- Concept: Multi-Task Learning Benefits
  - Why needed here: Single-task training would require paired image-ink data which is expensive; multi-task learning injects priors without paired data
  - Quick check question: How many different task types are used in InkSight's training mixture?

- Concept: Data Augmentation for Domain Adaptation
  - Why needed here: Synthetic rendered inks differ from real photos; augmentation bridges this domain gap
  - Quick check question: What types of augmentation are applied to synthetic ink images during training?

## Architecture Onboarding

- Component map: ViT B/16 encoder (frozen) → cross-attention → mT5 encoder-decoder → ink/text tokens
- Critical path: Image → ViT features → mT5 cross-attention → sequence generation → detokenization → digital ink
- Design tradeoffs: Frozen ViT for stability vs. unfrozen for detail capture; coordinate tokenization vs. alternative representations; multi-task vs. single-task training
- Failure signatures: Empty ink outputs (task confusion), background noise interpretation, missing details, duplicate strokes
- First 3 experiments:
  1. Test tokenization pipeline: Render synthetic ink, tokenize, detokenize, compare to original
  2. Test multi-task training: Train with recognition only, derendering only, and both; compare downstream performance
  3. Test augmentation impact: Train with and without data augmentation; evaluate on real vs. synthetic images

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does InkSight's performance scale with the amount of training data beyond the current datasets, particularly for rare handwriting styles or low-resource languages?
- Basis in paper: [inferred] The paper discusses limitations in generalization to unseen scripts and languages, noting that models trained only on certain languages perform worse on others, suggesting potential benefits from more diverse training data.
- Why unresolved: The paper primarily uses in-house datasets for larger models and publicly available datasets for the smaller model, but does not explore scaling training data or explicitly testing on very low-resource languages or extremely rare handwriting styles.
- What evidence would resolve it: Systematic evaluation of model performance with incrementally larger and more diverse training datasets, including rare scripts and underrepresented demographic groups, would clarify scaling behavior and highlight potential biases.

### Open Question 2
- Question: What are the most effective architectural or training strategies to improve InkSight's robustness to extreme variations in stroke width, style, and quality (e.g., very thick or thin strokes, highly stylized or degraded handwriting)?
- Basis in paper: [explicit] The paper acknowledges limitations with extreme stroke width/style variations and suggests that data augmentation and harder examples in the training mixture could help, but does not provide specific architectural solutions or quantify the impact of different augmentation strategies.
- Why unresolved: While the paper mentions these issues and suggests potential improvements, it does not conduct detailed ablation studies on architectural modifications (e.g., different encoder/decoder designs, specialized attention mechanisms) or extensive experimentation with advanced augmentation techniques.
- What evidence would resolve it: Comparative experiments testing various architectural designs and data augmentation methods, along with quantitative metrics for handling extreme stylistic variations, would identify the most effective strategies.

### Open Question 3
- Question: Can the geometric pseudo-labels generated by InkSight (digital ink sequences) be effectively used for unsupervised clustering or analysis of handwriting styles beyond the proposed sequence similarity methods?
- Basis in paper: [explicit] The paper proposes using generated digital ink sequences as geometric pseudo-labels for unsupervised clustering of handwriting based on shape and stroke properties, but does not explore alternative clustering algorithms or downstream applications.
- Why unresolved: The paper suggests this as a potential application but only briefly mentions using sequence comparison methods like DTW, without exploring other machine learning techniques (e.g., deep metric learning, graph-based clustering) or validating the usefulness of clusters for specific tasks.
- What evidence would resolve it: Experiments applying various clustering algorithms and downstream tasks (e.g., style classification, anomaly detection) to the generated ink sequences would demonstrate the practical utility and limitations of geometric pseudo-labels for unsupervised analysis.

## Limitations

- The system is limited to Latin script, raising questions about scalability to other writing systems
- Claims about "no paired data" are technically true but require the synthetic data generation pipeline, which represents a significant practical requirement
- The 87% valid tracing rate represents single-word evaluation rather than full-page or document-level performance

## Confidence

**High Confidence**: The core technical approach of using multi-task learning with frozen ViT and mT5 for ink conversion is well-supported by the experimental results and ablation studies. The claims about training stability with frozen encoders and the effectiveness of the multi-task setup are directly validated through systematic experimentation.

**Medium Confidence**: Claims about generalization to diverse handwriting styles, backgrounds, and simple sketches are supported by the ablation study and human evaluation, but the evaluation dataset appears relatively limited in scope. The assertion that no paired training data is needed is technically true but requires the synthetic data generation pipeline, which represents a significant practical requirement.

**Low Confidence**: The claim of 87% valid tracing rate should be viewed with caution as it represents single-word evaluation rather than full-page or document-level performance. The system's behavior on degraded handwriting, heavy occlusions, or non-Latin scripts is not well-characterized.

## Next Checks

1. **Full Document Processing**: Test the model on multi-line, multi-word handwritten documents to validate performance beyond single-word examples. Measure degradation in accuracy and stroke quality as document complexity increases.

2. **Cross-Script Generalization**: Evaluate the system on non-Latin scripts (Arabic, Chinese, Devanagari) to assess whether the Latin-script-specific pretraining limits generalization. Document performance drop and identify which components (recognition vs derendering) fail first.

3. **Ablation of Synthetic Data Quality**: Systematically vary the quality and diversity of synthetic training data to determine the minimum requirements for achieving the reported 87% valid tracing rate. This would reveal whether the claimed "no paired data" advantage is truly significant or if high-quality synthetic data generation is the real requirement.