---
ver: rpa2
title: 'LoKO: Low-Rank Kalman Optimizer for Online Fine-Tuning of Large Models'
arxiv_id: '2410.11551'
source_url: https://arxiv.org/abs/2410.11551
tags:
- kalman
- lora
- arxiv
- online
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LoKO, a Kalman filter-based optimizer for online
  fine-tuning of large models. The key idea is to cast parameter-efficient fine-tuning
  as a state estimation problem, leveraging the low-rank structure from LoRA to reduce
  computational complexity.
---

# LoKO: Low-Rank Kalman Optimizer for Online Fine-Tuning of Large Models

## Quick Facts
- arXiv ID: 2410.11551
- Source URL: https://arxiv.org/abs/2410.11551
- Reference count: 34
- Key outcome: LoKO achieves linear complexity in trainable parameters and outperforms AdamW/AdaGrad on online fine-tuning tasks across vision and language models.

## Executive Summary
This paper proposes LoKO, a Kalman filter-based optimizer for online fine-tuning of large models. The key innovation is casting parameter-efficient fine-tuning as a state estimation problem, leveraging LoRA's low-rank structure to reduce computational complexity. By approximating the covariance matrix diagonally and using EMA to estimate observation noise, LoKO achieves linear complexity in trainable parameters. Experiments on computer vision (DenseNet, ResNet, ViT) and language (RoBERTa) models show that LoKO consistently outperforms or matches AdamW and AdaGrad in convergence speed and accuracy across multiple datasets. Notably, LoKO's performance is robust to initialization and hyperparameter choices within a wide range.

## Method Summary
LoKO implements online fine-tuning using an Extended Kalman Filter (EKF) framework. The method casts parameter updates as a state estimation problem where trainable parameters are the state vector. LoRA provides low-rank decomposition, while diagonal approximation of the covariance matrix reduces computational complexity from quadratic to linear. EMA estimates observation noise covariance dynamically. The optimizer recursively updates parameters through prediction, pre-updating, and update steps, achieving faster convergence than gradient-based optimizers while maintaining robust performance across different initialization settings.

## Key Results
- LoKO achieves linear complexity in trainable parameters by diagonal covariance approximation and LoRA decomposition
- Outperforms AdamW and AdaGrad on MNIST, CIFAR-10/100, ImageNet100, SST-2, COLA, and MRPC across vision and language models
- Robust performance across wide range of hyperparameter settings and initialization values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Kalman filter enables faster and more stable convergence by incorporating both prediction and measurement updates, effectively performing second-order optimization without explicit Hessian computation.
- Mechanism: The EKF algorithm recursively estimates optimal parameters as a state estimation problem, where the process model is identity (no change between steps) and the observation model is the model's output. This structure allows the filter to adaptively adjust parameter updates based on prediction error, naturally balancing trust between current parameters and new data.
- Core assumption: The underlying system can be modeled as a nonlinear state-space model where the process noise is negligible and the observation noise covariance can be accurately estimated.
- Evidence anchors:
  - [abstract]: "cast PEFT as an optimal filtering/state estimation problem and present Low-Rank Kalman Optimizer (LoKO) to estimate the optimal trainable parameters in an online manner"
  - [section 4.1]: "we assume there are no feedback loops in machine learning model... this assumption enables us to consider the trainable parameters that are represented as the state vector of the process model"
  - [corpus]: Weak - corpus papers focus on LoRA variants but don't discuss Kalman-based optimization mechanisms
- Break condition: If the system exhibits strong temporal dependencies or feedback loops, the identity process model assumption breaks down, requiring more complex state transition functions.

### Mechanism 2
- Claim: Low-rank decomposition from LoRA combined with diagonal covariance approximation reduces computational complexity from quadratic to linear in the number of trainable parameters.
- Mechanism: By restricting parameter updates to low-rank matrices (A and B in LoRA) and approximating the covariance matrix as diagonal, the Kalman gain computation becomes tractable. The diagonal approximation eliminates the need for full matrix inversions, while the low-rank structure limits the parameter space size.
- Core assumption: The covariance matrix becomes approximately diagonal during training, which empirical evidence supports for feedforward networks.
- Evidence anchors:
  - [section 4.1]: "we employ a diagonal approximation for the covariance matrix P, a common approach to reduce the computational overhead from quadratic to linear"
  - [section 5.3]: "Our empirical findings demonstrate that as the fine-tuning algorithm progresses, the covariance matrix of the feed-forward neural network asymptotically approaches a (block-)diagonal configuration"
  - [corpus]: Weak - corpus papers discuss LoRA computational benefits but not the diagonal covariance approximation
- Break condition: If the parameter correlations remain strong throughout training (e.g., in highly coupled architectures), the diagonal approximation loses accuracy and performance degrades.

### Mechanism 3
- Claim: Exponential Moving Average (EMA) estimation of observation noise covariance provides robust and adaptive noise modeling without additional computational cost.
- Mechanism: The EMA approach (Rk = βR k-1 + (1-β) ˆRk) continuously updates the noise covariance estimate based on recent prediction errors, allowing the Kalman filter to adapt to changing data distributions and model uncertainty dynamically.
- Core assumption: The observation noise covariance changes slowly enough that EMA provides a good balance between responsiveness and stability.
- Evidence anchors:
  - [section 4.1]: "we employ an Exponential Moving Average (EMA) approach based on the definition of the covariance matrix for estimating the matrix Rk"
  - [section 4.2]: "we propose an alternative method for approximating the observation noise covariance, R k, with enhanced accuracy and without additional computation cost"
  - [corpus]: Weak - corpus papers don't discuss EMA-based noise estimation in optimization contexts
- Break condition: If the observation noise changes rapidly or has abrupt shifts, EMA may lag too much, requiring adaptive forgetting factors or alternative estimation methods.

## Foundational Learning

- Concept: Extended Kalman Filter (EKF) algorithm and its mathematical foundations
  - Why needed here: LoKO is built on EKF principles, so understanding prediction/update steps, Jacobian computation, and covariance propagation is essential
  - Quick check question: What are the three main steps in the EKF algorithm and how do they relate to parameter optimization?

- Concept: Low-Rank Adaptation (LoRA) technique and its mathematical formulation
  - Why needed here: LoKO leverages LoRA's parameter reduction, so understanding the decomposition W0 + BA and initialization strategies is crucial
  - Quick check question: How does LoRA reduce the number of trainable parameters from d×q to r×(d+q), and what are the initialization requirements?

- Concept: State-space modeling and nonlinear system identification
  - Why needed here: The paper casts fine-tuning as state estimation, requiring understanding of how neural networks can be represented as nonlinear observation functions
  - Quick check question: How do you formulate a neural network forward pass as an observation model in state-space representation?

## Architecture Onboarding

- Component map:
  LoRA decomposition layer -> EKF core -> EMA noise estimator -> Diagonal covariance approximation -> Jacobian calculator

- Critical path:
  1. Forward pass through model to get predictions
  2. Compute Jacobian matrix H_k
  3. Estimate observation noise R_k using EMA
  4. Calculate Kalman gain K_k
  5. Update parameters and covariance
  6. Repeat for each training sample

- Design tradeoffs:
  - Diagonal covariance approximation vs. full covariance: Linear vs. quadratic complexity, but potential loss of parameter correlation information
  - EMA forgetting factor β: Trade-off between responsiveness to new data and stability
  - LoRA rank selection: Balance between parameter efficiency and adaptation capacity

- Failure signatures:
  - Divergence: Covariance initialization too high or noise estimation unstable
  - Slow convergence: Covariance initialization too low or rank too small
  - Poor generalization: Overfitting due to insufficient noise modeling or inappropriate rank

- First 3 experiments:
  1. Implement LoKO on MNIST with DenseNet-121, compare convergence speed to AdamW with same LoRA configuration
  2. Test sensitivity to covariance initialization by varying the upper bound in uniform distribution [0, upper_bound]
  3. Evaluate robustness to OOD data by injecting rotated/color-jittered MNIST samples and measuring adaptation speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LoKO perform in online fine-tuning scenarios with significant concept drift or catastrophic forgetting?
- Basis in paper: [inferred] The paper mentions sensitivity to OOD data and uses EMA for noise estimation, but doesn't explore catastrophic forgetting scenarios.
- Why unresolved: The experiments focus on single-distribution classification tasks, and concept drift is not explicitly tested.
- What evidence would resolve it: Experiments with incremental learning benchmarks or continual learning datasets showing LoKO's ability to maintain performance on previously seen tasks.

### Open Question 2
- Question: What is the theoretical relationship between the initialization bounds of ˆp0 and the convergence properties of LoKO?
- Basis in paper: [explicit] The paper discusses the importance of ˆp0 initialization bounds but states "no general criterion exists for this selection."
- Why unresolved: While empirical bounds are provided, there's no theoretical framework connecting initialization ranges to convergence guarantees.
- What evidence would resolve it: Theoretical analysis proving conditions under which specific initialization ranges guarantee convergence.

### Open Question 3
- Question: How does LoKO scale to billion-parameter models, and what are the computational bottlenecks at that scale?
- Basis in paper: [inferred] The paper tests up to 355M parameters (RoBERTa-large) and discusses computational complexity, but doesn't explore billion-parameter scale.
- Why unresolved: The largest model tested is RoBERTa-large, and while complexity analysis is provided, practical scaling behavior at billion-parameter scale is unknown.
- What evidence would resolve it: Benchmarking LoKO on trillion-parameter models with detailed profiling of computational bottlenecks.

## Limitations

- The diagonal covariance approximation may miss important parameter correlations in complex architectures
- Experimental validation is limited to relatively small-scale datasets and models
- The method's performance in catastrophic forgetting and concept drift scenarios is not explored

## Confidence

High confidence in convergence speed improvements and robustness to initialization; Medium confidence in computational complexity benefits; Low-Medium confidence in scalability to billion-parameter models.

## Next Checks

1. **Scale Test**: Implement LoKO on a large-scale vision task (e.g., full ImageNet with ViT-Huge) and measure actual memory and compute time savings compared to AdamW, verifying the claimed linear complexity advantage.

2. **Architecture Generalization**: Apply LoKO to more diverse architectures including recurrent networks and diffusion models to test the universality of the diagonal covariance approximation assumption across different model families.

3. **Robustness Stress Test**: Design experiments with deliberate distribution shift and incremental learning scenarios to evaluate LoKO's performance in preventing catastrophic forgetting and adapting to new data distributions.