---
ver: rpa2
title: Enhancing Relevance of Embedding-based Retrieval at Walmart
arxiv_id: '2408.04884'
source_url: https://arxiv.org/abs/2408.04884
tags:
- relevance
- products
- retrieval
- product
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of relevance degradation in embedding-based
  retrieval (EBR) systems used in e-commerce search, specifically at Walmart.com.
  The authors propose several methods to improve EBR relevance, including a Relevance
  Reward Model (RRM) based on human relevance feedback to remove noise from training
  data and incorporate relevance objectives into model training.
---

# Enhancing Relevance of Embedding-based Retrieval at Walmart

## Quick Facts
- arXiv ID: 2408.04884
- Source URL: https://arxiv.org/abs/2408.04884
- Authors: Juexin Lin; Sachin Yadav; Feng Liu; Nicholas Rossi; Praveen R. Suram; Satya Chembolu; Prijith Chandran; Hrushikesh Mohapatra; Tony Lee; Alessandro Magnani; Ciya Liao
- Reference count: 36
- Primary result: Proposed methods significantly improve EBR relevance through RRM, typo-aware training, and enhanced negative sampling, validated via offline metrics and online A/B tests.

## Executive Summary
This paper addresses relevance degradation in embedding-based retrieval (EBR) systems at Walmart.com by introducing several key innovations. The authors develop a Relevance Reward Model (RRM) based on human feedback to remove noise from training data, incorporate typo-aware training to handle misspelled queries, and implement improved negative sampling strategies including semi-positive generation. These methods are integrated into a multi-objective loss function that balances engagement and relevance objectives. The approach is validated through extensive offline evaluation and successful online A/B testing, demonstrating significant improvements in both relevance metrics and business outcomes.

## Method Summary
The approach uses a dual-encoder architecture with DistilBERT to transform queries and product attributes into embeddings, trained with a multi-objective loss combining engagement and RRM-derived relevance labels. The RRM is a cross-encoder trained on human-annotated relevance judgments that revises training labels by identifying false positives. Training incorporates typo injection to improve robustness to misspellings, stratified sampling to ensure query diversity, and enhanced negative sampling with semi-positive generation. The system is deployed using FAISS-based ANN search for efficient retrieval, with a downstream re-ranker for final result ordering.

## Key Results
- RRM effectively reduces false positives in training data by leveraging human relevance feedback, improving EM Precision@20
- Multi-objective loss function balancing engagement and relevance objectives leads to better overall retrieval performance
- Typo-aware training significantly improves model robustness to misspelled queries, enhancing recall for typo-containing searches
- Online A/B testing shows substantial improvements in NDCG@5/10 and revenue lift compared to baseline systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RRM effectively reduces false positives in training data by leveraging human relevance feedback.
- Mechanism: RRM uses a cross-encoder architecture trained on human-annotated relevance judgments to predict relevance labels for query-product pairs. These predictions are then used to revise training labels, downgrading products that received high engagement but low relevance scores.
- Core assumption: Human relevance judgments are more reliable indicators of true relevance than engagement-based labels.
- Evidence anchors:
  - [abstract]: "We introduce a Relevance Reward Model (RRM) based on human relevance feedback. We utilize RRM to remove noise from the training data..."
  - [section 4.1]: "To eliminate these noises, we utilized the RRM to reduce false positives by adjusting labels accordingly."
  - [corpus]: Weak - no direct citations to similar RRM approaches in the corpus.

### Mechanism 2
- Claim: Multi-objective loss function balances engagement and relevance objectives for better retrieval performance.
- Mechanism: The proposed loss function combines a softmax loss based on engagement labels with another softmax loss based on RRM-derived relevance labels. This dual optimization encourages the model to retrieve products that are both relevant and engaging.
- Core assumption: Optimizing for both relevance and engagement simultaneously leads to better overall retrieval performance than optimizing for either alone.
- Evidence anchors:
  - [abstract]: "We utilize RRM to remove noise from the training data and distill it into our EBR model through a multi-objective loss."
  - [section 5.2]: "To complement the above engagement-driven loss, we introduced a relevance loss using the relevance label derived from the RRM..."
  - [section 6.3.1]: "Figure 3 reveals that optimizing solely on either the engagement loss or the relevance loss is insufficient."

### Mechanism 3
- Claim: Typo-aware training improves model robustness to misspelled queries.
- Mechanism: During training, queries are augmented with random typos (insertions, deletions, substitutions, transpositions) to simulate real-world misspellings. This forces the model to learn representations that are invariant to minor spelling variations.
- Core assumption: Misspelled queries in live traffic follow similar patterns to the injected typos during training.
- Evidence anchors:
  - [abstract]: "We incorporate typos into queries during training to enhance the model's robustness against misspelled queries."
  - [section 5.3]: "Similar to [34], we propose incorporating typo injection through query augmentation during the training process..."
  - [corpus]: Weak - no direct citations to typo-aware training approaches in the corpus.

## Foundational Learning

- Concept: Dual-encoder architecture for EBR
  - Why needed here: The dual-encoder architecture allows efficient retrieval of relevant products by computing query and product embeddings independently and then measuring their similarity.
  - Quick check question: What are the advantages of using a dual-encoder architecture over a cross-encoder for retrieval tasks?

- Concept: Approximate Nearest Neighbor (ANN) search
  - Why needed here: ANN search enables efficient retrieval of products that are semantically similar to a given query by searching in the embedding space rather than the original feature space.
  - Quick check question: How does ANN search differ from exact nearest neighbor search, and what are the trade-offs?

- Concept: Contrastive learning
  - Why needed here: Contrastive learning techniques, such as the softmax loss used in this work, help the model learn to distinguish between relevant and irrelevant products by pulling similar pairs closer and pushing dissimilar pairs apart in the embedding space.
  - Quick check question: What is the intuition behind using a softmax loss for learning embeddings in a retrieval task?

## Architecture Onboarding

- Component map:
  Query encoder -> Product encoder -> ANN index -> Downstream re-ranker

- Critical path:
  1. User submits query
  2. Query embedding is computed using query encoder
  3. ANN search retrieves top-K products based on embedding similarity
  4. Retrieved products are re-ranked using a downstream re-ranker
  5. Final results are presented to the user

- Design tradeoffs:
  - Dual-encoder vs. cross-encoder: Dual-encoders are more efficient for retrieval but less accurate than cross-encoders. The work uses a dual-encoder for efficiency and a separate RRM (cross-encoder) for label revision.
  - Engagement vs. relevance: The multi-objective loss balances these two objectives, but the optimal weighting may vary depending on the specific application and user preferences.

- Failure signatures:
  - High EM Recall but low EM Precision: The model is retrieving relevant products but also including many irrelevant ones. This could indicate issues with the negative sampling strategy or the RRM's ability to distinguish relevant from irrelevant products.
  - Low Order Recall: The model is not retrieving products that users ultimately purchase. This could indicate a mismatch between the relevance labels used during training and the actual user preferences.

- First 3 experiments:
  1. Ablation study on the RRM: Train models with and without the RRM and compare their performance on the EM Recall and EM Precision metrics.
  2. Hyperparameter tuning for the multi-objective loss: Experiment with different values of the weighting parameter ω and evaluate their impact on the trade-off between relevance and engagement.
  3. Evaluation of typo injection patterns: Compare the performance of models trained with different typo injection strategies (e.g., different types of typos, injection probabilities) on a held-out test set with real-world misspellings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the Relevance Reward Model (RRM) vary across different product categories or domains?
- Basis in paper: [explicit] The authors mention using a relevance judgment dataset with a 3-point scale (exact, substitute, irrelevant) but do not discuss performance differences across product categories.
- Why unresolved: The paper does not provide any analysis or discussion on how the RRM's performance might differ for various types of products or search queries.
- What evidence would resolve it: Conducting experiments to measure RRM performance on different product categories and analyzing the results to identify any significant variations.

### Open Question 2
- Question: What is the optimal balance between relevance and engagement objectives in the multi-objective loss function?
- Basis in paper: [explicit] The authors mention using a weighting parameter (ω) to balance engagement and relevance losses but do not provide a detailed analysis of how different ω values affect overall performance.
- Why unresolved: The paper only briefly mentions the selection of ω = 0.5 without discussing the impact of different values or the optimal balance between relevance and engagement.
- What evidence would resolve it: Conducting a comprehensive analysis of model performance with various ω values and identifying the optimal balance for different use cases or product categories.

### Open Question 3
- Question: How does the proposed EBR system perform in handling long-tail queries compared to head queries?
- Basis in paper: [inferred] The authors discuss handling misspelled queries and improving retrieval relevance but do not specifically address the performance differences between long-tail and head queries.
- Why unresolved: The paper does not provide any analysis or discussion on how the proposed methods affect the retrieval of products for long-tail queries compared to more common, head queries.
- What evidence would resolve it: Conducting experiments to measure and compare the performance of the EBR system on long-tail queries versus head queries, including metrics such as recall, precision, and engagement rates.

## Limitations

- The evaluation relies heavily on offline metrics without extensive statistical analysis of online A/B testing results
- Human relevance judgment dataset size and potential label bias are not fully characterized
- Specific hyperparameters for the multi-objective loss weighting and RRM configurations are not completely detailed

## Confidence

- **High Confidence**: The core architectural approach (dual-encoder + RRM) and general methodology are well-established in the literature. The offline evaluation methodology is sound.
- **Medium Confidence**: The effectiveness of specific innovations (typo-aware training, semi-positive generation) is demonstrated but not extensively validated across different scenarios or compared to simpler alternatives.
- **Low Confidence**: The online A/B testing results are presented without statistical details, making it difficult to assess the practical significance of the improvements.

## Next Checks

1. **Statistical Significance Testing**: Conduct A/B tests with proper statistical power analysis and report confidence intervals for all online metrics (NDCG, revenue lift) to verify the practical significance of improvements.

2. **Label Quality Analysis**: Perform ablation studies removing the RRM to quantify its exact contribution and analyze the distribution of human relevance judgments to assess potential label bias.

3. **Generalization Testing**: Evaluate the model's performance on queries with different typo patterns and product categories not well-represented in the training data to assess robustness beyond the reported metrics.