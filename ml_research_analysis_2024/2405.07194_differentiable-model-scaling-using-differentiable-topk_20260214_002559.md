---
ver: rpa2
title: Differentiable Model Scaling using Differentiable Topk
arxiv_id: '2405.07194'
source_url: https://arxiv.org/abs/2405.07194
tags:
- search
- methods
- differentiable
- cost
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Differentiable Model Scaling (DMS), a method
  for searching optimal width and depth in neural networks. DMS uses a differentiable
  top-k operator to directly model structural hyperparameters in a fully differentiable
  manner, making it efficient to optimize.
---

# Differentiable Model Scaling using Differentiable Topk

## Quick Facts
- arXiv ID: 2405.07194
- Source URL: https://arxiv.org/abs/2405.07194
- Authors: Kai Liu; Ruohui Wang; Jianfei Gao; Kai Chen
- Reference count: 33
- One-line primary result: Differentiable Model Scaling (DMS) improves top-1 accuracy on ImageNet by 1.4% for EfficientNet-B0 and 0.6% for DeiT-Tiny while requiring only 0.4 GPU days for searching

## Executive Summary
This paper introduces Differentiable Model Scaling (DMS), a novel approach for searching optimal width and depth in neural networks using a differentiable top-k operator. Unlike traditional NAS methods that require expensive retraining or suffer from non-differentiability issues, DMS directly models structural hyperparameters in a fully differentiable manner, enabling efficient gradient-based optimization. The method was evaluated across various vision and NLP tasks, demonstrating consistent performance improvements over state-of-the-art NAS methods while requiring significantly less computational resources.

## Method Summary
DMS uses a differentiable top-k operator to model structural hyperparameters (width and depth) directly and fully differentiably. The operator employs a learnable threshold parameter to select elements whose importance scores exceed this threshold, with importance scores normalized to ensure even distribution. A soft mask generated via sigmoid approximation enables gradient flow during optimization. The method optimizes structural parameters based on both task loss and resource constraint loss, with a logarithmic penalty applied when resource consumption exceeds targets. The search space encompasses width and depth scaling while maintaining generalizability across different network architectures.

## Key Results
- Improved top-1 accuracy on ImageNet: +1.4% for EfficientNet-B0 and +0.6% for DeiT-Tiny
- Outperformed state-of-the-art zero-shot NAS method ZiCo by 1.3% while requiring only 0.4 GPU days for searching
- Improved mAP by 2.0% for YOLOv8-n on COCO object detection
- Pruned Llama-7B outperformed prior methods with lower perplexity and higher zero-shot classification accuracy

## Why This Works (Mechanism)

### Mechanism 1
The differentiable top-k operator models structural hyperparameters directly and fully differentiably by using a learnable threshold parameter to select elements based on normalized importance scores. This avoids non-differentiability issues of prior methods. The core assumption is that normalizing element importance to an even distribution across [0,1] ensures smooth and differentiable top-k selection. Evidence shows DMS can model width and depth directly, with normalization converting uneven distributions to even values.

### Mechanism 2
The soft mask generated by the differentiable top-k operator effectively approximates hard mask selection during forward pass using a sigmoid function parameterized by λ. When λ is set to N (number of elements), the approximation error for all but the six elements near the threshold is less than 0.05, providing sufficient accuracy for the soft mask to simulate hard selection.

### Mechanism 3
The combination of task loss and resource constraint loss enables efficient optimization of structural hyperparameters through weighted sum optimization. When current resource consumption exceeds target, a logarithmic penalty guides structural parameters to meet resource targets without degrading task performance. The differentiable nature of the top-k operator allows gradients to flow through both components.

## Foundational Learning

- **Neural Architecture Search (NAS) methods**: Understanding NAS landscape is crucial for contextualizing DMS's contributions. Quick check: What are the two main categories of NAS methods based on search strategy, and what are their key differences?
- **Differentiable programming and gradient-based optimization**: DMS relies on differentiable operators and gradient descent for efficient search. Quick check: Why is differentiability important for efficient optimization in NAS, and how does DMS achieve it?
- **Model scaling and structural hyperparameters**: DMS focuses on scaling networks by searching optimal width and depth. Quick check: What are width and depth in neural networks, and why are they critical structural hyperparameters for model scaling?

## Architecture Onboarding

- **Component map**: Differentiable Top-k Operator -> Importance Normalization -> Soft Mask Generation -> Resource Constraint Loss -> DMS Pipeline
- **Critical path**: 
  1. Initialize supernet with random weights
  2. Compute element importance using Taylor method
  3. Normalize importance scores
  4. Generate soft masks using differentiable top-k
  5. Compute task loss and resource constraint loss
  6. Update structural parameters using gradients
  7. Prune model based on final masks
  8. Retrain pruned model
- **Design tradeoffs**: 
  - Search space granularity vs. search efficiency: Finer-grained search spaces provide more options but require more computation
  - Importance normalization vs. raw importance: Normalization ensures differentiability but may lose some information
  - Soft mask approximation vs. hard selection: Soft masks enable differentiability but may introduce approximation errors
- **Failure signatures**: 
  - Model fails to meet resource constraints: λresource may be too small or importance normalization may not be working
  - Performance degrades significantly: Soft mask approximation may be insufficient or search space may be too restrictive
  - Training instability: Importance normalization may not be effective or λ may be incorrectly set
- **First 3 experiments**:
  1. Implement differentiable top-k operator and verify it can select correct number of elements
  2. Test importance normalization on a simple network and verify even distribution
  3. Run DMS on a small CNN and compare performance with baseline model

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of DMS compare when searching for width and depth separately versus jointly? The paper states "The depth and width structure hyperparameters are trained jointly in our approach" but doesn't provide comparisons to separate searches. Evidence would require experiments comparing joint versus separate optimization of width and depth.

### Open Question 2
How does DMS perform when applied to larger language models beyond Llama-7B? The paper only demonstrates DMS on Llama-7B and mentions resource constraints prevented testing larger models. Evidence would require results of applying DMS to search architectures for larger language models.

### Open Question 3
How sensitive is DMS to the initial supernet architecture? The paper mentions searching on different supernet sizes but doesn't systematically study the impact of starting architecture. Evidence would require experiments comparing DMS performance when starting from different baseline architectures.

### Open Question 4
Can DMS effectively handle more complex structural hyperparameters beyond width and depth? The paper deliberately limits its search space to width and depth for generalizability but doesn't demonstrate whether DMS can handle additional hyperparameters. Evidence would require results showing DMS successfully searching for and optimizing additional architectural parameters.

## Limitations
- Implementation details for the differentiable top-k operator and importance normalization are not fully specified
- Resource constraint loss sensitivity to λresource hyperparameter is not thoroughly explored
- Effectiveness of importance normalization and its impact on differentiability is asserted but not thoroughly validated

## Confidence

**High Confidence**: Overall framework and methodology are well-established. Reported performance improvements align with expectations for effective NAS methods.

**Medium Confidence**: Theoretical justification for differentiable top-k operator is sound, but practical implementation details for faithful reproduction are not fully specified.

**Low Confidence**: Effectiveness of importance normalization process and its impact on differentiability is asserted but not thoroughly validated.

## Next Checks

1. Implement and validate the differentiable top-k operator by creating a minimal implementation and verifying it can select exactly k elements while maintaining differentiability. Test approximation accuracy for different values of λ.

2. Analyze importance score distributions by running the importance normalization process on several networks and empirically verifying even distributions across [0,1]. Measure impact on smoothness of selection function and gradient magnitude.

3. Conduct hyperparameter sensitivity analysis by systematically varying λresource and λ to understand their impact on resource constraint satisfaction and task performance. Identify ranges where the method is robust and where it fails.