---
ver: rpa2
title: 'Two Sparse Matrices are Better than One: Sparsifying Neural Networks with
  Double Sparse Factorization'
arxiv_id: '2409.18850'
source_url: https://arxiv.org/abs/2409.18850
tags:
- sparse
- pruning
- matrix
- matrices
- admm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes Double Sparse Factorization (DSF) to improve
  neural network compression by replacing each dense weight matrix with the product
  of two sparse matrices. DSF uses an alternating minimization algorithm with ADMM
  to find these sparse factors, achieving state-of-the-art layer-wise pruning results.
---

# Two Sparse Matrices are Better than One: Sparsifying Neural Networks with Double Sparse Factorization

## Quick Facts
- arXiv ID: 2409.18850
- Source URL: https://arxiv.org/abs/2409.18850
- Reference count: 35
- Primary result: DSF achieves better layer-wise pruning than magnitude pruning, reducing LLaMA2-13B by 50% while outperforming dense LLaMA2-7B in perplexity

## Executive Summary
This paper introduces Double Sparse Factorization (DSF), a method for compressing neural networks by replacing each dense weight matrix with the product of two sparse matrices. DSF uses an alternating minimization algorithm with ADMM to find these sparse factors, achieving state-of-the-art layer-wise pruning results. The approach is particularly effective for large language models and convolutional networks, maintaining accuracy advantages even after fine-tuning.

## Method Summary
DSF factorizes each dense weight matrix into two sparse matrices (W ≈ AB) using alternating minimization with ADMM subproblems. The algorithm iteratively optimizes one sparse factor while fixing the other, using warm-starting with dual variables to improve convergence. During layer-wise pruning, the second sparse factor is fixed to match the dense matrix, and only the first factor is optimized. The method requires approximately 10-20% more computation than single sparse matrix multiplication but achieves better reconstruction quality with the same number of non-zeros.

## Key Results
- DSF reduces LLaMA2-13B by 50% while maintaining better perplexity than dense LLaMA2-7B
- Outperforms magnitude pruning and Optimal Brain Compression for convolutional networks
- Accuracy improvements persist after fine-tuning, with higher starting accuracy and better retention
- Achieves better reconstruction error than single sparse pruning with equivalent non-zero count

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DSF provides better approximation of dense weight matrices compared to standard sparse pruning with the same number of non-zeros
- Mechanism: Factorizing into two sparse matrices allows covering more effective connections through the product AB, representing structural complexity that a single sparse matrix cannot
- Core assumption: The product of two appropriately sparse matrices can approximate the original dense matrix more accurately than one sparse matrix with the same total non-zeros
- Break condition: If alternating minimization gets stuck in poor local optima and cannot escape despite annealing attempts

### Mechanism 2
- Claim: DSF maintains accuracy advantages even after further fine-tuning of pruned models
- Mechanism: The double sparse representation captures more essential structure of the original dense matrix, and this structure is preserved and enhanced during fine-tuning rather than degraded
- Core assumption: The factorization structure discovered during pruning encodes important model characteristics beneficial during subsequent fine-tuning
- Break condition: If fine-tuning causes sparse factors to drift significantly from their optimized state, potentially losing the initial advantage

### Mechanism 3
- Claim: The ADMM-based alternating minimization algorithm effectively solves the non-convex factorization problem
- Mechanism: Iteratively optimizing one sparse factor while fixing the other, with warm-starting using dual variables, converges to a good local minimum despite NP-hardness
- Core assumption: The alternating minimization approach with ADMM subproblems provides a practical heuristic that finds sufficiently good solutions for the NP-hard factorization problem
- Break condition: If problem structure changes significantly (e.g., extremely rectangular matrices) where alternating approach becomes ineffective

## Foundational Learning

- Concept: Alternating Direction Method of Multipliers (ADMM)
  - Why needed here: ADMM solves sparse regression subproblems in the alternating minimization algorithm for finding two sparse factors
  - Quick check question: What are the three main steps in each ADMM iteration for solving constrained optimization problems?

- Concept: L0-constrained optimization
  - Why needed here: Core problem involves finding sparse matrices (minimizing ||A||0 + ||B||0), an L0-constrained optimization that's NP-hard
  - Quick check question: Why is L0-constrained optimization NP-hard, and how do heuristic methods like DSF address this?

- Concept: Layer-wise pruning vs global pruning
  - Why needed here: DSF is primarily evaluated in layer-wise pruning where each layer is pruned independently based on calibration inputs
  - Quick check question: What's the key difference between layer-wise and global pruning approaches in terms of computational complexity and solution quality?

## Architecture Onboarding

- Component map: Forward pass → ADMM iteration → mask update → weight update → convergence check
- Critical path: Forward pass → ADMM iteration → mask update → weight update → convergence check
- Design tradeoffs:
  - Storage overhead: 2x mask storage vs 1x, but values dominate memory usage
  - Inference speed: 10-20% slower than single sparse matrix multiplication
  - Solution quality: Better reconstruction vs standard pruning with same non-zero count
  - Complexity: NP-hard problem solved via heuristic instead of exact solution
- Failure signatures:
  - Poor convergence: ADMM iterations fail to reduce reconstruction error
  - Numerical instability: Matrix inversions become unstable in finalization step
  - Memory bottlenecks: Intermediate activations exceed GPU memory during fine-tuning
  - Mask collapse: Sparsity masks become too restrictive, losing model capacity
- First 3 experiments:
  1. Compare reconstruction error of DSF vs magnitude pruning on a small matrix (64x64) with 25% density
  2. Measure inference time overhead of DSF vs single sparse matrix multiplication on a CPU benchmark
  3. Test layer-wise pruning on a small CNN (e.g., ResNet-18) and compare accuracy vs standard pruning at 50% density

## Open Questions the Paper Calls Out

- Question: How does DSF perform on extremely high sparsity levels (e.g., 90%+) compared to lower sparsity levels where it currently excels?
  - Basis in paper: The paper shows DSF is superior at moderate sparsities (40-60%) but doesn't explore extreme sparsities where storage savings are greatest
  - Why unresolved: Experiments focused on 30-60% density ranges, leaving performance characteristics at very high sparsities unexplored
  - What evidence would resolve it: Empirical results comparing DSF to other methods at 80-95% sparsity levels across various model architectures

- Question: What is the optimal strategy for allocating nonzeros between the two sparse factors in DSF?
  - Basis in paper: "These allocations were determined manually in our experiments" and "we found that it is beneficial to give one of the matrices approximately 1/3 of the nonzeros and 2/3 to the other one."
  - Why unresolved: The paper uses heuristic allocation without systematic exploration of optimal distribution strategies across different matrix shapes and model types
  - What evidence would resolve it: Systematic study varying the ratio of nonzeros between factors across diverse matrix dimensions and model architectures, measuring reconstruction error and downstream task performance

- Question: How does DSF interact with and benefit from gradual pruning schedules during fine-tuning?
  - Basis in paper: "it is unclear how to integrate DSF with gradual pruning with fine-tuning the whole network between pruning steps."
  - Why unresolved: The paper only evaluates one-shot pruning scenarios and notes this integration challenge without exploring solutions
  - What evidence would resolve it: Experiments implementing gradual pruning with DSF where the model is fine-tuned between pruning steps, comparing to standard gradual pruning and one-shot DSF performance

## Limitations
- NP-hard optimization problem requires heuristic approaches that may not find optimal solutions
- 10-20% inference overhead compared to single sparse matrices may limit adoption in latency-sensitive applications
- Evaluation focuses primarily on layer-wise pruning, with global pruning performance unexplored

## Confidence
- High confidence in core mechanism: Mathematical foundation is sound, empirical results demonstrate consistent improvements across multiple model architectures and tasks
- Medium confidence in generalization claims: Results show advantages over specific baselines but don't extensively compare against full range of modern pruning techniques
- Medium confidence in fine-tuning persistence claims: Shows accuracy improvements persist after fine-tuning but analysis could be more comprehensive regarding different fine-tuning durations and learning rates

## Next Checks
1. **Convergence Analysis**: Systematically vary the number of ADMM iterations and annealing schedules to determine sensitivity of reconstruction quality to these hyperparameters
2. **Global vs Layer-wise Comparison**: Implement global pruning with DSF and compare against layer-wise approach to understand tradeoffs between these strategies
3. **Cross-Architecture Generalization**: Apply DSF to additional model architectures (transformers, MLPs, vision transformers) and tasks (NLP, vision, multimodal) to validate method's broad applicability