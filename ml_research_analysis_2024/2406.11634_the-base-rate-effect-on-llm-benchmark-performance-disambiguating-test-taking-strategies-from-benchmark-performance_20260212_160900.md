---
ver: rpa2
title: 'The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking
  Strategies from Benchmark Performance'
arxiv_id: '2406.11634'
source_url: https://arxiv.org/abs/2406.11634
tags:
- answer
- mmlu
- choice
- arxiv
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of base-rate probability (BRP)
  biases in large language model (LLM) benchmarking, specifically in the MMLU dataset.
  It demonstrates that BRP differences across answer tokens significantly affect task
  performance, leading to the conflation of task performance and test-taking ability.
---

# The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance

## Quick Facts
- arXiv ID: 2406.11634
- Source URL: https://arxiv.org/abs/2406.11634
- Authors: Kyle Moore; Jesse Roberts; Thao Pham; Oseremhen Ewaleifoh; Doug Fisher
- Reference count: 4
- Primary result: Nvr-X-MMLU successfully disambiguates test-taking ability from task performance, with LLaMa 3 being the only model to rise above random guessing on CF prompted Nvr-X-MMLU

## Executive Summary
This paper investigates base-rate probability (BRP) biases in LLM benchmarking, demonstrating that BRP differences across answer tokens significantly affect task performance on the MMLU dataset. The authors show that models often conflate test-taking strategy (favoring high BRP tokens) with actual task understanding. To address this, they introduce Nvr-X-MMLU, a novel variation of MMLU that remaps answer choices to disambiguate test-taking ability from task performance. The performance is measured as the minimum accuracy over four Nvr-X variations, providing a more accurate assessment of model task understanding.

## Method Summary
The method involves measuring BRP by generating control prompts in cloze format for each model and calculating average probabilities for each answer choice. The MMLU dataset is split by correct answer choice to measure accuracy for each subset, and Pearson's r correlation is calculated between accuracy and BRP. Counterfactual prompting is implemented by moving the target completion into the context and using a shared canary token. Nvr-X-MMLU is created by remapping answer choices to ensure the correct answer is never assigned to label X, and performance is measured as the minimum accuracy across all four variations.

## Key Results
- Strong correlation between BRP and accuracy in cloze tasks for several models, with GPT-2 showing near-zero accuracy when A is not correct
- Counterfactual prompting mitigates but does not eliminate the BRP effect
- Nvr-X-MMLU successfully disambiguates task performance from test-taking ability, with LLaMa 3 being the only model to rise above random guessing on CF prompted Nvr-X-MMLU

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Base-rate probability (BRP) differences across answer tokens directly affect cloze test accuracy in LLMs.
- Mechanism: Models learn from training data that certain tokens (e.g., "A") appear more frequently as answer choices. This creates an intrinsic bias where the model favors high BRP tokens regardless of semantic context, conflating test-taking strategy with actual task understanding.
- Core assumption: The frequency distribution of answer tokens in training data is reflected in the model's probability estimates for those tokens.
- Evidence anchors:
  - [abstract] "Using the MMLU dataset, we show that the base-rate probability (BRP) differences across answer tokens are significant and affect task performance"
  - [section] "Using the control prompts with the cloze test pattern, we find that all models tested show a strong intrinsic bias for answer choice A over all other choice labels, regardless of position"
  - [corpus] Weak evidence for this specific mechanism in related papers, though similar base-rate effects are discussed
- Break condition: When the model has sufficient context to override BRP-based preferences, or when answer tokens have uniform BRP distributions

### Mechanism 2
- Claim: Counterfactual (CF) prompting reduces but does not eliminate BRP effects.
- Mechanism: CF prompting moves the target completion into the context and uses a shared canary token, equalizing BRP across answer choices. However, the answer choice tokens themselves still influence probability estimates, creating residual BRP effects.
- Core assumption: The semantic context of answer choices affects token probability estimates even when the target token is standardized.
- Evidence anchors:
  - [abstract] "We find that counterfactual prompting does sufficiently mitigate the BRP effect"
  - [section] "When using CF prompts, we see much weaker BRP correlation with accuracy for all models except LLaMa 3"
  - [corpus] Limited evidence in corpus; related work discusses CF prompting but not specifically for BRP mitigation
- Break condition: When the model completely ignores contextual information from answer choices, or when BRP differences are negligible

### Mechanism 3
- Claim: Nvr-X-MMLU successfully disambiguates test-taking ability from task performance.
- Mechanism: By remapping answer choice content such that the correct answer is never assigned to label X, Nvr-X-MMLU forces models to rely on semantic understanding rather than BRP-based guessing strategies. Performance is measured as the minimum accuracy across all four variations.
- Core assumption: Models that understand the task will achieve consistent accuracy across all Nvr-X variations, while BRP-driven models will show significant variation.
- Evidence anchors:
  - [abstract] "We propose the Nvr-X-MMLU task, a variation of MMLU, which helps to disambiguate test-taking ability from task performance and reports the latter"
  - [section] "The performance of the model is measured as the minimum accuracy over the four Nvr-X variation sets"
  - [corpus] No direct evidence in corpus for this specific mechanism
- Break condition: When models develop alternative heuristics that work across all Nvr-X variations, or when answer content distribution correlates with labels

## Foundational Learning

- Concept: Base-rate probability and its effect on model predictions
  - Why needed here: Understanding BRP is crucial for recognizing why models may favor certain answer choices regardless of semantic context
  - Quick check question: If token "A" appears 30% of the time as an answer choice in training data, what BRP would you expect for this token?

- Concept: Counterfactual prompting methodology
  - Why needed here: CF prompting is used to mitigate BRP effects, so understanding its mechanism is essential for interpreting results
  - Quick check question: How does counterfactual prompting differ from standard cloze prompting in terms of what token probability is being measured?

- Concept: Statistical significance and correlation
  - Why needed here: The paper uses Pearson's r correlation to measure the relationship between BRP and accuracy, requiring understanding of statistical concepts
  - Quick check question: What does a Pearson correlation coefficient of 0.95 between BRP and accuracy suggest about their relationship?

## Architecture Onboarding

- Component map: MMLU dataset -> BRP measurement module -> Prompt generation -> Model inference -> Result aggregation -> Statistical analysis
- Critical path: Dataset → BRP measurement → Prompt generation → Model inference → Result aggregation → Statistical analysis
- Design tradeoffs: Using CF prompting increases computational cost (4x more inferences) but provides cleaner measurement of task understanding versus BRP effects.
- Failure signatures: (1) High correlation between BRP and accuracy suggests BRP effects dominate, (2) Inconsistent performance across Nvr-X variations indicates label bias, (3) Poor performance on CF variations suggests understanding is brittle.
- First 3 experiments:
  1. Measure BRP distribution across answer tokens using control prompts with cloze format
  2. Split MMLU by correct answer label and measure accuracy for each subset to quantify BRP effects
  3. Implement and evaluate CF prompting to assess its impact on BRP mitigation

## Open Questions the Paper Calls Out

- Question: How do other known test-taking heuristics (e.g., answer length, sequential runs of the same answer, numeric outliers) affect LLM behavior on benchmark tasks?
  - Basis in paper: [explicit] The paper explicitly states this as a limitation, noting that it only investigated simple heuristics mediated by BRP and that other heuristics should be explored in future work.
  - Why unresolved: The authors did not explore these additional heuristics, leaving their potential impact on LLM performance unknown.
  - What evidence would resolve it: Experimental results showing the prevalence and strength of these heuristics in LLM behavior, either supporting or refuting their significant impact on benchmark performance.

- Question: Do the identified BRP effects and test-taking strategies persist in larger language models (beyond 10B parameters) and when using 5-shot in-context learning?
  - Basis in paper: [explicit] The paper explicitly states this as a limitation, noting that resource constraints prevented testing with larger models or 5-shot ICL.
  - Why unresolved: The experiments were limited to models up to 10B parameters and did not use 5-shot ICL, so the generalizability of the findings to larger models and different prompting strategies is unknown.
  - What evidence would resolve it: Experimental results comparing BRP effects and test-taking strategies in larger models and with 5-shot ICL to the findings presented in the paper.

- Question: How do different interaction patterns (beyond cloze and counterfactual prompting) affect LLM performance on benchmark tasks?
  - Basis in paper: [inferred] The paper mentions that users interact with LLMs using a variety of patterns and that the discrepancy between cloze and CF results suggests model understanding can be brittle, implying that other interaction patterns might also impact performance.
  - Why unresolved: The paper only tested two specific interaction patterns (cloze and CF prompting), leaving the impact of other potential interaction patterns unknown.
  - What evidence would resolve it: Experimental results showing the performance of LLMs on benchmark tasks using various interaction patterns, including those not tested in the paper, to determine their impact on BRP effects and test-taking strategies.

## Limitations
- Data Dependency: The study's findings are heavily dependent on the MMLU dataset's structure and answer token distribution.
- Model Coverage: While multiple model families are tested, the study doesn't include other major model families like Claude, Gemini, or open-source alternatives.
- Prompt Sensitivity: The effectiveness of counterfactual prompting in mitigating BRP effects is shown but not thoroughly explored.

## Confidence
- High Confidence: The existence of BRP effects on cloze-style tasks and their impact on model accuracy.
- Medium Confidence: The effectiveness of Nvr-X-MMLU in disambiguating test-taking ability from task performance.
- Low Confidence: The claim that LLaMa 3 is the only model to rise above random guessing on CF prompted Nvr-X-MMLU.

## Next Checks
1. **Cross-Benchmark Validation**: Test the BRP hypothesis and Nvr-X-MMLU methodology on at least two additional LLM benchmarks (e.g., BigBench, HumanEval) to assess generalizability of findings across different task formats and answer distributions.

2. **Prompt Engineering Sensitivity**: Systematically vary prompt templates for counterfactual prompting and measure how this affects BRP mitigation. Test at least 5 different prompt formulations to establish robustness of the CF approach.

3. **Model Architecture Analysis**: Compare BRP effects across different model architectures (transformer variants, different attention mechanisms) to determine whether the observed effects are universal or architecture-specific. Include at least 3 additional model families not tested in the current study.