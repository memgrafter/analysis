---
ver: rpa2
title: Preference Alignment with Flow Matching
arxiv_id: '2405.19806'
source_url: https://arxiv.org/abs/2405.19806
tags:
- preference
- distribution
- reward
- flow
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Preference Flow Matching (PFM) is a framework for integrating human
  preferences into pre-trained models without requiring extensive fine-tuning. Instead
  of learning reward functions or directly optimizing policies, PFM learns a flow
  field that transforms less preferred outputs into more preferred ones using flow
  matching techniques.
---

# Preference Alignment with Flow Matching

## Quick Facts
- arXiv ID: 2405.19806
- Source URL: https://arxiv.org/abs/2405.19806
- Reference count: 35
- Preference Flow Matching (PFM) achieves comparable or superior performance to RLHF and DPO while requiring significantly fewer trainable parameters (1.2% in text experiments).

## Executive Summary
Preference Flow Matching (PFM) introduces a novel framework for aligning pre-trained models with human preferences without extensive fine-tuning. The method uses flow matching techniques to learn a vector field that transforms less preferred outputs into more preferred ones, eliminating the need for reward model estimation. By directly learning the transformation between marginal distributions of preference data, PFM avoids the overfitting issues common in traditional RLHF methods while remaining compatible with black-box APIs like GPT-4. The approach demonstrates strong performance across conditional image generation, text generation, and offline reinforcement learning tasks.

## Method Summary
PFM learns a flow field that transforms samples from a less preferred marginal distribution p0(y|x) to a more preferred marginal distribution p1(y|x) using flow matching techniques. Instead of optimizing for reward functions as in RLHF, PFM directly learns a vector field vθ(t,y|x) that maps between these distributions. The method collects preference pairs from a pre-trained reference policy, trains the flow matching model to learn the transformation, and applies the learned flow via an ODE solver during inference. PFM also supports iterative improvement through repeated application of learned flows. The approach eliminates reward model overfitting issues while requiring significantly fewer trainable parameters than traditional methods.

## Key Results
- PFM achieves comparable or superior performance to RLHF and DPO across multiple domains while using only 1.2% of trainable parameters in text experiments
- The method successfully handles conditional image generation (MNIST), text generation (IMDB sentiment), and offline RL (D4RL MuJoCo) tasks
- Iterative flow matching further improves alignment by progressively concentrating probability mass on the most preferred outputs
- PFM demonstrates robustness to reward overfitting issues common in traditional RLHF approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PFM learns a flow field that transforms less preferred outputs into more preferred ones without requiring reward model estimation.
- Mechanism: Instead of optimizing for reward functions as in RLHF, PFM directly learns a vector field vθ(t,y|x) that maps samples from the less preferred marginal distribution p0(y|x) to the more preferred marginal distribution p1(y|x) using flow matching techniques.
- Core assumption: The preference data collected from the reference policy can be characterized as samples from the marginal distributions p0 and p1, where p0 represents less preferred outputs and p1 represents more preferred outputs.
- Evidence anchors:
  - [abstract] "PFM utilizes flow matching techniques to directly learn from preference data, thereby reducing the dependency on extensive fine-tuning of pre-trained models."
  - [section 2.2] "The flow matching considers the task of fitting a mapping f : Rd → Rd that transforms p0 to p1, that is, if y− ∼ p0, then f(y−) ∼ p1."
  - [corpus] Weak - related works focus on preference-based RL but don't discuss flow matching specifically
- Break condition: If the preference data cannot be characterized as samples from well-defined marginal distributions p0 and p1, the flow matching approach would not be applicable.

### Mechanism 2
- Claim: PFM avoids reward model overfitting by not requiring explicit or implicit reward function estimation.
- Mechanism: By directly learning the flow between marginal distributions instead of first learning a reward model and then optimizing for it, PFM circumvents the overfitting issues that plague traditional RLHF methods where reward models can overfit in the finite data regime.
- Core assumption: The overfitting problem in RLHF is primarily caused by the need to estimate reward functions from limited preference data, which PFM eliminates by using a different optimization framework.
- Evidence anchors:
  - [section 2.1] "In typical RLHF scenarios, a model is initially trained to approximate a reward function based on human preferences... However, this approach can introduce complexities and potential biases in translating human preferences into numerical rewards."
  - [section 3.2] "Unlike other RLHF methods, our model robustly learns to align with the preference from the provided dataset... our method does not try to overfit beyond the unseen region."
  - [corpus] Weak - related works mention robustness but don't specifically address reward overfitting
- Break condition: If the flow matching model itself overfits to the preference data, PFM would still face overfitting issues despite avoiding reward model estimation.

### Mechanism 3
- Claim: Iterative flow matching can further improve alignment by progressively narrowing the distribution toward higher preference outputs.
- Mechanism: By repeatedly applying flow matching to the marginal distribution obtained from the previous iteration, the method progressively concentrates probability mass on the most preferred outputs, as formalized in Theorem 4.2.
- Core assumption: Each iteration of flow matching effectively shifts the source distribution toward regions of higher preference probability, and this process converges to a distribution concentrated on the optimal outputs.
- Evidence anchors:
  - [section 3.3] "We can also further improve the quality of alignment with iterative flow matching... we again collect a new preference data y−, y+ from the obtained marginal distribution p1"
  - [section 4] "the iteration converges to the uniform distribution of points y where the value Ey−∼πref (P(y > y −)) is the largest"
  - [corpus] Weak - related works don't discuss iterative flow matching approaches
- Break condition: If the iterative process fails to converge or if early iterations produce poor quality flows, the iterative approach would not provide meaningful improvements.

## Foundational Learning

- Concept: Flow matching for generative modeling
  - Why needed here: PFM builds directly on flow matching techniques to learn the transformation between less preferred and more preferred outputs
  - Quick check question: What is the key difference between flow matching and diffusion models in terms of their starting distributions?

- Concept: Preference-based reinforcement learning (PbRL) objectives
  - Why needed here: Understanding the standard PbRL framework helps explain why PFM's approach differs and why it might be more robust
  - Quick check question: What is the main optimization objective in standard RLHF and how does it differ from PFM's objective?

- Concept: Marginal distributions and preference modeling
  - Why needed here: PFM operates by characterizing the preference data as samples from marginal distributions, which is central to understanding how the method works
  - Quick check question: How are the marginal distributions p0(y|x) and p1(y|x) defined in terms of the reference policy and preference probabilities?

## Architecture Onboarding

- Component map:
  Reference model πref -> Preference dataset D -> Flow matching model vθ(t,y|x) -> ODE solver -> Improved samples

- Critical path:
  1. Collect preference data from reference model
  2. Train flow matching model to learn vθ(t,y|x)
  3. During inference, sample from reference model and apply learned flow via ODE solver
  4. (Optional) Iterate steps 1-3 with improved samples

- Design tradeoffs:
  - Using reference policy vs. true marginal p0 as source during inference: Reference policy is more practical but may introduce distribution shift
  - Fixed vs. variable length inputs: Fixed length requires autoencoding which adds complexity but enables flow matching
  - Iterative vs. single application: Iterative can improve results but requires more computation and potential data collection

- Failure signatures:
  - Poor alignment with preferences: Likely indicates the flow matching model failed to learn meaningful transformations
  - Reward overfitting in baseline methods but not in PFM: Expected behavior confirming PFM's robustness advantage
  - Distribution shift between source and target: May indicate need for better source distribution or iterative refinement

- First 3 experiments:
  1. MNIST conditional generation: Test basic flow matching capability on image data with clear preference structure
  2. IMDB sentiment generation: Evaluate PFM on text data with variable length inputs using autoencoding
  3. D4RL MuJoCo tasks: Assess PFM's performance on continuous control tasks with trajectory-based preferences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PFM change when applied to more complex NLP tasks involving long-form text beyond the fixed-size embedding approach used in the IMDB sentiment analysis experiments?
- Basis in paper: [explicit] The authors note that PFM currently relies on an autoencoder to handle variable-length texts and that "future research should explore ways to adapt the PFM framework to long-form texts"
- Why unresolved: The current implementation uses T5-based autoencoders for fixed-size embeddings, which may not scale well to longer documents or more complex language understanding tasks
- What evidence would resolve it: Experiments demonstrating PFM performance on long-form text tasks (like document summarization, story generation, or dialogue systems) compared to baseline methods would clarify its effectiveness in real-world NLP applications

### Open Question 2
- Question: Under what conditions does using the true marginal distribution p0 as the source distribution outperform using the reference policy πref in the flow matching framework?
- Basis in paper: [explicit] The authors show in Appendix B that using p0 as source distribution achieves the highest performance in walker2d experiments, but note this requires access to the preference model during inference
- Why unresolved: While theoretical analysis suggests p0 is optimal, the practical requirement of knowing the preference model during inference limits real-world applicability
- What evidence would resolve it: Comparative experiments across multiple domains showing when the performance gap between p0 and πref sources is significant enough to justify the additional computational cost of approximating p0

### Open Question 3
- Question: What are the theoretical guarantees for PFM performance when the preference model P(y > y′|x) violates the Bradley-Terry assumption or exhibits non-transitive preferences?
- Basis in paper: [inferred] The authors emphasize that PFM doesn't require Bradley-Terry assumptions unlike DPO and RLHF, but their theoretical analysis focuses on cases where preferences are well-behaved
- Why unresolved: Real-world preference data often exhibits inconsistencies, noise, and non-transitivity that could affect the flow matching approach differently than reward-based methods
- What evidence would resolve it: Theoretical analysis of PFM convergence properties under various preference model assumptions, combined with empirical experiments using synthetic preference data with controlled noise levels and inconsistencies

## Limitations

- The method's performance heavily depends on the quality and representativeness of the collected preference pairs, which may not capture complex preference structures in real-world scenarios
- While the paper claims reduced overfitting compared to RLHF, the theoretical analysis doesn't fully account for potential overfitting in the flow matching model itself
- The iterative approach lacks comprehensive analysis of convergence properties and may introduce instability in later iterations

## Confidence

- **High Confidence**: The core mechanism of using flow matching to transform between preference distributions is well-supported by theoretical analysis and experimental results. The MNIST image generation experiments provide particularly strong evidence for the basic approach.
- **Medium Confidence**: Claims about robustness to reward overfitting are plausible given the different optimization framework, but the evidence is primarily comparative rather than through direct measurement of overfitting metrics. The text generation experiments show promising results but rely on automated evaluation which may not capture true preference alignment.
- **Low Confidence**: The theoretical convergence properties of iterative PFM are only partially proven, and the paper lacks extensive ablation studies on critical hyperparameters and architectural choices that could affect real-world performance.

## Next Checks

1. **Overfitting Analysis**: Conduct controlled experiments comparing reward model overfitting in RLHF versus flow matching model overfitting in PFM using the same preference datasets, measuring generalization to held-out preference pairs.

2. **Distribution Shift Evaluation**: Systematically measure the distribution shift between the reference policy and the improved policy in PFM, comparing different sampling strategies and their impact on preference alignment quality.

3. **Iterative Stability Testing**: Perform extensive testing of the iterative PFM approach across all three domains, measuring convergence properties, preference score saturation points, and diversity maintenance across iterations.