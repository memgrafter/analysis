---
ver: rpa2
title: 'Sparse Upcycling: Inference Inefficient Finetuning'
arxiv_id: '2411.08968'
source_url: https://arxiv.org/abs/2411.08968
tags:
- upcycling
- inference
- arxiv
- dense
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates sparse upcycling\u2014converting a dense\
  \ model into a Mixture-of-Experts (MoE) architecture\u2014as an alternative to continued\
  \ pretraining for improving small LLM quality. The authors compare upcycling to\
  \ dense CPT across different model sizes (436M and 1.4B), compute budgets, and training\
  \ durations."
---

# Sparse Upcycling: Inference Inefficient Finetuning

## Quick Facts
- arXiv ID: 2411.08968
- Source URL: https://arxiv.org/abs/2411.08968
- Reference count: 24
- Sparse upcycling achieves better downstream quality than continued pretraining but with 34-44% inference slowdown

## Executive Summary
This paper evaluates sparse upcycling—converting a dense model into a Mixture-of-Experts (MoE) architecture—as an alternative to continued pretraining for improving small LLM quality. The authors compare upcycling to dense CPT across different model sizes (436M and 1.4B), compute budgets, and training durations. Upcycling achieves better downstream quality, with improvements over 20% relative to CPT on the Gauntlet Core Average benchmark in some scenarios. However, upcycled models are significantly slower at inference: in high-demand settings, max throughput decreases by 34-44% for larger models, even when using top-K=1 to match dense compute. The study shows upcycling requires substantial training FLOPs to offset inference costs, presenting a trade-off between model quality and inference efficiency for practitioners.

## Method Summary
The authors convert dense language models (436M and 1.4B parameters) into MoE architectures by duplicating their MLP layers into 8 experts each. They train these upcycled models using top-K=2 learned dropless routing with load balancing on higher-quality domain-specific data. The upcycling approach is compared against dense continued pretraining (CPT) on the same data using identical optimizer settings (Lion with cosine annealing and warmup). Quality is measured using the Gauntlet Core Average benchmark, while inference performance is benchmarked using vLLM on H100 GPUs with both top-K=1 and top-K=2 settings.

## Key Results
- Upcycling achieves up to 20% relative improvement over CPT on Gauntlet Core Average benchmark
- Inference throughput decreases by 34-44% for upcycled models even with top-K=1 matching dense compute
- Quality improvements require substantial training FLOPs, with diminishing returns beyond 40% of original pretraining budget

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse upcycling increases model quality by expanding parameter count without proportionally scaling active computation during training.
- Mechanism: The upcycling process duplicates the dense model's MLP layers into multiple expert sublayers. During training, only a subset of these experts is activated per token via a router, allowing the model to effectively have more parameters while maintaining similar training FLOPs as the original dense model.
- Core assumption: The router can effectively distribute tokens to different experts such that the additional parameters provide meaningful capacity for learning.
- Evidence anchors:
  - [abstract] "MoE architectures dynamically activate only a subset of weights (experts), allowing the model to expand its parameter count without proportionally scaling up training and inference FLOPs"
  - [section 2] "The typical MoE language model architecture modifies the transformer block: the attention weights stay the same, but rather than having a single multi-layer perceptron (MLP) process tokens, a routing function is introduced which dynamically routes tokens to a subset of different MLPs."
- Break condition: If the router fails to effectively distribute tokens across experts (e.g., due to poor initialization or training instability), the additional parameters may not contribute to quality improvement.

### Mechanism 2
- Claim: Continued pretraining after upcycling leads to better quality improvements than dense continued pretraining alone.
- Mechanism: The additional parameters introduced during upcycling provide increased capacity for learning domain-specific patterns from the continued pretraining data. The model can allocate different experts to specialize in different aspects of the new domain.
- Core assumption: The domain-specific data contains patterns that require more capacity than the original dense model had, and the additional experts can learn to capture these patterns.
- Evidence anchors:
  - [section 3.2] "In general, upcycling tends to achieve lower loss compared to the corresponding CPT run, showing that upcycling is indeed able to produce better quality models."
  - [section 3.2] "upcycling is able to achieve lower loss and improve downstream quality, albeit it requires a significant portion of the original compute budget."
- Break condition: If the domain data is not sufficiently complex or diverse, or if the continued pretraining duration is too short, the quality gains over dense CPT may not materialize.

### Mechanism 3
- Claim: The quality improvement from upcycling requires substantial training FLOPs to offset the inference overhead.
- Mechanism: While upcycling increases parameter count and quality, it also increases the total number of parameters that must be loaded and processed during inference. To achieve better quality than dense CPT, the upcycling process must effectively utilize the additional parameters through sufficient training, which requires significant computational resources.
- Core assumption: The relationship between parameter count, training duration, and quality improvement is such that the additional parameters require proportionally more training to be effectively utilized.
- Evidence anchors:
  - [abstract] "However, upcycled models are significantly slower at inference: in high-demand settings, max throughput decreases by 34-44% for larger models, even when using top-K=1 to match dense compute."
  - [section 4.2] "Dense models outperform their upcycled counterparts significantly at inference time"
- Break condition: If the quality improvement per training FLOP is lower for upcycling than for dense CPT, or if inference efficiency is a primary concern, upcycling may not be worthwhile.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding how MoE works is fundamental to understanding upcycling, as upcycling transforms dense models into MoE models.
  - Quick check question: In an MoE layer with 8 experts and top-K=2 routing, how many experts are activated per token?

- Concept: Dynamic sparsity vs. static sparsity
  - Why needed here: Upcycling introduces dynamic sparsity (only K experts active per token) rather than static sparsity (fixed parameter patterns). This affects both training and inference characteristics.
  - Quick check question: How does the computational complexity of an MoE layer with top-K=2 compare to a dense layer in terms of active parameters?

- Concept: Router training and load balancing
  - Why needed here: The router determines which tokens go to which experts, and poor router training can lead to unbalanced expert utilization, reducing the effectiveness of upcycling.
  - Quick check question: What is the purpose of adding a load balancing loss during MoE training, and how might it affect model quality?

## Architecture Onboarding

- Component map:
  Dense model checkpoint -> Expert duplication layer -> Router network -> Top-K gating -> Load balancing loss -> CPT data pipeline

- Critical path:
  1. Load dense checkpoint
  2. Duplicate MLP layers to create experts
  3. Initialize router weights randomly
  4. Train with load balancing loss
  5. Evaluate quality vs. inference cost

- Design tradeoffs:
  - Number of experts vs. inference speed: More experts = higher quality potential but slower inference
  - Top-K value vs. FLOPs: Higher K = more active parameters = more compute but potentially better quality
  - Expert size vs. parameter efficiency: Larger experts may learn better but reduce the total number that can fit in memory

- Failure signatures:
  - Poor router performance: Some experts are rarely used while others are overloaded
  - Quality plateau: Additional training doesn't improve metrics despite increased FLOPs
  - Inference bottleneck: Throughput significantly worse than expected based on active parameters

- First 3 experiments:
  1. Benchmark inference throughput of upcycled vs. dense models with top-K=1 to isolate the parameter count effect
  2. Train upcycled models with different numbers of experts (4, 8, 16) to find the optimal quality/inference tradeoff
  3. Compare quality progression curves for upcycling vs. dense CPT at different training durations to quantify the tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal top-K value for balancing inference speed and model quality in upcycled MoE models?
- Basis in paper: [inferred] The paper benchmarks top-K=1 and top-K=2, showing performance trade-offs between these values.
- Why unresolved: The authors only tested two values and didn't explore the full range of possible top-K settings to find an optimal balance point.
- What evidence would resolve it: A comprehensive sweep across multiple top-K values (e.g., 1, 2, 4, 8) measuring both inference throughput and downstream task performance to identify the sweet spot.

### Open Question 2
- Question: How does the quality improvement from upcycling scale with model size beyond 1.4B parameters?
- Basis in paper: [explicit] The paper only tested 436M and 1.4B dense models, leaving uncertainty about scalability.
- Why unresolved: The authors didn't test larger dense models (e.g., 7B or 13B) to see if upcycling benefits continue to grow or plateau.
- What evidence would resolve it: Experiments with multiple larger dense models upcycled to corresponding MoE sizes, comparing quality improvements across the scale range.

### Open Question 3
- Question: What inference optimizations could close the performance gap between dense and upcycled MoE models?
- Basis in paper: [explicit] The authors note "there is likely room for MoE-specific optimizations in vLLM" but don't explore them.
- Why unresolved: The paper benchmarks out-of-the-box performance without implementing any MoE-specific optimizations.
- What evidence would resolve it: Testing various MoE-specific optimizations (expert parallelism, dynamic batch sizing, specialized kernels) to quantify potential performance improvements.

## Limitations

- The exact composition of training datasets (both pretraining and domain-specific) is not disclosed, making exact reproduction difficult
- Quality improvements are measured on a single benchmark (Gauntlet Core Average) without ablation on individual tasks
- The inference slowdown measurements assume top-K=1, but real-world usage may involve higher K values
- The paper doesn't explore alternative routing mechanisms or their impact on quality vs. speed

## Confidence

- **High confidence**: Upcycling does increase parameter count and can improve model quality over CPT when given sufficient training compute
- **Medium confidence**: The magnitude of quality gains (up to 20% relative) and the inference slowdown (34-44%) are sensitive to dataset quality, training duration, and benchmarking setup
- **Low confidence**: The assertion that upcycling is "not viable" for inference-bound scenarios is overstated without considering potential optimizations

## Next Checks

1. Replicate the quality improvement claim on a publicly available benchmark (e.g., MMLU or HELM) using the same model sizes and training setup
2. Measure inference throughput with varying top-K values (1, 2, 4) to quantify the tradeoff between quality and speed in real-world settings
3. Test whether the quality gains persist when upcycling is applied to models trained on different pretraining corpora (e.g., Llama 2 or Mistral) to assess robustness