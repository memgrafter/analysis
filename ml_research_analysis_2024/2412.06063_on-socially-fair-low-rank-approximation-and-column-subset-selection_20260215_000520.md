---
ver: rpa2
title: On Socially Fair Low-Rank Approximation and Column Subset Selection
arxiv_id: '2412.06063'
source_url: https://arxiv.org/abs/2412.06063
tags:
- approximation
- fair
- algorithm
- low-rank
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies socially fair low-rank approximation and socially
  fair column subset selection, where the goal is to minimize the loss over all subpopulations
  of the data. The authors show that even constant-factor approximation to fair low-rank
  approximation requires exponential time under certain standard complexity hypotheses.
---

# On Socially Fair Low-Rank Approximation and Column Subset Selection

## Quick Facts
- arXiv ID: 2412.06063
- Source URL: https://arxiv.org/abs/2412.06063
- Authors: Zhao Song; Ali Vakilian; David P. Woodruff; Samson Zhou
- Reference count: 33
- Primary result: Even constant-factor approximation to fair low-rank approximation requires exponential time, but bicriteria approximation algorithms run in polynomial time.

## Executive Summary
This paper studies socially fair low-rank approximation and column subset selection, where the goal is to minimize the maximum loss across all subpopulations of the data. The authors establish that even constant-factor approximation requires exponential time under standard complexity assumptions like the exponential time hypothesis. On the positive side, they develop a 2^poly(k) algorithm for constant groups and constant-factor accuracy, as well as polynomial-time bicriteria approximation algorithms. The paper includes empirical evaluations demonstrating that their bicriteria algorithm performs better than standard low-rank approximation on fairness metrics.

## Method Summary
The paper presents three main approaches: (1) A hardness proof showing that constant-factor approximation to fair low-rank approximation is NP-hard by reducing from the Subspace(n-1, ∞) problem, (2) A (1 + ε)-approximation algorithm using a polynomial system solver with dimensionality reduction that runs in 2^poly(k) time for constant groups, and (3) Bicriteria approximation algorithms for both fair low-rank approximation and fair column subset selection that run in polynomial time using techniques like Dvoretzky's theorem and Lewis weight sampling. The empirical evaluation compares these algorithms against standard low-rank approximation on both synthetic and real-world datasets.

## Key Results
- Constant-factor approximation to fair low-rank approximation requires exponential time under the exponential time hypothesis
- A 2^poly(k) algorithm achieves (1 + ε)-approximation for constant groups and constant-factor accuracy
- Polynomial-time bicriteria approximation algorithms exist for both fair low-rank approximation and fair column subset selection
- Empirical results show the bicriteria algorithm outperforms standard low-rank approximation on fairness metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fair low-rank approximation requires exponential time under standard complexity assumptions, but can be approximated in 2^poly(k) time for constant groups and constant-factor accuracy.
- Mechanism: The hardness proof reduces from the Subspace(n-1, ∞) problem, which is NP-hard to approximate within any constant factor. The positive result uses a polynomial system solver with dimensionality reduction to check feasibility of α values.
- Core assumption: The exponential time hypothesis holds and the polynomial system solver can efficiently check the feasibility of α values.
- Evidence anchors:
  - [abstract] "even constant-factor approximation to fair low-rank approximation requires exponential time under certain standard complexity hypotheses"
  - [section 2.1] "Utilizing the NP-hardness of approximation of the Subspace(n - 1, ∞) problem, we show the NP-hardness of approximation of fair low-rank approximation"
- Break condition: If the polynomial system solver becomes infeasible for large k or if the exponential time hypothesis is false.

### Mechanism 2
- Claim: Bicriteria approximation algorithms for fair low-rank approximation and fair column subset selection run in polynomial time.
- Mechanism: For fair low-rank approximation, the algorithm uses Dvoretzky's theorem to embed L2 into Lp loss and then applies Lewis weight sampling. For fair column subset selection, it leverages the solution from fair low-rank approximation as a starting point.
- Core assumption: The Lewis weight sampling matrix and Gaussian matrices can be generated in polynomial time and provide the required distortion bounds.
- Evidence anchors:
  - [abstract] "there exist bicriteria approximation algorithms for fair low-rank approximation and fair column subset selection that run in polynomial time"
  - [section 2.3] "By Dvoretzky's Theorem, we have that (TGAHS)†TGA is a 'good' approximate solution to the original fair low-rank approximation problem"
- Break condition: If the distortion bounds from Dvoretzky's theorem or Lewis weight sampling become too large for practical parameters.

### Mechanism 3
- Claim: Empirical evaluations demonstrate that the bicriteria algorithm performs better than standard low-rank approximation on socially fair objectives.
- Mechanism: The algorithm optimizes the maximum loss across all subpopulations rather than just the overall loss, leading to better fairness guarantees.
- Core assumption: The synthetic and real-world datasets used in experiments are representative of the fairness concerns in practice.
- Evidence anchors:
  - [section 4.1] "Our empirical evaluations serve as a simple proof-of-concept demonstrating that our bicriteria algorithm can perform significantly better for socially fair low-rank approximation"
  - [section 4.2] "Our empirical evaluations in Figure 2 show that our algorithms can perform significantly better for socially fair low-rank approximation"
- Break condition: If the performance gap between the bicriteria algorithm and baseline disappears on different datasets or with different sensitive attributes.

## Foundational Learning

- Concept: Low-rank approximation and column subset selection
  - Why needed here: These are the fundamental problems being studied under a fairness constraint, where the goal is to minimize the maximum loss across all subpopulations.
  - Quick check question: What is the difference between low-rank approximation and column subset selection, and how are they related?

- Concept: Algorithmic fairness and social fairness
  - Why needed here: The paper studies socially fair algorithms that optimize performance across all subpopulations, which is a specific notion of fairness.
  - Quick check question: How does social fairness differ from other notions of fairness like individual fairness or disparate impact?

- Concept: Complexity theory and hardness assumptions
  - Why needed here: The paper proves hardness results under the exponential time hypothesis and NP-hardness, which are standard complexity assumptions.
  - Quick check question: What is the exponential time hypothesis, and how does it relate to NP-hardness?

## Architecture Onboarding

- Component map: Hardiness proof -> Polynomial system solver -> Bicriteria approximation -> Empirical evaluation
- Critical path:
  1. Check if input satisfies hardness conditions (constant number of groups, constant-factor accuracy)
  2. If yes, run polynomial system solver with dimensionality reduction
  3. If no, run bicriteria approximation algorithm using Dvoretzky's theorem and Lewis weight sampling
  4. For empirical evaluation, compare results against baseline on fairness metrics
- Design tradeoffs:
  - Runtime vs. approximation factor: Polynomial system solver gives better approximation but exponential runtime, while bicriteria algorithm gives worse approximation but polynomial runtime
  - Number of groups vs. hardness: Hardness results apply for constant number of groups, but algorithm complexity increases with more groups
  - Rank parameter k vs. feasibility: Polynomial system solver becomes infeasible for large k, requiring bicriteria approach
- Failure signatures:
  - Algorithm produces solutions with high maximum loss across subpopulations
  - Runtime exceeds expected bounds for given parameters
  - Numerical instability in polynomial system solver or Lewis weight sampling
- First 3 experiments:
  1. Run hardness proof reduction on small synthetic dataset to verify NP-hardness
  2. Compare polynomial system solver vs. bicriteria algorithm on dataset with varying number of groups
  3. Evaluate empirical performance on real dataset with different sensitive attributes and compare fairness metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we develop a polynomial-time approximation algorithm for socially fair low-rank approximation that achieves a constant factor approximation (not just bicriteria)?
- Basis in paper: [explicit] The paper shows that constant-factor approximation requires exponential time under standard complexity hypotheses, but gives bicriteria approximation algorithms running in polynomial time.
- Why unresolved: The paper establishes a separation between constant-factor approximation (requiring exponential time) and bicriteria approximation (polynomial time), but does not bridge this gap.
- What evidence would resolve it: Either an algorithm achieving constant-factor approximation in polynomial time, or a proof that no such algorithm exists under some complexity assumption.

### Open Question 2
- Question: Can the 2^poly(k) runtime algorithm for socially fair low-rank approximation be improved to run in poly(n) time for any constant number of groups?
- Basis in paper: [explicit] The paper gives a 2^poly(k) algorithm for constant number of groups, which is a substantial improvement over the naive n^poly(k) approach.
- Why unresolved: The paper does not explore whether further improvements are possible beyond the 2^poly(k) runtime.
- What evidence would resolve it: Either an algorithm achieving poly(n) runtime for constant number of groups, or a lower bound showing this is impossible under some complexity assumption.

### Open Question 3
- Question: Can the bicriteria approximation algorithms for socially fair column subset selection be improved to select exactly k columns (not O(k log k) columns) while maintaining polynomial runtime?
- Basis in paper: [explicit] The paper gives a bicriteria algorithm that selects O(k log k) columns in polynomial time, but does not achieve the exact k-column constraint.
- Why unresolved: The paper establishes that a bicriteria approach is possible but does not explore whether the exact constraint can be maintained.
- What evidence would resolve it: Either an algorithm achieving exact k-column selection in polynomial time with similar approximation guarantees, or a proof that this is impossible.

## Limitations
- The polynomial system solver may become infeasible for larger rank parameters k, limiting practical applicability
- Hardness results depend on the exponential time hypothesis, which remains unproven
- Empirical evaluations are based on a limited set of datasets and may not generalize to all scenarios

## Confidence
- High confidence: Theoretical foundations of hardness results and polynomial-time bicriteria approximation algorithms
- Medium confidence: Practical performance of bicriteria algorithm in real-world scenarios
- Low confidence: Scalability of (1 + ε)-approximation algorithm to large-scale problems

## Next Checks
1. Implement the polynomial system solver on synthetic datasets with varying rank parameters k to empirically verify the claimed 2^poly(k) runtime and identify practical limits of the (1 + ε)-approximation algorithm.

2. Evaluate the bicriteria approximation algorithm on a diverse set of real-world datasets with different sensitive attributes and group structures to assess its generalizability and robustness to various fairness concerns.

3. Conduct a sensitivity analysis of the hardness results by relaxing the exponential time hypothesis or exploring alternative complexity assumptions to understand the potential impact on claimed intractability of constant-factor approximation.