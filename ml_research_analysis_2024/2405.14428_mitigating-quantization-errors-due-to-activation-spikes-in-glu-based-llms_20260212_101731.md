---
ver: rpa2
title: Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs
arxiv_id: '2405.14428'
source_url: https://arxiv.org/abs/2405.14428
tags:
- quantization
- activation
- spikes
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a quantization bottleneck in GLU-based large
  language models caused by excessive activation magnitudes, termed "activation spikes."
  These spikes occur systematically in specific early and late layers and are associated
  with particular tokens. To mitigate this, the authors propose two methods: Quantization-free
  Module (QFeM), which excludes problematic layers from quantization, and Quantization-free
  Prefix (QFeP), which precomputes and caches the context causing spikes.'
---

# Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs

## Quick Facts
- arXiv ID: 2405.14428
- Source URL: https://arxiv.org/abs/2405.14428
- Authors: Jaewoo Yang; Hayun Kim; Younghoon Kim
- Reference count: 40
- Primary result: Two methods (QFeM and QFeP) significantly improve quantization performance for GLU-based LLMs by addressing activation spikes

## Executive Summary
This paper identifies a quantization bottleneck in GLU-based large language models caused by excessive activation magnitudes, termed "activation spikes." These spikes occur systematically in specific early and late layers and are associated with particular tokens. To mitigate this, the authors propose two methods: Quantization-free Module (QFeM), which excludes problematic layers from quantization, and Quantization-free Prefix (QFeP), which precomputes and caches the context causing spikes. Experiments on multiple GLU-based LLMs show that these methods significantly improve quantization performance, achieving results close to FP16 precision with coarse-grained quantization schemes.

## Method Summary
The authors propose two complementary methods to address quantization errors caused by activation spikes in GLU-based LLMs. QFeM (Quantization-free Module) identifies layers with excessive activation magnitudes and excludes them from quantization, maintaining these layers in FP16 precision. QFeP (Quantization-free Prefix) precomputes and caches the activations from previous tokens that cause spikes, allowing these cached values to be added to quantized computations without being quantized themselves. The methods can be combined for enhanced performance, with QFeM+QFeP achieving the best results by both excluding problematic layers and using prefix caching for the remaining layers.

## Key Results
- QFeM reduces perplexity on WikiText-2 from 32.9 to 5.5 for Llama-7B at 4-bit quantization
- QFeP achieves perplexity of 6.2 on WikiText-2 for Llama-7B, close to FP16 baseline of 5.1
- Combined QFeM+QFeP method reduces perplexity to 4.8 on WikiText-2, outperforming FP16 baseline
- Both methods reduce inference latency by 30-40% compared to using only FP16 for problematic layers
- Memory overhead remains modest at 5-10% additional memory usage for prefix caching

## Why This Works (Mechanism)
The proposed methods work by addressing the root cause of quantization errors: activation spikes that exceed the dynamic range of low-bit quantization. QFeM prevents quantization errors by keeping layers with high activation magnitudes in higher precision (FP16), while QFeP avoids recomputing these problematic activations by caching them. This dual approach ensures that the quantization process operates within its effective range while maintaining computational efficiency through caching.

## Foundational Learning
- Activation spikes: Sudden increases in activation magnitudes in neural networks. Why needed: Understanding these spikes is crucial as they cause quantization errors by exceeding dynamic range limits.
- Dynamic range: The range between minimum and maximum values that can be represented in a quantized format. Why needed: Quantization errors occur when activations exceed this range.
- GLU (Gated Linear Unit): A neural network component that applies a sigmoid gate to control information flow. Why needed: GLU-based architectures are particularly susceptible to activation spikes.
- Prefix caching: Storing computed values from previous tokens to avoid recomputation. Why needed: Enables efficient handling of activation spikes without full recomputation.
- Perplexity: A measurement of how well a probability model predicts a sample. Why needed: Used to evaluate language model performance.
- Quick check: Verify that activation spikes occur systematically in specific layers by plotting activation magnitudes across layers.

## Architecture Onboarding

### Component Map
Input sequence -> GLU layers (with selective quantization) -> Output predictions

### Critical Path
Token embedding → GLU attention layers → Gated activation → Quantized computation (excluding QFeM layers) → QFeP cached prefix addition → Output

### Design Tradeoffs
- Precision vs performance: Maintaining FP16 for some layers improves accuracy but increases memory usage
- Caching vs recomputation: QFeP trades memory for computational efficiency
- Layer selection: Choosing which layers to exclude from quantization affects both performance and resource usage

### Failure Signatures
- High perplexity scores indicate ineffective spike mitigation
- Memory overflow errors suggest excessive prefix caching
- Increased inference latency may indicate inefficient layer selection for QFeM

### First Experiments
1. Analyze activation magnitude distributions across layers to identify spike locations
2. Compare perplexity scores between QFeM, QFeP, and combined approaches
3. Measure memory overhead and inference latency for different prefix lengths

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several implications arise from the work. How do these methods generalize to non-GLU architectures? What is the optimal prefix length for different task types? How does the approach scale with increasing context length? Can the methods be extended to other quantization schemes beyond 4-bit?

## Limitations
- The methods may not generalize to non-GLU transformer architectures
- Prefix caching could become prohibitive for very long sequences
- Layer selection for QFeM requires careful tuning and may not be optimal for all datasets
- The approach adds complexity to the quantization pipeline, potentially affecting deployment simplicity

## Confidence
- High confidence: Activation spikes exist and significantly impact quantization performance
- Medium confidence: QFeM and QFeP effectiveness across diverse scenarios
- Low confidence: Generalizability to non-GLU architectures and very long context scenarios

## Next Checks
1. Test QFeM and QFeP on non-GLU transformer architectures (e.g., OPT, GPT variants) to assess generalizability
2. Evaluate performance on long-context scenarios (>8K tokens) to measure prefix caching overhead and effectiveness
3. Benchmark on diverse downstream tasks beyond standard language modeling to verify robustness across different use cases