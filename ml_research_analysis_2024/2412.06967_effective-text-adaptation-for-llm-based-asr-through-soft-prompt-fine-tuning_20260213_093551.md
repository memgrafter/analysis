---
ver: rpa2
title: Effective Text Adaptation for LLM-based ASR through Soft Prompt Fine-Tuning
arxiv_id: '2412.06967'
source_url: https://arxiv.org/abs/2412.06967
tags:
- prompt
- text
- domain
- adaptation
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a two-step soft prompt fine-tuning strategy
  to improve domain adaptation for LLM-based ASR systems. The method addresses the
  challenge of adapting ASR models to entity-heavy domains using only text data, without
  paired audio-text examples.
---

# Effective Text Adaptation for LLM-based ASR through Soft Prompt Fine-Tuning

## Quick Facts
- arXiv ID: 2412.06967
- Source URL: https://arxiv.org/abs/2412.06967
- Reference count: 0
- Key outcome: Up to 9% relative WER reduction and 18% relative EER reduction on music and chatbot domains

## Executive Summary
This paper introduces a two-step soft prompt fine-tuning strategy for adapting LLM-based ASR systems to entity-heavy domains using only text data. The approach addresses the challenge of domain adaptation without paired audio-text examples by learning domain-specific pseudo audio embeddings through soft prompt training, followed by decoder fine-tuning conditioned on these learned prompts. Experiments demonstrate significant improvements in both general and entity-specific recognition performance, with further gains when combined with external language model fusion.

## Method Summary
The method employs a two-step fine-tuning approach where a trainable soft prompt is first learned in the audio embedding space to serve as domain-specific pseudo audio embeddings, then the LLM decoder is fine-tuned conditioned on these learned prompts. This approach preserves the speech-text alignment learned during initial ASR training while incorporating domain knowledge from text-only data. The soft prompt acts as a bridge between domain-specific text and the audio embedding space, enabling effective adaptation without requiring paired audio-text examples in the target domain.

## Key Results
- Achieves up to 9% relative WER reduction and 18% relative EER reduction on music and chatbot domains
- Soft prompt length optimization (30 vs 50 tokens) shows domain-dependent performance (longer prompts better for longer utterances)
- External LM fusion provides additional 2-5% EER improvement when combined with soft prompt fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Soft prompt fine-tuning creates domain-specific pseudo audio embeddings that preserve the speech-text alignment learned during initial ASR training. By training a trainable soft prompt in the audio embedding space conditioned on target domain text, the method learns representations that mimic what audio embeddings would look like for that domain. This preserves the multimodal alignment while incorporating domain knowledge.

### Mechanism 2
Two-step fine-tuning (soft prompt learning followed by decoder fine-tuning) reduces catastrophic forgetting compared to pre-fine-tuning while avoiding condition mismatch compared to direct post-fine-tuning. The first step learns domain-specific prompt representations without modifying the decoder, preserving the original speech-text alignment. The second step fine-tunes the decoder conditioned on these learned prompts, ensuring consistency between training and inference conditions.

### Mechanism 3
External LM fusion combined with soft prompt fine-tuning provides complementary improvements by leveraging both prompt conditioning and additional language modeling capabilities. The soft prompt fine-tuning adapts the LLM decoder to domain-specific prompt conditions, while external LM fusion adds an additional language modeling layer that can capture broader domain patterns beyond what prompt conditioning alone provides.

## Foundational Learning

- **Multimodal alignment in LLM-based ASR**: Understanding how audio embeddings and text embeddings are aligned in the shared embedding space is crucial for grasping why soft prompts can serve as pseudo audio embeddings
  - Quick check: How does the audio encoder project audio features into the text embedding space, and why is this alignment important for the soft prompt approach?

- **Prompt conditioning in autoregressive language models**: The effectiveness of the soft prompt approach relies on the LLM decoder's ability to generate conditioned on prompts, which is fundamental to how the method works
  - Quick check: What happens to the generation process when prompts are mismatched between training and inference, and how does this relate to the condition mismatch problem?

- **Catastrophic forgetting in fine-tuning**: Understanding why pre-fine-tuning can degrade original performance helps explain why the two-step soft prompt approach is necessary
  - Quick check: What neural mechanisms cause catastrophic forgetting during fine-tuning, and how does freezing components help mitigate this?

## Architecture Onboarding

- **Component map**: Audio → Audio encoder → Soft prompt conditioning → LLM decoder → Text output
- **Critical path**: The soft prompt sits in the audio embedding space and conditions the decoder generation
- **Design tradeoffs**: Soft prompt length (30 vs 50 tokens): longer prompts work better for longer utterances (chatbot) but may overfit shorter ones (music); timing of fine-tuning: pre-fine-tuning risks catastrophic forgetting, post-fine-tuning with no prompt causes condition mismatch; external LM integration: adds computational overhead but provides complementary improvements
- **Failure signatures**: Poor domain entity recognition despite adaptation indicates soft prompt not learning meaningful pseudo audio embeddings; worse performance than baseline suggests catastrophic forgetting or condition mismatch issues; no improvement with external LM fusion may indicate overlapping representations between soft prompt and external LM
- **First 3 experiments**: 1) Implement basic post-fine-tuning with empty prompt to establish baseline condition mismatch; 2) Add soft prompt training step with frozen decoder to validate pseudo audio embedding learning; 3) Test different soft prompt lengths (30 vs 50) on both music and chatbot domains to establish optimal configuration

## Open Questions the Paper Calls Out

- How do domain-specific soft prompts learned through the proposed method relate to interpretable acoustic features that could explain their effectiveness?
- Can the soft prompt length be dynamically determined rather than fixed as a hyperparameter, and what optimization approach would be optimal?
- How does the proposed soft prompt approach scale to multi-domain adaptation where multiple domain-specific prompts must be learned and managed simultaneously?

## Limitations
- The paper does not specify soft prompt initialization and optimization details, making faithful reproduction challenging
- Evaluation dataset specifics are limited, restricting assessment of result robustness across different entity-heavy domains
- External LM integration specifics are not detailed, raising questions about the generality and implementation complexity of the combined approach

## Confidence
- **High confidence**: The core two-step fine-tuning methodology is well-specified and the reported improvements over baseline are statistically significant
- **Medium confidence**: The effectiveness of soft prompt fine-tuning compared to traditional methods like shallow fusion or adapter-based approaches
- **Low confidence**: The reproducibility of results given the unspecified implementation details (soft prompt initialization, optimization procedure, external LM integration specifics)

## Next Checks
1. **Implementation validation check**: Implement the soft prompt fine-tuning approach with different initialization strategies (random vs pretrained) and optimization procedures to determine which configuration reliably learns effective pseudo audio embeddings
2. **Cross-domain generalization check**: Test the soft prompt fine-tuning approach on additional entity-heavy domains beyond music and chatbot (e.g., medical terminology, technical jargon) to assess whether improvements generalize across different types of entity-heavy domains
3. **Comparison with alternative approaches**: Implement and compare soft prompt fine-tuning against adapter-based domain adaptation and shallow fusion methods that also use only text data