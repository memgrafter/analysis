---
ver: rpa2
title: How Redundant Is the Transformer Stack in Speech Representation Models?
arxiv_id: '2409.16302'
source_url: https://arxiv.org/abs/2409.16302
tags:
- layers
- speech
- transformer
- mimicker
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates redundancy in transformer-based speech
  representation models, revealing significant layer similarity and potential for
  pruning. Analysis of wav2vec2 and wavLM models using cosine similarity, CKA, and
  mutual kNN metrics shows a block-like structure with high similarity within blocks,
  suggesting two main processing steps.
---

# How Redundant Is the Transformer Stack in Speech Representation Models?

## Quick Facts
- arXiv ID: 2409.16302
- Source URL: https://arxiv.org/abs/2409.16302
- Authors: Teresa Dorszewski; Albert Kjøller Jacobsen; Lenka Tětková; Lars Kai Hansen
- Reference count: 0
- One-line primary result: Transformer stacks in speech models can be reduced by 95-98% while maintaining 95%+ performance through layer pruning and knowledge distillation

## Executive Summary
This paper investigates redundancy in transformer-based speech representation models, revealing significant layer similarity and potential for pruning. Analysis of wav2vec2 and wavLM models using cosine similarity, CKA, and mutual kNN metrics shows a block-like structure with high similarity within blocks, suggesting two main processing steps. Layer-wise pruning without retraining can remove 15-45% of layers while maintaining 95% accuracy by leveraging the Block Influence score. More dramatically, knowledge distillation via mimicking networks can replace the entire transformer stack with one or two layers, reducing parameters by 95-98% and inference time by up to 94% while preserving over 95% performance. These results demonstrate that transformer stacks in speech representation models are largely redundant for downstream tasks, enabling substantial efficiency gains through structural simplification.

## Method Summary
The study analyzes pre-trained wav2vec2 and wavLM speech models using layer similarity metrics (cosine similarity, CKA, mutual kNN) to identify redundant transformer layers. Layer-wise pruning removes 15-45% of layers without retraining by leveraging Block Influence scores that quantify layer importance. Knowledge distillation employs mimicking networks with 1-2 transformer or linear layers trained in two stages: first with MSE loss to mimic teacher representations for 50 epochs, then fine-tuned with NLL loss for 30 epochs. Experiments use the Speech Commands v0.02 dataset (35 word classes, 400+ speakers) and evaluate parameter reduction (95-98%) and inference time reduction (up to 94%) while maintaining ≥95% accuracy.

## Key Results
- Transformer layers exhibit block-level redundancy, enabling 15-45% pruning without retraining while maintaining 95% accuracy
- Knowledge distillation via mimicking networks can replace entire transformer stacks with 1-2 layers, reducing parameters by 95-98%
- Inference time can be reduced by up to 94% while preserving over 95% of original model performance
- Layer similarity analysis reveals two main processing blocks with high internal similarity across wav2vec2 and wavLM models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer layers in speech models exhibit block-level redundancy where groups of consecutive layers perform similar transformations.
- Mechanism: Layers within each block transform representations in nearly identical ways, allowing entire blocks to be removed or replaced without significant performance degradation.
- Core assumption: Similarity metrics (cosine, CKA, mutual kNN) accurately capture functional redundancy rather than just statistical correlation.
- Evidence anchors:
  - [abstract] "Our findings reveal a block-like structure of high similarity, suggesting two main processing steps and significant redundancy of layers."
  - [section 3.1] "Our analysis reveals that all models exhibit two primary blocks characterized by highly similar latent representations throughout each block"
  - [corpus] Weak - corpus papers focus on LLM redundancy rather than speech model block structure specifically
- Break condition: If similarity metrics fail to capture functional redundancy, or if downstream tasks require cross-block information flow that cannot be compressed.

### Mechanism 2
- Claim: Layer-wise pruning can remove 15-45% of layers without retraining by leveraging Block Influence scores that identify redundant layers.
- Mechanism: Block Influence score quantifies layer importance based on cosine similarity between consecutive layers, enabling structural layer selection rather than random pruning.
- Core assumption: The Block Influence score effectively identifies truly redundant layers rather than just similar ones.
- Evidence anchors:
  - [abstract] "We demonstrate the effectiveness of pruning transformer-based speech representation models without the need for post-training, achieving up to 40% reduction in transformer layers while maintaining over 95% of the model's predictive capacity."
  - [section 2.2] "For all heuristics, we prune by deleting whole transformer blocks in the order determined by the heuristic"
  - [corpus] Moderate - corpus includes papers on LLM pruning but not specifically on Block Influence score methodology
- Break condition: If pruned models experience catastrophic forgetting or if Block Influence score fails to generalize across different speech tasks.

### Mechanism 3
- Claim: Knowledge distillation via mimicking networks can replace the entire transformer stack with 1-2 layers while maintaining 95%+ performance.
- Mechanism: Mimicking layers learn to reproduce final-layer representations from teacher models, capturing essential transformations without intermediate complexity.
- Core assumption: Intermediate transformer representations are not essential for downstream classification tasks.
- Evidence anchors:
  - [abstract] "Furthermore, we employ a knowledge distillation method to substitute the entire transformer stack with mimicking layers, reducing the network size 95-98% and the inference time by up to 94%."
  - [section 2.3] "As experimental parameters, we consider the layer type, i.e. Transformer or linear mimic layers"
  - [corpus] Strong - multiple corpus papers (ShortGPT, SLEB) demonstrate transformer stack redundancy in LLMs
- Break condition: If mimicking layers fail to capture task-specific transformations or if downstream tasks require explicit intermediate representations.

## Foundational Learning

- Concept: Self-supervised speech representation learning
  - Why needed here: The study analyzes pre-trained speech models (wav2vec2, wavLM) that use self-supervised learning to create rich audio representations
  - Quick check question: What is the key difference between self-supervised and supervised learning in the context of speech representation models?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The research focuses specifically on transformer layers within speech models, requiring understanding of how transformers process sequential data
  - Quick check question: How do transformer encoder layers process input embeddings differently from recurrent neural networks?

- Concept: Knowledge distillation and model compression
  - Why needed here: The mimicking network approach is a form of knowledge distillation that transfers knowledge from large models to smaller ones
  - Quick check question: What is the fundamental difference between knowledge distillation and traditional model training?

## Architecture Onboarding

- Component map: Audio → Feature Extractor → Transformer Stack → Classification Layer → Output
- Critical path: Audio → Feature Extractor → Transformer Stack → Classification Layer → Output
- Design tradeoffs: Depth vs. efficiency (more layers provide better performance but higher computational cost), similarity metrics selection (different metrics capture different aspects of redundancy)
- Failure signatures: Performance drops below 95% threshold indicate either insufficient blocks remain or mimicking layers fail to capture essential transformations
- First 3 experiments:
  1. Compute layer similarity metrics (cosine, CKA, mutual kNN) to identify block structure in a pre-trained speech model
  2. Perform forward pruning to identify how many initial layers can be removed while maintaining performance
  3. Implement 1-layer mimicking network to test if transformer stack can be replaced while maintaining accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the block structure identified through similarity metrics reflect distinct functional roles in the processing pipeline?
- Basis in paper: [explicit] The paper identifies two main processing steps through similarity metrics and observes that pruning one block causes significant performance drops while pruning within blocks is possible.
- Why unresolved: While the paper demonstrates that both blocks are necessary for maintaining performance, it does not investigate whether the blocks serve different functional purposes or what specific transformations occur in each block.
- What evidence would resolve it: Systematic ablation studies examining which aspects of the downstream task each block contributes to, or analysis of what information is preserved/lost when each block is removed.

### Open Question 2
- Question: Are there architectural or training factors that determine the number and size of blocks in transformer-based speech models?
- Basis in paper: [inferred] The paper observes block structures in multiple models but notes that block numbers and dimensions vary across different models and tasks, suggesting architectural or training factors may influence this structure.
- Why unresolved: The paper demonstrates block structures exist but does not investigate what causes variation in block number and size across different models or whether these can be predicted or controlled.
- What evidence would resolve it: Comparative analysis of models with different architectures, training objectives, or dataset characteristics to identify factors that influence block formation.

### Open Question 3
- Question: How does the redundancy pattern change across different speech tasks or domains?
- Basis in paper: [inferred] All experiments focus on a single word classification task, though the paper discusses potential applications to other speech tasks like speech recognition and emotion detection.
- Why unresolved: The paper establishes redundancy in one specific task but does not examine whether the same patterns hold for other speech tasks or domains, which would affect generalizability of pruning and distillation approaches.
- What evidence would resolve it: Replication of similarity analysis and pruning experiments across diverse speech tasks (ASR, speaker ID, emotion detection) to identify task-dependent redundancy patterns.

### Open Question 4
- Question: Is there an optimal layer configuration that balances computational efficiency with task performance?
- Basis in paper: [explicit] The paper demonstrates that 15-45% of layers can be pruned while maintaining 95% performance, and that mimicking networks can reduce parameters by 95-98% with similar performance.
- Why unresolved: While the paper shows various levels of compression are possible, it does not systematically explore the trade-off between efficiency and performance across different tasks or identify optimal configurations for specific use cases.
- What evidence would resolve it: Comprehensive benchmarking of different pruned/mimicked configurations across multiple tasks to establish performance-efficiency trade-offs and identify task-specific optimal configurations.

## Limitations

- Scalability uncertainty to more complex speech tasks beyond 35-word classification, such as continuous speech recognition or multilingual models
- Block Influence score methodology may not generalize to other transformer variants or self-supervised speech models
- Assumption that similarity metrics capture functional redundancy rather than just statistical correlation may not hold for all speech representation tasks

## Confidence

- High confidence: Block-like structure discovery, 15-45% layer pruning without retraining, similarity metric validation
- Medium confidence: 95-98% parameter reduction through mimicking networks, 94% inference time improvement
- Low confidence: Universal applicability to all speech tasks, complete elimination of intermediate representations

## Next Checks

1. **Cross-task validation**: Test the pruning and mimicking approaches on continuous speech recognition and multilingual speech tasks to verify generalizability beyond 35-word classification

2. **Ablation of similarity metrics**: Compare the effectiveness of cosine similarity, CKA, and mutual kNN in identifying truly redundant layers by measuring post-pruning performance degradation across different metrics

3. **Fine-tuning after pruning**: Evaluate whether fine-tuning pruned models on downstream tasks can recover any lost performance compared to the strict "no retraining" constraint, establishing the true minimum viable architecture