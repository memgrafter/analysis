---
ver: rpa2
title: Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models
arxiv_id: '2411.14257'
source_url: https://arxiv.org/abs/2411.14257
tags: []
core_contribution: This paper uses sparse autoencoders to discover directions in large
  language models that encode self-knowledge about whether an entity is known or unknown
  to the model. These entity recognition directions are found in the base model and
  are causally relevant for knowledge refusal behavior in chat models.
---

# Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models

## Quick Facts
- arXiv ID: 2411.14257
- Source URL: https://arxiv.org/abs/2411.14257
- Reference count: 40
- Key outcome: Sparse autoencoders discover entity recognition directions in language models that encode self-knowledge about whether entities are known or unknown, enabling control over refusal and hallucination behaviors.

## Executive Summary
This paper investigates how large language models (LLMs) internally detect whether they know an entity, revealing mechanisms that drive knowledge refusal and hallucination behaviors. Using sparse autoencoders (SAEs), the authors discover linear directions in model representations that encode self-knowledge about entity recognition. These entity recognition directions are causally relevant—they can steer models to either refuse to answer about known entities or hallucinate attributes of unknown entities. The work demonstrates that steering with these directions regulates the model's attention to entity tokens, affecting factual recall and attribute extraction.

## Method Summary
The method involves training sparse autoencoders on Gemma 2 base model representations to discover interpretable features encoding entity recognition. The authors use entity templates to prompt models and categorize responses, then compute separation scores to identify latents that fire on known vs unknown entities. They apply activation steering by scaling these latents to influence model behavior, measuring changes in refusal rates and hallucinations. Mechanistic effects are validated through activation patching and attention score analysis, establishing causal links between entity recognition latents and model behaviors.

## Key Results
- SAEs discovered linear directions in model representations encoding self-knowledge about entity recognition
- Entity recognition directions are causally relevant for knowledge refusal behavior in chat models
- Steering with these directions controls model tendency to refuse answers or hallucinate information
- Entity recognition latents regulate attention to entity tokens, affecting attribute extraction and factual recall

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse autoencoders uncover linear directions in model representations that encode self-knowledge about whether the model recognizes an entity.
- Mechanism: SAEs project high-dimensional model representations into a larger sparse space, isolating features that fire almost exclusively on known or unknown entities. These directions are causally linked to knowledge refusal behavior.
- Core assumption: The Linear Representation Hypothesis holds—interpretable properties like entity recognition are encoded as linear directions in representation space.
- Evidence anchors:
  - [abstract] "Using sparse autoencoders as an interpretability tool, we discover that a key part of these mechanisms is entity recognition, where the model detects if an entity is one it can recall facts about."
  - [section 2] SAEs are described as finding a sparse, interpretable decomposition of model representations.
  - [corpus] Weak—corpus contains related work but not direct SAE-based evidence.
- Break condition: If entity recognition features are not linearly separable or SAE training fails to isolate meaningful features.

### Mechanism 2
- Claim: Entity recognition directions causally affect knowledge refusal by steering the model to either refuse or hallucinate based on whether the model "knows" the entity.
- Mechanism: By increasing activation of the unknown entity latent, the model's tendency to refuse answering about known entities increases. Conversely, increasing the known entity latent reduces refusal and induces hallucination on unknown entities.
- Core assumption: Steering with decoder latents (Equation 4) is sufficient to control high-level behaviors like refusal and hallucination.
- Evidence anchors:
  - [abstract] "These directions are causally relevant: capable of steering the model to refuse to answer questions about known entities, or to hallucinate attributes of unknown entities when it would otherwise refuse."
  - [section 5] Empirical results show steering with entity latents controls refusal rates across entity types.
  - [corpus] Weak—related work discusses hallucination mitigation but not direct steering of refusal.
- Break condition: If the steering coefficient is too large or small, or if the model's chat finetuning overrides the base model's behavior.

### Mechanism 3
- Claim: Entity recognition directions regulate the model's attention to entity tokens, affecting attribute extraction and factual recall.
- Mechanism: The entity recognition latents modulate attention scores of downstream heads that move entity attributes to the final token. Steering with unknown entity latents reduces attention to known entities, suppressing correct recall.
- Core assumption: Attention heads identified in prior work (Nanda et al., 2023; Geva et al., 2023) are the same ones affected by entity recognition latents.
- Evidence anchors:
  - [section 6] "Steering with the top unknown entity latent reduces the attention to the last token of the entity, even in prompts with a known entity."
  - [section 6] Activation patching shows causal link between latents and attention head behavior.
  - [corpus] Weak—related work on hallucination mitigation but not direct mechanistic attention regulation.
- Break condition: If attention patterns are not preserved across model scales or entity types.

## Foundational Learning

- Concept: Sparse Autoencoders (SAEs)
  - Why needed here: SAEs are the core tool for discovering interpretable directions in model representations that encode self-knowledge.
  - Quick check question: What is the purpose of the sparsity penalty in SAE training?
- Concept: Activation Patching
  - Why needed here: Used to establish causal links between model components (e.g., latents) and behaviors (e.g., attention, recall).
  - Quick check question: How does activation patching help differentiate correlation from causation?
- Concept: Linear Representation Hypothesis
  - Why needed here: Underlies the assumption that interpretable features like entity recognition are encoded as linear directions in representation space.
  - Quick check question: What evidence supports the claim that model representations are sparse linear combinations of interpretable directions?

## Architecture Onboarding

- Component map:
  - Input: Entity prompts (known/unknown)
  - SAE layer: Sparse autoencoder decomposition of residual streams
  - Latent space: Encoded features (entity recognition, uncertainty)
  - Steering module: Decoder latent-based activation steering
  - Output: Model behavior (refusal/hallucination/attention patterns)
- Critical path: Entity token → SAE latents → steering intervention → output behavior
- Design tradeoffs:
  - Using base model SAEs on chat model behavior—relies on finetuning repurposing existing mechanisms
  - Choosing steering coefficient—too low has no effect, too high causes instability
  - Entity type generalization—latents must work across diverse entity categories
- Failure signatures:
  - Latents activate on random tokens (>2% threshold) → specificity lost
  - Steering has no effect on chat model → finetuning overrides base mechanisms
  - Attention scores unchanged after steering → mechanism not causally linked
- First 3 experiments:
  1. Train SAEs on Gemma 2 base model, extract top known/unknown entity latents, test activation patterns on known/unknown prompts
  2. Steer chat model with extracted latents, measure change in refusal rates across entity types
  3. Perform activation patching to verify causal link between latents and attention head behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How general are the entity recognition directions across different model architectures and sizes?
- Basis in paper: [explicit] The paper found entity recognition directions in Gemma 2 2B and 9B models, but does not extensively test other architectures.
- Why unresolved: The study focused primarily on Gemma models, leaving uncertainty about whether similar mechanisms exist in other LLMs like Llama, Mistral, or Claude.
- What evidence would resolve it: Systematic testing of entity recognition directions across multiple model families, architectures (decoder-only, encoder-decoder), and scales would clarify generalizability.

### Open Question 2
- Question: Do entity recognition directions exist for non-entity knowledge types, such as numerical facts or procedural knowledge?
- Basis in paper: [inferred] The study focused exclusively on entity-based knowledge (players, movies, cities, songs) but did not explore other knowledge domains.
- Why unresolved: The paper demonstrates entity recognition mechanisms but does not investigate whether similar self-knowledge exists for other fact types.
- What evidence would resolve it: Finding and testing directions that recognize numerical facts, dates, or procedural knowledge in models would show whether the mechanism generalizes beyond entities.

### Open Question 3
- Question: How do entity recognition directions interact with the model's broader uncertainty mechanisms?
- Basis in paper: [explicit] The paper found directions representing uncertainty that discriminate between correct and incorrect answers, suggesting a relationship with entity recognition.
- Why unresolved: While both mechanisms were identified, the paper does not explore how they interact or whether one influences the other.
- What evidence would resolve it: Experiments manipulating both entity recognition and uncertainty directions simultaneously would reveal their interaction and relative importance.

### Open Question 4
- Question: Can entity recognition directions be used to improve model calibration on factual questions?
- Basis in paper: [inferred] The paper shows that steering with entity recognition directions affects refusal behavior, suggesting potential for controlling model responses.
- Why unresolved: The study demonstrated steering effects but did not investigate whether these could be used to improve the model's calibration or factual accuracy.
- What evidence would resolve it: Developing methods to use entity recognition directions for improving calibration metrics on factual question answering would demonstrate practical utility.

## Limitations

- The transfer of SAEs from base models to chat models remains the largest uncertainty, as it relies on chat finetuning repurposing existing base model mechanisms
- The exact configuration and hyperparameters of the SAEs (e.g., layer selection, sparsity targets) are underspecified, making exact replication difficult
- The entity datasets from Wikidata are not provided, limiting reproducibility

## Confidence

**High confidence**: The SAE-based discovery of entity recognition features in base models. The separation scores and activation patterns are well-documented with clear empirical support.

**Medium confidence**: The causal effectiveness of steering chat models using base model latents. While steering experiments show directional effects, the magnitude and consistency across different entity types vary.

**Low confidence**: The mechanistic claim that entity recognition latents directly regulate attention to entity tokens. The evidence from activation patching is suggestive but correlational, and the exact causal chain remains unclear.

## Next Checks

1. **Cross-model validation**: Test whether SAEs trained on one base model (e.g., Gemma 2B) successfully transfer to steer behavior in chat models of different sizes (9B, 27B) and different architectures.

2. **Latent specificity test**: Measure activation frequencies of top entity recognition latents on random vs. entity tokens to verify the >2% specificity threshold and ensure latents don't fire spuriously.

3. **Mechanistic ablation**: Perform targeted ablation of attention heads identified as mediating entity attribute extraction to confirm they are the same heads affected by entity recognition latents during steering interventions.