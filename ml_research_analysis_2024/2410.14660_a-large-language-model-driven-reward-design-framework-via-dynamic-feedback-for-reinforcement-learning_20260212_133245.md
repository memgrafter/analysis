---
ver: rpa2
title: A Large Language Model-Driven Reward Design Framework via Dynamic Feedback
  for Reinforcement Learning
arxiv_id: '2410.14660'
source_url: https://arxiv.org/abs/2410.14660
tags:
- reward
- feedback
- function
- code
- card
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CARD introduces a Coder-Evaluator framework that iteratively refines
  LLM-generated reward functions using dynamic feedback, eliminating the need for
  human intervention or parallel LLM queries. By incorporating Trajectory Preference
  Evaluation, the method avoids repetitive RL training while ensuring reward functions
  align with task objectives.
---

# A Large Language Model-Driven Reward Design Framework via Dynamic Feedback for Reinforcement Learning

## Quick Facts
- arXiv ID: 2410.14660
- Source URL: https://arxiv.org/abs/2410.14660
- Authors: Shengjie Sun; Runze Liu; Jiafei Lyu; Jing-Wen Yang; Liangpeng Zhang; Xiu Li
- Reference count: 40
- One-line primary result: CARD outperforms or matches baselines and even surpasses human-designed oracle rewards on 3 tasks while using fewer LLM queries than iterative baselines

## Executive Summary
CARD introduces a Coder-Evaluator framework that iteratively refines LLM-generated reward functions using dynamic feedback, eliminating the need for human intervention or parallel LLM queries. By incorporating Trajectory Preference Evaluation, the method avoids repetitive RL training while ensuring reward functions align with task objectives. On 12 manipulation tasks from Meta-World and ManiSkill2, CARD outperforms or matches baselines and even surpasses human-designed oracle rewards on 3 tasks. The approach balances performance with token efficiency, using fewer queries than iterative baselines while achieving superior task success rates.

## Method Summary
CARD is a framework for designing reward functions for reinforcement learning tasks using Large Language Models (LLMs) to iteratively generate and refine reward function code without human intervention. The framework uses a Coder component to generate reward function code and an Evaluator component to provide dynamic feedback through process feedback, trajectory feedback, and Trajectory Preference Evaluation (TPE). The Coder iteratively improves the reward function based on this feedback without requiring human intervention or parallel LLM queries. The method operates on environment descriptions and task goals provided as Pythonic code, which include details about the robot, objects, and their properties. RL training data (trajectories) and their associated rewards are used for feedback.

## Key Results
- CARD outperforms or matches baselines and even surpasses human-designed oracle rewards on 3 tasks
- Achieves superior task success rates while using fewer LLM queries than iterative baselines
- Successfully eliminates the need for human intervention or parallel LLM queries through dynamic feedback mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coder-Evaluator dynamic feedback loop enables automated reward function improvement without human intervention.
- Mechanism: The Coder generates reward code and the Evaluator provides structured feedback based on RL training results, trajectory analysis, and preference evaluation. This feedback guides the Coder to refine the reward function iteratively.
- Core assumption: LLM can effectively interpret and act on structured feedback to improve reward function code quality.
- Evidence anchors:
  - [abstract]: "CARD includes a Coder that generates and verifies the code, while a Evaluator provides dynamic feedback to guide the Coder in improving the code, eliminating the need for human feedback."
  - [section]: "The Coder iteratively improves the reward function based on the provided feedback."
  - [corpus]: Weak evidence - only 1 neighbor paper discusses automated reward design with LLMs, limited comparison of feedback mechanisms.

### Mechanism 2
- Claim: Trajectory Preference Evaluation (TPE) eliminates repetitive RL training by assessing reward function quality through trajectory preference sorting.
- Mechanism: TPE compares average per-step returns of successful vs. unsuccessful trajectories. If successful trajectories yield higher returns, the reward function is considered order-preserving and effective.
- Core assumption: Trajectory preference ordering correlates with human expectations of task success and reward function quality.
- Evidence anchors:
  - [abstract]: "We introduce Trajectory Preference Evaluation (TPE), which evaluates the current reward function based on trajectory preferences."
  - [section]: "Definition 4.1. Given a set of trajectories... a reward function r(s, a) designed by the Coder is order-preserving if the following condition holds..."
  - [corpus]: Weak evidence - no neighbor papers discuss TPE or preference-based reward evaluation without RL training.

### Mechanism 3
- Claim: Multi-type feedback (process, trajectory, preference) provides comprehensive guidance for reward function improvement.
- Mechanism: Process feedback captures training trends and sub-reward component effectiveness. Trajectory feedback compares successful vs. failed trajectories at step level. Preference feedback evaluates reward function order-preservation without RL training.
- Core assumption: Different feedback types address different aspects of reward function quality and together provide sufficient guidance for improvement.
- Evidence anchors:
  - [abstract]: "In addition to process feedback and trajectory feedback, we introduce Trajectory Preference Evaluation (TPE)"
  - [section]: "We introduce three key steps: (1) Reward Design... (2) Reward Introspection... and (3) Reward Improvement..."
  - [corpus]: Moderate evidence - multiple neighbor papers discuss different feedback mechanisms, but none combine all three types as comprehensively.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation of reinforcement learning
  - Why needed here: CARD operates within standard RL framework, generating reward functions for MDPs
  - Quick check question: What are the five components of an MDP tuple and how does CARD modify the reward function component?

- Concept: Large Language Model code generation and hallucination problems
  - Why needed here: CARD relies on LLM to generate reward function code, which may contain syntax errors or logical inconsistencies
  - Quick check question: How does CARD handle code generation errors differently from parallel sampling approaches like Eureka?

- Concept: Preference-based reinforcement learning and inverse reinforcement learning
  - Why needed here: CARD uses trajectory preference evaluation similar to preference-based RL but without requiring human preference labels
  - Quick check question: How does CARD's TPE differ from traditional preference-based RL in terms of data requirements and evaluation method?

## Architecture Onboarding

- Component map: Coder (LLM-based reward code generation + verification) -> Evaluator (feedback generation + TPE) -> RL training environment -> Trajectory storage system
- Critical path: Code generation → RL training → Feedback generation → Code refinement → (Repeat if needed) → Final reward function
- Design tradeoffs: Token efficiency vs. feedback quality, number of iterations vs. performance improvement, TPE accuracy vs. computational cost
- Failure signatures: Code execution errors, inconsistent feedback patterns, TPE accuracy below threshold, performance degradation after iterations
- First 3 experiments:
  1. Run zero-shot reward generation on simple Meta-World task (e.g., Drawer Open) and measure execution error rate
  2. Test TPE on pre-collected trajectories from oracle reward function to validate preference ordering
  3. Run single iteration of CARD on Handle Press task and compare performance to Text2Reward baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CARD's performance scale with increasingly complex environments that have longer task horizons and more objects?
- Basis in paper: [inferred] The paper demonstrates CARD on 12 manipulation tasks but focuses on relatively short-horizon problems. The authors mention future work on "more complex environments and tasks."
- Why unresolved: The current evaluation is limited to moderate complexity tasks. Scaling to environments with longer episodes, more objects, or higher-dimensional state spaces could reveal limitations in CARD's feedback mechanisms or token efficiency.
- What evidence would resolve it: Comparative experiments on benchmarks with progressively longer horizons (e.g., 1000+ steps), more objects (5+), or higher-dimensional observations, measuring success rates, token consumption, and iteration requirements.

### Open Question 2
- Question: What is the impact of using different LLMs (beyond GPT-4-1106-preview and GPT-3.5-turbo-1106) on CARD's reward function quality and token efficiency?
- Basis in paper: [explicit] The paper compares GPT-4-1106-preview vs GPT-3.5-turbo-1106 and notes that "GPT-4 outperforms GPT-3.5 on all tasks." It also mentions that "the token consumption of our method grows linearly with increasing iterations."
- Why unresolved: Only two LLM variants are tested, and the relationship between model capability, cost, and reward quality is unclear. The linear token growth suggests potential scaling issues.
- What evidence would resolve it: Experiments with a broader range of models (e.g., Claude, Gemini, open-source alternatives) across tasks, measuring success rates, token usage, and iteration counts, plus cost-performance trade-off analysis.

### Open Question 3
- Question: How does CARD's Trajectory Preference Evaluation perform when successful and failed trajectories have overlapping or ambiguous characteristics?
- Basis in paper: [explicit] The paper introduces TPE to avoid RL training but defines order-preserving rewards based on clear distinctions between successful and failed trajectories. It assumes "successful trajectories tend to be shorter than unsuccessful ones."
- Why unresolved: Real-world tasks may have trajectories where success/failure is not binary or where successful and failed trajectories share similar state-action patterns, making preference evaluation ambiguous.
- What evidence would resolve it: Experiments on tasks with noisy success criteria, partial successes, or continuous performance metrics, measuring TPE accuracy and whether it still reduces RL training iterations effectively.

## Limitations

- The Trajectory Preference Evaluation mechanism's effectiveness depends heavily on having sufficient high-quality trajectory data from successful and unsuccessful episodes
- The framework's reliance on LLM-generated code introduces potential for syntax errors and logical inconsistencies that may require multiple regeneration attempts
- CARD's performance on tasks with longer horizons or more complex reward structures remains untested and may face scalability challenges

## Confidence

- Method novelty: Medium - builds on existing LLM-based reward design but introduces unique feedback mechanisms
- Empirical claims: Medium - strong results on tested tasks but limited to moderate complexity problems
- Theoretical claims: Low - limited theoretical analysis of feedback mechanism convergence or TPE effectiveness bounds
- Reproducibility: Medium - core components described but some implementation details (prompt structure, TPE thresholds) are unclear

## Next Checks

1. Verify TPE accuracy on pre-collected trajectories from oracle reward functions to validate preference ordering mechanism
2. Test zero-shot reward generation on simple tasks to establish baseline execution error rates
3. Implement and test process feedback mechanism to identify ineffective sub-reward components before RL training