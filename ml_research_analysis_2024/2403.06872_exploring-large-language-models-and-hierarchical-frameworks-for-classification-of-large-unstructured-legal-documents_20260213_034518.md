---
ver: rpa2
title: Exploring Large Language Models and Hierarchical Frameworks for Classification
  of Large Unstructured Legal Documents
arxiv_id: '2403.06872'
source_url: https://arxiv.org/abs/2403.06872
tags:
- legal
- documents
- https
- language
- mesc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors develop a hierarchical multi-stage framework (MESc)
  to classify long unstructured legal documents without structural annotations. The
  approach divides documents into chunks, extracts embeddings from the last four layers
  of fine-tuned large language models, applies unsupervised clustering to approximate
  document structure, and uses transformer encoders for final classification.
---

# Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents

## Quick Facts
- arXiv ID: 2403.06872
- Source URL: https://arxiv.org/abs/2403.06872
- Reference count: 40
- Authors: Nishchal Prasad; Mohand Boughanem; Taoufiq Dkaki
- Primary result: Hierarchical framework (MESc) improves legal document classification accuracy by ~2 points over state-of-the-art methods

## Executive Summary
This paper introduces MESc, a hierarchical multi-stage framework for classifying long unstructured legal documents without requiring structural annotations. The approach divides documents into chunks, extracts embeddings from the last four layers of fine-tuned large language models, applies unsupervised clustering to approximate document structure, and uses transformer encoders for final classification. Experiments on Indian, EU, and US legal datasets demonstrate that combining multi-layer embeddings with structure information significantly improves performance over previous state-of-the-art methods, achieving a minimum total gain of approximately 2 points in classification accuracy.

## Method Summary
The method employs a hierarchical approach where legal documents are first split into overlapping chunks, then embeddings are extracted from the last four layers of a fine-tuned LLM. These embeddings undergo dimensionality reduction and clustering to approximate document structure. A transformer encoder processes the structured embeddings through multiple layers, and a feed-forward classifier produces the final prediction. The framework is specifically designed for long, unstructured legal documents where important information is unknown and the full document exceeds LLM token limits.

## Key Results
- MESc achieves minimum total performance gain of approximately 2 points over previous state-of-the-art methods
- Combining embeddings from last four LLM layers with structure information significantly improves classification accuracy
- Method is particularly effective for longer documents where important information is unknown, outperforming standalone LLMs in these scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical clustering of chunk embeddings approximates document structure without manual annotation
- Mechanism: Chunk embeddings are extracted from the last four layers of a fine-tuned LLM and clustered using HDBSCAN after dimensionality reduction with pUMAP. The resulting cluster labels act as proxies for structural roles across documents
- Core assumption: Similar parts of different legal documents produce embeddings that cluster together in the same latent space
- Evidence anchors: Abstract statement about approximating structure through unsupervised clustering; section explanation of clustering similar document parts together
- Break condition: If chunk embeddings do not capture structural semantics or clustering fails to separate structurally similar segments

### Mechanism 2
- Claim: Combining embeddings from the last four layers of the LLM yields richer features than using only the final layer
- Mechanism: The model concatenates embeddings from multiple transformer layers, exploiting complementary semantic information learned at different depths
- Core assumption: Each layer encodes different aspects of meaning, and their combination provides a more complete representation
- Evidence anchors: Abstract mention of using last four layers; section explanation of varied representations from different layers
- Break condition: If layer embeddings are highly redundant or concatenation harms optimization

### Mechanism 3
- Claim: The hierarchical encoder outperforms flat LLM baselines on long, unstructured legal documents
- Mechanism: Documents are split into overlapping chunks, each encoded and processed through additional transformer layers that model inter-chunk relationships
- Core assumption: Important information in legal documents is not uniformly distributed, and fragmenting preserves necessary context
- Evidence anchors: Abstract claim of 2-point performance gain; section observation about most documents fitting in few chunks
- Break condition: If document fits entirely within LLM's max input length, MESc offers no advantage

## Foundational Learning

- Concept: Hierarchical text classification
  - Why needed here: Legal documents exceed LLM token limits; splitting into chunks and reassembling allows full-document classification
  - Quick check question: How does MESc ensure that chunk order is preserved during classification?

- Concept: Multi-layer transformer embeddings
  - Why needed here: Different layers capture different semantic granularities; combining them enriches feature representation
  - Quick check question: What would happen if only the last layer embedding were used?

- Concept: Unsupervised structure approximation via clustering
  - Why needed here: Legal documents lack structural annotations; clustering provides implicit structure labels
  - Quick check question: How might noise in the embedding space affect clustering quality?

## Architecture Onboarding

- Component map: Tokenizer -> Chunk splitter (with overlap) -> LLM fine-tuning -> Embedding extraction (last 4 layers) -> Dimensionality reduction (pUMAP) -> Clustering (HDBSCAN) -> Transformer encoder layers -> Max pooling -> Feed-forward classifier -> Output layer
- Critical path: Fine-tune LLM -> Extract embeddings -> Cluster -> Encode inter-chunk relations -> Classify
- Design tradeoffs:
  - Longer chunk size preserves context but risks exceeding memory limits
  - More clustering clusters improve structure granularity but risk over-segmentation
  - Extra transformer encoder layers improve modeling but risk overfitting on small datasets
- Failure signatures:
  - Clustering produces many singleton clusters -> poor structure approximation
  - Validation loss spikes after few epochs -> overfitting in encoder layers
  - Performance drops when switching from last layer only to multi-layer concatenation -> feature redundancy or optimization issues
- First 3 experiments:
  1. Train MESc with p=1 (last layer only) and t=1 encoder layer; measure baseline
  2. Add clustering structure labels and re-evaluate; observe change
  3. Increase p to 4 layers, keep t=1; compare with step 2 to isolate layer concatenation effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MESc framework's performance compare when applied to legal documents in languages other than English?
- Basis in paper: The paper tests MESc on English-language legal datasets but does not explore performance on other languages
- Why unresolved: The authors do not provide data on the framework's cross-linguistic applicability
- What evidence would resolve it: Testing MESc on non-English legal corpora (e.g., French, German, Chinese) and comparing results to the English datasets

### Open Question 2
- Question: What is the optimal number of transformer encoder layers (t) in MESc for different document lengths and classification tasks?
- Basis in paper: The authors state "t ≥ 3 sometimes overfits the model in our experiments, hence we fix t = 2 for MESc" but do not explore the full range of t values
- Why unresolved: The paper does not systematically investigate how varying t affects performance across different document lengths and task complexities
- What evidence would resolve it: Conducting experiments with varying t values (e.g., t=1,2,3,4) across different document lengths and classification tasks

### Open Question 3
- Question: How does the quality of structure approximation through clustering affect the final classification performance in MESc?
- Basis in paper: The authors use HDBSCAN clustering on chunk embeddings to approximate document structure, but do not analyze the quality of this approximation
- Why unresolved: The relationship between clustering quality and classification accuracy is not explored
- What evidence would resolve it: Measuring clustering metrics (e.g., silhouette score, Davies-Bouldin index) and correlating them with classification performance

## Limitations
- Structure approximation mechanism relies on clustering performance without direct validation of cluster semantic coherence across domains
- Method's effectiveness for short documents is not empirically validated despite theoretical arguments
- Specific hyperparameters for dimensionality reduction and clustering remain unspecified, making exact reproduction challenging

## Confidence
- High confidence: The hierarchical framework architecture and general training procedure are clearly specified and reproducible
- Medium confidence: The empirical performance improvements over state-of-the-art methods are demonstrated, but the attribution to specific mechanisms is not fully isolated
- Low confidence: The semantic validity of unsupervised structure approximation across different legal domains, and the specific contribution of each architectural component to the observed performance gains

## Next Checks
1. **Structure Approximation Validation**: Run MESc on a subset of documents where manual structural annotations are available, then measure the alignment between automatically discovered clusters and ground-truth rhetorical roles using standard clustering evaluation metrics

2. **Layer Ablation Study**: Systematically test MESc performance using only single transformer layers (layer 1, layer 2, layer 3, layer 4) versus the multi-layer concatenation approach, across all three legal domains, to quantify the specific contribution of layer combination

3. **Document Length Sensitivity Analysis**: Create stratified test sets of short (≤2048 tokens), medium (2048-6144 tokens), and long (>6144 tokens) documents, then measure MESc performance degradation/improvement across these sets compared to a flat LLM baseline to validate the claimed advantage for longer documents