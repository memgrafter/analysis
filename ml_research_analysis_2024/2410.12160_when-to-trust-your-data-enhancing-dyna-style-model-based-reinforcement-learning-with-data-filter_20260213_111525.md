---
ver: rpa2
title: 'When to Trust Your Data: Enhancing Dyna-Style Model-Based Reinforcement Learning
  With Data Filter'
arxiv_id: '2410.12160'
source_url: https://arxiv.org/abs/2410.12160
tags:
- data
- filter
- algorithm
- estimated
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of Dyna-style model-based
  reinforcement learning algorithms when the estimated environmental model is inaccurate.
  The core method introduces an out-of-distribution (OOD) data filter that removes
  simulated data points significantly diverging from real environment data before
  using them in model-free training.
---

# When to Trust Your Data: Enhancing Dyna-Style Model-Based Reinforcement Learning With Data Filter

## Quick Facts
- arXiv ID: 2410.12160
- Source URL: https://arxiv.org/abs/2410.12160
- Authors: Yansong Li; Zeyu Dong; Ertai Luo; Yu Wu; Shuo Wu; Shuo Han
- Reference count: 23
- Primary result: Integrating an OOD data filter into MBPO achieves state-of-the-art performance with fewer environment interactions, even without model ensembles

## Executive Summary
This paper addresses the inefficiency of Dyna-style model-based reinforcement learning when the estimated environmental model is inaccurate. The authors introduce an out-of-distribution (OOD) data filter that removes simulated data points significantly diverging from real environment data before using them in model-free training. By filtering out-of-distribution simulated data based on distance to real data, the approach reduces model bias and improves the quality of model-free updates. The primary result shows that this filtering technique achieves state-of-the-art performance on MuJoCo benchmarks, requiring fewer interactions with the real environment compared to standard MBPO, even without using model ensembles.

## Method Summary
The method integrates an OOD data filter into the MBPO algorithm to enhance the quality of simulated data used for model-free training. The filter identifies and removes simulated state-action pairs whose distance to the nearest real state-action pair exceeds a threshold ϵk, using HNSW for efficient nearest-neighbor search. Two strategies are proposed for setting ϵk: static rejection (fixed threshold) and dynamic rejection (threshold increases linearly with training episodes). The filtered dataset Dreduct is then used alongside real data Dreal to update the policy and Q-network. Theoretical analysis bounds the error propagation through the filtering process under assumptions of Lipschitz continuity and small transition kernel variance.

## Key Results
- The OOD data filter reduces the impact of model bias by ensuring simulated data better mimics real data
- Theorem 4.3 bounds state shifting by ∥(ˆs, ˆa)⊤ − (s, a)⊤∥, showing filtering controls this distance
- Proposition 4.7 bounds ∥ˆθt+1 − θt+1∥ by C1∥d − ˆd∥2 + C2∥d − ˆd∥, with distance terms dominating when variances are small

## Why This Works (Mechanism)

### Mechanism 1
Filtering simulated data that is far from real data reduces the drift in rollout trajectories, improving model-free training efficiency. The OOD data filter identifies and removes simulated state-action pairs whose distance to the nearest real state-action pair exceeds a threshold ϵk. By eliminating these out-of-distribution points, the filtered dataset Dreduct better approximates Dreal in terms of Q-network updates, reducing model bias. Core assumption: The real transition kernel has small variance, and the distance between real and simulated data points is the dominant factor in trajectory drift.

### Mechanism 2
The OOD data filter reduces the parametric drift in Q-network updates by limiting the distance between real and simulated data points. Theorem 4.6 shows that the difference between Q-network parameters updated using Dreduct versus Dreal is bounded by the squared and linear terms of the distance between data points. By filtering out distant points, the OOD filter keeps Dreduct close to Dreal in Q-update space. Core assumption: The Q-network and reward function are Lipschitz continuous, and the transition kernel variance is small.

### Mechanism 3
Dynamic adjustment of the rejection threshold ϵk over training episodes balances exploration and filtering effectiveness. Early in training, ϵk is set to reject all but the first step of simulated rollouts, ensuring only reliable data is used. As the estimated model improves, ϵk increases linearly, allowing longer rollouts to be accepted, thus leveraging improved model accuracy. Core assumption: The estimated model accuracy improves monotonically with training, so increasing ϵk is safe.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and transition kernels
  - Why needed here: The paper models environment dynamics as Gaussian transition kernels and uses these to generate simulated data for training.
  - Quick check question: In an MDP with continuous states and actions, how is the next state distribution typically parameterized in Dyna-style algorithms?

- Concept: Lipschitz continuity and its role in bounding errors
  - Why needed here: Assumptions 4.1, 4.4, and 4.5 use Lipschitz conditions to bound the error propagation from model inaccuracies and data shifts.
  - Quick check question: Why does assuming the Q-network gradient is Lipschitz help in bounding the difference between parameters updated from real versus simulated data?

- Concept: Out-of-distribution (OOD) detection in reinforcement learning
  - Why needed here: The OOD data filter is a form of OOD detection applied to state-action pairs to filter simulated data that deviates too far from real data.
  - Quick check question: How does the nearest-neighbor search with HNSW enable efficient OOD detection in high-dimensional state spaces?

## Architecture Onboarding

- Component map: Environment interaction -> Model estimator -> Rollout generator -> OOD filter -> Model-free trainer
- Critical path:
  1. Collect real transition data (s, a, s′)
  2. Update the estimated model
  3. Generate simulated rollouts to create Dest
  4. Apply OOD filter to obtain Dreduct
  5. Train the model-free algorithm on Dreal ∪ Dreduct
- Design tradeoffs:
  - Rollout length L vs. filtering threshold ϵk: Longer rollouts increase exploration but require stricter filtering to control drift
  - Static vs. dynamic ϵk: Static is simpler but may be too conservative early or too permissive late; dynamic adapts but adds tuning complexity
  - Model ensemble vs. single model with filtering: Ensembles reduce variance but increase compute; filtering with a single model is cheaper but depends on effective thresholding
- Failure signatures:
  - If filtering removes too much data, model-free training slows and performance degrades
  - If filtering is too permissive, model bias propagates and destabilizes learning
  - If HNSW index is stale, nearest-neighbor queries become inaccurate, breaking the OOD filter
- First 3 experiments:
  1. Validate OOD filter on InvertedPendulum-V2 with static ϵk=5, comparing reward curves to baseline MBPO
  2. Test dynamic ϵk strategy on Walker2d-V2, confirming fewer interactions needed for same performance
  3. Run HalfCheetah-V2 without model ensemble, verifying state-of-the-art performance with filtering

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the OOD data filter change when using different types of model estimators (e.g., Gaussian processes vs neural networks) in the MBPO framework? The paper only tests one type of model estimator, and different estimators may have varying levels of accuracy and robustness, affecting the OOD data filter's performance.

### Open Question 2
What is the impact of the OOD data filter on exploration efficiency in environments with sparse rewards or complex dynamics? The paper focuses on dense reward environments (MuJoCo), and the OOD filter's effectiveness in environments with sparse rewards or complex dynamics is not explored.

### Open Question 3
How sensitive is the OOD data filter's performance to the choice of rejection level (ϵk) in different environments or training stages? The paper discusses two strategies for choosing ϵk (static and dynamic), but does not provide a systematic analysis of its sensitivity.

### Open Question 4
Can the OOD data filter be extended to handle multi-modal state distributions or non-Gaussian transitions in continuous control tasks? The theoretical analysis assumes Gaussian transitions, but real-world environments may have multi-modal or non-Gaussian dynamics.

## Limitations

- The paper does not provide a principled method for setting the rejection threshold ϵk beyond a linear increase strategy
- All experiments are conducted on standard MuJoCo benchmark tasks, limiting generalization to other domains
- Critical implementation details of the HNSW nearest neighbor search are not specified

## Confidence

- Performance Improvement Claim: Medium Confidence - Supported by experimental results but lacks comparison to recent methods and sensitivity analysis
- Theoretical Guarantees: Medium Confidence - Bounds rely on assumptions that may not hold in practice
- OOD Filtering Mechanism: High Confidence - Well-founded mechanism with consistent experimental support

## Next Checks

1. Conduct threshold sensitivity analysis by varying ϵk across multiple orders of magnitude on each environment
2. Design experiments with artificially increased model variance to test filtering effectiveness when core assumptions are violated
3. Apply the OOD filtering approach to non-MuJoCo environments such as Atari games or robotic manipulation tasks