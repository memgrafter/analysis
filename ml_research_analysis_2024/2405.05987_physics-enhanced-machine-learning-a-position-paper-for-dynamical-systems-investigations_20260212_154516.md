---
ver: rpa2
title: 'Physics-Enhanced Machine Learning: a position paper for dynamical systems
  investigations'
arxiv_id: '2405.05987'
source_url: https://arxiv.org/abs/2405.05987
tags:
- data
- learning
- dynamical
- systems
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses challenges in applying machine learning to
  dynamical systems in engineering, particularly around limited data, avoiding physically
  inconsistent predictions, handling uncertainty, and ensuring explainability. It
  introduces a framework for Physics-Enhanced Machine Learning (PEML) that integrates
  physics-based knowledge with data-driven approaches through four types of biases:
  observational, learning, inductive, and model form/discrepancy.'
---

# Physics-Enhanced Machine Learning: a position paper for dynamical systems investigations

## Quick Facts
- arXiv ID: 2405.05987
- Source URL: https://arxiv.org/abs/2405.05987
- Reference count: 35
- One-line primary result: Introduces a framework for Physics-Enhanced Machine Learning (PEML) that integrates physics-based knowledge with data-driven approaches through four types of biases to improve generalization, uncertainty quantification, and interpretability in dynamical systems.

## Executive Summary
This position paper addresses fundamental challenges in applying machine learning to dynamical systems in engineering, particularly when dealing with limited data, ensuring physical consistency, handling uncertainty, and maintaining interpretability. The paper introduces a comprehensive framework for Physics-Enhanced Machine Learning (PEML) that strategically integrates physics-based knowledge with data-driven approaches through four distinct types of biases: observational, learning, inductive, and model form/discrepancy. By recognizing that different engineering problems require different hybrid approaches, the framework provides a systematic way to choose between Physics-Informed, Physics-Guided, and Physics-Encoded strategies based on available information, user purposes, and system complexity.

## Method Summary
The PEML framework integrates physics-based knowledge with data-driven machine learning through four types of biases that constrain the solution space explored by ML algorithms. Observational biases involve using synthetic data from physics-based models to augment limited measurements, learning biases incorporate physics constraints into the training process (such as conservation laws), inductive biases encode physical principles into model architecture, and model form/discrepancy biases explicitly account for differences between assumed physics models and reality. The framework distinguishes three PEML strategies: Physics-Informed approaches use data-driven models with physics constraints, Physics-Guided approaches use physics-based models enhanced with data, and Physics-Encoded approaches combine both through hybrid architectures. This systematic approach enables better generalization when data is limited, improved uncertainty quantification by distinguishing aleatory and epistemic uncertainty sources, and more interpretable decision-making in complex engineering systems.

## Key Results
- PEML framework successfully addresses challenges of limited data, physically inconsistent predictions, uncertainty handling, and explainability in dynamical systems
- Four bias types (observational, learning, inductive, model form/discrepancy) provide systematic ways to integrate physics knowledge into ML models
- Three PEML strategies offer flexible approaches for different data availability and system complexity scenarios
- Framework enables improved generalization, uncertainty quantification, and interpretable decision-making in engineering applications

## Why This Works (Mechanism)

### Mechanism 1
Introducing physics and domain knowledge biases reduces the search space for ML models, improving generalization when data is limited. The framework proposes four types of biases (observational, learning, inductive, model form/discrepancy) that constrain the solution space explored by ML algorithms. By incorporating physical laws, conservation principles, or partial knowledge of governing equations, these biases guide the learning process toward physically consistent solutions rather than arbitrary correlations in limited data. The core assumption is that the underlying physics of the dynamical system is partially known and can be effectively encoded as constraints or priors in the ML architecture. This breaks down if the assumed physics is incorrect or incomplete, potentially restricting the solution space to exclude the true model.

### Mechanism 2
Hybrid physics-data models can better handle uncertainty by distinguishing between aleatory (inherent variability) and epistemic (lack of knowledge) uncertainty sources. The framework distinguishes between data-driven error (model form and parameter uncertainty) and physics-based error (model assumptions and parameter uncertainty). By explicitly modeling these different sources of uncertainty through probabilistic frameworks, hybrid models can provide more reliable uncertainty quantification and avoid overconfident predictions. The core assumption is that uncertainty can be meaningfully decomposed into reducible (epistemic) and irreducible (aleatory) components that can be separately quantified. This mechanism fails if uncertainty sources are misclassified or if the distinction between aleatory and epistemic uncertainty doesn't hold for the specific application.

### Mechanism 3
Different PEML strategies (physics-guided, physics-informed, physics-encoded) are optimal for different data availability and system complexity scenarios. The framework proposes that the choice of PEML strategy depends on three factors: available information (amount of physics knowledge vs. data), user purpose (specific task requirements), and system complexity. This allows practitioners to select the most appropriate hybrid approach rather than applying a one-size-fits-all solution. The core assumption is that the relationship between data availability, physics knowledge, and system complexity can be assessed to guide strategy selection. This breaks down if the assessment of available information or system complexity is inaccurate, potentially leading to suboptimal strategy selection.

## Foundational Learning

- Concept: Dynamical systems modeling
  - Why needed here: The paper focuses specifically on applying ML to dynamical systems in engineering, which have time-varying, potentially nonlinear behaviors that require specialized treatment
  - Quick check question: What distinguishes a dynamical system from a static system in terms of input-output relationships?

- Concept: Uncertainty quantification
  - Why needed here: The paper emphasizes dealing with uncertainty as a core challenge, distinguishing between aleatory and epistemic uncertainty sources
  - Quick check question: How would you explain the difference between aleatory and epistemic uncertainty to a non-technical stakeholder?

- Concept: Bayesian inference
  - Why needed here: The paper references probabilistic model updating strategies and Bayesian calibration as foundational techniques for hybrid physics-data modeling
  - Quick check question: In Bayesian inference, what role does the prior distribution play when updating model parameters with observed data?

## Architecture Onboarding

- Component map: The PEML framework consists of four bias types (observational, learning, inductive, model form/discrepancy) that can be integrated into ML architectures. The three PEML strategies (physics-guided, physics-informed, physics-encoded) represent different ways of combining physics-based and data-driven components. Core components include the physics model, data sources, bias implementation mechanisms, and uncertainty quantification modules.

- Critical path: 1) Assess available information (physics knowledge vs. data), 2) Choose appropriate PEML strategy, 3) Implement relevant biases into the ML architecture, 4) Validate model performance on unseen data, 5) Quantify and communicate uncertainty.

- Design tradeoffs: Physics-informed approaches require less data but may be limited by the accuracy of the physics model; physics-guided approaches provide strong physical consistency but may struggle with unmodeled phenomena; physics-encoded approaches offer flexibility but require careful balancing of physics and data components. More complex bias structures improve physical consistency but increase computational cost and implementation complexity.

- Failure signatures: Poor generalization to unseen conditions suggests inadequate bias specification or overfitting; physically inconsistent predictions indicate bias conflicts or incorrect physics assumptions; overconfident uncertainty estimates suggest insufficient consideration of irreducible uncertainty sources.

- First 3 experiments:
  1. Implement a simple physics-informed neural network with conservation law constraints on a benchmark dynamical system with limited data to verify basic bias integration
  2. Compare physics-guided vs. physics-informed approaches on the same system to understand tradeoff between data requirements and physical consistency
  3. Test uncertainty quantification by introducing controlled noise and observing whether the model correctly captures increased uncertainty in predictions

## Open Questions the Paper Calls Out

### Open Question 1
What metrics can reliably assess prediction quality and generalization performance of different PEML algorithms on specific tasks? The paper explicitly identifies the need for metrics, benchmarks, and validation strategies to rate PEML algorithm performance on specific tasks, noting that different PEML algorithms might return the same performance on training data but differ significantly on unseen data. This remains unresolved because the paper acknowledges this as an open challenge but does not propose specific metrics or validation frameworks. Current evaluation methods may not adequately capture generalization ability or performance on previously unseen conditions. Development and validation of benchmark datasets with controlled complexity, comprehensive metrics that capture both accuracy and generalization, and standardized testing protocols that compare different PEML approaches under consistent conditions would resolve this question.

### Open Question 2
How can automatic error detection and correction be implemented across data, physics/domain knowledge biases, and PEML model architecture? The paper explicitly identifies the need for automatic identification and correction of errors in data, physics/domain knowledge biases, and chosen PEML model architecture, including detecting wrong modelling assumptions, corrupted data, non-informative data, and architectural issues. This remains unresolved because current PEML implementations rely heavily on manual validation and expert oversight. There is no systematic framework for automatically detecting when biases are incorrect or when the data-driven component incorrectly overrides the physics-based component. Development of automated diagnostic tools that can identify specific types of errors, algorithms that can suggest corrections or flag problematic regions of the solution space, and validation frameworks that test these automated systems on complex dynamical systems with known ground truth would resolve this question.

### Open Question 3
What scalable solutions exist for complex nonlinear time-varying dynamical systems involving multi-scale, multi-physics interactions? The paper explicitly identifies the need for scalable solutions for complex problems, specifically mentioning mapping to low-dimensional spaces, high-precision learning from small datasets, dealing with non-smooth nonlinear phenomena, and efficiently quantifying uncertainties for multi-scale, multi-physics, nonlinear time-varying systems. This remains unresolved because current PEML approaches struggle with systems that exhibit discontinuous nonlinearities, time-varying behavior, or complex multi-physics interactions. The paper notes that tackling this complexity is "beyond state-of-the-art PEML strategies." Demonstrated PEML implementations that successfully handle real-world engineering systems with discontinuous nonlinearities (like friction), time-varying parameters, and coupled physics phenomena, along with validation against experimental data showing improved performance over traditional approaches, would resolve this question.

## Limitations
- Framework remains conceptual without specific algorithmic implementations or empirical validation
- Scalability to highly nonlinear, time-varying systems with multi-scale, multi-physics interactions is unproven
- Validation metrics and benchmarks for comparing PEML strategies are not well-established
- Computational complexity of hybrid approaches may limit real-time applications

## Confidence

- **High confidence**: The identification of core challenges in applying ML to dynamical systems (limited data, physical inconsistency, uncertainty, explainability) is well-supported by literature and industry experience.
- **Medium confidence**: The categorization of four bias types and three PEML strategies provides a useful conceptual framework, though practical implementation details are sparse.
- **Low confidence**: The framework's effectiveness in real-world engineering applications without extensive validation and case studies.

## Next Checks

1. Implement a benchmark study comparing all three PEML strategies (Physics-Informed, Physics-Guided, Physics-Encoded) on a standard dynamical system with controlled data availability to quantify the claimed tradeoffs.

2. Develop a systematic methodology for validating physics consistency in PEML predictions, including metrics that can automatically detect when physics biases are incorrectly specified.

3. Create a case study applying PEML to a real engineering system (e.g., structural dynamics, fluid mechanics) where both high-fidelity simulations and limited experimental data are available, measuring generalization to operating conditions not seen during training.