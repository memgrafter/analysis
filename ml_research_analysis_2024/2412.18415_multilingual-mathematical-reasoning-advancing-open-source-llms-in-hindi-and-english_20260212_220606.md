---
ver: rpa2
title: 'Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and
  English'
arxiv_id: '2412.18415'
source_url: https://arxiv.org/abs/2412.18415
tags:
- hindi
- dataset
- easy
- medium
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study advances mathematical reasoning in open-source large
  language models for both Hindi and English. It introduces a Decomposition Strategy
  for complex arithmetic, a Structured Solution Design with Curriculum Learning, and
  Bilingual Combined Training.
---

# Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English

## Quick Facts
- arXiv ID: 2412.18415
- Source URL: https://arxiv.org/abs/2412.18415
- Reference count: 40
- Key outcome: Open-source models improved by +6% on English datasets vs. Gemini Pro, matching Gemini on Hindi datasets

## Executive Summary
This study introduces a decomposition strategy for complex arithmetic, structured solution design with curriculum learning, and bilingual combined training to enhance mathematical reasoning in open-source LLMs for both Hindi and English. The approach achieves notable performance gains, with WizardMath 7B outperforming Gemini Pro by +6% on English datasets and matching Gemini on Hindi datasets. Bilingual training yields results comparable to individual language models, demonstrating effective cross-lingual mathematical reasoning.

## Method Summary
The study employs a three-pronged approach: (1) Decomposition Strategy breaks down complex arithmetic into manageable components using place-value operations, (2) Curriculum Learning progressively trains models on increasingly difficult problems, and (3) Bilingual Combined Training exposes models to parallel English-Hindi question-answer pairs. Models are fine-tuned on curated datasets (Enhanced HAWP, IndiMathQA, GSM8K, MATH, PRM800K) using supervised fine-tuning with structured solutions following defined phases (Data Identification, Problem Analysis, Theoretical Framework, Methodology Development, Computation, Solution).

## Key Results
- WizardMath 7B outperforms Gemini Pro by +6% on English datasets
- Bilingual training achieves results comparable to individual language models
- Models show improved performance on medium and hard problems after curriculum learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposition Strategy improves arithmetic accuracy by breaking large numbers into place-value components
- Mechanism: Multiplicand and dividend are split into place values, each part is operated on separately, then partial results are summed
- Core assumption: LLMs struggle with direct computation on large numbers but can handle smaller sub-calculations reliably
- Evidence anchors:
  - [abstract] Introduces Decomposition Strategy to simplify complex arithmetic operations
  - [section] "For multiplication, this involves breaking down the multiplicand into place value components—such as hundreds, tens, and ones—and multiplying each by the other multiplicand"
- Break condition: If LLM still hallucinates during sub-calculation or loses place value context, overall accuracy may not improve

### Mechanism 2
- Claim: Curriculum Learning improves reasoning by incrementally increasing problem complexity during fine-tuning
- Mechanism: Models are first trained on easy problems, then fine-tuned on medium problems, simulating human learning progression
- Core assumption: Gradual complexity increase helps the model internalize basic concepts before tackling harder problems
- Evidence anchors:
  - [abstract] "incorporates curriculum learning, progressively training models on increasingly difficult problems"
  - [section] "We apply the technique of Curriculum Learning to SLLMs, hypothesizing that by incrementally increasing the complexity of problems during fine-tuning..."
- Break condition: If the model overfits to easy problems or fails to transfer learned patterns to harder tasks

### Mechanism 3
- Claim: Bilingual Combined Training leverages stronger language performance to improve weaker language reasoning
- Mechanism: Training on parallel English-Hindi question-answer pairs allows English reasoning strengths to transfer to Hindi tasks
- Core assumption: Mathematical reasoning is language-independent, so learning in English helps Hindi reasoning
- Evidence anchors:
  - [abstract] "Adopting a bilingual approach that combines English and Hindi samples achieves results comparable to individual language models"
  - [section] "Our reasoning is grounded in the idea that by exposing the LLM to parallel data in English, a language it excels in..."
- Break condition: If language-specific nuances in Hindi math problems are too distinct from English, transfer may be ineffective

## Foundational Learning

- Concept: Place-value decomposition in arithmetic
  - Why needed here: Enables accurate computation of large-number multiplication and division
  - Quick check question: Given 543 x 27, can you decompose 543 into place values and compute partial products?

- Concept: Conditional probability and combinatorial counting
  - Why needed here: Required for correctly solving probability problems like "both children are boys given at least one is a boy"
  - Quick check question: What is the probability of drawing two red cards from a standard deck without replacement?

- Concept: Curriculum learning progression
  - Why needed here: Ensures foundational math skills are mastered before advancing to harder problems
  - Quick check question: If a student masters algebra basics, can they reliably solve quadratic equations?

## Architecture Onboarding

- Component map: Raw questions → Structured solutions → Curriculum batches (Easy → Medium)
- Training loop: Base model → SFT Easy → SFT Easy+Medium → Evaluation
- Critical path: Data preparation → Structured solution generation → Curriculum-based fine-tuning → Bilingual evaluation
- Design tradeoffs:
  - Fine-tune on small, curated datasets vs. larger but noisier ones
  - Structured solutions add overhead but reduce hallucinations
  - Bilingual training doubles data but may dilute language-specific performance
- Failure signatures:
  - Performance plateau on hard problems after curriculum training
  - Accuracy drop in one language after bilingual training
  - Model overfitting to curriculum sequence without generalization
- First 3 experiments:
  1. Evaluate base model on Easy problems → check baseline accuracy
  2. Apply Decomposition Strategy on HAWP → measure arithmetic gains
  3. Fine-tune on Easy set only → compare vs. base model on Easy problems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Decomposition Strategy generalize effectively to more complex datasets beyond HAWP, such as those involving multi-step or abstract reasoning problems?
- Basis in paper: [inferred] The paper mentions the Decomposition Strategy was tested only on HAWP, which contains single-operator word problems, and suggests exploring its application to more complex datasets as a future direction
- Why unresolved: The current study does not evaluate the strategy on datasets requiring multi-step reasoning or abstract problem-solving, limiting conclusions about its broader applicability
- What evidence would resolve it: Testing the Decomposition Strategy on diverse, complex datasets (e.g., MATH or PRM800K) and comparing performance improvements across problem types would clarify its generalizability

### Open Question 2
- Question: How does bilingual combined training impact the model’s ability to handle code-mixed inputs, such as Romanized Hindi or mixed-language queries?
- Basis in paper: [explicit] The paper acknowledges that it did not evaluate the model’s performance on Romanized Hindi inputs, which are common in India
- Why unresolved: The current study focuses on pure Hindi and English inputs, leaving the model’s robustness to code-mixed or Romanized inputs unexplored
- What evidence would resolve it: Evaluating model performance on code-mixed datasets and comparing results with pure-language inputs would demonstrate its adaptability to real-world usage scenarios

### Open Question 3
- Question: What is the long-term effect of Curriculum Learning on model performance as dataset size increases, and how does it scale with more complex or diverse topics?
- Basis in paper: [inferred] The paper shows Curriculum Learning improves performance on medium and hard problems in the current dataset, but does not explore scalability or long-term effects with larger or more diverse datasets
- Why unresolved: The study’s dataset is limited in size and scope, and the impact of Curriculum Learning on larger, more complex datasets remains untested
- What evidence would resolve it: Conducting experiments with significantly larger datasets and tracking performance trends over extended training periods would reveal the scalability and sustainability of Curriculum Learning benefits

## Limitations

- Decomposition Strategy effectiveness is currently limited to arithmetic operations, not tested on geometry or abstract mathematics
- Hindi mathematical reasoning evaluation relies on a limited dataset with potential cultural and educational bias
- Study focuses on relatively small 7B parameter models, scalability to larger models is unknown

## Confidence

- **High confidence**: Performance improvements on English datasets (+6% over Gemini Pro), bilingual training achieving parity with single-language models
- **Medium confidence**: Generalization of decomposition strategy to other mathematical domains, effectiveness of curriculum learning for long-term retention
- **Low confidence**: Transfer of English reasoning strengths to Hindi without domain-specific Hindi mathematical training data

## Next Checks

1. Test the decomposition strategy on non-arithmetic mathematical domains (geometry, algebra) to assess generalizability
2. Implement interleaved curriculum learning and compare retention versus the current sequential approach
3. Conduct cross-validation with larger Hindi mathematical datasets from different educational contexts to verify cultural robustness of results