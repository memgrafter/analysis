---
ver: rpa2
title: 'Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of the
  Noisy Channel'
arxiv_id: '2404.15219'
source_url: https://arxiv.org/abs/2404.15219
tags:
- dialogue
- system
- user
- acts
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first approach to build an end-to-end
  task-oriented dialogue system using only unlabeled dialogues and an API schema,
  without any turn-level annotations. The method leverages large language models and
  expectation-maximization to infer missing labels as latent variables, employing
  a noisy-channel prompting technique for improved pseudo-label quality.
---

# Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of the Noisy Channel

## Quick Facts
- arXiv ID: 2404.15219
- Source URL: https://arxiv.org/abs/2404.15219
- Reference count: 40
- This paper introduces the first approach to build an end-to-end task-oriented dialogue system using only unlabeled dialogues and an API schema, without any turn-level annotations.

## Executive Summary
This paper presents the first approach to build an end-to-end task-oriented dialogue system using only unlabeled dialogues and an API schema, without any turn-level annotations. The method leverages large language models and expectation-maximization to infer missing labels as latent variables, employing a noisy-channel prompting technique for improved pseudo-label quality. On the MultiWOZ benchmark, the proposed approach more than doubles the dialogue success rate of a strong GPT-3.5 baseline, achieving 68.3% success rate compared to 26.7%, and reaches 39.7% joint goal accuracy in dialogue state tracking—nearly matching the performance of methods using significantly larger models like OpenAI Codex.

## Method Summary
The method uses an unsupervised approach to build an end-to-end task-oriented dialogue system from unlabeled dialogues and an API schema. It employs a noisy-channel prompting technique and expectation-maximization (EM) algorithm to iteratively improve pseudo-labels for dialogue state tracking and system acts. The system predicts dialogue states and acts using text-to-code prompts, then re-ranks these predictions using a code-to-text prompt that conditions on the observed user utterance. The EM process uses these pseudo-labels to fine-tune a smaller LLM, which then produces improved labels in subsequent iterations. The approach achieves state-of-the-art results on MultiWOZ without requiring any turn-level annotations.

## Key Results
- Achieves 68.3% dialogue success rate on MultiWOZ, more than doubling the 26.7% of a strong GPT-3.5 baseline
- Reaches 39.7% joint goal accuracy in dialogue state tracking, nearly matching methods using larger models like OpenAI Codex
- Ablation studies show that noisy-channel inference is critical for success
- Contamination analysis confirms limited task-relevant data in pre-training does not explain strong performance

## Why This Works (Mechanism)

### Mechanism 1: Noisy-channel prompting for dialogue state tracking
- Claim: Noisy-channel prompting improves DST by re-ranking latent predictions according to the likelihood of the input utterance given the predicted state.
- Mechanism: Samples possible dialogue state changes using top-p sampling from a direct text-to-code prompt, then re-ranks these samples using a code-to-text prompt that conditions the observed user utterance on each predicted state.
- Core assumption: The correct dialogue state will have a higher likelihood of generating the observed user utterance than incorrect states.
- Evidence anchors: Abstract states "noisy-channel 'code-to-text' reranking approach... greatly improves our pseudo-label quality and final system."

### Mechanism 2: Expectation-Maximization for iterative improvement
- Claim: EM iteratively improves pseudo-label quality by using current predictions as in-context examples for the LLM and as training data for fine-tuning.
- Mechanism: E-step uses current pseudo-labels to derive training pairs; M-step fine-tunes a smaller LLM on these pairs and uses it to produce improved pseudo-labels.
- Core assumption: The LLM can learn to improve predictions by seeing its own outputs as training examples.
- Evidence anchors: Abstract mentions "Hard-EM approach which uses predictions as in-context examples for the LLM and as data for iteratively fine-tuning a final model."

### Mechanism 3: Dialogue act inference from system responses
- Claim: The system can learn effective dialogue acts and policy from unlabeled dialogues by reverse-engineering system's communicative intents from observed responses.
- Mechanism: Predicts dialogue acts for each system response using text-to-code prompt, then uses these predicted acts to delexicalize responses for training a response generator.
- Core assumption: Observed system responses contain sufficient information to infer underlying dialogue acts.
- Evidence anchors: Section 4.2 describes using text-to-code prompt for predicting dialogue acts in system responses.

## Foundational Learning

- Concept: Noisy-channel models in NLP
  - Why needed here: The system uses noisy-channel prompting to re-rank dialogue state predictions, requiring understanding of P(observed | latent) modeling.
  - Quick check question: How does the noisy-channel approach differ from standard prompting, and why might it improve prediction quality?

- Concept: Expectation-Maximization algorithm
  - Why needed here: The system uses EM to iteratively improve pseudo-labels, requiring understanding of E-step (computing expected labels) and M-step (maximizing likelihood given labels).
  - Quick check question: What are the key differences between Hard-EM and standard EM, and why might Hard-EM be more suitable for this task?

- Concept: Dialogue act tagging and delexicalization
  - Why needed here: The system infers dialogue acts from responses and uses them to delexicalize responses for training, requiring understanding of dialogue act representation and manipulation.
  - Quick check question: How does delexicalization help in training response generators, and what information is preserved versus discarded?

## Architecture Onboarding

- Component map: Unlabeled dialogues + API schema → DST Module → DAT Module → Noisy-channel Reranker → EM Loop → End-to-End Trainer
- Critical path: User utterance → DST prediction → Policy prediction → Response generation → System response
- Design tradeoffs:
  - StarCoder 15B for initial labeling vs. StarCoder 3B for EM fine-tuning (accuracy vs. efficiency)
  - Sampling k states for re-ranking vs. greedy decoding (accuracy vs. latency)
  - Including in-context examples vs. shortening prompts for efficiency (accuracy vs. context length limits)
- Failure signatures:
  - DST failures: Incorrect API calls, missing slot values, hallucinated slots
  - Policy failures: Inappropriate or suboptimal dialogue acts, not faithful to predicted acts
  - Response failures: Hallucinations, not grounded in predicted acts or API results
- First 3 experiments:
  1. Evaluate baseline zero-shot performance without any in-context examples or EM
  2. Evaluate noisy-channel re-ranking impact by comparing direct vs. noisy-channel inference
  3. Evaluate EM impact by comparing zero-shot, 1-step EM, and 2-step EM performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the unsupervised approach perform on other task-oriented dialogue datasets with different API schemas and domains?
- Basis in paper: [inferred] The paper only evaluates on MultiWOZ, a multi-domain dataset. The authors suggest future work could apply their method to other NLP tasks.
- Why unresolved: The method's generalization to different domains and API schemas is unknown. The paper does not provide evidence of performance on other datasets.
- What evidence would resolve it: Experiments on other task-oriented dialogue datasets with different API schemas and domains would demonstrate the method's generalization capabilities.

### Open Question 2
- Question: What is the impact of using different code-based LLMs, such as Code Llama or Codex, on the performance of the unsupervised approach?
- Basis in paper: [explicit] The paper uses StarCoder, a code-based LLM, but does not compare its performance to other code-based LLMs.
- Why unresolved: The paper does not provide evidence of the impact of using different code-based LLMs on the performance of the unsupervised approach.
- What evidence would resolve it: Experiments using different code-based LLMs, such as Code Llama or Codex, would demonstrate the impact of the choice of LLM on the performance of the unsupervised approach.

### Open Question 3
- Question: How does the unsupervised approach perform when the API schema is incomplete or contains errors?
- Basis in paper: [inferred] The paper assumes a well-defined API schema, but does not explore the impact of incomplete or erroneous schemas on the performance of the unsupervised approach.
- Why unresolved: The paper does not provide evidence of the approach's robustness to incomplete or erroneous API schemas.
- What evidence would resolve it: Experiments using incomplete or erroneous API schemas would demonstrate the approach's robustness to schema imperfections.

## Limitations
- The approach relies on a limited set of 30 unlabeled dialogues from MultiWOZ (approximately 2% of training data), raising questions about scalability to truly unlabeled datasets
- The noisy-channel reranking approach lacks detailed analysis of effectiveness across different error types or dialogue complexity levels
- The EM approach's convergence properties and sensitivity to initialization quality are not thoroughly explored

## Confidence
- **High Confidence**: The empirical results showing substantial improvements over GPT-3.5 baselines (68.3% vs 26.7% success rate) are well-supported by ablation studies and comparison to larger models. The contamination analysis methodology is sound.
- **Medium Confidence**: The claim that noisy-channel prompting is "instrumental" to the method is supported by ablation results but could benefit from more detailed error analysis. The EM approach's effectiveness is demonstrated but convergence properties remain uncertain.
- **Low Confidence**: The scalability claims to truly unlabeled datasets are not empirically validated, and the paper does not address potential domain adaptation challenges beyond the MultiWOZ benchmark.

## Next Checks
1. Conduct a detailed error analysis comparing direct prompting vs. noisy-channel reranking to identify specific error types (e.g., slot value hallucinations, API call errors) that the reranking approach successfully corrects.
2. Run EM iterations beyond the reported 2-step process to identify convergence patterns and determine if further iterations yield diminishing returns or potential degradation in performance.
3. Apply the approach to a truly unlabeled dataset (not MultiWOZ-derived) to validate scalability claims and assess performance degradation when moving away from the benchmark data distribution.