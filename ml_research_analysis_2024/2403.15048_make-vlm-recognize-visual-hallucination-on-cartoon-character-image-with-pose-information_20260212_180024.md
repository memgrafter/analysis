---
ver: rpa2
title: Make VLM Recognize Visual Hallucination on Cartoon Character Image with Pose
  Information
arxiv_id: '2403.15048'
source_url: https://arxiv.org/abs/2403.15048
tags:
- hallucination
- image
- pose
- images
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting semantic structural
  visual hallucinations in cartoon character images generated by text-to-image models.
  The authors propose a novel hallucination detection system based on vision-language
  models (VLMs) using pose-aware in-context visual learning (PA-ICVL).
---

# Make VLM Recognize Visual Hallucination on Cartoon Character Image with Pose Information

## Quick Facts
- arXiv ID: 2403.15048
- Source URL: https://arxiv.org/abs/2403.15048
- Reference count: 40
- Primary result: PA-ICVL improves GPT-4V hallucination detection accuracy from 50% to 78% and Gemini Pro Vision from 57% to 80%

## Executive Summary
This paper addresses the problem of detecting semantic structural visual hallucinations in cartoon character images generated by text-to-image models. The authors propose a novel hallucination detection system based on vision-language models (VLMs) using pose-aware in-context visual learning (PA-ICVL). The method leverages in-context learning to teach VLMs to detect hallucinations by providing paired examples of RGB images, pose information, and hallucination labels. Experiments show that PA-ICVL significantly improves hallucination detection accuracy compared to baseline methods.

## Method Summary
The proposed PA-ICVL method combines pose-aware guidance with in-context visual learning to teach VLMs hallucination detection. The approach extracts pose information from cartoon images using a fine-tuned pose estimator, then provides VLMs with paired examples containing RGB images, pose data, and hallucination labels. The system leverages the VLM's in-context learning capabilities to adapt to hallucination detection without requiring additional parameter training. Pose information is provided in textual format (joint coordinates) rather than visual representations to maximize precision and interpretability.

## Key Results
- GPT-4V detection accuracy improved from 50% to 78% with pose-aware in-context learning
- Gemini Pro Vision accuracy increased from 57% to 80% using the same approach
- Textual joint coordinates outperformed other pose representation formats including Gaussian heatmaps and overlaid images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pose-aware in-context visual learning improves VLM hallucination detection accuracy by providing additional structural cues that help VLMs distinguish between correct and hallucinated body parts.
- Mechanism: The VLM receives paired examples containing RGB images, pose maps, and hallucination labels. This enables the model to learn the correspondence between visual structure and semantic correctness, allowing it to identify structural anomalies like extra limbs or missing body parts that constitute hallucinations.
- Core assumption: VLMs can effectively utilize numerical pose information alongside visual data to make more accurate decisions about hallucination detection, and the pose information is sufficiently precise to highlight structural discrepancies.
- Evidence anchors:
  - [abstract]: "By incorporating pose guidance, we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual hallucinations compared to baseline methods relying solely on RGB images."
  - [section]: "In PA-ICVL, we do not perform additional parameter training or tuning on the given input-output pairs. Instead, we leverage a capability of in-context learning that gradually provides the model with both visual and pose information."
  - [corpus]: Weak - The corpus contains papers about hallucination evaluation in VLMs but none specifically address pose-guided hallucination detection in cartoon characters.
- Break condition: The pose estimator fails to accurately detect joints in cartoon characters, or the VLM cannot effectively integrate pose information with visual cues to distinguish hallucinations.

### Mechanism 2
- Claim: In-context visual learning allows VLMs to learn hallucination detection without requiring additional training or parameter updates, making the approach more efficient than traditional fine-tuning methods.
- Mechanism: The VLM learns from a small set of hallucination-aware examples provided in the prompt context. Through this few-shot learning approach, the model adapts to the specific task of detecting cartoon character hallucinations by observing the relationship between input images, pose information, and correct labels.
- Core assumption: VLMs possess sufficient in-context learning capabilities to generalize from a limited number of examples to the broader task of hallucination detection without parameter updates.
- Evidence anchors:
  - [abstract]: "Our approach is to leverage the emerging capability of large language model, in-context learning which denotes that VLM has seen some examples by user for specific downstream task, here hallucination detection."
  - [section]: "In PA-ICVL, we do not perform additional parameter training or tuning on the given input-output pairs. Instead, we leverage a capability of in-context learning that gradually provides the model with both visual and pose information."
  - [corpus]: Weak - While the corpus contains papers on hallucination evaluation in VLMs, none specifically address in-context learning approaches for hallucination detection.
- Break condition: The VLM requires more examples than can be practically provided in the prompt context, or the task complexity exceeds the VLM's in-context learning capabilities.

### Mechanism 3
- Claim: Using textual representation of pose information (joint coordinates) achieves better performance than visual pose representations (heatmaps or joint images) because it provides more precise, structured information that VLMs can process effectively.
- Mechanism: Converting pose information into textual format (joint coordinates) creates a more explicit, structured representation that the VLM's language processing capabilities can directly interpret and correlate with visual features to identify structural anomalies.
- Core assumption: VLMs have stronger language processing capabilities than visual processing capabilities for structured numerical data, making textual pose information more effective than visual pose representations.
- Evidence anchors:
  - [section]: "In contrast, these additional inputs Mgau, Mover and Mxy seems like to disturb the interpretation process in the case of Gemini 1.5 Pro... We argue that textual joint data can provide more precise posture information, enabling the VLM to effectively compare the input RGB image with the pose data to identify hallucinations."
  - [corpus]: Weak - The corpus contains papers about hallucination in VLMs but none specifically compare different modalities of pose information for hallucination detection.
- Break condition: The textual representation becomes too verbose to fit in the prompt context, or the VLM's visual processing capabilities are sufficient for the task, making textual representation unnecessary.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: This work relies on VLMs' ability to learn new tasks from a few examples provided in the prompt context without parameter updates, which is essential for the pose-aware hallucination detection approach.
  - Quick check question: What distinguishes in-context learning from traditional fine-tuning approaches in terms of model parameter updates?

- Concept: Vision-language model architecture
  - Why needed here: Understanding how VLMs process both visual and textual inputs is crucial for designing the input format (RGB images, pose maps, textual prompts) and interpreting why certain representations work better than others.
  - Quick check question: How do VLMs typically process visual information before combining it with textual embeddings?

- Concept: Pose estimation and human body structure
  - Why needed here: The approach leverages pose information to detect structural hallucinations in cartoon characters, requiring understanding of how pose estimators work and what constitutes normal vs. abnormal body structure.
  - Quick check question: What are the typical keypoints used in human pose estimation, and how would missing or extra keypoints indicate a hallucination?

## Architecture Onboarding

- Component map:
  Text-to-Image (TTI) Model -> Pose Estimator -> Vision-Language Model (VLM) -> Evaluation Framework

- Critical path:
  1. Generate cartoon character images using TTI model
  2. Extract pose information using fine-tuned pose estimator
  3. Create paired examples with RGB images, pose data, and hallucination labels
  4. Provide examples to VLM using in-context learning
  5. Evaluate VLM's hallucination detection performance

- Design tradeoffs:
  - Few-shot learning vs. fine-tuning: Few-shot (in-context) learning requires no parameter updates but may be less accurate; fine-tuning requires more data and computation but potentially better performance
  - Pose representation formats: Textual joint coordinates provide precision but may be verbose; visual heatmaps are compact but may lose detail; overlaid heatmaps provide context but may confuse the model
  - Dataset size: Larger datasets provide better coverage but require more annotation effort; smaller datasets are easier to create but may limit model generalization

- Failure signatures:
  - Detection accuracy around 50%: Model is guessing randomly, indicating it hasn't learned the task
  - Consistently missing hallucinations with extra body parts: Pose information may not be effectively highlighting structural anomalies
  - Poor performance on rotated images: Model may be relying on orientation-specific features rather than structural understanding
  - Degradation with increased prompt complexity: In-context learning capacity may be exceeded

- First 3 experiments:
  1. Baseline hallucination detection using only RGB images with in-context learning to establish the performance gap that pose information should address
  2. Evaluate different pose representation formats (Gaussian heatmaps, overlaid heatmaps, joint images, textual joint coordinates) to determine which works best
  3. Test model robustness to image transformations (flipping, rotation) to understand whether the approach captures structural rather than orientation-specific features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed PA-ICVL method perform on detecting hallucinations in cartoon images that contain partial body parts or non-human-like characters?
- Basis in paper: [explicit] The paper acknowledges limitations in handling non-human-like cartoon characters and mentions future work on detecting partial and detailed hallucinations.
- Why unresolved: The current evaluation focuses on complete human-like cartoon characters, and the pose estimator fails on non-human-like characters, limiting the method's generalizability.
- What evidence would resolve it: Experimental results showing PA-ICVL performance on datasets containing partial bodies, non-human-like characters, and more diverse cartoon styles would clarify its broader applicability.

### Open Question 2
- Question: Can the PA-ICVL approach be extended to detect hallucinations in other non-photorealistic domains beyond cartoons, such as pixel art, abstract illustrations, or different artistic styles?
- Basis in paper: [explicit] The paper mentions that cartoon style has "extremely wide appearance" and future work will aim to adapt the approach to various non/photorealistic styles.
- Why unresolved: The current method is specifically tuned for cartoon characters with five-head-figure proportions, and its effectiveness on other NPR domains remains untested.
- What evidence would resolve it: Testing PA-ICVL on diverse non-photorealistic datasets (pixel art, abstract illustrations, different artistic styles) with appropriate pose estimation would demonstrate its generalizability.

### Open Question 3
- Question: What is the impact of hallucination region localization on downstream applications like in-painting or image restoration, and how can this be improved beyond the current VLM limitations?
- Basis in paper: [explicit] Section 5.1 discusses failed attempts at hallucination region localization and suggests in-context learning with annotated bounding boxes as a potential improvement.
- Why unresolved: The current VLM cannot accurately extract hallucination regions, limiting applications like targeted restoration, and the proposed solution (annotated bounding boxes) requires additional manual annotation.
- What evidence would resolve it: Successful implementation of hallucination region localization using annotated bounding boxes or alternative approaches, followed by demonstration of effective in-painting/restoration, would validate this direction.

## Limitations
- The approach relies heavily on the quality and architecture of the fine-tuned pose estimator, which is not fully specified in the paper
- Performance is only evaluated on specific VLM models (GPT-4V and Gemini Pro Vision) without exploring generalization across different architectures
- Dataset creation depends on human annotation, introducing potential subjectivity in hallucination labeling
- The method's effectiveness on more complex hallucinations beyond simple body part additions or deletions remains untested

## Confidence

**High Confidence**: The claim that in-context learning can teach VLMs to detect hallucinations using paired examples has strong experimental support, with clear quantitative improvements from 50% to 78% accuracy for GPT-4V and 57% to 80% for Gemini Pro Vision. The mechanism of using pose information to provide structural cues is well-supported by ablation studies showing textual joint coordinates outperform visual representations.

**Medium Confidence**: The claim that this approach generalizes to other cartoon character styles and more complex hallucination types has moderate support, as the paper demonstrates effectiveness on a specific dataset but doesn't extensively test domain generalization or complex hallucination scenarios.

**Low Confidence**: The claim about optimal pose representation format is based on limited experimentation with only a few representation types, and the reasons why textual joint coordinates outperform other formats are not fully explored.

## Next Checks

1. **Generalization Testing**: Evaluate the PA-ICVL approach on cartoon character images generated from different TTI models (e.g., Midjourney, Stable Diffusion) and with different artistic styles to assess domain robustness.

2. **Complex Hallucination Detection**: Test the system's ability to detect more sophisticated hallucinations such as incorrect body proportions, anatomical inconsistencies, or contextually inappropriate objects to understand the limits of pose-based detection.

3. **Pose Estimator Dependency Analysis**: Conduct ablation studies by varying the accuracy and resolution of the pose estimator to quantify how sensitive the hallucination detection performance is to pose estimation quality, and explore whether the approach can work with less precise pose information.