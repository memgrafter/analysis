---
ver: rpa2
title: Learning fast changing slow in spiking neural networks
arxiv_id: '2402.10069'
source_url: https://arxiv.org/abs/2402.10069
tags:
- learning
- policy
- network
- spiking
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces lf-cs, a biologically plausible reinforcement
  learning algorithm for spiking neural networks that addresses the plasticity-stability
  paradox. The method uses two parallel networks operating on different timescales
  - one stable for data collection and another that quickly updates parameters - inspired
  by proximal policy optimization (PPO).
---

# Learning fast changing slow in spiking neural networks

## Quick Facts
- arXiv ID: 2402.10069
- Source URL: https://arxiv.org/abs/2402.10069
- Authors: Cristiano Capone; Paolo Muratore
- Reference count: 34
- Key outcome: Introduces lf-cs, a biologically plausible RL algorithm for spiking neural networks that uses two parallel networks on different timescales to solve the plasticity-stability paradox, outperforming standard methods on Atari Pong

## Executive Summary
This work addresses the plasticity-stability paradox in spiking neural networks by introducing lf-cs, a biologically plausible reinforcement learning algorithm inspired by proximal policy optimization (PPO). The method employs two parallel networks operating on different timescales - one stable for data collection and another that quickly updates parameters. This architecture enables efficient learning while maintaining stability, even when replaying experiences. The approach is validated on Atari Pong, demonstrating superior performance compared to standard biologically plausible RL methods.

## Method Summary
The lf-cs algorithm uses a two-network architecture with separate timescales to balance learning speed and stability. The reference network (πref) interacts with the environment on a slow timescale without updating weights, providing stable behavior for data collection. The future policy network (πnew) rapidly updates its parameters using experiences collected by πref. At episode boundaries, πnew's parameters are transferred to πref, updating the policy slowly. The method uses a simplified PPO surrogate loss with clipping that enables online learning while preventing large policy updates that could cause instability. Experience replay is incorporated with a stiffness parameter ε that controls policy updates to prevent divergence when replaying old experiences.

## Key Results
- Outperforms standard biologically plausible RL methods (e-prop) on Atari Pong
- Achieves better learning speed and stability through two-network architecture
- Shows particular benefits when experience replay is used
- Performs well across different game durations (100 vs 200 frames) and stiffness parameters

## Why This Works (Mechanism)

### Mechanism 1
The two-network decomposition allows stable data collection while enabling fast policy updates. πref interacts with the environment on a slow timescale without updating weights, providing stable behavior for data collection. πnew updates its parameters rapidly using the collected experiences, learning fast. At episode boundaries, πnew's parameters are transferred to πref, updating the policy slowly.

### Mechanism 2
The clipped surrogate function (Ls-clip) enables online learning while preventing large policy updates that could cause instability. The simplified Ls-clip replaces the min operation with a direct product of the clipped ratio and advantage, making the loss differentiable and suitable for online updates.

### Mechanism 3
The policy update control mechanism with stiffness parameter ε allows experience replay without policy divergence. When replaying experiences, the policy update is only accepted if the ratio πnew/πref is within the stiffness bound ε, preventing large policy changes that could occur when repeatedly applying updates from old experiences.

## Foundational Learning

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: PPO provides a framework for stable policy gradient updates by limiting policy changes, crucial for the two-network architecture to work effectively
  - Quick check question: How does PPO's clipped surrogate objective differ from standard policy gradient methods?

- Concept: Experience Replay
  - Why needed here: Experience replay allows the agent to learn from past experiences multiple times, improving sample efficiency, but requires mechanisms to prevent policy divergence
  - Quick check question: Why is experience replay particularly challenging for on-policy methods like PPO?

- Concept: Spiking Neural Networks (SNNs)
  - Why needed here: The work focuses on implementing the learning algorithm in biologically plausible spiking neural networks, which have different dynamics than traditional artificial neural networks
  - Quick check question: What are the key differences between spiking neural networks and traditional artificial neural networks that affect learning algorithms?

## Architecture Onboarding

- Component map:
  Environment -> πref (Reference Network) -> Memory Buffer -> πnew (Future Policy Network) -> Policy Update Control -> πref

- Critical path:
  1. πref interacts with environment, collects experiences
  2. Experiences stored in memory buffer
  3. πnew reads from buffer, computes updates using Ls-clip loss
  4. Policy update control checks if πnew/πref < ε
  5. If accepted, πnew parameters transferred to πref at episode end

- Design tradeoffs:
  - Fast vs. slow timescales: Balancing learning speed (fast updates) with stability (slow updates)
  - Stiffness parameter ε: Balancing between accepting updates (faster learning) and preventing divergence (more stability)
  - Experience replay frequency: More replays improve sample efficiency but increase risk of policy divergence

- Failure signatures:
  - Learning stalls: ε set too low, preventing any updates
  - Policy divergence: ε set too high, allowing large policy changes
  - Unstable learning: Poor synchronization between πref and πnew updates
  - Poor performance: Inadequate spiking neural network implementation or hyperparameters

- First 3 experiments:
  1. Test basic Pong learning without experience replay to verify the two-network architecture works
  2. Vary stiffness parameter ε to find optimal balance between learning speed and stability
  3. Test experience replay with different numbers of replays to evaluate sample efficiency improvements

## Open Questions the Paper Calls Out

### Open Question 1
How does the lf-cs algorithm scale to more complex Atari games with longer temporal horizons and larger state spaces? The paper only tests on pong, a relatively simple game, and performance on more complex games is unknown.

### Open Question 2
What is the impact of the stiffness parameter (ε) on learning dynamics and policy stability in more complex environments? The paper discusses ε's role in pong but doesn't explore its effects in more complex environments.

### Open Question 3
How does lf-cs compare to other state-of-the-art biologically plausible RL algorithms on complex tasks? While lf-cs outperforms e-prop on pong, its performance relative to other state-of-the-art algorithms on complex tasks is unknown.

## Limitations

- Biological plausibility concerns with the two-network architecture that may not map cleanly to neural substrates
- Limited testing only on Atari Pong, raising questions about generalization to more complex environments
- Stiffness parameter ε introduces a tunable hyperparameter without comprehensive guidance on optimal selection

## Confidence

**High Confidence:** The core claim that the two-network architecture with separate timescales can stabilize learning while maintaining plasticity is well-supported by experimental results on Atari Pong.

**Medium Confidence:** The assertion that experience replay significantly improves sample efficiency while maintaining stability through the policy update control mechanism is supported but could be more thoroughly analyzed.

**Low Confidence:** The claim about biological plausibility is the most uncertain aspect, as the two-network architecture and parameter synchronization at episode boundaries are not clearly grounded in biological mechanisms.

## Next Checks

1. **Cross-Environment Generalization Test:** Validate the lf-cs algorithm on at least two additional Atari games (e.g., Breakout and Space Invaders) to assess whether the observed benefits generalize beyond Pong.

2. **Stiffness Parameter Sensitivity Analysis:** Conduct a systematic grid search over the stiffness parameter ε across multiple orders of magnitude and analyze how this affects both learning speed and final performance.

3. **Ablation Study of Two-Network Architecture:** Implement and compare three variants: (a) single network with standard PPO, (b) two-network architecture without experience replay, and (c) full lf-cs method to isolate contributions of timescale separation versus experience replay.