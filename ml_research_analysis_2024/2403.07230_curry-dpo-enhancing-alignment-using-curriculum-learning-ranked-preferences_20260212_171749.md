---
ver: rpa2
title: 'Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences'
arxiv_id: '2403.07230'
source_url: https://arxiv.org/abs/2403.07230
tags:
- preference
- pairs
- responses
- curri-dpo
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Curri-DPO, which applies curriculum learning
  to Direct Preference Optimization (DPO) by using multiple preference pairs per prompt
  ordered from easy to hard. Curri-DPO outperforms standard DPO with a 7.43 MT-bench
  score and achieves up to 7.5% higher win rates on WizardLM, and up to 5.1% on UltraFeedback
  test sets.
---

# Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences

## Quick Facts
- arXiv ID: 2403.07230
- Source URL: https://arxiv.org/abs/2403.07230
- Reference count: 16
- Primary result: Curri-DPO achieves 7.43 MT-bench score and up to 7.5% higher win rates on WizardLM

## Executive Summary
Curry-DPO enhances Direct Preference Optimization (DPO) by applying curriculum learning to preference pairs, ordering them from easy to hard based on quality gaps. The method leverages multiple responses per prompt to create richer training signals and iteratively updates the reference model to progressively refine alignment. Experiments show Curri-DPO outperforms standard DPO across multiple benchmarks, achieving state-of-the-art results on MT-bench and significant improvements in win rates on various test sets.

## Method Summary
Curri-DPO uses multiple responses per prompt to generate preference pairs, then orders these pairs from easy to hard based on rating differences (e.g., highest vs lowest rated responses). The training proceeds iteratively with three epochs, using the previous iteration's model as the reference model. This curriculum approach starts with clear contrastive examples to build discriminative ability before introducing subtler distinctions, while the iterative updates allow progressive refinement of preference boundaries.

## Key Results
- Achieved 7.43 MT-bench score, outperforming standard DPO baseline
- Up to 7.5% higher win rates on WizardLM test sets
- Up to 5.1% higher win rates on UltraFeedback test sets
- Maintained strong performance on safety benchmarks including RealToxicityPrompts and BOLD

## Why This Works (Mechanism)

### Mechanism 1
Curriculum ordering of preference pairs (easy → hard) improves learning by starting with clear contrastive examples. Early training uses pairs with large quality gaps (e.g., highest vs lowest rated responses), which are easier for the model to distinguish. This builds discriminative ability before introducing subtler distinctions.

### Mechanism 2
Iterative DPO with previous-iteration model as reference improves alignment by maintaining a moving baseline. Each iteration updates the reference model to the previous iteration's policy, allowing progressive refinement of preference boundaries as the model becomes more sensitive to subtle quality differences.

### Mechanism 3
Using multiple preference pairs per prompt provides richer training signal than single pairs. Each prompt generates multiple ordered preference pairs, creating a curriculum within each prompt's response space. This augmentation helps the model learn more nuanced distinctions between response qualities.

## Foundational Learning

- Concept: Curriculum Learning
  - Why needed here: Organizes preference pairs from easy to hard, building discriminative ability progressively
  - Quick check question: What distinguishes an "easy" preference pair from a "hard" one in this context?

- Concept: Preference Optimization
  - Why needed here: The core alignment mechanism that learns to rank chosen responses above rejected ones
  - Quick check question: How does the Bradley-Terry model formalize pairwise preferences in DPO?

- Concept: Iterative Training with Reference Models
  - Why needed here: Allows progressive refinement by updating the baseline model each iteration
  - Quick check question: Why might using the SFT model as a static reference throughout training be suboptimal?

## Architecture Onboarding

- Component map: Data preparation (Multiple responses → quality ranking → preference pair generation → curriculum ordering) -> Training loop (SFT model → iteration 1 → iteration 2 → iteration 3) -> Evaluation (MT-bench, Vicuna, WizardLM, UltraFeedback, safety benchmarks)
- Critical path: Data preparation → curriculum ordering → iterative DPO training → evaluation
- Design tradeoffs:
  - More preference pairs = richer signal but higher computational cost
  - More iterations = better alignment but diminishing returns and training time
  - Rating method choice (GPT-4 vs human vs LogP) affects pair ordering and training dynamics
- Failure signatures:
  - Performance plateaus or degrades with additional iterations
  - Model overfits to easy pairs and performs poorly on harder distinctions
  - Training instability when reference model updates are too aggressive
- First 3 experiments:
  1. Compare single pair DPO (R1,R4) vs (R1,R3) vs (R1,R2) to verify "easy-to-hard" hypothesis
  2. Implement non-iterative multi-pair DPO (random order) vs iterative curriculum DPO
  3. Test reference model update strategy: static SFT vs previous iteration model

## Open Questions the Paper Calls Out

### Open Question 1
How does Curri-DPO perform when using all 4C2 = 6 possible preference pairs from the 4 responses per prompt, compared to the 3 pairs used in the main experiments? The paper briefly tested this variant and observed "marginal gains" but didn't conduct thorough investigation into why performance plateaus.

### Open Question 2
What are the theoretical limits of curriculum learning in DPO - at what point does ordering preference pairs stop providing benefits? The paper demonstrates benefits from curriculum ordering but doesn't explore the theoretical boundaries or conditions under which curriculum learning helps vs hurts.

### Open Question 3
How does Curri-DPO scale with dataset size - would performance continue to improve with 64K+ preference pairs as it did with 5K? The paper used only 5K samples due to computational constraints and found similar performance to the full 64K dataset, but didn't explore intermediate sizes or the scaling relationship.

### Open Question 4
How sensitive is Curri-DPO to the specific ranking criteria used to order preference pairs (GPT-4 scores vs human ratings vs LogP scores)? While the paper tested all three ranking methods, it didn't perform ablation studies or analyze which ranking criteria work best for different types of prompts or model sizes.

### Open Question 5
Can the curriculum learning approach be extended to other preference optimization methods like LiPO, SLiC, or IPO, and would it provide similar benefits? The paper only tested curriculum learning with DPO and mentioned other methods could benefit but didn't provide experimental validation.

## Limitations

- The paper lacks ablation studies comparing different curriculum strategies or validation that the "easy-to-hard" ordering is optimal
- The iterative reference update mechanism has no theoretical grounding or empirical comparison to alternative update strategies
- The claim that multiple preference pairs per prompt significantly improve learning over single pairs is plausible but not directly tested in this work

## Confidence

- Curri-DPO's improved MT-bench score (7.43) and win rates (up to 7.5% on WizardLM, 5.1% on UltraFeedback): **High** - Supported by direct experimental results and benchmark comparisons
- Curriculum ordering of preference pairs improves learning: **Medium** - Supported by results but lacks ablation studies on alternative ordering strategies
- Iterative reference model updates provide benefit: **Medium** - Mechanism described clearly but no comparison to static reference models

## Next Checks

1. Conduct ablation studies comparing different curriculum ordering strategies (rating gap, difficulty estimation, random) to isolate the contribution of curriculum learning
2. Test the iterative reference update mechanism against a static reference model (SFT model used throughout) to quantify the specific benefit of progressive refinement
3. Implement a multi-prompt evaluation where the same model is tested on prompts with varying difficulty levels to verify that curriculum training generalizes beyond the training distribution