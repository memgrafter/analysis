---
ver: rpa2
title: 'Ovis: Structural Embedding Alignment for Multimodal Large Language Model'
arxiv_id: '2405.20797'
source_url: https://arxiv.org/abs/2405.20797
tags:
- visual
- ovis
- embedding
- arxiv
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel multimodal large language model (MLLM)
  architecture called Ovis, which addresses the misalignment between the structural
  textual embeddings based on an embedding look-up table and the continuous embeddings
  generated directly by the vision encoder in current MLLMs. To align the visual and
  textual embeddings structurally, Ovis integrates an additional learnable visual
  embedding table into the visual encoder's process.
---

# Ovis: Structural Embedding Alignment for Multimodal Large Language Model

## Quick Facts
- arXiv ID: 2405.20797
- Source URL: https://arxiv.org/abs/2405.20797
- Reference count: 40
- Primary result: Outperforms open-source MLLMs of similar parameter scales and surpasses Qwen-VL-Plus on multimodal benchmarks

## Executive Summary
This paper addresses a fundamental misalignment issue in current Multimodal Large Language Models (MLLMs) where structural textual embeddings based on look-up tables differ from continuous visual embeddings generated by vision encoders. The proposed Ovis architecture introduces a learnable visual embedding table that structurally aligns visual and textual representations. By mapping visual patches to probabilistic combinations of visual words through a visual vocabulary, Ovis ensures both modalities are sampled from discrete codebooks before fusion, enabling more effective cross-modal learning.

## Method Summary
Ovis integrates a learnable visual embedding table into the visual encoder's process, where each image patch indexes the table multiple times to produce a probabilistic combination of indexed embeddings. This mirrors the text embedding generation process using a visual vocabulary. The model employs a three-stage training strategy: first training on visual captions to initialize the visual embedding table, then on visual descriptions to refine parameters, and finally on multimodal instruction datasets to enable instruction following. All stages optimize the same cross-entropy loss over generated text, avoiding separate reconstruction losses and ensuring visual embeddings remain useful for downstream tasks.

## Key Results
- Outperforms open-source MLLMs of similar parameter scales on various multimodal benchmarks
- Surpasses proprietary model Qwen-VL-Plus overall in comprehensive evaluations
- Demonstrates strong performance on high-resolution images despite using a 336px ViT backbone without high-resolution-boosted techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing a structured visual embedding table aligns the representation space of visual and textual inputs, enabling better cross-modal fusion.
- Mechanism: Visual tokens are mapped to a probabilistic distribution over a visual vocabulary, and each patch embedding is formed as the expectation over indexed visual embeddings. This mirrors the text embedding generation process and ensures both modalities are sampled from a discrete codebook before fusion.
- Core assumption: The expectation over multiple visual word embeddings captures richer patch semantics than a single nearest neighbor assignment, and the discrete visual vocabulary can be jointly optimized with LLM parameters without catastrophic forgetting.
- Evidence anchors: [abstract] "Ovis integrates an additional learnable visual embedding table into the visual encoder's process. To capture rich visual semantics, each image patch indexes the visual embedding table multiple times, resulting in a final visual embedding that is a probabilistic combination of the indexed embeddings."

### Mechanism 2
- Claim: Training in three stages with a joint textual generation loss prevents the visual embedding table from diverging from the language model's needs.
- Mechanism: Stage 1 trains the vision encoder tail and visual embedding table on image captions, Stage 2 adds visual description data, Stage 3 unfreezes the LLM for multimodal instruction tuning. All stages optimize the same cross-entropy loss over generated text, ensuring visual embeddings remain useful for downstream tasks.
- Core assumption: Joint optimization of visual and textual embeddings via the same loss function is sufficient to align them without needing auxiliary reconstruction or VQ losses.
- Evidence anchors: [section] "Instead of using an additional autoencoder with vector quantization over images and various other losses, as utilized in previous methods, Ovis leverages a joint textual generation loss and optimizes the parameters in a three-stage manner."

### Mechanism 3
- Claim: Using a linear head with softmax to produce a probability distribution over visual words yields sparse activations, enabling efficient indexing and reducing redundancy.
- Mechanism: The continuous visual token is projected to K logits and softmaxed to form a distribution over the visual vocabulary; most probability mass concentrates on a few words, so only a small subset of embeddings are weighted heavily.
- Core assumption: The softmax-normalized similarity captures the patch's semantic correspondence to visual words and is sparse enough to approximate a hard assignment while retaining smoothness for gradient flow.
- Evidence anchors: [section] "Ovis first maps the token into a probabilistic token, revealing its similarity among the entire visual vocabulary set. The probabilistic token captures the rich semantics within a single visual patch, which may contain patterns from multiple visual words."

## Foundational Learning

- Concept: Discrete visual tokenization and embedding tables
  - Why needed here: MLLMs need visual and textual inputs in compatible forms; using a visual embedding table mirrors the LLM's text embedding table and allows direct concatenation without dimension mismatch.
  - Quick check question: How does indexing a visual embedding table with a probabilistic token differ from directly using the continuous visual token from the vision encoder?

- Concept: Joint optimization of multimodal components
  - Why needed here: The visual embedding table and probabilistic token generator must stay aligned with the LLM's textual embedding space; separate training risks semantic drift.
  - Quick check question: Why does Ovis use the same textual generation loss across all training stages instead of separate losses for vision and language?

- Concept: Multimodal instruction tuning
  - Why needed here: Stage 3 fine-tunes the full model on instruction data so the aligned embeddings are usable for real-world multimodal tasks, not just reconstruction or captioning.
  - Quick check question: What would happen if the LLM were not unfrozen in Stage 3?

## Architecture Onboarding

- Component map: Vision Transformer (pre-trained) → visual tokens → linear head (W) → probabilistic token (vi) → visual embedding table (ek) → visual embedding (Vi) → LLM (with textual embedding table) → text generation

- Critical path: Image → ViT → W → visual embedding table → concatenated with textual embeddings → LLM → output text. Any failure in visual tokenization or embedding alignment blocks multimodal understanding.

- Design tradeoffs:
  - Adding a visual embedding table increases parameters but enables structural alignment; using the same table as text embeddings would simplify but risk modality mixing.
  - Sparse probability distributions reduce computation but require careful K selection to balance expressiveness and overfitting.

- Failure signatures:
  - Poor visual performance on perception tasks but good language generation suggests visual embedding table is not well learned.
  - Visual embeddings collapse to near-zero variance indicates softmax saturation or bad initialization of W.
  - Overfitting on small datasets shows visual vocabulary size K is too large relative to training data.

- First 3 experiments:
  1. Ablation: Replace Ovis' visual embedding table with direct continuous visual tokens fed to LLM; measure drop in multimodal benchmarks.
  2. Sensitivity: Vary visual vocabulary size K (e.g., 2^14, 2^16, 2^18) and observe impact on MathVista-Mini and RealWorldQA scores.
  3. Ablation: Train Ovis with only Stage 1 (caption data) and test on instruction-following benchmarks to quantify the importance of Stages 2 and 3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Ovis compare to other MLLMs when handling multi-image inputs, given that Ovis is currently trained solely on single-image samples?
- Basis in paper: [explicit] The paper mentions that Ovis is trained solely with single-image samples, posing challenges when confronted with scenarios requiring visual understanding across multiple images.
- Why unresolved: The paper does not provide any experimental results or analysis on Ovis' performance with multi-image inputs.
- What evidence would resolve it: Conducting experiments to evaluate Ovis' performance on benchmarks or datasets that require understanding and reasoning across multiple images, and comparing the results with other MLLMs designed for multi-image inputs.

### Open Question 2
- Question: What is the impact of increasing the visual vocabulary size (K) on Ovis' performance, and is there an optimal value for K?
- Basis in paper: [explicit] The paper sets the visual vocabulary size to 217 = 131,072, but does not explore the effects of varying this parameter.
- Why unresolved: The paper does not provide any ablation studies or analysis on the impact of different visual vocabulary sizes on Ovis' performance.
- What evidence would resolve it: Conducting experiments with different values of K and analyzing the corresponding performance of Ovis on various benchmarks, to identify the optimal visual vocabulary size for the model.

### Open Question 3
- Question: How does Ovis' performance on high-resolution images compare to other MLLMs that employ high-resolution-boosted techniques, such as dynamic high resolution or dual vision encoders?
- Basis in paper: [explicit] The paper mentions that Ovis employs a 336px ViT backbone and is not equipped with high-resolution-boosted techniques, but still exhibits impressive performance on high-resolution images in the RealWorldQA benchmark.
- Why unresolved: The paper does not provide a direct comparison between Ovis and other MLLMs that utilize high-resolution-boosted techniques on high-resolution image tasks.
- What evidence would resolve it: Conducting experiments to compare Ovis' performance with that of MLLMs employing high-resolution-boosted techniques on benchmarks or datasets that specifically focus on high-resolution images, and analyzing the differences in their performance.

## Limitations

- No explicit comparison with alternative visual embedding strategies (e.g., direct continuous token feeding)
- Visual tasks with high-resolution images may be limited without high-resolution-boosted techniques
- Potential hallucination risks require additional safety mechanisms

## Confidence

**High confidence** in:
- The problem statement about structural misalignment between visual and textual embeddings in MLLMs
- The three-stage training methodology description
- The basic architectural design of integrating a visual embedding table

**Medium confidence** in:
- Performance improvements over baseline models
- Effectiveness of probabilistic visual tokenization
- Generalization to unseen multimodal tasks

## Next Checks

1. **Ablation Study**: Replace Ovis' visual embedding table with direct continuous visual tokens from the vision encoder and measure the performance drop on MathVista-Mini and RealWorldQA.

2. **Vocabulary Size Sensitivity**: Train Ovis with different visual vocabulary sizes (2^14, 2^16, 2^18) to determine the optimal K and understand overfitting behavior.

3. **Stage Contribution Analysis**: Train Ovis with only Stage 1 (caption data) and test on instruction-following benchmarks to quantify the importance of Stages 2 and 3 in the training pipeline.