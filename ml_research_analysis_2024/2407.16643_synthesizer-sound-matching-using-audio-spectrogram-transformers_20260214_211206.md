---
ver: rpa2
title: Synthesizer Sound Matching Using Audio Spectrogram Transformers
arxiv_id: '2407.16643'
source_url: https://arxiv.org/abs/2407.16643
tags:
- synthesizer
- sound
- audio
- matching
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a synthesizer sound matching system using an
  Audio Spectrogram Transformer (AST) to automatically set synthesizer parameters
  to emulate input sounds. The approach trains on a large synthetic dataset of 1 million
  randomly generated samples from the Massive synthesizer, using 16 parameters selected
  for sonic diversity.
---

# Synthesizer Sound Matching Using Audio Spectrogram Transformers

## Quick Facts
- arXiv ID: 2407.16643
- Source URL: https://arxiv.org/abs/2407.16643
- Authors: Fred Bruford; Frederik Blang; Shahan Nercessian
- Reference count: 0
- One-line primary result: AST model significantly outperforms MLP and CNN baselines for synthesizer parameter inference with MSE 0.031 vs 0.077/0.094

## Executive Summary
This paper presents a synthesizer sound matching system using an Audio Spectrogram Transformer (AST) to automatically set synthesizer parameters to emulate input sounds. The approach trains on a large synthetic dataset of 1 million randomly generated samples from the Massive synthesizer, using 16 parameters selected for sonic diversity. The AST model significantly outperforms multi-layer perceptron and convolutional neural network baselines, achieving a mean-squared error of 0.031 (compared to 0.077 and 0.094) and spectral convergence of 0.616 (compared to 4.608 and 5.372) for parameter and audio reconstruction respectively. Audio examples demonstrate the model's ability to reconstruct both in-domain Massive presets and out-of-domain sounds like vocal imitations and other instruments, though some limitations exist in modeling oscillator pitch.

## Method Summary
The method trains an Audio Spectrogram Transformer on a synthetic dataset of 1 million randomly generated Massive synthesizer samples, each 4 seconds long at pitch C4. The model uses 64-bin Mel spectrograms as input and predicts 16 continuous parameters including oscillator positions, gains, filter cutoffs/resonance, envelope timings, and reverb mix. The AST architecture employs 12 encoder layers with 12 attention heads and 768 hidden size, followed by a small 3-layer MLP. Training uses MSE loss for 50 epochs with Adam optimizer (learning rate 5e-5). The model is compared against 5-layer MLP and 5-layer CNN baselines on both parameter reconstruction (MSE) and audio reconstruction (spectral convergence) metrics.

## Key Results
- AST achieves parameter MSE of 0.031 versus 0.077 (MLP) and 0.094 (CNN)
- AST achieves spectral convergence of 0.616 versus 4.608 (MLP) and 5.372 (CNN)
- Model successfully reconstructs out-of-domain sounds including vocal imitations and instrument samples
- AST demonstrates superior ability to capture subtle timbral relationships compared to CNN and MLP baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AST outperforms CNN and MLP baselines because it can capture long-range dependencies and global structure in audio spectrograms that CNNs miss due to local receptive fields.
- Mechanism: The transformer architecture processes the entire spectrogram as a sequence of patches, allowing each patch to attend to all other patches through self-attention. This global context enables better parameter inference from subtle timbral cues.
- Core assumption: The parameter space of Massive has relationships that span the entire spectrogram, not just local regions.
- Evidence anchors:
  - [abstract] "AST model significantly outperforms multi-layer perceptron and convolutional neural network baselines"
  - [section] "ASTs offer a natural solution to this problem due to their strong performance across supervised learning tasks in the music domain"
- Break condition: If parameter relationships are purely local (e.g., specific filter cutoff only affects nearby frequency regions), the global attention may be unnecessary overhead.

### Mechanism 2
- Claim: Using randomly generated datasets of paired parameters and rendered audio enables effective training of transformers despite their data hunger.
- Mechanism: Random parameter sampling from realistic distributions creates a diverse dataset covering the usable parameter subspace, providing sufficient training examples for the transformer to learn the mapping from audio to parameters.
- Core assumption: The random sampling strategy captures enough of the parameter space diversity to generalize to real-world sounds.
- Evidence anchors:
  - [section] "we create a synthetic dataset by randomly sampling parameter values and generating corresponding one-shots"
  - [section] "we sample them from within the distribution of parameters extracted across all the Massive factory presets"
- Break condition: If the random sampling misses critical regions of the parameter space that correspond to musically relevant sounds, the model will fail on those inputs.

### Mechanism 3
- Claim: The audio spectrogram transformer's patch-based processing enables effective handling of 2D spectrogram data without requiring 2D convolutions.
- Mechanism: By splitting the spectrogram into 16×16 patches with stride 6, the model converts 2D spatial information into a 1D sequence while preserving local spatial relationships within patches and global relationships through attention.
- Core assumption: The patch size and stride preserve sufficient spatial information for parameter inference.
- Evidence anchors:
  - [section] "AST splits the spectrogram into a series of patches across both axes of the spectrogram"
  - [section] "We use 16 × 16 patches with a stride of 6"
- Break condition: If patch boundaries cut across important spectral features, the model may lose critical information.

## Foundational Learning

- Concept: Transformer architecture and self-attention
  - Why needed here: Understanding how transformers process sequences and use self-attention is crucial for grasping why AST works better than CNNs
  - Quick check question: What is the key difference between how transformers and CNNs process spatial information in spectrograms?

- Concept: Audio spectrogram representation
  - Why needed here: The model works with Mel spectrograms as input, so understanding this representation is essential
  - Quick check question: How does a Mel spectrogram differ from a standard Fourier spectrogram, and why might this be beneficial for this task?

- Concept: Differentiable vs non-differentiable synthesizers
  - Why needed here: The paper contrasts its approach with differentiable synthesizer methods, so understanding this distinction is important
  - Quick check question: What are the practical advantages and disadvantages of requiring a synthesizer to be differentiable versus using a black-box approach?

## Architecture Onboarding

- Component map:
  Mel spectrogram → Patch embedding → Positional encoding → Transformer encoder → MLP → Parameter output

- Critical path:
  Mel spectrogram → Patch embedding → Positional encoding → Transformer encoder → MLP → Parameter output

- Design tradeoffs:
  - Patch size vs. resolution: Larger patches reduce sequence length but may lose detail
  - Transformer depth vs. training time: Deeper transformers may perform better but require more data and compute
  - Mel bin count (64 vs 128): Lower resolution speeds training with minimal performance impact

- Failure signatures:
  - Poor parameter reconstruction but reasonable audio: Model learned audio-space mapping but not parameter-space
  - Good parameter reconstruction but poor audio: Model learned parameter mapping but synthesizer implementation has issues
  - Systematic errors on certain parameter types: Model has bias in learning certain parameter relationships

- First 3 experiments:
  1. Compare AST with varying patch sizes (8×8, 16×16, 32×32) on validation set
  2. Test different spectrogram resolutions (32, 64, 128 bins) to find optimal balance
  3. Replace transformer with CNN baseline using similar parameter count to quantify transformer advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does varying the MIDI pitch of sounds in the training dataset affect the model's ability to match oscillator pitch in out-of-domain sounds?
- Basis in paper: [inferred] The paper notes that the model struggles with oscillator pitch matching, particularly when one oscillator is detuned, and suggests this could be due to not varying MIDI pitch in the training dataset.
- Why unresolved: The current dataset uses a fixed MIDI pitch (C4) for all rendered samples, limiting the model's exposure to pitch variations.
- What evidence would resolve it: Training a new model with a dataset that includes samples rendered at multiple pitches and evaluating its performance on out-of-domain sounds with detuned oscillators.

### Open Question 2
- Question: Would incorporating an audio-based loss function improve the model's ability to match sounds with subtle pitch differences, such as detuned oscillators?
- Basis in paper: [inferred] The paper suggests that using only a parameter-only loss function might be a limitation, especially for parameters like oscillator pitch where small changes can significantly affect sound.
- Why unresolved: The current model relies solely on parameter reconstruction loss (MSE) without considering the perceptual audio quality of the reconstructed sound.
- What evidence would resolve it: Comparing the performance of the current model with a new model that uses a combined parameter and audio-based loss function on sounds with subtle pitch differences.

### Open Question 3
- Question: How would the model's performance change if it were trained on a larger and more complex set of synthesizer parameters, including categorical parameters and binary switches?
- Basis in paper: [explicit] The paper states that future work aims to investigate multi-output models that concurrently model continuous and categorical parameters, and experiment with more complex parameter sets.
- Why unresolved: The current model is limited to 16 continuous parameters, which restricts its ability to model the full complexity of synthesizers like Massive.
- What evidence would resolve it: Training and evaluating the model on a dataset with a larger and more diverse set of parameters, including categorical and binary parameters, and comparing its performance to the current model.

## Limitations

- Reliance on Massive synthesizer as sole training source limits generalizability to other synthesis architectures
- Random parameter sampling may miss critical combinations that exist in real-world sound design practices
- Model performance on multi-timbral or complex layered sounds has not been evaluated

## Confidence

- **AST architecture superiority (High):** The quantitative comparison against MLP and CNN baselines provides strong evidence, with clear performance gaps in both parameter MSE (0.031 vs 0.077/0.094) and spectral convergence (0.616 vs 4.608/5.372).

- **Generalization to out-of-domain sounds (Medium):** While audio examples show reasonable results on vocal imitations and instrument sounds, systematic evaluation across diverse sound categories is lacking.

- **Minimal assumptions about synthesis architecture (Medium):** The approach works with Massive's 16 parameters, but it's unclear how well this scales to synthesizers with hundreds of parameters or different parameter types.

## Next Checks

1. Test the trained model on a completely different synthesizer (e.g., Serum or FM8) with minimal fine-tuning to assess true architecture-agnostic capability.

2. Conduct a human perceptual evaluation comparing synthesized outputs from the model against ground truth parameters and other baselines to validate the spectral convergence metric.

3. Evaluate model performance on multi-timbral sounds and complex layered patches to identify practical limitations in real-world usage scenarios.