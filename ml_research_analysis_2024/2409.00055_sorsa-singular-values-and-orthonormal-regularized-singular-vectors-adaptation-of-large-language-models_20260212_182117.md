---
ver: rpa2
title: 'SORSA: Singular Values and Orthonormal Regularized Singular Vectors Adaptation
  of Large Language Models'
arxiv_id: '2409.00055'
source_url: https://arxiv.org/abs/2409.00055
tags:
- arxiv
- sorsa
- lora
- singular
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SORSA, a novel parameter-efficient fine-tuning
  (PEFT) method for large language models. SORSA decomposes pre-trained weights into
  principal and residual components via SVD, training only the principal components
  while freezing the residuals.
---

# SORSA: Singular Values and Orthonormal Regularized Singular Vectors Adaptation of Large Language Models

## Quick Facts
- arXiv ID: 2409.00055
- Source URL: https://arxiv.org/abs/2409.00055
- Authors: Yang Cao; Zhao Song
- Reference count: 26
- Primary result: SORSA achieves 56.03% accuracy on GSM-8K, outperforming LoRA (42.30%) and full fine-tuning (49.05%)

## Executive Summary
This paper introduces SORSA, a novel parameter-efficient fine-tuning method for large language models that decomposes pre-trained weights into principal and residual components via SVD. Unlike LoRA which uses rank-1 updates, SORSA trains only the principal components while freezing the residuals, and applies an orthonormal regularizer to maintain the orthonormality of singular vectors during training. This approach improves conditioning and optimization efficiency, leading to faster convergence and better preservation of pre-trained knowledge structure.

## Method Summary
SORSA decomposes pre-trained weight matrices via SVD into U, Σ, V^T, then separates these into principal components (Up, Sp, Vp) and residual components (Ur, Sr, Vr). The principal components are trainable with an orthonormal regularizer that maintains Up and Vp as orthonormal matrices, while the residuals are frozen. During inference, the adapted weights are reconstructed by summing the updated principal components with the frozen residuals. The method is trained using AdamW with cosine annealing, mixed precision (TF32+BF16), and a regularizer coefficient of 4e-4.

## Key Results
- SORSA achieves 56.03% accuracy on GSM-8K compared to LoRA's 42.30% and full fine-tuning's 49.05%
- Demonstrated faster convergence than LoRA and PiSSA with a convergence rate of (1 - η(μtrain - γCreg))^t
- Showed significantly smaller changes in singular values and vectors compared to other PEFT methods, indicating better preservation of pre-trained knowledge structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SORSA improves conditioning of adapted weights by explicitly regularizing singular vectors to remain orthonormal during training
- Mechanism: The orthonormal regularizer penalizes deviation from orthonormal structure in Up and Vp matrices, preventing condition number increase that typically occurs in LoRA
- Core assumption: Maintaining orthonormal singular vectors preserves the model's original generalization capability while allowing task-specific adaptation
- Evidence anchors: Theorem 5.6 proves regularizer decreases condition number; section 4 describes orthonormal regularizer implementation

### Mechanism 2
- Claim: SORSA achieves faster convergence by better preserving spectral properties of pre-trained weights
- Mechanism: SVD-based initialization and orthonormal regularization create a smoother optimization landscape with better conditioning
- Core assumption: Frozen residual components preserve essential information from pre-training that would otherwise be lost
- Evidence anchors: Theorem 5.4 proves linear convergence rate; section 6.1 shows SORSA maintains better spectral properties

### Mechanism 3
- Claim: SORSA better preserves pre-trained knowledge structure by minimizing alterations to singular values and vectors
- Mechanism: Constrained optimization space prevents catastrophic forgetting seen in standard fine-tuning
- Core assumption: SVD captures the most important geometric structure of pre-trained weights
- Evidence anchors: Section 6.1 analysis shows SORSA demonstrates significantly smaller changes in both singular values and singular vectors compared to other methods

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: SORSA relies on SVD to decompose pre-trained weights into principal and residual components
  - Quick check question: What are the three components returned by SVD and how are they used in SORSA's initialization?

- Concept: Condition Number
  - Why needed here: The paper explicitly argues that a lower condition number leads to better generalization and more efficient optimization
  - Quick check question: How does the orthonormal regularizer mathematically reduce the condition number according to Theorem 5.6?

- Concept: Low-Rank Approximation
  - Why needed here: SORSA uses low-rank approximation to select the most significant singular components for training
  - Quick check question: Why does SORSA freeze the residual components rather than training them like PiSSA?

## Architecture Onboarding

- Component map: Pre-trained weight W₀ -> SVD(U, Σ, Vᵀ) -> Principal (Up, Sp, Vp) + Residual (Ur, Sr, Vr) -> Trainable adapter Wp = Up diag(Sp) Vpᵀ with orthonormal regularizer

- Critical path: 1) Perform SVD on pre-trained weight 2) Initialize Up, Sp, Vp from top-r components 3) Freeze Ur, Sr, Vr 4) Train Up, Sp, Vp with Ltrain + γLreg 5) Merge adapter with frozen components during inference

- Design tradeoffs:
  - Memory vs. performance: Higher rank r gives better performance but requires more parameters
  - Regularization strength: Higher γ preserves orthonormality better but may slow task-specific adaptation
  - Residual freezing: Preserves pre-trained knowledge but may limit adaptation to very different tasks

- Failure signatures:
  - Over-regularization: Training loss plateaus early, underfits target task
  - Under-regularization: Condition number increases, shows signs of catastrophic forgetting
  - Rank too low: Cannot capture sufficient task-specific information, underperforms
  - Rank too high: Loses parameter efficiency benefits, approaches full fine-tuning

- First 3 experiments:
  1. Compare convergence curves (loss vs. steps) of SORSA vs. LoRA on a simple downstream task
  2. Measure condition number evolution during training for SORSA with/without regularizer
  3. Analyze singular value and vector preservation (∆Σ, ∆D) for SORSA vs. other PEFT methods on the same task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the orthonormal regularizer affect the model's ability to learn task-specific features versus preserving pre-trained knowledge?
- Basis in paper: [explicit] The paper states that SORSA "better preserves the pre-trained model's underlying knowledge structure while making precise adjustments for the downstream task"
- Why unresolved: While the paper demonstrates that SORSA preserves pre-trained knowledge better than other methods, it doesn't quantify the trade-off between preserving general knowledge and learning task-specific features

### Open Question 2
- Question: What is the optimal rank r for the principal components in SORSA across different model architectures and tasks?
- Basis in paper: [inferred] The paper mentions that "r ≪ k" where k is the full rank, but doesn't provide specific guidance on how to choose r
- Why unresolved: The paper uses fixed ranks without exploring sensitivity to this hyperparameter or providing a principled way to select it

### Open Question 3
- Question: How does SORSA's performance scale with model size beyond 7B parameters?
- Basis in paper: [inferred] The paper only tests SORSA on 7B parameter models
- Why unresolved: The computational benefits and convergence properties may change significantly for larger models

### Open Question 4
- Question: What is the theoretical relationship between the condition number improvement from the regularizer and the observed reduction in overfitting?
- Basis in paper: [explicit] Theorem 5.6 proves that the regularizer leads to smaller condition numbers, and the paper observes reduced overfitting
- Why unresolved: While the paper establishes that SORSA improves conditioning and observes less overfitting, it doesn't formally connect these two phenomena

## Limitations

- Limited task diversity: Evaluation focuses primarily on math-focused datasets (GSM-8K, MATH) without testing on other downstream tasks
- Evaluation scope: Full fine-tuning baseline only achieved 49.05% accuracy without exploring optimal performance
- Implementation details: Several critical implementation details are underspecified, including exact orthonormal regularizer formulation

## Confidence

**High Confidence**:
- SORSA decomposes pre-trained weights via SVD into principal and residual components
- The orthonormal regularizer can mathematically reduce condition number when properly tuned
- SORSA achieves better GSM-8K accuracy than LoRA with rank 128 (56.03% vs 42.30%)

**Medium Confidence**:
- SORSA shows faster convergence than LoRA and PiSSA (convergence curves not fully reported)
- SORSA better preserves pre-trained knowledge structure (analysis limited to singular value/vector changes)
- SORSA demonstrates reduced overfitting compared to other methods (only shown on one dataset)

**Low Confidence**:
- SORSA provides "significant" efficiency gains on large models (only 7B models tested)
- SORSA's performance advantage generalizes across diverse downstream tasks (limited task diversity)
- The theoretical convergence rate translates to practical training speedups (theoretical vs. empirical gap)

## Next Checks

1. Generate side-by-side training curves (loss vs. steps) for SORSA, LoRA, and full fine-tuning on GSM-8K to verify the claimed faster convergence rate and measure time-to-accuracy for reaching 50% accuracy.

2. Evaluate SORSA on diverse downstream tasks including commonsense reasoning (HellaSwag), code generation (HumanEval+), and language understanding (GLUE) to assess whether the math-focused performance advantage generalizes.

3. During training, measure and plot the evolution of condition numbers for Wp in SORSA with/without regularizer, and compare with LoRA to verify that SORSA maintains lower condition numbers throughout training and that this correlates with better generalization.