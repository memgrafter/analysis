---
ver: rpa2
title: Your Transformer is Secretly Linear
arxiv_id: '2405.12250'
source_url: https://arxiv.org/abs/2405.12250
tags:
- linearity
- layers
- linear
- transformer
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals that transformer decoder layers exhibit surprisingly
  high linearity, with Procrustes similarity scores near 0.99 between sequential layers.
  By removing or linearly approximating the most linear blocks, significant model
  pruning is possible without major performance loss.
---

# Your Transformer is Secretly Linear

## Quick Facts
- arXiv ID: 2405.12250
- Source URL: https://arxiv.org/abs/2405.12250
- Authors: Anton Razzhigaev; Matvey Mikhalchuk; Elizaveta Goncharova; Nikolai Gerasimenko; Ivan Oseledets; Denis Dimitrov; Andrey Kuznetsov
- Reference count: 4
- One-line primary result: Transformer decoder layers exhibit high linearity, enabling significant pruning without major performance loss

## Executive Summary
This paper reveals that transformer decoder layers exhibit surprisingly high linearity, with Procrustes similarity scores near 0.99 between sequential layers. By removing or linearly approximating the most linear blocks, significant model pruning is possible without major performance loss. The authors introduce a cosine-similarity-based regularization that improves performance on benchmarks like Tiny Stories and SuperGLUE while reducing layer linearity. Results show linearity scores increase during fine-tuning, contrasting with pretraining dynamics, and models with regularization exhibit better expressiveness in linear probing tasks.

## Method Summary
The authors analyze embedding transformations between sequential transformer layers using Procrustes similarity to quantify linearity. They implement cosine-similarity-based regularization during pretraining to reduce layer linearity, then evaluate pruning strategies by removing or linearly approximating the most linear blocks. The approach is tested across multiple open-source transformer models (GPT, LLaMA, OPT, BLOOM) and custom models trained on TinyStories and Tiny-textbooks datasets, with performance measured through perplexity on WikiText and benchmarks including SuperGLUE and TinyStories.

## Key Results
- Transformer decoder layers show Procrustes similarity scores near 0.99, indicating near-perfect linear relationships between sequential embeddings
- Removing or linearly approximating highly linear layers does not significantly affect model performance or loss
- Cosine-similarity-based regularization improves performance on Tiny Stories and SuperGLUE benchmarks while reducing layer linearity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer decoder layers exhibit high linearity between sequential embeddings, quantified by Procrustes similarity scores near 0.99.
- Mechanism: The embedding transformation between adjacent layers can be approximated as a linear mapping because the contribution of each block to the residual stream has a low norm, and cosine similarity between adjacent embeddings is high.
- Core assumption: The low norm contribution of transformer blocks and high cosine similarity between sequential embeddings enable near-linear relationships.
- Evidence anchors:
  - [abstract] "We analyze embedding transformations between sequential layers, uncovering a near-perfect linear relationship (Procrustes similarity score of 0.99)."
  - [section] "the linearity scores of layers in all tested transformer decoders were found to be close to 1, indicating a high degree of linearity in embedding transformations."
  - [corpus] Weak evidence; neighboring papers discuss linearity in transformers but don't directly confirm this specific mechanism.
- Break condition: If transformer block contributions have high norms or cosine similarity between sequential embeddings drops significantly, the linearity assumption breaks down.

### Mechanism 2
- Claim: Removing or linearly approximating the most linear blocks does not significantly affect model performance.
- Mechanism: Since certain layers exhibit high linearity, they can be removed or replaced with linear approximations without substantial loss of information or functionality, as verified by maintaining perplexity and performance metrics.
- Core assumption: High linearity implies redundancy that can be removed or approximated without critical information loss.
- Evidence anchors:
  - [abstract] "Our experiments show that removing or linearly approximating some of the most linear blocks of transformers does not affect significantly the loss or model performance."
  - [section] "removing the most linear layers without a significant loss in performance" and "perplexity is less affected by pruning with linear replacements and following distillation."
  - [corpus] Weak evidence; neighboring papers discuss pruning but not specifically linear approximation based on this mechanism.
- Break condition: If non-linear components are critical for specific tasks or the linear approximations fail to capture essential features, performance will degrade.

### Mechanism 3
- Claim: Cosine-similarity-based regularization reduces layer linearity while improving model performance.
- Mechanism: By encouraging embeddings from sequential layers to align closer (cosine similarity closer to 1), the model adapts by enhancing non-linear processing in the residual stream, compensating for reduced variability and improving expressiveness.
- Core assumption: Aligning embeddings across layers forces the model to develop more expressive non-linear capabilities to maintain performance.
- Evidence anchors:
  - [abstract] "we introduce a cosine-similarity-based regularization, aimed at reducing layer linearity. This regularization improves performance metrics on benchmarks like Tiny Stories and SuperGLUE."
  - [section] "the most promising results are achieved using a cosine-based approach that encourages the embeddings of sequential layers to converge" and "embeddings from the model pretrained with regularization exhibit better performance."
  - [corpus] Weak evidence; no direct support from neighboring papers for this specific regularization approach.
- Break condition: If the regularization term is too strong, it may overly constrain the model, leading to underfitting or loss of necessary non-linear expressiveness.

## Foundational Learning

- Concept: Procrustes similarity analysis
  - Why needed here: Used to quantify the degree of linear dependence between embeddings of sequential layers by finding the optimal linear transformation that minimizes squared errors.
  - Quick check question: How does Procrustes similarity differ from standard L2 norm in evaluating linearity, and why is it more suitable here?

- Concept: Residual stream and layer contributions
  - Why needed here: Understanding how each transformer block contributes to the residual stream is crucial, as low norm contributions enable high linearity between layers.
  - Quick check question: What role does the residual connection play in maintaining linearity between sequential layers?

- Concept: Regularization techniques (cosine similarity vs MSE)
  - Why needed here: Different regularization approaches affect linearity and model performance differently; understanding their impact is key to optimizing model efficiency.
  - Quick check question: How does cosine-similarity-based regularization influence the alignment of embeddings across layers compared to MSE regularization?

## Architecture Onboarding

- Component map:
  Input embeddings → Transformer decoder layers (with residual connections) → Output embeddings
  Key components: Self-attention mechanisms, feed-forward networks, layer normalization, residual connections
  Linearity analysis focuses on transformations between sequential layer embeddings

- Critical path:
  Embedding transformation analysis → Linearity quantification (Procrustes similarity) → Pruning/linear approximation → Performance evaluation
  Regularization pretraining → Embedding alignment → Model expressiveness → Benchmark testing

- Design tradeoffs:
  High linearity allows for pruning but may reduce model capacity for complex tasks
  Regularization reduces linearity but improves performance; balance needed to avoid underfitting
  Linear approximations reduce computational cost but may miss critical non-linear features

- Failure signatures:
  Significant performance drop after pruning indicates over-reliance on removed linear layers
  Increased perplexity suggests linear approximations are insufficient
  Over-regularization leading to poor generalization on benchmarks

- First 3 experiments:
  1. Measure Procrustes similarity between sequential layers of a pretrained transformer to confirm high linearity scores.
  2. Remove the most linear layer(s) and evaluate performance impact on a validation set to test pruning viability.
  3. Apply cosine-similarity-based regularization during pretraining and compare linearity scores and benchmark performance against baseline models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between the residual component's low norm and the overall linearity of transformer layers?
- Basis in paper: [explicit] The paper states that "linearity decreases when the residual component is removed due to a consistently low output norm of the transformer layer" and shows this relationship in Figure 3.
- Why unresolved: While the paper observes this relationship, it doesn't explain the mechanistic connection between the residual component's norm and the linearity property.
- What evidence would resolve it: Experiments showing how modifying the residual connection's norm affects linearity scores, or theoretical analysis explaining why low residual norms lead to higher linearity.

### Open Question 2
- Question: How does the feature triggering regime hypothesis explain the long-tailed distribution of L2 errors in some transformer layers?
- Basis in paper: [explicit] The paper mentions this hypothesis when discussing Figure 9, which shows "long tailed distribution of L2 errors" in some layers of OPT-1.3B.
- Why unresolved: The paper only briefly mentions this hypothesis without providing a detailed explanation or testing it.
- What evidence would resolve it: Analysis of specific features that trigger non-linear behavior, or experiments showing how rare tokens with high non-linearity affect model behavior.

### Open Question 3
- Question: Why does cosine similarity regularization, which makes embeddings more similar across layers, lead to improved model expressiveness?
- Basis in paper: [explicit] The paper notes this is a "contradictory outcome" and suggests "the model may compensate for the reduction in variability by amplifying non-linear processing capabilities in the residual stream."
- Why unresolved: The paper only offers a hypothesis without experimental validation or detailed analysis of the underlying mechanisms.
- What evidence would resolve it: Detailed analysis of residual stream dynamics after regularization, or experiments showing how model expressiveness changes with different degrees of embedding similarity across layers.

## Limitations

- Study focuses primarily on decoder-only transformers, leaving generalization to encoder-decoder architectures unexplored
- Procrustes similarity metric may not capture all aspects of non-linearity critical for certain tasks
- Regularization approach requires careful hyperparameter tuning to avoid underfitting

## Confidence

- **High Confidence**: The core observation that transformer decoder layers exhibit high linearity (Procrustes similarity ~0.99) is well-supported by multiple experiments across different model architectures and datasets.
- **Medium Confidence**: The pruning and linear approximation results are convincing but could benefit from more extensive ablation studies across diverse tasks.
- **Medium Confidence**: The regularization approach improves performance metrics, but the mechanism by which cosine-similarity-based regularization enhances model expressiveness needs further theoretical grounding.

## Next Checks

1. **Cross-Architecture Validation**: Test linearity analysis on encoder-decoder transformers (e.g., T5, BART) to determine if the observed high linearity generalizes beyond decoder-only models.

2. **Task-Specific Performance Analysis**: Evaluate pruned models on tasks requiring complex reasoning or non-linear transformations to identify scenarios where linear approximations fail.

3. **Regularization Ablation Study**: Systematically vary the strength of cosine-similarity regularization to identify optimal regularization parameters and analyze the trade-off between linearity reduction and performance improvement.