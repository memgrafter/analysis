---
ver: rpa2
title: Can Transformers Reason Logically? A Study in SAT Solving
arxiv_id: '2410.07432'
source_url: https://arxiv.org/abs/2410.07432
tags:
- token
- each
- transformer
- attention
- literal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether decoder-only Transformers can perform
  logical reasoning by formally proving that such models can solve 3-SAT problems
  through Chain-of-Thought (CoT) reasoning involving deduction and backtracking. The
  authors provide a constructive proof that Transformers can simulate the DPLL SAT-solving
  algorithm, implement this construction as a PyTorch model using a custom tool (PARAT)
  that compiles NumPy-style specifications into Transformer weights, and empirically
  validate the model's correctness on random 3-SAT instances.
---

# Can Transformers Reason Logically? A Study in SAT Solving

## Quick Facts
- arXiv ID: 2410.07432
- Source URL: https://arxiv.org/abs/2410.07432
- Authors: Leyan Pan; Vijay Ganesh; Jacob Abernethy; Chris Esposo; Wenke Lee
- Reference count: 40
- This paper provides a constructive proof that decoder-only Transformers can solve 3-SAT problems through Chain-of-Thought reasoning involving deduction and backtracking.

## Executive Summary
This paper investigates whether decoder-only Transformers can perform logical reasoning by formally proving they can solve 3-SAT problems through Chain-of-Thought reasoning with deduction and backtracking. The authors provide a constructive proof that Transformers can simulate the DPLL SAT-solving algorithm, implement this construction as a PyTorch model using a custom tool (PARAT) that compiles NumPy-style specifications into Transformer weights, and empirically validate the model's correctness on random 3-SAT instances. Training experiments demonstrate that Transformers can learn logical reasoning procedures from CoT traces, achieving strong out-of-distribution generalization within the same problem size but showing limited ability to generalize to larger instances.

## Method Summary
The paper presents a theoretical construction showing decoder-only Transformers can decide 3-SAT using backtracking and deduction via Chain-of-Thought reasoning. The method encodes clauses and partial assignments as vectors where logical operations (satisfiability checking, conflict detection, unit propagation) can be approximated by attention heads and MLP layers. A custom tool called PARAT compiles NumPy-style specifications of these operations into Transformer weights. The authors implement this construction as a PyTorch model and validate it on random 3-SAT instances with varying difficulty distributions. Training experiments use Llama architecture with CoT traces as targets, evaluating both in-distribution and out-of-distribution generalization.

## Key Results
- The theoretical construction proves decoder-only Transformers can solve 3-SAT through CoT reasoning with deduction and backtracking
- Training on CoT traces allows Transformers to achieve OOD generalization within same variable count but shows limited length generalization
- Empirical validation demonstrates perfect accuracy on marginal datasets and strong performance on random 3-SAT instances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers can simulate the DPLL SAT-solving algorithm using Chain-of-Thought with parallel clause processing
- Mechanism: The construction encodes clauses and partial assignments as vectors where logical operations (satisfiability checking, conflict detection, unit propagation) can be approximated by attention heads and MLP layers operating on these encodings
- Core assumption: Vector encodings preserve logical relationships such that dot products and element-wise operations correspond to logical operations on clauses and assignments
- Evidence anchors: [abstract] "we prove by construction that decoder-only Transformers can decide 3-SAT... using backtracking and deduction via Chain-of-Thought"; [section] "Let F =V i∈[c] Ci be a 3-SAT formula... the following properties hold: 1. Satisfiability Checking: A |= F ⇐ ⇒ min i∈[c] E(Ci) · E(A) ≥ 1"

### Mechanism 2
- Claim: Soft attention with scaling can approximate saturated attention for SAT solving
- Mechanism: By scaling attention logits by a factor β, softmax attention can approximate the hard attention required for exact logical operations, with error decreasing exponentially in β
- Core assumption: There exists sufficient margin between relevant and irrelevant attention scores to make softmax concentrate on correct positions
- Evidence anchors: [abstract] "The trained models demonstrate strong out-of-distribution generalization... but has limited length generalization, which is consistent with the implications of our theoretical result"; [section] "Corollary C.8 (Softmax Attention Can Approximate Saturated Attention, implied by Lemma C.6)"

### Mechanism 3
- Claim: Transformers can learn logical reasoning procedures from CoT traces through training
- Mechanism: Training on algorithmic traces of the theoretical construction allows models to learn the reasoning patterns needed for SAT solving, achieving OOD generalization within same variable count
- Core assumption: The CoT traces contain sufficient information about the reasoning process for models to extract and apply the patterns
- Evidence anchors: [abstract] "our supporting training experiments suggest that training on CoT encoding 3-SAT reasoning traces allows Transformer models to achieve out-of-distribution generalization"; [section] "Results In Figure 3, our results indicate that performance degrades drastically beyond the training regime when the number of variables increases"

## Foundational Learning

- Concept: Boolean satisfiability and 3-SAT problem formulation
  - Why needed here: The entire construction builds on understanding how 3-SAT formulas work and how to represent them
  - Quick check question: Can you explain the difference between a clause, literal, and variable in 3-SAT?

- Concept: Vector encoding of logical structures
  - Why needed here: The proof relies on encoding clauses and assignments as vectors where dot products correspond to logical operations
  - Quick check question: How would you encode a partial assignment {x1=T, x2=F} as a vector given 4 variables?

- Concept: Attention mechanisms and their computational properties
  - Why needed here: Understanding how attention can approximate hard selection operations is crucial for the construction
  - Quick check question: What happens to softmax attention when logits are scaled by a large factor?

## Architecture Onboarding

- Component map: Token embedding layer → 7 transformer layers (each with multi-head attention + MLP) → Output projection layer
- Critical path: Input DIMACS formula → Clause/assignment encoding → Attention-based logical operations → Decision token generation → Repeat until SAT/UNSAT
- Design tradeoffs: Fixed architecture size vs. problem size (O(p²) parameters for p variables) vs. need for larger models for larger problems
- Failure signatures: Perfect accuracy on training distribution but poor length generalization; soft attention errors preventing correct logical deductions
- First 3 experiments:
  1. Verify compiled model achieves perfect accuracy on marginal dataset with p=6-10 variables
  2. Test how accuracy degrades as β scaling parameter decreases (measuring soft attention error impact)
  3. Train model on p∈[6,10] and test OOD on p∈[11,15] to confirm length generalization limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Transformer-based SAT solvers scale with problem size, particularly in terms of CoT length and computational efficiency, and what are the theoretical bounds on this scaling?
- Basis in paper: [explicit] The paper discusses the exponential worst-case CoT length and the non-uniform computational requirements of the Transformer construction for SAT-solving.
- Why unresolved: While the paper provides a theoretical construction and empirical validation, it does not fully explore the practical scalability limits of the model or provide comprehensive theoretical bounds on performance degradation with increasing problem size.
- What evidence would resolve it: Extensive experiments on SAT instances with varying problem sizes, coupled with theoretical analysis of the computational complexity and CoT length scaling, would provide insights into the practical and theoretical limits of Transformer-based SAT solvers.

### Open Question 2
- Question: Can Transformers learn to generalize their logical reasoning capabilities to larger SAT instances beyond those seen during training, and what architectural or training modifications might enable this?
- Basis in paper: [explicit] The paper shows that trained models can generalize within the same input lengths but struggle to generalize to larger instances, highlighting limitations in length generalization.
- Why unresolved: The paper does not explore potential architectural or training strategies that could overcome these generalization limitations, leaving open the question of whether Transformers can be adapted to handle larger problem sizes effectively.
- What evidence would resolve it: Experiments with modified architectures or training regimes, such as curriculum learning or architectural changes that promote length generalization, could demonstrate whether Transformers can be adapted to solve larger SAT instances.

### Open Question 3
- Question: How do the logical reasoning capabilities of Transformers compare to traditional SAT solvers in terms of efficiency and accuracy, and what are the trade-offs between the two approaches?
- Basis in paper: [inferred] The paper presents a theoretical construction and empirical validation of Transformer-based SAT solving, but does not directly compare its performance to traditional SAT solvers.
- Why unresolved: The paper focuses on the theoretical and empirical aspects of Transformer-based SAT solving without benchmarking against established SAT solvers, leaving the relative strengths and weaknesses of each approach unexplored.
- What evidence would resolve it: Comparative experiments measuring the efficiency and accuracy of Transformer-based SAT solvers against traditional solvers on a diverse set of SAT instances would elucidate the trade-offs and potential advantages of each approach.

## Limitations

- The theoretical construction requires O(p²) parameters for p variables, making it impractical for large SAT instances despite being theoretically valid
- The empirical validation focuses on relatively small problem sizes (p=6-15 variables) and doesn't demonstrate scalability to realistically-sized SAT problems
- The reliance on CoT traces for training introduces uncertainty about whether learned procedures capture genuine reasoning or merely memorize patterns specific to the training distribution

## Confidence

- High Confidence: The theoretical construction proving that decoder-only Transformers can simulate DPLL SAT solving is sound
- Medium Confidence: The claim that Transformers can learn logical reasoning procedures from CoT traces is moderately supported
- Low Confidence: The practical scalability of this approach to realistically-sized SAT problems is questionable

## Next Checks

1. **Scalability Stress Test**: Implement the construction for larger variable counts (p=20-50) and measure accuracy degradation to test practical viability

2. **Cross-Problem Generalization**: Train models on small SAT instances but test them on completely different logical reasoning tasks (e.g., graph coloring, planning problems) to validate genuine reasoning capabilities

3. **Attention Approximation Analysis**: Systematically vary the β scaling parameter and measure resulting accuracy on marginal datasets to quantify sensitivity to soft attention approximation