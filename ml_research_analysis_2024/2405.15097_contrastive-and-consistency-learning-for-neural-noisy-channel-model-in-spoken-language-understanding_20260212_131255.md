---
ver: rpa2
title: Contrastive and Consistency Learning for Neural Noisy-Channel Model in Spoken
  Language Understanding
arxiv_id: '2405.15097'
source_url: https://arxiv.org/abs/2405.15097
tags:
- noisy
- clean
- learning
- transcripts
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of improving intent classification
  in Spoken Language Understanding (SLU) under Automatic Speech Recognition (ASR)
  errors. The core method idea is a two-stage Contrastive and Consistency Learning
  (CCL) approach that aligns clean and noisy ASR transcripts at both word and utterance
  levels through token-based contrastive learning, followed by consistency learning
  to maintain coherence between clean and noisy latent features.
---

# Contrastive and Consistency Learning for Neural Noisy-Channel Model in Spoken Language Understanding

## Quick Facts
- arXiv ID: 2405.15097
- Source URL: https://arxiv.org/abs/2405.15097
- Authors: Suyoung Kim; Jiyeon Hwang; Ho-Young Jung
- Reference count: 33
- The paper proposes a two-stage Contrastive and Consistency Learning (CCL) approach that improves intent classification accuracy under ASR errors, achieving 73.12% accuracy on SLURP with 56% WER noisy transcripts.

## Executive Summary
This paper addresses the challenge of intent classification in Spoken Language Understanding (SLU) when faced with Automatic Speech Recognition (ASR) errors. The authors propose a two-stage Contrastive and Consistency Learning (CCL) method that first aligns clean and noisy ASR transcripts at both word and utterance levels through token-based contrastive learning, then maintains coherence between clean and noisy latent features through consistency learning. Experiments on four benchmark datasets (SLURP, Timers, FSC, SNIPS) demonstrate that CCL outperforms existing methods, achieving significant accuracy improvements particularly on datasets with high WER noisy transcripts.

## Method Summary
The proposed Contrastive and Consistency Learning (CCL) method employs a two-stage training approach. Stage 1 performs token-based contrastive learning using edit distance to align error tokens between clean and noisy transcripts, creating positive and negative pairs for contrastive loss. Stage 2 adds consistency learning to maintain coherence between clean and noisy latent features using L2 loss and KL divergence. The method uses two RoBERTa-based networks (reference and inference) and combines three loss functions: selective-token contrastive loss, utterance contrastive loss, and consistency loss. The model is trained on paired clean and noisy ASR transcripts and evaluated on intent classification accuracy and macro F1-score across four benchmark datasets.

## Key Results
- CCL achieves 73.12% accuracy on SLURP dataset with 56% WER noisy transcripts, a 2.59% improvement over the best baseline
- The method shows consistent performance improvements across all four benchmark datasets (SLURP, Timers, FSC, SNIPS)
- CCL maintains better performance on clean transcripts compared to Noisy-CE baseline while being more robust to varying WER levels
- The two-stage training approach (contrastive then consistency) demonstrates effectiveness in handling ASR errors compared to single-stage methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-based contrastive learning aligns ASR error tokens between clean and noisy transcripts at both word and utterance levels.
- Mechanism: Uses edit distance to identify positive token pairs (mismatched tokens requiring minimal edits) and applies contrastive loss to pull their representations closer while pushing negative pairs apart.
- Core assumption: Edit distance provides an accurate alignment between error tokens in clean and noisy transcripts.
- Evidence anchors:
  - [abstract] "correlates error patterns between clean and noisy ASR transcripts"
  - [section 3.3] "We propose the selective-token contrastive learning to create pairs between word tokens via an edit distance algorithm"
  - [corpus] Weak - corpus papers don't specifically address token-level ASR error alignment
- Break condition: If ASR errors are too severe (e.g., unrecognizable meaning), edit distance alignment becomes unreliable and contrastive learning fails.

### Mechanism 2
- Claim: Consistency learning maintains coherence between clean and noisy latent features to prevent misclassification.
- Mechanism: Projects encoder outputs through linear layers and applies L2 loss between clean and noisy latent features, plus KL divergence between their probability distributions.
- Core assumption: Clean transcript latent features contain better semantic information that noisy features should align with.
- Evidence anchors:
  - [abstract] "emphasizes the consistency of the latent features of the two transcripts"
  - [section 3.4] "The purpose of consistency learning is to prevent misclassification by additionally utilizing referenced Xc when ˆX is given as input"
  - [corpus] Weak - corpus papers focus on end-to-end approaches rather than consistency learning between clean/noisy pairs
- Break condition: If reference network is poorly trained or clean transcripts are too different from noisy ones, consistency loss provides misleading gradients.

### Mechanism 3
- Claim: Two-stage training (contrastive then consistency) progressively improves ASR robustness by first aligning tokens then maintaining feature coherence.
- Mechanism: Stage 1 performs token-based contrastive learning to correlate error patterns; Stage 2 adds consistency learning to preserve clean feature semantics in noisy predictions.
- Core assumption: Sequential training allows model to first learn error alignment before focusing on feature preservation.
- Evidence anchors:
  - [abstract] "performs a token-based contrastive learning followed by a consistency learning"
  - [section 3] "In this paper, we introduce Contrastive and Consistency Learning (CCL) to perform this two-stage"
  - [corpus] Weak - corpus papers don't describe two-stage contrastive-consistency approaches
- Break condition: If stages are not properly balanced (λctr, λcon hyperparameters), model may overfit to either token alignment or feature consistency at expense of overall performance.

## Foundational Learning

- Concept: Edit distance algorithms for sequence alignment
  - Why needed here: To identify corresponding tokens between clean and noisy transcripts when they're not sequentially aligned due to insertions, deletions, and substitutions
  - Quick check question: What edit operations (insert, delete, substitute) would align "up" → "off" and how many edits does it require?

- Concept: Contrastive learning objectives and negative sampling
  - Why needed here: To train model to distinguish between matching (positive) and non-matching (negative) token pairs across clean/noisy transcript pairs
  - Quick check question: How does the contrastive loss formula ensure that positive pairs are pulled together while negative pairs are pushed apart?

- Concept: BERT/RoBERTa encoder architectures and CLS token usage
  - Why needed here: Encoder provides contextual token representations, and CLS token captures utterance-level semantics for utterance contrastive learning
  - Quick check question: Why use the CLS token representation for utterance-level contrastive learning rather than averaging all token representations?

## Architecture Onboarding

- Component map: Reference Network -> RoBERTa encoder + linear projection; Inference Network -> RoBERTa encoder + linear projection; Token Alignment Module -> Edit distance algorithm; Loss Functions -> Selective-token contrastive loss, utterance contrastive loss, consistency loss

- Critical path: Clean transcript → Reference encoder → CLS representation → Selective contrastive loss; Noisy transcript → Inference encoder → CLS representation → Selective contrastive loss; Both → Linear projections → Consistency loss

- Design tradeoffs: Two-network architecture increases training complexity but maintains inference efficiency by only using inference network at test time

- Failure signatures:
  - Poor alignment between clean/noisy tokens suggests edit distance isn't capturing error patterns
  - Inconsistent predictions across seeds indicates stability issues with contrastive learning
  - Performance drop on clean transcripts suggests consistency learning is over-regularizing

- First 3 experiments:
  1. Test edit distance alignment on simple substitution pairs (up→off, light→night) to verify token matching
  2. Evaluate contrastive loss performance with only utterance-level alignment (no token-level) to isolate effects
  3. Compare consistency loss vs cross-entropy fine-tuning on clean transcript accuracy to measure regularization impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CCL method perform when ASR errors are so severe that the original meaning is unrecognizable?
- Basis in paper: [explicit] The paper mentions that when ASR errors are extremely severe, the model trained with CCL can predict wrong answers as the original meaning becomes unrecognizable.
- Why unresolved: The paper acknowledges this limitation but does not provide a solution or quantify the performance degradation in such extreme cases.
- What evidence would resolve it: Experiments comparing CCL performance on ASR transcripts with varying levels of error severity, particularly focusing on cases where the original meaning is significantly distorted.

### Open Question 2
- Question: Can the CCL method be effectively extended to multilingual ASR error handling?
- Basis in paper: [inferred] The paper mentions that most ASR error studies have centered on English and that expanding to multilingual ASR error problems is a future work.
- Why unresolved: The paper does not explore the performance of CCL on non-English languages or discuss the challenges of multilingual ASR error handling.
- What evidence would resolve it: Experiments evaluating CCL on multilingual ASR datasets and analyzing the method's effectiveness across different languages and error patterns.

### Open Question 3
- Question: How does the CCL method compare to fine-tuning Large Language Models (LLMs) on noisy ASR transcripts?
- Basis in paper: [explicit] The paper compares GPT4's ability to understand ASR errors with CCL and finds that CCL performs better in detecting and understanding a wide range of ASR errors.
- Why unresolved: While the paper shows CCL outperforms GPT4 on specific examples, it does not provide a comprehensive comparison of fine-tuned LLMs versus CCL in terms of accuracy, robustness, and computational efficiency.
- What evidence would resolve it: A systematic comparison of CCL with various fine-tuned LLMs on multiple benchmark datasets, measuring performance metrics such as accuracy, robustness to noise, and inference time.

## Limitations
- The edit distance-based token alignment may fail when ASR errors are too severe and original meaning becomes unrecognizable
- The method's performance advantage diminishes on datasets with lower WER, suggesting limited robustness to varying error characteristics
- The interaction between the three loss components (selective-token contrastive, utterance contrastive, and consistency) is not thoroughly analyzed

## Confidence

**High Confidence**: The overall experimental methodology is sound, with proper dataset preparation, baseline comparisons, and statistical significance testing. The reported accuracy improvements over baselines on SLURP are consistent and substantial (2.59% over best baseline at 56% WER).

**Medium Confidence**: The core mechanism of using edit distance for token alignment is reasonable and well-motivated, though its effectiveness depends heavily on the nature and severity of ASR errors. The two-stage training approach follows established machine learning practices.

**Low Confidence**: The claim that consistency learning "prevents misclassification" is weakly supported, as the paper does not provide detailed analysis of failure cases or show how consistency loss specifically improves robustness. The interaction between the three loss components is not thoroughly analyzed.

## Next Checks

1. **Edit Distance Alignment Validation**: Create a controlled test set with artificially generated ASR errors (substitutions, insertions, deletions) at varying severity levels and measure the accuracy of edit distance in correctly identifying semantically corresponding tokens. This will validate whether the token alignment assumption holds across different error types.

2. **Loss Component Ablation**: Implement ablations that remove each loss component (selective-token contrastive, utterance contrastive, consistency) individually and measure the impact on performance across all datasets. This will quantify the contribution of each mechanism and test the claim that two-stage training is necessary.

3. **Error Pattern Analysis**: Analyze model predictions on SLURP test set to identify specific error patterns where CCL succeeds versus baseline methods fail. Focus on cases where clean and noisy transcripts have minimal semantic overlap to test the robustness of the alignment mechanism under severe ASR degradation.