---
ver: rpa2
title: Model Editing at Scale leads to Gradual and Catastrophic Forgetting
arxiv_id: '2401.07453'
source_url: https://arxiv.org/abs/2401.07453
tags:
- editing
- edits
- facts
- edited
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper evaluates the scalability of two model editing methods
  (ROME and MEMIT) when multiple edits are made to a large language model. Key findings:
  (1) Both methods exhibit gradual forgetting - the model progressively loses the
  ability to recall previously edited facts and perform downstream tasks as more edits
  are made.'
---

# Model Editing at Scale leads to Gradual and Catastrophic Forgetting

## Quick Facts
- arXiv ID: 2401.07453
- Source URL: https://arxiv.org/abs/2401.07453
- Reference count: 7
- Key finding: Sequential model editing leads to both gradual and catastrophic forgetting of previously edited facts

## Executive Summary
This paper investigates the scalability of model editing methods when applied repeatedly to large language models. The authors find that both ROME and MEMIT exhibit two types of forgetting: gradual forgetting where the model progressively loses ability to recall previously edited facts, and catastrophic forgetting where a single edit can cause complete model failure. ROME is particularly susceptible to catastrophic forgetting through "disabling edits" that break the model entirely. The study highlights that evaluating model editing methods on downstream tasks is crucial for assessing their practical utility, and that current techniques have serious limitations when scaled to many edits.

## Method Summary
The authors evaluate ROME and MEMIT on the CounterFact dataset (21,919 counterfactual statements) using GPT-2XL. They sequentially apply edits to the model and measure efficacy scores, paraphrase scores, and neighborhood scores after each edit. The edited models are then evaluated on downstream tasks (SST2 and MRPC from GLUE benchmark). The study also tracks the normalized L2 distance between edited layer weights and original weights to analyze how weight drift correlates with forgetting behavior.

## Key Results
- Both ROME and MEMIT exhibit gradual forgetting - the model progressively loses ability to recall previously edited facts as more edits are made
- Both methods also exhibit catastrophic forgetting - a single edit can suddenly cripple the model, causing it to forget all previously edited facts
- ROME is more susceptible to catastrophic forgetting than MEMIT due to "disabling edits" that cause complete model failure
- Evaluating model editing methods on downstream tasks is crucial for assessing their practical utility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential edits cause the edited layer's weights to drift from their original configuration
- Mechanism: Each ROME edit modifies a single layer's weights while holding others constant. Repeated edits incrementally shift this layer away from its original values
- Core assumption: Knowledge is localized to specific layers, and editing requires only changing that layer
- Evidence anchors:
  - [section 3.4]: "As we edit one specific layer of the model continuously while keeping the rest of the model constant, we are constantly changing one part of the model while keeping the remaining part the same."
  - [section 3.4]: "Figure 4 shows the normalized L2 distance between the weights of the edited layer and the original weights of the layer as a function the number of edits made to the model."
- Break condition: When the L2 distance exceeds a threshold, the edited layer becomes incompatible with the rest of the model, triggering catastrophic forgetting

### Mechanism 2
- Claim: Disabling edits are fundamental to ROME but not MEMIT
- Mechanism: Certain facts cannot be successfully edited by ROME without breaking the model. These facts cause immediate model collapse when edited
- Core assumption: ROME's assumption that knowledge can be edited by changing a single layer is flawed for certain facts
- Evidence anchors:
  - [section 3.4]: "We can describe disabling edits as facts that ROME is unable to successfully edit without crippling the model."
  - [section 3.4]: "This also disproves the fundamental hypothesis behind ROME that knowledge in LLMs can be edited by changing the weights of a single layer inside the model."
- Break condition: When a disabling edit is applied, the model loses all functionality and cannot be recovered through further editing

### Mechanism 3
- Claim: Both gradual and catastrophic forgetting occur during sequential editing
- Mechanism: Gradual forgetting happens as the edited layer slowly drifts, causing progressive loss of previously edited facts and downstream task performance. Catastrophic forgetting occurs when the drift crosses a threshold, causing sudden complete failure
- Core assumption: The relationship between layer drift and model performance is continuous until a critical point
- Evidence anchors:
  - [section 3.2]: "Prior to this inflection point, Figure 2 shows an increasing relationship between the number of forgotten facts and the number of edits made to the model."
  - [section 3.4]: "The gradual increase in distance between the original weight and new weights leads to the gradual region of forgetting, whereas the spike in the distance with a single updates leads to catastrophic forgetting."
- Break condition: When layer drift exceeds compatibility threshold, all previously learned knowledge and capabilities are lost

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding how neural networks lose previously learned information when trained on new tasks is fundamental to interpreting the model editing failures
  - Quick check question: What distinguishes catastrophic forgetting from regular forgetting in neural networks?

- Concept: Layer-wise model architecture in transformers
  - Why needed here: Understanding how transformer layers process information and interact is crucial for interpreting why editing a single layer causes problems
  - Quick check question: How do transformer layers combine information from previous layers with their own internal representations?

- Concept: Knowledge localization in LLMs
  - Why needed here: The paper's analysis depends on understanding whether knowledge is stored in specific locations or distributed across the model
  - Quick check question: What evidence supports or contradicts the hypothesis that specific facts are localized to specific model layers?

## Architecture Onboarding

- Component map: Input layer -> Transformer blocks (self-attention + feed-forward) -> Output layer
- Knowledge editing targets specific layers within transformer blocks
- Downstream evaluation uses few-shot prompting on GLUE tasks

- Critical path:
  1. Select fact to edit from CounterFact dataset
  2. Apply ROME or MEMIT editing algorithm to target layer
  3. Evaluate efficacy, paraphrase, and neighborhood scores
  4. Measure forgetting of previously edited facts
  5. Test downstream task performance

- Design tradeoffs:
  - ROME: High efficacy but prone to catastrophic forgetting and disabling edits
  - MEMIT: Lower efficacy but more robust to sequential edits
  - Single-layer editing vs multi-layer editing approaches

- Failure signatures:
  - Sudden drops in efficacy score (>50% decrease)
  - Rapid increase in normalized L2 distance between original and edited weights
  - Complete loss of downstream task performance
  - Inability to successfully apply new edits

- First 3 experiments:
  1. Run ROME on a small subset (10-20 facts) and monitor L2 distance growth and efficacy decay
  2. Apply the same facts in reverse order to test for order dependency
  3. Compare ROME vs MEMIT on identical fact sequences to quantify differences in gradual vs catastrophic forgetting rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications or alternative approaches could prevent gradual and catastrophic forgetting in model editing methods?
- Basis in paper: [explicit] The paper identifies gradual and catastrophic forgetting as major limitations of current model editing methods (ROME and MEMIT) and states that new techniques are needed to counteract these phenomena when scaling
- Why unresolved: The paper does not propose specific solutions or modifications to prevent forgetting, only highlighting the problem and calling for future research to develop scalable model editing methods
- What evidence would resolve it: A proposed architectural modification or alternative approach that demonstrates reduced gradual and catastrophic forgetting when evaluated using the same metrics as the paper (downstream task performance, ability to recall previously edited facts, etc.) would resolve this question

### Open Question 2
- Question: How do disabling edits fundamentally differ from regular edits, and can they be predicted or avoided before applying model editing methods?
- Basis in paper: [explicit] The paper identifies "disabling edits" as single edits that cause complete model failure and discusses how they are fundamental to ROME but not MEMIT
- Why unresolved: The paper does not provide a detailed analysis of what makes certain edits disabling or how to identify them beforehand, only noting that they exist and have different behaviors in different methods
- What evidence would resolve it: A systematic analysis identifying common characteristics of disabling edits (e.g., specific types of facts, patterns in the knowledge being edited) and a method to predict or avoid them before applying edits would resolve this question

### Open Question 3
- Question: How do different model architectures (e.g., beyond GPT-2XL) affect the scalability and forgetting behavior of model editing methods?
- Basis in paper: [inferred] The paper only evaluates ROME and MEMIT on GPT-2XL due to computational constraints and suggests expanding experiments to other models in future work
- Why unresolved: The paper's findings are limited to one specific model architecture, and it is unclear how other architectures might behave differently under the same editing procedures
- What evidence would resolve it: Evaluating model editing methods on a diverse set of model architectures (e.g., different sizes, types, or pre-training objectives) and comparing their forgetting behavior and scalability would resolve this question

## Limitations

- The analysis relies on the assumption that knowledge localization exists in transformers, which remains debated in the literature
- The distinction between disabling edits and regular catastrophic forgetting is not clearly established - it's unclear whether disabling edits represent a fundamentally different failure mode or are simply early indicators of the same underlying mechanism
- The study uses a relatively small model (GPT-2XL) which may not generalize to larger, more capable models where knowledge distribution patterns could differ significantly

## Confidence

- **High confidence**: The empirical observation of gradual forgetting during sequential edits (multiple independent metrics show consistent patterns)
- **Medium confidence**: The characterization of disabling edits as a distinct failure mode (limited sample size and lack of alternative explanations)
- **Low confidence**: The claim that ROME's single-layer editing assumption is fundamentally flawed (correlation vs causation not rigorously established)

## Next Checks

1. Test whether disabling edits persist when using different random seeds or prompt templates, to distinguish between model-specific failures and general limitations of the editing approach
2. Apply the same sequential editing protocol to a larger transformer model (e.g., GPT-2 Large or GPT-2 XL with different random initialization) to assess scalability concerns
3. Implement a multi-layer editing baseline to compare against ROME's single-layer approach and determine whether catastrophic forgetting can be mitigated by editing multiple layers simultaneously