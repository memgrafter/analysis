---
ver: rpa2
title: 'CogBench: a large language model walks into a psychology lab'
arxiv_id: '2402.18225'
source_url: https://arxiv.org/abs/2402.18225
tags:
- llms
- task
- language
- option
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CogBench, a novel benchmark for evaluating
  large language models (LLMs) using ten behavioral metrics derived from seven cognitive
  psychology experiments. The benchmark assesses LLMs across domains including probabilistic
  reasoning, exploration, meta-cognition, learning, and risk-taking, providing both
  performance and behavioral insights.
---

# CogBench: a large language model walks into a psychology lab

## Quick Facts
- arXiv ID: 2402.18225
- Source URL: https://arxiv.org/abs/2402.18225
- Reference count: 40
- 35 LLMs evaluated across 10 behavioral metrics from 7 cognitive psychology experiments

## Executive Summary
This paper introduces CogBench, a novel benchmark for evaluating large language models (LLMs) using ten behavioral metrics derived from seven cognitive psychology experiments. The benchmark assesses LLMs across domains including probabilistic reasoning, exploration, meta-cognition, learning, and risk-taking, providing both performance and behavioral insights. The authors evaluated 35 diverse LLMs, revealing that larger models and those with Reinforcement Learning from Human Feedback (RLHF) perform better and align more closely with human behavior. Surprisingly, open-source models exhibit less risk-taking behavior, and code fine-tuning does not consistently improve performance. Prompt-engineering techniques like chain-of-thought and take-a-step-back were found to enhance probabilistic reasoning and model-based behaviors, respectively. CogBench offers a comprehensive, behaviorally grounded toolkit for understanding and comparing LLM capabilities, highlighting the importance of behavioral metrics alongside traditional performance benchmarks.

## Method Summary
CogBench evaluates LLMs using seven cognitive psychology experiments converted into text-based prompts, measuring ten behavioral metrics across domains like probabilistic reasoning, exploration, meta-cognition, learning, and risk-taking. The benchmark runs 35 diverse LLMs deterministically (temperature=0) through prompt-chaining that induces learning by concatenating past behavior. Behavioral scores are computed alongside performance metrics, with multi-level regression analysis examining model features like size and RLHF usage. The evaluation employs prompt-engineering techniques including chain-of-thought and take-a-step-back methods, with results visualized through UMAP clustering and statistical comparisons revealing that RLHF models behave roughly twice as human-like as non-RLHF models.

## Key Results
- RLHF'ed LLMs behave roughly 2× more similar to human behavior compared to models without RLHF
- Chain-of-thought prompting improves probabilistic reasoning by 9.01% on average
- Take-a-step-back prompting increases model-based behaviors by 118.59%
- Open-source models exhibit significantly less risk-taking behavior than proprietary models
- Code fine-tuning does not consistently improve performance across all tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLHF improves alignment between LLM behavior and human behavior.
- Mechanism: Reinforcement Learning from Human Feedback (RLHF) exposes models to curated human preferences during fine-tuning, which shapes their behavioral outputs toward human-like patterns.
- Core assumption: Human feedback captures behaviorally relevant dimensions that correlate with cognitive alignment.
- Evidence anchors:
  - [abstract] "Our results also show the importance of reinforcement learning from human feedback (RLHF; Christiano et al., 2017) in aligning LLMs with humans: RLHF'ed LLMs behave generally more human-like..."
  - [section] "Clear separation is evident between LLMs that incorporate RLHF and those that do not. LLMs with RLHF demonstrate behaviors that appear, on average, roughly 2× more similar to human behavior compared to the models without."
  - [corpus] Weak; no direct corpus evidence provided; only inferred from paper's internal comparison.
- Break condition: If RLHF only improves performance metrics without affecting behavioral metrics, the alignment claim fails.

### Mechanism 2
- Claim: Chain-of-thought prompting enhances probabilistic reasoning in LLMs.
- Mechanism: Breaking problems into smaller logical steps scaffolds intermediate representations, improving reasoning accuracy in tasks requiring Bayesian updating.
- Core assumption: Step-by-step decomposition improves handling of uncertainty and prior-likelihood integration.
- Evidence anchors:
  - [abstract] "We discover that chain-of-thought prompting improves probabilistic reasoning..."
  - [section] "Both CoT and SB techniques generally enhanced probabilistic reasoning compared to their base models, with CoT showing an average increase of 9.01%..."
  - [corpus] Weak; no external corpus support; only internal experiment comparison.
- Break condition: If probabilistic reasoning performance does not increase beyond baseline for tasks without intermediate reasoning steps.

### Mechanism 3
- Claim: Take-a-step-back prompting improves model-based behaviors in LLMs.
- Mechanism: Abstracting the problem by "taking a step back" encourages representation of underlying task structure, enabling more strategic planning.
- Core assumption: Abstraction supports model-based planning over model-free reactive behavior.
- Evidence anchors:
  - [abstract] "...while take-a-step-back prompting fosters model-based behaviors."
  - [section] "Specifically, CoT demonstrated a 64.59% increase, while SB showed a substantial increase of 118.59%."
  - [corpus] Weak; only internal experiment data; no external validation.
- Break condition: If SB prompting does not yield higher model-basedness scores compared to CoT in model-based reasoning tasks.

## Foundational Learning

- Concept: Cognitive behavioral metrics vs performance metrics
  - Why needed here: CogBench explicitly separates behavioral metrics (how a task is solved) from performance metrics (what score is achieved). This distinction is central to understanding the benchmark's value.
  - Quick check question: If an LLM solves a task perfectly but uses non-human exploration strategies, does it score high on performance or behavioral metrics?
- Concept: Multi-level regression analysis
  - Why needed here: Used to model the nested dependencies among fine-tuned versions of LLMs while testing hypotheses about model features.
  - Quick check question: What is the advantage of multi-level regression over simple regression when analyzing fine-tuned model families?
- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: A key feature tested for its effect on human-likeness and meta-cognition in LLMs.
  - Quick check question: How does RLHF differ from standard supervised fine-tuning in terms of behavioral outcomes?

## Architecture Onboarding

- Component map:
  - CogBench core: Seven cognitive psychology experiments → ten behavioral metrics
  - Prompting engine: Handles task prompts, chaining, and engineering techniques
  - Evaluation pipeline: Runs LLMs deterministically (temp=0) and records outputs
  - Analysis module: Computes behavioral scores, performs multi-level regressions
  - Visualization layer: UMAP for behavioral clustering, bar charts for metric comparison
- Critical path:
  1. Load experiment definitions (tasks + metrics)
  2. Generate prompts for each LLM
  3. Execute prompts and collect responses
  4. Compute performance and behavioral metrics
  5. Aggregate results across models
  6. Perform statistical analysis (multi-level regression, UMAP)
  7. Visualize and report findings
- Design tradeoffs:
  - Deterministic prompting (temp=0) vs exploration of stochastic behaviors
  - Limited context size handling vs completeness of task instructions
  - Behavioral depth vs breadth of tasks covered
- Failure signatures:
  - Context truncation leading to incomplete task responses
  - Non-deterministic outputs when temperature is accidentally set > 0
  - Missing or malformed metric calculations due to unexpected output formats
- First 3 experiments:
  1. Run probabilistic reasoning task with CoT prompting on GPT-4; verify posterior accuracy improvement
  2. Execute horizon task with RLHF vs non-RLHF models; compare directed exploration scores
  3. Apply UMAP on behavioral metrics for a small subset (e.g., GPT-4, Claude-2, LLaMA-2-70) to confirm clustering patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of training data influence the emergence of specific behavioral metrics like model-basedness in LLMs?
- Basis in paper: [inferred] The paper mentions that "the quality of the training data, rather than its sheer volume, plays a more determining role in performance" and that "identifying which factors constitute ‘quality’ in the data requires a deeper exploration." It also highlights the importance of transparency about data for evaluating how specific data features impact behavioral functionalities.
- Why unresolved: While the paper hints at the significance of data quality, it does not provide a detailed analysis of which specific data characteristics or features contribute to the development of model-based behaviors or other cognitive abilities in LLMs. This remains an open area for investigation.
- What evidence would resolve it: A comprehensive study analyzing the relationship between specific data attributes (e.g., diversity, complexity, domain-specificity) and the emergence of behavioral metrics like model-basedness across a wide range of LLMs, controlling for model size and other factors.

### Open Question 2
- Question: Do hidden pre-prompts or priming techniques significantly influence the risk-taking behavior of proprietary LLMs compared to open-source models?
- Basis in paper: [explicit] The paper finds that "proprietary models, which often have hidden pre-prompts, are more likely to take risks" and that "hidden pre-prompts can significantly influence the behavior of LLMs (Liu et al., 2023)." It also mentions that pre-prompts can act as a form of 'priming' that guides the model's responses.
- Why unresolved: While the paper observes a correlation between proprietary models and increased risk-taking, it does not delve into the specific mechanisms by which hidden pre-prompts or priming techniques shape this behavior. The extent and nature of this influence remain unclear.
- What evidence would resolve it: Controlled experiments comparing the risk-taking behavior of LLMs with and without specific hidden pre-prompts or priming techniques, measuring the impact on various behavioral metrics like risk-taking, exploration, and decision-making.

### Open Question 3
- Question: Which prompt-engineering technique (CoT vs. SB) is more effective for enhancing specific cognitive abilities in LLMs, and under what conditions?
- Basis in paper: [explicit] The paper finds that "CoT is particularly effective at enhancing probabilistic reasoning, while SB proves to be more relevant for promoting model-based behaviors." It also mentions that "examining specific behaviors can provide valuable context, potentially guiding future decisions on the selection of one reasoning technique over another."
- Why unresolved: While the paper provides initial evidence for the differential effectiveness of CoT and SB, it only examines a limited set of models and behavioral metrics. The generalizability of these findings and the conditions under which each technique excels remain to be determined.
- What evidence would resolve it: A comprehensive study systematically comparing the impact of CoT and SB on a wide range of cognitive abilities and behavioral metrics across diverse LLMs, identifying the specific contexts or task characteristics where each technique is most beneficial.

## Limitations

- Behavioral metrics lack external validation against human subject data beyond original psychology experiments
- Benchmark sensitivity to prompt engineering techniques may not transfer to real-world applications
- Multi-level regression analysis assumes independence of fine-tuned models that may share architectural similarities

## Confidence

- **High Confidence**: The core mechanism that RLHF improves alignment between LLM behavior and human behavior is supported by clear statistical separation in the experimental results (roughly 2× more human-like behavior)
- **Medium Confidence**: The effectiveness of chain-of-thought prompting for probabilistic reasoning (9.01% average increase) and take-a-step-back prompting for model-based behaviors (118.59% increase) are internally consistent but lack external validation
- **Low Confidence**: The generalizability of CogBench behavioral metrics beyond the seven specific cognitive psychology experiments is uncertain

## Next Checks

1. **External Validation Study**: Conduct a human subject replication of the seven CogBench experiments to establish baseline human behavioral metrics. Compare these against LLM outputs to validate whether the behavioral metrics truly capture human-like patterns or are artifacts of the experimental design.

2. **Cross-Domain Transfer Test**: Apply the chain-of-thought and take-a-step-back prompting techniques to non-CogBench cognitive tasks (e.g., commonsense reasoning, ethical decision-making) to determine if the performance gains generalize beyond the specific experimental contexts.

3. **Fine-tuning Sensitivity Analysis**: Systematically vary the number of fine-tuning examples and RLHF iterations across multiple model families to establish the dose-response relationship between RLHF intensity and behavioral alignment with human patterns.