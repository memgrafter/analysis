---
ver: rpa2
title: 'MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding
  Benchmark'
arxiv_id: '2406.01574'
source_url: https://arxiv.org/abs/2406.01574
tags:
- answer
- mmlu-pro
- total
- arxiv
- mmlu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMLU-Pro, a more challenging benchmark for
  evaluating multi-task language understanding. The authors enhanced the original
  MMLU by adding more complex, reasoning-focused questions, expanding the answer options
  from four to ten, and removing trivial and noisy questions.
---

# MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark

## Quick Facts
- **arXiv ID**: 2406.01574
- **Source URL**: https://arxiv.org/abs/2406.01574
- **Reference count**: 40
- **Primary result**: MMLU-Pro reduces chance-based guessing from 25% to 10% by expanding answer options from 4 to 10, achieving only 72.6% accuracy on GPT-4o

## Executive Summary
MMLU-Pro is a new benchmark designed to address limitations in the original MMLU by creating a more challenging and robust evaluation of multi-task language understanding. The benchmark expands answer options from 4 to 10, incorporates more complex reasoning-focused questions, and removes trivial and noisy questions. Testing over 50 models shows significantly lower performance (72.6% for GPT-4o) compared to MMLU, with reduced sensitivity to prompt variations (2% vs 4-5%). The benchmark requires chain-of-thought reasoning for optimal performance, making it more discriminative and better suited for evaluating advanced reasoning capabilities.

## Method Summary
MMLU-Pro was constructed by filtering out easy questions from MMLU, integrating complex questions from STEM websites, TheoremQA, and SciBench, and converting them into multiple-choice format with 10 answer options. The construction pipeline included initial filtering based on model performance, question collection from multiple sources, option augmentation using GPT-4-Turbo, and expert review to verify answer correctness and eliminate bad questions. Models were evaluated using a 5-shot Chain-of-Thought approach with specific prompt instructions.

## Key Results
- GPT-4o achieved 72.6% accuracy on MMLU-Pro, significantly lower than original MMLU performance
- MMLU-Pro demonstrates greater stability under varying prompts (2% sensitivity vs 4-5% in MMLU)
- Models require chain-of-thought reasoning to perform well, highlighting the benchmark's focus on complex problem-solving
- Even the best-performing models show substantial room for improvement, indicating the benchmark's challenging nature

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanding from 4 to 10 answer options increases difficulty and reduces chance-based correct guesses, improving benchmark robustness.
- Mechanism: By increasing the number of distractors from 4 to 10, the probability of a correct guess by chance drops from 25% to 10%. This forces models to engage more deeply with reasoning rather than relying on simple elimination or guessing.
- Core assumption: More distractors lead to better discrimination between models with genuine understanding and those that rely on shortcuts.
- Evidence anchors:
  - [abstract] "expanding the choice set from four to ten options"
  - [section] "MMLU-Pro has ten options, which contain 3x more distractors than MMLU"
  - [corpus] "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark" - corpus shows the expansion is a distinguishing feature.
- Break condition: If distractors are too obviously wrong or the model can exploit pattern recognition in option structure, the robustness gain may be negated.

### Mechanism 2
- Claim: Incorporating more complex, reasoning-focused questions raises the ceiling for model performance and better tests cognitive capabilities.
- Mechanism: The benchmark replaces trivial and noisy questions with more complex, multi-step reasoning problems that require deliberate problem-solving rather than simple recall.
- Core assumption: Models that can solve knowledge-based questions may fail on reasoning-intensive tasks, exposing limitations in reasoning rather than memorization.
- Evidence anchors:
  - [abstract] "integrating more challenging, reasoning-focused questions"
  - [section] "MMLU-Pro increases the portion of challenging college-level exam problems"
  - [corpus] "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding" - related work confirms emphasis on reasoning challenges.
- Break condition: If reasoning questions are not sufficiently varied or are solvable by pattern matching alone, the benchmark may not effectively differentiate reasoning capabilities.

### Mechanism 3
- Claim: Reducing sensitivity to prompt variations increases benchmark reliability and consistency.
- Mechanism: By designing questions and options that are less sensitive to minor changes in prompt phrasing or style, the benchmark reduces score volatility and provides more stable evaluations.
- Core assumption: The original MMLU's high sensitivity to prompts (4-5% variance) is reduced in MMLU-Pro to approximately 2%, improving reliability.
- Evidence anchors:
  - [abstract] "demonstrates greater stability under varying prompts"
  - [section] "With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in MMLU-Pro"
  - [corpus] "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark" - corpus shows related work on prompt sensitivity.
- Break condition: If prompt insensitivity is achieved at the cost of question clarity or if the benchmark becomes too rigid, it may lose effectiveness in evaluating true understanding.

## Foundational Learning

- Concept: Multiple-choice question (MCQ) design principles
  - Why needed here: Understanding how distractor quality and option count affect difficulty is crucial for designing a robust benchmark.
  - Quick check question: What is the probability of guessing correctly on a 4-option vs. 10-option MCQ?
- Concept: Chain-of-thought (CoT) reasoning
  - Why needed here: The benchmark specifically evaluates models' ability to perform multi-step reasoning, and CoT is a key technique for improving performance on such tasks.
  - Quick check question: How does CoT prompting differ from direct answering in terms of model performance on reasoning tasks?
- Concept: Benchmark contamination and dataset noise
  - Why needed here: Understanding how noisy or trivial questions affect benchmark validity is essential for creating a high-quality evaluation tool.
  - Quick check question: What are common sources of noise in MCQ datasets, and how can they be identified and removed?

## Architecture Onboarding

- Component map: Filtering MMLU -> Question collection from STEM sources -> Option augmentation with GPT-4-Turbo -> Expert review -> Final benchmark
- Critical path: The expert review process, which ensures answer correctness, removes false negatives, and eliminates bad questions, directly impacts benchmark quality
- Design tradeoffs: The choice between breadth (14 subject domains) and depth (complex questions within fewer subjects) affects the benchmark's utility
- Failure signatures: High sensitivity to prompt variations, low performance spread between models, or presence of obvious distractors indicate design failures
- First 3 experiments:
  1. Test model performance on a small subset with 4 vs. 10 options to measure the impact of distractor count
  2. Evaluate prompt sensitivity by running the same models with 24 different prompt styles
  3. Compare CoT vs. direct answering performance to assess the reasoning difficulty of the questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MMLU-Pro scale with increasing model size, and are there diminishing returns for models beyond a certain scale?
- Basis in paper: [inferred] The paper evaluates models up to 70B parameters, showing performance gaps, but does not explore models beyond this size.
- Why unresolved: The study does not test models larger than 70B parameters, leaving uncertainty about the performance ceiling.
- What evidence would resolve it: Evaluations on models with 100B+ parameters, such as GPT-4 or Claude-3, to determine if performance plateaus or continues improving.

### Open Question 2
- Question: To what extent do MMLU-Pro’s reasoning-focused questions require domain-specific knowledge versus general reasoning skills?
- Basis in paper: [explicit] The paper notes that 35% of GPT-4o’s errors stem from a lack of specific domain expertise, suggesting a balance between knowledge and reasoning.
- Why unresolved: The analysis does not quantify the exact contribution of domain knowledge versus reasoning in solving MMLU-Pro questions.
- What evidence would resolve it: Ablation studies where models are tested on subsets of questions requiring domain-specific knowledge versus those solvable through general reasoning.

### Open Question 3
- Question: How does MMLU-Pro’s robustness to prompt variations compare to other benchmarks under more diverse or adversarial prompt styles?
- Basis in paper: [explicit] The paper tests 24 different prompt styles and finds reduced sensitivity compared to MMLU, but does not explore adversarial or highly diverse prompts.
- Why unresolved: The study’s prompt variations are limited in scope, and the robustness under more extreme conditions remains untested.
- What evidence would resolve it: Testing MMLU-Pro with adversarial prompts, such as those with intentional ambiguities or misleading cues, to measure performance degradation.

## Limitations
- The paper provides detailed construction methodology but lacks open-source access to the final MMLU-Pro dataset, limiting independent verification of all design choices
- While prompt sensitivity reduction is reported, the exact nature of the 24 prompt variations used for testing is not fully specified
- The expert review process, while mentioned as critical, lacks detailed documentation on reviewer selection criteria and inter-rater reliability metrics

## Confidence
- High confidence: The fundamental claim that expanding answer options from 4 to 10 reduces chance-based guessing (25% to 10%)
- Medium confidence: The assertion that MMLU-Pro better discriminates model capabilities based on current results, as this depends on future model development
- Medium confidence: The reported 2% prompt sensitivity compared to MMLU's 4-5%, as this requires verification with different model families

## Next Checks
1. Replicate the prompt sensitivity test using the same 24 prompt styles on at least three different model families to verify the reported 2% variance
2. Conduct a controlled experiment comparing model performance on MMLU-Pro vs. original MMLU to quantify the exact difficulty increase across all 14 domains
3. Perform an ablation study by testing model performance on MMLU-Pro questions with 4 options vs. 10 options to isolate the impact of distractor count on benchmark discrimination