---
ver: rpa2
title: Towards Efficient Active Learning in NLP via Pretrained Representations
arxiv_id: '2402.15613'
source_url: https://arxiv.org/abs/2402.15613
tags:
- learning
- active
- data
- prepal
- acquisition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost of active learning
  when fine-tuning large language models (LLMs) for text classification. The proposed
  method, PRepAL, precomputes data representations using a pretrained LLM and fits
  a simple linear classifier during active learning iterations, avoiding costly fine-tuning
  until the desired amount of labeled data is acquired.
---

# Towards Efficient Active Learning in NLP via Pretrained Representations

## Quick Facts
- arXiv ID: 2402.15613
- Source URL: https://arxiv.org/abs/2402.15613
- Authors: Artem Vysogorets; Achintya Gopal
- Reference count: 10
- Primary result: PRepAL achieves similar accuracy to fine-tuning while being orders of magnitude faster

## Executive Summary
This paper addresses the computational inefficiency of active learning when fine-tuning large language models (LLMs) for text classification. The proposed PRepAL method precomputes data representations using a pretrained LLM and fits a simple linear classifier during active learning iterations, avoiding costly fine-tuning until the desired amount of labeled data is acquired. PRepAL yields similar performance to fine-tuning throughout the active learning loop but is significantly less computationally expensive. The method also demonstrates that data acquired using one pretrained LLM backbone can successfully transfer to fine-tune a different LLM, providing flexibility in model selection.

## Method Summary
PRepAL works by first extracting and caching embeddings from a pretrained LLM (BERT or RoBERTa) for the entire unlabeled dataset. During active learning iterations, instead of fine-tuning the LLM, a simple linear classifier (Logistic Regression) is trained on the precomputed representations. An acquisition function selects the next batch of samples to label, which are then added to the training set. This process repeats until the desired number of labeled samples is acquired. Finally, the target LLM is fine-tuned on the acquired labeled dataset. This approach drastically reduces computation during the active learning loop while maintaining competitive accuracy.

## Key Results
- PRepAL achieves 79.05% accuracy on QNLI with BERT in 7.5K seconds vs. 79.60% accuracy with AL+FT in 98K seconds
- Data acquired with PRepAL generalizes across pretrained networks (BERT to RoBERTa transfer)
- Sequential labeling (batch size 1) improves data quality in early active learning stages with limited labeled data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Precomputing LLM representations allows efficient retraining of a simple linear classifier during active learning iterations.
- Mechanism: By extracting fixed embeddings from a pretrained LLM once and storing them, subsequent active learning iterations only require fitting a lightweight classifier on these static representations rather than re-fine-tuning the entire LLM.
- Core assumption: The semantic information captured in the pretrained LLM's embeddings is sufficient to guide sample selection for active learning without requiring dynamic fine-tuning.
- Evidence anchors:
  - [abstract] "precomputes data representations using a pretrained LLM and fits a simple linear classifier during active learning iterations, avoiding costly fine-tuning"
  - [section 4] "total time spent retraining Logistic Regression on precomputed representations... over 39 active learning iterations is just 5 seconds, while all 39 re-fine-tuning cycles take almost 22 hours"
  - [corpus] Weak evidence - the corpus neighbors discuss active learning and LLMs but don't directly address the efficiency of precomputed representations.
- Break condition: If the pretrained embeddings don't capture task-relevant features, or if the classifier complexity needs to exceed simple linear models to achieve good performance.

### Mechanism 2
- Claim: Data acquired using one pretrained LLM backbone can successfully transfer to fine-tune a different LLM.
- Mechanism: The representations learned by different LLMs from the same text data capture similar semantic structures, allowing the same labeled data to be effective for fine-tuning different models.
- Core assumption: Different pretrained LLMs extract comparable feature representations from text data, making the acquired labeled data transferable between models.
- Evidence anchors:
  - [abstract] "The data acquired with our procedure generalizes across pretrained networks, allowing flexibility in choosing the final model"
  - [section 4] "data acquired by PRepAL with BERT can be successfully transferred to fine-tune a pretrained RoBERTa model"
  - [corpus] Weak evidence - the corpus neighbors don't discuss transferability between different LLMs.
- Break condition: If different LLMs learn fundamentally different feature representations that aren't aligned across models.

### Mechanism 3
- Claim: Sequential labeling (batch size of 1) can improve data quality in early active learning stages.
- Mechanism: By labeling and retraining on individual samples sequentially rather than in batches, the active learner can adapt more quickly to the most informative samples as they're labeled, potentially avoiding redundancy within batches.
- Core assumption: Individual sample informativeness varies enough that sequential selection captures more diverse and useful examples than batch selection.
- Evidence anchors:
  - [section 4] "using sequential labeling improves the ultimate model only in the beginning and when labeled data is still limited"
  - [section 4] "this may indicate that, while the diversity within each individual batch is poor, different batches at different iterations are diverse enough to match the quality"
  - [corpus] Weak evidence - the corpus neighbors don't discuss batch size effects in active learning.
- Break condition: If batch diversity is sufficient to match sequential diversity, or if computational overhead of sequential processing becomes prohibitive.

## Foundational Learning

- Concept: Pretrained language models and transfer learning
  - Why needed here: The entire approach relies on using representations from a pretrained LLM as a fixed feature extractor.
  - Quick check question: What is the key difference between feature extraction and fine-tuning when using pretrained LLMs?

- Concept: Active learning and acquisition functions
  - Why needed here: PRepAL integrates with standard active learning acquisition functions but requires understanding their limitations when used with static representations.
  - Quick check question: How do uncertainty-based acquisition functions like MaxEntropy differ from diversity-based ones like CoreSet in their selection criteria?

- Concept: Linear classifiers and their limitations
  - Why needed here: PRepAL uses simple linear classifiers during active learning, so understanding their capacity and limitations is crucial.
  - Quick check question: Under what conditions might a linear classifier fail to capture the decision boundary even with high-quality representations?

## Architecture Onboarding

- Component map:
  - Pretrained LLM (BERT/RoBERTa) for initial representation extraction
  - Data preprocessing pipeline for text to embeddings
  - Linear classifier (Logistic Regression) for active learning iterations
  - Active learning acquisition function module
  - Final fine-tuning pipeline for target LLM
  - Experiment tracking and evaluation components

- Critical path:
  1. Extract and cache embeddings from unlabeled data using pretrained LLM
  2. Initialize with small labeled set and train initial linear classifier
  3. Apply acquisition function to select next batch for labeling
  4. Label data and retrain linear classifier
  5. Repeat until desired labeled data size reached
  6. Fine-tune target LLM on acquired labeled data

- Design tradeoffs:
  - Fixed vs. dynamic representations: Fixed saves computation but may miss task-specific features that fine-tuning would capture
  - Linear vs. deeper classifier: Deeper models could capture more complex relationships but increase computation per iteration
  - Sequential vs. batch acquisition: Sequential may improve early-stage diversity but increases iteration overhead

- Failure signatures:
  - Poor performance despite efficient training: Likely indicates representations don't capture task-relevant features
  - Similar performance to random acquisition: Suggests acquisition function isn't working well with static representations
  - Large gap between PRepAL and AL+FT: May indicate task requires dynamic adaptation of representations

- First 3 experiments:
  1. Baseline test: Compare PRepAL with random acquisition vs. standard AL+FT on a small dataset to verify the efficiency-accuracy tradeoff
  2. Transfer test: Use data acquired with BERT to fine-tune RoBERTa and verify cross-model generalization
  3. Batch size test: Compare sequential (batch size 1) vs. batch acquisition to verify early-stage diversity benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PRepAL perform when applied to other NLP tasks beyond text classification, such as machine translation or question answering?
- Basis in paper: [inferred] The paper suggests that future research may explore PRepAL's viability for other types of downstream tasks, but does not provide empirical evidence.
- Why unresolved: The paper only evaluates PRepAL on text classification datasets and does not explore its applicability to other NLP tasks.
- What evidence would resolve it: Conducting experiments on various NLP tasks beyond text classification and comparing PRepAL's performance to other active learning methods.

### Open Question 2
- Question: How does the performance of PRepAL change when using different pretrained language models, such as ELECTRA or ALBERT, as the backbone?
- Basis in paper: [explicit] The paper only evaluates PRepAL using BERT and RoBERTa as the backbone models and does not explore other pretrained language models.
- Why unresolved: The paper does not provide empirical evidence on how PRepAL performs with other pretrained language models.
- What evidence would resolve it: Conducting experiments using different pretrained language models as the backbone and comparing their performance to PRepAL with BERT and RoBERTa.

### Open Question 3
- Question: How does the choice of acquisition function affect the performance of PRepAL in different scenarios, such as when the labeled data is scarce or abundant?
- Basis in paper: [explicit] The paper finds that uncertainty-based acquisition functions like MaxEntropy and VariationRatio perform consistently well with PRepAL, but does not explore how different acquisition functions perform in various scenarios.
- Why unresolved: The paper does not provide empirical evidence on how different acquisition functions perform with PRepAL in different scenarios.
- What evidence would resolve it: Conducting experiments using different acquisition functions with PRepAL in scenarios with varying amounts of labeled data and comparing their performance.

## Limitations
- Static representations may not capture task-specific features that dynamic fine-tuning would discover
- Linear classifier assumption may break down for datasets with complex, non-linear decision boundaries
- Cross-model transferability claims need more rigorous validation across diverse LLM architectures

## Confidence

- **High confidence**: The computational efficiency claims are well-supported with concrete timing comparisons showing 10x-100x speedups. The basic mechanism of precomputing representations and using a linear classifier is straightforward and verifiable.

- **Medium confidence**: The transferability claims between different LLM backbones are demonstrated but only on a limited set of models (BERT and RoBERTa). The sequential labeling benefits are observed but only in early stages with limited data.

- **Low confidence**: Claims about generalizability to other acquisition functions beyond those tested (MaxEntropy, BALD, BatchBALD) are not validated. The assumption that linear classifiers suffice for all text classification tasks needs more empirical support.

## Next Checks

1. **Representation quality test**: Systematically evaluate how PRepAL performance degrades when the pretrained representations lack task-relevant features (e.g., using a mismatched pretrained model for the task domain).

2. **Non-linear classifier test**: Compare PRepAL using linear classifiers against using simple neural classifiers (e.g., one hidden layer) during active learning iterations to quantify the impact of linearity assumption.

3. **Cross-architecture transfer test**: Validate transferability claims by acquiring data using smaller/faster models (e.g., DistilBERT) and fine-tuning larger models (e.g., DeBERTa, GPT-based models) to assess the generality of the transferability claim.