---
ver: rpa2
title: What Will My Model Forget? Forecasting Forgotten Examples in Language Model
  Refinement
arxiv_id: '2402.01865'
source_url: https://arxiv.org/abs/2402.01865
tags:
- forecasting
- examples
- forgetting
- learning
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the problem of catastrophic forgetting in language\
  \ model refinement\u2014where updating a model to fix errors causes it to forget\
  \ previously learned examples. The authors propose forecasting which upstream pretraining\
  \ examples will be forgotten after model updates, enabling more targeted replay\
  \ and reduced forgetting."
---

# What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement

## Quick Facts
- **arXiv ID**: 2402.01865
- **Source URL**: https://arxiv.org/abs/2402.01865
- **Authors**: Xisen Jin; Xiang Ren
- **Reference count**: 26
- **Primary result**: Representation-based forecasting model achieves up to 79.32 F1 in predicting forgotten examples, reducing forgetting by up to 8.3% EM Drop compared to random replay.

## Executive Summary
This paper tackles catastrophic forgetting in language model refinement by developing methods to forecast which upstream pretraining examples will be forgotten when updating a model to fix errors. The authors propose two approaches: a partially interpretable logit-change based model that transfers logit changes from online-learned to pretraining examples, and a black-box representation-based model that predicts forgetting using inner products of example representations. Experiments across BART and T5 models show the representation-based method consistently outperforms baselines, achieving up to 79.32 F1 in forecasting. When replaying examples predicted to be forgotten, they reduce forgetting by up to 8.3% EM Drop compared to random replay, demonstrating practical utility.

## Method Summary
The paper addresses catastrophic forgetting during language model refinement by developing forecasting models that predict which upstream pretraining examples will be forgotten when the model is updated to fix errors. Two approaches are introduced: a logit-change based method that transfers changes from online-learned examples to pretraining examples through a kernel approximation, and a representation-based method that uses inner products of learned example representations to predict forgetting. The representation-based approach adds frequency priors to improve out-of-domain generalization. Models are trained on pairs of online learned examples and pretraining examples, then used during sequential refinement to selectively replay examples predicted to be forgotten, reducing overall forgetting.

## Key Results
- Representation-based forecasting model achieves up to 79.32 F1 in predicting forgotten examples
- Selective replay of predicted examples reduces forgetting by up to 8.3% EM Drop compared to random replay
- Logit-change based forecasting works well for BART models but fails for T5 models
- Adding frequency priors improves out-of-domain generalization of the representation-based model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Changes in pre-softmax logit scores of pretraining examples proportionally copy changes from online learned examples during model updates, causing forgetting.
- Mechanism: When fixing prediction errors on new examples, the gradient updates affect the model parameters in a way that transfers logit changes to similar pretraining examples. This transfer disrupts the correct ordering of top prediction candidates in those pretraining examples, flipping their predictions.
- Core assumption: The kernel Θ(xj, xi) that measures inner products among gradients can be approximated by a low-dimensional similarity measure between example representations.
- Evidence anchors:
  - [abstract] "changes in pre-softmax logit scores of pretraining examples resemble that of online learned examples"
  - [section 3.2] "the logit change of the upstream pretraining example" and the relationship "ˆfi(xj) − ˆf0(xj) = Θ(xj, xi)Θ−1(xi, xi)[ ˆfi(xi) − ˆf0(xi)]"
- Break condition: If the kernel Θ(xj, xi) cannot be approximated by a low-dimensional similarity measure, or if the model fine-tunes beyond just the LM heads, the logit-change transfer mechanism fails.

### Mechanism 2
- Claim: Inner products of low-dimensional representations of example pairs directly predict forgetting with high accuracy.
- Mechanism: A trainable encoder h maps pairs of examples to low-dimensional vectors, and their inner product is mapped through a sigmoid to predict whether the pretraining example will be forgotten when learning the online example. The frequency prior term biases predictions based on how often each pretraining example has been forgotten historically.
- Core assumption: The interaction between two examples that causes forgetting can be captured by a simple inner product of their representations.
- Evidence anchors:
  - [abstract] "a black-box classifier based on inner products of example representations achieves better forecasting performance"
  - [section 3.3] "we directly map the inner products of the representations h(xj, yj)h(xi, yi)T to the binary label zij ∈ {0, 1} of forgetting"
- Break condition: If the representation space fails to capture the critical interactions between examples that cause forgetting, or if the frequency prior becomes inaccurate for new data distributions.

### Mechanism 3
- Claim: Adding frequency priors to the representation-based forecasting model improves out-of-domain generalization.
- Mechanism: The bias term bj represents the log odds that a pretraining example is forgotten, computed from historical forgetting frequencies. This prior helps the model focus on learning residuals beyond what frequency alone can explain.
- Core assumption: The frequency of forgetting for each example is predictive of future forgetting, and the model can learn interactions beyond this baseline.
- Evidence anchors:
  - [abstract] "representation-based forecasting model that is consistently effective across various setups"
  - [section 3.3] "we add a bias term to Eqn. 4 that represent the frequency priors of forgetting each upstream example"
- Break condition: If the frequency prior becomes stale or irrelevant for new tasks, or if the model overfits to frequency patterns without learning meaningful interactions.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The paper's core problem is that updating models to fix errors causes them to forget previously learned examples. Understanding catastrophic forgetting is essential to grasp why this problem matters and how the proposed solutions address it.
  - Quick check question: What is catastrophic forgetting, and why does it occur when fine-tuning language models on new tasks?

- Concept: Neural Tangent Kernels (NTKs) and gradient-based similarity
  - Why needed here: The logit-change based forecasting method relies on understanding how gradients of different examples interact through the kernel matrix Θ. This connects to NTK theory which studies how learning dynamics transfer between examples.
  - Quick check question: How do Neural Tangent Kernels measure similarity between examples in terms of their gradient contributions?

- Concept: Representation learning and inner product similarity
  - Why needed here: The representation-based forecasting method uses inner products of learned representations to predict forgetting. Understanding how representations capture semantic similarity is crucial for interpreting this approach.
  - Quick check question: How can inner products of learned representations capture meaningful similarity between different examples?

## Architecture Onboarding

- Component map:
  - Base Language Model (f0) -> Online Learning Examples (DR) -> Pretraining Examples (DPT) -> Forecasting Model g -> Replay Buffer

- Critical path:
  1. Evaluate base LM on new task to collect DR
  2. Train forecasting model g using DTrain R and DPT
  3. For each example in DTest R, use g to predict forgotten examples
  4. Fine-tune LM while selectively replaying predicted examples
  5. Measure EM Drop Ratio to evaluate forgetting reduction

- Design tradeoffs:
  - Interpretability vs performance: Logit-based method is partially interpretable but less effective on some models; representation-based is black-box but more accurate
  - Computational cost vs accuracy: Fixed logit-based is cheap but limited to head-only tuning; representation-based requires training but works for full fine-tuning
  - Generalization vs specialization: Adding frequency priors improves OOD performance but may overfit to training distribution

- Failure signatures:
  - Low F1 scores in forecasting indicate the model fails to capture example interactions
  - High EM Drop Ratio after refinement indicates forgetting is not being mitigated
  - Performance degradation on out-of-domain tasks suggests overfitting to training distribution

- First 3 experiments:
  1. Train representation-based forecasting model on BART0 with P3-Test as DR, evaluate F1 on held-out examples
  2. Apply representation-based forecasting to predict forgotten examples during BART0 refinement, measure EM Drop Ratio with and without replay
  3. Test representation-based forecasting on FLAN-T5 with MMLU as DR, compare performance to threshold-based baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the logit-change transfer phenomenon work well for BART models but fail for T5 models?
- Basis in paper: [explicit] The authors note that "This greatly simplified logit-based forecasting is effective on BART models, but fails on another model, T5."
- Why unresolved: The paper doesn't provide a detailed analysis of the structural or training differences between BART and T5 that might explain this discrepancy.
- What evidence would resolve it: Comparative analysis of the gradient dynamics and representation spaces of BART vs T5 during fine-tuning, potentially through ablation studies or visualization of the logit-change transfer process.

### Open Question 2
- Question: How does the similarity between upstream pretraining examples and online learned examples affect the likelihood of forgetting?
- Basis in paper: [inferred] The paper discusses that interactions between examples contribute to forgetting, but doesn't explicitly analyze the role of example similarity.
- Why unresolved: The paper focuses on developing forecasting models but doesn't deeply explore the nature of example interactions or similarity metrics that influence forgetting.
- What evidence would resolve it: Controlled experiments varying the semantic and task similarity between upstream and online examples while measuring forgetting rates, potentially using metrics like semantic similarity scores or task embedding distances.

### Open Question 3
- Question: Can the forecasting models be improved by incorporating temporal dynamics of the language model during sequential updates?
- Basis in paper: [explicit] The authors note that "Future works can improve the recall of forecasting" and show that precision remains stable while recall drops over time in sequential updates.
- Why unresolved: The current forecasting models use a static base model (f0) for training, not accounting for the evolving model state during sequential updates.
- What evidence would resolve it: Experiments comparing static forecasting models against dynamic models that update alongside the language model, measuring both forecasting accuracy and forgetting reduction in longer sequential update streams.

## Limitations
- The logit-change transfer mechanism shows significant model-specific variation, working well for BART but failing for T5 models without clear explanation of why
- The representation-based method is a "black box" with no theoretical grounding for why inner products of learned representations should predict forgetting
- The frequency prior addition introduces dependency on historical forgetting data that may not be available or representative in all scenarios

## Confidence
- **High Confidence**: The empirical demonstration that representation-based forecasting outperforms baseline methods across multiple model architectures and datasets. The experimental results showing 79.32 F1 score and 8.3% EM Drop reduction are clearly presented and reproducible based on the provided methodology.
- **Medium Confidence**: The claim that the logit-change mechanism works for BART models. While the paper shows this empirically, the theoretical justification is weak and the mechanism clearly fails for other architectures without clear explanation of why.
- **Low Confidence**: The general applicability of either forecasting mechanism beyond the specific experimental setup. The paper does not explore edge cases, alternative model architectures, or scenarios where the assumptions underlying either mechanism might break down.

## Next Checks
1. **Kernel Approximation Validation**: Systematically test the logit-change transfer mechanism across different model architectures (RNNs, CNNs, smaller LMs) to identify the precise conditions under which the kernel approximation Θ(xj, xi) ≈ h(xj)h(xi)T holds or fails. Measure the correlation between actual logit changes and predicted changes under this approximation.

2. **Representation Space Analysis**: Perform ablation studies on the representation-based method by varying the dimensionality of the learned representations, the encoder architecture, and the similarity metric. Analyze what specific features of the representation space correlate most strongly with forgetting predictions to provide interpretability to the "black box" approach.

3. **Cross-Domain Generalization Testing**: Evaluate both forecasting methods on significantly out-of-distribution tasks and data distributions not seen during training. Test scenarios where the frequency prior becomes stale, where example distributions shift dramatically, and where the model undergoes substantial architectural changes between pretraining and refinement phases.