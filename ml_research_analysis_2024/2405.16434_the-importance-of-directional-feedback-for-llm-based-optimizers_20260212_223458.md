---
ver: rpa2
title: The Importance of Directional Feedback for LLM-based Optimizers
arxiv_id: '2405.16434'
source_url: https://arxiv.org/abs/2405.16434
tags:
- feedback
- optimization
- agent
- prompt
- tunable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies using LLMs as optimizers for tasks in natural\
  \ language and numerical spaces. The key insight is that LLMs are especially effective\
  \ when given directional feedback\u2014guidance that specifies the direction of\
  \ improvement rather than just a scalar score."
---

# The Importance of Directional Feedback for LLM-based Optimizers

## Quick Facts
- **arXiv ID**: 2405.16434
- **Source URL**: https://arxiv.org/abs/2405.16434
- **Reference count**: 6
- **Primary result**: LLM-based optimizers achieve stable and efficient improvements when given directional feedback, outperforming baselines in constrained text generation and numerical optimization tasks.

## Executive Summary
This paper studies using LLMs as optimizers for tasks in natural language and numerical spaces, demonstrating that directional feedback—guidance specifying the direction of improvement rather than just scalar scores—significantly enhances LLM optimization performance. The authors formalize this distinction, drawing parallels to first-order feedback in classical optimization. They propose an LLM-based optimizer that synthesizes directional feedback from past optimization traces, showing that this approach achieves stable improvements across iterations. Experiments on numerical optimization tasks show LLMs can find solutions comparable to SGD when given directional or synthesized feedback, while performance degrades significantly without feedback. On a poem generation task with syllable constraints, the LLM-based optimizer consistently produces higher-reward outputs than baselines like Reflexion, demonstrating the value of prompt optimization in constrained text generation.

## Method Summary
The method involves using LLMs as optimizers that propose new prompts based on historical data from previous optimization iterations. The system can operate with explicit directional feedback, non-directional feedback, or synthesized feedback generated by an LLM-based feedback synthesizer module. The feedback synthesizer analyzes past (prompt, output, reward) tuples to infer what changes would improve future outputs, effectively creating synthetic directional guidance when explicit directional feedback is unavailable. The optimizer proposes new prompts, which are evaluated by a selector that accepts them if they improve upon previous prompts. This framework is tested on both numerical optimization tasks (Rosenbrock function and other classic optimization problems) and constrained text generation tasks (poem generation with syllable constraints).

## Key Results
- LLM-based optimizers achieve comparable performance to SGD on numerical optimization when provided with directional or synthesized feedback
- Without any feedback, LLM optimizer performance degrades significantly, highlighting the importance of guidance
- On poem generation with syllable constraints, the LLM-based optimizer with synthesized feedback consistently outperforms Reflexion baseline
- Directional feedback enables more efficient optimization by providing search direction information analogous to gradients in classical optimization

## Why This Works (Mechanism)

### Mechanism 1: Directional Feedback Enables Gradient-Like Search Direction
Directional feedback provides the optimizer with search direction information analogous to gradients in numerical optimization, enabling more efficient updates. When feedback specifies whether to increase or decrease specific parameters (e.g., "decrease x2" or "increase syllables"), the LLM optimizer can make targeted modifications rather than random exploration. The core assumption is that the LLM can interpret directional feedback and translate it into meaningful parameter adjustments in the text space.

### Mechanism 2: Feedback Synthesis Extends Applicability to Environments Without Rich Signals
An LLM can synthesize directional feedback from historical observations and rewards when explicit directional feedback is unavailable. The feedback synthesizer module analyzes past (prompt, output, reward) tuples to infer what changes would improve future outputs, effectively creating synthetic directional guidance. The core assumption is that the LLM possesses sufficient reasoning capability to identify patterns between input modifications and output rewards across multiple iterations.

### Mechanism 3: Historical Trace Enables Implicit Newton's Method-Like Optimization
LLMs can implicitly perform optimization similar to Newton's method by analyzing finite differences between past prompt-reward pairs. By examining how small changes in prompts affect rewards across iterations, the LLM can approximate gradient information without explicit computation. The core assumption is that the LLM can perform implicit gradient estimation through pattern recognition in historical data.

## Foundational Learning

- **Concept**: Gradient-based optimization principles
  - Why needed here: Understanding how directional feedback parallels gradient information in classical optimization helps explain why the LLM optimizer performs better with such feedback
  - Quick check question: What is the key difference between first-order and zero-order optimization methods?

- **Concept**: Text-to-text optimization and prompt engineering
  - Why needed here: The entire system revolves around optimizing text prompts to control LLM behavior, requiring understanding of how prompt modifications affect outputs
  - Quick check question: How does changing a prompt differ from changing numerical parameters in traditional optimization?

- **Concept**: Feedback classification and synthesis
  - Why needed here: The paper distinguishes between directional, non-directional, and synthesized feedback, each serving different roles in the optimization process
  - Quick check question: What makes feedback "directional" versus "non-directional" in the context of LLM optimization?

## Architecture Onboarding

- **Component map**: Environment -> Agent (LLM) -> Optimizer (LLM) -> Feedback Synthesizer (optional) -> Prompt Selector
- **Critical path**: 
  1. Agent generates output o₁ from initial prompt p₀
  2. Environment returns reward r₁ and feedback f₁
  3. Optimizer proposes new prompt p₁ using historical data
  4. Selector evaluates if p₁ improves over p₀
  5. Repeat until convergence or iteration limit
- **Design tradeoffs**:
  - Feedback type: Directional feedback provides better guidance but may not always be available
  - History buffer size: Larger buffers provide more context but increase computational cost
  - Step acceptance criteria: Strict criteria ensure improvement but may slow convergence
- **Failure signatures**:
  - No improvement over iterations: Check if feedback is too vague or optimizer prompt is ineffective
  - Degradation in performance: Verify reward calculation and step acceptance logic
  - Unstable behavior: Examine history buffer management and prompt synthesis quality
- **First 3 experiments**:
  1. Run with numerical optimization task (Rosenbrock function) with no feedback to establish baseline
  2. Add directional feedback to same task to verify improvement mechanism
  3. Implement feedback synthesizer and test on numerical task with only reward signals

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of LLM-based optimizers vary across different types of mathematical functions, particularly those with non-convex landscapes or multiple local minima?
- **Basis in paper**: [inferred] The paper evaluates LLM-based optimizers on classic optimization problems but does not explore non-convex or multi-modal landscapes in depth.
- **Why unresolved**: The paper focuses on a limited set of functions and does not test the robustness of LLM-based optimizers in more complex, non-convex scenarios.
- **What evidence would resolve it**: Experiments comparing LLM-based optimizers on a diverse set of non-convex and multi-modal functions, with detailed analysis of convergence and solution quality.

### Open Question 2
- **Question**: Can the feedback synthesis module be generalized to other domains beyond numerical optimization and poem generation, such as code generation or image synthesis?
- **Basis in paper**: [explicit] The paper discusses the feedback synthesizer's role in generating useful feedback from rewards and outputs, but does not explore its applicability to other domains.
- **Why unresolved**: The feedback synthesizer is only tested in the context of numerical optimization and poem generation, leaving its generalizability to other tasks unexplored.
- **What evidence would resolve it**: Implementation and evaluation of the feedback synthesizer in diverse domains like code generation or image synthesis, with quantitative and qualitative analysis of its effectiveness.

### Open Question 3
- **Question**: How does the choice of feedback granularity (e.g., line-by-line vs. holistic feedback) impact the performance of LLM-based optimizers in constrained text generation tasks?
- **Basis in paper**: [explicit] The paper mentions providing line-by-line feedback for poem generation but does not systematically compare different levels of feedback granularity.
- **Why unresolved**: The paper does not experiment with varying the granularity of feedback to assess its impact on optimization performance.
- **What evidence would resolve it**: Experiments comparing the performance of LLM-based optimizers under different feedback granularities, with analysis of trade-offs between feedback detail and optimization efficiency.

## Limitations

- Empirical validation is limited to only two domains (numerical optimization and constrained text generation), constraining generalizability claims
- The feedback synthesizer mechanism relies on the LLM's ability to extract patterns from historical data, which may not scale to more complex optimization landscapes
- The distinction between directional and non-directional feedback lacks formal characterization of what constitutes "directional" feedback in general problem domains

## Confidence

- **High Confidence**: The core empirical finding that directional feedback improves LLM-based optimization performance across both tested domains
- **Medium Confidence**: The theoretical framing of directional feedback as analogous to first-order information in classical optimization
- **Low Confidence**: The feedback synthesis mechanism's effectiveness in arbitrary environments without rich feedback signals

## Next Checks

1. **Cross-domain robustness test**: Apply the LLM optimizer with synthesized feedback to a third, structurally different domain (e.g., code optimization or protein design) to assess generalizability beyond the two tested domains.

2. **Feedback quality ablation**: Systematically vary the quality and specificity of provided directional feedback (e.g., using human-written vs. LLM-generated feedback of varying detail) to quantify how feedback quality impacts optimization performance.

3. **Scaling analysis**: Evaluate the feedback synthesizer's performance as the optimization horizon increases (e.g., 10 vs. 50 iterations) to determine whether pattern recognition degrades with longer optimization traces or whether the LLM can maintain effective synthesis over extended optimization sessions.