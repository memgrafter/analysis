---
ver: rpa2
title: Adversarial Fine-tuning of Compressed Neural Networks for Joint Improvement
  of Robustness and Efficiency
arxiv_id: '2403.09441'
source_url: https://arxiv.org/abs/2403.09441
tags:
- adversarial
- fine-tuning
- robustness
- standard
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether model compression and adversarial
  robustness can be jointly achieved. While adversarial training improves robustness,
  it comes with significant computational costs.
---

# Adversarial Fine-tuning of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency

## Quick Facts
- arXiv ID: 2403.09441
- Source URL: https://arxiv.org/abs/2403.09441
- Reference count: 12
- Primary result: Adversarial fine-tuning of compressed models achieves robustness comparable to adversarially trained uncompressed models while improving efficiency.

## Executive Summary
This paper investigates the joint optimization of model compression and adversarial robustness. The authors challenge the common assumption that compression inherently degrades robustness by systematically studying the effects of structured pruning and quantization on adversarial robustness. Through extensive experiments on Fashion-MNIST and CIFAR10, they demonstrate that compression techniques do not necessarily compromise robustness and that adversarial fine-tuning of compressed models can recover and even enhance robustness performance. The key insight is that fine-tuning compressed models with adversarial training can bridge the robustness gap while maintaining computational efficiency gains from compression.

## Method Summary
The approach involves a three-stage pipeline: initial training (standard or adversarial) for 20 epochs, compression through structured pruning (80% sparsity for Fashion-MNIST, 50% for CIFAR10) and INT8 quantization, followed by adversarial fine-tuning for 3 epochs. The method tests whether compression affects robustness and whether adversarial fine-tuning can restore or improve robustness in compressed models. Experiments compare compressed models against uncompressed baselines and evaluate robustness using PGD attacks with ℓ∞-norm. The study uses an 8-layer CNN for Fashion-MNIST and ResNet-18 for CIFAR10, with both standard and adversarial training as starting points.

## Key Results
- Compression does not inherently degrade adversarial robustness
- Adversarial fine-tuning of compressed models achieves robustness performance comparable to adversarially trained uncompressed models
- The approach maintains computational efficiency gains from compression while achieving robustness
- Three epochs of adversarial fine-tuning are sufficient to restore robustness in compressed models

## Why This Works (Mechanism)
The effectiveness stems from the interplay between model capacity and robustness optimization. Compression reduces model size but also eliminates redundant parameters that may not contribute meaningfully to robustness. Adversarial fine-tuning then optimizes the remaining parameters specifically for robustness under attack, effectively reallocating the model's representational capacity toward more robust features. The process leverages the fact that not all parameters equally contribute to robustness, and compression can actually help focus the model on more essential, robust representations. The short fine-tuning period is sufficient because it targets a compressed parameter space where the remaining weights have already been shaped by the initial adversarial training.

## Foundational Learning
- **Adversarial training**: Training models on adversarial examples to improve robustness against attacks; needed to understand the baseline robustness approach being compared against.
- **Structured pruning**: Removing entire filters or channels from neural networks based on importance metrics; needed to understand how model capacity reduction is achieved.
- **Quantization**: Reducing numerical precision of model weights (e.g., from 32-bit to 8-bit); needed to understand the computational efficiency aspect of compression.
- **PGD attacks**: Projected Gradient Descent attacks used to evaluate model robustness; needed to understand the threat model and evaluation metric.
- **Fine-tuning**: Additional training on pre-trained models to adapt to new objectives or recover performance; needed to understand how compressed models can recover robustness.
- **ℓ∞-norm attacks**: Adversarial attacks constrained by maximum perturbation per pixel; needed to understand the specific threat model used in evaluation.

## Architecture Onboarding

**Component Map**: Input Data -> Initial Training (20 epochs) -> Compression (Pruning + Quantization) -> Adversarial Fine-tuning (3 epochs) -> Evaluation

**Critical Path**: The critical path for achieving joint robustness and efficiency is the sequence from initial adversarial training through compression to adversarial fine-tuning. The initial adversarial training establishes a robustness baseline, compression reduces computational cost while potentially affecting robustness, and adversarial fine-tuning recovers and potentially enhances robustness in the compressed model. Each stage builds on the previous one, with fine-tuning being the key differentiator that enables compressed models to achieve comparable robustness to uncompressed models.

**Design Tradeoffs**: The main tradeoff is between model size/efficiency and robustness. Compression significantly reduces computational cost but may impact model capacity. The three-epoch fine-tuning represents a balance between computational overhead and robustness recovery. Using different pruning rates (80% for Fashion-MNIST, 50% for CIFAR10) reflects dataset-specific considerations about how much compression can be tolerated without catastrophic performance loss.

**Failure Signatures**: 
- Complete failure of robustness recovery after fine-tuning suggests either insufficient fine-tuning duration or that compression removed too much capacity.
- Significant accuracy drop after compression indicates aggressive pruning that removed important features.
- Poor baseline robustness suggests issues with the initial adversarial training procedure or attack evaluation.
- Inconsistent results across runs may indicate sensitivity to initialization or compression parameters.

**First Experiments**:
1. Train baseline models (8-layer CNN for Fashion-MNIST, ResNet-18 for CIFAR10) with standard and adversarial training for 20 epochs and evaluate initial performance.
2. Apply structured pruning (80% for Fashion-MNIST, 50% for CIFAR10) and INT8 quantization to trained models and measure impact on accuracy and robustness.
3. Perform adversarial fine-tuning for 3 epochs on compressed models and evaluate whether robustness is restored to levels comparable with uncompressed adversarially trained models.

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: Does adversarial fine-tuning of compressed models maintain robustness against diverse attack types beyond PGD-ℓ∞?
**Basis in paper**: The authors note they evaluated robustness only against PGD-ℓ∞ attacks and acknowledge that true robustness requires testing against diverse attack methods and norms.
**Why unresolved**: The paper's experiments are limited to a single attack type, leaving the generalizability of adversarial fine-tuning's effectiveness across different attack methods unknown.
**What evidence would resolve it**: Experiments showing adversarial fine-tuning's performance against a broader range of attacks (e.g., DeepFool, AutoAttack) and different ℓp norms would demonstrate its robustness across diverse threat models.

### Open Question 2
**Question**: How does the number of fine-tuning epochs affect robustness and efficiency gains for compressed models across different datasets and architectures?
**Basis in paper**: The authors found three epochs adequate for their experiments but acknowledge this may be task-, dataset-, and model-dependent.
**Why unresolved**: The optimal fine-tuning duration is presented as a hyperparameter requiring careful tuning, with no systematic study of its impact across varying conditions.
**What evidence would resolve it**: A comprehensive ablation study varying fine-tuning epochs across multiple datasets and architectures, measuring the trade-off between robustness gains and computational costs, would identify optimal fine-tuning strategies.

### Open Question 3
**Question**: Does adversarial fine-tuning transfer robustness when applied to compressed models derived from different base architectures?
**Basis in paper**: The authors did not perform cross-architectural experiments, such as training ResNet-18 on Fashion-MNIST or using smaller networks on CIFAR10.
**Why unresolved**: The paper's experiments are limited to specific model-dataset pairs, leaving the generalizability of adversarial fine-tuning's effectiveness across different architectures unexplored.
**What evidence would resolve it**: Experiments applying adversarial fine-tuning to compressed models derived from various architectures (e.g., VGG, DenseNet) on multiple datasets would demonstrate its transferability and robustness across architectural variations.

## Limitations
- Experiments limited to PGD-ℓ∞ attacks, not testing robustness against diverse attack types
- Only two datasets and two model architectures tested, limiting generalizability
- Three epochs of fine-tuning chosen empirically without systematic study of optimal duration
- No exploration of cross-architecture fine-tuning experiments
- Specific compression parameters (pruning rates, quantization method) may not generalize to other settings

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Compression doesn't inherently degrade robustness | Medium |
| Adversarial fine-tuning restores robustness in compressed models | Medium |
| Three epochs sufficient for fine-tuning | Low |
| Results generalize across datasets/architectures | Low |

## Next Checks
1. Verify the specific compression techniques and their implementation details, including the exact pruning method and quantization approach used.
2. Compare the performance of adversarially fine-tuned compressed models against both standard trained compressed models and adversarially trained uncompressed models across a range of compression levels.
3. Conduct ablation studies to isolate the effects of fine-tuning on robustness versus the effects of compression on robustness.