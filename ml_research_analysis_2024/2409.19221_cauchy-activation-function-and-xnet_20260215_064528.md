---
ver: rpa2
title: Cauchy activation function and XNet
arxiv_id: '2409.19221'
source_url: https://arxiv.org/abs/2409.19221
tags:
- activation
- function
- cauchy
- functions
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces the Cauchy Activation Function, a novel activation\
  \ function derived from the Cauchy Integral Theorem in complex analysis, and a new\
  \ class of neural networks called XNet. The Cauchy Activation Function is expressed\
  \ as \u03D5\u03BB1,\u03BB2,d(x) = \u03BB1 \u2217 x/(x2 + d2) + \u03BB2/(x2 + d2),\
  \ where \u03BB1, \u03BB2, d are trainable parameters."
---

# Cauchy activation function and XNet

## Quick Facts
- arXiv ID: 2409.19221
- Source URL: https://arxiv.org/abs/2409.19221
- Authors: Xin Li; Zhihong Xia; Hongkun Zhang
- Reference count: 40
- Primary result: XNet achieves 94.95% accuracy on MNIST and 92% on CIFAR-10 using Cauchy activation function, outperforming ReLU-based models

## Executive Summary
This paper introduces the Cauchy Activation Function, a novel activation function derived from the Cauchy Integral Theorem in complex analysis, and a new class of neural networks called XNet. The Cauchy Activation Function is expressed as ϕλ1,λ2,d(x) = λ1 ∗ x/(x2 + d2) + λ2/(x2 + d2), where λ1, λ2, d are trainable parameters. XNet significantly outperforms established benchmarks in image classification tasks such as MNIST and CIFAR-10, achieving 94.95% and 92% accuracy respectively, compared to 90% and 90% for ReLU activation. XNet also offers substantial advantages over Physics-Informed Neural Networks (PINNs) in solving Partial Differential Equations (PDEs), demonstrating higher-order accuracy and faster convergence.

## Method Summary
The paper presents XNet, a neural network architecture that utilizes the Cauchy Activation Function to improve approximation accuracy in both image classification and PDE solving tasks. The Cauchy activation function is implemented with trainable parameters λ1, λ2, d and replaces traditional activation functions like ReLU in standard CNN and ResNet architectures. For MNIST and CIFAR-10 classification, XNet employs modified CNN and ResNet9 architectures respectively. The networks are trained using the ADAM optimizer with adaptive learning rates over 20-30 epochs. For PDE solving, XNet replaces the standard sigmoid activation in PINN architectures, demonstrating improved accuracy and convergence on 1D heat equation, 2D Poisson equation, and 100D Allen-Cahn PDE problems.

## Key Results
- XNet achieves 94.95% accuracy on MNIST dataset, outperforming ReLU-based models at 90%
- XNet achieves 92% accuracy on CIFAR-10 dataset, outperforming ReLU-based models at 90%
- XNet demonstrates higher-order accuracy and faster convergence than PINNs for solving PDEs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Cauchy activation function achieves higher-order accuracy in function approximation compared to traditional activation functions.
- Mechanism: By leveraging the Cauchy integral formula from complex analysis, the activation function can represent smooth functions with polynomial convergence of arbitrary order. The formula allows weighted combinations of complex kernels to reconstruct target functions with precision.
- Core assumption: The target functions are sufficiently smooth (analytic or nearly analytic) to be well-approximated by complex kernels.
- Evidence anchors:
  - [abstract] "Cauchy Activation Function is expressed as ϕλ1,λ2,d(x) = λ1 ∗ x/(x2 + d2) + λ2/(x2 + d2), where λ1, λ2, d are trainable parameters."
  - [section] "Theoretically, the Cauchy activation function can approximate any smooth function to its highest possible order."
  - [corpus] Weak - no direct comparison with other activation function convergence rates in the corpus.
- Break condition: If the target function is highly non-analytic or contains discontinuities, the polynomial convergence advantage diminishes.

### Mechanism 2
- Claim: XNet achieves superior performance in high-dimensional problems by replacing multiplicative denominators with more efficient structures.
- Mechanism: Traditional CauchyNet struggles with computational complexity in high dimensions due to multiplicative terms in denominators. XNet restructures the network to avoid these multiplicative chains while preserving the approximation power through linear combinations of transformed features.
- Core assumption: The transformation preserves the universal approximation property while reducing computational burden.
- Evidence anchors:
  - [section] "Due to the novelty of the Cauchy activation function, structural changes to the neural networks are necessary to leverage its intrinsic efficiencies."
  - [section] "As typical computer vision problems are high dimensional, with the input data for a 30 × 30 pixel image being at least 900 dimensional, we need a different algorithm to handle high dimensional problems."
  - [corpus] Weak - no explicit structural comparison with standard architectures in the corpus.
- Break condition: If the linear combination approximation cannot maintain accuracy in very high dimensions, performance may degrade.

### Mechanism 3
- Claim: XNet outperforms PINNs in solving PDEs by providing higher-order accuracy and faster convergence.
- Mechanism: The Cauchy-based activation functions provide better local feature capture and decay properties, enabling more precise approximation of PDE solutions and their derivatives compared to standard sigmoid or tanh activations used in PINNs.
- Core assumption: PDE solutions can be well-approximated by analytic functions, making the Cauchy integral approach particularly effective.
- Evidence anchors:
  - [abstract] "XNet also offers substantial advantages over Physics-Informed Neural Networks (PINNs) in solving Partial Differential Equations (PDEs), demonstrating higher-order accuracy and faster convergence."
  - [section] "The 1-dimensional heat equation... We trained the original PINN with the sigmoid activation function and then modified the network to use a Cauchy activation function instead... Table 4 shows comparison results."
  - [corpus] Weak - corpus neighbors mention complex-PINN but no direct performance comparison.
- Break condition: If the PDE solution is non-analytic or has sharp discontinuities, the advantage may be reduced.

## Foundational Learning

- Concept: Cauchy integral theorem and formula in complex analysis
  - Why needed here: The activation function derivation relies on representing analytic functions via contour integrals over their boundary values.
  - Quick check question: What does the Cauchy integral formula tell us about the value of an analytic function inside a contour given its values on the boundary?

- Concept: Universal approximation theorem and its extensions
  - Why needed here: Understanding how different activation functions affect the network's ability to approximate arbitrary continuous functions.
  - Quick check question: How does the universal approximation theorem differ from the Cauchy approximation theorem in terms of convergence guarantees?

- Concept: Physics-Informed Neural Networks (PINNs) architecture and limitations
  - Why needed here: XNet is positioned as an improvement over PINNs, so understanding PINN architecture is crucial for appreciating the innovations.
  - Quick check question: What are the primary limitations of standard PINNs when solving high-dimensional PDEs?

## Architecture Onboarding

- Component map: Input → Hidden layer (Cauchy activation) → Output layer → Loss computation → Backpropagation
- Critical path: Input → Hidden layer (Cauchy activation) → Output layer → Loss computation → Backpropagation
- Design tradeoffs:
  - More complex activation function increases parameter count but provides better approximation
  - Single-layer architecture vs multi-layer standard networks for certain tasks
  - Local decay properties of activation function vs global coverage of ReLU
- Failure signatures:
  - Poor convergence with non-smooth target functions
  - Numerical instability in high dimensions if denominator terms become small
  - Overfitting with too many trainable parameters in λ1, λ2, d
- First 3 experiments:
  1. Replicate MNIST results with simple two-layer model comparing ReLU vs Cauchy activation
  2. Implement Poisson equation solver comparing tanh-based PINN vs Cauchy-based XNet
  3. Test CIFAR-10 classification with modified ResNet9 using Cauchy activation in later layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Cauchy activation function perform on more complex image datasets like ImageNet compared to CIFAR-10 and MNIST?
- Basis in paper: [explicit] The paper demonstrates significant improvements on MNIST and CIFAR-10 datasets, but does not explore larger, more complex datasets like ImageNet.
- Why unresolved: The paper focuses on relatively simple datasets, and the performance on larger, more complex datasets remains untested.
- What evidence would resolve it: Benchmarking XNet on ImageNet and comparing its performance to other state-of-the-art models would provide insights into its scalability and robustness.

### Open Question 2
- Question: Can the Cauchy activation function be adapted for other types of neural network architectures, such as transformers or graph neural networks?
- Basis in paper: [inferred] The paper focuses on CNNs and FNNs, but the general approximation theorem suggests potential applicability to other architectures.
- Why unresolved: The paper does not explore the use of the Cauchy activation function in architectures beyond CNNs and FNNs.
- What evidence would resolve it: Implementing the Cauchy activation function in transformers or graph neural networks and evaluating its impact on performance would clarify its versatility.

### Open Question 3
- Question: What is the theoretical limit of the Cauchy activation function's approximation accuracy for non-analytic functions?
- Basis in paper: [explicit] The paper assumes real-analytic functions for approximation but acknowledges the unpredictable nature of non-analytic functions.
- Why unresolved: The paper does not provide a detailed analysis of the function's performance on non-analytic functions.
- What evidence would resolve it: Conducting experiments with various non-analytic functions and analyzing the approximation errors would help determine the theoretical limits of the Cauchy activation function.

## Limitations

- Limited ablation studies on the impact of individual parameters (λ1, λ2, d) in the Cauchy activation
- No comparison with other modern activation functions beyond ReLU
- Limited testing on more diverse PDE types and higher-dimensional problems

## Confidence

- **High confidence**: The mathematical foundation of the Cauchy activation function is well-established through the Cauchy integral theorem
- **Medium confidence**: The experimental results for MNIST and CIFAR-10 classification accuracy
- **Medium confidence**: The PDE solving performance claims, though validation against more complex PDEs would strengthen this

## Next Checks

1. Conduct ablation studies to determine the sensitivity of performance to each parameter in the Cauchy activation function
2. Compare XNet performance against other modern activation functions (e.g., SiLU, GELU) on the same datasets
3. Test XNet on additional PDE types, particularly those with non-analytic solutions, to assess robustness limitations