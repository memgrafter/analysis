---
ver: rpa2
title: Recovering the state and dynamics of autonomous system with partial states
  solution using neural networks
arxiv_id: '2408.02050'
source_url: https://arxiv.org/abs/2408.02050
tags:
- data
- reference
- predicted
- 'true'
- pred
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates using deep hidden physics models to recover
  the states and dynamics of autonomous systems from partial state data. The method
  employs two neural networks: one to predict state values and another to predict
  the system dynamics.'
---

# Recovering the state and dynamics of autonomous system with partial states solution using neural networks

## Quick Facts
- **arXiv ID**: 2408.02050
- **Source URL**: https://arxiv.org/abs/2408.02050
- **Reference count**: 40
- **Primary result**: DHPMs can recover autonomous system dynamics from partial state data with O(10^-3) errors for full state cases and O(10^-2) for observed state dynamics in partial data cases

## Executive Summary
This paper presents a deep hidden physics model (DHPM) approach for recovering the states and dynamics of autonomous systems from partial state data. The method employs two neural networks: one to predict state values from time and another to predict system dynamics from states. When trained on complete state data, the model accurately recovers both states and dynamics with errors around O(10^-3) for states and O(10^-2) for dynamics. Interestingly, when only partial state data is provided, the model can still accurately predict the dynamics of the observed states, though it fails to recover unobserved states and their dynamics. The approach is tested on 2D linear/nonlinear systems and the Lorenz system, with training taking 2-12 minutes on an NVIDIA Tesla V100 GPU depending on system complexity.

## Method Summary
The method uses two neural networks: Ns predicts states from time, and Nf predicts dynamics from states. The total loss function combines data loss (difference between predicted and true states) and equation loss (residual of the differential equations). Training is performed simultaneously using Adam optimizer with different learning rates. The approach leverages collocation points throughout the domain to enforce physical consistency, and Latin Hypercube Sampling is used to generate these points. The candidate functions for the dynamics network are restricted to the states themselves, exploiting the known structure of autonomous systems.

## Key Results
- When full state data is provided, the model achieves O(10^-3) errors for state prediction and O(10^-2) for dynamics prediction
- With partial state data, the model accurately predicts dynamics of observed states (O(10^-2) error) but fails to recover unobserved states and their dynamics
- Training converges to O(10^-4) total loss in approximately 2 minutes for simpler systems and 12 minutes for the Lorenz system
- The method shows systematic improvement in error reduction with more training iterations across all tested systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The neural network can recover unobserved state dynamics by leveraging the coupled nature of autonomous systems and physical constraints.
- Mechanism: The physics-informed neural network uses the known dynamics of observed states and their coupling to infer the dynamics of unobserved states through the shared equations, even without direct observations.
- Core assumption: The coupling terms in the autonomous system's equations contain sufficient information to reconstruct missing state dynamics.
- Evidence anchors:
  - [abstract] "However in other cases, If the data loss of only one of the state is given, then the error for that state and dynamics are of order O(10−2). Its interesting to note that even though the other state information is not given to model, it's perfectly able to predict dynamics of given data state."
  - [section] "However if the data for particular state is not given in the training, the model fails to capture it's state and dynamics, however it accurately captures the dynamics of states which are given in the training."
- Break condition: If the coupling between states is weak or the system exhibits chaotic behavior with sensitive dependence on initial conditions, the inference may fail.

### Mechanism 2
- Claim: The collocation points and equation losses enforce physical consistency, allowing partial state recovery.
- Mechanism: By sampling collocation points throughout the domain and enforcing the differential equations at these points, the network learns the underlying physics that connects all states, enabling partial state recovery.
- Core assumption: The physical laws governing the system are sufficiently constrained by the collocation points and observed states to allow inference of unobserved states.
- Evidence anchors:
  - [abstract] "The parameters of these two neural networks can be learned by minimizing the total loss function Ltotal."
  - [section] "We define loss function for data and equation as, Ldata = 1/N Σ|u(xi, ti) − u∗(xi, ti)|² and Leqn = 1/M Σ|r(xj, tj)|²"
- Break condition: If too few collocation points are used or they are poorly distributed, the physical constraints may be insufficient for accurate inference.

### Mechanism 3
- Claim: The neural network architecture with separate networks for state prediction and dynamics prediction enables effective learning of autonomous system behavior.
- Mechanism: The two-network architecture allows the model to separately learn the mapping from time to states and from states to dynamics, which is particularly effective for autonomous systems where dynamics depend only on states.
- Core assumption: The separation of state prediction and dynamics prediction into two networks provides better learning capacity than a single network approach.
- Evidence anchors:
  - [abstract] "The method employs two neural networks: one to predict state values and another to predict the system dynamics."
  - [section] "we define residual r(x, t) as, r(x, t) = ∂u/∂t − N"
- Break condition: If the system has complex nonlinear interactions or long-range dependencies, the two-network architecture may not capture all necessary relationships.

## Foundational Learning

- **Concept**: Ordinary Differential Equations (ODEs) and their numerical solutions
  - Why needed here: The paper deals with autonomous systems described by ODEs, and understanding how these are solved numerically is crucial for interpreting the results.
  - Quick check question: What numerical method is used to generate the reference data for training and evaluation?

- **Concept**: Physics-informed neural networks (PINNs)
  - Why needed here: The core methodology relies on PINNs to learn both the solution and the underlying physics of autonomous systems.
  - Quick check question: How does the loss function in PINNs combine data loss and equation loss?

- **Concept**: Neural network architecture and training
  - Why needed here: Understanding how the two-network architecture works and how it's trained is essential for implementing and extending this approach.
  - Quick check question: What optimizer is used for training the neural networks in this paper?

## Architecture Onboarding

- **Component map**: Ns (state predictor) -> Nf (dynamics predictor) -> combined loss (data + equation)
- **Critical path**: Data generation → Network architecture definition → Loss function formulation → Training with Adam optimizer → Evaluation using L2 error metrics
- **Design tradeoffs**: The two-network architecture provides good separation of concerns but may not capture complex state-dynamics interactions as well as a single network
- **Failure signatures**: Poor performance on unobserved states, high equation loss indicating physical inconsistency, slow convergence during training
- **First 3 experiments**:
  1. Implement the two-network architecture for a simple 2D linear autonomous system with full state observations to verify basic functionality.
  2. Test partial state recovery by training with only one state's data and evaluating the model's ability to predict the other state's dynamics.
  3. Apply the method to a nonlinear autonomous system and compare performance with the linear case to understand the impact of nonlinearity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the mathematical reasons behind the neural network's ability to learn dynamics accurately even when only partial state data is provided during training?
- Basis in paper: [explicit] The authors note this behavior is "unanswered" and they are "looking for the mathematical reasons behind that."
- Why unresolved: The paper observes this phenomenon empirically but does not provide a theoretical explanation for why the network can learn dynamics of observed states without complete state information.
- What evidence would resolve it: A mathematical proof or theoretical framework explaining how partial state information enables accurate dynamics learning for observed states, potentially involving properties of the autonomous system structure.

### Open Question 2
- Question: How does the choice of candidate functions in the dynamics network affect the model's ability to recover hidden physics, particularly when dealing with different types of autonomous systems?
- Basis in paper: [explicit] The authors state "knowing prior that the dynamics will be dependent on states only. This fact reduces our efforts in searching for different combinations of candidate functions."
- Why unresolved: The paper only considers candidate functions consisting of states themselves, without exploring how including other candidate functions (like products of states, powers, etc.) might improve or degrade performance.
- What evidence would resolve it: Systematic experiments comparing performance across different candidate function sets for various autonomous systems, including both linear and nonlinear cases.

### Open Question 3
- Question: What is the relationship between the convergence speed of the total loss and the complexity of the autonomous system being modeled?
- Basis in paper: [explicit] The authors observe "it takes 6X more time in training for Lorenz as compared to other systems because it takes 6X more iterations for total loss to be converged at the similar order of magnitude."
- Why unresolved: While the authors note this observation, they don't explore the underlying reasons for this relationship or investigate whether it holds across other system types or dimensions.
- What evidence would resolve it: A comprehensive study measuring convergence times across various autonomous systems with different dimensionalities and nonlinearities, potentially revealing scaling laws or patterns.

## Limitations
- The method requires known candidate functions for system dynamics, limiting applicability to systems where equation forms are known or can be reasonably guessed
- Performance degrades significantly when more than one state is unobserved, with no recovery of unobserved state dynamics
- Computational cost increases substantially for complex systems like the Lorenz system (12 minutes vs 2 minutes for simpler systems)
- The approach is only tested on systems with up to three states, with no exploration of scalability to higher dimensions

## Confidence
- **High confidence**: The ability to accurately predict dynamics of observed states when partial data is provided
- **Medium confidence**: The failure to recover unobserved states and their dynamics
- **Medium confidence**: The claim that the two-network architecture is particularly effective for autonomous systems

## Next Checks
1. Test the method on autonomous systems with weak coupling between states to determine the minimum coupling strength required for successful partial state recovery.
2. Implement an ablation study comparing the two-network architecture against a single-network approach for the same autonomous systems to quantify the architectural benefit.
3. Evaluate the method's performance when candidate functions are misspecified (e.g., using polynomial approximations when the true system is exponential) to assess robustness to modeling assumptions.