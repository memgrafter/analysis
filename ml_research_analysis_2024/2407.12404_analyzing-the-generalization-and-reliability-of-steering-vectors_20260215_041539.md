---
ver: rpa2
title: Analyzing the Generalization and Reliability of Steering Vectors
arxiv_id: '2407.12404'
source_url: https://arxiv.org/abs/2407.12404
tags:
- steering
- steerability
- figure
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper rigorously evaluates the reliability and generalization
  of steering vectors (SVs), a technique for adjusting language model behavior at
  inference time. The authors find that SVs exhibit high variability across inputs,
  with many datasets showing substantial anti-steerability (opposite of desired effect).
---

# Analyzing the Generalization and Reliability of Steering Vectors

## Quick Facts
- arXiv ID: 2407.12404
- Source URL: https://arxiv.org/abs/2407.12404
- Reference count: 30
- Primary result: Steering vectors exhibit high variability and unreliability across inputs, with many datasets showing substantial anti-steerability (opposite of desired effect).

## Executive Summary
This paper rigorously evaluates the reliability and generalization of steering vectors (SVs), a technique for adjusting language model behavior at inference time by intervening on intermediate model activations. The authors find that SVs exhibit high variability across inputs, with many datasets showing substantial anti-steerability (opposite of desired effect). This unreliability is often driven by spurious biases in steerability, such as preferences for specific tokens or positions. While SVs generalize reasonably well out-of-distribution, their performance is not perfect and depends on the similarity of model behavior between source and target prompts. Overall, the study highlights significant limitations in the current form of SVs, indicating the need for further research to improve their reliability and generalization before they can be widely applied to guide model behavior.

## Method Summary
The study evaluates steering vectors using 40 Model-Written Evaluations datasets and additional datasets like TruthfulQA and sycophancy. Steering vectors are extracted using mean-difference of activations at multiple-choice option positions, then applied during inference by adding Î»*vector to activations at specified layers. The evaluation uses logit-difference propensity metrics across various steering multipliers to generate propensity curves, from which steerability scores are computed via linear fits. The analysis examines variance across inputs, spurious biases, out-of-distribution generalization, and cross-model correlations in steerability.

## Key Results
- Steering vectors show high variability in effectiveness across inputs within the same dataset
- Many datasets exhibit substantial anti-steerability, where vectors produce opposite effects to intended
- Steerability is largely a property of the dataset rather than the specific model architecture
- Out-of-distribution generalization depends on similarity of base model behavior between source and target prompts
- Spurious biases (token and position preferences) contribute to steerability variance but don't fully explain it

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Steering vectors work by linearly intervening on intermediate model activations to shift behavior in a desired direction.
- Mechanism: The steering vector is computed as the mean difference between positive and negative activation patterns, then added to the model's activations during inference. This exploits the linear representation hypothesis - that concepts are represented as directions in activation space.
- Core assumption: The target concept can be captured by a single global direction in activation space that generalizes across inputs.
- Evidence anchors:
  - [abstract] "Steering vectors (SVs) are a new approach to efficiently adjust language model behaviour at inference time by intervening on intermediate model activations."
  - [section 3] "We extract a steering vector vM D using the mean difference (MD) of positive and negative activations"
  - [corpus] Weak - related papers discuss decomposition and interpretation but don't directly validate the linear direction hypothesis
- Break condition: The concept requires non-linear combinations of features, or the representation is highly input-dependent rather than following a single global direction.

### Mechanism 2
- Claim: Steering effectiveness varies across inputs due to dataset-level properties rather than model-specific characteristics.
- Mechanism: Different datasets exhibit different degrees of steerability because the target behavior has varying degrees of linear representation in the model's internal representations. This is a property of the dataset-concept pairing, not the specific model architecture.
- Core assumption: Steerability is primarily determined by how well the target concept aligns with the model's learned representations, which is consistent across different model architectures.
- Evidence anchors:
  - [section 6] "Steerability is Mostly a Property of the Dataset" - showing high correlation between steerability in different models
  - [section 5] "Some Behaviours are Un-Steerable" - indicating dataset-level variation in effectiveness
  - [corpus] Weak - related work focuses on technical implementation rather than dataset-model relationships
- Break condition: The model architecture fundamentally changes how concepts are represented, making dataset-level predictions invalid across architectures.

### Mechanism 3
- Claim: Out-of-distribution generalization of steering vectors depends on similarity of base model behavior between source and target prompts.
- Mechanism: When the unsteered model exhibits similar behavior in both the training (source) and application (target) prompt settings, the steering vector generalizes better. This is because the linear direction extracted from the source setting remains relevant in the target setting.
- Core assumption: The steering vector captures behavior-relevant directions that transfer when the underlying model behavior is similar across contexts.
- Evidence anchors:
  - [section 6] "Model Propensity is Predictive of Steering Generalisation" - showing correlation between propensity similarity and relative steerability
  - [section 6] "ID and OOD Steerability are Correlated" - indicating some level of generalization
  - [corpus] Weak - related papers focus on technical aspects but don't explore this specific generalization mechanism
- Break condition: The prompt shift causes the model to use fundamentally different representations or reasoning processes, making the original steering direction irrelevant.

## Foundational Learning

- Concept: Linear representation hypothesis
  - Why needed here: Understanding that steering vectors rely on concepts being represented as directions in activation space is fundamental to grasping why they work and their limitations
  - Quick check question: If a concept requires combining multiple orthogonal directions in activation space, would a single steering vector be sufficient to control it?

- Concept: Mean difference aggregation for vector extraction
  - Why needed here: The mean difference method is the core technique for extracting steering vectors, and understanding its mechanics is crucial for interpreting results
  - Quick check question: What would happen to the steering vector if the dataset had an unequal number of positive and negative examples?

- Concept: Propensity curves and steerability metrics
  - Why needed here: These metrics are how steering effectiveness is quantified and compared across different settings
  - Quick check question: If a steering vector has negative steerability, what does this tell you about its effect on model behavior?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline (contrastive prompt construction) -> Model inference engine (with steering intervention capability) -> Steering vector extraction module (mean difference computation) -> Evaluation framework (propensity curve generation and steerability calculation) -> Analysis tools (variance decomposition, correlation analysis)

- Critical path:
  1. Construct contrastive prompts for dataset
  2. Extract activations at specified layer for positive/negative options
  3. Compute mean difference to get steering vector
  4. Apply steering vector during inference with varying multipliers
  5. Measure logit differences to generate propensity curves
  6. Fit linear models to extract steerability scores
  7. Compare steerability across distributions and models

- Design tradeoffs:
  - Layer selection: Lower layers capture more basic features but may lack semantic meaning; higher layers capture more abstract concepts but may be noisier
  - Aggregation method: Mean difference is simple but may average out important variations; more sophisticated methods could capture non-linear relationships
  - Prompt format: Multiple-choice is easier to evaluate but may not generalize to open-ended generation; open-ended is more realistic but harder to measure

- Failure signatures:
  - High variance in per-sample steerability indicates unreliable control
  - Negative steerability suggests the vector is having opposite effect
  - Poor out-of-distribution generalization indicates context dependence
  - Low steerability across all datasets suggests fundamental limitations of the approach

- First 3 experiments:
  1. Verify layer selection by computing steerability across all layers for a sample dataset and confirming optimal layer choice
  2. Test steerability bias by computing per-sample steerability split by option position/token and checking for systematic differences
  3. Validate generalization correlation by computing unsteered propensity similarity between prompt variations and comparing to relative steerability scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties of datasets lead to high steerability variance and anti-steerability?
- Basis in paper: [explicit] The paper shows that steerability varies widely across inputs and identifies spurious biases like token/position steerability bias, but finds much variance remains unexplained.
- Why unresolved: The paper only identifies a few specific biases and notes that "there is still a high degree of unexplained variance present in many datasets" without identifying the underlying causes.
- What evidence would resolve it: Systematic analysis of dataset properties (e.g., prompt complexity, concept ambiguity, training data distribution) correlated with steerability variance across multiple models and steering methods.

### Open Question 2
- Question: Can alternative steering vector extraction methods reduce spurious biases and improve reliability?
- Basis in paper: [explicit] The paper mentions alternatives like PCA and logistic regression exist but focuses on mean-difference, noting that "future work investigating whether other techniques for extracting and applying SVs can mitigate these biases" is needed.
- Why unresolved: The paper only evaluates mean-difference method and doesn't explore whether other aggregation methods could reduce the observed biases.
- What evidence would resolve it: Comparative evaluation of multiple steering vector extraction methods across the same datasets, measuring both steerability variance and bias mitigation.

### Open Question 3
- Question: What causes the correlation between model propensity similarity and steering vector generalization?
- Basis in paper: [explicit] The paper finds that "the similarity in the propensity of the model in two prompt settings is correlated with the relative steerability" but doesn't explain why this relationship exists.
- Why unresolved: The paper identifies the correlation but doesn't investigate the underlying mechanism or determine if this is a fundamental limitation or something that can be addressed.
- What evidence would resolve it: Analysis of how steering vectors interact with different model activation patterns across prompt variations, and whether this relationship holds for different model architectures or steering methods.

## Limitations
- High variability in steering effectiveness across inputs remains only partially explained, with significant unexplained variance
- Analysis is limited to two model architectures, raising questions about generalizability to other model families
- Focus on multiple-choice contrastive prompts limits applicability to open-ended generation scenarios
- The study doesn't explore alternative steering vector extraction methods that might reduce biases

## Confidence

**High confidence**: Core finding that steering vectors exhibit substantial variability and unreliability across inputs, supported by consistent patterns across multiple datasets and models.

**Medium confidence**: Claim that steerability is primarily a dataset property rather than model-specific, based on correlation between different models' steerability but limited to two architectures.

**Medium confidence**: Generalization findings showing correlation between propensity similarity and steerability generalization, but mechanism remains unclear.

## Next Checks

1. **Dataset bias decomposition**: Systematically analyze each dataset to quantify the contribution of token and position biases versus other sources of steerability variance, using controlled experiments that randomize option ordering and token selection.

2. **Cross-architecture validation**: Test steering vector reliability across a wider range of model architectures (including different families like GPT, Claude, and smaller models) to determine whether steerability patterns are truly dataset properties or also model-dependent.

3. **Open-ended steering evaluation**: Extend the analysis to open-ended generation tasks beyond multiple-choice prompts to assess whether the identified limitations of steering vectors generalize to more realistic application scenarios.