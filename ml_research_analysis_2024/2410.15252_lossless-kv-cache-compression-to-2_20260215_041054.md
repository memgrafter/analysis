---
ver: rpa2
title: Lossless KV Cache Compression to 2%
arxiv_id: '2410.15252'
source_url: https://arxiv.org/abs/2410.15252
tags:
- arxiv
- cache
- clla
- preprint
- quantization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cross-Layer Latent Attention (CLLA), a method
  for compressing the KV cache in large language models by more than 95% without performance
  loss. CLLA integrates attention head/dimension reduction, layer sharing, and quantization
  into a unified framework, achieving lossless performance on 15 diverse tasks while
  reducing KV cache size to less than 2% of the original.
---

# Lossless KV Cache Compression to 2%

## Quick Facts
- arXiv ID: 2410.15252
- Source URL: https://arxiv.org/abs/2410.15252
- Reference count: 15
- Achieves 95% KV cache compression to less than 2% of original size without performance loss

## Executive Summary
This paper introduces Cross-Layer Latent Attention (CLLA), a method for compressing the KV cache in large language models by more than 95% without performance loss. CLLA integrates attention head/dimension reduction, layer sharing, and quantization into a unified framework, achieving lossless performance on 15 diverse tasks while reducing KV cache size to less than 2% of the original. The key insight is to compress KV pairs into low-rank latent states and share them across layers, enabling extreme compression without degradation. CLLA-quant further quantizes these latent states to 4-bit integers, achieving a 50x reduction in KV cache memory. Extensive experiments validate the approach, demonstrating that careful design choices—such as sharing only latent vectors and applying quantization at the latent level—are critical for maintaining model performance.

## Method Summary
CLLA compresses KV cache by projecting key-value pairs into low-rank latent states (C = HWC) that are shared across multiple transformer layers. Each layer maintains its own projection matrices to reconstruct layer-specific KV pairs from the shared latent state (K = CWK, V = CWV). The method uses a sharing factor to determine how many layers share the same latent states. CLLA-quant applies 4-bit symmetric quantization to the compressed latent states using sub-channelwise grouping and RMS normalization. The model is trained on 300 billion tokens using 64 NVIDIA H800 GPUs with a 1.44B parameter architecture featuring 32 layers, 16 attention heads, and 17 experts in a MoE configuration.

## Key Results
- Achieves lossless performance on 15 diverse benchmarks including MMLU, Hellaswag, and CMMLU
- Reduces KV cache size to less than 2% of original (95% compression ratio)
- CLLA-quant achieves 50x reduction in KV cache memory through 4-bit quantization
- Outperforms baseline methods including MLA and SVDq on most evaluation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-Layer Latent Attention (CLLA) compresses KV cache by projecting key-value pairs into low-rank latent states and sharing these across layers.
- Mechanism: The method uses latent vectors C = HWC to represent compressed KV cache, which are then reconstructed via K = CWK, V = CWV. These latent states are shared across layers with each layer maintaining its own projection matrices to reconstruct KV pairs specific to that layer.
- Core assumption: Low-rank latent representation preserves sufficient information for accurate attention computation while enabling extreme compression.
- Evidence anchors:
  - [abstract] "The key insight is to compress KV pairs into low-rank latent states and share them across layers, enabling extreme compression without degradation."
  - [section 3.2] "CLLA involves compressing KV cache into low-rank latent states and subsequently allowing multiple layers to share these states."
  - [corpus] Weak - the corpus neighbors focus on other compression methods like quantization and pruning but don't specifically validate the low-rank latent approach.
- Break condition: If the low-rank approximation loses critical information needed for accurate attention computation, performance degradation would occur.

### Mechanism 2
- Claim: Quantization of latent states to 4-bit integers enables further compression without significant performance loss.
- Mechanism: CLLA-quant applies symmetric quantization Q(C) = intb(clip(C/S, Imin, Imax)) to the latent states, converting 16-bit floating-point values to 4-bit integers. The quantization is applied at the latent level after sharing, which helps mitigate quantization error.
- Core assumption: Quantization error at the latent level is less harmful than quantization at the raw KV level, and training with quantization allows the model to adapt.
- Evidence anchors:
  - [abstract] "CLLA-quant further quantizes these latent states to 4-bit integers, achieving a 50x reduction in KV cache memory."
  - [section 3.3] "The quantization likely enforces a form of robustness in the representations stored in the KV cache, ensuring that minor precision losses do not adversely affect the model's ability."
  - [corpus] Weak - corpus contains related quantization work (SVDq, GEAR) but doesn't specifically validate the latent-level quantization approach.
- Break condition: If quantization introduces significant error that propagates through the attention mechanism, performance would degrade.

### Mechanism 3
- Claim: Sharing only latent vectors (not KV projections) while maintaining layer-specific KV reconstruction provides better performance than full sharing strategies.
- Mechanism: CLLA shares only the compressed latent vectors C across layers while each layer uses its own projection matrices to reconstruct K and V from the shared latent state. This allows each layer to tailor the latent-to-KV projection to its specific needs.
- Core assumption: Layer-specific KV projections can better adapt to quantization loss and capture layer-specific information needs.
- Evidence anchors:
  - [section 3.2] "Although this approach requires more computational resources compared to variations that share both KV activation but not latent vector C, we believe this trade-off is justified by its superior performance relative to other methods."
  - [section 5.2] "Our analysis reveals that CLLA with exclusive sharing of KV latent vectors achieved the best performance."
  - [corpus] Weak - corpus doesn't contain specific validation of this sharing strategy.
- Break condition: If the computational overhead of maintaining separate KV projections outweighs the performance benefits, this design choice would be suboptimal.

## Foundational Learning

- Concept: Transformer attention mechanism and KV cache
  - Why needed here: Understanding how MHA works and why KV cache is essential for inference speed is fundamental to grasping why compression methods are valuable
  - Quick check question: What is the memory complexity formula for KV cache in standard MHA, and why does it grow with sequence length?

- Concept: Low-rank approximation and matrix factorization
  - Why needed here: CLLA relies on projecting KV cache into a low-rank latent space, which requires understanding how low-rank representations can approximate high-dimensional data
  - Quick check question: How does projecting KV cache into a lower-dimensional latent space enable compression, and what information might be lost in this process?

- Concept: Quantization and precision reduction
  - Why needed here: CLLA-quant applies 4-bit quantization to the compressed latent states, requiring understanding of quantization techniques and their impact on model performance
  - Quick check question: What is symmetric quantization, and how does applying it at the latent level differ from applying it directly to KV values?

## Architecture Onboarding

- Component map: Input → Latent projection → Shared latent state → Layer-specific KV reconstruction → Attention computation with RoPE → Output
- Critical path: Input → Latent projection (WC) → Shared latent state (C) → Layer-specific KV reconstruction (WK, WV) → Attention computation with RoPE → Output
- Design tradeoffs: CLLA trades computational overhead (maintaining multiple KV projection matrices) for better performance and robustness to quantization. The design prioritizes performance over minimal computation.
- Failure signatures: Performance degradation would manifest as increased perplexity on validation sets, particularly for tasks requiring precise attention over long contexts. Quantization errors might cause subtle degradation in task-specific metrics.
- First 3 experiments:
  1. Implement CLLA without quantization and compare validation perplexity against baseline MHA to verify the core compression mechanism works
  2. Add 4-bit quantization to the latent states and measure performance impact to validate the quantization strategy
  3. Compare different sharing strategies (sharing latent only vs. sharing latent and KV projections) to identify optimal configuration for downstream tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper bound for KV cache compression ratio without performance degradation in large language models?
- Basis in paper: [inferred] The paper demonstrates 95% compression to 2% of original size without performance loss, but doesn't establish theoretical limits.
- Why unresolved: The paper only explores practical implementations and doesn't derive theoretical limits for compression ratios.
- What evidence would resolve it: A formal proof establishing the maximum achievable compression ratio while maintaining performance, or experiments pushing compression beyond the current 2% threshold to identify the breaking point.

### Open Question 2
- Question: How does CLLA's performance scale with model size and sequence length compared to traditional approaches?
- Basis in paper: [explicit] The paper states "In the future, we will explore our CLLA series on larger scales of LLMs to verify its universality" but doesn't provide such results.
- Why unresolved: All experiments were conducted on a single 1.44B parameter model; scaling effects are unknown.
- What evidence would resolve it: Comprehensive experiments across multiple model sizes (e.g., 7B, 70B parameters) and varying sequence lengths to establish scaling patterns.

## Limitations

- The paper lacks ablation studies to isolate the relative contributions of latent compression, layer sharing, and quantization to overall performance
- Evaluation is limited to 15 tasks, which may not represent the full diversity of LLM applications requiring precise long-range dependencies
- The 300B token training corpus is proprietary, making independent validation difficult

## Confidence

**High Confidence** in the core compression mechanism: The fundamental approach of compressing KV cache into low-rank latent states and sharing them across layers is well-established in the literature and the paper provides clear mathematical formulation and implementation details. The 2% compression ratio is verifiable through the described architecture.

**Medium Confidence** in lossless performance claims: While the paper reports performance on 15 diverse benchmarks, the evaluation lacks comprehensive ablation studies to isolate which design choices are critical for maintaining performance. The proprietary training corpus also limits independent verification.

**Medium Confidence** in the quantization strategy: The paper claims 4-bit quantization achieves 50x compression without significant performance loss, but the quantization error analysis is limited. The claim that quantization at the latent level is less harmful than at the KV level is supported by experiments but not rigorously proven.

**Low Confidence** in scalability claims: The paper doesn't provide extensive analysis of how the method performs across different model sizes, sequence lengths, or hardware configurations. The computational overhead trade-offs are mentioned but not quantified in detail.

## Next Checks

1. **Ablation study validation**: Implement CLLA without quantization and then with quantization separately to measure the individual contribution of each component to the overall performance. This would clarify whether the 2% compression ratio or the lossless performance is more sensitive to specific design choices.

2. **Long-sequence performance testing**: Evaluate the method on tasks requiring very long-range dependencies (e.g., BookSum, PG-19) to verify that compression doesn't degrade performance on sequences significantly longer than those tested. This would validate the claim of effectiveness for "long-context reasoning tasks."

3. **Computational overhead analysis**: Measure and compare the actual inference time and memory usage of CLLA versus baseline methods across different sequence lengths and batch sizes. This would quantify the trade-off between the performance benefits of separate KV projections and the computational overhead they introduce.