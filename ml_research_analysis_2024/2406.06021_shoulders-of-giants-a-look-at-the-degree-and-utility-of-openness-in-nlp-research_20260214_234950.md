---
ver: rpa2
title: 'Shoulders of Giants: A Look at the Degree and Utility of Openness in NLP Research'
arxiv_id: '2406.06021'
source_url: https://arxiv.org/abs/2406.06021
tags:
- papers
- data
- language
- code
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study quantifies artefact reuse and sharing practices in NLP
  research, revealing that 98.9% of analyzed papers reused some artefact from previous
  research. However, 44-67% of papers that created new artefacts did not mention plans
  to release them, and many repository links were found to be broken or missing promised
  artefacts.
---

# Shoulders of Giants: A Look at the Degree and Utility of Openness in NLP Research

## Quick Facts
- arXiv ID: 2406.06021
- Source URL: https://arxiv.org/abs/2406.06021
- Reference count: 40
- 98.9% of analyzed papers reused artefacts from previous research, but 44-67% of papers creating new artefacts did not disclose plans to release them

## Executive Summary
This study systematically quantifies artefact reuse and sharing practices in NLP research by analyzing 6,414 papers from EMNLP and ACL conferences between 2017-2022. The research reveals that while nearly all papers (98.9%) reuse artefacts from previous work, a significant portion (44-67%) of papers creating new artefacts fail to disclose plans to release them. The study also highlights substantial disparities in language resource availability, with high-resource languages having dramatically more datasets and models available compared to low-resource languages. Platform usage patterns show GitHub as the dominant host for data and code, while Hugging Face leads for language models.

## Method Summary
The authors analyzed 6,414 papers from EMNLP and ACL conferences spanning 2017-2022 to quantify artefact reuse and sharing practices in NLP research. They examined paper text to identify mentions of reused artefacts and newly created resources, then tracked whether these artefacts were made publicly available through external platforms like GitHub and Hugging Face. The study also categorized languages into five classes based on resource availability and compared the median number of datasets and language models available across these categories. Repository links were checked for accessibility, and broken links were documented to assess the practical utility of shared resources.

## Key Results
- 98.9% of analyzed papers reused some artefact from previous research
- 44-67% of papers that created new artefacts did not mention plans to release them
- Language-wise disparity remains significant: class 0 languages (2191 languages) have median 0 datasets and 0 language models, while class 5 languages (7 languages) have medians of 657 datasets and 2601 language models

## Why This Works (Mechanism)
The study's effectiveness stems from its systematic approach to analyzing a large corpus of NLP papers over a six-year period. By focusing on two major conferences (EMNLP and ACL) and employing text analysis to identify artefact mentions, the methodology captures comprehensive data about reuse and sharing practices. The combination of quantitative analysis (reuse rates, disclosure percentages) with qualitative assessment (language classification, platform verification) provides both breadth and depth in understanding the current state of artefact sharing in NLP research.

## Foundational Learning
- Artefact reuse patterns in NLP research - needed to understand how research builds upon previous work; quick check: verify reuse rates across different conference years
- Language resource classification systems - needed to contextualize disparities in availability; quick check: examine how languages are categorized into the five classes
- Platform-specific artifact hosting practices - needed to interpret availability statistics; quick check: compare GitHub vs Hugging Face usage patterns
- Repository link verification methodology - needed to assess practical accessibility of shared resources; quick check: evaluate broken link detection process
- Artefact creation vs. reuse attribution - needed to understand disclosure patterns; quick check: verify methodology for distinguishing between created and reused artefacts

## Architecture Onboarding
- Component map: Paper text analysis -> Artefact identification -> Platform verification -> Language classification comparison
- Critical path: Text extraction and analysis forms the foundation for all subsequent findings about reuse and disclosure rates
- Design tradeoffs: The study prioritizes systematic analysis over comprehensive coverage, focusing on two major conferences rather than all NLP venues
- Failure signatures: Missing or broken repository links indicate practical barriers to artefact accessibility despite nominal sharing intentions
- First experiments: (1) Verify reuse rate calculation methodology; (2) Replicate language disparity analysis with different classification thresholds; (3) Test platform verification process on a sample of repositories

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis covers only EMNLP and ACL conferences from 2017-2022, limiting generalizability to broader NLP research landscape
- Cannot fully verify whether mentioned artefacts were actually created by authors or reused from other sources
- Platform-based verification may overstate actual accessibility due to broken links and missing promised artefacts

## Confidence
- Core finding (98.9% reuse rate): High
- Artefact non-disclosure estimate (44-67%): Medium
- Language disparity statistics: Medium

## Next Checks
1. Expand corpus to include additional NLP venues and pre-2017 papers to establish longer-term trends
2. Develop automated tools to verify actual availability and usability of linked resources rather than relying on repository existence
3. Conduct author surveys to understand reasons behind non-disclosure of artefacts and barriers to public resource availability