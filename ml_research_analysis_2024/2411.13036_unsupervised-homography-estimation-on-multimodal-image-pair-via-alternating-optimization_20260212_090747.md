---
ver: rpa2
title: Unsupervised Homography Estimation on Multimodal Image Pair via Alternating
  Optimization
arxiv_id: '2411.13036'
source_url: https://arxiv.org/abs/2411.13036
tags:
- learning
- image
- homography
- loss
- geometry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unsupervised homography estimation
  for multimodal image pairs, where traditional methods struggle due to significant
  modality gaps between images from different domains. The authors propose AltO, a
  two-phase alternating optimization framework inspired by Expectation-Maximization.
---

# Unsupervised Homography Estimation on Multimodal Image Pair via Alternating Optimization

## Quick Facts
- **arXiv ID**: 2411.13036
- **Source URL**: https://arxiv.org/abs/2411.13036
- **Reference count**: 40
- **Key outcome**: Proposes AltO framework achieving superior unsupervised homography estimation on multimodal image pairs compared to existing methods

## Executive Summary
This paper addresses the challenging problem of unsupervised homography estimation for multimodal image pairs, where traditional methods struggle due to significant modality gaps between images from different domains. The authors propose AltO, a two-phase alternating optimization framework inspired by Expectation-Maximization. The framework alternates between a Geometry Learning phase, which maximizes geometric similarity between warped and fixed images using an extended Barlow Twins loss, and a Modality-Agnostic Representation Learning phase, which uses standard Barlow Twins loss to create a shared feature space independent of modality. The method demonstrates superior performance compared to other unsupervised approaches across multiple datasets and is compatible with various registration network architectures.

## Method Summary
The AltO framework employs alternating optimization between two phases: Geometry Learning (GL) and Modality-Agnostic Representation Learning (MARL). In GL phase, a Geometry Barlow Twins (GBT) loss maximizes geometric similarity between warped and fixed images by treating spatial dimensions as batch dimension. In MARL phase, standard Barlow Twins loss aligns representations from different modalities. The method uses a ResNet-34 encoder with modified stem layer, trained with AdamW optimizer, one-cycle learning rate, batch size 16, and 200 epochs. Dynamic deformation augmentation is applied to moving images during training.

## Key Results
- AltO achieves significantly lower Mean Average Corner Error (MACE) than existing unsupervised methods on Google Map, Google Earth, and Deep NIR datasets
- Performance approaches that of supervised learning methods while requiring no ground truth homography labels
- The method is compatible with multiple registration network architectures including IHN, DHN, RAFT, and RHWF
- Ablation studies confirm the necessity of alternating optimization and Global Average Pooling for preventing trivial solutions

## Why This Works (Mechanism)

### Mechanism 1
Alternating optimization between Geometry Learning and Modality-Agnostic Representation Learning phases prevents trivial solutions by focusing on one gap type at a time. During GL phase, encoder/projector weights are frozen to focus on geometric alignment, while during MARL phase, registration network weights are frozen to focus on modality alignment.

### Mechanism 2
Geometry Barlow Twins loss maximizes geometric similarity by treating spatial coordinates (h,w) as batch dimension, enabling redundancy reduction across spatial positions while maintaining channel-wise correlation structure for feature-level geometric alignment.

### Mechanism 3
Global Average Pooling (GAP) is essential for modality-agnostic representation learning to avoid trivial solutions by forcing the model to focus on global feature similarity rather than local details that could lead to constant output solutions.

## Foundational Learning

- **Concept**: Homography estimation as geometric transformation
  - Why needed here: The entire paper builds on understanding homography as a 3x3 transformation matrix mapping points between two planes
  - Quick check question: What are the 8 degrees of freedom in a homography matrix when perspective effects are considered?

- **Concept**: Metric learning and contrastive losses
  - Why needed here: The method relies on Barlow Twins and its variants for both geometry and modality losses
  - Quick check question: How does Barlow Twins loss differ from InfoNCE in terms of the similarity matrix it tries to achieve?

- **Concept**: Alternating optimization and Expectation-Maximization framework
  - Why needed here: The core training algorithm alternates between two phases similar to EM algorithm
  - Quick check question: What is the key difference between standard gradient descent and alternating optimization in terms of parameter updates?

## Architecture Onboarding

- **Component map**: I_A → R → H_AB → ω → eI_A → E → features → GBT loss → backprop to R
  Also: I_A → E → P → z_A and I_B → E → P → z_B → Barlow Twins loss → backprop to E, P

- **Critical path**: Moving image flows through registration network to predict homography, which warps the image, then features are extracted and compared using Geometry Barlow Twins loss. Simultaneously, both images flow through shared encoder and projector to align modalities.

- **Design tradeoffs**: GAP prevents trivial solutions but loses spatial information; number of ResNet stages affects representation capacity vs spatial resolution; batch size impacts InfoNCE effectiveness but requires more memory.

- **Failure signatures**: Loss plateaus early (trivial solution), high MACE values (>23px) (registration network not learning), inconsistent training between phases (alternating schedule too aggressive).

- **First 3 experiments**:
  1. Implement basic alternating optimization with dummy losses to verify training flow and parameter updates
  2. Test Geometry Barlow Twins with synthetic aligned data to verify geometric alignment capability
  3. Validate Modality Barlow Twins with images from different distributions to verify modality alignment

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of AltO scale with larger batch sizes, particularly for the Geometry loss which considers spatial dimensions? The paper mentions Info NCE and Patch NCE benefit from larger numbers of contrasting instance pairs, but notes that the batch size for Modality loss is limited to 16.

### Open Question 2
How does the proposed Geometry Barlow Twins loss compare to other geometric distance metrics like MSE in terms of robustness to feature representation differences between modalities? The paper suggests Barlow Twins-based losses are more suitable for real-world scenarios but doesn't provide direct comparison.

### Open Question 3
What is the impact of removing the max pooling layer after the first convolution in the stem layer of ResNet-34 on the encoder's ability to capture fine structural details? The paper mentions this modification but doesn't provide an ablation study or quantitative analysis.

### Open Question 4
How does the choice of the balance factor λ in the Geometry Barlow Twins and Modality Barlow Twins losses affect the performance of AltO? The paper states λ is set to 0.005 but doesn't explore sensitivity to different values.

## Limitations

- The paper provides limited empirical evidence showing why simultaneous optimization fails, with the claim that geometry and modality gaps are sufficiently independent remaining largely theoretical
- Exact implementation details for handling spatial dimensions in Geometry Barlow Twins loss are not fully specified, potentially affecting reproducibility
- Results show compatibility with multiple registration networks but don't explore how sensitive the alternating optimization framework is to different backbone architectures

## Confidence

**High confidence**: Alternating optimization framework structure, two-phase training procedure, and overall problem formulation are well-defined and reproducible.

**Medium confidence**: Effectiveness of Geometry Barlow Twins loss and necessity of Global Average Pooling are supported by ablation studies, but theoretical foundations could be stronger.

**Low confidence**: Claims about independence of geometry and modality gaps and why alternating optimization prevents trivial solutions lack rigorous theoretical justification.

## Next Checks

1. **Ablation on loss components**: Conduct controlled experiments removing GAP from the projector to verify the paper's claim that this leads to trivial solutions, and test alternative ways to prevent trivial solutions.

2. **Phase independence analysis**: Design experiments to measure the actual independence between geometry and modality gaps by quantifying performance degradation when both phases are trained simultaneously versus alternating.

3. **Cross-architecture robustness**: Test the AltO framework with different encoder architectures (e.g., ConvNeXt, EfficientNet) and registration networks beyond those evaluated in the paper to verify claimed compatibility.