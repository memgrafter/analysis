---
ver: rpa2
title: 'HJ-Ky-0.1: an Evaluation Dataset for Kyrgyz Word Embeddings'
arxiv_id: '2411.10724'
source_url: https://arxiv.org/abs/2411.10724
tags:
- word
- dataset
- kyrgyz
- language
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HJ-Ky-0.1, the first evaluation dataset for
  Kyrgyz word embeddings, constructed by translating the Russian HJ dataset. The dataset
  contains 361 word pairs with similarity scores, designed to assess non-contextual
  word vector representations.
---

# HJ-Ky-0.1: an Evaluation Dataset for Kyrgyz Word Embeddings

## Quick Facts
- **arXiv ID**: 2411.10724
- **Source URL**: https://arxiv.org/abs/2411.10724
- **Reference count**: 38
- **Primary result**: First evaluation dataset for Kyrgyz word embeddings, showing fastText outperforms word2vec and Kyrgyz-trained models beat Russian-trained models

## Executive Summary
This paper introduces HJ-Ky-0.1, the first evaluation dataset for Kyrgyz word embeddings, constructed by translating the Russian HJ dataset. The dataset contains 361 word pairs with similarity scores, designed to assess non-contextual word vector representations. Experiments show that fastText embeddings trained on the Leipzig Corpus outperform both word2vec models and pre-trained fastText models, validating the dataset's effectiveness. Additionally, models trained on Kyrgyz corpora outperform those trained on Russian data, confirming the dataset's alignment with Kyrgyz language characteristics.

## Method Summary
The HJ-Ky-0.1 dataset was created by manually translating 361 word pairs from the Russian HJ dataset into Kyrgyz. The paper evaluates fastText and word2vec embeddings trained on different Kyrgyz and Russian corpora, comparing their performance using Spearman's and Pearson's correlation coefficients between human similarity scores and cosine similarities of word embeddings. Models were trained with dimensions 100/300, window size 5, 5/10 negative samples, and 10 epochs on tokenized and stemmed versions of the kir_newscrawl_2016_1M corpus.

## Key Results
- FastText models trained on Kyrgyz text outperform word2vec models on the HJ-Ky-0.1 dataset
- Kyrgyz-trained models outperform Russian-trained models on the Kyrgyz dataset
- Stemming reduces quality scores due to information loss in Kyrgyz morphology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset is effective because Kyrgyz embeddings outperform Russian embeddings on the same evaluation pairs, demonstrating language-specific alignment.
- Mechanism: The manual translation from Russian to Kyrgyz preserves semantic similarity while adapting to Kyrgyz-specific lexical and morphological features. When evaluated on this dataset, models trained on Kyrgyz text (especially fastText) achieve higher Spearman/Pearson correlations than those trained on Russian text, confirming that the dataset captures Kyrgyz semantic patterns.
- Core assumption: The manual translation process accurately maps Russian word pairs to semantically equivalent Kyrgyz pairs without distorting similarity judgments.
- Evidence anchors:
  - [abstract] "models trained on Kyrgyz corpora outperform those trained on Russian data, confirming the dataset's alignment with Kyrgyz language characteristics."
  - [section 6] "Despite the presence of lexical borrowings from Russian in the Kyrgyz dataset, Russian word embeddings trained on much larger text corpora performed significantly worse..."
  - [corpus] Weak - corpus neighbors are unrelated NLP topics, but this is expected as the paper is a resource dataset.

### Mechanism 2
- Claim: FastText embeddings outperform word2vec on this dataset because they better handle Kyrgyz morphology.
- Mechanism: FastText uses subword information (character n-grams), which is crucial for morphologically rich languages like Kyrgyz. This allows the model to generate embeddings for unseen words and capture morphological similarity, leading to better performance on similarity tasks.
- Core assumption: Kyrgyz morphology is sufficiently represented by character n-grams, and the morphological structure is consistent enough for subword modeling to be effective.
- Evidence anchors:
  - [abstract] "models trained on Kyrgyz corpora outperform those trained on Russian data, confirming the dataset's alignment with Kyrgyz language characteristics."
  - [section 5] "The fastText models for 157 languages, including Kyrgyz and Russian, were released online... These models were trained using the Continuous Bag-of-Words (CBOW) scheme... with a window size of 5, 10 negative samples, and a vector dimension of 300."
  - [corpus] Weak - corpus neighbors discuss morphological segmentation but not fastText.

### Mechanism 3
- Claim: The dataset quality is indirectly validated by the performance gap between stemming and non-stemming preprocessing.
- Mechanism: Stemming in Kyrgyz leads to information loss due to its agglutinative morphology. The drop in quality scores when using stemmed text compared to tokenized text confirms that the dataset benefits from preserving morphological detail, and that the models can exploit this detail effectively.
- Core assumption: The dataset contains word pairs where morphological similarity matters for semantic similarity judgments.
- Evidence anchors:
  - [section 6] "The 'rough' stemming approach resulted in reduced quality scores... For word2vec, stemming even led to the absence of representations for some words."
  - [abstract] "models trained on Kyrgyz corpora outperform those trained on Russian data, confirming the dataset's alignment with Kyrgyz language characteristics."
  - [corpus] Weak - corpus neighbors do not discuss stemming or morphology.

## Foundational Learning

- **Concept: Word embeddings and distributional semantics**
  - Why needed here: The paper evaluates word embeddings, so understanding how they work (predictive vs. count-based methods, distributional hypothesis) is essential.
  - Quick check question: What is the main idea behind the distributional hypothesis in NLP?

- **Concept: Evaluation metrics for word similarity (Spearman, Pearson correlation)**
  - Why needed here: The dataset is evaluated using these metrics to compare human similarity judgments with cosine similarity of embeddings.
  - Quick check question: How do Spearman's rank correlation and Pearson's correlation differ in evaluating word similarity datasets?

- **Concept: Morphological richness and its impact on NLP**
  - Why needed here: Kyrgyz is morphologically rich, and the paper discusses how this affects embedding performance and preprocessing choices.
  - Quick check question: Why might stemming be problematic for morphologically rich languages like Kyrgyz?

## Architecture Onboarding

- **Component map**: Dataset construction (translation from Russian HJ) -> Embedding model training (word2vec, fastText) -> Evaluation (correlation metrics) -> Comparison (Kyrgyz vs. Russian models, stemming vs. tokenization)
- **Critical path**: Translate HJ dataset -> train embeddings on Kyrgyz text -> evaluate on translated pairs -> compare performance across models and preprocessing
- **Design tradeoffs**: Manual translation ensures semantic alignment but is labor-intensive; stemming reduces vocabulary size but loses morphological information; fastText handles morphology but is more complex than word2vec
- **Failure signatures**: Low correlation scores across all models indicate dataset issues; poor performance of Kyrgyz models vs. Russian models suggests translation problems; stemming causing missing embeddings indicates preprocessing issues
- **First 3 experiments**:
  1. Train fastText and word2vec on a small Kyrgyz corpus and evaluate on HJ-Ky-0.1 to confirm the performance gap
  2. Compare tokenized vs. stemmed versions of the same corpus to observe the impact on evaluation scores
  3. Evaluate pre-trained Russian fastText embeddings on HJ-Ky-0.1 to confirm the language-specific performance difference

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the HJ-Ky-0.1 dataset compare to manually annotated Kyrgyz word similarity datasets in terms of correlation with human judgments?
  - Basis in paper: [inferred] The paper suggests future work will involve manual re-annotation and validation, indicating current results are based on translated rather than native annotations.
  - Why unresolved: The dataset was translated from Russian rather than created through native Kyrgyz annotators, so its alignment with Kyrgyz-specific semantic intuitions remains uncertain.
  - What evidence would resolve it: Direct comparison between HJ-Ky-0.1 scores and human similarity ratings from native Kyrgyz speakers for the same word pairs.

- **Open Question 2**: What is the optimal preprocessing strategy for Kyrgyz word embeddings, considering the trade-offs between morphological information retention and vocabulary size?
  - Basis in paper: [explicit] The paper shows stemming reduces quality scores but also causes some words to be excluded from the dataset.
  - Why unresolved: The study only tested basic tokenization versus rough stemming, without exploring more sophisticated morphological preprocessing approaches.
  - What evidence would resolve it: Comparative evaluation of multiple preprocessing strategies (lemmatization, morphological segmentation, character-level processing) on HJ-Ky-0.1 quality scores.

- **Open Question 3**: How does corpus quality versus quantity affect Kyrgyz word embedding performance on HJ-Ky-0.1?
  - Basis in paper: [explicit] The paper shows fastText trained on Leipzig Corpus outperforms CommonCrawl-Ky despite using less data.
  - Why unresolved: The study only compared two corpora; the relationship between corpus size, cleanliness, and embedding quality remains unclear.
  - What evidence would resolve it: Systematic experiments varying corpus size and quality (clean news vs. web crawls) while controlling for total word count.

- **Open Question 4**: Would expanding HJ-Ky-0.1 with synonym dictionary pairs improve its coverage and reliability for Kyrgyz word embedding evaluation?
  - Basis in paper: [explicit] The authors mention plans to expand the dataset with additional pairs derived from synonym dictionaries.
  - Why unresolved: The current dataset has only 361 pairs, which may be insufficient for comprehensive model evaluation.
  - What evidence would resolve it: Comparison of embedding quality scores using the original 361 pairs versus an expanded dataset, measuring stability and discriminative power.

## Limitations
- The dataset construction relies on manual translation from Russian to Kyrgyz, which introduces potential semantic drift that is difficult to quantify
- The evaluation is limited to non-contextual embeddings, excluding modern contextual models like BERT or its Kyrgyz variant, KyrgyzBERT
- The dataset contains only 361 word pairs, which may be insufficient for comprehensive model evaluation

## Confidence

- **High Confidence**: The claim that fastText outperforms word2vec on Kyrgyz due to morphological handling is well-supported by the experimental results and the theoretical advantages of subword modeling
- **Medium Confidence**: The assertion that Kyrgyz-specific models outperform Russian models is convincing but depends on the quality of the manual translation and the representativeness of the HJ dataset for Kyrgyz semantics
- **Low Confidence**: The dataset's overall quality and coverage are difficult to assess without manual re-annotation or comparison with additional Kyrgyz word similarity datasets

## Next Checks
1. Conduct a re-annotation study with multiple Kyrgyz speakers to measure inter-annotator agreement and identify potential translation inconsistencies in HJ-Ky-0.1
2. Expand the evaluation to include word pairs with varying degrees of morphological relatedness to quantify the impact of Kyrgyz morphology on embedding performance
3. Evaluate KyrgyzBERT or other contextual models on HJ-Ky-0.1 to determine if the dataset captures semantic nuances that non-contextual embeddings miss