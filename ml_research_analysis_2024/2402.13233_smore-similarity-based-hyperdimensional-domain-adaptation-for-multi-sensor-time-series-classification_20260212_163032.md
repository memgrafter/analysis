---
ver: rpa2
title: 'SMORE: Similarity-based Hyperdimensional Domain Adaptation for Multi-Sensor
  Time Series Classification'
arxiv_id: '2402.13233'
source_url: https://arxiv.org/abs/2402.13233
tags:
- domain
- data
- smore
- time
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SMORE addresses the challenge of distribution shift in multi-sensor
  time series classification by proposing a novel hyperdimensional computing (HDC)-based
  domain adaptation algorithm. The core idea is to dynamically customize test-time
  models by explicitly considering the domain context of each sample, enabling accurate
  predictions even for out-of-distribution data.
---

# SMORE: Similarity-based Hyperdimensional Domain Adaptation for Multi-Sensor Time Series Classification

## Quick Facts
- arXiv ID: 2402.13233
- Source URL: https://arxiv.org/abs/2402.13233
- Reference count: 23
- Key outcome: Achieves 1.98% higher accuracy than state-of-the-art deep neural network-based domain adaptation algorithms while providing 18.81x faster training and 4.63x faster inference

## Executive Summary
SMORE addresses the challenge of distribution shift in multi-sensor time series classification by proposing a novel hyperdimensional computing (HDC)-based domain adaptation algorithm. The core idea is to dynamically customize test-time models by explicitly considering the domain context of each sample, enabling accurate predictions even for out-of-distribution data. SMORE leverages the efficient and parallel operations of HDC to encode spatial and temporal dependencies in multi-sensor time series data, train domain-specific models, detect out-of-distribution samples, and adaptively ensemble models for inference. Experimental evaluations on three widely-used datasets demonstrate that SMORE achieves on average 1.98% higher accuracy than state-of-the-art deep neural network-based domain adaptation algorithms while providing 18.81x faster training and 4.63x faster inference.

## Method Summary
SMORE uses hyperdimensional computing (HDC) to encode spatial and temporal dependencies in multi-sensor time series data, trains domain-specific models, detects out-of-distribution (OOD) samples, and adaptively ensembles models for inference. The method involves preprocessing multi-sensor time series datasets by segmenting data into non-overlapping windows and labeling domains based on subject IDs. HDC encoding is implemented to map time series data to hyperdimensional space, preserving spatial and temporal dependencies. Domain-specific HDC models are trained for each domain, domain descriptors are constructed, and OOD detection and adaptive test-time modeling procedures are implemented.

## Key Results
- Achieves 1.98% higher accuracy than state-of-the-art deep neural network-based domain adaptation algorithms
- Provides 18.81x faster training and 4.63x faster inference compared to existing methods
- Shows superior efficiency and scalability on resource-constrained edge devices

## Why This Works (Mechanism)

### Mechanism 1
SMORE detects out-of-distribution (OOD) samples by measuring cosine similarity between the encoded query vector and domain descriptors. Each domain descriptor is a bundled hypervector representing the pattern of a specific domain. A test sample is labeled OOD if its similarity to all domain descriptors falls below a threshold ùõø‚àó. Core assumption: Domain descriptors preserve the distinct pattern of their respective domains in hyperdimensional space, and cosine similarity in this space effectively captures distributional differences.

### Mechanism 2
SMORE dynamically constructs test-time models by ensembling domain-specific models weighted by domain similarity. For OOD samples, all domain-specific models are weighted by their similarity to the sample and summed. For in-distribution samples, only domain-specific models from domains with similarity above ùõø‚àó are used. Core assumption: Weighted ensembling allows the model to adapt to the test sample's domain context without retraining, leveraging existing domain knowledge.

### Mechanism 3
Hyperdimensional computing operations enable efficient encoding and adaptation without iterative parameter updates. Spatial and temporal dependencies are captured through binding, bundling, and permutation operations on high-dimensional hypervectors. Classification is performed via similarity calculations rather than gradient descent. Core assumption: High-dimensional spaces provide sufficient orthogonality for information preservation, and similarity-based operations are sufficient for classification tasks.

## Foundational Learning

- **Concept: Domain adaptation and distribution shift**
  - Why needed here: SMORE's entire design addresses the problem of models failing when test data comes from a different distribution than training data.
  - Quick check question: What happens to model accuracy when you train on subjects 1-3 and test on subject 4 in activity recognition datasets?

- **Concept: Hyperdimensional computing fundamentals**
  - Why needed here: SMORE uses HDC operations (binding, bundling, permutation) for both encoding and classification.
  - Quick check question: Why does bundling allow checking for the existence of a hypervector in a set, and how does this differ from binding?

- **Concept: Cosine similarity and high-dimensional geometry**
  - Why needed here: SMORE relies on cosine similarity in hyperdimensional space for both OOD detection and classification.
  - Quick check question: In high-dimensional spaces, why do randomly generated hypervectors tend to be nearly orthogonal, and how does this property enable HDC operations?

## Architecture Onboarding

- **Component map**: Encoder -> Domain-specific models -> Domain descriptors -> OOD detector -> Test-time model constructor -> Classification prediction
- **Critical path**: Encode -> OOD detection -> Test-time model construction -> Classification prediction
- **Design tradeoffs**:
  - Dimensionality vs. efficiency: Higher dimensions improve accuracy but increase memory/computation
  - Threshold ùõø‚àó vs. false positives: Lower thresholds detect more OOD samples but risk misclassifying in-distribution samples
  - Number of source domains vs. model complexity: More domains require more models but improve adaptation capability
- **Failure signatures**:
  - Uniformly low cosine similarities indicate poor encoding or insufficient domain separation
  - Sudden accuracy drops when adding new domains suggest descriptor overlap
  - High inference latency on embedded devices indicates dimensionality or operation inefficiencies
- **First 3 experiments**:
  1. Train SMORE on one domain, test on another, measure OOD detection accuracy and classification performance
  2. Vary the dimensionality (d) and measure impact on accuracy, training time, and inference latency
  3. Sweep the OOD threshold (ùõø‚àó) and plot the trade-off between OOD detection rate and in-distribution classification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of the hyperparameter ùõø‚àó impact the model's performance across different datasets and applications? The paper discusses the impact of ùõø‚àó on the model's performance, but does not provide a comprehensive analysis across different datasets and applications.

### Open Question 2
How does the model's performance change when dealing with multi-domain scenarios where the target domain is a mixture of multiple source domains? The paper assumes that the target domain is a single domain, but in real-world scenarios, the target domain may be a mixture of multiple source domains.

### Open Question 3
How does the model's performance change when dealing with noisy or incomplete multi-sensor time series data? The paper assumes that the multi-sensor time series data is clean and complete, but in real-world scenarios, the data may be noisy or incomplete.

## Limitations
- The effectiveness of OOD detection heavily depends on the choice of threshold Œ¥*, which may require domain-specific tuning
- The assumption that domain descriptors can adequately capture domain-specific patterns may not hold for domains with significant overlap
- Memory requirements for storing high-dimensional hypervectors could become prohibitive for very large-scale applications

## Confidence
- **High confidence**: SMORE's core mechanism of using HDC for efficient encoding and similarity-based operations is well-supported by theoretical properties and experimental results
- **Medium confidence**: The effectiveness of adaptive test-time model construction depends on quality of domain descriptors and appropriateness of similarity threshold
- **Low confidence**: Generalizability to complex multi-label classification tasks and scenarios with highly overlapping domains remains uncertain

## Next Checks
1. Evaluate SMORE on a dataset from a completely different domain (e.g., healthcare monitoring or industrial IoT) to assess its ability to handle diverse time series characteristics and domain shifts
2. Adapt SMORE to handle multi-label classification tasks and evaluate performance on datasets like OPPORTUNITY or PAMAP2 with multiple simultaneous activities
3. Conduct detailed analysis of memory usage across different hypervector dimensionalities (d) and compare with traditional deep learning approaches to quantify trade-off between accuracy gains and memory requirements