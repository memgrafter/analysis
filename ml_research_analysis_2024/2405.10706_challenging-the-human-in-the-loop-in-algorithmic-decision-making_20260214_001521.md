---
ver: rpa2
title: Challenging the Human-in-the-loop in Algorithmic Decision-making
arxiv_id: '2405.10706'
source_url: https://arxiv.org/abs/2405.10706
tags:
- values
- decisions
- which
- information
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work highlights tensions in algorithmic decision-making (ADM)
  involving humans-in-the-loop, focusing on strategic decision-makers (SDMs) and practical
  decision-makers (PDMs). The study reveals that PDMs can significantly influence
  ADM outcomes, potentially misaligning with SDM goals.
---

# Challenging the Human-in-the-loop in Algorithmic Decision-making

## Quick Facts
- arXiv ID: 2405.10706
- Source URL: https://arxiv.org/abs/2405.10706
- Reference count: 28
- One-line primary result: Human decision-makers in algorithmic systems can significantly alter intended societal outcomes through deviations from algorithmic recommendations.

## Executive Summary
This paper challenges the assumption that human oversight in algorithmic decision-making (ADM) systems automatically ensures ethical outcomes. The authors identify a critical tension between strategic decision-makers (SDMs) who set system goals and practical decision-makers (PDMs) who implement them, showing that PDM deviations from algorithmic recommendations can substantially alter societal values even when SDM goals are well-defined. Through experiments with logistic regression classifiers and fairness regularization, the research demonstrates that misalignment between SDM and PDM values can degrade the realization of intended societal outcomes, particularly for fairness metrics like demographic parity. The study emphasizes that effective human-in-the-loop systems require tailored explanations to align SDM and PDM actions, rather than assuming oversight alone guarantees ethical functioning.

## Method Summary
The authors use a benchmark dataset (Boston housing data transformed into classification) to simulate ADM systems with SDM and PDM roles. They train logistic regression classifiers with fairness regularization to model algorithm selection based on societal values, then simulate PDM decision-making that can deviate from algorithmic recommendations. The methodology involves analyzing the impact of SDM-PDM misalignment on realized societal values through controlled experiments measuring changes in fairness metrics when PDMs alter treatment recommendations. Synthetic data is also used to test various scenarios of local versus global decision-making coordination among multiple PDMs.

## Key Results
- Even limited PDM interventions (deviating from algorithmic recommendations in a fraction of cases) can significantly alter societal values
- Misalignment between SDM values and PDM decision-making degrades the realization of intended societal goals
- Lack of coordination among multiple PDMs can prevent global optimization of fairness metrics, as local decisions may not sum to optimal global outcomes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The PDM's ability to deviate from the algorithm's recommendations creates a control transfer from SDM to PDM.
- Mechanism: If the PDM can alter the algorithm's treatment recommendations in a fraction of cases, their decisions will determine the realized values, even if those values differ from the SDM's intent.
- Core assumption: The PDM has both the power to alter decisions and the information (or bias) to act on that power.
- Evidence anchors:
  - [abstract]: "even limited PDM interventions can alter societal values"
  - [section 3.2]: "the PDM can alter the treatment, becoming T_i"
- Break condition: The PDM is constrained to near-zero deviation from the algorithm's recommendations, or the SDM can accurately predict and compensate for PDM behavior.

### Mechanism 2
- Claim: Misalignment between SDM values and PDM decision-making can degrade the realization of societal goals.
- Mechanism: When the SDM selects an algorithm based on its values but the PDM acts on different values or priorities, the resulting treatment policy diverges from the intended societal outcome.
- Core assumption: The PDM's values or constraints differ from those of the SDM, and this difference is not accounted for in the algorithm selection.
- Evidence anchors:
  - [section 4.1]: "Without appropriate information, the PDM cannot act optimally"
  - [section 5.2]: "Change of realized societal values when correcting the recommended actions using the ground truth"
- Break condition: SDM and PDM values are aligned, or the SDM accounts for PDM behavior in algorithm selection.

### Mechanism 3
- Claim: Lack of coordination among multiple PDMs can prevent global optimization of societal values.
- Mechanism: If each PDM optimizes locally based on their subpopulation, the combined result may not match the global optimum, especially for fairness metrics that depend on global population statistics.
- Core assumption: Multiple PDMs operate independently on different subpopulations without global coordination.
- Evidence anchors:
  - [section 5.2]: "Local decisions... can result in reduced realization of the desired values"
  - [section 5.2]: "If no global coordination among PDMs is ensured, local strategies can result in reduced realization of the desired values"
- Break condition: A coordination mechanism is established to align PDM actions with global societal goals.

## Foundational Learning

- Concept: Principal-agent problem
  - Why needed here: The tension between SDM and PDM is a specific case of the principal-agent problem, where the agent (PDM) may not act in the principal's (SDM's) best interest.
  - Quick check question: In the principal-agent framework, what is the primary source of conflict between the two parties?

- Concept: Demographic parity
  - Why needed here: The paper uses demographic parity as a fairness metric to quantify societal values, so understanding this concept is crucial for interpreting the experiments.
  - Quick check question: What does demographic parity require in terms of decision-making across different demographic groups?

- Concept: Local vs. global explanations
  - Why needed here: The paper distinguishes between explanations tailored to SDM and PDM, which relates to the local vs. global nature of explanations in XAI.
  - Quick check question: What is the key difference between local and global explanations in the context of machine learning models?

## Architecture Onboarding

- Component map:
  SDM -> Algorithm -> PDM -> Final Decision
  Explanation System (supports SDM and PDM separately)
  Coordination Layer (optional, for multiple PDMs)

- Critical path:
  1. SDM defines societal values and selects algorithm
  2. Algorithm generates predictions and recommendations
  3. PDM reviews recommendations and makes decisions
  4. Explanation system provides tailored information to SDM and PDM
  5. (Optional) Coordination layer aligns multiple PDMs

- Design tradeoffs:
  - SDM control vs. PDM autonomy: More SDM control may reduce PDM ability to correct algorithmic errors
  - Local optimization vs. global coordination: Local optimization may be more efficient but can harm global goals
  - Explanation complexity vs. usability: More detailed explanations may be more accurate but harder to understand

- Failure signatures:
  - SDM values not realized despite algorithm selection
  - PDM frequently deviates from algorithm recommendations
  - Fairness metrics degrade over time
  - Multiple PDMs produce conflicting outcomes

- First 3 experiments:
  1. Test impact of PDM deviation threshold on realized values
  2. Simulate misalignment between SDM and PDM values
  3. Evaluate effect of local vs. global coordination on fairness metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can SDM and PDM goals be effectively aligned in real-world ADM systems?
- Basis in paper: [explicit] The paper emphasizes the need for tailored explanations to align SDM and PDM actions.
- Why unresolved: Real-world ADM systems often involve complex interactions and unforeseen consequences, making alignment challenging.
- What evidence would resolve it: Empirical studies demonstrating successful alignment strategies in various ADM applications.

### Open Question 2
- Question: What are the specific challenges in coordinating multiple PDMs across different subpopulations?
- Basis in paper: [inferred] The paper discusses the potential issues arising from local decisions made by multiple PDMs.
- Why unresolved: Coordination among multiple PDMs is a complex task that requires further investigation into effective strategies.
- What evidence would resolve it: Case studies or simulations illustrating the impact of coordination strategies on ADM outcomes.

### Open Question 3
- Question: How can the SDM's values be effectively communicated to PDMs to ensure informed decision-making?
- Basis in paper: [explicit] The paper highlights the importance of the SDM's values being understood by PDMs.
- Why unresolved: Effective communication of values is crucial but challenging, especially in complex ADM systems.
- What evidence would resolve it: Research on communication methods that enhance understanding and alignment of values between SDMs and PDMs.

## Limitations

- The study relies on synthetic and transformed datasets rather than real-world ADM deployment scenarios, raising questions about ecological validity
- The assumption that PDMs have perfect information about their own decision-making process and consistent deviation patterns may not reflect real-world behavior
- The experimental setup simplifies complex human factors like time pressure, uncertainty, and contextual constraints that affect real PDMs

## Confidence

- Mechanism validity: Medium-High
- Theoretical framework: Medium-High
- Empirical validation: Medium
- Ecological validity: Low-Medium

## Next Checks

1. **Ecological Validity Test**: Deploy the SDM-PDM framework on a real-world ADM system (e.g., criminal justice risk assessment or healthcare triage) to validate whether the identified mechanisms operate under actual deployment conditions with real decision-makers facing genuine constraints.

2. **Behavioral Realism Assessment**: Conduct experiments with human PDMs making actual decisions under time pressure, with varying levels of information and explanation quality, to test whether the assumed PDM behavior (consistent deviation rates, rational value alignment) matches observed human behavior.

3. **Transferability Evaluation**: Test whether the observed effects of PDM interventions on societal values generalize across different fairness metrics (beyond demographic parity) and different types of ADM tasks (beyond binary classification), including regression and ranking problems common in real ADM applications.