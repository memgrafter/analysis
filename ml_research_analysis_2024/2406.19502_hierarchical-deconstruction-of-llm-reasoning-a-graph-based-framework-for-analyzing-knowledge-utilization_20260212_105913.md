---
ver: rpa2
title: 'Hierarchical Deconstruction of LLM Reasoning: A Graph-Based Framework for
  Analyzing Knowledge Utilization'
arxiv_id: '2406.19502'
source_url: https://arxiv.org/abs/2406.19502
tags:
- questions
- question
- reasoning
- what
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study deconstructs complex questions into a hierarchical\
  \ graph structure to analyze LLM reasoning, using Webb\u2019s Depth of Knowledge\
  \ framework to represent questions at varying complexity levels. A new DEPTH QA\
  \ dataset is created by mapping D3 questions into D2 and D1 sub-questions, allowing\
  \ measurement of forward discrepancy (simpler vs."
---

# Hierarchical Deconstruction of LLM Reasoning: A Graph-Based Framework for Analyzing Knowledge Utilization

## Quick Facts
- arXiv ID: 2406.19502
- Source URL: https://arxiv.org/abs/2406.19502
- Authors: Miyoung Ko; Sue Hyun Park; Joonsuk Park; Minjoon Seo
- Reference count: 40
- Primary result: Hierarchical graph-based deconstruction enables measurement of LLM reasoning gaps through forward/backward discrepancy metrics, with multi-turn interactions improving performance across model sizes

## Executive Summary
This study introduces a novel framework for analyzing large language model (LLM) reasoning by deconstructing complex questions into hierarchical graph structures based on Webb's Depth of Knowledge framework. The authors create the DEPTH QA dataset by mapping D3 questions into D2 and D1 sub-questions, enabling systematic measurement of reasoning capabilities across knowledge depths. Through extensive experiments with multiple LLM families, the framework reveals distinct discrepancy patterns between simpler and complex question performance, with significant improvements achieved through structured multi-turn interactions.

## Method Summary
The framework decomposes complex real-world questions into a graph where each question is represented as a node with edges to prerequisite sub-questions at shallower depths, forming a directed acyclic graph. Using Webb's Depth of Knowledge framework, D3 questions are systematically broken down into D2 and D1 components, creating the DEPTH QA dataset across math, CS, physics, environmental science, and life sciences domains. Models are evaluated using an LLM-as-a-judge approach with GPT-4 Turbo, measuring forward discrepancy (simpler vs complex question performance) and backward discrepancy (complex vs simpler question performance) to identify reasoning gaps and memorization patterns.

## Key Results
- Smaller models exhibit larger forward and backward discrepancies than larger models when solving hierarchical reasoning tasks
- Multi-turn interactions guiding models from simpler to complex questions consistently improve reasoning performance across all model sizes
- Distinct patterns emerge between forward and backward discrepancies, with larger models showing specific error patterns related to training data memorization
- The hierarchical graph framework successfully captures knowledge dependencies, with Min-K% probability distributions revealing memorization issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-based deconstruction enables hierarchical reasoning by explicitly mapping dependencies between knowledge depths
- Mechanism: Each complex question is represented as a node with edges to prerequisite sub-questions at shallower depths, forming a directed acyclic graph where solving deeper nodes requires synthesizing answers from connected shallower nodes
- Core assumption: Complex real-world questions can be meaningfully decomposed into sub-questions that capture all necessary conceptual knowledge
- Evidence anchors:
  - [abstract] "deconstructs complex real-world questions into a graph, representing each question as a node with predecessors of background knowledge needed to solve the question"
  - [section 3.2] "we define edges in our framework as transitions from a node at Dk to at least one direct successor node at Dk+1"
  - [corpus] FMR=0.66 suggests related work exists on graph structures for reasoning, though specific citations are missing
- Break condition: If sub-questions fail to capture all foundational concepts needed for complex questions, or if the decomposition introduces circular dependencies that prevent proper hierarchical reasoning

### Mechanism 2
- Claim: Forward and backward discrepancy metrics reveal model reasoning inconsistencies across knowledge depths
- Mechanism: Forward discrepancy measures performance drop from simpler to complex questions, while backward discrepancy measures cases where models answer complex questions but fail simpler prerequisites
- Core assumption: Discrepancies between performance on simpler vs complex questions indicate gaps in reasoning ability rather than memorization
- Evidence anchors:
  - [abstract] "quantify forward discrepancy, a discrepancy in LLM performance on simpler sub-problems versus complex questions. We also measure backward discrepancy where LLMs answer complex questions but struggle with simpler ones"
  - [section 4.2] "We observe distinct patterns when analyzing forward and backward discrepancies separately"
  - [corpus] Weak evidence - corpus doesn't provide supporting papers for this specific discrepancy measurement approach
- Break condition: If discrepancies are primarily driven by factors other than reasoning gaps (e.g., question difficulty variance, evaluation bias), the metrics lose diagnostic value

### Mechanism 3
- Claim: Multi-turn interactions with structured intermediate steps improve reasoning performance across model sizes
- Mechanism: Providing models with shallower questions in conversation history guides reasoning through intermediate knowledge acquisition, particularly beneficial for smaller models and complex questions
- Core assumption: Models can effectively use conversation history to build upon previously answered sub-questions when solving complex problems
- Evidence anchors:
  - [abstract] "guiding models from simpler to complex questions through multi-turn interactions improves performance across model sizes"
  - [section 4.5] "The multi-turn approach provides the most stable results across all depths, enhancing the performance of smaller models while causing minimal performance drops for larger models"
  - [corpus] FMR=0.51 suggests related work on multi-turn reasoning exists, though specific citations are missing
- Break condition: If models fail to maintain context across turns or if intermediate questions introduce confusion rather than clarity, the approach loses effectiveness

## Foundational Learning

- Concept: Depth of Knowledge framework (DOK)
  - Why needed here: Provides standardized categorization of question complexity levels (D1 recall, D2 application, D3 strategic) that enables systematic decomposition and analysis of reasoning requirements
  - Quick check question: What distinguishes D2 procedural knowledge from D3 strategic knowledge in the context of LLM reasoning evaluation?

- Concept: Graph theory and hierarchical dependencies
  - Why needed here: Understanding how directed acyclic graphs represent knowledge dependencies enables proper construction and analysis of the question hierarchy structure
  - Quick check question: How does representing questions as nodes with prerequisite edges differ from simple linear decomposition approaches?

- Concept: LLM evaluation and scoring methodologies
  - Why needed here: LLM-as-a-judge approach requires understanding how to prompt evaluation models and interpret scoring rubrics for assessing long-form reasoning responses
  - Quick check question: What are the key differences between instance-specific and common scoring rubrics when evaluating LLM reasoning outputs?

## Architecture Onboarding

- Component map: Data pipeline (question curation → decomposition → deduplication → augmentation → human verification) → Model inference (zero-shot, prompt-based, multi-turn) → LLM evaluation → Discrepancy analysis
- Critical path: D3 question selection → GPT-4 Turbo decomposition → Question quality verification → Model inference execution → LLM-as-a-judge evaluation → Discrepancy calculation and analysis
- Design tradeoffs: Manual vs automated decomposition (accuracy vs scalability), GPT-4 Turbo vs human annotation (cost vs quality), single vs multiple evaluation runs (efficiency vs reliability)
- Failure signatures: High forward discrepancies indicate reasoning gaps, high backward discrepancies suggest memorization reliance, inconsistent evaluation scores point to rubric issues
- First 3 experiments:
  1. Validate depth classification accuracy on small sample of TutorEval questions
  2. Test decomposition quality on 5 D3 questions across all domains
  3. Run zero-shot inference on LLaMA 2 7B Chat to establish baseline performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the hierarchical graph-based framework be extended to handle questions that require reasoning across multiple domains or disciplines?
- Basis in paper: [explicit] The paper discusses the current DEPTH QA dataset being limited to math, computer science, environmental science, physics, and life sciences.
- Why unresolved: The current dataset focuses on specific domains, and the paper does not explore how the framework would handle interdisciplinary questions that require knowledge from multiple fields.
- What evidence would resolve it: Experiments demonstrating the framework's ability to accurately deconstruct and reason through complex questions spanning multiple domains, along with an analysis of any performance degradation or improvements compared to single-domain questions.

### Open Question 2
- Question: What is the impact of different LLM architectures (e.g., transformer variants, attention mechanisms) on the forward and backward discrepancy patterns observed in the study?
- Basis in paper: [inferred] The paper analyzes various LLM families (LLaMA, Mistral, Mixtral) but does not specifically investigate how architectural differences affect discrepancy patterns.
- Why unresolved: While the paper compares different model sizes and capacities, it does not isolate the effects of architectural variations on reasoning discrepancies.
- What evidence would resolve it: Comparative studies of models with different architectural designs solving the same DEPTH QA questions, measuring discrepancy patterns and identifying which architectural features contribute most to reducing reasoning gaps.

### Open Question 3
- Question: How does the performance of the graph-based reasoning framework change when applied to real-time or interactive scenarios where users provide partial information or corrections during the reasoning process?
- Basis in paper: [inferred] The paper mentions multi-turn interactions but focuses on pre-determined question sequences rather than dynamic user interactions.
- Why unresolved: The study primarily examines static question-answering scenarios and does not explore how the framework adapts to real-time user input or corrections.
- What evidence would resolve it: Empirical results showing how the framework performs when users can provide intermediate feedback, corrections, or additional context during the reasoning process, and whether this improves overall accuracy and reduces discrepancies.

## Limitations

- DEPTH QA dataset contains only 91 D3 questions across five domains, potentially limiting statistical power for broader claims
- Reliance on GPT-4 Turbo for both question decomposition and evaluation introduces potential bias in the assessment process
- Multi-turn interaction approach may not generalize to all reasoning scenarios, particularly those requiring parallel rather than sequential knowledge integration

## Confidence

**High Confidence:** The hierarchical graph framework for representing knowledge dependencies is methodologically sound, supported by clear mathematical formulations and consistent experimental results across multiple model families.

**Medium Confidence:** The claim that guiding models through multi-turn interactions improves reasoning performance is supported by experimental data but may depend heavily on prompt engineering quality and specific question types.

**Low Confidence:** The assertion that smaller models exhibit larger discrepancies than larger models may be influenced by evaluation sensitivity rather than fundamental reasoning differences.

## Next Checks

1. **Cross-Validation with Human Evaluation:** Conduct blind human evaluations on a subset of DEPTH QA questions to verify the accuracy of LLM-as-a-judge scoring, particularly for identifying reasoning quality versus surface-level correctness.

2. **Training Data Analysis:** Perform controlled experiments to determine whether observed forward and backward discrepancies correlate with specific training data patterns, using models with known training corpora to isolate memorization effects.

3. **Generalization Testing:** Apply the hierarchical deconstruction framework to additional domains (e.g., humanities, social sciences) and question types (e.g., open-ended reasoning vs. factual recall) to assess whether the observed patterns hold beyond the current dataset scope.