---
ver: rpa2
title: An Empirical Comparison of Vocabulary Expansion and Initialization Approaches
  for Language Models
arxiv_id: '2407.05841'
source_url: https://arxiv.org/abs/2407.05841
tags:
- language
- initialization
- embeddings
- languages
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of initializing embeddings for
  new vocabulary items when expanding language models to support additional languages.
  The authors theoretically prove that embeddings initialized within the convex hull
  of existing embeddings preserve the model's behavior on the original language.
---

# An Empirical Comparison of Vocabulary Expansion and Initialization Approaches for Language Models

## Quick Facts
- arXiv ID: 2407.05841
- Source URL: https://arxiv.org/abs/2407.05841
- Reference count: 25
- Key outcome: Convex hull initialization methods preserve source language performance while enabling target language capabilities, with simpler approaches like multivariate initialization performing comparably to advanced methods.

## Executive Summary
This paper addresses the challenge of initializing embeddings for new vocabulary items when expanding language models to support additional languages. The authors theoretically prove that embeddings initialized within the convex hull of existing embeddings preserve the model's behavior on the original language. Based on this insight, they propose a novel method called Constrained Word2Vec (CW2V) that learns new embeddings within the convex hull without requiring cross-lingual embeddings. Experiments with RoBERTa and LLaMA2 models across four languages and five tasks show that CW2V performs equally well or better than more advanced techniques like OFA. Additionally, simpler methods like multivariate initialization achieve comparable performance, indicating that efficient large-scale multilingual continued pretraining can be achieved with simpler initialization methods.

## Method Summary
The method involves expanding a source language model's vocabulary to include target language tokens through tokenizer expansion, then initializing the new embeddings using either Constrained Word2Vec (CW2V) or simpler approaches like multivariate initialization. CW2V uses a constrained skip-gram architecture where the target embedding matrix is expressed as a weighted average of source embeddings, ensuring the convex hull constraint is maintained during learning. The models are then continually pre-trained on a mixture of source and target language data using MLM or LM objectives, followed by evaluation on downstream tasks including XNLI, NER, QA, MT, and XLSUM. The convex hull constraint theoretically guarantees preservation of source language performance while enabling target language capabilities.

## Key Results
- CW2V initialization method learns new embeddings within the convex hull without requiring cross-lingual embeddings, performing equally well or better than advanced methods like OFA
- Simpler initialization methods like multivariate initialization achieve comparable performance to CW2V, suggesting efficient large-scale multilingual continued pretraining can use simpler approaches
- Experiments across four languages (Hindi, Tamil, Russian, German) and five tasks show consistent preservation of English performance while enabling target language capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Initializing new embeddings within the convex hull of existing embeddings preserves the original model's behavior on the source language.
- Mechanism: The convex hull constraint ensures that new embeddings do not exceed the maximum projection of existing embeddings for any direction in the embedding space, thus maintaining the original probability distribution for source language tokens.
- Core assumption: Source language tokens dominate the probability distribution in the expanded vocabulary space when new embeddings are constrained within the convex hull.
- Evidence anchors: [abstract] "theoretically prove that embeddings initialized within the convex hull of existing embeddings preserve the model's behavior on the original language"; [section 3.2] "we can safely say that for the same word sequence w1:i−1, where each word in the sequence belongs to V s, the prefix hi−1 at the output layer remains the same"

### Mechanism 2
- Claim: Constrained Word2Vec (CW2V) learns new embeddings within the convex hull without requiring cross-lingual embeddings.
- Mechanism: CW2V uses a constrained skip-gram architecture where the target embedding matrix is expressed as a weighted average of source embeddings, ensuring the convex hull constraint is maintained during learning.
- Core assumption: The skip-gram objective can effectively learn the new embeddings while maintaining the convex hull constraint.
- Evidence anchors: [abstract] "propose a novel method called Constrained Word2Vec (CW2V) that learns new embeddings within the convex hull without requiring cross-lingual embeddings"; [section 3.4] "we propose a mechanism similar to Skip-gram (Mikolov et al., 2013) to obtain E t"

### Mechanism 3
- Claim: Simpler initialization methods like multivariate initialization perform comparably to advanced methods like OFA.
- Mechanism: Multivariate initialization samples from the multivariate Gaussian distribution of source embeddings, which has a high probability of placing new embeddings within the convex hull.
- Core assumption: The multivariate Gaussian distribution of source embeddings adequately represents the semantic space for new tokens.
- Evidence anchors: [abstract] "simpler methods like multivariate initialization achieve comparable performance, indicating that efficient large-scale multilingual continued pretraining can be achieved with simpler initialization methods"; [section 5.1] "simpler approaches like multivariate or mean initialization, which ensure new embeddings remain within the convex hull, are comparable with more advanced approaches such as OFA"

## Foundational Learning

- Concept: Convex Hull
  - Why needed here: The convex hull defines the set of all possible convex combinations of source embeddings, which is crucial for understanding the constraint that preserves the original model's behavior.
  - Quick check question: Can you explain why initializing new embeddings within the convex hull of existing embeddings preserves the original model's behavior?

- Concept: Skip-gram Model
  - Why needed here: The skip-gram model is the foundation for the Constrained Word2Vec (CW2V) approach, which learns new embeddings within the convex hull constraint.
  - Quick check question: How does the skip-gram objective function in the context of CW2V, and why is it suitable for this task?

- Concept: Multivariate Gaussian Distribution
  - Why needed here: The multivariate Gaussian distribution is used in simpler initialization methods like multivariate initialization, which samples new embeddings from the distribution of source embeddings.
  - Quick check question: Why does sampling from the multivariate Gaussian distribution of source embeddings have a high probability of placing new embeddings within the convex hull?

## Architecture Onboarding

- Component map: Source model (RoBERTa/LLaMA2) -> Tokenizer expansion (merge source and target vocabularies) -> Embedding initialization (CW2V or simpler methods) -> Continual pre-training (MLM/LM objective) -> Downstream tasks (XNLI, NER, QA, MT, XLSUM)

- Critical path: 1. Expand tokenizer with target language subwords 2. Initialize new embeddings using CW2V or simpler methods 3. Perform continual pre-training on source and target language data 4. Evaluate on downstream tasks

- Design tradeoffs: Convex hull constraint vs. flexibility in embedding initialization; Complexity of CW2V vs. simplicity of multivariate initialization; Computational cost of continual pre-training vs. performance gains

- Failure signatures: Significant performance degradation on source language tasks; Poor performance on target language tasks despite continual pre-training; Unstable training or convergence issues during continual pre-training

- First 3 experiments: 1. Initialize new embeddings using CW2V and evaluate on source language tasks to verify preservation of behavior 2. Initialize new embeddings using multivariate initialization and compare performance with CW2V on target language tasks 3. Perform continual pre-training with different initialization methods and evaluate on downstream tasks to identify the most effective approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical relationship between initialization methods that lie within the convex hull and those that do not, in terms of downstream task performance?
- Basis in paper: [explicit] The paper establishes that initializing within the convex hull preserves pre-expansion behavior but notes that the converse is not necessarily true - good initializations may exist outside the convex hull.
- Why unresolved: The paper demonstrates empirical performance differences but does not provide a complete theoretical characterization of how convex hull constraints relate to downstream task performance across different initialization strategies.
- What evidence would resolve it: A formal proof showing necessary and sufficient conditions for good initialization in terms of downstream performance metrics, or a comprehensive empirical study comparing convex hull and non-convex hull initializations across a wide range of tasks and model architectures.

### Open Question 2
- Question: How does the scaling factor applied to the covariance matrix in multivariate initialization affect the probability of the resulting embeddings being within the convex hull?
- Basis in paper: [explicit] The paper mentions that a scaling factor of 1e-5 was applied to the covariance matrix to ensure multivariate initialization remains within the convex hull with high confidence.
- Why unresolved: The paper does not provide a theoretical analysis of how different scaling factors impact the distribution of embeddings within or outside the convex hull, nor does it explore the optimal scaling factor for balancing diversity and hull constraints.
- What evidence would resolve it: A theoretical analysis showing the relationship between scaling factors and the probability of embeddings being within the convex hull, or an empirical study varying the scaling factor and measuring its impact on downstream task performance and hull adherence.

### Open Question 3
- Question: What is the impact of different continued pretraining strategies (e.g., varying the proportion of source vs. target language data) on catastrophic forgetting in English tasks?
- Basis in paper: [explicit] The paper observes an initial drop in English performance during continued pretraining but notes that performance improves with prolonged training without compromising performance on non-English tasks.
- Why unresolved: The paper does not explore how different pretraining strategies, such as adjusting the ratio of source to target language data or implementing specific regularization techniques, affect the balance between maintaining English performance and improving target language performance.
- What evidence would resolve it: Experiments comparing different pretraining strategies, such as varying the proportion of source vs. target language data, implementing regularization techniques, or using curriculum learning approaches, and measuring their impact on both English and target language performance across multiple tasks and model sizes.

## Limitations

- The theoretical guarantees assume source language tokens dominate the probability distribution in the expanded vocabulary space, but this assumption is not empirically validated across diverse language pairs.
- The empirical evaluation is limited to relatively high-resource languages and two model architectures, with modest performance gains over simpler methods.
- The generalizability to extremely low-resource languages or languages with very different typological features from English remains untested.

## Confidence

**High Confidence** - The theoretical framework for convex hull initialization is sound and mathematically proven. The empirical results showing that simpler methods like multivariate initialization perform comparably to advanced methods like OFA are robust across multiple languages and tasks.

**Medium Confidence** - The superiority of CW2V over multivariate initialization is shown but with modest margins that may not justify the additional complexity in all scenarios.

**Low Confidence** - The generalizability of these findings to extremely low-resource languages or languages with very different typological features from English remains untested.

## Next Checks

1. **Distribution Shift Analysis**: Measure the actual change in probability distributions for source language tokens before and after vocabulary expansion across different initialization methods to validate whether the convex hull constraint truly preserves original model behavior at the distributional level.

2. **Stress Test with Typologically Distant Languages**: Evaluate initialization methods with languages that have significantly different writing systems or grammatical structures from English to test the limits of the convex hull assumption.

3. **Extended Adaptation Stability**: Conduct long-term fine-tuning experiments where models are adapted on target language data for extended periods to monitor whether the convex hull constraint continues to preserve source language performance during prolonged adaptation.