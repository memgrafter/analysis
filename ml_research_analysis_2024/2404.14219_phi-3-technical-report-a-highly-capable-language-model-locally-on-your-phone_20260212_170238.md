---
ver: rpa2
title: 'Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone'
arxiv_id: '2404.14219'
source_url: https://arxiv.org/abs/2404.14219
tags:
- phi-3
- arxiv
- data
- language
- shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Phi-3 family of models demonstrates that highly capable language
  models can be built using a carefully curated, high-quality training dataset rather
  than simply scaling model size. Phi-3-mini, with only 3.8 billion parameters, achieves
  performance on par with much larger models like Mixtral 8x7B and GPT-3.5, reaching
  69% on MMLU and 8.38 on MT-bench.
---

# Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone

## Quick Facts
- arXiv ID: 2404.14219
- Source URL: https://arxiv.org/abs/2404.14219
- Reference count: 14
- Key outcome: Phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench, matching much larger models while running locally on phones

## Executive Summary
Phi-3 demonstrates that highly capable language models can be built using carefully curated, high-quality training datasets rather than simply scaling model size. The phi-3 family includes models ranging from 3.8B to 14B parameters, with phi-3-mini achieving performance on par with models like Mixtral 8x7B and GPT-3.5 while being small enough to run locally on mobile devices. The key innovation is a two-phase training approach using heavily filtered web data and synthetic data designed for the "data optimal regime" rather than the compute-optimal regime. This approach enables state-of-the-art performance with significantly reduced computational requirements.

## Method Summary
The Phi-3 models are trained using a two-phase approach with heavily filtered publicly available web data and synthetic data. The training focuses on the "data optimal regime" rather than the compute-optimal regime, emphasizing high-quality curated data over raw scale. The models use transformer decoder architectures with block-sparse attention and can be quantized to 4-bit for mobile deployment. Safety alignment is performed using red-teaming and curated datasets. The family includes phi-3-mini (3.8B), phi-3-small (7B), phi-3-medium (14B), phi-3.5-mini with multilingual and long-context capabilities, phi-3.5-MoE with mixture-of-experts architecture, and phi-3.5-Vision with multimodal capabilities.

## Key Results
- Phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench, matching performance of much larger models
- Model runs locally on phones with 4-bit quantization, occupying only ~1.8GB memory
- Achieves >12 tokens per second generation speed on mobile hardware
- Outperforms models with 10x more parameters on key benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-quality curated data can replace parameter scaling for achieving strong performance
- Mechanism: By filtering web data for "educational level" and adding synthetic reasoning data, the model learns more efficiently per parameter
- Core assumption: Data quality and relevance matter more than quantity for smaller models
- Evidence anchors:
  - [abstract]: "Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data"
  - [section]: "We try to calibrate the training data to be closer to the 'data optimal' regime for small models"
- Break condition: If the filtering criteria don't capture reasoning-relevant patterns or if synthetic data generation is poor quality

### Mechanism 2
- Claim: Two-phase training enables general knowledge then reasoning specialization
- Mechanism: Phase-1 teaches general language understanding, Phase-2 adds logical reasoning through more filtered web data and synthetic examples
- Core assumption: Sequential learning phases allow better knowledge organization than mixed training
- Evidence anchors:
  - [section]: "Pre-training is performed in two disjoint and sequential phases; phase-1 comprises mostly of web sources aimed at teaching the model general knowledge and language understanding. Phase-2 merges even more heavily filtered webdata (a subset used in Phase-1) with some synthetic data that teach the model logical reasoning"
  - [abstract]: "The model is also further aligned for robustness, safety, and chat format"
- Break condition: If the phases don't create complementary knowledge or if reasoning data quality is insufficient

### Mechanism 3
- Claim: Model architecture optimizations enable efficient inference on mobile devices
- Mechanism: Block-sparse attention and 4-bit quantization reduce memory footprint while maintaining performance
- Core assumption: Architectural efficiency can compensate for smaller parameter count in practical deployment
- Evidence anchors:
  - [section]: "phi-3-mini can be quantized to 4-bits so that it only occupies ≈ 1.8GB of memory... achieving more than 12 tokens per second"
  - [section]: "For each attention head, the blocksparse attention enforces different sparsity patterns over KV cache"
- Break condition: If quantization degrades performance below acceptable thresholds or if block-sparse patterns don't preserve attention quality

## Foundational Learning

- Concept: Scaling laws and data-optimal vs compute-optimal regimes
  - Why needed here: Understanding why this approach deviates from standard scaling laws is crucial for grasping the innovation
  - Quick check question: What's the key difference between training in "data optimal regime" vs "compute optimal regime"?

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Phi-3.5-MoE uses MoE to achieve better performance with fewer active parameters
  - Quick check question: How does the top2 routing mechanism in the 16x3.8B MoE model work to select experts?

- Concept: Safety alignment and red-teaming methodology
  - Why needed here: The report emphasizes safety post-training, which is increasingly important for deployed models
  - Quick check question: What are the key components of the safety alignment process described for phi-3 models?

## Architecture Onboarding

- Component map: Data preprocessing → two-phase pretraining → supervised finetuning → DPO → safety alignment → quantization/deployment

- Critical path: The two-phase training approach where phase-1 establishes general knowledge and language understanding, followed by phase-2 which adds logical reasoning and specialized skills through filtered data and synthetic examples

- Design tradeoffs:
  - Smaller models require higher data quality but enable mobile deployment
  - Block-sparse attention saves memory but may reduce attention expressiveness
  - Safety alignment improves alignment but may reduce raw capability

- Failure signatures:
  - Poor performance on factual recall tasks (seen in TriviaQA results)
  - Potential hallucinations in sensitive domains
  - Safety alignment may not fully prevent harmful outputs

- First 3 experiments:
  1. Test block-sparse attention quality by comparing attention patterns with dense attention on a small subset
  2. Validate synthetic data quality by human evaluation of generated reasoning examples
  3. Benchmark quantization impact by measuring performance degradation at different bit levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal data mixture for the "data optimal regime" for 14B parameter models, given that phi-3-medium's performance improvement from 7B to 14B was less than expected?
- Basis in paper: [explicit] The paper states "We observe that some benchmarks improve much less from 7B to 14B than they do from 3.8B to 7B, perhaps indicating that our data mixture needs further work to be in the 'data optimal regime' for 14B parameters model."
- Why unresolved: The paper acknowledges that their current data mixture may not be optimal for larger models, but does not provide specific data or experiments to determine the optimal mixture.
- What evidence would resolve it: Systematic experiments varying the data mixture components (web data, synthetic data, etc.) and their ratios for a 14B parameter model, measuring performance across multiple benchmarks to identify the optimal combination.

### Open Question 2
- Question: How can multilingual capabilities be effectively extended to small language models while maintaining their performance in English?
- Basis in paper: [inferred] The paper mentions "we mostly restricted the language to English" and "Exploring multilingual capabilities for Small Language Models is an important next step" with "some initial promising results on phi-3-small by including more multilingual data."
- Why unresolved: While the paper hints at promising initial results, it does not provide detailed analysis or results on the effectiveness of multilingual training for small models or the trade-offs involved.
- What evidence would resolve it: Comprehensive multilingual benchmark results comparing models trained with different levels of multilingual data, analyzing the impact on both multilingual and English performance, and investigating optimal strategies for multilingual training in small models.

### Open Question 3
- Question: What is the impact of search engine augmentation on the performance of small language models, particularly in addressing their limitations in factual knowledge?
- Basis in paper: [explicit] The paper demonstrates an example of phi-3-mini with search achieving accurate results compared to without search, and states "we believe such weakness can be resolved by augmentation with a search engine."
- Why unresolved: While the paper provides a single example, it does not quantify the general impact of search augmentation on small model performance or explore the optimal integration of search capabilities.
- What evidence would resolve it: Systematic evaluation of phi-3-mini's performance on a wide range of factual knowledge benchmarks with and without search augmentation, including analysis of the types of queries that benefit most from search and the optimal search integration strategy.

## Limitations

- The specific filtering criteria for "educational level" and "reasoning ability" in web data are not fully disclosed, making it difficult to assess generalizability
- Safety alignment effectiveness across different languages and cultural contexts remains uncertain
- Real-world mobile performance may vary significantly across different hardware configurations

## Confidence

**High Confidence**: The basic architectural claims about transformer decoder configuration (3.8B parameters, 3072 hidden dimension, 32 heads, 32 layers) and the two-phase training approach are well-documented and verifiable through the model specifications.

**Medium Confidence**: The performance claims on standard benchmarks (MMLU, MT-bench, GSM-8K) are credible given the detailed methodology and comparison with established models, but the real-world generalization and long-term stability of these results require further validation.

**Low Confidence**: Claims about the superiority of the "data optimal regime" over traditional scaling approaches and the specific impact of block-sparse attention on mobile deployment efficiency are less substantiated due to limited disclosure of implementation details and comparative studies.

## Next Checks

1. **Data Filtering Validation**: Conduct a systematic ablation study testing different levels of data filtering intensity on a subset of the training corpus to quantify the relationship between filtering stringency and final model performance.

2. **Cross-Platform Deployment Testing**: Evaluate phi-3-mini's performance across a diverse range of mobile devices (different CPU architectures, RAM configurations, and age ranges) to establish realistic performance bounds and identify hardware-specific optimization opportunities.

3. **Safety Alignment Robustness Testing**: Implement adversarial testing across multiple languages and cultural contexts to assess the robustness of the safety alignment process, including testing against emerging threat vectors and evaluating whether the alignment degrades over time with continued fine-tuning.