---
ver: rpa2
title: 'Learning Guided Automated Reasoning: A Brief Survey'
arxiv_id: '2403.04017'
source_url: https://arxiv.org/abs/2403.04017
tags:
- pages
- learning
- proof
- volume
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey examines the integration of machine learning (ML) and
  automated reasoning (AR) in theorem proving. The authors highlight the challenges
  of combinatorial explosion in AR systems and the potential of ML to guide and improve
  their performance.
---

# Learning Guided Automated Reasoning: A Brief Survey

## Quick Facts
- arXiv ID: 2403.04017
- Source URL: https://arxiv.org/abs/2403.04017
- Reference count: 40
- Authors: Lasse Blaauwbroek; David Cerna; Thibault Gauthier; Jan JakubÅ¯v; Cezary Kaliszyk; Martin Suda; Josef Urban

## Executive Summary
This survey examines the integration of machine learning (ML) and automated reasoning (AR) in theorem proving. The authors highlight the challenges of combinatorial explosion in AR systems and the potential of ML to guide and improve their performance. The survey covers various topics, including premise selection, proof guidance in different settings, and symbolic classification problems. Key findings include the effectiveness of syntactic and semantic features for characterizing mathematical knowledge, the success of k-nearest neighbors, naive Bayes, and decision trees for premise selection, and the use of graph neural networks for clause selection in saturation-based provers.

## Method Summary
The survey systematically reviews existing literature on ML-guided AR systems, organizing findings by different theorem proving paradigms (sequent calculus, tableau, resolution, superposition) and application areas (premise selection, proof guidance, symbolic classification). The authors analyze various ML approaches including statistical methods (k-NN, naive Bayes), decision trees, and neural architectures (graph neural networks), examining their effectiveness across different theorem proving domains and formal systems.

## Key Results
- Syntactic and semantic features are effective for characterizing mathematical knowledge
- k-nearest neighbors, naive Bayes, and decision trees show success for premise selection
- Graph neural networks are used for clause selection in saturation-based provers
- Tactic-based guidance in interactive theorem provers presents unique challenges and advantages

## Why This Works (Mechanism)
The integration of ML with AR systems works by addressing the combinatorial explosion problem inherent in automated reasoning. ML models can learn patterns from successful proofs to guide search strategies, prioritize relevant premises, and make informed decisions about which inference steps to explore. This learned guidance reduces the search space and computational resources needed to find proofs.

## Foundational Learning
- Premise selection - why needed: Identifies relevant theorems and definitions from large libraries to reduce search space
  - quick check: Effectiveness measured by hit-rate on ATP benchmarks
- Proof guidance strategies - why needed: Directs automated provers toward successful proof paths
  - quick check: Success rate on standard theorem proving benchmarks
- Clause selection - why needed: Determines which logical clauses to process next in saturation-based methods
  - quick check: Impact on proof search time and success rate
- Feature engineering for mathematical objects - why needed: Converts mathematical knowledge into ML-compatible representations
  - quick check: Classification accuracy on theorem/property prediction tasks
- Neural architectures for formal reasoning - why needed: Handles structured mathematical representations
  - quick check: Performance on proof reconstruction and generation tasks

## Architecture Onboarding

Component Map: Formal Libraries -> Feature Extraction -> ML Model -> AR System -> Proof Output

Critical Path: Premise Selection -> Proof Search -> Proof Verification

Design Tradeoffs:
- Model complexity vs. interpretability
- Generalizability vs. domain-specific performance
- Computational overhead vs. search space reduction

Failure Signatures:
- Overfitting to specific theorem libraries
- Poor generalization to novel domains
- Computational bottlenecks in feature extraction
- Guidance models leading to dead-end proof paths

First Experiments:
1. Baseline premise selection using k-NN on established benchmarks (MPTP, Mizar)
2. Clause selection performance comparison: neural vs. heuristic approaches
3. Proof guidance ablation study: with vs. without learned guidance

## Open Questions the Paper Calls Out
The survey identifies several open questions including the need for better benchmarks that reflect real-world theorem proving challenges, the development of more interpretable ML models for formal reasoning, and the integration of multiple AI paradigms (ML, symbolic reasoning, knowledge representation) to create more robust AR systems.

## Limitations
- Rapid evolution of AI-assisted theorem proving may make some findings outdated
- Potential publication bias in selection of examined systems
- Difficulty evaluating long-term impact without follow-up studies

## Confidence
- Syntactic and semantic features effectiveness: Medium
- k-NN and naive Bayes success for premise selection: High (established benchmarks), Medium (novel domains)
- Tactic-based guidance characterization: High (discussed systems), Medium (evolving provers)

## Next Checks
1. Verify the survey's coverage by comparing its reference list against recent conference proceedings (CADE, IJCAR, LPAR) to identify missing significant contributions
2. Replicate the reported performance of k-NN and naive Bayes on premise selection using the same datasets to confirm the claimed effectiveness
3. Contact active researchers in the field to identify any notable systems or approaches that may have been inadvertently omitted from the survey