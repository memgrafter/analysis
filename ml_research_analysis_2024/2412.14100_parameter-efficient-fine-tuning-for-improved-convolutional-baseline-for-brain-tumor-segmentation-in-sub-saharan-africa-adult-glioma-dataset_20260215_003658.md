---
ver: rpa2
title: Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain
  Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset
arxiv_id: '2412.14100'
source_url: https://arxiv.org/abs/2412.14100
tags:
- fine-tuning
- segmentation
- dataset
- peft
- brats-africa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of brain tumor segmentation in
  Sub-Saharan Africa, where domain shift and limited data hinder the performance of
  models trained on Western datasets. The authors propose a parameter-efficient fine-tuning
  (PEFT) approach using convolutional adapters with the MedNeXt architecture to adapt
  models pre-trained on the BraTS-2021 dataset to the BraTS-Africa dataset.
---

# Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset

## Quick Facts
- arXiv ID: 2412.14100
- Source URL: https://arxiv.org/abs/2412.14100
- Reference count: 37
- Mean Dice score of 0.80 achieved with PEFT, comparable to full fine-tuning at 0.77

## Executive Summary
This study addresses the challenge of brain tumor segmentation in Sub-Saharan Africa, where domain shift and limited data hinder the performance of models trained on Western datasets. The authors propose a parameter-efficient fine-tuning (PEFT) approach using convolutional adapters with the MedNeXt architecture to adapt models pre-trained on the BraTS-2021 dataset to the BraTS-Africa dataset. The PEFT method achieves a mean Dice score of 0.80, comparable to full fine-tuning (0.77), while significantly reducing training time and computational resources. Additionally, the model demonstrates high specificity (0.99) but lower sensitivity (0.75), indicating a tendency to oversegment.

## Method Summary
The proposed method uses a parameter-efficient fine-tuning approach with convolutional adapters based on the MedNeXt-S architecture. The adapters are placed sequentially within the MedNeXt blocks, consisting of depthwise convolution, expansion layer, and pointwise convolution. The model is first pre-trained on the BraTS-2021 dataset (1251 samples) and then fine-tuned on the BraTS-Africa dataset (60 training samples) using only the adapter parameters, which represent 11.2% of the total parameters. The approach addresses domain shift between Western and Sub-Saharan African MRI data while preventing overfitting on the small dataset.

## Key Results
- PEFT achieved a mean Dice score of 0.80, comparable to full fine-tuning at 0.77
- PEFT significantly reduced training time from ~10 hours to ~4 hours
- The model demonstrated high specificity (0.99) but lower sensitivity (0.75)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PEFT adapts a model pre-trained on Western MRI to Sub-Saharan Africa data with minimal parameter updates.
- Mechanism: Convolutional adapters add small trainable modules that transform frozen features, reducing domain shift.
- Core assumption: Feature maps from BraTS-2021 capture generalizable tumor patterns, but need local adaptation.
- Evidence anchors:
  - [abstract]: "models trained on BraTS-2021 dataset do not generalize well to BraTS-Africa as shown by 20% reduction in mean dice"
  - [section]: "We first show that models trained on BraTS-2021 dataset do not generalize well to BraTS-Africa"
- Break condition: If adapter modules do not learn effective domain-specific transformations, performance will remain close to baseline.

### Mechanism 2
- Claim: Adapter modules prevent overfitting on the small BraTS-Africa dataset.
- Mechanism: By training only a small fraction of parameters, the model avoids memorizing idiosyncrasies in the limited training set.
- Core assumption: The base model has already learned robust feature representations, so updating all weights risks overfitting.
- Evidence anchors:
  - [abstract]: "PEFT achieved comparable, or in some cases, slightly superior performance to full fine-tuned model"
  - [section]: "PEFT (0.80 mean dice) results in comparable performance to full fine-tuning (0.77 mean dice)"
- Break condition: If the base model is not sufficiently pre-trained or domain shift is too large, full fine-tuning may still outperform PEFT.

### Mechanism 3
- Claim: Adapter design inspired by MedNeXt blocks improves segmentation quality.
- Mechanism: Depthwise convolution + expansion layer + pointwise projection mimics MedNeXt residual structure, enabling efficient feature adaptation.
- Core assumption: Preserving the MedNeXt feature hierarchy while adapting only small portions maintains segmentation accuracy.
- Evidence anchors:
  - [section]: "Our proposed architecture has an expansion layer between the depthwise and point-wise convolutions, inspired by the MedNeXt block"
  - [section]: "The adapter layer results only in an additional 11.2% increase in #parameters (total 34.99M)"
- Break condition: If the adapter expansion ratio or layer placement is suboptimal, segmentation performance will degrade.

## Foundational Learning

- Concept: Domain adaptation in medical imaging
  - Why needed here: MRI characteristics differ between Western and Sub-Saharan Africa datasets, causing performance drop.
  - Quick check question: Why does a model trained on BraTS-2021 perform worse on BraTS-Africa despite both being brain tumor data?

- Concept: Parameter-efficient fine-tuning
  - Why needed here: Limited labeled data in BraTS-Africa; full fine-tuning risks overfitting and high compute cost.
  - Quick check question: How does updating only adapter parameters reduce overfitting compared to updating all model weights?

- Concept: Convolutional adapter modules
  - Why needed here: Need to adapt deep features without destroying pre-trained representations; adapters offer this balance.
  - Quick check question: What role does the expansion layer play in the adapter compared to a standard depthwise-pointwise design?

## Architecture Onboarding

- Component map:
  Frozen MedNeXt-S backbone (encoder + decoder) → ConvNeXt-inspired adapter blocks (depthwise conv + expansion + pointwise conv) → Skip connections between adapters and original features → GELU activation and layer normalization within adapters

- Critical path:
  MedNeXt encoder → adapter module → decoder → segmentation output
  (adapter parameters are the only trainable components during PEFT)

- Design tradeoffs:
  - Adapter placement: Sequential (better performance) vs. Parallel (slightly worse)
  - Expansion ratio: 2 used in this work; higher ratios increase capacity but also risk overfitting
  - Adapter size: 11.2% parameter increase; larger adapters could help but cost more compute

- Failure signatures:
  - Low improvement over baseline → adapter not learning domain-specific features
  - High variance in Dice scores → overfitting or instability in adapter training
  - Degraded performance vs. full fine-tuning → base model not transferable or adapters poorly designed

- First 3 experiments:
  1. Baseline: Train MedNeXt-S from scratch on BraTS-Africa only.
  2. Full fine-tuning: Load pre-trained MedNeXt-S on BraTS-2021, then fine-tune all weights on BraTS-Africa.
  3. PEFT with adapters: Load pre-trained MedNeXt-S, freeze backbone, train only adapter modules on BraTS-Africa.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed Conv-adapter-based PEFT method compare to other PEFT methods like LoRA or Adapters in the context of brain tumor segmentation on the BraTS-Africa dataset?
- Basis in paper: [explicit] The paper compares the proposed method to full fine-tuning but does not compare it to other PEFT methods.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the proposed Conv-adapter-based PEFT method compared to full fine-tuning, but does not explore its performance relative to other PEFT methods.
- What evidence would resolve it: A comparative study of the proposed method with other PEFT methods like LoRA or Adapters on the BraTS-Africa dataset, evaluating metrics such as Dice score, sensitivity, and specificity.

### Open Question 2
- Question: What is the impact of the expansion ratio in the Conv-adapter blocks on the segmentation performance and computational efficiency of the proposed method?
- Basis in paper: [inferred] The paper mentions that an expansion ratio of 2 is used in each MedNeXt block, but does not explore the impact of different expansion ratios on performance.
- Why unresolved: The paper does not provide an analysis of how varying the expansion ratio affects the segmentation results and computational efficiency.
- What evidence would resolve it: Experiments varying the expansion ratio in the Conv-adapter blocks and analyzing the resulting segmentation performance and computational efficiency.

### Open Question 3
- Question: How does the proposed method perform on other low-resource datasets with domain shift, beyond the BraTS-Africa dataset?
- Basis in paper: [inferred] The paper focuses on the BraTS-Africa dataset, which represents a specific case of domain shift and limited data, but does not explore the method's generalizability to other similar datasets.
- Why unresolved: The paper does not provide evidence of the method's performance on other low-resource datasets with domain shift.
- What evidence would resolve it: Application of the proposed method to other low-resource datasets with domain shift and evaluation of its performance using metrics like Dice score, sensitivity, and specificity.

### Open Question 4
- Question: What are the specific challenges and limitations of applying the proposed method to 3D brain tumor segmentation tasks with larger input sizes?
- Basis in paper: [inferred] The paper uses a relatively small input size of 128x128x128, but does not discuss the challenges and limitations of scaling up to larger input sizes for 3D segmentation tasks.
- Why unresolved: The paper does not address the potential difficulties and limitations of applying the proposed method to larger input sizes in 3D segmentation tasks.
- What evidence would resolve it: Experiments with larger input sizes and analysis of the resulting challenges and limitations in terms of computational efficiency, memory requirements, and segmentation performance.

## Limitations
- Limited generalizability due to small fine-tuning dataset (60 training samples)
- Tendency to oversegment tumors, as indicated by high specificity (0.99) but lower sensitivity (0.75)
- Computational efficiency gains demonstrated only on specific hardware and software configuration

## Confidence
- High confidence in the core claim that PEFT achieves comparable performance to full fine-tuning with reduced computational resources
- Medium confidence in the clinical utility given the oversegmentation tendency and limited validation on diverse Sub-Saharan African datasets
- Low confidence in scalability claims without testing on larger datasets and different hardware configurations

## Next Checks
1. Test the PEFT approach on a larger, more diverse Sub-Saharan African dataset to verify scalability and robustness across different imaging centers
2. Conduct ablation studies to determine the optimal adapter configuration (placement, expansion ratio, etc.) for different tumor subregions
3. Evaluate the model's performance across different MRI scanner manufacturers and field strengths to assess real-world applicability in Sub-Saharan Africa