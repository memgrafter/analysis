---
ver: rpa2
title: Best Practices for Distilling Large Language Models into BERT for Web Search
  Ranking
arxiv_id: '2411.04539'
source_url: https://arxiv.org/abs/2411.04539
tags:
- ranking
- bert
- arxiv
- loss
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DisRanker, a knowledge distillation framework
  that transfers ranking capabilities from large language models (LLMs) to BERT for
  web search ranking. The approach uses continued pre-training of LLMs on search data,
  supervised fine-tuning with rank loss using the end-of-sequence token, and hybrid
  point-wise and margin MSE loss for distillation.
---

# Best Practices for Distilling Large Language Models into BERT for Web Search Ranking

## Quick Facts
- arXiv ID: 2411.04539
- Source URL: https://arxiv.org/abs/2411.04539
- Reference count: 10
- Primary result: DisRanker achieves 3.593 PNR and 0.8536 nDCG@5, reducing latency from ~100ms to ~10ms while improving CTR and dwell time in production

## Executive Summary
This paper presents DisRanker, a knowledge distillation framework that transfers ranking capabilities from large language models (LLMs) to BERT for web search ranking. The approach uses continued pre-training of LLMs on search data, supervised fine-tuning with rank loss using the end-of-sequence token, and hybrid point-wise and margin MSE loss for distillation. Offline evaluations show DisRanker achieves 3.593 PNR and 0.8536 nDCG@5, outperforming BERT-large and instruction distillation methods. Online A/B testing demonstrates significant improvements: +0.47% page CTR, +0.58% user CTR, +1.2% dwell time, and +12% GSB score. The distilled BERT model reduces latency from ~100ms to ~10ms while maintaining ranking performance.

## Method Summary
DisRanker employs a three-stage pipeline to distill ranking knowledge from LLMs to BERT. First, domain-continued pre-training (CPT) leverages clickstream data to teach the LLM about query-document relevance relationships. Second, supervised fine-tuning (SFT) uses a rank loss function applied to the end-of-sequence token representation. Finally, knowledge distillation transfers the learned ranking capabilities to BERT using a hybrid point-wise and margin MSE loss function. The framework addresses the computational cost of using large LLMs in production by creating a smaller, faster model that maintains comparable ranking performance.

## Key Results
- Offline: DisRanker achieves 3.593 PNR and 0.8536 nDCG@5
- Online: +0.47% page CTR, +0.58% user CTR, +1.2% dwell time, +12% GSB score
- Latency reduction: ~100ms (7B LLM) → ~10ms (BERT-6)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continued Pre-Training on domain-specific clickstream data significantly improves LLM ranking performance by teaching the model query-document relevance relationships.
- Mechanism: The model learns to generate titles and summaries from queries, effectively learning the mapping between user intent and relevant content.
- Core assumption: The clickstream data contains high-quality examples of user-relevant query-document pairs that can be learned from.
- Evidence anchors:
  - [abstract] "we utilize clickstream data to propagate domain knowledge through Continued Pre-Training (CPT), using queries as inputs to generate titles and summaries that have captured user interest"
  - [section] "we introduce an additional phase of continue d pre-training that leverages search data to endow the model with a more refined comprehension of such relationships"
- Break condition: If the clickstream data is low quality, biased, or doesn't represent true user preferences, the CPT would propagate incorrect relevance signals.

### Mechanism 2
- Claim: Using the </s> token as a representative for query-document pairs works better than [CLS] for autoregressive models in ranking tasks.
- Mechanism: The autoregressive nature of LLMs means only the final token can observe all preceding tokens, making it the most complete representation of the entire sequence.
- Core assumption: The final token state in autoregressive models contains sufficient information to represent the relationship between query and document.
- Evidence anchors:
  - [abstract] "Given the inherent characteristics of autoregressive language models, only the final token </s> can encapsulate all preceding tokens"
  - [section] "we append an end-of-sequence token, </s>, to the input query-document sequence to represent the entirety"
- Break condition: If the </s> token representation doesn't capture enough contextual information for ranking, the model performance would degrade.

### Mechanism 3
- Claim: The hybrid Point-MSE and Margin-MSE loss function effectively transfers ranking knowledge while preventing overfitting.
- Mechanism: Point-MSE ensures the student matches absolute scores, while Margin-MSE preserves relative ranking order and adds regularization.
- Core assumption: Both absolute score matching and relative ranking preservation are necessary for effective knowledge distillation in ranking tasks.
- Evidence anchors:
  - [abstract] "we introduce a hybrid point-wise and margin MSE loss to transfer the ranking knowledge from LLMs to smaller models like BERT"
  - [section] "Point-MSE calculates the absolute difference between the LLM teacher and the BERT student, while Margin-MSE introduces a form of regularization and encourages the student model to learn the relative ranking from the teacher"
- Break condition: If the balance between point-wise and margin loss is incorrect, the student may either overfit to teacher scores or fail to learn proper ranking order.

## Foundational Learning

- Concept: Continued Pre-Training (CPT)
  - Why needed here: Standard LLM pre-training focuses on next-token prediction for general knowledge, but search ranking requires understanding specific query-document relevance relationships.
  - Quick check question: What type of data would be most effective for domain-specific CPT in web search ranking?

- Concept: Knowledge Distillation
  - Why needed here: LLMs are too slow and expensive for real-time search ranking, so we need to transfer their learned ranking capabilities to smaller, faster models like BERT.
  - Quick check question: What are the key differences between Point-MSE and Margin-MSE loss functions in knowledge distillation?

- Concept: Autoregressive vs. Bidirectional Models
  - Why needed here: Understanding why we use </s> token instead of [CLS] token is crucial for implementing the ranking loss correctly.
  - Quick check question: How does the autoregressive nature of LLMs affect which token can best represent a query-document pair?

## Architecture Onboarding

- Component map: GPT Decoder (Teacher) → Supervised Fine-Tuning → Scoring on Unlabeled Data → BERT Encoder (Student) with Hybrid Loss
- Critical path: 1. Domain-Continued Pre-Training on clickstream data 2. Supervised Fine-Tuning with ranking loss 3. Knowledge Distillation using hybrid loss
- Design tradeoffs:
  - Model size vs. latency: 7B LLM (~100ms) vs. BERT-6 (~10ms)
  - Distillation loss balance: β parameter between Point-MSE and Margin-MSE
  - Training data: SFT vs. KD datasets have different characteristics
- Failure signatures:
  - Low PNR but good nDCG@5: Margin-MSE may be too strong
  - High latency: Model too large or inefficient implementation
  - Poor generalization: Overfitting during CPT or SFT
- First 3 experiments:
  1. Test different β values (0.2, 0.4, 0.6) for hybrid loss to find optimal balance
  2. Compare </s> token vs. alternative token representations for query-document pairs
  3. Validate CPT effectiveness by comparing with models trained only on SFT data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of DisRanker vary across different domains beyond web search, such as e-commerce or document retrieval?
- Basis in paper: [inferred] The paper focuses on web search ranking but does not explore other domains.
- Why unresolved: The paper's experiments are limited to a commercial web search engine, and the effectiveness in other domains is not addressed.
- What evidence would resolve it: Testing DisRanker on datasets from different domains and comparing its performance with existing models would provide insights into its versatility.

### Open Question 2
- Question: What is the impact of the size of the continued pre-training dataset on the performance of DisRanker?
- Basis in paper: [explicit] The paper mentions using a collection of high-quality clickstream data for continued pre-training but does not explore the impact of dataset size.
- Why unresolved: The paper does not investigate how varying the size of the continued pre-training dataset affects the model's performance.
- What evidence would resolve it: Conducting experiments with different sizes of continued pre-training datasets and analyzing their impact on DisRanker's performance would clarify this.

### Open Question 3
- Question: How does the choice of the end-of-sequence token affect the performance of DisRanker in different contexts?
- Basis in paper: [explicit] The paper uses the end-of-sequence token, </s>, as a representative of the entire sentence but does not explore alternative tokens.
- Why unresolved: The paper does not explore the impact of using different tokens or strategies for representing query-document pairs.
- What evidence would resolve it: Experimenting with different tokens or strategies and evaluating their impact on DisRanker's performance would provide insights into this aspect.

## Limitations

- Domain-specificity of results: Improvements are based on internal A/B testing at Baidu, making generalizability unclear
- Model architecture details: Paper doesn't specify which GPT variant or provide full implementation details
- Hyperparameter sensitivity: β parameter for loss balance is not thoroughly explored

## Confidence

**High Confidence**: The fundamental mechanism of using continued pre-training on clickstream data to teach LLMs query-document relevance relationships is well-established in the literature and theoretically sound.

**Medium Confidence**: The specific architectural choices (using </s> token, the exact CPT implementation details, and the hybrid loss formulation) are supported by the experimental results, but the lack of ablation studies makes it difficult to assess the relative importance of each component.

**Low Confidence**: The magnitude of online improvements (+0.47% page CTR, etc.) cannot be independently verified without access to the same search environment, user base, and evaluation methodology used by Baidu.

## Next Checks

1. **Ablation Study**: Systematically remove each component (CPT, SFT with rank loss, hybrid distillation loss) and measure the degradation in PNR and nDCG@5 to quantify the contribution of each mechanism.

2. **Generalization Test**: Apply the DisRanker framework to a different search domain (e.g., academic search, product search) with a different clickstream dataset to verify if the performance improvements transfer across domains.

3. **Hyperparameter Sensitivity Analysis**: Conduct a grid search over β values (0.0 to 1.0 in increments of 0.1) and report the performance curve to identify whether the current choice is optimal or if performance is relatively insensitive to this parameter.