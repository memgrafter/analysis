---
ver: rpa2
title: 'LLMs are Biased Teachers: Evaluating LLM Bias in Personalized Education'
arxiv_id: '2410.14012'
source_url: https://arxiv.org/abs/2410.14012
tags:
- bias
- figure
- student
- levels
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates biases in large language models (LLMs) when
  acting as personalized educational tutors. Using novel Mean Absolute Bias (MAB)
  and Maximum Difference Bias (MDB) metrics across 27 demographic characteristics,
  experiments with over 17,000 educational explanations reveal significant bias in
  content selection and generation.
---

# LLMs are Biased Teachers: Evaluating LLM Bias in Personalized Education

## Quick Facts
- arXiv ID: 2410.14012
- Source URL: https://arxiv.org/abs/2410.14012
- Authors: Iain Weissburg; Sathvika Anand; Sharon Levy; Haewon Jeong
- Reference count: 40
- Frontier LLMs exhibit significant bias when personalizing educational content across 27 demographic characteristics, with income and disability status showing highest bias

## Executive Summary
This study systematically evaluates bias in large language models when acting as personalized educational tutors. Using novel Mean Absolute Bias (MAB) and Maximum Difference Bias (MDB) metrics across 27 demographic characteristics, experiments with over 17,000 educational explanations reveal that all tested frontier models exhibit similar bias patterns. The highest bias appears for income levels and disability status, while sex/gender and race/ethnicity show the lowest bias. These biases persist across different topics, roles (teacher vs. student), and even in mathematical content where linguistic complexity doesn't correlate with difficulty, suggesting LLMs risk perpetuating stereotypes and reinforcing educational inequality when used for personalized learning.

## Method Summary
The study evaluates LLM bias through two tasks: ranking educational content at appropriate difficulty levels for students with different demographic characteristics, and generating new educational content. Using 27 demographic attributes and 5 datasets totaling 17,000+ explanations across multiple topics and difficulty levels, the researchers employ Mean Absolute Bias (MAB) and Maximum Difference Bias (MDB) metrics to quantify bias. The analysis includes 9 LLMs including GPT-4o, Gemini 1.5 Pro, and Llama 3.1 405B, with statistical validation through 95% bootstrapping confidence intervals and Friedman tests for significance.

## Key Results
- All frontier models show similar bias patterns, with highest MAB relative to income levels and highest MDB relative to both income and disability status
- Bias persists across different topics, roles (teacher vs. student), and even in mathematical content where linguistic complexity doesn't correlate with difficulty
- The lowest bias appears for sex/gender and race/ethnicity characteristics, while income and disability status show the highest bias levels
- Similar bias patterns emerge across ranking versus generation tasks, indicating fundamental model-level associations between demographics and ability levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs generate biased educational content because their pretraining data contains societal stereotypes that persist through fine-tuning.
- Mechanism: The model learns statistical associations between demographic attributes and abilities/knowledge from web data, which reflects societal biases. When generating or ranking educational content, it applies these associations to predict appropriate difficulty levels.
- Core assumption: The pretraining corpus contains stereotypical associations between demographics and abilities that survive fine-tuning.
- Evidence anchors:
  - [abstract] "Our experiments, which utilize over 17,000 educational explanations across multiple difficulty levels and topics, uncover that models potentially harm student learning by both perpetuating harmful stereotypes and reversing them."
  - [section] "Extensive research has shown how bias in algorithms and machine learning systems can cause harm (Danks and London, 2017; Mehrabi et al., 2021). This has been evaluated in vector representations (Bolukbasi et al., 2016; Dev et al., 2020), task-specific models (Rudinger et al., 2018; CÃ¢mara et al., 2022), and language models in various settings (Li et al., 2020; Feng et al., 2023; Li et al., 2023)."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.481, average citations=0.0. Evidence suggests the broader literature on LLM bias in education exists but has low citation counts.

### Mechanism 2
- Claim: LLMs fail to differentiate between teacher and student roles, applying demographic biases regardless of the requested perspective.
- Mechanism: The model treats demographic attributes as predictive features for content difficulty rather than contextual information about the user. When asked to act as either teacher or student, it applies the same statistical associations.
- Core assumption: The model does not maintain separate reasoning processes for different roles but instead relies on demographic features uniformly.
- Evidence anchors:
  - [abstract] "We find that bias is similar for all frontier models, with the highest MAB along income levels while MDB is highest relative to both income and disability status."
  - [section] "In this section, we investigate if there are differences in ranking bias between our setting ('teacher role') and the traditional persona setting ('student role'). We find that biases perpetuate with similar patterns for both role settings."
  - [corpus] "The Life Cycle of Large Language Models: A Review of Biases in Education" - suggests role-based bias differences are an active research area.

### Mechanism 3
- Claim: LLMs perpetuate bias even in mathematical content where linguistic complexity doesn't correlate with difficulty.
- Mechanism: The model applies learned associations between demographics and ability levels beyond linguistic features, affecting content selection and generation even when the subject matter doesn't involve language-based complexity.
- Core assumption: The model's bias mechanisms operate independently of linguistic complexity and apply to all content types.
- Evidence anchors:
  - [abstract] "We find that bias is similar for all frontier models, with the highest MAB along income levels while MDB is highest relative to both income and disability status."
  - [section] "In this setting, we explore if bias patterns are prevalent in the mathematical setting, where linguistic complexity does not correlate with difficulty. We provide each model problem-solution pairs from the MATH-50 dataset... Surprisingly, similar bias patterns propagate, including instances of reverse bias."
  - [corpus] Weak evidence - no direct corpus support found for math-specific bias propagation.

## Foundational Learning

- Concept: Statistical bias in machine learning
  - Why needed here: Understanding how models learn and perpetuate statistical associations between features (demographics) and outcomes (content difficulty) is fundamental to interpreting the bias mechanisms.
  - Quick check question: If a model is trained on data where high-income students consistently receive more advanced explanations, what statistical bias might it learn when personalizing content?

- Concept: Readability metrics and linguistic complexity
  - Why needed here: The study uses Flesch-Kincaid, Gunning Fog, and Coleman-Liau indices to quantify content difficulty, requiring understanding of how these metrics work and their limitations.
  - Quick check question: What three linguistic features do the Flesch-Kincaid Grade Level formula consider when calculating readability scores?

- Concept: Bootstrap resampling and confidence intervals
  - Why needed here: The study uses bootstrap resampling to compute 95% asymmetric confidence intervals for bias metrics, requiring understanding of this statistical technique.
  - Quick check question: How does bootstrap resampling help estimate the uncertainty in bias metrics when the underlying data distribution is unknown?

## Architecture Onboarding

- Component map: Data ingestion -> Prompt template application -> Model inference -> Readability/complexity scoring -> Bias metric calculation -> Statistical analysis
- Critical path: Prompt generation -> Model inference -> Scoring -> Bias calculation (each step must complete successfully for valid results)
- Design tradeoffs: Using readability scores vs. human evaluation (scalability vs. accuracy), multiple models vs. deep analysis of fewer models (breadth vs. depth)
- Failure signatures: Model refusals, inconsistent scoring across metrics, statistical insignificance, high variance in bootstrap results
- First 3 experiments:
  1. Replicate ranking experiment with a single model and dataset to verify methodology
  2. Test different prompt templates to assess sensitivity to phrasing
  3. Apply alternative readability metrics to verify consistency of complexity scoring

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do initial biases in LLMs persist and influence long-term personalization outcomes in educational settings?
- Basis in paper: [inferred] from the "Limitations" section discussion about restricted student information and potential evolution of biases over time
- Why unresolved: The study only simulates initial interactions with limited student context, not tracking how biases might change as the LLM accumulates more information about students
- What evidence would resolve it: Longitudinal studies tracking LLM behavior across multiple interactions with the same students, measuring how initial biases evolve and affect learning outcomes over extended periods

### Open Question 2
- Question: What is the appropriate balance between stereotyping and accommodation in LLM teachers to optimize learning outcomes for diverse student groups?
- Basis in paper: [explicit] from the Discussion section discussing the tension between stereotyping LLM teachers and creating relatable LLM teachers
- Why unresolved: The paper identifies the issue but doesn't establish clear criteria for distinguishing harmful stereotyping from beneficial accommodation in educational contexts
- What evidence would resolve it: Empirical studies measuring learning outcomes across different levels of personalization, determining at what point personalization shifts from helpful to harmful for various demographic groups

### Open Question 3
- Question: How do LLMs infer demographic information from indirect cues in educational settings, and can this inference process be controlled or made transparent?
- Basis in paper: [explicit] from the "Inferring Characteristics from Education Data" section discussing how LLMs can reliably infer demographic details from various indirect signals
- Why unresolved: While the paper acknowledges this capability, it doesn't explore the mechanisms behind this inference or potential methods for mitigating unintended bias
- What evidence would resolve it: Analysis of LLM attention patterns and feature importance when making demographic inferences, along with interventions to reduce reliance on correlated but irrelevant attributes

## Limitations

- Reliance on automated readability metrics may not fully capture educational appropriateness compared to human expert evaluation
- High refusal rates from certain models (particularly Llama 3.1 405B) introduce potential selection bias in the results
- The study cannot distinguish between bias learned from pretraining data versus fine-tuning, limiting understanding of bias origins

## Confidence

**High Confidence**: The finding that all tested frontier models exhibit similar bias patterns across demographic characteristics, with income and disability status showing the highest bias. The methodology using MAB and MDB metrics is clearly specified and the statistical analysis appears sound.

**Medium Confidence**: The claim that biases persist across different topics and roles (teacher vs. student). While the experimental design supports this, the study doesn't fully explore whether specific prompting strategies could mitigate these biases.

**Low Confidence**: The assertion that LLMs perpetuate bias even in mathematical content where linguistic complexity doesn't correlate with difficulty. This finding is based on a single dataset (MATH-50) and lacks supporting literature from the corpus analysis.

## Next Checks

1. **Replicate with alternative difficulty metrics**: Validate the bias findings using human expert evaluation of educational appropriateness rather than automated readability scores to confirm that bias patterns persist when using gold-standard assessment methods.

2. **Test intervention strategies**: Apply known debiasing techniques (such as calibrated prompting or fine-tuning on balanced datasets) to determine whether the observed biases can be systematically reduced or eliminated.

3. **Examine intersectional effects**: Extend the analysis to explore how combinations of demographic characteristics (e.g., low-income students with disabilities) interact to produce compounded or mitigated bias effects that aren't visible in individual characteristic analysis.