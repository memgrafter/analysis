---
ver: rpa2
title: 'EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty'
arxiv_id: '2401.15077'
source_url: https://arxiv.org/abs/2401.15077
tags:
- eagle
- draft
- feature
- speculative
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to speed up large language model (LLM)
  decoding. It observes that auto-regressing at the feature level is easier than at
  the token level, and that incorporating the sampling result of the previous time
  step helps resolve the inherent uncertainty in feature-level auto-regression.
---

# EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty

## Quick Facts
- arXiv ID: 2401.15077
- Source URL: https://arxiv.org/abs/2401.15077
- Authors: Yuhui Li; Fangyun Wei; Chao Zhang; Hongyang Zhang
- Reference count: 9
- Primary result: 2.7x-3.5x speedup with doubled throughput for LLaMA2-Chat 70B while maintaining output distribution

## Executive Summary
This paper introduces EAGLE, a speculative sampling framework that accelerates large language model inference by operating at the feature level rather than the token level. The key insight is that auto-regressing at the second-to-top-layer features is more straightforward than at the token level, and incorporating the sampled token from one time step ahead resolves the inherent uncertainty in feature-level auto-regression. EAGLE achieves significant speedups while provably maintaining the original model's output distribution through a verification mechanism.

## Method Summary
EAGLE is a speculative sampling framework that auto-regresses at the feature level (second-to-top-layer) of LLMs while incorporating the token sequence advanced by one time step. The draft model consists of a trainable FC layer and decoder layer that predict next features from current features and shifted tokens. Training uses regression loss for feature prediction and classification loss for token prediction. The verification phase ensures distribution preservation through min(1, pj+i/ˆpj+i) acceptance probability. EAGLE employs tree attention to explore multiple candidates per step, with experimental evaluation showing 2.7x-3.5x speedup on LLaMA2-Chat 70B models.

## Key Results
- Achieves 2.7x-3.5x speedup ratio on LLaMA2-Chat 70B
- Doubles throughput while maintaining output distribution
- Effective across greedy and non-greedy settings
- Robust to different training data approaches

## Why This Works (Mechanism)

### Mechanism 1: Feature-level auto-regression is easier
- Claim: Auto-regressing at the feature level is easier than at the token level
- Mechanism: High-level features evolve in a more structured and regular way compared to raw tokens
- Core assumption: Feature space is smoother and more predictable than discrete token space
- Evidence: "autoregression at the feature (second-to-top-layer) level is more straightforward than at the token level"
- Break condition: If feature space becomes too noisy or representation is not regular

### Mechanism 2: Token context resolves feature uncertainty
- Claim: Incorporating one-step-ahead tokens resolves inherent uncertainty in feature auto-regression
- Mechanism: Sampling introduces randomness at token level; knowing sampled token removes ambiguity in feature prediction
- Core assumption: Next feature depends on sampled token, so knowing it removes uncertainty
- Evidence: "inherent uncertainty in feature (second-top-layer) level autoregression constrains its performance"
- Break condition: If features become conditionally independent of sampled tokens

### Mechanism 3: Distribution preservation through verification
- Claim: EAGLE maintains output distribution of original LLM while accelerating inference
- Mechanism: Verification phase with min(1, pj+i/ˆpj+i) acceptance probability preserves original distribution
- Core assumption: Verification step with proper acceptance probability maintains distribution
- Evidence: "distribution of the generated text remains unchanged for both the greedy and non-greedy settings"
- Break condition: If acceptance probability calculation is modified or verification is skipped

## Foundational Learning

- **Speculative sampling and verification**: Understanding how speculative sampling works is crucial to understanding EAGLE's approach and guarantees
  - Why needed: To understand how EAGLE maintains distribution while accelerating
  - Quick check: What is the acceptance probability formula used in speculative sampling?

- **Feature extraction in transformers**: EAGLE operates on second-to-top-layer features
  - Why needed: To understand what features represent and why they're useful
  - Quick check: What layer's output does EAGLE use as its feature representation?

- **Auto-regressive decoding**: EAGLE modifies standard auto-regressive decoding
  - Why needed: To understand how EAGLE differs from standard approaches
  - Quick check: In standard auto-regressive decoding, what is generated at each step?

## Architecture Onboarding

- **Component map**: Target LLM -> Feature extraction -> EAGLE draft model (FC + decoder layer) -> Feature-to-token conversion -> Token sampling -> Verification phase
- **Critical path**: Extract features from target LLM → Feed features and shifted tokens to EAGLE draft model → Generate next feature prediction → Convert feature to token distribution → Sample token → Verification phase with target LLM
- **Design tradeoffs**: 
  - Features vs tokens: Features provide more regularity but introduce uncertainty without token context
  - Tree vs chain drafting: Tree drafting explores more candidates but requires more computation
  - Training data: Fixed dataset vs target LLM-generated data affects training cost and performance
- **Failure signatures**:
  - Low acceptance rate: Draft model is not accurate enough
  - Distribution shift: Verification mechanism not working properly
  - High latency: Draft model overhead exceeds benefits
- **First 3 experiments**:
  1. Measure speedup ratio on MT-bench with temperature=0
  2. Compare acceptance rate with and without shifted tokens
  3. Test feature vs token auto-regression accuracy

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the precise relationship between dimensionality of LLM features and ease of auto-regressive prediction compared to token-level prediction?
  - Basis: Paper observes feature-level auto-regression is easier but doesn't explain why
  - Why unresolved: No theoretical explanation provided
  - Resolution evidence: Mathematical analysis comparing complexity of feature and token auto-regression

- **Open Question 2**: How does choice of training data affect EAGLE's performance and what is optimal approach?
  - Basis: Paper mentions EAGLE is not sensitive to training data but doesn't explore impact
  - Why unresolved: No comparison of different training data types
  - Resolution evidence: Experiments comparing performance with different training data types

- **Open Question 3**: How does EAGLE performance vary with different model sizes and architectures?
  - Basis: Evaluation focuses on LLaMA2-Chat variants with limited architectural diversity
  - Why unresolved: No analysis of scalability or performance across diverse architectures
  - Resolution evidence: Comprehensive study testing EAGLE across wide range of model sizes and architectures

## Limitations

- Lacks ablation studies isolating contribution of each mechanism (feature-level vs token context incorporation)
- Theoretical analysis doesn't formally prove distribution preservation under all conditions
- Doesn't address performance on extremely large models beyond 70B parameters
- Limited testing across diverse model architectures beyond LLaMA2-Chat

## Confidence

- **Medium confidence**: Feature-level auto-regression is inherently easier than token-level (limited comparative evidence)
- **High confidence**: Incorporating one-step-ahead tokens resolves feature uncertainty (well-supported by example and empirical results)
- **High confidence**: EAGLE maintains output distribution through verification (correctly applies established mechanisms)
- **Medium confidence**: Specific speedup numbers (2.7x-3.5x) for tested configurations

## Next Checks

- **Validation Check 1**: Conduct ablation study comparing EAGLE variants: feature-only auto-regression, feature auto-regression with token context, and standard token-level speculative sampling
- **Validation Check 2**: Test EAGLE across broader range of model architectures with different attention mechanisms and tokenization strategies
- **Validation Check 3**: Perform detailed analysis of draft model's error patterns to identify specific failure modes and systematic biases