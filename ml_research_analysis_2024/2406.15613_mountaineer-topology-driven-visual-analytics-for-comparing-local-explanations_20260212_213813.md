---
ver: rpa2
title: 'MOUNTAINEER: Topology-Driven Visual Analytics for Comparing Local Explanations'
arxiv_id: '2406.15613'
source_url: https://arxiv.org/abs/2406.15613
tags:
- data
- feature
- methods
- mapper
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mountaineer, a visual analytics tool for
  comparing local explanations of black-box machine learning models. It uses topological
  data analysis to create uniform graph representations of explanations, enabling
  intuitive comparison across different methods.
---

# MOUNTAINEER: Topology-Driven Visual Analytics for Comparing Local Explanations

## Quick Facts
- arXiv ID: 2406.15613
- Source URL: https://arxiv.org/abs/2406.15613
- Authors: Parikshit Solunke; Vitoria Guardieiro; Joao Rulff; Peter Xenopoulos; Gromit Yeuk-Yin Chan; Brian Barr; Luis Gustavo Nonato; Claudio Silva
- Reference count: 40
- Primary result: A visual analytics tool that uses topological data analysis to create uniform graph representations of local ML explanations, enabling intuitive comparison across different methods

## Executive Summary
MOUNTAINEER introduces a visual analytics approach for comparing local explanations of black-box machine learning models using topological data analysis. The system transforms high-dimensional feature attributions into uniform graph structures via the Mapper algorithm, enabling intuitive comparison across different explanation methods. By linking these topological graphs to original data distributions and model predictions through interactive views, MOUNTAINEER helps ML practitioners identify regions of disagreement between explanation methods and understand model behavior. Expert feedback validates the tool's utility and design choices, demonstrating its effectiveness in providing deeper insights into explanation techniques.

## Method Summary
MOUNTAINEER employs the Mapper algorithm to transform local explanation attributions into topological graph representations, where nodes represent clusters of similar explanations and edges indicate overlap between clusters. The system automatically selects optimal Mapper parameters (resolution, gain, clustering threshold) using stability analysis and Hausdorff distance metrics. Bottleneck distance between persistence diagrams quantifies topological differences between explanation methods. Interactive visual views (Projection, Mapper, Distance Matrix, Data Distribution, Feature Attribution) link topological structures back to original data, enabling users to identify which feature values cause explanation method disagreements. The tool is implemented as a Python package with Jupyter notebook integration for accessibility.

## Key Results
- Successfully identified regions of disagreement between explanation methods in credit risk prediction and census income classification tasks
- Demonstrated that bottleneck distance effectively quantifies topological differences between explanation methods
- Validated through expert feedback that MOUNTAINEER provides deeper insights into explanation techniques and helps ML practitioners make well-founded conclusions about model behavior

## Why This Works (Mechanism)

### Mechanism 1
The topological graph representations provide a uniform structure that enables comparison across explanation methods with different scales and interpretations. Mapper algorithm transforms high-dimensional feature attributions into node-link graphs where nodes represent clusters of similar explanations and edges represent overlap between clusters. This creates a common visual structure regardless of the original explanation method's scale. The core assumption is that the topological structure of explanation spaces is meaningful and stable enough to reveal differences between explanation methods.

### Mechanism 2
The bottleneck distance metric effectively quantifies topological differences between explanation methods. Bottleneck distance measures the cost of optimal matching between persistence diagrams derived from mapper graphs, providing a scalar measure of topological similarity. The core assumption is that topological persistence diagrams capture the essential differences between explanation methods' behavior across the feature space.

### Mechanism 3
Linking topological graphs back to original data distributions reveals which feature values cause explanation method disagreements. Nodes in mapper graphs are colored by feature values and linked to data distribution views, allowing users to identify which feature ranges correspond to topological differences between methods. The core assumption is that feature value distributions at nodes are representative of the explanations' behavior for those regions of the feature space.

## Foundational Learning

- **Concept: Topological Data Analysis (Mapper algorithm)**
  - Why needed here: Understanding how high-dimensional explanation attributions are transformed into graph structures is fundamental to interpreting Mountaineer's output.
  - Quick check question: What are the three key parameters required by the Mapper algorithm, and how do they affect the resulting graph structure?

- **Concept: Persistence diagrams and bottleneck distance**
  - Why needed here: These topological concepts provide the mathematical foundation for comparing explanation methods quantitatively.
  - Quick check question: How does the bottleneck distance between two persistence diagrams relate to the similarity of the underlying topological structures?

- **Concept: Feature attribution interpretation in ML explanations**
  - Why needed here: Understanding what feature attributions represent is essential for interpreting why different explanation methods might disagree.
  - Quick check question: Why might two explanation methods that produce different numerical attributions actually be conveying similar information about feature importance?

## Architecture Onboarding

- **Component map:** Mapper parameter selection (Delta, Bootstrap resolution, gain=0.4) -> Mapper graph construction using GUDHI library -> Graph summarization (Jaccard similarity threshold 0.9) -> Visual views (Projection, Mapper, Distance Matrix, Data Table, Distribution, Feature Attribution) -> Jupyter notebook integration via NotebookJS

- **Critical path:** Input: dataset, model predictions, explanation attributions -> Parameter selection (Delta threshold via Hausdorff distance, resolution via bootstrap stability) -> Mapper graph construction for each explanation method -> Graph summarization to reduce redundancy -> Visual rendering in Jupyter notebook -> Interactive exploration through linked views

- **Design tradeoffs:** Graph summarization reduces visual clutter but may hide fine-grained differences; Fixed gain parameter (0.4) provides consistency but may not be optimal for all datasets; Jupyter notebook integration maximizes accessibility but limits deployment options

- **Failure signatures:** Very dense mapper graphs indicate parameter selection issues (too low resolution or too high gain); Empty or near-empty mapper graphs suggest feature attribution vectors are too similar across observations; Inconsistent distance matrix values across runs may indicate numerical instability

- **First 3 experiments:** 
  1. Parameter sensitivity test: Run mapper with varying resolution parameters (5, 10, 20, 50) on a simple binary classification dataset and observe graph structure changes
  2. Comparison sanity check: Use two identical explanation methods and verify that bottleneck distance is near zero
  3. Integration test: Create a minimal Jupyter notebook with synthetic data and two explanation methods to verify all views render correctly and interactions work

## Open Questions the Paper Calls Out

### Open Question 1
Can the parameter selection strategy for Mapper be further optimized to reduce computational time, especially for larger datasets? The paper mentions that parameter selection is computationally costly and the step with the longest runtime in the framework. It also notes that runtime scales with the number of samples, making the tool impractical for larger datasets. The paper only presents a modified parameter selection strategy that significantly decreases runtime compared to GALE but doesn't explore further optimizations or potential trade-offs in accuracy.

### Open Question 2
How does the choice of lens function affect the utility and interpretability of the Mapper graphs in analyzing ML explanations? The paper mentions that the utility of generated topologies heavily depends on the choice of lens function. It uses predicted probabilities as the lens function for ML explanations but doesn't explore the impact of other potential lens functions. The paper only uses one lens function (predicted probabilities) and doesn't discuss or test the effects of alternative choices on the resulting Mapper graphs and insights gained.

### Open Question 3
How can Mountaineer be extended to support multi-class classification problems effectively? The paper mentions that Mountaineer can be employed for regression by using normalized model predictions as the lens function but doesn't discuss how to extend it to multi-class classification. The paper only demonstrates Mountaineer's use for binary classification and doesn't explore the challenges or potential solutions for adapting it to multi-class scenarios.

## Limitations
- Mapper parameter selection can be computationally expensive, especially for larger datasets, potentially limiting practical applicability
- The bottleneck distance metric may not capture all aspects of explanation quality or semantic similarity between methods
- Visual encoding choices could potentially bias interpretation of topological differences, though informed by expert feedback

## Confidence

- **High confidence:** The topological graph representation provides a uniform structure for comparison across explanation methods
- **Medium confidence:** Bottleneck distance effectively quantifies topological differences between explanation methods
- **Medium confidence:** Linking topological graphs to original data distributions reveals meaningful insights about explanation disagreements

## Next Checks

1. **Parameter Sensitivity Analysis:** Systematically vary Mapper parameters (resolution, gain, clustering threshold) across multiple datasets to assess the stability and robustness of the topological representations

2. **Alternative Distance Metrics:** Compare bottleneck distance results with other topological distance measures (Wasserstein distance, persistence landscapes) and correlation-based metrics to validate the choice of comparison metric

3. **Expert Validation Study:** Conduct a formal user study with ML practitioners using Mountaineer on real-world problems, measuring both the accuracy of insights derived and the time required compared to traditional explanation comparison methods