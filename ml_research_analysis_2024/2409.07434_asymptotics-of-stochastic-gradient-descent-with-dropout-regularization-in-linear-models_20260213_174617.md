---
ver: rpa2
title: Asymptotics of Stochastic Gradient Descent with Dropout Regularization in Linear
  Models
arxiv_id: '2409.07434'
source_url: https://arxiv.org/abs/2409.07434
tags:
- dropout
- matrix
- lemma
- theorem
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops an asymptotic theory for stochastic gradient
  descent (SGD) with dropout regularization in linear models. The authors establish
  geometric-moment contraction (GMC) for constant step-size SGD dropout iterates,
  proving the existence of a unique stationary distribution.
---

# Asymptotics of Stochastic Gradient Descent with Dropout Regularization in Linear Models

## Quick Facts
- arXiv ID: 2409.07434
- Source URL: https://arxiv.org/abs/2409.07434
- Authors: Jiaqi Li; Johannes Schmidt-Hieber; Wei Biao Wu
- Reference count: 40
- Primary result: Asymptotic theory for SGD with dropout regularization in linear models with online inference

## Executive Summary
This paper develops an asymptotic theory for stochastic gradient descent (SGD) with dropout regularization in linear models. The authors establish geometric-moment contraction (GMC) for constant step-size SGD dropout iterates, proving the existence of a unique stationary distribution. Leveraging this property, they provide quenched central limit theorems (CLT) for the difference between dropout and ℓ²-regularized iterates, enabling statistical inference. The CLT is extended to Ruppert-Polyak averaged SGD (ASGD) with dropout. An online estimator for the long-run covariance matrix of ASGD dropout is introduced, facilitating efficient recursive inference with computational and memory benefits. Numerical experiments demonstrate that the proposed confidence intervals achieve nearly nominal coverage probability for sufficiently large samples.

## Method Summary
The paper studies SGD with dropout regularization for linear regression using constant learning rates. The iterative update rule is β_k = β_{k-1} + αD_k x_k (y_k - x_k^⊤D_k β_{k-1}), where D_k is a dropout matrix with i.i.d. Bernoulli(p) entries. The method establishes geometric-moment contraction to prove a unique stationary distribution, then derives quenched CLTs for both iterative and averaged SGD. An online covariance estimator using non-overlapping batched means (NBM) is proposed for efficient inference. The approach works with both fixed design (y = Xβ* + ε) and random design (y_k = x_k^⊤β* + ε_k) settings.

## Key Results
- Geometric-moment contraction (GMC) ensures existence of unique stationary distribution for constant step-size SGD dropout iterates
- Quenched central limit theorems (CLT) provide asymptotic normality for difference between dropout and ℓ²-regularized iterates
- Online estimator for long-run covariance matrix enables efficient recursive inference with computational and memory benefits
- Proposed confidence intervals for ASGD with dropout achieve nearly nominal coverage probability for sufficiently large samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Geometric-moment contraction (GMC) ensures the existence of a unique stationary distribution for constant step-size SGD dropout iterates.
- Mechanism: The GMC property guarantees that the sequence of iterates converges exponentially fast to a stationary distribution, regardless of initialization.
- Core assumption: The learning rate α satisfies α∥X∥ < 2 and q > 1 for the q-th moment.
- Evidence anchors:
  - [abstract] "we establish the geometric-moment contraction (GMC) for constant step-size SGD dropout iterates to show the existence of a unique stationary distribution of the dropout recursive function."
  - [section 3.1] "Theorem 1 (Geometric-moment contraction of GD dropout)... There exists a unique stationary distribution πα which does not depend on the initialization ˜β0..."
  - [corpus] No direct corpus evidence found for GMC in SGD dropout; weak evidence from related works on convergence properties of SGD.
- Break condition: If the learning rate α does not satisfy α∥X∥ < 2, the GMC property may not hold, leading to non-convergence or non-stationarity.

### Mechanism 2
- Claim: Quenched central limit theorems (CLT) provide asymptotic normality for the difference between dropout and ℓ²-regularized iterates.
- Mechanism: The GMC property is leveraged to show that the difference between dropout iterates and ℓ²-regularized iterates converges to a normal distribution, enabling statistical inference.
- Core assumption: The learning rate α satisfies α ∈ (0, 2/∥X∥) and the design matrix X has no zero columns.
- Evidence anchors:
  - [abstract] "By the GMC property, we provide quenched central limit theorems (CLT) for the difference between dropout and ℓ²-regularized iterates, regardless of initialization."
  - [section 3.2] "Theorem 2 (Quenched CLT of iterative GD dropout)... For any k ∈ N, we have ˜βk(α) − ˜β√α ⇒ N (0, Ξ(0)), as α → 0..."
  - [corpus] Weak evidence from related works on asymptotic normality of SGD iterates; no direct evidence for quenched CLT in dropout settings.
- Break condition: If the design matrix X has zero columns or the learning rate α is outside the specified range, the CLT may not hold.

### Mechanism 3
- Claim: Online estimation of the long-run covariance matrix enables efficient recursive inference with computational and memory benefits.
- Mechanism: The non-overlapping batched means (NBM) method is extended to online versions by only including past SGD dropout iterates in each batch, reducing computational and memory requirements.
- Core assumption: The learning rate α satisfies the conditions in Theorem 5 and the sequence {ηm}m∈N is strictly increasing with ηm+1 − ηm → ∞ as m → ∞.
- Evidence anchors:
  - [abstract] "we further introduce an online estimator for the long-run covariance matrix of ASGD dropout to facilitate inference in a recursive manner with efficiency in computational time and memory."
  - [section 5] "Let η1, η2, ... be a strictly increasing integer-valued sequence satisfying η1 = 1 and ηm+1 − ηm → ∞ as m → ∞. For each m, we let Bm denote the block Bm = {ηm, ηm + 1, ..., ηm+1 − 1}."
  - [corpus] Weak evidence from related works on online estimation of covariance matrices; no direct evidence for NBM in dropout settings.
- Break condition: If the block sizes are not large enough or the learning rate α does not satisfy the conditions in Theorem 5, the online estimator may not converge to the true long-run covariance matrix.

## Foundational Learning

- Concept: Geometric-moment contraction (GMC)
  - Why needed here: GMC is crucial for establishing the existence of a unique stationary distribution for SGD dropout iterates, which is a prerequisite for asymptotic normality and statistical inference.
  - Quick check question: What is the condition on the learning rate α for GMC to hold in SGD dropout iterates?
- Concept: Quenched central limit theorems (CLT)
  - Why needed here: Quenched CLT provides asymptotic normality for the difference between dropout and ℓ²-regularized iterates, enabling statistical inference and confidence interval construction.
  - Quick check question: What is the role of the GMC property in proving quenched CLT for SGD dropout iterates?
- Concept: Online estimation of long-run covariance matrix
  - Why needed here: Online estimation of the long-run covariance matrix is essential for efficient recursive inference with computational and memory benefits, as it avoids the need to store all past iterates.
  - Quick check question: How does the non-overlapping batched means (NBM) method extend to online versions in the context of SGD dropout iterates?

## Architecture Onboarding

- Component map: SGD dropout iterates -> Geometric-moment contraction (GMC) -> Quenched central limit theorems (CLT) -> Online estimation of long-run covariance matrix -> Statistical inference with confidence intervals
- Critical path:
  1. Verify that the learning rate α satisfies the conditions for GMC and CLT.
  2. Establish the existence of a unique stationary distribution for SGD dropout iterates using GMC.
  3. Prove asymptotic normality for the difference between dropout and ℓ²-regularized iterates using quenched CLT.
  4. Implement the online estimation method for the long-run covariance matrix.
  5. Validate the coverage probability of the constructed confidence intervals.
- Design tradeoffs:
  - Computational efficiency vs. accuracy: The online estimation method trades off some accuracy for computational efficiency by using non-overlapping batched means.
  - Memory requirements: The online estimation method reduces memory requirements by only storing past iterates in each batch.
- Failure signatures:
  - Non-convergence of SGD dropout iterates: If the learning rate α does not satisfy the conditions for GMC, the iterates may not converge to a stationary distribution.
  - Inaccurate confidence intervals: If the online estimator for the long-run covariance matrix does not converge to the true value, the constructed confidence intervals may have incorrect coverage probabilities.
- First 3 experiments:
  1. Verify the geometric-moment contraction property for SGD dropout iterates with different learning rates α and design matrices X.
  2. Test the asymptotic normality of the difference between dropout and ℓ²-regularized iterates using quenched CLT.
  3. Evaluate the convergence and accuracy of the online estimator for the long-run covariance matrix under different settings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal range of learning rates α for SGD dropout to ensure both geometric-moment contraction and efficient convergence?
- Basis in paper: [explicit] Lemma 4 provides sufficient conditions for α in SGD dropout, but these are not tight bounds. The paper shows that α must satisfy α(q − 1) / [2(1 + αµq(v))q−2µq(v)2] < 1 for some q ≥ 2.
- Why unresolved: The sufficient conditions are not tight and may exclude valid learning rates. Numerical experiments (Table 1) suggest that the theoretical bounds might be conservative.
- What evidence would resolve it: Tight lower and upper bounds on α that guarantee geometric-moment contraction and convergence for a wide range of problem instances, validated through extensive numerical experiments.

### Open Question 2
- Question: How does the performance of online inference for SGD dropout compare to offline methods in terms of computational efficiency and statistical accuracy?
- Basis in paper: [explicit] The paper proposes an online estimator for the long-run covariance matrix and claims computational and memory benefits. However, no direct comparison with offline methods is provided.
- Why unresolved: The paper only provides theoretical guarantees and simulation results for the online estimator. No empirical comparison with established offline methods is given.
- What evidence would resolve it: Benchmark studies comparing the online estimator to state-of-the-art offline methods in terms of computational time, memory usage, and statistical accuracy (e.g., coverage probability) on real-world datasets.

### Open Question 3
- Question: Can the asymptotic normality results for SGD dropout be extended to non-linear models or deep neural networks?
- Basis in paper: [inferred] The paper focuses on linear models and assumes a fixed design matrix. The techniques used rely on the linearity of the model and may not directly apply to non-linear models.
- Why unresolved: The paper's theoretical framework is built upon the specific structure of linear models. Extending the results to non-linear models would require new techniques to handle the non-linearity and potential non-convexity.
- What evidence would resolve it: Extension of the geometric-moment contraction and quenched central limit theorems to non-linear models or deep neural networks, with rigorous proofs and empirical validation.

## Limitations
- Results rely on geometric-moment contraction requiring learning rate α to satisfy α∥X∥ < 2, potentially restricting practical applicability
- Asymptotic theory assumes design matrix X has no zero columns, which may not hold in real-world data
- Online covariance estimation introduces approximation errors that may affect coverage probability for finite samples

## Confidence
- **High confidence**: The existence of a unique stationary distribution for constant step-size SGD dropout iterates (Theorem 1) is well-supported by the GMC property and rigorous proof techniques.
- **Medium confidence**: The quenched CLT results (Theorems 2-4) are theoretically sound but rely on conditions that may be difficult to verify in practice, particularly the non-zero column assumption for X.
- **Medium confidence**: The online estimator for the long-run covariance matrix shows promising theoretical properties but empirical validation is limited to simulation studies with synthetic data.

## Next Checks
1. **Practical learning rate constraints**: Investigate the sensitivity of the GMC property to different learning rate schedules beyond the constant step-size assumption, and test the coverage probability of confidence intervals for α values approaching the theoretical bounds.
2. **Robustness to design matrix structure**: Validate the quenched CLT results when the design matrix X contains near-zero or exactly zero columns, which commonly occur in high-dimensional settings, and assess the impact on statistical inference.
3. **Real-world data performance**: Apply the proposed methodology to real-world datasets with structured features and evaluate the online covariance estimation accuracy compared to batch methods, particularly focusing on computational efficiency gains and memory savings.