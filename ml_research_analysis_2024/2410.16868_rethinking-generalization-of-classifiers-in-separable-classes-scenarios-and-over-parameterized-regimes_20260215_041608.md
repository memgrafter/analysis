---
ver: rpa2
title: Rethinking generalization of classifiers in separable classes scenarios and
  over-parameterized regimes
arxiv_id: '2410.16868'
source_url: https://arxiv.org/abs/2410.16868
tags:
- training
- error
- learning
- generalization
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines generalization in classifiers when classes
  are separable or the classifier is over-parameterized, both cases resulting in zero
  training error. The authors show that the proportion of "bad" global minima (those
  with high generalization error) decreases exponentially with the number of training
  samples n, even without additional regularization.
---

# Rethinking generalization of classifiers in separable classes scenarios and over-parameterized regimes

## Quick Facts
- arXiv ID: 2410.16868
- Source URL: https://arxiv.org/abs/2410.16868
- Reference count: 40
- This paper shows that the proportion of "bad" global minima decreases exponentially with training data size, even without regularization, and that over-parameterized networks can achieve better generalization with less data.

## Executive Summary
This paper examines generalization in classifiers when classes are separable or the classifier is over-parameterized, both cases resulting in zero training error. The authors show that the proportion of "bad" global minima (those with high generalization error) decreases exponentially with the number of training samples n, even without additional regularization. They derive bounds based solely on the density distribution of the true error, independent of the function set's size or complexity.

## Method Summary
The study trains neural networks (ResNet and MLP architectures) on MNIST and CIFAR-10 datasets until zero training error is reached. For each sample size n, 50 networks are trained and their test errors measured. The LAMB optimizer with cross-entropy loss and learning rate decay is used. Data is normalized without augmentation, and training samples are selected with equal representation per class. Networks are trained until zero training error is achieved, then evaluated on the full test set to measure generalization.

## Key Results
- The proportion of "bad" global minima diminishes exponentially with the number of training data n
- Expected generalization error decreases with 1/n for large n, converging to the minimal achievable error
- Larger networks (ResNet101 vs ResNet018) can achieve better generalization with less data in over-parameterized regime
- The structure of the classifier function set, rather than its size, determines generalization performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The proportion of "bad" global minima decreases exponentially with training data size n.
- **Mechanism**: For separable classes or over-parameterized regimes, the density of classifiers with true error ≥ ε shrinks as (1-ε)^n when sampling n training points.
- **Core assumption**: The function set H is discrete/finite and contains at least one perfect classifier (H0).
- **Evidence anchors**:
  - [abstract] "We show that in separable classes scenarios the proportion of 'bad' global minima diminishes exponentially with the number of training data n."
  - [section 2] Derives bound ⟨Frac{E(h) ≥ ε|S}⟩ ≤ |Hε|/|H0| · (1-ε)^n ≤ R·e^(-εn)
  - [corpus] Related paper on "Interpretable global minima of deep ReLU neural networks on sequentially separable data" supports existence of structured minima.
- **Break condition**: If H lacks a perfect classifier (H0 = ∅), the exponential decay no longer applies; bound becomes vacuous.

### Mechanism 2
- **Claim**: Learning curves depend on the shape of the density of classifiers (DOC), not the total size of H.
- **Mechanism**: True error averaged over global minima follows ⟨E⟩ = ∫E·Qn(E)dE, where Qn(E) is normalized DOC after n samples. Shape of D(E) controls convergence.
- **Core assumption**: D(E) is well-behaved (monotonic, bounded) and approximation step (9) holds.
- **Evidence anchors**:
  - [abstract] "Our analysis provides bounds and learning curves dependent solely on the density distribution of the true error for the given classifier function set."
  - [section 3] Models D(E) = (E-Emin)^(α-1)(Emax-E)^(β-1) and derives ⟨E_n⟩ = Emin + (E0-Emin)/(1+n/η)
  - [corpus] Weak; no direct corpus match for DOC shape argument, labeled as gap.
- **Break condition**: If D(E) is highly irregular or has multiple sharp peaks, integral approximation fails and learning curve breaks.

### Mechanism 3
- **Claim**: Over-parameterization can improve generalization by reducing η, the convergence scaling factor.
- **Mechanism**: Larger networks can have smaller Emin and smaller η, so error drops faster with n. Experiments show ResNet101 beats ResNet018 on MNIST despite 4× parameters.
- **Core assumption**: Increasing network capacity preserves DOC shape or improves it (smaller Emin, smaller η).
- **Evidence anchors**:
  - [section 3.2] Table 1 shows ResNet101: Emin=0.003, η=60 vs ResNet018: Emin=0.0035, η=65
  - [abstract] "This observation sheds light on the unexpectedly good generalization of over-parameterized Neural Networks."
  - [corpus] Related paper "Can overfitted deep neural networks in adversarial training generalize?" aligns with over-parameterization generalization debate.
- **Break condition**: If larger networks increase η or leave Emin unchanged, the benefit disappears; MLP experiments show this failure mode.

## Foundational Learning

- **Concept**: VC-dimension and uniform convergence bounds
  - **Why needed here**: Provides baseline contrast—shows why classical bounds fail in over-parameterized regimes (need n >> VC-dimension).
  - **Quick check question**: For a linear classifier in 2D (d=3), how many samples does VC bound require for ε=0.1? (Answer: n ≥ 410)

- **Concept**: Empirical Risk Minimization (ERM) and global minima
  - **Why needed here**: Core to the paper—ERM finds zero training error solutions; analysis focuses on their generalization.
  - **Quick check question**: If training error is zero, what is the relationship between H(S) and H0? (Answer: H0 ⊆ H(S))

- **Concept**: Density of States (DOS) analogy from statistical physics
  - **Why needed here**: Framework for understanding D(E) and Qn(E) as normalized distributions; connects to physical intuition.
  - **Quick check question**: In the low-temperature limit (large n), which energy states dominate? (Answer: Ground states with minimal error Emin)

## Architecture Onboarding

- **Component map**: Input preprocessing → CNN backbone (ResNet variants) → Classification head → Cross-entropy loss → LAMB optimizer
- **Critical path**: 1. Normalize images (channel-wise mean/std) 2. Forward pass through adjusted ResNet 3. Compute cross-entropy loss 4. LAMB update (no weight decay) 5. Verify zero training error after training 6. Evaluate on full test set
- **Design tradeoffs**: Small batch sizes for small n (to fit memory) vs large batches for speed; Fixed learning rate schedule vs adaptive decay; Depth vs width: ResNet101 converges faster but costs more compute
- **Failure signatures**: Non-zero training error → model capacity insufficient or optimization failed; Test error plateaus early → DOC shape unfavorable (large η) or poor initialization; High variance across runs → insufficient samples per class or noisy data
- **First 3 experiments**: 1. MNIST with ResNet018, n=10, verify zero training error and test error ≈ 0.35 2. CIFAR-10 with MLP3, n=100, confirm larger Emin (~0.36) than ResNet 3. MNIST with ResNet101, n=100, check faster convergence (smaller η) than ResNet018

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the structure of the classifier function set influence the shape of the density of classifiers (DOC) distribution?
- Basis in paper: [explicit] The paper states "The DOC depends on both the type of classification problem and the set of classifier functions used to solve the problem" and suggests this is an "intriguing avenue for future research"
- Why unresolved: While the paper shows that DOC shape determines learning curves, it doesn't provide a theoretical framework for how different classifier architectures would generate different DOC shapes
- What evidence would resolve it: A systematic study comparing DOC distributions across different network architectures (CNNs, MLPs, ResNets) and analyzing how architectural choices (depth, width, activation functions) affect the DOC shape

### Open Question 2
- Question: Are there cases where the approximation step (9) fails for small n values, and what are the conditions for its validity?
- Basis in paper: [explicit] The paper mentions "approximation step (9), which one might expect to be less accurate for small n" and discusses conditions for its validity in the Appendix
- Why unresolved: The paper uses this approximation but doesn't fully characterize when it breaks down or provide alternative approaches for small n
- What evidence would resolve it: Experimental validation showing the error bounds of this approximation across different n values and classification problems, plus development of more accurate approximations for small n

### Open Question 3
- Question: Can the model of D(E) be extended to account for non-constant intrinsic dimensionalities α and β that depend on E?
- Basis in paper: [explicit] The paper states "the real DOCs may deviate from our model, e.g., the intrinsic dimensionalities α and β may not be constant and depend on E or even be fractal"
- Why unresolved: The current model assumes constant α and β, but the authors acknowledge this may not reflect reality and suggest more complex models
- What evidence would resolve it: Empirical measurement of α and β values across different E ranges for various classification problems, and development of a more flexible DOC model that captures this variability

### Open Question 4
- Question: How does the relationship between network size and convergence speed (η) vary across different types of architectures and datasets?
- Basis in paper: [explicit] The paper observes that "Increasing network size not only led to smaller errors, but also faster learning (with less data)" for ResNets but "this trend does not hold in our experiments with fully connected MLPs"
- Why unresolved: The paper presents contradictory findings for different architectures but doesn't provide a theoretical explanation for when and why this relationship holds
- What evidence would resolve it: A comprehensive study comparing convergence speeds across multiple architectures (CNNs, ResNets, MLPs, transformers) and datasets to identify patterns in how network size affects η

## Limitations
- The assumption of a finite/discrete classifier set H may not hold for continuous DNN parameter spaces
- The derivation of the exponential decay bound (1-ε)^n relies on uniform sampling of training points
- The DOC shape model D(E) = (E-Emin)^(α-1)(Emax-E)^(β-1) is heuristic
- Results are specific to tested architectures (ResNet, MLP) and datasets (MNIST, CIFAR-10)

## Confidence
- High Confidence: The exponential decay of "bad" global minima proportion with n (Mechanism 1)
- Medium Confidence: The DOC shape determines learning curves independent of H's size (Mechanism 2)
- Medium Confidence: Over-parameterization can improve generalization by reducing η (Mechanism 3)

## Next Checks
1. Test the exponential decay bound with continuous function sets using discretized parameter spaces of varying granularity
2. Validate the DOC shape model across diverse classifier families (SVMs, decision trees) beyond neural networks
3. Experimentally verify η reduction in over-parameterized regimes for other architectural variations (depth-width tradeoffs, skip connections)