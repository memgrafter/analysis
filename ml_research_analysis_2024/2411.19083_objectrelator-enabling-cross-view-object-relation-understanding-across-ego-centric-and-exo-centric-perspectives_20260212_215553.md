---
ver: rpa2
title: 'ObjectRelator: Enabling Cross-View Object Relation Understanding Across Ego-Centric
  and Exo-Centric Perspectives'
arxiv_id: '2411.19083'
source_url: https://arxiv.org/abs/2411.19083
tags:
- object
- psalm
- image
- visual
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenging ego-exo object correspondence
  task, which aims to segment corresponding objects across drastically different ego-centric
  and exo-centric views. The authors propose ObjectRelator, a novel method that enhances
  the baseline PSALM model with two key innovations: Multimodal Condition Fusion (MCFuse)
  and SSL-based Cross-View Object Alignment (XObjAlign).'
---

# ObjectRelator: Enabling Cross-View Object Relation Understanding Across Ego-Centric and Exo-Centric Perspectives

## Quick Facts
- **arXiv ID:** 2411.19083
- **Source URL:** https://arxiv.org/abs/2411.19083
- **Reference count:** 40
- **Primary result:** Achieves IoU scores of 44.3% on Ego2Exo and 49.2% on Exo2Ego, outperforming state-of-the-art baselines on cross-view object correspondence

## Executive Summary
This paper addresses the challenging task of ego-exo object correspondence, where objects must be segmented across drastically different ego-centric and exo-centric views. The authors propose ObjectRelator, which enhances the baseline PSALM model with two key innovations: Multimodal Condition Fusion (MCFuse) for integrating visual and textual information, and SSL-based Cross-View Object Alignment (XObjAlign) for enforcing consistency across views. Extensive experiments on Ego-Exo4D and HANDAL-X datasets demonstrate significant improvements over baselines, achieving state-of-the-art performance in cross-view object segmentation.

## Method Summary
ObjectRelator builds upon the PSALM framework with two novel modules. MCFuse introduces language as an additional cue, integrating both visual masks and textual descriptions through cross-attention with a learnable residual connection to improve object localization. XObjAlign enforces cross-view consistency by minimizing the Euclidean distance between ego and exo object embeddings in a shared latent space. The model employs a two-stage training strategy: first training MCFuse with segmentation loss, then jointly optimizing all modules with both segmentation and consistency losses.

## Key Results
- Achieves IoU scores of 44.3% on Ego2Exo and 49.2% on Exo2Ego tasks
- Significant improvements over baseline PSALM model on Ego-Exo4D benchmark
- Introduces HANDAL-X as a new testbed for cross-view segmentation
- MCFuse and XObjAlign modules contribute 4.1% and 2.7% IoU improvement respectively

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Condition Fusion (MCFuse)
MCFuse improves object localization by fusing visual mask and textual descriptions through cross-attention with a residual connection. The ego text embedding and ego visual embedding are processed via cross-attention, with text as query and visual as key/value. The fused result is combined with the original visual embedding via a learnable residual connection. This allows the model to leverage both modalities while prioritizing visual cues.

### Mechanism 2: SSL-based Cross-View Object Alignment (XObjAlign)
XObjAlign enforces cross-view consistency by minimizing the Euclidean distance between ego and exo object embeddings in a shared latent space. After extracting ego visual embedding and exo visual embedding from the LLM, the Euclidean distance between these embeddings is computed as a consistency loss. This encourages the model to generate similar representations for the same object across views.

### Mechanism 3: Two-Stage Training Strategy
The two-stage training strategy allows MCFuse to be properly initialized before joint optimization with other modules. In stage S1, only MCFuse is trained using a subset of data. In stage S2, all modules are jointly trained. This prevents the new parameters in MCFuse from being overwhelmed by updates to the pre-trained LLM and other modules.

## Foundational Learning

- **Concept:** Multimodal fusion through cross-attention
  - Why needed here: Ego-exo object correspondence requires integrating information from different modalities (visual masks and text descriptions) to accurately locate objects across drastically different views.
  - Quick check question: How does cross-attention differ from simple concatenation when fusing visual and text embeddings?

- **Concept:** Self-supervised learning for cross-view alignment
  - Why needed here: The task requires establishing correspondences between objects in ego and exo views without explicit paired supervision for each view, necessitating a self-supervised consistency constraint.
  - Quick check question: What would happen if we used a contrastive loss instead of Euclidean distance for XObjAlign?

- **Concept:** Residual connections with learnable weights
  - Why needed here: When introducing new modalities, we need to balance between the original reliable visual information and the potentially helpful but uncertain textual information.
  - Quick check question: Why might a fixed ratio (e.g., 0.5) be less effective than a learnable weight for the residual connection?

## Architecture Onboarding

- **Component map:** Visual Encoder (Swin-B) → MM Projector → LLM (Phi-1.5) → MCFuse → Mask Generator → Output
- **Critical path:** Visual Encoder → MM Projector → LLM → MCFuse → Mask Generator → Output
  - The LLM serves as the central processing hub, with embeddings flowing through MCFuse before reaching the Mask Generator
- **Design tradeoffs:** Using a powerful LLM enables rich multimodal understanding but increases computational cost and memory requirements; the two-stage training adds complexity but improves stability when adding new components to pre-trained models; Euclidean distance for alignment is simple and effective but may not capture semantic similarity as well as learned metrics
- **Failure signatures:** Poor performance on Ego2Exo vs Exo2Ego suggests issues with ego-view queries (smaller, more complex backgrounds); high IoU but poor LE/CA indicates accurate object identification but imprecise localization; degradation when removing MCFuse shows over-reliance on textual information
- **First 3 experiments:** 1) Baseline validation: Run PSALM on Ego-Exo4D without any modifications to establish baseline performance and confirm the reported numbers; 2) MCFuse ablation: Implement MCFuse and test with only visual conditions (no text) to verify it doesn't degrade performance and understand its contribution; 3) XObjAlign ablation: Test the model with only MCFuse (no XObjAlign) to isolate the contribution of cross-view consistency enforcement

## Open Questions the Paper Calls Out
1. What is the impact of temporal information on cross-view object correspondence performance? The paper mentions "leaving the exploration of temporal information as future work" and evaluates a memory-based approach for first-frame queries.
2. How does ObjectRelator generalize to other cross-view scenarios beyond ego-exo perspectives? The authors mention ObjectRelator is validated on ego-exo perspectives and HANDAL-X, and note that generalization to new cross-view settings is strong.
3. What is the optimal balance between visual and textual prompts for cross-view object correspondence? The paper introduces MCFuse to combine visual and textual prompts, showing improvements, but notes that performance with only visual conditions is slightly lower than with both modalities.

## Limitations
- The effectiveness of MCFuse heavily depends on the quality of automatically generated text descriptions from LLaVA, which may not generalize well to objects with complex or ambiguous appearances
- The XObjAlign mechanism assumes that objects across views can be mapped to similar latent representations, but this may break down for extreme viewpoint changes or when object appearance is significantly altered by perspective differences
- The two-stage training strategy, while empirically effective, lacks rigorous ablation studies to justify the specific training schedule and subset selection

## Confidence
- **High confidence:** The core architecture design and overall performance improvements are well-supported by experimental results. The reported IoU scores (44.3% on Ego2Exo, 49.2% on Exo2Ego) represent clear improvements over baselines.
- **Medium confidence:** The specific contributions of MCFuse and XObjAlign modules are demonstrated through ablations, but the exact mechanisms by which they improve performance could be better understood through additional analysis.
- **Low confidence:** The generalizability of the approach to other cross-view scenarios beyond ego-exo correspondence, particularly with different degrees of viewpoint variation.

## Next Checks
1. **Robustness testing:** Evaluate ObjectRelator on a more diverse set of viewpoint variations, including synthetic rotations and occlusions, to assess the limits of cross-view alignment effectiveness.
2. **Text quality impact study:** Systematically vary the quality and informativeness of input text descriptions to quantify the sensitivity of MCFuse performance to description accuracy.
3. **Cross-dataset generalization:** Test the pretrained ObjectRelator model on other cross-view datasets (e.g., natural scene matching datasets) without fine-tuning to evaluate true generalization capabilities.