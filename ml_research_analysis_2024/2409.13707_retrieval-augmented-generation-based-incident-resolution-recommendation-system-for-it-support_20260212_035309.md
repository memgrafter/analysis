---
ver: rpa2
title: Retrieval Augmented Generation-Based Incident Resolution Recommendation System
  for IT Support
arxiv_id: '2409.13707'
source_url: https://arxiv.org/abs/2409.13707
tags:
- support
- case
- retrieval
- system
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a retrieval-augmented generation (RAG) system
  for IT support case solution recommendation, addressing two key challenges: domain
  coverage and model size constraints. The system combines an encoder-only classifier,
  dense embedding models for retrieval, and generative large language models for query
  and answer generation.'
---

# Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support

## Quick Facts
- arXiv ID: 2409.13707
- Source URL: https://arxiv.org/abs/2409.13707
- Reference count: 8
- Smaller models with RAG can match or outperform larger models in IT support answer generation

## Executive Summary
This paper presents a retrieval-augmented generation (RAG) system for IT support case solution recommendation that addresses two key challenges: domain coverage and model size constraints. The system combines an encoder-only classifier, dense embedding models for retrieval, and generative large language models for query and answer generation. Using real-world IT support cases and documentation, the authors demonstrate that smaller models leveraging retrieved context can match or outperform larger models in answer generation. Key results include 43% recall@3 for retrieval, with smaller models like Mixtral-8x7B-Instruct and Granite-13B-Chat-v2 outperforming GPT-4o in answer generation despite having significantly fewer parameters.

## Method Summary
The system architecture consists of four main components: an encoder-only classifier that identifies single-turn vs multi-turn cases, a query generation model that creates search queries from case descriptions, a dense embedding retriever with re-ranking to find relevant documentation, and an answer generation model that produces solutions using the retrieved context. The approach leverages IBM's Granite and Mixtral models fine-tuned on IT domain data, with retrieval performance enhanced through a Slate-30M embedding model and Slate-125M re-ranker. The system processes IT support cases by first classifying them, generating appropriate queries, retrieving relevant documents from a corpus of over 5 million support documents, and finally generating answers using the retrieved context.

## Key Results
- 43% recall@3 for retrieval, showing significant room for improvement in document retrieval
- Smaller models (Mixtral-8x7B-Instruct, Granite-13B-Chat-v2) outperform GPT-4o in answer generation quality despite fewer parameters
- Human evaluation shows the tool's recommendations rated as equally or more helpful than subject matter experts for 19 test cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller models with RAG can match or outperform much larger models in IT support answer generation.
- Mechanism: The retrieval component provides domain-specific context that compensates for the smaller model's lack of general knowledge, allowing it to generate accurate answers despite having fewer parameters.
- Core assumption: The retrieved documents contain sufficient and relevant information to answer the support queries.
- Evidence anchors:
  - [abstract] "smaller models like Mixtral-8x7B-Instruct and Granite-13B-Chat-v2 outperforming GPT-4o in answer generation despite having significantly fewer parameters"
  - [section] "when given the correct context, LLMs can typically generate responses that match the ground truth answers"
- Break condition: If retrieval consistently fails to find relevant documents, the smaller models will underperform due to lack of appropriate context.

### Mechanism 2
- Claim: Re-ranking retrieved documents significantly improves retrieval performance.
- Mechanism: The initial retrieval using a general-purpose embedding is refined by a task-specific fine-tuned re-ranker that better matches the query-document relevance in the IT support domain.
- Core assumption: The re-ranker model has been adequately trained on IT support specific data to improve relevance scoring.
- Evidence anchors:
  - [section] "Using training data based on 1,430 questions with up to three matching passages per question... The resulting IBM Slate-125M model was then distilled into the deployed IBM Slate-30M model"
  - [section] "recall before and after re-ranking with Google Search as a baseline in Figure 2" (showing improvement)
- Break condition: If the re-ranker model is poorly trained or the domain shifts significantly, the re-ranking may not provide meaningful improvement over initial retrieval.

### Mechanism 3
- Claim: Knowledge infusion through fine-tuning improves answer generation performance.
- Mechanism: Fine-tuning smaller models on domain-specific synthetic data created from seed examples and actual documents allows them to better understand IT support terminology and generate more accurate answers.
- Core assumption: The synthetic data generation process creates representative examples that capture the domain's question-answering patterns.
- Evidence anchors:
  - [section] "IBM's Granite 7B IL-Internal-Granite-7B-Base, a much smaller model, was fine-tuned with IT domain data to cater to the specific task of answer generation"
  - [section] "InstructLab-IT emerged as the best model" in human evaluation with "very noticeable improvement over both models"
- Break condition: If the synthetic data generation process produces low-quality or unrepresentative examples, the fine-tuned model may not generalize well to real support cases.

## Foundational Learning

- Concept: Dense vs. Sparse Retrieval
  - Why needed here: Understanding the difference between dense (embedding-based) and sparse (keyword-based like BM25) retrieval methods is crucial for appreciating why the system uses dense embeddings and re-ranking
  - Quick check question: What is the main advantage of dense retrieval over sparse retrieval when dealing with semantic similarity in IT support queries?

- Concept: Fine-tuning vs. In-Context Learning
  - Why needed here: The paper compares approaches where smaller models are either fine-tuned on domain data or use in-context learning through RAG, which affects model selection and deployment decisions
  - Quick check question: When would you choose to fine-tune a smaller model versus relying on RAG with a larger model?

- Concept: Inter-Annotator Agreement
  - Why needed here: The paper highlights challenges with labeling consistency among subject matter experts, which affects the quality of training and evaluation data
  - Quick check question: How might low inter-annotator agreement impact the reliability of model evaluation metrics in this IT support use case?

## Architecture Onboarding

- Component map: Encoder-only classifier → Query generator → Dense embedding retriever with re-ranking → Answer generator
- Critical path: Case → Classifier (single-turn check) → Query generator → Retriever (dense embedding + re-ranking) → Answer generator → Support agent
- Design tradeoffs: Model size vs. cost/privacy vs. performance; single ground truth link vs. list of acceptable links; synthetic data generation vs. manual annotation
- Failure signatures: Poor retrieval recall suggests embedding or re-ranking issues; incorrect classification suggests classifier threshold problems; poor answer quality despite good retrieval suggests generation model issues
- First 3 experiments:
  1. Test classifier on held-out single-turn vs multi-turn cases to verify threshold selection
  2. Compare retrieval recall@3 with and without re-ranking on validation set
  3. Evaluate answer generation quality using ground truth contexts with different model sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal classification threshold for the single-turn vs multi-turn classifier that balances precision and recall across different product types?
- Basis in paper: [explicit] The authors mention using a classification threshold of 0.1 but note that performance varies widely across products
- Why unresolved: The paper shows significant variation in classifier performance across different products (F1 ranging from 0.27 to 0.62) but doesn't explore optimal threshold tuning for each product type
- What evidence would resolve it: Systematic testing of different classification thresholds for each product type and analysis of the precision-recall trade-off would identify optimal thresholds

### Open Question 2
- Question: How can the retrieval component's performance be improved beyond the current 43% recall@3 to reduce the bottleneck effect on the overall RAG system?
- Basis in paper: [explicit] The authors identify retrieval as the major bottleneck, noting that "retrieval component in RAG is not a solved problem by any means" and showing 43% recall@3
- Why unresolved: While the authors acknowledge retrieval as a bottleneck and compare their results to Google search, they don't explore specific improvements to the retrieval system beyond re-ranking
- What evidence would resolve it: Comparative testing of hybrid retrieval systems, alternative dense embedding models, or improved re-ranking techniques with performance metrics

### Open Question 3
- Question: How would the system performance change if the ground truth data allowed for multiple correct document links instead of a single ground truth link per question?
- Basis in paper: [explicit] The authors note that "more than one link can potentially solve the same question and so neither annotator is necessarily wrong" and suggest considering a list of correct links
- Why unresolved: The current evaluation methodology assumes a single correct answer, which may not reflect the true nature of IT support queries where multiple documents could provide valid solutions
- What evidence would resolve it: Re-evaluation of the system using multi-label ground truth data and comparison of performance metrics with the current single-label approach

## Limitations

- System performance heavily depends on the quality and comprehensiveness of the support documentation corpus, which may have gaps for emerging IT issues
- The 43% recall@3 indicates that approximately 57% of relevant documents are not being retrieved, significantly impacting answer quality in real-world deployment
- Low inter-annotator agreement among subject matter experts raises concerns about the reliability of ground truth labels used for training and evaluation

## Confidence

**High Confidence**: The core claim that RAG enables smaller models to match or outperform larger models is well-supported by the empirical results showing Mixtral-8x7B-Instruct and Granite-13B-Chat-v2 outperforming GPT-4o in answer generation quality when given retrieved context.

**Medium Confidence**: The assertion that retrieval quality directly correlates with answer generation performance is supported by the correlation between recall@3 and answer quality metrics, but the relatively low recall rate (43%) suggests this relationship may be more complex in practice.

**Low Confidence**: The claim about the tool being "equally or more helpful than subject matter experts" is based on a small-scale human evaluation (19 cases) with potential labeling inconsistencies among the experts themselves, making the comparison less reliable.

## Next Checks

1. **Cross-validation of retrieval performance**: Test the retrieval system on a held-out set of cases with independently verified ground truth documents to assess whether the 43% recall@3 generalizes beyond the development set and identify specific failure patterns.

2. **Ablation study on re-ranking**: Compare retrieval performance with and without re-ranking across different query types and document categories to quantify the actual contribution of the re-ranker and identify scenarios where it may be unnecessary or even detrimental.

3. **Longitudinal field testing**: Deploy the system with actual support agents over an extended period, tracking both the recommendation acceptance rate and subsequent case resolution outcomes to validate the human evaluation findings in real-world conditions.