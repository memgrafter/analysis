---
ver: rpa2
title: 'Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language
  Models'
arxiv_id: '2403.17589'
source_url: https://arxiv.org/abs/2403.17589
tags:
- memory
- few-shot
- ours
- zero-shot
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dual Memory Networks (DMN), a versatile adaptation
  approach for vision-language models that effectively handles zero-shot, few-shot,
  and training-free few-shot settings. The method employs dynamic and static memory
  networks that accumulate knowledge from historical test samples and labeled training
  data respectively.
---

# Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models

## Quick Facts
- arXiv ID: 2403.17589
- Source URL: https://arxiv.org/abs/2403.17589
- Reference count: 40
- Key outcome: DMN achieves state-of-the-art performance across zero-shot, few-shot, and training-free few-shot settings, outperforming existing methods by over 3% in zero-shot adaptation

## Executive Summary
This paper introduces Dual Memory Networks (DMN), a versatile adaptation approach for vision-language models that effectively handles zero-shot, few-shot, and training-free few-shot settings. The method employs dynamic and static memory networks that accumulate knowledge from historical test samples and labeled training data respectively. Both memory networks use a flexible interactive strategy with optional learnable projection layers. Experiments on 11 datasets show that DMN achieves state-of-the-art performance across all three settings, with the dynamic memory network being particularly effective by leveraging previously neglected historical test knowledge.

## Method Summary
DMN adapts vision-language models like CLIP using two complementary memory networks: a dynamic memory network that stores historical test features during inference, and a static memory network that caches training data features. Both networks use cosine similarity-based attention to retrieve sample-adaptive classifiers. The method employs a flexible projection layer mechanism that can use identity functions for zero-shot and training-free settings or learnable layers for few-shot adaptation. The final prediction combines text classifier, dynamic memory classifier, and static memory classifier predictions with learned weights.

## Key Results
- DMN achieves over 3% improvement in zero-shot adaptation compared to existing methods
- Dynamic memory network effectively leverages historical test knowledge, showing the importance of this previously neglected information source
- Strong generalization to natural distribution shifts demonstrated across four ImageNet variants
- State-of-the-art performance across all three task settings (zero-shot, few-shot, training-free few-shot) on 11 datasets

## Why This Works (Mechanism)

### Mechanism 1
The dynamic memory network's ability to store and reuse historical test features improves few-shot performance by leveraging test data insights not available in training. During testing, features from previously seen test samples are stored in category-specific memory slots. When classifying a new sample, its feature is compared to these stored historical features using cosine similarity, producing a weighted combination of historical features that acts as a sample-adaptive classifier. This works because historical test samples contain useful information complementary to the text classifier and training data classifier.

### Mechanism 2
The static memory network caches labeled training data features to provide training-free few-shot adaptation. During training, features from labeled training samples are stored in category-specific memory slots. At test time, a new sample's feature is compared to these cached training features using cosine similarity, producing a weighted combination that acts as a sample-adaptive classifier. This enables adaptation without any test-time optimization by leveraging the cached knowledge from the limited training samples.

### Mechanism 3
The flexible projection layer mechanism allows the same memory interaction strategy to work across zero-shot, few-shot, and training-free few-shot settings by toggling between identity and learnable projections. In zero-shot and training-free few-shot settings, projection functions degenerate to identity functions, conducting memory interaction in the vanilla CLIP feature space. In few-shot settings, these projections are learnable linear layers initialized to zero, allowing the model to discover a more efficient feature space for memory interaction.

## Foundational Learning

- **Vision-language models like CLIP**: Learn aligned visual and textual representations from large-scale image-text pairs, enabling zero-shot classification via text features. *Why needed*: DMN builds upon CLIP's pre-trained representations as starting points for memory-based adaptation. *Quick check*: How does CLIP's contrastive pre-training enable zero-shot classification, and what are the limitations that DMN aims to address?

- **Memory networks**: Store and retrieve historical information to improve decision making using attention mechanisms to weight relevant memories. *Why needed*: Both dynamic and static memory networks in DMN are based on memory network principles, storing features and retrieving them via cosine similarity-based attention. *Quick check*: What is the difference between read-only and read-write memory networks, and why does DMN use both types?

- **Few-shot learning**: Adapts models to new tasks with very few labeled examples, often using meta-learning or parameter-efficient fine-tuning techniques. *Why needed*: DMN's static memory network is designed for few-shot adaptation, using cached training features to classify new samples without requiring extensive fine-tuning. *Quick check*: What are the main challenges in few-shot learning, and how does caching training features help address them?

## Architecture Onboarding

- **Component map**: CLIP model (frozen encoders) -> Dynamic memory network (read-write for test features) -> Static memory network (read-only for training features) -> Projection layers (identity or learnable) -> Final classifier combination layer (weighted sum of text, dynamic, and static predictions)
- **Critical path**: For each test sample, extract visual features with CLIP, update dynamic memory with current sample, read out sample-adaptive classifiers from dynamic and static memories, combine with text classifier, and output final prediction
- **Design tradeoffs**: Using memory networks adds storage overhead and computational cost during testing, but enables adaptation without test-time optimization. The choice between identity and learnable projections trades off simplicity and speed for potential performance gains
- **Failure signatures**: Poor performance on individual datasets (e.g., Food101), memory overflow if too many test samples are processed, overfitting if projection layers are learned with too few training samples, and degradation if historical test samples are irrelevant
- **First 3 experiments**:
  1. Implement and test the dynamic memory network alone on a few-shot dataset (e.g., Flowers102) to verify that storing and retrieving historical test features improves performance over zero-shot CLIP
  2. Implement and test the static memory network alone on a few-shot dataset to verify that caching training features enables training-free adaptation
  3. Combine both memory networks and test on a zero-shot dataset (e.g., ImageNet) to verify that the full DMN works across all three task settings without external training data

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- The search space for optimal classifier weights is not explicitly defined, affecting reproducibility
- The impact of memory length on performance across different dataset sizes is not thoroughly explored
- The paper lacks detailed analysis of memory network failure modes and edge cases

## Confidence

- **High Confidence**: The core mechanism of using historical test features via dynamic memory network is well-supported by results and intuitive reasoning
- **Medium Confidence**: The effectiveness of static memory for training-free few-shot adaptation, as it relies on the assumption that cached training features remain relevant
- **Medium Confidence**: The flexible projection layer mechanism's ability to generalize across all three settings, as the paper doesn't provide detailed ablation studies on this component

## Next Checks

1. **Ablation Study**: Test DMN performance with only dynamic memory, only static memory, and both disabled to quantify each component's contribution
2. **Memory Length Sensitivity**: Evaluate performance across different memory lengths (10, 50, 100, 200) to determine optimal settings for various dataset sizes
3. **Distribution Shift Analysis**: Test DMN on more diverse distribution shifts beyond the four ImageNet variants to validate robustness claims