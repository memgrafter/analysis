---
ver: rpa2
title: 'AIM 2024 Challenge on Video Saliency Prediction: Methods and Results'
arxiv_id: '2409.14827'
source_url: https://arxiv.org/abs/2409.14827
tags:
- saliency
- video
- prediction
- challenge
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The AIM 2024 Video Saliency Prediction Challenge focused on developing
  accurate saliency maps for video sequences using a newly collected AViMoS dataset.
  The dataset consists of 1500 videos with over 70 observers per video, collected
  using crowdsourced mouse tracking.
---

# AIM 2024 Challenge on Video Saliency Prediction: Methods and Results

## Quick Facts
- arXiv ID: 2409.14827
- Source URL: https://arxiv.org/abs/2409.14827
- Reference count: 40
- Primary result: The AIM 2024 Video Saliency Prediction Challenge focused on developing accurate saliency maps for video sequences using a newly collected AViMoS dataset.

## Executive Summary
The AIM 2024 Video Saliency Prediction Challenge aimed to advance video saliency prediction by introducing the AViMoS dataset, collected through crowdsourced mouse tracking with over 70 observers per video. Seven teams competed using Transformer-based architectures to process spatio-temporal information, with the top solution (CV_MM) achieving an AUC-Judd score of 0.894. The challenge demonstrated that despite decades of research, video saliency prediction remains an unsolved and competitive task.

## Method Summary
The challenge utilized the AViMoS dataset consisting of 1500 videos with crowdsourced mouse-tracking fixations. Participants employed Transformer-based architectures, primarily the UMT model, with multi-scale feature extraction and hierarchical decoding. Two teams additionally incorporated audio information. The mouse tracking data was processed by resampling to 100Hz and shifting by 300ms to approximate eye-tracking patterns. Most solutions focused on spatio-temporal feature extraction through encoder-decoder architectures.

## Key Results
- CV_MM achieved the top performance with AUC-Judd of 0.894, CC of 0.774, SIM of 0.635, and NSS of 3.464
- Seven teams participated, with most using Transformer-based architectures
- Two teams incorporated audio information in addition to visual features
- The AViMoS dataset was validated against conventional eye-tracking data showing high consistency

## Why This Works (Mechanism)

### Mechanism 1
Mouse tracking data can approximate eye-tracking-based saliency maps when properly collected and processed. The data is collected via a blurred screen except around the cursor, forcing users to move the cursor toward salient regions. The raw mouse coordinates are resampled to 100Hz and shifted by 300ms to account for inherent lag between eye and mouse movements.

### Mechanism 2
Transformer-based encoders effectively capture spatio-temporal features for video saliency prediction. The encoder extracts multi-level features from video frames using self-attention mechanisms. These features are integrated in the decoder phase to generate saliency maps.

### Mechanism 3
Incorporating audio information can improve video saliency prediction. Audio features are extracted and fused with visual features at multiple scales in the decoder, helping the model capture multimodal cues that influence human attention.

## Foundational Learning

- Concept: Saliency prediction fundamentals
  - Why needed here: Understanding what saliency maps represent and how they are evaluated is crucial for developing effective prediction models
  - Quick check question: What are the common metrics used to evaluate saliency prediction models, and what does each metric measure?

- Concept: Deep learning architectures for video processing
  - Why needed here: The challenge requires understanding of how to process video data using neural networks, particularly Transformer-based models
  - Quick check question: How do self-attention mechanisms in Transformers help capture spatio-temporal dependencies in videos?

- Concept: Multimodal learning
  - Why needed here: Two teams incorporated audio information, requiring knowledge of how to extract and fuse audio features with visual features
  - Quick check question: What are the common approaches for extracting audio features from video, and how can they be effectively fused with visual features?

## Architecture Onboarding

- Component map: Video frames -> Encoder (Transformer) -> Feature maps -> Decoder -> Saliency maps
- Critical path: Video frames → Encoder → Feature maps → Decoder → Saliency maps
- Design tradeoffs: Using high-resolution frames increases computational cost but may improve detail; incorporating audio adds complexity but captures multimodal cues; balancing model size and performance is crucial
- Failure signatures: Poor temporal consistency in generated saliency maps; inability to focus on salient regions resulting in diffuse predictions; overfitting to training data
- First 3 experiments: 1) Train simple CNN-based model on AViMoS dataset to establish baseline; 2) Implement Transformer-based encoder and compare with CNN baseline; 3) Add audio feature extraction and fusion to Transformer model and evaluate impact

## Open Questions the Paper Calls Out

### Open Question 1
How does the inclusion of audio data impact the accuracy of video saliency prediction models, and under what conditions is audio most beneficial? The paper mentions two teams adopted audio information with improved results, but doesn't analyze when audio is most beneficial or how it contributes to performance across different video types.

### Open Question 2
What are the limitations of using crowdsourced mouse tracking data compared to traditional eye-tracking data for video saliency prediction? While the paper validates AViMoS against eye-tracking data, it doesn't explore specific limitations or discrepancies between methods or how differences might affect model performance.

### Open Question 3
How can the computational efficiency of video saliency prediction models be improved without compromising accuracy, especially for real-time applications? The paper mentions computationally intensive Transformer architectures and a lightweight solution, but doesn't explore optimization strategies beyond the specific approach mentioned.

## Limitations

- Dataset collection relies on crowdsourced mouse tracking rather than traditional eye-tracking, which may introduce systematic biases despite validation efforts
- The 300ms shift applied to mouse coordinates is based on cross-validation with limited eye-tracking data, but optimal shift may vary across different video content types
- Challenge results show significant performance variation across different evaluation metrics, suggesting models may optimize for certain metrics at expense of others

## Confidence

- **High confidence**: Mouse tracking can approximate eye-tracking when properly collected and processed (supported by validation with conventional eye-tracking data)
- **Medium confidence**: Transformer-based architectures effectively capture spatio-temporal features for video saliency prediction (supported by top-performing solutions but limited corpus evidence)
- **Low confidence**: Audio information consistently improves video saliency prediction across diverse content (only two teams attempted this, with limited performance reporting)

## Next Checks

1. Conduct ablation studies on the mouse tracking preprocessing pipeline, varying the 300ms shift parameter and resampling rate to quantify their impact on model performance
2. Test the generalizability of top solutions on established eye-tracking datasets (e.g., DHF1K, UCF-sports) to verify whether mouse-tracking-trained models transfer effectively
3. Perform cross-dataset evaluation by training models on eye-tracking data and testing on mouse-tracking data (and vice versa) to measure the consistency gap between collection methods