---
ver: rpa2
title: Assessment and manipulation of latent constructs in pre-trained language models
  using psychometric scales
arxiv_id: '2409.19655'
source_url: https://arxiv.org/abs/2409.19655
tags:
- plms
- language
- constructs
- prompts
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EMPALC, a framework for assessing latent
  psychological constructs in pre-trained language models (PLMs) using psychometric
  scales reformulated as natural language inference (NLI) prompts. The method involves
  translating validated questionnaires into NLI prompts with carefully selected construct
  terms and intensifiers, normalizing entailment scores, and validating results through
  content validity, internal consistency, and construct validity tests.
---

# Assessment and manipulation of latent constructs in pre-trained language models using psychometric scales
## Quick Facts
- **arXiv ID**: 2409.19655
- **Source URL**: https://arxiv.org/abs/2409.19655
- **Reference count**: 10
- **Primary Result**: Introduces EMPALC framework to detect and manipulate psychological constructs in PLMs using psychometric scales reformulated as NLI prompts

## Executive Summary
This paper introduces EMPALC, a framework for assessing latent psychological constructs in pre-trained language models (PLMs) using psychometric scales reformulated as natural language inference (NLI) prompts. The method involves translating validated questionnaires into NLI prompts with carefully selected construct terms and intensifiers, normalizing entailment scores, and validating results through content validity, internal consistency, and construct validity tests. Applied to 88 models, the approach successfully detected human-like mental health constructs including anxiety (GAD-7), depression (PHQ-9), and Sense of Coherence (SoC-13), with Cronbach's alpha values above 0.71. The constructs showed expected correlations (e.g., anxiety and depression positively correlated, both negatively correlated with SoC) and could be manipulated through domain adaptation. This work provides a systematic method to interpret and potentially mitigate undesirable biases in PLMs using psychological tools.

## Method Summary
The EMPALC framework translates validated psychometric scales into NLI prompts by identifying construct terms (words semantically close to the target construct) and selecting appropriate intensifiers (adverbs like "often" or "sometimes"). Each questionnaire item becomes an NLI prompt where the model must determine if the statement "entails" the construct term. Entailment scores are normalized across items and models. The framework validates detected constructs using Cronbach's alpha for internal consistency, correlation analysis for construct validity, and content validity checks. The method was applied to 88 different PLM architectures to detect anxiety (GAD-7), depression (PHQ-9), and Sense of Coherence (SoC-13) constructs, then manipulated these constructs through domain adaptation training.

## Key Results
- Detected consistent anxiety, depression, and Sense of Coherence constructs across 88 different PLM architectures with Cronbach's alpha values above 0.71
- Observed expected human-like correlation patterns: anxiety and depression positively correlated, both negatively correlated with Sense of Coherence
- Successfully manipulated detected mental health constructs through domain adaptation, demonstrating practical control over latent psychological features
- Provided first systematic method for assessing and interpreting latent psychological constructs in language models using validated psychometric tools

## Why This Works (Mechanism)
The framework works by leveraging the distributional hypothesis - words appearing in similar contexts have related meanings - to map psychometric scale items onto model representations. By reformulating validated psychological questionnaires as NLI tasks, the method exploits the model's learned semantic relationships to detect consistent patterns that correspond to human-verified psychological constructs. The normalization process accounts for architectural differences between models, enabling cross-model comparison. Domain adaptation demonstrates that these constructs are not merely statistical artifacts but can be actively shaped through targeted training, suggesting they represent genuine emergent properties of the model's learned representations.

## Foundational Learning
- **Natural Language Inference (NLI)**: Binary classification task determining if one statement logically follows from another. Why needed: Enables quantitative measurement of semantic relationships between psychometric items and psychological constructs. Quick check: Verify model's ability to correctly classify simple entailment relationships before applying to psychometric prompts.

- **Psychometric validation**: Statistical techniques (Cronbach's alpha, construct validity, content validity) for ensuring measurement instruments measure what they claim to measure. Why needed: Provides rigorous framework for validating that detected constructs are consistent, meaningful, and interpretable. Quick check: Compare model-generated Cronbach's alpha values with established human psychometric thresholds.

- **Semantic space mapping**: Process of identifying words semantically close to psychological constructs using word embeddings or similar techniques. Why needed: Enables translation of abstract psychological concepts into concrete terms the model can process. Quick check: Verify that selected construct terms are indeed semantically close to target constructs using cosine similarity metrics.

## Architecture Onboarding
**Component Map**: Psychometric scales → NLI prompt generation → Construct term identification → Intesifier selection → Entailment scoring → Normalization → Validation (Cronbach's alpha, correlations, content validity)

**Critical Path**: Scale translation → Prompt formulation → Score normalization → Construct validation → Domain adaptation manipulation

**Design Tradeoffs**: 
- Using NLI format provides quantitative scores but may not capture nuance of psychological states
- Fixed construct terms enable reproducibility but may miss model-specific semantic variations
- Domain adaptation shows manipulation capability but may introduce confounds from task-specific training

**Failure Signatures**: 
- Low Cronbach's alpha values indicate inconsistent construct measurement
- Unexpected correlation patterns suggest construct misalignment or model artifacts
- Poor normalization indicates architectural biases affecting score comparability

**3 First Experiments**:
1. Validate basic NLI prompt functionality with simple entailment tasks before applying psychometric scales
2. Test construct term selection methodology on small subset of models to verify semantic proximity
3. Run single psychometric scale through complete EMPALC pipeline on one model to identify potential implementation issues

## Open Questions the Paper Calls Out
The paper acknowledges that the semantic space of PLMs may differ fundamentally from human semantic spaces, raising questions about whether detected constructs truly represent "psychological" phenomena or merely statistical correlations in the model's embedding space. The generalizability of the method to other psychological constructs beyond anxiety, depression, and Sense of Coherence remains untested, as does its applicability to languages other than English.

## Limitations
- Validation framework relies on human-generated psychometrics that may not translate perfectly to machine representations
- Semantic space differences between PLMs and human cognition may mean detected constructs don't represent genuine psychological phenomena
- Limited to three psychological constructs (anxiety, depression, Sense of Coherence) without broader validation across psychological domains
- English-language focus limits applicability to multilingual models and non-English speakers

## Confidence
**High**: Technical implementation of EMPALC framework is sound with clear methodology for prompt construction, entailment score normalization, and validity testing. Application of established psychometric validation techniques demonstrates methodological rigor.

**Medium**: Interpretation that detected constructs represent genuine "psychological" phenomena in PLMs. While framework successfully detects consistent patterns across models, ontological status of these constructs remains philosophically contested.

**Low**: External validity of manipulation experiments showing domain adaptation effects. While mental health constructs can be reduced through domain adaptation, ecological validity and practical implications require further investigation.

## Next Checks
1. **Cross-linguistic validation**: Apply EMPALC to multilingual PLMs and translate psychometric scales into multiple languages to assess whether constructs are language-dependent or universal across the model's learned representations.

2. **Temporal stability analysis**: Measure construct stability across different versions of the same model architecture and different training checkpoints to determine if detected psychological constructs persist through fine-tuning and pretraining updates.

3. **Comparative construct mapping**: Systematically map relationships between PLM-detected constructs and their human counterparts by correlating model outputs with human responses on same psychometric scales across diverse participant populations.