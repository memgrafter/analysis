---
ver: rpa2
title: 'Dissecting Deep RL with High Update Ratios: Combatting Value Divergence'
arxiv_id: '2403.05996'
source_url: https://arxiv.org/abs/2403.05996
tags:
- learning
- priming
- q-values
- divergence
- return
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work identifies value divergence\u2014not overfitting\u2014\
  as the key barrier to learning under high update-to-data ratios in deep RL. The\
  \ divergence is triggered by large gradients from out-of-distribution action predictions,\
  \ amplified by Adam\u2019s second moment."
---

# Dissecting Deep RL with High Update Ratios: Combatting Value Divergence

## Quick Facts
- arXiv ID: 2403.05996
- Source URL: https://arxiv.org/abs/2403.05996
- Reference count: 40
- Primary result: Output feature normalization mitigates value divergence in deep RL, enabling stable learning at high update-to-data ratios without network resets.

## Executive Summary
This work investigates the failure modes of deep reinforcement learning under high update-to-data (UTD) ratios, identifying value divergence as the primary barrier rather than overfitting. The divergence is triggered by out-of-distribution action predictions that generate large gradients, amplified by Adam's second moment momentum. The authors propose Output Feature Normalization (OFN), which projects critic encoder outputs onto the unit sphere, decoupling value scaling from earlier layers. This approach enables stable learning without network resets. On dm_control tasks, OFN with UTD=8 matches or exceeds SAC with UTD=32 and resets; on dog tasks, it matches a strong model-based baseline without models.

## Method Summary
The method introduces Output Feature Normalization (OFN) to SAC, where the critic encoder's output is projected onto the unit sphere before the final linear layer. This normalization decouples value scaling from earlier network layers, preventing gradient explosion during backpropagation. The approach is evaluated against standard SAC with and without network resets across dm_control MuJoCo tasks (DMC15-500k suite) and dog tasks. Experiments include priming studies to induce divergence, ablation studies on single critic, actor reset, and regularization, and comparison with model-based baselines.

## Key Results
- OFN mitigates value divergence during priming, achieving high rewards where standard SAC fails
- On dm_control tasks, OFN with UTD=8 matches or exceeds SAC with UTD=32 and resets
- On dog tasks, OFN matches TD-MPC2 model-based baseline without using models
- Actor resetting accounts for meaningful performance improvements under high UTD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Value divergence occurs due to out-of-distribution (OOD) action predictions triggering large gradients, amplified by Adam's second moment momentum.
- Mechanism: When the critic predicts Q-values for unseen actions, the resulting loss can be very large. Adam's second moment term accumulates these large gradients over time, causing Q-value estimates to diverge even on in-distribution data. The divergence propagates backward through the network layers due to multiplicative backpropagation through ReLU activations.
- Core assumption: The observed divergence is primarily caused by optimization dynamics rather than statistical overestimation bias from off-policy sampling.
- Evidence anchors:
  - [abstract]: "Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be linked to overestimation on unseen action prediction propelled by optimizer momentum."
  - [section 3.2]: "We conjecture that Q-value divergence starts with overestimated values of OOD actions. This overestimation could cause the optimizer to continually increase Q-values via its momentum leading to divergence."
  - [corpus]: Weak - no direct mentions of this specific mechanism, but related work on plasticity loss suggests optimization issues are underexplored.

### Mechanism 2
- Claim: Output Feature Normalization (OFN) decouples value scaling from earlier network layers by projecting critic encoder outputs onto the unit sphere.
- Mechanism: By normalizing the output features of the critic encoder to lie on the unit ball, the network separates the task of learning appropriate Q-value scales from the non-linear transformations in earlier layers. This prevents early layers from producing unbounded activations that could explode during backpropagation, while allowing the final linear layer to learn the correct scale for Q-values.
- Core assumption: The divergence primarily originates in early network layers where feature magnitudes can grow unchecked, rather than in the final value prediction layer.
- Evidence anchors:
  - [section 4.1]: "We hypothesize that we can address many of these problems by separating the scaling of the Q-values to the appropriate size from the earlier non-linear layers of the network and moving the Q-value scaling to the final linear layer."
  - [section 4.2]: "We find that OFN achieves high reward and most distinctly, Q-value divergence during priming is fully mitigated."
  - [corpus]: Weak - mentions related work using unit ball normalization but not specifically for this value divergence problem.

### Mechanism 3
- Claim: Actor resetting partially addresses high-UTD learning failures by refreshing the policy when it becomes too pessimistic or stuck in suboptimal exploration patterns.
- Mechanism: Under high update ratios, the actor may converge to overly conservative policies that fail to explore effectively, especially when combined with pessimistic value estimates. Resetting the actor introduces fresh exploration while keeping the critic's learned value estimates, allowing the agent to escape local optima in policy space.
- Core assumption: The actor optimization is sensitive to the interplay between value estimates and exploration noise, and periodic resets can break pathological convergence patterns.
- Evidence anchors:
  - [section 5.2]: "The actor-reset results are within variance of the full-resetting standard MLP baseline" and "actor resetting can account for a meaningful portion of OFN's performance decline when going from UTD = 8 to UTD = 32."
  - [section 5.3]: "We suspect that this might be related to the complexity of exploration" and "The ability of a random policy to obtain non-trivial reward and information about the environment has been shown to be a crucial factor."
  - [corpus]: Weak - no direct evidence, but related work on plasticity loss suggests actor networks may be particularly vulnerable to optimization issues.

## Foundational Learning

- Concept: Value function divergence and its distinction from statistical overestimation bias
  - Why needed here: The paper argues that high-UTD failures are caused by optimization-induced divergence rather than the well-known statistical overestimation from off-policy sampling. Understanding this distinction is crucial for applying the correct mitigation strategy.
  - Quick check question: If you observe exploding Q-values on in-distribution data during training, is this more likely a statistical bias problem or an optimization problem?

- Concept: The role of optimizer momentum terms in propagating errors through neural networks
  - Why needed here: Adam's second moment term accumulates gradient information over time, which can amplify small errors into large divergences. Understanding how momentum affects learning dynamics is essential for diagnosing and fixing the divergence problem.
  - Quick check question: How would replacing Adam with SGD with momentum affect the divergence phenomenon observed in the priming experiments?

- Concept: Feature normalization techniques and their impact on gradient flow
  - Why needed here: OFN projects features onto the unit sphere to prevent gradient explosion. Understanding how different normalization schemes affect the geometry of the feature space and gradient propagation is key to implementing and debugging this approach.
  - Quick check question: Why might projecting features onto the unit ball be more effective at preventing divergence than simply applying layer normalization?

## Architecture Onboarding

- Component map: State and action -> Critic encoder (MLP) -> OFN (unit ball projection) -> Linear layer -> Q-values
- Critical path: During forward pass, states and actions are processed by the critic encoder, then normalized via OFN to lie on the unit sphere, then scaled by the final linear layer to produce Q-values. During backpropagation, gradients flow through the normalization layer, which has a non-trivial gradient due to the projection operation.
- Design tradeoffs: OFN prevents divergence but may limit the representational capacity of early layers by constraining their output norms. The approach trades off some expressiveness for stability. Alternative normalizations like layer normalization would be weaker constraints but might preserve more capacity.
- Failure signatures: If OFN is implemented incorrectly (e.g., normalizing after the linear layer instead of before), divergence will persist. If the projection operation becomes numerically unstable (e.g., when feature norms approach zero), training may fail. If the environment requires very large or very small Q-value scales, the unit ball constraint might prevent learning appropriate values.
- First 3 experiments:
  1. Implement OFN by modifying the critic encoder to project its output onto the unit sphere before the final linear layer. Verify that feature norms are indeed bounded between 0 and 1.
  2. Repeat the priming experiment from Section 3 with OFN: collect 1,000 random samples, prime for 100,000 updates, then continue training. Compare Q-value divergence and final returns against the baseline without OFN.
  3. Test OFN on a high-UTD dm_control task (e.g., cheetah-run with UTD=32) and compare performance against standard SAC with and without resets. Monitor Q-value magnitudes throughout training to verify that divergence is mitigated.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the divergence in Q-values arise primarily from the statistical bias in off-policy sampling or from the optimization dynamics of neural networks?
- Basis in paper: [explicit] The paper suggests that the divergence is caused by prediction of out-of-distribution actions triggering large gradients, compounded by optimizer momentum, rather than statistical bias.
- Why unresolved: While the paper provides evidence that the divergence is linked to optimizer dynamics, it does not definitively rule out the contribution of statistical bias, especially in scenarios where the action space coverage is limited.
- What evidence would resolve it: Conducting experiments that isolate the effects of optimizer dynamics (e.g., using SGD without momentum) from those of statistical bias (e.g., using pessimistic updates) in environments with varying levels of action space coverage.

### Open Question 2
- Question: What are the specific mechanisms by which the actor optimization process fails under high update-to-data ratios, and how can these be addressed?
- Basis in paper: [inferred] The paper mentions that resetting the actor can improve performance, indicating that the actor optimization is a separate issue from the critic divergence. However, it does not delve into the specific causes of actor failures.
- Why unresolved: The paper focuses on the critic divergence and does not investigate the actor optimization process in detail. The actor's role in exploration and its interaction with the critic's divergence under high UTD ratios remain unclear.
- What evidence would resolve it: Detailed analysis of the actor's policy gradients and their interaction with the critic's Q-values during high UTD training, along with experiments that modify the actor's optimization process (e.g., using different policy gradient methods or exploration strategies).

### Open Question 3
- Question: How does the reward shaping impact the occurrence and severity of Q-value divergence, and what are the implications for designing reward functions in high-UTD learning?
- Basis in paper: [inferred] The paper suggests that environments with small and uninformative initial rewards are more prone to catastrophic divergence, implying a connection between reward shaping and divergence.
- Why unresolved: While the paper hints at a relationship between reward shaping and divergence, it does not explore this connection in depth. The specific mechanisms by which reward functions influence the optimization dynamics and divergence remain unclear.
- What evidence would resolve it: Experiments that systematically vary the reward function's informativeness and magnitude across different environments, coupled with analysis of the resulting Q-value trajectories and optimization dynamics. Additionally, investigating the use of reward shaping techniques that mitigate divergence while maintaining exploration efficiency.

## Limitations
- Actor optimization issues under high UTD remain unresolved, with performance declines observed even with OFN
- Exploration limitations in complex environments constrain the full potential of high-UTD learning
- Methodological details for dog task baseline comparison are insufficient for faithful reproduction

## Confidence
- High: The core claim about value divergence being the primary barrier to high-UTD learning is well-supported by priming experiments and ablation studies
- Medium: Actor reset findings show promise but the underlying causes of actor failures remain unclear
- Low: The exploration analysis acknowledges limitations but doesn't provide concrete solutions for complex environments

## Next Checks
1. Reproduce the priming experiment with exact UTD=32 settings to confirm divergence mitigation
2. Test OFN on additional tasks (e.g., from the RL Unplugged benchmark) to assess generalizability
3. Conduct a controlled ablation study isolating the impact of Adam's second moment on divergence