---
ver: rpa2
title: Semiparametric Token-Sequence Co-Supervision
arxiv_id: '2403.09024'
source_url: https://arxiv.org/abs/2403.09024
tags:
- space
- embedding
- sequence
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces semiparametric token-sequence co-supervision,
  a method that trains a language model using both traditional next token prediction
  (over a parametric token embedding space) and next sequence prediction (over a nonparametric
  sequence embedding space). The nonparametric space is constructed by a separate
  language model that condenses input text into a single embedding.
---

# Semiparametric Token-Sequence Co-Supervision

## Quick Facts
- arXiv ID: 2403.09024
- Source URL: https://arxiv.org/abs/2403.09024
- Reference count: 40
- Primary result: Co-supervised models outperform single-supervision baselines by 14.2 points average on information-seeking tasks

## Executive Summary
This paper introduces semiparametric token-sequence co-supervision, a method that trains a language model using both traditional next token prediction and next sequence prediction. The approach leverages a parametric token embedding space alongside a nonparametric sequence embedding space constructed by a separate language model. Experiments on 10 information-seeking datasets show that co-supervised models outperform those trained with each supervision separately by an average of 14.2 points, with particular effectiveness for out-of-domain datasets (6.6 point improvement). The method demonstrates improved generalization and robustness through better utilization of knowledge from the nonparametric space during generation.

## Method Summary
The method trains a generative model (Gen) with dual supervision: next token prediction (NTP) over a parametric token embedding space and next sequence prediction (NSP) over a nonparametric sequence embedding space. The nonparametric space is constructed by a separate autoregressive model (Embseq) that condenses input text into a single embedding. During training, Gen predicts the next token autoregressively while also predicting the current sequence embedding from Embseq when encountering an [NSP] token. The training objective combines NTP loss with NSP loss using a weighted sum, where NSP is trained using contrastive InfoNCE loss between Gen's query embeddings and Embseq's sequence embeddings.

## Key Results
- Co-supervised models achieve 14.2 point average improvement over single-supervision baselines on 10 information-seeking datasets
- Out-of-domain datasets show 6.6 point average improvement, demonstrating better generalization
- Models actively utilize knowledge from nonparametric space during generation rather than relying on parametric memorization
- Distributional alignment between Gen and Embseq (using same pretrained model) contributes to more stable training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Co-supervision stabilizes the nonparametric sequence embedding space by leveraging the robustness of the parametric token embedding space
- Mechanism: Gradients from both NTP and NSP supervision flow through Gen during training. The parametric space, already well-formed from pretraining, provides a stable foundation that regularizes the training of the nonparametric space constructed by Embseq. This dual supervision encourages the model to integrate both spaces into a common representation, preventing the nonparametric space from drifting into unstable configurations.
- Core assumption: The parametric token embedding space is robust and generalizable due to pretraining, and this robustness can be transferred to stabilize a newly introduced nonparametric embedding space during co-training
- Evidence anchors: [abstract], [section] on parametric robustness enhancing nonparametric stability
- Break condition: If the parametric space is poorly pretrained or highly task-specific, its stability may not transfer effectively, leading to unstable nonparametric space training

### Mechanism 2
- Claim: The model learns to actively utilize knowledge from the nonparametric sequence embedding space during generation rather than relying solely on parametric memorization
- Mechanism: By training with both NTP and NSP supervision simultaneously, the model is incentivized to integrate information from both spaces. During inference, when the model encounters an [NSP] token, it retrieves the most relevant sequence embedding from the nonparametric space and conditions generation on it. This forces the model to use external knowledge dynamically.
- Core assumption: The model can condition generation on both token-level and sequence-level embeddings, and training with both signals encourages this integration rather than simple memorization
- Evidence anchors: [abstract], [section] on model utilizing nonparametric space knowledge during generation
- Break condition: If the retrieval mechanism fails frequently or the nonparametric space is poorly constructed, the model may revert to relying on parametric memorization

### Mechanism 3
- Claim: Aligning the distributions of Gen and Embseq leads to more stable training and better performance
- Mechanism: When both Gen and Embseq are initialized from the same pretrained model, they share similar distributional priors and embedding spaces. This alignment reduces the domain gap between the parametric and nonparametric spaces, making the contrastive learning in NSP more effective.
- Core assumption: Distributional compatibility between the two models reduces the difficulty of aligning their output spaces, leading to more stable and effective co-training
- Evidence anchors: [section] on Llama2-7B showing highest performance with aligned distributions
- Break condition: If the two models are initialized from very different distributions, the alignment process may fail, leading to poor NSP performance and unstable training

## Foundational Learning

- Concept: Contrastive learning (InfoNCE loss)
  - Why needed here: NSP is trained using contrastive learning to align query embeddings with positive sequence embeddings and push them away from negative ones. This is essential for constructing a meaningful nonparametric sequence embedding space.
  - Quick check question: What is the role of in-batch negatives in the NSP loss calculation?

- Concept: Autoregressive language modeling
  - Why needed here: Both Gen and Embseq are autoregressive models; Gen generates tokens step-by-step, and Embseq constructs sequence embeddings autoregressively. Understanding how autoregressive models predict the next token is crucial for grasping how the parametric and nonparametric spaces interact.
  - Quick check question: How does the autoregressive property of Embseq affect the construction of the nonparametric sequence embedding space?

- Concept: Multi-task learning and shared representations
  - Why needed here: Co-supervision trains Gen on two tasks (NTP and NSP) simultaneously, encouraging the model to share and integrate representations from both the parametric and nonparametric spaces. This is key to the model's improved generalization and robustness.
  - Quick check question: How does training with both NTP and NSP supervision affect the internal representations of Gen compared to training with only NTP?

## Architecture Onboarding

- Component map: Input sequence -> Gen -> hidden state -> [NSP] token -> Gen's last hidden state (query embedding); Input sequence -> Embseq -> last hidden state of </s> token (sequence embedding); Calculate NSP loss (contrastive) between query and sequence embeddings; Calculate NTP loss (causal LM) over token embeddings; Backpropagate both losses through Gen; only NSP loss through Embseq; Update parameters; repeat

- Critical path: 1. Input sequence → Gen → hidden state → [NSP] token → Gen's last hidden state (query embedding). 2. Input sequence → Embseq → last hidden state of </s> token (sequence embedding). 3. Calculate NSP loss (contrastive) between query and sequence embeddings. 4. Calculate NTP loss (causal LM) over token embeddings. 5. Backpropagate both losses through Gen; only NSP loss through Embseq. 6. Update parameters; repeat.

- Design tradeoffs: Using the same pretrained model for Gen and Embseq improves stability but may limit diversity of representations; Larger batch sizes improve NSP performance due to more in-batch negatives but increase memory usage; Weight λ balances influence between NTP and NSP; too high destabilizes parametric space, too low underutilizes nonparametric space

- Failure signatures: NSP loss diverges or remains high: likely due to distributional mismatch between Gen and Embseq, or insufficient in-batch negatives; NTP performance degrades significantly: likely λ is too high, causing NSP to dominate training; Retrieval performance poor: likely nonparametric space is not well-formed; check if Embseq is trained properly or if batch size is too small

- First 3 experiments: 1. Train Gen with only NTP supervision (baseline) and evaluate correctness, retrieval, and grounding on a small subset of KILT. 2. Train Gen with only NSP supervision (using a fixed, pre-trained Embseq) and evaluate the same metrics. 3. Train Gen with co-supervision (NTP + NSP) using the same Embseq, and compare all metrics to the two baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of pretrained model for Embseq affect the stability and performance of the nonparametric sequence embedding space, and what factors contribute to this effect?
- Basis in paper: [explicit] The paper investigates the impact of using different pretrained models for Embseq (GPT2-large, TinyLlama, and Llama2-7B) on overall performance, finding that Llama2-7B shows the highest performance. It suggests that the specific distribution inherent to each pretrained language model influences the performance and that training Gen and Embseq with semiparametric token-sequence co-supervision enhances robustness compared to training each model separately.
- Why unresolved: While the paper demonstrates the importance of Embseq choice, it does not delve into the specific factors that contribute to the observed differences in performance. Understanding these factors could provide insights into optimizing the choice of Embseq for different tasks and datasets.
- What evidence would resolve it: Experiments comparing the performance of different Embseq models across a wider range of tasks and datasets, coupled with an analysis of the factors contributing to their relative strengths and weaknesses (e.g., model size, architecture, training data, etc.).

### Open Question 2
- Question: How does the interaction between the parametric token embedding space and the nonparametric sequence embedding space evolve during training, and what mechanisms drive this interaction?
- Basis in paper: [inferred] The paper suggests that semiparametric token-sequence co-supervision encourages a high interaction between the parametric token and nonparametric sequence spaces, leading to improved performance. It also mentions that the model tends to utilize knowledge from the nonparametric space during generation, shifting from rote learning to active knowledge utilization.
- Why unresolved: While the paper observes the interaction between the two spaces, it does not provide a detailed analysis of how this interaction evolves during training or the specific mechanisms that drive it. Understanding these aspects could provide insights into optimizing the co-supervision process and improving model performance.
- What evidence would resolve it: Experiments tracking the evolution of the interaction between the two spaces during training, potentially using techniques like probing or attention analysis, to identify the key mechanisms driving this interaction.

### Open Question 3
- Question: How does the balance between next token prediction (NTP) and next sequence prediction (NSP) supervision affect the model's performance across different tasks and datasets, and what factors influence this balance?
- Basis in paper: [explicit] The paper investigates the effect of the weight parameter λ, which balances the influence of NTP and NSP supervision on the model's training. It finds that different values of λ lead to varying performance across different metrics and tasks.
- Why unresolved: While the paper demonstrates the importance of balancing NTP and NSP supervision, it does not provide a comprehensive analysis of how this balance affects performance across different tasks and datasets. Understanding these effects could help optimize the co-supervision process for specific use cases.
- What evidence would resolve it: Experiments systematically varying the weight parameter λ across a wide range of tasks and datasets, coupled with an analysis of the factors that influence the optimal balance for each case (e.g., task complexity, dataset size, etc.).

## Limitations
- Limited mechanistic understanding of how Gen integrates parametric and nonparametric information during generation
- Sparse ablation studies on key hyperparameters like λ and batch size
- Training data filtering criteria not fully specified, making reproduction difficult

## Confidence
- **High confidence**: Experimental results demonstrating improved generalization and robustness on information-seeking tasks across 10 datasets
- **Medium confidence**: Proposed mechanism explaining why co-supervision stabilizes the nonparametric space through parametric robustness transfer
- **Medium confidence**: Claim that aligning Gen and Embseq distributions improves training stability

## Next Checks
1. Perform neural activation analysis with attention visualization to empirically verify whether Gen actively integrates information from the nonparametric sequence embedding space during generation
2. Conduct systematic hyperparameter sensitivity study varying λ and batch size to identify optimal ranges and understand stability effects
3. Run experiments with distributional mismatch by training Embseq using a different pretrained model (e.g., GPT2 or OPT) while keeping Gen fixed to test distributional alignment necessity