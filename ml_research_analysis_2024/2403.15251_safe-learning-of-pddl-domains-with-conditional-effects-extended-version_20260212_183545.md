---
ver: rpa2
title: Safe Learning of PDDL Domains with Conditional Effects -- Extended Version
arxiv_id: '2403.15251'
source_url: https://arxiv.org/abs/2403.15251
tags:
- action
- effects
- conditional-sam
- domain
- posante
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Conditional-SAM, the first algorithm for learning
  safe action models with conditional effects in PDDL domains. It proves that learning
  such models may require exponential samples in the worst case, but is tractable
  under reasonable assumptions about bounded antecedent size.
---

# Safe Learning of PDDL Domains with Conditional Effects -- Extended Version

## Quick Facts
- arXiv ID: 2403.15251
- Source URL: https://arxiv.org/abs/2403.15251
- Authors: Argaman Mordoch; Enrico Scala; Roni Stern; Brendan Juba
- Reference count: 4
- Key outcome: First algorithm for learning safe action models with conditional effects in PDDL domains, with polynomial complexity under bounded antecedent size

## Executive Summary
This paper introduces Conditional-SAM, the first algorithm capable of learning safe PDDL action models with conditional effects from observed trajectories. The key contribution is proving that while learning such models may require exponential samples in the worst case, the problem becomes tractable under reasonable assumptions about bounded antecedent size. The algorithm uses inductive rules to infer preconditions and conditional effects, and can be extended to handle lifted action models and universal quantifiers. Experimental results demonstrate the algorithm's ability to learn models that solve most test problems perfectly, though scalability to more complex domains remains challenging.

## Method Summary
Conditional-SAM is an algorithm that learns safe PDDL action models with conditional and universal effects from observed trajectories. It employs four inductive rules to infer preconditions (as supersets of the real preconditions) and conditional effects from action triplets in the trajectories. The algorithm supports lifted models and universal quantifiers, and provides safety guarantees by only removing literals from preconditions that were never observed in pre-states. The method has polynomial time and space complexity when the maximal antecedent size is fixed, and requires asymptotically optimal sample complexity for learning safe action models.

## Key Results
- Conditional-SAM can learn PDDL domains with conditional effects that solve most test problems perfectly (97% for Satellite domain)
- The algorithm achieves polynomial time and space complexity when maximal antecedent size is fixed at n
- Safety is guaranteed - any plan generated with the learned model will be valid in the real model
- Experimental results show scalability challenges on domains with complex universal effects (CityCar, Briefcase)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The algorithm's polynomial runtime under bounded antecedent size enables practical learning of safe action models.
- **Mechanism:** By limiting the maximal antecedent size to n, the algorithm restricts the search space for conditional effects. This bounded search space means the number of possible antecedents per literal grows polynomially, not exponentially, with the number of literals.
- **Core assumption:** The bound n on antecedent size is known and fixed in advance.
- **Evidence anchors:**
  - [abstract]: "is tractable under reasonable assumptions about bounded antecedent size"
  - [section]: "Theorem 4.1. The space complexity of Conditional-SAM is O(|A||F|n+1(e/n)^n)"
  - [corpus]: Weak evidence - corpus neighbors focus on LLM-based planning, not sample complexity.

### Mechanism 2
- **Claim:** The safety guarantee comes from conservative precondition inference.
- **Mechanism:** The algorithm infers preconditions as supersets of the real preconditions by only removing literals that were never observed in pre-states. This ensures any plan generated with the learned model will be valid in the real model.
- **Core assumption:** Observations are noise-free and fully observable (Assumption 1).
- **Evidence anchors:**
  - [abstract]: "proves that learning non-trivial safe action models with conditional effects may require an exponential number of samples"
  - [section]: "Theorem 3.1. The action model M′ learned by Conditional-SAM is safe w.r.t. the action model that generated the input trajectories T"
  - [corpus]: No direct evidence - corpus neighbors don't discuss safety guarantees.

### Mechanism 3
- **Claim:** The algorithm's completeness guarantee comes from asymptotic optimality in sample complexity.
- **Mechanism:** The algorithm requires a number of samples that is asymptotically optimal for learning safe action models with conditional effects. This means it uses the minimum number of samples needed to ensure safety with high probability.
- **Core assumption:** The distribution over problems and plans is known and fixed.
- **Evidence anchors:**
  - [abstract]: "proves that learning non-trivial safe action models with conditional effects may require an exponential number of samples"
  - [section]: "Theorem 4.3. Let D be a distribution over pairs ⟨P, Π⟩...Given m ≥ 1/ϵ(ln(3)|F||A| + 2 ln(2)|F||A|(2|F|e/n)^n + ln 1/δ) trajectories...Conditional-SAM returns an action model M′ such that with probability 1 − δ..."
  - [corpus]: No direct evidence - corpus neighbors don't discuss sample complexity.

## Foundational Learning

- **Concept:** PDDL domain modeling
  - Why needed here: The algorithm learns PDDL domains with conditional effects, so understanding PDDL syntax and semantics is crucial.
  - Quick check question: Can you explain the difference between a fluent and a literal in PDDL?

- **Concept:** Safe action model learning
  - Why needed here: The algorithm guarantees that any plan generated with the learned model is valid in the real model, which is the core contribution.
  - Quick check question: What is the difference between a safe action model and a complete action model?

- **Concept:** Inductive learning rules
  - Why needed here: The algorithm uses inductive rules to infer preconditions and conditional effects from trajectories, which is the core learning mechanism.
  - Quick check question: Can you explain how the "Not a precondition" rule works in the algorithm?

## Architecture Onboarding

- **Component map:**
  - Trajectory parser -> Action triplets -> Precondition inference engine -> Conditional effect inference engine -> Action model builder -> Safety checker -> Learned PDDL domain

- **Critical path:**
  - Trajectory → Action triplets → Precondition inference → Conditional effect inference → Action model builder → Safety checker → Learned PDDL domain

- **Design tradeoffs:**
  - Bounded antecedent size vs. expressiveness: Limiting antecedent size makes learning tractable but may miss complex conditional effects
  - Conservative inference vs. completeness: Inferring supersets of preconditions ensures safety but may make the learned model too restrictive

- **Failure signatures:**
  - High percentage of test problems unsolvable: May indicate over-approximation of preconditions
  - High resource consumption: May indicate complex universal preconditions or effects
  - Low semantic recall: May indicate insufficient observations to learn all preconditions

- **First 3 experiments:**
  1. Learn a simple PDDL domain with no conditional effects and verify correctness
  2. Learn a PDDL domain with simple conditional effects (e.g., one antecedent) and verify safety
  3. Learn a PDDL domain with universal effects and verify that planners can solve problems with the learned model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Conditional-SAM be extended to handle domains with disjunctive antecedents in conditional effects without incurring prohibitive computational complexity?
- Basis in paper: [explicit] The paper discusses supporting disjunctive antecedents as future work and notes that this would require changing the initialization of PosAnte to include every possible CNF clause, which would highly increase the algorithm's complexity.
- Why unresolved: The paper states that due to the prohibitive complexity, they decided not to support disjunctive antecedents in Conditional-SAM and leave this functionality for future work.
- What evidence would resolve it: Developing an algorithm extension that can handle disjunctive antecedents with reasonable computational complexity and evaluating its performance on benchmark domains.

### Open Question 2
- Question: How does the sample complexity of Conditional-SAM scale with the size of the action models (number of actions and fluents) in the domain?
- Basis in paper: [explicit] The paper presents sample complexity bounds that depend on the number of actions and fluents in the domain, but notes that the bound is tight and may be exponential in the worst case.
- Why unresolved: While the paper provides theoretical bounds, it does not empirically investigate how the sample complexity scales with the size of the action models in practice.
- What evidence would resolve it: Conducting experiments on domains with varying numbers of actions and fluents to empirically measure the sample complexity of Conditional-SAM.

### Open Question 3
- Question: Can Conditional-SAM be extended to handle domains with numeric conditions and effects in the action models?
- Basis in paper: [explicit] The paper mentions that supporting numeric conditions and effects is a direction for future work, as the current algorithm only handles Boolean fluents.
- Why unresolved: The paper focuses on domains with Boolean fluents and does not explore how to extend the algorithm to handle numeric conditions and effects.
- What evidence would resolve it: Developing an extension of Conditional-SAM that can handle numeric conditions and effects and evaluating its performance on benchmark domains with numeric fluents.

## Limitations

- The algorithm's performance degrades significantly on domains with complex universal effects, showing solver errors and high resource consumption
- Safety guarantees rely on noise-free, fully observable trajectories which may not hold in real-world scenarios
- While theoretically tractable under bounded antecedent size, the practical expressiveness limits remain unclear

## Confidence

- **High Confidence:** The polynomial complexity analysis under bounded antecedent size is well-supported by the theoretical proofs and follows from standard complexity arguments.
- **Medium Confidence:** The safety guarantee is formally proven but depends on ideal conditions (noise-free observations) that may not be realistic.
- **Low Confidence:** The practical scalability of the algorithm to complex domains with universal effects is demonstrated through limited experiments, and the specific implementation details for handling universal quantifiers are not fully specified.

## Next Checks

1. **Scalability Test:** Apply Conditional-SAM to domains with increasing complexity of universal effects (beyond CityCar and Briefcase) to assess practical limits of the algorithm.
2. **Robustness Evaluation:** Test the algorithm's performance under noisy or incomplete observations to evaluate the practical value of the safety guarantee.
3. **Expressiveness Analysis:** Systematically vary the bound on antecedent size and measure the trade-off between tractability and the ability to capture complex conditional effects.