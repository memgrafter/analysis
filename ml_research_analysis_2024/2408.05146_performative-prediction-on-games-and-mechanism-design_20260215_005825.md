---
ver: rpa2
title: Performative Prediction on Games and Mechanism Design
arxiv_id: '2408.05146'
source_url: https://arxiv.org/abs/2408.05146
tags:
- accuracy
- prediction
- performative
- agents
- success
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel performative prediction framework
  in which agents are inherently interdependent, as opposed to previous studies that
  focused on independent agents. The authors study a collective risk dilemma game,
  where agents decide to cooperate or defect based on predictions of others' actions.
---

# Performative Prediction on Games and Mechanism Design

## Quick Facts
- arXiv ID: 2408.05146
- Source URL: https://arxiv.org/abs/2408.05146
- Authors: António Góis; Mehrnaz Mofakhami; Fernando P. Santos; Gauthier Gidel; Simon Lacoste-Julien
- Reference count: 25
- Key outcome: Introduces a novel performative prediction framework where agents are interdependent, showing that maximizing prediction accuracy can lead to suboptimal social welfare outcomes in collective risk dilemma games.

## Executive Summary
This work explores performative prediction in multi-agent settings where agents' decisions depend on predictions of others' actions. Unlike previous studies focusing on independent agents, this research examines a collective risk dilemma game where agents decide to cooperate or defect based on predicted actions of others. The authors demonstrate that predictors optimizing for accuracy can induce low-cooperation equilibria, while welfare-maximizing predictors can achieve better social outcomes at the cost of prediction accuracy. By assuming knowledge of a Bayesian agent behavior model, they show how to achieve optimal trade-offs between accuracy and social welfare, with implications for mechanism design.

## Method Summary
The authors model a collective risk dilemma game where agents update their trust in predictions based on accuracy, then best-respond to predicted actions when trust is high. They use neural network predictors with different architectures (MLP, GNN, GNN+MLP) and optimize for either accuracy or social welfare using differentiable approximations of discrete agent decisions. The method involves a predictor neural network taking previous actions as input, agent decision modules computing best responses, trust update modules adjusting based on accuracy, and social welfare calculators aggregating group success. They employ gradient-based optimization using a decomposition of gradients into accuracy and steering components.

## Key Results
- Accuracy-maximizing predictors empirically induce low-cooperation states in collective risk dilemma games
- Welfare-maximizing predictors can prevent low-cooperation equilibria but at the expense of prediction accuracy
- Graph Neural Networks alone fail to optimize for welfare because they cannot differentiate outputs for structurally similar nodes
- Hybrid architectures combining GNNs with MLPs perform best when optimizing for social welfare

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Performative predictions can induce low-cooperation equilibria when accuracy is the sole objective
- Mechanism: Agents update their trust in predictions based on accuracy. When trust is high, agents follow predicted actions through a Bayesian decision rule. If the prediction suggests defection is optimal, agents defect, creating a self-fulfilling prophecy that minimizes social welfare.
- Core assumption: Agents follow a Bayesian update rule and best-respond to predicted actions when trust is high
- Evidence anchors:
  - [abstract] "maximizing prediction accuracy can lead to suboptimal social welfare outcomes"
  - [section 3.1] "accuracy maximizers empirically induce low-cooperation states"
  - [corpus] No direct evidence in corpus
- Break condition: If agents ignore predictions entirely (τt,i → 0) or if the prediction model is removed from the system

### Mechanism 2
- Claim: Welfare maximization requires balancing prediction accuracy with steering toward cooperation
- Mechanism: The gradient decomposition shows that welfare optimization requires both accuracy (maintaining trust) and steering (pushing predictions toward higher cooperation when trust exists). This creates a trade-off between accuracy and social welfare.
- Core assumption: The predictor has access to the inner behavior of agents for backpropagation
- Evidence anchors:
  - [section 3.2] "gradient can be decomposed in two components: accuracy and steering"
  - [section 3.2] "a predictor maximizing welfare can prevent this, but at the expense of accuracy"
  - [corpus] No direct evidence in corpus
- Break condition: If trust becomes zero or if the differentiable approximations fail to capture agent behavior accurately

### Mechanism 3
- Claim: Graph Neural Networks fail to optimize for welfare because they cannot differentiate outputs for structurally similar nodes
- Mechanism: GNNs output identical predictions for nodes with identical local neighborhoods. In scenarios requiring asymmetric strategies (one node cooperates while another defects), GNNs cannot provide the necessary differentiation, preventing optimal welfare outcomes.
- Core assumption: GNNs inherently cannot output different values for structurally equivalent nodes
- Evidence anchors:
  - [section 3.2] "GNNs alone, being the only model unable to do centralized coordination, are not able to promote cooperation"
  - [section 3.2] "some settings may require one to cooperate and the other to defect. A GNN however is unable to provide different outputs"
  - [corpus] No direct evidence in corpus
- Break condition: If the graph structure changes to eliminate symmetric nodes or if the coordination requirement is removed

## Foundational Learning

- Concept: Performative prediction
  - Why needed here: Understanding how predictions influence the very outcomes they aim to forecast is central to this work
  - Quick check question: If a prediction of high cooperation causes agents to actually cooperate, does this make the prediction more or less accurate?

- Concept: Bayesian updating in multi-agent systems
  - Why needed here: Agents dynamically adjust trust in predictions based on observed accuracy, requiring Bayesian reasoning
  - Quick check question: If a prediction has been 90% accurate historically, what is the probability an agent will trust it in the next round?

- Concept: Collective risk dilemma games
  - Why needed here: The CRD provides the game-theoretic foundation for studying how individual defection decisions aggregate to group outcomes
  - Quick check question: In a CRD with threshold T=0.5 and group size 4, how many cooperators are needed to avoid collective loss?

## Architecture Onboarding

- Component map: Predictor → Trust Update → Agent Decision → Social Welfare
- Critical path: Predictor neural network takes previous actions as input and outputs predictions, which influence agent decisions through trust updates, ultimately affecting social welfare outcomes
- Design tradeoffs:
  - Accuracy vs. welfare: Maximizing one often comes at the expense of the other
  - Model complexity: Simple MLPs vs. GNNs vs. hybrid architectures
  - Differentiability: Using sigmoid approximations for step functions enables gradient-based optimization
- Failure signatures:
  - All-defection equilibrium indicates accuracy-maximization without welfare consideration
  - Random trust patterns suggest poor prediction accuracy or unstable agent behavior
  - Symmetrical predictions from GNNs when asymmetric strategies are needed
- First 3 experiments:
  1. Run accuracy maximization on a small clique graph and observe the resulting cooperation level
  2. Switch to welfare maximization and measure the accuracy-welfare tradeoff
  3. Test different architectures (MLP, GNN, GNN+MLP) for welfare optimization on the same graph

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the model's predictions and outcomes change if agents had heterogeneous initial trust levels (τ0) or internal beliefs (α)?
- Basis in paper: [explicit] The paper assumes agents are initialized with prior τ0 = 1 and ignores their internal beliefs α in the simplified setting, but this is a simplifying assumption.
- Why unresolved: The paper only analyzes the simplified setting with homogeneous initial trust and no internal beliefs, but acknowledges this as a limitation.
- What evidence would resolve it: Running simulations with varying initial trust levels and internal beliefs, and comparing the resulting prediction accuracy and social welfare trade-offs.

### Open Question 2
- Question: What are the computational limitations of the differentiable proxy approaches used for optimizing social welfare, and how do they scale with larger populations?
- Basis in paper: [inferred] The paper uses differentiable approximations for agent decision rules and social welfare to enable gradient-based optimization, but doesn't discuss computational complexity.
- Why unresolved: The paper focuses on empirical results for small populations (20 nodes) and doesn't analyze the computational efficiency of the optimization approach.
- What evidence would resolve it: Analyzing the time and memory complexity of the optimization procedure, and testing it on larger population sizes to identify scaling bottlenecks.

### Open Question 3
- Question: How robust are the welfare-maximizing predictors to inaccuracies in the assumed Bayesian agent behavior model?
- Basis in paper: [explicit] The paper assumes knowledge of a Bayesian agent behavior model to achieve better accuracy-welfare trade-offs, but doesn't discuss model misspecification.
- Why unresolved: The paper takes the Bayesian model as given and doesn't explore what happens if the actual agent behavior deviates from the assumed model.
- What evidence would resolve it: Introducing model misspecification in simulations (e.g., using a different decision rule than assumed) and measuring the impact on prediction accuracy and social welfare outcomes.

## Limitations

- The core assumption that the predictor has access to the inner behavior of agents, enabling backpropagation through the entire system, may not hold in real-world applications where agent behavior is not transparent or differentiable
- The focus on accuracy maximization leading to low cooperation, while theoretically sound, may not fully capture the complexity of real multi-agent systems where other factors influence behavior
- The use of differentiable approximations of discrete agent decisions using sigmoid functions may not accurately represent real agent behavior in practical applications

## Confidence

- **High Confidence**: The mechanism by which accuracy maximization leads to low-cooperation equilibria (Mechanism 1) is well-supported by the theoretical framework and empirical results in Section 3.1
- **Medium Confidence**: The welfare maximization trade-off and gradient decomposition (Mechanism 2) are logically sound but rely heavily on the differentiable approximations working as intended in practice
- **Medium Confidence**: The claim about GNN limitations (Mechanism 3) is supported by empirical evidence but may be architecture-specific and could change with different implementations

## Next Checks

1. Test the sensitivity of results to the sigmoid approximation parameters (steepness) used to make agent decisions differentiable, to ensure findings are robust to these approximations

2. Validate the welfare maximization approach on larger, more complex graph structures to assess scalability and whether the accuracy-welfare trade-off persists in more realistic settings

3. Implement a variant where the predictor has only black-box access to agent behavior (no internal gradients) to evaluate the practical limitations of the approach and explore alternative optimization strategies