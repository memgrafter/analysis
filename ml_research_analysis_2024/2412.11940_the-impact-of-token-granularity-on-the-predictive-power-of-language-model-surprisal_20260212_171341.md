---
ver: rpa2
title: The Impact of Token Granularity on the Predictive Power of Language Model Surprisal
arxiv_id: '2412.11940'
source_url: https://arxiv.org/abs/2412.11940
tags:
- surprisal
- word
- reading
- language
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The impact of subword token granularity on language model surprisal
  has been largely overlooked in cognitive modeling, despite its direct encoding of
  statistical information about word length and frequency. This work evaluates how
  different token granularities influence surprisal's predictive power for both naturalistic
  reading times and garden-path constructions.
---

# The Impact of Token Granularity on the Predictive Power of Language Model Surprisal

## Quick Facts
- arXiv ID: 2412.11940
- Source URL: https://arxiv.org/abs/2412.11940
- Authors: Byung-Doh Oh; William Schuler
- Reference count: 19
- Primary result: Token granularity substantially impacts language model surprisal quality for cognitive modeling, with different granularities optimal for broad-coverage comprehension versus garden-path effects

## Executive Summary
This work investigates how subword token granularity influences language model surprisal's predictive power for both naturalistic reading times and garden-path constructions. Through regression experiments across multiple reading time corpora, the authors find that tokens defined by a vocabulary size of 8,000 yield the most predictive surprisal for broad comprehension. However, coarser-grained tokens result in higher surprisal for garden-path critical regions, suggesting greater sensitivity to syntactic ambiguity. The findings demonstrate that token granularity is a crucial factor in language model-based cognitive modeling, with optimal granularity levels depending on the specific linguistic phenomenon being studied.

## Method Summary
The study trains unigram language model tokenizers on Wiki-40B with vocabulary sizes ranging from 256 to 128,000 tokens. For each tokenizer, Mamba-2 language models of three sizes (Small, Medium, Large) are trained on the same corpus. Word-level surprisal is calculated with whitespace correction and evaluated through linear mixed-effects regression on five reading time corpora (Natural Stories, Brown, GECO, Dundee, Provo) and garden-path constructions (MV/RR, NP/S, NP/Z). The predictive power is measured through increases in regression model log-likelihood (∆LogLik) for naturalistic reading and estimated garden-path effects (GPE) for syntactic ambiguity.

## Key Results
- Tokens with vocabulary size of 8,000 produced the most predictive surprisal for naturalistic reading times
- Coarser-grained tokens assigned higher surprisal to critical regions of garden-path constructions
- Large language models showed much smaller differences in perplexity and ∆LogLik across different vocabulary sizes compared to smaller models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subword tokenization granularity directly encodes word length and frequency statistics, biasing surprisal estimates
- Mechanism: Finer granularity splits infrequent, longer words into many tokens, drastically lowering their per-token probability under uniform initialization; coarser granularity keeps such words intact, assigning them equal probability to common words
- Core assumption: Initial uniform token probabilities are the dominant source of surprisal bias before LM training
- Evidence anchors:
  - [abstract] "tokens defined by a vocabulary size of 8,000 resulting in surprisal that is most predictive"
  - [section] "a tokenizer with a very fine token granularity ... tokenizes the word journey into seven tokens, ... six magnitudes lower than that of to"
  - [corpus] No direct corpus support; inferred from controlled tokenizer training
- Break condition: If training data is heavily biased toward rare, long words, the initial granularity bias may be overridden early in training

### Mechanism 2
- Claim: Granularity shapes learned token representations, affecting word-to-word association quality
- Mechanism: Coarse tokens approximate full-word embeddings, preserving lexical co-occurrence statistics; fine tokens fragment words, forcing the LM to aggregate across multiple vectors for the same lexical relation
- Core assumption: Vector representation quality is a bottleneck for predicting distant word associations
- Evidence anchors:
  - [abstract] "tokens defined by a coarser granularity ... assigned higher surprisal to critical regions of garden-path constructions"
  - [section] "the LM would need to attend to seven separate vector representations for journey to predict travel later in the sequence"
  - [corpus] No direct corpus support; inference from architectural analysis
- Break condition: If the model uses cross-attention or residual connections that can merge fragmented representations, the granularity effect may diminish

### Mechanism 3
- Claim: Token granularity interacts with model size, altering how initial biases persist through training
- Mechanism: Small LMs retain granularity-induced biases; large LMs learn to compensate, reducing granularity impact on surprisal predictions
- Core assumption: Larger parameter counts enable more flexible redistribution of probability mass across tokens
- Evidence anchors:
  - [abstract] "different levels of granularity being more appropriate for modeling broad-coverage comprehension versus garden-path effects"
  - [section] "Large LMs show much smaller differences in both perplexity and ∆LogLik across different vocabulary sizes"
  - [corpus] No direct corpus support; inferred from comparative LM experiments
- Break condition: If the training objective or data distribution is highly constraining, even large LMs may fail to overcome granularity bias

## Foundational Learning

- Concept: Subword tokenization schemes (BPE, ULM)
  - Why needed here: They determine the vocabulary size and thus the granularity that feeds into LM training and surprisal calculation
  - Quick check question: How does a vocabulary size of 8,000 differ in token sequences from sizes of 256 or 128,000 for the same sentence?

- Concept: Autoregressive language modeling with surprisal
  - Why needed here: Surprisal (-log P(word|context)) is the metric linking LM predictions to human reading times
  - Quick check question: If a word is split into 7 tokens, how does that affect its surprisal relative to a single-token word under uniform initialization?

- Concept: Linear mixed-effects regression for psycholinguistic modeling
  - Why needed here: It quantifies surprisal's predictive power over reading times while controlling for word length, frequency, and spillover effects
  - Quick check question: What baseline predictors are included to isolate surprisal's unique contribution to reading time variance?

## Architecture Onboarding

- Component map: ULM tokenizer (Kudo, 2018) -> vocabulary of size V -> Mamba-2 LM (Dao & Gu, 2024) with SSM layers -> word probability aggregation -> corrected for leading whitespace -> LME regression (R package lme4) -> fit and held-out partitions

- Critical path:
  1. Train ULM tokenizers at V ∈ {256, 512, ..., 128000}
  2. For each V, train Mamba-2 LMs of three sizes (Small/Medium/Large)
  3. Tokenize corpora, compute surprisal, fit LME models
  4. Evaluate ∆LogLik on exploratory partition and GPE on garden-path stimuli

- Design tradeoffs:
  - Smaller V → longer token sequences, higher sequence length cost
  - Larger V → more word-like tokens, but risk of data sparsity
  - Mamba-2 vs. Transformer: better sequence handling but less mature tooling

- Failure signatures:
  - Perplexity flat across V → tokenization has minimal impact
  - LME convergence errors → insufficient random effect variance
  - GPE magnitude far below human baseline → tokenization cannot fully bridge cognitive gap

- First 3 experiments:
  1. Verify that initial surprisal bias follows the predicted inverse relationship with V
  2. Measure whether Small LMs retain this bias after training on Wiki-40B
  3. Compare raw surprisal differences (not just GPE) at critical garden-path words across V

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal token granularity for predicting naturalistic reading times compare to that for modeling syntactic garden-path effects?
- Basis in paper: [explicit] The paper found that finer-grained tokens (|V|=8,000) were most predictive for naturalistic reading times, while coarser-grained tokens resulted in higher surprisal for garden-path constructions
- Why unresolved: The paper notes that these are opposite trends but doesn't provide a theoretical explanation for why different levels of granularity would be optimal for these different phenomena
- What evidence would resolve it: Experiments directly comparing the same token granularities on both naturalistic and garden-path datasets, potentially with additional analyses of what linguistic information is encoded at different granularities

### Open Question 2
- Question: Does the advantage of medium-sized LMs (Small/Medium) over larger LMs persist when using larger naturalistic corpora for training?
- Basis in paper: [explicit] The paper found that smaller LMs showed stronger effects of token granularity, while Large LMs showed much smaller differences across vocabulary sizes
- Why unresolved: The experiments used a fixed training corpus size (1M articles), so it's unclear if the reduced sensitivity to granularity in Large LMs would persist with more data
- What evidence would resolve it: Training Large LMs on corpora of varying sizes and comparing their sensitivity to token granularity

### Open Question 3
- Question: How would character-level LMs perform on garden-path constructions compared to subword-based LMs?
- Basis in paper: [inferred] The paper discusses how token granularity affects the ability to learn word-to-word associations, with coarser-grained tokens facilitating this, and mentions prior work on character models for naturalistic reading
- Why unresolved: The paper focuses on subword tokenizers but doesn't directly compare to character-level models, especially for the syntactic garden-path phenomenon
- What evidence would resolve it: Training and evaluating character-level LMs on the same garden-path stimuli used in the paper

## Limitations
- The initial uniform probability bias may be overwritten during training on large-scale data
- Results are limited to a single tokenizer type (ULM) and LM architecture (Mamba-2)
- Reading time datasets vary substantially in preprocessing and analysis methods
- Garden-path constructions represent a limited set of syntactic ambiguity types

## Confidence
- High Confidence: The core finding that token granularity significantly affects surprisal predictive power is well-supported by regression experiments across five reading time corpora
- Medium Confidence: The mechanisms proposed for why granularity affects surprisal are plausible but not directly tested
- Low Confidence: The claim about large LMs compensating for granularity bias is inferred from comparative patterns rather than direct analysis of learned representations

## Next Checks
1. Design experiments to directly test whether initial probability bias or learned representation quality drives granularity effects
2. Replicate the core findings using BPE tokenization and transformer-based LMs to establish architecture-independent effects
3. Expand garden-path experiments to include additional ambiguity types and measure whether granularity effects vary systematically across ambiguity categories