---
ver: rpa2
title: Memorized Images in Diffusion Models share a Subspace that can be Located and
  Deleted
arxiv_id: '2406.18566'
source_url: https://arxiv.org/abs/2406.18566
tags:
- memorized
- subset
- memorization
- neurons
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to identify and mitigate memorization
  in diffusion models by localizing memorized content to a small subspace of neurons.
  The authors contrast neuron activations for memorized vs non-memorized prompts,
  revealing that memorization activates a common subspace across different prompt
  sets.
---

# Memorized Images in Diffusion Models share a Subspace that can be Located and Deleted

## Quick Facts
- arXiv ID: 2406.18566
- Source URL: https://arxiv.org/abs/2406.18566
- Authors: Ruchika Chavhan, Ondrej Bohdal, Yongshuo Zong, Da Li, Timothy Hospedales
- Reference count: 8
- Primary result: Memorization in diffusion models can be localized to a small subspace of neurons and eliminated through pruning without retraining

## Executive Summary
This paper presents a method to identify and mitigate memorization in diffusion models by localizing memorized content to a small subspace of neurons. The authors contrast neuron activations for memorized vs non-memorized prompts, revealing that memorization activates a common subspace across different prompt sets. They then apply a pruning-based approach to eliminate these memorized neurons without retraining. Experiments show that pruning reduces memorization (lower SSCD) while maintaining image quality (comparable CLIP scores and FID), and the pruned models are more robust to extraction attacks. The method is efficient as it does not require inference-time modifications or retraining, and the results generalize across different diffusion models.

## Method Summary
The method involves collecting neuron activations for memorized and null prompts across denoising steps and layers in a pre-trained Stable Diffusion model. Using the Wanda pruning method, importance scores are computed for feed-forward network weights based on neuron activations. Memorized neurons are identified where scores exceed those for null prompts, specifically in the second linear layer of GEGLU activations in UNet blocks. These neurons are then pruned through weight zeroing, and the model is evaluated on test prompts using SSCD for memorization, CLIP score for prompt alignment, and FID for generation quality. The approach does not require retraining or inference-time modifications.

## Key Results
- Pruning identified memorized neurons reduces memorization (lower SSCD) while maintaining generation quality (comparable CLIP scores and FID)
- Pruned models are more robust to training data extraction attacks compared to unpruned models
- The method generalizes across different diffusion models beyond Stable Diffusion v1.5
- Memorization is localized to a small subspace that can be efficiently identified and eliminated

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memorization in diffusion models can be localized to a small subspace of neurons.
- Mechanism: Different sets of memorized prompts activate a common subspace of neurons in feed-forward layers, specifically in the second linear layer of the GEGLU activation in UNet blocks. This shared subspace can be identified by comparing Wanda importance scores between memorized and non-memorized prompts.
- Core assumption: Memorization is not uniformly distributed across all neurons but concentrated in specific regions that can be isolated and identified.
- Evidence anchors:
  - [abstract] "Experiments reveal a surprising finding: many different sets of memorized prompts significantly activate a common subspace in the model"
  - [section] "We coin the term memorized neurons to represent these neurons. More interestingly, the set of memorized neurons identified for different subsets of memorized prompts are highly overlapped"
  - [corpus] Weak evidence - related papers discuss memorization localization but don't provide direct evidence for the subspace hypothesis
- Break condition: If different subsets of memorized prompts activate completely disjoint neuron sets, the subspace hypothesis would fail.

### Mechanism 2
- Claim: Pruning the identified memorized neurons reduces memorization without requiring retraining.
- Mechanism: By calculating importance scores using Wanda pruning method (element-wise product of weight magnitude and input feature ℓ2 norm), neurons that disproportionately contribute to memorized prompts can be identified. Pruning these neurons through weight zeroing eliminates their contribution to memorization.
- Core assumption: Removing memorized neurons will not significantly degrade general image generation quality since these neurons are specialized for memorization rather than general denoising.
- Evidence anchors:
  - [section] "Our approach involves posthoc surgery, wherein we selectively prune regions in weight space that act on these memorized neurons"
  - [section] "we showcase the robustness of the pruned model against training data extraction attacks"
  - [corpus] Weak evidence - pruning is discussed in related works but not specifically for memorization in diffusion models
- Break condition: If pruning memorized neurons causes significant degradation in image quality or FID scores, the pruning mechanism would fail.

### Mechanism 3
- Claim: The pruned model remains robust to extraction attacks and maintains generation quality.
- Mechanism: After pruning memorized neurons, the model loses its ability to exactly reproduce training images when prompted with memorized text, while retaining the capability to generate high-quality novel images. This is evidenced by improved SSCD (lower memorization) and maintained CLIP scores and FID.
- Core assumption: Memorization and general generation capabilities are partially separable, allowing selective removal of memorization without destroying generation ability.
- Evidence anchors:
  - [abstract] "we showcase the robustness of the pruned model against training data extraction attacks"
  - [section] "pruned models not only mitigate memorization but also retain their general image generation capabilities as evidenced by the low FID on the COCO30k dataset"
  - [corpus] Weak evidence - extraction attacks are mentioned in related papers but not specifically for pruned models
- Break condition: If extraction attacks still succeed on pruned models or if FID significantly degrades, the robustness mechanism would fail.

## Foundational Learning

- Concept: Diffusion models and latent space representation
  - Why needed here: Understanding how diffusion models work in latent space is crucial for grasping why pruning feed-forward layers affects memorization
  - Quick check question: How does operating in latent space differ from operating in pixel space, and why is this advantageous for diffusion models?

- Concept: Classifier-free guidance and its role in memorization
  - Why needed here: The paper leverages the insight that memorized prompts cause text-conditional predictions to dominate, which is key to identifying memorized neurons
  - Quick check question: How does the relationship |ϵθ(zt, t, p)| ≫ |ϵθ(zt, t)| indicate memorization, and why does this happen?

- Concept: Neural network pruning techniques and importance scoring
  - Why needed here: The Wanda pruning method is repurposed to identify memorized neurons based on collective weight and feature magnitude effects
  - Quick check question: What is the intuition behind using the element-wise product of weight magnitude and input feature ℓ2 norm for importance scoring?

## Architecture Onboarding

- Component map: Stable Diffusion v1.5 model → UNet blocks → Feed-forward networks (FFN) → GEGLU activation → Second linear layer → Wanda importance scoring → Neuron pruning → Evaluation

- Critical path: 1) Collect activations for memorized vs non-memorized prompts 2) Calculate Wanda importance scores 3) Identify memorized neurons through score comparison 4) Prune identified neurons 5) Evaluate on test prompts 6) Test robustness against extraction attacks

- Design tradeoffs: The approach trades computational overhead during pruning (collecting activations, calculating scores) for zero inference-time overhead, unlike inference-time mitigation methods. It also assumes that memorized neurons can be safely removed without retraining.

- Failure signatures: High FID after pruning indicates loss of general generation capability. High SSCD after pruning indicates failure to mitigate memorization. Low IOU between different subsets suggests memorization is not localized to a common subspace.

- First 3 experiments:
  1. Replicate the IOU analysis across different time steps and layers to verify that different subsets activate similar neuron sets
  2. Perform pruning on a small subset of prompts and evaluate SSCD and CLIP score changes
  3. Test extraction attack robustness before and after pruning on a fine-tuned model with duplicated images

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- The subspace hypothesis, while compelling, lacks rigorous testing across maximally diverse prompt sets to confirm whether overlap is truly necessary for memorization
- The Wanda pruning method's effectiveness is demonstrated but not compared against alternative importance scoring methods, leaving open questions about generalizability
- The theoretical explanation for why classifier-free guidance leads to memorization dominance is largely intuitive rather than rigorously derived

## Confidence
**High Confidence:** The experimental evidence for SSCD reduction and maintained generation quality (CLIP/FID) after pruning is robust, with multiple experiments showing consistent results across different prompt sets and evaluation metrics. The extraction attack resistance demonstration provides strong empirical support for the practical effectiveness of the approach.

**Medium Confidence:** The claim that memorization is localized to a common subspace is supported by IOU analysis but could be strengthened by testing with more diverse prompt sets or by ablation studies showing that pruning non-memorized neurons does not affect memorization. The assumption that memorized neurons can be safely removed without retraining is reasonable but not rigorously proven across different model architectures.

**Low Confidence:** The theoretical explanation for why classifier-free guidance leads to memorization dominance is largely intuitive rather than rigorously derived, and the paper does not explore whether alternative guidance mechanisms might avoid this issue entirely. The efficiency claims, while supported by the lack of inference-time overhead, do not account for the computational cost of the pruning process itself.

## Next Checks
1. **Subspace Generalization Test:** Evaluate IOU across 5-10 different prompt subsets including completely unrelated prompt categories (e.g., combining artistic prompts with medical imaging prompts) to determine whether the subspace hypothesis holds across maximally diverse memorization scenarios.

2. **Alternative Pruning Method Comparison:** Implement and compare against alternative importance scoring methods (e.g., Taylor expansion-based scores or gradient-based methods) to establish whether the Wanda method is optimal or merely sufficient for memorization mitigation.

3. **Long-term Generation Stability:** Conduct a longitudinal study evaluating FID and CLIP scores over 10,000+ generated images to detect potential degradation or mode collapse that might emerge from sustained pruning effects.