---
ver: rpa2
title: Meta-Task Prompting Elicits Embeddings from Large Language Models
arxiv_id: '2402.18458'
source_url: https://arxiv.org/abs/2402.18458
tags:
- sentence
- task
- embeddings
- arxiv
- metaeol
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MetaEOL is a novel unsupervised text embedding method that generates
  high-quality sentence embeddings from large language models (LLMs) without requiring
  model fine-tuning. The method leverages meta-task prompting to guide LLMs in producing
  embeddings through a series of carefully designed prompts that address multiple
  representational aspects.
---

# Meta-Task Prompting Elicits Embeddings from Large Language Models

## Quick Facts
- arXiv ID: 2402.18458
- Source URL: https://arxiv.org/abs/2402.18458
- Reference count: 30
- Primary result: Unsupervised LLM-based embeddings that outperform contrastive-trained models on STS benchmarks

## Executive Summary
MetaEOL is a novel unsupervised text embedding method that generates high-quality sentence embeddings from large language models (LLMs) without requiring model fine-tuning. The method leverages meta-task prompting to guide LLMs in producing embeddings through a series of carefully designed prompts that address multiple representational aspects. Experiments demonstrate that embeddings averaged from various meta-tasks yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. The approach offers a versatile and resource-efficient solution for embedding generation across diverse scenarios, with incremental integration of more meta-tasks leading to consistent improvements.

## Method Summary
MetaEOL generates sentence embeddings by prompting LLMs with multiple task-specific templates across four meta-tasks: Text Classification, Sentiment Analysis, Paraphrase Identification, and Information Extraction. Each prompt includes an "in one word" constraint that forces the LLM to compress sentence meaning into a single token representation. The method extracts embeddings from specific transformer layers (determined by a proportional layer selection strategy) and averages them across all meta-tasks. The approach requires no fine-tuning and works with various LLM sizes (7B, 13B, 70B parameters), though it does require multiple LLM inferences per sentence.

## Key Results
- MetaEOL achieves 84.7% average Spearman correlation on STS tasks, outperforming PromptEOL (80.3%) and Echo embeddings (82.8%)
- Outperforms contrastive-trained models on 6 of 7 STS benchmarks while maintaining competitive performance on transfer learning tasks
- Incremental addition of meta-tasks consistently improves performance across all tested STS datasets
- Demonstrates that last-layer embeddings are not always optimal, with layer -8 performing best for MetaEOL

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Meta-task prompting allows LLMs to generate embeddings that capture diverse semantic aspects without requiring model fine-tuning.
- **Mechanism**: By prompting the LLM with multiple task-specific templates (e.g., text classification, sentiment analysis, paraphrase identification, information extraction), the model is guided to represent sentences from multiple representational perspectives. Averaging these diverse embeddings produces a more comprehensive and general-purpose sentence representation.
- **Core assumption**: Each meta-task prompt elicits a distinct embedding that captures a unique aspect of the sentence's meaning, and these aspects are complementary rather than redundant.
- **Evidence anchors**:
  - [abstract] "leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects"
  - [section] "Our hypothesis is that each embedding captures a distinct representation customized for a specific feature viewpoint (meta-task)"
- **Break condition**: If meta-tasks are not sufficiently diverse or complementary, averaging embeddings may not improve performance over single-task approaches.

### Mechanism 2
- **Claim**: The "explicit one-word limitation" constraint forces LLMs to compress sentence meaning into a single token representation, which serves as the sentence embedding.
- **Mechanism**: By requiring the model to generate "in one word," the LLM must aggregate all relevant information from the sentence into the last token's hidden state. This creates a compact representation that captures the overall semantic content while the meta-task context guides which aspects to emphasize.
- **Core assumption**: The last token's hidden state effectively encodes the compressed meaning when constrained to one word output.
- **Evidence anchors**:
  - [abstract] "Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL)"
  - [section] "a constraint of 'in one word' is applied to avoid the model's tendency to generate long sentences such that the last token fails to capture the overall information"
- **Break condition**: If the LLM ignores the constraint or generates long sequences despite the instruction, the last token may not capture meaningful sentence information.

### Mechanism 3
- **Claim**: Increasing the number of meta-tasks leads to consistent performance improvements across STS benchmarks.
- **Mechanism**: Each additional meta-task provides a new perspective on sentence meaning. When embeddings from multiple tasks are averaged, they collectively capture more semantic dimensions, leading to better generalization across different STS datasets and downstream tasks.
- **Core assumption**: The meta-tasks are sufficiently diverse and their embeddings are complementary rather than redundant.
- **Evidence anchors**:
  - [section] "Incrementally integrating more meta-tasks (ranging from one to four) yields consistent improvements across STS tasks"
  - [section] "Table 4 shows that increasing the number of tasks leads to a consistent improvement in performance on average and nearly every individual STS task"
- **Break condition**: If additional meta-tasks become redundant or introduce noise, performance improvements may plateau or degrade.

## Foundational Learning

- **Concept**: Contrastive learning for sentence embeddings
  - **Why needed here**: Understanding why traditional approaches require contrastive training helps appreciate why MetaEOL's unsupervised approach is significant
  - **Quick check question**: Why do contrastive learning methods like SimCSE require labeled positive pairs or data augmentation strategies?

- **Concept**: Prompt engineering for LLMs
  - **Why needed here**: The effectiveness of MetaEOL depends on crafting prompts that guide LLMs to produce desired representations
  - **Quick check question**: How does the "in one word" constraint influence the LLM's generation behavior compared to open-ended prompts?

- **Concept**: Layer selection in transformer models
  - **Why needed here**: MetaEOL shows that the last layer isn't always optimal for sentence embeddings, requiring understanding of how different layers capture different semantic levels
  - **Quick check question**: What linguistic properties are typically captured by lower vs. higher layers in transformer architectures?

## Architecture Onboarding

- **Component map**: Prompt generation (GPT-4 templates) -> LLM inference (LLAMA2/Mistral) -> Layer selection (proportional strategy) -> Embedding aggregation (averaging) -> Downstream evaluation (STS + SentEval)

- **Critical path**: Prompt generation → LLM inference → Layer selection → Embedding averaging → Evaluation
- **Design tradeoffs**:
  - More meta-tasks improve performance but increase computational cost
  - Layer selection strategy balances performance vs. simplicity
  - Averaging vs. concatenation vs. max pooling for embedding combination
- **Failure signatures**:
  - Performance degrades when prompts are poorly designed or too similar
  - Layer selection mismatch causes suboptimal embeddings
  - Meta-tasks may not be complementary, leading to redundant information
- **First 3 experiments**:
  1. Baseline: Single meta-task (Text Classification only) vs. PromptEOL
  2. Incremental addition: Add Sentiment Analysis to Text Classification, measure STS performance gain
  3. Layer ablation: Test different output layers (-1, -3, -8) with MetaEOL on STS benchmarks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but suggests several directions for future research, including exploring multilingual contexts and incorporating more meta-tasks.

## Limitations
- Heavy computational cost due to requiring multiple LLM inferences per sentence
- Performance depends on quality of meta-task prompts generated by GPT-4
- Assumes meta-tasks capture complementary semantic aspects without empirical validation
- Limited evaluation to English sentence-level tasks only

## Confidence
- **High confidence**: Core claim that meta-task prompting can generate sentence embeddings from LLMs without fine-tuning is well-supported by experimental results
- **Medium confidence**: Claims about outperforming contrastive-trained models are supported but comparison methodology could be more rigorous
- **Low confidence**: Assumption that increasing meta-task diversity consistently improves performance without empirical validation of task complementarity

## Next Checks
1. **Task complementarity analysis**: Conduct correlation analysis between embeddings from different meta-tasks to quantify their semantic overlap
2. **Prompt sensitivity testing**: Systematically vary prompt formulations for each meta-task and measure performance impact
3. **Layer interpretability study**: Analyze the semantic properties captured by different transformer layers when used with MetaEOL prompts