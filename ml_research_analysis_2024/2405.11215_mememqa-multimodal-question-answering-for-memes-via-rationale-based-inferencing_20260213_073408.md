---
ver: rpa2
title: 'MemeMQA: Multimodal Question Answering for Memes via Rationale-Based Inferencing'
arxiv_id: '2405.11215'
source_url: https://arxiv.org/abs/2405.11215
tags:
- meme
- answer
- party
- because
- democratic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MemeMQA, a novel multimodal task for analyzing
  memes through question-answering with rationale-based inferencing. The authors curate
  MemeMQACorpus, a dataset of 1,880 question-answer pairs for 1,122 memes, and propose
  ARSENAL, a two-stage framework that leverages LLM-generated rationales for answer
  prediction and explanation generation.
---

# MemeMQA: Multimodal Question Answering for Memes via Rationale-Based Inferencing

## Quick Facts
- arXiv ID: 2405.11215
- Source URL: https://arxiv.org/abs/2405.11215
- Authors: Siddhant Agarwal; Shivam Sharma; Preslav Nakov; Tanmoy Chakraborty
- Reference count: 40
- Primary result: ~18% higher answer prediction accuracy vs. baselines on meme QA task

## Executive Summary
This paper introduces MemeMQA, a novel multimodal task for analyzing memes through question-answering with rationale-based inferencing. The authors curate MemeMQACorpus, a dataset of 1,880 question-answer pairs for 1,122 memes, and propose ARSENAL, a two-stage framework that leverages LLM-generated rationales for answer prediction and explanation generation. ARSENAL achieves ~18% higher answer prediction accuracy and leads in text generation quality across multiple metrics compared to competitive baselines. The framework demonstrates strong robustness to question diversity and confounding scenarios while highlighting the potential and limitations of multimodal LLMs in meme interpretation.

## Method Summary
The ARSENAL framework uses a two-stage approach for multimodal meme question answering. First, a multimodal LLM (LLaVA) generates detailed rationales from meme images and OCR text. These rationales are then used by a T5-based model to predict answers to questions about entity roles (hero, villain, victim) in memes. In the second stage, ARSENAL generates entity-specific rationales and uses T5 to produce explanations for the answers. The framework employs chain-of-thought reasoning and prompt diversification to improve robustness. The MemeMQACorpus dataset contains 1,880 QA pairs for 1,122 memes, focusing on US political memes with semantic role identification tasks.

## Key Results
- ARSENAL achieves ~18% higher answer prediction accuracy compared to competitive baselines
- Leads in explanation generation quality across multiple metrics (BLEU, ROUGE, METEOR, CHRF, BERTScore)
- Demonstrates strong robustness to question diversity through LLM-generated paraphrases
- Successfully handles confounding scenarios involving textual/visual mismatches and role inversion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Leveraging multimodal LLM-generated rationales improves both answer accuracy and explanation quality in meme interpretation.
- Mechanism: The system uses a multimodal LLM (LLaVA) to generate detailed, context-rich rationales from meme images, which are then used to guide a T5-based answer prediction module and an explanation generation module.
- Core assumption: The multimodal LLM can capture semantic nuances and implicit meanings in memes that are not apparent from OCR text alone, providing valuable contextual cues for downstream tasks.
- Evidence anchors:
  - [abstract] "ARSENAL, a novel two-stage multimodal framework that leverages the reasoning capabilities of LLMs to address MemeMQA"
  - [section] "We propose a multi-stage setup for ARSENAL to leverage individual strengths of MM-CoT and multimodal LLMs towards the overall objective of MemeMQA"
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.393, average citations=0.0" (Weak evidence; related work exists but no direct citations)
- Break condition: The multimodal LLM fails to generate accurate or relevant rationales, especially for memes with high visual-linguistic incongruity or subtle humor.

### Mechanism 2
- Claim: A two-stage framework with chain-of-thought reasoning outperforms one-stage methods in meme question answering.
- Mechanism: The first stage uses a multimodal CoT model with a QCM→LE prompt followed by QCMG→A to generate an answer. The second stage uses the predicted answer to generate an entity-specific rationale via LLaVA, which is then summarized by T5 to produce a concise explanation.
- Core assumption: Complex meme interpretation benefits from intermediate reasoning steps, and the intermediate rationale bridges the gap between raw meme content and structured answer/explanation generation.
- Evidence anchors:
  - [abstract] "ARSENAL achieves ~18% higher answer prediction accuracy and leads in text generation quality"
  - [section] "Our findings corroborate the applicability of the two-stage framework for MemeMQA"
  - [corpus] (No direct evidence; inference based on methodology description)
- Break condition: The intermediate rationale becomes a bottleneck, either due to noise or irrelevance, degrading downstream performance.

### Mechanism 3
- Claim: Prompting for question diversification improves model robustness to varied natural language queries.
- Mechanism: The system uses Llama-2-7b-chat to generate five paraphrases of each structured question, randomly selecting one to replace the original, thereby simulating diverse user questioning styles.
- Core assumption: MemeMQA models need to handle real-world variability in question phrasing, and synthetic diversification exposes robustness gaps not visible in structured datasets.
- Evidence anchors:
  - [abstract] "We analyze ARSENAL's robustness through diversification of question-set"
  - [section] "In our analysis, we evaluate the performance of ARSENAL and current baselines using more naturally framed questions"
  - [corpus] (No direct evidence; methodology detail only)
- Break condition: The LLM-generated paraphrases introduce semantic drift, causing the model to misinterpret the intended question.

## Foundational Learning

- Concept: Multimodal representation learning (vision + language)
  - Why needed here: Memes are inherently multimodal; understanding them requires integrating visual and textual cues that often convey complementary or even contradictory meanings.
  - Quick check question: Can you explain why a meme's humor might be lost if only the OCR text is considered?

- Concept: Chain-of-thought (CoT) reasoning in multimodal settings
  - Why needed here: Memes require complex reasoning that often involves common sense, cultural context, and inference across modalities; CoT provides intermediate reasoning steps to support this.
  - Quick check question: How does generating an intermediate rationale before the final answer help with meme interpretation?

- Concept: Text generation evaluation metrics (BLEU, ROUGE, METEOR, BERTScore, CHRF)
  - Why needed here: The task involves both answer selection and explanation generation; these metrics assess lexical and semantic alignment between generated and ground-truth explanations.
  - Quick check question: Why might BLEU-1 be higher than BLEU-4 for meme explanations?

## Architecture Onboarding

- Component map: Meme image + OCR text + question → LLaVA rationale → MM-CoT answer prediction → LLaVA entity-specific rationale → T5 explanation
- Critical path: LLaVA rationale → MM-CoT answer prediction → LLaVA entity-specific rationale → T5 explanation
- Design tradeoffs:
  - Modality fusion: Visual embeddings via DETR + gated cross-attention vs. end-to-end multimodal models
  - Rationale granularity: Generic vs. entity-specific rationales
  - Model size: Large multimodal LLMs for reasoning vs. smaller, fine-tuned models for efficiency
- Failure signatures:
  - Low accuracy but high BLEU → Model memorizes patterns but fails on reasoning
  - High accuracy but low explanation quality → Model picks correct answer but cannot justify it
  - High variance across runs → Sensitivity to prompt or initialization
- First 3 experiments:
  1. Validate OCR quality and impact on multimodal baselines
  2. Test one-stage vs. two-stage prompting configurations with unifiedqa-t5-base
  3. Evaluate ARSENAL with generic rationale vs. no rationale baseline

## Open Questions the Paper Calls Out
- How does ARSENAL's performance change when evaluated on memes from domains beyond US politics, such as healthcare or entertainment?
- What is the impact of different OCR quality levels on ARSENAL's answer prediction accuracy and explanation generation quality?
- How do different multimodal LLM architectures compare to ARSENAL in handling visual-linguistic incongruity specific to memes?

## Limitations
- Dataset scale: Only 1,122 memes with 1,880 QA pairs, limiting generalization
- Narrow scope: Focus on semantic role identification may not generalize to broader meme interpretation tasks
- Synthetic evaluation: Question diversification relies on LLM paraphrasing rather than real user queries

## Confidence
- Core claim (ARSENAL superiority): Medium confidence - supported by experimental results but limited by dataset size
- Multimodal rationale mechanism: Medium confidence - ablation studies support the claim but mechanism could use more granular analysis
- Robustness to question diversity: Medium confidence - synthetic diversification may not reflect real-world query variation

## Next Checks
1. Cross-dataset Generalization Test: Evaluate ARSENAL on external meme datasets (e.g., Hateful Memes, MM-IMDb) to assess whether the performance gains generalize beyond the curated MemeMQACorpus.
2. Human Evaluation of Explanation Quality: Conduct comprehensive human evaluation studies to validate the automatic metrics (BLEU, ROUGE, BERTScore) used for explanation quality assessment.
3. Robustness Under Real Query Distribution: Replace synthetic question diversification with actual user query logs or crowd-sourced question variations to test real-world performance with diverse user questions.