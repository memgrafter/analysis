---
ver: rpa2
title: 'SegXAL: Explainable Active Learning for Semantic Segmentation in Driving Scene
  Scenarios'
arxiv_id: '2408.04482'
source_url: https://arxiv.org/abs/2408.04482
tags:
- learning
- segmentation
- active
- semantic
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SegXAL, an explainable active learning framework
  for semantic segmentation in driving scenes. The key idea is to use an Explainable
  Error Mask (EEM) that combines entropy-based uncertainty with proximity-aware explainability
  from depth estimation and GradCAM to guide oracle annotation.
---

# SegXAL: Explainable Active Learning for Semantic Segmentation in Driving Scene Scenarios

## Quick Facts
- arXiv ID: 2408.04482
- Source URL: https://arxiv.org/abs/2408.04482
- Reference count: 37
- Primary result: 65.11 mIoU on Cityscapes, outperforming state-of-the-art AL methods

## Executive Summary
SegXAL introduces an explainable active learning framework for semantic segmentation in driving scenes by combining entropy-based uncertainty with proximity-aware explainability from depth estimation and GradCAM. The framework prioritizes annotation of nearby objects through an Explainable Error Mask (EEM), which is critical for autonomous driving safety. The approach supports both machine pseudo-labels and human expert manual annotation, with the manual mode achieving the best performance. Results on Cityscapes demonstrate significant improvements over existing methods, bridging the semantic gap between humans and machines through collaborative intelligence.

## Method Summary
SegXAL is an active learning framework for semantic segmentation in driving scenes that uses a U-Net backbone with an Explainable Error Mask (EEM) module. The EEM combines entropy-based uncertainty (EBU) with proximity-aware GradCAM (PAE) from depth estimation to guide oracle annotation, prioritizing nearby objects critical for driving safety. The framework supports two annotation modes: machine pseudo-labels and human manual annotation. Sample selection uses a DICE similarity coefficient threshold to determine convergence. The method is evaluated on the Cityscapes dataset with 30 classes at 1024×2048 resolution.

## Key Results
- Achieves 65.11 mIoU on Cityscapes, outperforming state-of-the-art AL methods by significant margins
- Manual annotation mode with DINOv2 depth maps performs best, showing 0.8% increase over machine annotation
- Novel DICE similarity-based high-confidence sample selection technique enables efficient stopping criteria

## Why This Works (Mechanism)

### Mechanism 1
The EEM module improves oracle annotation efficiency by combining uncertainty and proximity explainability. It fuses entropy-based uncertainty (EBU) with proximity-aware GradCAM (PAE) to produce a heat map that prioritizes nearby objects for annotation. This works because nearby objects are more critical in driving scenes for safety. The break condition occurs if depth estimation fails or GradCAM doesn't highlight relevant regions.

### Mechanism 2
The DICE similarity threshold provides a convergence signal for stopping oracle annotation. After each annotation cycle, DICE between predicted and reannotated masks is computed; if DICE exceeds a threshold, further querying stops. This works because high DICE similarity indicates diminishing returns from additional annotation. Break conditions include setting the threshold too low (stopping too early) or too high (wasting annotation budget).

### Mechanism 3
Human-in-the-loop improves model performance over machine-only pseudo-labels. Manual annotation leverages human contextual understanding to correct machine errors, especially for object-level boundaries. This works because humans provide more accurate and semantically meaningful labels in ambiguous regions. The break condition is when manual annotation becomes too slow or error-prone, diminishing the benefit.

## Foundational Learning

- Concept: Entropy as uncertainty measure
  - Why needed here: Quantifies pixel-wise prediction confidence to guide active learning queries
  - Quick check question: If a pixel has uniform class probabilities across all classes, what is its entropy value?

- Concept: GradCAM for visual explainability
  - Why needed here: Highlights image regions influential for predictions, enabling intuitive human feedback
  - Quick check question: In GradCAM, what does the ReLU operation accomplish in the final heatmap?

- Concept: Depth estimation for proximity awareness
  - Why needed here: Identifies nearby objects in driving scenes to prioritize safety-critical annotations
  - Quick check question: How does monocular depth estimation differ from stereo depth in terms of input requirements?

## Architecture Onboarding

- Component map: U-Net backbone -> Semantic segmentation -> EEM module (EBU + PAE) -> Oracle annotation (machine/human) -> DICE-based selection -> Retraining loop
- Critical path: Segmentation prediction -> EEM computation -> Oracle annotation -> DICE filtering -> Model update
- Design tradeoffs: Manual annotation gives higher accuracy but is slower; machine annotation is faster but less precise
- Failure signatures: High entropy spread across irrelevant regions, low DICE improvement over cycles, inconsistent manual labels
- First 3 experiments:
  1. Run SegXAL with only EBU (no PAE) and measure mIoU drop
  2. Vary DICE threshold values to find optimal stopping point
  3. Compare manual vs machine annotation on a small validation set for speed vs accuracy trade-off

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of SegXAL vary with different choices of depth estimation models beyond MiDaS and DINOv2? The paper only evaluates these two models and future work mentions extending to other domains, suggesting exploration of different depth models is relevant.

### Open Question 2
What is the impact of varying the entropy-based uncertainty (EBU) and proximity-aware XAI (PAE) weights (α and β) on the performance of SegXAL? The paper mentions equal contribution was used but states weights can be made learnable, indicating the effect of different combinations is unexplored.

### Open Question 3
How does SegXAL perform on driving datasets with significantly different characteristics, such as KITTI or BDD100K, compared to Cityscapes? The paper focuses on driving scenes but only evaluates on Cityscapes, leaving performance on other driving datasets with different characteristics unknown.

## Limitations
- The DICE similarity coefficient threshold for stopping oracle annotation is not clearly specified, making exact reproduction difficult
- Weighting parameters α and β in the EEM fusion equation are mentioned but their specific values and determination method are not provided
- The practical trade-offs in annotation speed and scalability between manual and machine annotation modes are not fully explored

## Confidence

**High Confidence:** The overall framework architecture combining entropy-based uncertainty, GradCAM explainability, and depth estimation for proximity awareness is well-defined and reproducible.

**Medium Confidence:** The reported performance improvements over state-of-the-art methods are convincing, though specific implementation details of the DICE-based sample selection mechanism could affect exact results.

**Medium Confidence:** The claim that manual annotation outperforms machine pseudo-labels is supported by results, but the practical trade-offs in terms of annotation speed and scalability are not fully explored.

## Next Checks

1. Implement SegXAL with only the Entropy-Based Uncertainty (EBU) module and measure the performance drop to quantify the contribution of the Proximity-Aware Explainability (PAE) component.

2. Experiment with different DICE threshold values to identify the optimal stopping point and evaluate how sensitive the model performance is to this parameter.

3. Compare manual annotation vs. machine pseudo-labels on a small validation set to quantify the speed-accuracy trade-off and determine practical applicability.