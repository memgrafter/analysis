---
ver: rpa2
title: 'Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for Large-Scale
  Product Retrieval Evaluation'
arxiv_id: '2409.11860'
source_url: https://arxiv.org/abs/2409.11860
tags:
- query
- human
- product
- annotation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a multimodal LLM-based framework for large-scale
  product retrieval evaluation in e-commerce. The approach uses LLMs to generate query-specific
  annotation guidelines and then leverages multimodal LLMs to assess relevance between
  search queries and product images/text descriptions.
---

# Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for Large-Scale Product Retrieval Evaluation

## Quick Facts
- arXiv ID: 2409.11860
- Source URL: https://arxiv.org/abs/2409.11860
- Reference count: 19
- Key outcome: LLM-based framework achieves 65-78% human-LLM agreement on product retrieval evaluation, reducing costs from €15,000 to €70-156 for 20,000 pairs

## Executive Summary
This work introduces a multimodal LLM-based framework for large-scale product retrieval evaluation in e-commerce. The approach uses LLMs to generate query-specific annotation guidelines and then leverages multimodal LLMs to assess relevance between search queries and product images/text descriptions. Evaluated against 20,000 human annotations across German and English queries, the method achieves human-LLM agreement scores of 65-78%, comparable to human inter-annotator agreement. The framework significantly reduces annotation costs and completion time while providing reliable bulk annotation capabilities. The approach has been deployed in production for continuous evaluation of high-volume queries.

## Method Summary
The framework operates through three main steps: (1) Query extraction from search logs, followed by LLM generation of query-specific requirements and annotation guidelines, (2) Product retrieval using a search engine and MLLM-based relevance assessment that combines textual descriptions with generated visual descriptions, and (3) Caching of intermediate results in a database for efficient reuse when evaluating new search engine configurations. The MLLM annotator assigns one of three relevance labels (irrelevant, acceptable substitute, highly relevant) to each query-product pair.

## Key Results
- Human-LLM agreement scores of 65-78% on German and English queries
- Cost reduction from €15,000 to €70-156 for evaluating 20,000 query-product pairs
- Time efficiency improvement from 3 weeks to 4-30 minutes
- Error analysis shows LLMs and humans make different types of mistakes, with LLMs being more reliable for bulk annotation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal LLMs can generate query-specific annotation guidelines that improve relevance assessment accuracy.
- Mechanism: By analyzing the search query, the LLM infers semantic requirements (e.g., brand, color, product type) and generates tailored annotation guidelines that explicitly define relevance criteria for that specific query.
- Core assumption: LLMs can accurately extract semantic requirements from natural language queries and translate them into actionable annotation guidelines.
- Evidence anchors: [abstract] "leveraging Multimodal LLMs for (i) generating tailored annotation guidelines for individual queries", [section 2] "The LLM infers its requirements and their importance"
- Break condition: If the LLM fails to extract key query requirements or generates ambiguous guidelines that lead to inconsistent relevance judgments.

### Mechanism 2
- Claim: Combining textual and visual product information through multimodal LLMs improves relevance assessment compared to text-only approaches.
- Mechanism: The MLLM generates textual descriptions of product images, which are combined with existing textual descriptions to provide richer product context for relevance assessment.
- Core assumption: Visual product features contain information not captured in textual descriptions that is relevant for determining query-product relevance.
- Evidence anchors: [abstract] "leveraging Multimodal LLMs for (ii) conducting the subsequent annotation task", [section 2] "MLLM-text utilises the same textual input as 'LLM-text', while also incorporating the product image"
- Break condition: If the visual description generation introduces errors or if visual features are not relevant to the query's semantic intent.

### Mechanism 3
- Claim: Caching intermediate results in the evaluation pipeline enables efficient comparison across multiple search engine configurations.
- Mechanism: The framework stores query requirements, annotation guidelines, product descriptions, and relevance scores in a database, allowing reuse when evaluating new search engine versions.
- Core assumption: The relevance of query-product pairs remains consistent across different search engine configurations, making cached annotations reusable.
- Evidence anchors: [section 2] "This caching serves two key functions in our pipeline. Firstly, it facilitates efficient retrieval and reuse", [section 4] "When evaluating a new search engine configuration"
- Break condition: If search engine changes significantly alter the relevance landscape or if cached annotations become outdated due to product catalog changes.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Enhances the quality of (M)LLM outputs and provides reasoning steps for debugging relevance assessments
  - Quick check question: How does chain-of-thought prompting differ from standard prompting in the context of query-product relevance assessment?

- Concept: Multimodal information fusion
  - Why needed here: Combines textual and visual product information to create richer context for relevance assessment
  - Quick check question: What are the key challenges in effectively fusing multimodal product information for relevance assessment?

- Concept: Query requirement extraction
  - Why needed here: Identifies the semantic components of search queries that determine product relevance
  - Quick check question: What are the most common types of query requirements in e-commerce search, and how do they impact relevance assessment?

## Architecture Onboarding

- Component map: Query extraction module -> LLM generator -> Search engine interface -> MLLM annotator -> Database -> Evaluation module
- Critical path: Query → LLM generator → Search engine → MLLM annotator → Database → Evaluation
- Design tradeoffs:
  - Text-only vs multimodal input: Simplicity vs potential accuracy improvement
  - Real-time vs cached evaluation: Freshness vs efficiency
  - Human-in-the-loop vs fully automated: Quality vs scalability
- Failure signatures:
  - Low agreement with human annotations indicates guideline generation or relevance assessment issues
  - High evaluation time suggests inefficient caching or parallel processing
  - Inconsistent results across search engine configurations may indicate caching problems
- First 3 experiments:
  1. Compare LLM-text vs MLLM-text performance on a small sample to validate multimodal benefits
  2. Test caching efficiency by evaluating the same query-product pairs with different search engine versions
  3. Validate query requirement extraction by comparing LLM-generated requirements against human-annotated requirements on a sample dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt engineering techniques (beyond few-shot prompting) affect the accuracy and reliability of LLM-based relevance judgments?
- Basis in paper: The paper mentions using few-shot prompting but notes that the results with GPT-3.5 Turbo were "significantly worse" and suggests that different LLM architectures affect performance
- Why unresolved: The paper only tested few-shot prompting across different LLM models but did not explore alternative prompt engineering approaches
- What evidence would resolve it: Comparative experiments testing various prompt engineering techniques on the same dataset, measuring agreement rates, error types, and computational costs

### Open Question 2
- Question: What is the optimal threshold for determining when human intervention is necessary versus relying on LLM judgments for bulk annotation work?
- Basis in paper: The paper identifies "hard disagreements" where LLM and human judgments differ significantly, but doesn't establish clear criteria for when human review should be triggered
- Why unresolved: The paper identifies that LLMs and humans make different types of errors but doesn't provide a systematic framework for deciding when to use LLM-only versus human-in-the-loop approaches
- What evidence would resolve it: Empirical studies measuring cost-benefit trade-offs of different human intervention thresholds, including error rates, annotation costs, and system performance impacts

### Open Question 3
- Question: How does the performance of the proposed framework vary across different product categories and query types in e-commerce?
- Basis in paper: The paper mentions evaluating 20,000 query-product pairs but doesn't analyze performance variations across product categories or query types
- Why unresolved: The paper provides aggregate performance metrics but doesn't examine whether certain product categories or query types are more challenging for LLMs
- What evidence would resolve it: Detailed analysis of performance metrics segmented by product category, query type, and other relevant dimensions, identifying patterns in LLM accuracy and error types

## Limitations

- Framework performance depends heavily on quality of LLM-generated query requirements and annotation guidelines, which are not directly validated against human annotations
- Cost reduction estimates assume stable query-product relevance over time without evidence of how well cached annotations maintain validity across product catalog changes
- Error analysis shows LLMs and humans make different types of mistakes, suggesting the framework may systematically miss certain relevance nuances that humans catch
- The 65-78% human-LLM agreement claim needs careful interpretation since evaluation methodology differs between comparison points

## Confidence

- Cost reduction claims (from €15,000 to €70-156): High confidence - based on concrete time and monetary comparisons
- Human-LLM agreement scores (65-78%): Medium confidence - while measured, the comparison to human inter-annotator agreement may not be apples-to-apples
- Multimodal benefits over text-only: Low confidence - limited direct comparison evidence provided in corpus
- Production deployment success: High confidence - explicitly stated as deployed for continuous evaluation

## Next Checks

1. Validate guideline generation quality: Take a stratified sample of 100 query-requirement pairs and have human experts rate the accuracy and completeness of LLM-generated requirements against actual query intent. Measure precision and recall of extracted requirements.

2. Test caching validity over time: Evaluate the same query-product pairs across three time points (e.g., monthly) using both the cached annotations and fresh LLM assessments to measure annotation drift and determine how frequently cached results need refreshing.

3. Compare multimodal vs text-only on identical samples: Run a controlled experiment where the same 1,000 query-product pairs are evaluated using both LLM-text and MLLM-text approaches, measuring not just accuracy but also identifying specific query types where visual information adds value versus cases where it introduces noise.