---
ver: rpa2
title: Transfer Learning with Self-Supervised Vision Transformers for Snake Identification
arxiv_id: '2407.06178'
source_url: https://arxiv.org/abs/2407.06178
tags:
- species
- images
- snake
- image
- dinov2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the application of Meta's DINOv2 vision transformer
  model for snake species identification as part of the SnakeCLEF 2024 competition.
  The approach involved training a linear classifier on features extracted by DINOv2
  from a dataset of over 182,000 images.
---

# Transfer Learning with Self-Supervised Vision Transformers for Snake Identification

## Quick Facts
- arXiv ID: 2407.06178
- Source URL: https://arxiv.org/abs/2407.06178
- Reference count: 11
- Primary result: 39.69 score using DINOv2 vision transformer for snake species identification

## Executive Summary
This paper explores the application of Meta's DINOv2 vision transformer model for snake species identification as part of the SnakeCLEF 2024 competition. The approach involved training a linear classifier on features extracted by DINOv2 from a dataset of over 182,000 images. Despite achieving a score of 39.69, the work demonstrates the potential of using pre-trained vision models for this species classification task. The findings indicate that while DINOv2 embeddings provide a strong foundation, there are opportunities for improvement through advanced neural network architectures and image segmentation techniques to isolate relevant snake features. The initial results lay the groundwork for more refined approaches in snake identification using transfer learning, with the potential for significant performance gains by addressing current limitations and incorporating advanced segmentation methods.

## Method Summary
The method uses Meta's DINOv2 vision transformer as a frozen feature extractor for snake species identification. The pipeline extracts [CLS] token embeddings from each image using the pre-trained DINOv2 model, then trains a linear classifier on these embeddings using negative log-likelihood loss. The dataset consists of 182,261 images across 1,784 snake species, with preprocessing handled through Apache Spark and Luigi orchestration. The model is trained using PyTorch Lightning with Adam optimizer, and predictions are made by taking the mode across multiple images per observation.

## Key Results
- Achieved 39.69 score in SnakeCLEF 2024 competition
- DINOv2 embeddings show species clustering in UMAP projections
- Current implementation has indexing bug causing zero F1 and accuracy scores
- Linear classifier performance suggests room for architectural improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DINOv2's self-supervised embeddings capture high-level visual semantics that are transferable to snake species identification.
- Mechanism: DINOv2 is pre-trained on a large dataset (LVD-142M) using masked auto-encoding, learning to reconstruct masked patches without requiring labeled data. This forces the model to learn robust, generalizable visual features.
- Core assumption: The visual features learned from a broad dataset are sufficiently representative of the snake-specific features needed for species identification.
- Evidence anchors:
  - [abstract] "explore and use Meta's DINOv2 vision transformer model for feature extraction to tackle species' high variability and visual similarity in a dataset of 182,261 images"
  - [section 2] "DINOv2 demonstrates strong out-of-distribution performance, making it ideal for dealing with various previously unseen snake photographs in varied scenarios"
- Break condition: If the visual domains are too dissimilar, the pre-trained features may not generalize effectively, leading to poor downstream performance.

### Mechanism 2
- Claim: Using the [CLS] token as a summary embedding for species classification is effective because it is trained to capture global image semantics.
- Mechanism: The [CLS] token in DINOv2 is processed through the transformer layers and aggregates information from all patches, serving as a global representation of the image. This makes it suitable for classification tasks.
- Core assumption: The [CLS] token's aggregated representation is sufficiently discriminative for distinguishing between snake species.
- Evidence anchors:
  - [section 3] "The [CLS] tokens can be seen to cluster distinctly for the images of some species, such as Indotyphlops Braminus... and Bitis Gabonica"
  - [section 4.1] "We pre-compute the DINOv2 embeddings for each image, using the base pre-trained model to transform each image and extract the [CLS] token"
- Break condition: If species classes are highly intermixed in the embedding space, the [CLS] token may not provide enough discriminative power for accurate classification.

### Mechanism 3
- Claim: Dimensionality reduction techniques like UMAP can reveal meaningful structure in embedding spaces for exploratory analysis.
- Mechanism: UMAP preserves local and global structure in high-dimensional data when projecting to lower dimensions, allowing visualization of clustering patterns that correlate with species labels.
- Core assumption: The embedding space has inherent structure that reflects species similarity, which UMAP can visualize effectively.
- Evidence anchors:
  - [section 3] "We visually inspect embeddings by reducing their dimension and scatter plotting the results in two dimensions using class labels as colors"
  - [section 3] "In Figure 1 we plot UMAP projections of these embeddings, both their [CLS] tokens and the DCT coefficients of the patch tokens"
- Break condition: If the embedding space is noisy or lacks structure, UMAP projections may not reveal meaningful patterns, limiting their utility for analysis.

## Foundational Learning

- Concept: Vision Transformers (ViTs) and their attention mechanism
  - Why needed here: Understanding how ViTs process images as sequences of patches and use self-attention to capture long-range dependencies is crucial for interpreting DINOv2's feature extraction capabilities.
  - Quick check question: How does a ViT's attention mechanism differ from convolutional layers in CNNs when processing image features?

- Concept: Self-supervised learning and pre-training strategies
  - Why needed here: DINOv2 uses self-supervised learning (masked auto-encoding) to learn visual features without labels. Understanding this process is key to appreciating why pre-trained embeddings can be effective for downstream tasks.
  - Quick check question: What is the primary objective function used in DINOv2's pre-training, and how does it enable learning without labeled data?

- Concept: Dimensionality reduction and visualization techniques (e.g., UMAP, PCA)
  - Why needed here: The paper uses UMAP to visualize high-dimensional embeddings and assess their clustering properties. Understanding these techniques is essential for interpreting the exploratory analysis results.
  - Quick check question: What are the key differences between UMAP and PCA in terms of how they preserve data structure during dimensionality reduction?

## Architecture Onboarding

- Component map: Image loading → preprocessing (binary conversion, metadata joining) → feature extraction with DINOv2 → classifier training → inference
- Key components: Luigi for orchestration, Apache Spark for distributed processing, HuggingFace Transformers for DINOv2, PyTorch Lightning for model training

- Critical path:
  - Precompute DINOv2 embeddings ([CLS] tokens) for all training images
  - Train a linear classifier on these embeddings using negative log-likelihood loss
  - Use the trained classifier to predict species for test images by taking the mode of predictions across multiple images per observation

- Design tradeoffs:
  - Using frozen DINOv2 embeddings vs. fine-tuning the entire model: Frozen embeddings are faster and require less data but may limit performance; fine-tuning could improve results but requires more resources and data
  - Simple linear classifier vs. deeper neural network: Linear classifiers are interpretable and fast but may underfit complex decision boundaries; deeper networks could capture more complex patterns but risk overfitting with limited data

- Failure signatures:
  - Suspiciously low F1 and accuracy scores (0.0) despite some clustering in embeddings suggest potential bugs in label indexing or preprocessing
  - High Track 2 scores (penalizing misclassifications) indicate systematic errors in predictions, possibly due to class imbalance or poor feature separability

- First 3 experiments:
  1. Verify label indexing consistency between training and inference to rule out the suspected bug causing zero performance
  2. Evaluate alternative classifiers (e.g., SVM, small MLP) on the same DINOv2 embeddings to assess if the linear model is the bottleneck
  3. Test different embedding strategies (e.g., using DCT coefficients or concatenating [CLS] with average pooling of patch tokens) to see if they improve classification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the indexing bug affecting label predictions be resolved to improve model performance?
- Basis in paper: [explicit] The paper mentions that the model performance is suspiciously bad due to a bug in implementing label indexing, where class IDs are mapped to a contiguous range for training but not in inference code.
- Why unresolved: The issue stems from a technical implementation error that needs debugging and correction in the inference code.
- What evidence would resolve it: Fixing the indexing bug and re-running the model to compare performance metrics (F1 score, accuracy) with and without the bug.

### Open Question 2
- Question: How can custom competition metrics related to venomous classification be incorporated into the loss function to better guide the optimizer?
- Basis in paper: [explicit] The paper suggests that future methods should include alternative losses that better fit the data distribution and identification constraints, as the current task metrics are non-differentiable.
- Why unresolved: The challenge lies in finding an appropriate loss function that can act as a surrogate for the non-differentiable competition metrics.
- What evidence would resolve it: Experimenting with different loss functions, such as asymmetric loss (ASL), and evaluating their impact on model performance against competition metrics.

### Open Question 3
- Question: How effective are image segmentation techniques in isolating relevant snake features for species classification?
- Basis in paper: [explicit] The paper proposes using image segmentation models like SAM and YOLOv8 to isolate snake features from the rest of the image, which could enhance model performance.
- Why unresolved: The effectiveness of these segmentation techniques has not been tested or validated in the context of the snake identification task.
- What evidence would resolve it: Conducting experiments with different segmentation models and evaluating their impact on the accuracy and efficiency of snake species classification.

## Limitations
- Suspected implementation bug causing zero F1 and accuracy scores undermines confidence in reported results
- Limited architectural exploration with only linear classifier tested on DINOv2 embeddings
- No baseline comparisons against traditional approaches or simpler feature extractors

## Confidence
- Low confidence: The core claim that DINOv2 embeddings provide a "strong foundation" for snake identification is questionable given the reported zero performance metrics and suspected implementation bugs.
- Medium confidence: The exploratory analysis showing some species clustering in embedding space is valid, but the practical utility for classification remains unproven due to the technical issues.
- High confidence: The observation that snake images exhibit high variability and visual similarity is well-supported and represents a genuine challenge for species identification.

## Next Checks
1. Verify label indexing consistency between training and inference to rule out the suspected bug causing zero performance
2. Evaluate alternative classifiers (e.g., SVM, small MLP) on the same DINOv2 embeddings to assess if the linear model is the bottleneck
3. Test different embedding strategies (e.g., using DCT coefficients or concatenating [CLS] with average pooling of patch tokens) to see if they improve classification accuracy