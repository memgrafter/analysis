---
ver: rpa2
title: Learning from negative feedback, or positive feedback or both
arxiv_id: '2410.04166'
source_url: https://arxiv.org/abs/2410.04166
tags:
- policy
- preference
- learning
- feedback
- only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a preference-based policy optimization algorithm
  that decouples learning from positive and negative feedback, enabling stable learning
  even when only one type of feedback is available. The method builds on expectation-maximization
  (EM) and extends it to explicitly incorporate dis-preferred examples through a novel
  objective function that maximizes likelihood of preferred outcomes while minimizing
  likelihood of dis-preferred ones, regularized by a KL term.
---

# Learning from negative feedback, or positive feedback or both

## Quick Facts
- arXiv ID: 2410.04166
- Source URL: https://arxiv.org/abs/2410.04166
- Reference count: 30
- Key outcome: A preference-based policy optimization algorithm that can learn from positive feedback, negative feedback, or both, demonstrating stable learning even when only one type of feedback is available

## Executive Summary
This paper introduces PMPO (Preference-based Maximum a Posteriori Policy Optimization), an algorithm that extends expectation-maximization to policy optimization with preference feedback. The key innovation is decoupling learning from positive and negative feedback through a novel objective function that maximizes likelihood of preferred outcomes while minimizing likelihood of dis-preferred ones, regularized by a KL term. The method demonstrates strong performance across synthetic benchmarks, continuous control tasks, offline RL, and language model alignment, particularly excelling in settings with unbalanced preference data or only negative feedback.

## Method Summary
PMPO is a preference-based policy optimization algorithm that builds on expectation-maximization (EM) framework. It optimizes an objective that maximizes the likelihood of preferred outcomes while minimizing the likelihood of dis-preferred outcomes, with KL regularization to maintain stability. The method explicitly handles both positive and negative feedback separately, allowing learning even when only one feedback type is available. It uses an iterative approach with E-steps finding variational distributions and M-steps optimizing the policy, and can incorporate learned value functions as additional feedback sources.

## Key Results
- PMPO achieves state-of-the-art performance on synthetic benchmark functions, outperforming both RL-based and preference-based methods
- The algorithm successfully learns from negative feedback alone when the KL regularization parameter β is sufficiently high (> 1.0)
- In language model alignment using Gemma 2B, PMPO achieves competitive results with 50% fewer preference labels compared to baselines
- The method demonstrates robustness to highly unbalanced preference data, automatically adjusting the KL term weight based on the ratio of positive to negative examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method can learn from negative feedback alone by maximizing the likelihood of dis-preferred outcomes while staying close to a reference distribution.
- Mechanism: The objective function includes a term that minimizes the log-likelihood of dis-preferred samples, coupled with a KL divergence regularization term to prevent the policy from drifting too far from the reference distribution.
- Core assumption: The reference distribution is sufficiently close to the desired policy and provides a stable baseline for learning.
- Evidence anchors:
  - [abstract]: "Our approach builds upon the probabilistic framework introduced in (Dayan & Hinton, 1997), which uses expectation-maximization (EM) to directly optimize the probability of positive outcomes"
  - [section 3.2]: "We show that this alone can have an positive effect on data efficiency and performance on certain tasks, notwithstanding the added flexibility."
  - [corpus]: Found 25 related papers; no direct evidence for this specific mechanism, but papers like "Negative-Prompt-driven Alignment for Generative Language Model" suggest interest in negative feedback learning.
- Break condition: If the reference distribution is too far from the optimal policy, the KL regularization may prevent effective learning from negative feedback.

### Mechanism 2
- Claim: The method can handle unbalanced preference data by automatically adjusting the KL term weight based on the ratio of positive to negative examples.
- Mechanism: The KL divergence term is weighted by the inverse of the normalization factor for dis-preferred examples, which effectively scales the regularization based on the amount of negative data available.
- Core assumption: The normalization factor accurately reflects the imbalance in the data.
- Evidence anchors:
  - [abstract]: "This decoupling enables control over the influence of each feedback type and, importantly, allows learning even when only one feedback type is present."
  - [section 3.2]: "This weighting implies that if the reference policy has more negative than positive examples for state x, the KL weight should be lower, permitting greater deviation from the reference policy."
  - [corpus]: Weak evidence; no direct support found in the corpus.
- Break condition: If the normalization factor is incorrectly estimated, the KL weight may be inappropriate, leading to suboptimal learning.

### Mechanism 3
- Claim: The method generalizes to scenarios where only unpaired feedback is available, without requiring paired preference comparisons.
- Mechanism: The objective function directly optimizes the likelihood of preferred and dis-preferred outcomes separately, without needing to model the relative preference between pairs of examples.
- Core assumption: The likelihood functions for preferred and dis-preferred outcomes can be defined independently based on the available information.
- Evidence anchors:
  - [abstract]: "This requirement limits their applicability in scenarios where only unpaired feedback--for example, either positive or negative--is available."
  - [section 3.3]: "As a result, it can be used even when only positive or only negative samples are available (this is in contrast to e.g. DPO (Rafailov et al., 2023) or IPO (Azar et al., 2023) which require relative scores of paired positive and negative examples for each query x)."
  - [corpus]: Weak evidence; no direct support found in the corpus.
- Break condition: If the likelihood functions are poorly defined, the method may not effectively distinguish between preferred and dis-preferred outcomes.

## Foundational Learning

- Concept: Expectation-Maximization (EM) algorithm
  - Why needed here: EM is used to iteratively optimize the likelihood of preferred outcomes and tighten the lower bound on the objective function.
  - Quick check question: What are the two steps of the EM algorithm, and how do they contribute to policy optimization in this context?

- Concept: Kullback-Leibler (KL) divergence
  - Why needed here: KL divergence is used to regularize the policy update, preventing it from deviating too far from the reference distribution and ensuring stable learning.
  - Quick check question: How does the KL divergence term in the objective function influence the policy update, and what happens if it is set too high or too low?

- Concept: Reinforcement Learning as Inference
  - Why needed here: This perspective frames policy optimization as maximizing the probability of preferred outcomes, which is the foundation for the PMPO algorithm.
  - Quick check question: How does the "RL as inference" framework differ from traditional RL approaches, and what advantages does it offer in this context?

## Architecture Onboarding

- Component map: Reference Policy -> Likelihood Functions -> KL Divergence Regularizer -> Objective Function
- Critical path:
  1. Initialize the reference policy
  2. Generate samples from the reference policy
  3. Evaluate the samples using the likelihood functions to determine preferred and dis-preferred outcomes
  4. Update the policy by optimizing the objective function using gradient ascent
  5. Repeat steps 2-4 until convergence
- Design tradeoffs:
  - Using a high KL weight (β) ensures stable learning from negative feedback but may slow down convergence
  - Using a low KL weight allows faster learning but may lead to instability when learning from negative feedback alone
  - The choice of likelihood functions affects the quality of the preference information and the effectiveness of the learning algorithm
- Failure signatures:
  - If the policy collapses to a degenerate solution, the KL weight may be too low
  - If the learning is too slow, the KL weight may be too high
  - If the policy does not improve, the likelihood functions may be poorly defined or the reference policy may be too far from the optimal policy
- First 3 experiments:
  1. Evaluate the performance of PMPO on a synthetic benchmark function with only negative feedback available
  2. Test the sensitivity of PMPO to the KL weight (β) when learning from both positive and negative feedback
  3. Compare the performance of PMPO to DPO and MPO on a continuous control task from the DeepMind Control Suite

## Open Questions the Paper Calls Out

- The paper does not explicitly call out open questions, but the following limitations suggest areas for future research:
  - How does PMPO perform in high-dimensional continuous control tasks compared to simpler benchmarks?
  - What is the impact of different KL weight (β) values across diverse task types and reward structures?
  - How does the performance scale with the size and quality of preference datasets, particularly for large language models?

## Limitations
- The effectiveness of learning from negative feedback relies heavily on the KL regularization term, with limited exploration of edge cases where the reference policy is poorly initialized
- Claims about superior performance in highly unbalanced preference scenarios are based on limited experiments
- The generalization to unpaired feedback scenarios is demonstrated but not extensively validated across diverse domains

## Confidence

- **High Confidence**: The core algorithmic contribution (decoupling positive/negative feedback in PMPO) is well-founded mathematically and the synthetic benchmark results are reproducible
- **Medium Confidence**: The continuous control and RGB stacking results are promising but depend on specific implementation details not fully specified in the paper
- **Low Confidence**: The claims about superior performance in highly unbalanced preference scenarios are based on limited experiments

## Next Checks
1. **KL Weight Sensitivity**: Run systematic ablation studies varying β across multiple orders of magnitude to identify the optimal range for learning from negative feedback alone
2. **Cross-Domain Generalization**: Test PMPO on additional domains beyond those presented (e.g., robotics manipulation, recommendation systems) to evaluate robustness across different preference feedback structures
3. **Reference Policy Impact**: Experiment with initializing from poor reference policies to quantify the method's robustness to initialization and identify conditions where learning from negative feedback fails