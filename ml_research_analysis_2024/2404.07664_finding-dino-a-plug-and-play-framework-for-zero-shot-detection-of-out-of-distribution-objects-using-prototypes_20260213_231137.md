---
ver: rpa2
title: 'Finding Dino: A Plug-and-Play Framework for Zero-Shot Detection of Out-of-Distribution
  Objects Using Prototypes'
arxiv_id: '2404.07664'
source_url: https://arxiv.org/abs/2404.07664
tags:
- prowl
- object
- objects
- detection
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces PROWL, a zero-shot framework for detecting
  and localizing out-of-distribution (OOD) objects in any scene without requiring
  domain-specific training or labeled data. The core idea leverages frozen features
  from self-supervised pre-trained models (DINOv2) to create a prototype feature bank
  for known object classes, then matches pixel features against these prototypes to
  detect anomalies.
---

# Finding Dino: A Plug-and-Play Framework for Zero-Shot Detection of Out-of-Distribution Objects Using Prototypes
## Quick Facts
- arXiv ID: 2404.07664
- Source URL: https://arxiv.org/abs/2404.07664
- Authors: Poulami Sinhamahapatra; Franziska Schwaiger; Shirsha Bose; Huiyu Wang; Karsten Roscher; Stephan Guennemann
- Reference count: 40
- Primary result: Zero-shot OOD detection framework achieving state-of-the-art performance across multiple domains using frozen self-supervised features and prototype matching

## Executive Summary
This paper introduces PROWL, a zero-shot framework for detecting and localizing out-of-distribution (OOD) objects in any scene without requiring domain-specific training or labeled data. The method leverages frozen features from self-supervised pre-trained models (DINOv2) to create a prototype feature bank for known object classes, then matches pixel features against these prototypes to detect anomalies. By incorporating unsupervised segmentation masks (from STEGO or CutLER), PROWL refines detection into precise instance-level OOD masks. Evaluated on road driving, rail, and maritime domains, PROWL achieves state-of-the-art zero-shot performance and competitive results against supervised baselines.

## Method Summary
PROWL creates a prototype feature bank using frozen features from self-supervised models like DINOv2, then matches pixel features against these prototypes to detect OOD objects. The method extracts features from minimal prototype samples (5-20 per class) for known object classes, computes cosine similarity between test image features and prototypes, and thresholds similarity scores to identify OOD pixels. Optionally, unsupervised segmentation masks (STEGO or CutLER) are combined with prototype heatmaps to refine pixel-level detections into instance-level OOD masks. The framework operates in a zero-shot manner, requiring only specification of known object classes without domain-specific training or labeled data.

## Key Results
- State-of-the-art zero-shot performance on road driving (SegmentMeIfYouCan, Fishyscapes), rail, and maritime domains
- Competitive results against supervised baselines, especially when combined with CutLER segmentation
- Generalizes across domains via simple ODD class specification without requiring domain-specific training
- Robust detection of OOD objects with precise localization when using unsupervised segmentation refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Frozen self-supervised features from DINOv2 provide robust and diverse visual representations that generalize across domains.
- Mechanism: DINOv2 features are pre-trained on large image corpora using self-supervised learning (DINO), which enables the model to learn rich, domain-agnostic feature representations without requiring labeled data. These features capture semantic correspondences across images and domains, allowing them to represent objects from any domain sufficiently for OOD detection.
- Core assumption: Self-supervised pre-training on diverse datasets produces features that are semantically meaningful and transferable across domains without fine-tuning.
- Evidence anchors:
  - [abstract] "leverages the rich and diverse features from frozen foundation models such as self-supervised pre-trained DINOv2 [28] to robustly capture the known object categories as prototypes"
  - [section] "Pre-trained features from foundation models like DINO [6] use knowledge distillation via a teacher-student network for learning in a self-supervised approach... Thus, we utilize the robust general-purpose visual frozen features from such feature extractors for object classes in ODD using a minimal number of samples from the train split."
  - [corpus] Weak: No direct evidence in corpus about DINOv2's generalization properties across domains.
- Break condition: If the pre-trained features fail to capture relevant semantic information for objects in a new domain, the prototype matching will not work effectively.

### Mechanism 2
- Claim: Prototype matching using cosine similarity between pixel features and prototype features enables zero-shot OOD detection.
- Mechanism: The method creates a prototype feature bank by extracting features from a few object samples per known class using the frozen DINOv2 model. For each test image, pixel features are compared to these prototypes using cosine similarity. Pixels with low similarity to any prototype are classified as OOD. This works because OOD objects will have feature representations dissimilar to any known class prototype.
- Core assumption: Cosine similarity in the feature space is a reliable measure of object similarity, and OOD objects will have sufficiently different features from known objects.
- Evidence anchors:
  - [abstract] "The similarity to those prototypes is then used to calculate a pixel-level similarity score for each given test image. This score can be thresholded to detect OOD pixels."
  - [section] "Using prototype feature bank P for K classes, K prototype heatmaps are calculated based on maximum cosine similarity between the prototype vector list pk and z, for respective class ck given as: hk = max(z Â· pk)"
  - [corpus] Weak: No direct evidence in corpus about the effectiveness of cosine similarity for OOD detection in this context.
- Break condition: If the threshold for determining OOD is set incorrectly, or if OOD objects have feature representations similar to known objects (causing false negatives), the method will fail.

### Mechanism 3
- Claim: Combining unsupervised foreground segmentation masks with prototype-based OOD detection refines results into precise instance-level OOD masks.
- Mechanism: Unsupervised segmentation methods like STEGO and CutLER generate foreground masks for all objects in a scene without considering class labels. These masks are then combined with the prototype heatmaps from Mechanism 2. If the majority of pixels in a foreground mask are classified as OOD by the prototype matching, the entire mask is labeled as OOD. This converts noisy pixel-level OOD predictions into clean instance-level masks.
- Core assumption: Unsupervised segmentation methods can reliably detect all foreground objects in a scene, and the majority vote within each mask will correctly identify OOD objects.
- Evidence anchors:
  - [abstract] "By incorporating unsupervised segmentation masks (from STEGO or CutLER), PROWL refines detection into precise instance-level OOD masks."
  - [section] "Since these models were trained on huge generic datasets in a self-supervised manner, they can reliably detect multiple foreground object masks in a scene without the notion of ODD/OOD object class... As shown in Figure 2, the OOD objects dinosaur and passenger car were correctly detected using prototype heatmaps in PROWL as they did not have high similarity with any of the ODD classes listed in the prototype bank, however, additional pixels were also spuriously detected as OOD. While PROWL in combination with foreground masks, correctly detected and precisely localised the exact OOD object masks as 'unknown'."
  - [corpus] Weak: No direct evidence in corpus about the effectiveness of combining unsupervised segmentation with prototype matching.
- Break condition: If the unsupervised segmentation fails to detect an OOD object as a foreground mask, or if it includes too much background in the masks, the refinement will not work correctly.

## Foundational Learning

- Concept: Self-supervised learning and vision transformers
  - Why needed here: The method relies on features extracted from self-supervised pre-trained vision transformers (DINOv2). Understanding how self-supervised learning works and how vision transformers extract features is crucial for understanding why the method works and how to modify it.
  - Quick check question: What is the difference between supervised and self-supervised learning, and why might self-supervised features be more generalizable?

- Concept: Prototype-based classification
  - Why needed here: The core detection mechanism is based on comparing pixel features to prototype features using similarity measures. Understanding prototype-based classification is essential for grasping how the method distinguishes between known and unknown objects.
  - Quick check question: How does prototype-based classification differ from traditional class-based classification, and what are the advantages and disadvantages of each approach?

- Concept: Unsupervised segmentation
  - Why needed here: The refinement step uses unsupervised segmentation methods to generate foreground masks. Understanding how these methods work (e.g., STEGO's contrastive clustering, CutLER's object detection) is important for understanding the refinement process and for potentially replacing or modifying these components.
  - Quick check question: How do unsupervised segmentation methods differ from supervised ones, and what are the trade-offs between them?

## Architecture Onboarding

- Component map: Input image -> DINOv2 feature extraction -> Prototype feature bank -> Cosine similarity matching -> OOD detection (thresholding) -> (Optional) Unsupervised segmentation refinement -> Output OOD masks

- Critical path:
  1. Extract features from test image using DINOv2
  2. Compute cosine similarity between pixel features and each prototype
  3. Threshold similarity scores to identify OOD pixels
  4. (Optional) Combine with unsupervised segmentation masks for refinement

- Design tradeoffs:
  - Using frozen features vs. fine-tuning: Frozen features provide generalization but may not be optimal for specific domains. Fine-tuning could improve performance but would require labeled data and would lose the zero-shot capability.
  - Pixel-level vs. instance-level OOD detection: Pixel-level detection is more precise but noisier. Instance-level detection via segmentation masks is cleaner but relies on the quality of the unsupervised segmentation.
  - Number of prototypes per class: More prototypes provide better coverage but increase computation. Fewer prototypes are faster but may miss variations within a class.

- Failure signatures:
  - High false positive rate: OOD objects are being missed or background is being classified as OOD. This could indicate that the similarity threshold is too low or that the prototypes don't adequately represent the known classes.
  - High false negative rate: Known objects are being classified as OOD. This could indicate that the similarity threshold is too high or that the DINOv2 features are not capturing the relevant information for those objects.
  - Noisy detections: Pixel-level OOD predictions are scattered and don't form coherent regions. This is expected without refinement and can be addressed by using the refinement step.

- First 3 experiments:
  1. Verify feature extraction: Pass a test image through the DINOv2 model and visualize the feature maps to ensure they are being extracted correctly and contain meaningful information.
  2. Test prototype matching: Create a small prototype bank with a few known objects and test the cosine similarity calculation on a test image containing those objects to verify that the similarity scores are reasonable.
  3. Evaluate OOD detection: Test the full pipeline on a dataset with known OOD objects (e.g., RoadAnomaly) with different threshold values to find a good balance between false positives and false negatives.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is PROWL's performance to the choice of prototype samples when creating the prototype feature bank?
- Basis in paper: [explicit] Section 5.3 discusses that performance saturates with 15+ prototypes and that different sets of prototype samples lead to similar performance, but also notes that further investigation is needed for samples from other datasets.
- Why unresolved: The paper only tested prototype sample selection on the Cityscapes dataset. It's unclear whether the observed insensitivity to prototype choice generalizes to other domains or if certain types of objects require more careful prototype selection.
- What evidence would resolve it: Systematic experiments varying prototype sample selection across multiple diverse datasets, showing how different prototype choices affect performance in each domain.

### Open Question 2
- Question: What is the optimal threshold strategy for INCS scores across different datasets and object sizes?
- Basis in paper: [explicit] Section 5.3 shows that optimal thresholds vary across datasets (RA: 0.55, FS Static: 0.7, RO: 0.6) and that smaller objects require lower thresholds but introduce more noise.
- Why unresolved: The paper uses fixed thresholds for comparison with supervised methods, but demonstrates that optimal thresholds vary. It's unclear how to automatically determine the best threshold for a given application without access to ground truth.
- What evidence would resolve it: Development of a principled method for threshold selection that adapts to object size distribution and dataset characteristics without requiring ground truth labels.

### Open Question 3
- Question: How does PROWL's performance degrade when encountering OOD objects that share visual features with known classes?
- Basis in paper: [inferred] The method relies on cosine similarity between pixel features and prototype features, which could struggle when OOD objects have similar visual appearance to known classes. The paper doesn't explicitly test this scenario.
- Why unresolved: The evaluation focuses on detecting clearly distinct OOD objects, but doesn't address cases where OOD objects might be visually similar to in-distribution classes (e.g., detecting a toy car as OOD when cars are known classes).
- What evidence would resolve it: Controlled experiments with OOD objects that visually resemble known classes but are semantically different, measuring false negative rates.

### Open Question 4
- Question: Can PROWL's zero-shot approach be extended to handle gradual domain shifts rather than sudden OOD detection?
- Basis in paper: [inferred] The current formulation treats detection as a binary OOD/known classification problem, but real-world scenarios often involve gradual shifts in object appearance or distribution.
- Why unresolved: The paper evaluates PROWL on benchmark datasets with clear OOD objects but doesn't address scenarios where the distinction between known and unknown becomes fuzzy over time.
- What evidence would resolve it: Implementation of a continuous anomaly score or uncertainty measure that captures degrees of distribution shift, validated on datasets with gradual domain adaptation scenarios.

## Limitations
- Performance depends on frozen DINOv2 features generalizing across all domains without fine-tuning, which may not hold for specialized environments
- Method's effectiveness is highly dependent on prototype feature bank quality and diversity, with optimal sampling strategy not fully explored
- Refinement step introduces dependency on unsupervised segmentation method's ability to correctly identify all foreground objects, which may fail in complex scenes

## Confidence
- **High Confidence**: Prototype matching mechanism using cosine similarity and general framework architecture are well-specified and reproducible
- **Medium Confidence**: Assumption that frozen self-supervised features will generalize across all domains without adaptation
- **Medium Confidence**: Effectiveness of combining unsupervised segmentation with prototype matching

## Next Checks
1. Cross-Domain Generalization Test: Evaluate PROWL's performance when known object classes (ODD) are specified differently across domains to verify true generalization capability
2. Prototype Bank Sensitivity Analysis: Systematically vary the number of prototype samples per class (1, 5, 10, 20) to identify optimal trade-off between computational efficiency and accuracy
3. Failure Case Analysis: Create synthetic test cases where OOD objects have visual characteristics similar to known objects to measure robustness to semantic ambiguity and identify failure modes