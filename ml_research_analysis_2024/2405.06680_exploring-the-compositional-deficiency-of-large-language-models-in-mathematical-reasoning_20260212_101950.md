---
ver: rpa2
title: Exploring the Compositional Deficiency of Large Language Models in Mathematical
  Reasoning
arxiv_id: '2405.06680'
source_url: https://arxiv.org/abs/2405.06680
tags:
- problems
- problem
- trap
- original
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates whether large language models (LLMs) exhibit\
  \ systematic compositionality\u2014the ability to combine learned knowledge components\
  \ to solve novel problems. To evaluate this, the authors construct MATHTRAP, a dataset\
  \ created by introducing logical traps into standard math word problems from MATH\
  \ and GSM8K."
---

# Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning

## Quick Facts
- **arXiv ID:** 2405.06680
- **Source URL:** https://arxiv.org/abs/2405.06680
- **Reference count:** 40
- **Primary result:** LLMs show significant compositional reasoning gaps, achieving less than half their original accuracy on trap problems that require combining learned knowledge components

## Executive Summary
This paper investigates whether large language models exhibit systematic compositionality - the ability to combine learned knowledge components to solve novel problems. The authors construct MATHTRAP, a dataset created by introducing logical traps into standard math word problems from MATH and GSM8K. These traps require models to combine knowledge from the original problem with understanding of the introduced logical flaw. Experiments reveal that while LLMs can identify traps when asked directly, they fail to spontaneously combine this knowledge when solving trap problems, with performance dropping significantly compared to original problems. Human evaluation demonstrates strong compositional reasoning capabilities that far exceed those of current LLMs.

## Method Summary
The authors created MATHTRAP by introducing logical traps into standard math word problems from existing datasets (MATH and GSM8K). These traps represent "unseen" cases unlikely to appear in training data, requiring models to combine knowledge from the original problem with understanding of the introduced logical flaw. The dataset construction methodology involves manual creation of traps that introduce specific types of logical errors or missing information. Multiple LLMs were evaluated on both original problems and their trap variants. Human evaluation was conducted with 5 participants to assess compositional reasoning capabilities. Several mitigation strategies were explored including natural language prompts, few-shot demonstrations, and fine-tuning approaches to improve model performance on trap problems.

## Key Results
- LLMs achieve less than half their original accuracy on trap problems compared to original problems
- While LLMs can identify traps when asked directly, they fail to spontaneously combine this knowledge during problem solving
- Human participants maintain 85.9% accuracy on trap problems compared to original problems, demonstrating strong compositional reasoning capabilities
- Existing mitigation strategies (prompting, few-shot, fine-tuning) show improvement but fail to close the gap with human performance

## Why This Works (Mechanism)
The paper demonstrates that LLMs struggle with systematic compositionality - the ability to recombine learned knowledge components to solve novel problems. This deficiency becomes apparent when models encounter problems that require combining information from different domains or understanding logical flaws. The mechanism appears to be that LLMs treat problems as surface-level patterns rather than deeply understanding the underlying logical structure and relationships between components.

## Foundational Learning
- **Systematic compositionality**: The ability to combine learned components to solve novel problems. Needed because it represents fundamental reasoning capability that goes beyond pattern matching. Quick check: Can the model solve problems requiring recombination of previously learned concepts?
- **Logical trap detection**: The ability to identify flaws or missing information in problem statements. Needed because compositional reasoning often requires recognizing when problems contain logical inconsistencies. Quick check: Can the model flag problems with missing or contradictory information?
- **Knowledge recombination**: The process of combining information from different domains or problem components. Needed because mathematical reasoning often requires synthesizing multiple concepts. Quick check: Can the model integrate concepts from different mathematical domains to solve problems?
- **Pattern vs. structure understanding**: The distinction between surface-level pattern matching and deep structural understanding. Needed because LLMs may rely on statistical patterns rather than true comprehension. Quick check: Does the model generalize to structurally similar but superficially different problems?
- **Training data contamination**: The potential for evaluation data to appear in training sets. Needed because this could explain some model behaviors. Quick check: Are evaluation problems truly novel or do they overlap with training data?
- **Prompt sensitivity**: The degree to which model performance varies with different prompting strategies. Needed because this affects the validity of performance comparisons. Quick check: How does performance change with different prompt formulations?

## Architecture Onboarding
- **Component map**: Problem -> Trap detection module -> Compositional reasoning module -> Answer generation -> Evaluation
- **Critical path**: The most important sequence is Trap detection → Compositional reasoning → Answer generation. If models cannot detect traps, they cannot engage compositional reasoning, making trap detection the critical first step.
- **Design tradeoffs**: The study explores three mitigation strategies - prompting (low computational cost, easy to implement), few-shot learning (requires demonstration examples, moderate improvement), and fine-tuning (high computational cost, best improvement but still insufficient).
- **Failure signatures**: Models show 50%+ accuracy drop on trap problems, can identify traps when asked directly but fail to combine this knowledge during problem solving, and existing mitigation strategies fail to close the human-model performance gap.
- **3 first experiments**: 1) Ablation study on specific trap types to identify which logical flaws most challenge models, 2) Test whether domain-specific fine-tuning improves trap problem performance beyond general approaches, 3) Evaluate whether intermediate training on compositional reasoning tasks improves final performance.

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Dataset construction methodology introduces potential confounds as traps are manually created and may not represent naturally occurring compositional reasoning failures
- The selection of traps (17.3% of dataset) is arbitrary and may bias results toward certain types of logical failures
- Human evaluation involves a small sample size (5 participants) with unspecified methodology for prompt design and answer evaluation

## Confidence
- **High confidence**: The finding that LLMs can identify traps when asked directly but fail to combine this knowledge during problem solving is directly observable in the results
- **Medium confidence**: The main claims about LLMs lacking systematic compositionality, as alternative explanations (training data contamination, prompt sensitivity) cannot be fully ruled out
- **Medium confidence**: The conclusion that existing mitigation strategies fail to close the gap with human performance, as the evaluation may not have explored the full space of potential interventions

## Next Checks
1. Conduct ablation studies testing whether specific trap types disproportionately affect performance, and whether models can solve compositional variants of the same problems without explicit traps
2. Expand human evaluation to larger sample sizes (n>20) with diverse mathematical backgrounds and more rigorous inter-rater reliability assessment
3. Test whether domain-specific fine-tuning on compositional reasoning tasks or intermediate training on related reasoning skills can improve trap problem performance beyond the general approaches evaluated here