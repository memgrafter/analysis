---
ver: rpa2
title: 'HART: Efficient Visual Generation with Hybrid Autoregressive Transformer'
arxiv_id: '2410.10812'
source_url: https://arxiv.org/abs/2410.10812
tags:
- hart
- generation
- tokens
- diffusion
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HART addresses the limitations of autoregressive visual generation
  models by introducing a hybrid tokenization approach that combines discrete and
  continuous latents. The method decomposes continuous latents into discrete tokens
  for capturing overall structure and continuous residual tokens for fine details,
  modeled by a lightweight diffusion module.
---

# HART: Efficient Visual Generation with Hybrid Autoregressive Transformer

## Quick Facts
- arXiv ID: 2410.10812
- Source URL: https://arxiv.org/abs/2410.10812
- Authors: Haotian Tang; Yecheng Wu; Shang Yang; Enze Xie; Junsong Chen; Junyu Chen; Zhuoyang Zhang; Han Cai; Yao Lu; Song Han
- Reference count: 23
- Key outcome: HART achieves state-of-the-art text-to-image generation quality, outperforming diffusion models with 4.5-7.7× higher throughput, 3.1-5.9× lower latency, and 6.9-13.4× lower MACs.

## Executive Summary
HART addresses the limitations of autoregressive visual generation models by introducing a hybrid tokenization approach that combines discrete and continuous latents. The method decomposes continuous latents into discrete tokens for capturing overall structure and continuous residual tokens for fine details, modeled by a lightweight diffusion module. This approach significantly improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K and generation FID from 7.85 to 5.38. HART achieves state-of-the-art text-to-image generation quality, outperforming diffusion models with 4.5-7.7× higher throughput, 3.1-5.9× lower latency, and 6.9-13.4× lower MACs.

## Method Summary
HART introduces a hybrid tokenizer that decomposes continuous latents from the autoencoder into discrete tokens representing the overall structure and continuous residual tokens capturing fine details. These latents are modeled by a scalable-resolution autoregressive transformer and a lightweight residual diffusion module, respectively. The model employs alternating training with 50% probability for continuous and discrete paths to maintain decoder consistency. Relative position embeddings enable faster convergence at higher resolutions, while the lightweight residual diffusion module reduces computational overhead compared to full diffusion models.

## Key Results
- Reconstruction FID improved from 2.11 to 0.30 on MJHQ-30K dataset
- Generation FID improved from 7.85 to 5.38, outperforming diffusion models
- 4.5-7.7× higher throughput, 3.1-5.9× lower latency, and 6.9-13.4× lower MACs compared to diffusion models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid tokenization decomposes continuous latents into discrete tokens for structure and continuous residual tokens for fine details.
- Mechanism: The visual encoder produces continuous latents that are quantized into discrete tokens while preserving the difference as residual tokens, which are then modeled by a lightweight diffusion module.
- Core assumption: The residual between continuous latents and quantized discrete tokens contains primarily high-frequency details that are difficult to represent with discrete codebooks.
- Evidence anchors:
  - [abstract]: "The hybrid tokenizer, which decomposes the continuous latents from the autoencoder into two components: discrete tokens representing the big picture and continuous tokens representing the residual components that cannot be represented by the discrete tokens."
  - [section]: "These two latents are then modeled by our hybrid transformer: the discrete latents are handled by a scalable-resolution V AR transformer, while the continuous latents are predicted by a lightweight residual diffusion module with 5% parameter and 10% runtime overhead."
  - [corpus]: Found 25 related papers, including Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens, Fast Autoregressive Models for Continuous Latent Generation, MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation, suggesting active research in combining discrete and continuous approaches.
- Break condition: If the residual tokens contain significant structural information beyond fine details, the autoregressive transformer cannot properly model the overall structure using only discrete tokens.

### Mechanism 2
- Claim: Alternating training of the hybrid tokenizer with 50% probability for continuous and discrete paths maintains decoder consistency.
- Mechanism: During training, the visual decoder randomly receives either discrete tokens (after VQ quantization) or continuous tokens (bypassing quantization), ensuring it can reconstruct from both representations.
- Core assumption: The decoder learns to handle both continuous and discrete inputs effectively when trained on both types with equal probability.
- Evidence anchors:
  - [section]: "During each training step, we randomly choose with equal probability (50%) whether to provide the decoder with discrete or continuous visual tokens for reconstructing the input image."
  - [section]: "Empirical results show that the HART tokenizer achieves comparable continuous rFID (i.e., reconstruction FID when the continuous path is activated) to the SDXL tokenizer (Podell et al., 2023), while its discrete rFID matches the performance of the original VQ tokenizer."
  - [corpus]: Weak evidence - the corpus contains related work on continuous latent generation but lacks specific studies on alternating training strategies for hybrid tokenizers.
- Break condition: If the decoder cannot effectively learn to handle both input types, reconstruction quality for one path will degrade significantly.

### Mechanism 3
- Claim: Relative position embeddings enable scalable-resolution autoregressive transformers to converge faster at higher resolutions.
- Mechanism: Converting absolute position embeddings to interpolation-compatible relative embeddings (sinusoidal for steps, hybrid 1D/2D rotary for tokens) allows the model to generalize across different resolutions without full retraining.
- Core assumption: Relative position embeddings provide better generalization across resolutions compared to absolute embeddings that are resolution-specific.
- Evidence anchors:
  - [section]: "We utilize sinusoidal PE for step embeddings, which naturally accommodates varying sampling steps in 256/512px (10 steps) and 1024px (14 steps) generation. For token index embeddings, we implement a hybrid approach: 1D rotary embeddings for text tokens and 2D rotary embeddings (Sun et al., 2024; Ma et al., 2024a; Wang et al., 2024) for visual tokens."
  - [section]: "We found these relative embeddings significantly accelerates HART convergence at higher resolutions."
  - [corpus]: Weak evidence - while the corpus contains related work on position embeddings and scaling, it lacks specific studies on the effectiveness of relative position embeddings for resolution scaling in autoregressive transformers.
- Break condition: If relative position embeddings do not generalize well across resolutions, the model may fail to converge or produce poor quality outputs at higher resolutions.

## Foundational Learning

- Concept: Vector quantization and discrete token representation
  - Why needed here: HART builds upon discrete autoregressive models by adding continuous components, so understanding VQ is essential for grasping the baseline approach
  - Quick check question: What is the primary limitation of discrete tokenizers that HART aims to address?

- Concept: Diffusion models and denoising processes
  - Why needed here: HART uses a lightweight residual diffusion module to model continuous residual tokens, requiring understanding of diffusion fundamentals
  - Quick check question: How does the residual diffusion module in HART differ from standard diffusion models in terms of computational requirements?

- Concept: Attention mechanisms and position embeddings
  - Why needed here: HART employs scalable-resolution transformers with relative position embeddings, requiring understanding of attention patterns and embedding strategies
  - Quick check question: What advantage do relative position embeddings provide over absolute position embeddings for resolution scaling?

## Architecture Onboarding

- Component map: Visual encoder -> Continuous latents -> Vector quantizer -> Discrete tokens + Residual tokens -> Scalable-resolution autoregressive transformer + Lightweight residual diffusion module -> Hybrid tokenizer (visual decoder) -> Final image

- Critical path:
  1. Input image -> Visual encoder -> Continuous latents
  2. Continuous latents -> Vector quantizer -> Discrete tokens + Residual tokens
  3. Text prompt -> LLM text encoder -> Text tokens
  4. Discrete tokens + Text tokens -> Scalable-resolution AR transformer -> Predicted discrete tokens
  5. Discrete tokens + Last layer hidden states -> Residual diffusion MLP -> Predicted residual tokens
  6. Discrete tokens + Residual tokens -> Hybrid tokenizer -> Final image

- Design tradeoffs:
  - Discrete vs continuous modeling: Discrete provides structure but lacks detail; continuous provides detail but is computationally expensive
  - Training complexity: Alternating training adds complexity but maintains decoder consistency
  - Inference efficiency: Lightweight residual diffusion reduces computational overhead compared to full diffusion models

- Failure signatures:
  - Poor reconstruction FID indicates tokenizer training issues or inadequate alternating strategy
  - Low generation FID with good reconstruction suggests problems in autoregressive modeling
  - High latency despite lightweight design points to inefficient implementation of relative position embeddings

- First 3 experiments:
  1. Train hybrid tokenizer with alternating training and measure reconstruction FID for both continuous and discrete paths
  2. Implement scalable-resolution transformer with relative position embeddings and test convergence at different resolutions
  3. Integrate residual diffusion module and evaluate the trade-off between diffusion steps and generation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the alternating training strategy in hybrid tokenization specifically contribute to better generation FID compared to using separate decoders for continuous and discrete latents?
- Basis in paper: [explicit] The authors state that despite achieving similar reconstruction FID, the single decoder with alternating training enables faster and better generation convergence compared to separate decoders, as shown in Figure 7.
- Why unresolved: While the authors demonstrate better generation performance, they do not provide a detailed theoretical explanation for why alternating training is superior. The mechanism by which this approach improves generation quality remains unclear.
- What evidence would resolve it: A comprehensive ablation study comparing different training strategies with detailed analysis of their impact on both reconstruction and generation FID, along with theoretical justification for the observed improvements.

### Open Question 2
- Question: What is the maximum theoretical efficiency gain achievable by implementing a sparse attention kernel based on the observed "sink + local" attention pattern in V AR?
- Basis in paper: [inferred] The authors observe that attention scores concentrate on the most recent two stages and initial three stages, with local attention patterns within each stage. They suggest that implementing a sparse attention kernel is feasible.
- Why unresolved: The paper identifies the potential for sparse attention but does not quantify the theoretical efficiency gains or provide empirical results demonstrating the impact of such an optimization.
- What evidence would resolve it: Detailed analysis of the attention patterns across different model sizes and resolutions, followed by implementation and benchmarking of sparse attention kernels to measure actual efficiency improvements.

### Open Question 3
- Question: How does the performance of HART scale with increasing model size beyond 2B parameters, and what are the practical limitations of scaling?
- Basis in paper: [explicit] The authors demonstrate results with 600M, 1B, and 2B parameter models but do not explore larger scales or discuss practical limitations.
- Why unresolved: While the authors show that larger models improve performance, they do not investigate the point of diminishing returns or practical constraints such as memory usage, training time, or inference efficiency.
- What evidence would resolve it: Empirical results from training and evaluating HART models with parameters exceeding 2B, including analysis of performance gains, computational requirements, and practical deployment considerations.

## Limitations

- The paper does not provide a detailed theoretical explanation for why alternating training is superior to using separate decoders for continuous and discrete latents
- Specific implementation details of the residual diffusion module (number of layers, hidden dimensions) are not fully described
- The paper lacks ablation studies showing the specific contribution of relative position embeddings versus other architectural choices

## Confidence

- **High confidence**: The hybrid tokenization approach combining discrete and continuous latents is well-supported by experimental results showing dramatic improvements in reconstruction FID (0.30 vs 2.11) and generation FID (5.38 vs 7.85). The core concept of decomposing continuous latents into structure-preserving discrete tokens and detail-preserving continuous residuals is clearly articulated and validated.

- **Medium confidence**: The alternating training strategy's effectiveness is supported by empirical results showing comparable reconstruction quality for both paths, but the paper lacks analysis of how different training ratios might affect performance. The claim that 50% probability is optimal is not substantiated through ablation studies.

- **Medium confidence**: The relative position embedding contribution to scalable-resolution convergence is mentioned as "significantly accelerates" convergence, but specific quantitative comparisons against absolute position embeddings or other relative embedding strategies are not provided.

## Next Checks

1. **Ablation study on alternating training ratio**: Systematically vary the probability of selecting discrete vs continuous paths during tokenizer training (25%, 50%, 75%) and measure the impact on reconstruction FID for both paths, generation FID, and decoder consistency.

2. **Residual diffusion module efficiency analysis**: Compare the computational overhead of the residual diffusion module against its contribution to generation quality by varying the number of diffusion steps and measuring the trade-off between MACs, latency, and FID scores.

3. **Position embedding comparison**: Implement and compare the scalable-resolution transformer with absolute position embeddings, 1D relative embeddings, and the proposed hybrid 1D/2D relative embeddings, measuring convergence speed and generation quality at 256px, 512px, and 1024px resolutions.