---
ver: rpa2
title: 'Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition'
arxiv_id: '2405.15216'
source_url: https://arxiv.org/abs/2405.15216
tags:
- data
- speech
- audio
- error
- librispeech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Denoising LM (DLM), a scaled error correction
  model that uses synthetic data from TTS systems to achieve state-of-the-art ASR
  performance without external audio data. By training on vast amounts of paired synthetic
  and real data with multiple noise augmentation strategies, DLM achieves 1.5% WER
  on LibriSpeech test-clean and 3.3% WER on test-other, matching self-supervised methods.
---

# Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition

## Quick Facts
- arXiv ID: 2405.15216
- Source URL: https://arxiv.org/abs/2405.15216
- Reference count: 40
- Achieves 1.5% WER on LibriSpeech test-clean and 3.3% WER on test-other using synthetic data augmentation

## Executive Summary
This paper introduces Denoising LM (DLM), a scaled error correction model that uses synthetic data from TTS systems to achieve state-of-the-art ASR performance without external audio data. By training on vast amounts of paired synthetic and real data with multiple noise augmentation strategies, DLM achieves 1.5% WER on LibriSpeech test-clean and 3.3% WER on test-other, matching self-supervised methods. The same DLM improves multiple ASR architectures and generalizes across datasets, demonstrating scalability and efficiency through simple greedy decoding. This work shows error correction models can surpass conventional neural LMs, opening new directions for ASR accuracy.

## Method Summary
DLM uses a transformer-based architecture trained on synthetic data generated from TTS systems paired with real transcripts. The model applies multiple noise augmentation strategies during training, including word deletion, insertion, and replacement. Unlike traditional language models that predict next tokens autoregressively, DLM performs non-autoregressive error correction by taking noisy text as input and producing denoised output. The training objective uses a masked language modeling approach where the model learns to reconstruct the original clean text from corrupted versions. This approach allows DLM to leverage vast amounts of synthetic data without requiring paired audio-transcript data.

## Key Results
- Achieves 1.5% WER on LibriSpeech test-clean and 3.3% WER on test-other
- Matches self-supervised methods without requiring external audio data
- Improves multiple ASR architectures including Whisper, Conformer, and RNN-T
- Generalizes across datasets including GigaSpeech, MLS, Common Voice, and FLEURS

## Why This Works (Mechanism)
DLM works by treating ASR error correction as a denoising task. The model is trained on synthetic pairs where clean text is corrupted with realistic errors that mimic ASR mistakes. By learning to denoise these corrupted inputs, the model develops robust error correction capabilities. The non-autoregressive nature allows for efficient inference through greedy decoding, while the synthetic data generation enables training on massive datasets without requiring paired audio data. The noise augmentation strategies during training help the model generalize to various types of ASR errors.

## Foundational Learning
- **Synthetic Data Generation**: Creating paired noisy-clean text from TTS systems - needed to scale training without real audio data; quick check: verify TTS quality and diversity
- **Noise Augmentation**: Applying realistic error patterns during training - needed to simulate ASR errors; quick check: analyze error distribution in augmented data
- **Non-autoregressive Decoding**: Predicting entire output sequences in parallel - needed for efficient inference; quick check: measure decoding speed vs autoregressive models
- **Masked Language Modeling**: Training objective that reconstructs corrupted text - needed to learn denoising capabilities; quick check: ablation on different corruption rates
- **Error Correction vs Next Token Prediction**: DLM predicts corrections rather than next tokens - needed for post-processing ASR output; quick check: compare performance on corrupted vs clean inputs

## Architecture Onboarding

Component Map:
TTS System -> Synthetic Data Generator -> DLM Training -> Error Correction Model

Critical Path:
Synthetic Text Generation -> Noise Augmentation -> DLM Training -> ASR Integration -> Greedy Decoding

Design Tradeoffs:
- Non-autoregressive decoding offers speed but may miss context-dependent corrections
- Synthetic data eliminates audio collection costs but may not capture all real-world acoustic variations
- Masked LM training enables massive scaling but requires careful noise schedule tuning

Failure Signatures:
- Poor performance on domain-specific terminology suggests insufficient synthetic data diversity
- Overcorrection of proper nouns indicates need for name entity handling in noise augmentation
- Degradation with highly corrupted inputs suggests model confidence calibration issues

First Experiments:
1. Test DLM on corrupted clean text to establish baseline correction capability
2. Measure decoding speed compared to autoregressive language models
3. Ablate noise augmentation strategies to identify most effective corruption types

## Open Questions the Paper Calls Out
The paper acknowledges that the heavy reliance on TTS systems raises questions about domain adaptation when real-world audio characteristics differ significantly from synthetic speech. It also notes that computational cost analysis is limited, and it's unclear how the model scales to truly massive datasets beyond the 960 hours of LibriSpeech used.

## Limitations
- Heavy reliance on TTS systems may limit domain adaptation for real-world audio characteristics
- Limited analysis of computational costs for scaling to larger datasets
- Does not thoroughly address performance on non-English languages or languages with different phonetic structures
- Claims of surpassing conventional neural LMs lack comprehensive comparative analysis

## Confidence

**Confidence Assessment:**
- DLM achieves state-of-the-art LibriSpeech performance: **High**
- DLM improves multiple ASR architectures: **High**
- Generalization across datasets: **Medium**
- Surpassing conventional neural LMs: **Medium**

## Next Checks
1. Test DLM on conversational speech datasets (e.g., Switchboard) to evaluate performance in noisy, spontaneous speech conditions
2. Conduct ablation studies on the TTS quality and noise augmentation parameters to quantify their individual contributions
3. Evaluate computational efficiency and training costs for scaling to datasets larger than 1000 hours