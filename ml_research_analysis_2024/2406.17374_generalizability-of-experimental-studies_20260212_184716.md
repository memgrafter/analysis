---
ver: rpa2
title: Generalizability of experimental studies
arxiv_id: '2406.17374'
source_url: https://arxiv.org/abs/2406.17374
tags:
- experimental
- kernel
- generalizability
- studies
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal framework to quantify the generalizability
  of machine learning experimental studies, addressing the lack of objective measures
  beyond significance testing. The key idea is to model experimental results as samples
  from an underlying distribution, using the Maximum Mean Discrepancy (MMD) to measure
  similarity between empirical distributions.
---

# Generalizability of experimental studies

## Quick Facts
- arXiv ID: 2406.17374
- Source URL: https://arxiv.org/abs/2406.17374
- Reference count: 40
- Primary result: Introduces a formal framework to quantify generalizability of ML experimental studies using MMD and kernel methods

## Executive Summary
This paper addresses the lack of objective measures for assessing the generalizability of machine learning experimental studies. The authors propose a probabilistic framework that treats experimental results as samples from an underlying distribution, enabling quantification of generalizability through Maximum Mean Discrepancy (MMD). The framework includes tailored kernels for common research questions and derives a power-law relationship to estimate the number of experiments needed for desired generalizability levels. Case studies demonstrate practical utility, and the genexpy Python package makes the framework accessible to researchers.

## Method Summary
The framework formalizes experimental studies as random variables with a true underlying distribution. Generalizability is measured as the probability that two realizations of a study yield similar results within a kernel-based distance threshold ε. The authors introduce Borda, Jaccard, and Mallows kernels for different research questions about rankings. A power-law relationship between sample size and MMD quantiles allows estimation of required experiments from preliminary data. The method is implemented in the open-source genexpy package.

## Key Results
- Introduces Borda, Jaccard, and Mallows kernels for different research questions about experimental rankings
- Derives power-law relationship to predict required sample size from preliminary experiments
- Demonstrates framework on categorical encoder benchmarks and LLM evaluations
- Shows generalizable results often require more experiments than currently performed
- Releases genexpy Python package for practical implementation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Experimental studies can be formalized as probability distributions over results, allowing generalizability to be quantified via sampling theory.
- Mechanism: By modeling experimental results as samples from an underlying true distribution, similarity between results from different experimental conditions can be measured using Maximum Mean Discrepancy (MMD) between empirical distributions.
- Core assumption: There exists a true distribution of experimental results, and valid experimental conditions serve as the sampling space.
- Evidence anchors:
  - [abstract] "The core idea of our approach is to assume the existence of a true distribution of experimental results."
  - [section 3.2] "The core aspect of our probabilistic model is a random variable, the experimental results."
  - [corpus] Weak evidence - corpus neighbors focus on logic/reasoning rather than experimental design.
- Break condition: If experimental conditions are not independent or if the result space is not well-defined, the probability model fails.

### Mechanism 2
- Claim: Different research questions can be formalized using appropriate kernels, enabling targeted generalizability measurement.
- Mechanism: The MMD framework allows customization through kernel choice - Borda kernel for alternative-specific consistency, Jaccard for stability of top performers, Mallows for overall ranking similarity.
- Core assumption: The research question can be expressed as a property of the experimental result distribution that a kernel can capture.
- Evidence anchors:
  - [section 4.2] "Research questions act as lenses, focusing on specific aspects of the results that are of interest for the experimenter."
  - [section 4.2] "The MMD can account for this by choosing an appropriate kernel, a symmetric and positive definite function."
  - [corpus] Weak evidence - corpus papers don't discuss kernel methods for experimental studies.
- Break condition: If the research question cannot be expressed through a symmetric positive definite kernel, the framework cannot capture it.

### Mechanism 3
- Claim: The number of experiments needed for generalizability follows a predictable power-law relationship, enabling efficient study planning.
- Mechanism: Theorem 4.2 establishes that there exists a power-law relationship between the number of experiments and the MMD quantile, allowing estimation of required sample size from preliminary experiments.
- Core assumption: The MMD quantile follows a power-law relationship with sample size, which can be estimated from preliminary data.
- Evidence anchors:
  - [section 4.5] "We have observed in our experiments that there exists a power-law relationship between the quantiles of MMD and n."
  - [section 4.5] "Theorem 4.2 suggests that one can use a small set of N preliminary experiments to estimate n*."
  - [section 5.3] "We treat the preliminary results as a sample of size N drawn from a known distribution P over the space of rankings."
- Break condition: If the power-law relationship doesn't hold for the specific experimental domain or kernel choice.

## Foundational Learning

- Concept: Maximum Mean Discrepancy (MMD) and kernel methods
  - Why needed here: MMD provides a principled way to measure distributional similarity that can be customized through kernel choice to match research questions.
  - Quick check question: How does changing the kernel in MMD affect what aspects of experimental results are being compared?

- Concept: Probability theory and empirical distributions
  - Why needed here: The framework treats experimental studies as sampling from an underlying distribution, requiring understanding of random variables and empirical CDFs.
  - Quick check question: What is the relationship between the empirical distribution of experimental results and the true underlying distribution?

- Concept: Ranking theory and permutation spaces
  - Why needed here: Experimental results are formalized as rankings with ties, requiring understanding of ranking spaces and their properties.
  - Quick check question: Why are rankings with ties preferred over raw performance metrics for this framework?

## Architecture Onboarding

- Component map: Probabilistic model -> Kernel selection -> Power-law estimation -> genexpy package
- Critical path:
  1. Define experimental factors and result space
  2. Choose appropriate kernel for research question
  3. Run preliminary experiments
  4. Estimate required sample size using power-law
  5. Validate generalizability
- Design tradeoffs:
  - Kernel specificity vs. generality: More specific kernels capture research questions better but are less flexible
  - Preliminary experiment size vs. estimation accuracy: More preliminary runs improve power-law estimation but increase cost
  - Ranking vs. raw performance: Rankings avoid dataset-specific effects but lose magnitude information
- Failure signatures:
  - Poor kernel choice: Results in uninformative generalizability estimates
  - Insufficient preliminary experiments: Leads to unreliable power-law estimation
  - Non-independent experimental conditions: Violates probability model assumptions
- First 3 experiments:
  1. Implement MMD calculation with a simple kernel (e.g., RBF) on synthetic ranking data
  2. Test kernel choice impact by comparing Borda vs. Mallows kernels on a small benchmark
  3. Validate power-law estimation by comparing predicted vs. actual required sample sizes on a known distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of kernel (Borda, Jaccard, Mallows) impact the generalizability assessment across different types of experimental studies?
- Basis in paper: [explicit] The paper introduces three different kernels (Borda, Jaccard, Mallows) and discusses their suitability for different research questions, showing that the choice of kernel can lead to different generalizability conclusions in case studies.
- Why unresolved: The paper provides theoretical justification and case study examples, but does not systematically compare how different kernels perform across a wide range of experimental study types or establish guidelines for kernel selection in practice.
- What evidence would resolve it: A comprehensive empirical study comparing generalizability assessments using different kernels across diverse experimental study types (e.g., regression benchmarks, classification tasks, LLM evaluations) would help establish best practices for kernel selection.

### Open Question 2
- Question: How does the power-law relationship between sample size and MMD quantiles (Theorem 4.2) depend on the underlying distribution of experimental results?
- Basis in paper: [explicit] Theorem 4.2 establishes a power-law relationship between sample size and MMD quantiles, but notes that the exponent β depends on the kernel, distribution P, and confidence level α.
- Why unresolved: The paper proves the existence of this relationship and provides empirical evidence, but does not characterize how the exponent β varies with different distributions of experimental results or provide theoretical bounds on this variation.
- What evidence would resolve it: A systematic analysis of the power-law exponent β across different families of distributions (e.g., uniform, normal, heavy-tailed) for experimental results would clarify the robustness of this relationship.

### Open Question 3
- Question: What is the optimal number of preliminary experiments needed to obtain a reliable estimate of n* for generalizability?
- Basis in paper: [explicit] Section 5.3 investigates how the number of preliminary experiments affects the accuracy of n* estimates, showing that 20 preliminary experiments provide reasonable estimates, but the analysis is limited to specific distributions and kernel choices.
- Why unresolved: The paper provides empirical evidence for one specific scenario (uniform distributions on rankings) but does not establish general guidelines for determining the optimal number of preliminary experiments across different study types or provide theoretical guarantees on convergence.
- What evidence would resolve it: A theoretical analysis establishing confidence intervals on n* estimates as a function of the number of preliminary experiments, along with empirical validation across diverse experimental study types, would provide practical guidance for experimenters.

## Limitations
- The framework assumes experimental results follow a true underlying distribution, which may not hold for all ML domains
- Power-law relationship between MMD quantiles and sample size needs broader validation across different experimental domains
- The framework's sensitivity to kernel choice and parameter tuning may lead to inconsistent generalizability assessments

## Confidence
- **High confidence**: The mathematical foundation of MMD and kernel methods is well-established in the literature
- **Medium confidence**: The power-law estimation approach shows promise in case studies but requires more extensive validation
- **Low confidence**: The framework's applicability to non-ranking result spaces remains unexplored

## Next Checks
1. Apply the framework to at least three diverse experimental domains (e.g., NLP, CV, RL) with different result spaces and verify the power-law relationship holds consistently
2. Systematically vary kernel parameters and preliminary experiment sizes to quantify their impact on n* estimation accuracy and identify robust parameter ranges
3. Design synthetic experimental studies with known ground truth distributions to test whether the framework correctly identifies when results are truly generalizable versus when apparent consistency is due to chance or bias