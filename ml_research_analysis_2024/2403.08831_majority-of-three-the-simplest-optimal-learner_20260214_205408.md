---
ver: rpa2
title: 'Majority-of-Three: The Simplest Optimal Learner?'
arxiv_id: '2403.08831'
source_url: https://arxiv.org/abs/2403.08831
tags:
- algorithm
- function
- bound
- lemma
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the simplest optimal PAC learner in the
  realizable setting, where ERM is suboptimal. The authors study the Majority-of-Three
  algorithm, which partitions data into three subsets, runs ERM on each, and returns
  the majority vote.
---

# Majority-of-Three: The Simplest Optimal Learner?

## Quick Facts
- arXiv ID: 2403.08831
- Source URL: https://arxiv.org/abs/2403.08831
- Reference count: 33
- Key outcome: Majority-of-Three algorithm achieves optimal PAC learning bounds in the realizable setting where ERM is suboptimal

## Executive Summary
This paper investigates the simplest optimal learning algorithm in the realizable PAC setting, where standard Empirical Risk Minimization (ERM) is known to be suboptimal by a logarithmic factor. The authors introduce the Majority-of-Three algorithm, which partitions data into three disjoint subsets, runs ERM on each, and returns the majority vote. They prove that this simple algorithm achieves the optimal in-expectation error bound of O(d/n) and provides a near-optimal high-probability bound. The work also establishes a lower bound showing that Simon's algorithm, which uses overlapping ERMs, is sub-optimal, and conjectures that Majority-of-Three is in fact optimal in the high-probability regime for all δ.

## Method Summary
The Majority-of-Three algorithm partitions the training data into three equal-sized disjoint subsets, trains an ERM classifier on each subset, and returns the majority vote of these three classifiers. The key insight is that by using disjoint subsets, the probability of two ERMs making a mistake on the same point is the square of the individual error probability, which is sufficiently small to achieve optimal bounds. The paper proves both in-expectation and high-probability error bounds using concentration inequalities and careful analysis of conditional distributions.

## Key Results
- Majority-of-Three achieves optimal in-expectation error bound of O(d/n), unattainable by single ERM
- Near-optimal high-probability bound of O(d/n log(log(min{n/d,1/δ})) + 1/n log(1/δ))
- Lower bound showing Simon's algorithm with overlapping ERMs is sub-optimal
- Conjecture that Majority-of-Three is optimal in high-probability regime for all δ

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Majority-of-Three achieves optimal in-expectation error bound by exploiting low-probability joint error events across disjoint subsets
- Mechanism: Partitioning data into three disjoint subsets S1, S2, S3 ensures that the probability of all three ERM classifiers making a mistake on the same point x is the square of the individual error probability px. This joint error probability is sufficiently small across the distribution to maintain optimal aggregate error.
- Core assumption: The error probability px of a single ERM on point x is independent across disjoint subsets and sufficiently small in aggregate.
- Evidence anchors:
  - [abstract]: "The authors study the Majority-of-Three algorithm, which partitions data into three subsets, runs ERM on each, and returns the majority vote."
  - [section]: "Using the above and Lemma 2.1 gives us E [errP (Maj(ˆfS1, ˆfS2, ˆfS3))] ≤ 3c d/n."
  - [corpus]: Weak. Corpus lacks direct evidence of majority voting error bounds.
- Break condition: If the disjoint subsets are not truly independent (e.g., due to overlapping support in the data distribution), the joint error probability could be higher than assumed, degrading performance.

### Mechanism 2
- Claim: The Majority-of-Three algorithm avoids the logarithmic factor penalty that affects single ERM by distributing the error burden across multiple subsets.
- Mechanism: Single ERM suffers from a ln(n/d) factor due to worst-case error concentration. By using three disjoint subsets and majority voting, the algorithm smooths out this concentration, achieving the optimal O(d/n) bound.
- Core assumption: The ln(n/d) penalty for single ERM is avoidable through proper aggregation of multiple ERMs.
- Evidence anchors:
  - [abstract]: "They prove that this algorithm achieves the optimal in-expectation error bound of O(d/n), which is unattainable by a single ERM."
  - [section]: "We note here that a single ERM alone is sub-optimal by a multiplicative ln(n/d) factor in-expectation."
  - [corpus]: Weak. Corpus lacks direct evidence of ln(n/d) penalties in single ERM.
- Break condition: If the ln(n/d) penalty is inherent to the VC dimension constraint rather than avoidable through aggregation, the Majority-of-Three bound may not hold in general.

### Mechanism 3
- Claim: The algorithm achieves near-optimal high-probability bounds by controlling the tail of the error distribution through careful analysis of conditional distributions.
- Mechanism: By partitioning the instance space into sets Ri based on error probability px and using union bounds with Markov's inequality, the algorithm controls the probability of large joint errors across subsets.
- Core assumption: The error distribution can be sufficiently characterized by partitioning based on px and applying concentration inequalities.
- Evidence anchors:
  - [abstract]: "They show a near-optimal high-probability bound of O(d/n log(log(min{n/d,1/δ})) + 1/n log(1/δ))."
  - [section]: "We will prove that there is a universal constant c > 0 such that the events E1 and E2 each happen with probability at least 1 − δ/2."
  - [corpus]: Weak. Corpus lacks direct evidence of high-probability error bounds.
- Break condition: If the error distribution has heavier tails than assumed, or if the partitioning strategy fails to capture the true error structure, the high-probability bound may not hold.

## Foundational Learning

- Concept: Vapnik-Chervonenkis (VC) dimension and its role in PAC learning bounds.
  - Why needed here: The VC dimension d determines the sample complexity and error bounds for the learning algorithm. The Majority-of-Three algorithm's optimality is proven in terms of d.
  - Quick check question: What is the VC dimension of a function class consisting of unions of at most 2d intervals on [0,1]?

- Concept: Empirical Risk Minimization (ERM) and its limitations in the realizable setting.
  - Why needed here: The paper shows that ERM alone is suboptimal, motivating the need for the Majority-of-Three algorithm.
  - Quick check question: Why does a single ERM suffer from a ln(n/d) factor in its error bound, while the Majority-of-Three algorithm does not?

- Concept: Concentration inequalities and their application to PAC learning bounds.
  - Why needed here: The high-probability bounds in the paper rely on concentration inequalities like Markov's inequality and union bounds.
  - Quick check question: How does Markov's inequality help control the joint error probability of two ERMs trained on disjoint subsets?

## Architecture Onboarding

- Component map: Data partitioning -> ERM training on each subset -> Majority voting
- Critical path: The critical path is the sequential execution of data partitioning, ERM training on each subset, and majority voting. The algorithm's performance depends on the quality of the ERM implementation and the independence of the subsets.
- Design tradeoffs: The choice of three subsets balances computational cost with error reduction. Using more subsets could further reduce error but at increased computational cost. Using fewer subsets may not achieve the optimal bound.
- Failure signatures: If the data partitioning introduces bias or correlation between subsets, the algorithm's error bounds may not hold. If the ERM implementation is not consistent (i.e., it does not always return a classifier that fits the training data), the algorithm may fail to achieve the optimal bounds.
- First 3 experiments:
  1. Verify that a single ERM on the full dataset exhibits the ln(n/d) factor in its error bound.
  2. Implement the Majority-of-Three algorithm and verify that it achieves the O(d/n) in-expectation bound on synthetic data.
  3. Test the high-probability bound by varying the confidence parameter δ and measuring the empirical error.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is Majority-of-Three optimal in the high-probability regime for all δ?
- Basis in paper: The authors conjecture this in Conjecture 1.3, stating that Majority-of-Three achieves the optimal error bound of O(d/n + 1/n log(1/δ)) for any parameter δ ∈ (0, 1).
- Why unresolved: The current analysis only proves a near-optimal high-probability bound with an additional log(log(min{n/d, 1/δ})) term.
- What evidence would resolve it: A proof showing that Majority-of-Three achieves the error bound O(d/n + 1/n log(1/δ)) for all δ, or a counterexample demonstrating that it cannot.

### Open Question 2
- Question: Can the log(log(min{n/d, 1/δ})) term in Theorem 1.2 be removed?
- Basis in paper: The authors note that the bound in Theorem 1.2 is sub-optimal due to this term, but it dominates for δ ≤ d^{-d}.
- Why unresolved: The proof technique used in Section 4 to derive Theorem 1.2 introduces this additional logarithmic factor.
- What evidence would resolve it: A new analysis that removes the log(log(min{n/d, 1/δ})) term while maintaining the high-probability guarantee, or a proof that this term is necessary for the given approach.

### Open Question 3
- Question: Are there other simple improper learning algorithms that are optimal in the realizable setting?
- Basis in paper: The authors discuss the simplicity of Majority-of-Three and ask what the simplest possible optimal algorithm is, given that ERM is suboptimal and Bagging has a more complex analysis.
- Why unresolved: The focus of the paper is on Majority-of-Three, and while it is shown to be optimal in expectation and near-optimal in high probability, the question of other simple optimal algorithms remains open.
- What evidence would resolve it: A proof that Majority-of-Three is the simplest optimal algorithm, or the discovery of another simple improper learning algorithm with optimal error bounds in both expectation and high probability.

## Limitations

- The high-probability bound includes an additional log(log(min{n/d, 1/δ})) term that may not be necessary
- The analysis relies on the assumption that disjoint subsets provide independent error guarantees, which may not hold for all data distributions
- The lower bound construction for Simon's algorithm is complex and its generalizability to other settings remains unclear

## Confidence

- High confidence: The in-expectation error bound of O(d/n) for Majority-of-Three
- Medium confidence: The near-optimal high-probability bound with the log(log(min{n/d, 1/δ})) term
- Medium confidence: The lower bound showing Simon's algorithm is suboptimal

## Next Checks

1. Implement the Majority-of-Three algorithm on real-world datasets with varying VC dimensions to empirically verify the O(d/n) in-expectation bound across different data distributions.

2. Construct and analyze a synthetic dataset where the three subsets are not truly independent (e.g., through correlated sampling) to test whether the algorithm's error bounds still hold under these conditions.

3. Reproduce the lower bound construction from Theorem 1.4 in a simplified setting to verify the suboptimality of Simon's algorithm and understand the relationship between the number of overlapping subsets and error bounds.