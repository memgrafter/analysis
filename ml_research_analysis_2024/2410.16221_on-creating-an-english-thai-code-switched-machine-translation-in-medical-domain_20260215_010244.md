---
ver: rpa2
title: On Creating an English-Thai Code-switched Machine Translation in Medical Domain
arxiv_id: '2410.16221'
source_url: https://arxiv.org/abs/2410.16221
tags:
- translation
- medical
- mask
- machine
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurately translating medical
  terminology from English to Thai, a task where standard machine translation approaches
  often underperform. The authors propose a method that preserves medical terminology
  in English within the translated text through code-switched (CS) translation.
---

# On Creating an English-Thai Code-switched Machine Translation in Medical Domain

## Quick Facts
- arXiv ID: 2410.16221
- Source URL: https://arxiv.org/abs/2410.16221
- Reference count: 28
- Authors: Parinthapat Pengpun; Krittamate Tiankanon; Amrest Chinkamol; Jiramet Kinchagawat; Pitchaya Chairuengjitjaras; Pasit Supholkhan; Pubordee Aussavavirojekul; Chiraphat Boonnag; Kanyakorn Veerakanjana; Hirunkul Phimsiri; Boonthicha Sae-jia; Nattawach Sataudom; Piyalitt Ittichaiwong; Peerat Limkonchotiwat
- One-line primary result: Medical professionals prefer code-switched translations that preserve English medical terminology over fully translated Thai versions, even at slight cost to fluency

## Executive Summary
This paper addresses the challenge of accurately translating medical terminology from English to Thai, where standard machine translation approaches often underperform due to lack of equivalent Thai terms. The authors propose a method that preserves medical terminology in English within the translated text through code-switched (CS) translation. They developed a process to generate CS medical translation data by masking medical keywords during initial translation, then having annotators post-process the results. A CS translation model was fine-tuned on this data and evaluated against strong baselines including Google NMT and GPT-3.5/GPT-4. The model demonstrated competitive performance on automatic metrics and was preferred by medical professionals in human evaluations, particularly for factual accuracy.

## Method Summary
The authors developed a code-switched medical translation system for English-Thai translation that preserves medical terminology in English. They first used GPT-4 to identify medical keywords in English text, masked these keywords, and translated the remaining text using Google NMT. The masked keywords were then replaced back into the translated text, creating code-switched examples. This dataset was used to fine-tune NLLB 3.3B, with multiple variants tested (64k, 128k, 30k, 40k, 70k samples). The approach was evaluated using both automated metrics (BLEU, COMET, CS F1) and human evaluation by medical doctors on factual accuracy and preference ranking.

## Key Results
- Medical professionals significantly preferred code-switched translations over fully translated Thai versions (CS F1 metric)
- The NLLB-4 model (filtered 64k dataset with masking) showed the best performance among open-source models
- GPT-4 with prompting achieved the highest factual accuracy (1.53 error rate) among all models
- Google NMT achieved top scores on automated metrics but was not preferred by medical professionals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code-switching preserves medical terminology accuracy better than full translation.
- Mechanism: By masking medical keywords during translation and keeping them in English within the target text, the system avoids the risk of mistranslating domain-specific terms that lack equivalent Thai expressions.
- Core assumption: Medical professionals prefer retaining English terms for critical concepts rather than risking semantic distortion through translation.
- Evidence anchors:
  - [abstract] "Our research prioritizes not merely improving translation accuracy but also maintaining medical terminology in English within the translated text through code-switched (CS) translation."
  - [section 1] "Translating medical terminology accurately is challenging due to the lack of equivalent Thai terms for some English medical keywords."
  - [corpus] Weak - related papers focus on general code-switching or speech translation, not medical terminology preservation.
- Break condition: If medical terminology translation accuracy improves significantly in the target language, the need for code-switching diminishes.

### Mechanism 2
- Claim: Fine-tuning on code-switched medical data improves performance on this specific task.
- Mechanism: Training NLLB on a dataset containing code-switched examples teaches the model to handle mixed-language input/output patterns, leading to better preservation of English medical terms while translating surrounding text.
- Core assumption: Models can learn effective code-switching behavior when exposed to sufficient paired examples during training.
- Evidence anchors:
  - [section 4.1] "We employed a state-of-the-art language translation model, NLLB. We utilized NLLB 3.3B as a base model and fine-tuned it on six variants of our training dataset."
  - [section 5] "NLLB-1 (initial 64k dataset) and NLLB-4 (filtered 64k dataset) with Mask... These models demonstrate remarkable results among open-source models."
  - [corpus] Weak - while CS-FLEURS covers code-switching, it's not medical domain specific.
- Break condition: If training data quality drops below a threshold, fine-tuning may not improve or could degrade model performance.

### Mechanism 3
- Claim: Human preference evaluation reveals limitations of automated metrics in medical translation.
- Mechanism: Medical professionals evaluating translations for factual accuracy and preference uncover mismatches between traditional MT metrics (BLEU, COMET) and what constitutes a "good" medical translation.
- Core assumption: Factual accuracy and medical term preservation are more important to medical professionals than general fluency.
- Evidence anchors:
  - [section 6.1] "Google NMT consistently achieves top scores across nearly all machine metrics... despite the lack of medical terminology preservation."
  - [section 6.2] "We found that MDs preferred CS translation over translating all words into the target language, as indicated by the CS F1 metric."
  - [section 4.2.2] "Our methodology is as follows... medical professionals assess each translation's factual correctness using our specific rubric."
- Break condition: If automated metrics are specifically designed and validated for medical domain translation quality, they may better align with human preferences.

## Foundational Learning

- Concept: Code-switching in language
  - Why needed here: Understanding that the approach deliberately mixes English and Thai rather than translating everything to Thai is fundamental to grasping the methodology.
  - Quick check question: What is the difference between code-switching and simple borrowing of foreign terms in translation?

- Concept: Named Entity Recognition (NER) for medical terms
  - Why needed here: The masking process relies on identifying medical keywords that should be preserved, requiring knowledge of how NER works in specialized domains.
  - Quick check question: How does medical NER differ from general NER, and why is GPT-4 particularly suited for this task?

- Concept: Translation Edit Rate (TER) and error metrics
  - Why needed here: Understanding why traditional metrics like BLEU may not capture the quality of medical translations requires familiarity with various evaluation approaches.
  - Quick check question: Why might a translation with lower BLEU score be preferred by medical professionals?

## Architecture Onboarding

- Component map:
  - English medical text -> GPT-4 keyword masking -> Monolingual translation -> Placeholder replacement -> CS dataset
  - Model training: NLLB fine-tuning on CS medical data
  - Evaluation: Automated metrics + human MD evaluation
  - Inference: Keyword masking during translation -> MT model -> Placeholder restoration

- Critical path:
  - Data quality -> Model fine-tuning -> Human evaluation -> Preference ranking
  - Any failure in medical keyword identification or data filtering directly impacts model performance

- Design tradeoffs:
  - CS preservation vs. fluency: Maintaining English terms improves accuracy but may reduce Thai readability
  - Data augmentation vs. cost: Back-translation improves dataset but requires LLM API calls
  - Model size vs. inference speed: Larger NLLB variants may perform better but slower

- Failure signatures:
  - Low CS F1 score: Model isn't preserving English medical terms adequately
  - Poor human preference despite good BLEU: Model is fluent but factually inaccurate
  - High factual error rate: Data quality issues or masking failures

- First 3 experiments:
  1. Test GPT-4 keyword extraction accuracy on a small sample with MD validation
  2. Compare CS F1 scores with and without masking on a validation set
  3. Run human evaluation on 10 samples comparing Google NMT vs. NLLB-1 with masking

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific types of medical terminology (e.g., anatomical terms vs. pharmaceutical names) impact the effectiveness of code-switching vs. full translation?
- Basis in paper: [explicit] The paper notes that some medical terms lack Thai equivalents, but doesn't analyze which types of terms are most problematic.
- Why unresolved: The analysis focuses on overall performance but doesn't break down by medical terminology category.
- What evidence would resolve it: Detailed analysis comparing CS effectiveness across different medical terminology categories (anatomical, pharmaceutical, symptoms, procedures, etc.).

### Open Question 2
- Question: What is the optimal threshold for code-switching vs. translation in medical texts, and how does this vary by text complexity?
- Basis in paper: [inferred] The paper shows CS is preferred but doesn't explore the optimal ratio of English to Thai terms or how this varies by context.
- Why unresolved: The study uses a fixed approach to code-switching without exploring the impact of different CS ratios.
- What evidence would resolve it: Systematic experiments varying the proportion of code-switched terms and measuring impact on both medical accuracy and readability.

### Open Question 3
- Question: How does the performance of the proposed CS approach generalize to other low-resource language pairs with specialized domains?
- Basis in paper: [explicit] The paper focuses specifically on English-Thai medical translation and acknowledges limited prior work in this specific context.
- Why unresolved: Results are specific to one language pair and domain, limiting generalizability.
- What evidence would resolve it: Replication studies testing the CS approach across different language pairs (e.g., English-Arabic, English-Spanish) and specialized domains (legal, technical, etc.).

## Limitations

- The study only evaluates on HIV medical domain, limiting generalizability to other medical specialties
- Exact GPT-4 prompting strategy for medical keyword extraction is not specified, making replication difficult
- Reliance on LLM API calls for data generation may not scale efficiently to larger datasets

## Confidence

**High confidence** in the core finding that medical professionals prefer code-switched translations for factual accuracy preservation. The human evaluation results are robust and show clear preference patterns.

**Medium confidence** in the relative performance of different model variants. While automated metrics show consistent patterns, the differences between NLLB models (64k vs 128k, with/without masking) are not always statistically significant, and the paper lacks confidence intervals.

**Low confidence** in the scalability of the data generation approach. The masking-and-replacement pipeline relies on LLM API calls and human annotation, which may not scale efficiently to larger datasets or other language pairs.

## Next Checks

1. **Prompt engineering validation**: Systematically test different GPT-4 prompts for medical keyword extraction on a held-out validation set with MD verification to determine optimal prompt structure and identify potential hallucination patterns.

2. **Domain generalization test**: Apply the same methodology to a different medical specialty (e.g., oncology or cardiology) to verify whether the code-switching advantage holds across medical domains beyond HIV.

3. **Cost-benefit analysis**: Measure the trade-off between translation accuracy gains and increased annotation costs by comparing human evaluation scores against the time/cost required for the masking-and-replacement data generation pipeline.