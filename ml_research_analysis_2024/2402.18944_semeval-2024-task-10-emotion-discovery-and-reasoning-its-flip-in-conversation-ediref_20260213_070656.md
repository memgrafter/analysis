---
ver: rpa2
title: 'SemEval 2024 -- Task 10: Emotion Discovery and Reasoning its Flip in Conversation
  (EDiReF)'
arxiv_id: '2402.18944'
source_url: https://arxiv.org/abs/2402.18944
tags:
- task
- emotion
- code-mixed
- teams
- trigger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SemEval-2024 Task 10 on Emotion Discovery and
  Reasoning its Flip in Conversation (EDiReF), which focuses on identifying emotions
  and finding the rationale behind their flips within monolingual English and Hindi-English
  code-mixed dialogues. The task comprises three distinct subtasks - emotion recognition
  in conversation for code-mixed dialogues, emotion flip reasoning for code-mixed
  dialogues, and emotion flip reasoning for English dialogues.
---

# SemEval 2024 -- Task 10: Emotion Discovery and Reasoning its Flip in Conversation (EDiReF)

## Quick Facts
- arXiv ID: 2402.18944
- Source URL: https://arxiv.org/abs/2402.18944
- Reference count: 17
- Most adept systems attained F1-scores of 0.70, 0.79, and 0.76 for the respective subtasks

## Executive Summary
This paper presents SemEval-2024 Task 10 on Emotion Discovery and Reasoning its Flip in Conversation (EDiReF), which focuses on identifying emotions and finding the rationale behind their flips within monolingual English and Hindi-English code-mixed dialogues. The task comprises three distinct subtasks - emotion recognition in conversation for code-mixed dialogues, emotion flip reasoning for code-mixed dialogues, and emotion flip reasoning for English dialogues. A total of 84 participants engaged in this task, with the most adept systems attaining F1-scores of 0.70, 0.79, and 0.76 for the respective subtasks.

## Method Summary
The task involves emotion recognition in code-mixed dialogues and identifying triggers for emotion flips. The dataset comprises manually annotated conversations from TV series "F.R.I.E.N.D.S" and "Sarabhai vs Sarabhai", including emotions and emotion flip triggers. Various approaches were used by participants, including LLMs (BERT, RoBERTa, GPT), classical ML methods (SVM, XGBoost), and rule-based methods. The evaluation metrics used are weighted F1-score for emotion recognition tasks and F1-score for trigger identification tasks.

## Key Results
- Most adept systems attained F1-scores of 0.70, 0.79, and 0.76 for the respective subtasks
- LLMs dominate approaches for both ERC and EFR, but machine learning methods also remain popular
- A total of 84 participants engaged in this task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based architectures outperform traditional ML in emotion recognition due to better contextual understanding of code-mixed and multilingual inputs.
- Mechanism: Pre-trained LLMs like BERT, RoBERTa, and DistilBERT capture rich linguistic patterns and can be fine-tuned for task-specific performance.
- Core assumption: The dataset size (~8500 utterances for ERC) is sufficient for LLMs to generalize but not so large that simpler models cannot perform well.
- Evidence anchors:
  - [abstract]: "the most adept systems attaining F1-scores of 0.70, 0.79, and 0.76 for the respective subtasks"
  - [section]: "LLMs dominate the approaches for both ERC and EFR, machine learning methods also remain popular among the participants"
  - [corpus]: "average neighbor FMR=0.518" (indicates moderate similarity to related work using LLMs)
- Break condition: If the dataset size were significantly larger (>100k utterances), simpler models like DistilBERT may lose their performance advantage due to limited capacity.

### Mechanism 2
- Claim: Classical machine learning methods (e.g., XGBoost) can outperform LLMs in emotion flip reasoning (EFR) because trigger patterns are often simple and rule-based.
- Mechanism: In many cases, emotion flips are triggered by the immediately preceding utterance, making a rule-based or shallow ML classifier effective.
- Core assumption: The majority of emotion flips in the dataset follow a predictable pattern where the i-1 utterance is the trigger.
- Evidence anchors:
  - [section]: "the fundamental objective of this task is to discern these trigger utterances ( u5 and u7) given a target emotion-flip utterance (u8)"
  - [section]: "a trigger is defined as any utterance within the contextual history of the target utterance"
  - [corpus]: "Found 25 related papers (using 8)" (suggests some prior work on rule-based approaches)
- Break condition: If the dataset contained more complex emotion flip patterns (e.g., multiple triggers spread across distant utterances), XGBoost and rule-based methods would underperform.

### Mechanism 3
- Claim: Speaker embeddings improve emotion recognition and flip reasoning by providing contextual speaker-specific information.
- Mechanism: Incorporating speaker identity into the model allows it to differentiate between speakers' emotional patterns and contextual influences.
- Core assumption: Different speakers have distinct emotional expression styles that can be learned from the data.
- Evidence anchors:
  - [section]: "speaker s and assigned an emotion label e ∈[anger, disgust, fear, sadness, surprise, joy, neutral]"
  - [section]: "Speaker B undergoes a transition in emotion (neutral→fear) between utterances u6 and u8"
  - [corpus]: "average citations=0.0" (limited prior work on speaker embeddings in this specific task)
- Break condition: If the dataset had too few speakers or if emotional patterns were speaker-independent, speaker embeddings would add noise rather than improve performance.

## Foundational Learning

- Concept: Code-mixing in NLP
  - Why needed here: The task involves Hindi-English code-mixed dialogues, requiring models to handle linguistic mixing.
  - Quick check question: What are the common challenges in processing code-mixed text compared to monolingual text?
- Concept: Emotion flip reasoning
  - Why needed here: Identifying the trigger utterance for an emotion flip is the core challenge of EFR subtasks.
  - Quick check question: How does emotion flip reasoning differ from standard emotion recognition?
- Concept: Evaluation metrics for classification tasks
  - Why needed here: Weighted F1-score is used to evaluate both ERC and EFR tasks, requiring understanding of precision, recall, and class imbalance.
  - Quick check question: Why is weighted F1-score preferred over accuracy for imbalanced emotion datasets?

## Architecture Onboarding

- Component map: Input -> Backbone -> Optional -> Output
  - Input: Dialogue with utterances, speakers, and emotions (for EFR)
  - Backbone: LLM (BERT/RoBERTa/DistilBERT) or classical ML (XGBoost/SVM)
  - Optional: Speaker embeddings, context window selection
  - Output: Emotion labels (ERC) or trigger utterance indices (EFR)
- Critical path:
  - Load and preprocess dialogue data
  - Encode utterances using LLM or classical features
  - Apply classification head (softmax for ERC, binary for EFR)
  - Evaluate using weighted F1-score
- Design tradeoffs:
  - LLM vs classical ML: LLMs capture complex context but require more data; classical ML is faster but may miss nuanced patterns
  - Context window size: Larger windows capture more context but increase computational cost
  - Speaker embeddings: Improve personalization but add complexity
- Failure signatures:
  - Low precision: Model predicts too many emotion flips or incorrect emotions
  - Low recall: Model misses actual emotion flips or emotion changes
  - High class imbalance: Neutral/emotion classes dominate, skewing metrics
- First 3 experiments:
  1. Fine-tune DistilBERT on ERC task with default hyperparameters
  2. Train XGBoost classifier on EFR task using i-1 utterance as trigger baseline
  3. Compare performance of LLM vs classical ML on a small validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can emotion-flip reasoning models be improved to handle implicit triggers, which are not explicitly mentioned in the dialogue?
- Basis in paper: [explicit] The paper discusses the challenge of implicit triggers, where emotion flips are caused by events external to the dialogue and not explicitly mentioned.
- Why unresolved: The paper acknowledges the existence of implicit triggers but does not provide a concrete solution for how models can be improved to handle them.
- What evidence would resolve it: A proposed method or approach that demonstrates improved performance on emotion-flip reasoning tasks when dealing with implicit triggers.

### Open Question 2
- Question: Can the performance gap between tasks A and B/C be reduced by incorporating more context or information about the dialogue?
- Basis in paper: [inferred] The paper shows a significant performance gap between tasks A (ERC in code-mixed dialogues) and tasks B/C (EFR in code-mixed and English dialogues). This suggests that the additional context required for EFR might be challenging for current models.
- Why unresolved: The paper does not explore whether incorporating more context or information about the dialogue could improve the performance of EFR models.
- What evidence would resolve it: An experiment that demonstrates improved performance on tasks B/C by incorporating additional context or information about the dialogue.

### Open Question 3
- Question: How does the performance of emotion-flip reasoning models vary across different languages and cultural contexts?
- Basis in paper: [explicit] The paper presents a task focused on emotion-flip reasoning in both English and Hindi-English code-mixed dialogues, highlighting the need for research in this area.
- Why unresolved: The paper does not provide a comparative analysis of the performance of emotion-flip reasoning models across different languages and cultural contexts.
- What evidence would resolve it: A study that evaluates the performance of emotion-flip reasoning models on datasets from different languages and cultural contexts, comparing the results to identify any variations.

## Limitations

- The paper lacks detailed implementation specifics for top-performing models, making direct reproduction challenging
- Limited discussion of cross-lingual transfer between English and Hindi-English code-mixed data
- No ablation studies on critical components like speaker embeddings or context window sizes

## Confidence

- **High Confidence**: The overall task formulation and dataset creation methodology are well-documented and reproducible
- **Medium Confidence**: The reported F1-scores (0.70, 0.79, 0.76) are reliable given the evaluation methodology, though implementation details vary
- **Low Confidence**: Specific architectural choices and hyperparameter settings for top-performing models are not sufficiently detailed

## Next Checks

1. **Reproduce baseline results**: Implement and evaluate a DistilBERT-based model on the provided dataset to verify the reported ERC F1-score of 0.70
2. **Cross-lingual consistency test**: Train a model on English data and evaluate on Hindi-English code-mixed data to assess language transfer capabilities
3. **Speaker embedding ablation**: Conduct experiments comparing models with and without speaker embeddings to quantify their impact on performance