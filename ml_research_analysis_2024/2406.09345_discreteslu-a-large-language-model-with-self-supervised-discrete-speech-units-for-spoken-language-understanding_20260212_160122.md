---
ver: rpa2
title: 'DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units
  for Spoken Language Understanding'
arxiv_id: '2406.09345'
source_url: https://arxiv.org/abs/2406.09345
tags:
- speech
- arxiv
- language
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiscreteSLU, a novel approach for spoken
  language understanding (SLU) using large language models (LLMs) with discrete speech
  units (DSU). The authors propose using DSU, generated from self-supervised speech
  encoders and k-means clustering, instead of continuous-valued speech encoder outputs.
---

# DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding

## Quick Facts
- arXiv ID: 2406.09345
- Source URL: https://arxiv.org/abs/2406.09345
- Reference count: 0
- Key outcome: Introduces DiscreteSLU, using discrete speech units (DSU) instead of continuous speech encoder outputs for spoken language understanding, showing robust performance on seen/unseen domains and instruction-following capability

## Executive Summary
This paper presents DiscreteSLU, a novel approach for spoken language understanding (SLU) that integrates large language models (LLMs) with discrete speech units (DSU) derived from self-supervised speech encoders. The authors propose replacing traditional continuous-valued speech encoder outputs with DSU, generated through k-means clustering of WavLM representations, and converting these to LLM token embeddings via a speech adapter. The model demonstrates robust performance across seen and unseen domains while maintaining instruction-following capabilities for spoken question answering tasks. Notably, the study finds that ASR task and datasets are not crucial for instruction-tuning in spoken question answering, and the model achieves zero-shot speech-to-text translation without explicit translation training.

## Method Summary
DiscreteSLU processes speech inputs through a self-supervised encoder (WavLM), generates discrete speech units via k-means clustering, and uses a speech adapter to convert these into LLM-compatible embeddings. The system is fine-tuned on multiple datasets (Tedlium 3 for ASR, SLUE-SQA5 for question answering, etc.) using LoRA with specified hyperparameters. The model architecture includes a speech encoder, speech adapter with two 2D convolution layers and transformer layers, and an LLM (Mistral-7B-v0.1). Training involves generating DSU from WavLM layer 21 representations, applying subword modeling for compression, and fine-tuning the entire system on diverse instruction datasets.

## Key Results
- DiscreteSLU shows robust performance on speech inputs from seen and unseen domains
- The model achieves zero-shot capability for speech-to-text translation without translation training
- Using MFCCs instead of WavLM embeddings degrades ASR performance but maintains similar SQA performance, suggesting transcription may not be necessary for certain understanding tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discrete speech units (DSU) preserve domain-general speech features while abstracting away domain-specific details
- Mechanism: DSU are generated via k-means clustering of self-supervised speech representations, creating a fixed vocabulary of speech patterns that capture phonetic and acoustic information independent of domain
- Core assumption: The k-means clustering process creates units that represent universal speech patterns rather than domain-specific characteristics
- Evidence anchors: Abstract mentions robust performance on seen/unseen domains; section discusses DSU as more general-purpose input; weak corpus evidence
- Break condition: If k-means clustering fails to capture meaningful speech patterns, or if domain-specific features are critical for task performance

### Mechanism 2
- Claim: DSU can replace traditional ASR transcription for certain language understanding tasks
- Mechanism: The model can directly map DSU to semantic understanding without intermediate transcription, leveraging pre-trained knowledge
- Core assumption: Language models can perform semantic reasoning directly from speech representations without explicit transcription
- Evidence anchors: Abstract mentions MFCC findings suggesting transcription may not be necessary; section discusses training on ASR data being beneficial for DSU but not MFCC; weak corpus evidence
- Break condition: If task requires precise lexical information, or if model lacks sufficient pre-trained knowledge for direct reasoning

### Mechanism 3
- Claim: Speech adapter bridges the representation gap between DSU and LLM token space
- Mechanism: The adapter converts DSU embeddings into continuous representations that align with LLM token embeddings, enabling seamless integration
- Core assumption: There exists a learnable mapping between DSU representations and LLM token space that preserves semantic information
- Evidence anchors: Abstract mentions use of speech adapter; section describes adapter converting DSU to continuous embedding sequence; weak corpus evidence
- Break condition: If representation spaces are fundamentally incompatible, or if adapter capacity is insufficient

## Foundational Learning

- Concept: Self-supervised speech representation learning
  - Why needed here: Provides rich speech features without requiring labeled data, enabling better DSU generation
  - Quick check question: What is the key difference between self-supervised and supervised speech representation learning?

- Concept: K-means clustering for discrete representation
  - Why needed here: Converts continuous speech features into discrete units that can be efficiently processed by LLMs
  - Quick check question: How does the choice of k (number of clusters) affect the quality of discrete speech units?

- Concept: Length reduction techniques for speech sequences
  - Why needed here: Reduces computational cost by compressing speech sequences while preserving important information
  - Quick check question: What are the tradeoffs between different length reduction methods (deduplication vs. subword modeling)?

## Architecture Onboarding

- Component map: Speech input → Self-supervised encoder → DSU generation → Speech adapter → LLM; Text instruction → Tokenizer → LLM embedding → Concatenation with speech adapter output; LLM output → Post-processing

- Critical path: 1. DSU generation from speech input; 2. Speech adapter processing; 3. LLM instruction following; 4. Output generation

- Design tradeoffs: DSU vs. continuous representations (DSU offers better domain generalization but may lose fine-grained acoustic information); Layer selection for DSU extraction (deeper layers capture more semantic information but may lose phonetic details); Length reduction (improves efficiency but may discard important information)

- Failure signatures: Poor ASR performance (may indicate DSU not capturing enough phonetic information); Weak instruction following (could suggest adapter not properly aligning DSU with LLM space); Domain-specific failures (might indicate DSU not abstracting away domain details effectively)

- First 3 experiments: 1. Compare DSU performance against continuous representations on seen domain data; 2. Test DSU performance on out-of-domain speech data; 3. Evaluate different layer selections for DSU extraction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DiscreteSLU compare to traditional ASR models when using MFCCs versus WavLM embeddings, and what are the implications for computational efficiency and accuracy?
- Basis in paper: The paper explores MFCCs as an alternative to WavLM, finding ASR degrades while SQA remains similar
- Why unresolved: Initial comparisons provided but no thorough analysis of tradeoffs between MFCCs and WavLM in terms of computational efficiency and accuracy
- What evidence would resolve it: Comprehensive evaluation comparing performance and computational efficiency of DiscreteSLU using MFCCs versus WavLM across range of speech tasks

### Open Question 2
- Question: What is the impact of using different layers of the self-supervised speech encoder on the performance of DiscreteSLU, and how does this affect the model's ability to generalize to unseen domains?
- Basis in paper: Paper investigates various types of DSUs from different layers, finding shallow layers result in significant ASR degradation but SQA remains similar
- Why unresolved: Provides initial insights but doesn't fully explore implications of different layers for performance and generalization
- What evidence would resolve it: Detailed analysis of impact of different encoder layers on performance across tasks and domains, including comparison of generalization capabilities

### Open Question 3
- Question: How does the use of DiscreteSLU with DSUs affect the model's ability to perform zero-shot tasks, such as speech-to-text translation, and what are the underlying mechanisms that enable this capability?
- Basis in paper: Model demonstrates zero-shot capability for speech-to-text translation without translation training
- Why unresolved: Presents finding but doesn't fully explore mechanisms behind zero-shot capabilities or potential for further generalization
- What evidence would resolve it: In-depth study of model's zero-shot capabilities across various tasks, including analysis of underlying mechanisms and experiments to test limits

## Limitations
- Performance gains on out-of-domain data are demonstrated only on limited domain shifts
- Comparison between DSU and traditional ASR transcription for language understanding is preliminary with conflicting results
- Claim that ASR tasks are "not crucial" appears to be an overstatement based on mixed evidence

## Confidence

- **High confidence**: The basic feasibility of using DSU with LLMs for SLU tasks
- **Medium confidence**: The domain generalization benefits of DSU
- **Low confidence**: The claim about ASR tasks being "not crucial" for instruction-following

## Next Checks

1. Cross-domain robustness validation: Test the model on a broader range of domain shifts beyond limited examples, including different accents, recording conditions, and languages

2. Ablation on speech adapter architecture: Systematically vary the speech adapter architecture to determine how much performance depends on specific design choices versus the DSU approach itself

3. Task-specific transcription requirement analysis: Conduct controlled experiments comparing DSU-based approaches against traditional ASR transcription across different task types to better understand when transcription is actually necessary