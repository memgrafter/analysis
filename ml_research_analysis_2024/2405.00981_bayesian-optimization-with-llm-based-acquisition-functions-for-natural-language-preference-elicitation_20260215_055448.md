---
ver: rpa2
title: Bayesian Optimization with LLM-Based Acquisition Functions for Natural Language
  Preference Elicitation
arxiv_id: '2405.00981'
source_url: https://arxiv.org/abs/2405.00981
tags:
- item
- user
- bayesian
- preference
- pebol
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PEBOL, a novel approach for natural language
  preference elicitation (NL-PE) that combines Bayesian optimization with large language
  models (LLMs). Traditional NL-PE methods struggle to balance exploration and exploitation
  during multi-turn dialogues, while conventional Bayesian PE methods cannot generate
  arbitrary natural language queries.
---

# Bayesian Optimization with LLM-Based Acquisition Functions for Natural Language Preference Elicitation

## Quick Facts
- arXiv ID: 2405.00981
- Source URL: https://arxiv.org/abs/2405.00981
- Authors: David Eric Austin; Anton Korikov; Armin Toroghi; Scott Sanner
- Reference count: 40
- Primary result: PEBOL achieves up to 131% improvement in MAP@10 after 10 turns compared to monolithic GPT-3.5 baseline

## Executive Summary
This paper introduces PEBOL, a novel approach for natural language preference elicitation that combines Bayesian optimization with large language models. Traditional NL-PE methods struggle to balance exploration and exploitation during multi-turn dialogues, while conventional Bayesian PE methods cannot generate arbitrary natural language queries. PEBOL addresses these limitations by using Natural Language Inference between user utterances and item descriptions to maintain Bayesian preference beliefs, and guiding LLM query generation with decision-theoretic strategies like Thompson Sampling and Upper Confidence Bound.

## Method Summary
PEBOL maintains Bayesian Beta beliefs over item utilities using NLI scores between user preferences and item descriptions. It employs decision-theoretic policies (TS/UCB) to select strategically important items, then uses an LLM to generate natural language yes/no queries about aspects of the selected item. The system supports both binary and probabilistic NLI updates, with the latter (PEBOL-P) showing better performance by preserving information from entailment probabilities. Experiments demonstrate effective preference learning while reducing computational burden by limiting LLM context to a single item description.

## Key Results
- PEBOL achieves up to 131% improvement in MAP@10 after 10 turns compared to monolithic GPT-3.5 baseline
- PEBOL-P (probabilistic NLI updates) outperforms PEBOL-B (binary updates), likely due to preserving information from entailment probabilities
- The method demonstrates effective preference learning while reducing over-exploration and over-exploitation risks
- Uses a much smaller 400M parameter NLI model for preference inference compared to the larger LLMs used for query generation

## Why This Works (Mechanism)

### Mechanism 1
Bayesian optimization with LLMs achieves better exploration-exploitation balance than monolithic LLMs in NL-PE. PEBOL uses Bayesian belief updates via NLI scores to maintain utility beliefs over items, then applies decision-theoretic policies (TS/UCB) to guide LLM query generation toward strategic items. Core assumption: NLI scores between user preferences and item descriptions can be treated as probabilistic observations for Bayesian updating. Evidence: PEBOL achieves up to 131% improvement in MAP@10 after 10 turns compared to a monolithic GPT-3.5 baseline. Break condition: If NLI scores are unreliable or biased, the Bayesian belief updates become inaccurate.

### Mechanism 2
Reducing LLM context from all items to a single strategically selected item description improves computational efficiency and query quality. Context acquisition function selects one item based on utility beliefs, then LLM generates a yes/no query about an aspect of that item, reducing context window burden. Core assumption: A single well-chosen item description provides sufficient information for the LLM to generate a meaningful, targeted query. Evidence: PEBOL reduces the context needed to prompt the LLM from all item descriptions to a single strategically selected item description. Break condition: If the single item context is too narrow, queries may become repetitive or miss important preference dimensions.

### Mechanism 3
Probabilistic NLI scores provide richer information than binary responses for Bayesian updating. PEBOL-P uses continuous entailment probabilities as soft observations in Beta posterior updates, rather than thresholding to binary ratings. Core assumption: Continuous entailment probabilities from NLI models capture nuanced user preferences better than binary like/dislike signals. Evidence: PEBOL-P performed better, likely due to PEBOL-B discarding valuable information from entailment probabilities. Break condition: If NLI model calibration is poor, probabilistic scores may mislead the Bayesian updates.

## Foundational Learning

- **Bayesian Optimization and Posterior Updating**
  - Why needed here: Core to maintaining and updating utility beliefs from NL feedback
  - Quick check question: How does the Beta-Bernoulli conjugate pair enable efficient posterior updates in PEBOL?

- **Natural Language Inference (NLI) and Entailment**
  - Why needed here: Provides the mechanism to convert NL preferences into numerical observations for Bayesian updating
  - Quick check question: Why must the hypothesis in NLI be more general than the premise for effective preference inference?

- **Exploration-Exploitation Tradeoff in Decision Theory**
  - Why needed here: Guides the selection of which item descriptions to use for query generation at each turn
  - Quick check question: How do TS and UCB differ in their approach to balancing exploration and exploitation?

## Architecture Onboarding

- **Component map**: User utterances → NLI model → Beta posterior updates → Context acquisition (TS/UCB) → LLM query generation → New user utterance (loop). Item descriptions are the static context.

- **Critical path**: The flow from user response → NLI score → Beta posterior update → context selection → LLM query generation is the performance-critical sequence that must be low-latency.

- **Design tradeoffs**: Larger NLI models might improve entailment accuracy but increase inference cost; simpler context selection policies (e.g., random) reduce complexity but hurt performance; binary vs. probabilistic observations trade information richness for implementation simplicity.

- **Failure signatures**: MAP@10 plateaus early (over-exploitation); MAP@10 stays near random (poor exploration or bad NLI calibration); sudden MAP drops (catastrophic forgetting or poor query history handling).

- **First 3 experiments**:
  1. Run PEBOL-P with TS and UCB on Yelp dataset, measure MAP@10 vs. turns, compare to random baseline.
  2. Compare PEBOL-B vs. PEBOL-P (binary vs. probabilistic NLI scores) on MovieLens, test different MNLI temperatures.
  3. Test effect of including aspect history in query generation prompts, measure impact on MAP@10 and query diversity.

## Open Questions the Paper Calls Out

### Open Question 1
How would PEBOL perform when extended to handle pairwise or setwise item comparisons rather than single-item context selection? The paper mentions this as a future research direction, noting that multi-item context selection could enable contrastive query generation. Unresolved because the current PEBOL implementation only selects one item at a time for context, limiting its ability to generate contrastive queries. Evidence needed: Experimental results comparing PEBOL's performance with single-item vs. pairwise/setwise context selection on the same datasets, measuring metrics like MAP@10 and user satisfaction.

### Open Question 2
How does PEBOL's performance scale with dataset size and item description length beyond the tested 100-item limit? The paper explicitly limits experiments to 100 items due to context window constraints with the MonoLLM baseline. Unresolved because the paper only tested on relatively small datasets (100 items) due to computational constraints. Evidence needed: Performance evaluation of PEBOL on larger datasets (1000+ items) with varying item description lengths, comparing MAP@10 and computational efficiency.

## Limitations

- **NLI Model Reliability**: The 400M parameter NLI model's calibration and generalization across diverse item descriptions remains uncertain, particularly for noisy or ambiguous user preferences.

- **Generalization to Non-YesNo Queries**: PEBOL's architecture assumes binary responses, limiting its applicability to more nuanced preference elicitation scenarios.

- **Computational Tradeoffs**: While PEBOL reduces LLM context requirements, the cumulative cost of NLI inference, Bayesian updating, and multiple query generations per turn may offset efficiency gains in real-time applications.

## Confidence

- **High Confidence**: PEBOL achieves superior MAP@10 compared to monolithic LLM baselines when using probabilistic NLI updates (PEBOL-P). The Bayesian optimization framework demonstrably improves exploration-exploitation balance.

- **Medium Confidence**: The 131% MAP@10 improvement claim is based on simulated users and controlled noise conditions. Real-world user variability and off-policy evaluation challenges may reduce this advantage.

- **Low Confidence**: Claims about computational efficiency gains relative to context reduction are not empirically validated against actual runtime measurements across different hardware configurations.

## Next Checks

1. **NLI Model Ablation Study**: Test PEBOL with increasingly larger NLI models (1B, 3B parameters) to quantify the tradeoff between entailment accuracy and computational overhead, measuring impact on MAP@10 and inference latency.

2. **Cross-Dataset Generalization**: Evaluate PEBOL on out-of-distribution item descriptions from domains not represented in training (e.g., technical products, academic papers) to assess robustness to domain shift.

3. **Human Evaluation**: Conduct user studies comparing PEBOL to traditional preference elicitation interfaces, measuring not just MAP@10 but user satisfaction, cognitive load, and natural language fluency of the interaction.