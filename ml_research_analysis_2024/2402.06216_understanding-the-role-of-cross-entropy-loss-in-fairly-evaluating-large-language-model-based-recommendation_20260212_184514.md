---
ver: rpa2
title: Understanding the Role of Cross-Entropy Loss in Fairly Evaluating Large Language
  Model-based Recommendation
arxiv_id: '2402.06216'
source_url: https://arxiv.org/abs/2402.06216
tags:
- recommendation
- cross-entropy
- loss
- ndcg
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the role of cross-entropy loss in fairly evaluating
  large language model (LLM)-based recommendation systems. It highlights that previous
  claims about LLM superiority are based on unfair comparisons, as conventional methods
  typically use pointwise/pairwise loss functions rather than cross-entropy.
---

# Understanding the Role of Cross-Entropy Loss in Fairly Evaluating Large Language Model-based Recommendation

## Quick Facts
- arXiv ID: 2402.06216
- Source URL: https://arxiv.org/abs/2402.06216
- Reference count: 40
- Key outcome: Previous claims about LLM superiority in recommendation are based on unfair comparisons using pointwise/pairwise losses; cross-entropy loss provides a more fair evaluation framework

## Executive Summary
This paper examines the role of cross-entropy loss in fairly evaluating large language model (LLM)-based recommendation systems. It reveals that previous claims about LLM superiority are based on unfair comparisons, as conventional methods typically use pointwise/pairwise loss functions rather than cross-entropy. The authors theoretically justify cross-entropy's superiority and propose practical approximations like noise contrastive estimation and scaled cross-entropy to bridge the computational gap. Experimental results across three public datasets demonstrate that existing LLM-based methods are not as effective as claimed for next-item recommendation when evaluated fairly using cross-entropy loss.

## Method Summary
The study evaluates SASRec with cross-entropy loss (CE), noise contrastive estimation (NCE), and scaled cross-entropy (SCE) on three public datasets (Beauty, MovieLens-1M, Yelp). The datasets are filtered to users/items with ≥5 interactions and split using leave-one-out. Training runs for 200-300 epochs depending on method. The evaluation compares these methods against baseline pointwise (BCE) and pairwise (BPR) losses, as well as existing LLM-based recommenders (P5, LlamaRec, E4S). Performance metrics include NDCG@5/10, HR@5/10, and MRR, focusing on next-item recommendation accuracy.

## Key Results
- Cross-entropy loss outperforms pointwise (BCE) and pairwise (BPR) losses on all three datasets for next-item recommendation
- Existing LLM-based methods (P5, LlamaRec, E4S) do not outperform conventional methods when all use cross-entropy loss
- SCE provides an effective approximation to full cross-entropy with only 100-500 sampled items vs. full catalog
- NCE shows training instability with plateaus in early epochs, while SCE maintains stable performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing cross-entropy loss implicitly optimizes ranking metrics like NDCG and RR
- Mechanism: Cross-entropy loss creates a lower bound relationship with ranking metrics - when cross-entropy is minimized, the lower bound of NDCG/RR is maximized
- Core assumption: The relationship between cross-entropy and ranking metrics holds under uniform sampling assumptions
- Evidence anchors:
  - [abstract] "Theoretically justify the superiority of cross-entropy" and "minimizing cross-entropy is equivalent to maximizing a lower bound of Normalized Discounted Cumulative Gain (NDCG) and Reciprocal Rank (RR)"
  - [section 4.1] Proposition 4.1 shows the inequality relationship between -log NDCG and cross-entropy loss

### Mechanism 2
- Claim: SCE (Scaled Cross-Entropy) provides a practical approximation to cross-entropy that maintains ranking capability
- Mechanism: SCE scales the sampled normalizing term by a weight α to compensate for sampling loss, maintaining a meaningful bound on NDCG even in early training stages
- Core assumption: The scaling factor α can effectively compensate for the magnitude loss caused by sampling
- Evidence anchors:
  - [section 4.3] "Scaling up the sampled normalizing term provides an effective and practical approximation to cross-entropy"
  - [section 5] Empirical results show SCE performs comparably to full cross-entropy with only 100-500 sampled items vs. full catalog

### Mechanism 3
- Claim: NCE with default settings fails to optimize meaningful bounds in early training stages due to exponential growth of normalizing term
- Mechanism: NCE's normalizing term grows exponentially with the number of positive samples, creating weak bounds when item representations are poorly distributed
- Core assumption: Early training stages have poorly distributed item representations leading to many items with non-negative scores
- Evidence anchors:
  - [section 4.2] "NCE with the default setting fails to optimize a meaningful bound in the early stages of training"
  - [section 4.2] Figure 3 shows NDCG remains almost constant at beginning of training with NCE

## Foundational Learning

- Concept: Cross-entropy loss and its relationship to ranking metrics
  - Why needed here: Understanding why cross-entropy performs better than pointwise/pairwise losses for ranking tasks
  - Quick check question: How does minimizing cross-entropy loss relate to maximizing NDCG according to the paper?

- Concept: Noise Contrastive Estimation (NCE) and its limitations
  - Why needed here: NCE is a common approximation technique but has specific failure modes in recommendation settings
  - Quick check question: Why does NCE with default settings struggle in early training stages according to the paper?

- Concept: Importance sampling and its relationship to SCE
  - Why needed here: SCE is morphologically equivalent to a specific form of importance sampling, understanding this helps explain its effectiveness
  - Quick check question: How does the proposal distribution in SCE differ from standard importance sampling approaches?

## Architecture Onboarding

- Component map: scoring function s_θ(q,v) -> loss function (CE, NCE, SCE, or BCE/BPR) -> sampling mechanism for approximating normalizing term
- Critical path: For SCE - compute scores → sample negative items → scale sampled normalizing term → compute loss → backpropagate
- Design tradeoffs: Full cross-entropy (O(|I|d)) vs approximations (O(Kd)) - trade computation for approximation quality
- Failure signatures: NCE shows plateaus in early training (Figure 3), BCE/BPR show significant performance degradation vs CE, SCE shows high variance when α is too large or K is too small
- First 3 experiments:
  1. Compare CE vs BCE/BPR on a small dataset to observe the performance gap
  2. Implement SCE with varying α values (1, 10, 100) to find optimal scaling
  3. Compare NCE with different c values (1, 5, 10, 50) to observe training stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact theoretical relationship between the cross-entropy loss and ranking metrics like NDCG and MRR in recommender systems?
- Basis in paper: [explicit] The paper provides theoretical justifications showing that minimizing cross-entropy is equivalent to maximizing a lower bound of NDCG and RR.
- Why unresolved: While the paper establishes a theoretical connection, the exact nature of this relationship and its implications for practical recommender system design are not fully explored.
- What evidence would resolve it: Further theoretical analysis or empirical studies that precisely quantify the impact of cross-entropy optimization on NDCG and MRR performance.

### Open Question 2
- Question: How does the choice of approximation method for cross-entropy (e.g., NCE vs. SCE) impact the ranking performance of LLM-based recommenders?
- Basis in paper: [explicit] The paper introduces two practical approximations (NCE and SCE) and discusses their effectiveness in bridging the gap between cross-entropy and ranking metrics.
- Why unresolved: The paper demonstrates that SCE is more consistent and reliable than NCE, but a comprehensive comparison of their impact on LLM-based recommender performance is lacking.
- What evidence would resolve it: Experimental results comparing the ranking performance of LLM-based recommenders trained with different cross-entropy approximations (e.g., NCE vs. SCE) on diverse datasets.

### Open Question 3
- Question: What are the optimal values for hyperparameters like the scaling factor (alpha) in SCE and the number of negative samples (K) for different datasets and recommendation tasks?
- Basis in paper: [inferred] The paper mentions that the choice of hyperparameters like alpha and K can impact the performance of SCE, but does not provide specific recommendations for optimal values.
- Why unresolved: The optimal values for these hyperparameters likely depend on factors like dataset characteristics and the specific recommendation task, making it difficult to determine a one-size-fits-all approach.
- What evidence would resolve it: Systematic experiments exploring the impact of different hyperparameter values on recommendation performance across various datasets and tasks.

### Open Question 4
- Question: How do LLM-based recommenders compare to conventional methods in terms of ranking performance when trained with the same loss function (e.g., cross-entropy)?
- Basis in paper: [explicit] The paper highlights that previous claims about LLM superiority are based on unfair comparisons, as conventional methods typically use pointwise/pairwise loss functions rather than cross-entropy.
- Why unresolved: While the paper demonstrates that conventional methods can outperform LLMs when trained with cross-entropy, a direct comparison of their ranking performance under the same loss function is not provided.
- What evidence would resolve it: Experimental results comparing the ranking performance of LLM-based and conventional recommenders trained with the same loss function (e.g., cross-entropy) on diverse datasets.

## Limitations

- Theoretical justification for cross-entropy superiority relies on sampling assumptions that may not hold in real-world recommendation scenarios with skewed item popularity distributions
- Claim that LLM-based methods are "not as effective as claimed" is based on comparing SASRec with cross-entropy against specific LLM baselines without full implementation details
- The scaling factor α=100 for SCE is determined through limited ablation studies without exploration of extreme scenarios

## Confidence

- High Confidence: The empirical demonstration that cross-entropy loss outperforms pointwise/pairwise losses (BCE/BPR) on standard sequential recommendation datasets
- Medium Confidence: The theoretical analysis connecting cross-entropy minimization to ranking metric optimization
- Low Confidence: The broader claim about LLM-based recommendation systems being "not as effective as claimed" based on this specific comparison

## Next Checks

1. **Sampling Strategy Robustness Test**: Conduct experiments varying the negative sampling distribution (uniform, popularity-based, random walk) to assess whether cross-entropy advantages persist under different sampling assumptions.

2. **Long Sequence Evaluation**: Test the proposed methods on datasets with longer user sequences (100+ items) to evaluate how well SCE and NCE scale compared to full cross-entropy as sequence length increases.

3. **Cross-Domain Transferability**: Apply the cross-entropy vs BCE/BPR comparison framework to non-sequential recommendation tasks (matrix factorization, two-tower models) to determine if the findings generalize beyond sequential recommendation.