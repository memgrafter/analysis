---
ver: rpa2
title: Enhancing Retrieval in QA Systems with Derived Feature Association
arxiv_id: '2410.03754'
source_url: https://arxiv.org/abs/2410.03754
tags:
- retrieval
- text
- context
- documents
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving retrieval in long-context
  question answering (QA) systems, particularly for subjective queries where answers
  are not explicitly stated. The proposed method, RAIDD (Retrieval from AI Derived
  Documents), enhances RAG systems by generating derived documents (summaries and
  questions) from the original text during ingestion, then using these to guide retrieval.
---

# Enhancing Retrieval in QA Systems with Derived Feature Association

## Quick Facts
- **arXiv ID:** 2410.03754
- **Source URL:** https://arxiv.org/abs/2410.03754
- **Reference count:** 19
- **Primary result:** RAIDD improves QA accuracy by up to 15% and retrieval of relevant context by up to 15% compared to vanilla RAG

## Executive Summary
This paper addresses the challenge of improving retrieval in long-context question answering (QA) systems, particularly for subjective queries where answers are not explicitly stated. The proposed method, RAIDD (Retrieval from AI Derived Documents), enhances RAG systems by generating derived documents (summaries and questions) from the original text during ingestion, then using these to guide retrieval. Experiments show RAIDD improves QA accuracy by up to 15% and retrieval of relevant context by up to 15% compared to vanilla RAG across multiple QA tasks. The method retains portability while better handling implicit information in text.

## Method Summary
RAIDD enhances traditional RAG systems by incorporating derived documents during the ingestion phase. The method generates AI-derived summaries and questions from source documents, which are then indexed alongside the original text. During retrieval, both the original query and these derived features are used to guide the search process. This approach allows the system to capture implicit information that might not be directly present in the source text, making it particularly effective for subjective queries where answers are not explicitly stated. The method is designed to be portable across different RAG architectures while improving the system's ability to retrieve relevant context for complex questions.

## Key Results
- RAIDD improves QA accuracy by up to 15% compared to vanilla RAG
- Retrieval of relevant context improves by up to 15% with RAIDD
- The method shows consistent performance improvements across multiple QA tasks

## Why This Works (Mechanism)
RAIDD works by augmenting the retrieval process with AI-generated derived features (summaries and questions) that capture implicit information from source documents. By indexing these derived features alongside the original text, the system can better match queries that require understanding of unstated context or subjective interpretation. The derived documents act as bridges between the explicit content and the implicit knowledge needed to answer complex questions, particularly those that require synthesis or inference rather than direct extraction.

## Foundational Learning
- **RAG (Retrieval-Augmented Generation):** Why needed: Core architecture being enhanced; Quick check: Can retrieve relevant documents given a query
- **Derived document generation:** Why needed: Creates additional retrieval signals; Quick check: Can LLM generate coherent summaries/questions from text
- **Vector embeddings for retrieval:** Why needed: Enables semantic search over derived features; Quick check: Can measure similarity between queries and derived content
- **Long-context handling:** Why needed: RAIDD targets long-document QA; Quick check: Can system process documents beyond typical context limits
- **Question-answering evaluation:** Why needed: Quantifies performance improvements; Quick check: Can compare exact match/F1 scores against baselines

## Architecture Onboarding
**Component map:** Original documents -> LLM summary/question generator -> Derived documents -> Indexing pipeline -> Vector database -> Retrieval engine -> LLM generator

**Critical path:** Query -> Retrieve derived features and original documents -> Re-rank based on combined signals -> Generate answer

**Design tradeoffs:** Storage vs performance (doubling storage for derived content), LLM dependency for generation quality, portability vs specialization

**Failure signatures:** Poor derived document quality leading to irrelevant retrievals, increased storage costs without proportional accuracy gains, performance degradation with specialized domain content

**Three first experiments:**
1. Test RAIDD retrieval performance on a small Wikipedia subset with simple factual queries
2. Measure storage overhead by comparing index sizes with and without derived documents
3. Evaluate RAIDD's performance on subjective questions requiring inference from SQuAD dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Storage requirements double due to derived document generation
- Performance heavily dependent on LLM quality for summary and question generation
- Limited testing on specialized domains like biomedical or legal text

## Confidence
- **High confidence:** Core retrieval improvements (up to 15% accuracy gains) across multiple benchmark datasets
- **Medium confidence:** Generalizability to specialized domains not tested in the paper
- **Medium confidence:** Cost-benefit tradeoffs in production systems due to doubled storage requirements

## Next Checks
1. **Ablation study on derived document quality:** Systematically vary the quality/quantity of generated summaries and questions to quantify their individual contributions to retrieval performance, and test performance degradation when derived content is noisy or incomplete.

2. **Scalability and cost analysis:** Measure end-to-end system performance including indexing time, storage costs, and query latency across document collections of 10x-100x the size tested, comparing against alternative long-context approaches like direct passage retrieval.

3. **Cross-domain generalization test:** Evaluate RAIDD on specialized corpora (medical literature, legal documents, or technical manuals) where implicit information patterns differ from general knowledge, measuring both retrieval accuracy and hallucination rates for subjective queries.