---
ver: rpa2
title: 'MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning'
arxiv_id: '2409.12147'
source_url: https://arxiv.org/abs/2409.12147
tags: []
core_contribution: The paper addresses the challenge of improving reasoning in large
  language models (LLMs) by overcoming limitations in current test-time refinement
  methods, which struggle with excessive refinement, error localization, and insufficient
  refinement. The proposed method, MAgICoRe, introduces a multi-agent, iterative,
  coarse-to-fine refinement framework that adaptively allocates resources based on
  problem difficulty, using external reward models for targeted feedback and iterative
  refinement.
---

# MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning

## Quick Facts
- arXiv ID: 2409.12147
- Source URL: https://arxiv.org/abs/2409.12147
- Authors: Justin Chih-Yao Chen; Archiki Prasad; Swarnadeep Saha; Elias Stengel-Eskin; Mohit Bansal
- Reference count: 35
- Primary result: Outperforms existing methods like Self-Consistency, Best-of-k, and Self-Refine across five math datasets and two models, achieving up to 6.8% improvement with fewer samples

## Executive Summary
MAgICoRe addresses the challenge of improving reasoning in large language models by introducing a multi-agent, iterative, coarse-to-fine refinement framework. The method overcomes limitations in current test-time refinement approaches by adaptively allocating resources based on problem difficulty, using external reward models for targeted feedback, and employing iterative refinement. MAGICORE demonstrates consistent performance gains across five math datasets and two models, with the ability to scale improvements over iterations while using fewer samples than competing methods.

## Method Summary
MAGICORE implements a multi-agent system with three specialized agents: Solver (generates initial reasoning chains), Reviewer (analyzes step-wise PRM scores to generate targeted feedback), and Refiner (incorporates feedback to improve solutions). The framework uses external Outcome Reward Models (ORM) and Process Reward Models (PRM) to evaluate solution-level and step-level correctness respectively. Problems are classified as easy or hard based on majority answer quality and answer distribution entropy, with easy problems solved via coarse aggregation and hard problems undergoing iterative refinement through bidirectional communication between Reviewer and Refiner agents.

## Key Results
- Achieves up to 6.8% improvement over baseline methods across five math datasets
- Demonstrates consistent gains with fewer samples than competing approaches
- Shows scalability with further improvements over multiple refinement iterations
- Ablations confirm importance of external reward models and multi-agent communication

## Why This Works (Mechanism)

### Mechanism 1
Selective refinement based on problem difficulty prevents over-correction. The framework classifies problems using reward model scores and answer distribution entropy, applying coarse aggregation to easy problems while reserving iterative refinement for hard ones. This approach assumes external reward models can accurately distinguish between easy and hard problems based on majority answer quality and confidence.

### Mechanism 2
Step-wise reward model scores enable targeted error localization. PRM provides local correctness scores for each reasoning step, allowing the Reviewer agent to generate specific feedback about problematic steps. This mechanism relies on the assumption that PRM scores can accurately identify which steps in a reasoning chain contain errors.

### Mechanism 3
Iterative refinement with bidirectional communication improves solution quality. The Reviewer and Refiner agents communicate iteratively, with the Reviewer generating feedback based on refined solutions and the Refiner incorporating this feedback to generate improved solutions. This approach assumes multiple rounds of feedback and refinement lead to better solutions than a single round.

## Foundational Learning

- **Reward Model (RM)**: A model that evaluates the quality of solutions or reasoning steps. Needed here for feedback mechanism and guiding refinement. Quick check: What is the difference between Outcome Reward Models (ORM) and Process Reward Models (PRM)?

- **Coarse-to-fine refinement**: A strategy applying different refinement levels based on problem difficulty. Needed here to balance computational efficiency with solution quality. Quick check: How does MAGICORE decide when to use coarse aggregation versus fine-grained refinement?

- **Multi-agent communication**: A system where multiple specialized agents interact to solve problems. Needed here because different agents specialize in different tasks, improving overall refinement effectiveness. Quick check: What is the role of each agent in MAGICORE's multi-agent system?

## Architecture Onboarding

- **Component map**: Solver -> ORM/PRM scoring -> Difficulty classification -> (Easy: Weighted SC | Hard: Reviewer -> Refiner -> ORM evaluation -> Iterate)
- **Critical path**: Solver generates initial solutions → ORM/PRM scoring → Difficulty classification → Easy problems use weighted Self-Consistency, hard problems undergo iterative Reviewer-Refiner communication until conditions are met
- **Design tradeoffs**: Computational cost vs. solution quality (more iterations improve quality but increase cost); model complexity vs. performance (using both ORM and PRM provides better guidance); agent specialization vs. simplicity (separate Reviewer and Refiner agents outperform combined roles)
- **Failure signatures**: Performance degradation on easy problems (indicates over-refinement or incorrect difficulty classification); lack of improvement across iterations (suggests Reviewer/Refiner communication is ineffective); high computational cost with minimal gains (indicates inefficient resource allocation)
- **First 3 experiments**: 1) Test difficulty classification accuracy on a small dataset to validate Condition 1 and 2; 2) Compare single-round vs. multi-round refinement on a subset of hard problems; 3) Evaluate the impact of using ORM-only vs. PRM-only vs. both for final answer selection

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the provided content. However, several implicit questions emerge from the limitations section and discussion of the approach.

## Limitations

- Reliance on external reward models for problem difficulty classification and error localization introduces potential points of failure
- Computational cost of running multiple agents and iterations may limit practical deployment
- Effectiveness of multi-agent communication loop is uncertain, particularly convergence to better solutions

## Confidence

**High Confidence Claims:**
- MAGICORE outperforms baseline methods on tested datasets
- Multi-agent framework with separate Solver, Reviewer, and Refiner agents is functional
- Iterative refinement generally improves solution quality

**Medium Confidence Claims:**
- Coarse-to-fine approach effectively balances computational cost and solution quality
- External reward models can accurately distinguish easy from hard problems
- Step-wise PRM scores enable meaningful error localization

**Low Confidence Claims:**
- Specific implementation details of agent prompts and reward model configurations
- Generalizability to domains beyond math reasoning
- Scalability to larger models or more complex reasoning tasks

## Next Checks

1. **Difficulty Classification Validation**: Test the accuracy of Condition 1 and Condition 2 (majority answer quality and answer distribution entropy) on a held-out validation set to verify that easy problems are correctly identified and that refinement is appropriately applied only to hard problems.

2. **Iteration Convergence Analysis**: Run controlled experiments comparing single-round vs. multi-round refinement on the same set of hard problems to determine whether additional iterations consistently improve solutions or if there's a point of diminishing returns.

3. **Reward Model Reliability Test**: Conduct ablation studies where the PRM is removed or replaced with a less accurate version to quantify how much the refinement quality depends on the reliability of step-wise scoring for error localization.