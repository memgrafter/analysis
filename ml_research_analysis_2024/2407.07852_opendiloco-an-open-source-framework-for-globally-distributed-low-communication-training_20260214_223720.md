---
ver: rpa2
title: 'OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication
  Training'
arxiv_id: '2407.07852'
source_url: https://arxiv.org/abs/2407.07852
tags:
- diloco
- steps
- training
- workers
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OpenDiLoCo provides an open-source implementation of DiLoCo, a
  method for training large language models with dramatically reduced communication.
  By using two optimizers (AdamW for local updates and SGD with Nesterov momentum
  for synchronization via pseudo-gradients), it enables training across poorly connected
  devices.
---

# OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training

## Quick Facts
- **arXiv ID**: 2407.07852
- **Source URL**: https://arxiv.org/abs/2407.07852
- **Reference count**: 5
- **One-line primary result**: DiLoCo achieves 90-95% compute utilization and 500× less communication while training 150M parameter models across two continents.

## Executive Summary
OpenDiLoCo provides an open-source implementation of DiLoCo, a method for training large language models with dramatically reduced communication. By using two optimizers (AdamW for local updates and SGD with Nesterov momentum for synchronization via pseudo-gradients), it enables training across poorly connected devices. The framework integrates with Hivemind for decentralized training and supports PyTorch FSDP for scaling. Experiments show DiLoCo achieves 90-95% compute utilization when training across two continents and three countries.

## Method Summary
DiLoCo uses a two-optimizer approach where workers perform local AdamW updates for many steps, then synchronize via all-reduce of pseudo-gradients computed as the difference between original and updated weights. An outer SGD with Nesterov momentum aggregates these pseudo-gradients. The framework supports both centralized (torch.distributed) and decentralized (Hivemind DHT) coordination, with FP16 all-reduce optimization to further reduce communication overhead.

## Key Results
- Achieves 90-95% compute utilization across two continents and three countries
- Reduces communication by 500× compared to standard distributed training
- Trains 150M parameter models with perplexity comparable to baseline using 8× larger batch size
- Successfully scales to 1.1B parameter models using the same approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-optimizer setup with local AdamW updates and periodic outer SGD+Nesterov steps enables efficient global training with low communication.
- Mechanism: Local workers independently update model parameters using AdamW for many steps (e.g., 500), then periodically synchronize via all-reduce of pseudo-gradients computed as difference between original and updated weights. Outer SGD with Nesterov momentum aggregates these pseudo-gradients.
- Core assumption: Local updates preserve model quality while drastically reducing communication frequency; pseudo-gradients adequately capture update direction.
- Evidence anchors:
  - [abstract] "By using two optimizers (AdamW for local updates and SGD with Nesterov momentum for synchronization via pseudo-gradients)"
  - [section] "DiLoCo is a local SGD algorithm (Stich, 2019) that leverages two distinct optimization processes: an inner optimizer and an outer optimizer."
- Break condition: If local update horizon is too long, models diverge; if too short, communication savings diminish.

### Mechanism 2
- Claim: FP16 all-reduce of pseudo-gradients maintains model performance while halving communication time.
- Mechanism: Pseudo-gradients computed in FP32 are stored in gradient buffer but can be safely all-reduced using FP16 precision without performance degradation, as verified experimentally.
- Core assumption: Precision loss in FP16 all-reduce does not materially affect convergence for these pseudo-gradient updates.
- Evidence anchors:
  - [section] "We also demonstrate that DiLoCo pseudo gradients can be effectively all-reduced using FP16 without any performance degradation."
  - [section] "We repeated the DiLoCo experiment, this time using FP16 for the pseudo gradient."
- Break condition: If gradient values exceed FP16 range or require high dynamic range, FP16 all-reduce may cause underflow/overflow.

### Mechanism 3
- Claim: Hivemind's decentralized DHT-based coordination enables training across poorly connected global nodes without master bottleneck.
- Mechanism: Workers maintain a distributed hash table (DHT) using libp2p for metadata exchange and synchronization, enabling peer-to-peer communication that works across NATs and variable bandwidth links.
- Core assumption: DHT-based coordination provides sufficient reliability and synchronization for distributed training without central coordination.
- Evidence anchors:
  - [section] "Hivemind utilizes a distributed hash table (DHT) spread across each worker to communicate metadata and synchronize them."
  - [section] "Unlike the torch.distributed implementation, our Hivemind implementation wraps both optimizers into a single optimizer class, making it compatible with popular training codebases."
- Break condition: If network partitions are severe or DHT membership is unstable, training may stall or diverge.

## Foundational Learning

- Concept: Local SGD and pseudo-gradient methods
  - Why needed here: DiLoCo fundamentally relies on local SGD with periodic synchronization via pseudo-gradients rather than full parameter averaging.
  - Quick check question: What is the key difference between local SGD and standard data parallel training in terms of communication frequency?

- Concept: Decentralized coordination and DHTs
  - Why needed here: Hivemind's implementation requires understanding of peer-to-peer systems and distributed hash tables for worker coordination.
  - Quick check question: How does a DHT-based system like Hivemind handle worker discovery and metadata synchronization differently from centralized master-worker architectures?

- Concept: Mixed precision training and gradient scaling
  - Why needed here: The implementation uses FP16 training with gradient scaling, and understanding when to apply the scaler (inner but not outer optimizer) is critical.
  - Quick check question: In mixed precision training, why should gradient scaling be applied during inner optimization steps but not during outer optimizer steps when computing pseudo-gradients?

## Architecture Onboarding

- Component map:
  Model -> Inner optimizer (AdamW) -> Local steps -> Compute pseudo-gradients -> All-reduce -> Outer optimizer (SGD+Nesterov) -> Zero grad

- Critical path: Forward/backward pass → Inner optimizer step → (after local_steps) → Compute pseudo-gradients → All-reduce → Outer optimizer step → Zero grad

- Design tradeoffs:
  - Local step size vs. convergence speed: Larger local steps reduce communication but may slow initial convergence
  - FP32 vs. FP16 all-reduce: FP16 halves communication time but requires gradients to fit within FP16 dynamic range
  - Hivemind vs. torch.distributed: Hivemind enables global decentralized training but may have higher latency than NCCL

- Failure signatures:
  - Poor perplexity convergence: May indicate too few local steps or inadequate outer learning rate
  - Communication bottlenecks: All-reduce time dominating training time suggests network or gradient size issues
  - Worker desynchronization: DHT membership instability or network partitions causing inconsistent states

- First 3 experiments:
  1. Single worker baseline: Run standard AdamW training to establish perplexity baseline
  2. Two-worker DiLoCo: Test with 50 local steps and FP32 all-reduce to verify basic functionality
  3. FP16 ablation: Compare FP16 vs FP32 all-reduce performance with 4 workers and 50 local steps to validate communication optimization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does DiLoCo's effectiveness diminish when scaling to more than 8 workers?
- Basis in paper: [explicit] "our ablation study shows using eight workers does not yet match the computational efficiency of Distributed Data Parallel (DDP) training when running for a shorter amount of steps."
- Why unresolved: The paper only tests up to 8 workers and shows diminishing returns, but doesn't explore the threshold beyond which DiLoCo becomes less effective or the optimal number of workers for different model sizes.
- What evidence would resolve it: Systematic experiments varying the number of workers (e.g., 16, 32, 64) while controlling for total compute and communication constraints to identify the point of diminishing returns.

### Open Question 2
- Question: Can DiLoCo maintain its communication efficiency advantage when training models larger than 1.1 billion parameters?
- Basis in paper: [inferred] "While we demonstrate that DiLoCo works at the billion parameter scale, we believe that further work is needed to make it effective with even larger batch sizes and more local steps."
- Why unresolved: The paper only scales DiLoCo to 1.1B parameters, and the authors explicitly state that more work is needed for larger models.
- What evidence would resolve it: Training experiments with model sizes of 3B, 10B, and 100B parameters while measuring communication reduction and convergence speed relative to DDP baselines.

### Open Question 3
- Question: What is the optimal local step size (H) for different worker counts and model sizes in DiLoCo?
- Basis in paper: [explicit] "In our initial experiment with the 150m model, we run for a total of 88,000 steps. For the scaled-up 1.1B parameter experiment, we limit it to 44,000 steps because of the 4× larger batch size."
- Why unresolved: The paper uses different local step sizes (500 for 150M model, 125 for 1.1B model) but doesn't systematically explore how this parameter should scale with worker count and model size.
- What evidence would resolve it: Grid search experiments varying local step sizes across different worker counts and model sizes to identify optimal configurations that balance convergence speed and communication efficiency.

## Limitations
- Scalability bounds: Method's scaling limits for larger models (e.g., 7B+ parameters) and worker counts remain unclear, with diminishing returns observed beyond 4 workers.
- Network heterogeneity: Experiments conducted across three countries with presumably stable connections; performance under highly unstable networks uncharacterized.
- Hyperparameter sensitivity: Optimal local step count and outer learning rate appear tuned for specific conditions; sensitivity to these parameters across different scenarios unexplored.

## Confidence

- **High confidence**: The core mechanism of DiLoCo (two-optimizer approach with local AdamW and periodic SGD+Nesterov synchronization) is well-established and the implementation details are clearly specified. The 500× communication reduction claim is supported by experimental evidence.
- **Medium confidence**: The FP16 all-reduce optimization maintains performance, though this depends on gradient magnitude staying within FP16 range. The Hivemind decentralized coordination works as described, but real-world deployment across heterogeneous networks may reveal edge cases.
- **Low confidence**: Claims about optimal worker count (4 workers) and the precise scaling relationship between local steps and convergence speed are based on limited experiments and may not generalize.

## Next Checks

1. **Network robustness test**: Deploy DiLoCo across workers with intentionally throttled bandwidth (100-500 Mbps) and high latency (100-500ms) to characterize failure modes and identify network thresholds for stable training.
2. **Larger scale validation**: Test DiLoCo with 16-32 workers and 7B+ parameter models to establish practical scaling limits and identify bottlenecks in outer optimizer effectiveness or communication overhead.
3. **Hyperparameter sensitivity analysis**: Systematically vary local step count (10-1000) and outer learning rate (0.01-0.1) across different model sizes to develop guidelines for hyperparameter selection in diverse training scenarios.