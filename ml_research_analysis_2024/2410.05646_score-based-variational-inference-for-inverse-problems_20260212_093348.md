---
ver: rpa2
title: Score-Based Variational Inference for Inverse Problems
arxiv_id: '2410.05646'
source_url: https://arxiv.org/abs/2410.05646
tags:
- diffusion
- reverse
- mean
- score
- posterior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently obtaining posterior
  mean estimates in inverse problems using diffusion models. While existing diffusion-based
  methods for inverse problems focus on generating samples from the posterior distribution,
  obtaining the posterior mean requires generating multiple samples, which is computationally
  expensive.
---

# Score-Based Variational Inference for Inverse Problems

## Quick Facts
- arXiv ID: 2410.05646
- Source URL: https://arxiv.org/abs/2410.05646
- Authors: Zhipeng Xue; Penghao Cai; Xiaojun Yuan; Xiqi Gao
- Reference count: 40
- Primary result: RMP achieves superior reconstruction performance compared to state-of-the-art algorithms on various image reconstruction tasks with lower computational complexity

## Executive Summary
This paper introduces Reverse Mean Propagation (RMP), a novel framework for efficiently obtaining posterior mean estimates in inverse problems using diffusion models. Unlike existing diffusion-based methods that require generating multiple samples to compute the posterior mean, RMP directly targets the posterior mean by tracking the mean of each reverse diffusion step. The method leverages variational inference to minimize reverse KL divergence at each reverse step, optimizing through natural gradient descent with score functions. Experiments demonstrate that RMP achieves significant improvements in reconstruction metrics (PSNR, SSIM, FID, LPIPS) across super-resolution, inpainting, and deblurring tasks while reducing computational complexity.

## Method Summary
RMP is a diffusion-based framework that directly estimates the posterior mean in inverse problems without requiring multiple posterior samples. The method characterizes the evolution of reverse diffusion conditional probabilities through their mean and covariance, implementing this through a variational inference problem that minimizes reverse KL divergence at each reverse step. The algorithm optimizes these divergences using natural gradient descent with score functions, while simultaneously propagating the mean through the reverse diffusion process. This approach enables deterministic posterior mean estimation with computational efficiency compared to sampling-based methods.

## Key Results
- RMP outperforms state-of-the-art algorithms (DPS, MCG) in PSNR, SSIM, FID, and LPIPS metrics across super-resolution, inpainting, and deblurring tasks
- VP-RMP achieves best FID and LPIPS performance (e.g., 13.90 FID for 4× super-resolution on FFHQ) while VE-RMP sometimes achieves better PSNR
- Computational complexity is reduced as RMP eliminates the need for multiple posterior samples to estimate the mean

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RMP can directly estimate the posterior mean without sampling multiple times
- Mechanism: The evolution of the conditional probability of the reverse diffusion process is characterized by the reverse mean and covariance. By tracking the mean at each reverse step, the posterior mean can be obtained deterministically
- Core assumption: The reverse conditional probability distribution pk(xk|xk+1, y) is Gaussian when the diffusion step size approaches zero
- Evidence anchors: [abstract]: "by analyzing the probability density evolution of the conditional reverse diffusion process, we prove that the posterior mean can be achieved by tracking the mean of each reverse diffusion step"; [section]: "Proposition 1 For diffusion models with forward process (3), the reverse conditional pk(xk|xk+1, y), ∀k = 0 · · · T − 1, is Gaussian when ∆t → 0"; [corpus]: Weak - the corpus contains related papers but none directly confirm the Gaussian property of reverse conditional distributions
- Break condition: If the reverse conditional distribution deviates significantly from Gaussianity (e.g., for highly non-linear measurement operators), the mean tracking approach may fail

### Mechanism 2
- Claim: RMP can be implemented through a variational inference problem that minimizes reverse KL divergence at each reverse step
- Mechanism: Instead of applying variational inference on the conditional posterior of x0, we focus on the joint conditional posterior of all latent variables {xk}T k=0. The KL divergence between variational joint posterior and true joint posterior can be decomposed into sequential KL divergences at each reverse step
- Core assumption: The variational reverse process can be chosen as a Markov chain that matches the reverse of the diffusion process
- Evidence anchors: [abstract]: "We show that RMP can be implemented by solving a variational inference problem, which can be further decomposed as minimizing a reverse KL divergence at each reverse step"; [section]: "Proposition 2 For a diffusion process with forward process (3), the KL divergence between variational q(x0:T |y) and joint posterior p(x0:T |y) defined in (10) equals KL(q||p) = Σ k=T-1 0 Z q(xk+1|y) Z qk(xk|xk+1, y) log qk(xk|xk+1, y) pk(xk|xk+1, y) dxkdxk+1"; [corpus]: Weak - corpus contains papers on score-based variational inference but none specifically address the decomposition of KL divergence in the RMP framework
- Break condition: If the true joint posterior cannot be well-approximated by the chosen variational distribution family, the KL divergence minimization may not converge to a good solution

### Mechanism 3
- Claim: Score-based gradient calculation with approximations can effectively optimize the variational inference problem
- Mechanism: The gradient of log pk(xk|xk+1, y) can be approximated using the prior score function (from pre-trained diffusion models), the likelihood score (approximated using MMSE estimation), and the reverse diffusion score. Natural gradient descent with these approximations can optimize the variational parameters
- Core assumption: The pre-trained score network can accurately approximate the score function of perturbed data distributions, and the likelihood approximation error is acceptable
- Evidence anchors: [abstract]: "We further develop an algorithm that optimizes the reverse KL divergence with natural gradient descent using score functions and propagates the mean at each reverse step"; [section]: "∇xk log pk(xk|xk+1, y) ≈ ∇ xk log p(xk+1|xk) + γk∇xk log p(y|x̂0(xk)) + sθ(xk, σk)"; [corpus]: Weak - corpus contains papers on score-based methods but none specifically address the approximation strategy used in RMP
- Break condition: If the pre-trained score network is not well-matched to the data distribution or the measurement operator, or if the likelihood approximation error is too large, the optimization may fail to converge

## Foundational Learning

- Concept: Gaussian distribution and its properties
  - Why needed here: The core mechanism relies on the Gaussian property of reverse conditional distributions
  - Quick check question: What are the mean and covariance of a Gaussian distribution N(x; μ, Σ)?

- Concept: Variational inference and KL divergence
  - Why needed here: The RMP framework is implemented through a variational inference problem that minimizes KL divergence
  - Quick check question: What is the relationship between maximizing ELBO and minimizing KL divergence in variational inference?

- Concept: Score functions and score matching
  - Why needed here: The algorithm uses score functions to approximate gradients for optimization
  - Quick check question: What is the score function of a probability distribution p(x)?

## Architecture Onboarding

- Component map: y -> forward diffusion -> {xk}T k=0 -> reverse mean propagation -> x̂0
- Critical path: The critical path is the reverse mean propagation, which depends on the quality of the pre-trained score network and the measurement operator
- Design tradeoffs: Tradeoff between accuracy and computational complexity: More reverse steps (larger T) can improve accuracy but increase computational cost; Tradeoff between approximation error and optimization stability: The approximations used in score-based gradient calculation may introduce error but make the optimization more stable
- Failure signatures: If the estimated posterior mean is far from the true posterior mean, it may indicate that the reverse conditional distribution is not well-approximated by Gaussian or the variational distribution is not expressive enough; If the algorithm fails to converge, it may indicate that the pre-trained score network is not well-matched to the data distribution or the measurement operator
- First 3 experiments: 1) Implement RMP for a simple linear inverse problem with known ground truth and compare the estimated posterior mean with the true posterior mean; 2) Test the sensitivity of RMP to the number of reverse steps (T) and the inner loop size (Tin) on a benchmark image reconstruction task; 3) Compare the performance of RMP with different likelihood score approximations (e.g., exact SVD-based vs. approximate) on a linear inverse problem

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, several unresolved issues can be identified:

### Open Question 1
- Question: How does the choice of diffusion model (VE vs VP) impact the accuracy of posterior mean estimation in RMP?
- Basis in paper: [explicit] The paper compares VE-RMP and VP-RMP, showing that VP-RMP often achieves better performance in FID and LPIPS metrics, but VE-RMP sometimes outperforms in PSNR
- Why unresolved: The paper does not provide a theoretical explanation for why one model might be preferred over the other in specific scenarios. It only presents empirical results
- What evidence would resolve it: A detailed theoretical analysis comparing the convergence properties and bias-variance trade-offs of VE and VP models in the context of posterior mean estimation

### Open Question 2
- Question: What is the optimal number of timesteps (T) and inner loop size (Tin) for RMP in different inverse problems?
- Basis in paper: [inferred] The paper uses fixed values of T and Tin for different tasks (e.g., T=400 for VP-RMP, T=30 for VE-RMP) but does not explore the impact of varying these parameters
- Why unresolved: The paper does not investigate how the choice of T and Tin affects the trade-off between computational complexity and reconstruction accuracy
- What evidence would resolve it: An ablation study systematically varying T and Tin across different inverse problems to identify optimal settings

### Open Question 3
- Question: How sensitive is RMP to the choice of hyperparameters (s1, ζ) in different inverse problems?
- Basis in paper: [explicit] The paper provides hyperparameter settings for different tasks but does not analyze their sensitivity or provide a method for automatic tuning
- Why unresolved: The paper does not explore the impact of hyperparameter variations on performance or discuss strategies for hyperparameter selection
- What evidence would resolve it: A sensitivity analysis showing how changes in s1 and ζ affect reconstruction quality across different inverse problems

### Open Question 4
- Question: Can RMP be extended to handle non-Gaussian noise models in inverse problems?
- Basis in paper: [inferred] The paper focuses on Gaussian noise but mentions that other noise models may apply. The current framework relies on Gaussian assumptions for the reverse diffusion process
- Why unresolved: The paper does not explore how RMP would perform with non-Gaussian noise or propose modifications to handle such cases
- What evidence would resolve it: Experiments comparing RMP's performance on inverse problems with non-Gaussian noise and theoretical analysis of how the framework would need to be adapted

## Limitations
- The core mechanism relies on the Gaussian property of reverse conditional distributions, which may not hold for highly non-linear measurement operators
- The variational inference framework assumes the true joint posterior can be well-approximated by the chosen variational distribution family, which may not always be the case
- The score-based gradient calculation uses approximations that introduce error, and the optimization stability depends on the quality of the pre-trained score network and the measurement operator

## Confidence
- Mechanism 1 (Gaussian reverse conditional): Medium - The theoretical proof assumes small diffusion step sizes, but the practical impact of finite step sizes is unclear
- Mechanism 2 (Variational inference decomposition): Medium - The decomposition is mathematically sound, but the effectiveness depends on the expressiveness of the variational distribution
- Mechanism 3 (Score-based optimization): Medium - The approximations used in score-based gradient calculation are reasonable, but their impact on optimization stability and convergence is not fully characterized

## Next Checks
1. Test RMP on inverse problems with non-linear measurement operators to assess the robustness of the Gaussian reverse conditional assumption
2. Compare the performance of RMP with different variational distribution families to evaluate the impact of the variational approximation
3. Analyze the sensitivity of RMP to the number of reverse steps (T) and the inner loop size (Tin) on a range of inverse problems to understand the tradeoff between accuracy and computational complexity