---
ver: rpa2
title: Model-based Preference Optimization in Abstractive Summarization without Human
  Feedback
arxiv_id: '2409.18618'
source_url: https://arxiv.org/abs/2409.18618
tags:
- decoding
- human
- summaries
- preference
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of hallucination in abstractive
  summarization by Large Language Models, where models often generate fluent but inaccurate
  summaries not supported by the source document. To tackle this, the authors propose
  Model-based Preference Optimization (MPO), a method that fine-tunes LLMs without
  human feedback by leveraging the inherent differences between deterministic (e.g.,
  beam search) and stochastic (e.g., temperature sampling) decoding strategies.
---

# Model-based Preference Optimization in Abstractive Summarization without Human Feedback

## Quick Facts
- **arXiv ID:** 2409.18618
- **Source URL:** https://arxiv.org/abs/2409.18618
- **Reference count:** 26
- **Primary result:** MPO significantly improves faithfulness and relevance in abstractive summarization without human feedback, outperforming SFT and human-preference methods like DPO and PPO on TL;DR and XSUM datasets.

## Executive Summary
This paper addresses the critical problem of hallucination in abstractive summarization by Large Language Models, where generated summaries often contain factually incorrect information not supported by the source document. The authors propose Model-based Preference Optimization (MPO), a novel fine-tuning approach that operates without human feedback by leveraging the inherent differences between deterministic (beam search) and stochastic (temperature sampling) decoding strategies. By treating beam search outputs as "chosen" samples and temperature-sampled outputs as "rejected" samples, MPO creates a preference dataset entirely generated by the model itself, enabling effective fine-tuning for faithfulness without expensive human annotations.

Experiments demonstrate that MPO significantly outperforms strong baselines including supervised fine-tuning and human-preference optimization methods on standard faithfulness metrics. The method achieves substantial improvements in AlignScore (up to 3.28) and FactCC (up to 7.92) compared to SFT, indicating better alignment with source documents and reduced hallucinations. Importantly, MPO generalizes across different model architectures including BART, showing its potential as a practical solution for improving summarization quality without the need for external preference data or human evaluation.

## Method Summary
MPO addresses hallucination in abstractive summarization by creating a self-supervised preference dataset without human feedback. The method exploits the difference between deterministic and stochastic decoding strategies: beam search generates conservative, high-probability summaries treated as "chosen" samples, while temperature sampling produces more diverse, lower-probability summaries treated as "rejected" samples. This preference signal is used to fine-tune the model via preference optimization objectives similar to DPO but without requiring external preference data. The approach effectively teaches the model to favor summaries that are both faithful to the source document and informative, while avoiding hallucinated content.

## Key Results
- MPO achieves up to 3.28 improvement in AlignScore over SFT, demonstrating significantly better faithfulness to source documents
- FactCC scores improve by up to 7.92 points with MPO compared to SFT, indicating reduced factual errors in generated summaries
- MPO outperforms human-preference methods like DPO and PPO on both TL;DR and XSUM datasets while requiring no human feedback or external metrics

## Why This Works (Mechanism)
MPO works by creating an implicit preference signal through contrasting outputs from different decoding strategies. Beam search tends to produce conservative, high-probability summaries that are more likely to be faithful to the source, while temperature sampling generates more diverse but potentially less reliable outputs. By treating these as preference pairs (beam search as preferred, temperature sampling as non-preferred), MPO effectively teaches the model to generate summaries that balance informativeness with faithfulness. This self-supervised approach captures the distinction between fluent but potentially hallucinated content and accurate, source-aligned summaries without requiring human annotations.

## Foundational Learning
- **Abstractive Summarization:** The task of generating concise summaries that capture the meaning of source documents using novel phrases rather than copying verbatim text. Needed to understand the core problem of hallucination in summarization systems. Quick check: Can the model generate coherent summaries that capture document meaning?
- **Hallucination in LLMs:** The generation of factually incorrect information not supported by the source document, a critical failure mode in summarization. Needed to motivate the development of faithfulness-focused fine-tuning methods. Quick check: Do generated summaries contain claims unsupported by the source?
- **Preference Optimization:** Fine-tuning methods that use pairwise comparisons (preferred vs non-preferred samples) rather than pointwise loss functions. Needed to understand how MPO differs from standard supervised fine-tuning. Quick check: Can the model learn from relative preferences rather than absolute labels?
- **Decoding Strategies:** Different methods for generating text from LLMs, including deterministic approaches like beam search and stochastic approaches like temperature sampling. Needed to understand how MPO creates preference pairs. Quick check: Do different decoding strategies produce measurably different output qualities?
- **Faithfulness Metrics:** Evaluation measures like AlignScore and FactCC that assess whether generated summaries accurately reflect source document content. Needed to quantify hallucination reduction. Quick check: Do metric scores correlate with actual factual accuracy?

## Architecture Onboarding

**Component Map:**
Encoder -> BART-based Summarization Model -> Decoder (with two decoding strategies: beam search and temperature sampling) -> Preference Pairs -> MPO Fine-tuning

**Critical Path:**
Source Document -> Encoder Processing -> Summarization Model -> Dual Decoding (beam search + temperature sampling) -> Preference Pair Formation -> MPO Objective Optimization -> Fine-tuned Model

**Design Tradeoffs:**
MPO trades the potential precision of human-annotated preferences for the scalability and cost-effectiveness of self-generated preferences. While human feedback might capture more nuanced quality distinctions, MPO's approach enables fine-tuning at scale without annotation costs. The method assumes that beam search outputs are reliably better than temperature-sampled outputs, which may not always hold but provides a practical signal for preference learning.

**Failure Signatures:**
- If beam search and temperature sampling produce similar quality outputs, the preference signal becomes weak and MPO may not improve faithfulness
- Over-reliance on preference optimization might lead to overly conservative summaries that lack informativeness
- The method may not generalize well to domains where deterministic decoding doesn't reliably produce faithful outputs
- Performance could degrade on tasks where the assumption about decoding strategy quality differences doesn't hold

**First Experiments:**
1. Compare MPO against SFT on a held-out test set using AlignScore and FactCC to verify faithfulness improvements
2. Evaluate MPO's performance on a human-annotated preference dataset to check alignment with human judgments
3. Test MPO on a different summarization domain (e.g., legal or medical) to assess generalizability beyond news summarization

## Open Questions the Paper Calls Out
None

## Limitations
- MPO's effectiveness relies on the assumption that beam search outputs are consistently more faithful than temperature-sampled outputs, which may not hold for all domains or tasks
- The method lacks direct human evaluation to confirm that metric improvements correspond to actual reductions in factual hallucinations as judged by annotators
- Performance improvements are validated only on specific datasets (TL;DR, XSUM) and model architectures (BART), limiting confidence in generalizability to other domains and LLM types

## Confidence

**High Confidence:** MPO's ability to improve faithfulness metrics (AlignScore, FactCC) on tested datasets
**Medium Confidence:** MPO's effectiveness without human feedback or external metrics
**Low Confidence:** Generalization of MPO's hallucination reduction to unseen domains and model architectures

## Next Checks
1. Conduct human evaluation studies to verify that MPO improvements in faithfulness metrics correspond to actual reductions in factual hallucinations as judged by annotators
2. Test MPO on diverse summarization domains (legal, medical, technical) and model architectures (decoder-only LLMs like GPT, encoder-decoder hybrids) to assess generalizability
3. Compare MPO's performance against human preference datasets (e.g., TL;DR preference annotations) to validate whether self-generated preferences align with human judgments