---
ver: rpa2
title: 'Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token
  Phenomena in LLMs'
arxiv_id: '2410.13835'
source_url: https://arxiv.org/abs/2410.13835
tags:
- attention
- tokens
- token
- arxiv
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates extreme-token phenomena in large language
  models, including attention sinks, value-state drains, and residual-state peaks.
  The authors show that these phenomena arise from an active-dormant mechanism in
  attention heads, where heads become sinks for specific input domains while remaining
  non-sinks for others.
---

# Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs

## Quick Facts
- **arXiv ID**: 2410.13835
- **Source URL**: https://arxiv.org/abs/2410.13835
- **Reference count**: 40
- **Primary result**: Extreme-token phenomena in LLMs arise from active-dormant mechanisms in attention heads driven by mutual reinforcement during pretraining.

## Executive Summary
This work investigates extreme-token phenomena in large language models, including attention sinks, value-state drains, and residual-state peaks. The authors show that these phenomena arise from an active-dormant mechanism in attention heads, where heads become sinks for specific input domains while remaining non-sinks for others. They propose a mutual reinforcement mechanism that dynamically drives these phenomena during pretraining. The authors demonstrate their findings through studies on simplified transformer architectures and tasks, theoretical analysis of training dynamics, and experiments on pretrained LLMs. They also propose strategies to mitigate extreme-token phenomena during pretraining, including replacing softmax with ReLU and Adam with SGD.

## Method Summary
The authors train transformers (1-3 layers) on a synthetic Bigram-Backcopy task using Adam optimizer, tracking attention weights, value states, and residual states across training steps. They analyze pre-trained LLMs to identify active-dormant attention heads and study training dynamics through intermediate checkpoints. The methodology includes ablation studies by zeroing out specific layers/heads, comparing different activation functions (SoftMax vs ReLU) and optimizers (Adam vs SGD), and validating theoretical predictions through empirical measurements of attention logits, value states, and residual state norms.

## Key Results
- Attention heads exhibit active-dormant behavior, being significant contributors for trigger tokens but minimal contributors for non-trigger tokens
- Mutual reinforcement mechanism drives formation of attention sinks and value-state drains during pretraining
- Residual-state peaks emerge in multi-layer transformers due to interaction between mutual reinforcement and Adam optimizer dynamics
- Mitigation strategies (ReLU attention, SGD optimization) successfully eliminate extreme-token phenomena without sacrificing task performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Attention heads in pre-trained models are governed by an active-dormant mechanism, exhibiting two phases.
- **Mechanism**: For any given token, an attention head is considered active if it makes a significant contribution to the residual state, and dormant if its contribution is minimal. On non-trigger tokens, the attention head assigns dominant weights to the ⟨s⟩ token, adding minimal value to the residual stream and having little impact on the model's output. On trigger tokens, the attention head assigns dominant attention weights to relevant context tokens, adding substantial value to the residual stream and significantly impacting the model's output.
- **Core assumption**: The model can accurately predict the next token without using the attention head for non-trigger tokens.
- **Evidence anchors**:
  - [abstract]: "we identify an active-dormant mechanism, where attention heads become sinks for specific input domains while remaining non-sinks for others."
  - [section]: "when trained on the BB task, the attention head is active for trigger tokens and dormant for non-trigger tokens."
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.5, average citations=0.0." (Weak corpus evidence)
- **Break condition**: If the attention head becomes necessary for accurate prediction on non-trigger tokens, it will no longer be dormant.

### Mechanism 2
- **Claim**: Attention sinks and value-state drains arise through mutual reinforcement during pretraining.
- **Mechanism**: Dynamically, the SoftMax mechanism shifts attention weights towards tokens that exhibit value-state drains, reinforcing these tokens as attention sinks. Attention sinks on these extreme tokens further suppress their value states, reinforcing their role as value-state drains. The mutual reinforcement stabilizes when all non-trigger tokens have large, nearly identical attention logits on the extreme token.
- **Core assumption**: The training dynamics favor the ⟨s⟩ token as the extreme token due to the causal mask.
- **Evidence anchors**:
  - [abstract]: "Our theoretical analysis of the training dynamics reveals that these phenomena are driven by a mutual reinforcement mechanism."
  - [section]: "Both the Bigram and Backcopy excess risks decrease to nearly zero within the first 1000 steps, with the Bigram excess risk approaching zero faster than the Backcopy risk. As the Backcopy risk decreases, the attention weights on the ⟨s⟩ token begin to increase, suggesting a connection between the formation of attention sinks and the backcopy function in the attention heads."
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.5, average citations=0.0." (Weak corpus evidence)
- **Break condition**: If the value states of the extreme token are no longer suppressed, the mutual reinforcement will break.

### Mechanism 3
- **Claim**: Residual-state peaks emerge in multi-layer transformers due to the interaction between mutual reinforcement and the Adam optimization algorithm.
- **Mechanism**: In the training dynamic of a multi-layer transformer, if the mutual reinforcement mechanism occurs in upper layers, the gradients of Res⟨s⟩ have the same direction (aligning with the null space of value matrices in upper layers and the Key⟨s⟩) along the training dynamics. The layer-norm operations cause the fast decay of the magnitude of the gradients. Adam induces diminishing gradients to be constant updates, leading to the linear growth for the norm of the residual state of the extreme token.
- **Core assumption**: The model structure is sufficient (three or more layers) and trained with Adam optimization.
- **Evidence anchors**:
  - [abstract]: "empirical results reveal that residual-state peaks arise from the interaction between this mutual reinforcement mechanism and the Adam optimization algorithm."
  - [section]: "When switching the training algorithm from Adam to SGD, attention sinks remain, but residual-state peaks disappear. Similarly, switching to ReLU attention, which lacks the mutual reinforcement mechanism, also eliminates residual-state peaks."
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.5, average citations=0.0." (Weak corpus evidence)
- **Break condition**: If the optimization algorithm is changed to SGD or ReLU attention is used, residual-state peaks will disappear.

## Foundational Learning

- **Concept**: SoftMax mechanism and its effect on attention weights
  - **Why needed here**: The SoftMax mechanism is crucial for understanding how attention weights are distributed and how they can be shifted towards certain tokens.
  - **Quick check question**: What happens to the attention weights if the SoftMax function is replaced with a ReLU activation?

- **Concept**: Gradient flow and its role in training dynamics
  - **Why needed here**: Understanding gradient flow is essential for analyzing how the model parameters change during training and how the mutual reinforcement mechanism drives the formation of extreme-token phenomena.
  - **Quick check question**: How does the gradient flow change when the value states of the extreme token are small and constant?

- **Concept**: Layer normalization and its impact on residual states
  - **Why needed here**: Layer normalization affects the magnitude of the gradients and plays a role in the formation of residual-state peaks in multi-layer transformers.
  - **Quick check question**: How does layer normalization contribute to the linear growth of the residual state norm in the extreme token?

## Architecture Onboarding

- **Component map**:
  Input sequence with positional embeddings -> Transformer layers with self-attention and MLP -> Attention heads with query, key, and value matrices -> Layer normalization and residual connections

- **Critical path**:
  1. Token embedding and positional encoding
  2. Self-attention computation (query, key, value, SoftMax)
  3. Value state calculation and residual state update
  4. MLP layer processing
  5. Layer normalization and residual connection

- **Design tradeoffs**:
  - SoftMax vs. ReLU activation in attention heads
  - Adam vs. SGD optimization algorithm
  - Number of transformer layers
  - Use of layer normalization

- **Failure signatures**:
  - Attention sinks: Dominant attention weights on specific tokens (e.g., ⟨s⟩)
  - Value-state drains: Significantly smaller value states for extreme tokens
  - Residual-state peaks: Linear growth of residual state norms for extreme tokens

- **First 3 experiments**:
  1. Train a one-layer transformer on the Bigram-Backcopy task and observe attention sinks and value-state drains.
  2. Modify the attention activation function from SoftMax to ReLU and retrain the model to verify the elimination of extreme-token phenomena.
  3. Train a three-layer transformer with Adam optimization and observe the emergence of residual-state peaks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the active-dormant mechanism be generalized to explain attention sinks in models trained on tasks other than language modeling?
- Basis in paper: [explicit] The paper demonstrates the active-dormant mechanism in transformers trained on the Bigram-Backcopy task and in pretrained LLMs, suggesting it may be a general phenomenon.
- Why unresolved: The paper focuses on language models and a simple synthetic task. It does not explore whether this mechanism applies to other domains like vision or audio processing.
- What evidence would resolve it: Training transformers on non-language tasks and analyzing attention head behavior to see if active-dormant patterns emerge.

### Open Question 2
- Question: What is the precise relationship between the Adam optimizer and the formation of residual-state peaks?
- Basis in paper: [explicit] The paper shows that switching from Adam to SGD eliminates residual-state peaks in the BB task, suggesting Adam's role in their formation.
- Why unresolved: The paper proposes a potential mechanism but does not provide a rigorous mathematical explanation for why Adam specifically causes linear growth of residual-state norms.
- What evidence would resolve it: A theoretical analysis of Adam's optimization dynamics in multi-layer transformers, explaining how it interacts with attention mechanisms to produce residual-state peaks.

### Open Question 3
- Question: Do extreme-token phenomena impact model performance on downstream tasks, and if so, how?
- Basis in paper: [inferred] The paper mentions that extreme-token phenomena pose challenges for inference, quantization, and interpretability, but does not directly measure their impact on task performance.
- Why unresolved: The paper proposes architectural modifications to mitigate extreme-token phenomena but does not evaluate whether these modifications improve or degrade performance on standard benchmarks.
- What evidence would resolve it: Systematic experiments comparing models with and without extreme-token phenomena on standard language modeling benchmarks, and analyzing the trade-offs between mitigating these phenomena and maintaining performance.

## Limitations

- The theoretical framework has high confidence for single-layer transformers but medium confidence when extending to multi-layer transformers and pre-trained LLMs due to architectural complexity
- The correlation between BB task findings and pre-trained LLM behavior shows promising alignment but lacks definitive causal verification
- Major uncertainties exist regarding precise conditions under which residual-state peaks emerge in deeper architectures

## Confidence

- **Active-dormant mechanism in single-layer transformers**: High
- **Mutual reinforcement dynamics**: High
- **Residual-state peaks in multi-layer transformers**: Medium
- **Active-dormant mechanism in pre-trained LLMs**: Medium
- **Mitigation strategy effectiveness**: Medium

## Next Checks

1. **Cross-architecture verification**: Train transformers with varying depths (1-6 layers) on BB task while systematically varying optimizer choices (Adam, SGD, AdamW) and attention functions (SoftMax, ReLU, GeLU) to precisely map conditions under which residual-state peaks emerge and disappear.

2. **Causal intervention study**: Design controlled experiments where attention heads are selectively activated/dormant across different input domains in pre-trained models, measuring causal impact on task performance and extreme-token formation rather than relying solely on correlational observations.

3. **Scaling and data diversity analysis**: Test whether active-dormant mechanisms persist across model scales (100M-10B parameters) and diverse training datasets (code, literature, scientific text) to determine if the phenomena are fundamental architectural features or artifacts of specific training conditions.