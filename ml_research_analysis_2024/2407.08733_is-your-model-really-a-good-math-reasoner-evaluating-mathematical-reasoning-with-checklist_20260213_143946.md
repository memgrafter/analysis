---
ver: rpa2
title: Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning
  with Checklist
arxiv_id: '2407.08733'
source_url: https://arxiv.org/abs/2407.08733
tags:
- angle
- step
- answer
- points
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MathCheck, a comprehensive checklist for evaluating
  mathematical reasoning abilities of large language models (LLMs) and multi-modal
  LLMs (MLLMs). The checklist includes four mathematical reasoning tasks (problem
  solving, answerable judging, outcome judging, process judging) and four problem
  variants (original problem, problem understanding, irrelevant disturbance, scenario
  understanding) to assess task generalization and reasoning robustness.
---

# Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist

## Quick Facts
- arXiv ID: 2407.08733
- Source URL: https://arxiv.org/abs/2407.08733
- Authors: Zihao Zhou; Shudong Liu; Maizhen Ning; Wei Liu; Jindong Wang; Derek F. Wong; Xiaowei Huang; Qiufeng Wang; Kaizhu Huang
- Reference count: 40
- Primary result: MathCheck evaluates mathematical reasoning abilities of LLMs/MLLMs using a comprehensive checklist with 4 tasks and 4 problem variants, showing better correlation with intelligence proxies than traditional benchmarks

## Executive Summary
MathCheck introduces a comprehensive checklist for evaluating mathematical reasoning in large language models, moving beyond traditional single-task benchmarks. The framework tests models across four mathematical reasoning tasks (problem solving, answerable judging, outcome judging, process judging) and four problem variants (original, understanding, disturbance, scenario) to assess task generalization and reasoning robustness. The authors demonstrate that frontier models like GPT-4o excel across various abilities, while many other models show significant performance decline, suggesting that traditional benchmarks may overestimate mathematical capabilities.

## Method Summary
MathCheck uses (M)LLMs as rewriters to automatically generate benchmark data from seed datasets (GSM8k for textual reasoning, GeoQA/UniGeo/Geometry3K for multi-modal reasoning), followed by human validation. The framework evaluates 26 LLMs and 17 MLLMs across 16 unit types (4 tasks × 4 variants) using F1 for classification tasks and accuracy for solving tasks. The automatic generation pipeline enables scalable benchmark creation while maintaining evaluation validity through human oversight of generated variants.

## Key Results
- Frontier models like GPT-4o excel across all MathCheck abilities, while many other models exhibit significant performance decline
- MathCheck shows significantly better correlation with compression efficiency (Pearson correlation coefficient of p = -0.915) than traditional benchmarks
- The framework better represents mathematical intelligence linearly and aligns more accurately with true mathematical abilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MathCheck's multi-task evaluation reveals true mathematical reasoning ability by exposing model weaknesses that single-task benchmarks miss
- Mechanism: By requiring models to perform problem solving, answerable judging, outcome judging, and process judging across four problem variants, MathCheck creates a multidimensional evaluation space that captures task generalization and reasoning robustness
- Core assumption: A model that truly understands mathematical concepts should perform consistently well across all task variants and problem types
- Evidence anchors:
  - [abstract]: "if a model really understands a problem, it should be robustly and readily applied across a diverse array of tasks"
  - [section 2.1]: "Testing models across different tasks on the same domain not only offers a comprehensive and profound evaluation of their capabilities"
  - [corpus]: Weak - corpus contains related benchmarks but no direct evidence for multi-task correlation with reasoning ability
- Break condition: If models show high performance on problem solving but fail on other tasks, indicating they may have learned superficial patterns rather than genuine understanding

### Mechanism 2
- Claim: MathCheck's automatic generation pipeline enables scalable, high-quality benchmark creation while maintaining evaluation validity
- Mechanism: Using (M)LLMs as rewriters with human validation creates diverse problem variants efficiently, allowing rapid expansion of benchmark coverage without sacrificing quality
- Core assumption: LLM rewriters with human oversight can generate meaningful problem variants that preserve mathematical logic while testing different aspects of reasoning
- Evidence anchors:
  - [section 2.3]: "we employ (M)LLMs (e.g., GPT-4-Turbo in our experiments) as engines to automatically generate our MATHCHECK data"
  - [section C.2]: "the rewriting pass rate is high, which reflects the effectiveness of our generation method"
  - [corpus]: Weak - corpus shows related automatic generation approaches but no specific validation of MathCheck's pipeline
- Break condition: If generated variants systematically bias evaluation toward certain model types or fail to capture meaningful reasoning challenges

### Mechanism 3
- Claim: MathCheck better represents mathematical intelligence linearly by correlating more strongly with compression efficiency and private data performance
- Mechanism: Traditional benchmarks show weak correlation with intelligence proxies (BPC-loss, private data), while MathCheck's multi-dimensional evaluation aligns more closely with these measures
- Core assumption: Mathematical intelligence should be measurable through compression efficiency and performance on uncontaminated private data
- Evidence anchors:
  - [section 3.4]: "the right sub-figure displays the correlation with our MATHCHECK -GSM, demonstrating that MATHCHECK -GSM exhibits a significantly better correlation with genuine intelligence, with a Pearson correlation coefficient of p = −0.915"
  - [section 3.4]: "Our method shows that many models, such as the Qwen series, have scores on our benchmark that align more accurately with their true mathematical abilities"
  - [corpus]: Weak - corpus contains related compression efficiency studies but no direct comparison with MathCheck
- Break condition: If correlation with intelligence proxies breaks down as model capabilities evolve or if private data performance diverges from MathCheck scores

## Foundational Learning

- Concept: Task generalization in AI evaluation
  - Why needed here: MathCheck's core innovation is testing whether models can handle multiple related tasks about the same problem, which requires understanding that generalization is a key measure of intelligence
  - Quick check question: Why might a model that solves arithmetic problems well still fail at determining if a problem is answerable?

- Concept: Robustness testing through perturbations
  - Why needed here: MathCheck uses problem understanding, irrelevant disturbance, and scenario understanding variants to test whether models have superficial or deep understanding
  - Quick check question: How does adding irrelevant information to a math problem test a model's reasoning robustness?

- Concept: Correlation with intelligence proxies
  - Why needed here: The paper argues MathCheck better represents mathematical intelligence by showing stronger correlation with compression efficiency and private data performance
  - Quick check question: Why would a benchmark that correlates better with compression efficiency be considered a better measure of intelligence?

## Architecture Onboarding

- Component map: Seed data collection -> LLM rewriter (GPT-4-Turbo) -> 4 problem variants generation -> 4 task-specific data construction -> Human validation -> Model evaluation -> Performance aggregation -> Correlation analysis

- Critical path: 1. Seed data collection and validation -> 2. Automatic generation of 4 problem variants -> 3. Task-specific data construction for each variant -> 4. Human validation and quality control -> 5. Model evaluation and performance aggregation -> 6. Correlation analysis with intelligence proxies

- Design tradeoffs: LLM generation vs expert creation (speed and scalability vs potential bias) | Binary classification tasks vs continuous scoring (simplicity vs granularity) | Multi-dimensional evaluation vs single score (comprehensiveness vs interpretability)

- Failure signatures: Low pass rate in human validation (>15% failure) | High correlation between original problem solving and other tasks (indicating insufficient diversity) | Poor correlation with intelligence proxies despite high overall scores

- First 3 experiments: 1. Run MathCheck generation on a small GSM8k subset and manually verify variant quality -> 2. Evaluate 3-5 diverse models and check if performance patterns match expectations -> 3. Compare correlation with GSM1k private data vs original GSM8k scores

## Open Questions the Paper Calls Out

- How can the MathCheck checklist paradigm be adapted for reasoning tasks that don't involve numerical or geometric components, such as causal reasoning or ethical reasoning?
- What is the optimal balance between the number of tasks and problem variants in the MathCheck checklist to maximize evaluation comprehensiveness without causing excessive computational overhead?
- How does the performance of models on MathCheck compare to their performance on other established benchmarks for mathematical reasoning, such as MATH or TheromQA, in terms of predicting real-world mathematical problem-solving ability?

## Limitations

- Claims about MathCheck's superiority rely heavily on correlation analyses with compression efficiency and private data performance, which remain somewhat speculative
- The automatic generation pipeline's quality and diversity beyond validation pass rates is not thoroughly demonstrated
- The framework's effectiveness across different model generations and training paradigms is not fully validated

## Confidence

- **High Confidence**: The core mechanism of multi-task evaluation revealing model weaknesses is well-supported by conceptual framework and initial experimental results
- **Medium Confidence**: The automatic generation pipeline's effectiveness is supported by validation pass rates but lacks detailed analysis of long-term quality and potential systematic biases
- **Medium Confidence**: Claims about better correlation with intelligence proxies are statistically significant but depend on the validity of compression efficiency and private data as intelligence measures

## Next Checks

1. Conduct ablation studies removing specific task variants to quantify each component's contribution to overall evaluation quality and correlation with intelligence proxies
2. Test MathCheck's robustness across different model generations and training paradigms to verify that performance patterns remain consistent and meaningful
3. Compare MathCheck's evaluation outcomes with human expert assessments of mathematical reasoning ability to validate the benchmark's face validity beyond statistical correlations