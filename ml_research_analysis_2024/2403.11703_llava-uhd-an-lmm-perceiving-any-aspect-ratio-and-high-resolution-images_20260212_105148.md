---
ver: rpa2
title: 'LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images'
arxiv_id: '2403.11703'
source_url: https://arxiv.org/abs/2403.11703
tags:
- image
- llav
- images
- visual
- aspect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates systematic flaws in visual encoding strategies
  of large multimodal models (LMMs) like GPT-4V and LLaVA-1.5. The authors expose
  that these models struggle with high-resolution images due to inefficient encoding
  methods, leading to performance degradation and potential vulnerabilities to adversarial
  attacks.
---

# LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images

## Quick Facts
- arXiv ID: 2403.11703
- Source URL: https://arxiv.org/abs/2403.11703
- Reference count: 40
- Key outcome: LLaVA-UHD achieves 6.4 accuracy improvement on TextVQA and supports 6x larger resolution images using 94% inference computation compared to LLaVA-1.5.

## Executive Summary
This paper addresses systematic flaws in visual encoding strategies of large multimodal models (LMMs) like GPT-4V and LLaVA-1.5, which struggle with high-resolution images due to inefficient encoding methods. The authors propose LLaVA-UHD, a model that efficiently processes images of any aspect ratio and high resolution through an innovative modularized visual encoding strategy. Comprehensive experiments on 9 benchmarks demonstrate that LLaVA-UHD outperforms established LMMs trained on 2-3 orders of magnitude more data while maintaining computational efficiency.

## Method Summary
LLaVA-UHD employs a modularized visual encoding strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding. The approach uses an image partitioner to compute optimal slice layouts based on ideal slice count and a score function minimizing deviation from ViT pretraining resolution. Each slice is encoded with interpolated position embeddings using CLIP-ViT-L/14, then compressed from 576 to 64 visual tokens using a perceiver resampler layer with cross-attention pooling. The compressed slice tokens are organized with spatial schema tokens ("," and "\n") to indicate horizontal and vertical positioning, then processed by an LLM (Vicuna-13B) through a two-stage training procedure combining pre-training and instruction-tuning.

## Key Results
- LLaVA-UHD achieves 6.4 accuracy improvement on TextVQA benchmark compared to LLaVA-1.5
- Supports 6x larger resolution images while using only 94% of inference computation
- Outperforms established LMMs trained on 2-3 orders of magnitude more data across 9 benchmarks
- Demonstrates robustness to images with extreme aspect ratios (1:6, 6:1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Variable-sized slice partitioning reduces deviation from pretraining resolution while supporting native image shapes.
- Mechanism: The partition strategy factorizes the ideal slice count N into m columns and n rows, then scores each factorization based on how closely the resulting slice aspect ratio matches the pretraining ViT ratio. It selects the best partition from N-1, N, and N+1 candidates to ensure flexibility.
- Core assumption: Smaller deviation from pretraining resolution preserves ViT capabilities while allowing variable aspect ratios.
- Evidence anchors: [abstract] "An image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding"; [section 3.1] "We define a score function to measure the deviation from the standard pretraining setting of ViT"; [corpus] Weak - related works mention variable-sized encoding but do not analyze slice aspect ratio deviation mathematically.
- Break condition: If the slice aspect ratio deviates significantly from pretraining, ViT performance degrades due to out-of-distribution embeddings.

### Mechanism 2
- Claim: Compression layer reduces LLM computation while maintaining representation quality.
- Mechanism: A shared perceiver resampler layer uses cross-attention with a fixed set of query vectors to condense visual tokens from 576 to 64 per slice, reducing quadratic cost in LLM processing.
- Core assumption: Cross-attention pooling preserves semantic content better than linear projection when reducing token count.
- Evidence anchors: [abstract] "a compression module that further condenses image tokens from visual encoders"; [section 3.2] "image tokens output by the visual encoders are resampled to a lower number using a set of query vectors via cross-attention"; [corpus] Weak - other works mention token reduction but do not compare cross-attention vs MLP in this context.
- Break condition: If compression ratio is too aggressive, fine-grained visual details necessary for tasks like TextVQA are lost.

### Mechanism 3
- Claim: Spatial schema enables LLM to reconstruct image layout from unordered slice tokens.
- Mechanism: Special tokens "," and "\n" are inserted between slice representations to indicate horizontal and vertical positioning, allowing LLM to infer spatial relationships without positional embeddings per slice.
- Core assumption: LLM can learn to interpret these schema tokens as spatial separators during instruction tuning.
- Evidence anchors: [abstract] "a spatial schema to organize slice tokens for LLMs"; [section 3.3] "we use ',' to separate the slice representations in a row, and use '\n' to separate different rows"; [corpus] Weak - no direct evidence in corpus that LLMs successfully learn spatial schema from these tokens.
- Break condition: If schema tokens are not consistently learned, LLM cannot correctly interpret slice positions, hurting spatial reasoning tasks.

## Foundational Learning

- Concept: Cross-attention pooling in perceiver resampler
  - Why needed here: Efficiently reduces visual token count while preserving semantic information for LLM processing
  - Quick check question: What is the computational complexity difference between cross-attention pooling and linear projection when reducing tokens from 576 to 64?

- Concept: Positional embedding interpolation
  - Why needed here: Adapts pretrained ViT position embeddings to arbitrary slice aspect ratios without retraining
  - Quick check question: How does 2D interpolation of position embeddings differ from simple 1D reshaping when adapting to non-square slices?

- Concept: Factorization of integer partitions
  - Why needed here: Determines optimal grid layout (m x n) for dividing images into slices while minimizing aspect ratio distortion
  - Quick check question: Given an image with 7 slices, what are all possible factorizations and how does each affect slice aspect ratio?

## Architecture Onboarding

- Component map: Native resolution image -> Image partitioner (computes N and selects best (m,n) factorization) -> Image slicer (divides image into variable-sized slices) -> ViT encoder (encodes each slice with interpolated position embeddings) -> Perceiver resampler (condenses slice tokens via cross-attention) -> Spatial decorator (adds "," and "\n" tokens between slices) -> LLM (processes overview + slice tokens with spatial schema) -> Text response

- Critical path: Image → Partition → Slice → ViT encode → Resample → Spatial decorate → LLM → Output

- Design tradeoffs:
  - Variable vs fixed slice sizes: Variable reduces padding waste but adds partitioning complexity
  - Cross-attention vs MLP compression: Cross-attention better preserves semantics but has higher per-token cost
  - Schema tokens vs learned position: Schema is simpler but requires LLM to learn interpretation

- Failure signatures:
  - High variance in slice aspect ratios → ViT performance degradation
  - Too aggressive compression → Loss of fine details for OCR tasks
  - LLM ignores schema tokens → Spatial reasoning failures

- First 3 experiments:
  1. Test slice partitioning on extreme aspect ratio images (1:6, 6:1) and verify aspect ratio bounds
  2. Compare cross-attention vs MLP compression on TextVQA accuracy vs computation
  3. Remove spatial schema tokens and measure impact on spatial reasoning benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the modularized visual encoding strategy in LLaVA-UHD affect model performance on tasks requiring global context, compared to models using single high-resolution images?
- Basis in paper: [inferred] The paper mentions that image slices are independently encoded, with interactions only in LLMs. This raises questions about the impact on tasks requiring global context.
- Why unresolved: The paper does not provide specific experiments or analysis comparing LLaVA-UHD's performance on tasks requiring global context with models using single high-resolution images.
- What evidence would resolve it: Experiments comparing LLaVA-UHD's performance on tasks requiring global context (e.g., scene understanding, image captioning) with models using single high-resolution images, along with ablation studies on the impact of slice interactions.

### Open Question 2
- Question: How does the choice of score function S(·) in the image partition strategy influence the model's ability to handle images with extreme aspect ratios?
- Basis in paper: [explicit] The paper describes the score function S(·) used to select the best partition, but does not provide a detailed analysis of its impact on handling images with extreme aspect ratios.
- Why unresolved: The paper only mentions that the advantage of LLaVA-UHD increases with more extreme aspect ratios, but does not provide a detailed analysis of how the score function S(·) contributes to this advantage.
- What evidence would resolve it: Experiments varying the score function S(·) and analyzing its impact on LLaVA-UHD's performance on images with extreme aspect ratios, along with a detailed analysis of the relationship between the score function and the model's ability to handle such images.

### Open Question 3
- Question: How does the compression layer in LLaVA-UHD affect the model's ability to retain fine-grained details in high-resolution images?
- Basis in paper: [explicit] The paper mentions that the compression layer condenses image tokens from visual encoders, but does not provide a detailed analysis of its impact on retaining fine-grained details.
- Why unresolved: The paper does not provide specific experiments or analysis on how the compression layer affects the model's ability to retain fine-grained details in high-resolution images.
- What evidence would resolve it: Experiments comparing LLaVA-UHD's performance on tasks requiring fine-grained details (e.g., optical character recognition, small object detection) with and without the compression layer, along with an analysis of the trade-off between compression and detail retention.

## Limitations

- Limited empirical validation of spatial schema mechanism: The paper claims special tokens enable LLMs to reconstruct image layout but provides minimal ablation evidence.
- Cross-attention vs MLP compression assumption unverified: The paper assumes cross-attention pooling preserves semantic content better than linear projection without experimental validation.
- ViT aspect ratio adaptation boundary unclear: The paper doesn't specify how much deviation from pretraining resolution is acceptable before performance degrades.

## Confidence

**High confidence**: The modularization strategy's basic implementation and computation efficiency claims are well-supported. The image partitioning algorithm and compression layer architecture are clearly specified with reasonable theoretical justification.

**Medium confidence**: The performance improvements on established benchmarks (6.4 accuracy gain on TextVQA, support for 6× larger resolution) are demonstrated but could benefit from more rigorous ablation studies isolating individual components' contributions.

**Low confidence**: The spatial schema's effectiveness relies heavily on LLM's ability to learn token interpretation during instruction tuning, which is not empirically validated through controlled experiments.

## Next Checks

1. **Ablation study of compression method**: Replace the perceiver resampler's cross-attention with a linear projection layer that reduces tokens from 576 to 64. Compare both accuracy and computational efficiency on TextVQA to validate the claimed advantage of cross-attention pooling.

2. **Spatial schema stress test**: Systematically remove the overview token, spatial schema tokens, or both from LLaVA-UHD and measure performance degradation across spatial reasoning benchmarks (POPE, MMBench) to quantify each component's contribution.

3. **Aspect ratio deviation analysis**: Create controlled experiments varying slice aspect ratios beyond the claimed [1:2, 2:1] bounds. Measure ViT encoder performance degradation as a function of aspect ratio deviation to establish the robustness boundary of the modularization strategy.