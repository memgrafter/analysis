---
ver: rpa2
title: 'FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning'
arxiv_id: '2406.00645'
source_url: https://arxiv.org/abs/2406.00645
tags:
- reward
- learning
- furl
- rewards
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reward misalignment when using
  pre-trained vision-language models (VLMs) as rewards in sparse-reward reinforcement
  learning tasks. The authors identify that zero-shot VLM rewards are often fuzzy
  and can mislead policy optimization.
---

# FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.00645
- Source URL: https://arxiv.org/abs/2406.00645
- Authors: Yuwei Fu; Haichao Zhang; Di Wu; Wei Xu; Benoit Boulet
- Reference count: 28
- Primary result: Achieves up to 76% average success rate on Meta-world MT10 tasks, outperforming SAC (37%) and VLM-only (0%) baselines.

## Executive Summary
This paper addresses reward misalignment when using pre-trained vision-language models (VLMs) as rewards in sparse-reward reinforcement learning tasks. The authors identify that zero-shot VLM rewards are inherently fuzzy, leading to poor alignment with actual task progress. They propose Fuzzy VLM reward-aided RL (FuRL), which combines reward alignment via contrastive learning and relay RL to escape local minima. Experiments show FuRL significantly outperforms baselines, achieving up to 76% average success rate on Meta-world tasks compared to 37% for SAC and 0% for VLM-only approaches.

## Method Summary
FuRL addresses fuzzy VLM rewards through two key mechanisms: reward alignment and relay RL. The method fine-tunes VLM representations using contrastive learning to better align with task progress while keeping the pre-trained VLM backbone frozen. Simultaneously, relay RL alternates between SAC and VLM policies to escape local minima and increase exploration diversity. The approach is evaluated on Meta-world MT10 benchmark tasks, showing significant improvements over baselines, and demonstrates effectiveness with both pixel-based observations and different VLM backbones like CLIP and LIV.

## Key Results
- FuRL achieves up to 76% average success rate on Meta-world MT10 tasks
- Outperforms SAC baseline (37%) and VLM-only approaches (0%)
- Demonstrates effectiveness with both pixel-based observations and different VLM backbones
- Shows robust performance across various Meta-world tasks including reach, push, pick-place, and drawer opening

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Zero-shot VLM rewards are inherently fuzzy, leading to reward misalignment in sparse-reward RL tasks.
- **Mechanism**: Pre-trained VLMs like CLIP generate embeddings that capture coarse semantics but miss task-specific details, resulting in rewards that correlate poorly with actual task progress. This fuzziness misleads policy optimization into local minima.
- **Core assumption**: The VLM was trained on a different domain than the RL task, causing representation mismatch.
- **Evidence anchors**:
  - [abstract]: "We first identify the problem of reward misalignment when applying VLM as a reward in RL tasks."
  - [section]: "We demonstrate that zero-shot VLM rewards are fuzzy: meaningful in capturing the coarse semantics but inaccurate in characterizing some details."
- **Break condition**: If the VLM is fine-tuned on domain-specific data before use, the fuzziness may be reduced or eliminated.

### Mechanism 2
- **Claim**: Reward alignment via contrastive learning improves VLM reward accuracy by fine-tuning projection heads.
- **Mechanism**: The method freezes the pre-trained VLM and learns two small MLPs (`fWL`, `fWI`) to project language and image embeddings. A ranking loss aligns these projections with sparse task rewards, making VLM rewards more consistent with task progress.
- **Core assumption**: Sparse task rewards provide a reliable signal for alignment, even though they are infrequent.
- **Evidence anchors**:
  - [section]: "We introduced a lightweight alignment method... freeze the pre-trained VLM and only append two small learnable networks fWL and fWI."
  - [section]: "Lpos-neg learns to generate a higher VLM reward for a positive sample than that of a negative sample."
- **Break condition**: If no successful trajectories are collected, alignment cannot proceed, and the method fails.

### Mechanism 3
- **Claim**: Relay RL mitigates local minima by alternating between VLM and SAC policies, increasing exploration diversity.
- **Mechanism**: At the start of each episode, a relay step `Ti` is sampled. The SAC agent and VLM agent alternate for `Ti` steps, collecting diverse data. This helps the VLM policy escape local minima caused by fuzzy rewards.
- **Core assumption**: The SAC agent can explore effectively in the early stages before the VLM policy is reliable.
- **Evidence anchors**:
  - [section]: "We introduce a simple exploration strategy based on the relay RL... maintained an extra SAC agent πSAC besides the current VLM agent πVLM."
  - [section]: "The motivation of the relay RL is to let the SAC agent help to escape the local minima once the VLM agent gets stuck."
- **Break condition**: If the relay policy is too dominant, it may override the VLM's learned preferences, slowing convergence.

## Foundational Learning

- **Concept**: Sparse reward MDPs
  - Why needed here: The paper focuses on tasks where reward is only given upon success, making exploration and learning difficult.
  - Quick check question: In a sparse reward setting, what happens if the agent never stumbles upon a successful trajectory?

- **Concept**: Vision-language models and embedding alignment
  - Why needed here: VLM rewards rely on cosine similarity between image and text embeddings; misalignment causes inaccurate rewards.
  - Quick check question: Why does freezing the VLM backbone and only fine-tuning projection heads help reduce overfitting?

- **Concept**: Contrastive learning and ranking losses
  - Why needed here: The reward alignment loss ranks positive vs. negative samples to improve reward signal quality.
  - Quick check question: What is the role of the margin parameter δ in the ranking loss?

## Architecture Onboarding

- **Component map**: Pre-trained VLM (CLIP/LIV) -> frozen backbone -> Two MLPs (`fWL`, `fWI`) -> projection heads -> SAC agent -> baseline RL policy -> Relay controller -> alternates between SAC and VLM policies -> Shared replay buffer -> stores transitions from both policies -> Reward alignment loss -> contrastive objective on successful/unsuccessful samples

- **Critical path**:
  1. Initialize VLM, SAC, and projection heads.
  2. Collect trajectories using relay RL (SAC/VLM alternation).
  3. If successful trajectories exist, update projection heads via reward alignment loss.
  4. Update VLM policy with aligned rewards.
  5. Evaluate VLM policy.

- **Design tradeoffs**:
  - Fine-tuning only projection heads vs. full VLM: lower compute, less risk of catastrophic forgetting.
  - Relay RL vs. intrinsic motivation: relay is simpler but adds a second policy; intrinsic reward could be lighter but harder to design.
  - Sparse vs. dense task reward: sparse is more realistic but harder; dense could speed up alignment but is less general.

- **Failure signatures**:
  - VLM policy never finds success → relay never disabled, alignment never starts.
  - High variance in reward → projection heads overfit to noise in sparse rewards.
  - SAC dominates too early → VLM policy never learns meaningful behavior.

- **First 3 experiments**:
  1. Run SAC baseline on a simple sparse task (e.g., reach-v2) to confirm baseline performance.
  2. Add VLM reward without alignment or relay; observe poor performance (as in Figure 2).
  3. Enable relay RL only; check if it helps escape local minima but alignment is still missing.

## Open Questions the Paper Calls Out
None explicitly called out in the provided material.

## Limitations
- Exact hyperparameters for contrastive alignment loss (margin δ, window size k) are underspecified
- Switching criteria for relay RL (when to disable it) are not precisely defined
- Scalability to more complex tasks or different VLM architectures beyond CLIP/LIV is not tested

## Confidence
- **High confidence**: The core mechanism of reward misalignment in zero-shot VLM rewards is well-supported by empirical results and ablation studies
- **Medium confidence**: The effectiveness of relay RL in escaping local minima is demonstrated, but the exact conditions under which it is most beneficial are not fully explored
- **Low confidence**: The scalability of FuRL to more complex tasks or different VLM architectures beyond CLIP/LIV is not tested

## Next Checks
1. Test FuRL on a wider range of sparse-reward tasks to evaluate generalizability and robustness
2. Conduct ablation studies on the relay RL schedule to determine optimal switching conditions
3. Explore the impact of different VLM backbones (e.g., BLIP, Flamingo) to assess scalability and domain adaptation