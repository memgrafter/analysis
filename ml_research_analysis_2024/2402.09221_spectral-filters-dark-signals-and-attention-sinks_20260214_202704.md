---
ver: rpa2
title: Spectral Filters, Dark Signals, and Attention Sinks
arxiv_id: '2402.09221'
source_url: https://arxiv.org/abs/2402.09221
tags:
- attention
- tuatara
- time
- llama2
- dark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Spectral filters partition the singular vectors of the embedding
  and unembedding matrices to isolate "dark" signals that lie in the subspace spanned
  by the smallest singular values. Experiments on LLaMa2 models show that filtering
  away large portions of the spectrum harms performance only when attention-sinking
  signals are removed.
---

# Spectral Filters, Dark Signals, and Attention Sinks

## Quick Facts
- arXiv ID: 2402.09221
- Source URL: https://arxiv.org/abs/2402.09221
- Reference count: 39
- Primary result: Spectral filtering can remove up to 25% of the spectrum without harming performance if attention-sinking signals are preserved

## Executive Summary
This paper investigates the role of "dark signals" - information encoded in the smallest singular values of transformer embedding and unembedding matrices - and their relationship to attention sinks. The authors demonstrate that models can maintain low perplexity even when suppressing 25% of the spectral components, provided attention-sinking signals are preserved. The Beginning-of-Sentence token acts as a critical attention sink by accumulating large dark-signal vectors, and suppressing this mechanism causes performance collapse. High-attention tokens also show elevated dark-signal prevalence, especially in upper layers, suggesting a broader pattern of attention-attention correlation.

## Method Summary
The authors apply spectral filters to intermediate representations in LLaMa2 models by partitioning the singular vectors of embedding and unembedding matrices into bands. They measure the impact on negative log-likelihood (NLL) when projecting vectors onto selected subspaces, and analyze the correlation between attention received by tokens and the prevalence of dark signals in their residual streams. Experiments use LLaMa2 models (7B, 13B, 70B parameters) on English prompts from CCNET and code from DeepMind Code Contest.

## Key Results
- Models maintain low perplexity when suppressing up to 25% of spectral components if attention-sinking signals are preserved
- The Beginning-of-Sentence token accumulates large dark-signal vectors that act as attention sinks
- Suppressing BoS dark signals causes performance collapse
- High-attention tokens show elevated dark-signal prevalence, especially in upper layers
- Performance degrades only when attention-sinking signals are removed, not simply when spectrum is reduced

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spectral filters isolate "dark signals" that lie in the subspace spanned by the smallest singular values of the embedding and unembedding matrices.
- Mechanism: The dark subspace captures signals that do not interfere with the immediate next token prediction but are essential for maintaining global features and long-range dependencies. By partitioning the singular vectors into bands, spectral filters can selectively suppress or preserve these signals.
- Core assumption: The smallest singular values encode non-sparse features critical for attention sinking but not directly tied to vocabulary logits.
- Evidence anchors:
  - [abstract] "Spectral filters partition the singular vectors of the embedding and unembedding matrices to isolate 'dark' signals that lie in the subspace spanned by the smallest singular values."
  - [section 4] "We split the singular vectors of Wu into 20 bands... Let Vu,j:k be the matrix obtained concatenating [Vu,j . . . Vu,k], and let Φu,j:k = Vu,j:kV ⊤ u,j:k (similarly for We)."
  - [corpus] "Weak. Neighbor papers discuss spectral filtering and graph attention but do not provide direct evidence for dark signal isolation in transformers."

### Mechanism 2
- Claim: The Beginning-of-Sentence (BoS) token accumulates a large dark-signal vector that acts as an attention sink.
- Mechanism: When attention heads are inactive or need to distribute excess attention, they route it to the BoS token. This requires a large, U-dark residual stream vector maintained by the L3 MLP. Suppressing this vector collapses performance.
- Core assumption: Attention normalization forces a constant amount of attention to be distributed, creating a need for a sink.
- Evidence anchors:
  - [abstract] "The Beginning-of-Sentence token accumulates a large dark-signal vector that acts as an attention sink, and suppressing it causes performance to collapse."
  - [section 5] "The MLP at L3 blasts off a vector of large norm and almost completely U-dark. This vector acts as an attention collector for heads in need of a sink."
  - [corpus] "Weak. Neighbor paper 'Attention Sinks and Compression Valleys...' discusses sinks but not the specific BoS mechanism."

### Mechanism 3
- Claim: Tokens receiving high attention also show elevated dark-signal prevalence, especially in upper layers.
- Mechanism: High-mean low-variance (HMLV) tokens accumulate dark signals similar to the BoS token, possibly serving as auxiliary attention sinks or encoding global features.
- Core assumption: High attention correlates with dark signal accumulation, indicating a functional role beyond the BoS token.
- Evidence anchors:
  - [abstract] "Tokens receiving high attention also show elevated dark-signal prevalence, especially in upper layers."
  - [section 7] "We define High-Mean Low-Variance (HMLV) token-layer pairs... We define the U-Dark ratio as a measure of the prevalence of U-Dark signals in a representation."
  - [corpus] "Missing. No neighbor paper provides evidence for attention-attention dark signal correlation."

## Foundational Learning

- Concept: Singular value decomposition (SVD) and spectral decomposition of matrices.
  - Why needed here: The paper partitions singular vectors into bands to define spectral filters. Understanding SVD is essential to grasp how the dark subspace is defined.
  - Quick check question: Given a matrix A, how do you compute its SVD and what do the singular vectors represent?

- Concept: Attention mechanisms in transformers and attention normalization.
  - Why needed here: The paper relies on the attention sink phenomenon, which depends on how attention scores are normalized and distributed.
  - Quick check question: In a standard transformer, how are attention scores computed and normalized before being used to weigh values?

- Concept: Residual stream and multi-layer transformer architecture.
  - Why needed here: The paper manipulates intermediate representations in the residual stream and studies how spectral filters affect layer-by-layer performance.
  - Quick check question: In a transformer, how does the residual stream flow through layers, and what role do MLP and attention components play?

## Architecture Onboarding

- Component map: Embedding matrix (We) -> Residual stream (RS) -> MLP layers/Attention heads -> Unembedding matrix (Wu) -> Next token prediction
- Critical path: Embedding → residual stream → MLP/attention updates → unembedding → next token prediction. The dark signals flow through this path but are not directly tied to vocabulary logits.
- Design tradeoffs: Preserving dark signals maintains attention sinking but may increase model capacity; suppressing them reduces capacity but risks performance collapse if sinks are removed.
- Failure signatures: Performance collapse when BoS dark signals are suppressed; repetitive generation patterns when attention sinks are inhibited; step-decrease in NLL when last 5% of singular vectors are removed.
- First 3 experiments:
  1. Apply spectral filters to intermediate representations and measure NLL on a validation set.
  2. Swap filtered components between samples to isolate the role of BoS dark signals.
  3. Generate continuations with and without dark signal preservation to assess qualitative impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the spectral filtering approach extend to other transformer architectures beyond LLaMa2, such as GPT or BERT?
- Basis in paper: [explicit] The paper focuses on LLaMa2 models and does not explore spectral filtering on other transformer architectures.
- Why unresolved: The paper does not provide any comparative analysis of spectral filtering effects on different transformer architectures, leaving the generalizability of the findings unclear.
- What evidence would resolve it: Experiments applying spectral filtering to a diverse set of transformer models (e.g., GPT-3, BERT, RoBERTa) and comparing the results to LLaMa2 would demonstrate the broader applicability of the approach.

### Open Question 2
- Question: What is the precise mechanism by which dark signals facilitate attention sinking, and how do they differ from other types of signals in the residual stream?
- Basis in paper: [explicit] The paper identifies dark signals as crucial for attention sinking but does not provide a detailed mechanistic explanation of how they enable this process.
- Why unresolved: While the paper demonstrates the importance of dark signals for attention sinking, it does not elucidate the specific interactions between dark signals and attention mechanisms that lead to this phenomenon.
- What evidence would resolve it: Detailed analysis of the attention mechanisms and their interactions with dark signals, potentially using techniques like attention visualization or ablation studies, would clarify the precise role of dark signals in attention sinking.

### Open Question 3
- Question: How do spectral filters affect the long-term performance and generalization of transformer models on downstream tasks?
- Basis in paper: [inferred] The paper focuses on the immediate effects of spectral filtering on perplexity and generation quality but does not investigate the long-term impact on model performance.
- Why unresolved: The paper does not provide any experiments or analysis of how spectral filtering might influence the model's ability to learn and generalize over extended training or fine-tuning on specific tasks.
- What evidence would resolve it: Long-term studies evaluating the performance of spectral-filtered models on various downstream tasks, comparing them to baseline models, would reveal the potential benefits and drawbacks of this approach for practical applications.

## Limitations

- The relationship between spectral properties and semantic content remains underexplored
- Alternative explanations for performance collapse when BoS signals are removed cannot be ruled out
- The role of HMLV tokens as auxiliary sinks is suggestive but not conclusively demonstrated
- Experiments focus only on LLaMa2 models, limiting generalizability to other architectures

## Confidence

**High confidence**: The existence of dark signals in the smallest singular subspaces is well-supported by empirical evidence across multiple model sizes and datasets. The correlation between attention patterns and dark signal prevalence is robust and reproducible.

**Medium confidence**: The BoS token serving as a primary attention sink is plausible given the evidence, but alternative sink mechanisms or training artifacts could explain the observations. The specific mechanism by which dark signals enable attention sinking needs further mechanistic validation.

**Low confidence**: The broader claim that dark signals are essential for transformer function beyond attention sinking lacks sufficient evidence. The role of HMLV tokens as auxiliary sinks is suggestive but not conclusively demonstrated.

## Next Checks

1. **Mechanistic ablation study**: Systematically disable specific attention heads or MLP components in isolation while preserving dark signals to determine which components actually require attention sinking for function. This would distinguish between dark signals being necessary for attention sinking versus being a correlated but non-causal phenomenon.

2. **Cross-architecture replication**: Apply the spectral filtering analysis to BERT, GPT, and other transformer variants to test whether dark signals and attention sinks are universal architectural features or LLaMa-specific artifacts. This would validate the generality of the findings.

3. **Training intervention experiment**: Train models with modified attention normalization that explicitly prevents attention accumulation at any token, then analyze whether dark signals still emerge and whether alternative mechanisms compensate. This would test whether dark signals are a consequence of architectural constraints or an emergent necessity.