---
ver: rpa2
title: Learning Randomized Algorithms with Transformers
arxiv_id: '2408.10818'
source_url: https://arxiv.org/abs/2408.10818
tags:
- randomized
- transformer
- algorithms
- algorithm
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores how randomization can be incorporated into transformer
  models to improve robustness and performance, especially in adversarial settings.
  It proposes a new training objective that encourages transformers to leverage input
  randomness, leading to the discovery of randomized algorithms within the model.
---

# Learning Randomized Algorithms with Transformers

## Quick Facts
- arXiv ID: 2408.10818
- Source URL: https://arxiv.org/abs/2408.10818
- Reference count: 40
- Key outcome: Transformers can learn randomized algorithms by optimizing a relaxed adversarial loss over multiple random seeds, improving worst-case performance in adversarial settings.

## Executive Summary
This paper explores how randomization can be incorporated into transformer models to improve robustness and performance, especially in adversarial settings. It proposes a new training objective that encourages transformers to leverage input randomness, leading to the discovery of randomized algorithms within the model. The key idea is to optimize a relaxed adversarial loss that considers multiple random seeds, pushing the model to distribute its predictions over seeds to avoid consistent failure on any input. Experiments on three tasks—associative recall, graph coloring, and grid world exploration—demonstrate that transformers trained with this objective exhibit proper randomization, achieving significantly better worst-case performance and scalability compared to deterministic baselines.

## Method Summary
The approach involves training transformers with random seed encoding (RSE) concatenated to input tokens. The training objective uses a q-norm relaxation of the min-max loss over multiple random seeds, encouraging the model to spread predictions and avoid consistent failure. For associative recall and graph coloring tasks, gradient descent optimization is used, while evolutionary strategies (PEPG) are employed for the grid world task. The model's performance is evaluated using success rate, 95th percentile performance, and majority voting across seeds.

## Key Results
- Transformers trained with q-norm adversarial objective show proper randomization, avoiding deterministic collapse
- Majority voting across seeds significantly improves worst-case performance, often reaching near-optimal results
- Randomized models achieve better worst-case performance and scalability compared to deterministic baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Randomization is learned by optimizing a relaxed adversarial loss (Lq) instead of ERM.
- Mechanism: The q-norm penalizes worst-case errors across seeds, forcing the model to spread predictions so no single input is consistently failed.
- Core assumption: There exists no single parameter that perfectly fits all data deterministically.
- Evidence anchors:
  - [abstract] "optimize a relaxed adversarial loss that considers multiple random seeds"
  - [section] "we propose the following practical training objective: arg min θ L̂q(θ) = ..."
  - [corpus] Weak evidence; corpus papers focus on adversarial robustness but not learned randomization in transformers.
- Break condition: If a deterministic parameter can fit the data perfectly, randomness loses incentive.

### Mechanism 2
- Claim: Majority voting across seeds amplifies success probability by leveraging the spread of predictions.
- Mechanism: Each seed underfits different parts of the input space; majority vote corrects individual failures, approaching optimal recall.
- Core assumption: The randomized model's predictions are conditionally independent given the input.
- Evidence anchors:
  - [abstract] "Majority voting across seeds further amplifies performance, often reaching near-optimal results"
  - [section] "Simply taking the majority vote among m predictions computed on different seeds boosts performance to a perfect success rate"
  - [corpus] No direct evidence; corpus papers do not address majority voting over seeds in transformer algorithms.
- Break condition: If predictions are highly correlated across seeds, majority voting provides little gain.

### Mechanism 3
- Claim: Providing a random seed as input via Random Seed Encoding (RSE) enables the transformer to use randomness internally.
- Mechanism: The seed vector is concatenated to tokens, allowing self-attention layers to condition computations on randomness and avoid deterministic collapse.
- Core assumption: The transformer architecture can propagate and utilize the seed information through layers.
- Evidence anchors:
  - [abstract] "provide randomness, denoted as a seed, as an additional input to the model"
  - [section] "we define two random variables X (over X) and R (over R). For any parameter θ, x → Aθ(x, R) is a randomized transformer model"
  - [corpus] Weak evidence; corpus papers focus on adversarial robustness or GNN training, not seed-based randomization in transformers.
- Break condition: If the seed is ignored (weights set to zero), the model collapses to deterministic behavior.

## Foundational Learning

- Concept: Adversarial training and robust optimization
  - Why needed here: The core idea is to train models to perform well in worst-case scenarios, not just average case.
  - Quick check question: Can you explain why optimizing L1 (ERM) does not encourage randomization, but Lq with q>1 does?

- Concept: Randomized algorithms and Yao's Minimax Principle
  - Why needed here: Understanding why randomization helps against oblivious adversaries is key to the theoretical motivation.
  - Quick check question: What is the difference between a Monte Carlo and a Las Vegas randomized algorithm?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The experiments rely on transformer models with self-attention, including variants like linear attention.
  - Quick check question: How does masking in self-attention enforce locality in the graph coloring task?

## Architecture Onboarding

- Component map:
  Input tokens -> Random Seed Encoding (RSE) -> Transformer blocks with self-attention -> Output distribution over discrete actions/classes

- Critical path:
  1. Tokenize input + append RSE
  2. Pass through transformer layers
  3. Compute output distribution
  4. Evaluate loss over multiple seeds
  5. Backpropagate through averaged q-norm loss

- Design tradeoffs:
  - More seeds (m) → better adversarial coverage but higher compute
  - Higher q → stronger worst-case focus but risk of over-regularization
  - Linear vs softmax attention → affects memory and speed

- Failure signatures:
  - Low variance in predictions across seeds → model collapsed to deterministic
  - High training loss but low test loss → overfitting to seed-specific patterns
  - Degraded performance with majority voting → predictions too correlated

- First 3 experiments:
  1. Train on associative recall with q=1 (ERM) and observe deterministic collapse.
  2. Train on associative recall with q=100 and m=30, check if majority voting improves worst-case recall.
  3. Vary q from 1 to 100 and plot success rate vs q to find randomization phase transition.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational intensity of the proposed adversarial training objective be scaled up for larger, more practical settings?
- Basis in paper: [explicit] The authors acknowledge that comparing to ERM trained models, the hyperparameter m scales memory and training/inference time by a factor of m, limiting scalability.
- Why unresolved: The paper does not provide concrete solutions or techniques to address this computational bottleneck for scaling up the approach.
- What evidence would resolve it: Developing and demonstrating novel techniques or architectural modifications that reduce the computational burden of the adversarial objective while maintaining or improving performance.

### Open Question 2
- Question: What are the key differences, advantages, and synergies between sampling from policies/probabilistic models versus input-level sampling as proposed in this paper?
- Basis in paper: [explicit] The authors highlight this as a key research direction, stating that methods like MCMC, variational inference, and latent variable models use randomness differently than their approach.
- Why unresolved: The paper does not provide a comprehensive analysis or experimental comparison of these different approaches to incorporating randomness.
- What evidence would resolve it: A thorough theoretical and empirical study comparing the performance, robustness, and generalization capabilities of models trained with input-level sampling versus other randomness-based techniques.

### Open Question 3
- Question: Can the proposed adversarial training objective be effectively applied to more complex domains like natural language processing and reinforcement learning?
- Basis in paper: [explicit] The authors speculate that their approach could allow for randomized algorithms to be learned in adversarial bandits and RL, but do not provide concrete experiments or results in these domains.
- Why unresolved: The paper only presents results on relatively simple tasks (associative recall, graph coloring, grid world exploration) and does not demonstrate the applicability of the method to more challenging, real-world problems.
- What evidence would resolve it: Experiments applying the adversarial training objective to tasks in NLP (e.g., text classification, machine translation) or RL (e.g., Atari games, robotics control) and showing improved performance compared to baseline methods.

## Limitations

- Theoretical gaps exist in formally establishing the space of algorithms that can be learned by the transformer architecture
- The Random Seed Encoding (RSE) method lacks detailed implementation specifications
- The adversarial training framework assumes oblivious adversaries, which may not reflect real-world adaptive adversaries

## Confidence

- **High Confidence**: The experimental methodology for associative recall and graph coloring tasks is well-specified, and the observation that ERM leads to deterministic collapse while q-norm objectives encourage randomization is reproducible and clearly demonstrated.
- **Medium Confidence**: The claim that majority voting amplifies performance relies on the assumption of conditionally independent predictions across seeds. While the experiments show this works in practice, the theoretical basis for this independence is not established.
- **Low Confidence**: The grid world results using evolutionary strategies are harder to verify due to unspecified hyperparameters and the complexity of the reinforcement learning setup. The comparison with deterministic baselines may be sensitive to implementation details.

## Next Checks

1. **Seed Information Flow Test**: Conduct an ablation study where the random seed input is replaced with a constant value or random noise with zero correlation to the actual seed. Measure whether prediction variance across seeds drops to zero, confirming that the transformer is actually using the seed information rather than ignoring it through learned weights.

2. **Correlation Analysis Across Seeds**: For a trained randomized model, compute the pairwise correlation of predictions across different seeds on the same input. If correlations are consistently high (>0.8), the model may not be properly randomizing, and majority voting gains would be limited.

3. **Adversarial Oracle Baseline**: Implement an optimal oblivious adversary that selects inputs to maximize loss given knowledge of the model's randomization strategy. Compare the q-norm trained model's performance against this oracle to quantify how close the learned randomization comes to theoretically optimal worst-case performance.