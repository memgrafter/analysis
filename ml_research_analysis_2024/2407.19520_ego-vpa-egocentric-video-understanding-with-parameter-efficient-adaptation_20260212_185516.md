---
ver: rpa2
title: 'Ego-VPA: Egocentric Video Understanding with Parameter-efficient Adaptation'
arxiv_id: '2407.19520'
source_url: https://arxiv.org/abs/2407.19520
tags:
- video
- prompts
- prompt
- basis
- ego-vpa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Ego-VPA addresses the problem of efficiently adapting large video
  foundation models (VFMs) to new egocentric video domains. Existing VFMs typically
  require full fine-tuning, which is computationally expensive, especially when adapting
  to multiple tasks.
---

# Ego-VPA: Egocentric Video Understanding with Parameter-efficient Adaptation

## Quick Facts
- arXiv ID: 2407.19520
- Source URL: https://arxiv.org/abs/2407.19520
- Reference count: 40
- Primary result: Achieves 33.8 mAP on Charades-Ego and 69.17% Mean Accuracy on EGTEA with only 0.84% of VFM parameters

## Executive Summary
Ego-VPA addresses the challenge of efficiently adapting large video foundation models to new egocentric video domains without expensive full fine-tuning. The method introduces a parameter-efficient prompt-tuning approach that uses local sparse approximation with shared basis prompts across frames and modalities. By learning a small set of orthogonal basis prompts that capture common semantic structures in egocentric videos, Ego-VPA can synthesize frame-specific and text-specific prompts while maintaining only 0.84% of the original model's parameters. The approach demonstrates state-of-the-art performance on Charades-Ego and EGTEA datasets, achieving results comparable to full fine-tuning while significantly reducing computational requirements.

## Method Summary
Ego-VPA employs a local sparse approximation mechanism where video frame and text features are projected into a low-dimensional latent space and reconstructed using a small subset of orthogonal basis prompts. These basis prompts are shared across all frames and modalities, enabling efficient context fusion and cross-modal transfer. The method learns three components: a prompt basis matrix F, video prompt synthesis functions (hvid, gvid), and text prompt synthesis functions (htxt, gtxt). During training, the model minimizes both contrastive loss for video-text alignment and cross-modal prompt synthesis loss. The approach uses intra-frame attention in early transformer layers and inter-frame attention in later layers to effectively capture both local and global video context.

## Key Results
- Achieves 33.8 mAP on Charades-Ego dataset, outperforming other prompt-tuning baselines
- Reaches 69.17% Mean Accuracy on EGTEA dataset with only 0.84% of VFM parameters
- Demonstrates comparable performance to full fine-tuning while requiring significantly fewer parameters
- Shows effectiveness with limited training data (10% of full dataset still achieves strong results)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local sparse approximation with shared basis prompts enables efficient context fusion across frames and modalities
- Mechanism: Frame features are projected into a low-dimensional latent prompt space, where each feature is approximated using a sparse subset of orthogonal basis prompts. These basis prompts are shared across all frames and modalities, allowing the model to capture common semantic structures while minimizing parameters
- Core assumption: Egocentric video frames share a common low-dimensional subspace of semantic content that can be efficiently represented by a small set of orthogonal basis prompts
- Evidence anchors: [abstract]: "It employs a local sparse approximation for each video frame/text feature using the basis prompts"; [section]: "Since the basis prompts are shared across frames and modalities, it models context fusion and cross-modal transfer in an efficient fashion"

### Mechanism 2
- Claim: Cross-modal prompt synthesis enables knowledge transfer between visual and textual representations
- Mechanism: The same basis prompts are used to synthesize both video and text prompts. When generating prompts for one modality, the basis prompts that best reconstruct the projected feature of the other modality are selected. This creates a shared semantic space where visual and textual information can be aligned and transferred
- Core assumption: Visual and textual descriptions of the same video event share underlying semantic content that can be captured by the same basis prompt space
- Evidence anchors: [section]: "Since it is trained over the entire dataset, the prompt basis is representative of all frames, allowing efficient cross-frame context modeling. In addition, since the basis prompts capture the semantic content of the video, the method can be naturally extended to cross-modal prompt synthesis"; [section]: "Intuitively, visual content in the video can benefit text domain features and vice versa"

### Mechanism 3
- Claim: Orthogonal basis prompts with local reconstruction minimize parameter count while maintaining expressiveness
- Mechanism: The basis prompts form an orthogonal basis for the latent prompt space, allowing closed-form solutions for the sparse approximation problem. Only the basis prompts and the mapping functions (encoders/decoders) between feature space and latent space need to be learned, dramatically reducing the number of trainable parameters compared to learning separate prompts for each frame or modality
- Core assumption: A low-dimensional orthogonal basis can efficiently represent the prompt space while maintaining the ability to reconstruct frame features with sufficient accuracy
- Evidence anchors: [abstract]: "Ego-VPA excels in lightweight adaptation (with only 0.84% learnable parameters)"; [section]: "Since the basis is optimized on the entire dataset, the subspace spanned by F is a global low dimensional approximation to the space of prompts"

## Foundational Learning

- Concept: Visual-language contrastive learning and InfoNCE loss
  - Why needed here: Understanding how the base Ego-VFM model aligns video and text representations is crucial for designing effective prompt-tuning methods that build upon this alignment
  - Quick check question: How does the InfoNCE loss encourage the video and text encoders to produce aligned representations?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The Ego-VFM uses TimeSformer with spatial and temporal attention blocks, and the prompt synthesis approach needs to work within this architecture
  - Quick check question: What is the difference between spatial and temporal attention in TimeSformer, and why is this distinction important for prompt placement?

- Concept: Parameter-efficient fine-tuning techniques (adapters, prompt tuning)
  - Why needed here: Ego-VPA is a prompt-tuning method, so understanding the landscape of parameter-efficient adaptation approaches is essential for positioning and comparing the method
  - Quick check question: What is the key difference between adapter-based methods and prompt-tuning methods in terms of where they inject learnable parameters?

## Architecture Onboarding

- Component map: Pretrained Ego-VFM backbone (frozen) -> Prompt synthesis module (hvid/hvid encoders, gvid/gtxt decoders, shared basis prompt matrix F) -> Loss function (InfoNCE + cross-modal prompt synthesis loss) -> Training loop (samples basis prompts, uses top-k selection during inference)

- Critical path: 1. Input video frames → video encoder → frame features; 2. Frame features → hvid encoder → latent space; 3. Latent space features → sparse basis prompt selection → selected basis prompts; 4. Selected basis prompts → gvid decoder → frame-specific prompts; 5. Prompts prepended to video encoder input → cross-frame attention → final video feature; 6. Text input → text encoder → text feature → htxt encoder → sparse basis prompt selection → gtxt decoder → text prompts; 7. Contrastive loss and prompt synthesis loss computed → gradients backpropagated to learn basis prompts and mapping functions

- Design tradeoffs: Prompt basis size (B) vs reconstruction accuracy (larger B allows better reconstruction but increases parameters); Reconstruction basis size (k) vs expressiveness (larger k allows more basis prompts per feature but may reduce the benefits of local approximation); Intra/inter-frame attention boundary (K) vs adaptation granularity (earlier layers adapt lower-level features, later layers fuse high-level semantics)

- Failure signatures: Poor performance on egocentric datasets (may indicate insufficient adaptation to domain-specific characteristics); High memory usage (could suggest inefficient basis prompt selection or too large basis size); Slow convergence (might indicate poor initialization of basis prompts or inappropriate learning rate)

- First 3 experiments: 1. Ablation study on prompt basis size B (train with different values like 5, 10, 20 and evaluate performance to find optimal tradeoff); 2. Comparison of prompt query strategies (compare top-k selection vs sampling strategy during training to validate effectiveness of sampling approach); 3. Cross-modal vs single-modal prompt synthesis (train variants with shared vs separate basis prompts for video and text to quantify benefit of cross-modal transfer)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Ego-VPA scale with different numbers of basis prompts (B) and reconstruction basis size (k)?
- Basis in paper: [explicit] The paper ablated the size of prompt basis B and reconstruction basis k in section 6.3, showing that performance saturates quickly with increasing B and that the optimal ratio is k/B < 1
- Why unresolved: While the paper provides ablation results for specific values of B and k, it does not explore the full range of possible values or provide a clear understanding of how these hyperparameters interact with each other and affect performance
- What evidence would resolve it: A comprehensive ablation study exploring a wider range of B and k values, as well as their interaction effects, would provide a clearer understanding of how these hyperparameters impact performance

### Open Question 2
- Question: Can the prompt basis F learned by Ego-VPA be transferred to other Ego-VFM models or even non-egocentric video understanding tasks?
- Basis in paper: [inferred] The paper proposes a novel prompt-tuning approach that learns a prompt basis F shared across modalities, which suggests that the learned basis could potentially be transferable
- Why unresolved: The paper only evaluates Ego-VPA on the LaViLa Ego-VFM model and does not explore its transferability to other models or tasks
- What evidence would resolve it: Experiments evaluating the transferability of the learned prompt basis F to other Ego-VFM models and non-egocentric video understanding tasks would provide insights into its generalizability

### Open Question 3
- Question: How does Ego-VPA perform when adapting Ego-VFMs to domains with limited training data?
- Basis in paper: [explicit] The paper conducted an ablation study on the amount of training data used, showing that Ego-VPA performs well even with limited data (10% of the full dataset)
- Why unresolved: While the paper demonstrates Ego-VPA's effectiveness with limited data, it does not explore the lower bounds of data requirements or compare its performance to other adaptation methods in extremely low-data regimes
- What evidence would resolve it: Experiments comparing Ego-VPA's performance to other adaptation methods in extremely low-data regimes (e.g., few-shot or zero-shot learning) would provide insights into its data efficiency

## Limitations

- Domain Generalization Gap: Evaluation limited to Charades-Ego and EGTEA datasets, with effectiveness on more diverse egocentric video domains untested
- Reconstruction Trade-offs: Sparse approximation may become bottleneck for highly complex or diverse egocentric scenes, forcing efficiency vs fidelity tradeoff
- Orthogonality Constraint Practicality: Computational overhead of maintaining orthogonality during training and potential limitations on expressiveness not discussed

## Confidence

**High Confidence**: The core mechanism of local sparse approximation using shared basis prompts is technically sound and experimental results demonstrate clear performance improvements over baseline prompt-tuning methods. Parameter efficiency claim (0.84% of VFM parameters) is well-supported by ablation studies.

**Medium Confidence**: Cross-modal prompt synthesis approach shows promise in ablation studies, but evidence for effectiveness is limited to performance comparisons rather than qualitative analysis of how visual and textual information actually transfer between modalities. Mechanism is plausible but benefit extent may vary by dataset semantic alignment.

**Low Confidence**: Generalization claims to other egocentric video domains and long-term stability of learned basis prompts are not empirically validated. Paper does not address potential catastrophic forgetting when adapting to multiple egocentric datasets sequentially, nor evaluate method's robustness to domain shifts within egocentric video space.

## Next Checks

1. **Domain Diversity Test**: Evaluate Ego-VPA on a third egocentric dataset with substantially different characteristics (e.g., EPIC-Kitchens-100 or large-v1) to assess generalization beyond the two datasets used in the paper. Compare performance degradation against full fine-tuning to quantify the trade-off between efficiency and adaptability.

2. **Reconstruction Fidelity Analysis**: Systematically vary the reconstruction basis size k and prompt basis size B while measuring both reconstruction error and downstream task performance. This would reveal whether the sparse approximation becomes a limiting factor for complex egocentric scenes and identify the optimal parameter-efficiency trade-off curve.

3. **Cross-modal Transfer Visualization**: Implement visualization techniques to analyze how basis prompts learned from one modality (e.g., text) influence prompt synthesis in the other modality (e.g., video). This could include t-SNE plots of basis prompts, attention weight analysis showing cross-modal influence, or qualitative examples of prompt synthesis outputs to validate the claimed cross-modal knowledge transfer mechanism.