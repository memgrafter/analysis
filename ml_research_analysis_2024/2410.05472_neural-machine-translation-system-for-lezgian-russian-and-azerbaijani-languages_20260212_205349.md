---
ver: rpa2
title: Neural machine translation system for Lezgian, Russian and Azerbaijani languages
arxiv_id: '2410.05472'
source_url: https://arxiv.org/abs/2410.05472
tags:
- translation
- language
- lezgian
- russian
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first neural machine translation (NMT)
  system for the Lezgian language, an endangered Northeast Caucasian language, along
  with Russian and Azerbaijani. The authors collected and aligned parallel corpora
  from religious texts (Bible, Quran) and a Lezgian-Azerbaijani encyclopedia, plus
  a large monolingual Lezgian corpus.
---

# Neural machine translation system for Lezgian, Russian and Azerbaijani languages

## Quick Facts
- arXiv ID: 2410.05472
- Source URL: https://arxiv.org/abs/2410.05472
- Reference count: 10
- Primary result: First NMT system for Lezgian language achieving BLEU scores up to 29.48

## Executive Summary
This paper presents the first neural machine translation system for the Lezgian language, an endangered Northeast Caucasian language, along with Russian and Azerbaijani. The authors collected and aligned parallel corpora from religious texts (Bible, Quran) and a Lezgian-Azerbaijani encyclopedia, plus a large monolingual Lezgian corpus. They developed a LaBSE-based sentence encoder and extended a pre-trained NLLB model with Lezgian vocabulary. Multiple experiments tested different training language pairs and data sources, achieving BLEU scores of 26.14 for Lezgian-Azerbaijani, 22.89 for Azerbaijani-Lezgian, 29.48 for Lezgian-Russian, and 24.25 for Russian-Lezgian.

## Method Summary
The authors developed a neural machine translation system for Lezgian, Russian, and Azerbaijani languages by collecting parallel corpora from religious texts (Bible, Quran) and a Lezgian-Azerbaijani encyclopedia. They created a LaBSE-based sentence encoder and extended a pre-trained NLLB model with Lezgian vocabulary. The training involved multiple language pair combinations and data sources, with experiments measuring translation quality using BLEU and ChrF++ metrics. They also evaluated Claude 3.5 Sonnet for zero-shot translation capabilities on Lezgian language tasks.

## Key Results
- Achieved BLEU scores of 26.14 for Lezgian-Azerbaijani translation
- Achieved BLEU scores of 29.48 for Lezgian-Russian translation
- Found that adding more language pairs did not improve translation quality
- Claude 3.5 Sonnet showed high Lezgian language proficiency but often refused translations due to safety guidelines

## Why This Works (Mechanism)
The system leverages pre-trained multilingual models (NLLB) extended with Lezgian vocabulary, allowing transfer learning from high-resource languages. The LaBSE-based sentence encoder provides semantic alignment between languages, while the parallel corpora from religious texts and encyclopedias provide structured training data. The monolingual corpus enables better language modeling through back-translation techniques, improving translation quality despite limited parallel data.

## Foundational Learning
- **BLEU score**: Measures translation quality by comparing n-gram overlap between machine and reference translations; needed to quantitatively evaluate model performance
- **LaBSE**: Language-agnostic BERT sentence embeddings that capture semantic meaning across languages; needed for cross-lingual sentence alignment
- **Back-translation**: Technique using target language monolingual data to create synthetic parallel data; needed to augment limited parallel corpora
- **Zero-shot translation**: Ability of models to translate between language pairs not explicitly trained; needed to evaluate model generalization capabilities
- **ChrF++ metric**: Character n-gram F-score that complements BLEU by being less sensitive to word order; needed for more robust evaluation

## Architecture Onboarding

**Component map:**
NLLB-base -> Extended vocabulary -> Parallel corpus training -> BLEU/ChrF++ evaluation -> Claude 3.5 Sonnet comparison

**Critical path:**
Data collection and alignment → Vocabulary extension → Model fine-tuning → Evaluation with BLEU/ChrF++ → Zero-shot testing with Claude

**Design tradeoffs:**
- Limited parallel data vs. model complexity: Chose NLLB base rather than larger models to prevent overfitting
- Religious text bias vs. data availability: Used available parallel corpora despite potential domain limitations
- Safety guidelines vs. evaluation completeness: Could not fully evaluate Claude's zero-shot capabilities due to refusal behavior

**Failure signatures:**
- BLEU scores below 20 indicate poor translation quality requiring additional data or architectural changes
- Claude 3.5 Sonnet safety refusals indicate potential issues with evaluating zero-shot capabilities
- Inconsistent translation quality across language pairs suggests data imbalance or architectural limitations

**First experiments:**
1. Train model on Bible corpus only to establish baseline performance
2. Add Quran corpus to measure impact of additional parallel data
3. Include monolingual corpus with back-translation to evaluate data augmentation benefits

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does incorporating monolingual Lezgian data during training consistently improve translation quality across different language pairs and model architectures?
- Basis in paper: [inferred] The authors collected 842K monolingual Lezgian sentences and developed a LaBSE-based sentence encoder, but the impact of monolingual data on translation quality is not explicitly evaluated in the experiments.
- Why unresolved: The experiments focus on the effects of different language pair combinations and data sources (Bible, Quran, Qusar encyclopedia) but do not isolate the contribution of monolingual data to the final translation quality.
- What evidence would resolve it: Comparative experiments training models with and without monolingual data on the same parallel corpora, measuring BLEU and ChrF++ scores for each setup.

### Open Question 2
- Question: How does the quality of zero-shot translation from Claude 3.5 Sonnet compare to fine-tuned NMT models when translating between Lezgian and Russian or Azerbaijani?
- Basis in paper: [explicit] The authors evaluated Claude 3.5 Sonnet for zero-shot translation on 100 test sentences from the Qusar dataset, finding it achieved high fluency but often refused to translate to Lezgian due to safety guidelines.
- Why unresolved: While the authors report BLEU and ChrF++ scores for Claude's translations from Lezgian to Russian/Azerbaijani, they do not compare these results to the performance of their fine-tuned NMT models on the same test set.
- What evidence would resolve it: Direct comparison of translation quality metrics (BLEU, ChrF++) between Claude 3.5 Sonnet and the NMT models on identical test sets for all language directions.

### Open Question 3
- Question: What is the impact of using contemporary LLMs for back-translation of monolingual data versus traditional statistical methods in improving low-resource language translation quality?
- Basis in paper: [explicit] The authors use offline back-translation with a pre-trained NLLB model to translate Qusar data from Azerbaijani to Russian, noting it provides a significant increase in translation quality.
- Why unresolved: The paper does not compare this approach to using contemporary LLMs for back-translation, which could potentially yield better quality translations and further improve the low-resource language model.
- What evidence would resolve it: Experiments comparing translation quality when using different back-translation methods (traditional vs. LLM-based) for the same monolingual data, measuring improvements in BLEU and ChrF++ scores.

## Limitations
- Corpus size remains relatively small with only 102k parallel sentences available for training
- Religious texts and encyclopedias as primary data sources may introduce domain-specific biases
- Finding that adding more language pairs did not improve translation quality suggests potential limitations in training methodology

## Confidence
- **High confidence**: The reported BLEU scores and their relative ranking across language pairs are credible, given the standardized evaluation methodology and clear reporting of results.
- **Medium confidence**: The conclusion that additional language pairs do not improve quality is based on limited experimental variation and could benefit from more extensive ablation studies.
- **Medium confidence**: The characterization of Claude 3.5 Sonnet's performance and safety-related refusal behavior is reasonable but based on a single evaluation without detailed statistical analysis of refusal rates across different prompts.

## Next Checks
1. Conduct ablation studies varying corpus size and domain composition to determine the impact of data quantity versus domain specificity on translation quality.
2. Test the NMT system on out-of-domain texts (news, social media, literature) to assess generalization beyond the religious and encyclopedic training data.
3. Perform human evaluation studies comparing model translations against reference translations to validate BLEU score reliability for this low-resource language pair.