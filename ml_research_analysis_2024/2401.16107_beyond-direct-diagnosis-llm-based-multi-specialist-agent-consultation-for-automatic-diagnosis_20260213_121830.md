---
ver: rpa2
title: 'Beyond Direct Diagnosis: LLM-based Multi-Specialist Agent Consultation for
  Automatic Diagnosis'
arxiv_id: '2401.16107'
source_url: https://arxiv.org/abs/2401.16107
tags:
- diagnosis
- symptoms
- medical
- automatic
- amsc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the task of automatic diagnosis in healthcare
  by modeling the process as a multi-specialist consultation, moving beyond direct
  diagnosis approaches. The authors propose the Agent-derived Multi-Specialist Consultation
  (AMSC) framework, which uses large language models (LLMs) as virtual general practitioners
  and specialized LLM-based agents as medical specialists, each equipped with domain-specific
  knowledge.
---

# Beyond Direct Diagnosis: LLM-based Multi-Specialist Agent Consultation for Automatic Diagnosis

## Quick Facts
- arXiv ID: 2401.16107
- Source URL: https://arxiv.org/abs/2401.16107
- Authors: Haochun Wang, Sendong Zhao, Zewen Qiang, Nuwa Xi, Bing Qin, Ting Liu
- Reference count: 6
- Primary result: Multi-specialist LLM consultation achieves superior diagnostic accuracy while requiring significantly less training time and parameters than fine-tuning approaches

## Executive Summary
This paper proposes the Agent-derived Multi-Specialist Consultation (AMSC) framework for automatic diagnosis, modeling the process as a collaborative multi-specialist consultation rather than direct diagnosis. The framework uses large language models as virtual general practitioners and specialized LLM-based agents as medical specialists, each equipped with domain-specific knowledge. These agents generate probability distributions over potential diseases, which are then adaptively fused using a self-attention-based Adaptive Probability Distribution Fusion (APDF) method. The approach demonstrates superior accuracy compared to existing baselines while significantly reducing training time and parameter requirements. Notably, the study finds that incorporating implicit symptoms does not necessarily improve diagnostic performance, challenging common assumptions in the field.

## Method Summary
The AMSC framework uses frozen open-source LLMs (Baichuan2-chat) as general practitioners, with specialized agent specialists equipped with domain-specific medical knowledge profiles from the National Institutes of Health. Each agent-specialist generates predictive distributions for possible diseases without parameter updates. The APDF method, which only requires training the fusion module, uses self-attention to adaptively combine these distributions. The framework processes patient data including explicit symptoms (directly reported by patients) and implicit symptoms (deduced from dialogues), though experiments show explicit symptoms alone suffice for high accuracy. The approach is evaluated on three real-world datasets: MuZhi-4 (4 pediatric diseases), MuZhi-10 (10 diseases), and Dxy (5 diseases from medical dialogue platforms).

## Key Results
- The AMSC framework achieves superior diagnostic accuracy compared to existing baselines
- Incorporating implicit symptoms does not necessarily improve diagnostic performance
- The approach requires significantly less parameter updating and training time, enhancing efficiency and practical utility
- The framework demonstrates effective handling of the multiple-choice question answering (MCQA) formulation for diagnosis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The AMSC framework reduces parameter updates by only training the adaptive fusion module, not the LLMs themselves.
- Mechanism: LLMs are frozen and serve as fixed specialist agents. Only the lightweight fusion network (APDF) is updated during training.
- Core assumption: The LLM representations are sufficiently discriminative for diagnostic classification without fine-tuning.
- Evidence anchors:
  - [abstract]: "our approach requires significantly less parameter updating and training time, enhancing efficiency and practical utility"
  - [section]: "the model only requires parameter training on the distribution fusion, it is also more efficient in terms of training costs"
- Break condition: If LLM representations are too general and fail to capture domain-specific disease knowledge, the frozen agents will underperform, necessitating fine-tuning.

### Mechanism 2
- Claim: Multi-specialist consultation improves diagnostic accuracy by aggregating diverse domain knowledge.
- Mechanism: Each agent-specialist is equipped with disease-specific knowledge, generating a probability distribution over diseases. The APDF layer adaptively fuses these distributions using self-attention.
- Core assumption: Different diseases have distinct symptom patterns that can be effectively modeled by separate specialist agents.
- Evidence anchors:
  - [abstract]: "Agent-derived Multi-Specialist Consultation (AMSC) framework to model the diagnosis process in the real world by adaptively fusing probability distributions of agents over potential diseases"
  - [section]: "agent-specialists generate predictive distributions for possible diseases or medical categories without parameter updating of LLMs"
- Break condition: If symptom overlap between diseases is too high, the agents' distributions may be highly correlated, reducing the benefit of fusion.

### Mechanism 3
- Claim: Explicit symptoms alone can achieve high diagnostic accuracy, making implicit symptom collection unnecessary.
- Mechanism: The framework relies only on patient-reported explicit symptoms, bypassing the costly implicit symptom inquiry phase.
- Core assumption: Explicit symptoms contain sufficient information for accurate diagnosis in many cases.
- Evidence anchors:
  - [abstract]: "incorporating implicit symptoms does not necessarily improve diagnostic performance, challenging common assumptions in the field"
  - [section]: "we scrutinize the influence of implicit symptoms on automated diagnosis, investigating the impact of various symptoms on the diagnostic outcomes"
- Break condition: If implicit symptoms contain unique diagnostic signals not present in explicit symptoms, excluding them will degrade performance.

## Foundational Learning

- Concept: Multiple-choice question answering (MCQA) formulation for diagnosis
  - Why needed here: Converts the diagnosis task into a format compatible with LLM token prediction
  - Quick check question: How does framing diagnosis as MCQA enable using LLMs without fine-tuning?

- Concept: Self-attention mechanism for adaptive fusion
  - Why needed here: Allows the model to learn which specialist agents are more reliable for specific inputs
  - Quick check question: What advantage does self-attention have over simple averaging for fusing specialist predictions?

- Concept: Domain-specific knowledge injection
  - Why needed here: Transforms general LLMs into specialist agents with disease-specific expertise
  - Quick check question: Why might adding disease-specific knowledge improve an LLM's diagnostic performance compared to using the base model alone?

## Architecture Onboarding

- Component map:
  Patient symptom input → Symptom-to-question template → Multiple specialist agents (LLM + disease knowledge) → Individual probability distributions → APDF fusion layer → Final diagnosis
  Knowledge profiles stored separately from LLMs
  APDF module: Linear projections + self-attention + output layer

- Critical path:
  Symptom input → Specialist agents → Fusion → Diagnosis
  Training path: Fusion layer only

- Design tradeoffs:
  - Freezing LLMs reduces training cost but limits adaptation
  - Using explicit symptoms only simplifies the pipeline but may miss diagnostic signals
  - One agent per disease scales linearly but increases fusion complexity

- Failure signatures:
  - Low accuracy on diseases with overlapping symptoms → Fusion layer can't disambiguate
  - Performance degradation with natural language inputs → Template-based MCQA fails on free text
  - Training instability in APDF → Self-attention weights not converging

- First 3 experiments:
  1. Baseline: Compare frozen LLM vs. fine-tuned LLM on same task to measure cost-accuracy tradeoff
  2. Ablation: Test fusion methods (mean, majority vote, linear classifier) against APDF
  3. Knowledge impact: Evaluate agents with correct vs. mismatched vs. irrelevant disease knowledge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating more nuanced symptom weighting or prioritization within the multi-specialist consultation framework lead to improved diagnostic accuracy compared to the current approach that treats all explicit symptoms equally?
- Basis in paper: [inferred] The paper found that including implicit symptoms did not significantly improve performance and noted that explicit symptoms are "primarily patient-reported and considered dominant." However, it did not explore whether different weighting schemes for explicit symptoms themselves might yield better results.
- Why unresolved: The current study focuses on the presence or absence of implicit symptoms rather than their relative weighting or prioritization compared to explicit symptoms, leaving open the question of optimal symptom integration strategies.
- What evidence would resolve it: Comparative experiments testing various symptom weighting schemes (e.g., based on symptom specificity, patient-reported severity, or clinical significance) against the baseline approach using uniform weighting of explicit symptoms.

### Open Question 2
- Question: How does the performance of the AMSC framework vary when applied to different medical specialties or disease categories beyond those tested in the current study?
- Basis in paper: [explicit] The authors note that their evaluation focused on pediatric, respiratory, digestive, and endocrine diseases, and they mention the framework's potential for generalization but do not empirically test this across diverse medical domains.
- Why unresolved: The study's limited scope to specific disease categories means the framework's generalizability to other medical specialties (e.g., neurology, oncology, or rare diseases) remains unverified.
- What evidence would resolve it: Systematic evaluation of the AMSC framework across diverse medical specialties using datasets from different clinical domains, with performance metrics compared to specialty-specific baselines.

### Open Question 3
- Question: What is the impact of integrating real-time patient feedback or clarification during the diagnostic process on the accuracy and reliability of the AMSC framework?
- Basis in paper: [inferred] The current framework processes static symptom descriptions without incorporating dynamic patient-practitioner interactions or real-time clarification, despite the importance of such interactions in clinical practice.
- Why unresolved: The study does not explore how the framework would handle iterative questioning or patient corrections, which are common in actual medical consultations and could affect diagnostic outcomes.
- What evidence would resolve it: Experimental comparison of the AMSC framework with and without mechanisms for incorporating real-time patient feedback, measuring changes in diagnostic accuracy and confidence scores across multiple iterations of patient-practitioner interaction.

## Limitations

- The medical knowledge profiles from NIH are not specified in detail, making it unclear how comprehensive or accurate they are for diagnostic purposes
- The approach requires manual creation of symptom-to-question templates, which could be a significant scalability bottleneck
- The finding that implicit symptoms don't improve performance is based on a specific set of datasets and may not generalize to other medical domains or patient populations

## Confidence

- **High confidence**: The core architectural approach of using frozen LLMs with domain-specific knowledge profiles is technically sound and the APDF fusion mechanism is well-specified. The claim about reduced training costs is directly verifiable from the described methodology.
- **Medium confidence**: The claim that explicit symptoms alone achieve high diagnostic accuracy is supported by the presented evidence but may not generalize beyond the evaluated datasets. The superiority over baseline methods is demonstrated but on a limited set of comparisons.
- **Low confidence**: The assertion that implicit symptoms do not improve performance challenges conventional medical practice and requires broader validation across diverse medical domains and patient populations.

## Next Checks

1. **Knowledge Profile Validation**: Test the AMSC framework with varying quality and completeness of medical knowledge profiles (correct, incorrect, incomplete) to determine the sensitivity of diagnostic accuracy to knowledge quality.

2. **Generalization Across Medical Domains**: Evaluate the framework on additional medical datasets covering different specialties (e.g., internal medicine, neurology) and disease categories to assess whether the explicit-symptoms-only finding holds across diverse medical contexts.

3. **Template Scalability Assessment**: Measure the time and expertise required to create symptom-to-question templates for a larger set of diseases (e.g., 50-100 diseases) to quantify the practical scalability limitations of the approach.