---
ver: rpa2
title: A generalized neural tangent kernel for surrogate gradient learning
arxiv_id: '2405.15539'
source_url: https://arxiv.org/abs/2405.15539
tags:
- function
- theorem
- holds
- activation
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical foundation for surrogate gradient
  learning (SGL) by generalizing the neural tangent kernel (NTK) framework. The key
  problem addressed is that gradient-based learning cannot be directly applied to
  networks with activation functions like the sign function due to ill-defined derivatives.
---

# A generalized neural tangent kernel for surrogate gradient learning

## Quick Facts
- arXiv ID: 2405.15539
- Source URL: https://arxiv.org/abs/2405.15539
- Authors: Luke Eilers; Raoul-Martin Memmesheimer; Sven Goedeke
- Reference count: 40
- Key outcome: The paper provides a theoretical foundation for surrogate gradient learning by generalizing the neural tangent kernel framework, showing that using surrogate derivatives prevents kernel divergence for activation functions with jumps like the sign function.

## Executive Summary
This paper addresses the fundamental challenge of applying gradient-based learning to neural networks with activation functions like the sign function, where true derivatives are ill-defined. The authors generalize the neural tangent kernel (NTK) framework to handle surrogate gradient learning (SGL) by introducing quasi-Jacobian matrices that use surrogate derivatives instead of true derivatives. They prove that this generalized NTK converges to a deterministic kernel in the infinite-width limit, both at initialization and during training, providing a theoretical foundation for understanding SGL dynamics.

## Method Summary
The authors develop a theoretical framework for surrogate gradient learning by extending the NTK to use quasi-Jacobian matrices that incorporate surrogate derivatives. They prove convergence of this generalized kernel to a deterministic limit in the infinite-width setting. The method involves approximating discontinuous activation functions with smooth functions (like error functions), computing the corresponding NTK, and showing that this converges to a well-defined limit as the approximation becomes exact. Numerical experiments compare the distribution of networks trained with SGL to predictions from the analytical generalized NTK on a simple regression task.

## Key Results
- Proves convergence of the surrogate gradient NTK (SG-NTK) to a deterministic kernel in the infinite-width limit at initialization and during training
- Shows that using surrogate derivatives prevents the NTK from diverging to infinity, which occurs when directly extending NTK to activation functions with jumps
- Demonstrates numerical agreement between SGL-trained networks and SG-NTK predictions for networks with sign activation functions
- Derives exact analytical expressions for the SG-NTK, identifying terms that emerge from SGL and prevent divergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Surrogate gradient learning (SGL) prevents divergence in the infinite-width limit by replacing the true derivative with a surrogate derivative in the NTK framework.
- Mechanism: In networks with activation functions like the sign function, the true Jacobian vanishes almost everywhere due to zero derivatives. This leads to a singular kernel in the NTK framework as width approaches infinity. SGL replaces the vanishing true derivative with a surrogate derivative in the quasi-Jacobian matrix, which prevents the kernel from diverging to infinity.
- Core assumption: The surrogate derivative provides meaningful gradient information that approximates the behavior of the true derivative at discontinuities.
- Evidence anchors:
  - [abstract] "we generalize the NTK to gradient descent with surrogate derivatives, i.e., SGL"
  - [section] "we generalize the NTK to gradient descent with surrogate derivatives, i.e., SGL"
  - [corpus] Weak - neighboring papers discuss NTK positivity and convergence but not specifically SGL mechanisms
- Break condition: If the surrogate derivative is poorly chosen (e.g., too small or inappropriate shape), the kernel may still diverge or fail to capture learning dynamics.

### Mechanism 2
- Claim: The generalized NTK with asymmetric kernels converges to a deterministic kernel in the infinite-width limit at initialization and during training.
- Mechanism: By defining quasi-Jacobian matrices that use surrogate derivatives instead of true derivatives, the authors create an asymmetric NTK framework. Theorem 2.4 proves that this generalized NTK converges to a deterministic kernel at initialization, and Theorem 2.5 extends this convergence to training time.
- Core assumption: The activation functions and surrogate derivatives satisfy the linear envelope property and are continuous except for finitely many jump points.
- Evidence anchors:
  - [abstract] "we prove convergence of this surrogate gradient NTK (SG-NTK) to a deterministic kernel in the infinite-width limit at initialization and during training"
  - [section] "Theorem 2.4 (Generalized version of Theorem 1 by Jacot et al. [2018])"
  - [corpus] Weak - neighboring papers discuss NTK convergence but not asymmetric generalizations
- Break condition: If the activation function or surrogate derivative doesn't satisfy the required continuity or linear envelope properties, convergence may fail.

### Mechanism 3
- Claim: The SG-NTK for sign activation functions with arbitrary surrogate derivatives converges to a finite limit as the activation function approaches the sign function.
- Mechanism: The authors approximate the sign function with scaled error functions erf_m and show that the corresponding SG-NTK converges to a well-defined limit Isign,˜σ as m approaches infinity. This limit kernel is non-singular and characterizes SGL dynamics.
- Core assumption: The surrogate derivative is bounded and Lipschitz continuous.
- Evidence anchors:
  - [abstract] "we derive exact analytical expressions in sections D.1, D.2 and E.2. In particular, we identify the terms that emerge from SGL and that prevent divergence"
  - [section] "we define Isign,˜σ := limm→∞ Ierf m,˜σ"
  - [corpus] Weak - neighboring papers don't discuss sign function approximations
- Break condition: If the surrogate derivative is unbounded or has pathological properties, the limit kernel may not exist or may be singular.

## Foundational Learning

- Concept: Gaussian processes as infinite-width network limits
  - Why needed here: The paper builds on the established result that wide neural networks converge to Gaussian processes, which forms the foundation for NTK theory
  - Quick check question: What is the covariance function Σ(L) for a network with depth L and activation function σ?

- Concept: Neural tangent kernel convergence
  - Why needed here: The NTK framework shows that in the infinite-width limit, the empirical NTK converges to a deterministic kernel that characterizes learning dynamics
  - Quick check question: What is the recursive formula for the analytic NTK Θ(L) in terms of the covariance function Σ(L)?

- Concept: Gradient flow vs. gradient descent
  - Why needed here: The paper analyzes continuous-time gradient flow, which is the theoretical foundation for understanding gradient descent in the infinite-width limit
  - Quick check question: How does the learning rule d/dt θ_t = -η ∇θL(f(X; θ_t); Y) relate to the NTK formulation?

## Architecture Onboarding

- Component map:
  Activation function σ (e.g., sign, erf_m) -> Surrogate derivative ˜σ -> Quasi-Jacobian matrix J(L),σ,˜σ -> Empirical generalized NTK ˆI(L) -> Analytic generalized NTK I(L)

- Critical path: σ → ˜σ → J(L),σ,˜σ → ˆI(L) → I(L)
  The surrogate derivative transforms the activation function into a learnable gradient, which flows through the quasi-Jacobian to form the empirical NTK, which converges to the analytic NTK.

- Design tradeoffs:
  - Activation function choice: Sign is biologically plausible but has zero derivatives; erf_m approximates it with smooth derivatives
  - Surrogate derivative choice: Should approximate true derivative behavior at discontinuities while being well-defined everywhere
  - Width scaling: The paper uses NTK parameterization with 1/√nl scaling; other scalings may change convergence properties

- Failure signatures:
  - NTK divergence: Indicates poor surrogate derivative choice or pathological activation function
  - Convergence failure: May indicate violation of linear envelope property or continuity assumptions
  - Poor empirical agreement: Suggests finite-width effects dominate or surrogate derivative is inadequate

- First 3 experiments:
  1. Implement SGL with sign activation and erf surrogate derivative on a simple regression task; compare to NTK predictions
  2. Vary the scale of the surrogate derivative; observe impact on kernel behavior and learning dynamics
  3. Test different surrogate derivatives (e.g., sigmoid, piecewise linear) on the same task; compare convergence and performance

## Open Questions the Paper Calls Out
The paper mentions the possibility of extending the SG-NTK to other activation functions with jumps but does not provide detailed analysis or numerical experiments for activation functions other than the sign function.

## Limitations
- The theoretical analysis is limited to specific activation functions (sign) and doesn't explore the full range of discontinuous activations
- Numerical experiments are restricted to simple regression tasks, leaving questions about generalization to more complex problems
- The paper doesn't provide systematic analysis of how network width and learning rate affect the agreement between SGL and SG-NTK predictions

## Confidence
- High Confidence: The mathematical framework for generalized NTK with surrogate derivatives is rigorously developed and the convergence proofs follow established NTK methodology. The mechanism preventing kernel divergence through surrogate derivatives is clearly demonstrated analytically.
- Medium Confidence: The numerical experiments showing agreement between SGL-trained networks and SG-NTK predictions are convincing for the specific case studied, but the limited scope prevents strong generalization claims.
- Low Confidence: The practical implications for choosing optimal surrogate derivatives and their impact on learning dynamics remain largely unexplored.

## Next Checks
1. Test the SG-NTK framework with other discontinuous activation functions (e.g., binary step, Heaviside) and different surrogate derivative choices to assess robustness and sensitivity.
2. Conduct experiments on classification tasks and higher-dimensional regression problems to evaluate the framework's applicability beyond the simple 2D case studied.
3. Analyze the effect of finite-width corrections by comparing SG-NTK predictions against SGL performance across a range of network widths to quantify when infinite-width approximations break down.