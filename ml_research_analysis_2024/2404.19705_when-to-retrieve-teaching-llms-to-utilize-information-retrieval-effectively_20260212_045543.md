---
ver: rpa2
title: 'When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively'
arxiv_id: '2404.19705'
source_url: https://arxiv.org/abs/2404.19705
tags:
- context
- question
- when
- questions
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of determining when large language
  models (LLMs) should utilize external information retrieval (IR) systems for question
  answering tasks. The authors propose a method called Adaptive Retrieval LLM (ADAPT-LLM)
  that trains LLMs to generate a special token, <RET, when they do not know the answer
  to a question, indicating the need for IR system usage.
---

# When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively

## Quick Facts
- arXiv ID: 2404.19705
- Source URL: https://arxiv.org/abs/2404.19705
- Reference count: 39
- One-line primary result: ADAPT-LLM achieves higher accuracy than baseline retrieval strategies in question answering tasks

## Executive Summary
This paper addresses the challenge of determining when large language models should utilize external information retrieval systems for question answering tasks. The authors propose ADAPT-LLM, a method that trains LLMs to generate a special <RET> token when they don't know an answer, indicating the need for retrieval. The model is fine-tuned on questions the base LLM cannot answer accurately, learning when to retrieve additional context versus relying on its parametric memory.

## Method Summary
ADAPT-LLM introduces a novel approach where LLMs are trained to decide when to use external information retrieval. The method involves identifying questions that the base LLM cannot answer accurately and annotating them with a special <RET> token. During fine-tuning, the model learns to generate this token when it needs additional context from a retrieval system. This creates a conditional retrieval mechanism where the LLM decides on a per-question basis whether to invoke the retrieval system or rely solely on its parametric knowledge.

## Key Results
- ADAPT-LLM achieves 36.77% accuracy on PopQA when trained on NQ, outperforming the always-retrieve baseline (35.86%)
- ADAPT-LLM achieves 38.15% accuracy on PopQA when trained on SQuAD, outperforming the always-retrieve baseline (36.59%)
- The method consistently outperforms three baseline configurations across evaluation datasets

## Why This Works (Mechanism)
ADAPT-LLM works by teaching the LLM to recognize its own knowledge limitations. When faced with questions beyond its parametric memory, the model learns to generate the <RET> token, triggering retrieval. This selective retrieval approach prevents unnecessary computational overhead while ensuring access to external information when truly needed. The fine-tuning process creates a decision boundary where the model can distinguish between questions it can answer from memory versus those requiring external context.

## Foundational Learning
- **Retrieval-augmented generation (RAG)**: Understanding how to combine parametric and external knowledge is crucial for modern question answering systems
- **Confidence estimation in LLMs**: The model must accurately assess when it lacks sufficient knowledge to answer
- **Fine-tuning strategies**: Specialized training approaches are needed to teach models new decision-making capabilities
- **Knowledge cutoff identification**: Recognizing when a question requires information beyond the model's training data
- **Token-based control mechanisms**: Using special tokens to trigger system behaviors
- **Question difficulty classification**: Distinguishing between answerable and unanswerable questions for the base model

## Architecture Onboarding

**Component Map:**
LLM base model -> Fine-tuning dataset (with <RET> annotations) -> ADAPT-LLM -> Question input -> Decision (Answer/RET) -> (If RET) Retrieval system -> Answer generation

**Critical Path:**
Question input → LLM decision (Answer vs. RET) → If RET, retrieval → Final answer generation

**Design Tradeoffs:**
The approach trades additional fine-tuning complexity for potentially significant efficiency gains by reducing unnecessary retrievals. The <RET> token mechanism adds a controlled decision point but requires careful annotation of training data.

**Failure Signatures:**
- Over-reliance on retrieval for questions answerable from parametric memory
- Under-utilization of retrieval when external context is genuinely needed
- Inconsistent decision-making across similar question types
- Potential label noise in training data affecting decision boundaries

**First Experiments:**
1. Evaluate baseline LLM performance on questions it cannot answer
2. Test <RET> token generation accuracy on a held-out validation set
3. Measure retrieval invocation frequency compared to always-retrieve baseline

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Evaluation is limited to a single dataset (PopQA), constraining generalizability
- Performance improvements are modest (1-2 percentage points), raising questions about practical significance
- The methodology for identifying "unanswerable" questions for training is not fully detailed
- Trade-off between retrieval costs and accuracy gains is not explicitly quantified

## Confidence
- **High**: The core methodology of training with <RET> tokens to indicate retrieval needs is clearly described and reproducible
- **Medium**: The reported performance improvements over baselines, while consistent, are modest and warrant cautious interpretation
- **Low**: Claims about the model's ability to effectively balance retrieval vs. parametric memory usage are supported by results but lack deeper analysis of failure cases

## Next Checks
1. Evaluate ADAPT-LLM across multiple diverse datasets (beyond PopQA) to assess robustness and generalizability
2. Conduct ablation studies removing the <RET> token mechanism to quantify its specific contribution to performance gains
3. Measure and report retrieval invocation frequency and associated computational costs to evaluate practical deployment trade-offs