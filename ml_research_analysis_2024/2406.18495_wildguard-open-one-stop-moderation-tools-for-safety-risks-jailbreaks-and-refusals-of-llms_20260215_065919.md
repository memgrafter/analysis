---
ver: rpa2
title: 'WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and
  Refusals of LLMs'
arxiv_id: '2406.18495'
source_url: https://arxiv.org/abs/2406.18495
tags:
- response
- guard
- prompts
- prompt
- wild
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WildGuard, a comprehensive open-source LLM
  safety moderation tool designed to identify harmful user prompts, detect unsafe
  model responses, and assess refusal rates. It addresses key limitations in existing
  tools, particularly their poor performance on adversarial jailbreak prompts and
  nuanced refusal detection.
---

# WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs

## Quick Facts
- arXiv ID: 2406.18495
- Source URL: https://arxiv.org/abs/2406.18495
- Reference count: 40
- Outperforms existing tools on adversarial prompts and refusal detection with up to 26.4% improvement

## Executive Summary
WildGuard is a comprehensive open-source LLM safety moderation tool that addresses critical gaps in existing solutions, particularly their poor performance on adversarial jailbreak prompts and nuanced refusal detection. The system detects harmful user prompts, unsafe model responses, and assesses refusal rates through a unified multi-task approach. Trained on WildGuardMix, a carefully balanced dataset of 92K examples spanning 13 risk categories and both vanilla and adversarial prompts, WildGuard achieves state-of-the-art performance while maintaining practical deployment efficiency.

## Method Summary
WildGuard is created through instruction-tuning Mistral-7B-v0.3 on WildGuardTrain, a dataset of 87K examples derived from WildGuardMix. The model uses a unified input-output format to handle three tasks simultaneously: prompt harmfulness detection, response harmfulness detection, and refusal detection. The training data combines synthetic generation, GPT-4 labeling for quality control, and careful balancing of vanilla (40%) and adversarial (40%) prompts alongside real-world examples. The model is evaluated on WildGuardTest (5,299 examples) and ten public benchmarks, demonstrating superior performance compared to ten strong baselines including Llama-Guard2 and Aegis-Guard.

## Key Results
- Achieves up to 26.4% improvement in refusal detection compared to existing tools
- Matches or exceeds GPT-4 performance on adversarial prompt harmfulness, surpassing by up to 3.9%
- Reduces jailbreak success rates from 79.8% to 2.4% when deployed as a moderation filter
- Outperforms ten baselines including Llama-Guard2 and Aegis-Guard across all three moderation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model achieves strong performance by balancing adversarial and vanilla prompts in training data.
- Mechanism: By constructing WildGuardMix with equal proportions of vanilla and adversarial prompts (40% each) alongside real-world and synthetic data, the model learns to generalize across both straightforward and sophisticated jailbreak attempts.
- Core assumption: Adversarial prompts are essential for training robust moderation models, and equal representation prevents bias toward either prompt type.
- Evidence anchors:
  - [abstract] "carefully balanced multi-task safety moderation dataset with 92K labeled examples that cover vanilla (direct) prompts and adversarial jailbreaks"
  - [section 3.1.3] "we sample from these sets the following numbers of prompts along with matched refusal and compliance responses: 6,062 vanilla harmful prompts, 2,931 vanilla benign prompts, 4,489 adversarial harmful prompts, and 4,339 adversarial benign prompts"

### Mechanism 2
- Claim: Multi-task training improves performance across all three moderation tasks simultaneously.
- Mechanism: Training on prompt harmfulness, response harmfulness, and refusal detection together creates shared representations that benefit each task through knowledge transfer.
- Core assumption: The three tasks share underlying patterns that can be learned jointly rather than in isolation.
- Evidence anchors:
  - [section 4.3] "we show that multi-task training improves the performance of WildGuard compared to single-task safeguards"
  - [section 3.3] "We design a unified input and output format to capture the three tasks"

### Mechanism 3
- Claim: The model matches or exceeds GPT-4 by using GPT-4-generated labels for training data.
- Mechanism: WildGuardTrain is constructed using GPT-4 to label synthetic responses and filter low-quality examples, creating high-quality training data that captures GPT-4's reasoning patterns.
- Core assumption: GPT-4's labeling decisions can be effectively learned by a smaller model through supervised training.
- Evidence anchors:
  - [section 3.1.3] "We further filter responses created through open LMs by assigning labels for each of our three target tasks using GPT-4"
  - [section 4.2] "WildGuard matches GPT-4 across tasks, and surpasses GPT-4 by up to 3.9% on adversarial prompt harmfulness"

## Foundational Learning

- Concept: Classification metrics (precision, recall, F1-score)
  - Why needed here: The paper evaluates model performance using F1 scores across multiple tasks and benchmarks, requiring understanding of how these metrics balance false positives and false negatives.
  - Quick check question: If a model has high precision but low recall on harmful prompt detection, what does this tell you about its behavior?

- Concept: Dataset balancing and sampling strategies
  - Why needed here: The construction of WildGuardMix involves careful balancing of different prompt types and harm categories to ensure robust performance across scenarios.
  - Quick check question: Why might equal representation of vanilla and adversarial prompts be important for training a moderation model?

- Concept: Instruction tuning methodology
  - Why needed here: WildGuard is created through instruction tuning of a base model (Mistral-7B) using a unified format for multiple tasks.
  - Quick check question: What advantage does instruction tuning offer over traditional fine-tuning for creating a multi-task moderation tool?

## Architecture Onboarding

- Component map: Synthetic prompt generation -> GPT-4 labeling and filtering -> Dataset balancing (vanilla/adversarial) -> Instruction tuning on Mistral-7B -> Unified multi-task head
- Critical path: Data preparation → Multi-task training → Evaluation → Deployment as moderation filter
- Design tradeoffs: Larger base models could improve performance but increase computational cost; more synthetic data improves coverage but may introduce distribution shift.
- Failure signatures: Poor performance on adversarial prompts indicates insufficient adversarial training data; high false positive rates suggest over-conservative filtering.
- First 3 experiments:
  1. Train baseline single-task models for each moderation task separately to establish performance ceiling.
  2. Vary the balance ratio of vanilla to adversarial prompts in training data to find optimal performance.
  3. Test different base models (Mistral, Llama2, Gemma) to identify best foundation for instruction tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of WildGuard change when tested on real-world user interactions beyond the LMSYS-CHAT-1M and WILD-CHAT datasets included in training?
- Basis in paper: Inferred from the use of IN-THE-WILD data sources for training, which suggests real-world user interactions were included but the paper does not report comprehensive testing on additional real-world datasets.
- Why unresolved: The paper demonstrates strong performance on synthetic and benchmark datasets, but real-world deployment may encounter different distributions of prompts and responses not fully captured in the training data.
- What evidence would resolve it: Testing WildGuard on a diverse set of real-world user interaction datasets not seen during training, comparing performance metrics across different domains (e.g., customer service, educational, creative writing) to identify potential domain-specific weaknesses.

### Open Question 2
- Question: What is the impact of WildGuard's performance when the base model is fine-tuned for specific downstream tasks beyond the general safety moderation demonstrated?
- Basis in paper: Explicit mention of experimenting with different base models (e.g., Llama2-7B, Llama3-8B, Gemma-7B) showing similar performance, suggesting potential for task-specific optimization.
- Why unresolved: The paper focuses on WildGuard as a general-purpose safety tool, but its effectiveness in specialized domains or when integrated with task-specific LLMs remains unexplored.
- What evidence would resolve it: Fine-tuning WildGuard on task-specific safety datasets (e.g., healthcare, finance) and evaluating its performance on both general and domain-specific safety benchmarks, comparing results to the general WildGuard model.

### Open Question 3
- Question: How does WildGuard's performance degrade under adversarial attacks specifically designed to evade its detection mechanisms?
- Basis in paper: Inferred from the emphasis on adversarial prompt detection and the significant reduction in jailbreak success rates, but the paper does not detail specific adversarial attack strategies against WildGuard itself.
- Why unresolved: While WildGuard shows robustness against known jailbreak tactics, new attack vectors or more sophisticated evasion techniques could potentially bypass its safeguards.
- What evidence would resolve it: Conducting targeted adversarial attacks against WildGuard, such as using gradient-based methods or prompt engineering to generate inputs that maximize its false negative rate, and measuring the resulting performance degradation compared to baseline models.

## Limitations

- Dataset composition uncertainty regarding the exact distribution and quality control mechanisms for synthetic data generation
- Generalization concerns about effectiveness against truly novel adversarial attacks not represented in WildGuardMix
- Computational trade-offs when scaling to larger models or different base architectures may not yield proportional improvements

## Confidence

- **High confidence**: Claims about WildGuard's performance relative to existing tools on established benchmarks with extensive comparative analysis against ten baselines
- **Medium confidence**: Claims about WildGuard's effectiveness as a moderation filter in real-world deployment scenarios, though specific implementation details and attack methodologies are not fully detailed
- **Medium confidence**: Claims about the optimal balance of vanilla and adversarial prompts in training data, though sensitivity to different ratios and generalizability to other safety tasks is not explored

## Next Checks

1. Generate and test WildGuard against novel adversarial prompt techniques not present in the WildGuardMix dataset to assess true robustness beyond benchmark performance.

2. Train and evaluate single-task versions of WildGuard to quantify the actual performance gains from multi-task learning and identify potential interference between tasks.

3. Systematically test WildGuard's refusal behavior across different demographic groups and content types to identify potential over-refusal patterns or discriminatory outcomes in safety moderation.