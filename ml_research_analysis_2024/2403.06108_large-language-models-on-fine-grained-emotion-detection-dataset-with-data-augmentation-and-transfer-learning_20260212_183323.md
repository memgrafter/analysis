---
ver: rpa2
title: Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation
  and Transfer Learning
arxiv_id: '2403.06108'
source_url: https://arxiv.org/abs/2403.06108
tags:
- dataset
- data
- goemotions
- emotion
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores improving fine-grained emotion detection on
  the GoEmotions dataset, a large manually annotated dataset with 58k Reddit comments
  labeled for 27 emotions and neutral. The authors focus on addressing the challenge
  of detecting subtle emotions in text, which is a complex issue in NLP with significant
  practical applications.
---

# Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning

## Quick Facts
- arXiv ID: 2403.06108
- Source URL: https://arxiv.org/abs/2403.06108
- Reference count: 6
- Primary result: Achieved macro-F1 score of 0.51 on GoEmotions taxonomy, slightly improved over original paper's 0.46

## Executive Summary
This paper explores improving fine-grained emotion detection on the GoEmotions dataset, which contains 58k Reddit comments labeled for 27 emotions plus neutral. The authors focus on addressing the challenge of detecting subtle emotions in text through fine-tuning transformer models, data augmentation, and transfer learning from the CARER dataset. The study presents experiments with BERT and RoBERTa models, finding that transfer learning and data augmentation can improve classification performance, particularly for underrepresented emotion categories.

## Method Summary
The study employs a multi-faceted approach to improve emotion detection on GoEmotions. The authors fine-tune BERT and RoBERTa models on the dataset, apply data augmentation techniques (DDA, BERT embeddings, ProtAugment) to minority classes, and implement transfer learning from the CARER dataset. The primary evaluation metric is macro-F1 score, with additional metrics including precision, recall, and accuracy. The methodology involves training on the GoEmotions training set and evaluating on the test set, with experiments designed to isolate the effects of different techniques on classification performance.

## Key Results
- Achieved macro-F1 score of 0.51 on the original GoEmotions taxonomy
- Data augmentation and transfer learning improved performance on underrepresented emotion categories
- CARER transfer learning provided a statistically significant 0.9% F1 score improvement over vanilla BERT
- BERT baseline outperformed RoBERTa baseline in macro-averaged F1 score despite RoBERTa's architectural advantages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning from CARER dataset improves GoEmotions classification performance
- Mechanism: Pre-training on a related emotion dataset provides domain-specific knowledge that helps the model better recognize emotion patterns in GoEmotions
- Core assumption: CARER and GoEmotions share underlying emotional semantics despite different taxonomies
- Evidence anchors: [abstract] "Transfer learning on a dataset with similar domain and different taxonomy could further improve the classification performance on GoEmotions dataset"; [section 4.6.3] "CARER's incorporation demonstrably improves performance. As evidenced by the 2nd and 3rd rows of the table, CARER-BERT achieves a statistically significant 0.9% F1 score improvement over vanilla BERT"
- Break condition: If the emotional taxonomies between CARER and GoEmotions are too different, transfer learning may not provide benefits or could even harm performance

### Mechanism 2
- Claim: Data augmentation on minority emotion classes improves classification performance
- Mechanism: Generating synthetic examples for underrepresented emotion categories provides more balanced training data, reducing bias toward majority classes
- Core assumption: Synthetic examples created through augmentation preserve the semantic meaning of the original minority class examples
- Evidence anchors: [abstract] "data augmentation and transfer learning can further improve the classification performance, particularly for underrepresented emotion categories"; [section 4.5] "all three augmentation methods on minority emotion categories can improve the BERT classification performance"
- Break condition: If augmentation introduces too much noise or creates examples that don't represent the true minority class distribution, performance could degrade

### Mechanism 3
- Claim: Fine-tuning RoBERTa on GoEmotions outperforms BERT despite initial hypothesis failure
- Mechanism: RoBERTa's training optimizations (dynamic masking, larger batch size, etc.) make it more effective at capturing nuanced emotional expressions in text
- Core assumption: The GoEmotions task benefits from RoBERTa's architectural improvements despite the dataset being relatively small
- Evidence anchors: [section 4.4] "RoBERTa optimizes the set of pre-training tasks, the pre-training configuration, and the pre-training dataset (Liu et al., 2019), it is expected to outperform the performance of the BERT baseline"; [section 4.4] "BERT baseline outperforms the RoBERTa baseline in terms of the macro-averaged F1 score after being trained with the same configuration"
- Break condition: If the GoEmotions dataset has characteristics that specifically align with BERT's pre-training, RoBERTa may not show superior performance

## Foundational Learning

- Concept: Fine-grained emotion classification
  - Why needed here: The GoEmotions dataset contains 27 distinct emotion categories plus neutral, requiring the model to distinguish subtle emotional differences
  - Quick check question: What is the difference between detecting "annoyance" versus "anger" in text?

- Concept: Multi-label classification
  - Why needed here: Each text example can have multiple emotion labels simultaneously, requiring the model to predict multiple classes per input
  - Quick check question: How does the model handle cases where a single sentence expresses both "joy" and "surprise"?

- Concept: Class imbalance handling
  - Why needed here: The dataset has severe imbalance with some categories having 10x more examples than others, requiring special techniques
  - Quick check question: What techniques can be used to address the 10:1 ratio between majority and minority emotion classes?

## Architecture Onboarding

- Component map: Data preprocessing pipeline -> Transformer-based model (BERT/RoBERTa) with classification head -> Data augmentation module (DDA, BERT embeddings, ProtAugment) -> Transfer learning component (CARER pre-training) -> Evaluation metrics (macro-F1, precision, recall)

- Critical path: 1. Load and preprocess GoEmotions dataset; 2. Apply data augmentation to minority classes; 3. Fine-tune transformer model on augmented data; 4. Evaluate performance and iterate

- Design tradeoffs: Augmentation complexity vs. performance gain; Transfer learning benefits vs. additional training time; Model size vs. inference efficiency

- Failure signatures: Performance degradation on minority classes despite augmentation; Overfitting on augmented data; Transfer learning not improving performance

- First 3 experiments: 1. Fine-tune BERT on original GoEmotions dataset to establish baseline; 2. Apply data augmentation to minority classes and retrain; 3. Implement transfer learning from CARER dataset and compare results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of modern LLMs like GPT-4 and Llama compare to fine-tuned models like BERT and RoBERTa on the GoEmotions dataset under zero-shot settings?
- Basis in paper: [explicit] The paper states that this is the first study to evaluate the capabilities of LLMs on the GoEmotions dataset and found that GPT-4's performance was significantly lower than fine-tuned models in terms of accuracy, precision, recall, and F1-score
- Why unresolved: The study only tested GPT-4 and did not include Llama due to its inability to correctly identify the 28 emotion categories. More comprehensive testing of different LLMs is needed to draw definitive conclusions
- What evidence would resolve it: Conducting a broader evaluation of various modern LLMs on the GoEmotions dataset under zero-shot settings, comparing their performance to fine-tuned models using the same metrics and test data

### Open Question 2
- Question: What are the specific reasons for the lower performance of GPT-4 on the GoEmotions dataset, and how can these issues be addressed?
- Basis in paper: [explicit] The paper identifies three main issues: hallucination (GPT-4 creating emotion labels not in the 28 categories), over-labelling (assigning multiple labels to sentences where fewer are typical), and over-interpretation (overly complex interpretation of simple sentences)
- Why unresolved: The study only identified the issues but did not explore solutions or further investigate the root causes of these problems
- What evidence would resolve it: Conducting in-depth analysis of GPT-4's outputs to understand the patterns in its errors, experimenting with different prompting strategies or fine-tuning approaches to mitigate these issues, and comparing the results to the baseline performance of fine-tuned models

### Open Question 3
- Question: How does the performance of emotion detection models vary across different datasets with diverse taxonomies and domains, and what factors contribute to these variations?
- Basis in paper: [explicit] The paper mentions that the original GoEmotions paper conducted transfer learning experiments on nine datasets with different domains and taxonomies, showing that GoEmotions can serve as a robust baseline for understanding emotions even with limited labeled data in different domains
- Why unresolved: The study only replicated the transfer learning experiments on one dataset (ISEAR) and did not explore the variations in performance across other datasets or investigate the factors contributing to these differences
- What evidence would resolve it: Conducting a comprehensive analysis of emotion detection models' performance on a wide range of datasets with diverse taxonomies and domains, investigating the factors that influence these variations (e.g., dataset size, label distribution, domain specificity), and identifying strategies to improve model performance across different datasets

## Limitations
- Lack of hyperparameter optimization and ablation studies to isolate individual technique contributions
- Limited evaluation to a single dataset without testing generalizability to other emotion detection benchmarks
- Unclear specific implementation details for ProtAugment and exact data subset used for LLM evaluation

## Confidence
**High Confidence:** The baseline results and general methodology for fine-tuning BERT/RoBERTa on GoEmotions are well-established and reproducible

**Medium Confidence:** The reported improvements from data augmentation and transfer learning are promising but not fully validated

**Low Confidence:** The specific implementation details for ProtAugment and the exact subset of data used for LLM evaluation are unclear, making faithful reproduction difficult

## Next Checks
1. Conduct statistical significance testing across multiple runs to verify that reported improvements are statistically significant and not due to random variation

2. Perform ablation studies by systematically removing each component (data augmentation, transfer learning, different augmentation methods) to quantify their individual contributions

3. Evaluate the best-performing model configuration on additional emotion detection datasets to assess generalizability beyond the GoEmotions dataset