---
ver: rpa2
title: A Tiny Supervised ODL Core with Auto Data Pruning for Human Activity Recognition
arxiv_id: '2408.01283'
source_url: https://arxiv.org/abs/2408.01283
tags:
- data
- human
- u1d441
- training
- power
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a tiny supervised on-device learning (ODL)
  core with automatic data pruning to address data drift in human activity recognition
  for resource-limited edge devices. The proposed system combines an ODL algorithm
  based on online sequential extreme learning machine (OS-ELM) with a novel label
  acquisition approach that queries a nearby teacher device only when needed.
---

# A Tiny Supervised ODL Core with Auto Data Pruning for Human Activity Recognition

## Quick Facts
- arXiv ID: 2408.01283
- Source URL: https://arxiv.org/abs/2408.01283
- Authors: Hiroki Matsutani; Radu Marculescu
- Reference count: 12
- Primary result: 55.7% communication reduction with 0.9% accuracy loss using auto data pruning for on-device learning

## Executive Summary
This paper addresses the challenge of data drift in human activity recognition for resource-limited edge devices by introducing a tiny supervised on-device learning (ODL) core with automatic data pruning. The proposed system combines an OS-ELM (Online Sequential Extreme Learning Machine) algorithm with a novel label acquisition approach that queries a nearby teacher device only when needed. To reduce redundant wireless communication, the paper proposes an automatic data pruning mechanism that skips label queries when prediction confidence is high, with the pruning threshold auto-tuned during runtime. The ODL core is implemented using a 45nm CMOS process, achieving only 3.39mW power consumption and 136.39kB memory size.

## Method Summary
The system uses OS-ELM for on-device learning, which employs random weights between input and hidden layers that can be replaced with a pseudorandom number generator to reduce memory requirements. The automatic data pruning mechanism calculates prediction confidence as the difference between the top two predicted class probabilities (P1 - P2). When this difference exceeds a threshold, the system skips querying the teacher device and uses its own prediction instead. The pruning threshold is auto-tuned during runtime by starting with a high value and gradually adjusting based on whether local predictions match teacher predictions. The system is implemented in 45nm CMOS with BLE communication to the teacher device.

## Key Results
- 55.7% reduction in communication volume compared to baseline with only 0.9% accuracy loss
- Up to 49.4% power consumption reduction achieved through automatic data pruning
- Successful recovery of accuracy after data drift compared to non-ODL approaches
- Memory size of 136.39kB and power consumption of 3.39mW in 45nm CMOS implementation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The automatic data pruning reduces communication volume and power consumption by skipping label queries when prediction confidence is high.
- Mechanism: The system calculates the difference between the top two predicted class probabilities (P1 - P2) as a confidence metric. If this difference exceeds a threshold (Ψ), the system skips querying the teacher device and uses its own prediction instead, reducing wireless communication overhead.
- Core assumption: High confidence predictions from the edge device are accurate enough to avoid teacher queries without significant accuracy loss.
- Evidence anchors:
  - [abstract] "Experiments using a human activity recognition dataset show that the proposed automatic data pruning reduces communication volume by 55.7% and power consumption by up to 49.4% with only 0.9% accuracy loss"
  - [section] "we propose a new approach to reduce the redundant wireless communication of the supervised ODL between the teacher and edge devices"
- Break condition: The pruning threshold becomes too aggressive, causing the system to skip queries when predictions are actually incorrect, leading to accuracy degradation beyond acceptable limits.

### Mechanism 2
- Claim: The auto-tuning of the pruning threshold (Ψ) maintains optimal performance across varying data distributions without manual intervention.
- Mechanism: The system starts with a high threshold value and gradually decreases it when consecutive predictions match the teacher's labels. If mismatches occur, the threshold increases. This adaptive approach finds the sweet spot between communication savings and accuracy.
- Core assumption: The data distribution doesn't change rapidly during the auto-tuning phase, allowing the system to converge on an appropriate threshold.
- Evidence anchors:
  - [section] "To properly choose Ψ at runtime, 1. Ψ is set to a high value at the startup time; 2. Ψ is then gradually decreased if the following condition is satisfied: P1 - P2 > Ψ or if the locally-predicted label matches the teacher's predicted label; 3. Ψ is increased if the locally-predicted label ≠ teacher's predicted label"
  - [abstract] "The data pruning threshold is automatically tuned, eliminating a manual threshold tuning"
- Break condition: Rapid data drift occurs during the auto-tuning phase, preventing the system from finding an appropriate threshold before the distribution changes again.

### Mechanism 3
- Claim: Using OS-ELM (Online Sequential Extreme Learning Machine) enables efficient on-device learning with minimal memory requirements.
- Mechanism: OS-ELM uses random weights between input and hidden layers that can be replaced with a pseudorandom number generator, significantly reducing memory storage requirements while maintaining learning capability.
- Core assumption: The random initialization of weights in OS-ELM is sufficient for the learning task, and replacing stored weights with a pseudorandom generator doesn't significantly impact performance.
- Evidence anchors:
  - [section] "To reduce the memory size of /u1D736 , the weights can be replaced with a pseudorandom number generator as in [8]"
  - [section] "We show that the required memory size for the core is smaller than the same-shaped multilayer perceptron (MLP)"
  - [section] "ODLHash achieves almost the same accuracies as ODLBase"
- Break condition: The pseudorandom number generator produces correlations or patterns that reduce the diversity of weight initializations, limiting the model's ability to learn complex patterns.

## Foundational Learning

- Concept: Data drift in machine learning
  - Why needed here: The system is specifically designed to address the challenge where input data distribution changes over time, which is common in human activity recognition across different subjects
  - Quick check question: What are the different types of data drift mentioned in the paper, and how might each affect a human activity recognition system?

- Concept: Online learning algorithms
  - Why needed here: The system uses OS-ELM, which allows model updates with each new data sample without requiring full retraining, essential for resource-constrained edge devices
  - Quick check question: How does online learning differ from batch learning, and why is it particularly suited for edge devices with limited memory?

- Concept: Confidence-based decision making
  - Why needed here: The pruning mechanism relies on calculating prediction confidence (P1 - P2) to decide whether to query the teacher device, requiring understanding of probability outputs from classification models
  - Quick check question: How is prediction confidence typically calculated in multi-class classification, and what are the limitations of using confidence thresholds for decision making?

## Architecture Onboarding

- Component map:
  Sensor data collection → Edge device processing → Prediction engine (OS-ELM) with random weights or pseudorandom generator → Confidence calculation module (P1 - P2 comparison) → Communication module (BLE) for teacher queries → Auto-tuning logic for threshold adjustment → Sequential training unit for model updates

- Critical path:
  Data sensing → Prediction → Confidence calculation → Teacher query decision → Communication (if needed) → Model update (if training)

- Design tradeoffs:
  - Memory vs. Accuracy: Smaller hidden layer sizes reduce memory but may impact accuracy
  - Communication frequency vs. Power consumption: More frequent teacher queries improve accuracy but increase power usage
  - Threshold tuning aggressiveness vs. Convergence time: Faster tuning may overshoot optimal values, while conservative tuning takes longer to adapt

- Failure signatures:
  - Accuracy degradation over time despite pruning (threshold too aggressive)
  - No communication savings despite high confidence predictions (threshold too conservative)
  - System crashes or hangs during training (memory allocation issues)
  - Inconsistent predictions across similar inputs (pseudorandom generator issues)

- First 3 experiments:
  1. Test accuracy with different hidden layer sizes (32, 128, 256 nodes) while measuring memory usage to find optimal balance
  2. Measure communication volume and accuracy with fixed thresholds (0.01, 0.08, 0.16, 0.32, 0.64) to establish baseline pruning performance
  3. Run auto-tuning with different initial thresholds and adjustment rates to find configuration that minimizes accuracy loss while maximizing communication reduction

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the abstract or conclusions. However, based on the experimental results and discussion, several areas for future work are implied:

## Limitations
- The effectiveness of pseudorandom weight generation in OS-ELM is assumed to match stored weights but may introduce performance variations not captured in experiments
- The auto-tuning algorithm's convergence behavior under rapid data drift conditions is not fully characterized
- BLE communication overhead and protocol specifics between teacher and edge devices are abstracted, potentially underestimating real-world energy costs

## Confidence
- High confidence: 49.4% power reduction with 0.9% accuracy loss (directly measured in experiments)
- Medium confidence: Auto-tuning algorithm effectiveness across diverse data distributions (validated only on HAR dataset)
- Low confidence: Long-term stability of pseudorandom weight generator performance (theoretical assumption based on [8])

## Next Checks
1. Test the OS-ELM model with actual stored weights versus pseudorandom generator weights across multiple runs to verify the claim of "almost same accuracies"
2. Evaluate the auto-tuning algorithm's performance under controlled data drift scenarios with varying drift rates to identify convergence failure modes
3. Measure real BLE communication energy consumption including connection establishment, message overhead, and retry mechanisms rather than assuming constant power per transmission