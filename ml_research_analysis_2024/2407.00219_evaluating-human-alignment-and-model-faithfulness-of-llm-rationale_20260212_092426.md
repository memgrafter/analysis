---
ver: rpa2
title: Evaluating Human Alignment and Model Faithfulness of LLM Rationale
arxiv_id: '2407.00219'
source_url: https://arxiv.org/abs/2407.00219
tags:
- faithfulness
- methods
- label
- rationales
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically evaluates LLM rationales through human\
  \ alignment and model faithfulness, comparing prompting-based methods (self-explanations\
  \ via prompts) with technical attribution-based methods (attention/gradient-based\
  \ token identification). Experiments on three classification datasets with human-annotated\
  \ rationales show that attribution-based methods align better with human reasoning,\
  \ especially after fine-tuning, and also demonstrate higher faithfulness by more\
  \ accurately reflecting the model\u2019s decision-making process."
---

# Evaluating Human Alignment and Model Faithfulness of LLM Rationale

## Quick Facts
- arXiv ID: 2407.00219
- Source URL: https://arxiv.org/abs/2407.00219
- Authors: Mohsen Fayyaz; Fan Yin; Jiao Sun; Nanyun Peng
- Reference count: 37
- This paper systematically evaluates LLM rationales through human alignment and model faithfulness, comparing prompting-based methods (self-explanations via prompts) with technical attribution-based methods (attention/gradient-based token identification).

## Executive Summary
This paper systematically evaluates LLM rationales through human alignment and model faithfulness, comparing prompting-based methods (self-explanations via prompts) with technical attribution-based methods (attention/gradient-based token identification). Experiments on three classification datasets with human-annotated rationales show that attribution-based methods align better with human reasoning, especially after fine-tuning, and also demonstrate higher faithfulness by more accurately reflecting the model's decision-making process. Notably, low pre-training accuracy and label bias in certain datasets undermine the reliability of faithfulness evaluations, but fine-tuning mitigates this issue and improves both alignment and faithfulness. These findings highlight the limitations of prompting-based rationales and emphasize the need for more robust interpretability methods.

## Method Summary
The study evaluates LLM rationales using two distinct approaches: prompting-based methods that generate self-explanations through carefully crafted prompts, and attribution-based methods that identify important tokens using attention weights, gradients, or saliency scores. The evaluation framework measures human alignment by comparing generated rationales with human-annotated ones using F1 score, and model faithfulness by measuring prediction flip rates after masking identified rationales. Experiments are conducted across three classification datasets (e-SNLI, FEVER, MedicalBios) using both pre-trained and fine-tuned versions of Llama2, Llama3, Mistral, and GPT models. Fine-tuning is performed using LoRA for 10 epochs to assess its impact on alignment and faithfulness metrics.

## Key Results
- Attribution-based methods align better with human reasoning than prompting-based methods, especially after fine-tuning
- Fine-tuning significantly improves both human alignment and model faithfulness by addressing label bias issues
- Low pre-training accuracy and label bias in certain datasets undermine faithfulness evaluation reliability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning LLMs for classification tasks improves both human alignment and model faithfulness of rationales.
- **Mechanism:** Fine-tuning addresses the issue of label bias and collapsing predictions in pre-trained LLMs. This allows the model to learn more nuanced decision boundaries and attend to relevant features, leading to rationales that are both more aligned with human reasoning and more faithful to the model's actual decision-making process.
- **Core assumption:** Low classification accuracy in pre-trained LLMs is primarily due to label bias and collapsing predictions, not inherent limitations in the model's ability to learn from the task.
- **Evidence anchors:**
  - [abstract] "Notably, low pre-training accuracy and label bias in certain datasets undermine the reliability of faithfulness evaluations, but fine-tuning mitigates this issue and improves both alignment and faithfulness."
  - [section 4.4.1] "This issue arises from pre-trained models being heavily biased toward specific labels in poorly performing datasets... Consequently, even masking the entire input does not change their biased predictions, resulting in extremely low faithfulness flip rates."
- **Break condition:** If fine-tuning does not significantly improve classification accuracy, or if the model continues to exhibit label bias after fine-tuning, the mechanism would break down.

### Mechanism 2
- **Claim:** Attribution-based methods (attention/gradient-based) produce rationales that are more aligned with human reasoning and more faithful to the model's decision-making process compared to prompting-based methods.
- **Mechanism:** Attribution methods directly leverage the model's internal processes (attention weights, gradients) to identify important input features. This provides a more direct and reliable reflection of the model's decision-making process compared to prompting, which relies on the model's ability to generate explanations without direct access to its internal state.
- **Core assumption:** The model's internal processes (attention, gradients) are a more reliable source of information about its decision-making process than its ability to generate explanations through prompting.
- **Evidence anchors:**
  - [abstract] "attribution-based methods align better with human reasoning, especially after fine-tuning, and also demonstrate higher faithfulness by more accurately reflecting the modelâ€™s decision-making process."
  - [section 4.2] "attribution-based methods tend to align more closely with human reasoning than prompting-based methods, especially after fine-tuning."
- **Break condition:** If the model's internal processes are not reliable indicators of its decision-making process, or if the attribution methods are not accurately capturing these processes, the mechanism would break down.

### Mechanism 3
- **Claim:** The choice of evaluation metric (perturbation-based) significantly impacts the observed faithfulness of LLM rationales.
- **Mechanism:** Perturbation-based metrics assume that removing critical features based on rationales would significantly affect model performance. However, if the model is heavily biased or exhibits collapsing predictions, masking any number of words may not impact its decision, leading to artificially low faithfulness scores. This issue is particularly relevant for LLMs pre-trained on language modeling tasks, which may not be well-suited for classification tasks without fine-tuning.
- **Core assumption:** Perturbation-based metrics are a valid measure of faithfulness, but their interpretation requires careful consideration of the model's performance and potential biases.
- **Evidence anchors:**
  - [abstract] "To evaluate faithfulness, unlike prior studies that excluded misclassified examples, we evaluate all instances and also examine the impact of fine-tuning and accuracy on alignment and faithfulness."
  - [section 4.4.1] "A noteworthy finding is that in the e-SNLI and FEVER datasets, where classification accuracy was notably low (Figure 2), both attribution-based and prompting-based methods result in a very small (Less than 5%) flip rate in pre-trained models."
- **Break condition:** If perturbation-based metrics are not a valid measure of faithfulness, or if alternative metrics are developed that are not affected by the issues described above, the mechanism would break down.

## Foundational Learning

- **Concept:** Understanding the difference between human alignment and model faithfulness in the context of LLM rationales.
  - **Why needed here:** The paper explicitly distinguishes between these two properties and evaluates them separately. Understanding the difference is crucial for interpreting the results and drawing meaningful conclusions.
  - **Quick check question:** Can you explain the difference between human alignment and model faithfulness in your own words? Provide an example to illustrate the distinction.

- **Concept:** Familiarity with attribution-based methods for interpreting LLM decisions (e.g., attention weights, gradients).
  - **Why needed here:** The paper compares attribution-based methods with prompting-based methods for extracting rationales. Understanding how these methods work is essential for evaluating their strengths and weaknesses.
  - **Quick check question:** Describe how attention weights or gradients can be used to identify important input features for a model's decision. What are the potential limitations of these methods?

- **Concept:** Understanding the challenges of evaluating LLM rationales, particularly the issues of label bias and collapsing predictions in pre-trained models.
  - **Why needed here:** The paper highlights these challenges and demonstrates how they can impact the evaluation of faithfulness. Understanding these issues is crucial for interpreting the results and drawing appropriate conclusions.
  - **Quick check question:** Explain how label bias and collapsing predictions can lead to artificially low faithfulness scores in perturbation-based evaluations. How does fine-tuning address these issues?

## Architecture Onboarding

- **Component map:** Datasets (e-SNLI, FEVER, MedicalBios) -> Models (Llama2, Llama3, Mistral, GPT-3.5-Turbo, GPT-4-Turbo) -> Methods (Prompting-based, Attribution-based) -> Evaluation metrics (Human alignment, Model faithfulness)

- **Critical path:**
  1. Extract rationales using prompting and attribution methods
  2. Evaluate human alignment by comparing generated rationales with human-annotated rationales (F1 score)
  3. Evaluate model faithfulness by masking important words and measuring flip rate of predicted labels
  4. Fine-tune models and repeat steps 1-3 to assess the impact of fine-tuning on alignment and faithfulness

- **Design tradeoffs:**
  - Prompting-based methods are more flexible and can generate free-form explanations, but they may not always be faithful to the model's decision-making process
  - Attribution-based methods are more direct and reliable, but they may be less interpretable for non-technical users
  - Fine-tuning improves both alignment and faithfulness, but it requires additional computational resources and may introduce new biases

- **Failure signatures:**
  - Low human alignment: Generated rationales do not match human-annotated rationales
  - Low model faithfulness: Masking important words does not significantly impact the model's predictions
  - Label bias: The model consistently predicts the same label regardless of the input
  - Collapsing predictions: The model predicts the same label for all inputs

- **First 3 experiments:**
  1. Evaluate human alignment and model faithfulness of prompting-based and attribution-based methods on a small subset of the e-SNLI dataset using pre-trained models
  2. Fine-tune the models on the e-SNLI dataset and repeat the evaluation to assess the impact of fine-tuning
  3. Compare the results across different datasets (e-SNLI, FEVER, MedicalBios) to identify any task-specific patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Small test sets (300 samples for e-SNLI and FEVER, 100 for MedicalBios) may not provide stable estimates of alignment and faithfulness metrics
- Perturbation-based faithfulness metrics can produce artificially low scores when models exhibit label bias or collapsing predictions
- Limited scope to classification tasks only, without examining generalization to other NLP tasks like generation or summarization

## Confidence
- **High Confidence:** Fine-tuning improves both human alignment and model faithfulness (consistent results across multiple datasets and models)
- **Medium Confidence:** Attribution-based methods outperform prompting-based methods in alignment and faithfulness (results may depend on specific implementations)
- **Low Confidence:** Generalizability of findings to other NLP tasks, model architectures, or larger datasets (limited experimental scope)

## Next Checks
1. Perform statistical significance testing on alignment and faithfulness metrics across multiple runs to establish whether observed differences between methods are meaningful rather than due to sampling variability

2. Implement and compare alternative faithfulness evaluation methods such as causal interventions, ablation studies with different masking strategies, or faithfulness metrics that account for model uncertainty and confidence scores

3. Extend the evaluation framework to additional NLP tasks (e.g., summarization, question answering) and model sizes to assess whether the observed patterns in alignment and faithfulness hold across different problem domains and model capacities