---
ver: rpa2
title: 'ClarQ-LLM: A Benchmark for Models Clarifying and Requesting Information in
  Task-Oriented Dialog'
arxiv_id: '2409.06097'
source_url: https://arxiv.org/abs/2409.06097
tags:
- provider
- seeker
- task
- dialogue
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ClarQ-LLM is a benchmark designed to evaluate LLMs' ability to
  ask clarification questions in task-oriented dialogues. It includes 31 task types
  with 10 unique dialogue scenarios each, featuring an information seeker and provider
  agent.
---

# ClarQ-LLM: A Benchmark for Models Clarifying and Requesting Information in Task-Oriented Dialog

## Quick Facts
- arXiv ID: 2409.06097
- Source URL: https://arxiv.org/abs/2409.06097
- Authors: Yujian Gan; Changling Li; Jinxia Xie; Luou Wen; Matthew Purver; Massimo Poesio
- Reference count: 40
- Primary result: Llama3.1 405B seeker agent achieved only 60.05% success rate, demonstrating benchmark's challenging nature

## Executive Summary
ClarQ-LLM is a benchmark designed to evaluate large language models' ability to ask clarification questions in task-oriented dialogues. The benchmark includes 31 task types with 10 unique dialogue scenarios each, requiring seeker agents to resolve uncertainties through dialogue to complete tasks. The benchmark uses a provider conversational agent to replicate human provider behavior and employs automated evaluation based on GPT-4o to measure task success. Tests with state-of-the-art models like Llama3.1 405B showed significant limitations, achieving only 60.05% success rate.

## Method Summary
ClarQ-LLM consists of 310 task-oriented dialogue scenarios where seeker agents interact with a provider agent to gather required information. The provider agent uses a tree structure to organize information delivery, with seekers asking clarification questions to resolve uncertainties. The benchmark evaluates performance using success rate, average query discrepancy, and average query length metrics. Experiments tested various seeker models including GPT-3.5, GPT-4o, Llama2-70B-Chat, Llama3.1-405B-Instruct, and ERNIE 4.0 across different modes (Chat vs Completion) and provider configurations (single-response vs multi-info provider).

## Key Results
- Llama3.1 405B-Instruct achieved only 60.05% success rate, demonstrating the benchmark's challenging nature
- Chat mode produces more natural, concise language but has lower success rates compared to Completion mode
- Multi-info provider modification improved success rates and reduced dialogue turns by combining multiple responses into single replies

## Why This Works (Mechanism)

### Mechanism 1
The provider agent's tree structure ensures consistent and logically ordered information delivery. The provider uses a pre-annotated tree structure that dictates the sequence of information delivery, with each node representing a piece of information and child nodes representing follow-up information based on seeker queries. This structure ensures logically ordered and comprehensive responses.

### Mechanism 2
Multi-response generation allows seekers to ask multiple questions simultaneously. The multi-info provider agent can combine multiple human provider responses into a single reply when seekers ask compound questions, preventing the limitation of only answering one question at a time from affecting seeker performance.

### Mechanism 3
Automated evaluation based on GPT-4o accurately measures task success. The evaluation system checks if all required information from human provider responses appears in the provider agent's Response I, and uses GPT-4o to compare against missing required information to determine if the seeker obtained everything needed.

## Foundational Learning

- **Task-oriented dialogue systems and their evaluation challenges**: Understanding why traditional metrics like BLEU fail for this benchmark and why success rate is more appropriate
  - Quick check: Why does ClarQ-LLM reject traditional evaluation metrics like BLEU and ROUGE?

- **Clarification question types and their role in information gathering**: The benchmark specifically tests seekers' ability to ask clarification questions to resolve uncertainties
  - Quick check: What distinguishes clarification questions from other types of questions in task-oriented dialogue?

- **Tree data structures and their application in dialogue management**: The provider agent uses tree structures to organize and deliver information logically
  - Quick check: How does a tree structure help ensure logical sequencing of information in dialogue?

## Architecture Onboarding

- **Component map**: Seeker agents (LLMs) -> Provider agent (GPT-4o or Llama3.1-405B) -> Task database (31 task types, 310 scenarios) -> Evaluation system -> Multi-info provider extension

- **Critical path**: 
  1. Seeker initiates conversation with provider
  2. Provider delivers initial information from tree node 1
  3. Seeker analyzes information and asks clarification questions
  4. Provider responds with appropriate Response I or generates Response II
  5. Dialogue continues until seeker gathers all required information or reaches turn limit
  6. Evaluation system automatically determines success based on information gathered

- **Design tradeoffs**:
  - Chat mode vs Completion mode: Chat mode produces more natural, concise language but has lower success rates; Completion mode has higher success rates but longer, more verbose responses
  - Single-response vs Multi-info provider: Single-response is simpler but limits seeker questioning style; multi-info is more flexible but may produce complex responses
  - Tree-based vs free-form provider responses: Tree-based ensures consistency but may not handle unexpected questions as naturally

- **Failure signatures**:
  - Seeker forgetting previously identified uncertainties despite them being in prompt
  - Seeker asking excessive clarification questions (high AQD)
  - Seeker producing very long responses (high AQL)
  - Provider agent misunderstanding seeker requests and giving incorrect Response I
  - Automated evaluation incorrectly marking success/failure

- **First 3 experiments**:
  1. Test basic functionality: Run GPT-4 seeker in Completion mode against GPT-4o provider on 10 simple tasks, verify success rate calculation
  2. Test multi-info provider: Run same seeker with multi-info provider, compare success rates and dialogue turns to baseline
  3. Test provider error handling: Deliberately ask unexpected questions to provider, verify it generates appropriate Response II and doesn't hallucinate

## Open Questions the Paper Calls Out

### Open Question 1
How does the ClarQ-LLM benchmark ensure that seeker agents do not rely on guessing to complete tasks, and what measures are in place to prevent such behavior? The paper mentions that ClarQ-LLM requires seekers to ask clarification questions to resolve uncertainties rather than making assumptions, but does not detail specific mechanisms or constraints implemented to prevent guessing behavior.

### Open Question 2
What are the limitations of using LLMs like GPT-4o and Llama3.1-405B as provider agents, and how do these limitations affect the evaluation of seeker agents? The paper acknowledges that provider agents can make errors which could potentially affect the fairness of evaluations, but does not provide a comprehensive analysis of how these limitations impact the overall reliability of the benchmark.

### Open Question 3
How does the performance of seeker agents in ClarQ-LLM compare to human performance in similar task-oriented dialogue scenarios, and what insights can be drawn from this comparison? While the paper provides success rate comparisons (human 85% vs best LLM 60.05%), it does not delve into qualitative differences in performance, such as the types of errors made by LLM seekers versus humans or the strategies employed by each.

## Limitations
- The automated evaluation using GPT-4o shows inherent uncertainty, with one evaluation result differing from manual review, suggesting potential reliability issues
- The provider agent's tree structure may artificially constrain the natural flow of conversation and create unrealistic scenarios
- The benchmark's focus on seeker behavior may not fully capture real-world scenarios where both agents need clarification capabilities

## Confidence
- **High confidence**: The benchmark successfully demonstrates that even state-of-the-art LLMs (Llama3.1 405B achieving only 60.05% success rate) struggle with clarification tasks, validating the benchmark's challenging nature
- **Medium confidence**: The automated evaluation method using GPT-4o is reliable enough for benchmark purposes, though with acknowledged limitations in edge cases
- **Low confidence**: The tree structure perfectly captures all logical conversation paths and that combining multiple responses into single replies doesn't negatively impact seeker performance

## Next Checks
1. Run a small-scale manual review of 50 randomly selected dialogue outcomes to quantify the accuracy rate of the GPT-4o automated evaluation compared to human judgment, establishing the true reliability of the success rate metric.

2. Test the benchmark with a modified provider agent that uses free-form responses rather than tree structures to assess whether the structured approach artificially constrains seeker performance or creates unrealistic conversation patterns.

3. Evaluate seeker agents using only Response II scenarios (where provider agent doesn't know the answer) to determine if the benchmark's focus on structured tree responses underestimates seeker capabilities in more open-ended clarification situations.