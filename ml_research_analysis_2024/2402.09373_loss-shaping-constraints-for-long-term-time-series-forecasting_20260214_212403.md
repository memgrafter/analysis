---
ver: rpa2
title: Loss Shaping Constraints for Long-Term Time Series Forecasting
arxiv_id: '2402.09373'
source_url: https://arxiv.org/abs/2402.09373
tags:
- loss
- forecasting
- constraints
- prediction
- window
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of uneven error distributions
  in multi-step time series forecasting, particularly in transformer architectures,
  where errors can vary significantly across prediction steps. The authors propose
  a constrained learning approach that enforces user-defined upper bounds on losses
  at each time step, aiming to find the best model in terms of average performance
  while shaping the loss distribution.
---

# Loss Shaping Constraints for Long-Term Time Series Forecasting

## Quick Facts
- arXiv ID: 2402.09373
- Source URL: https://arxiv.org/abs/2402.09373
- Reference count: 40
- Key outcome: Constrained learning approach reduces constraint violation and standard deviation of errors across prediction steps while maintaining competitive average performance in transformer-based time series forecasting

## Executive Summary
This work addresses the challenge of uneven error distributions in multi-step time series forecasting, particularly in transformer architectures, where errors can vary significantly across prediction steps. The authors propose a constrained learning approach that enforces user-defined upper bounds on losses at each time step, aiming to find the best model in terms of average performance while shaping the loss distribution. They formulate this as a resilient constrained learning problem with dynamically adapted constraints and provide approximation guarantees despite non-convexity. The proposed Primal-Dual algorithm is evaluated on popular time series benchmarks using state-of-the-art transformer models. Results show that the approach consistently reduces constraint violation and standard deviation of errors across the prediction window, often improving or maintaining competitive average performance compared to standard ERM, effectively altering the loss landscape to achieve more balanced error distributions.

## Method Summary
The method formulates time series forecasting as a constrained optimization problem where the model must respect user-defined upper bounds on losses at each prediction step while optimizing average performance. It leverages a resilient approach with learned relaxation to adaptively find constraint levels that balance error shaping with performance. The Primal-Dual algorithm alternates between minimizing the empirical Lagrangian with respect to model parameters and slack variables, and maximizing with respect to dual variables. Constraint levels are set to the median of ERM validation losses per time step. The approach is evaluated on transformer architectures (Reformer, Autoformer, Informer, and vanilla transformer) using datasets with varying context and prediction window lengths.

## Key Results
- Consistently reduces constraint violation on test data across different transformer-based architectures, prediction lengths, and datasets
- Reduces standard deviation of errors across the prediction window considerably in many cases
- Achieves competitive or improved average MSE compared to standard ERM while shaping the loss distribution

## Why This Works (Mechanism)

### Mechanism 1
The constrained learning formulation reduces maximum step-wise errors by explicitly bounding the loss at each time step, shaping the error distribution across the prediction window. By enforcing upper bounds on losses at each time step through constrained optimization, the method prevents any single time step from incurring excessively large errors, effectively controlling the spread of errors across the prediction window while still optimizing average performance. The core assumption is that constraint levels can be set at reasonable values (not overly restrictive) so that the problem remains feasible while still shaping the loss distribution. Evidence shows the approach consistently reduces constraint violation across different architectures and datasets. Break condition: if constraint levels are set too tightly relative to achievable performance, the problem becomes infeasible.

### Mechanism 2
The resilient approach with learned relaxation allows the method to adaptively find constraint levels that balance error shaping with average performance, improving feasibility and generalization. By introducing slack variables with a penalty cost, the method learns optimal constraint relaxations during training. This allows it to automatically adjust constraint tightness based on the model's capacity and data distribution, finding a sweet spot where constraints are effective but not overly restrictive. The core assumption is that the perturbation cost function h(ζ) can effectively balance the marginal cost of relaxation against the marginal impact on optimal cost. Evidence shows the resilient approach reduces standard deviation across the prediction window in many cases. Break condition: if the perturbation cost function is poorly chosen, the learned relaxations may not effectively balance constraint satisfaction with performance.

### Mechanism 3
The primal-dual algorithm with alternating updates enables practical optimization of the non-convex constrained problem, converging to good solutions despite non-convexity. The alternating minimization with respect to primal variables (θ, ζ) and maximization with respect to dual variables (λ) creates a saddle point optimization that can handle the non-convexity of the hypothesis class while still finding approximately feasible solutions. The core assumption is that recent duality results ensure a bounded duality gap despite non-convexity, allowing the primal-dual method to converge to solutions that are close to optimal. Evidence shows the algorithm effectively affects the distribution of losses across the window as intended. Break condition: if the duality gap is too large or learning rates are poorly tuned, the algorithm may converge slowly or to poor local optima.

## Foundational Learning

- Concept: Constrained optimization and duality theory
  - Why needed here: The method relies on formulating time series forecasting as a constrained optimization problem and leveraging duality results to handle non-convexity
  - Quick check question: Can you explain why we need the assumption of bounded duality gap for the primal-dual algorithm to work?

- Concept: Statistical learning theory and generalization bounds
  - Why needed here: Understanding how the constraint relaxation affects approximation guarantees and generalization is crucial for setting appropriate constraint levels
  - Quick check question: How does the choice of constraint level ϵ affect the approximation bounds in Theorems 1 and 2?

- Concept: Transformer architectures for time series forecasting
  - Why needed here: The method is evaluated on transformer models, so understanding their inductive biases and limitations helps interpret when the approach succeeds or fails
  - Quick check question: Why might the vanilla transformer show poor generalization and weaker correlations between train and test errors compared to specialized architectures?

## Architecture Onboarding

- Component map: Data preprocessing -> rolling window generation -> transformer model -> per-step MSE calculation -> constraint evaluation -> primal-dual update -> repeat
- Critical path: data → model forward pass → loss calculation → constraint evaluation → primal-dual update → repeat
- Design tradeoffs: tighter constraints give better error shaping but may hurt average performance; looser constraints may not effectively shape errors
- Failure signatures: training instability (if constraints too tight), poor constraint satisfaction (if constraints too loose), lack of generalization (if model capacity insufficient)
- First 3 experiments:
  1. Implement the baseline ERM training to establish per-step loss patterns and choose initial constraint levels
  2. Add constant constraints at the median of ERM validation losses and verify constraint satisfaction improves
  3. Implement the resilient approach with learned slack variables and compare constraint satisfaction and MSE to the constant constraint baseline

## Open Questions the Paper Calls Out

### Open Question 1
How do constraint levels chosen via grid search over quantiles of training/validation errors affect the trade-off between MSE reduction and constraint violation across different model architectures and datasets? The authors perform a grid search over six constraint levels based on 25th, 50th, and 75th percentiles of training and validation errors from ERM models, selecting the optimal value for each combination. What remains unresolved is how different quantile-based constraint levels systematically affect this trade-off. Evidence that would resolve this includes a comprehensive analysis comparing performance metrics across all six constraint levels for each model-dataset-prediction length combination.

### Open Question 2
Under what conditions does the resilient approach with slack variables provide better generalization than the constrained approach without slacks, and why? The authors observe that the resilient approach reduces standard deviation across the prediction window in many cases, but both approaches perform similarly in terms of constraint violation when measured against fixed constraint levels. What remains unresolved is why learned slack variables lead to improved generalization and which specific patterns in data or model behavior correlate with this improvement. Evidence that would resolve this includes identifying which types of datasets, architectures, or prediction lengths consistently benefit from the resilient approach, along with theoretical justification.

### Open Question 3
What is the relationship between the generalization gap (difference between train and test error shapes) and the effectiveness of loss shaping constraints in modifying the test error distribution? The authors find varying Spearman correlations between train and test errors across different models, with Informer and vanilla transformer showing significantly weaker correlations. What remains unresolved is establishing a quantitative relationship between the magnitude of the generalization gap and the success rate of the loss shaping approach in achieving the desired test error distribution. Evidence that would resolve this includes empirical data showing how the correlation between train and test error shapes correlates with the success rate of the constrained approach.

## Limitations
- Exact preprocessing steps for datasets (normalization, handling missing values) are not fully specified
- Implementation details of transformer architectures (specific layer configurations, attention mechanisms) may differ from original implementations
- Approximation guarantees assume certain conditions on constraint levels and perturbation costs that may not hold in practice

## Confidence
- **High confidence** in the core constrained learning framework and primal-dual optimization approach
- **Medium confidence** in specific implementation details and hyperparameter choices
- **Medium confidence** in empirical results, as they show consistent improvements but are evaluated on limited datasets and architectures

## Next Checks
1. Reproduce the baseline ERM results: Implement transformer architectures and ERM training to establish baseline per-step loss patterns and verify they match reported patterns
2. Validate constraint satisfaction: Implement constrained learning approach with constant constraint levels and verify that Mean Constraint Violation (MCV) is consistently reduced compared to ERM
3. Test sensitivity to constraint levels: Systematically vary constraint levels (both constant and resilient) to determine the range over which the approach maintains feasibility while improving error distribution