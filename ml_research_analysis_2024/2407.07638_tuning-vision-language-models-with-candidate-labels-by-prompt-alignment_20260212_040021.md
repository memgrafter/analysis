---
ver: rpa2
title: Tuning Vision-Language Models with Candidate Labels by Prompt Alignment
arxiv_id: '2407.07638'
source_url: https://arxiv.org/abs/2407.07638
tags:
- prompt
- learning
- labels
- candidate
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first study on tuning vision-language
  models (VLMs) with candidate labels through prompt learning. While prompt learning
  combined with partial-label learning (PLL) objectives can learn from candidate labels,
  performance degrades significantly as label ambiguity increases.
---

# Tuning Vision-Language Models with Candidate Labels by Prompt Alignment

## Quick Facts
- arXiv ID: 2407.07638
- Source URL: https://arxiv.org/abs/2407.07638
- Reference count: 40
- Key outcome: Framework improves VLM performance on candidate labels, achieving 9.7% accuracy gain on OxfordPets with 50% label ambiguity

## Executive Summary
This paper addresses the challenge of tuning vision-language models (VLMs) when training data contains candidate labels instead of true labels. The authors propose a novel framework that enhances the robustness of prompt learning by aligning model outputs with dynamically mixed class posteriors from both handcrafted and learnable prompts. Extensive experiments across 10 benchmark datasets demonstrate consistent improvements over vanilla prompt learning approaches, particularly under high label ambiguity conditions. The framework shows particular effectiveness when combined with various partial-label learning objectives, offering a practical solution for scenarios where clean annotations are expensive to obtain.

## Method Summary
The method introduces a framework for tuning VLMs with candidate labels through prompt learning combined with partial-label learning (PLL) objectives. It employs a dual-prompt approach: handcrafted prompts provide stable zero-shot predictions while learnable prompts adapt to the data. The framework dynamically mixes these predictions using a balancing factor that evolves during training, then aligns the mixed posterior with model outputs through a weighted cross-entropy loss. The approach is model-agnostic and can be combined with various PLL training objectives including PRODEN, CC, LW, PiCO, PLLCR, and CAVL. Training updates only the learnable prompt parameters while keeping the pre-trained VLM frozen.

## Key Results
- Outperforms vanilla prompt learning by 9.7% accuracy on OxfordPets with 50% label ambiguity
- Consistently improves performance across 10 benchmark datasets and 6 PLL objectives
- Shows robustness to increasing label ambiguity, with diminishing performance gaps compared to clean-label baselines
- Effective across diverse VLM tuning methods including linear probing and full fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt learning combined with partial-label learning (PLL) objectives can learn from candidate labels.
- Mechanism: By treating candidate labels as a soft supervision signal, the model can optimize parameters to maximize the likelihood of the true label being present in the candidate set.
- Core assumption: The true label is guaranteed to be included in the candidate set for each training example.
- Evidence anchors:
  - [abstract]: "We empirically demonstrate that prompt learning combined with the prevailing PLL training objectives in a vanilla way can learn from candidate labels."
  - [section 3.1]: Shows that vanilla prompt learning outperforms zero-shot inference on UCF101 and Caltech101 datasets when using PLL objectives.
  - [corpus]: Weak evidence; no direct citations found supporting the general claim that PLL works with VLMs.
- Break condition: If the true label is not guaranteed to be in the candidate set, the optimization becomes ill-posed.

### Mechanism 2
- Claim: The fixed class token in prompt learning introduces prior knowledge that improves robustness to label ambiguity.
- Mechanism: The class token acts as a regularizer, preventing overfitting to false-positive labels in the candidate set and maintaining zero-shot generalization ability.
- Core assumption: The pre-trained class token embeddings contain meaningful semantic information about the classes.
- Evidence anchors:
  - [section 3.1]: "We conjecture that it is the fixed class token that introduces the prior knowledge, enhancing the model's robustness."
  - [section 3.1]: Explains that the prior knowledge "acts as a regularizer, keeping the model from over-fitting to the false-positive labels."
  - [corpus]: No direct evidence found; this is an inference based on observed performance differences.
- Break condition: If the class token is randomly initialized or not semantically meaningful, the regularization effect disappears.

### Mechanism 3
- Claim: Aligning model output with a dynamically mixed class posterior from both handcrafted and learnable prompts improves robustness.
- Mechanism: The handcrafted prompt provides stable, zero-shot predictions while the learnable prompt adapts to the data; mixing them balances adaptation with generalization.
- Core assumption: The handcrafted prompt's predictions are more stable across different levels of label ambiguity than the learnable prompt's predictions.
- Evidence anchors:
  - [section 3.2]: "it dynamically mixes the class posteriors predicted by both the handcrafted and learnable prompt, followed by aligning the mixed class posterior with the model output."
  - [section 4.3]: Dynamic mixing strategy adjusts balancing factor based on training progress and dataset characteristics.
  - [corpus]: No direct evidence found; this is a novel contribution not supported by citations.
- Break condition: If the handcrafted prompt is poorly constructed, mixing with it could degrade rather than improve performance.

## Foundational Learning

- Concept: Partial-label learning (PLL) strategies
  - Why needed here: The paper deals with candidate labels where the true label is included but not identified, which is a PLL scenario.
  - Quick check question: What are the two main strategies for PLL and how do they differ in handling false-positive labels?

- Concept: Prompt learning in vision-language models
  - Why needed here: The paper proposes using prompt learning instead of full fine-tuning to adapt VLMs to downstream tasks with candidate labels.
  - Quick check question: How does prompt learning differ from linear probe in terms of parameter updates and regularization?

- Concept: Vision-language model architectures (dual-encoder)
  - Why needed here: Understanding how CLIP and similar models align visual and textual embeddings is crucial for grasping the prompt learning approach.
  - Quick check question: What is the role of the text encoder in CLIP during inference, and how do handcrafted prompts influence its output?

## Architecture Onboarding

- Component map:
  Pre-trained CLIP model (frozen) -> Learnable prompt tokens (16 context vectors) -> Handcrafted prompt tokens (text embeddings for class names) -> Text encoder (shared between handcrafted and learnable prompts) -> PLL training objective module (e.g., PRODEN, PiCO) -> Prompt alignment module (mixes posteriors, applies weighted cross-entropy)

- Critical path:
  1. Input image → image encoder → visual embedding
  2. Handcrafted prompt + class tokens → text encoder → handcrafted posteriors
  3. Learnable prompt + class tokens → text encoder → learnable posteriors
  4. Mix posteriors using dynamic α(t)
  5. Apply PLL loss + alignment loss
  6. Update only learnable prompt parameters

- Design tradeoffs:
  - Fixed vs. learnable class tokens: Fixed tokens provide regularization but limit adaptation.
  - Dynamic vs. static mixing: Dynamic mixing adapts to training progress but adds hyperparameter complexity.
  - Number of learnable context vectors: More vectors allow richer adaptation but increase risk of overfitting.

- Failure signatures:
  - Performance degrades with high label ambiguity → insufficient regularization
  - Model overfits to candidate labels → learnable prompt dominates mixing too early
  - Alignment loss dominates training → handcrafted prompt is poorly constructed

- First 3 experiments:
  1. Verify that vanilla prompt learning + PLL objective outperforms zero-shot on a small dataset with low label ambiguity.
  2. Test performance degradation as label ambiguity increases to confirm robustness issues.
  3. Implement and validate the prompt alignment framework on a medium-sized dataset, comparing against vanilla prompt learning across multiple PLL objectives.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed framework's performance scale with increasing numbers of candidate labels per instance?
- Basis in paper: [explicit] The paper mentions varying label ambiguity but does not specifically explore how performance changes with increasing number of candidate labels beyond the standard definition of label ambiguity.
- Why unresolved: The experiments focus on uniform flipping of labels with varying probabilities but do not explore scenarios with a fixed ambiguity level but increasing candidate set size.
- What evidence would resolve it: Experiments varying the number of candidate labels while keeping the true label fixed in the set would show whether performance scales linearly or if there's a threshold effect.

### Open Question 2
- Question: What is the impact of different handcrafted prompt qualities on the framework's performance across diverse downstream tasks?
- Basis in paper: [explicit] The ablation study shows performance varies with different handcrafted prompts, but it only tests a few examples and doesn't explore the relationship between prompt quality and task specificity.
- Why unresolved: The study uses a limited set of prompts and doesn't investigate whether task-specific prompts consistently outperform generic ones across all datasets.
- What evidence would resolve it: Systematic testing with a wide range of prompt qualities across multiple task domains would reveal if there's a generalizable relationship between prompt quality and downstream performance.

### Open Question 3
- Question: How does the framework's performance compare to other robust training methods (e.g., MixUp, CutMix) when applied to partial-label learning scenarios?
- Basis in paper: [inferred] The paper focuses on comparing against baseline PLL methods and prompt learning variants, but doesn't explore data augmentation techniques that could enhance robustness.
- Why unresolved: While the framework shows robustness, it's unclear if standard data augmentation methods could achieve similar or better results when combined with PLL objectives.
- What evidence would resolve it: Head-to-head comparisons between the proposed framework and robust training methods (with and without data augmentation) would determine if the framework offers unique advantages or if simpler methods suffice.

## Limitations
- Limited to image classification tasks, leaving open questions about applicability to other VLM tasks like visual question answering
- Handcrafted prompts require manual construction that may not scale to very large label spaces
- Label ambiguity simulation (uniform flipping) may not reflect real-world candidate label distributions where false positives often cluster around semantically similar classes

## Confidence
- **High Confidence (4/5):** The core claim that prompt learning with PLL objectives can learn from candidate labels is well-supported by empirical results across 10 datasets.
- **Medium Confidence (3/5):** The mechanism by which fixed class tokens provide regularization is plausible but relies on indirect evidence.
- **Medium Confidence (3/5):** The dynamic mixing strategy's effectiveness is demonstrated empirically but the theoretical justification for the specific mixing formula is not fully developed.

## Next Checks
1. **Ablation Study on Class Token Fixedness:** Remove the fixed class token constraint and retrain the model to quantify the exact contribution of this regularization mechanism. Compare performance degradation with and without fixed class tokens across different label ambiguity levels.

2. **Cross-Domain Evaluation:** Apply the framework to a non-CLIP VLM architecture (e.g., ALIGN or FLAVA) and evaluate on a task outside standard image classification, such as visual entailment or multi-label classification with natural candidate label distributions.

3. **Real-World Candidate Label Generation:** Instead of synthetic label ambiguity, evaluate the framework using automatically generated candidate labels from a weaker model or ensemble methods. Measure performance when candidate labels follow realistic error patterns rather than uniform random noise.