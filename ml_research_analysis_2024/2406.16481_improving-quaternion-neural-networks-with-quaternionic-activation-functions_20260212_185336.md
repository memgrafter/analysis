---
ver: rpa2
title: Improving Quaternion Neural Networks with Quaternionic Activation Functions
arxiv_id: '2406.16481'
source_url: https://arxiv.org/abs/2406.16481
tags:
- activation
- quaternion
- functions
- phase
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes novel quaternion activation functions that
  operate directly in quaternion space, addressing limitations of common split activation
  approaches. The key idea is to utilize quaternion polar representations by modifying
  either magnitude or phase, creating activation functions that respect quaternion
  algebra and utilize all four components.
---

# Improving Quaternion Neural Networks with Quaternionic Activation Functions

## Quick Facts
- arXiv ID: 2406.16481
- Source URL: https://arxiv.org/abs/2406.16481
- Authors: Johannes Pöppelbaum; Andreas Schwung
- Reference count: 40
- Key outcome: Phase-affecting quaternion activation functions outperform split activations on CIFAR-10 and SVHN datasets

## Executive Summary
This paper addresses a fundamental limitation in quaternion neural networks by proposing activation functions that operate directly in quaternion space rather than using split real-valued activations. The key innovation lies in leveraging quaternion polar representations, where activation functions can modify either magnitude or phase components independently. The authors introduce nine novel quaternion activation functions with comprehensive GHR derivative analysis, demonstrating consistent performance improvements over traditional split ReLU and Tanh activations across multiple network architectures and datasets.

## Method Summary
The paper proposes quaternion activation functions that utilize the polar representation of quaternions (magnitude and phase) to create non-linear transformations that respect quaternion algebra. Nine activation functions are introduced, categorized into magnitude-affecting (Norm, MagnitudeTanh, Cardioid) and phase-affecting (PhaseTanh variants, PhaseSin, ScaledPhaseSin) functions. These functions are integrated into quaternion neural network architectures (QVGG-S, QVGG11, QVGG16) and trained on CIFAR-10 and SVHN datasets using GHR calculus for gradient computation. The implementation uses quaternion 2D convolution with kernel size 3×3, average pooling with kernel size 2×2, and Adam optimizer with decaying learning rate.

## Key Results
- Phase-affecting activation functions (particularly PhaseSin) consistently outperform split ReLU and Tanh on both CIFAR-10 (82.65% mean accuracy) and SVHN (94.55% mean accuracy)
- All proposed quaternion activation functions show improved gradient flow compared to split activations, with phase-based functions maintaining sensitivity across the entire input range
- The QVGG-S architecture with PhaseSin activation achieved 82.65% mean accuracy on CIFAR-10, representing a significant improvement over split activation baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quaternion activation functions that modify phase achieve better performance by utilizing the multidimensional nature of quaternions and avoiding saturation effects.
- Mechanism: By affecting the phase while preserving magnitude, these activation functions create non-linearity through ratio changes between components without limiting the overall quaternion magnitude. This enables improved gradient flow since the functions remain sensitive across the entire input range.
- Core assumption: The quaternion space H provides mathematical properties that can be leveraged for activation functions, specifically through the polar representation where magnitude and phase can be manipulated independently.
- Evidence anchors:
  - [abstract]: "activation functions affecting the phase consistently prove to provide better performance"
  - [section]: "We aim for activation functions that leave the quaternion magnitude unchanged but instead incorporate non-linearity by modifying the respective components z0, z1, z2 and z3 such that ∥a∥ = ∥z∥"
  - [corpus]: Weak evidence - no direct corpus citations for this specific phase-based mechanism
- Break condition: If the assumption about phase manipulation being beneficial fails, or if the computational cost of trigonometric operations in the backward phase becomes prohibitive.

### Mechanism 2
- Claim: Quaternion activation functions outperform split activations by utilizing all quaternion components during calculation of each output component.
- Mechanism: Unlike split activations that treat quaternions as four separate real values, quaternion-specific activations use all four input components (z0, z1, z2, z3) to calculate each output component (a0, a1, a2, a3). This preserves the interrelationship between components established by the Hamilton product in quaternion convolution.
- Core assumption: The interrelationship between quaternion components established by Hamilton product is beneficial and should be preserved through the activation function.
- Evidence anchors:
  - [section]: "all quaternion components are utilized to calculate all output components, carrying out the benefit of the Hamilton product in e.g. the quaternion convolution to the activation functions"
  - [section]: "we require a0 = f (z0, z1, z2, z3), a1 = f (z0, z1, z2, z3), a2 = f (z0, z1, z2, z3) and a3 = f (z0, z1, z2, z3)"
  - [corpus]: Weak evidence - no direct corpus citations for this specific component utilization mechanism
- Break condition: If the computational overhead of using all components for each output calculation outweighs the performance benefits.

### Mechanism 3
- Claim: The proposed activation functions achieve improved gradient flow through consistent sensitivity across the input range, particularly for phase-affecting functions.
- Mechanism: The derivatives of phase-affecting activation functions show consistently high sensitivity values across most of the input range, avoiding the vanishing gradient problem common with saturated activation functions. This is evidenced by the GHR derivative calculations showing stable gradient magnitudes.
- Core assumption: Gradient flow quality directly correlates with network training performance and final accuracy.
- Evidence anchors:
  - [abstract]: "they prove to be sensitive on basically the whole input range, thus improved gradient flow can be expected"
  - [section]: "they are sensitive basically everywhere and yield somewhat constant gradient magnitudes noticeably greater than zero as long as not ∥Im(z)∥ → 0"
  - [section]: "all activation functions are well suited to be used in QNN due to their smooth gradient magnitudes without singularities and steadily high values"
- Break condition: If the relationship between derivative sensitivity and actual training performance is weaker than assumed, or if the computational cost of maintaining high sensitivity is prohibitive.

## Foundational Learning

- Concept: Quaternion algebra and polar representation
  - Why needed here: Understanding how quaternions can be represented in polar form (magnitude and phase) is fundamental to grasping why phase manipulation creates effective activation functions
  - Quick check question: How is a quaternion expressed in polar form, and what mathematical relationship connects the Cartesian and polar representations?

- Concept: GHR calculus for quaternion derivatives
  - Why needed here: The GHR calculus provides the framework for calculating derivatives of quaternion-valued functions, which is essential for understanding gradient flow properties of the proposed activation functions
  - Quick check question: What is the formula for calculating the derivative of a quaternion-valued function with respect to its quaternion input using GHR calculus?

- Concept: Hamilton product and quaternion convolution
  - Why needed here: Understanding how the Hamilton product interrelates quaternion components in convolution operations helps explain why preserving this interrelationship through activation functions is beneficial
  - Quick check question: How does the Hamilton product multiply two quaternions, and which components of the input quaternions contribute to each component of the output?

## Architecture Onboarding

- Component map:
  Input layer (32×32 quaternion-valued images) -> Quaternion 2D convolution (3×3 kernel) -> Activation layer (quaternion activation function) -> Quaternion average pooling (2×2 kernel) -> ... -> Output layer (real-valued fully connected)

- Critical path: Forward pass through convolution → activation → pooling layers, with backward pass computing gradients through activation functions using GHR calculus

- Design tradeoffs:
  - Phase-affecting activations offer better performance but require more trigonometric computations
  - Magnitude-affecting activations are computationally simpler but may suffer from saturation
  - Split activations are easiest to implement but ignore quaternion properties
  - Angle choice (ψ vs θ) affects performance, with ψ generally yielding better results

- Failure signatures:
  - Vanishing gradients: Observed as training stagnation or very slow convergence
  - Exploding gradients: Training instability, NaN values, or parameter divergence
  - Poor performance: Accuracy plateaus below expected levels despite adequate training time
  - Computational bottlenecks: Excessive training time due to complex trigonometric calculations

- First 3 experiments:
  1. Baseline comparison: Implement QVGG-S with split ReLU vs PhaseSin activation on CIFAR-10, measuring training curves and final accuracy
  2. Sensitivity analysis: Compare gradient magnitudes across input ranges for MagnitudeTanh vs PhaseTanh using GHR derivatives
  3. Ablation study: Test the effect of using angle θ vs ψ in PhaseTanh on QVGG-S, measuring performance difference on SVHN dataset

## Open Questions the Paper Calls Out

- How do quaternion activation functions perform on tasks beyond image classification, such as time series analysis, natural language processing, or speech recognition?
- What is the impact of modifying the rotation axis (n) in quaternion activation functions, and how can this be implemented while preserving axis length?
- How do the computational costs of quaternion activation functions compare to split activations in terms of training time and inference latency?

## Limitations

- Experimental validation limited to CIFAR-10 and SVHN datasets with relatively simple quaternion neural network architectures
- Computational overhead analysis lacks detailed measurements of inference time and energy consumption
- Comparison with advanced quaternion methods limited, focusing only on split ReLU and Tanh baselines

## Confidence

- **High Confidence**: The mathematical derivations of GHR calculus derivatives for all proposed activation functions are rigorous and verifiable
- **Medium Confidence**: The claim that phase-affecting activations consistently outperform magnitude-affecting ones is supported by experimental results but may depend on specific architectures and datasets
- **Low Confidence**: The assertion that utilizing all quaternion components in activation calculations significantly contributes to performance gains lacks direct empirical validation

## Next Checks

1. **Architecture Ablation Study**: Implement a controlled experiment where the only variable is the activation function (keeping all other architectural parameters constant) across multiple quaternion neural network architectures beyond QVGG-S, including deeper networks and different application domains like speech processing or time series analysis.

2. **Computational Efficiency Analysis**: Conduct a comprehensive benchmarking study measuring not just accuracy but also inference time, memory usage, and energy consumption for each proposed activation function across different hardware platforms (CPU, GPU, specialized accelerators) to quantify the practical deployment tradeoffs.

3. **Robustness and Generalization Testing**: Design experiments testing the proposed activation functions under adversarial conditions, including noisy inputs, out-of-distribution data, and domain shift scenarios to evaluate whether the performance benefits hold under real-world deployment challenges.