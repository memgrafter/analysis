---
ver: rpa2
title: 'GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework'
arxiv_id: '2407.10793'
source_url: https://arxiv.org/abs/2407.10793
tags:
- hallucination
- grapheval
- evaluation
- hallucinations
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GraphEval addresses the problem of detecting hallucinations in
  LLM outputs by representing the output as a Knowledge Graph (KG) and checking each
  triple against the context using NLI models. This approach improves balanced accuracy
  by 6.2% on average compared to using NLI models directly, while also identifying
  specific inconsistent triples.
---

# GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework

## Quick Facts
- **arXiv ID**: 2407.10793
- **Source URL**: https://arxiv.org/abs/2407.10793
- **Reference count**: 40
- **Key outcome**: GraphEval improves balanced accuracy by 6.2% over direct NLI use while identifying specific hallucinated triples

## Executive Summary
GraphEval addresses the critical problem of detecting hallucinations in LLM outputs by representing the output as a Knowledge Graph (KG) and checking each triple against the context using NLI models. This approach improves balanced accuracy by 6.2% on average compared to using NLI models directly, while also identifying specific inconsistent triples. The method is computationally efficient and explainable, providing insights into where hallucinations occur. GraphEval can be extended to correct hallucinations (GraphCorrect), which fixes 59.6% of detected hallucinations while maintaining high similarity with the original text.

## Method Summary
GraphEval constructs a Knowledge Graph from LLM output by converting the text into triples using an LLM (typically Claude 2), then checks each triple against the grounding context using Natural Language Inference (NLI) models. If any triple is classified as hallucinated (probability > 0.5), the output is flagged as containing hallucinations. GraphCorrect extends this by generating corrected triples for the identified hallucinated portions and reconstructing the text. The framework is evaluated on human-annotated hallucination detection benchmarks (SummEval, QAGS-C, QAGS-X) and achieves state-of-the-art performance with computational efficiency.

## Key Results
- 6.2% improvement in balanced accuracy compared to direct NLI model application
- GraphCorrect successfully fixes 59.6% of detected hallucinations
- Method is computationally efficient, requiring only one LLM call for KG construction
- Provides explainable results by identifying specific inconsistent triples

## Why This Works (Mechanism)
GraphEval leverages the structured representation of Knowledge Graphs to break down complex LLM outputs into atomic facts (triples) that can be individually verified against the context. This decomposition allows NLI models to focus on smaller, more precise statements rather than entire paragraphs, reducing the cognitive load and improving detection accuracy. The approach transforms a holistic reasoning problem into a set of binary classification tasks, each with clearer semantic boundaries.

## Foundational Learning
- **Knowledge Graph construction**: Converting natural language into subject-predicate-object triples is essential for decomposing complex outputs into verifiable facts. Quick check: Can your LLM generate coherent triples that preserve the original meaning?
- **Natural Language Inference**: NLI models determine whether a statement (triple) is entailed, neutral, or contradicted by the context. Quick check: Does your NLI model achieve reasonable accuracy on short factual statements?
- **Balanced accuracy**: This metric accounts for class imbalance by averaging recall obtained on each class, which is crucial when hallucination rates are low. Quick check: Are you measuring performance using metrics appropriate for imbalanced datasets?
- **Triple-level hallucination detection**: The framework identifies specific inconsistent facts rather than flagging entire outputs, enabling targeted corrections. Quick check: Can you map detected hallucinations back to specific parts of the original text?
- **Graph-based reasoning**: Knowledge Graphs enable structured reasoning about relationships between entities in the text. Quick check: Does your KG capture all relevant relationships without excessive granularity loss?
- **Hallucination correction**: Once hallucinations are identified, they can be systematically corrected by regenerating only the problematic triples. Quick check: Can your correction method maintain semantic coherence with the original non-hallucinated content?

## Architecture Onboarding

**Component map**: LLM output → KG construction → Triple verification → NLI model → Hallucination classification

**Critical path**: LLM output → KG construction (Claude 2) → NLI verification (HHEM/TRUE/TrueTeacher) → Decision threshold (0.5) → Output classification

**Design tradeoffs**: The framework trades some information loss during KG construction for computational efficiency and explainability. Using simpler NLI models instead of more expensive LLM-based methods reduces computational cost but may miss nuanced hallucinations.

**Failure signatures**: Information loss during KG construction may cause false negatives; poor NLI model performance may cause false positives; threshold selection at 0.5 may not be optimal for all domains.

**First experiments**:
1. Convert a sample LLM output into triples using the provided KG construction prompt
2. Verify each triple against context using a simple NLI model (e.g., HHEM) and record probabilities
3. Compare balanced accuracy of GraphEval versus direct NLI application on a small test set

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Information loss during KG construction may impact detection accuracy for complex semantic relationships
- Performance depends heavily on the quality of both KG construction and NLI model selection
- Evaluation focuses on controlled benchmarks rather than diverse real-world applications

## Confidence
- **High confidence**: The KG-based representation approach and its computational efficiency; the explainability advantage of identifying specific inconsistent triples
- **Medium confidence**: The 6.2% balanced accuracy improvement claim, as it depends on the quality and representativeness of the evaluation datasets
- **Medium confidence**: The GraphCorrect 59.6% success rate, given that this metric depends on the initial detection accuracy and the quality of the correction generation

## Next Checks
1. **Cross-model validation**: Test GraphEval with at least three different NLI models (beyond HHEM, TRUE, TrueTeacher) to establish robustness against model-specific biases and ensure the 6.2% improvement holds across different NLI backbones.

2. **Error propagation analysis**: Conduct ablation studies measuring hallucination detection accuracy when errors are introduced at each stage (KG construction → NLI classification → final decision) to quantify how weaknesses in early stages affect downstream performance.

3. **Real-world deployment simulation**: Apply GraphEval to LLM outputs from diverse domains (legal, medical, technical) with expert-annotated ground truth to validate that benchmark performance translates to practical applications where hallucinations may have higher stakes.