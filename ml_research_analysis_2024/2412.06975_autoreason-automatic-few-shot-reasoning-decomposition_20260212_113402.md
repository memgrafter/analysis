---
ver: rpa2
title: 'AutoReason: Automatic Few-Shot Reasoning Decomposition'
arxiv_id: '2412.06975'
source_url: https://arxiv.org/abs/2412.06975
tags:
- reasoning
- autoreason
- answer
- language
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoReason introduces a method to automatically generate Chain-of-Thought
  (CoT) reasoning traces from zero-shot prompts, enabling improved multi-step reasoning
  in large language models (LLMs). By decomposing implicit queries into explicit sub-questions,
  AutoReason transforms zero-shot prompts into few-shot reasoning traces, improving
  interpretability and accuracy.
---

# AutoReason: Automatic Few-Shot Reasoning Decomposition

## Quick Facts
- **arXiv ID**: 2412.06975
- **Source URL**: https://arxiv.org/abs/2412.06975
- **Reference count**: 40
- **Primary result**: AutoReason improves multi-step reasoning in LLMs by automatically generating Chain-of-Thought traces from zero-shot prompts, achieving 76.6% accuracy on StrategyQA with GPT-3.5-Turbo versus 55% with base prompting

## Executive Summary
AutoReason introduces a method to automatically generate Chain-of-Thought (CoT) reasoning traces from zero-shot prompts, enabling improved multi-step reasoning in large language models (LLMs). By decomposing implicit queries into explicit sub-questions, AutoReason transforms zero-shot prompts into few-shot reasoning traces, improving interpretability and accuracy. Tested on StrategyQA and HotpotQA datasets, AutoReason demonstrated significant accuracy improvements, particularly on StrategyQA, where GPT-3.5-Turbo achieved 76.6% accuracy compared to 55% with base prompting. While mixed results were observed on HotpotQA, AutoReason outperformed standard CoT prompting in most cases. The framework's modular design and adaptability to different LLMs make it a promising approach for advancing LLM reasoning capabilities.

## Method Summary
AutoReason is a two-step reasoning framework that automatically generates rationales from zero-shot prompts using a stronger LLM (GPT-4), which are then used as implicit few-shot examples by a weaker LLM (GPT-3.5-Turbo) to produce final answers. The system decomposes implicit queries into explicit sub-questions, creating interpretable reasoning chains. The framework includes a query formatter to prepare inputs, a rationale formatter to structure generated reasoning traces, and a scorer to evaluate answer correctness. By tailoring rationales to each specific query rather than using fixed exemplars, AutoReason aims to enhance reasoning specificity and relevance.

## Key Results
- GPT-3.5-Turbo achieved 76.6% accuracy on StrategyQA using AutoReason versus 55% with base prompting
- AutoReason outperformed standard CoT prompting in most tested scenarios
- Mixed results on HotpotQA, with AutoReason showing superior performance in some cases but regression in others
- The framework demonstrates significant potential for improving multi-step reasoning in weaker LLMs through automated rationale generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: AutoReason transforms zero-shot prompts into few-shot reasoning traces by decomposing implicit queries into explicit sub-questions
- **Mechanism**: The system uses GPT-4 to generate rationales from zero-shot prompts, which serve as implicit few-shot examples for GPT-3.5-Turbo
- **Core assumption**: Generated rationales are of sufficient quality to guide the weaker model toward correct reasoning
- **Evidence anchors**: [abstract] states AutoReason "decomposes the implicit query into several explicit questions" and [section 2.1] describes it as a framework that "effectively deconstructs zero-shot prompts...into few-shot reasoning traces"
- **Break condition**: Poor-quality or irrelevant rationales from GPT-4 will cause GPT-3.5-Turbo to fail

### Mechanism 2
- **Claim**: AutoReason provides interpretability by making reasoning steps explicit, improving performance on weaker LLMs
- **Mechanism**: Breaking down complex reasoning into explicit questions creates transparent reasoning chains that weaker models can follow
- **Core assumption**: Explicit reasoning steps are easier for models to process than implicit reasoning
- **Evidence anchors**: [abstract] mentions "This provides interpretability for the model, improving reasoning in weaker LLMs" and [section 1.1] discusses addressing "key limitations of current CoT prompting methods"
- **Break condition**: Explicit decomposition may add unnecessary complexity or introduce errors

### Mechanism 3
- **Claim**: AutoReason's query-specific rationale generation outperforms fixed few-shot exemplars
- **Mechanism**: Custom-generated rationales for each query are more relevant and context-specific than generic exemplars
- **Core assumption**: Custom rationales are more informative than generic exemplars for any given query
- **Evidence anchors**: [abstract] states AutoReason "distinguishes itself from existing methods by generating rationales from a zero-shot prompt automatically and tailoring them to each specific query"
- **Break condition**: Query-specific generation may introduce significant latency or GPT-4 may consistently fail to generate relevant rationales

## Foundational Learning

- **Concept: Chain-of-Thought prompting**
  - *Why needed here*: AutoReason builds on CoT methodology but automates exemplar generation
  - *Quick check question*: What is the key difference between zero-shot CoT and few-shot CoT prompting?

- **Concept: Implicit vs explicit reasoning**
  - *Why needed here*: AutoReason transforms implicit reasoning requirements into explicit reasoning steps
  - *Quick check question*: Why might decomposing implicit queries into explicit sub-questions improve reasoning performance?

- **Concept: Multi-step reasoning decomposition**
  - *Why needed here*: The core mechanism involves breaking down complex queries into manageable reasoning steps
  - *Quick check question*: How does decomposing a complex question into simpler sub-questions facilitate reasoning?

## Architecture Onboarding

- **Component map**: Query formatter → GPT-4 → Rationale formatter → GPT-3.5-Turbo → Scorer
- **Critical path**: 
  1. User query → Query formatter → GPT-4 → Rationale generation
  2. Rationale + query → Final answer prompt → GPT-3.5-Turbo → Answer generation
  3. Answer + query + correct answer → Scorer → Score calculation
- **Design tradeoffs**: 
  - Cost vs performance: GPT-4 usage increases costs but improves accuracy
  - Latency vs accuracy: Two-step process adds latency but provides better reasoning quality
  - Model dependency: Performance heavily depends on the stronger model's reasoning capabilities
- **Failure signatures**: 
  - Low accuracy on tasks not requiring multi-step reasoning
  - Regression when using on already-capable models
  - High variance in performance across different query types
- **First 3 experiments**:
  1. Compare base prompting vs AutoReason on StrategyQA dataset with GPT-3.5-Turbo
  2. Test AutoReason with different strength ratios (e.g., GPT-4.5 + GPT-3.5 vs GPT-4 + GPT-3.5)
  3. Evaluate performance degradation when generating poor-quality rationales intentionally

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the quality of automatically generated rationales impact the accuracy of final answers across different LLM strengths?
- **Basis in paper**: Explicit - "The quality of the generated rationales is crucial to the success of the method, and poor-quality rationales can lead to incorrect answers or hallucinations"
- **Why unresolved**: The paper demonstrates accuracy improvements but doesn't systematically analyze how rationale quality varies with different LLM combinations or prompt designs
- **What evidence would resolve it**: Comparative analysis of rationale quality scores across different LLM pairs and correlation between quality metrics and final answer accuracy

### Open Question 2
- **Question**: What is the optimal level of reasoning decomposition for different types of implicit reasoning tasks?
- **Basis in paper**: Inferred - the paper aims to address CoT limitations but lacks analysis of optimal decomposition depth
- **Why unresolved**: The paper uses a fixed decomposition approach without exploring whether different task types benefit from varying levels of reasoning trace granularity
- **What evidence would resolve it**: Ablation studies testing different numbers of reasoning steps for various task categories, measuring accuracy trade-offs

### Open Question 3
- **Question**: How does AutoReason's performance scale with task complexity beyond the tested datasets?
- **Basis in paper**: Explicit - "The current study's limited sample size and number of runs point to the need for more extensive testing across a broader range of tasks and domains"
- **Why unresolved**: Results are only reported on StrategyQA and HotpotQA, leaving uncertainty about performance on more complex or domain-specific reasoning tasks
- **What evidence would resolve it**: Extensive testing on diverse reasoning benchmarks including mathematical word problems, commonsense reasoning, and multi-domain QA tasks

## Limitations
- The framework's performance heavily depends on the quality of rationales generated by the stronger LLM
- Mixed results on HotpotQA suggest the approach may not generalize equally well across all reasoning tasks
- The two-step process using different model strengths introduces potential failure points and increased latency

## Confidence

**High Confidence claims**:
- AutoReason successfully decomposes implicit queries into explicit sub-questions
- The framework improves accuracy on StrategyQA with GPT-3.5-Turbo
- The two-step approach using different model strengths is technically feasible

**Medium Confidence claims**:
- AutoReason provides better interpretability than standard CoT
- Query-specific rationale generation outperforms fixed exemplars
- Performance improvements are primarily due to the decomposition mechanism

**Low Confidence claims**:
- AutoReason will generalize well to other reasoning tasks beyond StrategyQA and HotpotQA
- The framework maintains consistent performance across different model pairs and configurations
- Generated rationales are always beneficial to the weaker model

## Next Checks
1. **Rationale Quality Validation**: Systematically evaluate the quality of GPT-4-generated rationales across different query types through human evaluation and correlate quality scores with final answer accuracy
2. **Cross-Dataset Generalization**: Test AutoReason on additional reasoning datasets (CommonsenseQA, ARC Challenge) to assess generalization beyond StrategyQA and HotpotQA
3. **Ablation Study on Model Pairings**: Experiment with different combinations of strong/weak LLMs to determine whether performance gains are specific to the GPT-4 + GPT-3.5-Turbo pairing or represent a more general principle