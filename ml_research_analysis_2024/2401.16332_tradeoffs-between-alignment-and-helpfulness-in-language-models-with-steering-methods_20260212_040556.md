---
ver: rpa2
title: Tradeoffs Between Alignment and Helpfulness in Language Models with Steering
  Methods
arxiv_id: '2401.16332'
source_url: https://arxiv.org/abs/2401.16332
tags:
- behavior
- steering
- representation
- probability
- vectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the tradeoff between alignment and helpfulness
  in language models when using steering methods such as representation engineering.
  The authors propose a theoretical framework that provides bounds on how alignment
  improves and helpfulness decreases as the norm of injected steering vectors increases.
---

# Tradeoffs Between Alignment and Helpfulness in Language Models with Steering Methods

## Quick Facts
- arXiv ID: 2401.16332
- Source URL: https://arxiv.org/abs/2401.16332
- Reference count: 40
- One-line primary result: Alignment improves linearly while helpfulness decreases quadratically with steering vector norm, creating an efficient regime for small norms.

## Executive Summary
This paper studies the fundamental tradeoff between alignment and helpfulness in language models when using steering methods such as representation engineering. The authors develop a theoretical framework that provides mathematical bounds on how alignment improves and helpfulness decreases as the norm of injected steering vectors increases. They show that for small steering vector norms, alignment increases linearly while helpfulness decreases quadratically, indicating an efficient regime where steering can be cost-effective. Empirically, they validate these results by measuring alignment and helpfulness of Llama 2 models with injected PCA vectors, finding behavior matching the theoretical predictions.

## Method Summary
The paper proposes a theoretical framework for analyzing the tradeoff between alignment and helpfulness when using steering methods in language models. The method involves extracting PCA vectors from contrasting positive and negative examples, injecting these vectors into model representations at inference time with varying coefficient norms, and measuring the resulting alignment (via behavior expectation) and helpfulness (via probability of correct answers). The theoretical analysis provides bounds on alignment improvement (linear with norm) and helpfulness degradation (quadratic with norm), which are validated empirically on Llama 2 models across multiple datasets including AdvBench, StereoSet, MMLU, and HumanEval.

## Key Results
- Alignment improves linearly with steering vector norm while helpfulness decreases quadratically
- For small steering norms, there exists an efficient regime where alignment gains outpace helpfulness losses
- Theoretical bounds are validated empirically on Llama 2 models across multiple behaviors and datasets
- The optimal tradeoff point depends on the relative importance of alignment versus helpfulness for the specific application

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alignment improves linearly with the norm of steering vectors.
- Mechanism: Steering vectors injected into model representations change the last hidden layer in a way that linearly classifies positive and negative answer representations. The alignment score increases as a shifted hyperbolic tangent function of the steering vector norm.
- Core assumption: The change to the last hidden layer representation due to steering is linearly related to the injected vector norm, and the positive/negative answer representations are linearly separable in this space.
- Evidence anchors:
  - [abstract] "alignment increases linearly with the norm of steering vectors"
  - [section 3, Theorem 1] "the behavior expectation of the model conditioned on the query q satisfies: B[Pθ,re(·|q)] ≥ tanh(∆λ · re + arctanh(B0))"
  - [corpus] "Found 25 related papers... Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning"
- Break condition: If the steering vectors no longer linearly classify the answer representations, or if the norm-to-representation change relationship becomes nonlinear, the alignment gain will deviate from linear.

### Mechanism 2
- Claim: Helpfulness decreases quadratically with the norm of steering vectors.
- Mechanism: Steering introduces random noise into the model's logit space for tokens unrelated to the steering behavior. This noise increases with vector norm, degrading the probability of correct answers in a parabolic fashion.
- Core assumption: The change to the last hidden layer due to steering is random with respect to the representations of correct and incorrect answers, and this noise scales with the steering vector norm.
- Evidence anchors:
  - [abstract] "helpfulness decreases quadratically with the norm of steering vectors"
  - [section 3, Theorem 2] "the helpfulness of the model on the query is bounded... ≤ P0 / (P0 + (1 − P0) · α(1 − ϵ)(1 + λ2σ2β2 / 2 r2e))"
  - [corpus] "Multi-Attribute Steering of Language Models via Targeted Intervention"
- Break condition: If the noise introduced by steering becomes structured rather than random, or if the model adapts to compensate for the noise, the quadratic decay may not hold.

### Mechanism 3
- Claim: There exists an efficient regime for small steering vector norms where alignment improves faster than helpfulness decreases.
- Mechanism: For small norms, the linear increase in alignment outpaces the quadratic decrease in helpfulness, creating a region of effective steering where the model remains both aligned and useful.
- Core assumption: The linear and quadratic relationships hold for small steering vector norms, and the parameters allow the linear term to dominate initially.
- Evidence anchors:
  - [abstract] "for small representation engineering norms the helpfulness decreases quadratically while the alignment increase is linear, so there is a regime in which representation engineering can be cost-effective"
  - [section 3] "The combination of the two results shows alignment improves linearly with the norm of the steering vectors while helpfulness is decreased quadratically"
  - [corpus] "SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning"
- Break condition: If the initial alignment is very poor (B0 close to -1) or if the noise parameters (λσβ) are very large, the quadratic decay may dominate even at small norms.

## Foundational Learning

- Concept: Linear separability in latent space
  - Why needed here: The core theoretical result depends on steering vectors creating a linear decision boundary between aligned and misaligned answer representations.
  - Quick check question: Can you explain why PCA vectors extracted from contrasting positive and negative statements would form a linear classifier in the model's latent space?

- Concept: Behavior expectation as alignment metric
  - Why needed here: The paper quantifies alignment using the expected value of a binary behavior scoring function, which measures the difference in probabilities between aligned and misaligned responses.
  - Quick check question: How does the behavior expectation range from -1 to +1, and what does each extreme represent?

- Concept: Probability chain rule for sequence generation
  - Why needed here: The extension of results to multi-token answers relies on understanding how the probability of a full sequence is the product of probabilities at each decoding step.
  - Quick check question: If the probability of each token in a correct answer decreases quadratically with steering norm, what happens to the overall sequence probability?

## Architecture Onboarding

- Component map: Input text -> Transformer layers -> Hidden states -> Steering injection -> Output logits -> Token probabilities

- Critical path:
  1. Extract steering vectors by contrasting representations of positive/negative examples
  2. Inject scaled steering vectors into model layers during inference
  3. Measure behavior expectation for alignment and probability of correct answers for helpfulness
  4. Analyze tradeoff as a function of steering vector norm

- Design tradeoffs:
  - Steering vector norm vs. alignment gain: Higher norms give better alignment but harm helpfulness
  - Layer selection for injection: Different layers may have different effects on behavior
  - Steering method: PCA vs. mean-centering vs. VAE-based methods may yield different tradeoffs

- Failure signatures:
  - Alignment plateaus or decreases at high steering norms (suggesting interference or saturation)
  - Helpfulness decreases too rapidly (suggesting high noise parameters λσβ)
  - Steering has no effect on behavior (suggesting poor linear separability of representations)

- First 3 experiments:
  1. Reproduce alignment vs. norm curve for a simple behavior (e.g., harmlessness) on Llama 2 13B
  2. Measure helpfulness decay across different MMLU domains as a function of steering norm
  3. Sweep steering vector norm to find the optimal tradeoff point where alignment gain equals helpfulness loss rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between the steering vector norm and the linear classification margin ∆ in the final hidden layer?
- Basis in paper: [explicit] The paper assumes a linear classification margin ∆ between positive and negative answer representations in the final hidden layer, but does not provide a method to quantify or measure ∆ experimentally.
- Why unresolved: While the paper provides empirical estimates for λ and σ, it does not provide a direct method to measure ∆ or establish a precise relationship between ∆ and the steering vector norm.
- What evidence would resolve it: A systematic experimental study varying the steering vector norm and measuring the classification accuracy between positive and negative answer representations in the final hidden layer would establish the exact relationship between ∆ and the steering vector norm.

### Open Question 2
- Question: How do the theoretical bounds on alignment and helpfulness translate to real-world performance metrics beyond single-token and multiple-choice question answering?
- Basis in paper: [inferred] The paper focuses on theoretical bounds for alignment and helpfulness in the context of single-token and multiple-choice question answering, but does not explore their applicability to more complex tasks.
- Why unresolved: The theoretical framework is developed for simple scenarios, and its extension to real-world tasks with more nuanced performance metrics remains unexplored.
- What evidence would resolve it: Empirical studies evaluating the alignment and helpfulness of steered models on diverse real-world tasks, such as open-ended question answering, summarization, and dialogue, would provide insights into the practical relevance of the theoretical bounds.

### Open Question 3
- Question: What is the optimal balance between alignment and helpfulness when using steering methods in language models?
- Basis in paper: [explicit] The paper demonstrates a tradeoff between alignment and helpfulness, with alignment improving linearly and helpfulness decreasing quadratically with the steering vector norm, but does not provide a definitive answer on the optimal balance.
- Why unresolved: The optimal balance depends on the specific application and the relative importance of alignment and helpfulness, which varies across different use cases.
- What evidence would resolve it: A comprehensive study comparing the performance of steered models with different steering vector norms on a range of tasks, considering both alignment and helpfulness metrics, would provide insights into the optimal balance for different applications.

## Limitations
- The theoretical framework relies on simplifying assumptions about linear separability and random noise that may not hold in practice
- The analysis assumes a fixed probability of correct answers that doesn't change with steering, which may not be true for different question types
- The optimal tradeoff point depends heavily on the specific application and relative importance of alignment vs helpfulness

## Confidence

- **High confidence**: The linear relationship between steering vector norm and alignment improvement is well-supported by both theoretical derivation and empirical validation on multiple datasets.
- **Medium confidence**: The quadratic relationship between steering norm and helpfulness decay is theoretically justified but may be sensitive to the specific choice of steering vectors and the distribution of correct vs. incorrect answers in the evaluation set.
- **Medium confidence**: The existence of an efficient regime for small steering norms is supported by the theoretical analysis, but the exact boundaries of this regime may vary depending on the specific behavior being targeted and the model architecture.

## Next Checks

1. **Cross-behavior validation**: Test whether the linear alignment improvement and quadratic helpfulness decay hold for a diverse set of behaviors beyond harmlessness and racism, including helpfulness, honesty, and task-specific behaviors like code generation quality.

2. **Layer-wise analysis**: Investigate whether steering at different layers of the transformer produces different tradeoff curves, and whether there are optimal layers for balancing alignment and helpfulness.

3. **Correlation analysis**: Examine the correlation between steering-induced changes in alignment and changes in helpfulness across different steering norms to verify that the improvements are independent rather than traded off.