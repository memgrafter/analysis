---
ver: rpa2
title: 'AutoTimes: Autoregressive Time Series Forecasters via Large Language Models'
arxiv_id: '2402.02370'
source_url: https://arxiv.org/abs/2402.02370
tags:
- series
- time
- forecasting
- autotimes
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AutoTimes, an autoregressive approach to repurpose
  large language models (LLMs) for time series forecasting. Unlike prior non-autoregressive
  methods, AutoTimes maintains the autoregressive property of LLMs by independently
  embedding time series segments into the LLM's embedding space and autoregressively
  generating future predictions of arbitrary lengths.
---

# AutoTimes: Autoregressive Time Series Forecasters via Large Language Models

## Quick Facts
- arXiv ID: 2402.02370
- Source URL: https://arxiv.org/abs/2402.02370
- Reference count: 40
- Primary result: State-of-the-art time series forecasting using autoregressive LLMs with 0.1% trainable parameters and 5× speedup

## Executive Summary
AutoTimes presents a novel approach to time series forecasting using large language models (LLMs) in an autoregressive manner. Unlike previous non-autoregressive methods, AutoTimes maintains the autoregressive property of LLMs by independently embedding time series segments into the LLM's embedding space and generating future predictions autoregressively. The method uses textual timestamps as position embeddings to incorporate chronological information and align multivariate series, achieving state-of-the-art performance with minimal trainable parameters and significant computational efficiency.

## Method Summary
AutoTimes repurposes LLMs for time series forecasting by freezing LLM parameters and using minimal trainable components (0.1% of total parameters). The method segments time series into tokens, embeds them using SegmentEmbedding, and uses LLM-embedded textual timestamps as position embeddings. A frozen LLM processes these embeddings, followed by a SegmentProjection layer to map outputs back to time series segments. The model is trained with next token prediction loss and evaluated using rolling forecasting on seven real-world datasets.

## Key Results
- Achieves state-of-the-art performance on seven real-world time series datasets
- Uses only 0.1% trainable parameters while maintaining high accuracy
- Demonstrates over 5× speedup in training and inference compared to advanced LLM-based forecasters
- Shows strong zero-shot generalization and in-context forecasting capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Maintaining autoregressive property in LLM-based time series forecasting improves multi-step prediction accuracy compared to non-autoregressive approaches.
- **Mechanism**: AutoTimes preserves the autoregressive property by independently embedding time series segments into the LLM's embedding space and autoregressively generating future predictions of arbitrary lengths.
- **Core assumption**: The autoregressive nature of LLMs is essential for their strong generalization and multi-step generation capabilities.
- **Evidence anchors**:
  - [abstract]: "the inherent autoregressive property and decoder-only architecture of LLMs have not been fully considered"
  - [section]: "By refining the inconsistency of non-autoregressive LLM4TS methods, we propose to inherit the autoregressive property of LLMs"
- **Break condition**: The mechanism breaks if the autoregressive property is not maintained or if the segment embedding approach fails to capture temporal dependencies effectively.

### Mechanism 2
- **Claim**: Using textual timestamps as position embeddings improves multivariate time series alignment and forecasting performance.
- **Mechanism**: Textual timestamps are converted into position embeddings by the LLM, which allows the model to utilize chronological information and align simultaneous events from different variates without increasing context length.
- **Core assumption**: Textual timestamps embedded by the LLM can effectively capture date and periodicity information, providing better temporal alignment than numerical encoding.
- **Evidence anchors**:
  - [abstract]: "By introducing LLM-embedded textual timestamps, AutoTimes can utilize chronological information to align multivariate time series"
  - [section]: "we adopt LLM-embedded timestamps as position embeddings to utilize temporal information and align simultaneous events (segments) from different varieties"
- **Break condition**: The mechanism breaks if the LLM fails to properly embed textual timestamps or if the position embeddings don't provide meaningful chronological information for forecasting.

### Mechanism 3
- **Claim**: Freezing LLM parameters while adapting with minimal trainable parameters achieves state-of-the-art performance with significant computational efficiency.
- **Mechanism**: AutoTimes freezes the LLM and establishes token embedding and projection for time series, which only account for up to 0.1% total parameters, achieving over 5× training/inference speedup.
- **Core assumption**: The general-purpose token transition capability of LLMs can be effectively transferred to time series forecasting with minimal parameter adaptation.
- **Evidence anchors**:
  - [abstract]: "With the consistency of autoregression, it also inherits notable generalizability and scaling behavior of LLMs"
  - [section]: "To fully leverage the inherent token transitions of LLMs and reduce the training cost, we freeze the LLM and establish token embedding and projection for time series"
- **Break condition**: The mechanism breaks if the frozen LLM cannot effectively transfer its token transition capabilities or if the minimal parameter adaptation is insufficient for accurate time series forecasting.

## Foundational Learning

- **Concept**: Autoregressive models and their advantages over non-autoregressive approaches
  - **Why needed here**: Understanding the difference between autoregressive and non-autoregressive forecasting is crucial for grasping why AutoTimes' approach is effective
  - **Quick check question**: What is the key difference between autoregressive and non-autoregressive time series forecasting models?

- **Concept**: Position embeddings in transformer models
  - **Why needed here**: The use of textual timestamps as position embeddings is a novel approach in AutoTimes that requires understanding of how position information is typically encoded in transformers
  - **Quick check question**: How do standard transformer models incorporate position information into their embeddings?

- **Concept**: Zero-shot learning and in-context learning in LLMs
  - **Why needed here**: AutoTimes demonstrates these capabilities in time series forecasting, which are important for understanding its generalization and adaptation properties
  - **Quick check question**: What is the difference between zero-shot learning and in-context learning in large language models?

## Architecture Onboarding

- **Component map**: Time series → Segment Embedding → Position Embedding → Frozen LLM → Segment Projection → Output prediction

- **Critical path**: Input time series → Segment Embedding → Position Embedding → Frozen LLM → Segment Projection → Output prediction

- **Design tradeoffs**:
  - Freezing LLM vs. fine-tuning: Freezing provides computational efficiency but may limit task-specific optimization
  - Segment length selection: Affects the granularity of time series representation and model complexity
  - Position embedding approach: Textual timestamps vs. numerical encoding impacts temporal alignment capabilities

- **Failure signatures**:
  - Poor performance on long-term forecasting: May indicate issues with autoregressive generation or segment length selection
  - Degraded performance on multivariate series: Could suggest problems with position embedding alignment
  - High computational cost: Might indicate inefficient implementation or inappropriate segment length

- **First 3 experiments**:
  1. Ablation study comparing autoregressive vs. non-autoregressive approaches on a simple univariate dataset
  2. Evaluation of different position embedding strategies (textual vs. numerical) on multivariate forecasting
  3. Analysis of segment length impact on forecasting accuracy and computational efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AutoTimes' autoregressive approach compare to non-autoregressive methods when handling non-stationary time series with varying trends and seasonality?
- Basis in paper: [explicit] The paper mentions that AutoTimes inherits the autoregressive property of LLMs and maintains consistency with the decoder-only architecture, but does not explicitly compare its performance on non-stationary data against non-autoregressive methods.
- Why unresolved: The experiments focus on general performance and efficiency, but do not specifically analyze how well AutoTimes adapts to non-stationary patterns compared to other approaches.
- What evidence would resolve it: A comparative study of AutoTimes and non-autoregressive LLM4TS methods on datasets with known non-stationary characteristics, showing performance differences across varying trend and seasonality patterns.

### Open Question 2
- Question: What is the impact of segment length S on AutoTimes' forecasting accuracy and computational efficiency?
- Basis in paper: [explicit] The paper mentions that segment length is set to 96 for multivariate datasets and the prediction length for M3 and M4, but does not provide a detailed analysis of how different segment lengths affect performance.
- Why unresolved: While the paper demonstrates the effectiveness of AutoTimes, it does not explore the sensitivity of the model to different segment lengths, which could be crucial for optimizing performance and efficiency.
- What evidence would resolve it: A comprehensive ablation study varying the segment length S across different datasets and forecasting horizons, measuring both accuracy and computational cost to identify optimal segment lengths.

### Open Question 3
- Question: How does AutoTimes handle multivariate time series with complex inter-variable dependencies?
- Basis in paper: [explicit] The paper mentions that AutoTimes uses textual timestamps as position embeddings to align multivariate time series and adopts Channel Independence, but does not extensively analyze its performance on datasets with complex variable interactions.
- Why unresolved: The experiments focus on general performance, but do not specifically investigate how well AutoTimes captures and utilizes complex dependencies between variables in multivariate time series.
- What evidence would resolve it: A detailed analysis of AutoTimes' performance on datasets with known complex variable interactions, comparing it to methods specifically designed to handle multivariate dependencies, and examining the impact of different alignment strategies.

## Limitations

- Extensive ablation studies may not fully explore all potential failure modes or edge cases
- Generalizability of results to other LLM architectures or time series datasets remains uncertain
- Effectiveness of textual timestamps as position embeddings requires further validation compared to alternative strategies

## Confidence

- **High Confidence**: The autoregressive property of LLMs is essential for their strong generalization and multi-step generation capabilities, which are lost when converting decoder-only models to encoder-only forecasters.
- **Medium Confidence**: Using textual timestamps as position embeddings improves multivariate time series alignment and forecasting performance.
- **Medium Confidence**: Freezing LLM parameters while adapting with minimal trainable parameters achieves state-of-the-art performance with significant computational efficiency.

## Next Checks

1. **Cross-architecture validation**: Test AutoTimes with different LLM architectures (e.g., BERT, RoBERTa) to verify the robustness of the segment embedding and position embedding approaches across model families.

2. **Position embedding ablation**: Conduct a controlled experiment comparing textual timestamp embeddings with alternative position encoding methods (e.g., sinusoidal positional embeddings, numerical encoding) on the same forecasting tasks to isolate the impact of the position embedding strategy.

3. **Long-term forecasting stress test**: Evaluate AutoTimes' performance on extended forecasting horizons (e.g., 100+ steps) to assess the stability and accuracy of autoregressive generation over longer sequences, particularly for detecting any degradation in multi-step predictions.