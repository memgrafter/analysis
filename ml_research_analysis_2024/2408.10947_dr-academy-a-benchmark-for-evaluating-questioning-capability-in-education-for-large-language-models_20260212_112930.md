---
ver: rpa2
title: 'Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education
  for Large Language Models'
arxiv_id: '2408.10947'
source_url: https://arxiv.org/abs/2408.10947
tags:
- llms
- life
- what
- questions
- python
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces Dr.Academy, a benchmark to evaluate large
  language models'' (LLMs) capability to generate educational questions, focusing
  on their role as teachers. Drawing on Anderson and Krathwohl''s taxonomy, the benchmark
  spans six cognitive levels and three domains: general, monodisciplinary, and interdisciplinary.'
---

# Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models

## Quick Facts
- arXiv ID: 2408.10947
- Source URL: https://arxiv.org/abs/2408.10947
- Reference count: 21
- GPT-4 excels in general, humanities, and science domains; Claude2 performs best in interdisciplinary tasks.

## Executive Summary
Dr.Academy is a benchmark designed to evaluate large language models' (LLMs) ability to generate educational questions, positioning them as teachers. Built on Anderson and Krathwohl's taxonomy, it spans six cognitive levels and three domains: general, monodisciplinary, and interdisciplinary. The benchmark uses four metrics—consistency, relevance, coverage, and representativeness—to assess question quality. Experiments with 11 LLMs show GPT-4's strong performance in general and subject-specific domains, while Claude2 excels in interdisciplinary tasks. Automatic evaluation aligns well with human judgment, validating the benchmark's effectiveness.

## Method Summary
The benchmark leverages Anderson and Krathwohl's taxonomy to generate questions at six cognitive levels across general, monodisciplinary, and interdisciplinary domains. Questions are evaluated using four metrics: consistency, relevance, coverage, and representativeness. Automatic scoring via GPT-4 is validated against human evaluations, showing high correlation. The process involves generating contexts from SQuAD and MMLU datasets, prompting LLMs to produce questions, and assessing them through the defined metrics.

## Key Results
- GPT-4 outperforms other models in general, humanities, and science domains.
- Claude2 achieves the highest performance in interdisciplinary question generation.
- Automatic evaluation scores align closely with human assessments, confirming the benchmark's validity.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark leverages Anderson and Krathwohl's taxonomy to ensure questions align with distinct cognitive levels (memory, understanding, application, analysis, evaluation, creation).
- Mechanism: By mapping generated questions to six predefined educational levels, the benchmark creates a structured, theory-grounded way to measure the complexity and pedagogical value of questions.
- Core assumption: Educational taxonomies like Anderson and Krathwohl's reliably capture the cognitive demands of educational questioning.
- Evidence anchors:
  - [abstract] "utilizing Anderson and Krathwohl's taxonomy across general, monodisciplinary, and interdisciplinary domains"
  - [section] "we consider that high-quality questioning in the educational field must meet the following characteristics: i) achieve a higher level across the six domains including memory, understanding, application, analysis, evaluation, and creation"
- Break condition: If the taxonomy is misaligned with actual teaching practices or the cognitive levels are too rigid to capture question nuance, the metric loses validity.

### Mechanism 2
- Claim: The benchmark evaluates questions on four orthogonal dimensions (consistency, relevance, coverage, representativeness), enabling fine-grained quality assessment.
- Mechanism: Each dimension captures a distinct aspect of educational question quality, allowing the benchmark to detect specific strengths and weaknesses of LLMs rather than relying on a single aggregate score.
- Core assumption: These four dimensions are both necessary and sufficient to capture the core qualities of educational questions.
- Evidence anchors:
  - [abstract] "We apply four metrics, including relevance, coverage, representativeness, and consistency, to evaluate the educational quality of LLMs' outputs"
  - [section] "We adopt consistency, relevance, coverage, and representativeness to evaluate LLMs' generated questions in the general and monodisciplinary domains"
- Break condition: If the metrics are poorly calibrated or correlated in ways that mask important differences, they may not provide actionable insight.

### Mechanism 3
- Claim: Using GPT-4 to score questions aligns well with human perspectives, enabling scalable automatic evaluation.
- Mechanism: GPT-4 is prompted to score questions on the same four metrics, and high Pearson/Spearman correlation with human raters validates the approach.
- Core assumption: GPT-4 can reliably emulate human judgment on educational question quality when given clear rubrics.
- Evidence anchors:
  - [section] "The Pearson correlation coefficient reaching 0.947 and Spearman rank correlation reaching 0.870"
  - [section] "automatic and human evaluations for LLMs generally agree, showing a high positive correlation"
- Break condition: If GPT-4's scoring drifts from human standards or if the rubric itself is flawed, automatic scoring may give misleading results.

## Foundational Learning

- Concept: Anderson and Krathwohl's educational taxonomy
  - Why needed here: Provides the theoretical foundation for structuring questions by cognitive complexity.
  - Quick check question: What are the six cognitive levels in Anderson and Krathwohl's taxonomy?

- Concept: Question quality dimensions (consistency, relevance, coverage, representativeness)
  - Why needed here: These four metrics define the criteria used to assess educational question quality in the benchmark.
  - Quick check question: How does "coverage" differ from "representativeness" in the context of educational questions?

- Concept: Automated evaluation alignment with human judgment
  - Why needed here: Ensures that GPT-4 scoring is valid and can replace or augment human evaluation at scale.
  - Quick check question: What statistical measure is used to confirm GPT-4 scoring aligns with human ratings?

## Architecture Onboarding

- Component map: Context generation → LLM question generation → Metric scoring (manual or GPT-4) → Result aggregation and comparison
- Critical path: Generating high-quality contexts → Prompting LLMs to produce questions → Evaluating questions via metrics → Comparing results across models
- Design tradeoffs: Larger context datasets improve coverage but increase cost; automatic scoring is fast but depends on GPT-4's alignment with human judgment
- Failure signatures: Low consistency scores across models suggest the taxonomy or prompt is unclear; poor automatic-human alignment suggests metric definition issues
- First 3 experiments:
  1. Run a pilot with one domain and two models to validate metric rubric clarity.
  2. Compare automatic (GPT-4) vs. human scores for a subset to test alignment.
  3. Stress-test the coverage metric by generating questions from a known exhaustive context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific modifications to the evaluation metrics could enhance the assessment of LLMs' questioning capabilities across diverse educational contexts?
- Basis in paper: [explicit] The paper discusses the four metrics (consistency, relevance, coverage, and representativeness) used to evaluate the questioning capabilities of LLMs. It also mentions the potential for refining these metrics for more nuanced assessments.
- Why unresolved: The current metrics provide a general framework but may not capture the full complexity of effective questioning in varied educational settings. The paper does not explore alternative or additional metrics that could address these limitations.
- What evidence would resolve it: Experimental studies comparing the effectiveness of different sets of metrics in evaluating LLM-generated questions across various educational domains and contexts would provide insights into which modifications could improve the assessment process.

### Open Question 2
- Question: How can the integration of non-textual content, such as images or videos, impact the ability of LLMs to generate high-quality educational questions?
- Basis in paper: [inferred] The paper focuses on text-based question generation and does not explore the potential of incorporating non-textual content. This suggests a limitation in the current approach, as educational materials often include diverse media types.
- Why unresolved: The impact of non-textual content on question generation is not addressed, leaving uncertainty about whether LLMs can effectively utilize such content to enhance educational questioning.
- What evidence would resolve it: Comparative studies evaluating the performance of LLMs in generating questions from text-only versus multimodal content (text, images, videos) would reveal the potential benefits and challenges of integrating non-textual elements into the question generation process.

### Open Question 3
- Question: What role does student feedback play in improving the questioning capabilities of LLMs, and how can this feedback be systematically integrated into the evaluation process?
- Basis in paper: [inferred] The paper does not discuss the role of student feedback in refining LLM-generated questions, indicating a gap in understanding how real-world interactions could enhance question quality.
- Why unresolved: The absence of student feedback in the evaluation process suggests a potential disconnect between the generated questions and their practical effectiveness in educational settings.
- What evidence would resolve it: Studies incorporating student feedback into the iterative development and evaluation of LLM-generated questions would demonstrate the impact of real-world interactions on improving question quality and relevance.

## Limitations

- The evaluation prompts for GPT-4 are not specified, limiting reproducibility and robustness assessment.
- The 50% thresholds for relevance, coverage, and representativeness are arbitrary and lack empirical justification.
- The interdisciplinary domain generation process is insufficiently detailed, raising questions about result reliability.

## Confidence

- **High Confidence**: The overall benchmark design using Anderson and Krathwohl's taxonomy is well-grounded in educational theory, and the finding that GPT-4 excels in general, humanities, and science domains is consistent across multiple evaluation methods.
- **Medium Confidence**: The automatic evaluation approach shows strong correlation with human judgment (Pearson 0.947, Spearman 0.870), but the specific prompts and potential edge cases are not fully explored.
- **Low Confidence**: The arbitrary 50% thresholds for relevance, coverage, and representativeness lack empirical justification, and the interdisciplinary domain generation process is insufficiently detailed.

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically vary the GPT-4 evaluation prompts across a small sample of questions to measure the stability of scoring and identify potential prompt-induced biases.
2. **Threshold Robustness Testing**: Re-run the benchmark analysis using different threshold values (e.g., 40%, 60%) for relevance, coverage, and representativeness to assess the sensitivity of model rankings to these cutoffs.
3. **Interdisciplinary Domain Quality Audit**: Manually review a random sample of interdisciplinary contexts and questions to validate the quality of the domain generation process and ensure meaningful subject integration.