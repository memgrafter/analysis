---
ver: rpa2
title: Are Your LLMs Capable of Stable Reasoning?
arxiv_id: '2412.13147'
source_url: https://arxiv.org/abs/2412.13147
tags:
- g-pass
- reasoning
- qwen2
- llms
- b-instruct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces G-Pass@k, a novel evaluation metric that
  assesses both the potential performance and stability of large language models (LLMs)
  in complex reasoning tasks by measuring the probability of consistently generating
  correct solutions across multiple sampling attempts. Experiments on mathematical
  reasoning benchmarks reveal that current LLMs, including specialized and o1-like
  models, exhibit significant instability when evaluated under stringent correctness
  thresholds, with performance drops of up to 90% in some cases.
---

# Are Your LLMs Capable of Stable Reasoning?

## Quick Facts
- **arXiv ID**: 2412.13147
- **Source URL**: https://arxiv.org/abs/2412.13147
- **Reference count**: 40
- **Key outcome**: Introduces G-Pass@k metric revealing significant reasoning instability in LLMs, with performance drops up to 90% under stringent thresholds.

## Executive Summary
This paper introduces G-Pass@k, a novel evaluation metric that assesses both the potential performance and stability of large language models (LLMs) in complex reasoning tasks by measuring the probability of consistently generating correct solutions across multiple sampling attempts. Experiments on mathematical reasoning benchmarks reveal that current LLMs, including specialized and o1-like models, exhibit significant instability when evaluated under stringent correctness thresholds, with performance drops of up to 90% in some cases. The study finds that merely scaling model size does not consistently improve reasoning stability and that there is a substantial gap between models' theoretical performance potential and their actual stability. Additionally, the research explores factors affecting stability, showing that simple supervised fine-tuning does not significantly enhance reasoning consistency, while backtracking and self-reflection mechanisms in o1-like models contribute to improved stability.

## Method Summary
The paper evaluates reasoning stability using G-Pass@k, which measures the probability that all k attempts yield correct solutions (within threshold τ) for each question. The evaluation uses LiveMathBench and other mathematical reasoning benchmarks, generating n=48 samples per question with various LLMs including Llama, Yi, Gemma, Qwen, DeepSeek-Math, and o1-like models. A judge model (Qwen2.5-72B-Instruct) assesses solution consistency against reference answers. The study compares G-Pass@k with traditional metrics like Greedy Accuracy and Pass@k, and investigates factors affecting stability through hyperparameter sweeps on sampling parameters and analysis of different reasoning mechanisms.

## Key Results
- G-Pass@k reveals that current LLMs show significant reasoning instability, with performance drops up to 90% when evaluated under stringent correctness thresholds
- Scaling model size does not consistently improve reasoning stability across different model families
- o1-like models with backtracking and self-reflection mechanisms demonstrate improved stability compared to standard LLMs
- Supervised fine-tuning alone does not significantly enhance reasoning consistency or stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: G-Pass@k provides a more comprehensive evaluation of model stability than traditional metrics like Pass@k.
- Mechanism: G-Pass@k measures the probability that all k attempts yield correct solutions, or within a threshold τ, while Pass@k only measures if at least one attempt is correct.
- Core assumption: Models generate solutions independently and identically across attempts.
- Evidence anchors:
  - [abstract] "G-Pass@k, a novel evaluation metric that continuously assesses model performance across multiple sampling attempts, quantifying both the model's performance potential and its stability."
  - [section] "G-Pass@k represents the probability of generating at least one correct solution within k attempts for each question, as defined by the formula..."
  - [corpus] Weak or missing; no direct neighbor mentions G-Pass@k specifically.
- Break condition: If model generations are not independent or identically distributed, the hypergeometric approximation fails.

### Mechanism 2
- Claim: Merely increasing model size does not consistently improve reasoning stability.
- Mechanism: Larger models may not address the fundamental reasoning coherence issues that cause instability across samples.
- Core assumption: Reasoning stability is not a direct function of parameter count.
- Evidence anchors:
  - [section] "A comparison of models within the same series... reveals that despite a more than twofold difference in model size, their performance is similar across various metrics..."
  - [corpus] Weak; no neighbor papers directly address model size vs stability.
- Break condition: If scaling introduces architectural changes that improve reasoning consistency.

### Mechanism 3
- Claim: Long CoT reasoning with backtracking and self-reflection improves model stability.
- Mechanism: Extended reasoning paths allow the model to detect and correct errors during generation, unlike linear reasoning.
- Core assumption: Models can effectively backtrack and revise reasoning when given sufficient tokens.
- Evidence anchors:
  - [section] "An intriguing observation is that several o1-like models exhibit considerable robustness to sampled parameters... we believe that long COT reasoning aids the model in rectifying errors introduced by random sampling."
  - [corpus] Weak; no neighbor papers mention backtracking or self-reflection mechanisms.
- Break condition: If backtracking is superficial or doesn't actually correct reasoning errors.

## Foundational Learning

- Concept: Hypergeometric distribution approximation of binomial for sampling without replacement.
  - Why needed here: G-Pass@k uses hypergeometric distribution to estimate stability when sampling without replacement across k attempts.
  - Quick check question: What is the main difference between hypergeometric and binomial distributions in the context of G-Pass@k?
- Concept: Chain-of-Thought (CoT) reasoning.
  - Why needed here: Understanding how CoT models generate extended reasoning paths is critical for interpreting stability differences.
  - Quick check question: How does CoT differ from standard prompting in terms of token generation?
- Concept: Evaluation metrics for generative models (Greedy Accuracy, Pass@k, Majority Voting).
  - Why needed here: Knowing limitations of existing metrics explains why G-Pass@k was introduced.
  - Quick check question: What is the main limitation of Greedy Accuracy in evaluating reasoning stability?

## Architecture Onboarding

- Component map: Data ingestion → Judge pipeline (Qwen2.5-72B) → Metric computation (G-Pass@k) → Result aggregation
- Critical path: Generate n samples per question → Judge consistency with reference answers → Compute G-Pass@k for each τ threshold
- Design tradeoffs: G-Pass@k vs Pass@k (stability vs potential), larger n for accuracy vs inference cost
- Failure signatures: High Pass@k but low G-Pass@k indicates potential without stability; low n causes estimation variance
- First 3 experiments:
  1. Compare G-Pass@k@16τ for τ=0.25,0.5,0.75,1.0 on a small benchmark to see stability degradation
  2. Vary sampling parameters (temperature, top-p) to test metric robustness
  3. Test parallel vs serial reasoning on a general LLM to validate backtracking hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the introduction of backtracking and self-reflection mechanisms in o1-like models consistently improve reasoning stability across different mathematical domains beyond those tested?
- Basis in paper: [explicit] The paper suggests that backtracking and self-reflection in o1-like models contribute to improved stability, but only provides preliminary evidence.
- Why unresolved: The paper's mechanistic analysis is limited to a few examples and doesn't systematically test the impact across diverse mathematical problem types.
- What evidence would resolve it: Controlled experiments comparing o1-like models with and without enhanced backtracking mechanisms across multiple mathematical domains (e.g., algebra, geometry, combinatorics) while measuring G-Pass@k stability metrics.

### Open Question 2
- Question: Is there an optimal threshold τ value for G-Pass@k that balances evaluation of both potential and stability across different model sizes and reasoning capabilities?
- Basis in paper: [inferred] The paper introduces mG-Pass@k as an integrated metric but doesn't explore how different τ ranges affect evaluation outcomes for various model categories.
- Why unresolved: The paper primarily uses τ ∈ [0.5, 1.0] for mG-Pass@k without investigating whether different τ ranges might be more appropriate for different model types or problem difficulties.
- What evidence would resolve it: Systematic analysis of G-Pass@k curves across τ ∈ [0, 1] for various model categories, identifying patterns that suggest optimal τ ranges for different evaluation purposes.

### Open Question 3
- Question: Can supervised fine-tuning on reasoning tasks be optimized to improve both performance and stability simultaneously, rather than trading off between them?
- Basis in paper: [explicit] The paper finds that simple SFT does not significantly enhance reasoning stability and may even increase the performance-stability gap.
- Why unresolved: The paper only tests straightforward SFT approaches without exploring alternative fine-tuning strategies that might better preserve stability while improving performance.
- What evidence would resolve it: Comparative experiments testing different fine-tuning objectives (e.g., stability-aware loss functions, multi-task learning) that explicitly penalize performance-stability gaps while maintaining or improving accuracy.

### Open Question 4
- Question: What is the relationship between model scale and the effectiveness of test-time scaling methods like chain-of-thought reasoning on stability?
- Basis in paper: [explicit] The paper observes that merely scaling model size does not consistently enhance stable reasoning capabilities, suggesting base models have untapped potential.
- Why unresolved: The paper doesn't systematically investigate how different model scales respond to test-time scaling methods or whether smaller models might benefit more from such approaches.
- What evidence would resolve it: Experiments measuring G-Pass@k stability across various model sizes while applying different test-time scaling techniques (temperature, top-p, CoT length) to identify scaling relationships and optimal configurations.

## Limitations

- **Metric Generalization**: G-Pass@k's effectiveness depends heavily on the independence assumption between sampling attempts, which isn't empirically validated across different model architectures.
- **Benchmark Representativeness**: The newly constructed LiveMathBench may not fully represent the diversity of real-world reasoning tasks beyond mathematical domains.
- **Sample Size Requirements**: The choice of n=48 generations per question creates significant computational overhead without thorough investigation of how smaller sample sizes affect metric reliability.

## Confidence

**High Confidence** (Well-supported by evidence):
- G-Pass@k is a valid and useful metric for measuring reasoning stability beyond traditional Pass@k
- Current LLMs show significant instability in reasoning tasks when evaluated under stringent correctness thresholds
- Scaling model size alone does not consistently improve reasoning stability

**Medium Confidence** (Reasonable but needs more validation):
- The gap between theoretical performance potential and actual stability is substantial across models
- Long CoT reasoning with backtracking mechanisms improves stability, particularly in o1-like models
- Supervised fine-tuning alone doesn't significantly enhance reasoning consistency

**Low Confidence** (Emerging or under-explored):
- The exact mechanisms by which backtracking and self-reflection improve stability in o1-like models
- The relationship between sampling parameters and metric reliability across different model families
- Generalizability of findings beyond mathematical reasoning tasks

## Next Checks

1. **Independence Assumption Validation**: Design an experiment to test the independence assumption underlying G-Pass@k by measuring correlation between successive generations from the same model on identical prompts. Use different sampling temperatures and top-p values to see if independence varies with sampling strategy.

2. **Cross-Domain Stability Testing**: Apply G-Pass@k to non-mathematical reasoning benchmarks (e.g., commonsense reasoning, code generation, or multi-step decision making) to validate whether the stability patterns observed in mathematical reasoning generalize to other reasoning domains.

3. **Sample Size Sensitivity Analysis**: Systematically vary n from 4 to 48 generations per question on a subset of benchmarks to determine the minimum sample size required for G-Pass@k to provide stable and reliable measurements. Include confidence interval analysis for each n value.