---
ver: rpa2
title: 'StatBot.Swiss: Bilingual Open Data Exploration in Natural Language'
arxiv_id: '2406.03170'
source_url: https://arxiv.org/abs/2406.03170
tags:
- language
- dataset
- text-to-sql
- swiss
- citizenship
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces StatBot.Swiss, the first bilingual (English-German)
  Text-to-SQL benchmark based on real-world Swiss open government data. The dataset
  contains 455 complex NL/SQL-pairs across 35 large databases totaling 7.5 GB.
---

# StatBot.Swiss: Bilingual Open Data Exploration in Natural Language

## Quick Facts
- arXiv ID: 2406.03170
- Source URL: https://arxiv.org/abs/2406.03170
- Reference count: 40
- First bilingual Text-to-SQL benchmark based on real Swiss open government data, achieving up to 50.58% partial execution accuracy with GPT-3.5-Turbo and Mixtral-8x7B-Instruct

## Executive Summary
This work introduces StatBot.Swiss, the first bilingual (English-German) Text-to-SQL benchmark based on real-world Swiss open government data. The dataset contains 455 complex NL/SQL-pairs across 35 large databases totaling 7.5 GB. The authors evaluate GPT-3.5-Turbo and Mixtral-8x7B-Instruct using in-context learning strategies, finding that current LLMs struggle with complex queries, domain knowledge inference, and handling NULL values. The analysis reveals that German questions achieve higher accuracy than English ones despite being more linguistically diverse, attributed to native speaker curation.

## Method Summary
The authors created StatBot.Swiss by preparing 35 PostgreSQL databases from Swiss open government data, curating 455 NL/SQL pairs split 70% train/30% dev, and implementing in-context learning with zero-shot and few-shot strategies. They used similarity-based or random exemplar selection and evaluated using three accuracy metrics (strict, soft, partial) on GPT-3.5-Turbo-16k and Mixtral-8x7B-Instruct. The evaluation compared model performance across English and German subsets, analyzing query complexity and linguistic diversity through metrics like type-token ratio.

## Key Results
- StatBot.Swiss achieves up to 50.58% partial execution accuracy, with current LLMs struggling on complex queries
- German questions achieve higher accuracy than English despite greater linguistic diversity (TTR 0.961 vs 0.927)
- Similarity-based exemplar selection outperforms random selection in few-shot learning scenarios
- Models struggle with multi-column GROUP BY, nested SELECTs, numeric operations, and domain-specific knowledge inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: German NL questions achieve higher accuracy than English ones due to native speaker curation.
- Mechanism: Native speakers craft questions that align more naturally with database semantics, reducing linguistic ambiguity and improving model inference.
- Core assumption: Linguistic diversity and quality of natural language questions directly impact model performance.
- Evidence anchors: Abstract states German questions achieve higher accuracy due to native speaker curation; TTR analysis shows German questions are more linguistically diverse (0.961 vs 0.927).
- Break condition: If the dataset is re-annotated by non-native speakers or the linguistic diversity decreases, the accuracy gap may diminish or reverse.

### Mechanism 2
- Claim: Similarity-based exemplar selection outperforms random selection in few-shot learning scenarios.
- Mechanism: By selecting examples most similar to the target question, the model receives more relevant context, enhancing its ability to generate accurate SQL queries.
- Core assumption: The similarity between the exemplar and the target question is a good proxy for relevance and usefulness in the context of in-context learning.
- Evidence anchors: Abstract notes current LLMs struggle to generalize well; experimental results show similarity selection generally outperforms random selection.
- Break condition: If the similarity metric fails to capture semantic relevance or if the training examples are too sparse, the benefit of similarity-based selection may not materialize.

### Mechanism 3
- Claim: Enhanced in-context learning strategies incorporating broader Text-to-SQL knowledge improve model performance.
- Mechanism: Providing more comprehensive database metadata and context helps the model understand complex queries and domain-specific nuances.
- Core assumption: Models benefit from richer contextual information that goes beyond basic schema details.
- Evidence anchors: The paper uses textual representation of database information including specific data type information and foreign key constraint details; corpus statistics show average neighbor FMR=0.478.
- Break condition: If the additional context overwhelms the model or if the metadata is not accurately represented, the performance may not improve.

## Foundational Learning

- Concept: Text-to-SQL translation
  - Why needed here: Understanding the mapping from natural language to SQL is essential for developing effective benchmarks and evaluating model performance.
  - Quick check question: What are the key components of a Text-to-SQL system?

- Concept: In-context learning (ICL)
  - Why needed here: ICL allows models to perform tasks with minimal training data by providing examples within the prompt.
  - Quick check question: How does ICL differ from traditional supervised learning?

- Concept: Bilingual datasets
  - Why needed here: Bilingual datasets enable the evaluation of models across different languages, highlighting potential biases and performance disparities.
  - Quick check question: Why is it important to include multiple languages in a Text-to-SQL benchmark?

## Architecture Onboarding

- Component map: Database preparation -> Dataset curation -> Text-to-SQL translation -> Evaluation
- Critical path: Prepare databases → Curate dataset → Translate NL to SQL → Evaluate accuracy
- Design tradeoffs: Balancing query complexity with dataset size, choosing between zero-shot and few-shot learning, deciding granularity of database metadata
- Failure signatures: Low execution accuracy, inability to handle complex queries, language performance discrepancies
- First 3 experiments:
  1. Evaluate model performance on simple queries in both languages
  2. Test impact of similarity-based exemplar selection versus random selection
  3. Assess model's ability to handle queries requiring domain-specific knowledge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model performance vary when training examples are selected based on semantic similarity versus syntactic similarity to the target question?
- Basis in paper: [explicit] The paper compares random selection with similarity-based selection using cosine similarity scores, but only considers semantic similarity via multilingual sentence transformers
- Why unresolved: The paper does not explore whether syntactic structure matching might be more effective than semantic similarity for selecting training examples
- What evidence would resolve it: Experiments comparing similarity-based selection using syntactic features (e.g., SQL structure patterns) versus semantic features would reveal which approach yields better performance

### Open Question 2
- Question: Does the performance gap between German and English questions persist when both are generated by non-native speakers?
- Basis in paper: [inferred] The paper notes German questions achieve higher accuracy despite being more linguistically diverse, attributing this to native speaker curation versus non-native English questions
- Why unresolved: The paper only compares native German speakers to non-native English speakers, leaving open whether linguistic diversity or native speaker status drives the performance difference
- What evidence would resolve it: Creating parallel datasets with both languages written by non-native speakers would isolate whether linguistic diversity or native speaker status is the primary factor

### Open Question 3
- Question: What is the optimal number of training examples to include in prompts across different query complexity levels?
- Basis in paper: [explicit] The paper finds performance peaks at 5 examples for GPT-3.5 and 6 for Mixtral but does not analyze whether optimal example count varies by query difficulty
- Why unresolved: The analysis treats all queries uniformly without examining whether complex queries benefit from more or fewer examples than simple queries
- What evidence would resolve it: Experiments varying example count specifically for each hardness level (easy, medium, hard, extra, unknown) would reveal whether different query types require different training example amounts

### Open Question 4
- Question: How does incorporating database documentation and metadata affect model performance compared to schema-only representation?
- Basis in paper: [inferred] The paper mentions schema augmentation through metadata but does not experimentally compare this to basic schema representation
- Why unresolved: While the paper notes that metadata inclusion is crucial, it does not measure the performance difference between prompts with and without rich metadata
- What evidence would resolve it: Controlled experiments comparing model performance with basic CREATE TABLE statements versus prompts enriched with column descriptions, value ranges, and other metadata would quantify the impact of documentation

## Limitations

- Small sample size (455 NL/SQL pairs) may not generalize to broader bilingual Text-to-SQL performance conclusions
- Dataset characteristics specific to Swiss open government data may not represent other domains or languages
- Evaluation methodology relies on execution accuracy which may miss semantic nuances in complex queries

## Confidence

**High Confidence Claims**:
- StatBot.Swiss is the first bilingual Text-to-SQL benchmark based on real Swiss open government data (supported by clear methodology)
- Current LLMs show significant performance gaps between languages (supported by empirical results showing 50.58% partial execution accuracy)

**Medium Confidence Claims**:
- German questions achieve higher accuracy due to native speaker curation (supported by linguistic diversity metrics but limited by sample size)
- Similarity-based exemplar selection outperforms random selection in few-shot learning (supported by comparative results but not extensively validated)

**Low Confidence Claims**:
- Enhanced in-context learning strategies will significantly improve performance (based on limited experimental variations)
- Importance of considering user intent over exact matching in evaluation (based on qualitative observations without rigorous semantic analysis)

## Next Checks

1. **Dataset Expansion Validation**: Replicate the study using a 2-3x larger dataset with more balanced language distribution and additional languages to verify if the German accuracy advantage persists and whether the 50.58% partial execution accuracy remains consistent across different dataset sizes.

2. **Model Architecture Impact Study**: Evaluate the same benchmark using additional LLM architectures (including open-source models like LLaMA, Claude, and newer GPT versions) to determine if performance patterns are consistent across different model families or specific to the tested models.

3. **Semantic Intent Alignment Analysis**: Develop and apply a semantic intent alignment metric to quantify the gap between user intent and generated SQL, comparing this approach against traditional execution accuracy metrics to validate the claim that user intent consideration is crucial for meaningful evaluation.