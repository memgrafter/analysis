---
ver: rpa2
title: Navigating the Effect of Parametrization for Dimensionality Reduction
arxiv_id: '2411.15894'
source_url: https://arxiv.org/abs/2411.15894
tags:
- data
- parametric
- should
- structure
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Parametric dimensionality reduction methods often fail to preserve
  local structure compared to non-parametric methods. The authors identify insufficient
  repulsive forces on negative pairs as the key issue and show that methods using
  Negative Sampling (NEG) loss adapt better to parametrization than those using NCE-style
  losses.
---

# Navigating the Effect of Parametrization for Dimensionality Reduction

## Quick Facts
- arXiv ID: 2411.15894
- Source URL: https://arxiv.org/abs/2411.15894
- Reference count: 40
- Parametric dimensionality reduction methods often fail to preserve local structure compared to non-parametric methods.

## Executive Summary
Parametric dimensionality reduction methods frequently struggle to preserve local structure in data, a limitation that has been observed across various algorithms like parametric t-SNE, parametric UMAP, and parametric PaCMAP. This paper identifies insufficient repulsive forces on negative pairs as the key issue and demonstrates that methods using Negative Sampling (NEG) loss adapt better to parametrization than those using NCE-style losses. To address these problems, the authors propose ParamRepulsor, which employs hard negative mining and an enhanced repulsive force to improve separation between clusters. ParamRepulsor achieves state-of-the-art performance in local structure preservation among parametric methods while maintaining global structure quality.

## Method Summary
ParamRepulsor is a parametric dimensionality reduction method that uses a neural network projector with hard negative mining and an enhanced repulsive force. The method samples mid-near pairs as hard negatives and applies a contrastive loss with stronger repulsive forces on these pairs. It's trained using mini-batch SGD with the Adam optimizer on datasets including MNIST, Fashion-MNIST, USPS, COIL-20, COIL-100, 20 Newsgroups, and various scRNA-seq datasets.

## Key Results
- Parametric methods using NEG-style losses preserve local structure better than those using NCE-style losses
- ParamRepulsor achieves state-of-the-art local structure preservation among parametric methods
- The enhanced repulsive force on hard negative pairs significantly improves cluster separation

## Why This Works (Mechanism)

### Mechanism 1
Parametric dimensionality reduction methods lose local structure because they lack sufficient repulsive forces on negative pairs. The neural network projector's smoothness and limited capacity cause gradients for negative pairs to be smaller than needed for proper cluster separation.

### Mechanism 2
NEG loss functions adapt better to parametrization than NCE-style losses because NEG treats each negative pair independently, while NCE-style losses include terms that depend on all negative pairs in a batch, diluting gradient magnitude.

### Mechanism 3
Hard Negative Mining using mid-near pairs reduces false negatives and provides better gradients for local structure preservation. The mid-near sampling strategy makes it unlikely that these pairs are actually similar while ensuring they are challenging negatives that provide strong gradients for separation.

## Foundational Learning

- **Neighborhood Embedding (NE) algorithms and their two-phase optimization**
  - Why needed: Understanding the basic structure of NE algorithms is essential to grasp why parametrization affects local structure preservation
  - Quick check: What are the two main phases of NE algorithms, and what is the purpose of each phase?

- **Loss functions in contrastive learning (NEG vs NCE/InfoNCE)**
  - Why needed: The choice of loss function is a key factor in how well parametric methods preserve local structure
  - Quick check: How do NEG loss and NCE-style losses differ in their treatment of negative pairs, and why does this matter for parametric methods?

- **Hard Negative Mining in contrastive learning**
  - Why needed: Hard Negative Mining is the core innovation in ParamRepulsor for improving local structure preservation
  - Quick check: What is the challenge of Hard Negative Mining in unsupervised settings, and how does the mid-near sampling strategy address this challenge?

## Architecture Onboarding

- **Component map:**
  - High-dimensional data X -> Neural network projector fθ with parameter θ -> Low-dimensional embedding Y = fθ(X) -> Similarity construction (K-NN graph) -> Loss function with terms for nearest neighbors and negative pairs -> Optimization via mini-batch SGD (Adam)

- **Critical path:**
  1. Preprocess data and construct K-NN graph
  2. Initialize neural network projector with Kaiming initialization
  3. For each epoch:
     - Sample mini-batch
     - Find nearest neighbors, mid-near pairs, and random negatives for each point
     - Compute embeddings for all points in batch
     - Calculate loss with enhanced repulsive forces on mid-near pairs
     - Update parameters using Adam optimizer
  4. Return trained neural network projector

- **Design tradeoffs:**
  - Network depth vs. local structure preservation: Adding layers beyond 3 provides diminishing returns
  - Number of nearest neighbors in K-NN graph: Unlike non-parametric methods, this has minimal effect on parametric embeddings
  - Batch size vs. computational efficiency: Larger batches provide more negative samples but require more memory
  - Mid-near sampling pool size (h=6): Balances computational efficiency with quality of hard negatives

- **Failure signatures:**
  - Blurred cluster boundaries in visualizations
  - Lower k-NN accuracy and SVM accuracy on labeled datasets
  - Higher ratio of mean FP distance to NN distance in embeddings
  - Reduced triplet preservation ratio

- **First 3 experiments:**
  1. Run ParamRepulsor on MNIST with default parameters and visualize the embedding to check for clear cluster boundaries
  2. Compare 10-NN accuracy of ParamRepulsor vs. ParamUMAP and ParamPaCMAP on MNIST
  3. Measure the ratio of mean FP distance to NN distance for ParamRepulsor vs. other parametric methods on MNIST

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of neural network architecture (beyond depth) impact the performance of parametric dimensionality reduction methods?
- Basis in paper: [inferred] The paper focuses on depth (number of layers) but mentions that other architectures (e.g., residual connections, convolutional networks) are possible
- Why unresolved: The paper only tests shallow MLPs with fixed layer sizes, leaving the impact of other architectural choices unexplored
- What evidence would resolve it: Comparative experiments testing different architectures (e.g., CNNs, ResNets) on the same datasets while keeping depth constant

### Open Question 2
- Question: What is the optimal trade-off between local and global structure preservation in parametric methods for different dataset characteristics?
- Basis in paper: [explicit] The paper mentions that ParamRepulsor achieves state-of-the-art performance in local structure while maintaining global structure, but does not explore dataset-specific optimal trade-offs
- Why unresolved: The paper uses a fixed set of hyperparameters across all datasets without exploring dataset-specific tuning
- What evidence would resolve it: Systematic hyperparameter sweeps across datasets with varying cluster densities, manifold complexities, and dimensionalities

### Open Question 3
- Question: How does the computational complexity of ParamRepulsor scale with dataset size compared to other parametric methods?
- Basis in paper: [explicit] The paper mentions that ParamRepulsor is slower than ParamInfo-NC-t-SNE but does not provide detailed scaling analysis
- Why unresolved: The paper only compares on two large datasets without examining how time complexity scales with n
- What evidence would resolve it: Empirical time complexity analysis on datasets of varying sizes, including asymptotic scaling plots and memory usage measurements

## Limitations

- The paper provides limited theoretical justification for why neural network projectors specifically fail to generate sufficient repulsive forces for negative pairs
- The generalizability of results to very large datasets (>100k samples) and other distance metrics beyond Euclidean is not thoroughly explored
- The paper doesn't explore alternative architectures that might mitigate the parametric methods' limitations

## Confidence

**Confidence Labels:**
- **High**: The experimental results showing parametric methods' inferior local structure preservation compared to non-parametric methods
- **Medium**: The identification of insufficient repulsive forces as the primary cause of parametric methods' limitations
- **Medium**: The claim that NEG-style losses adapt better to parametrization than NCE-style losses
- **Medium**: The effectiveness of hard negative mining using mid-near pairs for improving local structure preservation

## Next Checks

1. Replicate the experiment comparing k-NN accuracy across ParamRepulsor, ParamUMAP, and ParamPaCMAP on at least two additional datasets not mentioned in the paper
2. Test ParamRepulsor with different neural network architectures (deeper networks, residual connections) to verify the claimed diminishing returns of network depth
3. Implement a variant of ParamRepulsor using NCE-style loss instead of NEG-style loss to empirically validate the claimed advantage of NEG-style losses for parametric methods