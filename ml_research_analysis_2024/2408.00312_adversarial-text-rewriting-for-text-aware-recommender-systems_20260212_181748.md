---
ver: rpa2
title: Adversarial Text Rewriting for Text-aware Recommender Systems
arxiv_id: '2408.00312'
source_url: https://arxiv.org/abs/2408.00312
tags:
- text
- recommender
- items
- atr-2ft
- descriptions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the vulnerability of text-aware recommender
  systems to adversarial product description rewriting attacks. The authors propose
  a framework called ATR (Adversarial Text Rewriting) that employs two strategies:
  ATR-2FT, which uses a two-phase fine-tuning process to optimize text generation
  for ranking performance, and ATR-ICL, which uses in-context learning with large
  language models to generate fluent, rank-boosting descriptions.'
---

# Adversarial Text Rewriting for Text-aware Recommender Systems

## Quick Facts
- arXiv ID: 2408.00312
- Source URL: https://arxiv.org/abs/2408.00312
- Authors: Sejoon Oh; Gaurav Verma; Srijan Kumar
- Reference count: 40
- Key outcome: ATR framework significantly increases predicted ranks of target items (5.4%-28.5% relative improvements) through adversarial text rewriting attacks on text-aware recommender systems

## Executive Summary
This paper investigates the vulnerability of text-aware recommender systems to adversarial product description rewriting attacks. The authors propose ATR (Adversarial Text Rewriting), a framework employing two strategies: ATR-2FT uses two-phase fine-tuning to optimize text generation for ranking performance, while ATR-ICL uses in-context learning with large language models to generate fluent, rank-boosting descriptions. Experiments on three datasets (Amazon Book, Amazon Electronics, and MovieLens) and four recommender systems show that ATR significantly increases the predicted ranks of target items compared to baselines, demonstrating that text-aware recommenders are susceptible to manipulation through adversarial text rewriting.

## Method Summary
The ATR framework attacks text-aware recommender systems by rewriting product descriptions to boost target items' predicted rankings. ATR-2FT employs two-phase fine-tuning: Phase 1 adapts a language model to the dataset's text domain, while Phase 2 jointly optimizes fluency and rank-boosting objectives through multi-objective fine-tuning. ATR-ICL uses in-context learning with carefully crafted prompts containing few adversarial examples to guide large language models in generating realistic, ranking-optimized descriptions without fine-tuning. The framework operates in both white-box settings (with gradient access) and black-box settings (using surrogate models to approximate target recommenders).

## Key Results
- ATR significantly increases average predicted ranks of target items compared to baselines, with relative improvements ranging from 5.4% to 28.5%
- ATR-2FT outperforms ATR-ICL in attack effectiveness due to direct optimization of ranking objectives during fine-tuning
- Black-box attacks using surrogate models achieve comparable performance to white-box attacks, demonstrating practical feasibility
- Generated text maintains high quality metrics (cosine similarity, perplexity, METEOR, BertScore) while successfully boosting rankings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-aware recommender systems rely on item descriptions as key input features, making them vulnerable to adversarial rewriting.
- Mechanism: By rewriting product descriptions to boost predicted rankings, attackers can promote target items across all users without changing the model or injecting fake profiles.
- Core assumption: Item descriptions significantly influence the model's predicted scores for users.
- Evidence anchors:
  - [abstract]: "Text-aware recommender systems incorporate rich textual features...The use of textual features helps mitigate cold-start problems..."
  - [section]: "Recent recommender systems have increasingly leveraged textual product descriptions as an important input feature..."
- Break condition: If the recommender model downweights text features relative to other signals, or if text embeddings are normalized to minimize rank shifts.

### Mechanism 2
- Claim: Two-phase fine-tuning (ATR-2FT) optimizes both domain adaptation and rank promotion simultaneously.
- Mechanism: Phase 1 adapts the language model to the dataset's text domain; Phase 2 jointly optimizes fluency and rank-boosting objectives.
- Core assumption: Fine-tuning on the same dataset that the recommender was trained on preserves semantic coherence while allowing rank optimization.
- Evidence anchors:
  - [section]: "In Phase 2, we fine-tune the text generation model to produce optimal descriptions for target items via multi-objective fine-tuning..."
  - [section]: "ATR-2FT can fine-tune a text generation model to produce ranking-optimized item descriptions (due to Lð‘ð‘Ÿð‘œð‘šð‘œð‘¡ð‘–ð‘œð‘›), while maintaining the fluency and semantic meaning (due to Lð‘¡ð‘’ð‘¥ð‘¡ âˆ’ð‘”ð‘’ð‘›.)"
- Break condition: If the text domain is too broad or the dataset too small, fine-tuning may not learn effective rank-boosting patterns.

### Mechanism 3
- Claim: In-context learning (ATR-ICL) leverages large language models without fine-tuning to generate fluent, rank-boosting text.
- Mechanism: Carefully crafted prompts with few adversarial examples guide the LLM to rewrite descriptions that are both realistic and ranking-optimized.
- Core assumption: LLMs can generalize from a small number of high-quality rewritten examples to produce new effective rewrites.
- Evidence anchors:
  - [section]: "The ICL consists of specific instructions for the language model and few examples related to the instructions..."
  - [section]: "Compared to the ATR-2FT, ATR-ICL produces more fluent and realistic text as it utilizes LLMs..."
- Break condition: If the LLM filters out adversarial examples or the prompt design fails to convey the rank-boosting goal.

## Foundational Learning

- Concept: Text embeddings and their role in recommender scoring
  - Why needed here: The attack manipulates text embeddings to change ranking predictions.
  - Quick check question: How does changing a product description affect the computed text embedding and subsequent ranking score?

- Concept: Gradient-based optimization and black-box model surrogation
  - Why needed here: ATR-2FT uses rank promotion loss requiring gradients; in black-box settings, a surrogate model is trained to mimic the target.
  - Quick check question: Why can't the attacker directly compute gradients from the black-box recommender?

- Concept: In-context learning and prompt engineering
  - Why needed here: ATR-ICL relies on LLMs responding to carefully structured prompts with examples.
  - Quick check question: What are the key components of a prompt that will produce ranking-optimized rewrites without fine-tuning?

## Architecture Onboarding

- Component map:
  - Text-aware recommender system (M) -> Language model (POINTER, OPT, Llama-2) -> ATR-2FT pipeline (Phase 1 + Phase 2 fine-tuning) -> ATR-ICL pipeline (prompt construction + LLM inference) -> Surrogate model trainer (for black-box attacks) -> Evaluation metrics (average predicted rank, Appear@20, text quality)

- Critical path:
  1. Load dataset and train/extract recommender model M
  2. Fine-tune language model via ATR-2FT (Phase 1 â†’ Phase 2) or prepare ATR-ICL prompts
  3. Generate rewritten descriptions for target items
  4. Evaluate ranking impact and text quality

- Design tradeoffs:
  - Fine-tuning (ATR-2FT) vs. in-context learning (ATR-ICL): more effective but costlier vs. cheaper but slightly less effective
  - Hard-constrained vs. soft-constrained text generation: keyword preservation vs. fluency
  - White-box vs. black-box: direct gradient access vs. surrogate model overhead

- Failure signatures:
  - No rank improvement: rank promotion loss too weak or surrogate inaccurate
  - Hallucinated text: loss of semantic coherence in generation
  - Low fluency: poor prompt design or insufficient fine-tuning

- First 3 experiments:
  1. Run ATR-2FT Phase 1 only on a small subset; verify domain adaptation by checking generated text coherence.
  2. Test ATR-ICL with 5 vs. 10 examples in prompt; measure effect on text quality and rank boost.
  3. Compare black-box vs. white-box rank improvements on HybridMF with OPT model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are text-aware recommender systems against more sophisticated adversarial attacks that combine text rewriting with other modalities like images or pricing?
- Basis in paper: [inferred] The paper notes this as a limitation, stating "ATR can be sub-optimal for multimodal recommenders (e.g., using text+image)."
- Why unresolved: The study focuses solely on text-based attacks, leaving the vulnerability to multimodal attacks unexplored.
- What evidence would resolve it: Experiments testing attacks that simultaneously manipulate text descriptions, product images, and pricing on multimodal recommender systems.

### Open Question 2
- Question: What are the long-term effects of adversarial text rewriting attacks on recommendation systems' overall accuracy and user trust?
- Basis in paper: [explicit] The authors acknowledge that attack impacts on overall recommendation accuracy are "insignificant" in short-term experiments but don't explore longer-term effects.
- Why unresolved: The study uses short-term evaluation metrics and doesn't assess how repeated attacks or user awareness might affect system performance over time.
- What evidence would resolve it: Longitudinal studies tracking recommendation accuracy, user engagement, and trust metrics over extended periods with simulated attack scenarios.

### Open Question 3
- Question: How can recommender systems be defended against text rewriting attacks while maintaining the utility of text features for legitimate purposes?
- Basis in paper: [explicit] The authors suggest adversarial training with rewritten text as a potential defense mechanism but don't explore its effectiveness.
- Why unresolved: The study focuses on attack methods without developing or testing comprehensive defense strategies.
- What evidence would resolve it: Comparative evaluation of different defense mechanisms (adversarial training, text verification systems, user feedback loops) against various attack scenarios.

## Limitations
- Attack effectiveness depends on access to white-box gradients or trainable surrogate models in black-box settings
- Framework assumes availability of large language models, which may not be accessible in all scenarios
- Evaluation focuses on ranking metrics without extensive user-level validation of promoted item relevance
- Study does not address potential defenses or mitigation strategies against such attacks

## Confidence
- High Confidence: The vulnerability of text-aware recommenders to adversarial text rewriting is well-established through systematic experiments across multiple datasets and model architectures
- Medium Confidence: The relative effectiveness of ATR-2FT versus ATR-ICL approaches is supported by experimental results, though trade-offs could benefit from more extensive ablation studies
- Medium Confidence: The generalizability of attack success across different recommender architectures is shown, but not all possible variants or real-world deployment scenarios are explored

## Next Checks
1. Test attack effectiveness on a larger, more diverse set of text-aware recommender architectures, including those using different text embedding methods (e.g., BERT, RoBERTa) and ranking objectives
2. Evaluate the persistence of attack effects over time by simulating dynamic user-item interactions and measuring how long promoted items maintain their boosted ranks
3. Investigate the minimum number of adversarial examples required in ATR-ICL prompts to achieve comparable performance to ATR-2FT, establishing practical cost-benefit trade-offs