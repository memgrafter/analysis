---
ver: rpa2
title: 3D Wasserstein generative adversarial network with dense U-Net based discriminator
  for preclinical fMRI denoising
arxiv_id: '2411.19345'
source_url: https://arxiv.org/abs/2411.19345
tags:
- fmri
- denoising
- data
- discriminator
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to denoising preclinical fMRI
  data using a 3D Wasserstein generative adversarial network with a dense U-Net based
  discriminator (3D U-WGAN). The method addresses the challenge of denoising preclinical
  fMRI data, which is inherently noisy due to physiological processes, hardware limitations,
  and external interferences.
---

# 3D Wasserstein generative adversarial network with dense U-Net based discriminator for preclinical fMRI denoising

## Quick Facts
- arXiv ID: 2411.19345
- Source URL: https://arxiv.org/abs/2411.19345
- Reference count: 37
- Achieves PSNR of 35.08 dB and SSIM of 0.895 on CTNI rsfMRI dataset, outperforming existing denoising methods

## Executive Summary
This paper introduces a 3D Wasserstein generative adversarial network with a dense U-Net based discriminator (3D U-WGAN) for denoising preclinical fMRI data. The method addresses the challenge of denoising 4D fMRI data by capturing both temporal and spatial information through a 3D dense U-Net discriminator within the WGAN framework. The model is trained using a combination of mean squared error, perceptual loss, and discriminator loss to generate high-quality denoised fMRI data. Experimental results demonstrate superior performance compared to existing state-of-the-art techniques in terms of image quality and structural preservation, with significant improvements in PSNR and SSIM metrics.

## Method Summary
The proposed 3D U-WGAN employs a 3D dense U-Net based discriminator to learn both global and local distinctions in fMRI data. The generator follows an encoder-decoder U-Net architecture that processes 4D fMRI data organized in either time-based or slice-based configurations. The model is trained using Adam optimizer with a combination of MSE loss for pixel-level accuracy, perceptual loss for preserving high-level features via VGG features, and adversarial loss from the discriminator. The Wasserstein loss with gradient penalty stabilizes training and improves image quality. The method is evaluated on CTNI preclinical fMRI dataset with 108 subjects and simulated data with Rician noise, demonstrating superior denoising performance compared to BM4D, DU-GAN, and RIRGAN baselines.

## Key Results
- Achieves PSNR of 35.08 dB and SSIM of 0.895 on CTNI rsfMRI dataset
- Outperforms BM4D, DU-GAN, and RIRGAN in both image quality and structural preservation
- Effectively restores activation patterns in simulated fMRI data while minimizing false positives
- Demonstrates superior performance on both resting-state and task-based fMRI analyses

## Why This Works (Mechanism)

### Mechanism 1
The dense U-Net discriminator provides both global and local feedback, improving denoising performance. The encoder captures global structural context while the decoder reconstructs per-pixel confidence maps, allowing the generator to learn from both broad patterns and fine details simultaneously. Core assumption: U-Net's skip connections preserve spatial resolution while learning hierarchical features, and the dense interconnections improve gradient flow. Evidence anchors: abstract states "Our method employs a 3D dense U-Net discriminator to learn both global and local distinctions" and section describes encoder-decoder roles. Break condition: If skip connections fail to maintain spatial correspondence between layers, local feedback becomes noisy and misleading.

### Mechanism 2
The Wasserstein loss with gradient penalty stabilizes training and improves image quality. WGAN replaces traditional GAN loss with Earth Mover's distance approximation, providing smoother gradients and better convergence properties. Core assumption: Wasserstein distance is a more meaningful measure of distribution similarity for high-dimensional image data than Jensen-Shannon divergence. Evidence anchors: section states "we adopt the improved version of WGAN with a gradient penalty to accelerate the convergence instead of conventional GANs." Break condition: If gradient penalty weight is mis-tuned, discriminator may collapse or fail to provide useful gradients.

### Mechanism 3
The combination of MSE, perceptual, and adversarial losses prevents over-smoothing while preserving structure. MSE ensures pixel-level accuracy, perceptual loss preserves high-level features via VGG features, and adversarial loss encourages realistic texture and detail. Core assumption: Different loss functions operate on complementary aspects of image quality and their combination addresses individual weaknesses. Evidence anchors: section mentions applying "adversarial loss related to the discriminator is applied in combination with mean-squared error (MSE) loss and perceptual loss" and notes MSE may result in loss of fine details. Break condition: If relative weights between losses are poorly balanced, network may overfit to one type of loss at expense of others.

## Foundational Learning

- Concept: Wasserstein GAN and gradient penalty
  - Why needed here: Provides stable training dynamics for high-dimensional fMRI data with complex noise patterns
  - Quick check question: What is the key difference between WGAN and standard GAN loss functions?

- Concept: 3D convolutional neural networks
  - Why needed here: fMRI data is 4D (3 spatial + time), requiring 3D convolutions to capture volumetric and temporal information
  - Quick check question: How does a 3D convolution differ from a 2D convolution in terms of input dimensions and kernel shape?

- Concept: Perceptual loss and feature extraction
  - Why needed here: MSE alone leads to over-smoothing; perceptual loss preserves texture and structural details
  - Quick check question: Which layer of VGG is typically used for perceptual loss and why?

## Architecture Onboarding

- Component map: Input → Generator (encoder-decoder U-Net) → Discriminator (3D dense U-Net) → Loss computation → Parameter update → Output
- Critical path: Input → Generator → Discriminator feedback → Loss computation → Parameter update → Output
- Design tradeoffs: Dense U-Net increases parameter count and memory usage vs standard U-Net; time-based vs slice-based data configuration affects temporal vs spatial emphasis
- Failure signatures: Training instability (discriminator loss spikes), Mode collapse (generated images become repetitive), Over-smoothing (high PSNR but low SSIM)
- First 3 experiments:
  1. Test time-based vs slice-based configuration on validation set to determine optimal 4D data arrangement
  2. Ablation study removing perceptual loss to quantify its contribution to structural preservation
  3. Hyperparameter sweep for loss weights (λMSE, λPerceptual, λDiscriminator) to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of 3D U-WGAN compare when applied to other preclinical imaging modalities such as ultrasound or optical imaging, where noise characteristics differ significantly from fMRI? Basis: paper focuses on preclinical fMRI denoising and demonstrates superior performance compared to other methods on fMRI data, but does not explore its application to other imaging modalities. Why unresolved: paper's scope is limited to fMRI data, and extending method to other modalities would require additional experiments. What evidence would resolve it: Comparative studies applying 3D U-WGAN to ultrasound or optical imaging data, evaluating denoising performance and comparing with existing methods tailored for those modalities.

### Open Question 2
What are the long-term effects of using 3D U-WGAN for denoising on downstream analyses such as machine learning model training or statistical inference in preclinical studies? Basis: paper demonstrates improved image quality and activation pattern restoration, but does not investigate impact of denoising on subsequent analyses or model training. Why unresolved: paper focuses on denoising process itself and its immediate effects on image quality, without exploring broader implications for downstream analyses. What evidence would resolve it: Longitudinal studies assessing impact of 3D U-WGAN denoising on performance of machine learning models trained on denoised data, as well as on statistical inference outcomes in preclinical studies.

### Open Question 3
How does the choice of hyperparameters, such as the patch size and loss function weights, affect the performance of 3D U-WGAN across different preclinical datasets with varying noise levels and image characteristics? Basis: paper mentions hyperparameters were set experimentally and discusses impact of patch size on performance, but does not provide comprehensive analysis of how different hyperparameter choices affect performance across diverse datasets. Why unresolved: paper's hyperparameter analysis is limited to specific dataset and noise level, and extending it to other datasets would require additional experiments. What evidence would resolve it: Systematic studies varying hyperparameters across multiple preclinical datasets with different noise levels and image characteristics, evaluating impact on denoising performance and identifying optimal settings for each scenario.

## Limitations
- Evaluation based on single CTNI dataset with 108 subjects, limiting generalizability to diverse preclinical fMRI acquisition protocols
- Lacks qualitative assessment of clinical utility or biological plausibility of restored activation patterns
- Hyperparameter optimization may be dataset-dependent without comprehensive cross-dataset validation

## Confidence
- High confidence: The fundamental architecture combining 3D convolutions with dense U-Net discriminator is technically sound and reported metrics are internally consistent
- Medium confidence: The comparative performance against BM4D, DU-GAN, and RIRGAN is valid within tested dataset, though cross-dataset validation is needed
- Medium confidence: The three-component loss function approach is theoretically justified, but optimal weight tuning may be dataset-dependent

## Next Checks
1. Cross-dataset validation: Test the trained model on an independent preclinical fMRI dataset from a different scanner or facility to assess generalizability
2. Ablation study on loss components: Systematically remove each loss component (MSE, perceptual, adversarial) to quantify individual contributions to final performance
3. Biological plausibility verification: Compare restored activation patterns in simulated data against ground truth using both quantitative metrics and qualitative assessment by neuroimaging experts