---
ver: rpa2
title: Average Certified Radius is a Poor Metric for Randomized Smoothing
arxiv_id: '2410.06895'
source_url: https://arxiv.org/abs/2410.06895
tags:
- training
- samples
- certified
- gaussian
- easy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The average certified radius (ACR) is a poor metric for evaluating
  the true robustness of models under randomized smoothing (RS). ACR can be arbitrarily
  large for a trivial classifier given enough certification budget, and it is much
  more sensitive to improvements on easy samples than on hard ones.
---

# Average Certified Radius is a Poor Metric for Randomized Smoothing

## Quick Facts
- arXiv ID: 2410.06895
- Source URL: https://arxiv.org/abs/2410.06895
- Authors: Chenhao Sun; Yuhao Mao; Mark Niklas MÃ¼ller; Martin Vechev
- Reference count: 40
- Primary result: Average certified radius (ACR) is a poor metric for evaluating true robustness of models under randomized smoothing

## Executive Summary
This paper demonstrates that average certified radius (ACR) is an unreliable metric for evaluating the true robustness of models under randomized smoothing (RS). The authors show that ACR can be arbitrarily large for trivial classifiers given sufficient certification budget, and it is disproportionately sensitive to improvements on easy samples rather than hard ones. Through theoretical analysis and empirical evidence, they reveal that current RS training strategies focus on easy inputs where base model accuracy under noise (pA) is close to 1, rather than improving general robustness. The paper concludes that ACR introduces a strong undesired bias in the field and should be discontinued as an evaluation metric for RS.

## Method Summary
The authors conduct both theoretical and empirical analyses to demonstrate the limitations of ACR as a robustness metric. Theoretically, they prove that ACR can be arbitrarily large for a trivial classifier when given enough certification budget, showing the metric's fundamental flaws. Empirically, they analyze various RS training strategies on CIFAR-10 and observe that all current methods reduce accuracy on hard inputs (where pA is small) while focusing on easy inputs (where pA is close to 1) to increase ACR. The authors then propose using the empirical distribution of pA as an alternative metric for RS evaluation. They demonstrate that by focusing on easy samples and amplifying simple Gaussian training, state-of-the-art ACR can be achieved without actual robustness training on the general data distribution.

## Key Results
- ACR can be arbitrarily large for trivial classifiers given sufficient certification budget
- Current RS training strategies uniformly reduce accuracy on hard inputs where pA is small
- All RS methods focus on easy inputs where pA is close to 1 to increase ACR
- State-of-the-art ACR can be achieved without training for general robustness by focusing on easy samples
- The empirical distribution of pA is proposed as a more reliable alternative metric

## Why This Works (Mechanism)
ACR fails as a robustness metric because it averages certified radii across all samples without considering the difficulty distribution of the data. The metric is heavily influenced by easy samples where the base model performs well under noise (high pA), allowing trivial classifiers to achieve high ACR scores. This creates a misalignment between ACR optimization and actual adversarial robustness, as models can game the metric by focusing solely on easy samples while neglecting hard ones where real-world attacks would be most effective.

## Foundational Learning
- **Randomized Smoothing (RS)**: A technique that adds noise to inputs during inference to create certifiably robust classifiers. Needed to understand the context of the paper's critique. Quick check: Can explain how RS provides probabilistic robustness guarantees.
- **Certified Radius**: The maximum perturbation radius for which a classifier's prediction is guaranteed to remain unchanged. Needed to understand what ACR measures. Quick check: Can compute certified radius from base model confidence.
- **Base Model Accuracy under Noise (pA)**: The accuracy of the base model when Gaussian noise is added to inputs. Needed to understand the proposed alternative metric. Quick check: Can measure pA distribution across a dataset.
- **Adversarial Robustness**: The ability of a model to maintain correct predictions under adversarial attacks. Needed to contextualize why ACR is problematic. Quick check: Can distinguish between certified and empirical robustness.
- **Gaussian Noise Training**: The process of training models with added Gaussian noise to improve performance under RS. Needed to understand current RS training approaches. Quick check: Can implement basic Gaussian noise augmentation.
- **Robustness-Accuracy Tradeoff**: The phenomenon where increasing model robustness often decreases standard accuracy. Needed to understand the broader context. Quick check: Can identify when a model is overfitting to easy samples.

## Architecture Onboarding

**Component Map:** Base Model -> Gaussian Noise Application -> Confidence Computation -> Certified Radius Calculation -> ACR Aggregation

**Critical Path:** The most important components are the base model's performance under noise (pA) and the certified radius computation, as these directly determine the ACR value and reveal the metric's biases.

**Design Tradeoffs:** The paper highlights the tradeoff between optimizing for ACR versus optimizing for actual adversarial robustness. ACR optimization can be achieved by focusing on easy samples, while true robustness requires addressing hard samples where pA is low.

**Failure Signatures:** When a model achieves high ACR but poor performance on hard samples (low pA inputs), or when ACR can be increased without improving general robustness, these indicate the metric's failure to capture true robustness.

**First Experiments:**
1. Measure pA distribution across easy and hard samples in your dataset to identify potential ACR gaming
2. Compare ACR values with empirical adversarial attack success rates to validate the metric's reliability
3. Implement a simple Gaussian noise training baseline and observe how focusing on easy samples affects ACR

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis primarily focuses on CIFAR-10 dataset, may not generalize to larger-scale datasets
- Claims about all current RS methods focusing on easy samples are based on specific empirical observations rather than comprehensive literature review
- Proposed alternative metric (pA distribution) requires further validation in practical deployment scenarios
- Does not explore whether the pA-robustness relationship holds across different architectures beyond those tested

## Confidence

| Claim | Confidence |
|-------|------------|
| ACR can be arbitrarily large for trivial classifiers | High |
| Current RS methods focus on easy samples | Medium |
| ACR optimization misaligns with true robustness | Medium |
| pA distribution is a better alternative metric | Medium |

## Next Checks
1. Conduct systematic experiments across multiple RS methods (including those not covered in the paper) to verify whether all approaches exhibit the same bias toward easy samples as claimed.
2. Validate the proposed pA distribution metric by testing whether models optimized for pA distribution actually achieve better worst-case robustness in adversarial attack scenarios compared to ACR-optimized models.
3. Test the robustness claims on larger-scale datasets (ImageNet) and different architectures (e.g., EfficientNet, Vision Transformers) to assess generalizability beyond CIFAR-10 experiments.