---
ver: rpa2
title: Towards Class-wise Robustness Analysis
arxiv_id: '2411.19853'
source_url: https://arxiv.org/abs/2411.19853
tags:
- classes
- class-wise
- adversarial
- class
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates class-wise robustness disparities in adversarially
  trained deep classification models, revealing that while some classes exhibit high
  accuracy, they are frequently misclassified as other classes, making them vulnerable
  to targeted attacks. The authors introduce the Class False Positive Score (CFPS)
  to quantify how often samples from other classes are misclassified as a specific
  target class, complementing traditional class-wise accuracy metrics.
---

# Towards Class-wise Robustness Analysis

## Quick Facts
- arXiv ID: 2411.19853
- Source URL: https://arxiv.org/abs/2411.19853
- Reference count: 40
- This work investigates class-wise robustness disparities in adversarially trained deep classification models, revealing that while some classes exhibit high accuracy, they are frequently misclassified as other classes, making them vulnerable to targeted attacks.

## Executive Summary
This paper introduces a novel analysis of class-wise robustness disparities in adversarially trained deep classification models on CIFAR-10. While traditional metrics focus on overall accuracy, the authors demonstrate that some classes, despite high accuracy, are frequently misclassified as other classes, creating vulnerabilities to targeted adversarial attacks. The Class False Positive Score (CFPS) is introduced as a complementary metric to quantify how often samples from other classes are misclassified as a specific target class. Through experiments on multiple robust architectures, the study reveals that classes like "cat" have high CFPS despite high accuracy, indicating susceptibility to targeted attacks, while "deer" shows low CFPS despite being a weak class in terms of accuracy. This analysis provides a more comprehensive understanding of model biases and latent space structures, helping identify classes that are prime targets for attacks and suggesting avenues for improving model robustness by addressing class-specific vulnerabilities.

## Method Summary
The authors evaluate adversarially trained models (ResNet-18, ResNet-50, DenseNet-169, PreActResNet-18, WideResNet-70-16, and DINOv2) on CIFAR-10 using standardized adversarial training from the robustbench benchmark. They introduce the Class False Positive Score (CFPS) to measure how often samples from other classes are misclassified as a specific target class. The evaluation includes clean accuracy, robust accuracy under common corruptions (CIFAR-10-C), and adversarial attacks (PGD with varying epsilon values). CFPS is calculated by counting misclassifications where samples from other classes are classified as the target class, then dividing by total misclassifications. The study compares CFPS with class-wise accuracy (CWA) and robust accuracy to provide a comprehensive assessment of class-wise vulnerabilities across different architectures.

## Key Results
- Classes like "cat" (C4) exhibit high CFPS despite high accuracy, indicating vulnerability to targeted adversarial attacks
- "Deer" (C5) shows low CFPS despite being a weak class in terms of accuracy, demonstrating that CFPS provides complementary information to accuracy metrics
- The study demonstrates that CFPS reveals class-wise vulnerabilities that traditional accuracy metrics miss, enabling better understanding of model biases and latent space structures
- Different architectures show varying patterns of class-wise vulnerabilities, with some classes being consistently more vulnerable across models

## Why This Works (Mechanism)
The paper's approach works because CFPS captures the frequency of false positives for each class, revealing hidden vulnerabilities that accuracy metrics miss. By measuring how often other classes are misclassified as a specific target class, CFPS identifies classes that are easily confused with many others, making them prime targets for adversarial attacks. This mechanism complements traditional accuracy metrics by providing insight into the model's decision boundaries and latent space structure, showing that high accuracy doesn't necessarily imply robustness to targeted attacks.

## Foundational Learning
- **Class-wise accuracy (CWA)**: Measures the accuracy for each individual class; needed to understand baseline performance per class, quick check: sum of all class accuracies equals overall accuracy.
- **Adversarial training**: Training models to be robust against adversarial attacks; needed to create models that can withstand perturbations, quick check: compare clean vs. robust accuracy.
- **PGD attacks**: Projected Gradient Descent attacks used to evaluate model robustness; needed to test model vulnerability under worst-case perturbations, quick check: success rate increases with epsilon value.
- **Common corruptions**: Real-world distortions like noise, blur, and weather effects; needed to evaluate model robustness beyond adversarial examples, quick check: compare clean accuracy to corruption accuracy.
- **Confusion matrices**: Show how often each class is predicted as every other class; needed to visualize misclassification patterns, quick check: diagonal elements represent correct predictions.
- **Robustbench benchmark**: Standardized collection of robust models and training procedures; needed to ensure fair comparison across architectures, quick check: models should have published accuracy baselines.

## Architecture Onboarding
- **Component map**: CIFAR-10 dataset -> Pre-trained adversarially trained models -> Forward pass -> Classification predictions -> CFPS calculation -> Accuracy/robustness metrics
- **Critical path**: Data loading → Model inference → Prediction counting → CFPS calculation → Result aggregation
- **Design tradeoffs**: The paper prioritizes comprehensive class-wise analysis over computational efficiency, using multiple architectures and extensive evaluation metrics at the cost of increased computation time.
- **Failure signatures**: CFPS values that don't correlate with accuracy indicate hidden vulnerabilities; inconsistent CFPS across architectures suggests architecture-dependent vulnerabilities.
- **First experiments**: 1) Verify model checkpoint versions from robustbench by checking clean accuracy baselines; 2) Implement CFPS calculation on a small subset and compare against paper results; 3) Reproduce PGD attack success rates for targeted attacks on the "cat" class using identical epsilon values.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do class-wise vulnerabilities and CFPS scores generalize to larger and more complex datasets beyond CIFAR-10?
- Basis in paper: The authors acknowledge that their analysis is limited to CIFAR-10 and assume similar biases exist across different datasets, but do not empirically verify this claim.
- Why unresolved: The paper only evaluates on CIFAR-10, which is a relatively simple dataset. Larger datasets with more classes and greater complexity might exhibit different patterns of class-wise vulnerabilities.
- What evidence would resolve it: Empirical analysis of CFPS scores and class-wise vulnerabilities on datasets like ImageNet, COCO, or domain-specific datasets to determine if similar patterns emerge.

### Open Question 2
- Question: What architectural modifications or training strategies can effectively reduce high CFPS scores for vulnerable classes like "cat" (C4)?
- Basis in paper: The authors identify that class C4 (cat) has high CFPS despite high accuracy, making it vulnerable to targeted attacks, but do not propose specific solutions.
- Why unresolved: While the paper identifies the problem of high CFPS for certain classes, it does not explore potential architectural or training modifications to address this issue.
- What evidence would resolve it: Experiments comparing different architectural designs, training strategies (e.g., class-balanced adversarial training), or regularization techniques to reduce CFPS for specific vulnerable classes.

### Open Question 3
- Question: How do different adversarial attack types (beyond PGD) affect the CFPS scores and class-wise robustness patterns observed in the study?
- Basis in paper: The authors primarily use PGD attacks for their analysis and note that class-wise vulnerabilities may not translate similarly across different attack scenarios.
- Why unresolved: The study focuses on PGD attacks, but other attack types (e.g., FGSM, C&W, transfer-based attacks) might produce different CFPS patterns and vulnerability assessments.
- What evidence would resolve it: Comprehensive evaluation of CFPS scores and class-wise robustness under various attack types, including white-box, black-box, and transfer-based attacks, to determine if patterns remain consistent.

## Limitations
- The analysis is limited to CIFAR-10 dataset, which may not generalize to more complex datasets with greater class diversity
- Exact adversarial training configurations (epsilon values, attack iterations, training schedules) are not fully specified, creating uncertainty in reproduction
- The study focuses primarily on PGD attacks, potentially missing vulnerabilities that would be exposed by other attack types

## Confidence
- High confidence: The core methodology for CFPS calculation and its utility in revealing class-wise robustness disparities
- Medium confidence: The relative ranking of classes by CFPS and the identification of "cat" as a high-risk target class
- Low confidence: The absolute CFPS values and exact numerical comparisons across different architectures due to unknown training details

## Next Checks
1. Verify model checkpoint versions from robustbench match those used in the paper by checking clean accuracy baselines before computing CFPS
2. Implement CFPS calculation with explicit counting rules for misclassification cases and compare against paper results on a small subset
3. Reproduce the PGD attack success rates for targeted attacks on the "cat" class using identical epsilon values and attack parameters to validate the reported vulnerability metrics