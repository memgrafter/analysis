---
ver: rpa2
title: Large Language Models as Planning Domain Generators
arxiv_id: '2405.06650'
source_url: https://arxiv.org/abs/2405.06650
tags:
- domain
- action
- pddl
- language
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates using large language models (LLMs) to automatically
  generate PDDL planning domains from natural language descriptions. A framework is
  introduced for evaluating generated domains by comparing the sets of plans for domain
  instances, avoiding manual evaluation.
---

# Large Language Models as Planning Domain Generators

## Quick Facts
- arXiv ID: 2405.06650
- Source URL: https://arxiv.org/abs/2405.06650
- Reference count: 7
- Primary result: LLMs can generate PDDL planning domains from natural language with 25% heuristic domain equivalence success rate

## Executive Summary
This work investigates using large language models (LLMs) to automatically generate PDDL planning domains from natural language descriptions. A framework is introduced for evaluating generated domains by comparing the sets of plans for domain instances, avoiding manual evaluation. The study evaluates 7 LLMs across 9 planning domains and three classes of natural language descriptions. Results show that larger LLMs, particularly those with high parameter counts, demonstrate moderate proficiency in generating correct planning domains from natural language, with the best model achieving 25% heuristically equivalent domain reconstruction.

## Method Summary
The study employs a framework where LLMs generate PDDL action schemas from natural language descriptions using in-context learning with examples. Seven different LLMs (including both coding and chat models) are evaluated across nine planning domains. The evaluation uses three classes of natural language descriptions (Base, Flipped, Random) and employs an automated framework that compares plan validity between original and generated domains to assess heuristic domain equivalence. The process generates PDDL actions action-by-action to manage context window constraints.

## Key Results
- Larger LLMs (70B+ parameters) show better performance for PDDL generation tasks
- The best model (LLaMA-2-70b) achieved 25% heuristically equivalent domain reconstruction
- 32% of generated actions were syntactically correct, but only 2% were semantically correct

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate syntactically and semantically valid PDDL domains when given structured natural language prompts with in-context examples
- Core assumption: The LLM has seen sufficient examples of structured code-like text during training to recognize and generate PDDL-like syntax
- Break condition: The LLM lacks sufficient exposure to structured, formal language patterns in training data, leading to syntax errors or incorrect semantics

### Mechanism 2
- Claim: Larger LLMs produce higher quality PDDL domains due to emergent abilities tied to model scale
- Core assumption: Emergent abilities scale with model size for structured code generation tasks
- Break condition: Model scale improvements plateau, or smaller models with task-specific fine-tuning outperform larger general models

### Mechanism 3
- Claim: Heuristic domain equivalence evaluation provides a scalable, automated metric for PDDL quality assessment
- Core assumption: Plan equivalence on a representative subset of problems implies domain equivalence
- Break condition: The problem subset is not representative, leading to false positives/negatives in domain equivalence assessment

## Foundational Learning

- Concept: Planning Domain Definition Language (PDDL) syntax and semantics
  - Why needed here: Understanding PDDL structure is essential for evaluating LLM-generated domains and designing effective prompts
  - Quick check question: What are the three main components of a PDDL action schema?

- Concept: In-context learning and few-shot prompting
  - Why needed here: The evaluation relies on providing the LLM with examples to learn the mapping from natural language to PDDL
  - Quick check question: How many context examples are used per prompt in the evaluation?

- Concept: Plan validation and heuristic equivalence checking
  - Why needed here: Automated evaluation depends on validating plans in both original and generated domains
  - Quick check question: What tool is used to validate plan correctness in the evaluation?

## Architecture Onboarding

- Component map:
  Natural language action descriptions → LLM with in-context examples → PDDL action output → Plan validator → Heuristic domain equivalence check

- Critical path:
  1. Generate prompts with action descriptions and context examples
  2. Run LLM to produce PDDL actions
  3. Parse and validate PDDL syntax
  4. Check plan validity in original and generated domains
  5. Classify results based on syntax, semantics, and domain equivalence

- Design tradeoffs:
  - Action-by-action generation vs. full domain generation (tradeoff between context window limits and coherence)
  - Number of context examples (more examples may improve quality but increase prompt length)
  - Subset of problems for plan validation (larger subsets increase confidence but require more computation)

- Failure signatures:
  - High syntax error rate: LLM struggles with PDDL syntax patterns
  - High semantic error rate: LLM understands syntax but not semantics of predicates
  - Low domain equivalence: LLM generates valid PDDL but incorrect domain semantics

- First 3 experiments:
  1. Test different numbers of context examples (1, 3, 5) to find optimal tradeoff between quality and prompt length
  2. Compare greedy vs. sampling decoding strategies for token selection
  3. Evaluate different description classes (base, flipped, random) to understand impact of predicate information

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of LLMs vary if the natural language descriptions included more detailed semantic information about the planning domain, such as object relationships or domain-specific constraints?
- Basis in paper: The paper discusses three classes of natural language descriptions but does not explore more complex semantic descriptions or the impact of including domain-specific constraints
- Why unresolved: The current study focuses on basic predicate descriptions and does not delve into the effects of richer semantic content or domain-specific constraints on the quality of generated PDDL domains
- What evidence would resolve it: Conducting experiments with LLMs using natural language descriptions that include detailed semantic information and domain-specific constraints, and comparing the performance to the current study's results

### Open Question 2
- Question: How does the choice of decoding strategy (e.g., greedy sampling, beam search) affect the quality of the generated PDDL domains, and is there an optimal strategy for this task?
- Basis in paper: The paper uses greedy sampling for token selection, but does not explore the impact of other decoding strategies on the quality of generated domains
- Why unresolved: The study does not compare the performance of different decoding strategies, leaving open the question of whether an alternative strategy could improve domain generation quality
- What evidence would resolve it: Performing experiments with various decoding strategies (e.g., beam search, sampling with temperature) and evaluating their impact on the quality of generated PDDL domains

### Open Question 3
- Question: What is the impact of using different evaluation metrics, such as plan length or plan cost, on the assessment of heuristic domain equivalence, and how might this affect the overall evaluation of LLM-generated domains?
- Basis in paper: The paper uses plan applicability for heuristic domain equivalence but does not explore the effects of different evaluation metrics like plan length or cost on this assessment
- Why unresolved: The study focuses on plan applicability without considering how other metrics might influence the evaluation of domain equivalence, potentially affecting the assessment of generated domains
- What evidence would resolve it: Conducting experiments using various evaluation metrics (e.g., plan length, plan cost) to assess heuristic domain equivalence and comparing the results to the current study's findings

## Limitations

- 25% success rate indicates 75% of domains fail to meet functional equivalence criterion
- Evaluation framework may not fully capture domain completeness or edge cases
- Focus on action-by-action generation may limit coherence and consistency across generated actions
- Weak supporting literature with limited citations in neighboring research area

## Confidence

**High Confidence Claims:**
- LLMs can generate syntactically valid PDDL actions with structured prompts and examples
- Larger LLMs (70B+ parameters) demonstrate better performance for PDDL generation tasks
- Plan validation provides an automated, scalable method for assessing domain quality

**Medium Confidence Claims:**
- The 25% heuristic domain equivalence rate represents meaningful progress in automated planning domain generation
- Action-by-action generation is an effective strategy given context window constraints
- The three classes of natural language descriptions provide meaningful variation for evaluation

**Low Confidence Claims:**
- LLM-generated domains will generalize to unseen planning domains not represented in the evaluation set
- The current evaluation framework captures all aspects of domain quality
- Plan equivalence on a subset of problems guarantees full domain equivalence

## Next Checks

1. **Cross-domain Generalization Test**: Evaluate the best-performing LLM on a held-out set of planning domains not used in the original study to assess generalization capabilities beyond the nine domains tested.

2. **Human Evaluation Validation**: Conduct manual inspection of a random sample of generated domains (both successful and unsuccessful cases) to validate the automated heuristic equivalence assessment and identify potential false positives/negatives.

3. **End-to-End Planning Performance**: Measure the actual planning performance (solution quality, computation time) of generated domains compared to human-designed domains on benchmark planning problems to assess practical utility beyond syntactic correctness.