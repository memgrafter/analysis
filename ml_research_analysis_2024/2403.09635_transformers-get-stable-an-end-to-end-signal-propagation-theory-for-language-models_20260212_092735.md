---
ver: rpa2
title: 'Transformers Get Stable: An End-to-End Signal Propagation Theory for Language
  Models'
arxiv_id: '2403.09635'
source_url: https://arxiv.org/abs/2403.09635
tags:
- rlxin
- gout
- xini
- propagation
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DeepScaleLM, a method to enable training of
  very deep transformer models with 1000+ layers. It develops a unified signal propagation
  theory and provides closed-form expressions for the moments of forward and backward
  signals through the transformer model.
---

# Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models

## Quick Facts
- arXiv ID: 2403.09635
- Source URL: https://arxiv.org/abs/2403.09635
- Reference count: 40
- This paper proposes DeepScaleLM, a method to enable training of very deep transformer models with 1000+ layers

## Executive Summary
This paper introduces DeepScaleLM, a theoretically grounded method for training very deep transformer models with 1000+ layers. The authors develop a unified signal propagation theory that provides closed-form expressions for the moments of forward and backward signals through transformer models. DeepScaleLM addresses fundamental training challenges like vanishing/exploding gradients and rank collapse through a novel initialization and scaling scheme that conserves unit output/gradient moments throughout the model.

The method enables training of deep models with hundreds of layers that consistently outperform shallow models across multiple tasks including language modeling, speech translation, and image classification. The improvements extend to downstream question answering performance and image classification robustness, demonstrating the practical value of the approach across different transformer variants and model scales.

## Method Summary
DeepScaleLM is built on a unified signal propagation theory for transformers that models the forward and backward signal moments throughout the network. The core innovation is a novel initialization and scaling scheme that conserves unit output and gradient moments at every layer, preventing signal degradation that typically occurs in deep networks. This is achieved through careful initialization of attention weights, layer normalization parameters, and relative positional embeddings, combined with scaling factors that maintain signal stability.

The method specifically addresses the interaction between layer normalization and attention mechanisms, which was previously overlooked in signal propagation analyses. By deriving closed-form expressions for the expected signal moments, DeepScaleLM provides a principled way to initialize and scale parameters that ensures stable training even for models with 1000+ layers.

## Key Results
- DeepScaleLM enables training of transformers with 100s of layers that outperform shallow models on language modeling, speech translation, and image classification tasks
- The method demonstrates consistent improvements across different transformer variants including standard, ALiBi, and LocalRNN architectures
- Improvements in robustness for image classification and better downstream QA performance are observed when using DeepScaleLM-initialized models

## Why This Works (Mechanism)
The success of DeepScaleLM stems from its ability to maintain stable signal propagation through deep transformer architectures. By ensuring that the forward and backward signals have unit variance at each layer, the method prevents the accumulation of noise and degradation that typically occurs in deep networks. This is particularly important for transformers due to their complex interaction between attention mechanisms and layer normalization.

The initialization scheme specifically addresses the unique challenges of transformer architectures, where the attention mechanism and layer normalization create non-linear signal transformations that can destabilize training in deep networks. By carefully balancing these components and maintaining signal moments, DeepScaleLM enables effective gradient flow throughout the entire depth of the model.

## Foundational Learning

**Signal Propagation Theory**
- Why needed: Understanding how signals flow through deep networks is essential for diagnosing training issues like vanishing/exploding gradients
- Quick check: Verify that forward and backward signals maintain stable variance across layers

**Layer Normalization in Transformers**
- Why needed: Layer normalization interacts with attention mechanisms in ways that affect signal stability
- Quick check: Ensure layer norm parameters are properly initialized to maintain unit signal moments

**Attention Mechanism Dynamics**
- Why needed: Self-attention creates complex signal transformations that can destabilize deep networks
- Quick check: Verify attention weight initialization preserves signal stability

**Gradient Flow Analysis**
- Why needed: Understanding gradient propagation is crucial for preventing vanishing/exploding gradients
- Quick check: Monitor gradient norms across layers during training

## Architecture Onboarding

**Component Map**
Input -> Embedding Layer -> N DeepScaleLM Layers -> Output Head

**Critical Path**
The critical path is the signal propagation through the stacked transformer layers, where DeepScaleLM's initialization scheme maintains stable forward and backward signal moments.

**Design Tradeoffs**
- Fixed sequence length requirement vs. flexibility: DeepScaleLM requires predefined sequence length, limiting adaptability
- Computational overhead of initialization: The initialization process is more complex than standard methods
- Improved training stability vs. parameter count: Enables much deeper models at the cost of increased parameters

**Failure Signatures**
- Rank collapse in attention matrices despite initialization
- Gradual degradation of signal variance in deeper layers
- Inconsistent improvements across different transformer variants

**3 First Experiments**
1. Compare training curves of standard vs. DeepScaleLM initialization on a 100-layer transformer
2. Measure attention matrix rank across layers for both initialization schemes
3. Evaluate gradient norm stability throughout training for different initialization methods

## Open Questions the Paper Calls Out
None

## Limitations
- The initialization method requires a predefined sequence length and must be recomputed for each different sequence length, which could be computationally expensive for large models
- While improvements are consistent, absolute performance gains for smaller models like TinyBERT are relatively modest (e.g., 0.5% improvement in SQuAD)
- The theoretical analysis makes simplifying assumptions about token independence and Gaussian signal distributions that may not hold exactly in practice

## Confidence
- High confidence: The theoretical framework for signal propagation in transformers and the core mechanism of moment conservation are well-established
- Medium confidence: The empirical results showing consistent improvements across different tasks and model sizes, though some improvements are modest
- Medium confidence: The claims about robustness improvements, as these are demonstrated on specific datasets and tasks

## Next Checks
1. Evaluate the method's performance on longer sequences to assess the practical impact of the sequence-length-dependent initialization requirement
2. Test the initialization scheme on additional tasks beyond those presented, particularly in domains with different statistical properties
3. Conduct ablation studies to isolate the individual contributions of different components of the initialization scheme (e.g., the effects of relative positional embeddings vs. gradient scaling)