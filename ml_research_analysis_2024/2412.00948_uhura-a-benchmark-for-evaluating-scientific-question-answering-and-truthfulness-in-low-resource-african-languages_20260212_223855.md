---
ver: rpa2
title: 'Uhura: A Benchmark for Evaluating Scientific Question Answering and Truthfulness
  in Low-Resource African Languages'
arxiv_id: '2412.00948'
source_url: https://arxiv.org/abs/2412.00948
tags:
- languages
- language
- across
- african
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Uhura, a new benchmark for evaluating scientific
  question answering and truthfulness in six low-resource African languages: Amharic,
  Hausa, Northern Sotho, Swahili, Yoruba, and Zulu. The authors created Uhura by translating
  existing English benchmarks (ARC-Easy and TruthfulQA) into these languages using
  professional translators.'
---

# Uhura: A Benchmark for Evaluating Scientific Question Answering and Truthfulness in Low-Resource African Languages

## Quick Facts
- arXiv ID: 2412.00948
- Source URL: https://arxiv.org/abs/2412.00948
- Reference count: 28
- Key outcome: Proprietary models (GPT-4o, o1-preview) significantly outperform open-source models (LLaMA, Gemma) across six low-resource African languages

## Executive Summary
This paper introduces Uhura, a new benchmark for evaluating large language models on scientific question answering and truthfulness in six low-resource African languages: Amharic, Hausa, Northern Sotho, Swahili, Yoruba, and Zulu. The authors created Uhura by translating existing English benchmarks (ARC-Easy and TruthfulQA) into these languages using professional translators. The evaluation reveals that proprietary models like GPT-4o and o1-preview significantly outperform open-source models like Meta's LLaMA and Google's Gemma across all languages. Additionally, all models perform better in English than in the African languages, indicating that current language models struggle with scientific questions and are more prone to generating false claims in low-resource African languages.

## Method Summary
The authors created Uhura by translating two English benchmarks (ARC-Easy for science questions and TruthfulQA for truthfulness) into six low-resource African languages using professional translators. The benchmark was evaluated using zero-shot and few-shot approaches with five different prompt templates per task. Models were accessed via HuggingFace for open-source models and OpenAI/Anthropic APIs for proprietary models. The evaluation measured accuracy using exact match or arg max(log prob) methods across both English and African languages.

## Key Results
- Proprietary models (GPT-4o, o1-preview) significantly outperform open-source models across all six African languages
- All models perform better in English than in African languages for both scientific questions and truthfulness tasks
- Providing in-context examples improves performance, with Swahili showing the greatest improvement (+21%) when comparing results with five examples to those without any examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The translation process enabled evaluation of LLMs in six low-resource African languages by converting existing English benchmarks into target languages.
- Mechanism: Professional translators with native language proficiency and cultural understanding adapted ARC-Easy and TruthfulQA questions into Amharic, Hausa, Northern Sotho, Swahili, Yoruba, and Zulu, preserving technical and contextual meaning.
- Core assumption: Human translators can accurately convey technical scientific concepts and cultural nuances across languages with significant structural differences.
- Evidence anchors:
  - [abstract] "The authors created Uhura by translating existing English benchmarks (ARC-Easy and TruthfulQA) into these languages using professional translators."
  - [section] "We recruited professional translators through the Masakhane NLP community... Each language had a dedicated coordinator who supervised and closely collaborated with the translators."
- Break condition: If translators lack domain expertise in science or cannot find culturally equivalent expressions for technical terms, translation quality degrades and benchmark validity is compromised.

### Mechanism 2
- Claim: The multi-way parallel nature of Uhura allows direct cross-linguistic comparison of LLM performance on identical question content.
- Mechanism: By maintaining question consistency across all six languages while preserving linguistic authenticity, the benchmark isolates language-specific performance differences from content variations.
- Core assumption: Parallel translation maintains semantic equivalence while respecting linguistic structures unique to each language.
- Evidence anchors:
  - [abstract] "Our dataset is multi-way parallel which enables us to evaluate the performance on similar questions across many languages."
- Break condition: If translation introduces content drift or if certain questions cannot be meaningfully translated, parallel comparison becomes invalid.

### Mechanism 3
- Claim: The benchmark reveals significant performance gaps between proprietary and open-source models across low-resource languages.
- Mechanism: Proprietary models like GPT-4o and o1-preview leverage extensive multilingual training data and optimization, while open-source models like LLaMA and Gemma show marked performance degradation in African languages.
- Core assumption: Performance differences reflect training data quality and multilingual optimization rather than model architecture alone.
- Evidence anchors:
  - [abstract] "The evaluation reveals that proprietary models like GPT-4o and o1-preview significantly outperform open-source models like Meta's LLaMA and Google's Gemma across all languages."
- Break condition: If proprietary models have undisclosed multilingual training advantages or if evaluation protocols favor certain model types, performance gap interpretation becomes uncertain.

## Foundational Learning

- Concept: Cross-linguistic evaluation methodology
  - Why needed here: Understanding how to design benchmarks that fairly compare model performance across languages with different linguistic structures and cultural contexts
  - Quick check question: What are the key challenges in creating parallel evaluation datasets across typologically diverse languages?

- Concept: In-context learning (ICL) in multilingual settings
  - Why needed here: The analysis examines how providing few-shot examples affects model performance differently across languages, revealing ICL capabilities
  - Quick check question: How does ICL performance vary between high-resource and low-resource languages, and what factors influence this variation?

- Concept: Cultural bias in benchmark design
  - Why needed here: The work highlights how Western-centric benchmarks may not translate well to African contexts, affecting both translation quality and model evaluation
  - Quick check question: What are the risks of using culturally biased benchmarks when evaluating models on low-resource languages?

## Architecture Onboarding

- Component map: Translated benchmark datasets -> Evaluation harness -> Model APIs -> Prompt templates -> Result aggregation -> Analysis
- Critical path: Translation → Prompt generation → Model evaluation → Result aggregation → Analysis
- Design tradeoffs: Human translation ensures quality but limits scalability; parallel format enables comparison but may not capture language-specific nuances; zero-shot evaluation shows baseline capabilities but doesn't optimize for language-specific prompting
- Failure signatures: Translation errors manifest as inconsistent performance patterns; prompt sensitivity indicates model instability; large performance gaps suggest training data limitations
- First 3 experiments:
  1. Run all models on a small subset of translated questions to verify pipeline functionality and identify any obvious translation or evaluation issues
  2. Test prompt sensitivity by running each model with different prompt templates on the same questions to establish baseline variability
  3. Compare performance across languages on a balanced subset to identify languages where models perform particularly well or poorly, informing further investigation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of in-context examples (k) that maximizes performance across different low-resource African languages?
- Basis in paper: [explicit] The paper mentions that providing in-context examples improves model performance, with Swahili showing the greatest improvement (+21%) when comparing results with five examples to those without any examples.
- Why unresolved: The paper only tested up to 20 examples and found that performance plateaus after five examples for some languages, but the optimal number may vary by language and task.
- What evidence would resolve it: A systematic study testing different values of k (e.g., 1, 2, 3, 4, 5, 10, 15, 20, 25) across all six languages and both tasks, measuring accuracy gains and computational costs.

### Open Question 2
- Question: How do cultural biases in English benchmarks affect the validity of performance comparisons across languages when translated?
- Basis in paper: [explicit] The authors identified numerous instances of cultural biases in leading English-language benchmarks, including questions assuming Western context and negative portrayals of Africa and the Global South.
- Why unresolved: The paper notes these biases exist but doesn't quantify their impact on model performance or develop methods to create culturally neutral benchmarks.
- What evidence would resolve it: A comparative study measuring model performance on culturally adapted versions of questions versus direct translations, or developing and evaluating culturally neutral benchmark questions.

### Open Question 3
- Question: What specific linguistic features or characteristics of low-resource African languages contribute to the performance gap between English and these languages?
- Basis in paper: [inferred] The paper observes that models perform better in English than African languages and that some languages like Swahili perform better than others, but doesn't analyze the underlying linguistic causes.
- Why unresolved: The paper identifies the performance gap but doesn't investigate whether it's related to specific linguistic features like morphology, syntax, script differences, or tokenization challenges.
- What evidence would resolve it: A detailed linguistic analysis correlating model performance with specific language characteristics, or controlled experiments modifying linguistic features to isolate their impact on performance.

## Limitations
- Translation quality and cultural context adaptation may introduce subtle biases affecting both translation quality and model evaluation outcomes
- Evaluation methodology relies on zero-shot and few-shot approaches without exploring other prompting strategies that might better suit low-resource languages
- Performance gaps between proprietary and open models could be partially attributed to tokenization strategies rather than just training data quality

## Confidence
- High Confidence: Proprietary models significantly outperform open-source models; all models perform better in English than African languages; benchmark enables cross-linguistic comparison
- Medium Confidence: Performance gaps reflect training data quality differences; translation quality is sufficient; five prompt templates adequately capture capabilities
- Low Confidence: Specific reasons for language-specific performance variations; extent of cultural bias impact; optimality of current tokenization strategies

## Next Checks
1. Conduct a blind review where native speakers evaluate translation quality and semantic equivalence across languages for a subset of questions
2. Systematically test a broader range of prompting strategies to determine if observed performance gaps persist across different approaches
3. Compare model performance using different tokenization strategies for each language to quantify how much tokenization affects performance differences between proprietary and open models