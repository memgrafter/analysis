---
ver: rpa2
title: Hierarchical Transformer for Electrocardiogram Diagnosis
arxiv_id: '2411.00755'
source_url: https://arxiv.org/abs/2411.00755
tags:
- transformer
- attention
- token
- hierarchical
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a hierarchical transformer architecture for
  ECG diagnosis that combines depth-wise convolutions, multi-scale feature aggregation
  via a CLS token, and an attention-gated module to learn inter-lead relationships.
  The model addresses limitations of previous approaches by using depth-wise convolutions
  to prevent cross-lead information mixing before the transformer, employing a CLS
  token to aggregate task-relevant information across transformer stages, and incorporating
  an attention-gated module to model dependencies between ECG leads.
---

# Hierarchical Transformer for Electrocardiogram Diagnosis

## Quick Facts
- arXiv ID: 2411.00755
- Source URL: https://arxiv.org/abs/2411.00755
- Authors: Xiaoya Tang; Jake Berquist; Benjamin A. Steinberg; Tolga Tasdizen
- Reference count: 14
- Primary result: State-of-the-art performance on 2020 PhysioNet/CinC Challenge dataset (macroFβ: 0.5778) and KCL potassium classification (macro-averaged AUC: 0.8232)

## Executive Summary
This paper introduces a hierarchical transformer architecture specifically designed for electrocardiogram (ECG) diagnosis that addresses key limitations of previous approaches. The model employs depth-wise convolutions to prevent cross-lead information mixing, uses a CLS token to aggregate multi-scale features across transformer stages, and incorporates an attention-gated module to model dependencies between ECG leads. The architecture successfully handles the multi-lead nature of ECG data while maintaining interpretability and achieving superior performance on two benchmark datasets.

## Method Summary
The hierarchical transformer processes 12-lead ECG signals through a four-layer depth-wise convolutional feature extractor that preserves lead-specific information. Three transformer stages progressively downsample spatial features while maintaining the same CLS token to aggregate multi-scale information. An attention-gated module with three linear layers models dependencies between leads before final classification. The model is trained with 10-fold cross-validation on the 2020 PhysioNet/CinC Challenge dataset (24 diagnoses) and the KCL potassium classification dataset, using macroFβ, Gβ, and challenge score for PhysioNet, and macro-averaged AUC for KCL.

## Key Results
- Achieves macroFβ of 0.5778, Gβ of 0.3407, and challenge score of 0.5980 on 2020 PhysioNet/CinC Challenge dataset
- Achieves macro-averaged AUC of 0.8232 on KCL potassium classification dataset
- Outperforms previous state-of-the-art methods including Res-SENet and ViT-based approaches
- Demonstrates effectiveness across both multi-label (24 diagnoses) and binary classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Depth-wise convolutions prevent cross-lead information mixing before transformer layers
- Mechanism: Each lead's spatial features are processed independently by dedicated convolutional filters, preserving lead-specific information
- Core assumption: Mixing lead information before the transformer would obscure diagnostically relevant patterns unique to each lead
- Evidence anchors:
  - [abstract]: "use depth-wise convolutions to prevent the mixing of potentially important yet implicit information across leads before the transformer"
  - [section 2]: "Since the input ECG data is a multi-channel(lead) one-dimensional(1D) sequence, we employ a four-layer feature extractor consisting of 1D depth-wise convolutions"
  - [corpus]: Weak - no direct corpus evidence found
- Break condition: If cross-lead interactions before transformer layers provide complementary information for diagnosis

### Mechanism 2
- Claim: Multi-scale feature aggregation via CLS token captures task-relevant information across transformer stages
- Mechanism: The same CLS token is maintained across three transformer stages, aggregating features from progressively downsampled spatial representations
- Core assumption: The CLS token can effectively capture and transfer multi-scale information while maintaining prediction-relevant features
- Evidence anchors:
  - [abstract]: "We employ a CLS token to aggregate task-relevant information from multi-scale representations across stages"
  - [section 2]: "By maintaining the same CLS token across the transformer, it aggregates multi-scale information"
  - [corpus]: Weak - no direct corpus evidence found
- Break condition: If the CLS token becomes saturated or loses discriminative information during multi-stage propagation

### Mechanism 3
- Claim: Attention-gated module models dependencies between ECG leads
- Mechanism: Three linear layers learn query-key-value relationships between channels, capturing lead-to-lead associations
- Core assumption: Inter-lead dependencies contain diagnostically relevant information not captured by individual lead processing
- Evidence anchors:
  - [abstract]: "incorporating an attention-gated module to model dependencies between ECG leads"
  - [section 2]: "To model dependencies between leads, we apply an attention-gated module comprising three linear layers"
  - [section Appendix A.3]: "This module comprises three linear layers designed to uncover latent dependencies between channels"
- Break condition: If lead dependencies are redundant or introduce noise rather than useful signal

## Foundational Learning

- Concept: Depth-wise separable convolutions
  - Why needed here: To process each ECG lead independently while maintaining computational efficiency
  - Quick check question: How do depth-wise convolutions differ from standard convolutions in their treatment of input channels?

- Concept: Multi-head self-attention (MSA)
  - Why needed here: To capture long-range temporal dependencies in sequential ECG signals
  - Quick check question: What is the role of query, key, and value matrices in the MSA mechanism?

- Concept: CLS token usage in transformers
  - Why needed here: To aggregate multi-scale features across transformer stages for final classification
  - Quick check question: How does the CLS token differ from standard input tokens in transformer architectures?

## Architecture Onboarding

- Component map: Input -> Depth-wise CNN encoder -> Three-stage transformer (with CLS token) -> Attention-gated module -> Classifiers -> Output
- Critical path: CNN feature extraction -> Transformer stages -> Attention-gated module -> Classification
- Design tradeoffs: Depth-wise convolutions vs. standard convolutions (preserving lead information vs. cross-lead mixing), multi-stage vs. single-stage transformer (multi-scale vs. computational cost), CLS token vs. all-embeddings (efficiency vs. information completeness)
- Failure signatures: Poor performance on lead-specific diagnoses, over-reliance on inter-lead attention, attention maps not aligning with clinical features
- First 3 experiments:
  1. Replace depth-wise convolutions with standard convolutions to test lead mixing hypothesis
  2. Remove attention-gated module to assess inter-lead dependency contribution
  3. Modify CLS token handling to pass all embeddings instead of just CLS token to test multi-scale aggregation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the depth-wise convolution layer specifically prevent cross-lead information mixing before the transformer stage, and what is the empirical evidence for this separation being beneficial?
- Basis in paper: [explicit] The paper states "We use depth-wise convolutions to prevent the mixing of potentially important yet implicit information across leads before the transformer" but doesn't provide empirical evidence or visualization of this separation.
- Why unresolved: The paper claims depth-wise convolutions maintain lead separation but doesn't show attention maps or other evidence demonstrating that leads remain uncombined through the depth-wise layers or quantify the benefit of this separation.
- What evidence would resolve it: Visualization of feature maps at each depth-wise layer showing no cross-lead mixing, ablation studies comparing performance with and without depth-wise convolutions, or attention analysis showing the model relies on individual leads rather than combined information.

### Open Question 2
- Question: How does the attention-gated module improve performance compared to simply concatenating lead features or using alternative inter-lead modeling approaches?
- Basis in paper: [explicit] The paper states "To model dependencies between leads, we apply an attention-gated module" but doesn't compare against simpler alternatives like feature concatenation or explain why this specific architecture is optimal.
- Why unresolved: The paper introduces the attention-gated module as a novel contribution but only shows overall model performance, not a controlled comparison showing the specific benefit of this module versus simpler approaches.
- What evidence would resolve it: Ablation studies comparing the attention-gated module to feature concatenation, simple attention mechanisms, or other inter-lead modeling approaches, along with visualization of the learned lead dependencies.

### Open Question 3
- Question: What is the optimal number of stages and MSA layers per stage for different ECG classification tasks, and how sensitive is performance to these architectural choices?
- Basis in paper: [inferred] The paper uses a three-stage transformer with unspecified number of MSA layers per stage, stating "each stage containing a stack of MSA layers, with the division of layers tailored to specific needs" without exploring the sensitivity to these architectural choices.
- Why unresolved: The paper presents a specific architecture but doesn't explore the hyperparameter space or provide guidance on how to adapt the number of stages/layers for different ECG tasks or dataset sizes.
- What evidence would resolve it: Comprehensive ablation studies varying the number of stages and MSA layers, analysis of performance versus computational cost trade-offs, and guidelines for selecting architecture based on task complexity or dataset characteristics.

## Limitations

- No ablation studies to isolate contributions of individual architectural components (depth-wise convolutions, CLS token aggregation, attention-gated module)
- Lack of corpus evidence for specific architectural choices, though general concepts have related support
- Performance metrics could potentially be achieved through alternative architectural approaches not explored

## Confidence

**High Confidence**: The overall performance improvements on both PhysioNet and KCL datasets are well-supported by the reported metrics. The hierarchical architecture successfully processes multi-lead ECG signals and achieves state-of-the-art results.

**Medium Confidence**: The individual mechanisms (depth-wise convolutions, multi-scale CLS token, attention-gated module) likely contribute to performance, but their specific contributions cannot be definitively attributed without ablation studies. The architectural choices are reasonable given the problem structure but not empirically validated in isolation.

**Low Confidence**: The claim that depth-wise convolutions are necessary to prevent "mixing of potentially important yet implicit information" is speculative without evidence showing degraded performance when using standard convolutions. Similarly, the effectiveness of the specific attention-gated module design over simpler alternatives remains unproven.

## Next Checks

1. **Ablation study on depth-wise convolutions**: Replace the depth-wise convolutional feature extractor with standard convolutions that allow cross-lead mixing, keeping all other components constant. Compare performance to determine if lead separation provides measurable benefit.

2. **CLS token vs. all-embeddings aggregation**: Modify the architecture to pass all transformer embeddings (rather than just the CLS token) to the final classification layers. This tests whether the multi-scale aggregation through a single CLS token is optimal or if richer representations would improve performance.

3. **Inter-lead attention contribution**: Remove the attention-gated module entirely and assess the impact on both overall performance and specific diagnoses that might rely on lead relationships. This isolates the contribution of modeled inter-lead dependencies.