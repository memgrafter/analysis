---
ver: rpa2
title: Finetuning Language Models to Emit Linguistic Expressions of Uncertainty
arxiv_id: '2409.12180'
source_url: https://arxiv.org/abs/2409.12180
tags:
- calibration
- uncertainty
- confidence
- expressions
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method to finetune language models to emit
  calibrated linguistic expressions of uncertainty alongside their predictions. It
  leverages the model's self-evaluation confidence, maps it to human-perceived linguistic
  uncertainty expressions, and uses supervised finetuning to teach the model to generate
  these expressions.
---

# Finetuning Language Models to Emit Linguistic Expressions of Uncertainty

## Quick Facts
- arXiv ID: 2409.12180
- Source URL: https://arxiv.org/abs/2409.12180
- Authors: Arslan Chaudhry; Sridhar Thiagarajan; Dilan Gorur
- Reference count: 40
- Primary result: Finetuning models to emit calibrated uncertainty expressions significantly improves ECE and Brier Score, especially when uncertainty is appended after the answer

## Executive Summary
This paper introduces a method to finetune language models to emit calibrated linguistic expressions of uncertainty alongside their predictions. The approach leverages the model's self-evaluation confidence, maps it to human-perceived linguistic uncertainty expressions, and uses supervised finetuning to teach the model to generate these expressions. Experiments on Q/A datasets (TriviaQA, AmbigQA) show that models finetuned this way produce well-calibrated uncertainty expressions, with Expected Calibration Error (ECE) and Brier Score significantly improved, especially when uncertainty is appended after the answer.

## Method Summary
The method involves generating predictions on training data, self-evaluating confidence on these predictions, calibrating confidence scores with isotonic regression, mapping calibrated scores to linguistic expressions, curating an augmented finetuning dataset, and finetuning the model to emit uncertainty. The approach uses self-evaluation prompts to elicit the model's confidence in its predictions, applies isotonic regression for post-processing to improve calibration, and maps confidence scores to linguistic expressions using a predefined framework. The finetuning dataset is created by augmenting predictions with uncertainty expressions, preferentially postfixed, and the model is trained using supervised learning with standard hyperparameters.

## Key Results
- Post-fixed uncertainty expressions yield the lowest calibration error compared to prefixed or interleaved methods
- Finetuned models show significant improvements in ECE and Brier Score across TriviaQA and AmbigQA datasets
- Isotonic regression on calibration sets achieves near-perfect calibration of confidence scores
- Larger models demonstrate better calibration of uncertainty expressions than smaller variants

## Why This Works (Mechanism)

### Mechanism 1
LLMs are well-calibrated in assessing their predictions for single-claim answers. The model's internal probability for the 'true' token, after normalization, aligns closely with the actual accuracy of its predictions when evaluated on a True/False self-evaluation task. Core assumption: The LLM's parametric knowledge includes reliable self-assessment capabilities for factuality. Evidence: Paper claims calibration but corpus lacks supporting studies. Break condition: If the model's parametric knowledge is corrupted or the task involves multi-hop reasoning, self-evaluation may become poorly calibrated.

### Mechanism 2
Isotonic regression on a small calibration set significantly improves the calibration of confidence scores. Non-parametric monotonic regression adjusts raw confidence scores to align with observed accuracy, reducing systematic bias. Core assumption: The relationship between confidence and accuracy is monotonic but may be distorted by model artifacts. Evidence: Paper mentions isotonic regression achieves near-perfect calibration but provides minimal empirical demonstration. Break condition: If the calibration set is too small or unrepresentative, isotonic regression may overfit or fail to correct miscalibration.

### Mechanism 3
Post-fixed uncertainty expressions (expression after the answer) yield better-calibrated finetuned models than prefixed or interleaved formats. During autoregressive decoding, adding uncertainty after the answer does not influence the answer's token sampling distribution, avoiding contamination of the main prediction. Core assumption: The sampling distribution of the answer is independent of subsequent uncertainty tokens. Evidence: Paper shows post-fixing yields lowest calibration error but lacks external validation. Break condition: If the model learns to condition the answer on the uncertainty expression during finetuning, post-fixing may no longer be advantageous.

## Foundational Learning

- Concept: Calibration and Expected Calibration Error (ECE)
  - Why needed here: To measure and optimize the alignment between predicted confidence and actual accuracy of uncertainty expressions
  - Quick check question: What does an ECE of 0.1 mean in terms of confidence-accuracy mismatch?

- Concept: Isotonic regression
  - Why needed here: To post-process raw confidence scores into well-calibrated probabilities without assuming a parametric form
  - Quick check question: Why is isotonic regression preferred over logistic regression for this task?

- Concept: Self-evaluation prompting
  - Why needed here: To elicit the model's confidence in its own predictions in a way that is consistent with its parametric knowledge
  - Quick check question: What is the difference between logits and self-evaluation confidence in this context?

## Architecture Onboarding

- Component map: LLM base model -> True/False self-evaluation prompt -> Isotonic regression calibrator -> Linguistic uncertainty mapper -> Uncertainty-augmented dataset curator -> Supervised finetuning pipeline
- Critical path: 1) Generate predictions on training data 2) Self-evaluate confidence on predictions 3) Calibrate confidence scores with isotonic regression 4) Map to linguistic expressions 5) Curate finetuning dataset 6) Finetune model to emit uncertainty
- Design tradeoffs: Using self-evaluation vs external judge (faster but potentially less reliable for complex reasoning), Post-fixing vs prefixing uncertainty (better calibration but less intuitive), Isotonic regression vs parametric calibration (non-parametric and robust but needs representative calibration set)
- Failure signatures: Poor calibration in finetuned model (issues in self-evaluation reliability or miscalibration of base model), Uncertainty expressions mismatch with model knowledge (contamination in dataset curation or incorrect linguistic mapping), High variance in ECE across bins (insufficient examples per bin or poor bin assignment)
- First 3 experiments: 1) Measure ECE and Brier score before and after isotonic regression on the calibration set 2) Compare calibration of prefixed vs postfixed vs interleaved uncertainty augmentation on a held-out set 3) Evaluate finetuned model calibration on AmbigQA and TruthfulQA datasets

## Open Questions the Paper Calls Out

- Does the placement of uncertainty expressions (prefix vs suffix vs interleaved) affect the model's core accuracy on question answering tasks? The paper compares different methods of uncertainty augmentation but does not report accuracy differences between these methods. The experiments focus on calibration metrics rather than accuracy, leaving open whether uncertainty expression placement impacts factual correctness.

- How well do the finetuned models generalize to out-of-distribution questions or different domains beyond the three tested datasets? The authors note their experiments are limited to TriviaQA, AmbigQA, and TruthfulQA, and explicitly exclude TruthfulQA from finetuning due to poor calibration. The study does not test cross-domain generalization or robustness to distributional shift.

- What is the impact of model size on the quality of uncertainty expressions, beyond the small and medium models tested? The paper observes that calibration improves with larger model sizes but only tests small and medium variants. The relationship between model scale and uncertainty expression quality is not explored for larger models.

## Limitations

- The paper's claims about LLM self-evaluation calibration robustness are weakly supported, with minimal empirical validation across diverse model families or reasoning types
- Isotonic regression effectiveness is asserted but lacks detailed analysis or comparison against alternative calibration methods
- Uncertainty expression placement strategy claims lack comprehensive validation beyond the tested conditions
- The evaluation focuses on technical calibration metrics rather than user-centered measures of expression quality or utility

## Confidence

- High confidence in the core finetuning methodology and experimental design due to clear procedural details and use of standard metrics and datasets
- Medium confidence in the calibration improvements claimed, as gains vary across datasets and placement strategies despite statistical significance
- Low confidence in the generalizability of self-evaluation calibration findings due to weak external evidence and limited scope of tested models and tasks

## Next Checks

1. Test the self-evaluation calibration hypothesis across diverse LLM architectures and multiple reasoning types to establish robustness of the self-evaluation mechanism

2. Replace isotonic regression with parametric calibration methods (Platt scaling, temperature scaling) and compare calibration performance to determine optimality

3. Conduct human evaluation studies where participants interact with models emitting calibrated uncertainty expressions versus models without such expressions, measuring whether calibrated uncertainty improves user decision-making or trust calibration in practical applications