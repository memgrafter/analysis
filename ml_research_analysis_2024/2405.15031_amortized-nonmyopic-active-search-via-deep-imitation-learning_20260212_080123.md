---
ver: rpa2
title: Amortized nonmyopic active search via deep imitation learning
arxiv_id: '2405.15031'
source_url: https://arxiv.org/abs/2405.15031
tags:
- search
- policy
- learning
- data
- cited
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the computational challenge of applying budget-aware
  nonmyopic active search (AS) to large-scale search problems by proposing a neural
  network-based imitation learning approach. The authors train a policy network using
  DAGGER to mimic the behavior of the state-of-the-art ENS policy, which has superlinear
  computational complexity.
---

# Amortized nonmyopic active search via deep imitation learning

## Quick Facts
- arXiv ID: 2405.15031
- Source URL: https://arxiv.org/abs/2405.15031
- Authors: Quan Nguyen; Anindya Sarkar; Roman Garnett
- Reference count: 40
- One-line primary result: Neural network policy trained via DAGGER closely approximates ENS performance while being orders of magnitude faster

## Executive Summary
This work addresses the computational challenge of applying budget-aware nonmyopic active search (AS) to large-scale search problems by proposing a neural network-based imitation learning approach. The authors train a policy network using DAGGER to mimic the behavior of the state-of-the-art ENS policy, which has superlinear computational complexity. The trained network learns a beneficial nonmyopic search strategy that balances exploration and exploitation, achieving competitive performance on real-world tasks while being significantly faster than ENS. Extensive experiments on diverse applications, including large-scale drug discovery problems with millions of candidates, demonstrate the effectiveness and scalability of the proposed approach.

## Method Summary
The authors propose to amortize the nonmyopic ENS policy by training a neural network using DAGGER imitation learning. They generate synthetic search problems using a Gaussian process with clustered point distributions, then train a 5-layer feedforward network to predict ENS actions from engineered features. The features include posterior probability, remaining budget, and neighbor-based statistics. During deployment, the trained network uses FAISS for efficient nearest neighbor search to quickly identify the next query point, achieving orders of magnitude speedup over the original ENS policy while maintaining competitive performance.

## Key Results
- ANS policy closely approximates ENS performance on drug discovery tasks with up to 6.7 million candidates
- ANS outperforms cheaper myopic baselines (UCB, ETC) by finding more targets on average
- ANS achieves 100-1000x speedup over ENS while maintaining similar target discovery rates

## Why This Works (Mechanism)

### Mechanism 1
The DAGGER imitation learning loop allows the neural network to progressively refine its policy by observing states it actually encounters, rather than only states from the expert's trajectory. At each training iteration, the current policy is rolled out on synthetic problems, its visited states are recorded, and the expert (ENS) provides actions for those states. These state-action pairs are added to the training set, ensuring the policy learns to act well in states it will actually see at test time.

### Mechanism 2
The four engineered features (posterior probability, remaining budget, neighbor probability sum, neighbor similarity sum) provide a compact yet informative state representation that captures both the current belief about each candidate and the budget-aware exploration-exploitation tradeoff. Each unlabeled point is represented by these features, concatenated into a feature vector. The neural network learns to map this feature vector to the expert's action (which point to query next), implicitly learning the exploration-exploitation balance.

### Mechanism 3
Training on synthetic problems with a Gaussian process prior allows the policy network to learn a generalizable search strategy that transfers to real-world problems with different structures and sizes. The synthetic problem generator creates diverse search spaces with clusters and dispersed points, labeled using a GP with varying prevalence rates. The policy network trained on these problems learns to handle different target distributions and search space structures.

## Foundational Learning

- Active Search (AS) as a sequential decision-making problem under a labeling budget.
  - Why needed here: Understanding AS is crucial for grasping the problem the policy network is trying to solve and the expert policy it is imitating.
  - Quick check question: In AS, what is the goal at each iteration, and how is the utility function defined?

- Imitation Learning, specifically DAGGER, for learning from an expert policy.
  - Why needed here: The paper's approach relies on imitating the expert policy ENS using DAGGER to overcome the challenge of learning from scratch in a large action space.
  - Quick check question: How does DAGGER differ from vanilla behavior cloning, and why is this difference important in the context of active search?

- Feature Engineering for State Representation in Reinforcement Learning.
  - Why needed here: The paper's success depends on crafting informative features that capture the essential information for the policy to make good decisions.
  - Quick check question: What are the four features used to represent each candidate point, and how do they relate to the exploration-exploitation tradeoff?

## Architecture Onboarding

- Component map: Synthetic problem generator -> DAGGER training loop -> Policy network (5-layer feedforward) -> Expert policy (ENS) -> Feature extraction module -> Deployment module

- Critical path:
  1. Generate synthetic problem (GP sample, thresholding)
  2. Run current policy on problem, collect states
  3. Query expert (ENS) for actions on those states
  4. Add state-action pairs to training set
  5. Train policy network on aggregated training set
  6. Deploy trained policy on real-world problems

- Design tradeoffs:
  - Feature engineering vs. raw state representation: Features provide interpretability and efficiency but may miss important information.
  - Synthetic vs. real training data: Synthetic data allows efficient DAGGER training but may not fully capture real-world complexity.
  - Imitation learning vs. reinforcement learning: Imitation learning leverages the expert policy but may not find better strategies.

- Failure signatures:
  - Policy consistently underperforms ENS: Possible issues with feature engineering, training data diversity, or network architecture.
  - Policy fails to generalize to new problem types: Synthetic problem generator may not produce diverse enough problems.
  - Policy training is unstable or slow: DAGGER implementation or hyperparameter choices may need adjustment.

- First 3 experiments:
  1. Train the policy network using only the posterior probability feature and compare performance to the full feature set on a small-scale problem.
  2. Vary the number of clusters and points in the synthetic problem generator and observe the effect on policy performance.
  3. Implement a simpler imitation learning algorithm (e.g., behavior cloning) and compare its performance to DAGGER on a small-scale problem.

## Open Questions the Paper Calls Out

### Open Question 1
How much does the performance gap between the proposed ANS policy and the expert ENS policy grow as the search space size increases beyond the tested multi-million scale? The paper demonstrates ANS outperforming cheaper baselines and closely approximating ENS performance on multi-million scale drug discovery tasks, but doesn't test beyond this scale.

### Open Question 2
Would alternative neural network architectures like transformers with permutation invariance properties significantly improve the policy's ability to approximate ENS behavior? The paper mentions Liu et al. [30] showed transformers can learn from datasets of different sizes, which could be useful in AS, suggesting this as a promising future direction.

### Open Question 3
What is the optimal balance between exploration and exploitation phases when refining the policy network using REINFORCE during repeated searches? The paper found that attempting to refine the policy using REINFORCE actually hurts performance, resulting in an increasing gap in reward.

## Limitations

- The approach depends heavily on the quality of the synthetic problem generator and the expert policy ENS, with no theoretical guarantees on approximation quality.
- The feature engineering approach may miss important information not captured by the four chosen features.
- Performance gains are primarily shown relative to myopic baselines, with less direct comparison to other nonmyopic approaches.

## Confidence

- High: Claims about computational speedup (direct timing measurements)
- High: Claims about competitive performance on benchmark tasks (direct comparisons to ENS)
- Medium: Claims about the general transferability of the learned policy (based on limited problem diversity)
- Medium: Claims about the sufficiency of the feature representation (based on empirical results without ablation studies)

## Next Checks

1. Perform an ablation study on the feature engineering by systematically removing each feature and measuring performance degradation.
2. Test the policy on problems with structures significantly different from GP-generated clusters (e.g., ring-shaped distributions or adversarial arrangements).
3. Compare the imitation-learned policy against alternative nonmyopic policies like UCB or IDS on small-scale problems where these baselines are computationally feasible.