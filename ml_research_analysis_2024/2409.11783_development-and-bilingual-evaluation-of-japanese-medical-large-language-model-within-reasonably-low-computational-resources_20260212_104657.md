---
ver: rpa2
title: Development and bilingual evaluation of Japanese medical large language model
  within reasonably low computational resources
arxiv_id: '2409.11783'
source_url: https://arxiv.org/abs/2409.11783
tags:
- medical
- japanese
- llms
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents JMedLLM-v1-7B, a 7B-parameter Japanese medical
  large language model (LLM) developed to address the computational and financial
  challenges of deploying large-scale medical AI in clinical settings. The model is
  built on Qwen2-7B and fine-tuned using Japanese medical corpora (naika-text) and
  translated USMLE data, enabling operation on modest hardware.
---

# Development and bilingual evaluation of Japanese medical large language model within reasonably low computational resources

## Quick Facts
- arXiv ID: 2409.11783
- Source URL: https://arxiv.org/abs/2409.11783
- Reference count: 40
- JMedLLM-v1-7B achieves over 50% accuracy in bilingual medical benchmarks, outperforming several 70B models while running on a single GPU

## Executive Summary
This paper presents JMedLLM-v1-7B, a 7B-parameter Japanese medical large language model developed to address computational and financial challenges of deploying large-scale medical AI in clinical settings. The model is built on Qwen2-7B and fine-tuned using Japanese medical corpora and translated USMLE data, enabling operation on modest hardware. Evaluation across four bilingual medical benchmarks shows the model achieves over 50% accuracy in both English and Japanese, surpassing existing 7B models and approaching the performance of 70B-parameter models. Notably, the model improves in both languages after Japanese medical fine-tuning, demonstrating effective cross-lingual knowledge transfer. Training required only 7.5 hours on 8 GPUs, and inference runs on a single GPU, making it accessible for clinical deployment.

## Method Summary
JMedLLM-v1-7B is developed by fine-tuning Qwen2-7B-Instruct through two sequential stages: medical full-parameter training (MFPT) on Japanese medical corpus (naika-text) for 5 epochs, followed by medical parameter-efficient fine-tuning (MPEFT) using LoRA on translated USMLE data for 5 epochs. The model is evaluated on four bilingual medical benchmarks (IgakuQA, MedQA, MedMCQA, MMLU/JMMLU) using zero-shot Chain-of-Thought prompting and Gestalt accuracy metric. The entire training process requires 7.5 hours on 8 GPUs for MFPT and 28.5 hours on 4 GPUs for MPEFT.

## Key Results
- Achieves over 50% accuracy in both English and Japanese medical benchmarks
- Outperforms several 70B-parameter models while using only 7B parameters
- Demonstrates cross-lingual knowledge transfer, improving performance in both languages after Japanese fine-tuning
- Requires only 7.5 hours on 8 GPUs for training and runs on a single GPU for inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual knowledge transfer improves performance in both languages when fine-tuning an English-centric base model on Japanese medical data.
- Mechanism: Fine-tuning on Japanese medical text activates shared latent medical knowledge in the model, which enhances its ability to answer medical questions in both Japanese and English.
- Core assumption: The base model already encodes foundational medical knowledge, and fine-tuning on Japanese data enhances this latent knowledge without erasing English capabilities.
- Evidence anchors:
  - [abstract] "We find that fine-tuning an English-centric base model on Japanese medical dataset improves the score in both language, supporting the effect of cross-lingual knowledge transfer."
  - [section 4] "an unexpected observation is that our models demonstrated improvements of +1.4% for MFPT and +2.9% for MPEFT, despite the fine-tuning training data consisting solely of Japanese texts."
  - [corpus] Weak - the corpus only lists related papers, not direct evidence for this mechanism.

### Mechanism 2
- Claim: Parameter-efficient fine-tuning (LoRA) with medical data achieves comparable performance to full-parameter fine-tuning while using fewer resources.
- Mechanism: LoRA adapts the model by learning low-rank updates to the weight matrices, allowing efficient specialization to the medical domain without retraining all parameters.
- Core assumption: The base model's architecture is sufficiently expressive that low-rank updates can capture domain-specific patterns effectively.
- Evidence anchors:
  - [abstract] "The model is built on Qwen2-7B and fine-tuned using Japanese medical corpora (naika-text) and translated USMLE data."
  - [section 2.2.2] "we apply LoRA (Low Rank Adaptation) [17], a parameter-efficient fine-tuning method that can drastically save computational resources (especially GPU memory) without significant performance loss compared to full-parameter training."
  - [corpus] Weak - corpus does not directly address LoRA performance.

### Mechanism 3
- Claim: Using domain-specific corpus (naika-text) before instruction-tuning (MPEFT) improves performance by activating latent medical knowledge.
- Mechanism: Continual pre-training on medical text aligns the model's internal representations with medical concepts before fine-tuning on question-answering data, leading to better downstream performance.
- Core assumption: The model's pre-training has not fully specialized in medical knowledge, and additional exposure to medical text enhances its ability to process medical queries.
- Evidence anchors:
  - [abstract] "The model is built on Qwen2-7B and fine-tuned using Japanese medical corpora (naika-text) and translated USMLE data."
  - [section 2.2.1] "we used the naika-text corpus, which is composed of 6120 lines of Japanese sentences (3.5M letters) extracted from the Journal of the Japanese Society of Internal Medicine."
  - [section 3.1] "it is observed that both the MFPT and MPEFT processes have steadily contributed to score improvement."
  - [corpus] Weak - corpus does not directly address the effect of naika-text.

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (LoRA)
  - Why needed here: Enables adaptation to medical domain without the computational cost of full fine-tuning, making deployment feasible on modest hardware.
  - Quick check question: What is the primary advantage of using LoRA over full fine-tuning in terms of resource usage?
  - Answer: LoRA drastically reduces GPU memory requirements and training time by only updating a small subset of parameters.

- Concept: Cross-lingual knowledge transfer
  - Why needed here: Demonstrates that Japanese medical fine-tuning can improve performance in both Japanese and English, maximizing the utility of the model.
  - Quick check question: Why might fine-tuning on Japanese medical data improve English medical question-answering performance?
  - Answer: The base model likely has latent medical knowledge that becomes more accessible when aligned with medical concepts in any language.

- Concept: Continual pre-training
  - Why needed here: Pre-training on domain-specific corpus (naika-text) before instruction-tuning helps the model internalize medical concepts, leading to better fine-tuning results.
  - Quick check question: What is the purpose of MFPT (Medical Full-Parameter Training) before MPEFT?
  - Answer: MFPT adapts the model's general knowledge to the medical domain, making the subsequent instruction-tuning more effective.

## Architecture Onboarding

- Component map: Qwen2-7B-Instruct -> MFPT on naika-text -> LoRA adapters -> MPEFT on USMLE -> Evaluation on medical benchmarks -> Single GPU inference

- Critical path:
  1. Load Qwen2-7B-Instruct
  2. Perform MFPT on naika-text for 5 epochs
  3. Apply LoRA adapters
  4. Perform MPEFT on USMLE for 5 epochs
  5. Evaluate on IgakuQA, MedQA, MedMCQA, MMLU/JMMLU
  6. Deploy for inference on single GPU

- Design tradeoffs:
  - Model size vs. performance: 7B parameters chosen to balance computational efficiency with medical accuracy
  - Language coverage: Japanese focus with English evaluation to demonstrate cross-lingual transfer
  - Training strategy: MFPT + MPEFT chosen over direct fine-tuning to maximize knowledge extraction
  - Inference settings: Deterministic beam search prioritized for reproducibility over diversity

- Failure signatures:
  - Poor performance on both languages: Indicates base model lacks foundational medical knowledge
  - Good Japanese performance but poor English: Suggests catastrophic forgetting of English capabilities
  - Good English performance but poor Japanese: Indicates insufficient Japanese medical data or adaptation
  - High variance across runs: May indicate insufficient training epochs or unstable LoRA configuration

- First 3 experiments:
  1. Compare MFPT vs. direct MPEFT to isolate the effect of medical corpus pre-training
  2. Test different LoRA rank values to find optimal parameter-efficiency tradeoff
  3. Evaluate zero-shot vs. few-shot performance to assess in-context learning capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed cross-lingual knowledge transfer in JMedLLM-v1 generalize to other language pairs beyond Japanese and English?
- Basis in paper: [inferred] The paper demonstrates that fine-tuning a Japanese medical corpus improves performance in both Japanese and English, suggesting cross-lingual transfer. However, it does not test other language pairs.
- Why unresolved: The study focuses solely on Japanese-English bilingual evaluation. Testing with other language pairs (e.g., Spanish-English or Chinese-English) would determine if the cross-lingual transfer effect is a general phenomenon or specific to Japanese.
- What evidence would resolve it: Fine-tuning a model with a medical corpus in a third language (e.g., Spanish) and evaluating its performance on both the third language and English benchmarks, comparing against models fine-tuned only on English data.

### Open Question 2
- Question: What is the optimal balance between medical domain-specific fine-tuning and general language model capabilities for achieving the best performance on medical question-answering tasks?
- Basis in paper: [explicit] The paper shows that both medical full-parameter training (MFPT) and medical parameter-efficient fine-tuning (MPEFT) contribute to performance improvements, but does not explore the optimal balance between these approaches.
- Why unresolved: The study applies both MFPT and MPEFT sequentially but does not experiment with varying the extent of each or exploring alternative fine-tuning strategies (e.g., domain-adaptive pretraining vs. instruction tuning).
- What evidence would resolve it: Systematic ablation studies varying the amount of MFPT vs. MPEFT, or comparing against other fine-tuning strategies like continual pretraining or instruction tuning, to identify which approach yields the best performance per computational cost.

### Open Question 3
- Question: How does the performance of JMedLLM-v1 compare to larger models (e.g., 70B parameters) when both are evaluated under identical computational constraints (e.g., same GPU memory limits)?
- Basis in paper: [explicit] The paper demonstrates that JMedLLM-v1-7B outperforms several 70B-parameter models in bilingual medical benchmarks, but the 70B models are evaluated with 4-bit quantization due to computational constraints.
- Why unresolved: The comparison between 7B and 70B models is confounded by quantization and different hardware setups. A fair comparison would require both model sizes to operate under the same computational constraints.
- What evidence would resolve it: Evaluating JMedLLM-v1-7B and a 70B model (e.g., OpenBioLLM-70B) on the same hardware with identical computational constraints (e.g., both using 4-bit quantization or both using full precision within the same memory budget), measuring both performance and inference latency.

## Limitations
- The cross-lingual knowledge transfer mechanism is observed empirically but lacks theoretical explanation for why Japanese fine-tuning improves both languages
- Evaluation is limited to multiple-choice questions, not testing open-ended clinical reasoning or real-world clinical decision-making
- While the model can run on a single GPU, real-world clinical deployment would require additional validation for latency, throughput, and EHR integration

## Confidence

**High Confidence:** The core claim that JMedLLM-v1-7B achieves over 50% accuracy in both English and Japanese medical benchmarks is well-supported by the reported evaluation results. The methodology for training and evaluation is clearly described, and the performance improvements over baseline models are statistically significant.

**Medium Confidence:** The claim about cross-lingual knowledge transfer improving both languages is supported by empirical evidence but lacks theoretical grounding. While the authors observe performance improvements in both languages after Japanese fine-tuning, the mechanism driving this transfer is not fully explained.

**Medium Confidence:** The assertion that the model can run on a single GPU for clinical deployment is technically accurate given the 7B parameter size, but real-world clinical deployment would require additional validation for latency, throughput, and integration with existing hospital systems.

## Next Checks
1. **Cross-lingual Transfer Mechanism:** Conduct ablation studies comparing models fine-tuned only on English medical data, only on Japanese medical data, and bilingual fine-tuning to isolate the specific contributions of each approach to cross-lingual performance.

2. **Open-ended Clinical Reasoning:** Evaluate the model on open-ended clinical scenarios and patient case studies beyond multiple-choice questions to assess its ability to handle real-world clinical decision-making and reasoning over longer contexts.

3. **Clinical Deployment Validation:** Test the model in simulated clinical workflows to measure inference latency, token generation costs, and integration requirements with electronic health record systems to verify the claimed deployment feasibility on modest hardware.