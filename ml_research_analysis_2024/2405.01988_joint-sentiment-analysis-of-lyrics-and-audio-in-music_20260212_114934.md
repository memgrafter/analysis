---
ver: rpa2
title: Joint sentiment analysis of lyrics and audio in music
arxiv_id: '2405.01988'
source_url: https://arxiv.org/abs/2405.01988
tags:
- music
- audio
- lyrics
- emotions
- sentiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated sentiment analysis of music by combining
  audio and lyrical information. Separate models for audio and lyrics were evaluated,
  with lyrics-only models outperforming audio-only models in valence detection.
---

# Joint sentiment analysis of lyrics and audio in music

## Quick Facts
- arXiv ID: 2405.01988
- Source URL: https://arxiv.org/abs/2405.01988
- Authors: Lea Schaab; Anna Kruspe
- Reference count: 27
- Primary result: Weighted fusion of audio and lyrics achieves best sentiment classification performance

## Executive Summary
This study investigates sentiment analysis of music by combining audio and lyrical information using Russell's circumplex model of affect. The research evaluates separate models for audio and lyrics, finding that lyrics-only models outperform audio-only models for valence detection. A weighted fusion approach combining both modalities achieves the best results, with optimal performance at a 60% audio and 40% text weighting. The analysis reveals challenges in classifying negative emotions and highlights the importance of considering both modalities, as audio and lyrics often convey different sentiments intentionally.

## Method Summary
The study employs separate models for audio and lyrics that are evaluated individually before being combined through fusion strategies. Audio features are extracted using the USC SAIL Short-Chunk CNN with residual connections, while multiple text models are tested including lyrics-specific, general sentiment, poem-specific, and multi-emotion variants. The VA Dataset (133 audio excerpts) and MIREX-like Dataset (903 audio clips, 764 lyrics) are preprocessed and mapped to quadrant classifications based on valence-arousal annotations. Fusion methods include weighted combination (60% audio, 40% text), probability averaging, and highest probability selection, with performance evaluated across the four quadrants of Russell's circumplex model.

## Key Results
- Lyrics-only models outperform audio-only models in valence detection
- Weighted fusion with 60% audio and 40% text weighting achieves best overall performance
- Classification of negative emotions remains challenging across all models
- Audio and lyrics often convey different sentiments intentionally, requiring multimodal analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal fusion improves valence classification when audio and text models have complementary strengths.
- Mechanism: Separate models process audio and lyrics, then weighted combination (60% audio, 40% text) yields better performance than either modality alone.
- Core assumption: Audio and lyrics convey different aspects of sentiment, and combining them captures more information than either modality individually.
- Evidence anchors:
  - [abstract]: "A weighted fusion approach combining both modalities achieved the best results, with a 60% audio and 40% text weighting."
  - [section]: "The best results were achieved with a weighting of 60% for audio and 40% for text. Compared to the first two fusion methods, the weighted approach showed improvements in nearly all metrics."
  - [corpus]: Weak evidence - corpus papers focus on AI-generated content detection rather than multimodal sentiment fusion, suggesting limited direct evidence for this specific mechanism.
- Break condition: If audio and lyrics convey highly correlated or redundant sentiment information, the fusion approach may not provide significant improvement.

### Mechanism 2
- Claim: Lyrics models outperform audio models for valence detection in music sentiment analysis.
- Mechanism: Text-based models (especially those fine-tuned for lyrics) capture semantic and emotional content more effectively than audio features for valence classification.
- Core assumption: The semantic content of lyrics directly conveys emotional valence more reliably than audio features alone.
- Evidence anchors:
  - [abstract]: "Separate models for audio and lyrics were evaluated, with lyrics-only models outperforming audio-only models in valence detection."
  - [section]: "Surprisingly, the best lyrics model surpasses the results of the audio model, confirming the relevance of lyrics for the valence recognition task."
  - [corpus]: No direct evidence - corpus papers focus on lyrics detection rather than comparing lyrics vs audio sentiment performance.
- Break condition: If audio features contain sufficient emotional information that is not captured in lyrics, or if lyrics are less available or less relevant for certain music genres.

### Mechanism 3
- Claim: Simple fusion strategies (probability averaging and highest probability selection) can achieve comparable results to more complex methods.
- Mechanism: Basic statistical combination of model outputs provides effective multimodal integration without requiring complex architecture.
- Core assumption: Simple aggregation methods can capture complementary information from different modalities effectively.
- Evidence anchors:
  - [section]: "The second method involved averaging the predictions of both models for each class and selecting the class with the higher average probability. Interestingly, the results from this averaging method mirrored those of the highest probability method, indicating no significant difference in performance between these two approaches."
  - [corpus]: Weak evidence - corpus papers do not provide evidence about fusion strategy effectiveness, focusing instead on detection tasks.
- Break condition: If the relationship between modalities is more complex, requiring weighted or learned fusion strategies rather than simple averaging.

## Foundational Learning

- Concept: Russell's circumplex model of affect
  - Why needed here: The study uses valence and arousal dimensions to classify emotions into four quadrants, which is fundamental to understanding the experimental setup and evaluation metrics.
  - Quick check question: How many dimensions does Russell's circumplex model use to represent emotions, and what are they called?

- Concept: Multimodal machine learning
  - Why needed here: The paper combines audio and text models, requiring understanding of how to process different data types and fuse their outputs effectively.
  - Quick check question: What are the two main approaches to combining outputs from different modality models mentioned in the paper?

- Concept: Sentiment analysis model evaluation
  - Why needed here: The paper evaluates models using precision, recall, F-score, and compares performance across different emotion classification approaches.
  - Quick check question: Which metric showed the best performance improvement when combining audio and text models compared to using either modality alone?

## Architecture Onboarding

- Component map: Data preprocessing → Audio model inference → Text model inference → Fusion layer → Final classification output

- Critical path: Data preprocessing → Individual modality model inference → Fusion decision → Final classification output

- Design tradeoffs:
  - Model complexity vs. performance: Simple fusion methods achieved similar results to weighted approaches
  - Modality weighting: 60/40 audio-text ratio worked best, but this may vary with different datasets
  - Token limitation: BERT models limited to 512 tokens required chunking strategy for longer lyrics

- Failure signatures:
  - Poor performance on negative emotions across all models suggests systematic challenges
  - Misclassifications between Q2 (negative valence, high arousal) and Q4 (negative valence, low arousal) indicate difficulty distinguishing emotional intensity
  - Inconsistent results across different text models suggest sensitivity to model selection

- First 3 experiments:
  1. Run audio-only model on VA dataset and compare quadrant classification accuracy to baseline
  2. Test different text models individually on the same dataset to identify best-performing architecture
  3. Implement simple fusion strategy (averaging predictions) and compare performance to individual models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal weighting ratio between audio and text modalities for music sentiment analysis across different music genres?
- Basis in paper: [explicit] The study found a 60% audio and 40% text weighting performed best, but this was tested on a specific dataset without genre-specific analysis.
- Why unresolved: The study used a general dataset without examining how different music genres might require different audio-text weightings due to their distinct characteristics.
- What evidence would resolve it: Genre-specific experiments testing various audio-text weightings on datasets categorized by music genre to determine optimal ratios for each genre type.

### Open Question 2
- Question: How can we develop a standardized emotion taxonomy that consistently maps across different annotation schemes and models for music sentiment analysis?
- Basis in paper: [explicit] The study noted significant challenges with emotion taxonomy inconsistencies and necessary adjustments affecting results.
- Why unresolved: Current research lacks agreement on emotion taxonomies, and the study had to make subjective adjustments to align different annotation schemes.
- What evidence would resolve it: A comprehensive study comparing multiple existing taxonomies and developing a unified framework validated across different datasets and annotation schemes.

### Open Question 3
- Question: What are the most effective multimodal deep learning architectures for simultaneously processing and fusing audio and lyrics information in music sentiment analysis?
- Basis in paper: [inferred] The study suggested developing novel multimodal models as future work but did not explore advanced architectures beyond simple fusion methods.
- Why unresolved: The study only tested basic fusion approaches (class selection, averaging, weighted combination) without exploring complex multimodal deep learning architectures.
- What evidence would resolve it: Comparative studies testing various multimodal architectures (e.g., transformer-based models, attention mechanisms) against current single-modality approaches.

## Limitations
- Limited dataset size (133 audio excerpts) may not generalize to broader music sentiment analysis
- Optimal 60-40 weighting ratio may be dataset-specific and not applicable across genres
- Systematic challenges in classifying negative emotions suggest potential model or dataset bias

## Confidence
- High Confidence: Lyrics models outperforming audio models for valence detection is supported by direct comparison results showing lyrics models achieving better performance metrics.
- Medium Confidence: The 60-40 weighted fusion approach as optimal is supported by experimental results but may be dataset-specific and not universally applicable.
- Low Confidence: The claim that simple fusion strategies achieve comparable results to more complex methods is based on limited evidence, with only two basic approaches tested.

## Next Checks
1. Evaluate the 60-40 weighted fusion approach on a larger, more diverse music dataset to verify if the optimal weighting ratio remains consistent across different music genres and cultural contexts.

2. Conduct a detailed error analysis specifically focused on negative emotion classifications to identify whether the poor performance stems from dataset imbalance, model architecture limitations, or inherent difficulties in distinguishing negative emotional states.

3. Modify the experimental framework to include temporal analysis of sentiment changes within songs, testing whether sentiment varies significantly across different sections and whether this affects overall classification accuracy.