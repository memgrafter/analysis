---
ver: rpa2
title: Rapid Switching and Multi-Adapter Fusion via Sparse High Rank Adapters
arxiv_id: '2407.16712'
source_url: https://arxiv.org/abs/2407.16712
tags:
- lora
- shira
- adapter
- adapters
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Sparse High Rank Adapters (SHiRA) for efficient
  model adaptation, addressing rapid adapter switching and concept-loss in multi-adapter
  fusion. SHiRA directly finetunes 1-2% of base model weights using gradient masking,
  enabling extreme sparsity without inference overhead.
---

# Rapid Switching and Multi-Adapter Fusion via Sparse High Rank Adapters

## Quick Facts
- arXiv ID: 2407.16712
- Source URL: https://arxiv.org/abs/2407.16712
- Authors: Kartikeya Bhardwaj; Nilesh Prasad Pandey; Sweta Priyadarshi; Viswanath Ganapathy; Rafael Esteves; Shreya Kadambi; Shubhankar Borse; Paul Whatmough; Risheek Garrepalli; Mart Van Baalen; Harris Teague; Markus Nagel
- Reference count: 18
- One-line primary result: SHiRA outperforms LoRA by up to 2.7% accuracy while modifying only 1% of parameters versus 66.72% for LoRA

## Executive Summary
This paper introduces Sparse High Rank Adapters (SHiRA), a parameter-efficient fine-tuning method that directly modifies 1-2% of base model weights using gradient masking. SHiRA enables rapid adapter switching and reduces concept loss in multi-adapter fusion scenarios. The method demonstrates superior performance on both language models (LLaMA-7B, LLaMA2-7B) and vision models (Stable Diffusion), achieving up to 2.7% better accuracy than LoRA while using significantly fewer trainable parameters.

## Method Summary
SHiRA uses gradient masking to selectively update only 1-2% of base model weights during training, creating high-rank adaptations without the need for low-rank projection matrices. The method employs different mask strategies (Structured, Random, Weight Magnitude, Gradient, SNIP) to determine which weights to finetune. During inference, SHiRA enables rapid adapter switching through scatter operations that efficiently update base weights with sparse adapter weights. The implementation uses PEFT for memory-efficient training, achieving up to 16.63% lower peak GPU memory compared to DoRA while maintaining competitive performance.

## Key Results
- SHiRA outperforms LoRA by 1.9-2.7% on commonsense reasoning tasks while modifying only 1% of parameters versus 66.72% for LoRA
- SHiRA matches DoRA performance with 16.63% lower peak GPU memory and faster training
- On vision tasks, SHiRA achieves superior HPSv2 scores for style transfer and eliminates artifacts in multi-adapter fusion
- Rapid switching latency is up to 10× faster than LoRA through scatter-based weight updates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: High sparsity (1-2% trainable weights) enables rapid switching without inference overhead
- **Mechanism**: By only modifying 1-2% of base weights and storing them as sparse weights with indices, SHiRA can rapidly switch adapters by overwriting base weights at specific indices rather than re-fusing entire weight matrices
- **Core assumption**: Sparse storage and scatter operations are more efficient than LoRA fusion for adapter switching
- **Evidence anchors**:
  - [abstract]: "This high sparsity incurs no inference overhead, enables rapid switching directly in the fused mode"
  - [section 3.2]: "we can simply extract them out and store them as sparse weights and their indices... adapter can be efficiently switched with different weights at inference"
  - [corpus]: Weak - no direct citations supporting this specific mechanism
- **Break condition**: If scatter operations become slower than LoRA fusion for the target hardware/weight dimensions, or if storing sparse weights+indices consumes more memory than expected

### Mechanism 2
- **Claim**: High sparsity reduces concept loss in multi-adapter fusion
- **Mechanism**: SHiRA's highly sparse masks (98-99% zeros) result in adapters that are more orthogonal to each other, reducing interference when multiple adapters are fused
- **Core assumption**: The orthogonality of sparse masks translates to reduced concept interference in practice
- **Evidence anchors**:
  - [section 3.2]: "AT1 A2 contains a much higher number of zeros compared to equivalent dense LoRA adapters... would result in significantly lower interference between adapters"
  - [section 4.2.2]: SHiRA-Struct and SNIP "are clearly better at capturing both concepts than LoRA" in multi-adapter scenarios
  - [corpus]: Weak - no direct citations supporting this orthogonality-based concept loss reduction
- **Break condition**: If the learned sparse masks are not truly orthogonal in practice, or if other factors dominate concept loss beyond adapter orthogonality

### Mechanism 3
- **Claim**: High rank sparse adapters can outperform low-rank LoRA adapters
- **Mechanism**: By directly finetuning a small percentage of base weights with gradient masking, SHiRA creates high-rank adaptations that capture more complex patterns than low-rank LoRA projections
- **Core assumption**: High rank is necessary for capturing complex adaptation patterns in LLMs and LVMs
- **Evidence anchors**:
  - [abstract]: "demonstrate that finetuning merely 1-2% parameters in the base model is sufficient for many adapter tasks and significantly outperforms Low Rank Adaptation (LoRA)"
  - [section 4.3.1]: SHiRA "outperform LoRA by 1.9-2.7% on an average on LLaMA-7B"
  - [section 2]: "Kalajdzievski (2023) shows that the high rank adapters can significantly outperform low rank adapters when used with correct scaling factors"
- **Break condition**: If the tasks being adapted are actually low-rank in nature, or if the 1-2% parameters are not optimally chosen for the specific adaptation task

## Foundational Learning

- **Concept**: Gradient masking for parameter-efficient training
  - **Why needed here**: SHiRA uses gradient masking to selectively update only 1-2% of weights during training
  - **Quick check question**: How does gradient masking differ from weight masking, and why is it appropriate for SHiRA's training approach?

- **Concept**: Sparse tensor operations and scatter operations
  - **Why needed here**: SHiRA's rapid switching implementation relies on scatter operations to efficiently update base weights with sparse adapter weights
  - **Quick check question**: What are the computational complexity differences between scatter operations and traditional matrix operations for updating weight tensors?

- **Concept**: Adapter orthogonality and concept interference
  - **Why needed here**: Understanding how adapter orthogonality affects concept loss in multi-adapter fusion is crucial for SHiRA's design rationale
  - **Quick check question**: How is adapter orthogonality typically measured, and what are the theoretical implications for concept preservation in multi-adapter settings?

## Architecture Onboarding

- **Component map**: Base model weights (frozen except for 1-2% trainable positions) -> Sparse mask (determines which weights are trainable) -> Gradient masking mechanism (applies during backpropagation) -> Sparse weight storage (stores modified weights and their indices) -> Scatter operation implementation (for efficient weight updates during inference)

- **Critical path**: Training → Mask creation → Gradient masking during backpropagation → Sparse weight extraction → Inference with scatter-based weight updates

- **Design tradeoffs**:
  - Sparsity level (1-2% chosen as optimal balance between performance and efficiency)
  - Mask type selection (Structured, Random, Weight Magnitude, Gradient, SNIP)
  - Training vs inference memory consumption (PEFT-based implementation reduces peak memory by ~16%)
  - Adapter switching speed vs storage requirements (sparse storage enables rapid switching)

- **Failure signatures**:
  - Performance degradation if sparsity is too high (>98% zeros)
  - Memory issues if sparse storage format is inefficient
  - Concept loss if mask selection doesn't capture task-relevant parameters
  - Training instability if gradient masking interferes with optimizer dynamics

- **First 3 experiments**:
  1. Compare SHiRA vs LoRA performance on a simple adaptation task (e.g., style transfer) with varying sparsity levels (1%, 2%, 5%)
  2. Benchmark adapter switching speed between SHiRA's scatter implementation and LoRA's fusion approach
  3. Test different mask types (SHiRA-Struct, SHiRA-Rand, SHiRA-WM, SHiRA-Grad, SHiRA-SNIP) on the same task to identify optimal mask strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for creating the sparse mask (SHiRA-Struct, SHiRA-Rand, SHiRA-WM, SHiRA-Grad, SHiRA-SNIP) across different model architectures and tasks?
- Basis in paper: [explicit] The paper presents five different mask strategies but does not provide a systematic comparison or guidelines for selecting the optimal strategy based on task characteristics or model architecture.
- Why unresolved: The experiments only show performance differences between strategies on specific tasks (LLaMA-7B, LLaMA2-7B, Stable Diffusion) without establishing general principles for strategy selection.
- What evidence would resolve it: A comprehensive ablation study comparing all mask strategies across diverse model architectures (transformers, diffusion models), task types (language, vision, multimodal), and parameter scales would identify patterns in strategy effectiveness.

### Open Question 2
- Question: What is the theoretical relationship between adapter sparsity and concept loss during multi-adapter fusion?
- Basis in paper: [explicit] The paper hypothesizes that high sparsity reduces concept loss but only provides empirical validation without theoretical justification for why this relationship exists.
- Why unresolved: The paper demonstrates reduced concept loss with SHiRA but doesn't explain the mathematical or theoretical mechanism behind this phenomenon.
- What evidence would resolve it: A formal analysis of the interference matrix (A^T_1 A_2) properties under different sparsity regimes, combined with theoretical bounds on concept loss as a function of sparsity level.

### Open Question 3
- Question: What is the relationship between adapter sparsity and downstream task performance across different model scales?
- Basis in paper: [inferred] The paper demonstrates effectiveness at 1-2% sparsity for specific models but doesn't explore how performance scales with sparsity levels or model size.
- Why unresolved: The experiments are limited to specific sparsity levels (1-2%) and model sizes (7B parameters) without exploring the full parameter space.
- What evidence would resolve it: A systematic study varying sparsity levels (e.g., 0.1%, 0.5%, 1%, 2%, 5%) and model scales (1B, 7B, 13B, 70B parameters) across multiple tasks to establish performance-sparsity scaling laws.

## Limitations

- **Implementation Dependency**: Performance claims heavily rely on the PEFT-based implementation, which may not generalize across different hardware or software environments without careful tuning.
- **Sparse Mask Generalization**: The optimal mask type varies by task, and the paper doesn't provide systematic guidelines for selecting the best mask strategy for new adaptation tasks.
- **Concept Loss Quantification**: Claims about reduced concept loss in multi-adapter fusion are supported qualitatively through visual examples but lack rigorous quantitative metrics or ablation studies.

## Confidence

**High Confidence**: The core mechanism of gradient masking to finetune 1-2% of base weights is well-established in parameter-efficient training literature. The empirical results showing SHiRA outperforming LoRA on commonsense reasoning (1.9-2.7% accuracy improvement) and matching DoRA performance with lower memory are supported by experimental data.

**Medium Confidence**: The claims about rapid switching (10× faster than LoRA) and reduced concept loss are supported by experimental results but rely on specific implementation details (scatter operations, PEFT optimization) that may not generalize across all hardware or model architectures without careful tuning.

**Low Confidence**: The theoretical justification for why high sparsity specifically reduces concept loss (orthogonality argument) is presented intuitively but lacks rigorous mathematical proof or extensive ablation studies demonstrating the relationship between sparsity, orthogonality, and concept preservation.

## Next Checks

1. **Cross-Architecture Generalization**: Implement SHiRA on a different LLM architecture (e.g., Falcon, MPT) and verify that the 1-2% sparsity with gradient masking maintains performance advantages over LoRA across different model families and tasks.

2. **Mask Selection Methodology**: Conduct systematic experiments to determine optimal mask selection criteria for different task types (classification vs generation vs vision tasks), potentially developing a decision tree or heuristic for choosing between SHiRA-Struct, SHiRA-Rand, SHiRA-WM, SHiRA-Grad, and SHiRA-SNIP.

3. **Concept Loss Quantification**: Design quantitative metrics for measuring concept preservation in multi-adapter fusion scenarios, including task-specific performance degradation measurements when fusing multiple adapters, and compare SHiRA against LoRA and DoRA under controlled ablation conditions.