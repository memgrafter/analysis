---
ver: rpa2
title: Confidence in the Reasoning of Large Language Models
arxiv_id: '2412.15296'
source_url: https://arxiv.org/abs/2412.15296
tags:
- confidence
- answer
- llms
- accuracy
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the confidence levels of large language
  models (LLMs) in their reasoning abilities, focusing on how their confidence correlates
  with accuracy. The authors measure confidence both qualitatively (by observing the
  tendency to change answers when prompted to reconsider) and quantitatively (through
  self-reported confidence scores).
---

# Confidence in the Reasoning of Large Language Models

## Quick Facts
- arXiv ID: 2412.15296
- Source URL: https://arxiv.org/abs/2412.15296
- Authors: Yudi Pawitan; Chris Holmes
- Reference count: 0
- Primary result: LLMs perform better than random guessing on reasoning tasks but lack reliable confidence measures

## Executive Summary
This paper investigates the confidence levels of large language models (LLMs) in their reasoning abilities, focusing on how their confidence correlates with accuracy. The authors measure confidence both qualitatively (by observing the tendency to change answers when prompted to reconsider) and quantitatively (through self-reported confidence scores). They evaluate three LLMs—GPT4o, GPT4-turbo, and Mistral—on two BIG Bench-Hard tasks (causal judgment and formal fallacies) and statistical puzzles. Results show that while LLMs perform significantly better than random guessing, there is a wide variability in their tendency to change answers. The overall accuracy for second answers is often worse than the first, and LLMs tend to overstate their confidence scores.

## Method Summary
The authors conducted experiments using three LLMs (GPT4o, GPT4-turbo, and Mistral) on reasoning tasks including BIG Bench-Hard challenges and statistical puzzles. They measured confidence through two approaches: qualitative assessment by prompting models to reconsider their answers, and quantitative assessment through self-reported confidence scores. The study examined the relationship between token-level probabilities and actual accuracy, finding that token probabilities only partially explain confidence measures. They also analyzed how parameter counts affect answer-changing behavior and the impact of iterative prompting on accuracy.

## Key Results
- LLMs perform significantly better than random guessing on reasoning tasks but show high variability in answer-changing behavior
- Overall accuracy for second answers is often worse than the first, contradicting the expectation that reconsideration improves accuracy
- Token-level probabilities only partially explain confidence measures, with accuracy being substantially lower than token probability except at extremely high values (>0.99999)

## Why This Works (Mechanism)
None

## Foundational Learning

### Concept 1: Token-level probabilities
- Why needed: Understanding how LLMs generate probabilities at the token level is crucial for interpreting confidence measures
- Quick check: Verify that token probabilities correlate with empirical accuracy rates across different reasoning tasks

### Concept 2: Answer consistency
- Why needed: Measuring how often LLMs maintain their initial answers versus changing them when prompted to reconsider
- Quick check: Compare consistency rates across different LLM models and reasoning task types

### Concept 3: Confidence calibration
- Why needed: Developing methods to align LLM confidence scores with actual accuracy
- Quick check: Test calibration methods across multiple reasoning domains to ensure generalizability

## Architecture Onboarding

### Component map
LLM architecture -> Token generation -> Probability scoring -> Confidence assessment -> Answer generation

### Critical path
Prompt input → Token probability calculation → Confidence scoring → Answer selection → Output generation

### Design tradeoffs
The study reveals tradeoffs between computational efficiency (parameter counts) and answer consistency, with larger models showing both better initial accuracy and greater tendency to change answers.

### Failure signatures
Overstated confidence scores, decreased accuracy upon reconsideration, and poor correlation between token probabilities and actual accuracy indicate fundamental limitations in LLM confidence mechanisms.

### First experiments
1. Test different prompting strategies to see if they systematically improve accuracy or amplify biases
2. Vary parameter counts while controlling for architectural differences to establish causal relationships
3. Develop calibration frameworks to map token probabilities to empirical accuracy rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do larger parameter counts affect LLMs' tendency to change initial answers, and is there a threshold beyond which this tendency becomes detrimental to accuracy?
- Basis in paper: The authors note that Mistral Large 2 (123 billion parameters) shows a greater tendency to change answers compared to Mistral Large, and speculate that GPT4o's improved computational efficiency might indicate fewer parameters than GPT4, which shows more answer-changing behavior.
- Why unresolved: The paper only observes correlations between parameter counts and answer-changing behavior without establishing causal relationships or identifying optimal parameter ranges.
- What evidence would resolve it: Systematic experiments varying parameter counts while controlling for other architectural differences, measuring answer consistency and accuracy across different reasoning tasks.

### Open Question 2
- Question: Can we develop a reliable confidence calibration method that correlates LLMs' token-level probabilities with actual accuracy, accounting for the contextual factors that currently create discrepancies?
- Basis in paper: The authors find that token-level probabilities only partially explain confidence measures, with accuracy being substantially lower than token probability except at extremely high values (>0.99999), and that qualitative confidence based on simple rethink prompts is substantially less than token probability even at high probability values.
- Why unresolved: The complex relationship between token probabilities, contextual factors, and actual accuracy suggests current calibration methods are insufficient for reliable confidence assessment.
- What evidence would resolve it: Development and validation of a calibration framework that accurately maps token probabilities to empirical accuracy rates across diverse reasoning tasks and contexts.

### Open Question 3
- Question: Does iterative prompting beyond simple "rethink" prompts lead to systematic improvements in accuracy, or does it primarily amplify existing biases in LLMs' reasoning?
- Basis in paper: The authors show that accuracy improvements from iterative prompting vary across models and tasks, with some models showing improved accuracy while others show decreased accuracy when prompted to reconsider.
- Why unresolved: The paper only tests simple rethink prompts and doesn't explore the effects of more complex iterative prompting strategies or their long-term impact on reasoning quality.
- What evidence would resolve it: Systematic testing of various iterative prompting strategies across multiple reasoning domains, measuring both accuracy improvements and changes in reasoning patterns over multiple iterations.

## Limitations

- LLMs lack an internally coherent sense of confidence, as evidenced by the discrepancy between self-reported scores and actual accuracy
- Current calibration methods are insufficient, with token-level probabilities only partially explaining confidence measures
- The study's findings are limited to specific reasoning tasks and may not generalize to all LLM applications

## Confidence

High confidence:
- LLMs perform significantly better than random guessing on reasoning tasks
- There is wide variability in answer-changing behavior across different models
- LLMs tend to overstate their confidence scores

Medium confidence:
- Token-level probabilities only partially explain confidence measures
- The relationship between parameter counts and answer-changing behavior requires further validation

## Next Checks

1. Replicate the study with a larger and more diverse set of reasoning tasks to confirm the generalizability of the findings
2. Investigate the impact of different prompting strategies on LLM confidence and answer consistency
3. Develop and test new methods for calibrating LLM confidence scores to improve their alignment with actual accuracy