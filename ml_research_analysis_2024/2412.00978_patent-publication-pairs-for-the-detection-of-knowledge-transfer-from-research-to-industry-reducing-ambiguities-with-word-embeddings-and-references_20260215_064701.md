---
ver: rpa2
title: 'Patent-publication pairs for the detection of knowledge transfer from research
  to industry: reducing ambiguities with word embeddings and references'
arxiv_id: '2412.00978'
source_url: https://arxiv.org/abs/2412.00978
tags:
- patent
- pairs
- data
- patents
- names
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors addressed the challenge of identifying knowledge transfer
  from research to industry by matching scholarly publications to patents, while reducing
  ambiguities in author name matching. They introduced two novel filtering features:
  (1) cosine similarity of document content using BERT word embeddings based on MeSH
  terms, and (2) identification of common references between publications and patents.'
---

# Patent-publication pairs for the detection of knowledge transfer from research to industry: reducing ambiguities with word embeddings and references

## Quick Facts
- arXiv ID: 2412.00978
- Source URL: https://arxiv.org/abs/2412.00978
- Reference count: 16
- Primary result: The authors addressed the challenge of identifying knowledge transfer from research to industry by matching scholarly publications to patents, while reducing ambiguities in author name matching.

## Executive Summary
This study tackles the challenge of identifying knowledge transfer from research to industry by matching scholarly publications to patents. The authors introduce novel filtering features including cosine similarity using BERT word embeddings based on MeSH terms, identification of common references, and automated statistical filtering of patent classes. Their approach was evaluated over a 5-year period using open data from EPO patents and PubMed publications, showing that combinations of three or more common author names, one common reference, or cosine similarity above 0.7 reliably identified valid publication-patent pairs. The complete open-source workflow enables reproducible matching and is being integrated into ZB MED's LIVIVO search portal.

## Method Summary
The authors developed a method to match scholarly publications to patents by normalizing author/inventor names, extracting MeSH terms, and generating BERT embeddings to compute cosine similarity. They identified patent-publication pairs through common names, then filtered these using three criteria: cosine similarity scores, common references (detected via DOI matching), and statistical filtering of patent classes based on IPC distributions. The approach uses name normalization, patent family grouping, and time window constraints to reduce false positives while maintaining recall. The complete workflow is implemented as an open-source web service.

## Key Results
- Using three or more common author/inventor names, one common reference, or cosine similarity above 0.7 reliably identified valid publication-patent pairs
- Manual evaluation confirmed high accuracy, with false pairs dropping to zero for four or more common names
- The automated statistical method for filtering permitted patent classes reduced irrelevant matches while maintaining academic patent coverage
- The approach was successfully implemented and integrated into ZB MED's LIVIVO search portal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Patent-publication pairs are identified by matching inventor/author names and filtering with similarity scores and common references.
- Mechanism: The system uses name normalization, removes duplicates, and applies filters based on shared references and BERT-based cosine similarity to reduce false pairs.
- Core assumption: Shared names and high content similarity reliably indicate a true knowledge transfer link.
- Evidence anchors:
  - [abstract]: "using three or more common author/inventor names, or one common reference, or cosine similarity above 0.7 reliably identified valid publication-patent pairs"
  - [section]: "the number of intersect names is at least 3... the number of common references is at least 1... cosine similarity is at least above the mean value"
- Break condition: Name normalization increases homonyms, potentially introducing false positives when unrelated individuals share names.

### Mechanism 2
- Claim: BERT embeddings of MeSH terms capture relevant topical similarity between patents and publications.
- Mechanism: MeSH terms are extracted from both document types, transformed into BERT vectors, and cosine similarity is computed to assess topical alignment.
- Core assumption: MeSH-based embeddings sufficiently represent domain-specific content overlap between patents and publications.
- Evidence anchors:
  - [abstract]: "cosine similarity of document content using BERT word embeddings based on MeSH terms"
  - [section]: "The condensed content is mapped to a word vector space... We used the BERT base model... provided satisfactory results"
- Break condition: If MeSH coverage is incomplete, important domain terms may be missing, reducing embedding quality.

### Mechanism 3
- Claim: Filtering by IPC (patent class) distributions reduces irrelevant patent-publication pairs.
- Mechanism: IPC class distributions for identified pairs are compared to baseline EPO patents; classes outside 1.5% deviation are excluded.
- Core assumption: Academic patents and sure pairs have significantly different IPC distributions from general EPO patents.
- Evidence anchors:
  - [section]: "we introduce a statistical process to select permitted patent classes automatically... we set a percentage limit of 1.5%... the patent classes of the subset 'sure pairs' that deviate from the baseline are used as filters"
  - [corpus]: No direct corpus evidence for statistical method effectiveness; weak external validation.
- Break condition: If domain overlap is small, statistical filtering may discard true academic patents.

## Foundational Learning

- Concept: Name normalization and disambiguation
  - Why needed here: Inventor/author names in patent and publication datasets are messy; normalization allows consistent matching.
  - Quick check question: What happens if normalization increases homonyms (different people with identical normalized names)?

- Concept: Word embeddings and cosine similarity
  - Why needed here: Documents must be compared numerically to assess topical similarity without manual reading.
  - Quick check question: Why does cosine similarity range between 0 and 1, and what does a value > 0.7 indicate in this context?

- Concept: Patent family grouping
  - Why needed here: Patents can be filed in multiple jurisdictions; grouping into families avoids duplicate matching of the same invention.
  - Quick check question: How does grouping by patent family affect the novelty requirement in publication-patent matching?

## Architecture Onboarding

- Component map: Raw data ingestion (EPO patents, PubMed publications) -> Name normalization pipeline -> MeSH extraction and embedding generation -> Patent family grouping logic -> IPC class statistical filter -> Similarity calculation (BERT cosine) -> Pair filtering and ranking engine -> Evaluation web service

- Critical path: Name normalization → Patent-family grouping → Initial pairing → Feature computation (MeSH embeddings, common references) → Filtering (IPC, thresholds) → Final ranking

- Design tradeoffs:
  - Using only BERT base vs. domain-adapted BioBERT: simpler but potentially less accurate for medical terms
  - Limiting to three languages (EN, DE, FR): reduces coverage but aligns with EPO data
  - Using MeSH only: focused but may miss non-indexed terms

- Failure signatures:
  - High false positives when homonyms pass similarity filters
  - Low recall if MeSH extraction misses domain-specific synonyms
  - Spurious IPC filtering if baseline distributions shift over time

- First 3 experiments:
  1. Run name normalization on a small patent-publication sample; check for homonym collisions.
  2. Compute BERT cosine similarity on a known valid pair; verify threshold behavior.
  3. Apply IPC filter to a synthetic dataset with artificially biased patent classes; confirm rejection rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are word embedding-based cosine similarity measures compared to other semantic similarity methods for patent-publication matching?
- Basis in paper: [explicit] The authors state they used BERT embeddings with MeSH terms as a baseline, acknowledging that domain-specific models like BioBERT might be better adapted to life sciences, and that large language models were not widely available at the time of the study.
- Why unresolved: The study used standard BERT and did not compare its performance against other embedding approaches (e.g., BioBERT, contextual language models) or semantic similarity methods.
- What evidence would resolve it: A controlled comparison of different embedding models and similarity metrics (BERT vs BioBERT vs contextual models, cosine similarity vs other measures) on the same dataset, with evaluation against manually validated matches.

### Open Question 2
- Question: What is the optimal time window between patent filing and publication date to maximize valid matches while minimizing false positives?
- Basis in paper: [explicit] The authors used a time window of 0.5 to 1.5 years between patent filing and publication, following Dornbusch and Neuhäusler's approach, but note this was arbitrary and may need refinement.
- Why unresolved: The study applied an arbitrary time window without testing alternative intervals or analyzing sensitivity to different time lags.
- What evidence would resolve it: Systematic analysis of match quality across different time windows (e.g., 0-1 year, 0-2 years, 1-3 years) with validation against known valid pairs to determine optimal range.

### Open Question 3
- Question: How does the performance of common reference matching compare to name-based matching in terms of precision and recall?
- Basis in paper: [explicit] The authors found that even one common reference was sufficient to confirm patent-publication pairs with high similarity values (>0.7), but did not provide quantitative comparison of precision and recall metrics for reference-based vs name-based matching.
- Why unresolved: The study showed qualitative findings about reference matching but did not measure its performance relative to name matching in terms of standard information retrieval metrics.
- What evidence would resolve it: Detailed precision, recall, and F1-score calculations comparing reference-based matching versus name-based matching, including false positive and false negative rates for each approach.

## Limitations
- Evaluation is based on open datasets from EPO and PubMed but lacks external validation with industry-specific or proprietary datasets
- The effectiveness of MeSH-based embeddings for non-medical domains remains untested
- Name normalization may introduce homonyms, potentially increasing false positives

## Confidence
- High confidence: The pairing mechanism using common author names and reference matching is well-established and validated
- Medium confidence: BERT-based cosine similarity on MeSH terms effectively captures topical similarity
- Medium confidence: The automated statistical filtering of patent classes reduces irrelevant matches
- Low confidence: The approach generalizes to non-medical domains without modification

## Next Checks
1. Test the approach on a different domain (e.g., engineering or computer science) using alternative ontologies to MeSH
2. Run a controlled experiment introducing homonyms into the name normalization process to quantify false positive impact
3. Validate the statistical patent class filtering by applying it to a time series of patents to detect distribution shifts