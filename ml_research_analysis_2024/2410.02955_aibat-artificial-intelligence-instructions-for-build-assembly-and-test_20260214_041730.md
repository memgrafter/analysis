---
ver: rpa2
title: 'AiBAT: Artificial Intelligence/Instructions for Build, Assembly, and Test'
arxiv_id: '2410.02955'
source_url: https://arxiv.org/abs/2410.02955
tags:
- ibat
- information
- assembly
- note
- drawing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AiBAT automates the generation of IBAT documents by using AI to
  extract information from assembly drawings and populate IBAT templates. The system
  combines custom image processing for text extraction with large language models
  to parse drawing notes and generate final IBAT steps.
---

# AiBAT: Artificial Intelligence/Instructions for Build, Assembly, and Test

## Quick Facts
- arXiv ID: 2410.02955
- Source URL: https://arxiv.org/abs/2410.02955
- Authors: Benjamin Nuernberger; Anny Liu; Heather Stefanini; Richard Otis; Amanda Towler; R. Peter Dillon
- Reference count: 40
- Primary result: 81.8% accuracy in parsing drawing notes and 90% accuracy in generating final IBAT steps

## Executive Summary
AiBAT automates the generation of IBAT (Instructions for Build, Assembly, and Test) documents by using AI to extract information from assembly drawings and populate IBAT templates. The system combines custom image processing for text extraction with large language models to parse drawing notes and generate final IBAT steps. In testing with three EFAB IBAT pairs, the system achieved 81.8% accuracy in parsing drawing notes and 90% accuracy in generating final IBAT steps, demonstrating strong potential for reducing the time and effort required for IBAT creation. The approach shows promise for scaling across NASA JPL's IBAT production, with estimated cost savings of $1.25 million annually.

## Method Summary
The AiBAT system uses a multi-stage approach to automate IBAT generation. First, it converts assembly drawing PDFs to images using ImageMagick, then applies LayoutParser with Faster R-CNN for note region detection. Custom OpenCV processing removes triangle shapes used to flag notes and detects column/row structures. Tesseract OCR extracts text from the processed regions. A large language model (Mistral 7B) parses the extracted notes into structured actions, information, and entities using few-shot prompting. Finally, the parsed information populates IBAT template substeps to generate the final assembly instructions. The system was tested on three EFAB IBAT pairs, measuring accuracy through R01 metrics for both note parsing and final step generation.

## Key Results
- 81.8% accuracy in parsing drawing notes into structured actions, information, and entities
- 90% accuracy in generating final IBAT steps from parsed note information
- Estimated $1.25 million annual cost savings for NASA JPL's IBAT production

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Custom image processing pipeline accurately extracts drawing notes from PDFs without selectable text
- Mechanism: Uses LayoutParser with Faster R-CNN for note detection, followed by rule-based column/row detection and triangle shape removal for flagged notes
- Core assumption: Assembly drawing PDFs follow consistent layout patterns where notes appear in predictable locations
- Evidence anchors:
  - [abstract] "The system combines custom image processing for text extraction with large language models"
  - [section IV-B] "We utilize OpenCV [39] to detect columns and rows based on simple rules such as ensuring that a certain percentage of consecutive pixels are white"
  - [corpus] Weak - no similar approaches found in corpus, custom approach not validated against alternatives
- Break condition: Assembly drawings use highly variable layouts or complex note formatting that breaks rule-based detection

### Mechanism 2
- Claim: LLM with few-shot prompting can parse drawing notes into structured actions, information, and entities
- Mechanism: Prompt template guides LLM to identify actionable steps, reference information, and physical components from unstructured note text
- Core assumption: Drawing notes contain sufficient semantic structure that LLMs can reliably extract meaning even without perfect OCR
- Evidence anchors:
  - [abstract] "The system combines custom image processing for text extraction with large language models to parse drawing notes"
  - [section V-A] "We ask the LLM to output: A list of actions, such as 'BOND' or 'SOLDER'; A list of information, such as statements referring to reference drawings; A list of entities, such as items, reference designators, tables, etc."
  - [corpus] Weak - corpus contains related AR assembly work but no direct validation of LLM parsing for technical drawings
- Break condition: Drawing notes use highly domain-specific terminology or ambiguous phrasing that confuses LLM parsing

### Mechanism 3
- Claim: Parsed note information can populate IBAT template substeps to generate final instructions
- Mechanism: LLM maps parsed actions/entities to appropriate template fields, filling in step-by-step assembly instructions
- Core assumption: IBAT templates have consistent structure that maps predictably to drawing note content
- Evidence anchors:
  - [abstract] "AiBAT automates the generation of IBAT documents by using AI to extract information from assembly drawings and populate IBAT templates"
  - [section V-B] "For final step generation, we utilize the parsed note and substeps of the IBAT template steps to output the final IBAT steps"
  - [corpus] Weak - no corpus evidence of similar template-filling approaches for aerospace assembly instructions
- Break condition: IBAT templates change structure or drawing notes require complex cross-referencing that breaks template mapping

## Foundational Learning

- OCR fundamentals and limitations
  - Why needed here: Understanding how text extraction works and fails helps debug information extraction pipeline
  - Quick check question: What are the two main failure modes of Tesseract OCR when processing assembly drawings?

- Multimodal LLM prompting strategies
  - Why needed here: Choosing between zero-shot, few-shot, and chain-of-thought affects parsing accuracy
  - Quick check question: What prompting approach was found too difficult for initial note parsing?

- Assembly drawing conventions
  - Why needed here: Understanding how drawing notes are formatted helps design better extraction rules
  - Quick check question: What special notation indicates flagged notes in assembly drawings?

## Architecture Onboarding

- Component map:
  - ImageMagick → PDF to image conversion
  - LayoutParser + Faster R-CNN → Note region detection
  - OpenCV → Column/row detection and triangle removal
  - Tesseract → OCR text extraction
  - LLM (Mistral 7B) → Note parsing and step generation
  - IBAT template → Structured output format

- Critical path: PDF → Image → Layout detection → OCR → LLM parsing → Template filling → IBAT output

- Design tradeoffs:
  - Custom image processing vs. multimodal models: More control but less generalizable
  - Few-shot vs. fine-tuning: Faster setup but potentially lower accuracy
  - On-premise vs. cloud LLMs: Better security but higher computational cost

- Failure signatures:
  - High character error rates → OCR or image quality issues
  - Incorrect triangle detection → Flagged note parsing failures
  - LLM returning empty lists → Prompt or model comprehension issues

- First 3 experiments:
  1. Run single assembly drawing through full pipeline and verify note extraction accuracy
  2. Test LLM parsing with known good and bad drawing note examples
  3. Validate final IBAT step generation against ground truth for one complete workflow

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific accuracy threshold should be established for the AiBAT system's %R01 scores to be considered reliable enough for production deployment?
- Basis in paper: [explicit] The authors discuss the need to determine if the AiBAT system is accurate enough for production deployment and suggest investigating whether %R01 metric scores should be above a certain threshold.
- Why unresolved: The paper does not provide a specific threshold value, and determining an appropriate accuracy threshold for production deployment requires balancing acceptable error rates against the severity of potential consequences.
- What evidence would resolve it: Empirical data from field testing showing the relationship between %R01 scores and actual error rates in production environments, along with risk assessment of potential consequences of errors.

### Open Question 2
- Question: How can the AiBAT system be generalized to handle IBAT workflows that do not utilize golden IBAT templates?
- Basis in paper: [explicit] The authors note that "most IBAT processes do not have templates available and we leave generalizing the capability of the system to support this to future work."
- Why unresolved: The current AiBAT system relies on golden IBAT templates, and developing a template-free approach would require significant changes to the system architecture and prompting strategies.
- What evidence would resolve it: Successful implementation and testing of the AiBAT system on IBAT workflows without templates, demonstrating comparable accuracy and efficiency.

### Open Question 3
- Question: What is the optimal strategy for dynamically updating few-shot prompts to improve the AiBAT system's accuracy and generalizability?
- Basis in paper: [explicit] The authors suggest that "programmatically or dynamically updating few-shot prompts has been shown to improve accuracy and may help generalize AiBAT further."
- Why unresolved: The paper does not provide specific details on how to implement dynamic prompt updates, and the optimal strategy likely depends on the specific use case and available data.
- What evidence would resolve it: Comparative studies of different dynamic prompt updating strategies, measuring their impact on AiBAT accuracy and generalizability across various IBAT workflows.

## Limitations

- Limited evaluation on only three EFAT pairs provides insufficient statistical power
- Export control restrictions prevent independent validation of templates and drawings
- Custom image processing may not scale to drawings with complex layouts or varied formatting
- 81.8% parsing accuracy leaves room for significant errors that could propagate through the pipeline

## Confidence

**High Confidence (4/5):**
- The custom image processing pipeline can successfully extract text from assembly drawing PDFs using LayoutParser, OpenCV, and Tesseract
- The LLM can generate structured output from drawing notes using few-shot prompting
- The overall system achieves measurable accuracy improvements over manual IBAT creation

**Medium Confidence (3/5):**
- The claimed 81.8% accuracy in parsing drawing notes is representative of broader performance
- The 90% accuracy in generating final IBAT steps generalizes beyond the tested examples
- The $1.25 million annual cost savings estimate is realistic for NASA JPL's scale

**Low Confidence (2/5):**
- The system will maintain accuracy across diverse assembly drawing styles and formats
- The approach will scale efficiently to the full range of JPL's IBAT production needs
- The custom image processing pipeline outperforms potential multimodal LLM alternatives

## Next Checks

1. **Cross-Division Testing**: Test the complete AiBAT pipeline on assembly drawings from multiple JPL divisions (not just EFAB) to assess generalizability across different engineering teams and satellite programs. Measure parsing accuracy and IBAT generation quality across at least 10-15 diverse drawing pairs.

2. **Alternative Layout Detection Methods**: Compare the custom image processing approach against a multimodal LLM (like GPT-4V or Gemini) for both note detection and OCR extraction. Evaluate accuracy, processing time, and implementation complexity to determine if the custom approach remains optimal as multimodal models improve.

3. **Error Propagation Analysis**: Conduct a detailed failure mode analysis by running the pipeline on drawings with known challenging characteristics (complex layouts, handwritten notes, poor image quality) and trace how errors in each stage (OCR, parsing, template filling) affect final IBAT quality. Identify which error types have the most significant impact on usability.