---
ver: rpa2
title: 'Thinking Forward: Memory-Efficient Federated Finetuning of Language Models'
arxiv_id: '2405.15551'
source_url: https://arxiv.org/abs/2405.15551
tags:
- spry
- client
- accuracy
- clients
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SPRY introduces a federated learning algorithm for finetuning\
  \ large language models that splits trainable weights among clients and uses Forward-mode\
  \ Automatic Differentiation to significantly reduce memory consumption (1.4-7.1\xD7\
  \ less than backpropagation) while maintaining accuracy comparable to backpropagation-based\
  \ methods. The algorithm achieves 5.2-13.5% higher accuracy and 1.2-20.3\xD7 faster\
  \ convergence compared to zero-order methods."
---

# Thinking Forward: Memory-Efficient Federated Finetuning of Language Models

## Quick Facts
- arXiv ID: 2405.15551
- Source URL: https://arxiv.org/abs/2405.15551
- Reference count: 40
- Primary result: Reduces memory usage 1.4-7.1× compared to backpropagation while maintaining accuracy

## Executive Summary
SPRY introduces a federated learning algorithm for finetuning large language models that splits trainable weights among clients and uses Forward-mode Automatic Differentiation to significantly reduce memory consumption. The method achieves 5.2-13.5% higher accuracy and 1.2-20.3× faster convergence compared to zero-order methods. Theoretical analysis proves that SPRY's global gradients are unbiased estimators of true gradients for homogeneous data distributions, while heterogeneity increases bias.

## Method Summary
SPRY is a federated learning algorithm that reduces memory consumption for fine-tuning large language models by splitting trainable weights among participating clients. Each client computes gradients using Forward-mode Automatic Differentiation with random perturbations, avoiding storage of intermediate activations. The server aggregates weights and updates the model using adaptive optimizers. The algorithm can operate in per-epoch or per-iteration communication modes, with layer weights or Jacobian-vector products transmitted between clients and server.

## Key Results
- Achieves 1.4-7.1× memory reduction compared to backpropagation-based methods
- Reduces memory usage from 33.9GB to 6.2GB when finetuning Llama2-7B with LoRA
- Achieves 5.2-13.5% higher accuracy and 1.2-20.3× faster convergence compared to zero-order methods
- Makes previously impossible federated deployments feasible on commodity edge devices

## Why This Works (Mechanism)

### Mechanism 1
Splitting trainable weights among clients reduces the number of weights each client must perturb, improving gradient estimation accuracy and reducing computation time. Each client only perturbs a small subset of weights, allowing forward-mode AD to generate closer approximations to true gradients with fewer computations. This breaks if the number of weights per client becomes too small, causing gradient estimates to become too noisy or unstable.

### Mechanism 2
Forward-mode AD reduces memory footprint by avoiding storage of intermediate activations during the forward pass. Instead of storing all activations for backpropagation, forward-mode AD only stores the previous layer's activation and computes gradients through Jacobian-vector products. This breaks if the largest activation in the forward pass is still too large for memory constraints.

### Mechanism 3
The aggregation of partial gradients from clients provides unbiased gradient estimates when data is homogeneous across clients. Under homogeneous data distribution, the bias term in Theorem 4.1 becomes zero, making the aggregated forward gradients unbiased estimators of true gradients. This breaks when data heterogeneity increases bias, degrading gradient estimate quality.

## Foundational Learning

- Concept: Forward-mode Automatic Differentiation
  - Why needed here: SPRY relies on forward-mode AD to compute gradients without storing intermediate activations, reducing memory usage
  - Quick check question: How does forward-mode AD compute gradients differently from backpropagation?

- Concept: Jacobian-vector product (jvp)
  - Why needed here: Forward-mode AD uses jvp to estimate gradients by measuring how weight perturbations affect outputs
  - Quick check question: What mathematical operation does jvp represent in forward-mode AD?

- Concept: Dirichlet distribution for data heterogeneity
  - Why needed here: SPRY's theoretical analysis depends on understanding how data heterogeneity affects gradient bias
  - Quick check question: How does changing the Dirichlet concentration parameter α affect data heterogeneity across clients?

## Architecture Onboarding

- Component map: Server -> Clients -> Communication -> Computation
- Critical path: Server assigns trainable layers to clients → Clients generate perturbations and compute forward gradients → Clients send updated weights or jvp values to server → Server aggregates weights and updates model → Repeat for each FL round
- Design tradeoffs: Layer splitting vs. full weight assignment (splitting reduces memory but increases communication rounds); Per-epoch vs. per-iteration communication (per-iteration reduces memory but increases communication frequency); Number of perturbations per batch (more perturbations improve accuracy but increase computation time)
- Failure signatures: Poor convergence (insufficient layer splitting or data heterogeneity issues); High memory usage (improper handling of intermediate activations); Communication bottlenecks (per-iteration communication mode with too many clients)
- First 3 experiments: Run SPRY on small dataset with RoBERTa Base to verify basic functionality and memory savings; Compare convergence speed and accuracy against FEDAVG with same dataset; Test different layer splitting strategies to find optimal configuration

## Open Questions the Paper Calls Out

1. How does the computational overhead of column-wise jvp computation in Forward-mode AD scale with model size, and what optimization strategies could reduce this overhead to match or exceed backpropagation efficiency?
2. How does the performance of SPRY vary across different heterogeneous data distributions, and what theoretical bounds can be established for the bias term in Theorem 4.1 under various Dirichlet concentration parameters?
3. What are the memory consumption trade-offs between SPRY and zero-order methods when considering the full pipeline including perturbation generation and gradient aggregation overhead?

## Limitations
- Theoretical analysis relies heavily on homogeneous data assumptions that may not hold in practical federated settings
- Performance gains for extremely large models may be specific to the 4-bit quantization regime used
- Comparison with BAFFLE+ is limited as it only works with linear layers, potentially underestimating competitiveness of baseline approaches

## Confidence
- High confidence: Memory reduction claims (1.4-7.1× less than backpropagation) are well-supported by empirical measurements
- Medium confidence: Accuracy claims (5.2-13.5% higher than zero-order methods) depend heavily on dataset characteristics
- Low confidence: Claim that SPRY makes "previously impossible federated deployments feasible" may be overstated

## Next Checks
1. Test SPRY's performance across a wider range of Dirichlet concentration parameters to quantify how bias affects convergence and accuracy in highly heterogeneous settings
2. Systematically measure and compare the actual communication costs between per-iteration and per-epoch modes across different network conditions and client counts
3. Validate SPRY's memory and performance benefits on non-Transformer architectures and smaller models to determine if advantages are architecture-specific or generalizable