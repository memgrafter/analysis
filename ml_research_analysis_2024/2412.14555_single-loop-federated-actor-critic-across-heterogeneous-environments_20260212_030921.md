---
ver: rpa2
title: Single-Loop Federated Actor-Critic across Heterogeneous Environments
arxiv_id: '2412.14555'
source_url: https://arxiv.org/abs/2412.14555
tags:
- learning
- policy
- federated
- where
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SFAC, a federated actor-critic method for
  multi-agent RL in heterogeneous environments. Agents perform two-level federated
  learning: FedC for value function aggregation and FedA for policy updates.'
---

# Single-Loop Federated Actor-Critic across Heterogeneous Environments

## Quick Facts
- arXiv ID: 2412.14555
- Source URL: https://arxiv.org/abs/2412.14555
- Reference count: 40
- Primary result: SFAC achieves convergence to near-stationary point with error proportional to environment heterogeneity while enjoying linear speedup in sample complexity

## Executive Summary
This paper introduces SFAC, a federated actor-critic method for multi-agent reinforcement learning in heterogeneous environments. The method employs two-level federated learning: FedC for value function aggregation and FedA for policy updates. Theoretical analysis shows SFAC converges to a near-stationary point with convergence error proportional to environment heterogeneity and enjoys linear speedup in sample complexity. Experiments on Lunar Lander and Cartpole demonstrate superior performance compared to A3C and validate theoretical insights on agent count and heterogeneity effects.

## Method Summary
SFAC implements a single-loop federated actor-critic algorithm where agents perform local actor-critic updates on heterogeneous MDPs and communicate gradients to central servers. The FedC server aggregates local value function updates to minimize average mean-squared projected Bellman error across agents, while the FedA server aggregates policy gradients. The single-loop architecture preserves critic memory across policy updates, avoiding initialization-from-scratch bias. Agents use linear function approximation for both value and policy functions, with separate Markov chains for actor and critic updates to prevent bias.

## Key Results
- Convergence to near-stationary point with error scaling proportionally to environment heterogeneity
- Linear speedup in sample complexity demonstrated experimentally with increasing agent count
- Superior performance compared to A3C baseline on Lunar Lander and Cartpole environments
- Theoretical convergence guarantees under Markovian sampling with function approximation

## Why This Works (Mechanism)

### Mechanism 1
FedC achieves near-optimal value function aggregation with convergence error scaling with environmental heterogeneity by minimizing average mean-squared projected Bellman error (MSPBE) across agents. This enables value function aggregation that benefits from linear speedup via agent collaboration while capturing heterogeneity effects. The mechanism assumes local MSPBEs can be combined into a global objective without loss of convergence guarantees. Evidence includes theoretical analysis showing convergence error is proportional to heterogeneity, though the corpus provides only weak support for MSPBE aggregation specifically.

### Mechanism 2
Single-loop architecture eliminates bias from critic drift by preserving critic memory across policy updates. By maintaining critic parameters throughout the process, the algorithm avoids the initialization-from-scratch problem that causes bias in double-loop approaches. The mechanism assumes the critic's memory provides sufficient information to maintain convergence despite policy changes. While the paper discusses this advantage, the corpus provides no supporting evidence for single-loop vs double-loop trade-offs.

### Mechanism 3
FedA achieves linear speedup in policy gradient estimation through federated aggregation of local gradients. Aggregating local policy gradients across agents reduces variance proportionally to the number of agents while maintaining convergence properties. The mechanism assumes local policy gradients are unbiased estimates of the global gradient, allowing safe aggregation. Theoretical analysis shows linear speedup, though corpus evidence is limited to general federated TD learning rather than specific policy gradient aggregation.

## Foundational Learning

- **Markov Decision Processes (MDPs) and value function approximation**: SFAC operates on MDPs with linear function approximation for both value and policy functions. Quick check: What property of linear function approximation makes it suitable for federated settings where agents have heterogeneous environments?

- **Federated learning aggregation mechanisms**: SFAC uses two-level aggregation - one for critics (FedC) and one for actors (FedA) - requiring understanding of federated optimization. Quick check: How does federated averaging differ from simple averaging when agents have different numbers of local updates?

- **Temporal Difference (TD) learning convergence analysis**: The convergence proof relies on properties of TD learning with Markovian sampling and function approximation. Quick check: What is the key difference between TD learning convergence analysis under IID sampling versus Markovian sampling?

## Architecture Onboarding

- **Component map**: Global server orchestrates FedC and FedA rounds, aggregates models → Critic agents perform TD learning on local MDPs, send gradients to FedC server → Actor agents generate trajectories, estimate advantages, send policy gradients to FedA server

- **Critical path**: FedC (T rounds) → FedA aggregation → Policy update → Repeat

- **Design tradeoffs**: Single-loop vs double-loop trades complexity for efficiency by preserving critic memory; Markovian vs IID sampling balances realism with analysis complexity; homogeneous vs heterogeneous updates accommodates agent heterogeneity

- **Failure signatures**: High variance in policy updates despite many agents (check FedA aggregation); critic parameters diverging across agents (check FedC convergence); slow convergence despite many agents (check linear speedup assumptions)

- **First 3 experiments**: 1) Test FedC convergence with synthetic MDPs of varying heterogeneity levels; 2) Validate linear speedup by varying agent count while keeping heterogeneity constant; 3) Compare single-loop vs double-loop performance on a benchmark environment

## Open Questions the Paper Calls Out

### Open Question 1
How does SFAC perform with asynchronous federated learning, where agents have different computation and communication capabilities? The paper mentions heterogeneous computation capabilities but doesn't explore asynchronous scenarios. Experimental results comparing synchronous vs. asynchronous SFAC performance under varying agent heterogeneity would resolve this.

### Open Question 2
What is the impact of using different neural network architectures for the critic and actor models on SFAC's convergence? The paper uses MLPs but doesn't explore architectural effects. Experimental results comparing SFAC performance with different neural network architectures for critic and actor would provide answers.

### Open Question 3
How does the performance of SFAC scale with the number of agents when environment heterogeneity is very high? The paper shows increasing agents accelerates training but only uses limited agents and moderate heterogeneity. Experimental results with large agent counts and very high heterogeneity would resolve this.

### Open Question 4
How does SFAC handle non-stationary environments where transition probabilities and reward functions change over time? The paper assumes stationary environments without discussing non-stationary scenarios. Experimental results in non-stationary environments with changing dynamics would address this.

## Limitations

- Theoretical analysis assumes linear function approximation with bounded features and requires problem-dependent constants that may be difficult to estimate in practice
- Markovian sampling assumption adds significant complexity to convergence proof and may not hold for all real-world environments
- Single-loop architecture's efficiency gains lack empirical validation and supporting theoretical analysis

## Confidence

- **Mechanism 1 (FedC convergence)**: Medium - Theoretical analysis is rigorous but relies on strong assumptions about feature boundedness and Markovian sampling
- **Mechanism 2 (Single-loop efficiency)**: Low - Limited empirical evidence and theoretical analysis; corpus provides no supporting evidence
- **Mechanism 3 (Linear speedup)**: Medium - Theoretical guarantees exist but depend on variance reduction assumptions that may not hold with highly heterogeneous agents

## Next Checks

1. **Heterogeneity sensitivity analysis**: Systematically vary environment heterogeneity levels and measure convergence error to validate the proportionality relationship claimed in theoretical analysis.

2. **Single-loop vs double-loop ablation**: Implement both architectures and compare convergence speed, stability, and final performance on benchmark environments to empirically validate claimed efficiency gains.

3. **Linear speedup verification**: Conduct controlled experiments varying agent count while holding heterogeneity constant, measuring sample complexity and variance reduction to confirm theoretical speedup predictions.