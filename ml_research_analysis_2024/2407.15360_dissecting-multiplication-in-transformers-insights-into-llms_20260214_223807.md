---
ver: rpa2
title: 'Dissecting Multiplication in Transformers: Insights into LLMs'
arxiv_id: '2407.15360'
source_url: https://arxiv.org/abs/2407.15360
tags:
- transformer
- tasks
- multiplication
- task
- digit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes why transformers struggle with arithmetic tasks
  like multiplication, despite excelling at complex NLP tasks. The authors find that
  transformers decompose multiplication into parallel subtasks (basic multiplication,
  carry, use carry, and cascaded carry) but struggle with computing successive carryovers
  and caching intermediate results.
---

# Dissecting Multiplication in Transformers: Insights into LLMs

## Quick Facts
- **arXiv ID**: 2407.15360
- **Source URL**: https://arxiv.org/abs/2407.15360
- **Reference count**: 7
- **Key outcome**: Tiny transformers with targeted improvements achieve over 99.9% accuracy on 5-digit integer multiplication, outperforming LLMs like GPT-4.

## Executive Summary
This paper analyzes why transformers struggle with arithmetic tasks like multiplication despite excelling at complex NLP tasks. The authors discover that transformers decompose multiplication into parallel subtasks (basic multiplication, carry, use carry, and cascaded carry) but face challenges with successive carryovers and caching intermediate results. They propose three targeted improvements: reversing answer digit order to aid carry calculation, increasing model depth to handle intermediate steps, and progressively training with more simple samples. These refinements enable a tiny transformer to achieve over 99.9% accuracy on 5-digit integer multiplication, significantly outperforming LLMs like GPT-4.

## Method Summary
The authors train decoder-only transformers with varying depth, attention heads, and data formats on 5-digit integer multiplication problems. They use Adam optimizer with learning rate 1e-4 for 2000 iterations, progressively increasing model complexity and data difficulty. The study analyzes attention patterns and loss curves to understand how transformers decompose multiplication tasks, then systematically tests three improvements: reversed answer digit order, increased model depth, and progressive training with simple samples.

## Key Results
- Transformers decompose multiplication into four parallel subtasks (basic multiplication, carry, use carry, and cascaded carry) that are learned sequentially
- Reversing answer digit order significantly improves carry calculation accuracy by enabling better access to previously generated digits
- Progressive training with simple samples and increased model depth enable transformers to achieve over 99.9% accuracy on 5-digit multiplication
- Tiny transformers with these improvements outperform GPT-4 on multiplication tasks despite being orders of magnitude smaller

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers decompose multiplication into parallel subtasks (basic multiplication, carry, use carry, and cascaded carry).
- Mechanism: Each subtask is computed in parallel by different attention heads, with sequential learning order. For example, the model first learns to multiply single digits without carry (BM w/o carry), then learns to compute carry (CA), then uses the carry (UC), and finally handles cascaded carry (UCFC).
- Core assumption: Multiplication is too complex for a single attention head to handle, so the model distributes subtasks across heads and learns them sequentially.
- Evidence anchors:
  - [abstract] "Our observations indicate that the model decomposes multiplication task into multiple parallel subtasks, sequentially optimizing each subtask for each digit to complete the final multiplication."
  - [section] "The loss curves in Fig. 2(a) show transformer learns each answer digit semi-independently... The convergence of these two task indicates that the model is able to accurately calculate carry."
- Break condition: If the model cannot learn the basic subtasks in sequence, or if subtasks interfere with each other, the multiplication accuracy will drop.

### Mechanism 2
- Claim: Reversing answer digit order aids carry calculation.
- Mechanism: When answer digits are reversed, the model can start calculations from lower digits and use previously generated answer digits to calculate carry more effectively. This allows better handling of cascaded carry cases.
- Core assumption: Carry calculation is easier when starting from lower digits because it can use already computed results.
- Evidence anchors:
  - [section] "The reversed transformer exhibits opposite attention pattern, the model attends to digits in a staircase pattern from right to left... Each attention head in reversed transformer is also responsible for different subtasks... the reversed transformer could access to previously generated answer digits, hence green head mainly focus on previous answer digits and is responsible for calculating cascaded carry."
  - [section] "We find that transformer has relatively low loss on simple UCFC cases... While transformer encounters high loss value when multiplier is other digits, which usually lead to cascaded UCFC case"
- Break condition: If the model still struggles with carry calculation even with reversed digits, or if the attention heads cannot effectively use previously generated digits.

### Mechanism 3
- Claim: Increasing model depth and using progressive training with simple samples improves multiplication accuracy.
- Mechanism: Deeper models can store intermediate results better, and progressive training allows the model to learn basic multiplication rules before tackling complex cases.
- Core assumption: Single-layer transformers lack capacity to handle multiple intermediate steps in m × m multiplication, and complex training data overwhelms the model.
- Evidence anchors:
  - [section] "This indicates that single-layer transformer lacks the capacity to handle the multiple intermediate step in m × m multiplication task."
  - [section] "Increase the proportion of simple samples bring significant accuracy promotion. This shows progressive learning reduce the overall training difficulty."
- Break condition: If increasing depth does not improve accuracy, or if progressive training does not help the model learn basic rules.

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanism
  - Why needed here: Understanding how transformers process sequences and how attention heads work is crucial to grasp the decomposition of multiplication tasks.
  - Quick check question: What is the role of multi-head self-attention in transformers, and how does it help in handling complex tasks?

- Concept: Carry calculation in arithmetic
  - Why needed here: Carry calculation is a key subtask in multiplication, and understanding how it works is essential to interpret the model's behavior.
  - Quick check question: How does carry calculation work in multi-digit multiplication, and why is it challenging for transformers?

- Concept: Progressive learning and curriculum design
  - Why needed here: The paper proposes progressive training with simple samples to improve multiplication accuracy, so understanding this concept is important.
  - Quick check question: What is progressive learning, and how can it help models learn complex tasks more effectively?

## Architecture Onboarding

- Component map: Input embeddings -> Multi-head Self-Attention (MSA) -> Feed-Forward (FF) layers -> Output mapping

- Critical path:
  1. Input tokens are embedded and combined with positional embeddings
  2. Embeddings are processed through MSA and FF layers
  3. Output embeddings are mapped to answer digits
  4. Answer digits are generated in autoregressive manner

- Design tradeoffs:
  - Model depth: deeper models can handle more intermediate steps but may be harder to train
  - Answer digit order: reversed order aids carry calculation but may affect other aspects
  - Training data complexity: progressive training with simple samples helps learning but may slow down overall training

- Failure signatures:
  - Low accuracy on carry calculation subtasks
  - Inability to handle cascaded carry cases
  - Poor performance on m × m multiplication compared to m × u

- First 3 experiments:
  1. Train a single-layer transformer on m × u multiplication and analyze subtask decomposition
  2. Compare accuracy of ordinal vs reversed answer digit formats
  3. Train transformers with different depths on m × m multiplication and analyze performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between simple and complex samples during training to maximize multiplication accuracy?
- Basis in paper: [explicit] The paper found that using too few simple samples makes it difficult for the model to learn basic rules, while too many cause the model to lack the ability to handle more difficult ones. Both extremes are detrimental to learning.
- Why unresolved: The paper only tested a limited range of simple sample proportions (0-100%) and found a general trend, but did not identify the exact optimal proportion. The relationship may be non-linear and could depend on model size and task complexity.
- What evidence would resolve it: Systematic experiments testing a wide range of simple sample proportions (e.g., 0-100% in 5% increments) for different model sizes and multiplication tasks, followed by mathematical analysis of the learning curves to identify the optimal proportion.

### Open Question 2
- Question: How does the attention mechanism in transformers handle cascaded carry calculations in multi-digit multiplication, and can this be optimized?
- Basis in paper: [explicit] The paper found that transformers struggle with cascaded carry calculations (UCFC) in multi-digit multiplication, particularly when multiple intermediate products overlap. This limitation was identified through analysis of attention patterns.
- Why unresolved: While the paper identified the problem, it did not provide a detailed explanation of how the attention mechanism specifically fails in these cases or propose concrete solutions to optimize it.
- What evidence would resolve it: Detailed analysis of attention patterns during cascaded carry calculations, combined with experiments testing different attention mechanisms or architectural modifications to improve carry handling.

### Open Question 3
- Question: Can the insights from this analysis be generalized to other arithmetic operations (e.g., division, exponentiation) and more complex mathematical tasks?
- Basis in paper: [inferred] The paper focused on multiplication and provided insights into transformer limitations and potential solutions. However, it did not explicitly test whether these insights apply to other arithmetic operations or more complex mathematical tasks.
- Why unresolved: The paper only analyzed multiplication in detail, so it's unclear whether the identified limitations and solutions are specific to multiplication or can be generalized to other mathematical operations.
- What evidence would resolve it: Systematic analysis of transformer performance and internal mechanisms for other arithmetic operations (division, exponentiation) and more complex mathematical tasks, followed by testing of the proposed solutions to see if they improve performance across different operations.

## Limitations
- Limited to 5-digit multiplication problems, raising questions about generalizability to larger numbers
- Synthetic data dependency may not reflect real-world numerical processing scenarios
- Focus on decoder-only transformers may not apply to encoder-decoder architectures

## Confidence

**High Confidence**: The observation that transformers decompose multiplication into parallel subtasks is well-supported by attention visualization and loss analysis.

**Medium Confidence**: The claim that reversing answer digit order significantly improves carry calculation accuracy has strong empirical support, but the underlying mechanism explanation relies heavily on attention pattern interpretation.

**Medium Confidence**: The assertion that progressive training with simple samples improves performance is supported by accuracy metrics, but alternative curriculum learning strategies weren't explored.

**Low Confidence**: The claim that these improvements enable transformers to outperform GPT-4 is based on limited comparison without accounting for fine-tuning differences.

## Next Checks

1. **Cross-digit generalization test**: Validate whether the three proposed improvements (reversed digit order, increased depth, progressive training) maintain their effectiveness when scaling from 5-digit to 10-digit or 20-digit multiplication problems.

2. **Alternative architecture comparison**: Implement the same multiplication task using an encoder-decoder transformer architecture with the proposed improvements to determine if the decomposition mechanism and performance gains are architecture-dependent.

3. **Robustness evaluation**: Test model performance on multiplication problems with varied formatting, spacing, and presentation styles (including word-form numbers like "five hundred twenty-three") to assess generalization beyond the controlled synthetic format.