---
ver: rpa2
title: 'Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language
  Models'
arxiv_id: '2403.18093'
source_url: https://arxiv.org/abs/2403.18093
tags:
- retrieval
- legal
- score
- phase
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multi-phase retrieval pipeline for legal
  document retrieval that integrates BM25 pre-ranking, BERT-based re-ranking, and
  LLM prompting. The approach addresses the challenge of retrieving relevant legal
  articles from a large corpus by gradually filtering candidates through three phases.
---

# Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models

## Quick Facts
- arXiv ID: 2403.18093
- Source URL: https://arxiv.org/abs/2403.18093
- Reference count: 12
- Primary result: Achieved F2 score of 0.8085 on COLIEE 2023 legal document retrieval, outperforming best competition team by 4%

## Executive Summary
This paper presents a three-phase retrieval pipeline for legal document retrieval that combines BM25 pre-ranking, BERT-based re-ranking, and LLM prompting. The approach addresses the challenge of retrieving relevant legal articles from a large corpus by gradually filtering candidates through three phases. The key innovation is using prompting with large language models (GPT-3.5-turbo and GPT-4) as the final re-ranking phase to leverage their reasoning capabilities. Experimental results on the COLIEE 2023 dataset show significant improvements in retrieval accuracy, with the best configuration achieving an F2 score of 0.8085, outperforming the best competition team by 4%.

## Method Summary
The approach implements a multi-phase retrieval system: Phase 1 uses BM25 to efficiently filter the large legal corpus to top-500 candidates based on lexical relevance. Phase 2 employs a multilingual BERT model trained via multi-task learning to re-rank the top-30 candidates from Phase 1 based on semantic similarity. Phase 3 applies LLM prompting (using GPT-3.5-turbo or GPT-4) with an adaptive sliding window technique to evaluate logical relevance through reasoning. The final ensemble combines BERT and LLM scores using weighted averaging to produce the final retrieval results.

## Key Results
- Achieved F2 score of 0.8085, outperforming the best competition team by 4%
- LLM prompting phase significantly improved precision scores while maintaining recall
- The BERT+LLM ensemble configuration showed superior performance compared to individual models
- Error analysis revealed limitations with complex legal situations involving multiple actors and cases with many noisy candidates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-phase retrieval pipeline progressively filters legal documents by combining lexical, semantic, and reasoning capabilities.
- Mechanism: BM25 pre-ranking uses term frequency to rapidly filter candidates; BERT-based re-ranking refines this set using semantic similarity; LLM prompting applies logical reasoning and broad domain knowledge to identify the most relevant documents.
- Core assumption: Each phase addresses a different limitation of the previous phase—BM25 handles lexical relevance efficiently, BERT handles semantic understanding, and LLM handles complex reasoning.
- Evidence anchors: [abstract] "The approach addresses the challenge of retrieving relevant legal articles from a large corpus by gradually filtering candidates through three phases."; [section] "Due to the considerable number of articles present in the Legal Statute Corpus, ensuring efficient retrieval time and optimizing computational complexity necessitates the implementation of a multi-phase retrieval system to gradually filter candidates from the corpus."
- Break condition: If the LLM phase cannot handle the token limits or fails to improve precision/recall beyond the BERT phase, the multi-phase advantage disappears.

### Mechanism 2
- Claim: LLM prompting improves precision by leveraging reasoning capabilities that semantic similarity alone cannot provide.
- Mechanism: The LLM applies zero-shot prompting to evaluate each candidate article's logical relevance to the query, not just semantic overlap. This captures nuanced legal relationships and implicit reasoning.
- Core assumption: Legal queries often require reasoning beyond simple keyword or semantic matching, and LLMs can perform this reasoning when prompted correctly.
- Evidence anchors: [abstract] "The LLM prompting phase particularly improves precision scores while maintaining recall."; [section] "queries may not always directly address the core issue and may require logical reasoning to find relevant legal articles related to the query."
- Break condition: If queries are simple keyword matches or if the LLM fails to interpret legal reasoning, the precision gain will not materialize.

### Mechanism 3
- Claim: Ensembling BERT and LLM scores creates a more robust relevance measure than either model alone.
- Mechanism: The ensemble formula Rreranking−phase−2 = β ∗ BERT score + γ ∗ LLM score combines semantic understanding (BERT) with reasoning capability (LLM), weighting each according to validation performance.
- Core assumption: BERT and LLM capture complementary aspects of relevance—semantic similarity vs. logical reasoning—so their combination yields better results than either individually.
- Evidence anchors: [section] "Combining the scores of the BERT-based re-ranking model with the scores from the LLMs prompting output... maintains recall scores from the two preceding phases."; [section] "Both our proposed retrieval systems using the GPT-4 model surpass participants in precision and recall scores."
- Break condition: If BERT and LLM scores are highly correlated (as noted in error analysis), the ensemble offers little additional benefit.

## Foundational Learning

- Concept: Information retrieval metrics (precision, recall, F1/F2)
  - Why needed here: The paper evaluates performance using these metrics to measure trade-offs between retrieving relevant documents and avoiding irrelevant ones.
  - Quick check question: If a system retrieves 8 relevant documents out of 10 total relevant documents, and returns 12 documents in total, what are its recall and precision?

- Concept: Vector similarity and semantic embeddings
  - Why needed here: BERT-based re-ranking relies on computing semantic similarity between query and document embeddings to filter candidates beyond lexical matching.
  - Quick check question: How does BERT compute the relevance score between a query and a legal article in this pipeline?

- Concept: Prompt engineering and zero-shot learning
  - Why needed here: The LLM phase uses zero-shot prompting to generate relevance scores without task-specific fine-tuning, requiring careful prompt design to elicit correct reasoning.
  - Quick check question: What format does the paper specify for the LLM output, and why is that important?

## Architecture Onboarding

- Component map: Query → BM25 pre-ranking → BERT-based re-ranking → LLM prompting → Ensemble → Output
- Critical path: Query → BM25 → BERT → LLM → Ensemble → Output
- Design tradeoffs:
  - Token limit constraints force sliding window approach in LLM phase, potentially missing context across article boundaries.
  - Ensembling increases computational cost and complexity but improves robustness.
  - Zero-shot prompting avoids fine-tuning but may underperform if prompts are not well crafted.
- Failure signatures:
  - Low recall after BM25: Indicates too aggressive top-k cutoff.
  - Low precision after BERT: Suggests semantic similarity alone is insufficient for legal reasoning.
  - Inconsistent LLM scores: May indicate prompt ambiguity or token limit truncation.
- First 3 experiments:
  1. Vary top-k in BM25 pre-ranking and measure recall/precision to find optimal balance.
  2. Test BERT-only pipeline (BM25 + BERT) vs full pipeline (with LLM) to quantify LLM contribution.
  3. Compare zero-shot vs few-shot prompting in LLM phase to assess impact of prompt design.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can retrieval accuracy be improved for legal queries involving multiple actors and complex legal situations?
- Basis in paper: [explicit] The error analysis section notes that queries involving complex legal situations with 2 or more actors are particularly challenging, with half of the low recall queries (recall <= 0.5) or queries that saw recall decline after LLM re-ranking being such cases.
- Why unresolved: Current LLM prompting techniques struggle with complex reasoning required for multi-actor legal scenarios, as evidenced by the significant drop in recall for these cases.
- What evidence would resolve it: Comparative experiments testing different prompting strategies, chain-of-thought reasoning, or specialized fine-tuning on multi-actor legal scenarios, showing improved recall and F2 scores for these complex queries.

### Open Question 2
- Question: What is the optimal ensemble strategy for combining BM25, BERT, and LLM relevance scores in legal document retrieval?
- Basis in paper: [explicit] The paper employs a weighted ensemble of BM25, BERT, and LLM scores, but notes that while ensembling improves overall F2 scores, it comes with trade-offs between precision and recall.
- Why unresolved: The current weighted ensemble approach shows improvements but still has limitations, particularly in cases with many noisy candidates or complex legal situations.
- What evidence would resolve it: Systematic experiments testing various ensemble methods (weighted sum, rank-based fusion, neural network-based fusion) across different query types, demonstrating superior performance on both simple and complex legal queries.

### Open Question 3
- Question: How can the retrieval pipeline be optimized to reduce noisy candidates before the LLM re-ranking phase?
- Basis in paper: [explicit] Error analysis shows that in cases where precision and recall are both 0, it is often due to the only matching articles getting eliminated, resulting in 0 precision and 0 recall at the same time, particularly when many noisy candidates are present.
- Why unresolved: The current pipeline relies on BM25 and BERT re-ranking to filter candidates, but this is insufficient for queries with high numbers of noisy candidates, as evidenced by 14/101 samples unable to recall any matching article.
- What evidence would resolve it: Comparative experiments testing improved candidate filtering techniques such as query expansion, enhanced cross-encoder training, or other pre- and post-processing methods, demonstrating reduced noisy candidates and improved recall for previously problematic queries.

## Limitations
- The system struggles with complex legal scenarios involving multiple actors, showing significant drops in recall for these cases.
- Error analysis reveals limitations with cases containing many noisy candidates, where the pipeline fails to retrieve any relevant articles.
- The adaptive sliding window approach for LLM prompting may miss contextual relationships across article boundaries due to token limit constraints.

## Confidence
- Multi-phase pipeline effectiveness: Medium - Supported by empirical results but limited by error analysis showing failure cases
- LLM reasoning improvement: Medium - Precision gains are demonstrated but mechanism not fully validated for complex scenarios
- Ensemble approach superiority: Low-Medium - Results show improvement but correlation between BERT and LLM scores raises questions about complementarity

## Next Checks
1. **Cross-phase correlation analysis**: Measure Pearson correlation coefficients between BM25, BERT, and LLM scores across all queries to quantify redundancy and validate the architectural assumption of complementary capabilities.

2. **Complex query stress test**: Design and test queries specifically targeting multi-actor scenarios and high-noise situations to characterize the failure modes identified in error analysis and measure current limitations.

3. **Prompt engineering ablation**: Systematically vary the LLM prompt structure (few-shot vs zero-shot, different formatting) while keeping other phases constant to isolate the contribution of prompt design to precision improvements.