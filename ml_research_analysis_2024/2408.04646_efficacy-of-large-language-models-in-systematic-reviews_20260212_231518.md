---
ver: rpa2
title: Efficacy of Large Language Models in Systematic Reviews
arxiv_id: '2408.04646'
source_url: https://arxiv.org/abs/2408.04646
tags:
- llms
- gpt-4o
- papers
- llama
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the effectiveness of Large Language Models
  (LLMs) in replicating systematic reviews on the relationship between ESG factors
  and financial performance. Researchers hand-coded 88 recent papers and used 238
  papers from a prior review as training data.
---

# Efficacy of Large Language Models in Systematic Reviews

## Quick Facts
- arXiv ID: 2408.04646
- Source URL: https://arxiv.org/abs/2408.04646
- Authors: Aaditya Shah; Shridhar Mehendale; Siddha Kanthi
- Reference count: 27
- Base Llama 3 generally outperformed GPT-4o, though all models struggled with certain classifications

## Executive Summary
This study evaluated Large Language Models (LLMs) for systematic review tasks, specifically analyzing the relationship between ESG factors and financial performance. Researchers tested base models (Llama 3 8B, GPT-4o), a custom ChatGPT, and a fine-tuned GPT-4o Mini on 88 recent papers while using 238 papers as training data. The fine-tuned GPT-4o Mini achieved 28.3% higher overall accuracy than base models on the primary classification task, while the Custom GPT showed 3.0-15.7% improvements on other tasks. Results indicate that LLMs can effectively assist in systematic reviews when properly fine-tuned and prompted, offering potential for faster ESG analysis while maintaining accuracy.

## Method Summary
The study tested four LLM configurations (Llama 3 8B, GPT-4o, Custom GPT, fine-tuned GPT-4o Mini) on nine prompts using abstracts and full PDFs from 88 hand-coded ESG papers. Researchers compared these against human-coded classifications across three classification dimensions: relationship type, financial metrics, and sustainability implementation categories. The fine-tuned GPT-4o Mini used the 238-paper corpus from a prior review with batch size 1, 3 epochs, and learning rate multiplier 1.8, while the Custom GPT employed chain-of-thought prompting with confidence scoring.

## Key Results
- Fine-tuned GPT-4o Mini outperformed base LLMs by 28.3% average accuracy on primary classification task
- Custom GPT showed 3.0-15.7% improvement on average accuracy for prompts 2 and 3
- Base Llama 3 generally outperformed GPT-4o across all prompts tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning LLMs on domain-specific datasets significantly improves their accuracy in systematic review tasks compared to base models.
- Mechanism: Fine-tuning adjusts the model's internal parameters to better capture domain-specific patterns and terminology, reducing the gap between general language understanding and specialized domain knowledge required for accurate classification.
- Core assumption: The training data used for fine-tuning is representative of the task domain and contains sufficient examples of the classification patterns needed.
- Evidence anchors:
  - [abstract] "The fine-tuned GPT-4o Mini model outperformed the base LLMs by 28.3% on average in overall accuracy on prompt 1."
  - [section] "Using the 238 papers in the old corpus of studies, we fine-tuned the smaller GPT-4o Mini model using the OpenAI API. We used a batch size of 1, 3 epochs, and a learning rate multiplier of 1.8."
- Break condition: If the training data is not representative or contains biases that mislead the model, or if the fine-tuning process overfits to the training set without generalizing.

### Mechanism 2
- Claim: Chain-of-thought prompting improves LLM accuracy by guiding the model through logical reasoning steps.
- Mechanism: Chain-of-thought prompting breaks down complex classification tasks into sequential reasoning steps, reducing cognitive load and helping the model focus on relevant features rather than making holistic judgments.
- Core assumption: The model can follow multi-step reasoning instructions and benefits from explicit decomposition of the classification process.
- Evidence anchors:
  - [abstract] "The 'Custom GPT' showed a 3.0% and 15.7% improvement on average in overall accuracy on prompts 2 and 3, respectively."
  - [section] "They asked the model to take specific steps in determining its response. In addition, the model was asked to return a reasoning and confidence score for its prediction."
- Break condition: If the model fails to follow the reasoning steps correctly or if the decomposition introduces additional complexity that confuses rather than clarifies.

### Mechanism 3
- Claim: LLMs struggle with certain classification categories due to imbalanced training data and difficulty distinguishing between similar concepts.
- Mechanism: When certain classes are underrepresented or semantically similar to other classes, models tend to default to majority classes or make systematic errors in distinguishing between them.
- Core assumption: The classification task contains inherent difficulty in distinguishing between certain categories, particularly when semantic boundaries are fuzzy.
- Evidence anchors:
  - [abstract] "Base Llama 3 generally outperformed GPT-4o, though all models struggled with certain classifications."
  - [section] "All three LLMs struggled with papers in the 'S' and 'Other' categories."
- Break condition: If additional training data or different prompting strategies can resolve the classification difficulties, or if the model architecture fundamentally cannot distinguish between the problematic categories.

## Foundational Learning

- Concept: Systematic review methodology and classification frameworks
  - Why needed here: The study relies on replicating established systematic review classifications (positive, negative, mixed for relationships; market-based, accounting-based, both for financial metrics; ESG, E, S, G, CSR for sustainability metrics)
  - Quick check question: What are the three main classification questions used in this study, and how many categories does each have?

- Concept: Fine-tuning vs. prompting strategies
  - Why needed here: The study compares fine-tuned models against base models with different prompting strategies (basic, contextual, chain-of-thought)
  - Quick check question: What is the key difference between the fine-tuning approach used for GPT-4o Mini and the prompting approach used for the Custom GPT?

- Concept: Evaluation metrics for classification tasks
  - Why needed here: The study uses accuracy, F1 scores, and weighted power mean calculations to evaluate model performance
  - Quick check question: Why might F1 scores be more informative than simple accuracy when evaluating classification performance on imbalanced datasets?

## Architecture Onboarding

- Component map: Data ingestion pipeline (PDF processing, abstract extraction) → LLM inference layer (base models, fine-tuned models, custom GPT) → Evaluation engine (accuracy calculation, F1 scoring) → Results visualization
- Critical path: Training data preparation → Fine-tuning process → Prompt engineering → Model evaluation → Results analysis
- Design tradeoffs: Using abstracts only for Llama 3 vs. full PDFs for GPT-4o (speed vs. comprehensiveness), fine-tuning on subset of prompts vs. general capability, chain-of-thought prompting complexity vs. potential accuracy gains
- Failure signatures: Low agreement between prompt variations (indicates model inconsistency), poor performance on minority classes (indicates bias or insufficient training data), confidence score discrepancies between correct and incorrect classifications
- First 3 experiments:
  1. Test base models on small subset of papers with different prompt types to establish baseline performance and identify which prompt structure works best
  2. Fine-tune GPT-4o Mini on the 238-paper corpus and evaluate on the 88-paper test set to measure improvement magnitude
  3. Compare agreement rates between different prompt versions to assess model consistency and identify which categories are most problematic

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal fine-tuning dataset size and composition needed to maximize LLM performance on ESG literature analysis?
- Basis in paper: [explicit] The paper shows that fine-tuned GPT-4o Mini outperformed base models by 28.3% on average, but only used 238 papers for training
- Why unresolved: The paper only tested one fine-tuning dataset size and composition, without exploring how different training data quantities or ESG-specific features might affect performance
- What evidence would resolve it: Comparative studies testing multiple training dataset sizes (e.g., 50, 100, 238, 500 papers) and compositions (general vs. ESG-specific) to identify performance thresholds

### Open Question 2
- Question: Can LLMs achieve human-level accuracy in systematic reviews through iterative prompt engineering alone, without fine-tuning?
- Basis in paper: [inferred] The paper shows prompt variations (A-C) had inconsistent effects on accuracy, with some improvements noted but not reaching human-level performance
- Why unresolved: The paper tested only three prompt variations without systematic exploration of prompt engineering techniques or iterative refinement
- What evidence would resolve it: Head-to-head comparisons of prompt-engineered vs. fine-tuned models using standardized evaluation protocols across multiple systematic review tasks

### Open Question 3
- Question: What is the relationship between LLM confidence scores and actual accuracy in ESG literature classification tasks?
- Basis in paper: [explicit] The paper reports confidence scores but finds "no significant difference between the confidence scores when comparing both LLMs against each other and when comparing the scores for correct and incorrect classifications"
- Why unresolved: The paper only reports descriptive statistics without testing the predictive validity of confidence scores for accuracy
- What evidence would resolve it: Statistical analysis of confidence score distributions for correct vs. incorrect classifications across multiple models and tasks to establish predictive validity metrics

### Open Question 4
- Question: How do different LLM architectures (transformer variants, attention mechanisms) perform specifically on ESG literature analysis tasks?
- Basis in paper: [inferred] The paper tested only Llama 3 and GPT-4 variants, without exploring other LLM architectures or their specific design choices
- Why unresolved: The paper provides limited architectural comparison, focusing on two model families without testing alternatives
- What evidence would resolve it: Comparative evaluation of multiple LLM architectures (BERT variants, OPT, LLaMA variants, etc.) on standardized ESG literature datasets with detailed performance breakdowns

## Limitations
- Dataset size of 88 test papers is relatively small for drawing broad conclusions about LLM performance in systematic reviews
- Models showed significant struggles with "S" (social) and "Other" classification categories, suggesting potential systematic biases
- Abstracts were used for Llama 3 while full PDFs were used for GPT-4o, introducing a confounding variable

## Confidence
- High confidence: The core finding that fine-tuned models outperform base models in systematic review classification tasks is well-supported by the 28.3% accuracy improvement data
- Medium confidence: The claim that chain-of-thought prompting provides consistent improvements (3.0-15.7%) is moderately supported but shows variable effects across different prompt types
- Low confidence: The assertion that all models struggle equally with "S" and "Other" categories lacks sufficient diagnostic analysis

## Next Checks
1. Cross-validation on independent ESG datasets: Test the fine-tuned GPT-4o Mini and Custom GPT models on at least 100 additional hand-coded ESG papers from different time periods and databases to assess whether the 28.3% accuracy improvement generalizes beyond the initial test set.

2. Ablation study on model inputs: Conduct controlled experiments using both abstracts and full PDFs for all models (not just Llama 3) to isolate whether input comprehensiveness or model architecture drives performance differences.

3. Minority class performance analysis: Systematically examine misclassifications in "S" and "Other" categories to determine whether targeted fine-tuning on these specific classes could resolve the performance gap, or whether these represent fundamental limitations in LLM classification capabilities for nuanced ESG distinctions.