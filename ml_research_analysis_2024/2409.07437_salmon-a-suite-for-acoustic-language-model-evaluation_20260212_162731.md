---
ver: rpa2
title: 'Salmon: A Suite for Acoustic Language Model Evaluation'
arxiv_id: '2409.07437'
source_url: https://arxiv.org/abs/2409.07437
tags:
- speech
- arxiv
- acoustic
- background
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SALMon introduces a benchmark for evaluating speech language models
  on acoustic properties beyond semantics. It uses a modeling-based approach to test
  whether models assign higher likelihood to recordings with consistent acoustic elements
  compared to ones with unnatural changes.
---

# Salmon: A Suite for Acoustic Language Model Evaluation

## Quick Facts
- arXiv ID: 2409.07437
- Source URL: https://arxiv.org/abs/2409.07437
- Reference count: 40
- Primary result: SALMon benchmark reveals significant performance gaps in speech language models' acoustic modeling capabilities compared to human raters

## Executive Summary
SALMon introduces a benchmark for evaluating speech language models on acoustic properties beyond semantics. It uses a modeling-based approach to test whether models assign higher likelihood to recordings with consistent acoustic elements compared to ones with unnatural changes. The benchmark covers acoustic consistency (gender, speaker, background, sentiment, room impulse response) and semantic-acoustic alignment (background and sentiment alignment with spoken content). Evaluations on multiple SLMs reveal significant performance gaps compared to human raters, with even the best models scoring far below human-level performance on most tasks. Expressive models like pGSLM show some advantage in speaker consistency, but none excel at acoustic-semantic alignment, highlighting a need for improved acoustic modeling in SLMs.

## Method Summary
SALMon evaluates speech language models using a likelihood-based approach that compares model scores between recordings with consistent acoustic properties and those with unnatural modifications. The benchmark creates positive and negative sample pairs by splitting recordings and replacing acoustic elements like speaker identity, background noise, sentiment, or room impulse response. Models are scored based on how often they assign higher probability to the original (correct) samples versus modified (incorrect) ones. The evaluation covers both acoustic consistency tasks and acoustic-semantic alignment tasks, with human performance serving as a reference baseline.

## Key Results
- SLMs show significant performance gaps on SALMon tasks compared to human raters, with scores far below human-level performance
- Expressive models like pGSLM demonstrate some advantage in speaker consistency tasks but fail at acoustic-semantic alignment
- All evaluated models achieve near-random performance on background and sentiment alignment tasks, indicating fundamental limitations in joint acoustic-text modeling
- Current SLMs struggle particularly with background noise modeling and acoustic-semantic alignment, suggesting a need for improved acoustic representation learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The modeling-based approach (likelihood comparison of correct vs. incorrect samples) effectively measures acoustic understanding by forcing the model to differentiate subtle acoustic variations.
- Mechanism: The model is presented with two samples - one with consistent acoustic properties and one with an unnatural change mid-sample. By computing likelihood ratios, the approach captures whether the model has learned to model the joint distribution of acoustic elements and text.
- Core assumption: Higher likelihood on correct samples indicates genuine acoustic modeling rather than memorization or spurious correlations.
- Evidence anchors:
  - [abstract] "The proposed benchmarks both evaluate the consistency of the inspected element and how much it matches the spoken text."
  - [section] "This approach makes the benchmark fast to compute even for large models."
  - [corpus] Weak - corpus neighbors don't discuss likelihood-based evaluation approaches.
- Break condition: If the model learns to discriminate based on text-only cues or background artifacts rather than genuine acoustic modeling, the metric becomes unreliable.

### Mechanism 2
- Claim: The split between acoustic consistency and acoustic-semantic alignment tasks provides a hierarchical evaluation of model capabilities.
- Mechanism: Acoustic consistency tests basic modeling of acoustic elements in isolation, while alignment requires joint reasoning over both text and acoustic streams. This structure reveals whether models can handle increasingly complex acoustic-semantic relationships.
- Core assumption: Performance degradation from consistency to alignment tasks indicates limitations in joint acoustic-text modeling rather than isolated acoustic understanding.
- Evidence anchors:
  - [abstract] "The proposed benchmarks both evaluate the consistency of the inspected element and how much it matches the spoken text."
  - [section] "The acoustic consistency benchmark tests whether the model assigns higher likelihood to the original recording... The acoustic-semantic alignment metric checks if the model assigns higher scores to samples where the acoustic information matches the spoken content."
  - [corpus] Weak - corpus neighbors don't discuss hierarchical evaluation structures.
- Break condition: If models perform similarly on both tasks, the distinction between consistency and alignment may not capture meaningful differences in model capabilities.

### Mechanism 3
- Claim: Using discrete speech representations (HuBERT units) enables efficient acoustic modeling while maintaining semantic information.
- Mechanism: Discrete tokens provide a compressed representation that can be processed by language models, allowing joint modeling of acoustic and semantic information in a single framework.
- Core assumption: Discrete representations preserve sufficient acoustic information for the evaluated tasks while being computationally tractable for large-scale language modeling.
- Evidence anchors:
  - [abstract] "Much like text language models, speech language models... use a next token prediction objective... there are two main approaches: audio encoding into the text LM latent, or vocabulary expansion."
  - [section] "As semantic tokens were shown to mostly discard prosody... expressive SLMs augment these speech representations with prosodic features through separate streams of pitch or style tokens."
  - [corpus] Weak - corpus neighbors don't discuss discrete speech representation choices.
- Break condition: If discrete representations discard critical acoustic information needed for the evaluated tasks, model performance will be artificially limited regardless of architecture quality.

## Foundational Learning

- Concept: Likelihood-based evaluation
  - Why needed here: The benchmark relies on comparing model likelihoods between correct and incorrect samples to measure acoustic understanding.
  - Quick check question: How would you compute the score for a task where a model correctly identifies 8 out of 10 samples?
  - Answer: Score = 8/10 = 0.8

- Concept: Forced alignment
  - Why needed here: For tasks requiring word-level acoustic changes, the benchmark needs precise word boundaries to split audio correctly.
  - Quick check question: What tool is mentioned for obtaining word-level alignments in the paper?
  - Answer: Montreal Forced Aligner

- Concept: Discrete speech tokenization
  - Why needed here: Understanding the difference between semantic tokens (HuBERT) and expressive tokens (pitch/style) is crucial for interpreting model performance differences.
  - Quick check question: Which tokenization approach is mentioned as being "ill-equipped to model background noise"?
  - Answer: HuBERT units

## Architecture Onboarding

- Component map:
  - Data generation pipeline -> Model interface -> Evaluation script -> Visualization tools

- Critical path:
  1. Generate sample pairs (original vs. modified)
  2. Compute model likelihoods for each sample
  3. Calculate score using Equation 1
  4. Aggregate results across tasks
  5. Compare against human baseline

- Design tradeoffs:
  - Modeling vs. generation: Modeling approach is faster but may miss some aspects of acoustic understanding
  - Discrete vs. continuous representations: Discrete tokens are efficient but may lose information
  - Task difficulty: Balance between trivial tasks (human performance ~100%) and challenging ones

- Failure signatures:
  - Random performance (~0.5) indicates model cannot distinguish acoustic elements
  - Near-human performance suggests the task may be too easy or the model has strong acoustic capabilities
  - Inconsistent performance across similar tasks may indicate dataset quality issues

- First 3 experiments:
  1. Run the evaluation script on a simple baseline (e.g., random model) to verify the pipeline works
  2. Evaluate a known expressive model (e.g., pGSLM) to establish a performance reference point
  3. Test with a cascaded ASR+LLM pipeline to verify it achieves near-random performance as claimed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can speech language models be effectively trained to improve their acoustic modeling capabilities, particularly for tasks like background noise and sentiment consistency?
- Basis in paper: [explicit] The paper highlights that current models struggle with acoustic consistency tasks, such as background noise and sentiment consistency, and suggests that this may be due to a lack of expressive modeling or training data diversity.
- Why unresolved: The paper identifies the problem but does not propose a specific solution or training method to address these acoustic modeling challenges.
- What evidence would resolve it: Demonstrating a new training approach or model architecture that significantly improves performance on acoustic consistency tasks, with quantitative results showing a reduction in the performance gap between current models and human performance.

### Open Question 2
- Question: What are the limitations of using HuBERT tokens in speech language models for acoustic modeling, and how can these limitations be overcome?
- Basis in paper: [explicit] The paper suggests that HuBERT units are likely ill-equipped to model background noise and other acoustic elements, and that this could be a reason for poor performance in acoustic consistency tasks.
- Why unresolved: While the paper identifies the limitations of HuBERT tokens, it does not explore alternative tokenization methods or architectural changes that could overcome these limitations.
- What evidence would resolve it: Comparing the performance of models using different tokenization methods or architectural changes on acoustic consistency tasks, with a focus on improvements in modeling background noise and other acoustic elements.

### Open Question 3
- Question: How can speech language models be designed to better integrate text and acoustic information for tasks that require reasoning over both modalities?
- Basis in paper: [explicit] The paper notes that while expressive models like Spirit-LM can model sentiment to some extent, they achieve near-random performance on semantic-acoustic alignment tasks, indicating a lack of ability to jointly model text and acoustic content.
- Why unresolved: The paper highlights the issue but does not propose a specific approach or architecture to improve the integration of text and acoustic information in speech language models.
- What evidence would resolve it: Developing a model architecture or training method that demonstrates improved performance on semantic-acoustic alignment tasks, with a focus on reasoning over both text and acoustic information, and showing a significant reduction in the performance gap between current models and human performance.

## Limitations

- The benchmark's likelihood-based evaluation approach may not fully capture all aspects of acoustic understanding and could be biased by implementation details
- Discrete speech representations like HuBERT units may discard critical acoustic information needed for accurate evaluation, particularly for background noise modeling
- Synthetic data generation for acoustic-semantic alignment tasks may introduce artifacts that don't reflect real-world acoustic conditions

## Confidence

**High Confidence:**
- The modeling-based approach effectively measures acoustic understanding by comparing likelihood scores between correct and incorrect samples
- The split between consistency and alignment tasks provides meaningful differentiation of model capabilities
- Human baselines serve as appropriate reference points for model performance

**Medium Confidence:**
- Discrete speech representations preserve sufficient acoustic information for the evaluated tasks
- The benchmark adequately captures real-world acoustic understanding challenges
- Synthetic data generation produces valid test samples for acoustic-semantic alignment

**Low Confidence:**
- The exact likelihood computation methodology doesn't introduce biases
- Manual filtering process for generated text samples doesn't affect benchmark validity
- The evaluation script implementation matches the intended methodology

## Next Checks

1. **Validation of likelihood computation methodology**: Implement a controlled experiment comparing model likelihoods against ground-truth acoustic feature distances for a subset of samples. This would verify whether higher likelihood actually correlates with acoustic similarity, addressing concerns about the fundamental measurement approach.

2. **Discrete representation ablation study**: Run the SALMon benchmark using both HuBERT units and continuous acoustic features on a subset of tasks. This would quantify the information loss from discretization and help determine whether the performance gap is due to representation choice versus model capability.

3. **Synthetic data quality assessment**: Conduct a perceptual study where human raters evaluate the acoustic quality and naturalness of synthetic samples generated for the acoustic-semantic alignment tasks. This would validate whether the synthetic data introduces artifacts that could unfairly disadvantage models or whether it provides a reasonable approximation of real acoustic-semantic relationships.