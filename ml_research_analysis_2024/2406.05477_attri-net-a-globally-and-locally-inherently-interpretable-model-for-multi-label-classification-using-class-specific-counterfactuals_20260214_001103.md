---
ver: rpa2
title: 'Attri-Net: A Globally and Locally Inherently Interpretable Model for Multi-Label
  Classification Using Class-Specific Counterfactuals'
arxiv_id: '2406.05477'
source_url: https://arxiv.org/abs/2406.05477
tags:
- explanations
- class
- guidance
- attribution
- attri-net
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Attri-Net is an inherently interpretable multi-label classifier
  for medical imaging that generates class-specific counterfactual attribution maps
  to explain disease evidence. The method uses a task-switching class attribution
  generator to produce pixel-level attribution maps for each disease, which are then
  used by logistic regression classifiers for prediction.
---

# Attri-Net: A Globally and Locally Inherently Interpretable Model for Multi-Label Classification Using Class-Specific Counterfactuals

## Quick Facts
- arXiv ID: 2406.05477
- Source URL: https://arxiv.org/abs/2406.05477
- Reference count: 39
- Achieves class sensitivity up to 0.951 and disease sensitivity up to 0.965 on chest X-ray datasets

## Executive Summary
Attri-Net is an inherently interpretable multi-label classifier for medical imaging that generates class-specific counterfactual attribution maps to explain disease evidence. The method uses a task-switching class attribution generator to produce pixel-level attribution maps for each disease, which are then used by logistic regression classifiers for prediction. Local explanations are derived from the weighted attribution maps, while global explanations are obtained from class centers and classifier weights. The model incorporates guidance mechanisms to align predictions with human knowledge using bounding box annotations. Evaluations on chest X-ray datasets show Attri-Net achieves high-quality explanations with class sensitivity scores up to 0.951 and disease sensitivity up to 0.965, while maintaining classification performance comparable to state-of-the-art models (AUC around 0.78-0.87). The global explanation capability successfully identifies shortcut learning behavior, and model guidance significantly improves disease localization.

## Method Summary
Attri-Net generates class-specific attribution maps using a StarGAN-based class attribution generator with task-switching via AdaIN layers. For each disease, the generator produces an additive residual map that transforms the input image into a counterfactual without that disease. These attribution maps are then downsampled by a factor of 32 and fed into logistic regression classifiers for prediction. The model is trained end-to-end with five loss terms: adversarial loss for realistic counterfactuals, classification loss for disease prediction, regularization loss for attribution map quality, center loss for class separation, and optional guidance loss for aligning attributions with human annotations. Training uses Adam optimizer with batch size 4 for 100k steps, with the discriminator updated 5 times per generator step.

## Key Results
- Achieves class sensitivity scores up to 0.951, indicating high-quality local explanations
- Disease sensitivity reaches 0.965 with guidance, demonstrating superior localization of disease regions
- Classification AUC ranges from 0.78-0.87, comparable to state-of-the-art models
- Global explanation successfully identifies shortcut learning patterns in controlled experiments
- Model guidance improves disease localization by 6.6% compared to non-guided training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Class-specific counterfactual attribution maps enable faithful local explanations by directly encoding the evidence used for classification.
- Mechanism: The class attribution generator Mc(x) learns to produce additive residual maps that transform input images into counterfactuals without class c. These maps contain disease-specific evidence that logistic regression classifiers use directly for prediction.
- Core assumption: The residual between an image and its counterfactual version captures all disease-specific evidence needed for classification.
- Evidence anchors:
  - [abstract] "Attri-Net first counterfactually generates class-specific attribution maps to highlight the disease evidence, then performs classification with logistic regression classifiers based solely on the attribution maps."
  - [section 3.2] "Mc captures the changes required for each pixel in the input to remove the positive effect of class c from the image."
  - [corpus] Weak - corpus papers discuss interpretable classification but don't specifically address counterfactual-based attribution maps for multi-label scenarios.
- Break condition: If the residual doesn't capture all disease-specific information, or if the logistic regression cannot linearly separate classes from the attribution maps.

### Mechanism 2
- Claim: Task-switching mechanism enables single network to generate multiple disease-specific attribution maps without interference.
- Mechanism: Task codes are injected through AdaIN layers throughout the class attribution generator, allowing the network to switch between diagnostic tasks and generate distinct attribution maps for each disease.
- Core assumption: AdaIN layers can effectively modulate network features to focus on different diseases based on task codes.
- Evidence anchors:
  - [section 3.2] "we introduce a task switch mechanism based on recent work to enable the class attribution generator to switch between various diagnostic tasks... Each task code is a one-hot encoding spatially upsampled by a factor of 20."
  - [section 3.7] "To enable the task-switching functionality, we replaced instance normalization layers in the original StarGAN architecture with adaptive instance normalization (AdaIN) modules."
  - [corpus] Moderate - AdaIN for style transfer is well-established, but task-switching for multi-label medical diagnosis is novel.
- Break condition: If task codes interfere with each other or if AdaIN modulation is insufficient to create truly distinct disease-specific features.

### Mechanism 3
- Claim: Center loss and model guidance improve attribution map quality by enforcing class separation and alignment with human knowledge.
- Mechanism: Center loss pulls attribution maps toward learned class centers, increasing inter-class separation. Guidance loss encourages attributions to focus on annotated disease regions.
- Core assumption: Attributions that are closer to class centers and aligned with human annotations will be more discriminative and clinically meaningful.
- Evidence anchors:
  - [section 3.4] "the center loss calculates the L2 distance between the attribution map M(x,tc) and the corresponding class center... This reduces the intra-class distance while increasing the inter-class separation."
  - [section 3.5] "The guidance loss encourages the model to focus on the regions within the guidance mask that contain task-relevant features while ignoring the regions outside."
  - [section 4.4.1] "when trained with guidance, our Attri-Net achieved the highest disease sensitivity score across all datasets, indicating superior localization of disease-relevant regions."
  - [corpus] Moderate - guidance mechanisms are established in literature, but combining them with center loss for attribution maps is novel.
- Break condition: If guidance masks are incorrect or if center loss causes overfitting to training data distribution.

## Foundational Learning

- Concept: Adversarial training for image-to-image translation
  - Why needed here: Enables the class attribution generator to produce realistic counterfactual images that fool the discriminator, ensuring generated attribution maps contain valid disease evidence.
  - Quick check question: How does the Wasserstein GAN loss formulation ensure the generated counterfactuals are realistic and contain only the targeted disease information?

- Concept: Logistic regression on spatial feature maps
  - Why needed here: Provides interpretable classification where each pixel's contribution to the prediction is weighted and visible, enabling direct explanation from the attribution maps.
  - Quick check question: Why does using a simple logistic regression classifier on downsampled attribution maps provide both classification and explanation simultaneously?

- Concept: Multi-label classification vs multi-class classification
  - Why needed here: Understanding the difference is crucial because Attri-Net generates independent attribution maps for each disease label, unlike single-label approaches that compete for attention.
  - Quick check question: How does the task-switching mechanism specifically address the challenges of multi-label scenarios where multiple diseases can co-occur in the same image?

## Architecture Onboarding

- Component map: Input → Task encoder → Class attribution generator (with AdaIN) → Attribution maps → Discriminator (with AdaIN) + Logistic regression classifiers + Center loss module + Optional guidance loss
- Critical path: Image → M(x,tc) → Logistic regression → Prediction
- Design tradeoffs: Using separate forward passes for each class (vs multi-channel output) trades computation for cleaner class-specific attributions; logistic regression trades some classification performance for interpretability.
- Failure signatures: Noisy attribution maps suggest discriminator issues; poor classification with clean maps suggests logistic regression weights aren't capturing relevant features; guidance not improving localization suggests guidance masks are incorrect or insufficient.
- First 3 experiments:
  1. Train with only classification loss and adversarial loss - verify attribution maps capture disease evidence but may be noisy
  2. Add regularization and center loss - observe cleaner, more discriminative attribution maps
  3. Add guidance loss with ground truth annotations - measure improvement in disease sensitivity and localization accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Attri-Net's global explanation capability perform in detecting more complex shortcut learning patterns beyond fixed spatial artifacts, such as spurious correlations that vary across different image regions or datasets?
- Basis in paper: [explicit] The paper demonstrates Attri-Net's ability to detect a synthetic shortcut learning scenario involving a fixed spatial tag signal, but notes that real-world shortcuts can be more diverse and spatially variable.
- Why unresolved: The evaluation is limited to a controlled synthetic scenario with a fixed spatial location, and the paper acknowledges the need to investigate more complex shortcut learning conditions.
- What evidence would resolve it: Experiments applying Attri-Net's global explanation to datasets with naturally occurring shortcuts of varying spatial patterns, and quantitative metrics for detecting these more complex shortcuts.

### Open Question 2
- Question: What is the trade-off between annotation effort and the benefits of model guidance in Attri-Net, particularly for diseases with small or varying lesion regions?
- Basis in paper: [inferred] The paper explores pseudo-guidance using limited ground truth annotations and shows improvement in disease sensitivity, but notes that the effect varies between diseases and that pixel-level lesion annotation requires extensive expert effort.
- Why unresolved: The paper provides qualitative observations about disease-specific improvements but lacks a systematic quantification of the annotation effort versus guidance benefit across different disease types and annotation strategies.
- What evidence would resolve it: A comprehensive study quantifying the relationship between annotation quantity/quality and guidance effectiveness across multiple diseases, including cost-benefit analysis of different guidance strategies.

### Open Question 3
- Question: How does Attri-Net's classification and explanation performance compare to post-hoc explanation methods when applied to 3D medical imaging domains like CT or MRI?
- Basis in paper: [explicit] The paper suggests potential extension to other imaging modalities including 3D domains but limits evaluation to chest X-ray datasets.
- Why unresolved: The Attri-Net architecture is designed for 2D images, and the paper only evaluates on chest X-ray datasets without testing on 3D medical imaging modalities.
- What evidence would resolve it: Implementation and evaluation of Attri-Net on 3D medical imaging datasets, comparing its performance and explanations against post-hoc methods applied to 3D CNN architectures.

### Open Question 4
- Question: What is the optimal balance between the different loss terms (adversarial, classification, regularization, center, and guidance) for different disease types and datasets?
- Basis in paper: [explicit] The paper provides an ablation study on the loss terms but uses fixed hyperparameters across all experiments and diseases.
- Why unresolved: The ablation study shows the effects of individual losses but doesn't explore disease-specific or dataset-specific optimal weightings of the loss terms.
- What evidence would resolve it: A systematic study varying the loss term weights across different diseases and datasets to determine optimal configurations for each scenario.

## Limitations

- Evaluation relies heavily on pseudo-guidance masks generated from bounding boxes, which may not accurately represent disease regions
- Model's computational efficiency is not thoroughly evaluated, with multiple forward passes required per image
- Generalization of explanation quality to other medical imaging domains beyond chest X-rays remains untested

## Confidence

- **High confidence**: Classification performance (AUC scores) and comparison with baseline models, as these use standard evaluation metrics
- **Medium confidence**: Local explanation quality metrics (class sensitivity, disease sensitivity), which depend on pseudo-ground truth and may not reflect true clinical utility
- **Low confidence**: Global explanation capability for shortcut learning detection, as the paper doesn't validate whether identified shortcuts actually represent clinically meaningful correlations

## Next Checks

1. Test the model with varying quality and completeness of guidance masks to assess robustness of explanation quality to imperfect supervision

2. Conduct human evaluation studies with radiologists to validate whether the generated attribution maps align with clinical reasoning and improve diagnostic confidence

3. Evaluate the model on a different medical imaging domain (e.g., dermatology or pathology) to test generalization of both classification and explanation capabilities