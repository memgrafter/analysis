---
ver: rpa2
title: 'LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations'
arxiv_id: '2410.02707'
source_url: https://arxiv.org/abs/2410.02707
tags:
- answer
- error
- exact
- llms
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates whether large language models (LLMs) encode\
  \ information about the truthfulness of their own outputs and how this can be leveraged\
  \ for error analysis. The core finding is that truthfulness information is localized\
  \ in specific tokens\u2014particularly the exact answer tokens\u2014rather than\
  \ distributed uniformly across the response."
---

# LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations

## Quick Facts
- arXiv ID: 2410.02707
- Source URL: https://arxiv.org/abs/2410.02707
- Reference count: 40
- Primary result: Truthfulness information in LLMs is localized in exact answer tokens rather than uniformly distributed, enabling better error detection but limited generalization across tasks.

## Executive Summary
This paper investigates whether large language models encode information about the truthfulness of their own outputs and how this can be leveraged for error analysis. The core finding is that truthfulness information is localized in specific tokens—particularly the exact answer tokens—rather than distributed uniformly across the response. By training classifiers on these targeted internal representations, error detection performance improves significantly compared to baselines that use logits or probe the last generated token. However, the study also reveals that such error detectors do not generalize well across different tasks, suggesting multiple, skill-specific encodings of truthfulness rather than a universal one. Moreover, by analyzing repeated generations, the authors find that the types of errors an LLM makes (e.g., consistently wrong, occasionally correct, or generating many distinct wrong answers) can be predicted from its internal representations. Finally, the paper demonstrates a discrepancy between the model's internal encoding and its external behavior: even when the model internally encodes the correct answer, it may still consistently generate incorrect responses.

## Method Summary
The authors evaluate four LLMs (Mistral-7B, Mistral-7B-Instruct, Llama3-8B, Llama3-8B-Instruct) across 10 datasets spanning factual QA, sentiment analysis, reasoning, and arithmetic tasks. They generate responses using greedy decoding and extract exact answer tokens using an instruct LLM. Probing classifiers (logistic regression) are trained on intermediate activations of exact answer tokens to detect errors, compared against baselines using logits, probabilities, and p(True). The study evaluates error detection performance, generalization across tasks, and predictability of error types from internal representations.

## Key Results
- Truthfulness information is concentrated in exact answer tokens rather than distributed uniformly across responses.
- Error detectors using exact answer token representations achieve higher AUC (e.g., 0.81 vs 0.73) compared to baselines using last generated tokens or logits.
- Error detection performance does not generalize well across different tasks, indicating skill-specific truthfulness mechanisms.
- Error types (consistently wrong, occasionally correct, many distinct wrong answers) are predictable from LLM internal representations.
- LLMs can internally encode correct answers while consistently generating incorrect responses, revealing a disconnect between internal knowledge and external behavior.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Truthfulness signals in LLMs are localized in specific tokens, particularly the exact answer tokens, rather than distributed uniformly.
- Mechanism: During generation, internal representations encode strong truthfulness signals at the tokens that correspond to the final answer, peaking at the exact answer tokens. These signals are weaker before and after these tokens.
- Core assumption: LLMs have a mechanism that concentrates truthfulness encoding at the moment of generating the final answer, reflecting their internal confidence in correctness.
- Evidence anchors:
  - [abstract] "we discover that the truthfulness information is concentrated in specific tokens"
  - [section] "we find that truthfulness information is concentrated in the exact answer tokens"
- Break condition: If answer tokens are not identifiable or if the model uses different strategies for truthfulness encoding, this mechanism fails.

### Mechanism 2
- Claim: LLMs encode multiple, skill-specific notions of truthfulness rather than a universal one.
- Mechanism: Different tasks require different skills (e.g., factual retrieval, common-sense reasoning). The internal representations encode truthfulness signals that are specific to these skills and do not generalize across tasks requiring different abilities.
- Core assumption: LLMs have distinct internal representations for different types of truthfulness, corresponding to the specific skills required for each task.
- Evidence anchors:
  - [abstract] "truthfulness encoding is not universal but rather multifaceted"
  - [section] "LLMs have many 'skill-specific' truthfulness mechanisms rather than universal ones"
- Break condition: If a universal truthfulness encoding exists or if skill-specific mechanisms overlap significantly, this mechanism fails.

### Mechanism 3
- Claim: LLMs encode information about the types of errors they are likely to make, which can be predicted from internal representations.
- Mechanism: The internal representations of LLMs contain signals that correlate with different error types (e.g., consistently wrong, occasionally correct, many distinct wrong answers). By analyzing these representations, one can predict the pattern of errors the model will make.
- Core assumption: The internal states of LLMs capture not just whether an output is correct or wrong, but also the nature of the errors they tend to produce.
- Evidence anchors:
  - [abstract] "internal representations can also be used for predicting the types of errors the model is likely to make"
  - [section] "we find that error types are predictable from the LLM representations"
- Break condition: If error types cannot be predicted from internal representations or if the model's behavior does not correlate with these predictions, this mechanism fails.

## Foundational Learning

- Concept: Token selection in error detection
  - Why needed here: The choice of token (e.g., last generated token, exact answer token) significantly impacts the effectiveness of error detection methods.
  - Quick check question: Why is the exact answer token more effective for error detection than the last generated token?

- Concept: Probing classifiers
  - Why needed here: Probing classifiers are used to detect truthfulness signals in LLMs by training on intermediate activations. Understanding how they work is crucial for interpreting the results.
  - Quick check question: How does a probing classifier differ from using logits or probabilities for error detection?

- Concept: Generalization in machine learning
  - Why needed here: The paper explores whether error detectors trained on one dataset can generalize to others, which is essential for practical applications.
  - Quick check question: What factors might affect the ability of an error detector to generalize across different tasks?

## Architecture Onboarding

- Component map: Input prompt -> LLM processing -> Generation of response -> Extraction of exact answer tokens -> Probing of internal representations -> Error detection/classification
- Critical path: Input prompt → LLM processing → Generation of response → Extraction of exact answer tokens → Probing of internal representations → Error detection/classification
- Design tradeoffs: Using exact answer tokens improves error detection but requires additional computation to identify these tokens. Generalization across tasks is limited, so task-specific detectors may be necessary.
- Failure signatures: Poor error detection performance when using wrong tokens, failure to generalize across tasks, misalignment between internal representations and external behavior.
- First 3 experiments:
  1. Test error detection performance using different token selections (last generated token vs. exact answer token).
  2. Evaluate generalization of probing classifiers across different datasets.
  3. Analyze the types of errors by resampling model responses and correlating with internal representations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are there universal features of truthfulness encoding in LLMs that can be leveraged for error detection across different tasks?
- Basis in paper: [explicit]
- Why unresolved: The paper demonstrates that error detectors trained on one task do not generalize well to others, with generalization occurring only within tasks requiring similar skills (e.g., factual retrieval). This challenges the notion of a universal truthfulness encoding mechanism.
- What evidence would resolve it: Further research could analyze the intermediate representations of LLMs across a wider range of tasks to identify any common patterns or features that could be used for error detection across different domains.

### Open Question 2
- Question: Can the types of errors an LLM makes be predicted from its internal representations?
- Basis in paper: [explicit]
- Why unresolved: While the paper shows that error types are predictable from the LLM's internal representations, the exact mechanisms and features used for this prediction are not fully understood. Further investigation is needed to identify the specific patterns and signals that correlate with different error types.
- What evidence would resolve it: More detailed analysis of the intermediate activations and their relationship to the different error types could shed light on the underlying mechanisms and potentially lead to more targeted error mitigation strategies.

### Open Question 3
- Question: How can the discrepancy between an LLM's internal encoding and external behavior be addressed to reduce errors?
- Basis in paper: [explicit]
- Why unresolved: The paper reveals a significant disconnect between the model's internal representation of truthfulness and its actual behavior during generation. Understanding the reasons behind this discrepancy and developing methods to align the internal and external states could lead to significant improvements in error reduction.
- What evidence would resolve it: Research into the mechanisms that influence the model's generation process, such as likelihood bias or other factors, could help identify ways to better leverage the model's internal knowledge and reduce the occurrence of errors.

## Limitations

- Token identification method uncertainty: The paper relies on an instruct LLM to extract "exact answer tokens" from responses, introducing potential bias and error that directly impacts all downstream analyses.
- Generalization failure pattern: While the paper shows probing classifiers don't generalize well across tasks, it doesn't explain why certain task pairs transfer better than others, and the limited number of datasets (10) and models (4) constrains generalizability.
- Correlation vs causation in error type prediction: The ability to predict error types from internal representations is demonstrated, but the causal mechanism remains unclear—it's not established whether internal representations cause the error patterns or merely correlate with them.

## Confidence

**High confidence**: The core finding that error detection performance improves when probing exact answer tokens versus last generated tokens or logits. This is supported by clear numerical results (e.g., Figure 2 shows 0.81 vs 0.73 AUC) and multiple ablation studies.

**Medium confidence**: The claim about multiple, skill-specific truthfulness mechanisms. While the generalization failure is well-documented, the explanation for why this occurs is more speculative. The paper provides evidence of the phenomenon but limited mechanistic understanding.

**Low confidence**: The prediction of error types from internal representations. The methodology is sound, but the practical significance and reliability of these predictions across diverse scenarios needs further validation.

## Next Checks

**Validation Check 1**: Replicate the exact answer token extraction using multiple independent instruct LLMs and measure inter-annotator agreement. This would quantify the reliability of the token identification step that underpins all subsequent analyses.

**Validation Check 2**: Systematically vary the layer depth and token positions for probing classifiers to determine whether the observed optimal positions are consistent across different model architectures and dataset types. This would test whether the "exact answer token" finding is robust or dataset-specific.

**Validation Check 3**: Conduct cross-dataset experiments with progressively similar task types (e.g., QA to reading comprehension to fact-checking) to quantify how task similarity affects generalization of error detectors. This would provide a more granular understanding of the skill-specific truthfulness mechanisms.