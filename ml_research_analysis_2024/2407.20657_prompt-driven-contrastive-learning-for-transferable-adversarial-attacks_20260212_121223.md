---
ver: rpa2
title: Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks
arxiv_id: '2407.20657'
source_url: https://arxiv.org/abs/2407.20657
tags:
- adversarial
- clip
- attack
- features
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PDCL-Attack, a novel method for enhancing
  transferable adversarial attacks by leveraging the CLIP vision-language model and
  prompt learning. The key idea is to use prompt-driven contrastive learning to guide
  the perturbation generator toward more robust and transferable adversarial examples.
---

# Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks

## Quick Facts
- arXiv ID: 2407.20657
- Source URL: https://arxiv.org/abs/2407.20657
- Authors: Hunmin Yang; Jongoh Jeong; Kuk-Jin Yoon
- Reference count: 40
- Key outcome: Introduces PDCL-Attack, a novel method for enhancing transferable adversarial attacks by leveraging the CLIP vision-language model and prompt learning

## Executive Summary
This paper introduces PDCL-Attack, a novel method for enhancing transferable adversarial attacks by leveraging the CLIP vision-language model and prompt learning. The key idea is to use prompt-driven contrastive learning to guide the perturbation generator toward more robust and transferable adversarial examples. Specifically, the method employs text features from ground-truth class labels of input images and adversarial image features from CLIP to provide effective loss gradients for training the generator. Additionally, the paper introduces a prompt learning stage to improve the robustness of CLIP to distribution shifts. The method achieves state-of-the-art attack transferability across various domains and model architectures, surpassing previous approaches.

## Method Summary
PDCL-Attack operates in two phases: first, prompt context training to optimize learnable context words for CLIP's text encoder, improving robustness to distribution shifts; second, perturbation generator training using contrastive loss between CLIP image/text features and surrogate model mid-layer features. The method separates heterogeneous surrogate and CLIP features to avoid incompatibility issues while leveraging their complementary strengths for cross-model transferability and semantic guidance.

## Key Results
- Achieves state-of-the-art attack transferability across various domains and model architectures
- Surpasses previous approaches in both attack success rate and perceptual image quality metrics
- Demonstrates effectiveness on multiple datasets including ImageNet, CUB-200-2011, Stanford Cars, and FGVC Aircraft

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using text features from ground-truth class labels as negatives and adversarial text features (least similar) as positives in a contrastive loss enhances transferable adversarial attack effectiveness.
- Mechanism: The contrastive loss pulls adversarial image features toward the least similar text feature (adversarial text) while pushing them away from text features of ground-truth labels. This creates a stronger perturbation direction that generalizes better across models.
- Core assumption: The semantic alignment in CLIP's embedding space allows text features from class labels to serve as meaningful anchors for feature manipulation.
- Evidence anchors:
  - [abstract] "text features from ground-truth class labels of input images and adversarial image features from CLIP to provide effective loss gradients"
  - [section 3.2] "our method leverages CLIP's text features from ground-truth (GT) class labels and CLIP's perturbed image features to provide effective loss gradients for training the generator"
- Break condition: If the semantic alignment between image and text features degrades (e.g., with domain shifts or model variations), the contrastive loss would provide less effective guidance.

### Mechanism 2
- Claim: Prompt learning improves CLIP's robustness to distribution shifts, which in turn enhances the effectiveness of text-driven feature guidance for adversarial attacks.
- Mechanism: Training learnable context words for the prompt optimizes the text encoder to produce more generalizable text features. These robust features then provide better guidance for crafting transferable adversarial perturbations.
- Core assumption: Optimized prompts can significantly improve CLIP's feature representations for out-of-distribution examples.
- Evidence anchors:
  - [abstract] "the paper introduces a prompt learning stage to improve the robustness of CLIP to distribution shifts"
  - [section 3.1] "context optimization has further improved the few-shot performance of the employed CLIP model" and "this trained context enhances CLIP's robustness to distribution shifts"
- Break condition: If the prompt learning stage doesn't significantly improve CLIP's robustness to distribution shifts, the enhanced feature guidance would provide minimal benefit.

### Mechanism 3
- Claim: Separating surrogate model features from CLIP features (rather than mixing them) enables more effective training of the perturbation generator.
- Mechanism: By keeping surrogate and CLIP features in separate spaces, the model can leverage the complementary strengths of both - mid-layer features for cross-model transferability and CLIP features for semantic guidance - without feature incompatibility issues.
- Core assumption: Heterogeneous features from different models can conflict when mixed directly in a contrastive loss.
- Evidence anchors:
  - [section 3.2] "Our loss design differs from GAMA [2] since we separate heterogeneous surrogate and CLIP features"
  - [section A.3] "mixing of incompatible features extracted from models of different architectures and training methodologies could conflict, thus hindering optimization"
- Break condition: If the feature spaces of surrogate and CLIP models are sufficiently compatible, mixing them might not cause significant issues and could potentially simplify the training process.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: The method uses a contrastive loss to manipulate adversarial features by pulling them toward adversarial text features and pushing them away from ground-truth text features.
  - Quick check question: What is the difference between using ground-truth text features as negatives versus using random negative samples in contrastive learning?

- Concept: Vision-language models (VLMs) like CLIP
  - Why needed here: The method leverages CLIP's joint image-text embedding space to extract text features from class labels and use them as semantic anchors for adversarial perturbation.
  - Quick check question: How does CLIP map images and text into the same embedding space, and why does this enable cross-modal contrastive learning?

- Concept: Adversarial transferability
  - Why needed here: The method aims to create perturbations that are effective across different models and domains, not just the surrogate model.
  - Quick check question: What factors typically limit the transferability of adversarial examples, and how might semantic guidance help overcome these limitations?

## Architecture Onboarding

- Component map: Clean images → Generator → Projected adversarial images → CLIP/Surrogate feature extraction → Contrastive/surrogate loss computation → Generator update
- Critical path: Clean images → Generator → Projected adversarial images → CLIP/Surrogate feature extraction → Contrastive/surrogate loss computation → Generator update
- Design tradeoffs:
  - Using CLIP adds computational overhead but provides semantic guidance that improves transferability
  - Separate surrogate and CLIP features avoids incompatibility issues but requires maintaining two separate feature spaces
  - Learnable prompts improve robustness but require additional training complexity
- Failure signatures:
  - If the generator fails to converge, check the balance between surrogate loss and contrastive loss
  - If attack transferability doesn't improve, verify that the prompt learning stage is actually improving CLIP's robustness
  - If feature separation causes issues, consider whether the surrogate and CLIP feature spaces are sufficiently different
- First 3 experiments:
  1. Train the perturbation generator with only the surrogate loss (no CLIP guidance) and measure baseline transferability
  2. Add CLIP features without prompt learning to isolate the effect of semantic guidance
  3. Train the prompter separately on ImageNet and evaluate its robustness to distribution shifts before integrating it into the attack pipeline

## Open Questions the Paper Calls Out
None explicitly stated in the provided text.

## Limitations
- The method relies heavily on CLIP's semantic alignment between image and text features, which may degrade with domain shifts or model variations
- The prompt learning stage adds complexity and computational overhead without fully established necessity through ablation studies
- The perturbation generator architecture is not fully specified, creating reproduction challenges

## Confidence
- High confidence: The general approach of using contrastive learning with CLIP features is sound and well-supported by the results
- Medium confidence: The specific design choices (separating surrogate and CLIP features, prompt learning) are justified but could benefit from more extensive ablation studies
- Medium confidence: The experimental methodology is rigorous, but the lack of complete architectural details creates reproduction uncertainty

## Next Checks
1. Conduct an ablation study comparing PDCL-Attack with and without prompt learning across multiple domain shift scenarios to quantify the exact contribution of the prompter
2. Test the method's robustness to CLIP model variations by evaluating attacks using different versions of CLIP or alternative vision-language models
3. Perform a feature space analysis to measure the semantic alignment between CLIP image and text features across the target datasets, correlating this with attack effectiveness