---
ver: rpa2
title: Leveraging 2D Information for Long-term Time Series Forecasting with Vanilla
  Transformers
arxiv_id: '2405.13810'
source_url: https://arxiv.org/abs/2405.13810
tags:
- time
- series
- attention
- data
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of capturing both temporal and
  covariate information in long-term time series forecasting using vanilla Transformers.
  The proposed GridTST model treats time series data as a 2D grid, where time steps
  form the x-axis and variates form the y-axis.
---

# Leveraging 2D Information for Long-term Time Series Forecasting with Vanilla Transformers

## Quick Facts
- **arXiv ID**: 2405.13810
- **Source URL**: https://arxiv.org/abs/2405.13810
- **Reference count**: 40
- **Primary result**: GridTST achieves state-of-the-art performance in long-term time series forecasting by treating data as 2D grid with dual attention mechanisms

## Executive Summary
This paper addresses the challenge of capturing both temporal and covariate information in long-term time series forecasting using vanilla Transformers. The proposed GridTST model treats time series data as a 2D grid, where time steps form the x-axis and variates form the y-axis. It employs a combination of horizontal attention (focusing on time tokens) and vertical attention (focusing on variate tokens) to model cross-time and cross-variate dependencies simultaneously. The model also incorporates patching techniques to handle longer sequences efficiently. GridTST consistently achieves state-of-the-art performance across various real-world datasets, demonstrating its effectiveness in capturing complex temporal and multivariate patterns for accurate long-term forecasting.

## Method Summary
GridTST treats multivariate time series as a 2D grid where the x-axis represents time steps and the y-axis represents variates. The model applies a dual slicing strategy: vertical slicing combines variates at each time step into time tokens, while horizontal slicing embeds individual series across all time steps into variate tokens. It uses horizontal attention to capture temporal dependencies within each variate and vertical attention to capture inter-variate dependencies at each time step. The model incorporates patching techniques to reduce computational complexity by grouping consecutive time steps into segments, reducing the sequence length from T to approximately T/S. The attention sequencing can be configured as channel-first, time-first, or alternating, with experiments showing alternating (vertical then horizontal) yields optimal performance.

## Key Results
- GridTST achieves state-of-the-art performance across seven real-world datasets including Weather, Traffic, Electricity, Illness, ETTh1, ETtm1, and Solar-Energy
- The model consistently outperforms existing methods on both short-term (24-60 steps) and long-term (96-720 steps) forecasting tasks
- GridTST demonstrates efficient handling of datasets with large numbers of variates, such as the Traffic dataset with 862 variates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GridTST's dual slicing strategy enables simultaneous modeling of temporal and variate relationships without losing either dimension's structure
- Mechanism: By treating input as 2D grid, separate attention mechanisms capture temporal dependencies (horizontal) and inter-variate dependencies (vertical) in unified representation
- Core assumption: 2D grid representation can effectively encode both relationships without excessive redundancy
- Evidence anchors: Abstract description of grid representation; section 2 introduction of multi-directional attentions

### Mechanism 2
- Claim: Patching technique reduces computational complexity while preserving local semantic information
- Mechanism: Grouping consecutive time steps into patches reduces sequence length from T to T/S, quadratically decreasing attention complexity
- Core assumption: Local semantic information can be effectively captured by aggregating consecutive time steps
- Evidence anchors: Section 3.2 description of PatchTST approach and complexity reduction

### Mechanism 3
- Claim: Alternating attention sequence captures complex multivariate correlations before modeling temporal dependencies
- Mechanism: Vertical attention first captures inter-variate relationships, then horizontal attention operates on enriched representation
- Core assumption: Capturing variable relationships before temporal modeling provides better foundation
- Evidence anchors: Section 3.2 experimental results showing vertical-then-horizontal yields best performance

## Foundational Learning

- **Attention mechanism and multi-head attention**: Understanding how attention weights are computed and how multiple heads capture different data aspects is crucial for understanding GridTST's dual attention approach
  - Quick check: How does the scaled dot-product attention formula work, and what role does the scaling factor √dk play?

- **Transformer architecture components**: Understanding encoder structure, position encoding, and residual connections is essential as GridTST builds on vanilla Transformer
  - Quick check: What are the three main components of each Transformer layer, and how do they interact through residual connections?

- **Time series characteristics and forecasting**: Understanding unique properties of time series data (temporal dependencies, multivariate relationships, forecasting objectives) is necessary to appreciate why GridTST's approach is beneficial
  - Quick check: What are the key differences between time series forecasting and other sequence prediction tasks like language modeling?

## Architecture Onboarding

- **Component map**: Input [batch_size, num_variates, num_patches, d_model] → Patching layer → Horizontal/Vertical Attention → FFN → Output
- **Critical path**: Input → Patching → Horizontal/Vertical Attention → FFN → Output
- **Design tradeoffs**:
  - Patch size vs. temporal resolution: Larger patches reduce computation but may lose fine-grained patterns
  - Attention sequence order: Different datasets may benefit from different attention orderings
  - Memory vs. performance: Sampling strategies can reduce memory usage at potential cost to accuracy
- **Failure signatures**:
  - Poor performance on datasets with strong univariate patterns: May indicate vertical attention is not adding value
  - Memory errors with many variates: Suggests need for variate sampling strategy
  - Degraded performance with long lookback windows: May indicate attention dilution
- **First 3 experiments**:
  1. Compare GridTST with and without patching on dataset with long sequences to verify computational benefits
  2. Test different attention sequencing orders (channel-first, time-first, alternate) on validation set to determine optimal configuration
  3. Evaluate performance with different patch sizes (P) and strides (S) to find optimal tradeoff between efficiency and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GridTST's performance compare to state-of-the-art models on datasets with extremely long time series or very large number of variates?
- Basis in paper: [explicit] Paper mentions GridTST can handle large number of variates efficiently and can leverage BetterTransformer for scalability with extremely long time series
- Why unresolved: Paper demonstrates strong performance but doesn't provide results for datasets with extremely long time series or very large number of variates
- What evidence would resolve it: Experiments on datasets with extremely long time series (millions of time steps) and very large number of variates (thousands of variables) comparing to state-of-the-art models

### Open Question 2
- Question: How does the choice of patch length (P) and stride (S) in patching technique affect GridTST's performance?
- Basis in paper: [inferred] Paper mentions using patching but doesn't provide detailed analysis of how patch length and stride affect performance
- Why unresolved: Patching is key component and choice of patch length and stride could significantly impact model's ability to capture local semantic information
- What evidence would resolve it: Experiments with different patch lengths and strides on various datasets analyzing impact on performance and computational efficiency

### Open Question 3
- Question: How does GridTST perform in real-world scenarios with noisy or incomplete data?
- Basis in paper: [inferred] Paper demonstrates strong performance on real-world datasets but doesn't explicitly address handling noisy or incomplete data
- Why unresolved: Real-world time series data often contains noise, missing values, or outliers which can affect forecasting model performance
- What evidence would resolve it: Experiments on real-world datasets with varying levels of noise and missing data comparing GridTST's performance to other models in handling data imperfections

## Limitations
- Hyperparameter specifications (patch size, stride, layers, heads) are not provided, requiring dataset-specific tuning
- Computational complexity remains quadratic in number of tokens, potentially limiting scalability to very long sequences
- Claims about alternating attention being universally optimal are empirically observed without theoretical justification
- Instance normalization impact across diverse datasets is mentioned but not rigorously evaluated

## Confidence

**High Confidence**: Core architectural contributions (dual slicing strategy, horizontal/vertical attention mechanisms, patching technique) are well-defined and mathematically coherent, with strong theoretical framework support.

**Medium Confidence**: Empirical results demonstrating state-of-the-art performance are promising but lack detailed hyperparameter specifications and have limited scope of direct comparisons with recent methods.

**Low Confidence**: Claims about scalability and efficiency improvements are not fully substantiated, with actual performance gains and memory savings not quantified or compared against alternative approaches.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Conduct systematic ablation study varying patch size P, stride S, number of layers, and number of attention heads across all seven datasets to identify optimal configurations and understand sensitivity to critical hyperparameters.

2. **Scalability Benchmarking**: Evaluate GridTST on synthetic datasets with progressively larger lookback windows (L = 336, 672, 1344, 2688) and number of variates (N = 10, 50, 100, 500) to empirically measure computational complexity, memory usage, and performance degradation, comparing against theoretical complexity analysis.

3. **Attention Mechanism Isolation**: Design controlled experiments where horizontal and vertical attention mechanisms are applied independently (rather than in combination) on datasets with known characteristics (strong univariate patterns vs. strong inter-variate correlations) to isolate and quantify individual contributions of each attention type to overall forecasting accuracy.