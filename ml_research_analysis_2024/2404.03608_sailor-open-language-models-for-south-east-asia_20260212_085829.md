---
ver: rpa2
title: 'Sailor: Open Language Models for South-East Asia'
arxiv_id: '2404.03608'
source_url: https://arxiv.org/abs/2404.03608
tags:
- data
- https
- language
- languages
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sailor, a family of open language models
  ranging from 0.5B to 7B parameters, tailored for South-East Asian languages. The
  models are continually pre-trained from Qwen1.5, accepting 200B to 400B tokens covering
  English, Chinese, Vietnamese, Thai, Indonesian, Malay, and Lao.
---

# Sailor: Open Language Models for South-East Asia

## Quick Facts
- arXiv ID: 2404.03608
- Source URL: https://arxiv.org/abs/2404.03608
- Reference count: 40
- Key outcome: Family of 0.5B-7B parameter models tailored for SEA languages, outperforming SeaLLM and Sea-Lion on benchmark tasks

## Executive Summary
This paper introduces Sailor, a family of open language models specifically designed for South-East Asian languages. The models range from 0.5B to 7B parameters and are continually pre-trained from Qwen1.5 using 200B-400B tokens covering English, Chinese, Vietnamese, Thai, Indonesian, Malay, and Lao. The work demonstrates strong performance across four benchmark tasks - commonsense reasoning, question answering, reading comprehension, and examination - showing that Sailor models outperform or match existing SEA-focused LLMs like SeaLLM and Sea-Lion.

## Method Summary
The Sailor models employ several key technical innovations for SEA language adaptation. BPE dropout is used to enhance subword segmentation robustness across diverse languages. Aggressive data cleaning and deduplication processes ensure high-quality training data. The team employs small proxy models to optimize the data mixture strategy, balancing between high-resource languages (English, Chinese) and low-resource SEA languages. The models accept 200B-400B tokens during continual pre-training, with careful attention to multilingual representation and cross-lingual transfer capabilities.

## Key Results
- Sailor models outperform SeaLLM and Sea-Lion on SEA language benchmarks
- Strong performance across commonsense reasoning, question answering, reading comprehension, and examination tasks
- Demonstrated effectiveness of BPE dropout for multilingual robustness

## Why This Works (Mechanism)
The success of Sailor models stems from their targeted approach to SEA language challenges. The BPE dropout technique prevents overfitting to specific tokenization patterns, crucial for languages with different morphological structures. The aggressive data cleaning ensures that low-resource languages aren't overwhelmed by high-resource language data during training. The proxy model optimization allows for efficient exploration of the large data mixture space, finding optimal balances between different language families and task domains.

## Foundational Learning
1. **Multilingual Tokenization**: BPE dropout prevents vocabulary collapse across languages by randomly dropping merge operations during training. Why needed: Different SEA languages have vastly different morphological structures. Quick check: Compare tokenization stability across languages with and without BPE dropout.

2. **Data Mixture Optimization**: Using proxy models to explore the data mixture space efficiently. Why needed: The combinatorial space of data mixtures across 7 languages and multiple domains is enormous. Quick check: Verify that proxy model recommendations improve downstream performance.

3. **Continual Pre-training Strategy**: Building on Qwen1.5 rather than training from scratch. Why needed: Leverages existing strong base model capabilities while adapting to SEA languages. Quick check: Compare performance against training from scratch with same total compute budget.

## Architecture Onboarding

**Component Map**: Qwen1.5 -> BPE Dropout -> Data Cleaning -> Proxy Optimization -> Continual Pre-training

**Critical Path**: Data preparation (cleaning + deduplication) → Proxy model optimization → Tokenization setup → Continual pre-training → Evaluation

**Design Tradeoffs**: The team chose continual pre-training over full training to leverage existing capabilities while focusing resources on SEA language adaptation. This trades some potential performance gains from custom architecture for faster development and lower compute costs.

**Failure Signatures**: 
- Poor performance on low-resource languages indicates insufficient data mixture
- Tokenization issues suggest BPE dropout parameters need adjustment
- Performance degradation on high-resource languages suggests over-adaptation to SEA languages

**First 3 Experiments**:
1. Ablation study: Compare performance with and without BPE dropout across all SEA languages
2. Data mixture sensitivity: Vary the proportion of SEA language data from 10% to 90% and measure performance impact
3. Cross-lingual transfer: Test English-trained models on SEA languages versus SEA-trained models on English

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Data quality and representativeness concerns, particularly balance between high-resource and low-resource SEA languages
- Benchmark coverage limited to established tasks without real-world deployment metrics
- Limited comparative analysis against broader multilingual models like BLOOM or GPT-3 variants

## Confidence

- **Technical Methodology (BPE dropout, data cleaning)**: High
- **Performance Claims vs. SeaLLM/Sea-Lion**: Medium
- **Generalizability to All SEA Languages**: Low

## Next Checks
1. Conduct ablation studies varying the proportion of SEA language data to quantify its impact on performance across different language families.

2. Test Sailor models on out-of-domain SEA language tasks (e.g., domain-specific terminology, regional dialects) not covered in current benchmarks.

3. Perform human evaluation studies with native SEA language speakers to assess model outputs for cultural appropriateness and linguistic accuracy.