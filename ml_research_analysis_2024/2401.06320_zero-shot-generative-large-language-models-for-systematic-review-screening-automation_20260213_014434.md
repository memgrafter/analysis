---
ver: rpa2
title: Zero-shot Generative Large Language Models for Systematic Review Screening
  Automation
arxiv_id: '2401.06320'
source_url: https://arxiv.org/abs/2401.06320
tags:
- systematic
- review
- screening
- recall
- reviews
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the use of zero-shot generative large language
  models (LLMs) for automating systematic review screening, where candidate documents
  are assessed for inclusion. The study investigates eight different LLMs, including
  instruction fine-tuned variants, and introduces a calibration technique to achieve
  a targeted recall.
---

# Zero-shot Generative Large Language Models for Systematic Review Screening Automation

## Quick Facts
- arXiv ID: 2401.06320
- Source URL: https://arxiv.org/abs/2401.06320
- Authors: Shuai Wang; Harrisen Scells; Shengyao Zhuang; Martin Potthast; Bevan Koopman; Guido Zuccon
- Reference count: 40
- One-line primary result: Calibrated zero-shot LLMs achieve 95% recall with work saved score of 0.500

## Executive Summary
This paper evaluates the use of zero-shot generative large language models (LLMs) for automating systematic review screening, where candidate documents are assessed for inclusion. The study investigates eight different LLMs, including instruction fine-tuned variants, and introduces a calibration technique to achieve a targeted recall. The primary results show that instruction fine-tuning improves screening effectiveness, and calibration renders LLMs practical for achieving a predefined recall threshold. The calibrated setting with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches, with the best model achieving a balanced accuracy of 0.729 and a work saved score of 0.500.

## Method Summary
The paper evaluates eight zero-shot generative LLMs (including Llama, Alpaca, Guanaco, Falcon, and Llama2 variants) for systematic review screening using two approaches: uncalibrated (direct yes/no token probability comparison) and calibrated (normalized score with threshold θ). The models are tested on five standard test collections including CLEF TAR datasets and a Seed Collection dataset. Performance is measured using balanced accuracy, F3 score, precision, recall, success rate, and work saved score. An ensemble method combining the two most effective LLMs with a BioBERT baseline using CombSUM fusion is also evaluated.

## Key Results
- Instruction fine-tuning consistently improves screening effectiveness across all models
- Calibration enables LLMs to achieve ≥95% recall on 95% of topics with work saved score of 0.500
- Ensembled zero-shot LLMs outperform individual models and fine-tuned BioBERT baseline
- LLaMa2-7b-ins achieves highest balanced accuracy (0.729) when calibrated
- Zero-shot LLMs save significant screening time compared to state-of-the-art approaches

## Why This Works (Mechanism)

### Mechanism 1: Token Likelihood Calibration
Calibration adjusts the decision threshold to achieve a predefined recall target, making the LLM-based screening practical for real-world systematic reviews. The model computes a score as the difference between the likelihood of "yes" and "no" tokens, which is normalized across all documents for a topic and compared against a threshold θ that is set to ensure a minimum recall rate (e.g., 0.95). Core assumption: The token likelihood difference correlates well with the actual relevance of a document to the systematic review topic.

### Mechanism 2: Instruction-Based Fine-Tuning
Instruction-based fine-tuning improves screening effectiveness compared to base LLMs. Models like Alpaca, Guanaco, and LLaMa2-instruct are fine-tuned with instructional data to follow specific tasks better, aligning the model's internal representations with the inclusion/exclusion decision logic needed for screening. Core assumption: The instructional fine-tuning corpus covers the kind of decision logic required for systematic review screening.

### Mechanism 3: Model Ensembling
Ensembling multiple zero-shot LLMs with a BioBERT baseline improves balanced accuracy and work saved score compared to individual models. CombSUM fusion aggregates the normalized scores from multiple models, reducing variance and exploiting complementary strengths of different architectures and parameter sizes. Core assumption: Different models make uncorrelated errors, so combining them yields better overall performance.

## Foundational Learning

- **Token likelihood normalization**
  - Why needed here: Without normalization, raw differences in token probabilities are not comparable across documents due to varying text lengths and vocabularies.
  - Quick check question: How would you normalize scores if one document has P(yes)=0.9 and another has P(yes)=0.2, assuming min=0.1 and max=0.9?

- **Recall vs. precision trade-off**
  - Why needed here: Systematic reviews require high recall to avoid missing relevant studies, even at the cost of lower precision.
  - Quick check question: If a model has recall=0.98 but precision=0.1, is it acceptable for a systematic review?

- **Ensemble methods and CombSUM**
  - Why needed here: Combining multiple models reduces individual model bias and improves overall performance.
  - Quick check question: What happens to ensemble performance if all models make the same error on a subset of documents?

## Architecture Onboarding

- **Component map**: LLM prompt engine → token probability extractor → score calculator → normalizer → threshold comparator → decision output. Calibration component computes θ from training or seed data.
- **Critical path**: Prompt generation → LLM inference → token likelihood extraction → normalization → threshold check → include/exclude decision.
- **Design tradeoffs**: Larger models (13B) may have higher recall but lower work saved score; smaller models (7B) may be more efficient but less accurate. Calibration adds preprocessing but ensures recall targets.
- **Failure signatures**: If recall target is not met, check normalization step or threshold setting; if precision is too low, consider adjusting calibration threshold or model ensembling.
- **First 3 experiments**:
  1. Compare uncalibrated vs. calibrated LLaMa2-7b-ins on a small CLEF dataset to observe recall improvement.
  2. Test different θ values (0.95 vs. 1.0) to see impact on success rate and WSS.
  3. Ensemble LLaMa2-7b-ins with BioBERT and measure B-AC improvement over individual models.

## Open Questions the Paper Calls Out

- **Prompt formulation effects**: How do different prompt formulations affect the effectiveness of zero-shot LLM screening? The paper mentions that "generative LLMs are sensitive to prompt formulation" and conducted preliminary investigations, but did not deeply discuss the effects of alternative prompt formulations due to page constraints.

- **Fine-tuning with domain-specific data**: What is the impact of fine-tuning generative LLMs using domain-specific data for systematic review screening? The paper compared zero-shot LLMs to a fine-tuned model (Bio-SIEVE) but did not explore fine-tuning generative LLMs themselves, only briefly mentioning it as a potential avenue for future research.

- **Seed study quality and quantity**: How does the quality and quantity of seed studies affect the calibration of LLMs for systematic review screening? The paper introduced a calibration method using seed studies but noted that "LlaMa2-13b-ins displays more volatile effectiveness in this setting, possibly due to the varying quality and quantity of seed documents across different topics."

## Limitations

- Calibration mechanism's effectiveness relies on unstated correlation between token likelihood differences and document relevance, lacking direct corpus evidence.
- Instruction fine-tuning claims depend on unstated details about the instructional data quality and coverage for medical systematic review topics.
- Study uses only five test collections from CLEF TAR, limiting generalizability to other systematic review domains.

## Confidence

- **High Confidence**: The claim that calibrated zero-shot LLMs can achieve ≥95% recall with reasonable work saved scores (WSS), based on demonstrated success rates and calibrated model performance metrics.
- **Medium Confidence**: The assertion that instruction fine-tuning consistently improves screening effectiveness, supported by reported performance comparisons but lacking detailed corpus evidence of the fine-tuning data's relevance to systematic review tasks.
- **Medium Confidence**: The claim that ensembling improves balanced accuracy and WSS, based on reported outcomes but without corpus evidence showing error correlation between models or the effectiveness of CombSUM fusion in this specific context.

## Next Checks

1. **Validate Token Likelihood Correlation**: Extract token likelihood differences from a subset of CLEF TAR documents and statistically test their correlation with actual relevance labels to confirm the calibration mechanism's foundational assumption.

2. **Analyze Error Correlation in Ensembles**: Compute error correlation matrices between the eight LLMs on a validation set to verify that models make sufficiently uncorrelated errors, justifying the claimed ensemble benefits.

3. **Test Generalizability Beyond CLEF TAR**: Apply the calibrated zero-shot LLM approach to at least two systematic review datasets from different domains (e.g., clinical trials, social sciences) to assess whether the reported performance metrics (B-AC ~0.73, WSS ~0.50) generalize beyond the CLEF TAR collections.