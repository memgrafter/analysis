---
ver: rpa2
title: 'Federated Distillation: A Survey'
arxiv_id: '2404.08564'
source_url: https://arxiv.org/abs/2404.08564
tags:
- data
- knowledge
- local
- learning
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive review of federated distillation
  (FD), which integrates knowledge distillation into federated learning to address
  challenges such as high communication costs, model heterogeneity, and privacy concerns.
  FD enables flexible knowledge transfer by exchanging soft labels instead of model
  parameters, reducing communication overhead and allowing heterogeneous model architectures.
---

# Federated Distillation: A Survey

## Quick Facts
- arXiv ID: 2404.08564
- Source URL: https://arxiv.org/abs/2404.08564
- Authors: Lin Li; Jianping Gou; Baosheng Yu; Lan Du; Zhang Yiand Dacheng Tao
- Reference count: 40
- Key outcome: This survey provides a comprehensive review of federated distillation (FD), which integrates knowledge distillation into federated learning to address challenges such as high communication costs, model heterogeneity, and privacy concerns. FD enables flexible knowledge transfer by exchanging soft labels instead of model parameters, reducing communication overhead and allowing heterogeneous model architectures.

## Executive Summary
Federated distillation (FD) is an emerging paradigm that integrates knowledge distillation into federated learning to overcome key limitations of traditional federated learning approaches. By replacing model parameter exchange with logits exchange, FD significantly reduces communication costs while enabling heterogeneous model architectures across clients. The survey systematically categorizes FD approaches based on their strategies for addressing heterogeneity, client drift, catastrophic forgetting, communication costs, and privacy concerns, while also exploring applications across various domains including computer vision, natural language processing, and healthcare.

## Method Summary
Federated distillation works by having clients train local models on private data and exchange soft labels (logits) instead of model parameters. Clients download aggregated logits from the server, perform knowledge distillation using public data, fine-tune with private data, and upload new logits. The server aggregates these logits to create the next round of aggregated knowledge. This approach enables communication-efficient training with heterogeneous model architectures while preserving privacy through reduced information leakage compared to parameter sharing.

## Key Results
- FD reduces communication costs by exchanging logits instead of model parameters, with logits having smaller dimensions than full model weights
- FD enables heterogeneous model architectures by eliminating the need for identical models across clients through logits-based knowledge transfer
- FD enhances privacy protection by limiting information leakage compared to conventional parameter-based federated learning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation reduces communication costs in federated learning by replacing model parameter exchange with logits exchange
- Mechanism: The paper states "distillation-based FL simply requires the output of the local model prediction function, which generally has a smaller dimension compared to the model parameters"
- Core assumption: Logits contain sufficient information for effective model training without needing full parameter exchange
- Evidence anchors: [abstract] "FD enables more flexible knowledge transfer between clients and the server, surpassing the mere sharing of model parameters"
- Break condition: If logits are insufficient for model convergence or if adversarial attacks exploit logits information

### Mechanism 2
- Claim: KD enables heterogeneous model architectures in federated learning by transferring logits instead of parameters
- Mechanism: The paper explains "FD enables more flexible knowledge transfer between clients and the server, surpassing the mere sharing of model parameters. By eliminating the need for identical model architectures across clients and the server"
- Core assumption: Logits from different architectures can be meaningfully aggregated and used for training
- Evidence anchors: [abstract] "By eliminating the need for identical model architectures across clients and the server, FD mitigates the communication costs associated with training large-scale models"
- Break condition: If model architecture differences are too large for logits to bridge, or if aggregation fails to preserve model quality

### Mechanism 3
- Claim: KD improves privacy protection by reducing information leakage compared to parameter sharing
- Mechanism: The paper notes "Unlike conventional FL, which communicates via model parameters and may inadvertently reveal original data, FD's use of logits information makes compromising private data more challenging"
- Core assumption: Logits contain less sensitive information than model parameters
- Evidence anchors: [abstract] "Moreover, FD enhances model robustness. Unlike conventional FL, which communicates via model parameters and may inadvertently reveal original data, FD's use of logits information makes compromising private data more challenging"
- Break condition: If advanced attacks can reconstruct private data from logits, or if differential privacy needs compromise performance

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Understanding how teacher-student model knowledge transfer works is fundamental to grasping FD mechanisms
  - Quick check question: How does temperature scaling in softmax affect knowledge transfer quality in KD?

- Concept: Federated Learning Basics
  - Why needed here: Understanding client-server communication, aggregation, and challenges is essential for FD context
  - Quick check question: What are the main communication bottlenecks in traditional FL that FD aims to address?

- Concept: Model Heterogeneity
  - Why needed here: Understanding why different clients might need different model architectures is crucial for FD design
  - Quick check question: What are the main sources of model heterogeneity in real-world FL deployments?

## Architecture Onboarding

- Component map: Clients -> Public dataset -> Server -> Clients
- Critical path: 1. Clients download aggregated logits from server, 2. Clients perform knowledge distillation using public data, 3. Clients fine-tune with private data, 4. Clients upload new logits to server, 5. Server aggregates and repeats
- Design tradeoffs: Logits dimension vs. information richness, Public dataset quality vs. privacy protection, Model heterogeneity support vs. aggregation complexity, Communication frequency vs. convergence speed
- Failure signatures: Model divergence despite aggregation, Privacy leakage through logits analysis, Communication overhead remains high, Heterogeneous models fail to converge
- First 3 experiments: 1. Compare convergence speed of FD vs traditional FL on homogeneous models, 2. Test privacy leakage by attempting to reconstruct data from logits, 3. Evaluate model performance when clients use different architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can federated distillation be effectively applied to object detection, semantic segmentation, and video analysis tasks that involve small datasets, where current research is limited?
- Basis in paper: [inferred] The paper mentions that current FD applications primarily focus on image classification tasks and highlights the need to explore FD's applicability in different scenarios like object detection, image tracking, and anomaly detection
- Why unresolved: The paper acknowledges that the application scope of FD is limited to classification tasks and calls for further exploration in other domains, but does not provide specific solutions or evidence for these applications
- What evidence would resolve it: Empirical studies demonstrating successful application of FD to object detection, semantic segmentation, and video analysis tasks, along with quantitative performance comparisons to existing methods

### Open Question 2
- Question: What are the most effective strategies for mitigating catastrophic forgetting in federated distillation, particularly when dealing with complex data distributions and diverse client populations?
- Basis in paper: [explicit] The paper discusses various approaches to address catastrophic forgetting in FD, including using historical data generators, historical teacher models, and increasing knowledge transfer during training
- Why unresolved: The paper acknowledges the effectiveness of different strategies but does not provide a definitive solution or compare their performance in various scenarios
- What evidence would resolve it: Comparative studies evaluating the effectiveness of different strategies for mitigating catastrophic forgetting in FD under various conditions, along with theoretical analysis of their strengths and weaknesses

### Open Question 3
- Question: How can federated distillation be designed to balance communication efficiency and prediction accuracy, particularly when dealing with large-scale models and resource-constrained edge devices?
- Basis in paper: [explicit] The paper discusses the trade-off between communication efficiency and prediction accuracy in FD, highlighting the need for lightweight models that minimize communication costs while maintaining accuracy
- Why unresolved: The paper acknowledges the importance of this trade-off but does not provide a definitive solution or evaluate the performance of different approaches in achieving this balance
- What evidence would resolve it: Empirical studies comparing the communication efficiency and prediction accuracy of different FD approaches, along with theoretical analysis of the trade-offs involved in designing communication-efficient FD models

## Limitations
- Limited empirical validation of communication reduction benefits in real-world scenarios
- Privacy enhancement claims lack rigorous testing against advanced attack methods
- Scalability to large-scale deployments with diverse client capabilities remains unproven

## Confidence
- High: The categorization framework for FD approaches and their applications is well-structured and logically consistent
- Medium: The theoretical benefits of communication reduction and heterogeneity support through KD
- Low: Privacy enhancement claims and the effectiveness of FD against real-world privacy attacks

## Next Checks
1. Conduct empirical studies comparing FD vs traditional FL on communication costs and model performance across multiple benchmark datasets and heterogeneous model architectures
2. Perform privacy attacks on FD systems to quantify actual privacy leakage compared to parameter-based FL
3. Evaluate FD convergence and stability under realistic non-IID data distributions and varying numbers of participating clients