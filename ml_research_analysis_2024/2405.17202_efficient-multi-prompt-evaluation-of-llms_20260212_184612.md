---
ver: rpa2
title: Efficient multi-prompt evaluation of LLMs
arxiv_id: '2405.17202'
source_url: https://arxiv.org/abs/2405.17202
tags:
- prompt
- templates
- performance
- llms
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently evaluating large
  language models (LLMs) across many prompt templates, which is important because
  LLMs can be sensitive to prompt variations. The proposed PromptEval method estimates
  the full performance distribution and quantiles across a large set of prompts using
  a small fraction of total evaluations.
---

# Efficient multi-prompt evaluation of LLMs

## Quick Facts
- arXiv ID: 2405.17202
- Source URL: https://arxiv.org/abs/2405.17202
- Authors: Felipe Maia Polo; Ronald Xu; Lucas Weber; Mírian Silva; Onkar Bhardwaj; Leshem Choshen; Allysson Flavio Melo de Oliveira; Yuekai Sun; Mikhail Yurochkin
- Reference count: 40
- Key outcome: PromptEval estimates performance distribution and quantiles across 100+ prompt templates using budget equivalent to 2-4 single-prompt evaluations

## Executive Summary
This paper addresses the challenge of efficiently evaluating large language models across many prompt templates, which is important because LLMs can be sensitive to prompt variations. The proposed PromptEval method uses an Item Response Theory-inspired model to estimate the full performance distribution and quantiles across a large set of prompts using only a small fraction of total evaluations. By borrowing strength across prompts and examples, PromptEval can accurately estimate performance quantiles with budgets equivalent to just 2-4 single-prompt evaluations on benchmarks like MMLU, BBH, and LMentry.

## Method Summary
PromptEval addresses the multi-prompt evaluation problem by treating it as an Item Response Theory (IRT) estimation task. The method fits a logistic regression model using prompt and example covariates to estimate latent performance parameters, then computes conditional expectations to estimate performance scores for each prompt. These estimates are aggregated to form the performance distribution and quantiles. The approach is theoretically grounded with proofs of consistency under conditions where the number of unseen examples grows sufficiently fast and the model is correctly specified.

## Key Results
- Accurately estimates performance quantiles across 100+ prompt templates on MMLU with budget equivalent to 2-4 single-prompt evaluations
- Outperforms baseline "avg" method in Wasserstein distance metrics for performance distribution estimation
- Improves performance in downstream applications like LLM-as-a-judge and best prompt identification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PromptEval accurately estimates performance quantiles across 100 prompt templates with budget equivalent to 2-4 single-prompt evaluations
- Core assumption: Correctness scores follow Bernoulli distribution parameterized by prompt/example-specific parameters that can be estimated accurately from small fraction of evaluations
- Evidence: Abstract states method works with budget equivalent to two single-prompt evaluations on MMLU
- Break condition: If relationship between prompts and performance is inconsistent across examples or evaluations are too sparse

### Mechanism 2
- Claim: PromptEval improves upon baselines in LLM-as-a-judge and best prompt identification applications
- Core assumption: Performance distribution across prompts is stable enough to estimate from limited data and useful for downstream applications
- Evidence: Figure 4 shows lower Wasserstein loss compared to baseline "avg" method
- Break condition: If performance distribution is highly unstable or applications require absolute rather than relative performance estimates

### Mechanism 3
- Claim: PromptEval consistently estimates performance distribution and quantiles as number of prompts and examples grows
- Core assumption: IRT model is correctly specified and maximum likelihood estimator converges to true parameters as prompts and examples increase
- Evidence: Theorem 4.4 proves consistency under specified conditions
- Break condition: If IRT model is misspecified or evaluations grow too slowly relative to prompts and examples

## Foundational Learning

- Concept: Item Response Theory (IRT)
  - Why needed here: PromptEval adapts IRT models (which assess individuals' latent abilities through standardized tests) to assess LLM performance across prompts
  - Quick check question: What is the key assumption of IRT models that makes them suitable for PromptEval?

- Concept: Logistic Regression
  - Why needed here: Core of PromptEval involves fitting logistic regression model using prompt and example covariates to estimate performance parameters
  - Quick check question: How does logistic regression relate to the Bernoulli model assumed for correctness scores in PromptEval?

- Concept: Wasserstein Distance
  - Why needed here: Used to measure difference between true and estimated performance distributions, providing metric for evaluation quality
  - Quick check question: Why is Wasserstein distance particularly suitable for comparing performance distributions in this context?

## Architecture Onboarding

- Component map:
  Data collection -> Model fitting -> Performance estimation -> Distribution estimation -> Quantile computation

- Critical path:
  1. Sample prompt-example pairs using balanced sampling strategy
  2. Collect correctness scores for sampled pairs
  3. Fit logistic regression model with prompt and example covariates
  4. Compute estimated performance Si for each prompt
  5. Aggregate Si estimates to form performance distribution
  6. Compute quantiles from the distribution

- Design tradeoffs:
  - One-hot encoding vs. learned embeddings for prompts: One-hot is simpler but doesn't capture similarities; embeddings capture similarities but require more data
  - Linear vs. neural network functions for fψ and gγ: Linear is simpler and interpretable but may miss complex relationships; neural networks can capture complex relationships but require more data and are less interpretable

- Failure signatures:
  - High variance in estimated performance across different random seeds
  - Poor fit of logistic regression model (low R² or high cross-entropy loss)
  - Estimated quantiles that deviate significantly from empirical quantiles
  - Performance estimates inconsistent across different subsets of data

- First 3 experiments:
  1. Compare PromptEval with baseline "avg" method on small dataset with known ground truth to verify basic functionality
  2. Test PromptEval on different numbers of prompt templates (10, 50, 100) to understand scaling behavior
  3. Evaluate PromptEval's performance on different types of covariates (one-hot, embeddings, discrete features) to understand impact on accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PromptEval perform when number of prompt templates is small versus large?
- Basis in paper: The paper discusses benefits of using multiple prompts and trade-offs in selecting prompt sets
- Why unresolved: Paper doesn't provide detailed analysis of performance variation with number of templates
- What evidence would resolve it: Experiments measuring PromptEval performance with varying numbers of prompt templates (10, 50, 100, 200) compared to baseline method

### Open Question 2
- Question: What is optimal budget allocation strategy for sampling evaluations across prompt templates and examples?
- Basis in paper: Paper mentions sampling strategy affects stability of estimates and introduces Algorithm 2 for balanced sampling
- Why unresolved: Paper doesn't explore different sampling strategies or provide theoretical justification for balanced sampling approach
- What evidence would resolve it: Experiments comparing different sampling strategies (random, stratified, active learning) and analyzing impact on accuracy and efficiency

### Open Question 3
- Question: How does PromptEval handle prompt templates that are semantically similar or contain overlapping information?
- Basis in paper: Paper doesn't address issue of prompt similarity or redundancy
- Why unresolved: Paper assumes prompt templates are independent and doesn't discuss handling similar templates
- What evidence would resolve it: Analysis of impact of prompt similarity on PromptEval performance and methods to handle redundant or similar templates

## Limitations
- Effectiveness depends heavily on quality and informativeness of covariates used for prompts and examples
- Theoretical guarantees rely on assumptions about model specification and data growth that may not always hold in practice
- Method assumes correctness scores follow Bernoulli distribution, which may not hold for all LLM tasks or evaluation settings

## Confidence

- **High Confidence:** Empirical demonstration that PromptEval can estimate performance quantiles with budgets equivalent to 2-4 single-prompt evaluations (directly validated on three benchmarks)
- **Medium Confidence:** Theoretical consistency proofs (rely on assumptions about model specification and data growth)
- **Medium Confidence:** Improvement over baselines in downstream applications like LLM-as-a-judge (provides evidence but doesn't extensively explore edge cases)

## Next Checks

1. **Robustness to Covariate Choice:** Systematically test PromptEval with different types of covariates (one-hot, embeddings, discrete features) across three benchmarks to understand sensitivity to covariate selection and whether EmbFT consistently outperforms alternatives

2. **Scaling Behavior Investigation:** Evaluate PromptEval's performance as number of prompts increases from 10 to 200+ to identify point where accuracy degrades significantly and determine practical limits on number of prompts that can be efficiently evaluated

3. **Failure Mode Analysis:** Deliberately construct scenarios where IRT assumptions are violated (e.g., prompts with highly non-linear performance relationships) and test whether PromptEval produces reasonable estimates or fails predictably, helping identify when method should not be applied