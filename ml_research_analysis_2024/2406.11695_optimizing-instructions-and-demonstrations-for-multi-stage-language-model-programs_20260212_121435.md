---
ver: rpa2
title: Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs
arxiv_id: '2406.11695'
source_url: https://arxiv.org/abs/2406.11695
tags:
- program
- given
- score
- dspy
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes the problem of optimizing prompts in multi-stage
  Language Model Programs, addressing the challenges of proposal and credit assignment.
  The authors introduce MIPRO, a novel optimizer that combines strategies for generating
  effective instructions and demonstrations, and uses Bayesian optimization for efficient
  credit assignment.
---

# Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs

## Quick Facts
- arXiv ID: 2406.11695
- Source URL: https://arxiv.org/abs/2406.11695
- Reference count: 40
- Primary result: MIPRO optimizer outperforms baseline optimizers on 5 of 7 tasks with up to 13% accuracy improvement

## Executive Summary
This paper addresses the challenge of optimizing prompts in multi-stage Language Model Programs, where individual module performance is latent and only overall task metrics are available. The authors introduce MIPRO, a novel optimizer that combines strategies for generating effective instructions and demonstrations, and uses Bayesian optimization for efficient credit assignment. Evaluated on seven diverse tasks, MIPRO demonstrates significant performance improvements over baseline optimizers, highlighting the importance of jointly optimizing both instructions and few-shot demonstrations. The study provides a benchmark and insights for practitioners seeking to optimize LM programs.

## Method Summary
MIPRO formalizes LM program optimization as a discrete search over instruction and demonstration parameters. The method employs three key strategies: bootstrapping demonstrations by sampling successful traces from training data, using a grounding approach to provide context for instruction proposal, and applying Bayesian optimization with a surrogate model to efficiently assign credit across modules. The optimizer iteratively proposes parameter combinations, evaluates them on minibatches, and updates the model to focus search on promising regions. MIPRO is evaluated against baseline optimizers on seven diverse tasks including HotPotQA, Iris, and HoVer, demonstrating significant performance improvements.

## Key Results
- MIPRO outperforms baseline optimizers on five of seven tasks, achieving up to 13% accuracy improvement
- Optimizing bootstrapped demonstrations alone yields significantly better performance than optimizing instructions alone
- Optimizing both instructions and few-shot examples together generally leads to the best overall performance
- Optimizing instructions becomes more essential for tasks with conditional rules that are not immediately obvious to the LM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing bootstrapped demonstrations is key to achieving the best performance across most tasks.
- Mechanism: Bootstrap Demonstrations strategy samples inputs from training data, runs them through the program, and keeps traces that produce sufficiently high-scoring outputs. These successful traces are treated as labeled demonstrations (input/output examples) for each module. The optimization problem then reduces to selecting combinations of demonstrations that serve as effective few-shot examples for prompting.
- Core assumption: Traces that produce high-scoring outputs are correct and contain valuable information about successful reasoning behavior.
- Evidence anchors:
  - [abstract]: "optimizing bootstrapped few-shot examples is often essential for realizing the greatest performance gains"
  - [section]: "we find that optimizing bootstrapped demonstrations alone yields significantly better performance than optimizing instructions alone"
  - [corpus]: Weak evidence - the corpus contains related work on prompt optimization but no direct comparison of demonstration vs instruction optimization

### Mechanism 2
- Claim: Jointly optimizing both instructions and few-shot demonstrations using MIPRO is the most effective approach in five out of seven settings.
- Mechanism: MIPRO uses a Bayesian surrogate model to explicitly learn the sensitivity of task-level scores to module-level parameters (instructions and demonstrations). It proposes combinations of instructions and demonstrations, evaluates them on minibatches, and updates the model to focus search on promising regions.
- Core assumption: A Bayesian surrogate model can effectively capture the joint contributions between parameter choices and guide efficient search.
- Evidence anchors:
  - [abstract]: "MIPRO outperforms baseline optimizers on five of seven tasks... by as high as 13% accuracy improvement"
  - [section]: "optimizing both instructions and few-shot examples together generally leads to the best overall performance"
  - [corpus]: Weak evidence - corpus contains related Bayesian optimization work but no direct comparison to MIPRO's approach

### Mechanism 3
- Claim: Instruction optimization is most important for tasks with conditional rules that are not immediately obvious to the LM and not expressible via a few examples.
- Mechanism: Grounding strategy provides the proposal LM with relevant context (data patterns, program control flow, demonstrations, previously evaluated prompts) to craft instructions better suited to the task. This is particularly valuable when rules cannot be inferred from examples alone.
- Core assumption: Providing context about task structure and requirements enables the proposal LM to generate more effective instructions than example-only approaches.
- Evidence anchors:
  - [abstract]: "optimizing instructions becomes more essential for tasks with conditional rules"
  - [section]: "optimizing free-form instructions can have the most impact on tasks with subtle rules that cannot be properly inferred through a few examples"
  - [corpus]: Weak evidence - corpus contains work on instruction tuning but no direct comparison of instruction vs example effectiveness for conditional tasks

## Foundational Learning

- Concept: Language Model Programs and their optimization challenges
  - Why needed here: The paper studies prompt optimization for multi-stage LM programs where individual module performance is latent and only overall task metrics are available
  - Quick check question: Why is credit assignment across modules particularly challenging in LM program optimization compared to single-module prompting?

- Concept: Bayesian optimization and surrogate models
  - Why needed here: MIPRO uses a Bayesian surrogate model to learn which parameter combinations are most promising, allowing efficient search over discrete instruction and demonstration candidates
  - Quick check question: How does the Tree Structured Parzen Estimator (TPE) sampling rule differ from random search in exploring the parameter space?

- Concept: Bootstrap demonstrations and rejection sampling
  - Why needed here: The Bootstrap Demonstrations strategy generates effective few-shot examples by running training inputs through the program and keeping successful traces as demonstrations
  - Quick check question: What threshold determines whether a trace's demonstrations are considered "correct" and kept for optimization?

## Architecture Onboarding

- Component map: Initialize -> Propose -> Update -> ExtractOptimizedSets
- Critical path: Initialize (bootstrap demonstrations + propose instructions) → Propose (Bayesian sampling of parameter combinations) → Update (evaluate on minibatch, update surrogate model) → ExtractOptimizedSets (select top-performing configuration)
- Design tradeoffs: Joint optimization of instructions and demonstrations vs separate optimization; full evaluation vs minibatch evaluation; LLM-based credit assignment vs Bayesian surrogate model
- Failure signatures: Poor performance due to (1) bootstrap demonstrations reinforcing errors, (2) proposal LLM generating ineffective instructions despite grounding, (3) surrogate model failing to capture true parameter sensitivities
- First 3 experiments:
  1. Run MIPRO on HotPotQA with default parameters to verify basic functionality and establish baseline performance
  2. Compare MIPRO with Bootstrap Random Search on Iris to isolate the value of instruction optimization
  3. Test Grounding ablation on HoVer to measure the impact of context provision on instruction quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do optimization dynamics differ across extremely low or high budget scenarios for LM program optimizers?
- Basis in paper: Inferred from Limitations section stating "This work studies a set of optimizers under a fixed budget, but does not examine how optimization dynamics might differ across extremely low or high budget scenarios."
- Why unresolved: The paper focuses on evaluating optimizers under fixed budgets without exploring how performance changes at different budget levels.
- What evidence would resolve it: Experimental results comparing optimizer performance across varying budget levels (e.g., 5, 50, 500 trials) to identify budget-specific trade-offs and optimal strategies.

### Open Question 2
- Question: How do different proposer LM models affect the performance of LM program optimizers?
- Basis in paper: Inferred from Limitations section noting "In our experiments, we also use a fixed proposer LM and task LM. Future research should assess whether the proposed methods demonstrate consistent performance when employing different models."
- Why unresolved: The study uses specific models (GPT-3.5 for proposer, Llama-3-8B for task) without exploring model variations.
- What evidence would resolve it: Systematic comparison of optimizer performance using different combinations of proposer and task models across various tasks.

### Open Question 3
- Question: Can LM program optimizers learn complex task rules without handwritten seed prompts?
- Basis in paper: Inferred from Limitations section stating "a limitation of the optimizers introduced in this work is their restricted ability to infer the rules governing complex tasks without a handwritten seed prompt."
- Why unresolved: Current optimizers rely on initial prompts or demonstrations to understand task structure.
- What evidence would resolve it: Development and evaluation of optimizers that can infer task rules from dataset patterns alone, without any initial task specification.

## Limitations

- Limited Evidence on Module-Level Performance: While MIPRO shows strong task-level performance improvements, the paper does not provide detailed analysis of how module-level optimization affects overall performance.
- Ablation Study Limitations: The paper reports that optimizing both instructions and demonstrations together yields the best results on five of seven tasks, but the ablation studies only show average performance across tasks rather than task-specific breakdowns.
- Task Selection Bias: The seven benchmark tasks may not represent the full diversity of multi-stage LM program challenges, with simpler classification tasks potentially skewing results.

## Confidence

**High Confidence**: The core claim that MIPRO outperforms baseline optimizers on five of seven tasks is directly supported by experimental results in the paper, with specific accuracy improvements up to 13%.

**Medium Confidence**: The claim that optimizing bootstrapped demonstrations is key to achieving best performance is supported by results but lacks detailed analysis of demonstration quality and module-level effects.

**Medium Confidence**: The assertion that instruction optimization is most important for tasks with conditional rules is supported by task-level results but lacks direct evidence about instruction effectiveness versus demonstration effectiveness for specific rule types.

## Next Checks

1. **Module-Level Analysis**: Implement module-level evaluation metrics to verify that bootstrapped demonstrations actually represent correct reasoning behavior, not just metric-inflating outputs. This would validate the core assumption that high-scoring traces contain valuable reasoning information.

2. **Task-Specific Ablations**: Conduct detailed ablation studies for each individual task, comparing the performance of instruction-only optimization, demonstration-only optimization, and joint optimization. This would provide clearer evidence for the relative importance of each component across different task types.

3. **Complexity Scaling Test**: Evaluate MIPRO on increasingly complex multi-stage LM programs with more than seven modules to test whether the Bayesian surrogate model can effectively capture parameter sensitivities in larger search spaces, validating the assumption that the approach scales to real-world complexity.