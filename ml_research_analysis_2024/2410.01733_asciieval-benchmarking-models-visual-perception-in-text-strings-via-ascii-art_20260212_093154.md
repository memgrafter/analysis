---
ver: rpa2
title: 'ASCIIEval: Benchmarking Models'' Visual Perception in Text Strings via ASCII
  Art'
arxiv_id: '2410.01733'
source_url: https://arxiv.org/abs/2410.01733
tags:
- ascii
- visual
- performance
- data
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ASCIIEval, a novel benchmark designed to
  evaluate both Large Language Models (LLMs) and Multimodal Large Language Models
  (MLLMs) on their ability to perceive visual semantics embedded in text strings through
  ASCII art. The benchmark includes over 3K samples organized into a detailed 3-layer
  classification tree, along with a 10K-sample training set.
---

# ASCIIEval: Benchmarking Models' Visual Perception in Text Strings via ASCII Art

## Quick Facts
- arXiv ID: 2410.01733
- Source URL: https://arxiv.org/abs/2410.01733
- Reference count: 40
- Key outcome: Proprietary models outperform open-source counterparts by up to 20.01% in ASCII art recognition, with MLLMs showing trade-offs between text recognition and visual perception

## Executive Summary
This paper introduces ASCIIEval, a novel benchmark designed to evaluate both Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) on their ability to perceive visual semantics embedded in text strings through ASCII art. The benchmark includes over 3K samples organized into a detailed 3-layer classification tree, along with a 10K-sample training set. Experiments across 50+ models reveal that proprietary models (e.g., GPT-5) outperform open-source counterparts by significant margins (up to 20.01%) in image-based recognition tasks. For text-only inputs, open-source LLMs lag behind proprietary models, though some (e.g., Gemma-3-27B) show strong scaling trends. A key finding is that open-source MLLMs trade off fine-grained text recognition for collective visual perception, leading to performance degradation. Models also struggle with cross-modal synergy, performing worse when both text and image inputs are provided. The paper proposes rationale-assisted fine-tuning for LLMs and low-resolution prompting with vision backbone-focused fine-tuning for MLLMs to improve performance. The benchmark highlights the need for models to balance text and visual recognition and to develop more effective cross-modal fusion techniques.

## Method Summary
The paper constructs ASCIIEval benchmark with 3,526 samples across 359 concepts, plus ASCIITune training set with 11,836 samples. Models are evaluated across three input modalities (text-only, image-only, text-image) using zero-shot evaluation with standardized prompts. Performance is measured using macro-accuracy and micro-accuracy for multiple-choice question answering. The study evaluates 50+ models including various LLMs (Llama, Qwen, Gemma, DeepSeek) and MLLMs, comparing proprietary and open-source models. Enhancement strategies include rationale-assisted fine-tuning for LLMs and low-resolution prompting with supervised fine-tuning for MLLMs.

## Key Results
- Proprietary models (GPT-5) outperform open-source models by up to 20.01% in ASCII art recognition tasks
- Open-source LLMs lag behind proprietary models in text-only settings, though some show strong scaling trends
- MLLMs exhibit trade-off between fine-grained text recognition and collective visual perception, leading to performance degradation
- Models perform worse when both text and image inputs are provided simultaneously, indicating cross-modal synergy challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASCII art can be represented as both text and image modalities, enabling cross-modal evaluation of visual perception in models.
- Mechanism: The modality-agnostic nature of ASCII art allows testing whether models can recognize visual patterns embedded in text strings and whether multimodal models can align text and image representations of the same visual concept.
- Core assumption: Models trained on text data can learn to extract 2D visual structure from character arrangements, and multimodal models can integrate congruent information across modalities.
- Evidence anchors:
  - [abstract] "It depicts concepts through careful arrangement of characters, which can be formulated in both text and image modalities."
  - [section 1] "Visual information in these artifacts is situated in the middle of text strings and images, and can be readily expressed in both formats containing identical content."
  - [corpus] Weak evidence - related work focuses on ASCII art generation and jailbreak attacks but not systematic cross-modal evaluation of visual perception.
- Break condition: If models show no correlation between text-only and image-only performance, or if cross-modal input consistently degrades performance, the modality-agnostic assumption fails.

### Mechanism 2
- Claim: Large language models demonstrate visual perception ability through text strings, but performance is limited compared to proprietary models.
- Mechanism: LLMs can recognize visual patterns in ASCII art by associating character arrangements with concepts, though open-source models lag behind proprietary ones in this capability.
- Core assumption: Pre-training on large text corpora enables LLMs to capture 2D structures and visual semantics embedded in text.
- Evidence anchors:
  - [abstract] "Given textual input, language models shows their visual perception ability on ASCII art concepts."
  - [section 5.1] "All of the models in Fig 2(a) exceeds a random baseline (25%), confirming their fundamental competence on visual perception through text strings."
  - [corpus] Moderate evidence - related work on ASCII art understanding is limited, suggesting this is an underexplored capability.
- Break condition: If models perform at random chance levels or show no correlation with established benchmarks like TableEval and SGP-Bench, the visual perception assumption is invalid.

### Mechanism 3
- Claim: Multimodal models exhibit a trade-off between fine-grained text recognition and collective visual perception.
- Mechanism: MLLMs optimized for OCR and text extraction struggle with abstract visual patterns in ASCII art, while those focused on visual perception may miss fine textual details.
- Core assumption: Model architecture and training objectives create competing priorities between reading individual characters and perceiving overall visual structure.
- Evidence anchors:
  - [abstract] "For image inputs, we reveal that open-source MLLMs suffer from a trade-off between fine-grained text recognition and collective visual perception."
  - [section 6.1] "Our analysis indicates a negative correlation between ASCII art recognition and OCR-centric benchmarks."
  - [corpus] Weak evidence - no direct corpus support for this specific trade-off mechanism.
- Break condition: If models can simultaneously excel at both OCR and ASCII art recognition without performance degradation, the trade-off assumption fails.

## Foundational Learning

- Concept: Cross-modal learning and fusion
  - Why needed here: Understanding how models integrate information from different modalities is crucial for evaluating ASCIIEval's cross-modal synergy tests.
  - Quick check question: What architectural components enable effective cross-modal fusion, and how might they fail when modalities are congruent but processed separately?

- Concept: Visual perception in language models
  - Why needed here: Evaluating whether LLMs can extract visual semantics from text requires understanding how they process spatial and structural information.
  - Quick check question: How do transformer-based models capture 2D spatial relationships from 1D text sequences, and what limitations might exist?

- Concept: Benchmark design and evaluation metrics
  - Why needed here: Constructing and interpreting ASCIIEval requires understanding how to measure visual perception capabilities across different model types and input modalities.
  - Quick check question: What makes a good benchmark for visual perception in text, and how do you ensure tasks isolate the capability being tested?

## Architecture Onboarding

- Component map:
  - Online galleries → manual cleaning → 3-layer classification tree
  - Text-only, Image-only, Text-Image settings with consistent prompts
  - Macro-accuracy per concept, micro-accuracy per sample, correlation analysis
  - Rationale-assisted fine-tuning for LLMs, low-resolution prompting and supervised fine-tuning for MLLMs

- Critical path:
  1. Construct high-quality ASCIIEval test set with diverse categories and consistent formatting
  2. Evaluate models across all three input modalities using standardized prompts
  3. Analyze performance patterns and identify trade-offs between text and visual recognition
  4. Develop and test enhancement strategies for both LLMs and MLLMs
  5. Document findings and propose future research directions

- Design tradeoffs:
  - Text vs. image inputs: Text-only tests pure visual perception while image-only tests multimodal integration
  - Open-source vs. proprietary models: Different resource constraints and training objectives affect performance
  - Fine-grained vs. collective perception: Models may optimize for character recognition at the expense of overall visual understanding

- Failure signatures:
  - Random chance performance indicates models cannot extract visual information from text
  - Consistent degradation in text-image setting suggests inability to fuse congruent modalities
  - Negative correlation with OCR benchmarks confirms trade-off between text and visual recognition

- First 3 experiments:
  1. Test baseline performance of representative open-source and proprietary models on ASCIIEval
  2. Evaluate correlation between ASCIIEval performance and established visual/textual benchmarks
  3. Implement low-resolution prompting on Qwen2.5-VL-8B and measure performance changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be enhanced to achieve intrinsic 2D spatial perception of text strings rather than relying on compositional reasoning through rationale-assisted fine-tuning?
- Basis in paper: [explicit] The paper discusses rationale-assisted fine-tuning improving LLM performance on ASCII art recognition, but notes this improvement doesn't fundamentally enhance LLMs' ability, instead using a divide-and-conquer strategy that helps models perform compositional reasoning at inference time.

- Why unresolved: The authors identify tokenization as a bottleneck, where consecutive characters are concatenated arbitrarily, destroying vertical coherence crucial for ASCII art. They suggest exploring alternative input representations but don't propose specific solutions.

- What evidence would resolve it: Demonstration of an LLM architecture or preprocessing method that achieves comparable or better ASCII art recognition performance without requiring rationale generation, showing genuine 2D spatial understanding rather than pattern matching.

### Open Question 2
- Question: What architectural modifications would enable MLLMs to achieve effective cross-modal synergy when both text and image inputs are provided simultaneously, rather than experiencing performance degradation?
- Basis in paper: [explicit] The paper finds that "none of the models could successfully benefit from the simultaneous provision of both modalities" and that "model performance is sensitive to the length of the ASCII art, with this sensitivity varying across input modalities."

- Why unresolved: Current MLLMs appear to be confounded by concurrent inputs, leading to higher error rates. The authors suggest prioritizing elucidating internal mechanisms of modal conflict while developing architectures capable of dynamic fusion, but don't propose specific solutions.

- What evidence would resolve it: An MLLM architecture that shows improved or at least non-degraded performance on the Text-Image setting compared to either modality alone, with clear evidence of effective fusion rather than interference.

### Open Question 3
- Question: How can open-source MLLMs be developed to balance fine-grained text recognition and holistic visual perception without experiencing the seesaw effect identified in this paper?
- Basis in paper: [explicit] The paper reveals "a seesaw effect between OCR and ASCII art recognition: an overemphasis on improving OCR will inadvertently impair models' ability to perceive collective visual signals."

- Why unresolved: The paper shows that newer-generation open-source MLLMs underperform their ancestors, suggesting a shift away from core visual interpretation capabilities. While they propose post-hoc solutions like low-resolution prompting and supervised fine-tuning, these are described as merely "post-hoc solutions."

- What evidence would resolve it: An open-source MLLM that achieves high performance on both ASCII art recognition and traditional OCR tasks, demonstrating the ability to balance these competing skills intrinsically rather than through specialized prompting or fine-tuning.

## Limitations

- Small test set size (3.5K samples) may not provide sufficient statistical power for robust conclusions
- Correlation analysis between ASCIIEval and established benchmarks is based on limited number of models
- Proprietary models have unknown architectures and training procedures, preventing detailed analysis of superior performance
- Enhancement strategies lack detailed hyperparameter specifications, making reproduction challenging

## Confidence

- **High confidence**: Proprietary models significantly outperform open-source models (up to 20.01% gap) - well-supported by comprehensive evaluation across 50+ models
- **Medium confidence**: Trade-off between fine-grained text recognition and collective visual perception in MLLMs - supported by correlation analysis but lacks direct causal evidence
- **Low confidence**: Cross-modal synergy failure - based on observed performance degradation but lacks systematic investigation of why congruent inputs produce worse results

## Next Checks

1. **Statistical power validation**: Conduct power analysis to determine if the 3.5K sample size provides sufficient statistical power to detect meaningful differences between model groups, and consider expanding the test set if necessary.

2. **Architectural ablation study**: Perform controlled experiments with MLLMs where OCR and visual perception components are selectively disabled or enhanced to directly test the trade-off hypothesis and identify which architectural elements drive the observed behavior.

3. **Cross-modal fusion analysis**: Systematically vary the alignment between text and image inputs (congruent vs. incongruent) to determine whether the performance degradation in text-image settings is due to modality-specific processing failures or fundamental limitations in cross-modal fusion capabilities.