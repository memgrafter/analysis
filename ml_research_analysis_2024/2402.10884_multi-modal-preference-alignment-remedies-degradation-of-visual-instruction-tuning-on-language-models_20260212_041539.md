---
ver: rpa2
title: Multi-modal Preference Alignment Remedies Degradation of Visual Instruction
  Tuning on Language Models
arxiv_id: '2402.10884'
source_url: https://arxiv.org/abs/2402.10884
tags:
- llav
- language
- instruction
- preference
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses performance degradation in multi-modal large
  language models (MLLMs) after visual instruction tuning, where models like LLaVA
  show reduced language instruction-following capabilities. The authors propose using
  Direct Preference Optimization (DPO) with a 5k-sample VQA preference dataset annotated
  by Gemini across five quality metrics to restore and enhance both language and visual
  instruction performance.
---

# Multi-modal Preference Alignment Remedies Degradation of Visual Instruction Tuning on Language Models

## Quick Facts
- **arXiv ID**: 2402.10884
- **Source URL**: https://arxiv.org/abs/2402.10884
- **Reference count**: 7
- **Key outcome**: DPO with 5k-sample VQA preference dataset achieves 6.73 on MT-Bench, surpassing Vicuna (6.57) and LLaVA (5.99) while improving visual instruction performance

## Executive Summary
This study addresses performance degradation in multi-modal large language models (MLLMs) after visual instruction tuning, where models like LLaVA show reduced language instruction-following capabilities. The authors propose using Direct Preference Optimization (DPO) with a 5k-sample VQA preference dataset annotated by Gemini across five quality metrics to restore and enhance both language and visual instruction performance. Their DPO model achieves a 6.73 score on MT-Bench, surpassing both Vicuna (6.57) and LLaVA (5.99), while also improving visual instruction performance (+4.9% on MM-Vet, +6% on LLaVA-Bench) with minimal degradation on visual knowledge benchmarks. The approach demonstrates data efficiency, requiring only 5k examples compared to 80k+ needed by traditional methods.

## Method Summary
The authors collect 5k samples from SciGraphQA and LRV-Instruct datasets, generate 4 completions per sample using LLaVA-1.5-13B with temperature=0.7, and obtain granular annotations from Gemini using the HelpSteer labeling guide. These granular annotations are converted to preference pairs by selecting top-scoring response as preferred and responses with at least 2 points lower as rejected. The DPO model is then trained with specified hyperparameters using Deepspeed Stage-3 on 4x A100 80GB GPUs. The trained model is evaluated on MT-Bench, MM-Vet, LLaVA-Bench, PoPe, MM-Bench, and AlpacaEval benchmarks using greedy decoding.

## Key Results
- DPO model achieves 6.73 on MT-Bench, surpassing Vicuna (6.57) and LLaVA (5.99)
- Improves visual instruction performance by +4.9% on MM-Vet and +6% on LLaVA-Bench
- Requires only 5k preference examples compared to 80k+ needed by traditional methods
- Shows minimal degradation on visual knowledge benchmarks while improving language instruction-following

## Why This Works (Mechanism)
The approach works by addressing the degradation that occurs when visual instruction tuning interferes with a model's language capabilities. By using preference learning through DPO with high-quality annotated data, the model can be aligned to maintain both visual reasoning and language instruction-following capabilities simultaneously. The preference dataset provides fine-grained quality signals across multiple dimensions (Helpfulness, Correctness, Coherence, Complexity, Verbosity) that guide the optimization process more effectively than traditional supervised learning approaches.

## Foundational Learning
- **Multi-modal Large Language Models**: Need to understand the architecture combining vision and language transformers for handling both image and text inputs. Quick check: Verify model can process both image and text inputs through separate encoders.
- **Direct Preference Optimization (DPO)**: Understanding the preference learning framework that optimizes models based on pairwise comparisons rather than absolute labels. Quick check: Confirm preference pairs are properly formatted for DPO training.
- **Visual Instruction Tuning**: Recognizing how additional visual training data can degrade language capabilities due to catastrophic forgetting. Quick check: Measure performance drop on language-only tasks after visual tuning.
- **Preference Dataset Quality**: Importance of high-quality annotations for effective preference learning. Quick check: Validate annotation consistency through inter-annotator agreement.
- **Model Alignment Tax**: Understanding the performance degradation that can occur when aligning models to specific objectives. Quick check: Monitor performance on pre-training tasks during fine-tuning.
- **Cross-Modal Generalization**: How models transfer learning between visual and language domains. Quick check: Test model performance on mixed-modal versus single-modal tasks.

## Architecture Onboarding

**Component Map**: LLaVA-v1.5-13B base model -> LoRA adapters for fine-tuning -> DPO optimization -> Preference dataset (5k samples) -> Gemini annotations -> Evaluation benchmarks

**Critical Path**: Base model → LoRA fine-tuning → DPO training → Preference alignment → Benchmark evaluation

**Design Tradeoffs**: The approach trades computational efficiency (5k samples vs 80k+) for potentially reduced generalization coverage. Using synthetic annotations from Gemini provides scalability but introduces potential biases compared to human annotations.

**Failure Signatures**: Performance degradation on language tasks indicates overfitting to visual preferences; poor results on MM-Bench suggest alignment tax issues; low MT-Bench scores indicate insufficient language capability restoration.

**First Experiments**:
1. Train DPO with 1k samples to establish baseline performance and verify implementation
2. Test cross-model transfer by applying 13b preferences to 7b model
3. Conduct ablation study on preference dataset size (1k, 3k, 5k, 10k) to find optimal scaling

## Open Questions the Paper Calls Out
- **Open Question 1**: How scalable is DPO alignment beyond 6K preference examples for larger and more complex MLLMs? While data scaling analysis suggests significant improvements up to a 6K preference dataset, the full extent of scalability beyond this threshold remains unexplored. The experiments only tested up to 6K examples, and the paper acknowledges that as foundational models like LLaVA evolve in complexity and size, the effectiveness of DPO might encounter diminishing returns.
- **Open Question 2**: How transferable are preference datasets between different MLLMs - can a dataset generated by one model be effectively used to fine-tune other models? While initial cross-model transfer experiments showed promise, the paper only tested one direction (13b to 7b) and acknowledges the need for broader validation across different model architectures and sizes.
- **Open Question 3**: How does DPO alignment impact MLLM performance in critical real-world domains where reliability is essential? The paper focuses on benchmark performance but doesn't test in critical domains like healthcare, legal, or safety-critical applications where unreliable responses could have serious consequences.

## Limitations
- Synthetic preference data from Gemini introduces potential annotation biases
- Small 5k-sample dataset may not capture full diversity of multi-modal instruction scenarios
- Reliance on proprietary model outputs for evaluation creates dependency chain issues

## Confidence
- **High confidence**: DPO can improve both language and visual instruction-following performance on tested benchmarks
- **Medium confidence**: Data efficiency claim, as comparison baseline uses different evaluation protocols
- **Medium confidence**: Minimal alignment tax claim, as study primarily reports gains on same benchmarks used for training

## Next Checks
1. Conduct ablation studies on dataset size (1k, 2.5k, 10k samples) to verify 5k is truly optimal
2. Implement human evaluation on subset of responses to validate synthetic preference data quality
3. Test DPO model's generalization on entirely separate multi-modal reasoning datasets not used in training