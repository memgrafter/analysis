---
ver: rpa2
title: 'A Nurse is Blue and Elephant is Rugby: Cross Domain Alignment in Large Language
  Models Reveal Human-like Patterns'
arxiv_id: '2405.14863'
source_url: https://arxiv.org/abs/2405.14863
tags:
- llms
- explanations
- humans
- human
- mappings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether large language models (LLMs) can
  perform cross-domain alignment tasks, where concepts from one domain (e.g., "doctor")
  are mapped to another domain (e.g., "color"). The authors prompt several LLMs with
  cross-domain mapping tasks and analyze their responses at both population and individual
  levels.
---

# A Nurse is Blue and Elephant is Rugby: Cross Domain Alignment in Large Language Models Reveal Human-like Patterns

## Quick Facts
- arXiv ID: 2405.14863
- Source URL: https://arxiv.org/abs/2405.14863
- Reference count: 12
- Primary result: LLMs perform cross-domain alignment at population levels matching human patterns

## Executive Summary
This study investigates whether large language models can perform cross-domain alignment tasks, mapping concepts across different domains (e.g., professions to colors) and providing explanations for their choices. The authors evaluate several LLMs using human-annotated datasets and find that models can perform such mappings with substantial agreement to human behavior at the population level. Some models even surpass individual-level agreement with the most popular human mappings. The models also generate explanations that align with human reasoning patterns across seven similarity categories.

## Method Summary
The researchers created a cross-domain mapping task using 75 statements across 12 domains, each annotated by 20 human participants. They prompted several LLMs (Flan, Llama, and Mistral) with four different templates per statement and used majority voting to generate predictions. The models' responses were evaluated using M@K metrics (matching top K human answers) and compared to human distributions. For explanations, the researchers fine-tuned a RoBERTa-large SetFit classifier on human explanations and used it to categorize model explanations into seven similarity types.

## Key Results
- LLMs achieve M@1 scores ranging from 8.1% to 24.3% and M@3 scores from 14.9% to 36.5%, with Llama-7B scoring highest
- Models produce explanations distributed across seven similarity categories similar to human distributions
- Larger models do not necessarily outperform smaller ones, possibly due to semantic equivalence divergence under strict word-matching evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can perform cross-domain alignment at above-chance levels, matching human patterns.
- Mechanism: The models leverage distributed semantic representations learned during pretraining to map concepts across domains, using similarity structures such as perceptual, associative, or abstract alignment.
- Core assumption: The training corpus contains sufficient signal for these cross-domain mappings to be encoded in the model's embeddings.
- Evidence anchors:
  - [abstract] "Results show that LLMs can perform such mappings, reaching substantial agreement with human behavior at the population level."
  - [section] "The M@1 score ranges between 8.1% − 24.3%, and the M@3 ranges between 14 .9 − 36.5. Llama-7B scores the highest in both M@1 and M@3 with 24.3% and 36 .5% respectively."
  - [corpus] Weak - neighbor papers discuss alignment but not cross-domain conceptual mapping.
- Break condition: If the training data lacks sufficient cross-domain co-occurrence patterns, the model's mappings will revert to random guessing.

### Mechanism 2
- Claim: LLMs produce explanations for their mappings that align with human reasoning categories.
- Mechanism: The model's internal representations map to similar abstract dimensions (e.g., perceptual similarity, common mediators) that humans use when justifying cross-domain choices.
- Core assumption: The same latent dimensions that organize human semantic knowledge are reflected in the model's learned representations.
- Evidence anchors:
  - [abstract] "the models mostly provide valid explanations and deploy reasoning paths similar to humans"
  - [section] "We see that the models' explanation categories are distributed in a very similar way to humans, suggesting they rely on similar types of similarity in their representations."
  - [corpus] Weak - corpus neighbors focus on alignment methods but not explanation quality in cross-domain tasks.
- Break condition: If the explanation generation is driven by surface-level token patterns rather than genuine reasoning, the categories will diverge from human distributions.

### Mechanism 3
- Claim: Larger models do not necessarily perform better on cross-domain alignment, possibly due to semantic equivalence divergence.
- Mechanism: While larger models may generate more diverse or semantically equivalent responses, the strict word-matching metric penalizes them, masking true performance.
- Core assumption: The evaluation metric M@K does not capture semantic equivalence, leading to underestimation of larger models' performance.
- Evidence anchors:
  - [section] "Interestingly, we see that larger models do not necessarily score higher. A possible reason for this might be that their responses are less similar to those of humans, but still acceptable or even semantically equivalent"
  - [section] "larger models do not necessarily score higher. A possible reason for this might be that their responses are less similar to those of humans, but still acceptable or even semantically equivalent"
  - [corpus] Weak - no direct corpus support for model size vs semantic equivalence in alignment tasks.
- Break condition: If semantic equivalence is properly measured, the performance gap between model sizes may disappear or reverse.

## Foundational Learning

- Concept: Cross-domain alignment task
  - Why needed here: Forms the experimental basis; understanding the task is essential for interpreting model behavior and explanations.
  - Quick check question: What distinguishes cross-domain alignment from within-domain similarity tasks?

- Concept: Semantic similarity categories
  - Why needed here: Used to classify both human and model explanations; critical for comparing reasoning processes.
  - Quick check question: Which of the seven similarity categories would best explain mapping "thunder" to "drum"?

- Concept: M@K evaluation metric
  - Why needed here: Core metric for scoring model performance; understanding its construction and limitations is essential for result interpretation.
  - Quick check question: How does M@any differ from M@1, and why might it be more appropriate here?

## Architecture Onboarding

- Component map:
  - Data layer: Human cross-domain mapping dataset (75 statements, 20 annotators each)
  - Model layer: Flan, Llama, and Mistral LLMs with prompt templates
  - Evaluation layer: M@K metrics, explanation classifiers, BERTScore for similarity
  - Analysis layer: Distribution comparison, individual vs population agreement

- Critical path:
  1. Load human mapping dataset and preprocess into prompt templates
  2. Generate model responses using majority vote over templates
  3. Compute M@K scores against human responses
  4. Generate explanations for model mappings
  5. Classify explanations into predefined categories
  6. Compare distributions and similarities with human explanations

- Design tradeoffs:
  - Prompt templates vs single prompts: Multiple templates reduce noise but increase variance
  - Greedy decoding vs sampling: Greedy gives deterministic responses; sampling allows exploration of reasoning paths
  - M@K vs semantic similarity metrics: M@K is strict but may underestimate true alignment

- Failure signatures:
  - Low M@any scores indicate models are producing non-human-like mappings
  - High "Other" category in explanations suggests model explanations are not fitting human reasoning patterns
  - Large gap between M@K and semantic similarity metrics indicates evaluation mismatch

- First 3 experiments:
  1. Run cross-domain mapping task with a single prompt template and greedy decoding; compare M@1 to random baseline
  2. Add sampling-based decoding (temperature=0.7) and compare explanation diversity and quality
  3. Test semantic equivalence metric (e.g., SBERT similarity) alongside M@K to detect underestimation of model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs truly rely on perceptual similarity for cross-domain mappings, or do they merely generate explanations that appear to do so?
- Basis in paper: [explicit] The authors note that LLMs produce explanations based on perceptual similarity (e.g., mapping pineapple to football due to shared texture and shape) despite being trained only on text.
- Why unresolved: The paper acknowledges that the link between the explanations provided and the actual factors LLMs rely on during the mapping process is "highly non-trivial" and requires further verification.
- What evidence would resolve it: Controlled experiments manipulating the input text to include or exclude perceptual descriptions, or ablation studies on the model's internal representations, could help determine whether perceptual features are genuinely used during the mapping process.

### Open Question 2
- Question: How do semantic equivalence and surface form differences impact the accuracy of cross-domain mapping evaluation?
- Basis in paper: [explicit] The authors note that their M@K metric is sensitive to the surface form of responses and does not account for semantic equivalence, potentially underestimating model performance.
- Why unresolved: The paper defers the development of a metric that assesses semantic equivalence to future work.
- What evidence would resolve it: Developing and applying a semantic equivalence metric (e.g., using word embeddings or paraphrase detection) to the cross-domain mapping task would allow for a more accurate comparison between human and LLM responses.

### Open Question 3
- Question: Are the observed similarities between LLM and human explanations due to genuine conceptual alignment, or are they artifacts of the explanation generation process?
- Basis in paper: [inferred] The authors observe that LLM explanations distribute similarly across categories compared to human explanations, but they also note that LLM explanations tend to be longer and more structured.
- Why unresolved: The paper does not directly address whether the similarity in explanation categories reflects genuine conceptual alignment or simply differences in explanation style.
- What evidence would resolve it: Comparing the explanations generated by LLMs to those generated by humans when prompted with the same task, controlling for explanation length and structure, could help determine whether the similarities are substantive or superficial.

## Limitations
- The M@K metric's strict word-matching approach may underestimate model performance, particularly for larger models that generate semantically equivalent but lexically different responses.
- The human dataset represents only 20 annotators per statement, raising questions about whether the population-level agreement captured truly reflects general human reasoning patterns.
- The explanation analysis relies on a predefined set of seven similarity categories derived from human explanations, potentially missing novel reasoning patterns from the models.

## Confidence
- High confidence: LLMs can perform cross-domain alignment at above-chance levels (M@K scores 8.1-36.5% across models)
- Medium confidence: Model explanations align with human reasoning categories in distribution, though not necessarily in quality or depth
- Medium confidence: Larger models do not necessarily outperform smaller ones, potentially due to semantic equivalence divergence under current evaluation metrics

## Next Checks
1. **Semantic equivalence validation**: Re-run the cross-domain alignment task using SBERT or other semantic similarity metrics alongside M@K to determine if larger models are penalized for generating semantically equivalent but lexically different responses
2. **Individual annotator analysis**: Perform statistical analysis to determine whether the human population-level agreement is driven by consistent patterns across all annotators or dominated by a subset of consistent responders
3. **Alternative explanation taxonomies**: Conduct qualitative analysis of a random sample of model explanations to identify any reasoning patterns that fall outside the seven predefined similarity categories, potentially revealing novel forms of cross-domain reasoning