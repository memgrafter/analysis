---
ver: rpa2
title: Improve Cross-Architecture Generalization on Dataset Distillation
arxiv_id: '2402.13007'
source_url: https://arxiv.org/abs/2402.13007
tags:
- dataset
- distillation
- pool
- main
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of limited cross-architecture generalization
  in dataset distillation, where distilled datasets perform well on the training architecture
  but poorly on others. The authors propose a "model pool" method that trains the
  distilled dataset using multiple model architectures sampled from a probability
  distribution, and integrate knowledge distillation into the testing phase.
---

# Improve Cross-Architecture Generalization on Dataset Distillation

## Quick Facts
- arXiv ID: 2402.13007
- Source URL: https://arxiv.org/abs/2402.13007
- Authors: Binglin Zhou; Linhao Zhong; Wentao Chen
- Reference count: 40
- Primary result: Using model pool and knowledge distillation together achieves up to 22.5% accuracy on VGG11 (vs. 14.3% baseline) and 27.25% on ResNet18 (vs. 14.3% baseline) when using 1 image per class on CIFAR-10

## Executive Summary
This paper addresses the problem of limited cross-architecture generalization in dataset distillation, where distilled datasets perform well on the training architecture but poorly on others. The authors propose a "model pool" method that trains the distilled dataset using multiple model architectures sampled from a probability distribution, and integrate knowledge distillation into the testing phase. They demonstrate that combining these approaches significantly improves cross-architecture generalization performance compared to baseline methods on CIFAR-10, with the model pool and knowledge distillation together achieving up to 22.5% accuracy on VGG11 (vs. 14.3% baseline) and 27.25% on ResNet18 (vs. 14.3% baseline) when using 1 image per class.

## Method Summary
The method combines two key innovations: a model pool approach during distillation and knowledge distillation during testing. During distillation, a probability distribution selects models from a pool (ConvNet as main model with 0.9 probability, others with 0.1 probability) to create synthetic datasets that satisfy multiple training objectives. During testing, a fixed ConvNet teacher model trained on the distilled dataset provides knowledge distillation to various student architectures through KL divergence loss. The combined approach forces the distilled dataset to be robust across different architectures while ensuring cross-architecture feature transfer during deployment.

## Key Results
- Model pool approach alone improves cross-architecture performance: 22.5% accuracy on VGG11 (vs. 14.3% baseline) and 27.25% on ResNet18 (vs. 14.3% baseline) with 1 image per class
- Knowledge distillation during testing further improves performance across all tested architectures
- The combined approach consistently outperforms both baseline gradient matching and individual component methods
- Results demonstrate significant improvement over standard dataset distillation methods that use single architectures

## Why This Works (Mechanism)

### Mechanism 1
Using a model pool during distillation improves cross-architecture generalization by forcing the distilled dataset to satisfy multiple training objectives. During the distillation process, each training step samples a model architecture from a predefined distribution (e.g., 90% main ConvNet, 10% others). This causes the synthetic dataset to be optimized for gradients that are robust across different architectures, not just one. The core assumption is that the global optimum of the distilled dataset for one architecture is sufficiently similar to that of other similar architectures; the main model anchors convergence.

### Mechanism 2
Knowledge distillation applied during testing transfers discriminative features from a fixed teacher model to various student models, improving their performance on the distilled dataset. A ConvNet teacher model is trained on the distilled dataset. During testing, other student architectures (e.g., MLP, VGG11, ResNet18) are trained with a loss that includes both classification loss and KL divergence between their outputs and the teacher's outputs. The core assumption is that the teacher model learns useful features from the distilled dataset that are generalizable to other architectures.

### Mechanism 3
The combination of model pool and knowledge distillation addresses both the training diversity and deployment generalization problems. Model pool diversifies the training objective across architectures, while knowledge distillation ensures that any student architecture tested on the distilled dataset benefits from the teacher's learned representation. The core assumption is that the two mechanisms are complementary; model pool prevents overfitting to one architecture, knowledge distillation ensures cross-architecture feature transfer.

## Foundational Learning

- Concept: Dataset distillation
  - Why needed here: The entire method relies on generating a small synthetic dataset that can train models as well as the full dataset
  - Quick check question: What is the objective function used in gradient matching-based dataset distillation?

- Concept: Knowledge distillation
  - Why needed here: Used to transfer knowledge from a fixed teacher model to student models during testing
  - Quick check question: How does the KL divergence term in knowledge distillation help align student and teacher predictions?

- Concept: Model ensemble / multi-model training
  - Why needed here: Model pool is conceptually similar to ensemble methods but used during distillation rather than deployment
  - Quick check question: What is the difference between bagging and boosting in model ensemble methods?

## Architecture Onboarding

- Component map:
  - Model pool: a set of model architectures with associated sampling probabilities
  - Main model: ConvNet with 90% probability, anchors convergence
  - Auxiliary models: ConvNets with varied activation, normalization, pooling layers
  - Teacher model: fixed 3-layer ConvNet for knowledge distillation
  - Student models: various architectures (MLP, LeNet, AlexNet, VGG11, ResNet18)
  - Training loop: sample model, update distilled dataset to match gradients
  - Testing loop: train student with classification + KL loss from teacher

- Critical path:
  1. Initialize distilled dataset
  2. For each iteration:
     - Sample a model from the model pool
     - Compute gradient matching loss between original and distilled dataset
     - Update distilled dataset
  3. After distillation, train teacher on distilled dataset
  4. For each student model:
     - Train on distilled dataset with classification + KL loss from teacher
     - Evaluate on original test set

- Design tradeoffs:
  - Larger model pool → better generalization but harder convergence
  - Higher teacher temperature → smoother gradients but less discriminative
  - More iterations → better distilled dataset but higher compute cost

- Failure signatures:
  - Training loss does not decrease → model pool too diverse
  - Student performance worse than baseline → teacher not informative
  - Distilled dataset collapses to single mode → gradient matching loss too strong

- First 3 experiments:
  1. Run baseline gradient matching with ConvNet only, measure cross-architecture performance
  2. Add model pool (ConvNet main + 10% others), measure improvement and training stability
  3. Add knowledge distillation during testing, measure final student performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal probability distribution for selecting models from the model pool to maximize cross-architecture generalization performance?
- Basis in paper: [explicit] The authors mention that the main model is chosen with a probability of 0.9, but note this was chosen based on prior knowledge and could be optimized
- Why unresolved: The paper does not conduct experiments to find the optimal probability distribution, only testing one fixed value
- What evidence would resolve it: Systematic experiments varying the probability of selecting the main model (e.g., 0.5, 0.7, 0.9, 0.95) while keeping other factors constant to identify which yields the best cross-architecture performance

### Open Question 2
- Question: How does increasing network depth beyond 3 layers affect the cross-architecture generalization performance of distilled datasets?
- Basis in paper: [explicit] The authors note that following the original gradient matching paper, they used a depth of 3, but consider that modern models often have much greater depth (30+ layers) and this could be explored
- Why unresolved: The paper only experiments with a 3-layer ConvNet for the main model and does not test deeper architectures
- What evidence would resolve it: Experiments using the model pool approach with deeper architectures (e.g., 10, 20, 30 layers) for the main model to measure how depth affects generalization across different target architectures

### Open Question 3
- Question: What is the relationship between the similarity of models in the model pool and the convergence of the distillation process?
- Basis in paper: [explicit] The authors hypothesize that similar models in the pool help convergence because they have similar global optimal parameters, but this is based on intuition rather than systematic testing
- Why unresolved: The paper only tests one configuration where models are similar to the main model, without exploring the full spectrum from very diverse to very similar models
- What evidence would resolve it: Experiments comparing model pool configurations ranging from very diverse models (e.g., MLP, LeNet, AlexNet, ResNet) to highly similar models (e.g., ConvNets with minor architectural variations) to quantify the trade-off between generalization and convergence

## Limitations
- The approach is only validated on CIFAR-10 dataset, limiting generalizability to larger-scale datasets like ImageNet
- The model pool size and composition is fixed rather than optimized through systematic hyperparameter search
- No theoretical analysis provided to explain why the combination of model pool and knowledge distillation works synergistically

## Confidence

- High: The knowledge distillation mechanism during testing is well-established in the literature
- Medium: The model pool approach shows consistent improvements across architectures but theoretical justification is limited
- Low: The claim that combining both methods is synergistic lacks direct experimental validation

## Next Checks

1. Perform gradient landscape analysis comparing ConvNet and VGG11 to quantify similarity assumptions
2. Conduct ablation studies isolating the effects of model pool sampling probability versus knowledge distillation temperature
3. Test the approach on a different dataset (e.g., CIFAR-100) to verify generalizability beyond CIFAR-10