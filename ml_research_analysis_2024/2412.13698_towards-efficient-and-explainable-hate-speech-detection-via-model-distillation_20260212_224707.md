---
ver: rpa2
title: Towards Efficient and Explainable Hate Speech Detection via Model Distillation
arxiv_id: '2412.13698'
source_url: https://arxiv.org/abs/2412.13698
tags:
- hate
- speech
- https
- detection
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently and transparently
  detecting hate speech in online platforms. The authors propose using model distillation
  to transfer knowledge from a large 70B parameter LLM to a smaller 8B parameter model,
  enabling both classification and explanation of hate speech.
---

# Towards Efficient and Explainable Hate Speech Detection via Model Distillation

## Quick Facts
- arXiv ID: 2412.13698
- Source URL: https://arxiv.org/abs/2412.13698
- Authors: Paloma Piot; Javier Parapar
- Reference count: 40
- Key outcome: Distills a 70B LLM to an 8B model achieving F1-score of 0.85, outperforming the teacher model while being 54% faster, using 86% less GPU memory, and costing 86% less

## Executive Summary
This paper addresses the challenge of efficiently and transparently detecting hate speech in online platforms. The authors propose using model distillation to transfer knowledge from a large 70B parameter LLM to a smaller 8B parameter model, enabling both classification and explanation of hate speech. The method involves extracting rationales using Chain-of-Thought prompting from the large model, then fine-tuning the smaller model using a multi-task learning framework to predict both labels and explanations. Experimental results demonstrate that the distilled model achieves an F1-score of 0.85 for classification, outperforming the larger teacher model and matching its quality of explanations while being significantly more efficient.

## Method Summary
The authors employ a knowledge distillation approach that leverages Chain-of-Thought prompting with Few-Shot examples to extract explanations from a large 70B parameter teacher model. These rationales and labels are then used to fine-tune a smaller 8B parameter student model using a multi-task learning framework that simultaneously learns classification and explanation generation. The training data comes from balanced subsamples of the MetaHate dataset, which combines 36 diverse human-curated hate speech datasets. The distilled model is evaluated against the teacher model, base model, and four baselines using F1-score for classification and human evaluation for explanation quality.

## Key Results
- Distilled model achieves F1-score of 0.85 for classification, outperforming the 70B teacher model
- Generated explanations match the quality of the larger model according to human evaluation
- Significant efficiency improvements: 54% faster inference, 86% less GPU memory usage, and 86% lower cost
- Multi-task learning with rationales improves classification performance compared to single-task approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation transfers both classification capability and explanation quality from a large 70B model to a smaller 8B model.
- Mechanism: The large model generates rationales via Chain-of-Thought prompting, then these rationales and labels are used to fine-tune the smaller model using a multi-task learning framework.
- Core assumption: The larger model's rationales are sufficiently high quality to improve the smaller model's performance.
- Evidence anchors:
  - [abstract] "we propose distilling big language models by using Chain-of-Thought to extract explanations that support the hate speech classification task"
  - [section] "This framework enables modelsto learn both classification (generating labels) and explanation (providing supporting rationales)"
  - [corpus] Found 25 related papers with average neighbor FMR=0.434, suggesting moderate relevance to explainable hate speech detection

### Mechanism 2
- Claim: The multi-task learning approach improves classification performance by incorporating intermediate reasoning steps.
- Mechanism: The smaller model learns to generate rationales (intermediate reasoning) alongside labels, which enhances its understanding of the classification task.
- Core assumption: The rationale generation process provides meaningful intermediate reasoning that improves classification accuracy.
- Evidence anchors:
  - [section] "The rationale generation helps the model produce intermediate reasoning steps, which can enhance its label prediction accuracy, i.e. by reasoning step-by-step, the model can better analyse the messages, improving its accuracy in the labelling"
  - [abstract] "we demonstrate that distilled models deliver explanations of the same quality as larger models while surpassing them in classification performance"
  - [corpus] Weak evidence - related work focuses on different aspects of explainable detection but doesn't specifically validate this mechanism

### Mechanism 3
- Claim: Using curated multi-source data improves model generalization across different social media platforms.
- Mechanism: The MetaHate dataset combines 36 diverse human-curated datasets, providing varied examples of hate speech across different platforms and contexts.
- Core assumption: The diversity in training data leads to better generalization than models trained on single-source data.
- Evidence anchors:
  - [section] "We prepared two hate speech datasets as explained in §4.1. We conducted our experiments on two subsamples of MetaHate [35]. MetaHate is a meta-collection of 1.2 million entries, labelled in a binary fashion, compiled from 36 different datasets across various social networks"
  - [abstract] "We propose a hate speech detection approach that uses sample data from 36 diverse human-curated datasets, enhancing its generalisation capabilities"
  - [corpus] No direct evidence in corpus - this is a unique aspect of this paper's approach

## Foundational Learning

- Concept: Chain-of-Thought prompting
  - Why needed here: Enables the large model to generate detailed explanations alongside classifications, which are essential for the distillation process
  - Quick check question: What is the difference between zero-shot and few-shot Chain-of-Thought prompting, and why might few-shot be preferable for this task?

- Concept: Multi-task learning framework
  - Why needed here: Allows the distilled model to simultaneously learn classification and explanation generation, which improves both capabilities
  - Quick check question: How does combining classification and rationale generation tasks in a single model affect the loss function and training dynamics?

- Concept: Knowledge distillation techniques
  - Why needed here: Transfers capabilities from a computationally expensive large model to an efficient smaller model while maintaining performance
  - Quick check question: What are the key differences between knowledge distillation in traditional transformer models versus LLMs?

## Architecture Onboarding

- Component map:
  Teacher model (Llama-3-70B-Instruct) -> Data preparation pipeline -> Distillation training loop -> Evaluation framework -> Human evaluation component

- Critical path: Teacher model inference → Rationale extraction → Multi-task fine-tuning → Model evaluation → Human evaluation

- Design tradeoffs:
  - Model size vs. efficiency: 70B teacher provides better explanations but requires significant resources; 8B student is more practical but may lose some capability
  - Data quantity vs. quality: Using curated subset of MetaHate ensures quality but limits scale
  - Computational cost vs. performance: 4-bit quantization reduces memory requirements but may slightly impact performance

- Failure signatures:
  - Classification performance drops significantly after distillation
  - Generated explanations lack coherence or relevance to the input
  - Human evaluators rate explanations as incomplete or incorrect
  - Model shows bias toward certain types of hate speech or platforms

- First 3 experiments:
  1. Compare classification performance of teacher model vs. distilled model on MiniMetaHate Eval dataset using F1-score
  2. Evaluate explanation quality through human annotation on a 100-entry subsample, measuring completeness and correctness
  3. Measure efficiency improvements by comparing inference speed, memory usage, and cost between teacher and distilled models

## Open Questions the Paper Calls Out

- How do the distilled models perform across different languages and cultural contexts?
- What is the optimal number of examples needed for the Chain-of-Thought prompting to achieve maximum performance?
- How does the quality of human-authored rationales compare to LLM-generated rationales for distillation?

## Limitations

- Limited sample size: Experiments use only balanced subsamples of 2001 posts each from the full 1.2 million entry dataset
- Human evaluation methodology: Explanation quality assessment lacks detailed methodology including inter-annotator agreement rates and specific rubrics
- Efficiency metrics: Claims of 54% faster inference and 86% less memory/cost lack detailed experimental setup information

## Confidence

- High Confidence: Classification performance results (F1-score of 0.85) are well-supported by experimental comparisons with baselines and teacher model
- Medium Confidence: Claims about explanation quality matching larger models are supported by human evaluation but lack detailed methodology and statistical validation
- Low Confidence: Generalization claims based on 36-source dataset are not fully validated as experiments only use subsamples

## Next Checks

1. Expand dataset evaluation: Test the distilled model on larger samples from the full MetaHate dataset and on external hate speech datasets not used in training to validate generalization claims across different social media platforms and content types.

2. Rigorous explanation quality assessment: Implement a more comprehensive human evaluation framework with detailed rubrics, multiple annotator agreement metrics, and comparison against human-generated explanations to strengthen the explanation quality claims.

3. Real-world efficiency benchmarking: Conduct controlled experiments measuring inference speed, memory usage, and cost across different hardware configurations, batch sizes, and operational scenarios to validate the efficiency claims under practical deployment conditions.