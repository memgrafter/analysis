---
ver: rpa2
title: Generalisation First, Memorisation Second? Memorisation Localisation for Natural
  Language Classification Tasks
arxiv_id: '2408.04965'
source_url: https://arxiv.org/abs/2408.04965
tags:
- layers
- memorisation
- layer
- tasks
- trec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines where memorisation occurs in transformer-based
  language models across 12 natural language classification tasks. Using label perturbation
  to enforce memorisation, the authors apply four localisation techniques: layer swapping,
  layer retraining, gradient analysis, and probing.'
---

# Generalisation First, Memorisation Second? Memorisation Localisation for Natural Language Classification Tasks

## Quick Facts
- arXiv ID: 2408.04965
- Source URL: https://arxiv.org/abs/2408.04965
- Reference count: 40
- Primary result: Memorisation is a gradual, task-dependent process distributed across transformer layers rather than confined to specific ones.

## Executive Summary
This paper investigates where memorisation occurs in transformer-based language models across 12 natural language classification tasks. Using label perturbation to enforce memorisation, the authors apply four localisation techniques: layer swapping, layer retraining, gradient analysis, and probing. Results show memorisation is a gradual, task-dependent process rather than confined to specific layers. Early layers are most crucial for memorisation overall, with the depth of memorisation correlating with a model's generalisation ability. Tasks with higher generalisation performance rely more on deeper layers for memorisation.

## Method Summary
The study fine-tunes 4 transformer models (BERT-base, OPT-125m, Pythia-160m, GPT-Neo-125m) on 12 classification tasks with 15% label perturbation. Four localisation techniques are applied: layer swapping between models trained with original and perturbed labels, layer retraining with weights reset to pre-training, gradient analysis using L1-norm gradients for noisy examples, and probing with MLP classifiers to predict if examples are noisy or clean.

## Key Results
- Memorisation is distributed across layers rather than localized to specific ones
- Early layers are most crucial for memorisation overall, with deeper layers becoming more important for tasks with higher generalisation performance
- Different localisation methods capture different aspects of memorisation and should not be used in isolation
- The depth of memorisation correlates with a model's generalisation ability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memorisation is distributed across layers rather than confined to specific ones.
- Mechanism: When label perturbation is applied, the model gradually adjusts its internal representations to align noisy examples with their new labels. This process begins in early layers and continues progressively deeper, involving multiple layers cooperatively rather than a single layer performing all the memorisation.
- Core assumption: The hidden state transformations across layers collectively enable memorisation, and earlier layers set the foundation for later adjustments.
- Evidence anchors:
  - [abstract] "Our results indicate that memorisation is a gradual process rather than a localised one"
  - [section] "memorisation starts early, and interventions are more successful when conducted before the hidden state has moved too far away from class yb"
  - [corpus] Weak - no direct evidence in corpus, inferred from experimental results
- Break condition: If a single layer were sufficient for memorisation, then swapping/retraining that layer would fully revert the memorisation error, but results show only minor effects from modifying individual layers.

### Mechanism 2
- Claim: Task complexity influences which layers are most important for memorisation.
- Mechanism: Tasks requiring higher-level understanding (like NLI or word sense disambiguation) involve deeper layers in their processing pipeline. Consequently, these tasks also rely more heavily on deeper layers for memorisation. Conversely, simpler tasks like sentiment classification or topic classification primarily use earlier layers for both processing and memorisation.
- Core assumption: The depth at which classification occurs in clean examples predicts where memorisation will occur for noisy examples.
- Evidence anchors:
  - [abstract] "Tasks with higher generalisation performance rely more on deeper layers for memorisation"
  - [section] "tasks with similar classification initiations can still have different crossings" (crossing = first layer where noisy examples align with new class)
  - [corpus] Weak - corpus provides no direct evidence, inferred from experimental observations
- Break condition: If task complexity didn't correlate with memorisation depth, we would see no relationship between classification initiation depth and memorisation error patterns across tasks.

### Mechanism 3
- Claim: Different localisation methods capture different aspects of memorisation and should not be used in isolation.
- Mechanism: Layer swapping/retraining methods identify which layers are redundant for memorisation (i.e., removing them doesn't affect performance), gradient analysis identifies which layers need largest updates to "forget" noisy labels, and probing identifies which layers contain information distinguishing clean from noisy examples. Each method has different sensitivities and failure modes.
- Core assumption: Each method has inherent biases and limitations that affect their reliability in different contexts.
- Evidence anchors:
  - [abstract] "The results do not always align – which underscores that we should not overly rely on one localisation method"
  - [section] "probing and gradient analyses could not accurately pinpoint those two layers" in control setup
  - [corpus] Weak - corpus provides no direct evidence, inferred from experimental comparisons
- Break condition: If one method were sufficient, we would see perfect agreement between all localisation techniques, but results show only moderate correlations between them.

## Foundational Learning

- Concept: Transformer architecture and layer-wise processing
  - Why needed here: The paper investigates memorisation localization across transformer layers, requiring understanding of how information flows and transforms through the network
  - Quick check question: How does a transformer process an input token through its layers, and what changes occur at each layer?

- Concept: Gradient-based analysis and its limitations
  - Why needed here: The paper uses gradient norms to identify layers important for memorisation, requiring understanding of how gradients reflect parameter importance and why they might be unreliable
  - Quick check question: What information do gradients provide about parameter importance, and what are common failure modes of gradient-based analysis methods?

- Concept: Probing classifiers and representation analysis
  - Why needed here: The paper uses probing classifiers to determine which layers contain information about whether examples are noisy, requiring understanding of how probing works and what it reveals about representations
  - Quick check question: How does a probing classifier work, and what does it tell us about the information content of hidden states at different layers?

## Architecture Onboarding

- Component map: Input → Tokenization → Embedding → Layer processing (12 transformer blocks) → Classification head → Loss computation → Backpropagation → Parameter updates. For localisation: swap/retrain specific layers → Measure performance change on noisy examples.

- Critical path: Data → Tokenization → Embedding → Layer processing (12 blocks) → Classification head → Loss computation → Backpropagation → Parameter updates. For localisation: swap/retrain specific layers → Measure performance change on noisy examples.

- Design tradeoffs: The choice of 12-layer models balances computational tractability with sufficient depth for meaningful layer-wise analysis. Label perturbation (15%) provides clear signal for memorisation but may not reflect natural memorisation patterns. Multiple localisation methods provide complementary views but require careful interpretation due to their different failure modes.

- Failure signatures: Individual layer modifications having minimal effect (indicating distributed memorisation), gradient analysis failing to identify correct layers in control setup, probing performance varying widely across tasks and models, and inconsistent results across localisation methods.

- First 3 experiments:
  1. Implement layer swapping for a single dataset (e.g., RTE) to verify that modifying individual layers has minimal effect on memorisation error
  2. Run gradient analysis on the same dataset to compare results with layer swapping and identify any discrepancies
  3. Apply probing to determine which layers contain information distinguishing clean from noisy examples for the same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of memorisation (e.g., factual vs. verbatim vs. noise-based) compare in terms of layer localization across transformer architectures?
- Basis in paper: [explicit] The paper notes that previous work on localisation has focused on different types of memorisation (facts, verbatim text, noise), yielding conflicting results about which layers are most important.
- Why unresolved: This study only examines one type of memorisation (noise-based through label perturbation), while acknowledging that different types might behave differently.
- What evidence would resolve it: Comparative studies applying the same localisation techniques to models trained with different types of memorisation tasks (facts, verbatim sequences, noise labels) would clarify whether localisation patterns are consistent across memorisation types.

### Open Question 2
- Question: What architectural or training factors determine whether memorisation is more localized to early vs. late layers in transformer models?
- Basis in paper: [explicit] The paper observes that tasks with better generalisation performance rely more on deeper layers for memorisation, suggesting task properties influence memorisation localization.
- Why unresolved: While the study identifies correlations between task difficulty/generalisation and memorisation depth, it doesn't establish causal factors or explain why certain tasks shift memorisation to different layers.
- What evidence would resolve it: Systematic experiments varying model architecture (depth, width), training procedures, and task properties (complexity, label set size) while measuring memorisation localization could identify the key determinants.

### Open Question 3
- Question: Can we develop more reliable localisation techniques that accurately pinpoint memorisation to specific layers rather than identifying layers that are not crucial?
- Basis in paper: [explicit] The paper's control experiments show that layer swapping and retraining are good at identifying which layers are not crucial for memorisation, but less reliable at identifying which layers are most important.
- Why unresolved: Current techniques (especially probing and gradients) have low accuracy in the control setup, and even the most reliable techniques (swapping/retraining) have limitations in positively identifying crucial layers.
- What evidence would resolve it: Development and validation of new localisation methods (e.g., combining multiple signals, using causal interventions, or leveraging interpretability techniques) that demonstrate high accuracy in controlled settings where ground truth is known.

## Limitations

- Artificial memorization setup through label perturbation may not reflect natural memorization patterns in real-world scenarios
- Focus on 12-layer transformer models may not capture memorization patterns in larger or smaller architectures
- Reliance on controlled laboratory conditions rather than real-world deployment scenarios

## Confidence

**High Confidence**: The finding that memorization is distributed across layers rather than localized to specific ones is well-supported by consistent results across multiple localization methods and control experiments.

**Medium Confidence**: The relationship between task complexity and memorization depth shows consistent patterns across datasets, though individual task variations suggest this may not be a universal rule.

**Low Confidence**: The specific mechanisms by which earlier layers set the foundation for later memorization adjustments remain largely speculative.

## Next Checks

1. Test the layer-wise memorization patterns in larger transformer architectures (e.g., BERT-large, GPT-3) to verify whether the gradual, distributed memorization pattern holds across different model scales.

2. Apply the localization techniques to models trained on naturally occurring noisy datasets (e.g., web-scraped data with inherent label errors) rather than artificially perturbed labels.

3. Conduct finer-grained analysis of the memorization process during training by examining memorization progression at multiple checkpoints rather than just final epochs.