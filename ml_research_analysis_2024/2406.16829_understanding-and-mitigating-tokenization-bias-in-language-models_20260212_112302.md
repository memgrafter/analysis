---
ver: rpa2
title: Understanding and Mitigating Tokenization Bias in Language Models
arxiv_id: '2406.16829'
source_url: https://arxiv.org/abs/2406.16829
tags:
- encode
- tokens
- token
- algorithm
- prefix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies a fundamental bias introduced by tokenization\
  \ in language models: when a model is trained on subword units, its predictions\
  \ for the next character can differ from the true character-level distribution,\
  \ even under optimal training. This bias stems from the tokenization algorithm\u2019\
  s implicit constraints on which token sequences are valid."
---

# Understanding and Mitigating Tokenization Bias in Language Models

## Quick Facts
- **arXiv ID:** 2406.16829
- **Source URL:** https://arxiv.org/abs/2406.16829
- **Reference count:** 40
- **Primary result:** This paper identifies a fundamental bias introduced by tokenization in language models and proposes algorithms to correct it.

## Executive Summary
This paper identifies a fundamental bias introduced by tokenization in language models: when a model is trained on subword units, its predictions for the next character can differ from the true character-level distribution, even under optimal training. This bias stems from the tokenization algorithm's implicit constraints on which token sequences are valid. To address this, the authors propose the Maximum Prefix Correction (MPC) algorithm, which recursively computes unbiased estimates of next-character probabilities by considering all valid tokenizations that could produce a given string. They prove correctness and demonstrate that MPC accurately recovers the true transition probabilities in a Markov chain experiment, while the standard approach fails.

## Method Summary
The authors train a GPT-2 model on tokenized versions of a synthetic 3rd order Markov chain data, then implement and run the Maximum Prefix Correction (MPC) algorithm to compute unbiased character-level probabilities from the trained tokenized model. For Byte-Pair Encoding (BPE), they implement the Byte-Pair Correction (BPC) algorithm. Both methods involve recursive computation that considers all valid tokenizations. The core idea is to marginalize over all tokens that could validly follow a given prefix, then recursively handle cases where the next character spans multiple tokens by extracting the first token and continuing with the remainder.

## Key Results
- The Maximum Prefix Correction (MPC) algorithm accurately recovers the true transition probabilities in a Markov chain experiment, while the standard approach fails
- The MPC and BPC algorithms do not require fine-tuning and scale linearly with sequence length
- The work shows that token-free behavior can be simulated from a tokenized model, suggesting that language models implicitly learn character-level information despite being trained on tokenized data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokenization introduces a sampling bias that cannot be removed by more training or data because the conditioning context shifts from characters to tokens.
- Mechanism: When a string is tokenized using longest prefix matching, some valid next-character sequences become unreachable because their required token sequences are invalid under the vocabulary constraints. This creates zero-probability events that persist even with optimal training.
- Core assumption: The tokenization algorithm enforces a specific mapping from strings to token sequences that is not invertible at the character level.
- Evidence anchors:
  - [abstract] "we show that popular encoding schemes, such as maximum prefix encoding (MPE) and byte-pair-encoding (BPE), induce a sampling bias that cannot be mitigated with more training or data."
  - [section] "This bias stems from the tokenization algorithm's implicit constraints on which token sequences are valid."
  - [corpus] Weak - related work focuses on performance impacts rather than theoretical bias mechanisms.
- Break condition: If the vocabulary is modified to include all possible substrings or if a different encoding scheme that preserves character-level transitions is used.

### Mechanism 2
- Claim: The Maximum Prefix Correction (MPC) algorithm recovers unbiased character-level probabilities by recursively considering all valid tokenizations that could produce a given string.
- Mechanism: MPC marginalizes over all tokens that could validly follow a given prefix, then recursively handles cases where the next character spans multiple tokens by extracting the first token and continuing with the remainder.
- Core assumption: The language model's output probabilities can be combined through marginalization to recover the true character-level distribution.
- Evidence anchors:
  - [abstract] "propose the Maximum Prefix Correction (MPC) algorithm, which recursively computes unbiased estimates of next-character probabilities by considering all valid tokenizations that could produce a given string."
  - [section] "Our method consists of two stages. In the first stage, the idea is to identify the condition when P(xN_{n+1}|t_i) = P(xN_{n+1}|x_n)."
  - [corpus] Weak - no direct corpus evidence for MPC correctness, only theoretical claims.
- Break condition: If the language model does not follow the assumed probability structure or if the vocabulary is too large for practical computation.

### Mechanism 3
- Claim: Token-free behavior can be simulated from a tokenized model because the model implicitly learns character-level information despite being trained on tokenized data.
- Mechanism: By applying the bias correction algorithms (MPC for MPE, BPC for BPE), the model's output can be transformed to match what a character-level model would produce, effectively recovering the underlying character-level understanding.
- Core assumption: The tokenized language model learns some representation of character-level dependencies even though it's trained on token-level data.
- Evidence anchors:
  - [abstract] "As a result, we show that one can simulate token-free behavior from a tokenized language model."
  - [section] "This result implies that it is theoretically possible to simulate the behavior of another language model trained using different vocabulary without any fine-tuning."
  - [corpus] Moderate - related work on tokenization impacts suggests models learn beyond pure token-level patterns.
- Break condition: If the model is trained with techniques that explicitly prevent character-level learning or if the vocabulary is too restrictive.

## Foundational Learning

- Concept: Autoregressive language models and conditional probability
  - Why needed here: The entire bias correction framework relies on understanding how autoregressive models predict the next token given previous context, and how this relates to character-level predictions.
  - Quick check question: In an autoregressive model, what is the relationship between P(x_{n+1}|x_n) and P(t_{i+1}|t_i) when tokenization is involved?

- Concept: Tokenization algorithms (MPE, BPE) and valid/invalid encodings
  - Why needed here: The bias arises specifically from how these algorithms map strings to token sequences, and the MPC/BPC algorithms work by identifying and handling invalid encodings.
  - Quick check question: What makes an encoding "invalid" under MPE or BPE, and why does this create sampling bias?

- Concept: Marginalization and recursive probability computation
  - Why needed here: The MPC and BPC algorithms use marginalization to sum over all valid tokenizations, and recursion to handle cases where characters span multiple tokens.
  - Quick check question: How does marginalization over token sequences help recover the true character-level probability distribution?

## Architecture Onboarding

- Component map: Tokenizer -> Language model -> Bias correction module -> Output layer

- Critical path:
  1. Input string is tokenized
  2. Tokenized sequence is fed to language model
  3. Raw token-level probabilities are obtained
  4. Bias correction algorithm processes probabilities
  5. Corrected character-level probabilities are output

- Design tradeoffs:
  - Computational cost vs. accuracy: MPC and BPC scale linearly with sequence length but may become expensive for very long sequences
  - Vocabulary size vs. bias: Larger vocabularies reduce but don't eliminate bias
  - Tokenization method vs. correction complexity: MPE allows simpler MPC, while BPE requires more complex BPC

- Failure signatures:
  - Zero-probability outputs for valid next characters
  - Incorrect character-level probabilities compared to ground truth
  - Algorithm timeouts or memory issues for long sequences
  - Inconsistent results between different tokenization schemes

- First 3 experiments:
  1. Verify the bias exists: Compare character-level vs token-level probabilities on a simple Markov chain with known ground truth
  2. Test MPC correctness: Apply MPC to the same Markov chain and verify recovery of ground truth probabilities
  3. Benchmark performance: Measure runtime and memory usage of MPC/BPC on sequences of increasing length

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the MPC algorithm be extended to handle tokenizations beyond MPE and BPE, such as SentencePiece or WordPiece?
- Basis in paper: [explicit] The paper demonstrates MPC for MPE and BPC for BPE, but acknowledges the need for general algorithms applicable to other tokenization schemes.
- Why unresolved: The current algorithms are tailored to specific tokenization schemes. Extending them requires understanding the unique properties of other algorithms like SentencePiece or WordPiece.
- What evidence would resolve it: Development and validation of a generalized algorithm that works across multiple tokenization schemes, with formal proofs of correctness and complexity analysis.

### Open Question 2
- Question: How does tokenization bias affect the performance of language models in downstream tasks like translation or summarization?
- Basis in paper: [inferred] The paper identifies tokenization bias but does not explore its impact on specific tasks. It suggests that bias might lead to suboptimal performance in tasks sensitive to morphological structure or domain-specific vocabulary.
- Why unresolved: The paper focuses on theoretical aspects of tokenization bias and its correction. Empirical studies linking bias to task performance are needed to understand practical implications.
- What evidence would resolve it: Comparative experiments evaluating task performance with and without bias correction, measuring metrics like BLEU score for translation or ROUGE for summarization.

### Open Question 3
- Question: What is the relationship between the order of the Markov chain and the effectiveness of bias correction algorithms?
- Basis in paper: [explicit] The paper validates the MPC algorithm on a 3rd-order Markov chain but does not explore higher-order chains or their impact on bias correction.
- Why unresolved: The complexity and behavior of bias correction might change with higher-order dependencies in the data. Understanding this relationship is crucial for applying the method to real-world language models.
- What evidence would resolve it: Experiments on Markov chains of varying orders, analyzing the accuracy of bias correction and the computational complexity of the MPC algorithm as a function of chain order.

## Limitations

- The MPC and BPC algorithms have not been validated on real-world language modeling tasks beyond synthetic experiments
- The computational complexity may become prohibitive for longer sequences or larger vocabularies, though theoretical linear scaling is claimed
- The assumption that tokenized language models implicitly learn character-level information may not hold for all architectures or training approaches

## Confidence

**High Confidence**: The existence of tokenization bias and its fundamental nature (cannot be resolved by more training or data). The proof that MPC correctly recovers unbiased character-level probabilities in the Markov chain experiment is rigorous and well-demonstrated.

**Medium Confidence**: The claim that one can simulate token-free behavior from tokenized models. While the theory is sound and the synthetic experiment supports this, real-world validation on actual language modeling tasks is needed to confirm this generalizes beyond controlled experiments.

**Low Confidence**: The practical scalability of MPC and BPC algorithms for real-world applications. The paper provides theoretical complexity analysis but lacks empirical validation on longer sequences or larger vocabularies that would be encountered in production settings.

## Next Checks

1. **Real-world Language Modeling Task**: Apply MPC/BPC to a pre-trained language model (e.g., GPT-2) on a standard language modeling benchmark like WikiText or LM1B, comparing character-level perplexity before and after correction. This would validate whether the theoretical benefits translate to practical performance gains.

2. **Scalability Analysis**: Systematically measure the runtime and memory usage of MPC/BPC algorithms on sequences of varying lengths (100, 1000, 10000 tokens) and vocabularies of different sizes. This would identify practical limits and help determine when approximation methods might be necessary.

3. **Cross-Architecture Generalization**: Test whether the bias correction framework works across different language model architectures (Transformer, LSTM, CNN-based) and training objectives (causal, masked, prefix). This would establish whether the approach is architecture-agnostic or if certain designs are more amenable to bias correction.