---
ver: rpa2
title: 'Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large Language
  Models'
arxiv_id: '2410.20745'
source_url: https://arxiv.org/abs/2410.20745
tags:
- shopping
- product
- tasks
- online
- mmlu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Shopping MMLU, a comprehensive multi-task
  benchmark for evaluating Large Language Models (LLMs) in online shopping scenarios.
  The benchmark consists of 57 tasks covering 4 major shopping skills: concept understanding,
  knowledge reasoning, user behavior alignment, and multi-linguality, derived from
  real-world Amazon data.'
---

# Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2410.20745
- Source URL: https://arxiv.org/abs/2410.20745
- Reference count: 40
- Primary result: Introduces a comprehensive multi-task benchmark for evaluating LLMs in online shopping scenarios, revealing insights about knowledge transfer, task correlations, and instruction fine-tuning effectiveness

## Executive Summary
This paper introduces Shopping MMLU, a comprehensive multi-task benchmark for evaluating Large Language Models (LLMs) in online shopping scenarios. The benchmark consists of 57 tasks covering 4 major shopping skills: concept understanding, knowledge reasoning, user behavior alignment, and multi-linguality, derived from real-world Amazon data. The evaluation includes over 20 existing LLMs, revealing valuable insights about building versatile LLM-based shop assistants. Key findings include: proprietary models like Claude-3 Sonnet remain state-of-the-art but open-source models are catching up; tasks and skills in Shopping MMLU share significant knowledge, indicating potential for joint improvement; general knowledge transfers well to online shopping domain; instruction fine-tuning shows mixed results depending on base model strength; and few-shot learning remains challenging.

## Method Summary
Shopping MMLU is constructed from real-world Amazon data and comprises 57 tasks across four major shopping skills. The benchmark evaluates over 20 existing LLMs using zero-shot prompting with the same prompts to eliminate variance from different few-shot examples. Models are tested across various task types including multiple choice, retrieval, ranking, NER, and generation tasks, with appropriate metrics calculated for each. The evaluation focuses on macro averages of task-wise metrics including accuracy, hit rate@3, NDCG, micro F1, ROUGE-L, BLEU, and sentence transformer similarity. The paper provides dataset access and evaluation code for reproducibility, though exact model checkpoints and detailed data filtering criteria are not fully specified.

## Key Results
- Proprietary models like Claude-3 Sonnet remain state-of-the-art but open-source models are catching up
- Tasks and skills in Shopping MMLU share significant knowledge, indicating potential for joint improvement
- General knowledge transfers well to online shopping domain; strong models on general LLM benchmarks remain strong on Shopping MMLU

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs trained on general domain data transfer well to the specific domain of online shopping
- Mechanism: The diverse nature of online shopping concepts, relations, and tasks overlaps with general knowledge patterns learned during pre-training
- Core assumption: The underlying patterns and relationships in online shopping are not fundamentally different from those in general text
- Evidence anchors:
  - [abstract]: "General knowledge transfers well to online shopping domain; strong models on general LLM benchmarks remain strong on Shopping MMLU"
  - [section]: "General knowledge transfers well to the specific domain of online shopping. Strong models on general LLM benchmarks remain strong on Shopping MMLU"
  - [corpus]: Weak - corpus neighbors are primarily MMLU variants, not shopping-specific benchmarks
- Break condition: If online shopping concepts are too domain-specific and disconnected from general language patterns, transfer would fail

### Mechanism 2
- Claim: Multi-task learning works well for online shopping because tasks share significant knowledge
- Mechanism: The various shopping skills (concept understanding, knowledge reasoning, user behavior alignment, multi-linguality) are interconnected, allowing models to leverage shared representations
- Core assumption: Tasks and skills in online shopping are not isolated but share underlying knowledge structures
- Evidence anchors:
  - [abstract]: "Tasks and skills in Shopping MMLU share significant knowledge, indicating potential for joint improvement"
  - [section]: "the more the shared knowledge, the more likely we can jointly improve all tasks in Shopping MMLU"
  - [corpus]: Weak - corpus neighbors don't provide evidence about task correlation in shopping domains
- Break condition: If shopping tasks are too diverse and lack common underlying patterns, multi-task learning would be ineffective

### Mechanism 3
- Claim: Instruction fine-tuning effectiveness depends on base model strength and task familiarity
- Mechanism: Stronger base models have more general knowledge to retain during fine-tuning, while domain-specific fine-tuning only works on tasks and skills included in the training data
- Core assumption: Fine-tuning can enhance task-specific performance but may also cause catastrophic forgetting of general knowledge
- Evidence anchors:
  - [abstract]: "instruction fine-tuning shows mixed results depending on base model strength"
  - [section]: "general domain IFT may lead to overfitting and hence compromise the contained knowledge in strong base models, while domain-specific IFT works only on strong base models and observed tasks and skills"
  - [corpus]: Weak - corpus neighbors don't provide evidence about instruction fine-tuning effects
- Break condition: If fine-tuning causes severe catastrophic forgetting or if the base model is too weak to benefit from instruction tuning

## Foundational Learning

- Concept: Multi-task learning and knowledge transfer
  - Why needed here: Understanding how LLMs can leverage knowledge across different shopping tasks and domains
  - Quick check question: Why might a model trained on general language tasks perform well on specific shopping tasks?

- Concept: Instruction fine-tuning and catastrophic forgetting
  - Why needed here: Understanding how to effectively adapt LLMs to specific domains without losing general capabilities
  - Quick check question: What happens when you fine-tune a strong model on a small, domain-specific dataset?

- Concept: In-context learning and few-shot adaptation
  - Why needed here: Understanding how LLMs can adapt to new tasks with limited examples, which is crucial for online shopping scenarios
  - Quick check question: Why might adding few-shot examples sometimes hurt LLM performance instead of helping?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Model evaluation -> Analysis pipeline
  Raw Amazon data → task-specific question-answer pairs → multiple choice, retrieval, ranking, NER, generation tasks with appropriate metrics → task-wise and skill-wise correlation analysis, comparison with task-specific methods

- Critical path:
  1. Load and preprocess dataset
  2. Define evaluation prompts for each task type
  3. Run inference on target models
  4. Calculate metrics and aggregate results
  5. Analyze correlations and patterns

- Design tradeoffs:
  - Zero-shot vs few-shot evaluation: Zero-shot is more realistic but may underestimate model capabilities
  - Task diversity vs depth: Broad coverage of tasks vs detailed evaluation of specific skills
  - Proprietary vs open-source models: State-of-the-art performance vs accessibility and reproducibility

- Failure signatures:
  - Poor performance on concept understanding tasks: Indicates lack of domain-specific knowledge
  - Low correlations between tasks: Suggests tasks are too diverse for effective multi-task learning
  - Degradation after fine-tuning: Indicates catastrophic forgetting or overfitting

- First 3 experiments:
  1. Run baseline evaluation on a small subset of tasks to verify pipeline functionality
  2. Test correlation analysis on a subset of tasks to ensure metric calculation is correct
  3. Compare a few representative models (e.g., ChatGPT, LLaMA2-70B) across all skills to validate comprehensive evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of domain-specific instruction fine-tuning compare to general instruction fine-tuning across different base model sizes and strengths?
- Basis in paper: [explicit] The paper states that domain-specific IFT only works on sufficiently strong base models and observed tasks/skills, while general IFT helps in most cases but less so for stronger base models.
- Why unresolved: The paper provides some comparisons but doesn't fully explore the interaction between base model strength, general vs domain-specific IFT, and their effects across all Shopping MMLU skills.
- What evidence would resolve it: Comprehensive experiments comparing general and domain-specific IFT across various base model sizes and strengths on all Shopping MMLU skills, measuring both absolute performance and improvements over base models.

### Open Question 2
- Question: What is the optimal approach for few-shot learning in domain-specific tasks like online shopping, considering the mixed results with both standard and chain-of-thought prompting?
- Basis in paper: [explicit] The paper shows that few-shot learning remains challenging on Shopping MMLU, with in-context examples leading to worse performances for many models and tasks, though CoT helps with numeric reasoning.
- Why unresolved: The paper only explores basic few-shot prompting and CoT, without investigating other few-shot techniques like self-consistency, retrieval-augmented prompting, or adaptive example selection.
- What evidence would resolve it: Systematic evaluation of various few-shot learning techniques on Shopping MMLU tasks, comparing their effectiveness and identifying conditions under which each approach works best.

### Open Question 3
- Question: How transferable are the insights from Shopping MMLU to other domain-specific LLM development efforts, and what are the key factors that determine successful knowledge transfer?
- Basis in paper: [explicit] The paper discusses how online shopping characteristics (domain-specific concepts, implicit knowledge, human behaviors, multi-linguality) exist in other user-oriented services and may benefit domain-specific LLM development in other fields.
- Why unresolved: While the paper suggests broader applicability, it doesn't empirically test the transferability of Shopping MMLU insights to other domains or identify what makes certain domains more amenable to similar approaches.
- What evidence would resolve it: Experiments applying Shopping MMLU methodologies and findings to other domain-specific benchmarks, measuring performance gains and identifying which characteristics are most predictive of successful knowledge transfer.

## Limitations

- Data Source Bias: The benchmark is constructed from Amazon data, which may not generalize to other shopping platforms or cultural contexts.
- Evaluation Methodology Gaps: Zero-shot evaluation may underestimate models' true capabilities, and few-shot learning remains challenging without systematic exploration of optimal strategies.
- Reproducibility Concerns: Lack of detailed specifications for exact model checkpoints, API versions, and precise data filtering criteria could affect result reproducibility.

## Confidence

- High Confidence: The finding that general knowledge transfers well to online shopping domain is well-supported by consistent performance patterns across multiple strong models on both general and shopping-specific benchmarks.
- Medium Confidence: The claim about instruction fine-tuning showing mixed results depending on base model strength is supported by observed patterns but lacks detailed ablation studies to isolate specific factors driving these differences.
- Low Confidence: The assertion that tasks and skills share significant knowledge is primarily based on correlation analysis without establishing causation or testing whether explicitly modeling these relationships improves performance.

## Next Checks

1. **Cross-Platform Generalization Test**: Evaluate Shopping MMLU performance on models trained primarily on non-Amazon e-commerce data to quantify platform-specific biases and assess generalizability across shopping ecosystems.

2. **Few-Shot Optimization Study**: Systematically vary the number, quality, and format of few-shot examples for each task type to identify optimal few-shot strategies and better understand the "challenging" nature of few-shot learning reported in the paper.

3. **Knowledge Transfer Causality Analysis**: Design experiments that explicitly encourage or discourage knowledge sharing between tasks (e.g., through model architecture or training procedures) to determine whether observed correlations reflect true transferable knowledge or mere statistical artifacts.