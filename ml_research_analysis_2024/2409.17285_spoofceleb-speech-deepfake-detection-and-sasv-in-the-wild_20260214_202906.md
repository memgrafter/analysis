---
ver: rpa2
title: 'SpoofCeleb: Speech Deepfake Detection and SASV In The Wild'
arxiv_id: '2409.17285'
source_url: https://arxiv.org/abs/2409.17285
tags:
- speech
- data
- dataset
- spoofceleb
- sasv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpoofCeleb is the first dataset for Speech Deepfake Detection (SDD)
  and Spoofing-robust Automatic Speaker Verification (SASV) using real-world, noisy
  data as both bona fide and spoofing sources. To enable TTS training on such data,
  a fully automated pipeline processes VoxCeleb1 into TITW-Easy.
---

# SpoofCeleb: Speech Deepfake Detection and SASV In The Wild

## Quick Facts
- arXiv ID: 2409.17285
- Source URL: https://arxiv.org/abs/2409.17285
- Reference count: 40
- First dataset for Speech Deepfake Detection (SDD) and SASV using real-world, noisy data

## Executive Summary
SpoofCeleb is the first dataset designed specifically for Speech Deepfake Detection (SDD) and Spoofing-robust Automatic Speaker Verification (SASV) using real-world, noisy data as both bona fide and spoofing sources. The dataset addresses a critical gap in the field by providing in-the-wild data that better represents real-world conditions compared to controlled laboratory settings. It contains over 2.5 million utterances from 1,251 speakers, generated using 23 TTS systems trained on VoxCeleb1 data processed through a fully automated pipeline. The dataset includes carefully balanced training, validation, and evaluation sets with controlled protocols to enable rigorous benchmarking of SDD and SASV systems.

## Method Summary
The SpoofCeleb dataset was created through a systematic process that transforms VoxCeleb1 data into a format suitable for TTS training on noisy real-world audio. A fully automated pipeline processes VoxCeleb1 data to create TITW-Easy, which serves as the foundation for training 23 different TTS systems. These systems generate synthetic speech that mimics the characteristics of the original speakers, producing over 2.5 million utterances across 1,251 speakers. The dataset is structured with balanced splits for training, validation, and evaluation, with protocols designed to control for various factors that could influence model performance. This approach ensures that the resulting dataset captures the challenges of real-world speech while maintaining the consistency needed for reliable benchmarking.

## Key Results
- Strong SDD performance with RawNet2 achieving 1.12% EER on evaluation set
- Effective SASV with a-DCF of 0.29 on evaluation set
- Zero-shot models show poor performance, highlighting the need for in-domain training data

## Why This Works (Mechanism)
The dataset's effectiveness stems from its use of real-world, noisy data as both legitimate and spoofed sources, creating a more realistic training environment than synthetic or controlled datasets. By processing VoxCeleb1 through an automated pipeline and training multiple TTS systems, the dataset captures the acoustic variability and quality imperfections present in real-world scenarios. The balanced protocols ensure that models are tested on representative conditions while the large scale (2.5 million utterances) provides sufficient diversity for robust learning. The focus on celebrities from YouTube provides natural speech patterns and conversational dynamics that are difficult to replicate in laboratory settings.

## Foundational Learning
- **TTS System Training**: Why needed - To generate synthetic speech that mimics real speakers for spoofing detection training. Quick check - Verify that 23 different TTS systems are successfully trained and producing diverse outputs.
- **VoxCeleb1 Processing Pipeline**: Why needed - To convert controlled dataset into format suitable for noisy real-world training. Quick check - Confirm automated pipeline maintains speaker identity while adding realistic noise characteristics.
- **Speaker Verification Fundamentals**: Why needed - To understand how spoofing affects authentication systems. Quick check - Ensure baseline systems can distinguish between genuine and synthetic speech before training on SpoofCeleb.
- **In-the-wild Data Characteristics**: Why needed - To capture real-world acoustic variability and quality variations. Quick check - Validate that dataset includes diverse recording conditions, background noise, and quality levels.
- **Balanced Dataset Protocols**: Why needed - To ensure fair evaluation across different conditions and speaker groups. Quick check - Verify training/validation/evaluation splits maintain statistical consistency.

## Architecture Onboarding

**Component Map**: VoxCeleb1 -> TITW-Easy Processing -> TTS Training (23 systems) -> SpoofCeleb Generation -> Training/Validation/Evaluation Splits

**Critical Path**: The pipeline from VoxCeleb1 processing through TTS training to dataset generation represents the core workflow, with each stage building on the previous to create progressively more realistic spoofing scenarios.

**Design Tradeoffs**: The use of automated processing trades some control over individual sample quality for scalability and consistency across the dataset. The focus on celebrity speakers provides rich conversational data but may limit generalization to non-celebrity speakers.

**Failure Signatures**: Models may overfit to specific TTS system characteristics rather than learning generalizable spoofing detection features. Some spoofing attacks may be underrepresented if the automated pipeline struggles with particularly noisy samples.

**First 3 Experiments**:
1. Evaluate baseline SDD performance using standard models (e.g., RawNet2) on SpoofCeleb evaluation set
2. Test SASV robustness using ASV systems trained on SpoofCeleb against both genuine and synthetic speech
3. Compare zero-shot performance of models trained on controlled datasets against those trained on SpoofCeleb

## Open Questions the Paper Calls Out
None

## Limitations
- Automated pipeline may introduce quality variations that affect attack difficulty
- Celebrity-focused dataset may not represent general speaker population
- Evaluation set's controlled nature may not capture full real-world complexity

## Confidence

**High confidence**: Dataset scale (2.5 million utterances from 1,251 speakers) and baseline results (1.12% EER, a-DCF of 0.29) are well-supported by described experiments.

**Medium confidence**: Dataset's uniqueness in addressing in-the-wild gap is reasonable but could benefit from more extensive comparison with existing datasets.

**Low confidence**: Claim about some attacks being less challenging due to training difficulty lacks thorough quantification.

## Next Checks
1. Evaluate model performance on out-of-domain speakers and acoustic conditions not represented in celebrity-focused dataset
2. Analyze quality variations in TTS-generated speech across different attack types to quantify automated pipeline impact
3. Compare SpoofCeleb-trained models against state-of-the-art approaches on other in-the-wild datasets (WildSpoof, AUDETER) for cross-dataset validation