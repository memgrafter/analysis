---
ver: rpa2
title: 'Video-Text Dataset Construction from Multi-AI Feedback: Promoting Weak-to-Strong
  Preference Learning for Video Large Language Models'
arxiv_id: '2411.16201'
source_url: https://arxiv.org/abs/2411.16201
tags:
- score
- preference
- mllms
- video
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the scarcity of high-quality video-text preference
  data for multimodal large language models (MLLMs). The authors propose a pipeline
  that samples responses from a diverse set of MLLMs and uses an external scoring
  function to construct high-quality preference pairs, forming the MMAIP-V dataset.
---

# Video-Text Dataset Construction from Multi-AI Feedback: Promoting Weak-to-Strong Preference Learning for Video Large Language Models

## Quick Facts
- arXiv ID: 2411.16201
- Source URL: https://arxiv.org/abs/2411.16201
- Authors: Hao Yi; Qingyang Li; Yulan Hu; Fuzheng Zhang; Di Zhang; Yong Liu
- Reference count: 40
- This paper tackles the scarcity of high-quality video-text preference data for multimodal large language models (MLLMs) by proposing a pipeline that samples responses from a diverse set of MLLMs and uses an external scoring function to construct high-quality preference pairs, forming the MMAIP-V dataset.

## Executive Summary
This paper addresses the critical challenge of limited high-quality video-text preference data for training multimodal large language models (MLLMs). The authors propose a comprehensive pipeline that leverages a diverse zoo of MLLMs to generate responses, uses an external scoring function to evaluate quality, and constructs preference pairs for training. To maximize the utility of this data, they introduce Iter-W2S-RLAIF, an iterative weak-to-strong reinforcement learning framework with parameter extrapolation. The approach significantly improves VQA alignment performance on both in-domain and out-of-domain benchmarks, achieving state-of-the-art results.

## Method Summary
The method consists of three main components: (1) MMAIP-V dataset construction using a diverse MLLMs zoo and external scoring function, (2) Iter-W2S-RLAIF training framework that iteratively updates reference models and performs parameter extrapolation, and (3) a vision-based evaluation scheme that reduces bias by incorporating video information. The pipeline samples responses from multiple well-aligned MLLMs, evaluates them using GPT-4o, constructs preference pairs, and trains models using iterative DPO with parameter extrapolation to fully exploit the preference data.

## Key Results
- MMAIP-V dataset significantly improves VQA alignment performance compared to existing datasets
- Iter-W2S-RLAIF achieves state-of-the-art performance on both in-domain and out-of-domain benchmarks
- The method increases average scores by up to 0.3 points and ratio improvements of over 4%
- Vision-based evaluation scheme provides more reliable assessment by incorporating video information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using multiple AI models with different capabilities improves response diversity and quality for preference learning.
- Mechanism: The pipeline samples responses from a diverse MLLMs zoo and uses an external scoring function to select high-quality preferred pairs.
- Core assumption: Different MLLMs have complementary strengths and weaknesses, and their combined responses provide better coverage of the response space.
- Evidence anchors:
  - [abstract] "sample from the response distribution set of well-aligned MLLMs and utilize fine-grained external scoring functions to evaluate response quality"
  - [section 3.1] "we sample from the response distribution set of well-aligned MLLMs and utilize fine-grained external scoring functions to evaluate response quality"
  - [corpus] "Disentangling Length Bias In Preference Learning Via Response-Conditioned Modeling" - shows importance of diverse response sampling for preference learning
- Break condition: If all MLLMs in the zoo have similar architectures/capabilities, the diversity benefit disappears.

### Mechanism 2
- Claim: Iterative weak-to-strong reinforcement learning with parameter extrapolation improves alignment capabilities.
- Mechanism: The framework iteratively updates the reference model and performs parameter extrapolation to fully exploit preference data.
- Core assumption: A stronger reference policy enhances DPO performance when the reference policy is compatible.
- Evidence anchors:
  - [section 4.1] "This method can explore more parameters space by updating the reference model πref iteratively"
  - [section 4.2] "parameters extrapolation methods have yet to be effectively validated for improving the alignment capabilities of MLLMs"
  - [corpus] "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation" - shows benefit of iterative approaches
- Break condition: If the reference model becomes too strong relative to the student model, the learning signal may be lost.

### Mechanism 3
- Claim: Vision-based evaluation reduces bias and provides more complete information compared to text-only matching.
- Mechanism: The evaluation scheme incorporates video information and performs fine-grained multi-perspective evaluations.
- Core assumption: Ground truth answers may be incomplete or incorrect, and visual information is necessary for proper evaluation.
- Evidence anchors:
  - [section 5] "We argue that previous evaluations are biased and lack vision information by matching only the ground truth and responses while ignoring vision information"
  - [section 6.4] "This also further confirms the unreliability of groundtruth"
  - [corpus] "Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization" - shows importance of multimodal evaluation
- Break condition: If the vision evaluation model itself is biased or unreliable, the benefits disappear.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Forms the core training algorithm for aligning MLLMs with preference data
  - Quick check question: What is the difference between DPO and traditional RLHF approaches?

- Concept: Reinforcement Learning from AI Feedback (RLAIF)
  - Why needed here: Provides the framework for using AI-generated preferences instead of human preferences
  - Quick check question: How does RLAIF differ from RLHF in terms of data collection and evaluation?

- Concept: Parameters extrapolation
  - Why needed here: Enhances model alignment capabilities without additional training
  - Quick check question: What is the mathematical formula for parameter extrapolation in this context?

## Architecture Onboarding

- Component map: MLLMs zoo → response sampling → external scoring → pair construction → Iterative DPO + parameter extrapolation → Vision-based evaluation
- Critical path: Data construction → Training → Evaluation
- Design tradeoffs:
  - Using multiple MLLMs increases diversity but also computational cost
  - Iterative training improves alignment but requires more training time
  - Vision-based evaluation is more comprehensive but requires more complex infrastructure
- Failure signatures:
  - Low diversity in negative responses indicates poor sampling strategy
  - Plateauing performance in iterative training suggests reference model is too strong
  - Inconsistent evaluation scores suggest vision model bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MMAIP-V compare to human-annotated VQA preference datasets in terms of model alignment quality?
- Basis in paper: [inferred] The paper mentions that manual annotation is costly and unreliable, suggesting MMAIP-V is a better alternative, but does not directly compare its performance to human-annotated data.
- Why unresolved: The paper focuses on demonstrating the effectiveness of MMAIP-V over existing AI-generated datasets but does not include a direct comparison with human-annotated datasets.
- What evidence would resolve it: A comparative study evaluating the alignment quality of models trained on MMAIP-V versus those trained on human-annotated datasets, using the same evaluation metrics and benchmarks.

### Open Question 2
- Question: What is the impact of varying the number of frames extracted from videos on the performance of Iter-W2S-RLAIF?
- Basis in paper: [explicit] The paper mentions extracting 8 frames uniformly from videos but does not explore the impact of different frame extraction strategies.
- Why unresolved: The paper does not experiment with different numbers of frames or different extraction strategies, leaving the optimal configuration unclear.
- What evidence would resolve it: Experiments varying the number of frames (e.g., 4, 8, 12) and extraction strategies (e.g., uniform, key-frame based) to determine their effect on model performance.

### Open Question 3
- Question: How does the choice of scoring function (e.g., GPT-4o) affect the quality and diversity of the generated VQA preference pairs in MMAIP-V?
- Basis in paper: [explicit] The paper uses GPT-4o as the scoring function but acknowledges the lack of a trained reliable reward model, implying that the choice of scoring function could impact results.
- Why unresolved: The paper does not explore alternative scoring functions or validate the robustness of GPT-4o's scoring against other methods.
- What evidence would resolve it: Comparative experiments using different scoring functions (e.g., human annotators, other LLMs) to assess their impact on the quality and diversity of the generated preference pairs.

## Limitations

- The evaluation primarily focuses on in-domain benchmarks with limited validation on truly out-of-distribution data
- The vision-based evaluation scheme introduces new potential biases from the vision model itself
- The computational overhead of maintaining a diverse MLLMs zoo and running iterative training is not thoroughly quantified

## Confidence

- **High Confidence**: The mechanism of using external scoring functions for preference pair construction (Section 3.1) is well-established in preference learning literature and the paper provides clear implementation details.
- **Medium Confidence**: The iterative weak-to-strong framework with parameter extrapolation shows promise, but the paper lacks ablation studies isolating the individual contributions of iteration vs. extrapolation.
- **Medium Confidence**: The vision-based evaluation scheme addresses important limitations of text-only matching, but the specific vision model used and its potential biases are not thoroughly examined.

## Next Checks

1. Conduct ablation studies isolating the effects of (a) MLLMs zoo diversity, (b) iterative training, and (c) parameter extrapolation on final performance metrics.

2. Test the trained models on video-text datasets from different domains or created with different annotation protocols to assess true out-of-distribution performance.

3. Evaluate the sensitivity of results to different vision models in the evaluation pipeline, including testing with vision models of varying capabilities and potential biases.