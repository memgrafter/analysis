---
ver: rpa2
title: Towards Certification of Uncertainty Calibration under Adversarial Attacks
arxiv_id: '2405.13922'
source_url: https://arxiv.org/abs/2405.13922
tags:
- calibration
- certified
- acce
- confidence
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of neural network calibration
  to adversarial attacks, demonstrating that attacks can severely degrade calibration
  metrics like the Brier score and expected calibration error while leaving accuracy
  unchanged. The authors introduce certified calibration, providing worst-case bounds
  on these metrics under adversarial perturbations.
---

# Towards Certification of Uncertainty Calibration under Adversarial Attacks

## Quick Facts
- arXiv ID: 2405.13922
- Source URL: https://arxiv.org/abs/2405.13922
- Reference count: 40
- Key outcome: This paper addresses the vulnerability of neural network calibration to adversarial attacks, demonstrating that attacks can severely degrade calibration metrics like the Brier score and expected calibration error while leaving accuracy unchanged. The authors introduce certified calibration, providing worst-case bounds on these metrics under adversarial perturbations. They develop analytic bounds for the Brier score and approximate bounds for the expected calibration error using a mixed-integer program solved via ADMM. Additionally, they propose adversarial calibration training (ACT) methods that incorporate calibration adversaries into training, significantly improving certified calibration. Experiments on CIFAR-10 and ImageNet show ACT reduces approximate certified calibration error by up to 12% and certified Brier scores by up to 11% without harming accuracy, particularly for larger certified radii. The proposed methods enable reliable uncertainty estimation in safety-critical applications under adversarial threats.

## Executive Summary
This paper addresses the critical vulnerability of neural network calibration to adversarial attacks, demonstrating that calibration metrics like the Brier score and expected calibration error (ECE) can be severely degraded while accuracy remains unchanged. The authors introduce certified calibration as a framework for providing provable worst-case bounds on calibration under adversarial perturbations. They develop both analytical bounds for the Brier score and approximate bounds for ECE via mixed-integer programming, and propose adversarial calibration training (ACT) methods that significantly improve certified calibration performance without harming accuracy.

## Method Summary
The authors propose a framework for certifying and improving neural network calibration under adversarial attacks. They first develop certified confidence bounds using Gaussian smoothing, then compute worst-case bounds on calibration metrics: analytical bounds for the Brier score and approximate bounds for ECE via a mixed-integer program solved using ADMM. To improve certified calibration, they introduce adversarial calibration training (ACT) that incorporates calibration adversaries into training, minimizing cross-entropy loss under attacks that maximize calibration metrics. Experiments on CIFAR-10 and ImageNet demonstrate that ACT significantly improves certified calibration without harming accuracy, particularly for larger certified radii.

## Key Results
- Adversarial attacks can degrade calibration metrics (Brier score, ECE) by up to 100% while leaving accuracy unchanged
- ACT reduces approximate certified calibration error by up to 12% and certified Brier scores by up to 11% compared to baseline adversarial training
- The proposed methods maintain accuracy while improving certified calibration, particularly for larger certified radii
- ACCE certification via MIP is computationally expensive but provides tighter bounds than CBS in many cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial attacks can significantly degrade calibration metrics like ECE and Brier score while leaving accuracy unchanged.
- Mechanism: By targeting confidence scores directly through (η,ω)-ACE attacks, adversaries increase the gap between predicted confidence and empirical accuracy, maximizing calibration error estimators without altering predicted labels.
- Core assumption: The calibration error estimator is sensitive to the distribution of confidence scores across bins, and adversaries can exploit this by shifting confidences into bins where the accuracy-confidence mismatch is largest.
- Evidence anchors:
  - [abstract] "We show that attacks can significantly harm calibration, and thus propose certified calibration as worst-case bounds on calibration under adversarial perturbations."
  - [section 2.2] "Galil and El-Yaniv (2021) show that such attacks can cause significant harm in applications where confidence scores are used in decision processes at a sample level, e.g., for risk estimation and credit lending."
- Break condition: If confidence certificates become too tight or binning schemes are adaptive to adversarial shifts, the attack surface for increasing calibration error may diminish.

### Mechanism 2
- Claim: Certified calibration provides provable worst-case bounds on calibration under adversarial perturbations using analytical and approximate methods.
- Mechanism: The CBS is maximized analytically by shifting confidences to the bounds that maximize the squared error between confidence and correctness, while ACCE is approximated via a mixed-integer program that jointly optimizes bin assignments and confidence values under certification constraints.
- Core assumption: Gaussian smoothing provides valid certificates on both prediction invariance (C1) and confidence bounds (C2), and these certificates are tight enough to enable worst-case calibration bounds.
- Evidence anchors:
  - [abstract] "Specifically, we produce analytic bounds for the Brier score and approximate bounds via the solution of a mixed-integer program on the expected calibration error."
  - [section 3.1] "Theorem 3.3. Let l, u be the bounds onz, and z be the output of a certified classifier. Further, let c ∈ RN be the indicator that predictions are correct. The upper bound on the Brier score is given by: maxl≤z≤u TLBS(z, c) = 1/N ∥c − l c − u (1 − c) ∥2 2"
- Break condition: If certificate tightness degrades significantly with larger perturbation radii, or if the MIP becomes intractable for large-scale problems, the practical utility of certified calibration bounds may be limited.

### Mechanism 3
- Claim: Adversarial calibration training (ACT) improves certified calibration by incorporating calibration adversaries into training.
- Mechanism: ACT minimizes the cross-entropy loss under adversaries that maximize calibration metrics (Brier score or ACCE), effectively training models to be robust to confidence shifts that would otherwise degrade calibration.
- Core assumption: Training on calibration adversaries generalizes to improve certified calibration performance on unseen data, and the trade-off between accuracy and calibration can be managed through careful hyperparameter selection.
- Evidence anchors:
  - [abstract] "we propose novel calibration attacks and demonstrate how they can improve model calibration through adversarial calibration training."
  - [section 4] "Let LCalibration be a loss on the calibration, and letθ ∈ Θ be the model parameters. We propose the following objective: minθ∈Θ LCE[¯fθ(xn + γ∗n), yn] s.t. γ∗n = arg max∥γn∥2≤ϵ LCalibration"
- Break condition: If calibration adversaries overfit to the training distribution or if the calibration-calibration trade-off becomes too severe, ACT may fail to generalize or harm accuracy.

## Foundational Learning

- Concept: Gaussian smoothing for certified robustness
  - Why needed here: The entire certified calibration framework relies on Gaussian smoothing to obtain certificates on both prediction invariance and confidence bounds.
  - Quick check question: How does Gaussian smoothing provide certified radii for prediction invariance, and what are the key parameters (σ, α) that control certificate tightness?

- Concept: Calibration metrics (ECE, Brier score) and their estimators
  - Why needed here: Understanding how calibration is measured and estimated is crucial for both attacking calibration and defending against such attacks.
  - Quick check question: What is the difference between top-label and multi-label calibration, and how do binning schemes affect ECE estimation?

- Concept: Mixed-integer programming and ADMM for non-convex optimization
  - Why needed here: The ACCE approximation requires solving a non-convex optimization problem over bin assignments and confidence values, which is formulated as a MIP and solved via ADMM.
  - Quick check question: How does the ADMM algorithm handle the non-convexity of the MIP, and what are the key hyperparameters (learning rates, ρ schedules) that affect convergence?

## Architecture Onboarding

- Component map:
  - Data pipeline: Input images → Gaussian smoothing → certified model (¯F, ¯f, ¯z)
  - Certification module: Computes certified radii (Rn) and confidence bounds (l, u) using CDF or Standard certificates
  - Calibration bounds: CBS (analytical) and ACCE (MIP/ADMM) modules that compute worst-case calibration under attack
  - Training module: Standard AT and ACT variants that incorporate calibration adversaries
  - Evaluation module: Computes certified accuracy, ACCE, and CBS on test data

- Critical path:
  1. Train base model with Gaussian smoothing
  2. Obtain certificates on predictions and confidences
  3. Compute certified calibration bounds (CBS, ACCE)
  4. Train with ACT to improve certified calibration
  5. Evaluate certified metrics on test set

- Design tradeoffs:
  - Smoothing parameter σ: Larger σ gives tighter confidence certificates but may harm accuracy
  - Number of bins M: More bins give finer-grained calibration but increase MIP complexity
  - ADMM hyperparameters: Affect convergence speed and solution quality
  - ACT vs AT: ACT improves calibration but may require more compute and careful hyperparameter tuning

- Failure signatures:
  - ADMM not converging: Check constraint satisfaction, adjust ρ scheduling, or try different initializations
  - ACCE much larger than CBS: Indicates potential issues with MIP formulation or solver parameters
  - ACT degrading accuracy: May need to reduce attack strength or adjust weight learning rates

- First 3 experiments:
  1. Verify Gaussian smoothing certificates on a small dataset (e.g., CIFAR-10) and compare Standard vs CDF bounds
  2. Implement the CBS bound computation and validate it against empirical Brier scores under various perturbations
  3. Run the ACCE MIP with ADMM on a small subset of data to understand the optimization dynamics and constraint satisfaction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the trade-off between calibration and accuracy at large certified radii?
- Basis in paper: [explicit] Section K discusses the association between calibration, certified accuracy, and average certified radius, noting that for larger radii, optimal CBS, CA, and ACCE no longer coincide.
- Why unresolved: The paper identifies the phenomenon but does not investigate the underlying causes, mentioning that certified accuracy and ACR benefit from large margins between top and runner-up confidence, which can increase overconfidence and harm calibration.
- What evidence would resolve it: Empirical studies varying smoothing parameters, model architectures, and attack methods to isolate factors contributing to the trade-off. Theoretical analysis linking confidence margins to calibration degradation.

### Open Question 2
- Question: How can the MIP-based ACCE optimization be extended to optimize adaptive binning ECE?
- Basis in paper: [explicit] Section L mentions the capability of the MIP to optimize adaptive binning ECE but does not explore it due to limitations of the baseline against which they compare.
- Why unresolved: The paper uses equal-width binning for the ECE estimator and does not investigate adaptive binning schemes that could potentially yield tighter calibration bounds.
- What evidence would resolve it: Implementing the MIP with adaptive binning schemes and comparing the resulting ACCE bounds to those obtained with equal-width binning. Evaluating the computational cost and practical benefits of adaptive binning.

### Open Question 3
- Question: What is the impact of using different confidence certificates (Standard vs CDF) on the ACCE bounds?
- Basis in paper: [explicit] Section 5.2 mentions using the CDF certificate on confidences instead of the Standard certificate as these are significantly tighter and thus will be used in practice. However, the paper does not investigate the impact of this choice on the ACCE bounds.
- Why unresolved: The paper uses the CDF certificate for the experiments but does not compare the resulting ACCE bounds to those obtained using the Standard certificate.
- What evidence would resolve it: Computing ACCE bounds using both the Standard and CDF certificates for the same models and certified radii. Analyzing the differences in the resulting bounds and their implications for practical deployment.

## Limitations

- Computational complexity: ACCE certification requires solving a mixed-integer program that scales poorly with dataset size and number of bins
- Trade-off between certificate tightness and accuracy: Larger smoothing parameters give tighter certificates but may degrade model accuracy
- Hyperparameter sensitivity: ACT methods require careful tuning to balance calibration improvement against accuracy preservation

## Confidence

**High Confidence**: The CBS analytical bounds are mathematically sound and can be verified directly. The observation that adversarial attacks can degrade calibration while preserving accuracy is empirically demonstrated and theoretically supported.

**Medium Confidence**: The ACCE MIP formulation and ADMM solution are complex, with confidence dependent on proper constraint satisfaction and convergence. The ACT training effectiveness is demonstrated but may require dataset-specific tuning.

**Low Confidence**: The generalization of ACT across different architectures and datasets is not thoroughly explored, and the long-term stability of calibrated models under distribution shift is not addressed.

## Next Checks

1. **Constraint Verification**: Systematically check constraint satisfaction in the ACCE MIP across different problem instances and verify that the ADMM algorithm consistently finds feasible solutions within the certified radii.

2. **Hyperparameter Sensitivity**: Conduct ablation studies on ACT hyperparameters (attack strength, learning rates, ρ scheduling) to establish robust configurations that consistently improve certified calibration without harming accuracy.

3. **Scalability Assessment**: Evaluate the computational scaling of ACCE certification with respect to dataset size and number of bins, and benchmark against alternative certification approaches for large-scale problems.