---
ver: rpa2
title: 'Fredformer: Frequency Debiased Transformer for Time Series Forecasting'
arxiv_id: '2406.09009'
source_url: https://arxiv.org/abs/2406.09009
tags:
- frequency
- time
- forecasting
- series
- components
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Fredformer addresses frequency bias in Transformer-based time
  series forecasting, where models tend to over-focus on high-amplitude low-frequency
  features while neglecting important high-frequency components. The method introduces
  a frequency-debiased Transformer framework that mitigates this bias through three
  key innovations: frequency refinement with local normalization to eliminate proportional
  differences, sub-frequency-independent learning with channel-wise attention to ensure
  equal treatment of all frequency components, and frequency-wise summarization for
  forecasting.'
---

# Fredformer: Frequency Debiased Transformer for Time Series Forecasting

## Quick Facts
- arXiv ID: 2406.09009
- Source URL: https://arxiv.org/abs/2406.09009
- Authors: Xihao Piao; Zheng Chen; Taichi Murayama; Yasuko Matsubara; Yasushi Sakurai
- Reference count: 40
- Primary result: Fredformer achieves 60 top-1 and 20 top-2 rankings out of 80 total cases on 8 real-world time series datasets, addressing frequency bias in Transformer models

## Executive Summary
Fredformer addresses a critical limitation in Transformer-based time series forecasting where models disproportionately focus on high-amplitude low-frequency features while neglecting important high-frequency components. The method introduces a frequency-debiased Transformer framework that mitigates this bias through three key innovations: frequency refinement with local normalization to eliminate proportional differences, sub-frequency-independent learning with channel-wise attention to ensure equal treatment of all frequency components, and frequency-wise summarization for forecasting. Experimental results demonstrate leading performance across eight diverse real-world datasets, with a lightweight variant using Nystr√∂m approximation reducing computational complexity without sacrificing accuracy.

## Method Summary
Fredformer transforms time series data using DFT, segments the frequency spectrum into sub-frequency bands through patching, and normalizes each band independently to eliminate amplitude dominance. Channel-wise attention is then applied within each sub-frequency band to learn cross-channel correlations independently of amplitude differences. The method reconstructs the time-domain signal using IDFT for forecasting. A lightweight variant employs Nystr√∂m approximation for the attention mechanism to reduce computational complexity. The framework is evaluated on eight multivariate time series datasets including Weather, ETT, Electricity, Traffic, and Solar-Energy, using MSE and MAE as evaluation metrics.

## Key Results
- Achieved 60 top-1 and 20 top-2 rankings out of 80 total cases across eight real-world datasets
- Outperformed state-of-the-art baselines including iTransformer, PatchTST, Crossformer, FEDformer, Stationary, and Autoformer
- Lightweight Nystr√∂m approximation variant maintains accuracy while reducing computational costs
- Demonstrated consistent improvement across multiple look-back windows (L=96) and prediction lengths (H‚àà{96,192,336,720})

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local frequency normalization eliminates amplitude dominance by normalizing each frequency band independently, preventing low-frequency components from overwhelming high-frequency ones.
- Mechanism: The method applies patching to segment the frequency spectrum into sub-frequency bands, then normalizes each band independently along the channel axis. This ensures that within each sub-frequency band, the amplitude differences between key frequency components are minimized, promoting equal attention to all key frequencies.
- Core assumption: The proportional differences in amplitude between key frequency components across different frequency bands are the primary cause of frequency bias in Transformer models.
- Evidence anchors:
  - [abstract]: "frequency refinement with local normalization to eliminate proportional differences"
  - [section]: "We further normalize each Wùëõ along the ùëÅ -axis: W‚àóùëõ = ùúé (Wùëõ) ùëõ = 1, 2, . . . , ùëÅ"
  - [corpus]: Weak - corpus papers discuss frequency decomposition but do not specifically address local normalization across sub-frequency bands
- Break condition: If the key frequency components are not consistent across historical and future time series, or if the amplitude differences are not the primary driver of attention bias.

### Mechanism 2
- Claim: Channel-wise attention within each sub-frequency band ensures fair learning of all frequencies by focusing on cross-channel correlations rather than amplitude-based prioritization.
- Mechanism: After local normalization, the method deploys frequency local independent Transformer encoders to learn the importance of each normalized sub-frequency band independently. The attention mechanism computes relationships among channels for each sub-frequency band, ensuring that features of each sub-frequency are calculated independently.
- Core assumption: The Transformer's self-attention mechanism can be directed to focus on cross-channel correlations within frequency bands rather than being influenced by amplitude differences.
- Evidence anchors:
  - [abstract]: "sub-frequency-independent learning with channel-wise attention to ensure equal treatment of all frequency components"
  - [section]: "Channel-wise attention is proposed in the work of [24, 48]. We include these studies as the baselines and the results in Sec. 5.2."
  - [corpus]: Moderate - corpus papers discuss channel-wise correlation and attention mechanisms, supporting the general approach
- Break condition: If cross-channel correlations are not meaningful or if the normalization step fails to properly balance the amplitude differences.

### Mechanism 3
- Claim: Frequency-wise summarization through IDFT reconstruction preserves the learned debiased frequency features while maintaining forecasting accuracy.
- Mechanism: The method summarizes all learned frequency information from the sub-frequency bands using linear projections followed by IDFT to reconstruct the time-domain signal for forecasting. This preserves the debiased representation of key frequency components learned during the channel-wise attention phase.
- Core assumption: The IDFT can accurately reconstruct the time-domain signal from the debiased frequency features without reintroducing frequency bias.
- Evidence anchors:
  - [abstract]: "frequency-wise summarization for forecasting"
  - [section]: "Given the learned features of the sub-frequenciesW‚Ä≤ = {ùíò‚Ä≤1, ùíò‚Ä≤2, . . . ,ùíò‚Ä≤ùëÅ } of the historical time series X, the frequency-wise summarizing operation contains linear projections and IDFT: X‚Ä≤ = IDFT(A‚Ä≤) A‚Ä≤ = Linear(W‚Ä≤)"
  - [corpus]: Weak - corpus papers discuss frequency decomposition but do not specifically address the reconstruction step's role in preserving debiased features
- Break condition: If the IDFT reconstruction introduces artifacts or if the linear projection fails to properly aggregate the sub-frequency features.

## Foundational Learning

- Concept: Discrete Fourier Transform (DFT) and Inverse DFT (IDFT)
  - Why needed here: The method relies on transforming time series data into the frequency domain for processing, then reconstructing it back to the time domain for forecasting.
  - Quick check question: What is the mathematical relationship between time-domain signals and their frequency-domain representations via DFT and IDFT?

- Concept: Self-attention mechanism in Transformers
  - Why needed here: The method uses channel-wise attention within each sub-frequency band to learn dependencies and features across channels.
  - Quick check question: How does the self-attention mechanism compute the weighted sum of values based on the similarity between queries and keys?

- Concept: Frequency decomposition and spectrum analysis
  - Why needed here: The method builds upon frequency decomposition to represent time series and uses spectrum analysis to identify key frequency components.
  - Quick check question: How can frequency decomposition help in identifying important temporal variations in time series data?

## Architecture Onboarding

- Component map:
  DFT backbone -> Frequency refinement with patching -> Local normalization within sub-frequency bands -> Channel-wise attention learning -> Linear projection -> IDFT reconstruction -> Forecasting output

- Critical path:
  1. DFT transformation of input data
  2. Frequency segmentation and local normalization
  3. Channel-wise attention learning
  4. Linear projection and IDFT reconstruction
  5. Forecasting output

- Design tradeoffs:
  - Local normalization vs. global normalization: Local normalization prevents amplitude dominance but may lose some global frequency relationships
  - Channel-wise attention vs. standard attention: Channel-wise attention ensures fair treatment of frequencies but increases computational complexity
  - Patch length selection: Affects granularity of frequency features and model performance

- Failure signatures:
  - If frequency bias persists: Check if local normalization is properly implemented and if key frequency components are correctly identified
  - If forecasting accuracy drops: Verify IDFT reconstruction and linear projection steps
  - If computational costs are too high: Consider the Nystr√∂m approximation variant

- First 3 experiments:
  1. Test with synthetic data containing known frequency components to verify local normalization effectiveness
  2. Compare channel-wise attention vs. standard attention on a simple dataset
  3. Evaluate different patch lengths on a validation set to find optimal granularity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the frequency bias phenomenon generalize to other deep learning architectures beyond Transformers, such as CNNs or RNNs?
- Basis in paper: [inferred] The paper identifies frequency bias as a specific issue in Transformer models where they disproportionately focus on high-amplitude low-frequency features. However, the analysis is limited to Transformers.
- Why unresolved: The authors only empirically analyze and address frequency bias within the Transformer framework, leaving open whether similar biases exist in other architectures.
- What evidence would resolve it: Comparative studies applying Fredformer's methodology to CNNs, RNNs, or other sequence models on time series forecasting tasks, measuring relative error metrics across frequency components.

### Open Question 2
- Question: What is the optimal patching strategy for different types of time series data with varying frequency characteristics?
- Basis in paper: [explicit] The paper uses a non-overlapping patching operation on DFT coefficients with patch length S as a hyperparameter, but only tests limited values (8, 16, 32, 48) on one dataset.
- Why unresolved: The authors demonstrate that patch length affects performance but do not systematically explore how to determine optimal patch sizes for different data characteristics.
- What evidence would resolve it: Empirical studies across diverse datasets with varying frequency distributions, automated methods for determining optimal patch lengths, or theoretical analysis linking patch size to signal properties.

### Open Question 3
- Question: How does the Fredformer framework perform on irregularly sampled or non-stationary time series data?
- Basis in paper: [inferred] All benchmark datasets used have regular sampling intervals and assumed stationarity, as standard DFT assumes uniform sampling and the frequency bias formulation relies on consistent key frequency components.
- Why unresolved: The authors validate their approach on eight regularly-sampled datasets but do not address challenges posed by missing data, irregular sampling, or non-stationary signals where frequency components change over time.
- What evidence would resolve it: Performance comparisons on datasets with irregular sampling patterns, evaluation of Fredformer's robustness to missing data, or extension of the frequency bias formulation to handle non-stationary signals.

## Limitations

- The empirical evidence for frequency bias elimination relies heavily on relative error Œî‚Çñ measurements, but the method's sensitivity to key frequency component selection is not thoroughly explored.
- While the paper demonstrates performance improvements, the ablation studies don't fully isolate whether gains come from frequency debiasing specifically versus general architectural improvements.
- Claims about computational efficiency gains from Nystr√∂m approximation lack supporting complexity analysis and runtime benchmarks.

## Confidence

- **High Confidence**: The core observation that Transformers exhibit frequency bias is well-supported by experimental evidence showing improved performance after debiasing.
- **Medium Confidence**: The three proposed mechanisms work synergistically, but the relative contribution of each component is not rigorously established through comprehensive ablation studies.
- **Low Confidence**: Claims about computational efficiency gains from Nystr√∂m approximation lack supporting complexity analysis and runtime benchmarks.

## Next Checks

1. **Key Frequency Stability Test**: Conduct experiments across datasets with varying temporal patterns to verify that identified key frequency components remain stable and that local normalization consistently eliminates amplitude dominance.

2. **Component Contribution Analysis**: Implement ablation studies that systematically remove each debiasing component (local normalization, channel-wise attention) to quantify their individual contributions to performance improvements.

3. **Computational Complexity Validation**: Measure actual runtime and memory usage for both full Fredformer and Nystr√∂m approximation variants across different dataset sizes, comparing against claimed efficiency improvements.