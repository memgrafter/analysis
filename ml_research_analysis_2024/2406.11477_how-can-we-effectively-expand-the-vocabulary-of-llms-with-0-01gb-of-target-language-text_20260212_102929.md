---
ver: rpa2
title: How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of Target
  Language Text?
arxiv_id: '2406.11477'
source_url: https://arxiv.org/abs/2406.11477
tags:
- language
- vocabulary
- target
- source
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates vocabulary expansion for large language
  models (LLMs) in low-resource settings, aiming to speed up inference and improve
  performance for non-English languages using minimal target language data (~0.01GB,
  30K sentences). Previous work assumed access to large target language datasets,
  but this study explores adaptation with only 30K sentences.
---

# How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of Target Language Text?

## Quick Facts
- **arXiv ID**: 2406.11477
- **Source URL**: https://arxiv.org/abs/2406.11477
- **Reference count**: 40
- **Primary result**: Vocabulary expansion in low-resource settings (0.01GB target data) using heuristic-based embedding initialization and selective fine-tuning achieves faster inference (up to 4.58x) and competitive or improved task performance.

## Executive Summary
This paper investigates vocabulary expansion for large language models (LLMs) in extremely low-resource settings, using only ~0.01GB of target language data (30K sentences). Traditional vocabulary expansion assumes access to large target language datasets, but this study explores adaptation with minimal data. The authors evaluate embedding initialization methods (random, heuristic-based like mean and align, and auxiliary models) and training strategies (fine-tuning top/bottom layers, short sequence lengths, and multi-token prediction objectives) across ten typologically diverse languages and multiple tasks. Results show that heuristic-based initialization methods (Mean, Align) and training strategies like fine-tuning only top and bottom two layers with a short sequence length outperform the widely used full-model fine-tuning with random initialization in low-resource settings. Models adapted with these strategies achieve faster inference while maintaining or even improving task performance compared to the source model, particularly for generation tasks.

## Method Summary
The authors propose a vocabulary expansion approach for LLMs in low-resource settings (0.01GB target data). They evaluate three embedding initialization methods: random initialization, heuristic-based (Mean and Align), and auxiliary models. For training strategies, they explore fine-tuning different model layers (top 2, bottom 2, or all layers), using short sequence lengths, and employing multi-token prediction objectives. The study uses mT5 models (300M-3B parameters) fine-tuned on ten typologically diverse languages across four task types: machine translation, summarization, reading comprehension, and general knowledge. The expanded vocabularies range from 50 to 5,000 tokens, with the best results typically found at 500-1,000 tokens for translation and 100 tokens for summarization.

## Key Results
- Heuristic-based embedding initialization methods (Mean, Align) outperform random initialization in low-resource settings, with Mean initialization showing the best overall performance.
- Fine-tuning only the top and bottom two layers with short sequence lengths achieves faster inference (up to 4.58x speedup) while maintaining or improving task performance compared to full-model fine-tuning.
- Vocabulary expansion with ElChat, a post-hoc training-free method, can recover degraded performance on classification tasks and source language knowledge retention.
- Larger vocabularies generally lead to worse task performance but faster inference up to a certain point, with 500-1,000 tokens recommended for MT and 100 for SUM.

## Why This Works (Mechanism)
The effectiveness of heuristic-based embedding initialization (Mean, Align) stems from leveraging statistical information from the target language data to create more informed initial embeddings compared to random initialization. The Mean method uses average embeddings from the target language corpus, while Align uses cross-lingual alignment techniques. By fine-tuning only the top and bottom layers with short sequences, the model can adapt to the new vocabulary while preserving most of the original knowledge in the middle layers, resulting in faster inference due to reduced computational overhead. The multi-token prediction objective helps the model learn better representations for the expanded vocabulary by predicting multiple tokens simultaneously.

## Foundational Learning

**Tokenization and Vocabulary**
- Why needed: LLMs rely on tokenizers to convert text into discrete units for processing. The vocabulary size directly impacts model capacity and inference speed.
- Quick check: BPE (Byte-Pair Encoding) tokenization is used, which merges frequent character pairs iteratively to build the vocabulary.

**Embedding Initialization**
- Why needed: The quality of initial token embeddings affects how quickly and effectively the model can learn new vocabulary representations during fine-tuning.
- Quick check: Three methods are compared - random initialization (baseline), Mean (average embeddings from target corpus), and Align (cross-lingual alignment).

**Layer-wise Fine-tuning**
- Why needed: Different layers in LLMs capture different types of information, with lower layers typically handling more general linguistic features and higher layers more task-specific information.
- Quick check: Fine-tuning strategies include top 2 layers, bottom 2 layers, or all layers to find the optimal balance between adaptation and knowledge preservation.

## Architecture Onboarding

**Component Map**
Source Model -> Tokenizer -> Embedding Layer -> Encoder/Decoder Layers -> Output Layer -> Target Task

**Critical Path**
Tokenization → Embedding Initialization → Layer-wise Fine-tuning → Inference

**Design Tradeoffs**
- Vocabulary size vs. task performance: Larger vocabularies enable faster inference but may degrade task performance
- Fine-tuning scope vs. knowledge retention: Fine-tuning more layers improves adaptation but risks forgetting source language knowledge
- Training data size vs. effectiveness: Minimal target data (0.01GB) challenges traditional vocabulary expansion approaches

**Failure Signatures**
- Degraded source language performance indicates excessive fine-tuning or poor embedding initialization
- Slow inference despite vocabulary expansion suggests suboptimal vocabulary size or layer selection
- Poor target language task performance indicates insufficient adaptation or inappropriate initialization

**3 First Experiments**
1. Compare random vs. Mean initialization on a low-resource language to establish baseline effectiveness
2. Test top 2 vs. bottom 2 vs. all layers fine-tuning to identify optimal layer selection
3. Evaluate different vocabulary sizes (50-5,000 tokens) to find the sweet spot for inference speed vs. performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal vocabulary size (|Vnew|) for different language families and scripts to maximize inference speed while maintaining competitive performance in low-resource settings?
- Basis in paper: The paper investigates varying vocabulary sizes from 50 to 5,000 tokens and finds that larger vocabularies generally lead to worse task performance but faster inference up to a certain point. It recommends setting |Vnew| between 500-1K for MT and 100 for SUM.
- Why unresolved: The paper only tests a limited set of languages (Burmese, Sinhala, Telugu) and does not explore the relationship between vocabulary size and language family characteristics or script types. The optimal vocabulary size may vary significantly across different language families and scripts.
- What evidence would resolve it: Comprehensive experiments across diverse language families (e.g., Sino-Tibetan, Dravidian, Niger-Congo) and scripts (e.g., logographic, abugida, syllabary) with varying vocabulary sizes would reveal patterns and optimal settings for different language types.

### Open Question 2
- Question: How do different tokenization strategies (e.g., BPE, Unigram, SentencePiece) affect the effectiveness of vocabulary expansion and the resulting inference speedups in low-resource settings?
- Basis in paper: The paper primarily uses BPE-based tokenizers and discusses heuristic-based initialization methods that rely on BPE tokenization. It mentions that Merge assumes the use of a BPE-based tokenizer but does not explore other tokenization strategies.
- Why unresolved: The paper does not compare the performance of vocabulary expansion across different tokenization strategies. Different tokenization algorithms may have varying effects on vocabulary fragmentation, embedding initialization, and ultimately, inference efficiency.
- What evidence would resolve it: Systematic comparison of vocabulary expansion performance using different tokenization strategies (BPE, Unigram, SentencePiece) across multiple languages and tasks would reveal which tokenization approach is most effective for vocabulary expansion in low-resource settings.

### Open Question 3
- Question: What is the long-term impact of vocabulary expansion on model performance and knowledge retention when applied to larger LLMs (e.g., 70B+ parameters) in low-resource settings?
- Basis in paper: The paper acknowledges the limitation of only experimenting with models up to 9B parameters and states that scaling experiments to larger LLMs is a valuable direction for future work to confirm the generalizability of their findings.
- Why unresolved: The paper's findings are based on models up to 9B parameters, and it is unclear whether the same strategies and trade-offs apply to larger LLMs. Larger models may have different training dynamics, knowledge retention capabilities, and sensitivity to vocabulary expansion in low-resource settings.
- What evidence would resolve it: Extensive experiments applying vocabulary expansion to larger LLMs (70B+ parameters) across multiple languages and tasks, measuring both short-term performance and long-term knowledge retention, would reveal whether the strategies scale effectively and maintain their benefits.

## Limitations
- The study focuses on extremely low-resource settings (0.01GB target data), which may limit generalizability to scenarios with moderate or abundant target language data.
- Results are demonstrated on ten typologically diverse languages, but the sample size may not capture all linguistic phenomena, particularly for languages with unique morphological or syntactic properties.
- Inference speed improvements (up to 4.58x) are reported but not independently verified through systematic benchmarking, raising questions about reproducibility.

## Confidence
- **High Confidence**: The superiority of heuristic-based embedding initialization methods (Mean, Align) over random initialization in low-resource settings.
- **Medium Confidence**: The assertion that fine-tuning only top and bottom layers with short sequence lengths outperforms full-model fine-tuning.
- **Low Confidence**: The generalization of inference speed improvements (up to 4.58x) to other model sizes, hardware setups, or real-world deployment scenarios.

## Next Checks
1. Reproduce inference speed benchmarks on different hardware configurations and with larger target language datasets to verify the claimed 4.58x speedup is consistent and not dataset-specific.
2. Conduct ablation studies on the ElChat post-hoc method to determine whether it independently recovers classification performance or if improvements are due to confounding factors like fine-tuning strategies.
3. Test the proposed methods on additional low-resource languages, particularly those with non-Latin scripts or agglutinative morphology, to assess robustness and identify potential failure modes.