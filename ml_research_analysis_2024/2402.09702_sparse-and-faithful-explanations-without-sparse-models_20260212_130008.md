---
ver: rpa2
title: Sparse and Faithful Explanations Without Sparse Models
arxiv_id: '2402.09702'
source_url: https://arxiv.org/abs/2402.09702
tags:
- all-opt
- explanations
- sparse
- optimization
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Sparse Explanation Value (SEV), a novel\
  \ metric for measuring decision sparsity in machine learning models that captures\
  \ how simply individual predictions can be explained using a small number of features.\
  \ Unlike global sparsity metrics, SEV measures the minimum number of feature changes\
  \ needed to alter a prediction, either from a reference point (SEV+) or from a query\
  \ point (SEV\u2212)."
---

# Sparse and Faithful Explanations Without Sparse Models

## Quick Facts
- arXiv ID: 2402.09702
- Source URL: https://arxiv.org/abs/2402.09702
- Authors: Yiyang Sun; Zhi Chen; Vittorio Orlandi; Tong Wang; Cynthia Rudin
- Reference count: 31
- Key outcome: SEV measures how simply individual predictions can be explained using minimal feature changes, showing that existing models can provide sparse explanations without being globally sparse

## Executive Summary
This paper introduces the Sparse Explanation Value (SEV) metric to measure how simply individual predictions from machine learning models can be explained. Unlike traditional sparsity metrics that focus on global model structure, SEV captures the minimum number of feature changes needed to alter a prediction from either a reference point (SEV+) or a query point (SEV-). The authors demonstrate that many existing models already exhibit low SEV values despite not being globally sparse, and they develop optimization algorithms to further reduce SEV while maintaining accuracy.

## Method Summary
The authors propose SEV as a local explanation metric that measures the minimal number of feature changes needed to alter a prediction. They introduce two variants: SEV+ (from reference point to decision boundary) and SEV- (from query point to decision boundary). To optimize SEV, they develop Vol-Opt, which samples points within the cell containing the query and optimizes based on coverage, and All-Opt, which finds decision boundaries between each query and its reference point. These methods aim to reduce SEV values while maintaining model accuracy, with the theoretical guarantee that Vol-Opt monotonically decreases SEV+ across queries.

## Key Results
- Many existing models exhibit low SEV values (often 1-3) despite not being globally sparse
- Vol-Opt and All-Opt can reduce SEV+ to 1 for all queries across various datasets with negligible performance loss
- SEV optimization outperforms existing local explanation methods (LIME, SHAP, DiCE) in both sparsity and interpretability
- The method demonstrates effectiveness on heart disease, German credit, and FICO datasets

## Why This Works (Mechanism)
The approach works by recognizing that local explanation sparsity (measured by SEV) is distinct from global model sparsity. By optimizing for minimal feature changes needed to alter predictions locally, the method captures interpretability at the individual prediction level rather than requiring sparse models globally. The optimization algorithms effectively navigate the decision boundary to find sparse explanations without sacrificing accuracy.

## Foundational Learning
- **Decision boundaries**: Understanding how models partition feature space is crucial for identifying minimal feature changes
  - Why needed: SEV measures distance to decision boundaries
  - Quick check: Verify model has well-defined decision boundaries for classification tasks

- **Local vs global interpretability**: Recognizing the distinction between explaining individual predictions versus entire model behavior
  - Why needed: SEV focuses on local explanation sparsity
  - Quick check: Ensure evaluation considers both local and global interpretability aspects

- **Feature attribution methods**: Understanding how existing methods like LIME and SHAP assign importance to features
  - Why needed: SEV is compared against these baselines
  - Quick check: Review how feature importance is calculated in target datasets

- **Optimization in high-dimensional spaces**: The algorithms must efficiently navigate complex feature spaces
  - Why needed: Finding minimal feature changes requires efficient search
  - Quick check: Verify computational feasibility for dataset dimensionality

## Architecture Onboarding

**Component map**: SEV metric -> Vol-Opt/All-Opt optimization -> Decision boundary analysis -> Accuracy preservation

**Critical path**: Input query → Calculate SEV → Optimize model to reduce SEV → Verify accuracy → Output sparse explanation

**Design tradeoffs**: 
- Local sparsity vs global model simplicity
- Computational cost of optimization vs interpretability gains
- Minimal feature changes vs faithfulness to original model behavior

**Failure signatures**:
- SEV optimization degrades model accuracy
- Computational complexity becomes prohibitive for high-dimensional data
- Explanations become too specific to individual queries to provide general insights

**First experiments**:
1. Calculate SEV values for baseline models on test datasets
2. Apply Vol-Opt optimization and measure SEV reduction
3. Compare optimized models against LIME/SHAP in terms of both sparsity and accuracy

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Computational efficiency at scale remains uncertain, as the paper focuses on moderate-sized datasets without extensive runtime analysis
- Empirical evaluation covers limited model architectures and data types, raising generalizability questions
- Comparison with existing methods focuses primarily on sparsity metrics rather than other interpretability aspects like faithfulness or human understandability

## Confidence
- High confidence in the validity of the SEV metric definition and its theoretical foundation
- Medium confidence in the empirical results showing SEV reduction effectiveness across datasets
- Medium confidence in the comparison with existing local explanation methods given the limited scope of benchmarks
- Low confidence in scalability claims due to lack of extensive computational analysis

## Next Checks
1. Conduct runtime complexity analysis and benchmarking on larger, high-dimensional datasets to verify scalability claims
2. Perform user studies to evaluate whether lower SEV values actually improve human understanding and trust in model decisions
3. Test the SEV optimization methods across a broader range of model architectures, including deep neural networks and ensemble methods, to assess generalizability