---
ver: rpa2
title: A*Net and NBFNet Learn Negative Patterns on Knowledge Graphs
arxiv_id: '2412.05114'
source_url: https://arxiv.org/abs/2412.05114
tags:
- fact
- graph
- rule
- training
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This technical report investigates performance differences between
  a rule-based approach and GNN architectures NBFNet and ANet for knowledge graph
  completion. The key finding is that a substantial fraction of the performance difference
  can be explained by one unique negative pattern on each dataset that is hidden from
  the rule-based approach.
---

# A*Net and NBFNet Learn Negative Patterns on Knowledge Graphs

## Quick Facts
- arXiv ID: 2412.05114
- Source URL: https://arxiv.org/abs/2412.05114
- Reference count: 26
- Key finding: Performance differences between rule-based and GNN approaches can be explained by negative patterns that GNNs exploit but rule-based methods cannot.

## Executive Summary
This technical report investigates why GNN architectures (NBFNet and A*Net) outperform rule-based approaches for knowledge graph completion. The key finding is that GNNs exploit specific negative patterns that rule-based methods cannot express. On WN18RR, the Only-One-Tail rule regarding the hypernym relation allows GNNs to penalize incorrect predictions. On FB15K-237, the Only-One-Link rule across all relations serves the same function. The study demonstrates this through synthetic datasets and perturbation experiments showing that GNNs achieve significant performance improvements by exploiting these hidden negative patterns.

## Method Summary
The study compares rule-based approaches using AnyBURL for rule mining with logistic regression aggregation against GNN architectures NBFNet and A*Net. Both methods are evaluated on WN18RR and FB15K-237 benchmarks. The rule-based approach learns weighted rule combinations, while GNNs perform message passing over the training graph. Performance is measured using filtered MRR and Hits@X metrics. Perturbation experiments are conducted by adding or deleting facts from the training graph to test how each approach responds to structural changes.

## Key Results
- GNNs achieve 9.7 MRR points improvement over rule-based approaches on the zoo dataset
- On WN18RR, GNNs exploit the Only-One-Tail rule while rule-based approaches cannot
- On FB15K-237, GNNs exploit the Only-One-Link rule across all relations
- Adding existence features to rule-based approaches improves performance but doesn't fully close the gap

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GNNs exploit negative patterns by penalizing scores of incorrect facts during message passing.
- Mechanism: The GNNs perform message passing over the training graph during inference. When a candidate entity already has a connection to the source entity via a different relation, the model assigns a lower score, effectively penalizing incorrect predictions.
- Core assumption: The GNNs can access and use structural information from the training graph during inference to identify negative patterns.
- Evidence anchors:
  - [abstract] "Models can achieve a predictive performance advantage by penalizing scores of incorrect facts opposed to providing high scores for correct facts."
  - [section] "Interestingly, although a connection between the source entity and the candidate entity is needed, the pattern is independent of their particular joint path representation."
  - [corpus] Weak - no direct mention of message passing or inference-time access to training graph.
- Break condition: If the GNNs were restricted from accessing training graph information during inference, this mechanism would fail.

### Mechanism 2
- Claim: Rule-based approaches cannot exploit negative patterns due to language bias limitations.
- Mechanism: The rule-based approach uses a specific syntax that only allows positive rules. Negative rules like "if entity follows someone, it cannot follow another" cannot be expressed in this language bias.
- Core assumption: The rule-based approach is constrained by its predefined syntax and cannot learn or express negative patterns.
- Evidence anchors:
  - [section] "The language bias is exactly defined by the allowed rule syntax which does not include negative rules."
  - [section] "While it is possible to add artificial negative facts to the training procedure, we choose the most pragmatic approach that does not involve manipulating the training data."
  - [corpus] Weak - no direct mention of language bias or syntax limitations.
- Break condition: If the rule-based approach could learn or express negative rules, this mechanism would break.

### Mechanism 3
- Claim: The Only-One-Link rule exploits structural bias in benchmark construction.
- Mechanism: The FB15K-237 dataset was constructed such that entities connected by any relation in the training set will never appear together in a fact in the test set. GNNs can exploit this by penalizing candidates with existing connections to the source entity.
- Core assumption: The benchmark construction introduces a structural bias that can be exploited as a negative pattern.
- Evidence anchors:
  - [section] "However, due to the specific construction of the whole dataset, the rule applies from the training set to the test set for all relations."
  - [section] "Two entities e1, e2 that appear in some fact with any relation, e.g., p(e1, e2) âˆˆ Gtrain, will never appear together in a fact in the test set by construction [20]."
  - [corpus] Weak - no direct mention of benchmark construction or structural bias.
- Break condition: If the dataset construction did not introduce this structural bias, this mechanism would fail.

## Foundational Learning

- Concept: Message passing in graph neural networks
  - Why needed here: Understanding how GNNs propagate information through the graph is crucial for grasping how they exploit negative patterns during inference.
  - Quick check question: How does message passing allow a GNN to "remember" connections between entities that exist in the training graph when making predictions?

- Concept: Rule-based knowledge graph completion
  - Why needed here: Understanding the limitations of rule-based approaches (particularly their inability to express negative patterns) is essential for appreciating why GNNs outperform them.
  - Quick check question: What types of rules can traditional rule-based KGC approaches express, and what types are they fundamentally unable to represent?

- Concept: Benchmark construction and its impact on model performance
  - Why needed here: The FB15K-237 results depend heavily on understanding how the dataset was constructed, which creates exploitable patterns.
  - Quick check question: How does the specific construction of FB15K-237 create conditions where a negative pattern (OOL rule) becomes applicable from training to test data?

## Architecture Onboarding

- Component map:
  Knowledge Graph -> Triples (subject, relation, object) with entities E and relations P
  Rule-based approach -> AnyBURL for rule mining + linear aggregation function
  GNN architectures -> NBFNet and A*Net with path representations and message passing
  Evaluation framework -> Filtered ranking metrics (MRR, Hits@X) for head and tail queries

- Critical path:
  1. Load training KG and split into train/validation/test
  2. Train rule-based model (mine rules, learn aggregation weights)
  3. Train GNN models (configure hyperparameters, especially ROH)
  4. Perform perturbation experiments to test negative pattern exploitation
  5. Compare performance metrics across approaches

- Design tradeoffs:
  - Rule-based approaches offer interpretability but cannot express negative patterns
  - GNNs can exploit negative patterns but lack interpretability
  - The ROH hyperparameter significantly affects GNN behavior and must be carefully configured
  - Existence features can augment rule-based approaches but require external knowledge

- Failure signatures:
  - Rule-based models failing to improve when existence features are added (indicates inability to exploit negative patterns)
  - GNN performance dropping significantly when ROH is disabled on FB15K-237
  - Perturbation experiments showing minimal score changes (indicates pattern not exploited)
  - Random perturbations having similar effects to targeted negative pattern perturbations

- First 3 experiments:
  1. Replicate the zoo dataset experiment: Train rule-based and GNN models, observe the dramatic performance difference for the specific query
  2. Test ROH hyperparameter on FB15K-237: Compare GNN performance with ROH on vs off
  3. Apply existence features to rule-based approach on WN18RR: Measure improvement when adding hypernym-specific existence feature

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance difference between GNNs and rule-based approaches change on other knowledge graph benchmarks with different structural characteristics?
- Basis in paper: [explicit] The paper focuses on WN18RR and FB15K-237, finding that performance differences can be explained by negative patterns specific to these datasets.
- Why unresolved: The paper only investigates two specific benchmarks and their unique negative patterns. Other knowledge graphs may have different structural properties that could lead to different patterns being exploited by GNNs.
- What evidence would resolve it: Testing the GNNs and rule-based approaches on a diverse set of knowledge graph benchmarks with varying structural properties, and analyzing the specific patterns exploited in each case.

### Open Question 2
- Question: Can the rule-based approach be extended to learn more general negative patterns, not just existence features?
- Basis in paper: [explicit] The paper shows that adding existence features to the rule-based approach improves performance on WN18RR and FB15K-237.
- Why unresolved: The paper only explores adding existence features as a way to extend the rule-based approach. More general negative patterns might exist that cannot be captured by simple existence features.
- What evidence would resolve it: Developing and testing new rule-based approaches that can learn more complex negative patterns, and comparing their performance to GNNs on various benchmarks.

### Open Question 3
- Question: How do overlapping positive and negative patterns influence the final score assigned by GNNs?
- Basis in paper: [explicit] The paper mentions that facts could simultaneously activate positive and negative patterns, but does not provide a detailed analysis of how this influences the final score.
- Why unresolved: The paper focuses on identifying specific negative patterns but does not investigate how these patterns interact with positive patterns learned by the GNNs.
- What evidence would resolve it: Conducting experiments that analyze the interaction between positive and negative patterns learned by GNNs, and developing methods to quantify their combined influence on the final score.

## Limitations

- The paper's explanation of how GNNs exploit negative patterns during inference lacks direct evidence from the corpus
- The rule-based approach's inability to express negative patterns is asserted but not thoroughly validated
- The FB15K-237 results depend heavily on understanding the dataset construction, which is only briefly referenced
- The study only investigates two specific benchmarks, limiting generalizability to other knowledge graphs

## Confidence

- **High**: The existence of a performance gap between rule-based and GNN approaches on the synthetic datasets
- **Medium**: The identification of specific negative patterns (Only-One-Tail, Only-One-Link) that explain the performance difference
- **Low**: The exact mechanisms by which GNNs exploit these patterns during inference

## Next Checks

1. Verify the message passing implementation in NBFNet/A*Net to confirm they can access training graph information during inference
2. Test whether adding negative rules to the rule-based approach's language bias eliminates the performance gap
3. Construct a modified FB15K-237 dataset where the structural bias is removed to test if GNN performance drops accordingly