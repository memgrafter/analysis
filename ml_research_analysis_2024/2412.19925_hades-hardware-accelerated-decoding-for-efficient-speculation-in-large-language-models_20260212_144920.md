---
ver: rpa2
title: 'HADES: Hardware Accelerated Decoding for Efficient Speculation in Large Language
  Models'
arxiv_id: '2412.19925'
source_url: https://arxiv.org/abs/2412.19925
tags:
- decoding
- speculative
- hardware
- language
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents HADES, the first specialized hardware accelerator
  designed to support speculative decoding in large language models (LLMs). Speculative
  decoding is a technique that speeds up LLM inference by using a smaller draft model
  to generate candidate tokens, which are then verified by the larger target model.
---

# HADES: Hardware Accelerated Decoding for Efficient Speculation in Large Language Models

## Quick Facts
- arXiv ID: 2412.19925
- Source URL: https://arxiv.org/abs/2412.19925
- Authors: Ze Yang; Yihong Jin; Xinhe Xu
- Reference count: 26
- Primary result: First specialized hardware accelerator for speculative decoding in LLMs, achieving 6.99x-7.74x higher token verification throughput and 117.95x-159.66x better energy efficiency than GPUs

## Executive Summary
This paper presents HADES, the first specialized hardware accelerator designed to support speculative decoding in large language models. Speculative decoding speeds up LLM inference by using a smaller draft model to generate candidate tokens that are then verified by the larger target model. HADES implements this approach entirely in hardware, achieving significant performance and energy efficiency improvements over traditional GPU-based approaches.

The key innovation is a hardware pipeline that accelerates the verification phase of speculative decoding, avoiding memory bottlenecks and kernel launch overhead that plague general-purpose GPUs. By optimizing at the hardware level, HADES delivers substantial gains in both throughput and energy efficiency while maintaining compatibility with existing LLM accelerator architectures.

## Method Summary
HADES is a custom hardware accelerator that implements the verification phase of speculative decoding in LLMs. The system uses specialized hardware pipelines to verify speculated tokens in parallel, avoiding the memory bottlenecks and kernel launch overhead of general-purpose GPUs. The accelerator is designed as a modular add-on that doesn't require changes to the original LLM accelerator architecture, maintaining compatibility with existing systems.

The implementation focuses on the verification-only aspect of speculative decoding, using a draft model (GPT-2 124M parameters) to generate tokens that are verified by a target model (GPT2-XL 1.5B parameters). The hardware design includes buffering logits in SRAM and pipelining verification steps to maximize throughput. Performance is measured in verification tokens/second and tokens/second/Watt (energy efficiency), with comparisons against NVIDIA A100 and A6000 GPUs.

## Key Results
- Achieves 6.99x and 7.74x higher token verification throughput compared to NVIDIA A100 and A6000 GPUs respectively
- Delivers 117.95x and 159.66x better energy efficiency (tokens/second/Watt) than A100 and A6000 GPUs
- Maintains compatibility with existing LLM accelerator architectures through modular design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HADES accelerates speculative decoding by implementing the verification phase entirely in hardware.
- Mechanism: The accelerator uses specialized hardware pipelines to verify speculated tokens in parallel, avoiding the memory bottlenecks and kernel launch overhead of general-purpose GPUs.
- Core assumption: Verification is a sequential but memory-bound operation that can be effectively parallelized in hardware.
- Evidence anchors:
  - [abstract] "HADES implements this approach entirely in hardware, achieving significant performance and energy efficiency improvements."
  - [section V] "HADES achieves 6.99x and 7.74x better tokens/sec than A100 and A6000, respectively."
  - [corpus] "Speculative decoding improves LLM inference by generating and verifying multiple tokens in parallel, but existing systems suffer from suboptimal performance due to a mismatch between dynamic speculation and static runtime assumptions."
- Break condition: If the verification phase becomes compute-intensive rather than memory-bound, the hardware acceleration advantage diminishes.

### Mechanism 2
- Claim: HADES achieves superior energy efficiency by optimizing hardware-level speculative decoding operations.
- Mechanism: The hardware accelerator minimizes energy consumption by reducing memory traffic and optimizing power usage during the verification process.
- Core assumption: Hardware-level optimization can significantly reduce energy consumption compared to software-based implementations.
- Evidence anchors:
  - [abstract] "while also delivering 117.95x and 159.66x better energy efficiency."
  - [section V] "HADES shows massive energy efficiency gain, achieving 159.66x and 117.95x better tokens/sec/Watt than A6000, A100, respectively."
  - [corpus] "Modern large language model (LLM) applications exhibit diverse service-level objectives (SLOs), from low-latency requirements in interactive coding assistants to more relaxed constraints in data wrangling tasks."
- Break condition: If the energy savings from hardware optimization are offset by increased power consumption in other parts of the system.

### Mechanism 3
- Claim: HADES maintains compatibility with existing LLM accelerators while providing speculative decoding support.
- Mechanism: The accelerator is designed as a modular add-on that doesn't require changes to the original LLM accelerator architecture.
- Core assumption: A modular design approach allows for easier integration and broader compatibility with different hardware and software systems.
- Evidence anchors:
  - [section III] "We believe that our accelerator should adhere to that philosophy, where our accelerator does not require changes to the accelerator for the model itself, if one were to choose to integrate it."
  - [section IV] "We first try to use OpenHLS to lower Python to RTL using HLS techniques. However, due to the lack of maintenance in the codebase, we cannot successfully translate it."
  - [corpus] "Speculative decoding leverages idle GPU compute by using a lightweight drafter to propose K tokens, which the LLM verifies in parallel, boosting token throughput."
- Break condition: If the modular design introduces significant performance overhead or compatibility issues with certain hardware configurations.

## Foundational Learning

- Concept: Speculative Decoding
  - Why needed here: Understanding speculative decoding is crucial for grasping how HADES accelerates LLM inference.
  - Quick check question: What is the main advantage of using a smaller draft model in speculative decoding?

- Concept: Hardware Acceleration
  - Why needed here: Knowledge of hardware acceleration principles is essential for understanding how HADES achieves its performance gains.
  - Quick check question: How does hardware acceleration differ from software-based optimization in terms of performance and energy efficiency?

- Concept: Transformer Architecture
  - Why needed here: Familiarity with transformer architecture is necessary to understand the computational challenges addressed by HADES.
  - Quick check question: What makes transformer-based models memory-bound during inference?

## Architecture Onboarding

- Component map: Input buffer -> Draft model processing unit -> Verification pipeline -> Output buffer -> Control unit
- Critical path: 1. Token generation by draft model 2. Parallel verification of speculated tokens 3. Acceptance/rejection decision 4. Token appending to prompt sequence
- Design tradeoffs: Hardware complexity vs. performance gain, Flexibility in handling different model sizes vs. specialization, Energy efficiency vs. raw throughput
- Failure signatures: High rejection rate of speculated tokens, Bottlenecks in the verification pipeline, Increased energy consumption without corresponding performance gains
- First 3 experiments: 1. Benchmark verification throughput with different gamma factors 2. Compare energy efficiency against software-based speculative decoding 3. Test compatibility with various existing LLM accelerator architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal hardware architecture design for end-to-end speculative decoding in LLMs, considering the trade-offs between verification speed and overall system efficiency?
- Basis in paper: [explicit] The paper discusses the challenges of implementing end-to-end speculative decoding and mentions the need for a modular design that can adapt to different gamma factors.
- Why unresolved: The paper focuses on the verification phase of speculative decoding due to time limitations, leaving the end-to-end implementation unexplored.
- What evidence would resolve it: A comprehensive evaluation of different hardware architectures for end-to-end speculative decoding, comparing their performance and energy efficiency across various model sizes and gamma factors.

### Open Question 2
- Question: How does the choice of draft model affect the performance and energy efficiency of hardware-accelerated speculative decoding in LLMs?
- Basis in paper: [inferred] The paper mentions that the draft model is smaller than the target model and that the optimal gamma factor can vary depending on the model pair, suggesting that the choice of draft model could impact performance.
- Why unresolved: The paper does not explore the effects of different draft model choices on the performance and energy efficiency of the hardware-accelerated speculative decoding.
- What evidence would resolve it: A systematic study comparing the performance and energy efficiency of hardware-accelerated speculative decoding using different draft models for various target models.

### Open Question 3
- Question: What are the scalability limits of hardware-accelerated speculative decoding for LLMs, and how can these limits be overcome?
- Basis in paper: [explicit] The paper mentions scalability issues and the need to maintain performance gains across different model sizes and architectures.
- Why unresolved: The paper does not investigate the scalability limits of the proposed hardware-accelerated speculative decoding approach.
- What evidence would resolve it: An analysis of the performance and energy efficiency of hardware-accelerated speculative decoding as the size and complexity of the target model increase, along with proposed solutions to overcome any identified limitations.

## Limitations

- Implementation details of custom Verilog modules are not fully specified, making it difficult to reproduce results accurately
- Energy efficiency measurements lack detailed methodology and may not account for all energy consumption sources
- Compatibility claims are based on design philosophy rather than comprehensive testing with various hardware configurations

## Confidence

- High Confidence: Performance claims (token verification throughput) - Well-supported by experimental results and comparisons with established GPU baselines
- Medium Confidence: Energy efficiency claims - Based on measurements, but with limited detail on methodology and potential unaccounted energy sources
- Medium Confidence: Compatibility claims - Supported by design philosophy but lacking concrete compatibility tests and measurements

## Next Checks

1. Obtain or create complete specifications of the HADES hardware implementation, including pipeline stages, buffer sizes, and control logic, then analyze design decisions and their impact on performance and energy efficiency

2. Conduct extensive compatibility tests with various existing LLM accelerator architectures to validate claimed compatibility and identify potential integration issues or performance overhead

3. Investigate and validate the energy efficiency measurement methodology used in the paper, including accounting for all energy consumption sources and potential sources of error, then conduct independent energy measurements to verify claimed efficiency improvements