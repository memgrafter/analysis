---
ver: rpa2
title: 'GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry
  Problem-Solving'
arxiv_id: '2402.10104'
source_url: https://arxiv.org/abs/2402.10104
tags:
- problems
- geometry
- subset
- math
- geoeval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'GeoEval is a benchmark for evaluating large language models (LLMs)
  and multi-modal models (MMs) on geometry problem-solving. It includes 5,050 problems
  across four subsets: GeoEval-2000, GeoEval-backward, GeoEval-aug, and GeoEval-hard.'
---

# GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving

## Quick Facts
- arXiv ID: 2402.10104
- Source URL: https://arxiv.org/abs/2402.10104
- Reference count: 21
- GeoEval includes 5,050 geometry problems across four subsets

## Executive Summary
GeoEval is a comprehensive benchmark designed to evaluate large language models (LLMs) and multi-modal models (MMs) on geometry problem-solving capabilities. The benchmark encompasses 5,050 problems spanning flat, solid, and analytic geometry, with both text-only and diagram-based inputs across four distinct subsets. Evaluation was conducted in a zero-shot manner using instruction prompts to assess models' reasoning abilities on geometry problems.

The results reveal significant performance gaps, with WizardMath models achieving the highest accuracy of 55.67% on the main GeoEval-2000 subset but only 6.00% on the hardest subset. GPT-series models demonstrated improved performance on rephrased problems, suggesting that self-rephrasing could enhance problem-solving capabilities. While multi-modal models generally outperformed LLMs, both struggled with complex geometry problems, highlighting the need for further advancements in reasoning skills.

## Method Summary
The GeoEval benchmark was constructed with 5,050 geometry problems distributed across four subsets: GeoEval-2000 (main dataset), GeoEval-backward, GeoEval-aug (augmented), and GeoEval-hard. Problems cover flat geometry, solid geometry, and analytic geometry domains, with inputs available in both text-only and diagram-based formats. Models were evaluated using zero-shot instruction prompting, where they received problems without any fine-tuning or task-specific training. Performance was measured using accuracy metrics across all subsets, with particular attention to the challenging GeoEval-hard subset that tests advanced reasoning capabilities.

## Key Results
- WizardMath models achieved highest accuracy of 55.67% on GeoEval-2000 but only 6.00% on GeoEval-hard subset
- GPT-series models performed better on rephrased problems, indicating potential benefits of self-rephrasing
- Multi-modal models generally outperformed LLMs across all geometry domains
- All models struggled significantly with the hardest geometry problems, suggesting fundamental limitations in current reasoning capabilities

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of geometry problem types and the zero-shot evaluation methodology, which provides a realistic assessment of models' inherent reasoning capabilities without task-specific optimization. The inclusion of both text and diagram inputs, along with multiple difficulty levels, creates a robust evaluation framework that reveals both strengths and limitations of current models in geometric reasoning.

## Foundational Learning
- **Geometry Problem Types**: Understanding flat, solid, and analytic geometry is essential for creating comprehensive evaluation criteria and interpreting model performance across different domains.
- **Multi-modal Processing**: Knowledge of how models handle both text and visual inputs is needed to understand performance differences between LLMs and MMs.
- **Zero-shot Evaluation**: Understanding the limitations and benefits of zero-shot prompting is crucial for interpreting benchmark results and comparing against other evaluation methodologies.
- **Mathematical Reasoning**: Grasping the nature of mathematical problem-solving is important for understanding why models struggle with certain geometry problems.
- **Model Pre-training**: Knowledge of how mathematical pre-training affects performance helps explain the varying results across different model architectures.
- **Problem Rephrasing**: Understanding how rephrasing affects problem-solving performance is key to interpreting the GPT-series model results.

## Architecture Onboarding
**Component Map**: Input Parser -> Problem Categorizer -> Reasoning Engine -> Answer Validator -> Performance Tracker
**Critical Path**: Input Parser -> Reasoning Engine -> Answer Validator
**Design Tradeoffs**: Zero-shot evaluation provides realistic capability assessment but may underutilize models' potential with fine-tuning; diagram-based problems add complexity but may introduce interpretation challenges
**Failure Signatures**: Consistent low performance across hard problems indicates reasoning limitations rather than data quality issues; performance gaps between subsets suggest difficulty calibration effectiveness
**3 First Experiments**: (1) Compare zero-shot vs fine-tuned performance on same problems, (2) Test model performance with and without diagram inputs, (3) Evaluate different prompting strategies on the same problem sets

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark may not fully capture models' capabilities with different prompting strategies or fine-tuning approaches
- Performance differences between LLMs and MMs could be influenced by specific implementation details rather than inherent architectural advantages
- Focus on Chinese-language problems may limit generalizability to other educational contexts
- Zero-shot evaluation methodology may not represent optimal model performance

## Confidence
- High confidence in benchmark utility and comparative model performance, as results align with known capabilities of mathematical models
- High confidence in mathematical pre-training benefits, given clear performance differences between specialized and general-purpose models
- Medium confidence in need for further advancements, as low performance on hard problems could reflect benchmark difficulty rather than fundamental limitations
- Medium confidence in claims about multi-modal advantages, as implementation details may influence results

## Next Checks
1. Replicate the evaluation using different prompting strategies and fine-tuning approaches to establish baseline variability
2. Conduct cross-linguistic validation using translated problems to assess cultural and language dependency
3. Analyze model failures on GeoEval-hard problems to determine whether they stem from reasoning limitations or other factors such as diagram interpretation challenges