---
ver: rpa2
title: Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures
arxiv_id: '2410.07698'
source_url: https://arxiv.org/abs/2410.07698
tags:
- lozo
- gradient
- low-rank
- fine-tuning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory inefficiency of fine-tuning large
  language models by proposing a low-rank zeroth-order optimization method (LOZO)
  that leverages the low-rank structure of LLM gradients. Unlike existing zeroth-order
  methods that use full-rank perturbations, LOZO employs a novel low-rank gradient
  estimator that maintains gradient approximations within low-dimensional subspaces,
  closely matching the true gradient structure.
---

# Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures

## Quick Facts
- arXiv ID: 2410.07698
- Source URL: https://arxiv.org/abs/2410.07698
- Reference count: 40
- Primary result: LOZO achieves 2-6% accuracy improvement over MeZO while using only a quarter of memory for LLM fine-tuning

## Executive Summary
This paper addresses the memory inefficiency of fine-tuning large language models by proposing a low-rank zeroth-order optimization method (LOZO) that leverages the low-rank structure of LLM gradients. Unlike existing zeroth-order methods that use full-rank perturbations, LOZO employs a novel low-rank gradient estimator that maintains gradient approximations within low-dimensional subspaces, closely matching the true gradient structure. The method incorporates a lazy sampling strategy that resamples perturbation matrices less frequently, allowing better subspace exploration while reducing memory overhead. Extensive experiments across multiple model scales (350M to 66B parameters) and tasks show that LOZO and its momentum variant (LOZO-M) outperform existing zeroth-order methods while maintaining similar memory efficiency.

## Method Summary
LOZO introduces a low-rank gradient estimator that approximates true gradients using low-dimensional subspaces defined by sampled matrices U and V. The method employs a lazy sampling strategy where V is resampled only every ν iterations, allowing the algorithm to explore one low-rank subspace for longer periods before switching. This approach maintains gradient approximations within low-rank structures while reducing memory overhead. LOZO-M extends this with momentum integration that incurs negligible additional memory costs due to the fixed subspace during each ν-iteration block. The method is specifically designed for memory-efficient fine-tuning of large language models where traditional backpropagation would require prohibitive memory usage.

## Key Results
- LOZO and LOZO-M outperform existing zeroth-order methods (MeZO, MeZO-LoRA) by 2-6% accuracy points
- On RoBERTa-large, LOZO achieves 61.6-80.4% accuracy across tasks, approaching full fine-tuning performance (63.4-84.8%)
- Uses only a quarter of the memory compared to full fine-tuning while maintaining competitive accuracy
- Demonstrates faster convergence rates and smaller loss oscillations compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LOZO's low-rank gradient estimator closely matches the true gradient structure of LLM fine-tuning.
- Mechanism: Instead of full-rank random perturbations (RGE), LOZO samples low-rank matrices U and V to approximate gradients, ensuring the estimated gradient retains a low-rank structure matching the true gradient.
- Core assumption: LLM fine-tuning gradients exhibit low-rank structure that can be captured by low-dimensional subspaces.
- Evidence anchors: Abstract mentions low-rank gradient estimator maintaining approximations within low-dimensional subspaces; section 3 states LLM gradients exhibit low-rank structures; corpus provides weak related evidence.

### Mechanism 2
- Claim: Lazy sampling strategy improves exploration of low-rank subspaces while reducing memory overhead.
- Mechanism: Instead of resampling V at every iteration, LOZO samples V only every ν iterations, allowing the algorithm to explore one low-rank subspace for longer periods before switching.
- Core assumption: Multiple consecutive updates within the same subspace provide better approximation of true gradients than frequent subspace switching.
- Evidence anchors: Section 4.1 explains lazy sampling allows sufficient exploration of low-rank subspaces over longer periods; section 4.2 proposes the lazy sampling strategy; corpus provides weak related evidence.

### Mechanism 3
- Claim: LOZO-M momentum integration incurs negligible additional memory overhead.
- Mechanism: Since V remains fixed for ν iterations, only the low-rank momentum term N = βN_{t-1} + (1-β)c_t U_t needs to be stored, avoiding full-rank momentum storage.
- Core assumption: The low-rank structure can be maintained in momentum updates without sacrificing convergence properties.
- Evidence anchors: Abstract states low-rank nature enables momentum integration with negligible extra memory costs; section 4.4 explains LOZO-M requires storing only N_t with minimal additional overhead; corpus provides moderate related evidence.

## Foundational Learning

- Concept: Zeroth-order optimization (ZO) using finite differences
  - Why needed here: LOZO relies on ZO methods to avoid backpropagation and activation storage, enabling memory-efficient fine-tuning.
  - Quick check question: How does ZO gradient estimation differ from first-order methods in terms of computational requirements and memory usage?

- Concept: Low-rank matrix structures and their properties
  - Why needed here: Understanding low-rank approximations is crucial for grasping how LOZO's gradient estimator works and why it's effective.
  - Quick check question: What is the relationship between matrix rank and dimensionality reduction in the context of gradient approximations?

- Concept: Subspace optimization methods
  - Why needed here: LOZO can be interpreted as a subspace optimization method, solving the problem iteratively across different low-rank subspaces.
  - Quick check question: How does random coordinate minimization relate to LOZO's approach of optimizing within low-rank subspaces?

## Architecture Onboarding

- Component map: Low-rank gradient estimator (LGE) -> Lazy sampling module -> Momentum integration (LOZO-M) -> Perturbation computation -> Parameter update logic

- Critical path: 1. Sample U matrix (every iteration) -> 2. Check if V needs resampling (every ν iterations) -> 3. Apply perturbations to weights -> 4. Compute forward passes and finite differences -> 5. Calculate low-rank gradient estimate -> 6. Update weights using gradient estimate -> 7. (Optional) Update momentum term

- Design tradeoffs:
  - Memory vs. accuracy: Higher rank r improves approximation quality but increases memory usage
  - Exploration vs. exploitation: Larger ν allows better subspace exploration but may slow convergence
  - Speed vs. memory: LOZO-M adds convergence benefits but requires careful implementation to maintain memory efficiency

- Failure signatures:
  - Training loss plateaus or increases: May indicate poor subspace selection or rank mismatch
  - Memory usage exceeds expectations: Could signal incorrect low-rank implementation
  - Slow convergence: Might result from suboptimal ν or rank parameters
  - Gradient estimates becoming unstable: Could indicate numerical issues with perturbation scale

- First 3 experiments:
  1. Single-layer linear model with known low-rank gradients to validate LGE approximation quality
  2. Small transformer model on synthetic task to test lazy sampling strategy impact
  3. Full-scale model comparison with baseline MeZO to verify performance gains and memory savings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between the rank parameter r and the lazy sampling interval ν for different types of downstream tasks?
- Basis in paper: The paper explores how different values of r and ν affect performance on various datasets, showing that small ν can hinder convergence for tasks with significant loss reduction, while having minimal impact on tasks with stable or slightly decreasing loss.
- Why unresolved: While the paper provides empirical evidence of how r and ν impact specific tasks, it does not offer a theoretical framework or systematic approach to determine the optimal combination of these parameters for new, unseen tasks or datasets.
- What evidence would resolve it: A comprehensive study analyzing the relationship between task characteristics (e.g., dataset size, complexity, loss landscape) and optimal r and ν values, potentially through a meta-learning approach or by deriving theoretical bounds on convergence rates as a function of these parameters.

### Open Question 2
- Question: How does the performance of LOZO compare to other memory-efficient fine-tuning methods (e.g., LoRA, Adapters) when considering both memory usage and accuracy across different model scales and task types?
- Basis in paper: The paper mentions that LOZO achieves performance comparable to full fine-tuning while using only a quarter of the memory, and outperforms MeZO and MeZO-LoRA. However, it does not directly compare LOZO to other memory-efficient methods like LoRA or Adapter-based approaches.
- Why unresolved: A comprehensive benchmark comparing LOZO to all major memory-efficient fine-tuning methods across various model scales and task types would provide a clearer picture of its relative strengths and weaknesses.
- What evidence would resolve it: Systematic experiments comparing LOZO, LoRA, and Adapter-based methods on a wide range of tasks and model scales, measuring both memory usage and accuracy, to determine which method performs best under different conditions.

### Open Question 3
- Question: Can the principles of low-rank gradient estimation and lazy sampling in LOZO be extended to other optimization problems beyond fine-tuning large language models?
- Basis in paper: The paper demonstrates that LOZO's low-rank gradient estimation captures the true gradient structure in LLM fine-tuning and that the lazy sampling strategy prevents abrupt model changes. These principles could potentially be applied to other optimization problems where gradients exhibit low-rank structures or where exploration of a subspace over multiple iterations is beneficial.
- Why unresolved: While the paper focuses on LLM fine-tuning, it does not explore whether the LOZO algorithm's core ideas can be generalized to other domains or optimization problems.
- What evidence would resolve it: Successful application and analysis of LOZO's principles (low-rank gradient estimation and lazy sampling) to other optimization problems, such as training other types of neural networks, optimizing hyperparameters, or solving inverse problems in scientific computing.

## Limitations
- Dependence on appropriate rank selection for different model scales and tasks
- Potential sensitivity to the lazy sampling interval parameter ν
- Assumes LLM fine-tuning gradients consistently exhibit low-rank structure across all layers and tasks

## Confidence
- Memory efficiency improvements: High
- Performance improvement claims: Medium
- Theoretical foundation: Medium

## Next Checks
1. **Gradient Structure Validation**: Conduct systematic analysis of gradient rank distributions across different model layers and tasks to verify the low-rank assumption that underpins LOZO's effectiveness.

2. **Parameter Sensitivity Analysis**: Perform extensive hyperparameter sweeps for rank r and lazy sampling interval ν across various model sizes to identify optimal configurations and robustness boundaries.

3. **Cross-Architecture Generalization**: Test LOZO on non-Transformer architectures (e.g., RNNs, CNNs) and specialized model families to evaluate the method's broader applicability beyond standard LLMs.