---
ver: rpa2
title: Generating Uncontextualized and Contextualized Questions for Document-Level
  Event Argument Extraction
arxiv_id: '2404.04770'
source_url: https://arxiv.org/abs/2404.04770
tags:
- event
- questions
- argument
- question
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces strategies to generate questions for document-level
  event argument extraction, including uncontextualized questions based on templates
  or GPT-4 prompts, and contextualized questions generated by a T5 model trained with
  weak supervision. Combining uncontextualized and contextualized questions improves
  F1 scores by up to 3.0 points, especially for inter-sentential arguments.
---

# Generating Uncontextualized and Contextualized Questions for Document-Level Event Argument Extraction

## Quick Facts
- arXiv ID: 2404.04770
- Source URL: https://arxiv.org/abs/2404.04770
- Reference count: 27
- Primary result: Combining uncontextualized and contextualized questions improves F1 scores by up to 3.0 points for document-level event argument extraction

## Executive Summary
This paper introduces strategies to generate questions for document-level event argument extraction, including uncontextualized questions based on templates or GPT-4 prompts, and contextualized questions generated by a T5 model trained with weak supervision. The key insight is that combining these two question types leverages the strengths of both approaches: uncontextualized questions provide broad coverage of event-argument roles while contextualized questions capture document-specific semantic relationships. The approach achieves state-of-the-art performance on RAMS and successfully transfers to WikiEvents without requiring domain-specific components. The weakly supervised question generation model captures both argument and contextual cues, producing diverse and nuanced questions that improve extraction accuracy, particularly for inter-sentential arguments.

## Method Summary
The method transforms document-level event argument extraction into a question-answering task. Uncontextualized questions are generated using template-based patterns or GPT-4 prompts that focus solely on event triggers and argument roles without document context. Contextualized questions are generated by a T5 model fine-tuned on LLM-generated questions from the RAMS dataset, which incorporates both the event trigger and document context. These questions are then combined and used to train a BART model for argument extraction. The approach does not require manual annotation of questions, relying instead on weak supervision through LLM-generated questions, making it scalable across different corpora.

## Key Results
- Combining uncontextualized and contextualized questions improves F1 scores by up to 3.0 points on RAMS
- The approach achieves state-of-the-art performance on RAMS and transfers effectively to WikiEvents without domain-specific components
- Weakly supervised question generation captures both argument and contextual cues, producing diverse and nuanced questions
- Most errors involve plausible but semantically incorrect roles or coreferent mentions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining uncontextualized and contextualized questions improves F1 scores by up to 3.0 points, especially for inter-sentential arguments.
- Mechanism: Uncontextualized questions provide baseline coverage of event-argument roles without document-specific noise, while contextualized questions capture semantic interactions between event arguments and contextual cues from the document. The combination allows the model to learn both general role patterns and document-specific argument relationships.
- Core assumption: The semantic relevance across different event arguments and non-event argument entities from the document provides valuable context that improves extraction accuracy.
- Evidence anchors:
  - [abstract]: "Combining uncontextualized and contextualized questions is beneficial, especially when event triggers and arguments appear in different sentences."
  - [section 5.1]: "Combining template-based and weakly supervised questions from RAMS yields the best results for both base and large models, showcasing a 3.0 F1 improvement (48.5 vs. 51.5) for the base model and a 2.2 F1 improvement (52.3 vs. 54.5) for the large model."
  - [corpus]: The improvement is specifically noted for inter-sentential arguments, though the paper doesn't provide per-role F1 breakdowns for the combined approach.
- Break condition: If the contextualized questions become too noisy or deviate significantly from the event and argument role of interest, they may introduce more confusion than benefit.

### Mechanism 2
- Claim: Weakly supervised question generation models transfer effectively across corpora without domain-specific components.
- Mechanism: The weakly supervised T5 model trained on RAMS questions learns to generate event-grounded questions based on document content, event triggers, and argument roles. This learned capability transfers to new corpora like WikiEvents because the model captures general patterns of question generation rather than corpus-specific features.
- Core assumption: Event-grounded question generation can be learned from one corpus and applied to different domains, events, and argument roles.
- Evidence anchors:
  - [abstract]: "Our approach does not have corpus-specific components, in particular, the question generation strategies transfer across corpora."
  - [section 5.2]: "We reuse the question generation models used with RAMS, including the weakly supervised model trained with questions obtained via prompting with documents, event triggers and arguments from RAMS. In other words, there is no WikiEvents-specific component or fine-tuning."
  - [corpus]: The paper demonstrates successful transfer from RAMS to WikiEvents, though specific per-role performance on WikiEvents is not detailed.
- Break condition: If the target corpus has significantly different event types, argument roles, or document structures that weren't represented in the source corpus training data.

### Mechanism 3
- Claim: Template-based uncontextualized questions sometimes result in unnatural or ungrammatical questions, but still provide effective baseline performance.
- Mechanism: Simple template-based question generation using fixed patterns like "What is the [argument role] of event [event trigger]?" creates consistent question structures that can be answered by the model, even if the questions are not always natural language. The simplicity ensures broad coverage across all roles.
- Core assumption: Even suboptimal question formulations can provide useful training signals for the downstream QA model.
- Evidence anchors:
  - [section 3.1]: "Uncontextualized questions sometimes result in unnatural or ungrammatical questions as exemplified below" with examples showing awkward phrasing.
  - [section 5.1]: Template-based questions achieve 48.5 F1 with base model and 52.3 F1 with large model, which are strong baselines.
  - [corpus]: The paper notes that template-based questions are used as the foundation for combining with contextualized questions, suggesting their effectiveness despite quality issues.
- Break condition: If the templates become too rigid or fail to capture the semantic nuances required for certain argument roles, leading to consistently poor performance.

## Foundational Learning

- Concept: Question-answering formulation of event argument extraction
  - Why needed here: This work transforms event argument extraction into a QA task, allowing the use of established QA model architectures (BART) and evaluation metrics. This formulation enables zero-shot and few-shot learning capabilities.
  - Quick check question: Can you explain how transforming event argument extraction into a question-answering task changes the evaluation approach from span-based to answer-based?

- Concept: Weak supervision for question generation
  - Why needed here: Manual question generation for all event-argument pairs across multiple corpora would be prohibitively expensive. Weak supervision using LLM-generated questions provides scalable training data.
  - Quick check question: What are the advantages and disadvantages of using LLM-generated questions as weak supervision compared to manual annotation?

- Concept: Transfer learning across event extraction corpora
  - Why needed here: The ability to transfer question generation models from one corpus (RAMS) to another (WikiEvents) without domain-specific components is crucial for practical deployment across different domains and event types.
  - Quick check question: How does the transfer of question generation models differ from traditional transfer learning approaches in NLP?

## Architecture Onboarding

- Component map: Event trigger + document -> Question generation (templates/GPT-4/T5) -> QA pairs -> BART fine-tuning -> Argument extraction
- Critical path:
  1. Receive event trigger and document
  2. Generate uncontextualized questions (template/GPT-4)
  3. Generate contextualized questions (weakly supervised T5)
  4. Combine both question sets
  5. Fine-tune BART on combined question-answer pairs
  6. Use fine-tuned BART to extract arguments
- Design tradeoffs:
  - Uncontextualized vs contextualized: Uncontextualized questions are simpler but miss document context; contextualized questions capture more information but may introduce noise
  - Template complexity: Simple templates ensure coverage but may produce unnatural questions; complex templates improve quality but reduce coverage
  - Training data size: More training data improves performance but increases computational cost and potential for noise
- Failure signatures:
  - Low precision but high recall: Model predicts many spans but most are incorrect - likely due to noisy contextualized questions
  - High precision but low recall: Model misses many arguments - likely due to overly restrictive templates or insufficient training data
  - Consistent mislabeling of certain roles: Model learned incorrect associations - likely due to corpus-specific biases in training data
- First 3 experiments:
  1. Test baseline performance using only template-based uncontextualized questions with BART
  2. Test performance using only weakly supervised contextualized questions to measure standalone contribution
  3. Test combined approach with template-based and weakly supervised questions to measure synergistic effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of weakly supervised question generation models transfer to domains or corpora that differ significantly from the training data in terms of event types, argument roles, and document structure?
- Basis in paper: Explicit
- Why unresolved: The paper demonstrates successful transfer from RAMS to WikiEvents, but this is a relatively small domain shift. The performance on more divergent domains remains untested.
- What evidence would resolve it: Systematic evaluation of the weakly supervised question generation models on corpora from diverse domains (e.g., scientific literature, social media, legal documents) with varying event structures and argument roles.

### Open Question 2
- Question: What is the impact of using alternative question generation strategies, such as reinforcement learning or active learning, on the quality and diversity of generated questions for event argument extraction?
- Basis in paper: Inferred
- Why unresolved: The paper explores template-based, prompt-based, and weakly supervised question generation, but does not investigate more advanced techniques like reinforcement learning or active learning.
- What evidence would resolve it: Comparative evaluation of event argument extraction performance using questions generated by different strategies, including reinforcement learning and active learning approaches.

### Open Question 3
- Question: How does the performance of event argument extraction models vary when using questions generated by different question generation strategies, and what factors contribute to the differences in performance?
- Basis in paper: Explicit
- Why unresolved: The paper compares the performance of models using different question generation strategies, but does not provide a detailed analysis of the factors contributing to the differences in performance.
- What evidence would resolve it: In-depth analysis of the generated questions, including their quality, diversity, and relevance to the event and argument roles, to identify the factors that influence model performance.

## Limitations
- Performance claims are based on specific datasets (RAMS and WikiEvents) with particular event schemas, limiting generalizability to other domains
- The quality of LLM-generated questions used for weak supervision could vary significantly with different prompting strategies or base models
- While zero-shot and few-shot capabilities are demonstrated, the performance gap compared to fine-tuned models suggests these capabilities may be more limited than implied

## Confidence
- **High Confidence**: The core mechanism that combining uncontextualized and contextualized questions improves performance is well-supported by experimental results showing 3.0 F1 improvement on RAMS. The transfer capability across corpora without domain-specific components is also strongly demonstrated.
- **Medium Confidence**: The claim about contextualized questions capturing "semantic interactions between event arguments and contextual cues" is supported but could benefit from more detailed analysis of which contextual features are most valuable. The effectiveness of weakly supervised question generation relies on assumptions about the quality of LLM-generated questions that are not fully validated.
- **Low Confidence**: The assertion that the approach enables "zero-shot and few-shot event argument extraction" with GPT-3 is presented but the actual performance gap (lower than fine-tuned BART) suggests this capability may be more limited than implied.

## Next Checks
1. **Per-Role Performance Analysis**: Conduct detailed breakdown of F1 scores for each argument role when combining uncontextualized and contextualized questions, particularly focusing on inter-sentential arguments where the largest improvements are claimed.
2. **Cross-Corpus Transfer Robustness**: Test the transfer capability by training the weakly supervised question generation model on RAMS and evaluating on multiple additional datasets beyond WikiEvents, including those with significantly different event schemas or domains.
3. **Question Quality Evaluation**: Implement a systematic evaluation of the generated questions (both template-based and LLM-generated) for grammaticality, relevance, and informativeness, and correlate these quality metrics with downstream extraction performance.