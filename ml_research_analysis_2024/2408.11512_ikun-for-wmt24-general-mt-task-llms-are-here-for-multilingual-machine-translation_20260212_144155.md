---
ver: rpa2
title: 'IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation'
arxiv_id: '2408.11512'
source_url: https://arxiv.org/abs/2408.11512
tags:
- translation
- data
- machine
- languages
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents IKUN and IKUN-C, two multilingual machine translation
  systems for the WMT24 General MT Task, built on Llama-3-8B and Mistral-7B-v0.3 LLMs.
  The systems address the challenge of adapting LLMs to multilingual MT, particularly
  for underrepresented languages.
---

# IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation

## Quick Facts
- arXiv ID: 2408.11512
- Source URL: https://arxiv.org/abs/2408.11512
- Authors: Baohao Liao; Christian Herold; Shahram Khadivi; Christof Monz
- Reference count: 18
- Primary result: IKUN-C achieved 6 first-place and 3 second-place finishes in automatic evaluation across both constrained and open tracks.

## Executive Summary
This paper presents IKUN and IKUN-C, two multilingual machine translation systems for the WMT24 General MT Task, built on Llama-3-8B and Mistral-7B-v0.3 LLMs. The systems address the challenge of adapting LLMs to multilingual MT, particularly for underrepresented languages. The approach involves expanding tokenizer vocabularies for efficiency, continuous pre-training on monolingual data to improve knowledge of target languages, and fine-tuning on high-quality parallel data. IKUN-C (constrained) achieved 6 first-place and 3 second-place finishes in automatic evaluation, while IKUN (open) secured 1 first-place and 2 second-place finishes across both tracks. These results demonstrate that LLMs can be effectively adapted for multilingual MT, narrowing the gap with traditional systems.

## Method Summary
The approach uses a two-stage process: first, continuous pre-training on monolingual data to improve LLMs' proficiency in underrepresented languages, then fine-tuning on high-quality parallel data. IKUN-C specifically expanded Mistral's tokenizer vocabulary by adding sub-words from Chinese, Japanese, Hindi, and Icelandic to reduce tokenized sentence length and improve training efficiency. Both systems used LoRA fine-tuning with learning rate 2e-4, batch size 128, sequence length 512, and weight decay 0.01.

## Key Results
- IKUN-C achieved 6 first-place and 3 second-place finishes in automatic evaluation across both constrained and open tracks.
- IKUN secured 1 first-place and 2 second-place finishes across both tracks.
- The systems successfully adapted LLMs for multilingual MT, demonstrating competitive performance against traditional approaches.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanding tokenizer vocabulary improves translation efficiency for underrepresented languages.
- Mechanism: Adding subwords specific to low-resource languages reduces sequence length during tokenization, lowering GPU memory consumption and enabling more effective training.
- Core assumption: The pre-trained tokenizer's inefficiency is primarily due to lack of language-specific subwords.
- Evidence anchors:
  - [abstract] "We identified that certain LLMs exhibit inefficiencies in tokenizing sentences from languages that are underrepresented in the pre-training data. To address this, we extended the existing vocabulary to reduce the tokenized sentence length."
  - [section] "We opted to expand the vocabulary by incorporating new sub-words to reduce the length of tokenized sentences, thereby enhancing training efficiency."
- Break condition: Adding too many new subwords could overwhelm the model's capacity and require extensive additional pre-training.

### Mechanism 2
- Claim: Continuous pre-training on monolingual data improves LLMs' proficiency in underrepresented languages.
- Mechanism: Exposing LLMs to extensive monolingual data in target languages helps the model learn language-specific patterns and improves transfer learning capabilities.
- Core assumption: Pre-trained LLMs can effectively learn from monolingual data to improve multilingual translation.
- Evidence anchors:
  - [abstract] "Secondly, we enriched the LLMs with knowledge across the 10 target languages through continued pre-training. This step is particularly crucial for underrepresented languages, as it facilitates transfer learning."
  - [section] "Given that our selected LLMs, specifically Llama-3-8B and Mistral-7B-v0.3, are primarily pre-trained on English, it is necessary to incorporate knowledge from other languages through further pre-training."
- Break condition: Pre-training may overfit to monolingual data and not generalize well to translation tasks.

### Mechanism 3
- Claim: Fine-tuning on high-quality parallel data produces better translation performance than using more data with lower quality.
- Mechanism: Selective use of clean, high-quality parallel data ensures the model learns accurate translation patterns without being confused by noise.
- Core assumption: Data quality is more important than quantity for fine-tuning LLMs on translation tasks.
- Evidence anchors:
  - [section] "Previous studies (Wu et al., 2024; Zhou et al., 2023) have demonstrated that the quality of fine-tuning data is a critical factor in achieving optimal performance."
  - [section] "In light of this, we exclusively utilize high-quality parallel data for the fine-tuning phase."
- Break condition: Insufficient quantity of high-quality data may limit model performance.

## Foundational Learning

- Concept: Tokenizer efficiency and vocabulary size
  - Why needed here: Understanding how tokenizer vocabulary affects model performance is crucial for adapting LLMs to new languages
  - Quick check question: What happens to sequence length when adding language-specific subwords to the tokenizer vocabulary?

- Concept: Continuous pre-training vs. fine-tuning
  - Why needed here: Knowing when and how to apply each approach is essential for effective LLM adaptation
  - Quick check question: What's the difference between continuous pre-training and fine-tuning, and when should each be used?

- Concept: Parallel data quality vs. quantity trade-off
  - Why needed here: Understanding how data quality affects model performance helps in dataset selection
  - Quick check question: Why might using fewer high-quality examples be better than using more low-quality examples for fine-tuning?

## Architecture Onboarding

- Component map:
  - Tokenizer (modified or unmodified) -> Pre-trained LLM (Llama-3-8B or Mistral-7B-v0.3) -> Continuous pre-training module -> Fine-tuning module -> Parallel data sources (Flores-200, NTREX-128, WMT16-23)

- Critical path:
  1. Evaluate tokenizer efficiency
  2. Modify tokenizer if necessary
  3. Continuous pre-training on monolingual data
  4. Fine-tuning on parallel data
  5. Evaluation and iteration

- Design tradeoffs:
  - Modified tokenizer vs. unmodified (IKUN vs. IKUN-C)
  - Amount of monolingual data vs. pre-training duration
  - Data quality vs. quantity for fine-tuning

- Failure signatures:
  - High GPU memory usage (tokenizer inefficiency)
  - Poor performance on low-resource languages (insufficient pre-training)
  - Overfitting to training data (excessive fine-tuning)

- First 3 experiments:
  1. Compare tokenizer efficiency across languages using length ratio metric
  2. Test continuous pre-training impact on translation quality for underrepresented languages
  3. Evaluate fine-tuning performance with different data quality/quantity combinations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different vocabulary sizes for low-resource languages impact the efficiency and translation quality of LLMs in multilingual MT?
- Basis in paper: [explicit] The paper discusses expanding tokenizer vocabularies for languages like Chinese, Japanese, Hindi, and Icelandic to improve efficiency.
- Why unresolved: The paper does not explore the effects of varying vocabulary sizes on translation quality or efficiency.
- What evidence would resolve it: Experiments comparing models with different vocabulary sizes for low-resource languages, measuring both tokenization efficiency and translation performance.

### Open Question 2
- Question: What is the optimal balance between the amount of monolingual data used for pre-training and the resulting translation performance in multilingual MT?
- Basis in paper: [explicit] The paper uses a sampling strategy for monolingual data pre-training but does not explore the impact of varying the amount of data.
- Why unresolved: The paper does not investigate how different amounts of monolingual data affect the final translation quality.
- What evidence would resolve it: Comparative studies using different quantities of monolingual data for pre-training, with subsequent evaluation of translation performance.

### Open Question 3
- Question: How does the choice of fine-tuning data sources and quality affect the overall performance of multilingual MT systems based on LLMs?
- Basis in paper: [explicit] The paper emphasizes using high-quality parallel data for fine-tuning but does not explore the impact of different data sources or quality levels.
- Why unresolved: The paper does not assess how variations in fine-tuning data sources or quality influence translation outcomes.
- What evidence would resolve it: Experiments comparing systems fine-tuned on different combinations of parallel data sources, with varying quality levels, and measuring their impact on translation performance.

## Limitations
- The comparison between IKUN and IKUN-C confounds multiple variables by using different base LLMs rather than just tokenizer modifications.
- The evaluation lacks proper ablation studies to isolate the contribution of each component (tokenizer expansion, pre-training, fine-tuning).
- The approach requires substantial computational resources for continuous pre-training on massive monolingual corpora.

## Confidence
- **Medium confidence** in the core claim that "LLMs can be effectively adapted for multilingual MT" - Supported by competitive WMT24 results but lacks proper ablation studies.
- **Medium confidence** in the claim that "expanding tokenizer vocabulary improves translation efficiency" - Theoretically sound but lacks empirical evidence separating tokenizer impact from other components.
- **Low confidence** in the claim that "continuous pre-training on monolingual data is crucial for underrepresented languages" - Stated as key innovation but no ablation showing performance with and without this step.
- **Medium confidence** in the claim that "high-quality parallel data is more important than quantity" - Supported by selective data approach but no direct comparisons of quality vs quantity trade-offs.

## Next Checks
- **Validation Check 1: Ablation study on tokenizer expansion** - Run experiments comparing IKUN-C with an identical system using the original Mistral tokenizer but all other components the same.
- **Validation Check 2: Monolingual pre-training ablation** - Compare the full three-stage approach against a simpler two-stage approach that skips continuous pre-training.
- **Validation Check 3: Baseline comparison with established systems** - Benchmark against strong traditional multilingual MT systems like NLLB or mBART using the same evaluation metrics and datasets.