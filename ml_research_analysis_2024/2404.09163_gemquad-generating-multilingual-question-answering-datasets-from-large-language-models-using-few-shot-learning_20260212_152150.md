---
ver: rpa2
title: 'GeMQuAD : Generating Multilingual Question Answering Datasets from Large Language
  Models using Few Shot Learning'
arxiv_id: '2404.09163'
source_url: https://arxiv.org/abs/2404.09163
tags:
- data
- synthetic
- performance
- dataset
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GeMQuAD, a semi-supervised learning approach
  for generating high-quality multilingual question-answering datasets using few-shot
  learning. The method extends the WeakDAP framework to iteratively filter high-quality
  synthetic data generated through 1-shot in-context learning on AlexaTM 20B, without
  fine-tuning the LLM.
---

# GeMQuAD : Generating Multilingual Question Answering Datasets from Large Language Models using Few Shot Learning

## Quick Facts
- arXiv ID: 2404.09163
- Source URL: https://arxiv.org/abs/2404.09163
- Reference count: 13
- Generates high-quality multilingual QA datasets using few-shot learning without LLM fine-tuning, outperforming translation-augmented models by 0.22/1.68 F1/EM points for Hindi and 0.82/1.37 F1/EM points for Spanish on MLQA.

## Executive Summary
This paper introduces GeMQuAD, a semi-supervised learning approach for generating high-quality multilingual question-answering datasets using few-shot learning. The method extends the WeakDAP framework to iteratively filter high-quality synthetic data generated through 1-shot in-context learning on AlexaTM 20B, without fine-tuning the LLM. The approach outperforms machine translation-augmented models by 0.22/1.68 F1/EM points for Hindi and 0.82/1.37 F1/EM points for Spanish on MLQA, and surpasses models trained on English-only datasets by 5.05/6.50 F1/EM points for Hindi and 3.81/3.69 F1/EM points for Spanish. The framework is generalizable to other LLMs and tasks, providing a cost-effective solution for low-resource multilingual settings.

## Method Summary
The GeMQuAD approach uses AlexaTM 20B with 1-shot in-context learning to generate synthetic QA pairs in target languages (Hindi/Spanish) from XQUAD contexts. These pairs are filtered through an iterative semi-supervised process where a weak labeler (initially trained on English gold data) progressively identifies high-quality examples by matching its predicted answers with LLM-generated answers. The filtered silver dataset is then used to fine-tune XLM-R-Base sequentially (silver data first, then gold data) for k=2 rounds until performance plateaus or new data volume is less than 1% of total generated data. The final model is evaluated on MLQA and XQUAD datasets using F1 and Exact Match metrics.

## Key Results
- Outperforms XLMRbMT (translation-augmented) by 0.22/1.68 F1/EM points for Hindi and 0.82/1.37 F1/EM points for Spanish on MLQA
- Surpasses XLMRbbaseline (English-only) by 5.05/6.50 F1/EM points for Hindi and 3.81/3.69 F1/EM points for Spanish
- Demonstrates effective cross-lingual transfer, with improvements in German, Arabic, Vietnamese, and Chinese not included in student model's fine-tuning data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative semi-supervised filtering identifies high-quality synthetic QA pairs by leveraging a weak labeler that progressively improves its ability to match the LLM's answers.
- Mechanism: The weak labeler (initially trained on English gold data) evaluates synthetic QA pairs by predicting answer spans. Pairs where its predicted answer matches the LLM-generated answer are filtered into a silver dataset. As the student model is fine-tuned on this silver dataset, it becomes a stronger labeler, enabling it to find more high-quality pairs in subsequent iterations.
- Core assumption: Cross-lingual transfer from English gold data to target languages (Hindi/Spanish) is sufficient for the weak labeler to recognize valid QA pairs in the target language.
- Evidence anchors:
  - [abstract] "Through our approach, we iteratively identify high-quality data to enhance model performance, especially for low-resource multilingual setting..."
  - [section] "As the labeler has not seen any data in target language during training, the Extractive QA performance of the model on target language is limited, and mainly relies on the cross-lingual transfer ability of the base model..."
- Break condition: If cross-lingual transfer is poor, the weak labeler will misclassify many synthetic pairs, causing noisy silver data and model degradation.

### Mechanism 2
- Claim: Prioritizing fine-tuning on silver data first, then gold data, yields better performance than fine-tuning on combined data at once.
- Mechanism: Sequential fine-tuning allows the student model to first adapt to target-language patterns from the synthetic data, then refine with high-quality English gold data. This order prevents the model from being distracted by mixed-domain signals and reinforces learned patterns with authoritative examples.
- Core assumption: The synthetic silver data, though noisier than gold, captures useful target-language structure that benefits early-stage adaptation.
- Evidence anchors:
  - [section] "The student model gets fine-tuned first on the data filtered from semi-supervised approach (silver data) and then on the gold data... Similar analysis is reported by (Riabi et al., 2021) too in their works."
- Break condition: If the silver dataset is too noisy or too small, the early adaptation step may introduce harmful biases that the gold data cannot fully correct.

### Mechanism 3
- Claim: Using a large pre-trained LLM (AlexaTM 20B) with few-shot ICL can generate diverse synthetic QA pairs without the cost of fine-tuning the LLM.
- Mechanism: AlexaTM 20B is prompted with a single annotated example in the target language and generates Q&A pairs from unseen contexts. Diversity is encouraged by randomizing the example choice and sampling parameters (temperature, top_k, top_p). The method avoids expensive LLM fine-tuning while leveraging the model's in-context learning ability.
- Core assumption: In-context learning on a single example is sufficient for the LLM to infer the task structure and generate coherent, contextually relevant QA pairs.
- Evidence anchors:
  - [abstract] "...utilizing just a single annotated example in ICL to generate data, providing a cost-effective development process."
  - [section] "We have used sampling approach (do_sample=True), with the temperature set to 0.9, and randomly picked top_k & top_p ranging between 50 to 100 and 0.5 to 0.95 respectively for each example..."
- Break condition: If the LLM cannot generalize well from one example, generated pairs will be low quality, limiting the benefit of subsequent filtering.

## Foundational Learning

- Concept: In-context learning (ICL) in LLMs
  - Why needed here: ICL allows generating synthetic data without fine-tuning the LLM, keeping costs low while exploiting the model's reasoning ability.
  - Quick check question: How does changing temperature or top_p values affect the diversity and quality of ICL-generated QA pairs?

- Concept: Semi-supervised learning with weak supervision
  - Why needed here: The silver dataset acts as weak labels; the iterative filtering progressively improves label quality without requiring manual annotation in the target language.
  - Quick check question: What is the stopping criterion for the iterative filtering, and why is it important to monitor both model improvement and new data volume?

- Concept: Cross-lingual transfer in multilingual models
  - Why needed here: The student model (XLM-R-Base) is initially trained on English data; its ability to generalize to Hindi/Spanish is critical for both the weak labeler's initial performance and the downstream QA task.
  - Quick check question: How would you measure the effectiveness of cross-lingual transfer from English to Hindi/Spanish in this pipeline?

## Architecture Onboarding

- Component map:
  - LLM (AlexaTM 20B) -> 1-shot ICL prompt generator -> Synthetic QA pairs
  - Weak labeler (XLM-R-Base trained on English gold) -> Quality filter -> Silver dataset
  - Student model (XLM-R-Base) -> Iterative fine-tuning (silver -> gold) -> Final QA model
  - Evaluation datasets (MLQA, XQUAD) -> Performance metrics (F1/EM)

- Critical path:
  1. Generate synthetic QA pairs via ICL.
  2. Filter high-quality pairs using weak labeler.
  3. Fine-tune student model on silver, then gold data.
  4. Re-evaluate synthetic data with improved student model.
  5. Repeat until stopping criteria met.

- Design tradeoffs:
  - Cost vs. Quality: Using 1-shot ICL keeps LLM costs low but risks lower initial data quality; iterative filtering mitigates this.
  - Model size vs. Deployment: Large teacher LLM for generation but small student model for deployment balances performance and efficiency.
  - Data volume vs. Precision: Filtering reduces data size but improves precision; over-filtering risks losing useful examples.

- Failure signatures:
  - Stagnant silver dataset size across iterations -> Weak labeler cannot identify more good pairs.
  - Performance drop in early fine-tuning steps -> Silver data too noisy or misaligned with gold data.
  - Poor cross-lingual performance -> Weak labeler cannot transfer from English to target language effectively.

- First 3 experiments:
  1. Generate synthetic data using 1-shot ICL with AlexaTM 20B; verify diversity via sampling parameters.
  2. Apply weak labeler to synthetic dataset; measure initial F1/EM on MLQA/XQUAD and confirm cross-lingual transfer.
  3. Fine-tune student model sequentially (silver -> gold); compare performance against combined fine-tuning baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using different large language models (LLMs) beyond AlexaTM 20B for generating synthetic data in the GeMQuAD framework?
- Basis in paper: [explicit] The paper mentions that "this is a generalised framework which can be applied to data generated from any LLM and for any type of task."
- Why unresolved: The paper primarily focuses on using AlexaTM 20B for generating synthetic data. While it claims generalizability, it does not provide empirical evidence or comparison with other LLMs.
- What evidence would resolve it: Conducting experiments with different LLMs (e.g., GPT-3, PaLM) to generate synthetic data and comparing the performance of the resulting models on the QA task would provide insights into the impact of the choice of LLM.

### Open Question 2
- Question: How does the performance of the GeMQuAD approach vary with different numbers of iterations in the semi-supervised learning process?
- Basis in paper: [explicit] The paper states that the iterative process stops if the improvement in model performance for k rounds is below a threshold or the new data volume is less than a certain percentage.
- Why unresolved: The paper uses k=2 as the stopping criterion but does not explore how varying k or other parameters (e.g., the improvement threshold e, data volume threshold v) affects the model's performance.
- What evidence would resolve it: Conducting experiments with different values of k, e, and v to analyze their impact on the model's performance and identifying the optimal parameter settings.

### Open Question 3
- Question: How does the GeMQuAD approach perform on languages other than Hindi and Spanish, especially in terms of cross-lingual transfer?
- Basis in paper: [explicit] The paper mentions that "We observed good improvements in languages that are not included in the student model’s fine-tuning data, such as German, Arabic, Vietnamese, and Chinese."
- Why unresolved: While the paper provides some evidence of improved cross-lingual performance, it does not extensively evaluate the approach on a wide range of languages or provide a detailed analysis of cross-lingual transfer capabilities.
- What evidence would resolve it: Evaluating the GeMQuAD approach on a broader set of languages and conducting a comprehensive analysis of cross-lingual transfer performance, including comparisons with other cross-lingual methods.

## Limitations

- Heavy dependence on cross-lingual transfer from English gold data to target languages, which may not work well for morphologically rich or syntactically different languages
- Limited evaluation scope with only two target languages (Hindi and Spanish) and no ablation studies on iterative filtering hyperparameters
- No quantitative cost comparison to validate the "cost-effective" claim, and no testing on languages from different language families

## Confidence

**High Confidence**: The core mechanism of using few-shot ICL to generate synthetic data and iteratively filtering it with a weak labeler is technically sound and well-supported by the experimental results.

**Medium Confidence**: The claim that sequential fine-tuning (silver → gold) is superior to combined fine-tuning has theoretical support from related work, but lacks direct ablation studies within this specific framework.

**Low Confidence**: The assertion that this approach is "cost-effective" compared to alternatives lacks quantitative support and cost comparisons.

## Next Checks

1. **Cross-lingual transfer analysis**: Conduct a detailed study measuring how well the weak labeler (trained on English) performs on synthetic data in Hindi and Spanish before any iterative improvement. Report precision/recall metrics for the initial filtering step and analyze failure patterns to understand the limits of cross-lingual transfer in this setup.

2. **Hyperparameter sensitivity**: Perform an ablation study varying the stopping criteria (k, e, v) and sampling parameters (temperature, top_k, top_p) to quantify their impact on final QA performance. This would help establish the robustness of the method and identify optimal configurations for different resource levels.

3. **Generalization test**: Apply the GeMQuAD framework to a third target language (e.g., Swahili or Bengali) from a different language family than Hindi/Spanish. Compare performance against the reported results to assess whether the method generalizes beyond Indo-European languages and whether additional modifications are needed for truly low-resource settings.