---
ver: rpa2
title: An Efficient Post-hoc Framework for Reducing Task Discrepancy of Text Encoders
  for Composed Image Retrieval
arxiv_id: '2406.09188'
source_url: https://arxiv.org/abs/2406.09188
tags:
- text
- triplets
- image
- target
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the task discrepancy problem in projection-based\
  \ zero-shot composed image retrieval (ZS-CIR), where the pre-trained text encoder's\
  \ alignment task (text \u2194 image) conflicts with the target CIR task (image +\
  \ text \u2194 image). The authors propose a post-hoc framework called Reducing Task\
  \ Discrepancy of Text Encoders (RTD) that efficiently updates only the text encoder\
  \ using cheap text triplets, instead of expensive CIR triplets."
---

# An Efficient Post-hoc Framework for Reducing Task Discrepancy of Text Encoders for Composed Image Retrieval

## Quick Facts
- arXiv ID: 2406.09188
- Source URL: https://arxiv.org/abs/2406.09188
- Authors: Jaeseok Byun; Seokhyeon Jeong; Wonjae Kim; Sanghyuk Chun; Taesup Moon
- Reference count: 40
- Primary result: Achieves performance comparable to or exceeding resource-intensive synthetic CIR triplet-based approaches while requiring only 23 minutes of additional training on 4 A100 GPUs

## Executive Summary
This paper addresses the task discrepancy problem in projection-based zero-shot composed image retrieval (ZS-CIR), where the pre-trained text encoder's alignment task (text ↔ image) conflicts with the target CIR task (image + text ↔ image). The authors propose a post-hoc framework called Reducing Task Discrepancy of Text Encoders (RTD) that efficiently updates only the text encoder using cheap text triplets, instead of expensive CIR triplets. The key innovation is target-anchored text contrastive learning that aligns concatenated reference-conditioning text embeddings with target caption embeddings while maintaining pre-trained image-text alignment. RTD also introduces hard negative-based refined batch sampling and a refined concatenation scheme to further mitigate training-inference discrepancies.

## Method Summary
RTD is a post-hoc framework that updates only the text encoder of pre-trained CLIP models to address task discrepancy in projection-based ZS-CIR methods. The framework uses target-anchored text contrastive learning with text triplets (reference caption, conditioning text, target caption) instead of expensive CIR triplets. The method aligns concatenated reference-conditioning text embeddings with frozen target caption embeddings while maintaining the original image-text alignment. RTD incorporates refined batch sampling with hard negatives and a refined concatenation scheme that addresses modality gaps between text and image embeddings. The framework can be integrated with existing projection-based CIR methods and requires minimal additional training time while preserving fast inference speed.

## Key Results
- RTD consistently improves performance across multiple projection-based ZS-CIR methods (SEARLE, Pic2Word, LinCIR, Context-I2W, FTI4CIR) on benchmark datasets (CIRR, CIRCO, FashionIQ)
- Achieves performance comparable to or exceeding resource-intensive synthetic CIR triplet-based approaches
- Requires only 23 minutes of additional training on 4 A100 GPUs (up to 100× faster than full fine-tuning approaches)
- Maintains fast inference speed while improving retrieval accuracy

## Why This Works (Mechanism)

### Mechanism 1
Text encoder task discrepancy between pre-training (text ↔ image) and CIR task (image + text ↔ image) degrades performance. Pre-trained text encoder lacks training on concatenated embeddings (projected image + conditioning text), leading to misalignment with target images. Core assumption: Text encoder performance degrades when handling composite queries formed by concatenation rather than standard captions. Evidence: [abstract] "a task discrepancy of text encoders between the original pre-training task of the encoders (text ↔ image) and the target CIR task (image + text ↔ image), which potentially negatively impacts CIR performance"

### Mechanism 2
Target-anchored text contrastive learning reduces task discrepancy by aligning concatenated embeddings with target captions. Text encoder updates embeddings of concatenated (reference + conditioning) captions to align with frozen target caption embeddings, maintaining pre-trained image-text alignment. Core assumption: Frozen target caption embeddings serve as reliable anchors that preserve original CLIP alignment while allowing text encoder adaptation. Evidence: [abstract] "We devise a novel target-anchored text contrastive learning designed to enhance the capability of the text encoder for CIR"

### Mechanism 3
Refined batch sampling with hard negatives improves contrastive learning efficiency. Including (Tr+c, Tt) and (Tr, Tr) pairs in same batch creates implicit hard negatives that force better discrimination between concatenated and reference texts. Core assumption: Tr+c derived from Tr creates semantic similarity that makes them effective hard negatives for contrastive learning. Evidence: [abstract] "a hard negative-based refined batch sampling strategy"

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: The method relies on aligning embeddings through contrastive learning using symmetric InfoNCE loss to pull positive pairs together and push negative pairs apart
  - Quick check question: What is the mathematical form of the InfoNCE loss and how does it ensure alignment between positive pairs?

- Concept: Modality gap in multimodal embeddings
  - Why needed here: The refined concatenation scheme addresses the modality gap between text and image embeddings when using textual representations in place of visual ones
  - Quick check question: Why does directly replacing image embeddings with text embeddings in the projection module cause performance issues?

- Concept: Text-to-image retrieval pipeline
  - Why needed here: Understanding the complete pipeline from reference image → projection → concatenation → retrieval is essential for implementing and debugging the method
  - Quick check question: What are the exact steps from input query to final retrieved image in projection-based CIR methods?

## Architecture Onboarding

- Component map: Frozen CLIP visual encoder → Projection module (trained by baseline method) → RTD-updated text encoder → Text-to-image retrieval
- Critical path: Text encoder update loop: generate text triplets → extract embeddings → apply contrastive loss → update text encoder parameters
- Design tradeoffs: Text-only training vs full fine-tuning (efficiency vs complete adaptation), frozen vs trainable target anchors (stability vs flexibility)
- Failure signatures: Performance degradation when text encoder loses alignment with image encoder, minimal improvement when hard negative pairs are ineffective, training instability with poor text triplet quality
- First 3 experiments:
  1. Verify that replacing frozen text encoder with RTD-updated version improves CIRR validation R@1 score
  2. Test different noise scales in refined concatenation scheme to find optimal performance
  3. Compare performance with different text triplet generation strategies (rule-based vs LLM-based) to validate method robustness

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of RTD scale with increasing model size beyond ViT-G/14, particularly for very large vision-language models like BLIP-2 or Flamingo? The paper mentions evaluating RTD with ViT-B/32, ViT-L/14, and ViT-G/14 backbones, but notes "we have not yet explored or tested the extensibility of RTD to other CIR approaches that achieve strong performance, such as those utilizing human-annotated CIR triplets (supervised) [3], synthetically generated CIR triplets [16, 22, 45], or training-free methods [20]." The performance characteristics of RTD with much larger VLMs or alternative architecture families remain unknown.

### Open Question 2
What is the impact of RTD on zero-shot generalization to completely unseen domains or tasks beyond the CIR benchmarks tested? "We have primarily focused on evaluating the integrability of RTD with representative projection-based CIR methods" and "further examination of the data and the model is essential prior to practical deployment" regarding societal impacts. The paper evaluates only on specific CIR datasets (CIRR, CIRCO, FashionIQ, GeneCIS, COCO object composition) without testing generalization to novel domains or downstream tasks that might benefit from improved text-image alignment.

### Open Question 3
How robust is RTD to noisy or adversarial conditioning texts, and what are the failure modes when the conditioning text contains ambiguous or contradictory instructions? While RTD improves alignment between concatenated and target captions, the paper doesn't investigate how the method performs when conditioning texts are ambiguous, contradictory, or adversarially crafted to break the retrieval system.

## Limitations
- Text triplet generation relies on external LLM-based datasets (Compodiff) or rule-based templates, whose quality and diversity could impact performance
- Method assumes that target caption embeddings remain well-aligned with image embeddings after freezing, which may not hold for all domains
- Refined concatenation scheme's effectiveness depends on proper noise injection parameters that are not fully specified

## Confidence
- **High confidence**: The core mechanism of target-anchored text contrastive learning and its ability to reduce task discrepancy (supported by consistent performance improvements across 5 methods and 5 datasets)
- **Medium confidence**: The efficiency claims (100× faster training) due to limited information about baseline training times and potential hardware variations
- **Medium confidence**: The refined batch sampling strategy's contribution to performance, as ablation studies show improvements but don't isolate its specific impact from the refined concatenation scheme

## Next Checks
1. **Text triplet quality validation**: Generate a small set of text triplets using both LLM-based and rule-based methods, then manually verify semantic consistency and diversity to ensure the method's robustness to different triplet generation strategies.

2. **Target anchor alignment verification**: After training with RTD, measure the cosine similarity between frozen target caption embeddings and corresponding image embeddings across multiple CIR datasets to verify that the pre-trained alignment is maintained.

3. **Modality gap assessment**: Implement a controlled experiment comparing the refined concatenation scheme against direct text embedding replacement in the projection module to quantify the impact of the modality gap on retrieval performance.