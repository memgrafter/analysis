---
ver: rpa2
title: 'FlyKD: Graph Knowledge Distillation on the Fly with Curriculum Learning'
arxiv_id: '2403.10807'
source_url: https://arxiv.org/abs/2403.10807
tags:
- labels
- graph
- pseudo
- learning
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FlyKD proposes a novel knowledge distillation framework that generates
  pseudo labels on the fly from random graphs to overcome memory limitations while
  using curriculum learning to stabilize training on noisy pseudo labels. The method
  generates random graphs weighted by node degrees to create diverse pseudo labels,
  then gradually transitions from ground truth to these noisier labels during training.
---

# FlyKD: Graph Knowledge Distillation on the Fly with Curriculum Learning

## Quick Facts
- **arXiv ID**: 2403.10807
- **Source URL**: https://arxiv.org/abs/2403.10807
- **Reference count**: 17
- **Primary result**: 1.16% relative improvement over baseline models on PrimeKG drug repurposing task

## Executive Summary
FlyKD introduces a novel knowledge distillation framework that generates pseudo labels on the fly from random graphs to overcome memory limitations while using curriculum learning to stabilize training on noisy pseudo labels. The method generates random graphs weighted by node degrees to create diverse pseudo labels, then gradually transitions from ground truth to these noisier labels during training. Evaluated on PrimeKG for drug repurposing, FlyKD achieves 1.16% relative improvement over baseline models, outperforming both vanilla knowledge distillation and LSPGCN. The curriculum learning component proves critical, as removing it causes a 0.68% drop in performance. The work demonstrates that stabilizing optimization over noisy pseudo labels through curriculum learning is more important than the quality of pseudo labels themselves, opening new research directions for knowledge distillation on graphs.

## Method Summary
FlyKD addresses memory limitations in graph knowledge distillation by generating pseudo labels on the fly from random graphs rather than storing them. The framework creates random graphs weighted by node degrees to produce diverse pseudo labels, then employs curriculum learning to gradually transition from ground truth to these noisier labels during training. This approach allows the model to learn from large-scale graphs without the prohibitive memory costs of storing pseudo labels. The random graph generation strategy combined with curriculum learning creates a stable training process that outperforms both traditional knowledge distillation approaches and the LSPGCN baseline on the PrimeKG drug repurposing dataset.

## Key Results
- Achieves 1.16% relative improvement over baseline models on PrimeKG for drug repurposing
- Outperforms both vanilla knowledge distillation and LSPGCN baselines
- Curriculum learning component is critical - removing it causes 0.68% performance drop
- Demonstrates that stabilizing optimization over noisy pseudo labels is more important than pseudo label quality

## Why This Works (Mechanism)
The framework works by generating random graphs weighted by node degrees to create pseudo labels on the fly, then using curriculum learning to gradually transition from clean ground truth labels to these noisier pseudo labels. This approach overcomes memory limitations of traditional knowledge distillation while maintaining training stability. The random graph generation provides diverse pseudo labels that capture different aspects of the graph structure, while the curriculum learning schedule prevents the model from being overwhelmed by noise too early in training. The gradual transition allows the model to first learn from reliable ground truth labels before adapting to the noisier pseudo labels, resulting in better generalization and performance.

## Foundational Learning
- **Graph Knowledge Distillation**: Why needed - Traditional distillation requires storing pseudo labels for large graphs, which is memory-prohibitive. Quick check - Can the model generate useful pseudo labels without storing them?
- **Curriculum Learning**: Why needed - Gradual transition from clean to noisy labels prevents optimization instability. Quick check - Does the schedule effectively balance learning from ground truth versus pseudo labels?
- **Random Graph Generation**: Why needed - Creates diverse pseudo labels without storing them. Quick check - Do randomly generated graphs capture meaningful structural information?
- **Node Degree Weighting**: Why needed - Ensures random graphs reflect important structural properties. Quick check - Does weighting by degree improve pseudo label quality compared to uniform random generation?
- **Memory-Efficient Training**: Why needed - Enables knowledge distillation on graphs too large to store in memory. Quick check - What are the actual memory savings compared to baseline approaches?
- **Graph Structure Learning**: Why needed - Understanding how models learn from graph topology is crucial for effective distillation. Quick check - How does the model balance structural versus feature information?

## Architecture Onboarding

**Component Map**: Random Graph Generator -> Pseudo Label Generator -> Curriculum Scheduler -> GNN Model -> Loss Function

**Critical Path**: Random Graph Generator produces weighted random graphs → Pseudo Label Generator creates labels from these graphs → Curriculum Scheduler controls transition from ground truth to pseudo labels → GNN Model learns from both sources → Loss Function combines both label types

**Design Tradeoffs**: 
- Memory efficiency vs. pseudo label quality (random generation sacrifices some label quality for memory savings)
- Curriculum learning complexity vs. training stability (added complexity improves optimization)
- Random graph diversity vs. computational cost (more diverse graphs require more computation)

**Failure Signatures**:
- Poor performance indicates curriculum learning schedule issues or insufficient random graph diversity
- Memory overflow suggests random graph generation or pseudo label creation is not properly optimized
- Training instability suggests curriculum learning parameters need adjustment

**First Experiments**:
1. Test random graph generation quality by comparing pseudo label distributions to ground truth
2. Validate curriculum learning schedule by measuring training stability with different transition rates
3. Benchmark memory usage against baseline knowledge distillation approaches

## Open Questions the Paper Calls Out
None

## Limitations
- 1.16% relative improvement may not justify added complexity in all settings
- Random graph generation strategy may not generalize to domains where graph structure carries critical semantic information
- Results primarily validated on PrimeKG drug repurposing dataset, limiting generalizability

## Confidence
- **Memory efficiency claim**: Medium - Valid but depends on specific graph sizes and hardware
- **Performance improvement claim**: Medium - Statistically positive but relatively modest gain
- **Curriculum learning importance claim**: Medium - Demonstrated through ablation but mechanism needs more validation
- **Generalizability claim**: Low - Results primarily on one dataset and application domain

## Next Checks
1. Test FlyKD on diverse graph datasets beyond biomedical applications to assess generalizability across domains with varying structural properties
2. Conduct ablation studies to isolate the contribution of random graph generation quality versus curriculum learning schedule parameters
3. Evaluate the framework's performance on extreme scale graphs (billions of edges) to verify the claimed memory efficiency benefits in production-scale scenarios