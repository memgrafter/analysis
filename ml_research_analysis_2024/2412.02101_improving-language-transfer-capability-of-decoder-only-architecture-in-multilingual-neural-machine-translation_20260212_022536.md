---
ver: rpa2
title: Improving Language Transfer Capability of Decoder-only Architecture in Multilingual
  Neural Machine Translation
arxiv_id: '2412.02101'
source_url: https://arxiv.org/abs/2412.02101
tags:
- decoder-only
- language
- translation
- architecture
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the underperformance of decoder-only architectures
  in multilingual neural machine translation (MNMT) compared to encoder-decoder models.
  The authors attribute this to a lack of language transfer capability, where decoder-only
  architectures fail to encode source tokens with sufficient target language features.
---

# Improving Language Transfer Capability of Decoder-only Architecture in Multilingual Neural Machine Translation

## Quick Facts
- arXiv ID: 2412.02101
- Source URL: https://arxiv.org/abs/2412.02101
- Reference count: 40
- This paper proposes a Two-stage Decoder-only (TDO) architecture and Instruction-level Contrastive Learning (InstruCL) to improve language transfer capability in multilingual neural machine translation

## Executive Summary
This paper addresses the underperformance of decoder-only architectures in multilingual neural machine translation (MNMT) compared to encoder-decoder models. The authors identify that decoder-only architectures lack effective language transfer capability, particularly in encoding source tokens with sufficient target language features. To address this, they propose a Two-stage Decoder-only (TDO) architecture that divides the decoding process into two stages, with the first stage excluding target tokens to implicitly boost cross-language transfer. Additionally, they introduce Instruction-level Contrastive Learning (InstruCL) to supervise source token representations and improve zero-shot translation performance.

## Method Summary
The proposed approach consists of two main components: the Two-stage Decoder-only (TDO) architecture and Instruction-level Contrastive Learning (InstruCL). The TDO architecture splits the decoding process into two stages - the first stage processes source tokens without target tokens to enhance cross-language transfer, while the second stage follows standard decoder-only processing with target tokens. InstruCL is designed to supervise source token representations by contrasting similar and dissimilar instruction representations, helping the model better encode target language features during source processing. The method is evaluated on TED-19 and OPUS-100 datasets, demonstrating significant improvements in zero-shot translation while maintaining competitive performance in supervised translation.

## Key Results
- TDO models with enhancements achieve up to 3.39 BLEU improvement in zero-shot translation
- Improvements also observed in chrF++ (6.99), BERTScore (3.22), and COMET (4.81) metrics
- Models maintain competitive performance in supervised translation scenarios
- Results show consistent gains across multiple evaluation metrics

## Why This Works (Mechanism)
The paper's approach works by addressing the fundamental limitation of decoder-only architectures in multilingual settings - their inability to effectively encode source tokens with target language features. The two-stage decoding mechanism first processes source tokens in isolation, allowing the model to implicitly learn cross-language transfer without interference from target tokens. This is followed by standard decoding with target tokens. The Instruction-level Contrastive Learning further enhances this by providing explicit supervision on how source tokens should be represented with target language features, particularly beneficial for zero-shot translation scenarios where direct supervision is unavailable.

## Foundational Learning

**Multilingual Neural Machine Translation**: Understanding how neural models handle multiple language pairs simultaneously - needed to grasp the context of language transfer challenges; quick check: can you explain how MNMT differs from bilingual NMT?

**Decoder-only Architecture**: Understanding transformer-based models that generate text without separate encoding - needed to understand the baseline limitations; quick check: can you describe how decoder-only models differ from encoder-decoder models?

**Zero-shot Translation**: Translation between language pairs not seen during training - needed to appreciate the significance of zero-shot improvements; quick check: can you explain why zero-shot translation is challenging for decoder-only models?

**Contrastive Learning**: Training method that learns by comparing similar and dissimilar examples - needed to understand InstruCL mechanism; quick check: can you describe how contrastive learning differs from standard supervised learning?

**Language Transfer**: The ability to leverage knowledge from one language when translating to another - needed to understand the core problem being addressed; quick check: can you explain why language transfer is more challenging for decoder-only models?

## Architecture Onboarding

**Component Map**: Source tokens -> Stage 1 (No target tokens) -> Stage 2 (With target tokens) -> Output

**Critical Path**: Source input → Stage 1 processing → Stage 2 processing → Target output. The critical innovation is the separation of the first stage where target tokens are excluded, allowing implicit cross-language transfer before standard decoding begins.

**Design Tradeoffs**: The two-stage approach adds computational complexity but addresses the fundamental limitation of decoder-only models in multilingual settings. The tradeoff between increased computation and improved language transfer capability appears favorable given the consistent performance gains across multiple metrics.

**Failure Signatures**: If language transfer is still insufficient, the model may show limited improvement in zero-shot translation while supervised translation remains competitive. If InstruCL is ineffective, improvements may be limited to specific language pairs or domains.

**First Experiments**: 1) Test TDO architecture without InstruCL to isolate the contribution of two-stage decoding, 2) Apply InstruCL to standard decoder-only models to assess its standalone effectiveness, 3) Evaluate performance on additional multilingual datasets beyond TED-19 and OPUS-100.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Results rely heavily on TED-19 and OPUS-100 datasets, which may not represent real-world translation diversity
- Improvement margins (1-5 points) suggest limited impact on already well-performing systems
- Added computational complexity from InstruCL without clear guidance on scaling to larger models or more languages

## Confidence

**Decoder-only architecture limitations**: High confidence - solid empirical evidence across multiple baselines and comparisons

**Two-stage decoding mechanism effectiveness**: Medium confidence - improvements demonstrated but individual component contributions not fully disentangled

**Zero-shot translation improvements**: High confidence - consistent gains across multiple metrics (3.39 BLEU, 6.99 chrF++, 3.22 BERTScore, 4.81 COMET)

## Next Checks

1. Conduct ablation studies to quantify individual contributions of two-stage decoding and InstruCL through controlled experiments isolating each component

2. Test TDO architecture on additional multilingual datasets beyond TED-19 and OPUS-100 to verify generalizability across different language pairs and domains

3. Evaluate scaling behavior by testing approach on larger model sizes (10B+ parameters) and more language pairs to assess practical deployment potential