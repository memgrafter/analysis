---
ver: rpa2
title: Effective and Robust Adversarial Training against Data and Label Corruptions
arxiv_id: '2405.04191'
source_url: https://arxiv.org/abs/2405.04191
tags:
- data
- learning
- label
- training
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of defending deep neural networks
  against the co-existence of data perturbations (poisoning attacks) and label noise,
  which severely degrade model performance in real-world scenarios. The authors propose
  an Effective and Robust Adversarial Training (ERAT) framework that combines hybrid
  adversarial training with semi-supervised learning based on class-rebalancing sample
  selection.
---

# Effective and Robust Adversarial Training against Data and Label Corruptions

## Quick Facts
- arXiv ID: 2405.04191
- Source URL: https://arxiv.org/abs/2405.04191
- Authors: Peng-Fei Zhang; Zi Huang; Xin-Shun Xu; Guangdong Bai
- Reference count: 40
- One-line primary result: Achieves 8.3%, 13.5%, and 10.7% average improvements on CIFAR-10, CIFAR-100, and Tiny-ImageNet under various corruption settings

## Executive Summary
This paper addresses the critical challenge of defending deep neural networks against the simultaneous presence of data perturbations (poisoning attacks) and label noise in real-world scenarios. The authors propose an Effective and Robust Adversarial Training (ERAT) framework that combines hybrid adversarial training with semi-supervised learning based on class-rebalancing sample selection. By generating multiple types of adversarial perturbations and identifying clean labels through a novel scoring mechanism, ERAT demonstrates superior robustness compared to existing methods while maintaining competitive performance on clean data.

## Method Summary
ERAT integrates hybrid adversarial training with semi-supervised learning to defend against co-existing data and label corruptions. The framework first uses a scoring function to identify clean labels from noisy ones, then applies class-rebalancing to prevent majority-class bias in the selected dataset. During training, it employs uniform sampling of multiple adversarial perturbation types to enhance model robustness without prior knowledge of corruption specifics. The semi-supervised component utilizes pseudo-labels from weakly augmented unlabeled data to maximize information usage while relying on the clean labeled set for accurate supervision.

## Key Results
- Achieves 8.3% average improvement on CIFAR-10 across various corruption settings
- Achieves 13.5% average improvement on CIFAR-100 under different data and label noise combinations
- Achieves 10.7% average improvement on Tiny-ImageNet with multiple corruption types
- Outperforms state-of-the-art methods including DivideMix, UNICON, and MSPL in most scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid adversarial training with uniform sampling of multiple perturbation types enables defense against unknown corruption patterns without prior knowledge.
- Mechanism: The perturbation generation module uniformly samples from multiple ℓp-norm bounded attacks (ℓ∞ and ℓ2) to generate adversarial examples. The classifier is trained to maintain semantic consistency between original and perturbed data across augmentation views. This creates a robust model that generalizes to unseen corruption types.
- Core assumption: Uniform sampling from multiple perturbation types is sufficient to cover the space of potential real-world corruptions, and the model can learn to be invariant to these variations.
- Evidence anchors:
  - [abstract]: "We propose a hybrid adversarial training surrounding multiple potential adversarial perturbations"
  - [section III-D]: "we propose a hybrid adversarial training strategy by uniformly sampling different categories of perturbations in each training step"
  - [corpus]: No direct evidence found for effectiveness of uniform sampling specifically; corpus shows related work on robustness but not this exact sampling strategy
- Break condition: If the set of sampled perturbations doesn't adequately represent the space of real-world corruptions, or if uniform sampling misses critical perturbation patterns that require weighted sampling.

### Mechanism 2
- Claim: The scoring-based class-rebalancing sample selection effectively identifies and removes noisy labels while preventing class imbalance.
- Mechanism: A scoring function measures agreement between model predictions and given labels (S(x,y) = ||Pθ(x) - y||²₂). Samples with lower scores are more likely to have clean labels. The method then rebalances selection across classes by calculating class-level scores and adjusting sampling rates to prevent majority-class bias.
- Core assumption: The model's predictions on early training data can reliably distinguish clean from noisy labels, and class imbalance in selected samples significantly impacts model quality.
- Evidence anchors:
  - [section III-C]: "a novel scoring operation is designed to measure how much the model agrees with the given labels and disagrees with other labels" and "we propose to rearrange the separation result, ensuring neglected classes to be selected more frequently"
  - [corpus]: No direct evidence found for this specific scoring-based rebalancing approach; corpus shows related work on noisy label learning but not this exact method
- Break condition: If the scoring function fails to distinguish clean from noisy labels (e.g., early predictions are too uncertain), or if rebalancing introduces new biases by over-selecting minority classes.

### Mechanism 3
- Claim: Semi-supervised learning with pseudo-labels on unlabeled data enhances model robustness by utilizing information from samples with noisy labels.
- Mechanism: After separating data into labeled (clean) and unlabeled (noisy) sets, the model uses pseudo-labels from weakly augmented unlabeled data as supervision. This allows the model to learn from all available data while relying on the clean labeled set for accurate supervision.
- Core assumption: Model predictions on weakly augmented unlabeled data can serve as reliable pseudo-labels for semi-supervised learning, and the clean labeled set is sufficiently accurate to guide learning.
- Evidence anchors:
  - [section III-E]: "For unlabeled data, we use model predictions on weakly augmented data as Pseudo-labels" and the objective function L_u includes pseudo-label supervision
  - [corpus]: No direct evidence found for this specific combination of adversarial training with semi-supervised learning using pseudo-labels; corpus shows related work on both components separately
- Break condition: If pseudo-labels are too inaccurate due to model uncertainty, or if the clean labeled set is too small to provide effective supervision.

## Foundational Learning

- Concept: Adversarial training and its role in defense against data poisoning
  - Why needed here: The method relies on generating adversarial examples to improve model robustness against data perturbations
  - Quick check question: What is the fundamental difference between standard training and adversarial training in terms of the optimization objective?

- Concept: Semi-supervised learning with pseudo-labels
  - Why needed here: The method uses pseudo-labels from unlabeled data as supervision to improve learning efficiency
  - Quick check question: How does using pseudo-labels differ from using ground truth labels in terms of potential risks and benefits?

- Concept: Label noise detection and handling
  - Why needed here: The method needs to identify and remove noisy labels while maintaining class balance
  - Quick check question: What are the key challenges in distinguishing clean labels from noisy labels in the presence of class imbalance?

## Architecture Onboarding

- Component map:
  Perturbation Generation Module -> Classification Model -> Scoring Module -> Selection Module -> Augmentation Pipeline

- Critical path:
  1. Warmup phase with conventional CE loss
  2. Scoring phase to assess label quality
  3. Class-rebalancing selection to partition dataset
  4. Hybrid adversarial training with uniform perturbation sampling
  5. Semi-supervised learning with pseudo-labels

- Design tradeoffs:
  - Computational cost vs. robustness: Multiple perturbations increase training time but improve defense
  - Sample selection vs. label retention: Aggressive cleaning may remove useful data, conservative cleaning may retain noise
  - Rebalancing strength vs. accuracy: Stronger rebalancing prevents class bias but may select less reliable samples

- Failure signatures:
  - Degraded performance on clean data indicates over-regularization
  - Sensitivity to specific corruption types suggests insufficient perturbation coverage
  - Class imbalance in final model predictions indicates rebalancing issues

- First 3 experiments:
  1. Test baseline performance on clean data with varying ϵ values to find optimal defense budget
  2. Compare single-defense variants (ERAT∞ and ERAT2) against hybrid approach to validate uniform sampling benefit
  3. Evaluate class-rebalancing effectiveness by comparing with non-rebalanced selection under severe label noise

## Open Questions the Paper Calls Out

- How does the proposed ERAT method perform under different class imbalance scenarios in real-world datasets?
- What is the theoretical guarantee for the robustness of ERAT against data corruption?
- How does ERAT perform when data and label corruption processes are not independent?

## Limitations
- The paper lacks detailed implementation specifics for the scoring function and class-rebalancing algorithm, making faithful reproduction challenging.
- The uniform sampling strategy for perturbation generation is not empirically validated against alternative sampling strategies.
- The computational overhead of generating multiple adversarial perturbations is acknowledged but not thoroughly analyzed.

## Confidence

- **High Confidence**: The hybrid adversarial training framework combining multiple perturbation types is well-established and likely effective. The improvement claims (8.3%, 13.5%, 10.7% on CIFAR-10/100, Tiny-ImageNet) are supported by extensive experimental results.
- **Medium Confidence**: The scoring-based class-rebalancing sample selection mechanism is novel but lacks ablation studies isolating its contribution. The effectiveness of uniform sampling over weighted sampling strategies is assumed but not tested.
- **Low Confidence**: The semi-supervised learning component's contribution is difficult to isolate given the complex interplay with adversarial training and sample selection.

## Next Checks

1. **Component Ablation**: Implement ERAT without the class-rebalancing component to quantify its specific contribution to performance improvements across different noise types.
2. **Sampling Strategy Comparison**: Compare uniform perturbation sampling against weighted sampling strategies that prioritize certain attack types based on corruption severity.
3. **Real-world Validation**: Test ERAT on a real-world corrupted dataset (e.g., from common corruption benchmarks or real-world noise sources) to verify claims beyond controlled experimental conditions.