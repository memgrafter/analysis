---
ver: rpa2
title: 'TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness'
arxiv_id: '2402.12545'
source_url: https://arxiv.org/abs/2402.12545
tags:
- language
- question
- response
- arxiv
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TrustScore, a framework for evaluating the
  trustworthiness of LLM responses in closed-book question answering. TrustScore is
  based on the concept of Behavioral Consistency, which assesses whether an LLM's
  response aligns with its intrinsic knowledge by checking if the model consistently
  selects its original answer among distractors.
---

# TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness

## Quick Facts
- **arXiv ID**: 2402.12545
- **Source URL**: https://arxiv.org/abs/2402.12545
- **Reference count**: 40
- **Primary result**: TrustScore achieves strong correlations with human judgments in closed-book question answering, outperforming existing reference-free metrics

## Executive Summary
This paper introduces TrustScore, a framework for evaluating the trustworthiness of LLM responses in closed-book question answering. TrustScore is based on the concept of Behavioral Consistency, which assesses whether an LLM's response aligns with its intrinsic knowledge by checking if the model consistently selects its original answer among distractors. The framework can also integrate with fact-checking methods to evaluate alignment with external knowledge sources. Experiments on FLAN-T5, LLaMA, and GPT-3.5 show that TrustScore achieves strong correlations with human judgments, outperforming existing reference-free metrics and matching the performance of reference-based metrics.

## Method Summary
TrustScore consists of two main components: TrustBC (Behavioral Consistency Evaluator) and TrustFC (Fact-Checking Module). TrustBC generates multi-choice questions with distractors and checks if the LLM consistently selects its original response across multiple iterations. Distractors are generated using a priority-based substitution algorithm that prioritizes entities, nouns, and numbers. TrustFC retrieves evidence from external knowledge bases and uses entailment models to assess factual consistency. The final TrustScore integrates both components based on a priority hierarchy where factual consistency is prioritized over behavioral consistency.

## Key Results
- TrustScore achieves Pearson's r correlations of 0.73-0.87 with human judgments across different LLM models
- Outperforms existing reference-free metrics in closed-book question answering tasks
- Matches the performance of reference-based metrics while operating without gold references

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Behavioral consistency evaluation detects whether an LLM's response aligns with its parametric knowledge by checking if the model consistently selects its original answer among distractors.
- **Core assumption**: An LLM that consistently chooses its own response among distractors is more likely to have generated that response from its intrinsic knowledge rather than fabrication.
- **Break condition**: The mechanism breaks if the LLM shows inconsistent behavior by selecting different answers across iterations, suggesting the original response may not be grounded in its parametric knowledge.

### Mechanism 2
- **Claim**: Integration of behavioral consistency with fact-checking provides a more comprehensive evaluation of response trustworthiness.
- **Core assumption**: A trustworthy response should be both internally consistent (behavioral) and externally verifiable (factually consistent).
- **Break condition**: The integration breaks when external knowledge is unavailable or insufficient, at which point only behavioral consistency evaluation remains applicable.

### Mechanism 3
- **Claim**: The priority-based substitution algorithm generates high-quality distractors that make the behavioral consistency test more challenging and reliable.
- **Core assumption**: High-quality distractors that closely resemble the original response make it harder for the LLM to distinguish the correct answer, thus providing a more rigorous test of behavioral consistency.
- **Break condition**: The mechanism breaks if the substitution algorithm fails to generate sufficiently challenging distractors, making the behavioral consistency test too easy to pass.

## Foundational Learning

- **Concept**: Closed-book question answering
  - Why needed here: TrustScore is specifically designed for closed-book settings where answers must come from the LLM's parametric knowledge without external context
  - Quick check question: What distinguishes closed-book question answering from open-book or context-based question answering?

- **Concept**: Behavioral consistency
  - Why needed here: The core mechanism of TrustScore relies on evaluating whether an LLM consistently chooses its own responses, which is a form of behavioral consistency
  - Quick check question: How does behavioral consistency differ from factual consistency in evaluating LLM responses?

- **Concept**: Reference-free evaluation metrics
  - Why needed here: TrustScore operates without gold references, distinguishing it from traditional reference-based evaluation metrics
  - Quick check question: What are the advantages and limitations of reference-free evaluation compared to reference-based evaluation?

## Architecture Onboarding

- **Component map**: TrustScore -> TrustBC (Behavioral Consistency Evaluator) -> TrustFC (Fact-Checking Module)
- **Critical path**: (1) Generate original response to question, (2) Generate multi-choice questions with distractors, (3) Run behavioral consistency checks with multiple iterations, (4) If external knowledge is available, retrieve evidence and run fact-checking, (5) Integrate behavioral and factual consistency scores according to the priority hierarchy
- **Design tradeoffs**: The main tradeoff is between comprehensiveness and practicality. TrustBC provides reference-free evaluation but may miss factual errors that contradict external knowledge. TrustFC provides stronger factual verification but requires accessible external knowledge bases.
- **Failure signatures**: TrustScore may fail when: (1) The substitution algorithm generates poor distractors that are easily distinguishable from the original response, (2) The fact-checking module cannot retrieve relevant evidence from external knowledge bases, (3) The LLM exhibits inconsistent behavior due to randomness or lack of confidence rather than lack of knowledge, (4) The entailment model in fact-checking is unreliable or biased.
- **First 3 experiments**:
  1. Test TrustBC in isolation by generating multi-choice questions for a set of known answers and verifying that the LLM consistently selects its original responses across multiple iterations.
  2. Test the priority-based substitution algorithm by generating distractors for sample responses and evaluating their quality based on similarity to the original and ability to challenge the LLM.
  3. Test the integration mechanism by running TrustScore on responses with available external knowledge and verifying that factual consistency scores are properly weighted higher than behavioral consistency scores.

## Open Questions the Paper Calls Out

- How does TrustScore perform on long-form questions that require elaborate and in-depth answers?
- Can TrustScore be adapted to other natural language tasks beyond question answering?
- How does the performance of TrustScore change with different maximum check limits in the behavior consistency evaluator?

## Limitations

- The framework focuses on closed-book question answering, limiting generalizability to other LLM tasks
- Effectiveness depends heavily on the availability and quality of external knowledge bases
- The priority-based substitution algorithm could introduce bias by systematically replacing certain types of information

## Confidence

- **High Confidence**: The core concept of using behavioral consistency to evaluate LLM responses shows promise, with correlation with human judgments (0.73-0.87) demonstrating statistical significance
- **Medium Confidence**: The integration mechanism between TrustBC and TrustFC is logically sound but lacks extensive empirical validation across diverse domains
- **Low Confidence**: Specific implementation details of the priority-based substitution algorithm remain unclear, making it difficult to assess distractor quality consistency

## Next Checks

1. **Cross-Domain Validation**: Test TrustScore on LLM responses from domains beyond question answering (e.g., code generation, creative writing, summarization) to assess generalizability of the behavioral consistency mechanism.

2. **Distractor Quality Analysis**: Conduct a systematic evaluation of distractor quality by having human annotators rate distractor similarity to original responses and difficulty level, then correlate these ratings with TrustScore performance.

3. **Knowledge Base Sensitivity Analysis**: Evaluate how TrustScore performance varies with different external knowledge sources and when no external knowledge is available, quantifying the tradeoff between behavioral and factual consistency in various information scenarios.