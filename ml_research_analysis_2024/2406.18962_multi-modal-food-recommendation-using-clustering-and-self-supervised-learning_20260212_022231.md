---
ver: rpa2
title: Multi-modal Food Recommendation using Clustering and Self-supervised Learning
arxiv_id: '2406.18962'
source_url: https://arxiv.org/abs/2406.18962
tags:
- recommendation
- food
- multi-modal
- features
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLUSSL introduces a novel food recommendation framework that addresses
  the challenge of effectively incorporating multi-modal recipe information. The method
  uses unsupervised clustering to transform continuous multi-modal features into discrete
  prototype nodes, then constructs modality-specific bipartite graphs for each type
  of feature (ingredients, images, text).
---

# Multi-modal Food Recommendation using Clustering and Self-supervised Learning

## Quick Facts
- arXiv ID: 2406.18962
- Source URL: https://arxiv.org/abs/2406.18962
- Reference count: 38
- Key outcome: Achieves Recall@20 scores of 0.1372 and 0.1516 on Allrecipes and Food.com datasets, representing improvements of 16.5% and 10.3% over state-of-the-art methods

## Executive Summary
CLUSSL introduces a novel food recommendation framework that addresses the challenge of effectively incorporating multi-modal recipe information. The method uses unsupervised clustering to transform continuous multi-modal features into discrete prototype nodes, then constructs modality-specific bipartite graphs for each type of feature (ingredients, images, text). Graph convolutional networks are employed to learn representations from these graphs, while a self-supervised learning objective ensures independence between representations from different modalities.

## Method Summary
The CLUSSL framework processes multi-modal recipe data through a two-stage approach. First, it applies unsupervised clustering to continuous features from ingredients, images, and text to generate discrete prototype nodes. These prototypes serve as anchors in modality-specific bipartite graphs that connect recipes to their respective feature representations. Second, graph convolutional networks (GCNs) learn representations from these bipartite graphs, with a self-supervised learning objective that encourages independence between different modality representations. This approach addresses the sparsity and heterogeneity challenges inherent in multi-modal food recommendation data.

## Key Results
- Achieves Recall@20 scores of 0.1372 and 0.1516 on Allrecipes and Food.com datasets
- Outperforms state-of-the-art baselines by 16.5% and 10.3% improvements
- Demonstrates consistent performance gains across different evaluation metrics

## Why This Works (Mechanism)
The framework's effectiveness stems from its innovative approach to handling multi-modal recipe data. By clustering continuous features into discrete prototypes, it reduces dimensionality while preserving semantic relationships. The bipartite graph construction creates a structured representation that captures the relationships between recipes and their multi-modal features. The GCN-based learning leverages these graph structures to extract meaningful representations, while the self-supervised learning objective ensures that each modality contributes unique information rather than redundant features.

## Foundational Learning
- Unsupervised Clustering: Why needed - transforms continuous features into discrete prototypes for efficient processing. Quick check - verify prototype quality through internal clustering metrics.
- Bipartite Graph Construction: Why needed - creates structured relationships between recipes and features. Quick check - ensure graph connectivity and reasonable degree distributions.
- Graph Convolutional Networks: Why needed - learns representations from graph-structured data. Quick check - validate learned embeddings through visualization or downstream tasks.
- Self-supervised Learning: Why needed - ensures independence between modality representations. Quick check - measure cross-modal correlation before and after training.
- Prototype-Based Representation: Why needed - reduces dimensionality while preserving semantic information. Quick check - compare prototype-based vs raw feature performance.
- Multi-modal Fusion: Why needed - combines information from different data types. Quick check - ablation study on individual vs combined modalities.

## Architecture Onboarding
Component Map: Raw Features -> Clustering -> Prototype Nodes -> Bipartite Graphs -> GCNs -> Self-supervised Objective -> Final Representations

Critical Path: Clustering (quality of prototypes) → Bipartite Graph Construction (structural integrity) → GCN Learning (representation quality) → Self-supervised Objective (modality independence)

Design Tradeoffs: The framework trades computational complexity for improved representation quality by using clustering and graph-based approaches. The self-supervised objective adds training complexity but ensures better modality separation.

Failure Signatures: Poor clustering quality leads to noisy prototypes and degraded performance. Graph construction issues create information bottlenecks. Weak self-supervised learning fails to prevent modality correlation.

First Experiments:
1. Cluster quality validation - measure silhouette scores and visual inspection of prototype clusters
2. Graph connectivity analysis - verify bipartite graph structure and connectivity patterns
3. Self-supervised learning effectiveness - measure cross-modal correlation before and after training

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Clustering quality variability - unsupervised clustering introduces performance variability that requires rigorous validation
- Information bottleneck risks - bipartite graph construction may oversimplify complex multi-modal relationships
- Modality independence assumptions - the self-supervised objective assumes modalities should be independent, which may not hold for correlated features

## Confidence
High confidence: Experimental methodology and baseline comparisons
Medium confidence: Clustering effectiveness claims
Low confidence: Modality independence claims

## Next Checks
1. Conduct ablation studies to isolate the impact of clustering quality on recommendation performance
2. Implement cross-modal consistency checks to verify that independent representations maintain semantic coherence
3. Test framework scalability with larger, more diverse datasets to evaluate real-world applicability