---
ver: rpa2
title: 'GraphNarrator: Generating Textual Explanations for Graph Neural Networks'
arxiv_id: '2410.15268'
source_url: https://arxiv.org/abs/2410.15268
tags:
- explanation
- node
- graph
- explanations
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GraphNarrator is the first method to generate natural language
  explanations for Graph Neural Networks on Text-Attributed Graphs. It uses saliency-based
  explanations to create structured textual hints, then fine-tunes a language model
  with iteratively improved pseudo-labels via expert iteration to optimize faithfulness
  and brevity.
---

# GraphNarrator: Generating Textual Explanations for Graph Neural Networks

## Quick Facts
- arXiv ID: 2410.15268
- Source URL: https://arxiv.org/abs/2410.15268
- Reference count: 23
- Primary result: First method to generate natural language explanations for Graph Neural Networks on Text-Attributed Graphs

## Executive Summary
GraphNarrator introduces the first method for generating natural language explanations for Graph Neural Networks (GNNs) operating on Text-Attributed Graphs (TAGs). The approach addresses the challenge of making GNN decisions interpretable by combining saliency-based explanations with iterative refinement using expert iteration. By generating structured textual hints and fine-tuning a language model with progressively improved pseudo-labels, GraphNarrator achieves high-quality explanations that balance faithfulness to model decisions with brevity for human understanding.

## Method Summary
GraphNarrator employs a two-stage approach: first generating pseudo-labels through saliency-based explanations and LLM verbalization, then iteratively refining these explanations using expert iteration with three training objectives (faithfulness to important inputs, faithfulness to predictions, and brevity). The method is model-agnostic and can work with any TAG model architecture. It uses information-theoretic metrics like PMI to quantify explanation quality, achieving high simulatability scores while maintaining concise explanations through a knowledge distillation process.

## Key Results
- Achieves 0.97 simulatability score on average across three datasets
- Attains 0.418 PMI-10% and 0.315 brevity metrics on average
- Outperforms zero-shot LLM baselines in explanation quality

## Why This Works (Mechanism)

### Mechanism 1
GraphNarrator generates high-quality explanations through iterative improvement of pseudo-labels using expert iteration. The method creates structured textual hints from saliency-based explanations, then refines them through objective-based feedback to optimize faithfulness and brevity. The core assumption is that LLMs can generate meaningful explanations when guided by saliency information and iteratively refined through training objectives.

### Mechanism 2
Explanation quality is measured using information-theoretic metrics that quantify faithfulness to important inputs and predictions while encouraging brevity. Three objectives are proposed: faithfulness to important inputs (PMI between explanation and masked important tokens), faithfulness to predictions (PMI between explanation and output), and brevity (ratio of explanation length to input length).

### Mechanism 3
GraphNarrator achieves high simulatability by generating explanations that accurately reflect model predictions while remaining concise. The combination of saliency-based guidance, iterative refinement, and information-theoretic objectives creates explanations that are both faithful to model decisions and interpretable by humans.

## Foundational Learning

- **Graph Neural Networks (GNNs) and their operation on text-attributed graphs**: Understanding how GNNs process both node features and graph structure in text-attributed graphs is fundamental since GraphNarrator is designed specifically to explain GNN decisions on these inputs.
  - Quick check: How do GNNs aggregate information from both node features and graph structure in text-attributed graphs?

- **Saliency-based explanation methods and their limitations**: GraphNarrator builds upon saliency methods but addresses their limitations in providing human-understandable explanations for text-attributed graphs.
  - Quick check: What are the key limitations of node/edge importance scores when applied to text-attributed graphs?

- **Expert iteration and iterative self-improvement in machine learning**: The core innovation involves using expert iteration to iteratively improve explanation quality without ground truth labels.
  - Quick check: How does expert iteration differ from standard supervised learning when ground truth labels are unavailable?

## Architecture Onboarding

- **Component map**: Input TAG → Saliency explainer → Verbalizer → Pseudo-label generator LLM → Expert iteration (multiple loops) → Explainer LLM → Natural language explanation
- **Critical path**: The data flows from raw TAG input through saliency analysis, verbalization, initial explanation generation, iterative refinement, and finally to the end-to-end explainer
- **Design tradeoffs**: Model-agnostic approach provides flexibility but may sacrifice performance compared to specialized architectures; iterative improvement ensures higher quality at the cost of training time; brevity vs. completeness trade-off affects explanation detail
- **Failure signatures**: Low simulatability scores indicate explanations don't reflect predictions; low PMI scores suggest missing important features; high brevity ratio means explanations are too verbose; training instability shows expert iteration fails to converge
- **First 3 experiments**: 1) Run GraphNarrator on Cora dataset to verify explanation format; 2) Compare simulatability and PMI scores against zero-shot LLM baselines; 3) Test different saliency explainer choices (LIME vs. SHAP) on final explanation quality

## Open Questions the Paper Calls Out

### Open Question 1
What is the impact of using different saliency-based explainers (e.g., LRP, Input×Grad, Saliency) on the quality of GraphNarrator's explanations? The paper mentions that GraphNarrator is model-agnostic and can work with various saliency-based explainers, but does not compare their performance.

### Open Question 2
How does the choice of distribution for sampling thresholds (τ) in the masked token prediction task affect the faithfulness to important inputs (fS) metric? The paper states that different thresholds are sampled in each iteration but does not specify which distribution is used or discuss how different distributions might affect the metric.

### Open Question 3
How does the performance of GraphNarrator scale with increasing graph size and complexity? The paper discusses effectiveness on three datasets but does not address how the method performs on larger or more complex graphs, which is important for real-world applications.

## Limitations
- Lack of ground truth explanations forces reliance on potentially error-prone pseudo-labels that may propagate through expert iteration
- Method's effectiveness depends heavily on the quality of underlying saliency explainer, with different methods potentially producing inconsistent results
- Trade-off between faithfulness and brevity may not be robust across diverse datasets or model architectures, and scalability to larger graphs hasn't been demonstrated

## Confidence

**High Confidence**: The general approach of combining saliency-based explanations with LLM generation is methodologically sound and well-supported by related work in XAI.

**Medium Confidence**: Specific implementation details and hyperparameter choices are not fully specified, making exact replication challenging and claimed performance metrics difficult to verify.

**Low Confidence**: The assumption that iterative expert iteration with pseudo-labels can reliably improve explanation quality without ground truth supervision is the weakest link in the methodology.

## Next Checks

1. **Implementation Validation**: Replicate the GraphNarrator pipeline on Cora dataset and verify that explanations are generated in the expected hierarchical Saliency Paragraph format, comparing qualitatively against paper examples.

2. **Metric Reliability Test**: Evaluate consistency of PMI-based metrics across different saliency explainers (LIME, SHAP, LRP) on the same dataset to measure variation in faithfulness scores.

3. **Simulatability Benchmark**: Conduct controlled user study where participants predict model outputs based solely on GraphNarrator explanations, comparing prediction accuracy against baseline methods to verify claimed 0.97 simulatability score.