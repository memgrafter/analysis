---
ver: rpa2
title: Evaluating Large Language Models as Generative User Simulators for Conversational
  Recommendation
arxiv_id: '2403.09738'
source_url: https://arxiv.org/abs/2403.09738
tags:
- simulators
- movies
- human
- movie
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a protocol to evaluate LLM-based user simulators
  for conversational recommendation. The protocol includes five tasks that measure
  key properties: mentioning diverse items, expressing binary and open-ended preferences,
  requesting recommendations, and giving feedback.'
---

# Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation

## Quick Facts
- arXiv ID: 2403.09738
- Source URL: https://arxiv.org/abs/2403.09738
- Authors: Se-eun Yoon; Zhankui He; Jessica Maria Echterhoff; Julian McAuley
- Reference count: 38
- Primary result: LLM-based user simulators show significant gaps from human behavior in conversational recommendation tasks, but prompting with interaction history and personality traits can reduce these gaps.

## Executive Summary
This paper introduces a comprehensive protocol to evaluate large language model-based user simulators for conversational recommendation systems. The protocol consists of five tasks designed to measure key behaviors including item diversity, preference expression, recommendation requests, and feedback coherence. Through systematic evaluation of various prompting strategies, the study reveals that while LLM simulators capture some human-like behaviors, they exhibit significant deviations in item diversity, preference alignment, and request personalization compared to real human users. The research demonstrates that interaction history prompting and personality trait addition can substantially improve simulator performance.

## Method Summary
The study evaluates LLM-based user simulators using a five-task protocol applied to movie recommendation scenarios. The method uses four datasets (ReDial conversations, Reddit recommendation posts, MovieLens ratings, IMDB reviews) and three language models (gpt-3.5-turbo, gpt-4, text-davinci-003) with various prompting strategies. Simulators are prompted with demographic information, personality traits (pickiness levels), and interaction history. Evaluation metrics include entropy for diversity, correlation coefficients for preference alignment, sentiment analysis for preference expression, and coherence measures for feedback. All evaluations are conducted zero-shot without task-specific training.

## Key Results
- LLM simulators show significantly lower item diversity compared to humans, with entropy scores revealing popularity bias in generated items.
- Preference alignment improves substantially when simulators are endowed with personality traits, particularly pickiness levels.
- Interaction history prompting yields higher item diversity than demographic information alone, suggesting strong conditioning effects.
- Recommendation requests from simulators are less personalized and diverse than human requests, often defaulting to generic formats.
- Feedback coherence remains a challenge, with simulators occasionally generating incoherent responses to subtle recommendation nuances.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The protocol decomposes evaluation into five independent tasks to capture distinct simulator behaviors.
- Mechanism: By isolating behaviors (item mention diversity, binary preference correlation, open-ended preference expression, request diversity, and feedback coherence), the protocol enables systematic measurement of deviations from human behavior.
- Core assumption: Each task independently captures a necessary dimension of user simulator realism without overlap.
- Evidence anchors:
  - [abstract] "This protocol is comprised of five tasks, each designed to evaluate a key property..."
  - [section 2] "Each task prompts a simulator and stores the outcomes of a population of simulators."
  - [corpus] Weak evidence - the corpus mentions similar decomposition but lacks detailed validation.

### Mechanism 2
- Claim: Prompting with interaction history (IH) significantly improves item diversity in simulator outputs.
- Mechanism: Providing specific interaction histories ("trigger items") conditions the simulator to generate more diverse and personalized item mentions, mimicking real user behavior.
- Core assumption: Interaction history acts as a strong conditioning signal that overcomes the simulator's tendency toward popular items.
- Evidence anchors:
  - [section 4] "Comfortingly, prompting with interaction history yields much higher diversity than prompting with demographic information..."
  - [section 4] "This suggests that interaction history ('trigger' items) is a strong condition for generating diverse items."
  - [corpus] Weak evidence - the corpus mentions IH but doesn't provide comparative results.

### Mechanism 3
- Claim: Adding personality traits (specifically pickiness) to demographic information improves preference alignment and realism.
- Mechanism: Simulating different levels of pickiness (not picky, moderately picky, extremely picky) creates more varied and realistic preference distributions, better matching human behavior patterns.
- Core assumption: Real users have varying levels of pickiness that affect their preferences, and simulators need to reflect this variability.
- Evidence anchors:
  - [section 4] "Endowing simulators with varying levels of pickiness not only diversifies preferences but also improves correlation..."
  - [section 4] "This suggests that picky simulators can successfully discern low-rated movies."
  - [corpus] Weak evidence - the corpus mentions personality traits but lacks quantitative results.

## Foundational Learning

- Concept: Evaluation protocol design for generative systems
  - Why needed here: Traditional evaluation metrics don't capture the nuanced behaviors required for conversational recommendation simulators.
  - Quick check question: Can you explain why standard accuracy metrics are insufficient for evaluating generative user simulators?

- Concept: Interaction history as conditioning signal
  - Why needed here: Understanding how past interactions influence future behavior is crucial for creating realistic simulators.
  - Quick check question: How would you modify the protocol if interaction history had no effect on item diversity?

- Concept: Personality trait simulation in language models
  - Why needed here: Real users exhibit diverse personality traits that affect their preferences and behaviors.
  - Quick check question: What other personality traits beyond pickiness might be important for conversational recommendation?

## Architecture Onboarding

- Component map:
  Data processing pipeline (ReDial, Reddit, MovieLens, IMDB datasets) -> Simulator execution engine (OpenAI models with various prompting strategies) -> Evaluation metrics calculator (entropy, correlation coefficients, sentiment analysis) -> Result aggregation and comparison system -> Visualization components for result analysis

- Critical path:
  1. Dataset preparation and preprocessing
  2. Simulator population generation with task-specific prompts
  3. Simulator execution and output collection
  4. Evaluation metric computation
  5. Human vs. simulator comparison
  6. Result visualization and analysis

- Design tradeoffs:
  - Zero-shot evaluation vs. fine-tuning for better performance
  - Dataset diversity vs. domain-specific accuracy
  - Model selection complexity vs. evaluation comprehensiveness
  - Prompt complexity vs. execution efficiency

- Failure signatures:
  - Low entropy in item mentions indicates popularity bias
  - Constant positive rates regardless of ratings indicate poor preference alignment
  - Low word diversity with high embedding diversity suggests repetitive vocabulary usage
  - High proportion of incoherent feedback indicates understanding gaps

- First 3 experiments:
  1. Run ItemsTalk task with all baselines and compare entropy scores to establish baseline diversity
  2. Execute BinPref task with demographic vs. demographic+pickiness configurations to measure preference alignment improvement
  3. Implement RecRequest task to quantify request personalization and diversity differences between simulators and humans

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can prompting with interaction history consistently improve item diversity across different language models and datasets?
- Basis in paper: [explicit] The paper found that prompting with interaction history yielded higher diversity in item mentions compared to demographic information, but also noted variations across different models and datasets.
- Why unresolved: The paper's experiments were limited to a specific set of models and datasets, and the effects of interaction history prompting may vary depending on the model's architecture, training data, and the nature of the dataset.
- What evidence would resolve it: Conducting a comprehensive study with a wider range of language models and datasets, systematically varying the interaction history prompting strategies, and comparing the results to a control group using demographic information prompting.

### Open Question 2
- Question: How does the addition of personality traits, such as pickiness, affect the alignment of simulated user preferences with human preferences?
- Basis in paper: [explicit] The paper observed that adding pickiness personality improved the correlation between simulator preferences and human preferences, but the extent of improvement and its generalizability to other personality traits remain unclear.
- Why unresolved: The paper's experiments focused on a specific personality trait (pickiness) and its impact on preference alignment. The effects of other personality traits and their interactions with different models and datasets are not explored.
- What evidence would resolve it: Designing experiments to systematically vary personality traits, model choices, and datasets, and measuring the impact on preference alignment using correlation coefficients and other relevant metrics.

### Open Question 3
- Question: What are the key factors that contribute to the diversity of recommendation requests generated by language model-based simulators?
- Basis in paper: [inferred] The paper found that simulators generated less diverse and less personalized recommendation requests compared to real users, suggesting that the models may lack the ability to capture the nuances and context of user requests.
- Why unresolved: The paper's analysis focused on the diversity of requests at a surface level, but did not delve into the underlying factors that influence request diversity, such as the model's understanding of user intent, context, and preferences.
- What evidence would resolve it: Conducting a detailed analysis of the factors influencing request diversity, including the model's ability to understand user intent, context, and preferences, and exploring techniques to improve the model's understanding and generation of diverse and personalized requests.

## Limitations
- The study focuses exclusively on movie recommendations, limiting generalizability to other recommendation domains.
- Zero-shot prompting without fine-tuning may not represent the best achievable performance from LLM-based simulators.
- The evaluation relies on a limited set of prompting strategies and model configurations without exploring temperature settings or other generation parameters.

## Confidence

High confidence in protocol design and task decomposition, as evidenced by systematic comparisons between simulator outputs and human behavior across multiple datasets.

Medium confidence in specific quantitative findings due to reliance on zero-shot prompting, domain limitation to movies, and limited exploration of generation parameters.

Low confidence in scalability claims, as the study only tests a limited set of prompting strategies and model configurations without fine-tuning or parameter sensitivity analysis.

## Next Checks

1. **Domain Generalization Test**: Apply the protocol to non-movie recommendation domains (e.g., restaurants, books) to evaluate whether the identified simulator behaviors and proposed improvements generalize across recommendation types.

2. **Parameter Sensitivity Analysis**: Systematically vary temperature settings, max tokens, and other generation parameters to quantify their impact on simulator performance across all five tasks, particularly for interaction history prompting effectiveness.

3. **Human-in-the-Loop Validation**: Conduct a controlled user study where human participants interact with both simulator variants (with and without personality traits) to measure qualitative differences in conversation quality and preference alignment beyond automated metrics.