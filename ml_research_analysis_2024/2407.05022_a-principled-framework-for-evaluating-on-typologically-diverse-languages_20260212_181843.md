---
ver: rpa2
title: A Principled Framework for Evaluating on Typologically Diverse Languages
arxiv_id: '2407.05022'
source_url: https://arxiv.org/abs/2407.05022
tags:
- language
- languages
- sampling
- typological
- diverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of evaluating multilingual NLP
  models across typologically diverse languages. The authors propose a principled
  framework for sampling languages based on typological diversity, arguing that phylogenetic
  groupings are insufficient proxies for typological diversity.
---

# A Principled Framework for Evaluating on Typologically Diverse Languages

## Quick Facts
- arXiv ID: 2407.05022
- Source URL: https://arxiv.org/abs/2407.05022
- Authors: Esther Ploeger; Wessel Poelman; Andreas Holck Høeg-Petersen; Anders Schlichtkrull; Miryam de Lhoneux; Johannes Bjerva
- Reference count: 33
- Primary result: Framework consistently retrieves more typologically diverse language samples than previous methods, providing more realistic multilingual model performance estimates

## Executive Summary
This paper addresses the critical problem of evaluating multilingual NLP models across typologically diverse languages. The authors argue that current practices of using phylogenetic groupings as proxies for typological diversity are flawed and inconsistent. They propose a principled framework that calculates typological distances between languages using features from the Grambank database, then applies MaxSum or MaxMin algorithms to select diverse language sets. The framework demonstrates superior performance in retrieving typologically diverse samples and provides more realistic estimates of multilingual model performance compared to convenience sampling.

## Method Summary
The framework works by first calculating pairwise typological distances between languages using Euclidean distance on binary features from Grambank, with special handling for missing values. It then applies either MaxSum (maximizing total distance) or MaxMin (maximizing minimum distance) sampling algorithms to select diverse language sets. The method includes preprocessing steps like normalization, binarization, language cropping, and macro-language removal. The framework is evaluated using diversity metrics (MPD, FVO, FVI, entropy) and downstream task performance on topic classification, machine translation, and subword segmentation tasks.

## Key Results
- MaxSum and MaxMin sampling algorithms consistently retrieve more typologically diverse language samples than convenience, random, or phylogenetic sampling methods
- Typologically diverse sampling leads to more realistic performance estimates for multilingual models compared to convenience sampling
- The framework's performance plateau around 20 languages suggests this may be an adequate sample size for feature coverage
- Experiments demonstrate the framework's effectiveness across multiple downstream tasks including topic classification, machine translation, and subword segmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Phylogenetic language groupings are insufficient proxies for typological diversity
- Mechanism: Languages grouped by phylogeny may have low feature overlap, meaning they don't capture typological distance
- Core assumption: Typological similarity correlates with phylogenetic similarity
- Evidence anchors:
  - [abstract] "However, 'typologically diverse' language samples have been found to vary considerably in this regard, and popular sampling methods are flawed and inconsistent."
  - [section] "Empirical Arguments. Beyond theoretical reasons, it is unclear to what extent phylogenetic groupings actually imply typological diversity."
  - [corpus] Found 25 related papers; FMR range 0.52-0.65 suggests moderate topical relevance but no direct validation of phylogeny-insufficiency claim
- Break condition: If phylogenetic distance consistently predicts feature overlap across diverse language families

### Mechanism 2
- Claim: MaxSum and MaxMin sampling retrieve more typologically diverse language sets than convenience or random sampling
- Mechanism: By calculating pairwise typological distances and maximizing either total (MaxSum) or minimum (MaxMin) distance, the framework selects outlier languages
- Core assumption: Higher pairwise distances correspond to greater typological diversity
- Evidence anchors:
  - [abstract] "Our systematic methods consistently retrieve more typologically diverse language selections than previous methods in NLP."
  - [section] "Figure 6 shows that our methods consistently retrieve more diverse samples than all baselines."
  - [corpus] No direct experimental replication in corpus, but FMR scores suggest topical alignment
- Break condition: If downstream task performance does not improve on typologically diverse samples

### Mechanism 3
- Claim: Typologically diverse sampling leads to more realistic multilingual model performance estimates
- Mechanism: By including languages with diverse morphosyntactic properties, the evaluation set better reflects true cross-linguistic generalization
- Core assumption: Models tested on diverse languages generalize better across unseen typological features
- Evidence anchors:
  - [abstract] "Experiments on downstream tasks...demonstrate that typologically diverse sampling provides more realistic performance estimates than convenience sampling."
  - [section] "We provide evidence that this affects generalizability in multilingual model evaluation."
  - [corpus] Corpus includes downstream task papers (Taxi1500, FloRes-200, tokenization), supporting evaluation claim
- Break condition: If convenience sampling consistently matches or exceeds diverse sampling in downstream generalization

## Foundational Learning

- Concept: Typological distance calculation from binary features
  - Why needed here: Forms the core of the sampling framework; enables diversity measurement
  - Quick check question: How does the framework handle missing feature values in distance computation?
- Concept: Sampling as optimization problem (MaxSum/MaxMin diversity)
  - Why needed here: Determines which languages to select given a distance matrix
  - Quick check question: What is the computational complexity of brute-force MaxSum vs. the implemented heuristic?
- Concept: Downstream evaluation design for multilingual models
  - Why needed here: Demonstrates practical impact of diverse sampling on model assessment
  - Quick check question: How does zero-shot evaluation differ from fine-tuned evaluation in measuring generalization?

## Architecture Onboarding

- Component map: Typological feature extraction → distance matrix computation → MaxSum/MaxMin sampling → evaluation pipeline → downstream task modules
- Critical path: Feature vector creation → distance calculation → sampling algorithm → sample evaluation → downstream task benchmarking
- Design tradeoffs: Grambank coverage vs. representativeness; MaxSum outlier inclusion vs. MaxMin independence; computational cost vs. diversity gain
- Failure signatures: Low MPD/FVO/FVI metrics despite sampling; downstream performance no better than convenience baseline; sampling frame too small for desired diversity
- First 3 experiments:
  1. Run framework with default Grambank features and compare MaxSum vs. MaxMin for k=10
  2. Apply framework to Taxi1500 and compute accuracy distributions across sampling methods
  3. Evaluate tokenization fairness across Latin-script languages using diverse vs. convenience samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice between MaxSum and MaxMin sampling algorithms affect downstream performance in specific NLP tasks?
- Basis in paper: [explicit] The paper states that MaxSum provides more conservative performance estimates than MaxMin, with MaxSum being more suitable for worst-case evaluation scenarios while MaxMin better reflects general distributions
- Why unresolved: While the paper demonstrates differences in downstream performance, it doesn't systematically investigate how task-specific characteristics might make one algorithm preferable over the other
- What evidence would resolve it: Controlled experiments comparing MaxSum and MaxMin sampling across multiple task types (classification, generation, segmentation) while analyzing how typological feature distributions correlate with performance variations

### Open Question 2
- Question: What is the optimal sample size for typologically diverse language sampling that balances representativeness with practical feasibility?
- Basis in paper: [explicit] The paper shows that diversity metrics (FVI, entropy) tend to plateau around 20 languages, suggesting this might be adequate for feature coverage, but doesn't provide systematic guidance on sample size selection
- Why unresolved: The paper identifies a potential plateau point but doesn't investigate whether this holds across different typological databases, task types, or research objectives
- What evidence would resolve it: Systematic analysis of diversity metric saturation points across multiple corpus frames, tasks, and research objectives to establish principled guidelines for sample size selection

### Open Question 3
- Question: How does bibliographic bias in typological databases affect the validity of conclusions drawn from typologically diverse sampling?
- Basis in paper: [explicit] The paper acknowledges that Grambank has incomplete feature coverage and mentions that bibliographic bias in typological databases is less severe than in NLP research, but doesn't quantify this effect
- Why unresolved: The paper doesn't investigate how missing typological features or uneven documentation across languages might skew the calculated distances and subsequent sampling results
- What evidence would resolve it: Empirical studies comparing sampling results using different subsets of typological features, analysis of how missing data patterns correlate with sampling outcomes, and validation of sampling diversity using independent typological datasets

## Limitations
- The framework's reliance on Grambank data introduces coverage gaps for languages with sparse typological documentation
- Binary feature space may oversimplify complex typological relationships and miss nuanced similarities
- The paper lacks thorough validation of the framework's performance on language families with sparse Grambank representation

## Confidence
- **High Confidence**: The framework's ability to increase measured typological diversity (MPD, FVO, FVI, entropy) compared to baseline sampling methods
- **Medium Confidence**: The claim that typologically diverse sampling provides more realistic performance estimates, requiring further validation across different tasks
- **Medium Confidence**: The assertion that phylogenetic groupings are insufficient proxies for typological diversity, lacking direct empirical validation

## Next Checks
1. Replicate the framework's sampling on a held-out set of languages not present in Grambank to assess generalization to typologically undersampled regions
2. Compare the downstream task performance of typologically diverse samples against samples selected using alternative diversity criteria (e.g., phoneme inventory diversity, syntactic feature diversity)
3. Conduct a sensitivity analysis on the framework's performance with respect to the choice of diversity metric (MPD vs. FVO vs. FVI vs. entropy) and sampling algorithm (MaxSum vs. MaxMin)