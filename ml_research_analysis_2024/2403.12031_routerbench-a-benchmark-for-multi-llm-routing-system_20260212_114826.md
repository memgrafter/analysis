---
ver: rpa2
title: 'RouterBench: A Benchmark for Multi-LLM Routing System'
arxiv_id: '2403.12031'
source_url: https://arxiv.org/abs/2403.12031
tags:
- router
- cost
- routing
- performance
- routers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RouterBench introduces a comprehensive benchmark and theoretical
  framework for evaluating LLM routing systems, addressing the challenge of selecting
  optimal models across diverse tasks while balancing cost and performance. The benchmark
  comprises over 405k inference outcomes from 11 models across 8 datasets, enabling
  systematic evaluation of routing strategies without additional inference.
---

# RouterBench: A Benchmark for Multi-LLM Routing System

## Quick Facts
- arXiv ID: 2403.12031
- Source URL: https://arxiv.org/abs/2403.12031
- Authors: Qitian Jason Hu; Jacob Bieker; Xiuyu Li; Nan Jiang; Benjamin Keigwin; Gaurav Ranganath; Kurt Keutzer; Shriyash Kaustubh Upadhyay
- Reference count: 40
- Primary result: Introduces comprehensive benchmark with over 405k inference outcomes from 11 models across 8 datasets for evaluating LLM routing systems

## Executive Summary
RouterBench introduces a comprehensive benchmark and theoretical framework for evaluating LLM routing systems, addressing the challenge of selecting optimal models across diverse tasks while balancing cost and performance. The benchmark comprises over 405k inference outcomes from 11 models across 8 datasets, enabling systematic evaluation of routing strategies without additional inference. The theoretical framework defines AIQ (Average Improvement in Quality) as a unified metric for comparing routing systems, incorporating linear interpolation and non-decreasing convex hulls to characterize cost-performance trade-offs. Experiments demonstrate that routing systems can achieve 2-5× cost reductions compared to individual LLMs while maintaining comparable performance.

## Method Summary
RouterBench constructs a comprehensive benchmark by collecting inference outcomes from 11 different LLMs across 8 diverse datasets, totaling over 405k samples. The theoretical framework introduces AIQ as a unified metric for evaluating routing systems, using linear interpolation and non-decreasing convex hulls to analyze cost-quality trade-offs. Three routing strategies are implemented: predictive routers (KNN and MLP) that estimate performance scores for each model, cascading routers that sequentially evaluate cheaper models first, and a Zero router baseline that constructs optimal convex hulls. The evaluation methodology enables comparison across different cost-quality points without requiring additional inference runs.

## Key Results
- Zero router achieves AIQ of 0.763 as strong baseline
- KNN and MLP routers achieve AIQs of 0.773 and 0.769 respectively
- Cascading routers show superior performance with AIQ up to 0.901
- Routing systems achieve 2-5× cost reductions while maintaining comparable performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero router serves as a strong baseline by constructing a non-decreasing convex hull of all LLMs
- Mechanism: The Zero router probabilistically selects among LLMs to maximize expected output quality for any given cost level, creating an optimal cost-performance trade-off curve
- Core assumption: Linear interpolation between routers can achieve any point on the convex hull connecting their cost-quality coordinates
- Evidence anchors:
  - [abstract]: "The zero router (baseline) achieves an AIQ of 0.763, while KNN and MLP routers achieve AIQs of 0.773 and 0.769 respectively on ROUTER BENCH"
  - [section]: "We define the Zero router (Rzero) as a router that selects LLMs from {LLM1, ..., LLMm} based on their collective non-decreasing convex hull"
  - [corpus]: Weak - related papers mention routing systems but don't specifically discuss zero router baseline
- Break condition: When no single LLM dominates across all tasks, the convex hull construction may not capture the true optimal routing

### Mechanism 2
- Claim: Cascading routers with low error rates can achieve superior performance by sequentially evaluating cheaper models first
- Mechanism: Queries are routed through increasingly expensive models until one meets quality threshold or budget is exhausted
- Core assumption: The scoring function g can accurately predict whether a model's output meets quality requirements
- Evidence anchors:
  - [abstract]: "Cascading routers show superior performance with AIQ values up to 0.901, particularly effective in compound AI systems"
  - [section]: "A key component of its operation is a scoring function g : text → [0, 1] paired with a threshold t (the 'judge')"
  - [corpus]: Weak - corpus mentions PROTEUS and CARROT but doesn't detail cascading router mechanisms
- Break condition: When the scoring function has error rates above 0.1, performance degrades rapidly as shown in Figure 5

### Mechanism 3
- Claim: Predictive routers using KNN or MLP can route effectively without pre-generating all LLM outputs
- Mechanism: The router predicts performance scores for each LLM based on input features and routes to the model maximizing the score
- Core assumption: Performance predictors trained on subset of data can generalize to unseen inputs
- Evidence anchors:
  - [abstract]: "KNN and MLP routers achieve AIQs of 0.773 and 0.769 respectively on ROUTER BENCH"
  - [section]: "To estimate P for each input across models, we implemented two supervised regression approaches: k-nearest neighbors (KNN) and multi-layer perceptron (MLP)"
  - [corpus]: Weak - LLMRank paper mentions prompt-aware routing but doesn't detail KNN/MLP approaches
- Break condition: When predictors fail to capture task-specific performance patterns, routing decisions become suboptimal

## Foundational Learning

- Concept: Convex hull optimization in cost-quality space
  - Why needed here: The routing framework relies on finding optimal trade-offs between cost and performance across multiple models
  - Quick check question: Given two routers at points (0.2, 0.8) and (0.8, 0.4), what is the maximum achievable quality at cost 0.5?

- Concept: Supervised regression for performance prediction
  - Why needed here: Predictive routers need to estimate which LLM will perform best on a given input without actually running inference
  - Quick check question: If a KNN router with k=5 neighbors achieves 75% accuracy on validation data, what would you expect its routing accuracy to be?

- Concept: Cascading decision thresholds
  - Why needed here: Cascading routers need to determine when to stop evaluating cheaper models and accept a response
  - Quick check question: If a query has total budget $2 and costs per model are $0.1, $0.5, $1.0, what's the maximum number of models that can be evaluated?

## Architecture Onboarding

- Component map: Dataset collection pipeline -> Benchmark construction -> Router training -> AIQ calculation -> Performance comparison
- Critical path: Data collection → Benchmark construction → Router training → AIQ calculation → Performance comparison
- Design tradeoffs: Zero router provides strong baseline but may not capture complex routing patterns; cascading routers are simple but sensitive to error rates; predictive routers require training but can generalize
- Failure signatures: Poor AIQ values indicate routing inefficiencies; cascading routers with error >0.2 show performance collapse; predictive routers fail when out-of-domain inputs occur
- First 3 experiments:
  1. Run Oracle router to establish upper bound on achievable performance
  2. Implement Zero router and verify it creates non-decreasing convex hull
  3. Test KNN router with varying k values on a single dataset to find optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do cascading routers perform on domain-specific tasks requiring long-tail skills, such as translation of low-resource languages?
- Basis in paper: [explicit] The paper suggests incorporating domain-specific tasks that require long-tail skills in future versions of ROUTER BENCH.
- Why unresolved: The current benchmark does not include such tasks, limiting the evaluation of cascading routers in these scenarios.
- What evidence would resolve it: Testing cascading routers on a benchmark that includes domain-specific tasks requiring long-tail skills and comparing their performance to other routing strategies.

### Open Question 2
- Question: What is the impact of incorporating latency and throughput as evaluation criteria in ROUTER BENCH?
- Basis in paper: [inferred] The paper mentions that ROUTER BENCH currently focuses on performance and economic cost, but suggests including more evaluation criteria like latency and throughput in future iterations.
- Why unresolved: The current benchmark does not include latency and throughput, limiting the comprehensive understanding of router capabilities and limitations.
- What evidence would resolve it: Evaluating routing strategies on a benchmark that includes latency and throughput metrics and comparing their performance to the current evaluation criteria.

### Open Question 3
- Question: How do routing strategies perform when dealing with two-stage routing, which encompasses retrievers and LLMs?
- Basis in paper: [explicit] The paper highlights the challenge of implementing two-stage routing and its potential to refine router evaluations on standard RAG tasks.
- Why unresolved: The current evaluation within the RAG context was limited to models possessing inherent retrieval capabilities, not addressing the two-stage routing challenge.
- What evidence would resolve it: Testing routing strategies on a benchmark that includes two-stage routing scenarios and comparing their performance to single-stage routing approaches.

## Limitations

- The theoretical framework's convex hull assumption may not capture all practical routing scenarios with non-linear performance relationships
- Benchmark effectiveness is limited to the 8 datasets used, potentially missing edge cases in specialized domains
- KNN and MLP predictors' generalization capabilities remain uncertain when applied to substantially different input distributions

## Confidence

- **High confidence**: The AIQ metric calculation and convex hull construction methodology are well-defined and reproducible. The cost-performance trade-offs demonstrated through empirical results are consistent across multiple routing strategies.
- **Medium confidence**: The Zero router baseline implementation and its claimed performance (AIQ of 0.763) are reasonable given the methodology, but the exact selection probabilities across different cost-quality combinations may vary based on implementation details.
- **Low confidence**: The cascading router performance claims, particularly the error rate sensitivity analysis, may be affected by unspecified details in the scoring function implementation and judge threshold determination.

## Next Checks

1. **Robustness testing**: Evaluate routing performance across additional datasets beyond the 8 included in ROUTER BENCH to assess generalization to new domains and task types.

2. **Embedding model sensitivity**: Systematically test KNN routers with different embedding models and k values to identify optimal configurations and quantify performance variance.

3. **Real-time performance validation**: Implement the routing system in a live serving environment to measure actual latency, cost savings, and quality trade-offs under production conditions.