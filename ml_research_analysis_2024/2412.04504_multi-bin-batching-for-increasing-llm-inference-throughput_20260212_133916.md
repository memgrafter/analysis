---
ver: rpa2
title: Multi-Bin Batching for Increasing LLM Inference Throughput
arxiv_id: '2412.04504'
source_url: https://arxiv.org/abs/2412.04504
tags:
- throughput
- time
- bins
- system
- requests
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a multi-bin batching algorithm to improve LLM
  inference throughput by grouping requests with similar predicted execution times
  into predetermined bins. This approach minimizes resource underutilization caused
  by varying generation lengths in standard batching systems.
---

# Multi-Bin Batching for Increasing LLM Inference Throughput
## Quick Facts
- arXiv ID: 2412.04504
- Source URL: https://arxiv.org/abs/2412.04504
- Reference count: 40
- Key outcome: Multi-bin batching algorithm achieves up to 70% throughput improvement by grouping LLM requests with similar predicted execution times into predetermined bins.

## Executive Summary
This paper introduces multi-bin batching, a novel approach to improve LLM inference throughput by grouping requests with similar predicted execution times into predetermined bins. The method addresses the inefficiency of standard batching systems where requests with varying generation lengths cause resource underutilization. By minimizing the maximum service time within each batch, the approach achieves significant throughput gains. Theoretical analysis proves asymptotic throughput optimality as the number of bins increases, with experiments demonstrating consistent improvements across different settings.

## Method Summary
The multi-bin batching algorithm groups incoming requests into k predetermined bins based on predicted output lengths (which determine execution times). Requests are batched within each bin into groups of size B, with each batch's service time equal to the maximum request time in that batch. The method uses queueing-theoretical analysis to show throughput optimality as bin count increases, and employs a BERT-based predictor for bin assignment in end-to-end experiments using real LLM models on NVIDIA A40-80G GPUs.

## Key Results
- Multi-bin batching achieves up to 70% throughput improvement over standard batching in simulated scenarios with oracle bin assignments
- Theoretical analysis proves asymptotic throughput optimality as the number of bins increases
- Experiments with real LLM models (Phi-3.5-mini-instruct) show consistent throughput gains across different batch sizes and request loads

## Why This Works (Mechanism)

### Mechanism 1
Grouping requests by predicted execution time into bins reduces the maximum service time within each batch, thereby improving overall throughput. Requests with similar execution times are placed into the same bin, so when a batch is formed, the variation in service times is minimized. This reduces the "waste of computation" that occurs when hardware waits for the slowest request in a standard batch to finish. The core assumption is that execution time of a request is predictable and can be estimated from its output length. Evidence anchors include the abstract statement about grouping requests with similar predicted execution times and section discussions on improving LLM inference throughput by grouping requests based on predicted output lengths. The break condition occurs if prediction errors are too large, leading to suboptimal batching and throughput degradation.

### Mechanism 2
As the number of bins increases, the throughput approaches the theoretical maximum, achieving asymptotic throughput optimality. With more bins, the service time distribution within each bin becomes narrower, approaching the average service time of a single request. This minimizes the inefficiency caused by the maximum service time in a batch. The core assumption is that the service time distribution is uniform or can be approximated as such within each bin. Evidence anchors include the abstract statement about throughput approaching the system's theoretical maximum as bin count increases, and section analysis showing throughput approaches theoretical maximum. The break condition occurs if overhead of managing many bins outweighs benefits, or if the system cannot form batches quickly enough due to high bin count.

### Mechanism 3
The system is robust to symmetrical prediction errors, maintaining throughput gains even when requests are occasionally mis-binned. Errors that place a request in an adjacent bin still result in relatively similar service times, so the impact on batch service time is limited. The system can tolerate some misclassification without significant throughput loss. The core assumption is that prediction errors are symmetrical and localized (i.e., a request is mis-binned to an adjacent bin with some probability). Evidence anchors include section evaluation of robustness under symmetrical inaccuracies in length prediction. The break condition occurs if errors are systematic or non-symmetrical, leading to consistent mis-binning that increases batch service times.

## Foundational Learning

- Concept: Queueing theory and M/G/1 queue models
  - Why needed here: The paper models the LLM inference system as a queueing system to analyze throughput and latency, using standard queueing assumptions like Poisson arrivals and service time distributions
  - Quick check question: In an M/G/1 queue, what is the relationship between arrival rate λ, service rate μ, and utilization ρ?

- Concept: Order statistics for uniform distributions
  - Why needed here: The expected maximum service time in a batch of uniformly distributed requests is derived using order statistics, which is critical for calculating throughput
  - Quick check question: For B i.i.d. uniform random variables on [a, b], what is the expected value of the maximum?

- Concept: Convex optimization and decision boundaries
  - Why needed here: The optimal bin boundaries are found by minimizing the expected service time, which involves setting up and solving a convex optimization problem over the bin edges
  - Quick check question: If a function is convex, what condition must hold at its minimum?

## Architecture Onboarding

- Component map: Request predictor -> Bin assignment module -> Batch formation -> Central queue -> Server

- Critical path:
  1. Request arrives → predictor estimates length → bin assignment
  2. Bin accumulates requests → batch of size B formed → batch added to central queue
  3. Server processes batch (service time = max request time in batch)
  4. Output returned to user

- Design tradeoffs:
  - More bins → higher throughput but increased latency and complexity in batch formation
  - Larger batch size B → better parallelism but higher variance in service time
  - Prediction accuracy → directly impacts bin assignment quality and throughput gains

- Failure signatures:
  - Low throughput despite many bins → predictor inaccuracy or batch formation delays
  - High latency with many bins → overhead in managing bins or slow batch dispatch
  - System underutilization → batch size too large or arrival rate too low

- First 3 experiments:
  1. Simulate with oracle bin assignments and known service times to establish upper bound on throughput improvement
  2. Replace oracle with a learned predictor (e.g., BERT-based) and measure throughput vs. prediction accuracy
  3. Introduce symmetrical prediction errors and measure robustness of throughput to misclassification

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of multi-bin batching change when the service time distribution deviates significantly from uniform or exponential (e.g., bimodal or heavy-tailed distributions)? The paper analyzes performance under uniform and exponential service time distributions but does not explore other distributional assumptions. The theoretical analysis relies on specific properties of these distributions (e.g., order statistics for uniform, maximum of exponentials), which may not generalize to other distributions. What evidence would resolve it: Experimental or theoretical analysis of multi-bin batching performance under various non-standard service time distributions, comparing throughput and latency to uniform/exponential cases.

### Open Question 2
What is the optimal number of bins when considering the trade-off between throughput improvement and increased latency due to batch formation time? The paper notes that increasing bins improves throughput asymptotically but mentions a trade-off with latency, without quantifying the optimal point. While the paper provides theoretical upper bounds on throughput gains, it does not derive a concrete model for latency that incorporates batch formation overhead. What evidence would resolve it: A unified model combining throughput gains and latency penalties as a function of bin count, identifying the point of diminishing returns.

### Open Question 3
How does the performance of multi-bin batching degrade under different error models for length prediction (e.g., non-symmetric errors or errors correlated with actual length)? The paper evaluates robustness under symmetric prediction errors but does not explore other error patterns. Real-world predictors may exhibit systematic biases or length-dependent error rates, which could affect binning efficiency differently than symmetric errors. What evidence would resolve it: Experiments varying error models (e.g., increasing error probability for longer outputs) and analyzing throughput/latency impacts across bin counts.

## Limitations
- Theoretical throughput optimality assumes ideal conditions including accurate service time prediction and negligible overhead from managing multiple bins
- 70% throughput improvement figure is based on simulated scenarios with oracle bin assignments, representing an upper bound rather than real-world performance
- BERT-based predictor evaluation shows only modest improvements over random binning, suggesting practical gains may be significantly lower than theoretical maximums

## Confidence
- Theoretical throughput optimality: High
- 70% throughput improvement claim: Medium
- Predictor robustness under symmetrical errors: Medium
- Practical implementation benefits: Low

## Next Checks
1. Implement the multi-bin batching system with a learned predictor (e.g., BERT-based) on a real LLM serving platform and measure actual throughput improvements compared to standard batching across varying request loads and batch sizes

2. Conduct experiments with non-symmetrical prediction error models (systematic over/under-prediction) to assess robustness under more realistic error conditions and identify failure modes

3. Measure and analyze the overhead costs of bin management (including latency introduced by waiting for batch formation) across different numbers of bins to determine the optimal bin count that balances throughput gains against operational overhead