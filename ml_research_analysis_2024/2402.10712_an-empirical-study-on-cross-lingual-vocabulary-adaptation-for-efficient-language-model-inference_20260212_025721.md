---
ver: rpa2
title: An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language
  Model Inference
arxiv_id: '2402.10712'
source_url: https://arxiv.org/abs/2402.10712
tags:
- language
- source
- lapt
- heuristics
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how cross-lingual vocabulary adaptation
  (CVA) affects the inference efficiency and downstream performance of generative
  LLMs across typologically diverse languages. It evaluates five CVA methods on four
  models (BLOOM-1B/7B, TigerBot-7B, Mistral-7B) for German, Japanese, Arabic, and
  Swahili, on NLI, MC, SUM, and SPAN tasks.
---

# An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference

## Quick Facts
- arXiv ID: 2402.10712
- Source URL: https://arxiv.org/abs/2402.10712
- Reference count: 40
- Primary result: CVA speeds up inference by up to 271.5% in 95 out of 96 cases

## Executive Summary
This paper investigates cross-lingual vocabulary adaptation (CVA) for improving inference efficiency of generative LLMs across typologically diverse languages. The authors evaluate five CVA methods on four models (BLOOM-1B/7B, TigerBot-7B, Mistral-7B) for German, Japanese, Arabic, and Swahili across NLI, MC, SUM, and SPAN tasks. Results show substantial inference speedups with minimal performance degradation when using appropriate initialization methods and LoRA ranks. The study also finds that in-language prompting outperforms English prompting in zero-shot settings, contrary to prior work.

## Method Summary
The study applies CVA by replacing/expanding source tokenizer vocabularies with target language tokens, initializing new embeddings using five methods (Random, CLP, Heuristics, FOCUS, CLP+), applying LoRA adapters to all linear layers, and running LAPT on target language data. Models are evaluated on downstream tasks using both in-language and English prompts in zero/few-shot settings. The approach focuses on maintaining source model knowledge while improving inference efficiency for target languages.

## Key Results
- CVA achieved inference speedups of up to 271.5% in 95 out of 96 experimental cases
- Multilingual source models maintained downstream performance comparable to original models after CVA
- In-language prompting outperformed English prompting in 11 of 16 zero-shot cases
- Low LoRA rank (r=8) sufficed for zero-shot settings while higher ranks helped few-shot learning
- Heuristics-based initialization was recommended for multilingual sources while similarity-based methods for non-multilingual sources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing or expanding the source tokenizer vocabulary with target language tokens reduces input sequence length and improves inference speed.
- Mechanism: The original English-centric tokenizer overfragments non-English text, leading to longer sequences and higher computational cost. CVA adapts the vocabulary to better match the target language's subword structure, reducing fragmentation.
- Core assumption: Tokenization quality directly impacts sequence length and inference speed, which is independent of model size.
- Evidence anchors:
  - [abstract] "recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs."
  - [section 5.1] "CVA substantially contributes to LLM inference speedups of up to 271.5%"
  - [corpus] Weak: Related work focuses on vocabulary adaptation but not explicit speed comparisons.
- Break condition: If the target tokenizer does not substantially reduce token count compared to source, speedups will be minimal.

### Mechanism 2
- Claim: Target vocabulary initialization method affects downstream performance, with heuristics-based initialization sufficient for multilingual source models.
- Mechanism: Different initialization strategies (random, heuristics, semantic similarity-based) provide different starting points for embedding adaptation. Multilingual source models retain more transferable knowledge, making simpler initialization adequate.
- Core assumption: The degree of overlap between source and target vocabularies influences the effectiveness of simpler initialization methods.
- Evidence anchors:
  - [abstract] "adapting LLMs that have been pre-trained on more balanced multilingual data results in downstream performance comparable to the original models."
  - [section 5.2] "Simple Heuristics-based initialization should be used to save computational costs when the source model is sufficiently multilingual"
  - [corpus] Weak: Related work mentions initialization but doesn't quantify multilingual source vs. non-multilingual source performance differences.
- Break condition: If source model lacks multilingual pretraining, heuristics-based initialization will underperform semantic similarity methods.

### Mechanism 3
- Claim: LoRA rank selection involves a tradeoff between computational efficiency and few-shot performance.
- Mechanism: Low LoRA rank (r=8) maintains reasonable zero-shot performance with minimal computational overhead, while higher ranks improve few-shot learning at increased cost.
- Core assumption: Few-shot performance requires more expressive adaptation than zero-shot performance.
- Evidence anchors:
  - [section 6] "A low rank (r = 8) is a good starting point in zero-shot settings considering performance and computational costs; r = 32 or larger is recommended in few-shot settings."
  - [section 5.2] "Performance improves with r in the few-shot setting" but "performance does not generally increase with r in the zero-shot setting."
  - [corpus] Weak: No direct corpus evidence on LoRA rank tradeoffs for CVA.
- Break condition: If computational budget allows, using higher rank universally may improve both settings without significant penalty.

## Foundational Learning

- Concept: Subword tokenization (BPE, Unigram, WordPiece)
  - Why needed here: Understanding how tokenizers fragment text differently across languages is crucial to grasping why CVA improves inference speed.
  - Quick check question: What happens to token count when a German word like "Donaudampfschifffahrtsgesellschaft" is tokenized by an English-centric tokenizer vs. a German-specific one?

- Concept: Cross-lingual transfer learning
  - Why needed here: CVA builds on principles of transferring knowledge from one language to another by adapting model components.
  - Quick check question: How does initializing target embeddings with source embeddings (or weighted averages) facilitate cross-lingual transfer?

- Concept: Parameter-efficient fine-tuning (LoRA)
  - Why needed here: LoRA is the adaptation method used in this study to keep computational costs manageable while adapting models to target languages.
  - Quick check question: What is the relationship between LoRA rank and the number of trainable parameters?

## Architecture Onboarding

- Component map:
  Source model (Ms) with tokenizer Ts and vocabulary Vs -> Target model (Mt) with tokenizer Tt and vocabulary Vt -> Target vocabulary initialization method -> LAPT module -> LoRA adapter modules -> Downstream task evaluation pipeline

- Critical path:
  1. Replace/expand source embedding and output matrices to match target vocabulary size
  2. Initialize new embeddings using chosen method (Random, Heuristics, CLP, FOCUS, CLP+)
  3. Apply LoRA adapters to all linear layers
  4. Run LAPT on target language data
  5. Evaluate inference speed and downstream performance

- Design tradeoffs:
  - Speed vs. accuracy: More sophisticated initialization methods may improve accuracy but increase computational cost
  - Memory vs. performance: Higher LoRA rank improves few-shot performance but increases memory usage
  - Multilingual vs. monolingual sources: Multilingual sources allow simpler initialization but may have smaller speedups

- Failure signatures:
  - No speedup observed: Target tokenizer does not reduce token count compared to source
  - Performance degradation: Inappropriate initialization method for source model type
  - Training instability: Too high learning rate or insufficient LoRA dropout

- First 3 experiments:
  1. Measure token count reduction when switching from BLOOM tokenizer to German tokenizer on German text
  2. Compare downstream performance using Random vs. Heuristics initialization on BLOOM-7B for German
  3. Evaluate few-shot performance with LoRA ranks 8, 32, and 64 on Japanese span prediction task

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond those mentioned in the limitations section.

## Limitations
- Experiments limited to generative LLMs up to 7B parameters, may not generalize to larger models or encoder-only architectures
- Computational resource constraints limited LAPT steps and checkpoint evaluations
- Study uses specific task datasets that may not generalize to other domains or task types
- Results may not extend to languages beyond the four tested (German, Japanese, Arabic, Swahili)

## Confidence
- High: CVA improves inference efficiency (speedups up to 271.5% observed in 95/96 cases)
- Medium: Multilingual source models maintain downstream performance comparable to original models after CVA
- Medium: In-language prompting outperforms English prompting in zero-shot settings
- Low: LAPT steps improve performance in 69.5% of zero-shot cases and 59.4% of few-shot cases (based on limited evaluation points)

## Next Checks
1. Replicate the study with additional language pairs (e.g., Slavic languages, Southeast Asian languages) and model architectures (encoder-only models, smaller models) to test generalizability
2. Conduct ablation studies to isolate the contribution of each CVA component (vocabulary initialization, LAPT, LoRA) to downstream performance and inference speed
3. Evaluate the impact of CVA on other task types (e.g., question answering, named entity recognition) and domains (e.g., biomedical, legal) to assess robustness across diverse applications