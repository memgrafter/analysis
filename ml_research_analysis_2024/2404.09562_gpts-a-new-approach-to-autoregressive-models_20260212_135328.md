---
ver: rpa2
title: "\u03C3-GPTs: A New Approach to Autoregressive Models"
arxiv_id: '2404.09562'
source_url: https://arxiv.org/abs/2404.09562
tags:
- order
- sequence
- tokens
- training
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes \u03C3-GPT, a method that enables autoregressive\
  \ models to generate sequences in arbitrary orders rather than being restricted\
  \ to a fixed order like left-to-right. The core idea is to add a double positional\
  \ encoding: one for the input order and one for the output order, allowing the model\
  \ to understand both the position of tokens and the next token in the shuffled sequence."
---

# σ-GPTs: A New Approach to Autoregressive Models

## Quick Facts
- arXiv ID: 2404.09562
- Source URL: https://arxiv.org/abs/2404.09562
- Authors: Arnaud Pannatier; Evann Courdier; François Fleuret
- Reference count: 40
- One-line primary result: σ-GPT enables autoregressive models to generate sequences in arbitrary orders, achieving similar or better performance than left-to-right models while supporting conditional density estimation and burst sampling.

## Executive Summary
This paper introduces σ-GPT, a method that modifies autoregressive transformers to generate sequences in arbitrary orders rather than being restricted to left-to-right generation. The core innovation is double positional encoding that tracks both input and output positions, enabling the model to understand the logical generation order. This modification allows for conditional density estimation, natural infilling support, and burst sampling via a novel token-based rejection sampling scheme. The method was evaluated on language modeling, maze solving, and aircraft trajectory prediction tasks, showing comparable or superior performance to standard left-to-right trained models.

## Method Summary
σ-GPT extends standard causal transformers by adding a second positional encoding layer. Each token contains its value, its position in the original sequence, and the position of the next token in the shuffled sequence. The model is trained on randomly shuffled sequences with double positional encoding using standard cross-entropy loss. During inference, it can generate in arbitrary order or use burst sampling with token-based rejection sampling. The rejection sampling algorithm samples tokens in parallel at all positions, then accepts or rejects them based on multiple shuffled orders until the first rejection, leveraging KV-caching for efficiency.

## Key Results
- σ-GPT achieves state-of-the-art perplexity of 4.74 on WikiText-103 when combined with curriculum learning
- The method reduces the number of model evaluations by an order of magnitude compared to autoregressive generation in burst sampling scenarios
- σ-GPT outperforms left-to-right models on aircraft vertical rate prediction and matches or exceeds performance on language modeling and maze solving tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The double positional encoding allows the model to predict tokens in arbitrary orders by explicitly encoding both input and output positions.
- Mechanism: Each token contains three pieces of information: its value, its current position in the original sequence, and the position of the next token in the shuffled sequence. This allows the transformer to attend to tokens based on their logical generation order rather than their physical position.
- Core assumption: Transformers can learn to use positional information to reorder generation sequences when provided with explicit position encoding.
- Evidence anchors:
  - [abstract] "by simply adding a positional encoding for the output, this order can be modulated on-the-fly"
  - [section 2.2] "each token needs to have information about its position and the one of the next token in the shuffled sequence"
  - [corpus] Weak evidence - corpus neighbors focus on GPT variants but don't directly address positional encoding mechanisms
- Break condition: If the positional encoding scheme cannot disambiguate between input and output positions, or if the transformer cannot learn to use this information effectively.

### Mechanism 2
- Claim: Token-based rejection sampling enables burst generation by accepting multiple tokens in parallel while maintaining coherent sequences.
- Mechanism: The model samples tokens at all positions in parallel, then evaluates them under multiple shuffled orders. Tokens are accepted until the first rejection, ensuring coherence. The KV-caching mechanism allows efficient parallel evaluation.
- Core assumption: The model's conditional distributions remain coherent enough that parallel sampling followed by order-based validation produces valid sequences.
- Evidence anchors:
  - [abstract] "it also allows sampling in one shot multiple tokens dynamically according to a rejection strategy"
  - [section 2.4] "we can leverage that fact and sample tokens in parallel at every position of the sequence"
  - [corpus] Moderate evidence - corpus includes "Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling" suggesting parallel sampling research
- Break condition: If the model's conditional distributions become incoherent when conditioned on partially generated sequences, leading to frequent rejections.

### Mechanism 3
- Claim: Random order training creates harder learning problems that require more training data but enable better generalization and conditional density estimation.
- Mechanism: Training in random order prevents the model from relying on local sequential information, forcing it to learn global sequence statistics and enabling conditional predictions at any point.
- Core assumption: Learning harder problems during training leads to models that can perform more complex inference tasks during generation.
- Evidence anchors:
  - [section 3.2] "Training in random order was plateauing at a higher left-to-right validation perplexity"
  - [section 3.6] "models trained in left-to-right, fractal, and random order are in a memorizing regime" at small dataset sizes
  - [corpus] Weak evidence - corpus doesn't directly address training difficulty but includes "Tracking GPTs" suggesting third-party analysis
- Break condition: If the increased training difficulty leads to worse final performance or if the benefits of conditional density estimation don't outweigh the training costs.

## Foundational Learning

- Concept: Positional encoding in transformers
  - Why needed here: The double positional encoding is the core architectural change that enables arbitrary order generation
  - Quick check question: What are the two types of positional information encoded in σ-GPT and why are both needed?

- Concept: Chain rule of probability
  - Why needed here: Understanding why arbitrary order generation is theoretically possible but practically challenging
  - Quick check question: According to the chain rule, why should the order of modeling not matter theoretically?

- Concept: Rejection sampling
  - Why needed here: The burst sampling algorithm relies on rejection sampling principles to maintain sequence coherence
  - Quick check question: How does the token-based rejection sampling differ from standard rejection sampling in terms of accepting multiple tokens?

## Architecture Onboarding

- Component map: σ-GPT extends standard causal transformers by adding a second positional encoding layer. The input receives standard sinusoidal positional encoding for input positions, while a second layer encodes output positions. The model architecture remains otherwise unchanged.
- Critical path: Training involves shuffling sequences, adding double positional encoding, and training with standard cross-entropy loss. Inference involves either autoregressive generation in arbitrary order or burst generation using the rejection sampling algorithm.
- Design tradeoffs: Random order training increases computational cost and training time but enables conditional density estimation and burst generation. The double positional encoding adds minimal parameter overhead but requires careful implementation.
- Failure signatures: Training plateaus at higher perplexity than left-to-right models, frequent rejections during burst sampling indicating incoherent conditional distributions, or inability to perform conditional density estimation.
- First 3 experiments:
  1. Implement double positional encoding and verify it can learn simple shuffled sequence tasks
  2. Test conditional density estimation on a toy dataset with known ground truth
  3. Implement and test burst sampling on a synthetic product law dataset to verify order of magnitude speedup

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of order during training (random vs. fractal vs. left-to-right) affect the model's ability to generalize versus memorize, particularly for smaller datasets?
- Basis in paper: [explicit] The paper discusses how training in random order is harder and leads to more memorization, especially in smaller datasets. It also compares training in fractal order to random order.
- Why unresolved: The paper provides some insights but doesn't fully explore the relationship between order choice and memorization/generalization across various dataset sizes and tasks.
- What evidence would resolve it: Systematic experiments varying dataset sizes and comparing generalization performance across different training orders (random, fractal, left-to-right) on diverse tasks.

### Open Question 2
- Question: Can the token-based rejection sampling scheme be further optimized to reduce the number of steps needed for generation, especially for complex distributions like permutations?
- Basis in paper: [explicit] The paper introduces a token-based rejection sampling scheme and evaluates it on synthetic tasks, showing it can reduce the number of steps compared to autoregressive generation. However, it notes that the optimal number of steps for permutations is not fully explored.
- Why unresolved: The paper provides a proof-of-concept but doesn't explore advanced optimization techniques or heuristics for rejection sampling, particularly for challenging distributions.
- What evidence would resolve it: Experiments comparing different rejection sampling strategies (e.g., adaptive order selection, dynamic step size) and their impact on the number of steps needed for generation across various distributions.

### Open Question 3
- Question: How does the double positional encoding scheme impact the model's ability to capture long-range dependencies compared to standard positional encodings used in left-to-right models?
- Basis in paper: [explicit] The paper introduces the double positional encoding as a key innovation for σ-GPT, but doesn't directly compare its impact on long-range dependency modeling to standard positional encodings.
- Why unresolved: The paper focuses on the benefits of the double positional encoding for enabling shuffled autoregression but doesn't investigate its effect on the model's ability to capture long-range dependencies, which is crucial for many tasks.
- What evidence would resolve it: Experiments comparing the performance of σ-GPT with double positional encoding to standard left-to-right models with different positional encoding schemes on tasks requiring long-range dependency modeling (e.g., machine translation, document summarization).

## Limitations

- The computational overhead of double positional encoding and random order training may not justify the benefits for standard generation tasks
- The burst sampling algorithm's effectiveness depends heavily on the model's ability to maintain coherent conditional distributions during parallel sampling
- The curriculum learning approach introduces additional hyperparameters and complexity that may not generalize well across different tasks and datasets

## Confidence

**High Confidence**: The core architectural modification (double positional encoding) is technically sound and the theoretical justification for arbitrary-order generation based on the chain rule is correct. The rejection sampling algorithm is well-defined and implementable.

**Medium Confidence**: The empirical results showing comparable or slightly better performance to left-to-right models are credible, but the magnitude of benefits varies significantly across tasks. The claimed order-of-magnitude speedup in burst sampling needs more rigorous validation across diverse sequence lengths and domains.

**Low Confidence**: The assertion that random order training creates harder learning problems that lead to better generalization lacks strong empirical support. The comparison with diffusion models is limited and doesn't establish clear advantages for σ-GPT in parallel generation scenarios.

## Next Checks

1. **Ablation study on positional encoding**: Remove either the input or output positional encoding and measure the impact on both performance and burst sampling efficiency to quantify the contribution of each component.

2. **Scaling analysis of burst sampling**: Systematically vary sequence length from 10 to 10,000 tokens and measure the actual speedup factor achieved by token-based rejection sampling, comparing against the claimed order-of-magnitude improvement.

3. **Cross-domain generalization test**: Train σ-GPT on a diverse set of tasks (e.g., music generation, code completion, and molecular structure prediction) to evaluate whether the arbitrary-order capability provides consistent benefits across different sequence types and dependency structures.