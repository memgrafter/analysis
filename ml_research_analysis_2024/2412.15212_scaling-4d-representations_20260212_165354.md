---
ver: rpa2
title: Scaling 4D Representations
arxiv_id: '2412.15212'
source_url: https://arxiv.org/abs/2412.15212
tags:
- video
- tasks
- learning
- size
- frames
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates self-supervised learning from video on non-semantic
  4D vision tasks, including depth estimation, point and object tracking, and camera
  pose estimation. Using a large-scale masked auto-encoding (MAE) approach with transformer
  video models, the authors scale models from 20M to 22B parameters, training on 170M
  videos.
---

# Scaling 4D Representations

## Quick Facts
- arXiv ID: 2412.15212
- Source URL: https://arxiv.org/abs/2412.15212
- Reference count: 40
- Key outcome: Scaling transformer models with MAE on large video datasets improves 4D task performance, with the 22B model achieving best results across depth estimation, point/object tracking, and camera pose estimation.

## Executive Summary
This paper investigates scaling self-supervised learning from video to learn 4D representations for non-semantic vision tasks. Using a large-scale masked auto-encoding (MAE) approach with transformer video models ranging from 20M to 22B parameters, the authors train on 170M videos and evaluate on tasks including depth estimation, point and object tracking, and camera pose estimation. Results show consistent improvement across all tasks with increasing model size, with the largest 22B model achieving state-of-the-art performance. The study demonstrates that scaling video MAE is an effective approach for learning rich 4D representations that generalize across multiple geometric and temporal understanding tasks.

## Method Summary
The method uses SimpleMAE, a variant of masked auto-encoding that randomly masks 95% of space-time patches without tube masking, target normalization, or custom decoders. Transformer video models are pretrained on 170 million 30-second videos at 16-frame clips (224x224 or 256x256 resolution) using AdamW optimizer with bfloat16 precision. Models are scaled from 20M to 22B parameters. For downstream tasks, attention-based readouts are trained on top of frozen or finetuned encoder features. The evaluation protocol includes both end-to-end finetuning and frozen feature evaluation to isolate pretraining quality, with comparisons against baselines including VideoPrism, V-JEPA, and DinoV2.

## Key Results
- Consistent improvement across all 4D tasks with increasing model size from 20M to 22B parameters
- The 22B model significantly outperforms prior image and video models on depth estimation, point/object tracking, and camera pose estimation
- Image models underperform on 4D tasks compared to video models, demonstrating the importance of temporal representation
- Attention-based readouts on frozen features achieve performance close to full finetuning while being computationally cheaper

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling transformer models with masked auto-encoding (MAE) on large video datasets leads to consistent performance improvements on 4D tasks.
- Mechanism: As model size increases from 20M to 22B parameters, the capacity to learn rich spatial and temporal representations improves, leading to better performance on tasks requiring 4D understanding like camera pose estimation, point tracking, and depth estimation.
- Core assumption: Larger models trained on more data can capture more complex patterns in video data, and these patterns are beneficial for 4D tasks.
- Evidence anchors:
  - [abstract] "Results show consistent improvement across all tasks with increasing model size, outperforming prior image and video models."
  - [section 4] "Our scaled up models, 4DS-G, e and j – explained in Sec. 3.1 – significantly improve over all the baselines, achieving top results on all 4D tasks."
  - [corpus] Weak - the corpus papers are on different topics and do not provide direct evidence for this scaling claim.
- Break Condition: If performance plateaus or degrades with further scaling, or if the improvements are due to factors other than model size (e.g., better optimization, more data, etc.).

### Mechanism 2
- Claim: SimpleMAE, a variant of MAE without target normalization, tube masking, or custom decoders, is effective for learning video representations.
- Mechanism: By randomly masking 95% of space-time patches and training the model to reconstruct them, the model learns to fill in missing information, which is a form of self-supervision that encourages learning of spatial and temporal structure.
- Core assumption: The task of reconstructing masked patches is a good proxy for learning useful representations for 4D tasks.
- Evidence anchors:
  - [section 3.1] "We mask in all cases randomly 95% of 2x16x16 space-time patches (no tricks like tube masking or any special masking pattern), feeding the remaining to the transformer model."
  - [section 4] "Our conclusions in all settings are: 1. Image models are not competitive. 2. VideoMAE and V-JEPA both perform well."
  - [corpus] Weak - the corpus papers do not provide evidence for the effectiveness of this specific variant of MAE.
- Break Condition: If other forms of self-supervision (e.g., contrastive learning, future prediction) prove more effective, or if the 95% masking ratio is not optimal.

### Mechanism 3
- Claim: Attention-based readouts on top of frozen video encoder features are effective for adapting pretrained models to downstream 4D tasks.
- Mechanism: The attention-based readout allows the model to focus on relevant parts of the video features for each task, without requiring finetuning of the entire model.
- Core assumption: The features learned by the video encoder are general enough to be useful for a variety of 4D tasks with minimal adaptation.
- Evidence anchors:
  - [section 3.3] "We followed an evaluation protocol designed to fairly compare arbitrary backbones over multiple tasks in the same setting. We tried both end-to-end finetuning and frozen evaluation, which puts more emphasis on the quality of the pretraining and is much cheaper computationally."
  - [section 4] "Compared to linear readouts, attention-based ones offer much better performance on top of frozen features (sometimes close to full finetuning) while not increasing parameter count significantly and have become preferred recently [8]"
  - [corpus] Weak - the corpus papers do not provide direct evidence for the effectiveness of this specific readout approach.
- Break Condition: If other readout architectures (e.g., linear probes, MLPs) prove more effective, or if finetuning the entire model is necessary for good performance.

## Foundational Learning

- Concept: Masked Auto-encoding (MAE)
  - Why needed here: MAE is the self-supervised learning method used to pretrain the 4DS models. Understanding MAE is crucial for understanding how the models learn from video data.
  - Quick check question: What is the main idea behind MAE and how does it differ from other self-supervised learning methods like contrastive learning?

- Concept: Transformer models
  - Why needed here: The 4DS models are transformer-based models. Understanding transformers is essential for understanding the architecture and capabilities of these models.
  - Quick check question: What are the key components of a transformer model and how do they enable learning from sequential data like video?

- Concept: 4D representations
  - Why needed here: The paper focuses on 4D tasks that require understanding of spatial and temporal structure in videos. Understanding what 4D representations are and why they are important is crucial for understanding the motivation and goals of the paper.
  - Quick check question: What are the key differences between 3D and 4D representations, and why are 4D representations important for understanding videos?

## Architecture Onboarding

- Component map: Video encoder -> MAE decoder -> Attention-based readout -> Downstream task evaluation
- Critical path: Pretrain video encoder using MAE on large video dataset -> Train attention-based readout on frozen encoder features for each downstream task -> Evaluate performance on 4D tasks
- Design tradeoffs: Model size vs. computational cost (larger models perform better but are more expensive), masking ratio in MAE (higher ratio encourages more reconstruction but may make task too difficult), choice of downstream tasks (should cover range of 4D understanding capabilities)
- Failure signatures: Poor performance on downstream tasks (indicates issues with pretraining, readout choice, or generalization), overfitting to pretraining data (if dataset not diverse enough or models too large), computational inefficiency (if models too large or implementation not optimized)
- First 3 experiments:
  1. Train a small 4DS model (e.g., 20M parameters) on a subset of the pretraining data and evaluate its performance on a simple downstream task (e.g., depth estimation)
  2. Vary the masking ratio in MAE and observe its effect on the performance of the pretrained models on downstream tasks
  3. Compare the performance of attention-based readouts to other readout architectures (e.g., linear probes, MLPs) on a few downstream tasks

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions but the following areas remain unexplored based on the evaluation:

1. How do 4DS models perform on tasks requiring longer temporal context than the 16-frame input used in this paper?
2. What is the optimal layer depth for extracting features from 4DS models for each downstream task, and how does this optimal depth change with model scale?
3. How do 4DS models perform on semantic video understanding tasks compared to non-semantic 4D tasks?

## Limitations

- Evaluation focuses exclusively on non-semantic 4D tasks, leaving unclear whether scaling benefits extend to semantic video understanding
- The 22B parameter model requires significant computational resources, raising questions about practical deployment
- While comparing against image and video baselines, the paper doesn't extensively explore alternative self-supervised learning methods like contrastive learning or future prediction

## Confidence

**High Confidence**: The core finding that scaling transformer models with MAE improves 4D task performance is well-supported by consistent results across multiple model sizes (20M to 22B parameters) and tasks. The ablation showing image models underperform on 4D tasks provides strong evidence for the importance of temporal representation.

**Medium Confidence**: The effectiveness of SimpleMAE (95% random masking without tube masking or target normalization) is demonstrated, but the paper doesn't extensively compare against other masking strategies or investigate whether the specific masking ratio is optimal. The choice of attention-based readouts over linear probes is supported by performance gains, but more thorough ablation studies would strengthen this claim.

**Low Confidence**: The claim that these representations are truly "4D" is somewhat ambiguous, as the evaluation focuses on tasks that require either spatial understanding (depth), temporal understanding (tracking), or both (camera pose), but doesn't directly test integrated 4D spatial-temporal reasoning. The paper also doesn't address potential domain shift issues when applying web video pretraining to different video domains.

## Next Checks

1. **Task Generalization**: Evaluate the pretrained models on semantic video understanding tasks (action classification, video captioning) to determine whether scaling benefits extend beyond non-semantic 4D tasks, and investigate potential trade-offs between semantic and geometric capabilities.

2. **Alternative Self-Supervised Methods**: Implement and compare against contrastive learning approaches (like MoCo or SimCLR for video) and future prediction methods to establish whether MAE scaling is the most effective path for 4D representation learning.

3. **Masking Strategy Ablation**: Systematically vary the masking ratio (70%, 85%, 95%, 99%) and compare random masking against tube masking or other structured patterns to determine the optimal masking strategy for 4D task performance and whether the 95% ratio is indeed optimal.