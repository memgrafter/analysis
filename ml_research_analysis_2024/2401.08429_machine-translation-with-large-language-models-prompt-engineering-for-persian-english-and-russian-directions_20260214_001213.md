---
ver: rpa2
title: 'Machine Translation with Large Language Models: Prompt Engineering for Persian,
  English, and Russian Directions'
arxiv_id: '2401.08429'
source_url: https://arxiv.org/abs/2401.08429
tags:
- translation
- language
- machine
- https
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the machine translation capabilities of popular\
  \ large language models (LLMs) such as GPT-3.5, GPT-4, PaLM2, Llama-2-70b, Claude-2-100k,\
  \ and Perplexity AI for Persian, English, and Russian language pairs. The authors\
  \ investigate two prompting methods\u2014n-shot learning and a tailored translation\
  \ prompt framework\u2014both separately and in combination, using a total of 15-50\
  \ sentences sampled from three well-known literary works."
---

# Machine Translation with Large Language Models: Prompt Engineering for Persian, English, and Russian Directions

## Quick Facts
- arXiv ID: 2401.08429
- Source URL: https://arxiv.org/abs/2401.08429
- Reference count: 0
- Primary result: PaLM2 shows most consistent improvement with combined prompts and examples across Persian-English-Russian translation tasks

## Executive Summary
This study evaluates machine translation capabilities of popular LLMs (GPT-3.5, GPT-4, PaLM2, Llama-2-70b, Claude-2-100k, Perplexity AI) for Persian, English, and Russian language pairs. The authors investigate two prompting methods—n-shot learning and a tailored translation prompt framework—both separately and in combination, using 15-50 sentences from three literary works. Translation quality was assessed through automatic metrics (BLEU, chrF++, COMET) and comprehensive human evaluation. Results show that multilingual models like PaLM2 perform competitively across both high- and low-resource languages, with PaLM2 showing the most consistent improvement when combining prompts and examples. Perplexity AI also excelled in understanding prompts and maintaining performance with increased data. In contrast, models like Claude-2-100k performed poorly on low-resource language directions, and GPT-3.5 was sensitive to lengthy prompts. Overall, LLMs can produce fluent translations but still require careful post-editing, especially for distant or low-resource languages.

## Method Summary
The study sampled 15-50 sentences from three literary works ("My Uncle Napoleon" in Persian, "The Beggar Boy at Christ's Christmas Tree" in Russian, and "The Tell-Tale Heart" in English) with their published professional translations as references. The authors compared two prompt strategies—n-shot learning with carefully selected examples and a tailored translation prompt framework from "Keyword Everywhere"—across all six language directions. Translations were generated using official APIs for six LLMs, then evaluated with automatic metrics (BLEU, chrF++, COMET) and comprehensive human evaluation involving linguists and translators, focusing on eight error categories per language direction.

## Key Results
- PaLM2 consistently improved performance when combining prompt frameworks with examples across all language directions
- Perplexity AI excelled in prompt comprehension and maintained performance with increased data
- Claude-2-100k performed poorly on low-resource language directions, particularly Persian-Russian
- GPT-3.5 showed sensitivity to lengthy prompts, with performance degrading as prompt complexity increased

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PaLM2's superior multilingual translation performance stems from its 78% English and 22% other languages training data proportion.
- Mechanism: Exposure to diverse multilingual data enables PaLM2 to better regulate and apply translation patterns across different language families, improving bilingual abilities and analytical linguistics skills.
- Core assumption: Multilingual data proportion in pretraining correlates with cross-lingual translation quality.
- Evidence anchors:
  - [abstract] "We hypothesize that this is due to the large multilingual data proportion in its training data, which is 78% English and 22% for other languages."
  - [section 3.4.1 Results] "We hypothesize that this may be due to the fact that this model can further improve its bilingual abilities as its training has involved in multilingual data and can further improve this ability as it regulates and applies it more often."
  - [corpus] Weak evidence - corpus neighbors do not directly support this mechanism.
- Break Condition: If a model with similar multilingual data proportion but different architecture underperforms, or if models with less multilingual data but better architecture perform better.

### Mechanism 2
- Claim: In-context learning with carefully selected examples improves translation quality more than random examples.
- Mechanism: The model learns translation patterns more effectively when examples are relevant and high-quality, aligning with the input-output patterns expected.
- Core assumption: Example relevance and quality in n-shot prompting directly impact translation performance.
- Evidence anchors:
  - [section 2.1 Data Sets] "For the n-shot technique, we implement examples, preferably from the same work not to interrupt with the intonation, style, etc."
  - [section 3.1 Prompting Strategies] "Using techniques like in-context learning with demonstrations, the translation performance could be further improved."
  - [corpus] Moderate evidence - corpus includes "TreePrompt: Leveraging Hierarchical Few-Shot Example Selection" which supports example selection importance.
- Break Condition: If random examples perform equally well, or if carefully selected examples degrade performance due to overfitting.

### Mechanism 3
- Claim: Translation prompt frameworks with standardized templates outperform raw n-shot prompting alone.
- Mechanism: Tailored prompting frameworks provide clearer task instructions and context, leading to better translation outputs.
- Core assumption: Structured prompt frameworks are more effective than simple example-based prompting.
- Evidence anchors:
  - [section 3.1 Prompting Strategies] "we used a well-known standard prompting framework specialized for translation and localization purposes adopted from 'Keyword Everywhere'."
  - [section 3.4 Comparison of n-shot, prompt enhanced and their combinations] "PaLM was the most efficient model, the efficiency improved with all our three stings; including increasing shots, applying a prompting framework, and a combination of both."
  - [corpus] Weak evidence - corpus neighbors do not directly address prompt framework effectiveness.
- Break Condition: If raw n-shot prompting consistently matches or exceeds framework-enhanced performance across models and language pairs.

## Foundational Learning

- Concept: Multilingual pretraining data distribution
  - Why needed here: Understanding how different proportions of language data affect translation quality across language families.
  - Quick check question: If a model is trained on 90% English and 10% other languages, how might this affect its performance on Persian-English translation versus English-Russian?

- Concept: In-context learning mechanics
  - Why needed here: Grasping how few-shot examples guide model behavior without parameter updates.
  - Quick check question: What happens to translation quality when you increase the number of examples from 2 to 10 for a distant language pair like Persian-Russian?

- Concept: Prompt engineering for translation tasks
  - Why needed here: Knowing how to structure prompts to elicit optimal translation behavior from LLMs.
  - Quick check question: How would you modify a translation prompt to emphasize preserving literary style versus literal accuracy?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Literary text sampling and alignment
  - Model selection: GPT-3.5, GPT-4, PaLM2, Llama-2-70b, Claude-2-100k, Perplexity AI
  - Prompt engineering: N-shot examples + translation framework templates
  - Evaluation pipeline: BLEU, chrF++, COMET + human linguistic error analysis
  - Analysis layer: Error categorization and performance comparison

- Critical path:
  1. Sample test sentences from literary works
  2. Design and implement prompt variations
  3. Run translations across all model/prompt combinations
  4. Evaluate with automatic and human metrics
  5. Analyze error patterns and performance trends

- Design tradeoffs:
  - Sample size vs. human evaluation consistency
  - Prompt complexity vs. model input length limitations
  - Automatic metrics vs. nuanced human judgment
  - Resource allocation across multiple model APIs

- Failure signatures:
  - Performance degradation with increased shot count
  - Inconsistent outputs across model runs
  - High hallucination rates in low-resource directions
  - Poor correlation between automatic metrics and human judgment

- First 3 experiments:
  1. Zero-shot translation comparison across all six language directions
  2. N-shot scaling experiment (0, 1, 2, 5, 10 examples) for Persian-English pair
  3. Prompt framework vs. raw prompting comparison for Russian-Persian direction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do large language models perform on machine translation tasks involving distant language pairs with extremely low-resource availability, beyond the English-Persian-Russian combinations studied?
- Basis in paper: [inferred] The paper notes that LLMs face challenges with low-resource and distant languages, particularly when both source and target languages are low-resource and from different families, but only tested three specific language pairs.
- Why unresolved: The study's scope was limited to English, Persian, and Russian combinations, leaving performance on other distant language pairs unexplored.
- What evidence would resolve it: Systematic testing of LLMs on diverse distant language pairs (e.g., Korean-Turkish, Swahili-Hindi) with varying resource levels and linguistic distances, comparing performance to traditional MT systems.

### Open Question 2
- Question: What specific prompt engineering strategies most effectively mitigate translation errors and hallucinations in large language models across different language directions?
- Basis in paper: [explicit] The authors identified various linguistic errors and hallucinations but did not systematically test prompt variations to address them, noting that certain models showed sensitivity to prompt length and content.
- Why unresolved: While the paper categorized errors, it did not experiment with targeted prompt modifications to reduce these issues, leaving optimal prompt strategies unclear.
- What evidence would resolve it: Controlled experiments testing multiple prompt structures, example selections, and framework variations to measure their impact on reducing specific error types across language pairs.

### Open Question 3
- Question: How does the performance of large language models on machine translation tasks evolve as models are trained with increasing proportions of multilingual versus monolingual data?
- Basis in paper: [explicit] The authors hypothesized that PaLM's superior multilingual performance may be due to its 78% English and 22% other languages training data, contrasting with GPT's 7% non-English data, but did not test this systematically.
- Why unresolved: The study did not experiment with models trained on different data compositions to isolate the effect of multilingual training data proportion on translation quality.
- What evidence would resolve it: Comparative testing of LLMs with controlled variations in multilingual data proportions during training, measuring translation performance across language pairs with different resource levels.

## Limitations
- Small test corpus (15-50 sentences per language pair) may not capture full variability of translation challenges across domains
- Exact prompt templates and example selection criteria remain unspecified, making exact reproduction challenging
- Study focuses exclusively on literary text, limiting generalizability to technical or conversational domains

## Confidence
- High Confidence: PaLM2's consistent improvement with combined prompts, GPT-3.5 sensitivity to lengthy prompts
- Medium Confidence: Hypothesis about PaLM2's multilingual data proportion driving performance
- Low Confidence: Claim about Perplexity AI's superior prompt comprehension abilities

## Next Checks
1. **Cross-domain validation**: Test the same model/prompt combinations on non-literary text (technical documentation, news articles, conversational dialogue) to assess generalizability beyond the literary domain used in this study.

2. **Prompt template replication**: Create multiple variations of the prompt templates and example selection strategies to determine which specific elements drive performance improvements, particularly for the PaLM2 model's success.

3. **Error analysis expansion**: Conduct a more granular error analysis focusing on specific linguistic phenomena (idioms, syntactic structures, cultural references) to identify which error categories most significantly impact overall translation quality across different language pairs.