---
ver: rpa2
title: 'FissionVAE: Federated Non-IID Image Generation with Latent Space and Decoder
  Decomposition'
arxiv_id: '2408.17090'
source_url: https://arxiv.org/abs/2408.17090
tags:
- latent
- decoder
- data
- client
- fissionvae
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FissionVAE, a federated learning approach
  for non-IID image generation that addresses the challenge of blending visual features
  across heterogeneous data distributions. The core innovation involves decomposing
  the latent space according to client group priors and implementing decoder branches
  specific to each data group, preventing the mixing of unrelated visual features
  during aggregation.
---

# FissionVAE: Federated Non-IID Image Generation with Latent Space and Decoder Decomposition

## Quick Facts
- **arXiv ID**: 2408.17090
- **Source URL**: https://arxiv.org/abs/2408.17090
- **Reference count**: 40
- **Primary result**: FID scores of 42.11 (Mixed MNIST) and 109.10 (CHARM), outperforming baseline federated VAE models

## Executive Summary
This paper addresses the challenge of federated non-IID image generation by introducing FissionVAE, a method that decomposes the latent space according to client group priors and constructs decoder branches tailored to individual client groups. The core innovation prevents the blending of unrelated visual features during model aggregation, which is a common problem when standard federated VAEs are applied to heterogeneous data distributions. Experiments on two composite datasets (Mixed MNIST and CHARM) demonstrate significant improvements in generation quality, with FID scores substantially outperforming baseline federated VAE approaches.

## Method Summary
FissionVAE introduces a federated learning approach that decomposes the latent space into distinctive priors for different client groups while maintaining cross-group encoder aggregation. The method constructs group-specific decoder branches to prevent unrelated visual features from blending during generation. A hierarchical VAE architecture with explicit prior encoding enables better control over generation and supports heterogeneous decoder architectures. The approach explores various strategies for encoding Gaussian priors to enhance decomposition effectiveness, with wave encoding showing particular promise. The method is evaluated on two composite datasets: Mixed MNIST (MNIST + FashionMNIST) and CHARM (cartoon faces, human faces, animals, vessels, and remote sensing images).

## Key Results
- FissionVAE achieves FID scores of 42.11 on Mixed MNIST and 109.10 on CHARM
- Wave encoding strategy for prior distributions enhances performance, particularly as the number of client groups increases
- Latent space decomposition prevents conflicting interpretations across heterogeneous data groups
- Group-specific decoder branches maintain distinct visual characteristics per client group

## Why This Works (Mechanism)

### Mechanism 1: Latent Space Decomposition
- **Claim**: Latent space decomposition prevents conflicting interpretations of the same latent space across heterogeneous data groups.
- **Mechanism**: By assigning each client group its own Gaussian prior (e.g., N(-1, 1) vs N(1, 1)), the encoder maps each group's data to distinct regions of the latent space. This eliminates the blending that occurs when disparate data distributions are forced into a single shared latent space.
- **Core assumption**: Different image types have sufficiently distinct feature distributions that separate latent priors can capture without overlap.
- **Evidence anchors**: [abstract]: "decomposes the latent space according to client group priors"; [section]: "To mitigate the problem of mixed latent space interpretation, FissionVAE decomposes the latent space into distinctive priors"
- **Break condition**: When client groups share overlapping visual features or when the number of groups is too large for Gaussian separation to remain distinct.

### Mechanism 2: Group-Specific Decoder Branches
- **Claim**: Group-specific decoder branches prevent blending of unrelated visual features during model aggregation.
- **Mechanism**: After latent space decomposition, the decoder is split into branches, each trained only on data from its corresponding client group. During aggregation, only decoder branches from the same group are combined, preventing the mixing of texture features from different domains.
- **Core assumption**: Maintaining separate decoder weights for different visual domains is more effective than allowing them to blend during aggregation.
- **Evidence anchors**: [abstract]: "constructing decoder branches tailored to individual client groups"; [section]: "to prevent the blending of unrelated visual features in the generated outputs, FissionVAE employs tailored decoder branches for each client group"
- **Break condition**: When decoder architectures become too dissimilar to aggregate effectively, or when computational resources limit the ability to maintain separate branches.

### Mechanism 3: Hierarchical VAE Architecture
- **Claim**: Hierarchical VAE architecture with explicit prior encoding enables better control over generation and supports heterogeneous decoder architectures.
- **Mechanism**: The two-level latent representation (z1 and z2) allows for explicit control over the first-level latent space through predefined priors, while the second level maintains a standard normal distribution. This structure enables sampling from specific group priors for controlled generation and allows different decoder architectures per group.
- **Core assumption**: A hierarchical structure with explicit prior control provides more meaningful latent representations than flat VAE architectures for complex, heterogeneous data.
- **Evidence anchors**: [abstract]: "We further extends FissionVAE by introducing the hierarchical inference architecture"; [section]: "we show that the branching architecture can be enhanced by integrating hierarchical inference"
- **Break condition**: When the hierarchical structure introduces too much stochasticity relative to the available training data, causing degradation in generation quality.

## Foundational Learning

- **Concept**: Variational Autoencoders and Evidence Lower Bound (ELBO)
  - Why needed here: Understanding how VAEs balance reconstruction quality with latent space regularization is essential for grasping why standard federated VAEs fail with non-IID data and how FissionVAE's modifications help.
  - Quick check question: What are the two terms that make up the ELBO in a standard VAE, and what does each term encourage during training?

- **Concept**: Federated Averaging and non-IID data challenges
  - Why needed here: The paper's innovations directly address the problems that arise when Federated Averaging is applied to heterogeneous data distributions in generative modeling.
  - Quick check question: Why does Federated Averaging typically degrade performance when client data distributions are non-IID, and how does this problem manifest differently for generative versus discriminative models?

- **Concept**: Gaussian mixture modeling and latent space clustering
  - Why needed here: FissionVAE's approach relies on effectively partitioning the latent space into distinct Gaussian regions, which requires understanding how different Gaussian distributions can represent different data clusters.
  - Quick check question: What properties of Gaussian distributions make them suitable (or unsuitable) for representing distinct data clusters in high-dimensional latent spaces?

## Architecture Onboarding

- **Component map**: Server-side: global encoder aggregation (cross-group), group-specific decoder aggregation; Client-side: local encoder/decoder training, group assignment; Shared: prior distribution definitions, hierarchical inference modules (if used).
- **Critical path**: Data → Local Encoder → Group-specific Latent Space → Group-specific Decoder → Generated Image; Aggregation happens separately for encoder (cross-group) and decoder (within-group).
- **Design tradeoffs**: Decoupled latent space improves separation but may reduce sample efficiency; decoder branches prevent feature mixing but increase model complexity and storage requirements; hierarchical structure offers better control but introduces more parameters and potential for instability.
- **Failure signatures**: Mode collapse (all generated images look similar); Feature bleeding (generated images contain mixed elements from different classes); Poor reconstruction quality despite good generation; Degradation in performance as number of client groups increases.
- **First 3 experiments**:
  1. Baseline comparison: Implement standard FedVAE on Mixed MNIST to establish baseline FID and IS scores.
  2. Latent space decomposition: Implement FissionVAE with latent space decomposition only, measure improvement in reconstruction quality and generation separation.
  3. Full FissionVAE: Add decoder branches to the previous implementation, evaluate whether this further improves generation quality and reduces feature mixing artifacts.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does FissionVAE scale to environments with many more client groups beyond the tested five in CHARM?
- **Basis in paper**: [explicit] The paper mentions that "as the number of client groups increases, explicit latent space decoupling provides more direct signal to the VAE about the intra-group difference, allowing the model to better capture the data distribution for different groups."
- **Why unresolved**: The current experiments only test up to five client groups, and the paper acknowledges that developing more sophisticated strategies to scale latent space and decoder decomposition will be crucial for real-world federated learning scenarios with many client groups.
- **What evidence would resolve it**: Experiments testing FissionVAE performance with significantly more client groups (e.g., 10, 20, or 50) would show whether the approach maintains effectiveness or requires architectural modifications for scalability.

### Open Question 2
- **Question**: Can FissionVAE maintain privacy guarantees when using the wave encoding strategy for prior distributions?
- **Basis in paper**: [explicit] The paper shows that "swapping the prior distributions of the MNIST and FashionMNIST groups in the encoder-inclusive pathway leads to severe mode collapse," suggesting that "the group-level privacy may be preserved by maintaining the confidentiality of prior distributions."
- **Why unresolved**: While the paper demonstrates that incorrect prior distributions produce poor quality outputs, it doesn't formally analyze whether this provides actual privacy guarantees against membership inference or reconstruction attacks.
- **What evidence would resolve it**: Formal privacy analysis (e.g., differential privacy metrics) showing that knowledge of prior distributions alone doesn't reveal membership information, combined with adversarial attacks attempting to recover data from prior distributions.

### Open Question 3
- **Question**: What is the optimal trade-off between latent space decoupling and decoder branch decomposition for different numbers of client groups?
- **Basis in paper**: [explicit] The paper notes that "FissionVAE+L shows little gain on the Mixed MNIST dataset while the generative performance is further improved on CHARM dataset," and "Enforcing latent space decoupling on FissionVAE+D even lowers the performance on FID" for the simpler dataset.
- **Why unresolved**: The paper observes different performance patterns but doesn't provide a systematic analysis of when each component is most beneficial or how to automatically determine the optimal combination for a given number of client groups.
- **What evidence would resolve it**: Systematic experiments varying the number of client groups and measuring the relative contribution of latent space decoupling versus decoder branching, potentially leading to a decision framework for choosing the optimal configuration.

## Limitations
- The effectiveness of latent space decomposition using Gaussian priors may degrade as the number of client groups increases, though the paper doesn't extensively explore this scalability limit.
- The hierarchical VAE extension adds significant architectural complexity without clear evidence that the benefits outweigh the additional parameters and training instability risks.
- The choice of Gaussian priors for latent space decomposition, while intuitive, may not be optimal for all types of heterogeneous data distributions.

## Confidence

- **High Confidence**: The core mechanism of decoder branching to prevent feature mixing (FID scores show clear improvement over baselines)
- **Medium Confidence**: The effectiveness of latent space decomposition using Gaussian priors (supported by experimental results but limited theoretical justification)
- **Low Confidence**: The necessity and optimality of the hierarchical VAE extension (limited ablation studies and no comparison to alternative architectural choices)

## Next Checks

1. **Scalability test**: Evaluate FissionVAE performance with increasing numbers of client groups (3→5→10) to identify the point where Gaussian separation becomes ineffective and measure degradation in FID scores.
2. **Prior distribution comparison**: Replace Gaussian priors with alternative distributions (e.g., uniform, von Mises-Fisher) for latent space decomposition and compare generation quality across all metrics to determine if Gaussians are optimal.
3. **Architectural ablation**: Test a flat VAE with decoder branching against the full hierarchical implementation to isolate whether the hierarchical structure provides measurable benefits beyond simple decoder decomposition.