---
ver: rpa2
title: 'RakutenAI-7B: Extending Large Language Models for Japanese'
arxiv_id: '2403.15484'
source_url: https://arxiv.org/abs/2403.15484
tags:
- japanese
- language
- rakutenai-7b
- performance
- shots
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RakutenAI-7B introduces a suite of Japanese-oriented large language
  models based on the Mistral 7B-v0.1 architecture, with extended vocabulary (48k
  tokens) to better handle Japanese text. The models achieve the best performance
  on Japanese LM-Harness benchmarks among open 7B models, scoring 62.83 average across
  tasks, and maintaining competitive English performance (60.50 average).
---

# RakutenAI-7B: Extending Large Language Models for Japanese

## Quick Facts
- arXiv ID: 2403.15484
- Source URL: https://arxiv.org/abs/2403.15484
- Reference count: 7
- Primary result: State-of-the-art Japanese performance among open 7B models (62.83 average on Japanese LM-Harness)

## Executive Summary
RakutenAI-7B is a suite of Japanese-oriented large language models based on the Mistral 7B-v0.1 architecture, specifically designed to handle Japanese text more effectively. The model extends the vocabulary to 48k tokens, significantly improving Japanese language processing capabilities while maintaining competitive English performance. RakutenAI-7B achieves the best performance on Japanese LM-Harness benchmarks among open 7B models, with further improvements through instruction and chat tuning variants.

## Method Summary
The model is built by fine-tuning the Mistral 7B-v0.1 architecture with extended vocabulary (48k tokens) optimized for Japanese text processing. The training pipeline uses carefully filtered datasets to ensure high-quality outputs, with specific attention to Japanese language patterns and structures. Safety fine-tuning is incorporated to mitigate harmful content generation. The model is released under Apache 2.0 license, making it accessible for both research and industry applications.

## Key Results
- Achieves 62.83 average score on Japanese LM-Harness benchmarks, outperforming all other open 7B models
- Maintains competitive English performance at 60.50 average while excelling in Japanese tasks
- Instruction-tuned variant (RakutenAI-7B-instruct) improves Japanese performance to 68.74 average

## Why This Works (Mechanism)
The extended vocabulary size (48k tokens) specifically addresses the challenges of Japanese text, which requires handling multiple character sets and complex tokenization patterns. The model leverages the strong foundational architecture of Mistral 7B-v0.1 while adapting it to Japanese linguistic structures. The careful dataset filtering and safety fine-tuning ensure that the model produces high-quality, contextually appropriate outputs while minimizing harmful content generation.

## Foundational Learning

**Japanese tokenization patterns**: Japanese requires handling kanji, hiragana, katakana, and Latin characters within the same text. Understanding these patterns is crucial for effective language modeling and proper context handling.

**Multi-character set processing**: Japanese text combines multiple writing systems that need to be processed coherently. This complexity requires specialized vocabulary extension and tokenization strategies.

**Context window management**: Japanese language often relies on implicit context and honorifics that affect meaning. Proper context handling is essential for generating appropriate and culturally sensitive responses.

**Safety fine-tuning principles**: Understanding how to effectively filter and fine-tune models to avoid harmful content generation while maintaining performance is critical for responsible AI deployment.

## Architecture Onboarding

**Component map**: Tokenizer (48k tokens) -> Mistral 7B-v0.1 architecture -> Fine-tuning pipeline -> Safety filters -> Instruction/chat tuning

**Critical path**: Tokenization -> Attention mechanism -> Transformer layers -> Output generation

**Design tradeoffs**: Extended vocabulary improves Japanese performance but increases memory requirements and computational complexity. Safety fine-tuning may slightly reduce model expressiveness but ensures responsible deployment.

**Failure signatures**: Potential issues include improper handling of honorifics, context misinterpretation in multi-character set text, and generation of culturally insensitive content if safety fine-tuning is insufficient.

**First experiments**: 1) Evaluate tokenization efficiency on mixed Japanese-English text samples, 2) Test context handling with honorific variations, 3) Assess safety fine-tuning effectiveness on harmful content prompts

## Open Questions the Paper Calls Out

None specified in the source material.

## Limitations

- Evaluation scope limited to Japanese LM-Harness benchmarks, potentially missing diverse Japanese use cases
- Comparison based only on 7B-parameter open models, excluding larger or proprietary Japanese models
- Safety fine-tuning effectiveness not independently verified

## Confidence

**High confidence**: Performance claims on Japanese LM-Harness benchmarks (62.83 average), competitive English performance (60.50 average), and instruction-tuned improvements (68.74 average)

**Medium confidence**: Safety fine-tuning effectiveness claims due to lack of independent verification

## Next Checks

1. Conduct evaluation on additional Japanese language benchmarks beyond Japanese LM-Harness to verify performance generalization across diverse domains

2. Perform systematic testing of safety fine-tuning effectiveness using established harmful content generation benchmarks

3. Compare model performance against larger parameter models (13B, 34B) and proprietary Japanese LLMs to establish competitive positioning more comprehensively