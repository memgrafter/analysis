---
ver: rpa2
title: Exploring the Capability of ChatGPT to Reproduce Human Labels for Social Computing
  Tasks (Extended Version)
arxiv_id: '2407.06422'
source_url: https://arxiv.org/abs/2407.06422
tags:
- chatgpt
- annotation
- dataset
- human
- gpt-rater
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates ChatGPT's capability to replace human annotators
  for social computing tasks. The authors re-annotate seven datasets using ChatGPT,
  covering COVID-19 controversies, social bot deception, cyberbullying, clickbait
  news, and the Russo-Ukrainian War.
---

# Exploring the Capability of ChatGPT to Reproduce Human Labels for Social Computing Tasks (Extended Version)

## Quick Facts
- **arXiv ID**: 2407.06422
- **Source URL**: https://arxiv.org/abs/2407.06422
- **Reference count**: 40
- **Key outcome**: ChatGPT achieves an average annotation F1-score of 72.00% across seven social computing datasets, with GPT-Rater predicting performance with 95.00% F1-score on clickbait detection

## Executive Summary
This study evaluates ChatGPT's capability to replace human annotators for social computing tasks, including COVID-19 controversies, social bot deception, cyberbullying, clickbait news, and the Russo-Ukrainian War. The authors re-annotate seven public datasets using ChatGPT and develop GPT-Rater, a tool that predicts annotation correctness. Results show ChatGPT achieves an average F1-score of 72.00% across datasets, with highest performance on clickbait headlines (89.66% accuracy) and lowest on COVID-19 hate speech (52.24% accuracy). GPT-Rater demonstrates high effectiveness, achieving an average F1-score of 95.00% on clickbait headlines and exceeding 75% for five out of seven datasets.

## Method Summary
The authors collected seven public datasets covering five social computing tasks and used the OpenAI API with gpt-3.5-turbo-0631 model to generate annotations. They employed a standardized prompt template for classification tasks and compared ChatGPT's annotations against human annotations using weighted F1-score. GPT-Rater was implemented by training classifiers on document embeddings extracted using OpenAI's text-embedding-ada-002 model. The tool predicts ChatGPT's annotation correctness by training on subsets of annotated data and testing on held-out portions, with performance evaluated across multiple classification algorithms.

## Key Results
- ChatGPT achieves an average annotation F1-score of 72.00% across seven datasets
- Highest performance on clickbait headlines (89.66% accuracy), lowest on COVID-19 hate speech (52.24% accuracy)
- GPT-Rater predicts ChatGPT's annotation correctness with 95.00% F1-score on clickbait headlines and exceeds 75% for five out of seven datasets

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT can substitute human annotators for social computing tasks, but performance varies significantly by task and label type. Large language models encode contextual patterns from training data that align with human judgment on specific annotation tasks, enabling automated labeling that approximates human quality when the task structure matches model capabilities. The tasks selected (vaccine stance, hate speech, fake news, social bot detection, cyberbullying, clickbait, Russo-Ukrainian stance) are sufficiently represented in ChatGPT's training data and align with its learned patterns. Break condition: When task complexity exceeds model training distribution, requires nuanced cultural context, or involves adversarial examples not present in training data.

### Mechanism 2
GPT-Rater can predict ChatGPT's annotation correctness using document embeddings and binary classification. Document embeddings capture semantic features that correlate with ChatGPT's decision-making patterns, allowing a classifier to predict whether ChatGPT will correctly label a given item. The semantic features captured by text embeddings are sufficient to distinguish between items ChatGPT will label correctly versus incorrectly. Break condition: When embedding space fails to capture task-specific features that determine ChatGPT's correctness, or when model behavior is too stochastic.

### Mechanism 3
ChatGPT's annotation performance is predictable and correlates with GPT-Rater's predictions. ChatGPT's behavior follows consistent patterns based on input characteristics, making it possible to build a reliable predictor for its performance. ChatGPT's annotation decisions are not purely random but follow systematic patterns that can be learned and predicted. Break condition: When ChatGPT's behavior changes due to model updates, fine-tuning, or when facing completely novel task domains.

## Foundational Learning

- **Concept: F1-score and classification metrics**
  - Why needed here: The paper evaluates ChatGPT's performance using F1-score, precision, and recall, which are essential for understanding classification quality
  - Quick check question: If a classifier has precision of 80% and recall of 70%, what is its F1-score? (Answer: 2*(0.8*0.7)/(0.8+0.7) = 0.741)

- **Concept: Document embeddings and semantic representation**
  - Why needed here: GPT-Rater uses text embeddings to represent input features for classification, requiring understanding of how text is converted to numerical representations
  - Quick check question: What is the primary purpose of using document embeddings in machine learning classification tasks? (Answer: To convert text into numerical vectors that capture semantic meaning for algorithmic processing)

- **Concept: Spearman correlation and statistical significance**
  - Why needed here: The paper reports correlation between ChatGPT and GPT-Rater performance using Spearman correlation with p-values
  - Quick check question: What does a Spearman correlation coefficient of 0.964 indicate about the relationship between two ranked variables? (Answer: Very strong positive monotonic relationship)

## Architecture Onboarding

- **Component map**: Data collection -> ChatGPT annotation pipeline -> Performance evaluation -> GPT-Rater prediction pipeline
- **Critical path**: Load human-annotated dataset -> Generate ChatGPT annotations using prompt template -> Compare ChatGPT labels with human labels to calculate performance metrics -> Extract document embeddings for all items -> Train GPT-Rater classifier on subset of data -> Evaluate GPT-Rater's prediction accuracy on held-out data
- **Design tradeoffs**: Simple imperative templates vs. complex few-shot examples (simplicity favors reproducibility but may limit performance) | Multiple algorithms tested but no ensemble approach (tradeoff between complexity and marginal gains) | GPT-Rater works with small subsets but performance varies by task (efficiency vs. accuracy tradeoff)
- **Failure signatures**: Low F1-score on certain datasets indicates task complexity beyond model capabilities | High variance in predictions suggests instability in ChatGPT's behavior | Poor GPT-Rater performance on new domains indicates feature space mismatch | API parsing failures (>3%) suggest prompt-template incompatibility
- **First 3 experiments**: Reproduce ChatGPT annotations on a single dataset with different prompt variations to establish baseline performance range | Train GPT-Rater on 10%, 20%, 50%, and 80% subsets of the same dataset to map training data requirements | Test GPT-Rater on a new, unseen dataset from the same task category to validate generalization capability

## Open Questions the Paper Calls Out

### Open Question 1
How can prompt engineering strategies be optimized to improve ChatGPT's annotation performance across different social computing tasks? The authors acknowledge that they only used a single prompt format and mention this as a major theme for future work, stating "Prompt design is a major theme of future work, which we believe we yield better results." The paper only tested one prompt formulation approach, leaving the potential impact of alternative prompt engineering techniques unexplored. Comparative experiments testing multiple prompt engineering strategies (e.g., few-shot prompting, Chain-of-Thought, role-based prompting) across the seven datasets, measuring performance improvements over the baseline single prompt approach would resolve this.

### Open Question 2
What are the specific linguistic or semantic features that cause ChatGPT to perform differently across various labels within the same annotation task? The authors observe "significant gaps (over 25%) exist between labels' accuracy on five out of seven datasets" and note variations in performance across individual labels. While the paper identifies that performance varies by label, it doesn't analyze the underlying linguistic or semantic characteristics that contribute to these differences. Detailed linguistic analysis of correctly vs. incorrectly annotated items, identifying features such as sentiment intensity, syntactic complexity, or topic specificity that correlate with ChatGPT's annotation success or failure would resolve this.

### Open Question 3
How does ChatGPT's performance on social computing annotation tasks compare to specialized fine-tuned models trained on the same datasets? The paper focuses on ChatGPT's general capabilities but doesn't benchmark against domain-specific models, despite mentioning that "supervised methods are being introduced for classification tasks." The study establishes ChatGPT's baseline performance but doesn't contextualize it against traditional machine learning approaches that might be better suited for specific tasks. Head-to-head comparisons of ChatGPT's annotation performance against state-of-the-art supervised models trained on each dataset, measuring accuracy, efficiency, and resource requirements would resolve this.

## Limitations

- ChatGPT's performance varies significantly across tasks, with high accuracy on clickbait detection but struggling with nuanced tasks like COVID-19 hate speech
- GPT-Rater's effectiveness drops for more complex annotation tasks, indicating potential limitations in capturing features that determine ChatGPT's correctness across diverse domains
- The study doesn't benchmark against specialized fine-tuned models, leaving unclear how ChatGPT compares to traditional machine learning approaches for specific tasks

## Confidence

- **High Confidence**: ChatGPT's ability to perform basic classification tasks like clickbait detection and stance classification with reasonable accuracy
- **Medium Confidence**: GPT-Rater's predictive capability for ChatGPT's performance, particularly for simpler tasks with clear categorical distinctions
- **Low Confidence**: GPT-Rater's effectiveness for complex social computing tasks requiring nuanced judgment, and the tool's generalizability to entirely new domains

## Next Checks

1. Test GPT-Rater on a new, unseen dataset from a different social computing domain (e.g., mental health discourse or political polarization) to evaluate domain transfer capability and identify failure modes
2. Conduct ablation studies varying prompt complexity (few-shot examples vs. simple templates) to determine optimal prompt design for different task types and measure the impact on both ChatGPT's performance and GPT-Rater's predictive accuracy
3. Evaluate ChatGPT's performance on adversarial examples or edge cases specifically designed to test the boundaries of its training data coverage, comparing results with human annotator performance to identify systematic biases or failure patterns