---
ver: rpa2
title: Intersectional Unfairness Discovery
arxiv_id: '2405.20790'
source_url: https://arxiv.org/abs/2405.20790
tags:
- bias
- sensitive
- intersectional
- attributes
- bggn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles intersectional fairness discovery in AI models,
  addressing the limitation of existing research that focuses on single sensitive
  attributes. The authors propose a Bias-Guided Generative Network (BGGN) that treats
  bias values as rewards to efficiently generate diverse high-bias intersectional
  sensitive attributes.
---

# Intersectional Unfairness Discovery

## Quick Facts
- arXiv ID: 2405.20790
- Source URL: https://arxiv.org/abs/2405.20790
- Authors: Gezheng Xu; Qi Chen; Charles Ling; Boyu Wang; Changjian Shui
- Reference count: 40
- Primary result: Bias-Guided Generative Network (BGGN) discovers more diverse and high-bias intersectional sensitive attributes than traditional search algorithms

## Executive Summary
This paper addresses the challenge of discovering intersectional unfairness in AI models by proposing a Bias-Guided Generative Network (BGGN). Unlike existing research that focuses on single sensitive attributes, BGGN can efficiently generate diverse high-bias intersectional sensitive attributes by treating bias values as rewards during the generative process. The method leverages variational inference to align the generative model's distribution with high-bias regions of the attribute space, enabling the discovery of unseen but potentially high-bias attribute combinations. Experiments on CelebA and Toxic datasets demonstrate BGGN's effectiveness in identifying intersectional subgroups that traditional search methods miss.

## Method Summary
The method involves training a predictive model to identify bias in intersectional sensitive attributes, then using variational inference to create a generative model that prioritizes high-bias attribute combinations. BGGN consists of an encoder and decoder trained using a two-stage process: first pre-training a standard VAE, then fine-tuning it with a bias-guided objective that maximizes the bias value while maintaining attribute diversity. The generated attributes are evaluated by using them as prompts for modern generative AI models like LLaMA and Midjourney to assess their potential for producing biased outputs.

## Key Results
- BGGN discovers significantly more diverse and high-bias intersectional sensitive attributes than traditional search algorithms on both CelebA and Toxic datasets
- The method successfully generates unseen but potentially high-bias intersectional attributes that, when used as prompts, frequently result in biased outputs from modern generative AI models
- BGGN demonstrates consistent performance across different bias thresholds, showing robustness in discovering intersectional unfairness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BGGN can generate high-bias intersectional sensitive attributes more efficiently than enumeration or search methods.
- Mechanism: BGGN treats the bias value (prediction loss) as a reward signal and uses variational inference to align the generative model's distribution with the distribution of bias values. This focuses generation on high-bias regions of the attribute space.
- Core assumption: The distribution of intersectional sensitive attributes that yield high prediction loss can be modeled and sampled efficiently using variational inference with bias as a reward.
- Evidence anchors:
  - [abstract] "By treating each bias value as a reward, BGGN efficiently generates high-bias intersectional sensitive attributes."
  - [section 3.1] "To achieve this, we design a Bias-Guided Generative Network (BGGN) such that pθ(a) follows the distribution of the bias value, i.e., the prediction loss of a model f(x) on the intersectional sensitive attribute a"
- Break condition: If the bias landscape is too sparse or discontinuous, variational inference may fail to focus generation effectively.

### Mechanism 2
- Claim: BGGN can discover unseen but potentially high-bias intersectional sensitive attributes.
- Mechanism: By modeling the generative process, BGGN can generalize beyond observed combinations of attributes to generate plausible unseen combinations that may have high bias.
- Core assumption: The learned latent space in the VAE captures the semantic structure of intersectional sensitive attributes well enough to interpolate or extrapolate to unseen combinations.
- Evidence anchors:
  - [abstract] "Furthermore, BGGN generates unseen but potentially high-bias attributes, which when used as prompts for modern generative AI models, frequently result in biased outputs, highlighting potential unfairness in these systems."
  - [section 5.2] "BGGN could generate unseen but potentially high-bias intersectional sensitive attributes. To assess the validity of discovery in the real-world, we formulate these a as prompts and employ modern generative AI models such as LLaMA (Touvron et al., 2023a) and Midjourney (Midjourney, 2024) to produce text and images."
- Break condition: If the attribute space has strong combinatorial constraints or dependencies not captured by the generative model, unseen combinations may be invalid or nonsensical.

### Mechanism 3
- Claim: The bias-guided objective improves generation efficiency compared to standard generative modeling.
- Mechanism: Standard VAE learns pdata(a), which is dominated by low-bias attributes. BGGN's objective explicitly maximizes bias value Lf(a) during generation, shifting focus to high-bias regions.
- Core assumption: The bias value (prediction loss) is a meaningful and differentiable signal that can guide the generative process effectively.
- Evidence anchors:
  - [abstract] "Experiments on real-world text and image datasets demonstrate that BGGN discovers significantly more diverse and high-bias intersectional sensitive attributes than traditional search algorithms."
  - [section 4] "In this step, we fine-tune the generative model pθ(a|z) to maximize the reward r(a) = log qϕ(z|a) + L̂f(a)."
- Break condition: If the bias signal is too noisy or if the predictive model f(x) is poorly calibrated, the bias-guided objective may lead to degenerate generation.

## Foundational Learning

- Concept: Variational Inference
  - Why needed here: BGGN uses variational inference to learn a generative model that can sample intersectional sensitive attributes according to their bias values.
  - Quick check question: What is the Evidence Lower Bound (ELBO) in variational inference, and how does it relate to the KL divergence between the approximate and true posterior distributions?

- Concept: Bias Metrics and Fairness Notions
  - Why needed here: The paper defines bias in terms of prediction loss on intersectional sensitive attributes. Understanding different fairness metrics is crucial for contextualizing this approach.
  - Quick check question: How does the prediction loss criterion for bias differ from other fairness notions like demographic parity or equalized odds?

- Concept: Generative Adversarial Networks (GANs) and VAEs
  - Why needed here: BGGN is based on a VAE architecture. Understanding the differences between GANs and VAEs, and their respective strengths and weaknesses, is important for grasping BGGN's design choices.
  - Quick check question: What are the key differences between GANs and VAEs in terms of their training objectives and the types of distributions they can model?

## Architecture Onboarding

- Component map:
  Predictive model f(x) -> Bias value predictor L̂f(a) -> Bias-Guided Generative Network (BGGN) -> Modern generative AI models (e.g., LLaMA, Midjourney)

- Critical path:
  1. Train predictive model f(x) on original dataset
  2. Collect bias values for all intersectional sensitive attributes in the dataset
  3. Train bias value predictor L̂f(a) to approximate Lf(a)
  4. Pre-train a standard VAE on the intersectional sensitive attributes
  5. Fine-tune the VAE using the bias-guided objective to obtain BGGN
  6. Generate diverse high-bias intersectional sensitive attributes using BGGN
  7. Evaluate the bias of the generated attributes by using them as prompts for modern generative AI models

- Design tradeoffs:
  - VAE vs. GAN: VAE provides a principled probabilistic framework and allows for efficient sampling, but may suffer from posterior collapse. GANs can generate sharper samples but are harder to train and may not provide a well-defined latent space.
  - Bias as reward vs. other objectives: Using bias as a reward directly targets the discovery of unfair subgroups, but may neglect other important aspects of the attribute distribution.

- Failure signatures:
  - Mode collapse: BGGN generates only a limited set of high-bias attributes, failing to capture the diversity of the bias landscape
  - Degenerate generation: BGGN generates nonsensical or invalid intersectional sensitive attributes, indicating that the generative model has overfit to the bias signal
  - Poor generalization: BGGN fails to discover unseen high-bias attributes, suggesting that the learned latent space does not capture the semantic structure of the attribute space well

- First 3 experiments:
  1. Train BGGN on a small synthetic dataset with known bias patterns and verify that it can discover the biased subgroups
  2. Compare the diversity and bias level of attributes generated by BGGN and a standard VAE on a real-world dataset
  3. Evaluate the bias of BGGN-generated attributes using modern generative AI models and compare the results to attributes from the original dataset

## Open Questions the Paper Calls Out

- Question: How does the performance of BGGN compare to other generative models specifically designed for handling bias, such as FairGen or FairMix?
  - Basis in paper: [inferred] The paper compares BGGN to a standard VAE but does not explore other bias-aware generative models.
  - Why unresolved: The authors focus on demonstrating the effectiveness of BGGN compared to simpler methods (search algorithms, standard VAE) but do not benchmark against more advanced bias-aware generative models.
  - What evidence would resolve it: Empirical results comparing BGGN to other state-of-the-art bias-aware generative models on the same datasets and evaluation metrics would provide a clear answer.

- Question: What are the long-term effects of using BGGN-generated prompts on the bias of large language models or image generators?
  - Basis in paper: [inferred] The paper shows that BGGN-generated prompts can lead to biased outputs from modern generative AI models, but does not investigate the potential for these prompts to further amplify or perpetuate bias over time.
  - Why unresolved: The study focuses on a single interaction between BGGN-generated prompts and generative AI models. The long-term impact on model behavior and bias is not explored.
  - What evidence would resolve it: Longitudinal studies tracking the performance and bias of generative AI models when repeatedly exposed to BGGN-generated prompts would provide insights into potential long-term effects.

- Question: How does the choice of bias threshold (τ) affect the diversity and quality of the discovered intersectional sensitive attributes?
  - Basis in paper: [explicit] The paper evaluates BGGN under different bias thresholds but does not provide a detailed analysis of how the threshold impacts the results.
  - Why unresolved: While the paper shows that BGGN performs well under different thresholds, it does not explore the relationship between the threshold and the characteristics of the discovered attributes.
  - What evidence would resolve it: A comprehensive study varying the bias threshold and analyzing the resulting diversity, quality, and bias of the discovered attributes would shed light on the impact of this parameter.

- Question: Can BGGN be extended to handle non-binary sensitive attributes or continuous-valued attributes?
  - Basis in paper: [inferred] The paper focuses on binary sensitive attributes, but the authors do not discuss the potential for extending BGGN to handle more complex attribute types.
  - Why unresolved: The current formulation of BGGN is tailored for binary attributes, and the authors do not provide insights into how the method could be adapted for other attribute types.
  - What evidence would resolve it: A modified version of BGGN that can handle non-binary or continuous attributes, along with empirical results demonstrating its effectiveness, would address this question.

## Limitations

- The effectiveness of BGGN heavily depends on the quality of the bias value predictor L̂f(a), and poor approximation could compromise the entire bias-guided generation process
- The experiments are limited to two specific datasets (CelebA and Toxic), raising questions about the approach's generalizability to other domains and attribute types
- The validation using modern generative AI models (LLaMA, Midjourney) introduces additional sources of bias that are not fully accounted for in the analysis

## Confidence

- High confidence: The core mechanism of using variational inference with bias as a reward to guide generation towards high-bias intersectional attributes is well-explained and theoretically sound
- Medium confidence: The experimental results showing improved diversity and bias detection compared to traditional search methods are promising, but the limited scope of datasets and lack of ablation studies reduce confidence in the general applicability of the approach
- Low confidence: The claim that BGGN can reliably discover unseen but potentially high-bias attributes is intriguing but not thoroughly validated

## Next Checks

1. Conduct ablation studies to evaluate the impact of different predictive models f(x) and bias metrics on the performance of BGGN to understand robustness and generalizability

2. Perform experiments on additional datasets from diverse domains (e.g., healthcare, finance) to assess scalability and domain-transferability, including investigation of non-binary attributes

3. Implement a more rigorous validation pipeline for discovered unseen attributes using multiple generative models and human evaluation to assess validity and potential for bias, comparing bias levels to real-world datasets