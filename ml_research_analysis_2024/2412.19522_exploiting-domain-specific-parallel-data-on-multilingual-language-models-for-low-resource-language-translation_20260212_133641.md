---
ver: rpa2
title: Exploiting Domain-Specific Parallel Data on Multilingual Language Models for
  Low-resource Language Translation
arxiv_id: '2412.19522'
source_url: https://arxiv.org/abs/2412.19522
tags:
- data
- size
- language
- final
- pair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of building domain-specific Neural
  Machine Translation (NMT) systems for low-resource languages (LRLs) using multilingual
  sequence-to-sequence Language Models (msLMs). When parallel data is limited for
  a specific language or domain, auxiliary domain data can be leveraged through fine-tuning
  or continuous pre-training of the msLM.
---

# Exploiting Domain-Specific Parallel Data on Multilingual Language Models for Low-resource Language Translation

## Quick Facts
- arXiv ID: 2412.19522
- Source URL: https://arxiv.org/abs/2412.19522
- Reference count: 40
- Primary result: Multi-domain ITTL performs best for in-domain tasks, but its advantage diminishes as target domain data increases

## Executive Summary
This paper addresses the challenge of building domain-specific Neural Machine Translation (NMT) systems for low-resource languages (LRLs) using multilingual sequence-to-sequence Language Models (msLMs). When parallel data is limited for a specific language or domain, auxiliary domain data can be leveraged through fine-tuning or continuous pre-training of the msLM. The study evaluates various strategies, including single-domain fine-tuning, multi-domain fine-tuning, and Intermediate Task Transfer Learning (ITTL), under both in-domain and out-domain test scenarios. It also investigates the impact of domain divergence on NMT performance using Jensen-Shannon Divergence (JSD).

## Method Summary
The study evaluates multiple strategies for leveraging auxiliary domain parallel data for domain-specific NMT in low-resource languages. These strategies include: (1) Single-domain fine-tuning (FT), (2) Multi-domain fine-tuning, (3) Single-domain ITTL, and (4) Multi-domain ITTL. The experiments are conducted on five languages (Sinhala, Tamil, Kannada, Gujarati, Hindi) using various domain-specific corpora. The study uses mBART as the base model and evaluates performance using SentencePiece BLEU (spBLEU). Domain divergence is quantified using Jensen-Shannon Divergence (JSD) to analyze how domain relatedness affects strategy effectiveness.

## Key Results
- Bitext denoising pre-training with small parallel data sets (<50k sentences) does not yield significant gains.
- Multi-domain ITTL performs best for in-domain tasks, but its advantage diminishes as target domain data increases.
- For out-domain tasks, the best strategy depends on domain relatedness and dataset sizes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bitext denoising pre-training does not yield significant gains when parallel data size is below 50k sentences.
- Mechanism: Pre-training with denoising objectives aims to improve the representation of parallel data, but if the data is too small, the model cannot learn robust enough representations to surpass baseline fine-tuning performance.
- Core assumption: The size of parallel data is a critical factor for effective pre-training with denoising objectives.
- Evidence anchors:
  - [abstract]: "Bitext denoising pre-training with small parallel data sets (<50k sentences) does not yield significant gains."
  - [section]: "When the parallel data set size is small (less than 50k, in our experiments), pre-training with bitext denoising yields no gains for both in- and out-domain setups."
  - [corpus]: Weak evidence - the paper uses only three languages with varying representation in mBART, limiting generalizability.
- Break condition: If parallel data size increases significantly beyond 50k sentences, pre-training might start to show gains.

### Mechanism 2
- Claim: Multi-domain Intermediate Task Transfer Learning (ITTL) is the best strategy for in-domain tasks, but its advantage diminishes as target domain data increases.
- Mechanism: Using auxiliary domain data in an intermediate task helps the model learn domain-agnostic features, which can be beneficial when fine-tuning on the target domain. However, as the target domain data grows, the model can learn these features directly, reducing the need for the intermediate task.
- Core assumption: The size of the target domain data affects the necessity and effectiveness of multi-domain ITTL.
- Evidence anchors:
  - [abstract]: "Multi-domain ITTL performs best for in-domain tasks, but its advantage diminishes as target domain data increases."
  - [section]: "In-domain results are reported in Tables 5-8... Multi-domain ITTL is the best-performing technique... However, on average, its gains over the second-best technique... is less than 1 spBLEU."
  - [corpus]: Moderate evidence - the paper uses three domain-specific corpora and one open-domain corpus, providing a reasonable variety for testing this mechanism.
- Break condition: If the target domain data is very large (e.g., 100k sentences), the benefit of multi-domain ITTL may disappear entirely.

### Mechanism 3
- Claim: For out-domain tasks, the best strategy depends on domain relatedness and dataset sizes.
- Mechanism: When the target domain is out-of-domain, the model needs to adapt to a new domain. The effectiveness of different strategies (e.g., single-domain ITTL, multi-domain ITTL, multi-domain FT) depends on how related the auxiliary domain is to the target domain and the sizes of the datasets.
- Core assumption: Domain divergence, quantified by JSD, is a key factor in determining the effectiveness of different strategies for out-domain tasks.
- Evidence anchors:
  - [abstract]: "For out-domain tasks, the best strategy depends on domain relatedness and dataset sizes."
  - [section]: "We use the results for the FLORES test set to explain our observations in Tables 9-12... Here, baseline refers to vanilla FT with the dataset used for the final task..."
  - [corpus]: Strong evidence - the paper uses multiple domain-specific corpora with varying degrees of divergence from the FLORES test set, allowing for a detailed analysis of this mechanism.
- Break condition: If the auxiliary domain is highly related to the target domain, or if the dataset sizes are very large, the optimal strategy may change.

## Foundational Learning

- Concept: Domain divergence
  - Why needed here: Understanding how different domains relate to each other is crucial for selecting the best strategy for out-domain tasks.
  - Quick check question: How is domain divergence quantified in this paper, and what does a high JSD value indicate?

- Concept: Intermediate Task Transfer Learning (ITTL)
  - Why needed here: ITTL is a key strategy for leveraging auxiliary domain data, and understanding its mechanisms is essential for interpreting the results.
  - Quick check question: What are the two main variations of ITTL explored in this paper, and how do they differ?

- Concept: Fine-tuning vs. continuous pre-training
  - Why needed here: The paper compares these two approaches for utilizing auxiliary domain data, and understanding their differences is crucial for interpreting the results.
  - Quick check question: What are the two pre-training objective functions used in this paper, and how do they differ from fine-tuning?

## Architecture Onboarding

- Component map:
  - Multilingual sequence-to-sequence Language Models (msLMs) -> Fine-tuning techniques (Single-domain FT, Multi-domain FT, Single-domain ITTL, Multi-domain ITTL) -> Pre-training techniques (Bitext denoising, Monolingual denoising) -> Evaluation metrics (SentencePiece BLEU)

- Critical path:
  - Select appropriate msLM (e.g., mBART)
  - Prepare parallel data from target and auxiliary domains
  - Choose fine-tuning or pre-training strategy
  - Train model using selected strategy
  - Evaluate model performance using spBLEU
  - Analyze impact of domain divergence using JSD

- Design tradeoffs:
  - Single-domain vs. multi-domain fine-tuning: Single-domain is simpler but may not leverage auxiliary data as effectively; multi-domain can improve performance but is more complex.
  - Fine-tuning vs. continuous pre-training: Fine-tuning is faster and requires less data, but continuous pre-training may lead to better performance with sufficient data.
  - Intermediate task size vs. final task size: A larger intermediate task can provide more robust features, but a larger final task can lead to better direct learning.

- Failure signatures:
  - Poor performance on target domain: May indicate that the auxiliary domain is not sufficiently related or that the strategy is not effective.
  - Overfitting to auxiliary domain: May occur if the auxiliary domain is too large or too different from the target domain.
  - No improvement over baseline: May indicate that the strategy is not effective for the given data sizes or domain configurations.

- First 3 experiments:
  1. Compare vanilla fine-tuning with multi-domain fine-tuning using small auxiliary data (1k sentences) for an in-domain task.
  2. Compare single-domain ITTL with multi-domain ITTL using large auxiliary data (25k sentences) for an out-domain task.
  3. Compare bitext denoising pre-training with vanilla fine-tuning using very small parallel data (1k sentences) for an in-domain task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does intermediate task transfer learning (ITTL) outperform other fine-tuning strategies for low-resource language translation?
- Basis in paper: [explicit] The paper identifies that multi-domain ITTL performs best for in-domain tasks, but its advantage diminishes with larger target domain datasets. It also notes that for out-domain tasks, the best strategy depends on domain relatedness and dataset sizes.
- Why unresolved: The paper provides general guidelines but does not offer precise thresholds or conditions under which ITTL is consistently optimal.
- What evidence would resolve it: Detailed experiments comparing ITTL with other strategies across a wider range of domain relatedness and dataset sizes, identifying specific thresholds where ITTL becomes or ceases to be the best approach.

### Open Question 2
- Question: How does the size of the parallel corpus impact the effectiveness of bitext denoising pre-training for low-resource languages?
- Basis in paper: [explicit] The paper finds that bitext denoising pre-training with small parallel data sets (<50k sentences) does not yield significant gains, but does not explore larger datasets.
- Why unresolved: The study only tests up to 25k sentences, leaving open the question of whether larger datasets might improve pre-training effectiveness.
- What evidence would resolve it: Experiments with bitext denoising pre-training using larger parallel corpora (e.g., 50k to 100k sentences) to determine if and when this approach becomes beneficial.

### Open Question 3
- Question: What is the optimal balance between single-domain and multi-domain fine-tuning for domain-specific translation tasks in low-resource languages?
- Basis in paper: [inferred] The paper suggests that simply combining data from multiple domains in ITTL does not consistently improve performance due to domain divergence, but does not explore the optimal balance between single-domain and multi-domain approaches.
- Why unresolved: The study does not provide a clear strategy for when to use single-domain versus multi-domain fine-tuning, especially considering the impact of domain divergence.
- What evidence would resolve it: Comparative experiments varying the balance of single-domain and multi-domain fine-tuning, possibly with domain divergence metrics, to identify conditions where one approach is superior.

## Limitations
- Limited Language Coverage: The study focuses on three low-resource languages and two high-resource languages, which may not generalize to the full spectrum of low-resource languages.
- Dataset Size Constraints: The parallel data sizes used (ranging from 1k to 25k sentences) may not capture the full spectrum of low-resource scenarios.
- Single Model Architecture: The study exclusively uses mBART as the base multilingual sequence-to-sequence language model.

## Confidence
- High Confidence: The observation that multi-domain ITTL performs best for in-domain tasks is supported by consistent results across multiple language pairs and domain configurations in the experimental results.
- Medium Confidence: The finding that bitext denoising pre-training doesn't yield significant gains below 50k sentences is based on the specific experimental setup and dataset sizes tested, with reasonable evidence from the results tables.
- Medium Confidence: The observation that strategy effectiveness for out-domain tasks depends on domain relatedness and dataset sizes is supported by the JSD analysis and experimental results, though the relationship is complex and may not be fully captured.

## Next Checks
1. **Language Generalization Test**: Replicate the experiments with a broader set of low-resource languages (at least 5-7 additional languages) to validate whether the findings hold across different language families and typological characteristics.
2. **Dataset Size Boundary Test**: Conduct experiments with parallel data sizes both below and above the 50k threshold (e.g., 10k, 50k, 100k, 200k sentences) to more precisely determine the boundary conditions for when bitext denoising pre-training becomes effective.
3. **Architecture Transfer Test**: Repeat the core experiments using a different multilingual model architecture (e.g., mT5 or mBART-large) to determine if the findings are model-specific or generalizable across architectures.