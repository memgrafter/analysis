---
ver: rpa2
title: Prompting or Fine-tuning? Exploring Large Language Models for Causal Graph
  Validation
arxiv_id: '2406.16899'
source_url: https://arxiv.org/abs/2406.16899
tags:
- causal
- language
- llms
- prompt
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether Large Language Models (LLMs) can\
  \ validate causal graphs\u2014a task traditionally requiring human expertise. It\
  \ compares two approaches: (1) prompt-based LLMs using zero/few-shot prompting and\
  \ (2) fine-tuned LLMs trained on causal relation classification."
---

# Prompting or Fine-tuning? Exploring Large Language Models for Causal Graph Validation

## Quick Facts
- arXiv ID: 2406.16899
- Source URL: https://arxiv.org/abs/2406.16899
- Reference count: 36
- Key outcome: Fine-tuned LLMs outperform prompt-based models for causal graph validation, achieving up to 20.5-point higher F1 scores

## Executive Summary
This study investigates whether Large Language Models (LLMs) can validate causal graphs—a task traditionally requiring human expertise. It compares two approaches: (1) prompt-based LLMs using zero/few-shot prompting and (2) fine-tuned LLMs trained on causal relation classification. Experiments on biomedical and open-domain datasets reveal that fine-tuned models consistently outperform prompt-based ones, with up to 20.5-point higher F1 scores. Even smaller fine-tuned models (e.g., BERT) exceed larger prompt-based models. The findings suggest that fine-tuning is more effective for causal relation extraction, likely due to its ability to learn domain-specific patterns, whereas prompt-based models struggle with implicit causality expressions.

## Method Summary
The paper compares prompt-based and fine-tuned LLMs for causal relation classification using four datasets: three biomedical (GENEC, DDI, COMAGC) and one open-domain (SEMEVAL). For prompt-based approaches, three template variations are tested (two-choices with/without context, three-choices with context) in both zero-shot and few-shot settings. For fine-tuned approaches, BERT variants are used for biomedical data while GPT is used for classification and relation extraction formats. Models are evaluated using 5-fold cross-validation with F1 score as the primary metric.

## Key Results
- Fine-tuned models achieve up to 20.5-point higher F1 scores than prompt-based models
- Including context sentences in prompts improves performance for prompt-based LLMs
- Domain-specific fine-tuning (BioBERT, PubMedBERT) provides advantages over general LLMs for biomedical data
- Even smaller fine-tuned models (BERT) outperform larger prompt-based models

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuned LLMs achieve higher F1 scores because they are exposed to explicit causal patterns in training data, while prompt-based LLMs must infer causality implicitly from general knowledge. Supervised fine-tuning allows the model to learn domain-specific representations of causal relationships through labeled examples, whereas prompt-based models rely solely on pre-trained knowledge without task-specific adaptation. The core assumption is that causality in text is often implicit and ambiguous, making it difficult for models to extract without explicit training examples. This is supported by findings showing fine-tuned models consistently outperform prompt-based ones, achieving up to a 20.5-point improvement in F1 score—even when using smaller-parameter language models. If training data is insufficient or contains noisy labels, the fine-tuning advantage may diminish or reverse.

### Mechanism 2
Including contextual sentences in prompts improves performance for prompt-based LLMs because it provides additional evidence beyond pre-trained knowledge. Context sentences give the model specific information about the entity pair, allowing it to make more informed decisions rather than relying solely on general knowledge. The core assumption is that pre-trained knowledge alone is insufficient for accurate causal relation classification without task-specific context. This is evidenced by the finding that Single-Prompt types B and C (with-context) consistently outperform type A (no-context). If context sentences are irrelevant or misleading, performance may degrade rather than improve.

### Mechanism 3
Domain-specific fine-tuning (e.g., BioBERT, PubMedBERT) provides advantages over general LLMs because biomedical causality expressions differ from open-domain patterns. Models pre-trained and fine-tuned on domain-specific corpora learn specialized vocabulary and syntactic patterns unique to that domain, improving performance on domain-specific tasks. The core assumption is that causality is expressed differently across domains, requiring domain adaptation for optimal performance. This is supported by the use of BioBERT and PubMedBERT for biomedical datasets, though fine-tuned models require sufficient expert-annotated data for training, which can be a significant bottleneck. If domain-specific patterns overlap significantly with general patterns, the benefit of domain adaptation may be minimal.

## Foundational Learning

- **Concept**: Causal relation classification as binary classification task
  - Why needed here: The paper frames causal graph validation as determining whether causal relationships exist between entity pairs, which is fundamentally a classification problem
  - Quick check question: How would you represent "smoking causes lung cancer" as a binary classification problem?

- **Concept**: Fine-tuning vs prompt-based learning paradigms
  - Why needed here: The paper compares two distinct approaches to adapting pre-trained LLMs for causal relation classification
  - Quick check question: What are the key differences in data requirements between fine-tuning and prompt-based learning?

- **Concept**: Zero-shot vs few-shot learning
  - Why needed here: The paper experiments with both zero-shot (no examples) and few-shot (limited examples) prompt-based approaches
  - Quick check question: How does the number of examples in few-shot prompting affect model performance?

## Architecture Onboarding

- **Component map**: Entity pair → Context extraction → Model selection (prompt vs fine-tuned) → Prediction → Evaluation
- **Critical path**: Entity pair → Context extraction → Model selection (prompt vs fine-tuned) → Prediction → Evaluation
- **Design tradeoffs**: Fine-tuning requires labeled data but achieves higher performance; prompt-based methods require no training data but depend heavily on prompt quality; context inclusion improves prompt-based performance but increases token costs; domain-specific models improve biomedical performance but require domain expertise
- **Failure signatures**: Low recall with high precision (model is conservative, missing causal relations); low precision with high recall (model overpredicts causality); high variance across folds (model is sensitive to training data distribution); context-dependent performance (model relies too heavily on specific wording)
- **First 3 experiments**: 1) Compare single-prompt (no-context) vs single-prompt (with-context) on small dataset to verify context importance; 2) Test few-shot prompting with varying numbers of examples (n=1,5,10) to find optimal training sample size; 3) Compare general BERT vs domain-specific BioBERT on biomedical dataset to verify domain adaptation benefits

## Open Questions the Paper Calls Out

### Open Question 1
Does the size of the training data provided in few-shot prompts affect the performance of prompt-based LLMs for causal relation classification? The paper notes that "the limited size of the training samples may be a factor, and increasing the amount of training data could potentially improve results" but was unable to test this due to OpenAI API token limitations. This remains unresolved due to API constraints. Experiments with larger few-shot prompts (e.g., 50+ examples) that systematically vary the number of training samples would resolve this question.

### Open Question 2
Can fine-tuning smaller-parameter models like BERT achieve comparable performance to larger models like GPT for causal relation classification when sufficient training data is available? The paper shows that fine-tuned BERT models outperform prompt-based LLMs even when using smaller-parameter models, suggesting fine-tuning is more effective than model size alone. This remains unresolved as the study only tested specific model combinations. Systematic experiments training different sized models (BERT-base, BERT-large, GPT-2, GPT-3) on datasets of varying sizes would resolve this question.

### Open Question 3
How does the implicit nature of causal language affect the performance of prompt-based versus fine-tuned models? The authors hypothesize that "causality is rarely written explicitly with causal cues" and that fine-tuning exposes models to "various ways in which causal relationships can be expressed in text." This remains unresolved as the paper does not empirically test whether models perform differently on explicitly versus implicitly stated causal relationships. Analysis comparing model performance on sentences containing explicit causal markers versus those with implicit causality would resolve this question.

## Limitations
- The study focuses on causal relation classification rather than full causal graph validation, which involves additional complexities like handling cyclic dependencies and indirect causal chains
- Evaluation metrics don't capture whether models can identify indirect causal relationships or handle temporal ordering that often matters in causal reasoning
- The comparison assumes causal relationships are sufficiently explicit in training data for fine-tuning to work effectively, which may not hold for domains with highly implicit causal language

## Confidence

- **High confidence**: The core finding that fine-tuned models outperform prompt-based models on this specific task is well-supported by the experimental results across multiple datasets and model variants
- **Medium confidence**: The explanation that fine-tuning works better because it exposes models to explicit causal patterns is plausible but could benefit from additional ablation studies
- **Medium confidence**: The contextual prompt mechanism's effectiveness is demonstrated but the optimal amount and type of context remains unexplored

## Next Checks

1. **Ablation study on context importance**: Systematically vary the amount and relevance of context sentences in prompt-based approaches to determine the minimum effective context and whether context quality matters more than quantity

2. **Generalization to indirect causality**: Test whether fine-tuned models can identify indirect causal relationships (A → B → C) that aren't explicitly stated in training data, which would better validate their causal reasoning capabilities

3. **Domain adaptation limits**: Experiment with cross-domain fine-tuning where models trained on one domain (e.g., biomedical) are evaluated on another (e.g., social sciences) to quantify how domain-specific the learned patterns are and whether transfer learning is possible