---
ver: rpa2
title: On the Adversarial Transferability of Generalized "Skip Connections"
arxiv_id: '2410.08950'
source_url: https://arxiv.org/abs/2410.08950
tags:
- adversarial
- attacks
- transferability
- attack
- skip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that skip connections in deep neural networks
  facilitate the generation of highly transferable adversarial examples. The authors
  propose the Skip Gradient Method (SGM), which introduces a decay parameter to reduce
  gradients from residual modules during backpropagation, thereby utilizing more gradients
  from skip connections.
---

# On the Adversarial Transferability of Generalized "Skip Connections"

## Quick Facts
- arXiv ID: 2410.08950
- Source URL: https://arxiv.org/abs/2410.08950
- Authors: Yisen Wang; Yichuan Mo; Dongxian Wu; Mingjie Li; Xingjun Ma; Zhouchen Lin
- Reference count: 40
- Primary result: SGM significantly improves black-box adversarial transferability across various architectures

## Executive Summary
This paper introduces the Skip Gradient Method (SGM), a technique that enhances adversarial transferability by selectively reducing gradients from residual modules while preserving gradients from skip connections during backpropagation. The method is based on the observation that skip connections carry more transferable adversarial information than residual modules. Experiments on ImageNet demonstrate that SGM significantly improves black-box attack success rates across diverse architectures including ResNets, Vision Transformers, and models with varying-length paths. The approach generalizes beyond skip connections to any architecture with parallel processing paths of different lengths.

## Method Summary
SGM introduces a decay parameter γ that reduces gradients flowing through residual modules during backpropagation, shifting focus to gradients from skip connections. For ResNet-like architectures, this means output = x + f(x) where gradients from f(x) are scaled by γ while gradients from x remain unchanged. The method extends to Vision Transformers by applying γ to MLP layers and γ² to attention layers, and generalizes to any architecture with varying-length paths by decaying gradients based on the number of parametric layers in each path. The approach is evaluated using standard adversarial attacks like PGD with and without SGM, measuring black-box transferability success rates.

## Key Results
- SGM achieves 75.2% average transferability across 10 different target models, outperforming existing state-of-the-art methods
- The method generalizes successfully to Vision Transformers, achieving 57.5% average transferability to ResNet architectures
- SGM improves targeted attack transferability and maintains effectiveness against ensemble models and defensive techniques

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SGM exploits the architectural property that skip connections carry more transferable adversarial information than residual modules.
- **Mechanism**: By introducing a decay factor γ that reduces gradients flowing through residual modules during backpropagation, SGM shifts gradient focus from local (residual module) to global (skip connection) feature information. This enhances the alignment between attack direction and data distribution perturbation direction (AAI metric).
- **Core assumption**: Gradients from skip connections inherently encode more transferable features than those from residual modules in the same building block.
- **Evidence anchors**:
  - [abstract] "using more gradients from the skip connections rather than the residual modules according to a decay factor during backpropagation allows one to craft adversarial examples with high transferability"
  - [section] "if we remove more gradients that go through the residual modules while keeping the gradients through skip connections, the success rate of the black-box attack increases significantly"
  - [corpus] Weak evidence - no direct neighbor papers discuss architectural gradient decay methods
- **Break condition**: If residual modules in a specific architecture actually encode more transferable features than skip connections, or if the AAI alignment is not improved by gradient decay.

### Mechanism 2
- **Claim**: SGM generalizes to any architecture with length-varying paths, not just skip connections.
- **Mechanism**: For architectures like Inception or NAS models with multiple parallel processing paths of different lengths, SGM decays gradients by γ for each parametric layer in the path. Shorter paths (fewer parametric layers) thus contribute more to the final gradient, mimicking skip connection behavior.
- **Core assumption**: The length of a processing path (number of parametric layers) correlates with gradient transferability - shorter paths are more transferable.
- **Evidence anchors**:
  - [abstract] "extend SGM to more advanced architectures, including Vision Transformers (ViTs) and models with length-varying paths"
  - [section] "Since the paths of varying lengths are almost ubiquitous in modern deep models, SGM is a generic method that can be applied to various model architectures"
  - [corpus] Weak evidence - no direct neighbor papers discuss path-length-based gradient decay
- **Break condition**: If path length does not correlate with transferability in certain architectures, or if parametric vs non-parametric layer distinction doesn't hold.

### Mechanism 3
- **Claim**: SGM improves targeted attack transferability by preserving more class-relevant feature information.
- **Mechanism**: In targeted attacks, the adversary needs to induce misclassification to a specific class. SGM's gradient decay preserves more global feature information from skip connections, which helps maintain the adversarial effect needed for precise targeted misclassification rather than just generic misclassification.
- **Core assumption**: Targeted attacks require more global feature preservation than untargeted attacks to achieve specific class redirection.
- **Evidence anchors**:
  - [abstract] "further demonstrate that SGM can even improve the transferability on ensembles of models or targeted attacks"
  - [section] "the success rates of targeted black-box attacks using ResNet-50 as the source model against 10 different target models"
  - [corpus] Weak evidence - no direct neighbor papers discuss targeted attack improvements via architectural gradient decay
- **Break condition**: If targeted attacks actually benefit more from local residual module information rather than global skip connection information.

## Foundational Learning

- **Concept**: Residual network gradient decomposition
  - **Why needed here**: Understanding how gradients flow through skip connections versus residual modules is fundamental to implementing SGM correctly
  - **Quick check question**: In a ResNet block where output = x + f(x), what is the gradient of the loss with respect to x if we only consider the skip connection contribution?

- **Concept**: Vision Transformer architecture
  - **Why needed here**: SGM must be extended to ViTs which have different building blocks (attention + MLP + skip connections)
  - **Quick check question**: How many parallel paths exist in a standard ViT block, and which ones should be decayed by γ vs γ²?

- **Concept**: Neural Architecture Search (NAS) and varying path lengths
  - **Why needed here**: SGM's extension to NAS models requires understanding how to identify and decay gradients in architectures without explicit skip connections
  - **Quick check question**: In a NAS cell with multiple parallel operations, how do you determine which operations are "parametric" and should have their gradients decayed?

## Architecture Onboarding

- **Component map**: Input → Block 1 (skip + residual) → Block 2 (skip + residual) → ... → Block L (skip + residual) → Output
- **Critical path**: For ResNet-like: Input → Block 1 → Block 2 → ... → Block L → Output. SGM modifies gradient flow at each block junction.
- **Design tradeoffs**:
  - Higher γ (less decay): Preserves more local information but may reduce transferability
  - Lower γ (more decay): Increases transferability but may lose class-relevant information
  - Architecture-specific tuning required for optimal γ
- **Failure signatures**:
  - Transferability decreases when γ is too low (γ < 0.2 for ResNet-like)
  - No improvement when γ = 1.0 (no decay applied)
  - Architecture mismatch: applying ResNet-specific decay patterns to non-ResNet architectures
- **First 3 experiments**:
  1. Implement SGM on ResNet-18 with γ = 0.6 and compare black-box transferability against baseline PGD on VGG19
  2. Extend SGM to ViT-B and test transferability to ResNet-50 and other ViTs
  3. Apply SGM to Inception-V3 and test transferability to models with and without skip connections

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SGM's effectiveness vary with different architectures that have skip connections but different residual module complexities (e.g., varying depths or channel widths)?
- Basis in paper: [inferred] The paper tests SGM on various architectures (ResNet, ViT, Inception, NAS) but does not explore how residual module complexity affects SGM's performance within the same architectural family.
- Why unresolved: The experiments focus on architectural differences rather than internal architectural variations that could affect gradient propagation.
- What evidence would resolve it: Experiments comparing SGM performance across ResNet variants with different depths (e.g., ResNet-18 vs ResNet-152) or different channel configurations.

### Open Question 2
- Question: Can SGM be extended to non-differentiable or hybrid architectures where some modules are not amenable to gradient-based attacks?
- Basis in paper: [explicit] The paper states SGM works for "almost all networks as long as they have varying-length paths" but does not address non-differentiable components.
- Why unresolved: The paper's theoretical analysis and experiments assume fully differentiable networks.
- What evidence would resolve it: Experiments applying SGM to hybrid models combining differentiable and non-differentiable components, or architectures with hard attention mechanisms.

### Open Question 3
- Question: What is the relationship between SGM's decay parameter γ and the specific layer depths or receptive fields in the target architecture?
- Basis in paper: [explicit] The paper shows γ affects transferability but doesn't establish a clear mapping between γ values and architectural properties like receptive field size or layer depth.
- Why unresolved: The selection of γ appears empirical rather than theoretically grounded in architectural characteristics.
- What evidence would resolve it: A systematic study correlating optimal γ values with architectural metrics like receptive field size, number of layers, or feature map dimensions.

## Limitations
- The theoretical analysis linking gradient decay to feature alignment (AAI metric) remains largely qualitative rather than mathematically rigorous
- The optimal decay parameter γ appears to be architecture-dependent but lacks a principled method for selection
- Claims about targeted attack improvements and defense scenarios have fewer experiments and less detailed analysis

## Confidence
- **High confidence**: SGM improves transferability on standard ResNet architectures
- **Medium confidence**: SGM generalizes to ViTs and NAS models
- **Low confidence**: Claims about targeted attack improvements and defense scenarios

## Next Checks
1. **Architecture ablation**: Test SGM on architectures without clear skip connections (DenseNet, MobileNet) to verify the claimed generalization to "any architecture with varying-length paths"
2. **γ sensitivity analysis**: Systematically vary γ across multiple orders of magnitude (0.1, 0.3, 0.5, 0.7, 0.9) for each architecture to establish a principled tuning strategy
3. **Theoretical validation**: Implement gradient visualization to empirically verify the claim that SGM shifts focus from local to global features, comparing gradient norms and directions with/without SGM