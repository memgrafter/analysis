---
ver: rpa2
title: Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated Tokens
arxiv_id: '2410.14655'
source_url: https://arxiv.org/abs/2410.14655
tags:
- training
- generated
- learning
- methods
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two training methods to address the discrepancy
  between training and inference in large language models. During training, models
  typically use ground-truth tokens as input, but during inference, they rely on their
  own generated tokens.
---

# Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated Tokens

## Quick Facts
- arXiv ID: 2410.14655
- Source URL: https://arxiv.org/abs/2410.14655
- Reference count: 22
- Models trained with BASH and RAC outperform standard SFT on summarization, QA, and math QA tasks

## Executive Summary
This paper addresses the critical discrepancy between training and inference in large language models, where models are trained on ground-truth tokens but must generate text using their own previously generated tokens during inference. This mismatch can lead to compounding errors and unpredictable behavior. The authors propose two complementary methods: Batch-Scheduled Sampling (BASH) which creates an offline dataset mixing ground-truth tokens with model-generated tokens, and Reference-Answer-based Correction (RAC) which incorporates self-correction capabilities by conditioning on ground-truth answers. Both methods show consistent improvements across summarization, general question-answering, and math question-answering tasks when compared to standard supervised fine-tuning and other baselines.

## Method Summary
The paper introduces two training methods to bridge the training-inference gap in LLMs. BASH creates an offline dataset by stochastically interleaving ground-truth tokens with model-generated tokens during training, using a mixing factor β to control exposure to self-generated content. RAC explicitly incorporates self-correction by constructing target sequences from the model's own generated tokens, conditioned on ground-truth answers, enabling the model to learn correction capabilities. Both methods are combined with standard SFT training for stability, and the combined approach shows consistent improvements across multiple benchmark tasks while maintaining computational efficiency through offline dataset generation.

## Key Results
- On GSM8K math QA, BASH achieved 60.22% accuracy compared to 56.76% for standard SFT
- On general QA tasks, RAC achieved 10.37% length-controlled win rate compared to 8.03% for standard SFT
- Both methods improved downstream alignment performance when used as initialization for preference-based fine-tuning
- Methods show consistent improvements across different iterations, unlike some baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Batch-Scheduled Sampling (BASH) bridges training-inference gap by exposing models to their own generated tokens during training in an offline manner.
- Mechanism: Creates a modified dataset offline by stochastically interleaving ground-truth tokens with model-generated tokens during training, using a mixing factor β to control exposure.
- Core assumption: Exposure to model-generated tokens during training will improve model's ability to handle its own generations during inference without causing distribution collapse.
- Evidence anchors:
  - [abstract] "First, we train the model in a manner akin to how samples are generated during inference... we expose the model to its own generations, adopting the scheduled sampling... for LLMs but in an offline and batch manner"
  - [section] "To mitigate the limitations discussed in SCS for large models, we propose a simple yet effective offline approach... This is done in an offline manner, modifying the context window by interleaving ground-truth tokens with those generated by the model"
- Break condition: If β approaches 1, the sequence distribution mismatch becomes too large and training becomes ineffective

### Mechanism 2
- Claim: Reference-Answer-based Correction (RAC) enables self-correction by incorporating ground-truth answers into the context during token generation.
- Mechanism: Constructs target sequences from model's own generated tokens conditioned on ground-truth answers, allowing the model to learn self-correction capabilities.
- Core assumption: Conditioning on ground-truth answers while generating correction targets will enable the model to learn to correct its own errors without external supervision.
- Evidence anchors:
  - [abstract] "Our second approach is Reference-Answer-based Correction, where we explicitly incorporate a self-correction capability into the model during training"
  - [section] "RAC corrects the error by replacing the wrong token... This is achieved by forcing the model to fit ¯z that differ from the original generated response"
- Break condition: When generated token equals correction token, causing model collapse

### Mechanism 3
- Claim: Combining BASH and RAC with standard SFT training provides complementary benefits for bridging training-inference gap.
- Mechanism: Joint optimization where BASH handles exposure to model-generated tokens while RAC handles self-correction, both combined with SFT for stability.
- Core assumption: The different approaches address complementary aspects of the training-inference gap and their combination provides better results than either alone.
- Evidence anchors:
  - [section] "To balance optimizing the objective function using generated data and ground-truth data, we combine Eq. (1) and Eq. (5)"
  - [section] "Similar to BASH's algorithm, we combine Eq. (1) and Eq. (7) by first optimizing Eq. (1) alone for several iterations"
- Break condition: If training becomes too difficult due to conflicting objectives

## Foundational Learning

- Concept: Exposure Bias
  - Why needed here: Understanding why teacher-forcing during training creates problems at inference time is fundamental to grasping the motivation for these methods
  - Quick check question: What happens when a model trained with teacher-forcing encounters its own generated tokens during inference?

- Concept: Scheduled Sampling
  - Why needed here: BASH is an offline/batch variant of scheduled sampling, so understanding the original concept is essential
  - Quick check question: How does scheduled sampling differ from standard teacher-forcing during training?

- Concept: Self-correction Mechanisms
  - Why needed here: RAC's core innovation is incorporating self-correction into training, which requires understanding how models can learn to correct their own errors
  - Quick check question: What makes self-correction during training different from external correction mechanisms?

## Architecture Onboarding

- Component map: Original dataset -> BASH generation -> RAC generation -> Combined training
- Critical path: 1. Generate BASH dataset by interleaving ground-truth and model-generated tokens
  2. Generate RAC dataset by creating correction targets from model generations
  3. Train with combined SFT + BASH/RAC objectives
  4. Fine-tune with preference data if needed
- Design tradeoffs:
  - BASH: Computational efficiency vs. sequence distribution mismatch
  - RAC: Self-correction capability vs. potential for model collapse
  - Both: Need for careful β parameter tuning vs. simplicity of implementation
- Failure signatures: Training divergence or collapse, No improvement over baseline SFT, Excessive computational overhead, Generation quality degradation
- First 3 experiments:
  1. Implement BASH with different β values (0.1, 0.2, 0.3) and compare performance on a small summarization task
  2. Implement RAC and verify it can correct simple arithmetic errors in generated responses
  3. Combine BASH and RAC with SFT and compare against SFT baseline on general QA task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal mixing factor β for BASH that balances performance and training stability across different tasks and model sizes?
- Basis in paper: [explicit] The paper mentions that "it is important to keep the value of β small to avoid making the optimization problem harder to solve" and uses β = 0.2 in their experiments, but also notes that "depending on the value of β, there may be a distribution mismatch between the ground truth sequences y and the mixed ones g."
- Why unresolved: The paper only experiments with β = 0.2 without exploring how different values affect performance. The optimal β may vary depending on task complexity, model size, and training stage.
- What evidence would resolve it: A systematic study varying β across different values (e.g., 0.1, 0.2, 0.3, 0.4, 0.5) on multiple tasks and model sizes, measuring both training stability and downstream performance metrics.

### Open Question 2
- Question: How does the effectiveness of BASH and RAC change when applied to larger language models (e.g., 70B+ parameters) and more complex tasks beyond summarization and question answering?
- Basis in paper: [explicit] The paper states that "our method is scalable and can be applied to LLMs" and demonstrates effectiveness on 1B and 7B models, but acknowledges that "our methods show nearly monotonic improvements and consistent behavior across different iterations, unlike methods such as SPIN" which suggests potential scalability benefits.
- Why unresolved: The paper only tests on relatively small models (1B and 7B parameters) and three specific tasks. Larger models and more complex tasks may have different training dynamics and error propagation patterns.
- What evidence would resolve it: Experiments applying BASH and RAC to models with 70B+ parameters on tasks like code generation, long-form reasoning, or multi-step planning, measuring performance improvements and training efficiency.

### Open Question 3
- Question: What is the relationship between the self-correction capability of RAC and the quality of the reference answers used for conditioning? Does RAC performance degrade when reference answers are noisy or incomplete?
- Basis in paper: [explicit] The paper introduces RAC to "explicitly incorporate a self-correction capability into the model" by conditioning on reference answers, but doesn't investigate how answer quality affects performance. It mentions that RAC "may struggle to identify complex reasoning errors" in a failure case.
- Why unresolved: The paper doesn't explore scenarios where reference answers might be imperfect, which is realistic in many real-world applications where ground truth may be ambiguous or noisy.
- What evidence would resolve it: Experiments where reference answers are systematically corrupted (e.g., with random noise, partial information, or logical errors) to measure how RAC's performance degrades, and comparison with alternative correction mechanisms that don't rely on reference answers.

## Limitations
- BASH may struggle with sequence distribution mismatch as mixing factor β increases
- RAC's self-correction mechanism may not scale well to complex multi-step reasoning tasks
- Both methods require careful hyperparameter tuning with unclear optimal selection guidelines

## Confidence
**High Confidence Claims:**
- The fundamental problem of training-inference gap in LLMs is well-established
- Empirical improvements on GSM8K math QA task are statistically significant
- Basic mechanism of BASH (offline scheduled sampling) is theoretically sound

**Medium Confidence Claims:**
- Superiority of BASH and RAC over all baseline methods across all tasks
- Scalability of these methods to larger models and more complex tasks
- Long-term stability and generalization of the improvements

**Low Confidence Claims:**
- Exact optimal value of β parameter for BASH across different tasks
- Effectiveness of RAC for correcting complex multi-step reasoning errors
- Computational efficiency claims when scaling to industrial-sized datasets

## Next Checks
1. **Parameter Sensitivity Analysis**: Conduct a comprehensive ablation study varying the β parameter in BASH across multiple orders of magnitude (0.05, 0.1, 0.2, 0.3, 0.5) on the GSM8K dataset to determine the optimal range and identify potential stability issues at extreme values.

2. **Error Propagation Analysis**: Design a controlled experiment where known errors are injected into model generations at different positions, then measure how effectively RAC can correct these errors versus standard SFT. This would validate the self-correction mechanism's effectiveness for different error types.

3. **Scaling Experiment**: Implement BASH and RAC on a larger model (e.g., 7B parameters) and a more complex task (e.g., long-form reasoning or code generation) to test the methods' scalability and identify any emerging failure modes that weren't apparent in the smaller-scale experiments.