---
ver: rpa2
title: 'M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And
  A Retrieval-Aware Tuning Framework'
arxiv_id: '2411.06176'
source_url: https://arxiv.org/abs/2411.06176
tags:
- multimodal
- document
- question
- page
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces M-LongDoc, a benchmark of 851 samples for
  evaluating multimodal models on super-long documents (hundreds of pages) containing
  text, figures, and tables. The authors address the challenge of processing lengthy
  multimodal documents by proposing a retrieval-aware tuning framework that trains
  models to effectively use relevant content while ignoring distracting information
  from retrieved pages.
---

# M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework

## Quick Facts
- arXiv ID: 2411.06176
- Source URL: https://arxiv.org/abs/2411.06176
- Authors: Yew Ken Chia; Liying Cheng; Hou Pong Chan; Chaoqun Liu; Maojia Song; Sharifah Mahani Aljunied; Soujanya Poria; Lidong Bing
- Reference count: 32
- 851 QA pairs for evaluating multimodal models on super-long documents

## Executive Summary
This paper introduces M-LongDoc, a benchmark of 851 samples for evaluating multimodal models on super-long documents (hundreds of pages) containing text, figures, and tables. The authors address the challenge of processing lengthy multimodal documents by proposing a retrieval-aware tuning framework that trains models to effectively use relevant content while ignoring distracting information from retrieved pages. To support this, they construct a training corpus of 10,070 samples. Experiments show that their approach achieves a 4.6% relative improvement in answer correctness compared to baseline open-source models, demonstrating effectiveness in handling complex multimodal long documents. The benchmark includes an automated evaluation framework for open-ended solutions, revealing that current models struggle more with figure and table-based questions than text-based ones.

## Method Summary
The method involves constructing a benchmark with 851 QA pairs across academic, financial, and product domains using documents averaging 210.8 pages. The retrieval-aware tuning framework uses LoRA fine-tuning with rank 64, alpha 32, learning rate 1e-4 for 1 epoch on 10,070 training samples. The approach unifies supervised fine-tuning and retrieval-augmented generation by including both gold evidence pages and potentially irrelevant pages from other modalities during training. For evaluation, multiple judge models (GPT-4o, Claude 3.5, Gemini 1.5 Pro) score each answer from 1-5, with scores aggregated to provide a final correctness measure.

## Key Results
- 4.6% relative improvement in answer correctness compared to baseline open-source models
- Significant performance gap between text-based questions and figure/table-based questions
- 88.9% Pearson correlation between automated judge models and human preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-aware multimodal tuning improves model robustness against irrelevant retrieved content by exposing models to distractor pages during training
- Mechanism: The framework unifies supervised fine-tuning and retrieval-augmented generation by including both gold evidence pages and potentially irrelevant pages from other modalities during training. This forces the model to learn to identify and utilize only the most relevant content while ignoring distracting information
- Core assumption: Models can learn to distinguish relevant from irrelevant content when both are presented together during training, and this learned ability transfers to test-time retrieval scenarios
- Evidence anchors:
  - [abstract]: "Our approach achieves a relative improvement of 4.6% for the correctness of model responses, compared to the baseline open-source models."
  - [section 3]: "To enhance the robustness of multimodal models against potentially irrelevant retrieved content, we propose a retrieval-aware tuning approach for multimodal document reading. This framework unifies supervised fine-tuning and retrieval augmented generation by including distracting content from other modalities and pages in each document."

### Mechanism 2
- Claim: Multimodal models have inherent bias toward text content over visual content (figures and tables) even when trained on interleaved multimodal data
- Mechanism: The preliminary study shows significantly lower performance on figure-based and table-based questions compared to text-based questions across multiple models, indicating that models struggle more with image-based content or are biased toward textual information
- Core assumption: The performance gap between text and image-based questions reflects a fundamental limitation in how models process multimodal information rather than just differences in question difficulty or content complexity
- Evidence anchors:
  - [abstract]: "Experiments show that our tuning approach achieves a relative improvement of 4.6% for the correctness of model responses, compared to the baseline open-source models."
  - [section 2.4]: "Notably, as shown in Table 2, we observe significantly lower performance for figure-based and table-based questions, as compared to text-based questions. We believe that this discrepancy suggests that current models are weaker in processing image-based contents in multimodal documents, or may be biased towards the textual content, even when they are trained on interleaved multimodal data."

### Mechanism 3
- Claim: Automated evaluation using multiple judge models with detailed evaluation guides can achieve high agreement with human preferences for open-ended multimodal document understanding tasks
- Mechanism: The evaluation framework leverages multiple leading multimodal models to score each answer based on correctness criteria, with sampling and aggregation to reduce intra-model bias and provide fine-grained scores
- Core assumption: Multiple diverse judge models can collectively provide reliable and consistent evaluation of complex, open-ended answers without requiring reference answers or human annotation for each sample
- Evidence anchors:
  - [abstract]: "Experiments show that our tuning approach achieves a relative improvement of 4.6% for the correctness of model responses, compared to the baseline open-source models."
  - [section 2.3]: "To provide a more reliable evaluation and reduce intra-model bias (Verga et al., 2024), we leverage multiple judges to evaluate each candidate answer. Specifically, each judge model Mj is provided with the evaluation guide g, ground-truth evidence page as context c, question q, and candidate answer a, and instructed to assign a correctness score from 1 to 5."
  - [section 2.4]: "For the samples in this preliminary study, we observed a Pearson correlation of 88.9% with p < 0.001 between the final aggregated score from the judge models, and the human annotator. Thus, we believe that our evaluation framework can achieve a very high agreement with human preferences despite the open-ended and in-depth nature of the answers."

## Foundational Learning

- Concept: Multimodal document processing requires understanding interleaved text, figures, and tables
  - Why needed here: The benchmark specifically targets documents with hundreds of pages containing mixed text, figures, and tables, requiring models to process and reason across different content types
  - Quick check question: Can you explain the difference between processing text-only documents versus multimodal documents with interleaved figures and tables?

- Concept: Retrieval-augmented generation and its challenges with long documents
  - Why needed here: The work uses retrieval-based approaches to handle lengthy documents instead of processing entire documents, but identifies challenges with irrelevant content in retrieved pages
  - Quick check question: What are the main advantages and limitations of using retrieval-augmented generation for processing long multimodal documents?

- Concept: Supervised fine-tuning vs. retrieval-aware training paradigms
  - Why needed here: The proposed approach unifies traditional supervised fine-tuning with retrieval-aware training by including both relevant and irrelevant content during training
  - Quick check question: How does including distractor pages during training differ from standard supervised fine-tuning approaches?

## Architecture Onboarding

- Component map: Document preprocessing (text extraction + object detection for figures/tables) → Question generation pipeline (semi-automated with model verification) → Retrieval system (ColPali for page retrieval) → Multimodal models (GPT-4o, Claude 3.5, Gemini 1.5 Pro, Qwen2-VL, LLaVA-OneVision) → Evaluation framework (multiple judge models with aggregation)
- Critical path: Document preprocessing → Question generation with verification → Retrieval of top-k pages → Multimodal model inference → Automated evaluation with judge aggregation
- Design tradeoffs: Using extracted text and separate images for figures/tables vs. rendered page images (better content distinction but requires more complex processing), automated vs. human verification (scalability vs. quality), multiple judge models vs. single reference (reliability vs. complexity)
- Failure signatures: Performance degradation on figure/table questions suggests multimodal bias, OOM issues with increased retrieved content indicates distractor impact, variance in judge scores suggests evaluation instability
- First 3 experiments:
  1. Compare retrieval performance of different methods (BM25, BGE-M3, JINA-CLIP, ColPali) using MRR scores on gold evidence pages
  2. Test impact of removing image inputs vs. using rendered pages vs. extracted content on model performance
  3. Evaluate training with different ratios of distractor to relevant pages to find optimal training configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the retrieval-aware tuning framework perform on longer documents beyond the 200+ pages in M-LongDoc, and what are the scaling limits of this approach?
- Basis in paper: Inferred from the discussion of computational costs and limitations when processing very long documents, as well as the mention of potential "out-of-memory (OOM) issues" when increasing retrieved content
- Why unresolved: The paper only evaluates the approach on documents averaging 210.8 pages. The authors acknowledge computational challenges but don't explore how the method scales to documents with thousands of pages or different document structures
- What evidence would resolve it: Empirical results showing performance metrics (correctness scores, computational efficiency) on documents ranging from 500 to 5000 pages, including analysis of memory usage and processing time

### Open Question 2
- Question: How does the model's performance vary when different types of distractor content are included during retrieval-aware training, and what is the optimal mix of relevant vs. irrelevant content?
- Basis in paper: Inferred from the observation that models can be "easily distracted by irrelevant content" and the proposal to include "potentially irrelevant pages" during training, but without systematic analysis of distractor types
- Why unresolved: The paper mentions including distracting content but doesn't analyze how different types of distractors (text vs. figures vs. tables, from same vs. different domains) affect learning outcomes
- What evidence would resolve it: Controlled experiments varying distractor content types and ratios, with performance metrics showing how different combinations affect model robustness and correctness scores

### Open Question 3
- Question: How does the automated evaluation framework's reliability change when evaluating answers to more subjective or open-ended questions that don't have clear right/wrong answers?
- Basis in paper: Explicit - the authors acknowledge "some degree of subjectiveness" in their framework and only report 88.9% Pearson correlation with human scoring on a subset of 100 samples
- Why unresolved: The paper only evaluates the automated framework on a subset and doesn't explore edge cases where human evaluators might disagree significantly or where questions require nuanced judgment
- What evidence would resolve it: Comprehensive analysis including inter-annotator agreement studies, edge case identification, and comparison of automated vs. multiple human evaluations across different question types and difficulty levels

### Open Question 4
- Question: What is the relative contribution of the different components of the retrieval-aware tuning approach (fine-tuning vs. RAG augmentation vs. distractor inclusion) to the observed 4.6% improvement?
- Basis in paper: Inferred from the description of the unified framework combining supervised fine-tuning and retrieval-augmented generation, but without ablation studies
- Why unresolved: The paper reports overall improvement but doesn't break down which specific elements of the training approach drive the performance gains
- What evidence would resolve it: Ablation studies systematically removing each component (fine-tuning only, RAG only, no distractors) and comparing performance to isolate the contribution of each element

### Open Question 5
- Question: How does the performance gap between open-source and proprietary models change when both are trained using the retrieval-aware framework, and what architectural differences limit open-source model performance?
- Basis in paper: Explicit - the authors note that "open-source models have worse performance in answering table-related questions" and their framework "significantly and consistently enhances the performance of Qwen2-VL"
- Why unresolved: While the paper shows improvement for one open-source model, it doesn't explore whether the framework can close the gap with proprietary models or what specific architectural limitations prevent open-source models from matching performance
- What evidence would resolve it: Comparative studies training multiple open-source models with the framework and measuring performance relative to proprietary models, along with architectural analysis identifying bottlenecks in open-source approaches

## Limitations
- The evaluation framework relies on automated judge models, which may miss nuanced aspects of answer quality or develop systematic biases
- The corpus size for training (10,070 samples) and evaluation (851 samples) may not capture the full diversity of super-long document scenarios
- The reported 4.6% relative improvement comes from a limited set of baseline models and may not generalize to other architectures

## Confidence

**High Confidence**: The identification of multimodal bias toward text over visual content (figures/tables) - supported by clear performance discrepancies across multiple models in the preliminary study

**Medium Confidence**: The effectiveness of retrieval-aware tuning framework - supported by empirical results but limited by evaluation scope and potential judge model biases

**Medium Confidence**: The automated evaluation framework's reliability - backed by statistical correlation with human judgments but dependent on the quality and diversity of judge models

## Next Checks

1. **Cross-domain validation**: Test the retrieval-aware tuning approach on documents from additional domains not represented in the current benchmark (e.g., medical, legal, technical documentation) to assess generalizability

2. **Judge model diversity analysis**: Systematically evaluate how different judge model combinations affect the final aggregated scores and identify potential bias patterns or convergence issues

3. **Scalability assessment**: Measure model performance and computational requirements as document length increases beyond 200 pages to determine practical limits of the approach