---
ver: rpa2
title: 'Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data
  Sources'
arxiv_id: '2409.08239'
source_url: https://arxiv.org/abs/2409.08239
tags:
- data
- synthetic
- table
- question
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Source2Synth is a synthetic data generation and curation method
  that produces high-quality datasets for complex tasks like multi-hop and tabular
  question answering. The method generates synthetic examples grounded in real-world
  sources (documents or tables), uses a seed attribute to condition the generation,
  and applies a two-stage curation process: fine-tuning an intermediate model on one
  half of the data and filtering the other half based on answerability.'
---

# Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources

## Quick Facts
- **arXiv ID**: 2409.08239
- **Source URL**: https://arxiv.org/abs/2409.08239
- **Reference count**: 40
- **Key outcome**: 22.57% improvement on HotpotQA and 25.51% improvement on WikiSQL using synthetic data generation grounded in real-world sources

## Executive Summary
Source2Synth is a synthetic data generation and curation method that produces high-quality datasets for complex tasks like multi-hop and tabular question answering. The method generates synthetic examples grounded in real-world sources (documents or tables), uses a seed attribute to condition the generation, and applies a two-stage curation process: fine-tuning an intermediate model on one half of the data and filtering the other half based on answerability. Applied to multi-hop question answering, it achieves a 22.57% improvement in accuracy on HotpotQA over fine-tuned baselines; for tabular question answering, it yields a 25.51% improvement on WikiSQL. The approach enables effective fine-tuning without human annotations and scales performance by increasing synthetic data volume.

## Method Summary
Source2Synth generates synthetic data for multi-hop and tabular question answering by grounding the generation process in real-world sources like Wikipedia articles and WikiSQL tables. The method selects task-specific seeds from source entries, generates synthetic examples with intermediate reasoning steps, and applies a two-stage curation process. First, the dataset is split in half, with one portion used to fine-tune an intermediate LLM (LLMSynth). Then, LLMSynth filters the remaining portion by checking answerability across multiple attempts. The curated dataset is used to fine-tune the final LLM, enabling effective fine-tuning without human annotations.

## Key Results
- Achieves 22.57% improvement in soft exact match accuracy on HotpotQA over fine-tuned baselines
- Yields 25.51% improvement in exact match accuracy on WikiSQL over fine-tuned baselines
- Demonstrates effective scaling with synthetic data volume, improving performance as more synthetic data is added to the fine-tuning mix

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grounding synthetic data generation in real-world sources improves data quality and diversity.
- Mechanism: Using real-world sources (Wikipedia articles for MHQA, WikiSQL tables for TQA) as the basis for synthetic example generation ensures that the generated data is anchored in factual information and exhibits natural diversity patterns.
- Core assumption: Real-world sources contain sufficient complexity and variety to generate diverse, realistic synthetic examples when properly conditioned.
- Evidence anchors:
  - [abstract] "Source2Synth takes as input a custom data source and produces synthetic data examples with intermediate reasoning steps"
  - [section] "By basing the data generation process on real-world sources, Source2Synth steers the examples to be more realistic, diverse, and factually correct"
  - [corpus] Weak evidence - corpus shows related work on synthetic data generation but limited direct comparison to real-source grounding
- Break condition: If real-world sources are too homogeneous or contain systematic biases, the generated synthetic data may inherit these limitations.

### Mechanism 2
- Claim: Two-stage curation (fine-tuning on half the data, filtering the other half) significantly improves dataset quality.
- Mechanism: The first half of synthetic data is used to fine-tune an intermediate LLM (LLMSynth), which is then used to filter the second half by checking answerability across multiple attempts (k=3).
- Core assumption: A model trained on curated synthetic data can effectively distinguish between high and low quality examples in a new dataset.
- Evidence anchors:
  - [abstract] "Our method improves the dataset quality by discarding low-quality generations based on their answerability"
  - [section] "The filtering step consists of using the fine-tuned model LLMSynth to predict the output of the question in the synthetic example for k tries. If the output cannot be predicted, it is assumed that the example is low quality"
  - [corpus] Weak evidence - corpus contains related filtering approaches but not this specific two-stage method
- Break condition: If the intermediate model overfits to the first half or fails to generalize, the filtering may become ineffective.

### Mechanism 3
- Claim: Conditional generation using task-specific seeds creates more realistic and varied synthetic examples.
- Mechanism: For each source entry, a specific seed (entity for MHQA, factual statement for TQA) is selected to condition the generation process, ensuring consistency and relevance throughout the synthetic example creation.
- Core assumption: Seeds can be effectively identified and used to guide the generation of diverse yet realistic synthetic examples.
- Evidence anchors:
  - [abstract] "Source2Synth takes as input a custom data source and produces synthetic data examples with intermediate reasoning steps"
  - [section] "To create a synthetic example, we first generate a task-specific seed or attribute chosen at random from each entry in the source data. The seed anchors the creation of the entry"
  - [corpus] Moderate evidence - corpus shows related work on conditional generation but limited direct comparison to seed-based conditioning
- Break condition: If seeds fail to capture the diversity needed for realistic examples or if the generation process becomes too constrained by the seeds.

## Foundational Learning

- Concept: Chain-of-thought reasoning decomposition
  - Why needed here: Both MHQA and TQA tasks require breaking complex problems into simpler steps (Q1→Q2→Q for MHQA, SQL generation→execution→answer for TQA)
  - Quick check question: How would you decompose the question "What's the tallest building in the city where the 2024 Olympics were held?" into a chain of thought?

- Concept: SQL execution and result parsing
  - Why needed here: TQA task requires generating valid SQL queries from natural language and executing them to obtain answers
  - Quick check question: Given a table with columns [Country, Year, Arrivals], what SQL query would find the country with most arrivals in 2012?

- Concept: Rejection sampling for quality control
  - Why needed here: Curation process uses k-try rejection sampling to filter out low-quality examples where the model cannot produce correct answers
  - Quick check question: If a model gets 2 out of 3 attempts correct on a synthetic example, should it be kept or rejected according to the paper's method?

## Architecture Onboarding

- Component map: Data Source → Seed Selection → Example Generation → Slice Split (50/50) → Fine-tune LLMSynth (slice 0) → Filter/Impute slice 1 → Curated Dataset → Fine-tune LLMCurated
- Critical path: The filtering step is critical - if LLMSynth fails to identify low-quality examples, the final curated dataset will contain noise that degrades downstream performance.
- Design tradeoffs: Using real sources vs. synthetic-only generation provides better grounding but requires access to appropriate source data; the 50/50 split balances having enough data for fine-tuning vs. having enough for curation.
- Failure signatures: If accuracy plateaus or degrades when adding more synthetic data, this may indicate filtering is not effectively removing low-quality examples or the generation process is producing repetitive patterns.
- First 3 experiments:
  1. Generate synthetic examples using a small subset of sources (e.g., 10 Wikipedia articles) and evaluate the diversity and quality of generated questions
  2. Test the filtering mechanism by intentionally including low-quality examples and measuring LLMSynth's ability to reject them
  3. Compare performance of LLMSynth trained on slice 0 vs. the same model without any filtering to quantify the curation benefit

## Open Questions the Paper Calls Out

- **Question**: How does the performance of Source2Synth scale with the number of hops in multi-hop question answering beyond two?
  - **Basis in paper**: Inferred from the limitation section stating "The MHQA number of hops is restricted to two in this paper. However, Source2Synth can be extended to a number of hops greater than two."
  - **Why unresolved**: The paper only evaluates Source2Synth on two-hop questions and does not provide empirical evidence for its effectiveness on longer reasoning chains. The extension methodology is suggested but not validated.
  - **What evidence would resolve it**: Empirical results showing accuracy and quality metrics for three-hop, four-hop, and n-hop questions, comparing against baselines and measuring degradation as hop count increases.

- **Question**: What is the optimal balance between real-world source data and synthetic data in the fine-tuning mixture for maximum performance?
  - **Basis in paper**: Explicit - "In Figure 5, we study how performance changes when adding more synthetic data in the fine-tuning data mix, which includes 500 samples from the HPQA train split."
  - **Why unresolved**: While the paper explores scaling with synthetic data quantity, it doesn't systematically investigate the optimal ratio of real to synthetic data, nor does it explore whether different ratios work better for different task types or model sizes.
  - **What evidence would resolve it**: A comprehensive ablation study varying the percentage of real vs. synthetic data across multiple tasks, measuring performance trade-offs and identifying sweet spots for different scenarios.

- **Question**: How does the quality of the intermediate LLM (LLMSynth) affect the final curated dataset and downstream performance?
  - **Basis in paper**: Inferred from the method description where LLMSynth is used for both filtering and imputation, but its impact is not systematically studied.
  - **Why unresolved**: The paper uses the base LLM for LLMSynth without exploring whether a stronger or weaker base model leads to better curation quality, or whether the curation process is robust to LLMSynth quality variations.
  - **What evidence would resolve it**: Experiments varying the quality and size of the LLM used for LLMSynth, measuring resulting dataset quality and final task performance to determine sensitivity to this choice.

- **Question**: Can Source2Synth be extended to handle multi-table tool use scenarios involving SQL JOIN operations?
  - **Basis in paper**: Explicit - "Multi-table tool use is not supported. Source2Synth cannot handle queries that require to aggregate information contained in multiple tables e.g. SQL join statements."
  - **Why unresolved**: The paper explicitly states this limitation but doesn't explore potential solutions or extensions to handle more complex tabular reasoning that spans multiple tables.
  - **What evidence would resolve it**: A modified version of Source2Synth that can generate and evaluate multi-table SQL queries, with experimental results showing performance on JOIN-heavy benchmarks compared to single-table baselines.

- **Question**: How does the choice of seed generation method affect the diversity and quality of synthetic examples?
  - **Basis in paper**: Inferred from the seed generation descriptions for both tasks, which use different approaches (entity sampling for MHQA, fact generation for TQA) without comparing alternatives.
  - **Why unresolved**: The paper uses specific seed generation strategies but doesn't explore whether different seed selection methods (e.g., random vs. topical coherence, different prompting strategies) impact the resulting dataset quality or model generalization.
  - **What evidence would resolve it**: Comparative experiments using different seed generation methods for the same tasks, measuring diversity metrics, answerability rates, and downstream performance to identify optimal seed strategies.

## Limitations
- The method cannot handle multi-table tool use scenarios involving SQL JOIN operations, limiting its applicability to more complex tabular reasoning tasks.
- The paper does not provide sufficient detail on how LLM prompts are constructed or how hyperparameters (like the k value for filtering attempts) were selected.
- There is no analysis of how source data characteristics might impact the synthetic data generation process or downstream performance.

## Confidence
- **High Confidence**: The 22.57% improvement on HotpotQA and 25.51% improvement on WikiSQL demonstrate measurable performance gains from the Source2Synth approach. The two-stage curation mechanism (fine-tune + filter) is clearly described and its role in dataset quality improvement is well-supported.
- **Medium Confidence**: The claim that grounding synthetic data in real-world sources improves diversity and realism is supported by the methodology but lacks direct empirical validation. The effectiveness of seed-based conditional generation is demonstrated through results but not through ablation studies or qualitative analysis.
- **Low Confidence**: The paper does not provide sufficient detail on how the LLM prompts are constructed or how hyperparameters (like the k value for filtering attempts) were selected, making it difficult to assess the robustness of the approach to implementation details.

## Next Checks
1. **Ablation Study**: Compare Source2Synth performance with and without real-source grounding to quantify the specific contribution of this mechanism to the observed improvements.
2. **Quality Analysis**: Conduct a detailed analysis of synthetic examples that were filtered out to understand what types of errors the curation process catches and whether important patterns might be inadvertently removed.
3. **Bias Assessment**: Analyze the synthetic datasets for potential biases introduced during generation and evaluate whether these biases are reflected in the fine-tuned model's behavior on downstream tasks.