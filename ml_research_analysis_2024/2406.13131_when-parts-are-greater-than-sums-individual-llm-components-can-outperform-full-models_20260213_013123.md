---
ver: rpa2
title: 'When Parts Are Greater Than Sums: Individual LLM Components Can Outperform
  Full Models'
arxiv_id: '2406.13131'
source_url: https://arxiv.org/abs/2406.13131
tags:
- components
- accuracy
- component
- language
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies in-context learning (ICL) by decomposing LLM
  outputs into individual contributions from attention heads and MLPs. The authors
  identify three types of components: good-performing ones that outperform the full
  model, bad-performing ones that perform below chance, and label-biased components
  that always predict the same label.'
---

# When Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models

## Quick Facts
- arXiv ID: 2406.13131
- Source URL: https://arxiv.org/abs/2406.13131
- Reference count: 40
- This paper identifies good-performing LLM components that outperform full models in in-context learning and proposes component reweighting to improve accuracy by 6.0% on 8 tasks

## Executive Summary
This paper introduces a novel decomposition of transformer architectures that enables exact isolation of individual component contributions to model outputs. By analyzing attention heads and MLPs separately, the authors discover that some components consistently outperform the full model while others perform below chance. Based on these findings, they propose component reweighting, a method that learns to emphasize good-performing components using few labeled examples, achieving significant accuracy improvements over standard in-context learning with minimal computational overhead.

## Method Summary
The method decomposes transformer models into individual attention head and MLP contributions through residual connections and early decoding. For each component, the authors extract its direct contribution to the output logits by projecting its activations through the output embedding matrix. They then train a linear reweighting layer that scales these component contributions based on few labeled examples from the target task. The approach caches component activations during few-shot demonstration processing and applies the learned weights at inference time, requiring only O(|Y| × N) parameters where N is the number of components.

## Key Results
- Good-performing components (those attending to label tokens) outperform full models on average
- Component accuracies show high correlation (0.54 IoU) across different demonstration sets and prompt templates
- Component reweighting improves average accuracy by 6.0% over 24-shot ICL across 8 tasks on Llama-2-7B
- The method adds minimal computational overhead with only O(|Y| × N) additional parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Good-performing components exist because the model's residual stream aggregates individual contributions that can be evaluated in isolation
- **Mechanism**: The transformer architecture naturally decomposes into individual component contributions through residual connections. Each attention head and MLP adds a direct contribution to the residual stream, which can be isolated and evaluated independently via early decoding (projecting through the output embedding matrix)
- **Core assumption**: The decomposition is exact - the sum of individual component contributions equals the full model output
- **Evidence anchors**:
  - [abstract] "We introduce a new view of in-context learning by decomposing the Transformer architecture (Vaswani et al., 2017). Our decomposition is exact—a mathematically equivalent formula for the model's outputs"
  - [section 2.1] "We view this information as the direct contributions of components to the output logits, and derive a formula for logits, P j gj, where gj is the direct contribution of the component indexed by j"
  - [corpus] Weak evidence - related papers discuss component pruning and analysis but don't establish this exact decomposition mechanism
- **Break condition**: If residual connections are modified or if early decoding through the output embedding matrix doesn't capture the full contribution of components

### Mechanism 2
- **Claim**: Component accuracies are transferable across prompts because certain components consistently perform well regardless of demonstration variations
- **Mechanism**: Some components have task-specific knowledge that generalizes across different prompt formulations. The top-performing components identified on one prompt template tend to remain effective on minimally contrastive templates, suggesting they capture task-relevant features rather than prompt-specific artifacts
- **Core assumption**: Components encode task-relevant information that is independent of specific prompt variations
- **Evidence anchors**:
  - [abstract] "We find that component accuracies are well-correlated across different demonstration sets and perturbations of prompt templates"
  - [section 4.1] "The accuracies of the internal components are highly consistent across different choices of demonstrations, having strong correlations and an average of 0.54 IoU"
  - [corpus] Moderate evidence - the brain-like synergistic core paper suggests similar ideas about consistent component behavior across contexts
- **Break condition**: If components are primarily capturing prompt-specific patterns rather than task-relevant features, or if prompt variations are too large

### Mechanism 3
- **Claim**: Component reweighting works because it learns to emphasize good-performing components while suppressing bad ones using few labeled examples
- **Mechanism**: By training a linear layer to weight component activations based on few labeled examples, the method learns to amplify contributions from components that perform well on the task while reducing the impact of components that perform poorly or are label-biased
- **Core assumption**: The few labeled examples provide sufficient signal to distinguish good from bad components
- **Evidence anchors**:
  - [abstract] "we propose component reweighting, which learns to linearly re-scale the component activations from a few labeled examples"
  - [section 5.1] "Compared to prior work that selects prompts from a large pool of labeled data to improve ICL accuracy, component reweighting softly selects components by learning weights from few-shot examples"
  - [corpus] Weak evidence - no direct corpus support for this specific reweighting mechanism
- **Break condition**: If the few labeled examples don't provide enough signal to identify good components, or if the linear reweighting model is too simple to capture complex interactions

## Foundational Learning

- **Concept**: Residual connections in transformers
  - Why needed here: Understanding how individual components contribute to the final output through residual connections is fundamental to the decomposition approach
  - Quick check question: If a transformer has 12 layers with both attention heads and MLPs in each layer, how many total components contribute to the residual stream?

- **Concept**: Early decoding and output embedding projection
  - Why needed here: The method relies on projecting component activations through the output embedding matrix to evaluate their individual contributions
  - Quick check question: What does the operation "U · Cj" represent in the context of evaluating individual component contributions?

- **Concept**: Correlation and IoU metrics for component agreement
  - Why needed here: The paper uses these metrics to measure how consistently components perform across different prompts and templates
  - Quick check question: If two templates have Pearson correlation r = 0.81 for component accuracies, what does this tell us about component behavior across templates?

## Architecture Onboarding

- **Component map**: Input embedding layer → Multiple transformer blocks → Output embedding layer
  - Each transformer block contains: LayerNorm → Multi-head attention → Residual connection → LayerNorm → MLP → Residual connection
  - Total components: All attention heads (one per head per layer) plus all MLPs (one per layer)

- **Critical path**: 
  1. Forward pass with K-shot ICL to extract component activations
  2. Cache component contributions (U · Cj) for training
  3. Train linear reweighting layer on few labeled examples
  4. At inference, combine component contributions with learned weights

- **Design tradeoffs**:
  - Memory vs. speed: Caching all component contributions requires O(|Y| × N × |Dtrain|) space but enables training without the LLM
  - Simplicity vs. expressiveness: Linear reweighting is simple but may miss complex interactions between components
  - Few-shot vs. many-shot: The method works with few examples but performance may improve with more data

- **Failure signatures**:
  - Low correlation between component accuracies across prompts suggests components are capturing prompt-specific rather than task-relevant features
  - Poor performance of reweighted model compared to full model indicates bad components aren't being properly suppressed or good components aren't being properly amplified
  - High variance in component performance across runs suggests instability in the reweighting process

- **First 3 experiments**:
  1. Verify the decomposition by checking if sum of individual component outputs equals full model output
  2. Test component transferability by comparing top components across different prompt templates
  3. Validate reweighting effectiveness by comparing against baselines on a simple binary classification task

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations section and the novel nature of the approach, several questions arise from the analysis and results presented.

## Limitations
- The method is limited to classification tasks and has not been tested on generation or other task types
- The approach requires caching component activations, which adds memory overhead proportional to the number of demonstrations
- The effectiveness of linear reweighting on tasks with more complex output spaces or nuanced relationships between components remains uncertain

## Confidence
- **High confidence** in the existence of good-performing and bad-performing components, supported by strong correlation metrics (0.54 IoU) across different demonstrations
- **Medium confidence** in the transferability of component accuracies across prompt templates, as correlation coefficients (r=0.81) suggest consistency but don't guarantee practical significance
- **Medium confidence** in the effectiveness of component reweighting, with reported 6.0% accuracy improvement but limited ablation studies on the linear reweighting design choice

## Next Checks
1. Verify component decomposition accuracy by testing whether the sum of individual component outputs exactly matches the full model output across multiple random inputs and temperature settings
2. Conduct ablation studies on the linear reweighting layer by comparing against alternative weighting schemes (non-linear, attention-based) to determine if the improvement is specific to the linear approach
3. Test component reweighting robustness by evaluating performance degradation when using random demonstrations versus task-relevant ones, to confirm the method captures genuine task knowledge rather than prompt artifacts