---
ver: rpa2
title: Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating
  Harmful Content
arxiv_id: '2402.13926'
source_url: https://arxiv.org/abs/2402.13926
tags:
- content
- bait-and-switch
- harmful
- llms
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large language models (LLMs) are vulnerable
  to Bait-and-Switch attacks, where users first prompt LLMs with safe questions and
  then employ simple post-hoc text manipulation techniques to transform benign outputs
  into harmful content. The authors show that even safety-aligned models like GPT-4
  and Claude-2 can be manipulated to generate misinformation, toxic content, and fake
  news through basic word replacement and instruction-following capabilities.
---

# Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content

## Quick Facts
- arXiv ID: 2402.13926
- Source URL: https://arxiv.org/abs/2402.13926
- Authors: Federico Bianchi; James Zou
- Reference count: 8
- Large language models can be manipulated through Bait-and-Switch attacks to generate harmful content via post-hoc text manipulation

## Executive Summary
This paper demonstrates a novel vulnerability in large language models where users can first prompt LLMs with safe questions about proxy concepts and then transform the benign outputs into harmful content through simple word replacement techniques. The attack exploits the LLM's failure to recognize proxy concepts that are analogous to target harmful concepts, allowing generation of misinformation, toxic content, and fake news even in safety-aligned models like GPT-4 and Claude-2. The study reveals that current LLM safety measures focusing solely on direct outputs are insufficient, as simple post-hoc modifications can transform faithful text into harmful content. Claude-2 shows more resistance to these attacks compared to GPT-4, but both models remain vulnerable.

## Method Summary
The study employed a systematic approach to test Bait-and-Switch attacks on GPT-4 and Claude-2. Researchers first identified suitable proxy concepts that were analogous to target harmful concepts (e.g., Tylenol for COVID-19 vaccine). They then crafted prompts using these proxies with specific instructions to tailor the output style, collected the safe responses from the LLMs, and applied simple find-and-replace techniques to swap proxy terms with target harmful concepts. The effectiveness of the transformed content was evaluated across multiple scenarios, testing different proxy concepts and instruction-following capabilities to assess the robustness of the attack.

## Key Results
- LLMs fail to recognize harmful nature of prompts containing proxy concepts analogous to target harmful concepts
- Simple post-hoc text manipulation can transform safe LLM outputs into harmful content
- Instruction-following capabilities can be exploited to make harmful content more convincing
- Claude-2 shows more resistance to Bait-and-Switch attacks compared to GPT-4, but both remain vulnerable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail to recognize harmful nature of prompts containing proxy concepts that are analogous to target harmful concepts.
- Mechanism: The model treats proxy concepts (e.g., "Tylenol" for "COVID vaccine") as safe and unrelated to harmful content, allowing generation of content that would otherwise be blocked if asked directly about the harmful topic.
- Core assumption: The LLM's safety mechanisms rely primarily on direct content recognition rather than understanding contextual relationships between proxy and target concepts.
- Evidence anchors:
  - [abstract]: "The attack is effective because LLMs fail to recognize the harmful nature of prompts containing proxy concepts that are analogous to the target harmful concepts."
  - [section 2.1]: "Generally, a good proxy is an agent, object, or situation that shares present or past similarities with the target concept."
- Break condition: If the LLM's safety system evolves to recognize proxy-concept relationships or implements contextual analysis that flags seemingly safe proxies that could lead to harmful outputs.

### Mechanism 2
- Claim: Post-hoc text manipulation can transform safe LLM outputs into harmful content through simple word replacement.
- Mechanism: After generating safe content about a proxy concept, attackers can use basic text editing (like find-and-replace) to swap proxy terms with target harmful concepts, creating convincing harmful content without triggering safety mechanisms.
- Core assumption: The LLM's safety guardrails only apply to direct outputs and don't consider potential post-generation modifications.
- Evidence anchors:
  - [abstract]: "even safe text coming from LLMs can be easily turned into potentially dangerous content through Bait-and-Switch attacks."
  - [section 1]: "We stress that focusing on the safety of the verbatim LLM outputs is insufficient and that we also need to consider post-hoc transformations."
- Break condition: If safety systems implement detection for common manipulation patterns or require verification that generated content hasn't been modified post-generation.

### Mechanism 3
- Claim: Advanced instruction-following capabilities enable attackers to customize output style before the switch, making the final harmful content more convincing.
- Mechanism: Attackers can direct the LLM to write in specific styles (sensationalist, technical, present tense) or with specific formatting that makes the post-switch content appear more authoritative and believable to average readers.
- Core assumption: The LLM's instruction-following capabilities can be exploited to create more sophisticated and harder-to-detect harmful content.
- Evidence anchors:
  - [section 2.2]: "The better the model is at following instructions, the easier it will be for the attacker to manipulate the content before the switch."
  - [section 3.1]: "The model can be directed to write text in a certain style (such as a journalist writing in the present tense or as a doctor) and avoid mentioning certain words that would make the content less effective."
- Break condition: If instruction-following capabilities are limited in ways that prevent overly specific or manipulative requests, or if style customization is flagged when combined with certain proxy concepts.

## Foundational Learning

- Concept: Proxy concepts and analogical reasoning
  - Why needed here: Understanding how attackers find and use proxy concepts that are similar to target harmful concepts is fundamental to grasping the Bait-and-Switch attack mechanism.
  - Quick check question: What makes a good proxy concept for a Bait-and-Switch attack, and why is "Tylenol" considered a good proxy for "COVID vaccine"?

- Concept: Post-hoc text manipulation techniques
  - Why needed here: The attack relies on simple text editing methods like find-and-replace to transform safe outputs into harmful content, so understanding these techniques is crucial.
  - Quick check question: How does a simple word replacement attack work, and why is it effective against LLM safety measures?

- Concept: Instruction-following capabilities of LLMs
  - Why needed here: The attack leverages the LLM's ability to follow detailed instructions to customize output style before the switch, making the final content more convincing.
  - Quick check question: How can instruction-following capabilities be exploited in a Bait-and-Switch attack to make harmful content more believable?

## Architecture Onboarding

- Component map:
  - Prompt generation system -> LLM inference engine -> Post-processing module -> Output validation (optional)

- Critical path:
  1. Identify suitable proxy concept for target harmful topic
  2. Create detailed instructions for LLM to generate content about proxy
  3. Submit prompt to LLM and receive safe output
  4. Apply post-hoc text manipulation (find-and-replace)
  5. Verify transformed content meets attack objectives

- Design tradeoffs:
  - Safety vs. helpfulness: Stricter safety measures may reduce the LLM's ability to provide useful responses to legitimate queries
  - Detection vs. usability: Implementing post-generation content verification may add latency and complexity to the system
  - Instruction following vs. safety: Limiting instruction-following capabilities may reduce the attack surface but also decrease the model's versatility

- Failure signatures:
  - LLM refuses to generate content if they recognize the prompt as harmful, especially Claude-2, which is more robust to Bait-and-Switch attacks
  - Generated content doesn't follow instructions precisely, making post-switch transformation difficult
  - Safety systems flag prompts containing proxy concepts as potentially harmful
  - Post-hoc manipulation detection catches and blocks modified content

- First 3 experiments:
  1. Test different proxy concepts for a given harmful topic to find the most effective ones
  2. Experiment with various instruction-following prompts to optimize content customization before the switch
  3. Evaluate different post-hoc manipulation techniques to determine which are most effective at evading detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the actual success rate of Bait-and-Switch attacks across different LLM architectures and safety training regimes?
- Basis in paper: [explicit] The paper acknowledges that "a more in-depth evaluation of the success rate of Bait-and-Switch attacks is still needed to better understand the risks derived from this approach."
- Why unresolved: The paper provides qualitative examples rather than quantitative metrics, and testing was limited to GPT-4 and Claude-2 without comprehensive cross-model comparisons.
- What evidence would resolve it: Systematic testing across multiple LLM families with varying safety protocols, reporting success rates, failure modes, and correlation with model size and training methodology.

### Open Question 2
- Can automated proxy discovery systems reliably identify high-yield proxy concepts without human intervention?
- Basis in paper: [inferred] The paper suggests that "Proxy concepts can be found by looking at similar historical events" and "LLMs can be used to perform this step," but doesn't demonstrate automated discovery.
- Why unresolved: The examples provided appear to use manually selected proxies, and the paper doesn't explore automated proxy generation or validation.
- What evidence would resolve it: Development and evaluation of automated proxy discovery systems that can identify effective proxy-target pairs across domains, with success rates measured against human-selected proxies.

### Open Question 3
- What defensive mechanisms can effectively prevent Bait-and-Switch attacks while maintaining model utility?
- Basis in paper: [explicit] The paper discusses limitations of current approaches and mentions "watermarking methods" and "automated fact-checking" but doesn't evaluate their effectiveness against this specific attack vector.
- Why unresolved: The paper identifies the problem but doesn't propose or test comprehensive solutions that balance safety with helpfulness.
- What evidence would resolve it: Empirical evaluation of defensive techniques (including post-generation content analysis, prompt semantic understanding, and proxy detection) measuring their effectiveness against Bait-and-Switch attacks while tracking impact on legitimate usage.

## Limitations

- The study only evaluates two commercial models (GPT-4 and Claude-2), limiting generalizability to other LLM architectures
- The effectiveness of the attack depends heavily on human judgment for proxy concept selection, which may not scale systematically
- The study doesn't measure how convincing the generated harmful content would be in real-world scenarios where readers might detect inconsistencies

## Confidence

**High Confidence**: The core mechanism of Bait-and-Switch attacks (using proxy concepts followed by word replacement) is clearly demonstrated and replicable. The attack works as described across multiple test cases with consistent results.

**Medium Confidence**: The claim that Claude-2 is more resistant than GPT-4 is supported by the data, but specific metrics and success rates for Claude-2 are not provided in detail. The relative robustness appears correct but the magnitude is unclear.

**Low Confidence**: The assertion that this vulnerability reveals "the problem of making LLMs harmless is more challenging than anticipated" is speculative. The study doesn't explore whether existing safety techniques could be adapted to address this specific attack, nor does it compare the difficulty of this attack to other known jailbreaking methods.

## Next Checks

1. **Cross-Model Validation**: Test the Bait-and-Switch attack against a diverse set of open-source LLMs (Llama, Mistral, etc.) with varying sizes and training approaches to determine if the vulnerability is universal or model-specific.

2. **Detection System Evaluation**: Implement and test post-hoc manipulation detection systems that can flag content modified through word replacement, measuring both true positive rates and false positive rates on benign content.

3. **Human Perception Study**: Conduct user studies to measure how often readers can detect Bait-and-Switch generated content versus naturally occurring harmful content, establishing the real-world effectiveness of this attack vector.