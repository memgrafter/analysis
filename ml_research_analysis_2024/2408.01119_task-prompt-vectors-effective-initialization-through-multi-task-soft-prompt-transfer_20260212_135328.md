---
ver: rpa2
title: 'Task Prompt Vectors: Effective Initialization through Multi-Task Soft-Prompt
  Transfer'
arxiv_id: '2408.01119'
source_url: https://arxiv.org/abs/2408.01119
tags:
- task
- prompt
- vectors
- tasks
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Task Prompt Vectors introduces a method for creating task vectors
  from soft prompts in large language models, enabling efficient multi-task transfer.
  By subtracting the initial soft prompt weights from their fine-tuned versions, the
  approach generates task prompt vectors that can be combined via arithmetic operations.
---

# Task Prompt Vectors: Effective Initialization through Multi-Task Soft-Prompt Transfer

## Quick Facts
- arXiv ID: 2408.01119
- Source URL: https://arxiv.org/abs/2408.01119
- Reference count: 40
- Task prompt vectors enable parameter-efficient multi-task transfer through arithmetic combinations of soft prompt differences

## Executive Summary
Task Prompt Vectors introduces a method for creating task vectors from soft prompts in large language models, enabling efficient multi-task transfer. By subtracting the initial soft prompt weights from their fine-tuned versions, the approach generates task prompt vectors that can be combined via arithmetic operations. Experiments on 19 datasets show that these vectors are independent of random initialization, allowing for robust reuse across different tasks. Task prompt vector combinations maintain single-task performance and outperform state-of-the-art baselines in zero-shot and few-shot settings, providing a parameter-efficient alternative for multi-task learning.

## Method Summary
The method creates task prompt vectors by first training soft prompts on source tasks, then computing the element-wise difference between fine-tuned weights and their random initialization. These vectors capture task-specific knowledge in the embedding space and can be combined through arithmetic addition to create multi-task initializations. The approach is evaluated across 19 datasets spanning 8 task types, demonstrating effectiveness for zero-shot and few-shot learning scenarios. Task prompt vectors serve as initialization for prompt tuning, requiring only the modification of soft prompt parameters rather than full model weights.

## Key Results
- Task prompt vectors maintain single-task performance when combined across tasks
- Performance is independent of random initialization for most observed tasks
- Outperforms state-of-the-art baselines in zero-shot and few-shot learning settings
- Enables parameter-efficient transfer learning without full model fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task prompt vectors are independent of random initialization
- Mechanism: Element-wise difference between tuned soft prompt weights and their random initialization creates a direction in embedding space that is consistent regardless of starting point
- Core assumption: The optimization trajectory for prompt tuning converges to a similar subspace regardless of initialization
- Evidence anchors:
  - [abstract] "Experimental results on 12 NLU datasets show that task prompt vectors can be used in low-resource settings to effectively initialize prompt tuning on similar tasks"
  - [section 4.2] "The performance of task prompt vectors is independent of the random initialization for the majority of observed tasks"
  - [corpus] Weak evidence - no direct corpus support for initialization independence
- Break condition: If the optimization landscape has multiple isolated minima with different characteristics, initialization independence would fail

### Mechanism 2
- Claim: Task prompt vector combinations maintain single-task performance
- Mechanism: Arithmetic addition of task prompt vectors combines knowledge from multiple tasks while preserving the core task-specific information
- Core assumption: Similar tasks have overlapping subspaces in the embedding space that can be linearly combined
- Evidence anchors:
  - [abstract] "task prompt vector combinations maintain single-task performance and outperform state-of-the-art baselines in zero-shot and few-shot settings"
  - [section 4.3] "most binary classification tasks retain their single-task performance on both tasks"
  - [corpus] Weak evidence - corpus doesn't directly address vector combination performance
- Break condition: If tasks are too dissimilar, the addition would cause negative transfer and degrade performance

### Mechanism 3
- Claim: Task prompt vectors enable efficient zero-shot and few-shot learning
- Mechanism: Pre-trained task prompt vectors provide a good initialization for new tasks by capturing relevant task-specific directions in the embedding space
- Core assumption: Similar tasks share enough common structure that a prompt tuned for one task can effectively initialize another
- Evidence anchors:
  - [abstract] "task prompt vector combinations maintain single-task performance and outperform state-of-the-art baselines in zero-shot and few-shot settings"
  - [section 4.3] "Task prompt vector combinations are good initializations for zero-shot and few-shot learning"
  - [corpus] Weak evidence - corpus doesn't specifically address zero-shot performance
- Break condition: If the target task is too dissimilar from any source task, the initialization would provide little benefit

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: Task prompt vectors are a PEFT method that only modifies soft prompt parameters rather than full model weights
  - Quick check question: What fraction of model parameters does prompt tuning typically modify?

- Concept: Vector arithmetic in high-dimensional spaces
  - Why needed here: Task prompt vectors rely on element-wise addition and subtraction to combine task knowledge
  - Quick check question: What property of vectors allows task prompt vectors to be added together while preserving useful information?

- Concept: Cosine similarity for direction comparison
  - Why needed here: Used to measure similarity between task prompt vectors to identify related tasks
  - Quick check question: What does a high cosine similarity between two task prompt vectors indicate about the tasks they represent?

## Architecture Onboarding

- Component map:
  - Input layer: Original text input
  - Soft prompt layer: Learnable prompt embeddings prepended to input
  - Transformer encoder: Frozen pre-trained language model
  - Output layer: Task-specific prediction head
  - Task prompt vectors: Stored as element-wise differences for transfer

- Critical path: Soft prompt initialization → Prompt tuning on source task → Create task prompt vector → Apply to new task initialization → Fine-tune

- Design tradeoffs:
  - Memory vs. performance: Storing task prompt vectors requires additional memory but enables faster adaptation
  - Task similarity vs. negative transfer: Combining vectors from dissimilar tasks can degrade performance
  - Initialization quality vs. training time: Better initialization reduces fine-tuning time but requires source task training

- Failure signatures:
  - Performance degradation when applying task prompt vectors to dissimilar tasks
  - Inconsistent results across different random initializations (indicates broken independence assumption)
  - Vector combinations that fail to retain single-task performance

- First 3 experiments:
  1. Train soft prompts on MNLI and QNLI, create task prompt vectors, apply to a new initialization and verify performance match
  2. Combine task prompt vectors from SST2 and Yelp, apply to SST5 initialization, compare with random initialization
  3. Test zero-shot performance by applying task prompt vector combinations to completely unseen tasks and measure performance relative to random initialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do task prompt vectors behave in multilingual settings with cross-lingual transfer?
- Basis in paper: [inferred] The paper notes it focused on monolingual English datasets and suggests multilingual models as a potential extension, indicating this remains unexplored.
- Why unresolved: The authors explicitly limited their study to English datasets and monolingual models, so cross-lingual behavior is unknown.
- What evidence would resolve it: Experiments applying task prompt vectors across multilingual models and datasets, measuring cross-lingual transfer performance and vector similarity.

### Open Question 2
- Question: What is the theoretical explanation for why task prompt vectors enable linear interpolation in the non-linear soft prompt space?
- Basis in paper: [explicit] The authors hypothesize about convex sub-space shape and linear approximations but state theoretical analysis is beyond scope, calling it a hypothesis for future work.
- Why unresolved: The paper provides empirical findings but does not offer formal theoretical justification for the observed behavior.
- What evidence would resolve it: Mathematical proofs or formal analysis showing the conditions under which task prompt vectors enable linear interpolation, or explanations of the underlying geometric properties.

### Open Question 3
- Question: How does task prompt vector subtraction perform for machine unlearning compared to other methods?
- Basis in paper: [explicit] The authors mention that task prompt vector subtraction could enable unlearning by negating vectors for unwanted tasks, but this remains unexplored.
- Why unresolved: The paper only mentions this as a potential application without any experimental validation.
- What evidence would resolve it: Comparative experiments measuring unlearning effectiveness of task prompt vector subtraction versus existing unlearning techniques on various tasks.

## Limitations

- Limited evidence for cross-lingual transfer capabilities since only monolingual English datasets were used
- No theoretical justification provided for why linear operations work in the non-linear soft prompt space
- Scalability analysis for larger models and more diverse task sets is missing from the evaluation

## Confidence

**High Confidence:** The core methodology of creating task prompt vectors through element-wise subtraction is well-defined and the experimental setup is clearly described. The finding that vector combinations maintain single-task performance is well-supported by the reported results.

**Medium Confidence:** The claim about initialization independence is supported by experimental evidence but lacks theoretical justification. The effectiveness of task prompt vectors for zero-shot and few-shot learning is demonstrated empirically but the mechanism behind why this works across diverse task types needs more rigorous investigation.

**Low Confidence:** The paper does not provide sufficient evidence for the optimal combination strategies when dealing with more than two tasks, nor does it address potential negative transfer when combining highly dissimilar tasks. The scalability analysis for larger models and more diverse task sets is missing.

## Next Checks

1. **Initialization Robustness Test:** Systematically vary random initialization seeds (e.g., 10+ seeds) for soft prompt tuning across all 19 datasets and measure both cosine similarity between resulting task prompt vectors and downstream performance consistency. This will validate the claimed independence from initialization more rigorously.

2. **Task Dissimilarity Boundary:** Design experiments to identify the boundary of task similarity where task prompt vector combinations begin to degrade performance. Test combinations across the full spectrum from highly similar (e.g., SST2 + SST5) to highly dissimilar (e.g., MNLI + MATH) tasks and measure the performance drop-off curve.

3. **Zero-Shot Generalization Analysis:** Evaluate task prompt vector performance on completely unseen tasks beyond the 19 datasets used in training. Measure performance relative to random initialization across multiple model scales (T5-small, T5-base, T5-large) to assess scalability and generalization limits.