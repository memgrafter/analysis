---
ver: rpa2
title: 'Leveraging Large Language Models in Code Question Answering: Baselines and
  Issues'
arxiv_id: '2411.03012'
source_url: https://arxiv.org/abs/2411.03012
tags:
- code
- dataset
- question
- answering
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of using large language models\
  \ (LLMs) for code question answering (Code Q&A) in Python, aiming to improve automated\
  \ responses to code-related queries. The core method involves fine-tuning pre-trained\
  \ LLMs\u2014specifically StarCoder and DeepSeek-Coder\u2014on a unified dataset\
  \ compiled from CodeQA, CS1QA, and PCSD."
---

# Leveraging Large Language Models in Code Question Answering: Baselines and Issues

## Quick Facts
- arXiv ID: 2411.03012
- Source URL: https://arxiv.org/abs/2411.03012
- Reference count: 28
- Primary result: Fine-tuning StarCoder with grammar correction yields BLEU-4 scores of 8.89% on unified test set

## Executive Summary
This paper explores fine-tuning large language models for code question answering in Python using a unified dataset from CodeQA, CS1QA, and PCSD. The authors evaluate StarCoder and DeepSeek-Coder models with three preprocessing strategies: no grammar correction, grammar correction using LanguageTool, and augmentation with generated code summaries via CodeT5+. Results show grammar correction significantly improves performance over no preprocessing, with StarCoder achieving 8.89% BLEU-4 on the test set, while summary augmentation provides less benefit. Manual error analysis identifies insufficient context and unclear questions as primary failure points.

## Method Summary
The study fine-tunes pre-trained LLMs (StarCoder and DeepSeek-Coder) using qLoRA for efficient adaptation on a unified dataset compiled from multiple code QA sources. Three preprocessing strategies are evaluated: no grammar correction, grammar correction using LanguageTool, and augmentation with generated code summaries using CodeT5+. The models are trained on both original and augmented datasets, with performance measured using BLEU-4 scores on both unified testing data and the ClassEvalQA benchmark. Manual error analysis examines 50 samples to identify failure modes.

## Key Results
- StarCoder with grammar-corrected data achieves 8.89% BLEU-4 on unified test set
- DeepSeek-Coder shows less improvement, likely due to smaller model size
- Grammar correction outperforms summary augmentation for performance improvement
- Manual analysis reveals insufficient context and unclear questions as key failure points

## Why This Works (Mechanism)
The effectiveness stems from fine-tuning pre-trained LLMs on domain-specific code question answering data, allowing models to learn the mapping between natural language questions and code responses. Grammar correction improves input quality by standardizing question phrasing, reducing noise in the training data. The qLoRA technique enables efficient fine-tuning by freezing most model weights while adapting a small subset. The unified dataset approach provides diverse training examples across multiple code QA sources, improving generalization.

## Foundational Learning
- qLoRA: Parameter-efficient fine-tuning method that freezes most model weights while adapting a small subset, needed for resource-efficient adaptation; quick check: monitor training memory usage vs full fine-tuning
- BLEU-4: Evaluation metric measuring n-gram overlap between generated and reference answers, needed for standardized comparison; quick check: compare with alternative metrics like ROUGE or code-specific metrics
- CodeQA datasets: Specialized datasets containing code-related questions and answers, needed for supervised learning; quick check: verify dataset quality and balance across topics
- LanguageTool grammar correction: Text preprocessing tool for standardizing question phrasing, needed to reduce input noise; quick check: compare corrected vs original questions for semantic preservation
- CodeT5+: Code-specific pre-trained model for generating summaries, needed for data augmentation; quick check: validate generated summaries maintain code intent
- Python-specific fine-tuning: Language-specific adaptation approach, needed for domain relevance; quick check: test on cross-language code QA tasks

## Architecture Onboarding
**Component Map:** Question -> Preprocessing (Grammar Correction/Summary Augmentation) -> LLM (StarCoder/DeepSeek-Coder) -> Answer Generation
**Critical Path:** Input question → Grammar correction → Model inference → Output code answer
**Design Tradeoffs:** Grammar correction improves quality but may alter question intent vs. summary augmentation increases dataset size but adds noise; model size vs. performance (StarCoder vs DeepSeek-Coder)
**Failure Signatures:** Insufficient context leading to incomplete answers, unclear questions causing irrelevant responses, grammar correction altering question meaning
**First Experiments:** 1) Compare BLEU-4 scores across preprocessing methods on held-out validation set, 2) Analyze model outputs for common error patterns, 3) Test generalization on out-of-distribution code questions

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond noting the need for higher-quality, human-annotated datasets for future improvements.

## Limitations
- BLEU-4 scores remain modest (8.89% on test, 4.24% on benchmark), indicating limited practical utility
- Manual error analysis covers only 50 samples, potentially missing broader failure modes
- Focus exclusively on Python limits generalizability to other programming languages
- BLEU-4 may not capture functional correctness of generated code

## Confidence
- High confidence: Grammar correction improves model performance over no preprocessing
- Medium confidence: StarCoder outperforms DeepSeek-Coder due to size differences
- Medium confidence: Summary augmentation is less effective than grammar correction
- Low confidence: Findings generalize to other programming languages or larger implementations

## Next Checks
1. Test fine-tuned models on real-world Code Q&A dataset with human-annotated answers for practical validation
2. Conduct functional correctness evaluation where generated code is executed to verify outputs, not just BLEU-4 comparison
3. Perform ablation studies varying dataset size and quality to determine minimum viable training characteristics