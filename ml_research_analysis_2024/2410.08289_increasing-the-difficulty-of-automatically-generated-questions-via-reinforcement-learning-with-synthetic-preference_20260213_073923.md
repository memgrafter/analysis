---
ver: rpa2
title: Increasing the Difficulty of Automatically Generated Questions via Reinforcement
  Learning with Synthetic Preference
arxiv_id: '2410.08289'
source_url: https://arxiv.org/abs/2410.08289
tags:
- question
- questions
- answer
- difficulty
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a cost-effective approach to generate domain-specific
  Machine Reading Comprehension (MRC) datasets with increased difficulty using Reinforcement
  Learning from Human Feedback (RLHF) with synthetic preference data. The method leverages
  the performance of existing question-answering models on a subset of SQuAD to create
  a difficulty metric, assuming that more challenging questions are answered correctly
  less frequently.
---

# Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference

## Quick Facts
- arXiv ID: 2410.08289
- Source URL: https://arxiv.org/abs/2410.08289
- Authors: William Thorne; Ambrose Robinson; Bohua Peng; Chenghua Lin; Diana Maynard
- Reference count: 15
- Primary result: RLHF with synthetic preference data generates more difficult questions that are answered correctly less frequently by QA models

## Executive Summary
This paper presents a cost-effective approach for generating domain-specific Machine Reading Comprehension (MRC) datasets with increased difficulty using Reinforcement Learning from Human Feedback (RLHF) with synthetic preference data. The method leverages existing QA model performance on SQuAD to create a difficulty metric, assuming that questions answered correctly less frequently are inherently more difficult. By fine-tuning LLaMa-2-7B-chat using Proximal Policy Optimization (PPO) with this synthetic data, the authors demonstrate that the resulting models produce questions that are more challenging for QA models to answer correctly, while maintaining human-assessed answerability rates.

## Method Summary
The approach involves fine-tuning LLaMa-2-7B-chat using PPO with synthetic pairwise preference data derived from evaluating multiple QA models (RoBERTa-large, DeBERTa-large, RetroReader) on SQuAD. Questions are scored based on how many QA models fail to answer them correctly, creating a difficulty metric. This synthetic preference data is used to train a reward model, which guides the PPO fine-tuning process to generate more difficult questions. The methodology uses LoRA adapters for parameter-efficient fine-tuning and includes format critics to ensure generated questions meet quality standards.

## Key Results
- PPO-trained models generate questions answered correctly less frequently by QA models compared to zero-shot and supervised fine-tuned models
- Human evaluation shows equivalent or improved rates of answerability for questions generated by PPO-trained models
- The approach demonstrates promise for generating challenging MRC datasets, particularly for domains where manual dataset creation is expensive

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Difficulty can be modeled as a function of QA model performance on questions
- Mechanism: Questions answered correctly less frequently by QA models are inherently more difficult
- Core assumption: High-performing QA models capture human-like understanding of question difficulty
- Evidence anchors:
  - [abstract] "This method leverages the performance of existing question-answering models on a subset of SQuAD to create a difficulty metric, assuming that more challenging questions are answered correctly less frequently."
  - [section 3.2] "Each question is assigned a score based on the number of models that failed to correctly answer the question."
  - [corpus] Weak evidence - only 5 citations total, suggesting limited external validation of this specific difficulty modeling approach
- Break condition: If QA models' performance metrics diverge significantly from human assessments of difficulty, or if model performance plateaus making discrimination impossible

### Mechanism 2
- Claim: Synthetic preference data can replace expensive human annotations in RLHF
- Mechanism: Use QA model accuracy as proxy for human preference ranking
- Core assumption: Questions that QA models answer incorrectly serve as effective negative samples
- Evidence anchors:
  - [abstract] "Rather than relying on costly human annotations, we generate synthetic preference data by evaluating question-answering model performance on a subset of SQuAD"
  - [section 3.2] "These scores are used to place questions into a pairwise ranking setup against other questions for the same input context."
  - [section 4.4] "To assess the quality of generated questions relative to our SQuAD test split, we intentionally avoid n-gram based metrics"
- Break condition: If synthetic preferences don't correlate with human preferences, or if the reward model overfits to QA model idiosyncrasies

### Mechanism 3
- Claim: LoRA adapters enable efficient fine-tuning while maintaining model quality
- Mechanism: Parameter-efficient fine-tuning reduces memory overhead for RLHF
- Core assumption: LoRA adapters preserve essential model capabilities while adding task-specific behavior
- Evidence anchors:
  - [section 4.1] "We conduct our experiments with LLaMa2-7B-chat and apply LoRA adapters to all linear layers for all models."
  - [section 4.1] "This drastically lowers the number of tunable parameters over full-finetuning, enabling training on a single A100 80GB GPU."
  - [section 4.1] "This memory efficiency further allowed us to implement sample packing"
- Break condition: If LoRA adapters introduce catastrophic forgetting or if the rank is insufficient for capturing difficulty-related patterns

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Provides framework for learning from synthetic preference data rather than human labels
  - Quick check question: Can you explain how the reward model is trained and used to update the policy in standard RLHF?

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: Algorithm for stable policy updates during RL fine-tuning
  - Quick check question: What is the purpose of the clipping mechanism in PPO's objective function?

- Concept: Question Answering Evaluation Metrics
  - Why needed here: Understanding how model performance translates to difficulty assessment
  - Quick check question: How do Exact Match and F1 scores differ in evaluating QA model performance?

## Architecture Onboarding

- Component map: SQuAD → SFT → Reward Model → PPO → Final model generation
- Critical path: SQuAD data preprocessing → Format critics → Deduplication → Difficulty modeling → RLHF training → Evaluation
- Design tradeoffs:
  - LoRA vs full fine-tuning: Memory efficiency vs potential performance
  - Synthetic vs human preferences: Cost vs quality of preference data
  - Pairwise vs pointwise ranking: Training stability vs sample efficiency
- Failure signatures:
  - Format critic failures: Model not learning correct output structure
  - Low valid generation rates: Reward model not capturing quality signals
  - Unchanged QA accuracy: PPO not affecting question difficulty
- First 3 experiments:
  1. Run format critics on zero-shot generations to establish baseline failure modes
  2. Train reward model with and without input text to verify importance of context
  3. Compare PPO training stability with and without rule-based reward component

## Open Questions the Paper Calls Out

## Open Question 1
- Question: How do the proposed RLHF-based question generation methods perform when applied to diachronic cultural heritage datasets compared to synchronic datasets like SQuAD?
- Basis in paper: Inferred from the Discussion section, which mentions that cultural heritage datasets typically present diachronic challenges that add complexity to question generation, and future work will examine how the training paradigm fares under the unique challenges presented by such a varied industry.
- Why unresolved: The paper focuses on using SQuAD, a synchronic dataset, for experimental purposes. It acknowledges the need for evaluation on diachronic cultural heritage datasets but does not provide empirical results for such datasets.
- What evidence would resolve it: Empirical results showing the performance of the RLHF-based question generation methods on diachronic cultural heritage datasets, comparing them to synchronic datasets like SQuAD in terms of question difficulty, quality, and relevance to the domain.

## Open Question 2
- Question: Can the proposed RLHF-based question generation approach be extended to manipulate multiple abstract properties of questions simultaneously, such as difficulty, answer type, and temporal complexity?
- Basis in paper: Explicit from the Conclusion section, which mentions that the approach can be extended further, allowing for the manipulation of multiple abstract properties simultaneously through multi-reward model setups.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the approach in increasing question difficulty. While it mentions the potential for extending the approach to other properties, it does not provide empirical evidence or a detailed methodology for doing so.
- What evidence would resolve it: Empirical results showing the effectiveness of the RLHF-based question generation approach in manipulating multiple abstract properties of questions simultaneously, such as difficulty, answer type, and temporal complexity, using multi-reward model setups.

## Open Question 3
- Question: How does the computational cost and efficiency of the proposed RLHF-based question generation approach compare to other state-of-the-art methods for generating domain-specific MRC datasets?
- Basis in paper: Inferred from the Limitations section, which acknowledges that despite using LoRA and multi-adapter training, the approach still required approximately 15 GPU hours on an A100 80GB, restricting the potential audience for this approach.
- Why unresolved: The paper does not provide a direct comparison of the computational cost and efficiency of the proposed approach to other state-of-the-art methods for generating domain-specific MRC datasets. It only mentions the computational requirements of the proposed approach.
- What evidence would resolve it: A comparative analysis of the computational cost and efficiency of the proposed RLHF-based question generation approach and other state-of-the-art methods for generating domain-specific MRC datasets, considering factors such as training time, hardware requirements, and scalability.

## Limitations
- Domain-dependent methodology tied to availability of QA model performance data
- Synthetic preference generation relies on limited set of 3 QA models
- Indirect evaluation methodology using downstream QA model performance rather than direct human assessment

## Confidence

**High confidence** in the core methodology - the use of RLHF with synthetic preferences is technically sound and the implementation details (LoRA adapters, PPO training, pairwise ranking) are well-established approaches with clear empirical validation.

**Medium confidence** in the difficulty metric - while the assumption that QA model performance correlates with human-perceived difficulty is reasonable and grounded in existing literature, the specific implementation using only 3 models and pairwise comparisons on SQuAD data may not generalize to all domains or question types.

**Medium confidence** in the evaluation - the human evaluation showing equivalent or improved answerability is promising, but the sample sizes (20 questions per model) are relatively small. The use of LLM-based evaluation adds another layer of uncertainty.

## Next Checks

1. **Direct human difficulty validation**: Conduct a controlled study where human annotators rate question difficulty independently of the synthetic preference generation process. Compare human ratings against both the synthetic preference scores and downstream QA model performance to validate the core assumption.

2. **Domain transfer evaluation**: Test the methodology on a domain with very different characteristics from SQuAD (e.g., medical or legal text) to assess how well the difficulty modeling transfers. Measure whether QA model performance on synthetic preferences still correlates with human difficulty assessments in this new domain.

3. **Ablation study on QA model count**: Systematically vary the number of QA models used to generate synthetic preferences (1, 2, 3, 5 models) and measure the impact on both the quality of generated questions and the correlation between synthetic preferences and human assessments. This would help determine the minimum viable number of models needed for effective difficulty modeling.