---
ver: rpa2
title: Knowledge Fusion of Large Language Models
arxiv_id: '2401.10491'
source_url: https://arxiv.org/abs/2401.10491
tags:
- llms
- llama-2
- fusion
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of integrating knowledge from
  multiple large language models (LLMs) with different architectures and capabilities.
  The proposed method, FUSE LLM, addresses this by leveraging the generative distributions
  of source LLMs to externalize their collective knowledge and unique strengths.
---

# Knowledge Fusion of Large Language Models

## Quick Facts
- arXiv ID: 2401.10491
- Source URL: https://arxiv.org/abs/2401.10491
- Authors: Fanqi Wan; Xinting Huang; Deng Cai; Xiaojun Quan; Wei Bi; Shuming Shi
- Reference count: 31
- Primary result: FUSE LLM achieves significant performance gains across reasoning, commonsense, and code generation benchmarks by fusing knowledge from multiple LLMs

## Executive Summary
This paper introduces FUSE LLM, a method for integrating knowledge from multiple large language models with different architectures and capabilities. The approach leverages the generative distributions of source LLMs to externalize their collective knowledge and unique strengths, which are then transferred to a target LLM through lightweight continual training. Experiments demonstrate that FUSE LLM outperforms individual source LLMs and baselines across various benchmarks, particularly in complex tasks where combining diverse capabilities provides significant advantages.

## Method Summary
FUSE LLM operates by first applying multiple source LLMs to a text corpus to generate probability distributions over tokens. These distributions are aligned across different tokenizers using a minimum edit distance (MinED) strategy, then fused using a minimum cross-entropy (MinCE) selection function. The target LLM is then trained to match these fused distributions through a combination of standard causal language modeling and a fusion objective that aligns its predicted distributions with those of the source models. This process allows the target model to inherit the collective knowledge of multiple source models while maintaining its own capabilities.

## Key Results
- FUSE LLM outperforms individual source LLMs (Llama-2, MPT, OpenLLaMA) on reasoning (BBH), commonsense (CS), and code generation (ME) benchmarks
- The MinCE fusion function consistently outperforms weighted averaging (AvgCE) across all tested benchmarks
- The MinED token alignment strategy reduces information loss compared to exact matching, particularly for tokens with minor surface differences
- Performance gains are most pronounced on complex reasoning tasks that benefit from diverse knowledge sources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The probabilistic distribution matrix of an LLM encodes its understanding of a text sequence.
- Mechanism: Each row of the distribution matrix represents the model's predicted probability for the next token given the preceding tokens. Different models produce different distributions for the same text, reflecting their unique knowledge.
- Core assumption: The generative distributions capture meaningful aspects of the model's internal knowledge rather than just random noise.
- Evidence anchors:
  - [abstract] "By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths"
  - [section 3.1] "we argue that the probabilistic distribution matrix can reflect its certain inherent knowledge in understanding the text"
  - [corpus] Weak - the paper does not provide empirical evidence that the distributions actually capture semantic understanding vs. surface patterns.
- Break condition: If the distributions primarily capture surface-level patterns rather than deeper semantic understanding, the fusion would not meaningfully combine capabilities.

### Mechanism 2
- Claim: Token alignment with minimum edit distance (MinED) preserves more useful information than exact matching (EM).
- Mechanism: When tokenizers produce slightly different tokens for the same word, MinED maps tokens based on minimal edit distance rather than requiring exact string matches, reducing information loss in the distribution matrices.
- Core assumption: Tokens with minimal edit distance

## Foundational Learning
The paper builds on the observation that LLMs encode knowledge in their generative distributions. By treating these distributions as representations of model understanding, the authors create a framework for knowledge transfer between models. This leverages existing work on knowledge distillation and model fusion, but applies it to the more complex setting of heterogeneous architectures. The approach assumes that token-level distributions contain sufficient information to capture model capabilities, which is supported by the empirical success but not rigorously proven.

## Architecture Onboarding
FUSE LLM is designed to work with any pre-trained LLMs, regardless of their underlying architecture. The method requires only that source models can generate token-level probability distributions over a text corpus. The target model can be any LLM that supports standard causal language modeling objectives. The token alignment step handles differences between tokenizer vocabularies, making the approach robust to architectural variations. This flexibility allows the method to be applied across different model families without requiring architectural modifications.

## Open Questions the Paper Calls Out
The authors acknowledge several open questions, including the scalability of the approach to very large corpora, the impact of different fusion strategies on model capabilities, and the interpretability of what knowledge is being transferred. They also note that the current method focuses on text-to-text generation and suggest future work exploring multimodal extensions. The paper calls for more systematic investigation into how different types of knowledge (factual, procedural, commonsense) are preserved or transformed during the fusion process.

## Limitations
The approach requires significant computational resources for training, as it involves running multiple large models over a text corpus and then training the target model with additional objectives. The token alignment process may still lose some information when tokenizers produce very different representations. The method assumes that the source models are reasonably capable, as fusing weak models would likely produce poor results. There is also a potential for overfitting to the training corpus if it is not sufficiently diverse. The paper does not address potential biases that might be amplified through the fusion process.

## Confidence
High confidence in the core claims based on the empirical results presented. The method shows consistent improvements across multiple benchmarks and model combinations. The mechanism explanations are plausible given the observed performance gains, though the exact nature of how knowledge is represented in token distributions remains somewhat unclear. The limitations section appropriately identifies potential weaknesses without overstating the method's capabilities. The approach appears to be a meaningful contribution to the field of model fusion and knowledge transfer.

## Next Checks
- Verify the experimental methodology and ensure proper controls were used
- Examine the diversity of the training corpus and its impact on fusion quality
- Investigate the computational overhead compared to alternative approaches
- Analyze the robustness of the method across different domains and task types
- Explore the interpretability of what knowledge is being transferred between models
- Test the approach with additional model architectures beyond those presented
- Evaluate the long-term stability of the fused models during continued training