---
ver: rpa2
title: A Question Answering Based Pipeline for Comprehensive Chinese EHR Information
  Extraction
arxiv_id: '2402.11177'
source_url: https://arxiv.org/abs/2402.11177
tags:
- questions
- information
- pipeline
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel pipeline that automatically generates
  training data for transfer learning of QA models to extract information from Chinese
  EHRs. The key idea is to leverage existing entity and relation annotations in EHRs
  to automatically construct QA pairs, which are then used to train an extractive
  QA model via transfer learning.
---

# A Question Answering Based Pipeline for Comprehensive Chinese EHR Information Extraction

## Quick Facts
- **arXiv ID:** 2402.11177
- **Source URL:** https://arxiv.org/abs/2402.11177
- **Reference count:** 40
- **Primary result:** QA-based pipeline achieves F1 scores of 0.965 (NER) and 0.953 (relation extraction) on Chinese EHR data

## Executive Summary
This paper proposes a novel pipeline that automatically generates training data for transfer learning of QA models to extract information from Chinese EHRs. The key idea is to leverage existing entity and relation annotations in EHRs to automatically construct QA pairs, which are then used to train an extractive QA model via transfer learning. The pipeline handles challenges like discontinuous answer spans and many-to-one relationships through preprocessing and postprocessing steps. Experimental results show the QA model achieves competitive performance on both NER and relation extraction tasks in EHRs, with F1 scores of 0.965 and 0.953 respectively. The model also generalizes well to yes-no questions and demonstrates potential for few-shot learning.

## Method Summary
The pipeline automatically converts entity and relation annotations from Chinese EHRs into QA pairs. It preprocesses the data by splitting paragraphs into sentences to handle discontinuous answer spans and merges adjacent spans. The pipeline also constructs "impossible questions" with plausible answers to improve the model's answerability judgment. A pre-trained RoBERTa-base-Chinese model is fine-tuned on these automatically generated QA pairs using a Retro-Reader architecture. Postprocessing merges sentence-level outputs to form final answers. The resulting model performs both NER-like and relation extraction tasks within a unified QA framework.

## Key Results
- QA model achieves F1 scores of 0.965 on Named Entity Recognition and 0.953 on relation extraction tasks
- Model demonstrates strong generalization to yes-no questions without additional training
- Shows potential for few-shot learning scenarios with limited labeled data
- Handles both continuous and discontinuous answer spans effectively through preprocessing

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The pipeline's preprocessing step that splits paragraphs into sentences and merges adjacent answer spans enables extractive QA models to handle discontinuous answer spans.
- **Mechanism:** By splitting paragraphs into sentences and ensuring each sentence has at most one answer span, the model can handle discontinuous answers that are distributed across sentences. Adjacent spans within the same sentence are merged to form a single answer.
- **Core assumption:** Most discontinuous answers in EHRs can be handled by sentence-level processing and merging adjacent spans within sentences.
- **Evidence anchors:** [abstract] "The pipeline handles challenges like discontinuous answer spans and many-to-one relationships through preprocessing and postprocessing steps." [section] "To address this issue, we handle cases of many-to-one correspondence during preprocessing as follows: If two answer spans are adjacent or separated only by punctuations, we merge them into one span..."

### Mechanism 2
- **Claim:** The construction of impossible questions with plausible answers improves the model's ability to judge answerability.
- **Mechanism:** By including impossible questions with plausible answers, the model learns to distinguish between answerable and unanswerable questions, improving its judgment of answerability.
- **Core assumption:** The inclusion of impossible questions with plausible answers enhances the model's understanding of answerability in EHR contexts.
- **Evidence anchors:** [abstract] "Our pipeline incorporates a preprocessing module to handle challenges posed by extraction types that is not readily compatible with extractive QA frameworks, including cases with discontinuous answers and many-to-one relationships." [section] "Furthermore, to adhere to the format of the SQuAD dataset, we include 'plausible answers' for impossible questions..."

### Mechanism 3
- **Claim:** Transfer learning from pre-trained general domain QA models to EHR-specific tasks improves performance on both NER and relation extraction.
- **Mechanism:** By leveraging pre-trained QA models and fine-tuning them on automatically generated EHR QA pairs, the model adapts to the specific characteristics of EHR text and performs well on both NER and relation extraction tasks.
- **Core assumption:** Pre-trained QA models can be effectively adapted to EHR-specific tasks through transfer learning on automatically generated QA pairs.
- **Evidence anchors:** [abstract] "Experimental results show the QA model achieves competitive performance on both NER and relation extraction tasks in EHRs, with F1 scores of 0.965 and 0.953 respectively." [section] "With transfer learning, we observed satisfactory performance. The final model demonstrates competency across various IE subtasks in EHRs..."

## Foundational Learning

- **Concept:** Extractive Question Answering
  - **Why needed here:** The pipeline relies on extractive QA models to identify answer spans within the EHR text.
  - **Quick check question:** What is the difference between extractive and generative QA models?

- **Concept:** Named Entity Recognition (NER)
  - **Why needed here:** The pipeline performs NER-like tasks to identify entities such as diseases, abnormalities, and body parts in the EHR text.
  - **Quick check question:** What are the common entity types found in EHR text?

- **Concept:** Relation Extraction
  - **Why needed here:** The pipeline extracts relationships between entities in the EHR text, such as the relationship between a disease and its associated body part.
  - **Quick check question:** What are some common types of relationships found in EHR text?

## Architecture Onboarding

- **Component map:** Preprocessing -> QA Model (RoBERTa-base-Chinese) -> Postprocessing
- **Critical path:** Preprocessing → QA Model → Postprocessing
- **Design tradeoffs:**
  - Using extractive QA models instead of generative models to avoid hallucination.
  - Splitting paragraphs into sentences to handle discontinuous answers, but potentially losing context across sentences.
  - Constructing impossible questions with plausible answers to improve answerability judgment, but potentially introducing noise.
- **Failure signatures:**
  - Incorrectly merged answer spans leading to incomplete or incorrect answers.
  - Poor performance on yes-no questions due to the extractive nature of the model.
  - Failure to handle discontinuous answers that span across multiple sentences.
- **First 3 experiments:**
  1. Evaluate the pipeline's performance on a small subset of EHR data with known answers to verify the correctness of the preprocessing and postprocessing steps.
  2. Compare the performance of the fine-tuned QA model with a baseline model that does not use transfer learning to assess the effectiveness of transfer learning.
  3. Test the pipeline's ability to handle yes-no questions by creating a set of yes-no questions based on the EHR data and evaluating the model's performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the QA model's performance on EHR information extraction compare to state-of-the-art traditional IE methods?
- **Basis in paper:** [explicit] The paper states the QA model achieves competitive performance on NER and relation extraction tasks, with F1 scores of 0.965 and 0.953 respectively. However, it does not provide direct comparison to traditional IE methods.
- **Why unresolved:** The paper focuses on demonstrating the effectiveness of the QA-based approach, but does not benchmark it against existing IE methods.
- **What evidence would resolve it:** Experimental results comparing the QA model's performance to traditional IE methods on the same EHR dataset, using metrics like F1 score, precision, and recall.

### Open Question 2
- **Question:** How well does the QA model generalize to different types of EHR data and medical domains?
- **Basis in paper:** [explicit] The paper mentions the model generalizes well to yes-no questions and demonstrates potential for few-shot learning. However, it does not extensively evaluate the model's performance on diverse EHR types or medical domains.
- **Why unresolved:** The paper primarily focuses on the pipeline's effectiveness on a single hospital's EHR data, without exploring its generalizability to other medical domains or EHR formats.
- **What evidence would resolve it:** Experiments evaluating the QA model's performance on EHR data from multiple hospitals, different medical specialties, and diverse EHR formats, comparing its performance across these variations.

### Open Question 3
- **Question:** What are the limitations of the QA model in handling complex or ambiguous EHR information extraction tasks?
- **Basis in paper:** [inferred] The paper acknowledges some limitations, such as the model's difficulty in answering questions related to overall information and its potential to mix up the relativity between clauses. However, it does not comprehensively explore the model's limitations in handling complex or ambiguous extraction tasks.
- **Why unresolved:** The paper focuses on demonstrating the pipeline's effectiveness, but does not thoroughly investigate its limitations or failure cases in handling complex EHR information extraction scenarios.
- **What evidence would resolve it:** In-depth analysis of the QA model's performance on challenging EHR extraction tasks, including cases with ambiguous or incomplete information, nested entities, and complex dependency structures.

## Limitations

- **Limited empirical validation of preprocessing robustness**: The paper demonstrates strong performance on automatically generated QA pairs but provides limited validation of how robust the preprocessing steps are to diverse EHR formats and annotation styles.

- **Transfer learning effectiveness uncertainty**: While the paper reports competitive F1 scores, there's limited comparison to state-of-the-art supervised approaches trained on large annotated EHR datasets.

- **Answerability judgment generalization concerns**: The inclusion of impossible questions with plausible answers to improve answerability judgment is conceptually sound, but the paper doesn't extensively evaluate how well this generalizes to questions that fall outside the training distribution.

## Confidence

- **High confidence**: The fundamental premise that extractive QA models can be adapted for EHR information extraction tasks through transfer learning. The experimental results showing F1 scores above 0.95 provide strong empirical support.

- **Medium confidence**: The preprocessing strategy for handling discontinuous answer spans and many-to-one relationships. While the method is described clearly, the paper lacks detailed analysis of edge cases where these strategies might fail.

- **Medium confidence**: The generalization to yes-no questions and few-shot learning scenarios. The paper mentions these capabilities but provides limited quantitative evidence of performance in these settings.

## Next Checks

1. **Error analysis on preprocessing failure cases**: Manually examine 100+ examples where the preprocessing steps produce incorrect answer spans, particularly focusing on discontinuous answers that span multiple sentences. Document the frequency and types of failures to better understand the limitations of the sentence-level splitting approach.

2. **Direct comparison with supervised baselines**: Train a fully supervised NER and relation extraction model on the same EHR data using standard architectures (e.g., BERT-based NER, graph-based relation extraction). Compare performance directly against the QA-based approach to validate whether transfer learning from automatically generated data truly matches supervised performance.

3. **Stress test on out-of-distribution questions**: Create a benchmark of 200+ questions that are semantically similar to training questions but semantically impossible to answer from the given EHR text. Evaluate the model's answerability judgment accuracy to verify that plausible answer training generalizes beyond the training distribution.