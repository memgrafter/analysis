---
ver: rpa2
title: Towards Understanding the Robustness of LLM-based Evaluations under Perturbations
arxiv_id: '2412.09269'
source_url: https://arxiv.org/abs/2412.09269
tags:
- score
- response
- metrics
- human
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the effectiveness of Google Gemini 1 as an\
  \ automated evaluator for subjective text quality metrics in summarization and dialog\
  \ tasks. The authors compare Gemini\u2019s performance against human judgments using\
  \ four prompting strategies: zero-shot, knowledge-prompt, few-shot, and chain-of-thought."
---

# Towards Understanding the Robustness of LLM-based Evaluations under Perturbations

## Quick Facts
- **arXiv ID**: 2412.09269
- **Source URL**: https://arxiv.org/abs/2412.09269
- **Reference count**: 11
- **Key outcome**: LLM-based evaluators show consistent performance across prompting strategies but poor alignment with human judgments under adversarial perturbations.

## Executive Summary
This study evaluates Google Gemini 1's effectiveness as an automated evaluator for subjective text quality metrics in summarization and dialog tasks. The authors compare Gemini's performance against human judgments using four prompting strategies and test robustness by introducing perturbations that invert rating scales. Results show that while Gemini demonstrates consistent evaluations across prompting strategies, its alignment with human judgments is limited and drops significantly under perturbed conditions. The model is particularly vulnerable to adversarial prompts, with matching scores between perturbed and knowledge-prompt evaluations ranging from 2.76% to 86.36% across metrics. Sentiment analysis of justifications reveals more negative evaluations under perturbed conditions, suggesting that while LLMs show promise as evaluators, they require substantial improvements to reliably assess subjective metrics.

## Method Summary
The study employs Google Gemini 1 as an automated evaluator for subjective text quality metrics in summarization and dialog tasks. Four prompting strategies are tested: zero-shot, knowledge-prompt, few-shot, and chain-of-thought. Human judgments serve as the gold standard for comparison. To assess robustness, the authors introduce perturbations that invert rating scales, creating adversarial conditions. The evaluation framework measures consistency across prompting strategies using Krippendorff's alpha and alignment with human judgments. Sentiment analysis is applied to the model's justifications to examine qualitative differences between perturbed and non-perturbed conditions.

## Key Results
- Gemini shows consistent performance across prompting strategies (Krippendorff's alpha: 0.49-0.78) but poor alignment with human judgments
- Under perturbed conditions, alignment drops significantly (alpha: -0.49 to -0.86), with matching scores ranging from 2.76% to 86.36%
- Sentiment analysis reveals more negative evaluations under perturbed conditions

## Why This Works (Mechanism)
The study reveals that LLM-based evaluators can maintain internal consistency across different prompting strategies while being highly sensitive to adversarial perturbations. The mechanism behind this vulnerability appears to be the model's strong reliance on explicit instructions in prompts, causing it to invert its judgments when rating scales are inverted. This suggests that LLMs lack robust grounding in the actual quality assessment task and instead follow surface-level instructions, making them susceptible to manipulation through prompt engineering.

## Foundational Learning

### Prompt Engineering
**Why needed**: Different prompting strategies can significantly affect LLM performance in evaluation tasks
**Quick check**: Test multiple prompting approaches (zero-shot, few-shot, chain-of-thought) on the same evaluation task

### Krippendorff's Alpha
**Why needed**: Measures inter-rater reliability and agreement between different evaluation sources
**Quick check**: Calculate alpha values between human and LLM judgments to quantify alignment

### Adversarial Prompting
**Why needed**: Tests model robustness by introducing deliberately misleading or contradictory instructions
**Quick check**: Invert rating scales in prompts to observe model's vulnerability to manipulation

## Architecture Onboarding

### Component Map
Evaluation Task -> Prompting Strategy -> LLM Response -> Sentiment Analysis -> Human Judgment Comparison

### Critical Path
The critical path flows from prompt generation through LLM evaluation to comparison with human judgments. The perturbation mechanism operates as an external intervention that tests the robustness of this path by modifying the input prompts while keeping other components constant.

### Design Tradeoffs
The study prioritizes robustness testing over ecological validity by using adversarial perturbations that may not reflect real-world conditions. This tradeoff reveals fundamental vulnerabilities but may overestimate practical risks in typical evaluation scenarios.

### Failure Signatures
Model vulnerability manifests as dramatic shifts in evaluation outcomes when rating scales are inverted, with some metrics showing near-complete reversal of judgments. The sentiment analysis reveals qualitative shifts toward more negative evaluations under perturbed conditions.

### First Experiments
1. Test Gemini's consistency across all four prompting strategies on a common evaluation task
2. Apply perturbation to a single metric and measure alignment drop with human judgments
3. Conduct sentiment analysis on justifications from both perturbed and non-perturbed evaluations

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations

- Evaluation limited to Google Gemini 1 model, making results unclear for other LLM-based evaluators
- Focus on only two tasks (dialog and summarization) and three subjective metrics limits generalizability
- Adversarial perturbations may not reflect real-world evaluation scenarios, potentially overestimating practical risks

## Confidence

- **High Confidence**: Consistent performance across prompting strategies (Krippendorff's alpha 0.49-0.78)
- **Medium Confidence**: Poor alignment with human judgments under perturbed conditions
- **Low Confidence**: Sentiment analysis conclusions about evaluation quality differences

## Next Checks

1. Replicate evaluation framework across multiple LLM models (GPT-4, Claude, etc.) to assess generalizability
2. Test robustness findings using naturally occurring perturbations in real-world evaluation scenarios
3. Conduct human validation studies on perturbed evaluations to verify LLM judgment alterations align with human assessment under similar conditions