---
ver: rpa2
title: 'Break the Chain: Large Language Models Can be Shortcut Reasoners'
arxiv_id: '2406.06580'
source_url: https://arxiv.org/abs/2406.06580
tags:
- reasoning
- arxiv
- language
- chain
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically evaluates Chain-of-Thought (CoT) reasoning
  in language models, identifying limitations like high token consumption and limited
  applicability. It proposes "break the chain" strategies and ShortcutQA dataset to
  assess heuristic shortcut reasoning.
---

# Break the Chain: Large Language Models Can be Shortcut Reasoners

## Quick Facts
- **arXiv ID:** 2406.06580
- **Source URL:** https://arxiv.org/abs/2406.06580
- **Reference count:** 17
- **Primary result:** "Break the chain" strategies reduce token usage and maintain or improve accuracy compared to traditional Chain-of-Thought methods.

## Executive Summary
This paper critically evaluates Chain-of-Thought (CoT) reasoning in language models, identifying limitations like high token consumption and limited applicability. It proposes "break the chain" strategies and the ShortcutQA dataset to assess heuristic shortcut reasoning. Experiments across various models and tasks show that CoT methods are not essential for performanceâ€”models maintain or improve accuracy with shortcut prompts while reducing token usage. The ShortcutQA dataset, designed for shortcut reasoning, further validates the robustness and efficiency of this approach. Results suggest that intuitive, efficient reasoning methods can outperform traditional step-by-step CoT, enhancing problem-solving in real-world applications.

## Method Summary
The paper introduces "break the chain" strategies that disrupt the step-by-step progression of Chain-of-Thought (CoT) prompting by disordering reasoning steps in in-context examples. It also proposes shortcut reasoning prompts that instruct models to bypass intermediate reasoning steps and jump directly to conclusions. The ShortcutQA dataset is developed to evaluate these heuristic shortcut reasoning methods. Experiments compare the performance of traditional CoT methods with "break the chain" strategies across various models, tasks, and model sizes, measuring both accuracy and token usage.

## Key Results
- Breaking the chain of thought in few-shot prompts does not significantly degrade model performance on arithmetic reasoning tasks.
- Shortcut reasoning prompts can reduce token usage while maintaining or improving accuracy compared to traditional CoT methods.
- Smaller models benefit more from CoT prompting than larger models, but "break the chain" strategies can narrow this performance gap.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Breaking the chain of thought in few-shot prompts does not significantly degrade model performance.
- **Mechanism:** Disordering the reasoning steps in in-context examples disrupts the explicit step-by-step progression expected in CoT, yet models can still arrive at correct answers.
- **Core assumption:** Language models can reconstruct or bypass the disrupted reasoning chain using their internal knowledge.
- **Evidence anchors:**
  - [abstract] "Our few-shot experiments reveal that Large Language Models (LLMs) are not adversely affected by disrupted Chain-of-Thought (CoT) demonstrations, casting doubts on the effectiveness of few-shot CoT methods."
  - [section 5] "In arithmetic reasoning, performance on the MultiArith dataset decreases slightly from 99.00% to 98.33% with 'breaking the chain', while in GSM8K, the decrease is marginal, from 74.60% to 74.22%."
- **Break condition:** If models relied entirely on the explicit chain order for reasoning, performance would drop significantly; instead, the minimal change indicates internal reasoning resilience.

### Mechanism 2
- **Claim:** Shortcut reasoning prompts can outperform traditional CoT by reducing token usage and maintaining or improving accuracy.
- **Mechanism:** By instructing models to bypass intermediate reasoning steps and jump directly to conclusions, shortcut prompts exploit the model's ability to use heuristics or shortcuts akin to human cognitive strategies.
- **Core assumption:** Large language models possess latent heuristic reasoning capabilities that can be activated through appropriate prompting.
- **Evidence anchors:**
  - [abstract] "Our comprehensive experiments... reveal that LMs maintain effective performance with 'break the chain' strategies."
  - [section 5] "Our evaluations span both OpenAI models and open-source models, showing consistent results across platforms."
  - [corpus] "System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts" suggests dynamic shortcut utilization in models.
- **Break condition:** If shortcut reasoning were not viable, models would fail to maintain accuracy; instead, they show robust performance, indicating effective shortcut utilization.

### Mechanism 3
- **Claim:** Smaller models benefit more from CoT prompting than larger models, narrowing the performance gap with "break the chain" strategies.
- **Mechanism:** As model size decreases, the structured guidance of CoT becomes more critical for performance; disrupting this guidance affects smaller models more, but "break the chain" strategies can mitigate this impact.
- **Core assumption:** The reasoning capabilities of smaller models are less developed, making them more dependent on explicit reasoning structures.
- **Evidence anchors:**
  - [section 5] "Smaller models typically experience more substantial enhancements with Chain-of-Thought (CoT) prompts compared to their larger counterparts. Notably, as model size increases, the efficacy of 'break the chain' strategies becomes more pronounced."
  - [corpus] "Mentor-KD: Making Small Language Models Better Multi-step Reasoners" indicates that smaller models benefit significantly from structured reasoning.
- **Break condition:** If model size did not affect CoT dependency, performance gaps would not narrow; instead, the data shows size-dependent effects.

## Foundational Learning

- **Concept:** Chain-of-Thought (CoT) prompting
  - **Why needed here:** Understanding CoT is essential to grasp why "breaking the chain" is significant and how it differs from traditional prompting methods.
  - **Quick check question:** What is the primary purpose of CoT prompting in language models?
    - **Answer:** To elicit step-by-step reasoning from language models to solve complex problems.

- **Concept:** Heuristic reasoning and shortcuts
  - **Why needed here:** The paper's core contribution is leveraging human-like heuristics and shortcuts; understanding these concepts is crucial to comprehend the proposed methods.
  - **Quick check question:** How do heuristic shortcuts differ from step-by-step reasoning in problem-solving?
    - **Answer:** Heuristic shortcuts bypass detailed procedural steps to quickly exploit reasoning clues, similar to intuitive leaps in human reasoning.

- **Concept:** Dataset benchmarking and evaluation
  - **Why needed here:** The introduction of the ShortcutQA dataset is central to the paper's methodology; understanding dataset benchmarking is necessary to appreciate the evaluation process.
  - **Quick check question:** Why is it important to have a specialized dataset like ShortcutQA for evaluating shortcut reasoning?
    - **Answer:** It allows for a focused assessment of a model's ability to employ heuristic shortcuts, which is not captured by traditional reasoning datasets.

## Architecture Onboarding

- **Component map:** Language Model -> Prompting Interface -> Evaluation Pipeline -> Answer Extraction -> Accuracy Calculation

- **Critical path:**
  1. Input question and prompt into the LM.
  2. LM generates a reasoned response and answer.
  3. Concatenate the original question, prompt, and generated response.
  4. Apply answer extraction prompt to retrieve the final answer.
  5. Compare the extracted answer with the ground truth to compute accuracy.

- **Design tradeoffs:**
  - **Token Efficiency vs. Accuracy:** Shortcut prompts reduce token usage but must maintain or improve accuracy.
  - **Model Dependency:** Larger models may rely less on explicit CoT, while smaller models benefit more from structured reasoning.
  - **Generalizability:** Methods should work across different model architectures and sizes.

- **Failure signatures:**
  - Significant drop in accuracy when "breaking the chain" indicates models heavily rely on explicit step order.
  - Increased token usage without accuracy improvement suggests ineffective shortcut utilization.
  - Inconsistent performance across different model sizes may indicate size-dependent limitations.

- **First 3 experiments:**
  1. **Few-shot CoT vs. Break the Chain:** Compare performance on MultiArith and GSM8K datasets to assess the impact of disrupting in-context examples.
  2. **Zero-shot Shortcut Reasoning:** Test shortcut prompts on arithmetic and commonsense reasoning tasks to evaluate efficiency gains.
  3. **Model Size Analysis:** Run experiments with Qwen models of varying sizes to observe the relationship between model size and CoT dependency.

## Open Questions the Paper Calls Out
None explicitly stated in the provided text.

## Limitations
- The study primarily focuses on arithmetic and commonsense reasoning tasks, leaving the generalizability of "break the chain" strategies to other domains uncertain.
- The effectiveness of shortcut reasoning prompts is tested on a limited set of models and tasks, raising questions about broader applicability.
- The claim that smaller models benefit more from CoT prompting is based on a relatively small sample of model sizes, limiting the strength of this conclusion.

## Confidence
- **High:** The minimal performance degradation when breaking the chain of thought in few-shot prompts.
- **Medium:** The effectiveness of shortcut reasoning prompts in reducing token usage while maintaining accuracy.
- **Low:** The claim that smaller models benefit more from CoT prompting than larger models.

## Next Checks
1. Test the "break the chain" strategies on a broader range of datasets, including those outside of arithmetic reasoning, to assess generalizability.
2. Evaluate the effectiveness of shortcut reasoning prompts across a wider variety of model architectures and sizes to determine robustness.
3. Conduct experiments with a larger sample of model sizes to strengthen the claim about the relationship between model size and CoT dependency.