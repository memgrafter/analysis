---
ver: rpa2
title: Hierarchical Delay Attribution Classification using Unstructured Text in Train
  Management Systems
arxiv_id: '2402.04108'
source_url: https://arxiv.org/abs/2402.04108
tags:
- delay
- attribution
- level
- code
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the automation of train delay attribution
  classification using machine learning on unstructured text reports from the Swedish
  Transport Administration. The problem involves assigning hierarchical delay codes
  (up to 200 possible codes) to train delays based on free-text descriptions.
---

# Hierarchical Delay Attribution Classification using Unstructured Text in Train Management Systems

## Quick Facts
- arXiv ID: 2402.04108
- Source URL: https://arxiv.org/abs/2402.04108
- Authors: Anton Borg; Per Lingvall; Martin Svensson
- Reference count: 40
- Primary result: Hierarchical classification achieves F1-scores of 0.876 and 0.737 at levels 2 and 3, outperforming flat classification

## Executive Summary
This study investigates the automation of train delay attribution classification using machine learning on unstructured text reports from the Swedish Transport Administration. The research addresses the challenge of assigning hierarchical delay codes (up to 200 possible codes) to train delays based on free-text descriptions. Two approaches were evaluated: a flat classification and a hierarchical classification method, both using Random Forest and SVM algorithms. The hierarchical approach significantly outperformed the flat approach, achieving F1-scores of 0.876 and 0.737 at levels 2 and 3, respectively, compared to 0.797 and 0.723 for the flat approach. The models achieved an F1-score of 0.889-0.901 for level 1 classification, approaching but not matching the performance of human operators (0.992).

## Method Summary
The study used unstructured text reports from Swedish Transport Administration containing free-text descriptions of train delays and corresponding delay attribution codes collected over 30 days in May-June 2023 (34,901 instances, reduced to 21,484 after preprocessing). Two approaches were evaluated: flat classification and hierarchical classification, both using Random Forest and SVM algorithms. Text was transformed using TF-IDF with 1-3 grams and top 1000 features. The hierarchical approach trained separate models for each parent code, while the flat approach treated all codes as a single classification problem. Models were evaluated using F1-score with 10-fold cross-conformal validation, comparing against baseline uniform classifier and manual classification performance.

## Key Results
- Hierarchical approach achieved F1-scores of 0.876 and 0.737 at levels 2 and 3, significantly outperforming flat approach (0.797 and 0.723)
- Level 1 classification achieved F1-scores of 0.889-0.901, approaching human operator performance (0.992)
- Hierarchical models consistently outperformed flat models due to reduced solution space (approximately 6 possible classes vs 43)
- Certain delay codes (particularly J-based codes) showed lower performance, indicating model limitations

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical classification outperforms flat classification by reducing the effective number of classes at each decision node. By structuring the classification into levels (D/F/I/J/O → specific subcategories → detailed codes), the model only needs to choose from a small subset of classes at each step, improving accuracy and reducing confusion between similar codes. This assumes the hierarchical structure reflects genuine semantic relationships between delay types.

### Mechanism 2
TF-IDF with n-grams effectively captures domain-specific terminology patterns in Swedish train delay descriptions. The combination of unigrams, bigrams, and trigrams captures both individual terms and common phrases characteristic of train delay descriptions, enabling the model to distinguish between different types of delays. This assumes the unstructured text contains sufficient discriminative patterns for classification.

### Mechanism 3
Conformal prediction provides reliable uncertainty estimates that improve classification reliability. By estimating the model's confidence and removing uncertain predictions, conformal prediction reduces false positives and improves overall system reliability for decision support. This assumes the model's calibration set adequately represents the uncertainty distribution.

## Foundational Learning

- Concept: Hierarchical classification structure
  - Why needed here: The 200+ delay attribution codes are organized in a 3-level hierarchy, and this structure can be leveraged to improve classification accuracy by reducing the decision space at each level
  - Quick check question: How many top-level codes exist in the Swedish delay attribution hierarchy?

- Concept: Text feature engineering with TF-IDF
  - Why needed here: Unstructured text descriptions need to be transformed into numerical features that machine learning algorithms can process, and TF-IDF with n-grams captures both individual terms and phrase patterns
  - Quick check question: What are the three types of n-grams used in the feature extraction process?

- Concept: Statistical significance testing
  - Why needed here: To determine whether performance differences between algorithms and approaches are meaningful rather than due to random variation
  - Quick check question: Which test was used to compare hierarchical vs flat approaches for level 2 and 3 classification?

## Architecture Onboarding

- Component map: Text preprocessing → TF-IDF feature extraction → Model training (Random Forest/SVM) → Conformal prediction calibration → Evaluation (F1-score) → Hierarchical decision path
- Critical path: Text preprocessing → TF-IDF → Model training/calibration → Prediction with uncertainty estimation
- Design tradeoffs: Hierarchical approach reduces solution space but increases complexity; TF-IDF is simple but may miss semantic relationships that deep learning would capture
- Failure signatures: Low F1-scores on specific codes (e.g., J-based codes), high standard deviation in hierarchical approach, significant performance drop between day 0 and day 10 for certain codes
- First 3 experiments:
  1. Run flat classification with TF-IDF on all data and compare F1-scores to hierarchical approach
  2. Test different n-gram ranges (1-2, 1-3, 2-3) to see impact on classification accuracy
  3. Implement and evaluate conformal prediction with different confidence thresholds

## Open Questions the Paper Calls Out

### Open Question 1
How does the inclusion of additional contextual information alongside numeric train identifiers affect classification accuracy? The paper mentions that instances containing only numeric values (train identifiers) pose challenges for the models, as they cannot interpret the meaning of these numbers. The authors suggest augmenting the unstructured text with supplementary contextual information as a potential solution, but do not provide experimental results on how this affects accuracy.

### Open Question 2
How do the classification results change when using more advanced language representation techniques, such as BERT or GPT, compared to TF-IDF? The authors acknowledge that more advanced techniques could potentially improve the model's understanding of contextual information, but do not experiment with or compare these results.

### Open Question 3
How does the classification performance vary across different railway companies or regions in Sweden? While the dataset contains events from the whole of Sweden, the paper does not analyze the classification performance across different railway companies or regions.

## Limitations

- Dataset restricted to a single month (May-June 2023), potentially missing seasonal variations in delay patterns
- Significant performance gap remains between automated classification (F1 0.889-0.901) and manual classification (F1 0.992)
- Models struggle with certain code categories (J-based codes), suggesting limitations in handling specific delay types

## Confidence

- **High Confidence**: Hierarchical approach outperforming flat classification (F1-scores of 0.876 vs 0.797 at level 2, and 0.737 vs 0.723 at level 3) - directly supported by experimental results with statistical significance testing
- **Medium Confidence**: TF-IDF feature extraction method's effectiveness for Swedish train delay descriptions - while results show good performance, limitations for capturing semantic relationships are acknowledged
- **Medium Confidence**: Feasibility of machine learning as decision support tool - demonstrated through performance metrics, though gap to human-level performance suggests it cannot fully replace operators

## Next Checks

1. Test model performance on a different time period (e.g., winter months) to assess seasonal robustness and generalizability beyond the May-June dataset
2. Implement alternative feature extraction methods (such as word embeddings or transformer-based approaches) to evaluate whether semantic relationships can be better captured than with TF-IDF
3. Conduct ablation studies removing conformal prediction to quantify its contribution to uncertainty estimation and overall classification performance