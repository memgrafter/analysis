---
ver: rpa2
title: 'Framework for Curating Speech Datasets and Evaluating ASR Systems: A Case
  Study for Polish'
arxiv_id: '2408.00005'
source_url: https://arxiv.org/abs/2408.00005
tags:
- speech
- dataset
- datasets
- available
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The research presents a comprehensive framework for curating speech
  datasets and evaluating ASR systems, addressing the challenge of underutilized speech
  datasets in Polish. By surveying and cataloging over 24 datasets, the study created
  a benchmark dataset from 24 openly available sources, evaluating 25 ASR system-model
  combinations.
---

# Framework for Curating Speech Datasets and Evaluating ASR Systems: A Case Study for Polish

## Quick Facts
- arXiv ID: 2408.00005
- Source URL: https://arxiv.org/abs/2408.00005
- Reference count: 39
- One-line primary result: A comprehensive framework for curating speech datasets and evaluating ASR systems, demonstrating that normalization techniques can reduce error rates by up to 15.22 percentage points in Polish ASR evaluation.

## Executive Summary
This research presents a comprehensive framework for curating speech datasets and evaluating ASR systems, addressing the challenge of underutilized speech datasets in Polish. By surveying and cataloging over 24 datasets, the study created a benchmark dataset from 24 openly available sources, evaluating 25 ASR system-model combinations. The framework supports various datasets, systems, and metrics, ensuring consistent ASR evaluation. Key findings include significant variations in system performance across datasets, models, and speaker demographics, with normalization techniques reducing error rates by up to 15.22 percentage points. The results, available on interactive dashboards, provide valuable insights for researchers and industry professionals, highlighting the importance of normalization and metadata utilization in ASR evaluation.

## Method Summary
The research presents a framework for curating speech datasets and evaluating ASR systems, focusing on the Polish language. The method involves surveying over 24 Polish speech datasets, curating a benchmark dataset from 24 openly available sources, and evaluating 25 ASR system-model combinations. The framework supports various datasets, systems, and metrics, ensuring consistent ASR evaluation. Key steps include dataset curation with standardized splits and metadata extraction, normalization techniques to reduce error rates, and evaluation using multiple metrics such as WER, MER, and CER. The study also analyzes speaker demographics and their impact on system performance, providing insights into bias and robustness.

## Key Results
- Normalization techniques reduce error rates by up to 15.22 percentage points by addressing spelling and formatting variations in reference transcripts.
- Significant variations in system performance across datasets, models, and speaker demographics, with metadata revealing systematic performance differences.
- Model size correlates with ASR accuracy for Whisper models, but efficiency gains are possible with smaller architectures like NeMo models.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Normalization techniques reduce error rates by up to 15.22 percentage points by addressing spelling and formatting variations in reference transcripts.
- **Mechanism**: Reference and hypothesis text normalization (e.g., lowercasing, punctuation removal, blank removal, lexicon-based normalization) standardizes the textual representation before computing WER/MER/CER, reducing superficial mismatches that inflate error metrics.
- **Core assumption**: The ASR system output and reference transcriptions use inconsistent casing, punctuation, or whitespace, which are irrelevant to semantic correctness but affect lexical metric calculations.
- **Evidence anchors**:
  - [abstract]: "normalization techniques reducing error rates by up to 15.22 percentage points"
  - [section 3.1]: "Normalization techniques resulted in significant reductions in error rates for all types of metrics (SER, WER, MER, CER). Applying all methods reduced WER by 16.07 p.p. for the PELCRA dataset and 15.52 p.p. for the BIGOS dataset"
- **Break condition**: If the ASR system output is already normalized, or if normalization removes content that affects meaning (e.g., punctuation that disambiguates words), performance gains may be reduced or reversed.

### Mechanism 2
- **Claim**: Speaker metadata (age, gender) reveals systematic performance differences, enabling targeted model improvement.
- **Mechanism**: By partitioning evaluation data by speaker metadata, the framework identifies subsets where ASR systems perform poorly, suggesting bias or domain mismatch.
- **Core assumption**: Speaker demographics correlate with acoustic or linguistic features (accent, speaking rate, vocabulary) that affect recognition accuracy.
- **Evidence anchors**:
  - [abstract]: "significant variations across different systems, datasets, and speaker demographics"
  - [section 3.5]: "Figure 7 shows the standard deviation of WER in all age groups. Lower values indicate a more consistent accuracy for all groups"
- **Break condition**: If metadata is incomplete or inaccurate, or if demographic factors do not correlate with acoustic characteristics, the insights will be unreliable.

### Mechanism 3
- **Claim**: Model size correlates with ASR accuracy for Whisper models, but efficiency gains are possible with smaller architectures (e.g., NeMo models).
- **Mechanism**: Larger models have more parameters to capture linguistic and acoustic variability, leading to lower WER. However, certain architectures (e.g., NeMo QuartzNet) achieve competitive performance with fewer parameters due to optimized design.
- **Core assumption**: Model capacity (parameter count) is a primary driver of ASR accuracy, but architectural efficiency can offset raw size.
- **Evidence anchors**:
  - [abstract]: "significant variations across different systems, datasets, and speaker demographics"
  - [section 4.1.3]: "Figure 4a shows that as model size increases, WER decreases, indicating better performance. This trend holds for models of the same type, e.g., whisper models"
- **Break condition**: If the dataset is too small or homogeneous, larger models may overfit, negating the expected accuracy gain.

## Foundational Learning

- **Concept**: Speech signal preprocessing (e.g., sampling rate, bit depth, amplitude normalization)
  - **Why needed here**: Ensures consistent input to ASR systems and accurate comparison across datasets with different recording characteristics.
  - **Quick check question**: What sampling rate and bit depth were used to standardize the BIGOS dataset audio files?
    - **Answer**: 16 kHz sampling rate and 16 bits per sample (see section 2.3.3, audio file curation).

- **Concept**: Evaluation metrics for ASR (SER, WER, MER, CER, WIL)
  - **Why needed here**: Different metrics capture different aspects of recognition quality; normalization impact and bias analysis depend on metric choice.
  - **Quick check question**: Which metric showed the largest reduction due to normalization?
    - **Answer**: WER, reduced by up to 16.07 percentage points (section 3.1).

- **Concept**: Metadata curation and speaker profiling
  - **Why needed here**: Enables nuanced analysis (e.g., by age, gender, speech rate) and identifies bias or robustness issues.
  - **Quick check question**: What metadata fields are included in the BIGOS utterance data object?
    - **Answer**: Speaker ID, gender, age, speech rate (words/sec, chars/sec), utterance length, and more (Table 7).

## Architecture Onboarding

- **Component map**: Dataset survey tool -> Curation pipeline (splits, normalization, metadata) -> Evaluation engine (metrics, normalization methods) -> Dashboard (interactive results) -> Open repositories (datasets, tools, results)
- **Critical path**:
  1. Survey and catalog available datasets (keyword search, manual inspection).
  2. Curate benchmark dataset (splits, normalization, metadata extraction).
  3. Evaluate ASR systems across datasets and metrics.
  4. Publish results and tools for community use.
- **Design tradeoffs**:
  - Open vs. proprietary datasets: impacts reproducibility and dataset diversity.
  - Manual vs. automatic metadata extraction: trade-off between accuracy and scalability.
  - Normalization scope: aggressive normalization may improve metrics but risk semantic loss.
- **Failure signatures**:
  - Inconsistent metadata -> biased or misleading subgroup analyses.
  - Missing normalization -> inflated error rates, obscuring true system performance.
  - Incomplete dataset splits -> data leakage, overfitting, or unreliable benchmarks.
- **First 3 experiments**:
  1. **Baseline evaluation**: Run all ASR systems on BIGOS dataset without normalization; record WER/MER/CER for each system-model pair.
  2. **Normalization impact**: Apply each normalization method separately and in combination; compare error rate reductions.
  3. **Metadata analysis**: Partition BIGOS by speaker age and gender; compute system accuracy per group; identify significant performance gaps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific normalization techniques affect the performance of ASR systems across different languages and dialects?
- Basis in paper: [explicit] The paper demonstrates that normalization techniques significantly reduce error rates, with up to 15.22 percentage points reduction in WER for Polish.
- Why unresolved: The study focuses on Polish, but it is unclear how these techniques would perform with other languages, especially those with different phonetic or orthographic characteristics.
- What evidence would resolve it: Conducting similar evaluations across multiple languages and dialects, analyzing the impact of normalization on error rates in each case.

### Open Question 2
- Question: What are the critical factors that influence the robustness of ASR systems to speech rate variations?
- Basis in paper: [explicit] The paper identifies a correlation between speech rate and WER, noting that higher rates increase WER, indicating limited robustness for faster speech.
- Why unresolved: While the correlation is noted, the underlying factors contributing to this limitation are not fully explored, such as model architecture or training data diversity.
- What evidence would resolve it: Analyzing the performance of different ASR models with varied training data and architectures across a wide range of speech rates.

### Open Question 3
- Question: How can ASR evaluation frameworks be enhanced to incorporate semantic metrics and manual transcription verification?
- Basis in paper: [inferred] The paper suggests future research should include manual transcriptions and annotations to assess test data quality and integrating semantically informed metrics.
- Why unresolved: The current framework lacks these capabilities, which could provide deeper insights into ASR performance and error criticality.
- What evidence would resolve it: Developing and testing an enhanced evaluation framework that includes semantic metrics and manual verification, measuring its impact on ASR system assessment.

## Limitations
- Dataset Coverage: The study relied primarily on openly available sources, potentially excluding valuable proprietary datasets.
- Metadata Quality: Inconsistent or missing metadata could affect the reliability of demographic-based performance analyses.
- Normalization Impact Boundaries: The study does not fully explore the threshold where normalization might remove semantically meaningful information.

## Confidence
- **High Confidence**: Normalization techniques reduce WER by up to 15.22 percentage points. Supported by direct quantitative evidence from multiple datasets (PELCRA and BIGOS).
- **Medium Confidence**: Speaker demographic metadata reveals systematic performance differences. While statistically supported, metadata quality and completeness across datasets introduce uncertainty.
- **Medium Confidence**: Model size correlates with ASR accuracy for Whisper models. Evidence shows clear trends, but architectural efficiency gains (e.g., NeMo) suggest the relationship is not absolute.

## Next Checks
1. **Metadata Completeness Audit**: Verify the completeness and accuracy of speaker demographic metadata across all datasets used in the BIGOS benchmark. Identify datasets with missing or inconsistent metadata and assess the impact on demographic-based performance analyses.
2. **Normalization Boundary Test**: Systematically evaluate the impact of aggressive normalization (including punctuation removal and lexicon-based normalization) on datasets where punctuation carries semantic meaning. Measure changes in both error rates and semantic preservation.
3. **Cross-Platform Reproducibility**: Replicate the evaluation framework using a different set of ASR systems (e.g., adding Kaldi-based systems or newer Whisper variants) on the same BIGOS dataset to verify the consistency of performance trends across different evaluation contexts.