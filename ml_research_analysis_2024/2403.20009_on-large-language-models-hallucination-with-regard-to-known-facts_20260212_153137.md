---
ver: rpa2
title: On Large Language Models' Hallucination with Regard to Known Facts
arxiv_id: '2403.20009'
source_url: https://arxiv.org/abs/2403.20009
tags:
- subject
- knowledge
- language
- output
- lens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why large language models (LLMs) sometimes
  "hallucinate" when recalling facts they should know. The authors identify this as
  a failure in knowledge recall rather than knowledge absence, by comparing model
  behavior on correct and incorrect answers to the same factoid question.
---

# On Large Language Models' Hallucination with Regard to Known Facts

## Quick Facts
- arXiv ID: 2403.20009
- Source URL: https://arxiv.org/abs/2403.20009
- Reference count: 9
- Primary result: LLMs hallucinate on known facts due to failed knowledge recall, not knowledge absence

## Executive Summary
This paper investigates why large language models sometimes generate incorrect outputs when they possess the correct knowledge. The authors demonstrate that hallucinations occur when models fail to retrieve known facts during inference, rather than lacking the knowledge entirely. Through residual stream analysis using Logit and Tuned Lens techniques, they observe distinct probability dynamics between correct and hallucinated outputs. The study reveals that MLP modules contribute more significantly to incorrect outputs than attention modules, and provides a framework for detecting hallucinations with 88% accuracy using output token dynamics as features.

## Method Summary
The authors analyze Llama2-7B-chat's behavior on a modified COUNTER FACT dataset containing over 30k factoid questions. They employ Logit Lens and Tuned Lens techniques to observe output token probabilities across transformer layers, conduct module ablation studies to identify contributions of attention and MLP modules, and train an SVM classifier using dynamic probability curves as features. The analysis focuses on first-token outputs using greedy decoding to isolate knowledge recall dynamics from decoding strategy effects.

## Key Results
- Hallucinations occur when correct answers rarely achieve high probability ranks during inference (30% vs 78% for correct outputs)
- MLP modules contribute more to incorrect outputs than attention modules by suppressing correct token probabilities and promoting erroneous outputs
- Output token probability dynamics differ significantly between correct and hallucinated cases, with correct answers showing sharp increases in later layers
- An SVM classifier using these dynamic patterns achieves 88% accuracy in detecting hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hallucinations arise from failed knowledge recall rather than knowledge absence
- Mechanism: When the model fails to extract correct knowledge during inference, incorrect tokens begin competing early in the residual stream and are reinforced by MLP modules, eventually dominating the output
- Core assumption: The model has actually memorized the correct facts but fails to retrieve them during inference
- Evidence anchors:
  - [abstract] "We investigate the phenomenon of LLMs possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics"
  - [section] "Known fact hallucination arises from failed knowledge recall. Our analysis shows that when model generates incorrect outputs, on average the correct answers pop to the top rank with a 30% frequency across the layers during inference, which is significantly lower than the 78% frequency when the output is correct"
  - [corpus] Found 25 related papers with average neighbor FMR=0.517, suggesting this is a well-studied but not fully understood phenomenon
- Break condition: If the model genuinely lacks the knowledge (not just fails to recall it), this mechanism would not apply

### Mechanism 2
- Claim: MLP modules contribute more to incorrect outputs than attention modules
- Mechanism: While attention modules perform semantic parsing, MLP modules both suppress correct answer probabilities and actively promote incorrect token decoding in later layers
- Core assumption: MLP layers have disproportionate influence on final output decisions compared to attention layers
- Evidence anchors:
  - [section] "MLP modules have a more significant impact on incorrect outputs than attention modules. In contrast to attention modules, the Multi-Layer Perception (MLP) not only diminishes the probability of the correct answer when producing incorrect outputs but also contributes to generating erroneous outputs in the final decoding layer"
  - [section] "The MLP exerts a stronger inhibitory effect towards the end of the model, particularly contributing significantly to erroneous output decoding"
- Break condition: If attention mechanisms were found to be equally or more responsible for incorrect outputs

### Mechanism 3
- Claim: Output token probability dynamics differ between correct and hallucinated cases
- Mechanism: Correct answers show sharp probability increases in later layers (information extraction point), while hallucinated outputs maintain low probabilities throughout or show early, sustained probability that bypasses proper extraction
- Core assumption: The temporal evolution of token probabilities in the residual stream can distinguish between correct and incorrect outputs
- Evidence anchors:
  - [abstract] "In hallucinated cases, the output token's information rarely demonstrates abrupt increases and consistent superiority in the later stages of the model"
  - [section] "In the residual stream generating correct outputs, the information of the output token shows a steep increase in the middle to later layers, while erroneous outputs tend to speculate from shallower layers"
  - [section] "Using these dynamic patterns as features, the authors train an SVM classifier that achieves 88% accuracy in detecting hallucinations"
- Break condition: If probability dynamics prove unreliable across different model architectures or tasks

## Foundational Learning

- Concept: Residual stream analysis
  - Why needed here: The paper relies on observing how token probabilities evolve through transformer layers to understand hallucination mechanisms
  - Quick check question: Can you explain what the residual stream represents in a transformer model and how it changes across layers?

- Concept: Logit and Tuned Lens techniques
  - Why needed here: These methods map hidden states to vocabulary space, allowing observation of token probability dynamics at each layer
  - Quick check question: What is the key difference between Logit Lens and Tuned Lens, and why might one be preferred over the other for this analysis?

- Concept: Module ablation studies
  - Why needed here: Understanding whether attention or MLP modules contribute more to hallucinations requires selectively disabling components and observing effects
  - Quick check question: How would you design an ablation study to test whether attention or MLP modules are more responsible for hallucinations?

## Architecture Onboarding

- Component map:
  Input embeddings → L transformer blocks (each with attention + MLP) → Unembedding matrix → Output probabilities
- Critical path:
  Token embedding → Layer-by-layer transformation through attention and MLP → Final unembedding → Output decoding
  For hallucination detection: Track specific token probabilities through residual stream using lens techniques
- Design tradeoffs:
  Using greedy decoding vs. sampling strategies (fixed for consistency)
  Focusing on first token output vs. full sequence (simplified analysis)
  Logit Lens simplicity vs. Tuned Lens accuracy (both used for comparison)
- Failure signatures:
  Low probability of correct answers throughout inference
  Early and sustained probability for incorrect tokens
  MLP suppression of correct tokens in later layers
  Absence of "information extraction point" for correct answers
- First 3 experiments:
  1. Reproduce the probability dynamics comparison between correct and hallucinated cases using Logit and Tuned Lens on a small dataset
  2. Conduct module ablation to verify MLP vs. attention contributions by zeroing out mlp_t or attn_t at each layer and measuring output changes
  3. Train a simple classifier (e.g., logistic regression) on probability curves to test hallucination detection capability on a held-out set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different decoding strategies (e.g., beam search, sampling) affect the observed hallucination patterns and their detection accuracy?
- Basis in paper: [inferred] The paper uses greedy decoding and explicitly mentions that "we fix the model's decoding strategy as greedy" to avoid decoding strategy influence on analysis.
- Why unresolved: The authors chose greedy decoding to isolate the effects of knowledge recall dynamics, but this leaves open how other common decoding strategies might influence the patterns they observe.
- What evidence would resolve it: Systematic comparison of output token dynamics and hallucination detection accuracy across multiple decoding strategies using the same experimental setup.

### Open Question 2
- Question: Can the hallucination detection approach generalize to other knowledge structures beyond simple triplet completion, such as multi-hop reasoning or open-domain QA?
- Basis in paper: [explicit] The authors acknowledge that their analysis "relies on triplet knowledge datasets, potentially limiting the generalizability of our findings to other types of knowledge structures or domains."
- Why unresolved: The study focuses specifically on object completion in triplet knowledge, and the authors note that "the inference dynamics observed in subject parsing and information extraction might differ concerning alternate data representations."
- What evidence would resolve it: Testing the SVM classifier on datasets with different knowledge structures and reasoning complexity while comparing detection accuracy.

### Open Question 3
- Question: What specific architectural modifications to transformer models could reduce hallucinations in known facts while preserving general language capabilities?
- Basis in paper: [inferred] The analysis identifies MLP modules as having more significant impact on incorrect outputs than attention modules, suggesting potential architectural intervention points.
- Why unresolved: While the paper identifies module-level contributions to hallucinations, it does not explore how to modify these modules to reduce hallucinations without degrading overall model performance.
- What evidence would resolve it: Experiments with modified transformer architectures that adjust MLP/attention module behaviors during knowledge recall, measuring both hallucination reduction and general capability preservation.

## Limitations
- Analysis is based on a relatively small dataset of 30k examples, potentially missing diverse hallucination scenarios
- Study focuses specifically on factoid questions ending with answer objects, not capturing hallucinations in complex reasoning tasks
- Logit Lens and Tuned Lens methods may not capture all relevant information about residual stream dynamics for multi-token outputs

## Confidence

**High Confidence**: The observation that correct answers show sharp probability increases in later layers while hallucinated outputs maintain low probabilities throughout - this finding is directly observable through the lens techniques and has strong empirical support.

**Medium Confidence**: The claim that MLP modules contribute more to incorrect outputs than attention modules - while the ablation studies provide evidence, the exact quantification of module contributions may vary with different model architectures or tasks.

**Medium Confidence**: The overall framework for understanding hallucinations as failed knowledge recall rather than knowledge absence - this interpretation is plausible but requires further validation across different model families and knowledge types.

## Next Checks
1. Test whether the observed probability dynamics and module contributions hold across different transformer architectures (e.g., BERT, GPT-3, PaLM)
2. Validate hallucination detection accuracy on out-of-domain knowledge structures beyond simple triplet completion
3. Experiment with modified transformer architectures that adjust MLP/attention module behaviors during knowledge recall to measure hallucination reduction effects