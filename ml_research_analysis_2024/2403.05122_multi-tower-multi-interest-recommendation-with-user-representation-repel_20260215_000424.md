---
ver: rpa2
title: Multi-Tower Multi-Interest Recommendation with User Representation Repel
arxiv_id: '2403.05122'
source_url: https://arxiv.org/abs/2403.05122
tags:
- user
- multi-interest
- learning
- item
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of representing multiple user
  interests in sequential recommendation systems, where existing methods struggle
  with discrepancies between training and deployment objectives, limited access to
  item information, and difficulties in industrial adoption. To tackle these issues,
  the authors propose a novel Multi-Tower Multi-Interest Learning (MTMI) framework
  that employs multiple user towers and an item tower, allowing for independent generation
  of user representations and item representations.
---

# Multi-Tower Multi-Interest Recommendation with User Representation Repel

## Quick Facts
- arXiv ID: 2403.05122
- Source URL: https://arxiv.org/abs/2403.05122
- Reference count: 40
- Primary result: MTMI significantly outperforms existing multi-interest learning methods on three large-scale datasets

## Executive Summary
This paper introduces Multi-Tower Multi-Interest (MTMI), a novel framework for sequential recommendation that addresses key challenges in multi-interest representation learning. The authors propose a two-tower architecture with multiple user towers and one item tower that independently generates user and item representations, aligning training and deployment objectives. The framework employs an Inverse Distance Weighted Loss (IDW Loss) to measure weighted distances between user representations and target items, combined with User Representation Repel (URR) to encourage diversity. Experimental results on Amazon Books, Gowalla, and Retail Rocket datasets demonstrate significant improvements over state-of-the-art multi-interest learning methods in recommendation accuracy and generalizability.

## Method Summary
MTMI is a multi-tower multi-interest framework that generates multiple user representations through independent user towers and item representations through a single item tower. The model employs an Inverse Distance Weighted Loss to measure the weighted distance between user representations and target item representations, with closer representations receiving higher weights. User Representation Repel explicitly pushes different user representations apart to encourage diversity. The framework is designed to align training and deployment objectives by independently generating user and item representations, making it suitable for industrial deployment using k-nearest neighbors (kNN) algorithms for candidate retrieval.

## Key Results
- MTMI outperforms existing multi-interest learning methods (MIND, ComiRec) on three large-scale public datasets
- Significant improvements in Recall, Hit Rate, and NDCG@20/50 metrics across all datasets
- User Representation Repel and IDW Loss contribute to improved recommendation performance and representation diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-tower architecture aligns training and deployment objectives by independently generating user and item representations.
- Mechanism: During training, multiple user towers generate diverse user representations while a single item tower generates target item representations. These are compared using IDW Loss that pulls user representations toward the target item while pushing them apart from each other.
- Core assumption: Independent generation of user and item representations in training better matches the deployment phase using kNN.
- Evidence anchors: [abstract] "We address these challenges by proposing a novel multi-tower multi-interest framework with user representation repel."
- Break condition: If independent generation during training doesn't lead to better deployment performance with kNN.

### Mechanism 2
- Claim: IDW Loss improves learning by weighting distance between user representations and target item based on inverse distance.
- Mechanism: IDW Loss calculates weighted distance between each user representation and target item, with closer representations receiving higher weights, encouraging focus on most relevant representation.
- Core assumption: Weighting based on inverse distance leads to better learning than uniform weighting or label-aware attention.
- Evidence anchors: [section] "We use an Inverse Distance Weighted Loss to assess the weighted distance between the generated user representations and the target item representation."
- Break condition: If IDW Loss doesn't improve performance compared to other loss functions.

### Mechanism 3
- Claim: User Representation Repel improves diversity of user representations by explicitly pushing them apart.
- Mechanism: URR adds a term to the loss function that maximizes distance between different user representations, encouraging generation of diverse and distinct representations for different user interests.
- Core assumption: Explicitly pushing user representations apart improves their diversity and leads to better recommendation performance.
- Evidence anchors: [abstract] "We address these challenges by proposing a novel multi-tower multi-interest framework with user representation repel."
- Break condition: If URR doesn't lead to improved performance or negatively impacts ability to capture user interests.

## Foundational Learning

- Concept: Metric Learning
  - Why needed here: MTMI uses metric learning techniques like triplet loss and IDW Loss to measure similarity between user and item representations.
  - Quick check question: What is the difference between contrastive loss and triplet loss in metric learning?

- Concept: Multi-Interest Learning
  - Why needed here: MTMI is a multi-interest learning framework that generates multiple user representations to capture diverse user interests.
  - Quick check question: How does multi-interest learning differ from single-user representation models in sequential recommendation?

- Concept: Two-Tower Architecture
  - Why needed here: MTMI is an extension of the two-tower architecture commonly used in industrial recommendation systems.
  - Quick check question: What are the advantages of the two-tower architecture over single-tower models in candidate matching?

## Architecture Onboarding

- Component map: Embedding Layer -> Representation Extraction (Towers) -> IDW Loss -> User Representation Repel
- Critical path: User behavior sequences and item features are embedded, processed through multiple user towers and one item tower, compared using IDW Loss, and diversity is enforced through User Representation Repel
- Design tradeoffs:
  - More user towers increase model capacity but also increase computational cost
  - IDW Loss and URR add complexity to loss function but may improve performance
  - Two-tower architecture simplifies deployment but may limit interactions between user and item information
- Failure signatures:
  - Poor performance on user representation diversity metrics (e.g., intra-user cosine similarity)
  - Overfitting indicated by large gap between training and validation performance
  - Slow convergence or unstable training due to complex loss function or large number of parameters
- First 3 experiments:
  1. Compare MTMI with single-tower baselines (MIND, ComiRec) on MovieLens 100k to validate multi-tower design
  2. Ablation study to assess impact of IDW Loss and URR on model performance
  3. Hyperparameter tuning to find optimal number of user towers (K), distance scaling factor (alpha), and divergence parameter (beta)

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but suggests potential future work in inter-tower communication mechanisms to balance diversity and coherence of user representations.

## Limitations
- Effectiveness relies heavily on quality of user behavior sequences and item features
- Computational complexity analysis is incomplete, particularly impact of increasing user towers on training and inference time
- Generalization to other recommendation tasks (cold-start, context-aware) is unexplored

## Confidence
- High: Multi-tower architecture and use of kNN for candidate retrieval are well-established industrial techniques
- Medium: IDW Loss and User Representation Repel are novel contributions demonstrated only through experimental results on three datasets
- Low: Paper lacks thorough analysis of model limitations, potential failure modes, and hyperparameter sensitivity

## Next Checks
1. Conduct an ablation study to assess impact of each component (IDW Loss, User Representation Repel, number of user towers) on performance across diverse datasets
2. Perform sensitivity analysis to determine optimal hyperparameter values (K, alpha, beta) and their impact on performance and computational complexity
3. Evaluate MTMI performance on other recommendation tasks like cold-start or context-aware recommendation to assess generalization capabilities