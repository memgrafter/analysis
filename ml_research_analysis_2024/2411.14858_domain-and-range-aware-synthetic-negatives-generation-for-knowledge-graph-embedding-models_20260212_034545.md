---
ver: rpa2
title: Domain and Range Aware Synthetic Negatives Generation for Knowledge Graph Embedding
  Models
arxiv_id: '2411.14858'
source_url: https://arxiv.org/abs/2411.14858
tags:
- negatives
- knowledge
- multi-class
- graph
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a negative sampling strategy for knowledge
  graph embedding models that generates corruptions respecting the domain and range
  constraints of relations. The method combines type-constrained sampling with random
  uniform sampling to address class imbalance and avoid resampling the same entity.
---

# Domain and Range Aware Synthetic Negatives Generation for Knowledge Graph Embedding Models

## Quick Facts
- arXiv ID: 2411.14858
- Source URL: https://arxiv.org/abs/2411.14858
- Authors: Alberto Bernardi; Luca Costabello
- Reference count: 40
- Key outcome: Domain and range aware negative sampling improves KGE performance across multiple datasets, with notable gains on Hetionet (+150% MRR) and FB15k-237 (+10% MRR)

## Executive Summary
This paper introduces a negative sampling strategy for knowledge graph embedding models that generates corruptions respecting domain and range constraints of relations. The method combines type-constrained sampling with random uniform sampling to address class imbalance and avoid resampling the same entity. Experiments on FB15k-237, WN18RR, and Hetionet show consistent improvements across multiple KGE models, with notable gains on Hetionet (+150% MRR) and FB15k-237 (+10% MRR). The approach is efficient, requiring minimal computational overhead, and is validated across diverse datasets.

## Method Summary
The method implements a hybrid negative sampling strategy that combines domain/range-aware corruptions with random uniform sampling. For each positive triple, it generates a configurable fraction (ν) of negatives using type constraints from the ontology or relation patterns, and the remaining fraction using random sampling. This addresses the class imbalance problem in KGs where some entity types are over-represented, while also ensuring diverse negative examples. The implementation uses AmpliGraph library and requires minimal computational overhead compared to pure random sampling.

## Key Results
- Consistent performance improvements across FB15k-237, WN18RR, and Hetionet datasets
- +10% MRR improvement on FB15k-237 compared to random uniform sampling baseline
- +150% MRR improvement on Hetionet dataset
- Minimal computational overhead (5ms per training step on 2M triples)
- Robust across multiple KGE models (TransE, DistMult, ComplEx-N3, RotatE)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hybrid sampling strategy (ν fraction of domain/range-aware negatives, 1-ν fraction random) improves diversity while maintaining semantic relevance.
- Mechanism: The random sampling component mitigates the class imbalance problem by introducing entities from over-represented classes that would otherwise be absent from type-constrained negatives.
- Core assumption: The dataset contains entity classes with significantly different cardinalities, and the ontology provides meaningful constraints.
- Evidence anchors: [abstract]: "Combining it with random uniform sampling [3], we mitigate this issue while also limiting the risk of generating trivial random negatives."

### Mechanism 2
- Claim: Using domain and range constraints from the ontology reduces the generation of false negatives compared to pure random sampling.
- Mechanism: By restricting corruptions to entities that are valid in the domain or range of a relation, the method avoids generating triples that are likely to be true but missing from the KG.
- Core assumption: The ontology accurately reflects the valid entity types for each relation.
- Evidence anchors: [abstract]: "Different generation strategies can heavily affect the quality of the embeddings."

### Mechanism 3
- Claim: The method scales efficiently because it requires minimal computational overhead.
- Mechanism: Domain and range constraints can be precomputed and stored, allowing fast lookup during training.
- Core assumption: The number of entities in each domain/range class is manageable.
- Evidence anchors: [section]: "The overhead introduced by our strategy is minimal, as ComplEx-N3 run on Hetionet (over 2 million triples) took an average ∼5ms per step extra overhead."

## Foundational Learning

- Concept: Knowledge Graph Embedding (KGE) models and link prediction task
  - Why needed here: The entire method is built on training KGE models for link prediction, where negatives are essential for learning to discriminate true from false triples.
  - Quick check question: In a triple (s, p, o), what are the three components, and what does link prediction aim to do with them?

- Concept: Negative sampling strategies in KGE
  - Why needed here: The paper directly compares and extends existing negative sampling methods (random, type-constrained).
  - Quick check question: What is the main drawback of pure random negative sampling in KGs with class imbalance?

- Concept: Domain and range constraints in ontologies
  - Why needed here: The method leverages these constraints to generate semantically relevant negatives.
  - Quick check question: If a relation "bornIn" has domain "Person" and range "Location", what would be a valid negative triple generated using these constraints?

## Architecture Onboarding

- Component map: Data loader -> Negative sampler -> KGE model -> Trainer -> Evaluator
- Critical path: 1. Load training triples 2. Generate η negatives using hybrid strategy 3. Pass through KGE model 4. Compute loss and update embeddings 5. Periodically evaluate on validation set
- Design tradeoffs: Hyperparameter ν balancing semantic relevance vs. diversity; embedding size vs. model capacity; number of negatives η vs. training time
- Failure signatures: Degraded performance on datasets with uninformative ontologies; overfitting to narrow entity types; slow convergence from noisy ontology
- First 3 experiments: 1. Implement with ν=0 (pure random) and compare to baseline 2. Implement with ν=1 (pure type-constrained) and observe performance drop 3. Sweep ν from 0 to 1 to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal value of ν vary across different knowledge graph sizes and structures?
- Basis in paper: [explicit] The paper shows ν impacts performance differently across datasets
- Why unresolved: The experiments only cover three datasets, which may not be representative
- What evidence would resolve it: Systematic experiments across KGs with varying entity/relation distributions

### Open Question 2
- Question: Does the performance gain from ontology-based sampling transfer to more complex KGE models like TuckER or A*Net?
- Basis in paper: [inferred] The paper deliberately limits analysis to four traditional KGE models
- Why unresolved: The authors explicitly state this is left for future work
- What evidence would resolve it: Direct comparison across traditional and complex KGE architectures

### Open Question 3
- Question: What is the computational overhead of ontology-based sampling when scaling to billion-edge knowledge graphs?
- Basis in paper: [explicit] The paper reports minimal overhead on Hetionet but doesn't test larger graphs
- Why unresolved: The experiments only cover relatively small benchmark datasets
- What evidence would resolve it: Performance measurements on large-scale industrial knowledge graphs

## Limitations
- Evaluation relies heavily on ontology quality - gains may not generalize to KGs with noisy or incomplete schema information
- The dramatic Hetionet improvement (+150% MRR) may reflect dataset-specific characteristics rather than general method superiority
- Method assumes meaningful domain and range constraints exist, which may not hold for all KGs

## Confidence
- **High Confidence**: The efficiency claims regarding computational overhead are well-supported by the 5ms per step measurement
- **Medium Confidence**: The performance improvements on FB15k-237 (+10% MRR) are credible but depend on implementation details
- **Low Confidence**: The dramatic Hetionet improvement may reflect dataset-specific factors rather than general method superiority

## Next Checks
1. **Ablation on Ontology Quality**: Test the method on datasets with varying ontology completeness to measure sensitivity to structural information quality
2. **Cross-Domain Generalization**: Apply the sampling strategy to KG datasets from different domains to verify that improvements are not dataset-specific artifacts
3. **False Negative Rate Analysis**: Quantify the reduction in false negative generation by measuring overlap between generated negatives and missing but true triples