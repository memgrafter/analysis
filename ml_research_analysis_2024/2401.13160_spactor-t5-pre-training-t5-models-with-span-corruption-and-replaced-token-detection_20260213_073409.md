---
ver: rpa2
title: 'SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced Token
  Detection'
arxiv_id: '2401.13160'
source_url: https://arxiv.org/abs/2401.13160
tags:
- pre-training
- language
- baseline
- size
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes SpacTor, a two-stage pre-training method for\
  \ T5 models that combines span corruption (SC) and replaced token detection (RTD).\
  \ In the first stage, SC and RTD are jointly trained for \u03C4 iterations, with\
  \ RTD providing stronger input token attention."
---

# SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced Token Detection

## Quick Facts
- arXiv ID: 2401.13160
- Source URL: https://arxiv.org/abs/2401.13160
- Authors: Ke Ye; Heinrich Jiang; Afshin Rostamizadeh; Ayan Chakrabarti; Giulia DeSalvo; Jean-François Kagy; Lazaros Karydas; Gui Citovsky; Sanjiv Kumar
- Reference count: 40
- One-line primary result: SpacTor-T5 achieves same performance as standard SC pre-training with 50% fewer iterations and 40% less compute, or improves performance by up to 1.5 points on average with same compute budget.

## Executive Summary
SpacTor-T5 introduces a two-stage pre-training curriculum for T5 models that combines span corruption (SC) and replaced token detection (RTD) objectives. In the first stage, SC and RTD are jointly trained for τ iterations, with RTD providing stronger input token attention. In the second stage, only SC is used for the remaining iterations. This approach achieves the same downstream performance as standard SC pre-training with 50% fewer iterations and 40% less compute, or improves performance by up to 1.5 points on average with the same compute budget.

## Method Summary
SpacTor-T5 uses a two-stage pre-training curriculum where span corruption (SC) and replaced token detection (RTD) are jointly trained for τ iterations, then only SC is used for the remaining iterations. An auxiliary generator model (1/3 to 1/4 size of the discriminator) replaces masked tokens with plausible alternatives, while the discriminator learns to detect replaced tokens and denoise SC masks. The approach leverages RTD's all-token attention in early training stages while avoiding generator noise degradation in later stages by transitioning to SC-only training.

## Key Results
- Achieves same downstream performance as standard SC pre-training with 50% fewer iterations
- Reduces total FLOPs by 40% compared to baseline SC training
- Improves performance by up to 1.5 points on average across multiple benchmarks with same compute budget

## Why This Works (Mechanism)

### Mechanism 1
The two-stage curriculum fixes the decay in downstream performance that occurs when using the hybrid objective throughout training. In early training, RTD's all-token attention is beneficial and generator noise is tolerable. However, as training progresses and the discriminator becomes stronger, the noise from replaced tokens increasingly interferes with span corruption learning, causing downstream performance to deteriorate. The two-stage curriculum transitions to SC-only training after τ iterations, eliminating this noise.

### Mechanism 2
The generator's MLM replacements introduce plausible but misleading context that degrades span corruption learning. The generator replaces masked tokens with plausible alternatives, creating corrupted text where some tokens are correct replacements and others are the generator's predictions. When this corrupted text is used as the encoder input for span corruption training, the discriminator must learn to fill in the original spans while also dealing with the noise from the generator's replacements. This noise interferes with the span corruption objective, particularly as training progresses and the generator becomes more confident in its incorrect replacements.

### Mechanism 3
RTD's all-token attention provides complementary information to span corruption in early training stages. In standard span corruption, the model only needs to attend to tokens within the corrupted spans to predict the missing content. RTD requires the model to attend to all tokens to detect which ones have been replaced. This forces the model to develop a more comprehensive understanding of token relationships and context, which is particularly beneficial when the model is still weak and token correlations are not well-established.

## Foundational Learning

- Concept: Pre-training objectives for language models (span corruption, masked language modeling, replaced token detection)
  - Why needed here: Understanding how different pre-training objectives work and their strengths/weaknesses is essential to grasp why combining them and then separating them at the right time improves performance.
  - Quick check question: What is the key difference between span corruption and masked language modeling in terms of what tokens the model needs to attend to for prediction?

- Concept: Curriculum learning and staged training
  - Why needed here: The paper's main contribution is a two-stage training curriculum. Understanding how curriculum learning works and why staged transitions can improve learning is crucial.
  - Quick check question: In what scenario might a curriculum that starts with harder examples and transitions to easier ones be beneficial?

- Concept: Token-level vs. span-level masking and their effects on attention patterns
  - Why needed here: The paper uses both span-level masking (for span corruption) and token-level masking (for the generator). Understanding how these different masking strategies affect what the model learns is important for understanding the mechanism.
  - Quick check question: How does the attention pattern required for predicting a single masked token differ from that required for predicting a span of missing tokens?

## Architecture Onboarding

- Component map: Generator G -> Discriminator D (encoder-decoder) -> Shared token embedder
- Critical path: Input text → Span corruption → MLM masking → Generator replacement → RTD/SC training → downstream fine-tuning
- Design tradeoffs:
  - Generator size: Too small → poor replacements, too large → wasted compute and potential overfitting
  - MLM ratio: Too low → generator doesn't change input enough, too high → discriminator sees mostly noise
  - τ (transition point): Too early → miss RTD benefits, too late → span corruption degraded by noise
  - Loss weights: Need to balance three objectives without one dominating

- Failure signatures:
  - Poor downstream performance that degrades over pre-training steps → generator noise overwhelming span corruption
  - No improvement over baseline → τ too early or loss weights unbalanced
  - High variance in downstream metrics → instability in the hybrid training phase

- First 3 experiments:
  1. Implement SPACTORBase(∞) and verify the performance decay pattern shown in Figure 3 by monitoring validation SC loss and downstream metrics
  2. Test different MLM ratios (5%, 10%, 15%, 20%, 25%) to find the optimal balance between generator quality and noise
  3. Implement SPACTORBase(τ) with τ=120K and τ=250K to verify the two-stage curriculum improves over both SPACTORBase(0) and SPACTORBase(∞)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal transition point τ for SpacTor training, and how does it vary with model size and task complexity?
- Basis in paper: [explicit] The paper tests τ values of 60K, 120K, and 250K iterations for the base model, finding that 60K is too early and 120K-250K work better. However, the optimal τ is not definitively determined.
- Why unresolved: The paper only explores a limited range of τ values and focuses primarily on T5-Base and T5-Large models. Different model sizes and task complexities might require different transition points.
- What evidence would resolve it: Systematic experiments varying τ across a wider range (e.g., 30K-500K) for multiple model sizes (e.g., Small, Base, Large, XL) and diverse task sets would reveal the optimal transition strategy.

### Open Question 2
- Question: How does SpacTor's performance compare to other hybrid pre-training approaches like PEGASUS when applied to encoder-decoder architectures?
- Basis in paper: [explicit] The paper mentions PEGASUS as a related work but focuses on differences in methodology rather than direct performance comparison. The authors state that SpacTor combines RTD and SC while PEGASUS denoises MLM in the encoder.
- Why unresolved: The paper doesn't provide empirical comparisons between SpacTor and PEGASUS on the same benchmarks. This leaves uncertainty about whether SpacTor's improvements are due to its specific methodology or simply the hybrid approach in general.
- What evidence would resolve it: Direct head-to-head comparisons of SpacTor and PEGASUS (or other hybrid methods) on identical pre-training datasets, model architectures, and downstream tasks would clarify the relative effectiveness of each approach.

### Open Question 3
- Question: What are the underlying mechanisms that cause the performance decay when using RTD and SC together for extended periods?
- Basis in paper: [inferred] The paper observes that RTD improves early training but performance degrades after ~250K steps. It suggests that RTD introduces noise that eventually overwhelms its benefits, but doesn't provide a detailed mechanistic explanation.
- Why unresolved: The paper identifies the phenomenon but doesn't explain the specific reasons why RTD's benefits diminish over time or what aspects of the training dynamics change. The authors mention that the discriminator's SC performance diverges from baseline but don't explore why.
- What evidence would resolve it: Detailed analysis of attention patterns, gradient flows, and token-level predictions during different training phases could reveal how RTD affects the model's learning trajectory. Experiments isolating specific components (e.g., testing RTD without token replacement, or varying the generator quality) would help identify the causal factors.

## Limitations

- The optimal transition point τ (200K-250K steps) and effectiveness of the two-stage curriculum may be specific to T5's architecture, the span corruption hyperparameters used, or the C4 dataset characteristics
- The empirical support for why the two-stage curriculum works is limited to observing performance patterns rather than systematic ablation studies of underlying mechanisms
- The paper doesn't test whether findings transfer to BERT-style models, different pre-training datasets, or alternative span corruption configurations

## Confidence

- **High confidence**: The empirical claim that SpacTor-T5 achieves the same downstream performance as standard SC pre-training with 50% fewer iterations is well-supported by the experimental results across multiple benchmarks and model sizes
- **Medium confidence**: The claim that SpacTor-T5 improves performance by up to 1.5 points on average with the same compute budget has moderate support, though the magnitude of improvement varies across benchmarks
- **Low confidence**: The mechanistic explanation for why the two-stage curriculum works - specifically that generator noise degrades span corruption learning in later training stages - is weakly supported by indirect evidence rather than controlled experiments

## Next Checks

1. **Mechanism isolation experiment**: Create a controlled experiment that separately measures the contribution of (a) RTD's all-token attention benefit in early training, (b) generator noise degradation in late training, and (c) the transition timing. This could involve training with SC only but forcing all-token attention in early stages, or training with hybrid objectives but using clean (non-generator) inputs throughout.

2. **Hyperparameter sensitivity analysis**: Systematically vary generator size (1/5, 1/4, 1/3 of discriminator), MLM ratio (5%, 10%, 15%, 20%, 25%), and span length (2, 3, 4, 5) across multiple τ values to identify the robust operating regime. Report how these variations affect both the iteration reduction claim and performance improvement claim.

3. **Cross-architecture generalization test**: Apply the SpacTor curriculum to a BERT-style model with MLMs (rather than T5's encoder-decoder structure) and to models pre-trained on different datasets (e.g., Wikipedia-only vs. C4). Compare whether the same τ values and performance patterns emerge, or whether the curriculum needs to be adapted for different architectures and data distributions.