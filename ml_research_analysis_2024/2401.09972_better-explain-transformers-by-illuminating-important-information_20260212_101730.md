---
ver: rpa2
title: Better Explain Transformers by Illuminating Important Information
arxiv_id: '2401.09972'
source_url: https://arxiv.org/abs/2401.09972
tags:
- information
- attention
- heads
- important
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a method to explain Transformer-based models
  by focusing on important information and eliminating irrelevant information in the
  layer-wise relevance propagation (LRP) process. The key idea is to identify and
  mask attention heads that focus on irrelevant information, such as those not related
  to core syntactic relations or fixed relative positions.
---

# Better Explain Transformers by Illuminating Important Information

## Quick Facts
- arXiv ID: 2401.09972
- Source URL: https://arxiv.org/abs/2401.09972
- Reference count: 16
- Primary result: Improves Transformer explanation by 3-33% over 8 baselines by masking irrelevant attention heads during LRP

## Executive Summary
This paper proposes a method to enhance explanations of Transformer models by identifying and masking attention heads that focus on irrelevant information during Layer-wise Relevance Propagation (LRP). The approach statistically analyzes attention head behavior to identify heads focusing on core syntactic relations and fixed relative positions, then uses masks to preserve only important information flows during explanation computation. Experimental results demonstrate consistent improvements of 3-33% over eight baseline methods on both classification and question-answering tasks.

## Method Summary
The method builds on Layer-wise Relevance Propagation (LRP) by first identifying important attention heads through statistical analysis of attention patterns across training data. It generates syntactic masks based on heads that consistently focus on core dependency relations (nsubj, dobj, amod, advmod) and positional masks for heads that exhibit high focus on specific relative positions. During LRP computation, only these identified important heads contribute to relevance propagation, while irrelevant heads are masked. The final attribution scores are computed through a rollout operation combining LRP relevance from important heads with attention gradients, producing explanations that better reflect the model's true reasoning process.

## Key Results
- Outperforms eight baseline methods on both classification and question-answering datasets
- Achieves 3-33% improvement on explanation metrics (AOPC, LOdds, Precision@20)
- Demonstrates that irrelevant information can distort output attribution scores and should be masked during explanation computation
- Shows effectiveness across multiple Transformer architectures (BERT, GPT-2, RoBERTa) and diverse NLP tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masking irrelevant attention heads during LRP prevents their negative influence on attribution scores.
- Mechanism: The proposed method identifies important attention heads (syntactic and positional) and masks others during relevance propagation, ensuring only relevant information contributes to final attributions.
- Core assumption: Attention heads focusing on syntactic relations and fixed relative positions capture the most important information for understanding model decisions.
- Evidence anchors: [abstract] "Experimental results demonstrate that irrelevant information does distort output attribution scores and then should be masked during explanation computation."

### Mechanism 2
- Claim: Important information flows through specific attention heads that can be statistically identified from dataset patterns.
- Mechanism: The method analyzes attention head behavior across training data to identify heads that consistently focus on core syntactic relations or specific relative positions, creating masks that preserve only these important flows.
- Core assumption: Attention heads exhibit consistent behavior across similar data points, allowing reliable identification of important vs. irrelevant heads through statistical analysis.
- Evidence anchors: [section] "We first obtain the distribution of the k-th syntactic relation at each token position... The attention head mask for syntactic relations... can be derived as follows"

### Mechanism 3
- Claim: Combining important relevance from syntactic and positional heads with rollout of weighted attention gradients produces superior attribution scores.
- Mechanism: The method computes relevance only from important heads during LRP, then combines this with attention gradients using a rollout operation to produce final attribution scores that better reflect true model reasoning.
- Core assumption: The combination of layer-wise relevance from important heads with attention gradients captures both the internal flow of importance and the final decision-making process.
- Evidence anchors: [section] "Compared to eight baselines on both classification and question-answering datasets, our method consistently outperforms with over 3% to 33% improvement on explanation metrics"

## Foundational Learning

- Concept: Layer-wise Relevance Propagation (LRP)
  - Why needed here: The proposed method builds upon LRP as the foundation for computing token attribution scores through backward relevance propagation.
  - Quick check question: What is the key conservation property that LRP must maintain during relevance propagation?

- Concept: Attention mechanisms in Transformers
  - Why needed here: Understanding how attention heads capture different types of information (syntactic relations, positional information) is crucial for identifying which heads to preserve during explanation.
  - Quick check question: How do multi-head attention mechanisms allow different heads to specialize in different types of information processing?

- Concept: Syntactic dependency parsing
  - Why needed here: The method relies on identifying specific syntactic relations (nsubj, dobj, amod, advmod) as indicators of important information flow.
  - Quick check question: What distinguishes core syntactic relations from other dependency relations in terms of their importance for sentence understanding?

## Architecture Onboarding

- Component map:
  Input -> Head identification -> Mask generation -> Relevance computation -> Attribution output -> Evaluation
  Pre-trained model -> Statistical analysis -> Syntactic/positional masks -> LRP with masks -> Rollout combination -> AOPC/LOdds/Precision@20

- Critical path:
  1. Load pre-trained model and dataset
  2. Analyze attention head behavior to generate masks
  3. Compute LRP relevance with masks applied
  4. Combine with attention gradients using rollout
  5. Evaluate attribution quality against ground truth
  6. Compare with baseline methods

- Design tradeoffs:
  - Mask granularity vs. computational cost: Finer masks identify more specific important heads but increase computation time
  - Head identification criteria: Stricter thresholds (higher 両synt, 両pos) reduce noise but may miss some important information
  - Dataset dependency: Mask generation depends on dataset statistics, requiring recomputation for new datasets

- Failure signatures:
  - Poor AOPC/LOdds scores despite correct implementation indicate masks may be too restrictive or identifying wrong heads
  - High variance across runs suggests instability in head identification process
  - Similar performance to random masking indicates masks aren't capturing meaningful patterns

- First 3 experiments:
  1. Implement head identification and mask generation on a small dataset (SST-2), visualize identified heads across layers to verify expected patterns (positional in early layers, syntactic in later layers)
  2. Compare LRP with and without masks on a single example to demonstrate the effect of masking irrelevant heads
  3. Run full evaluation pipeline on SST-2 with ablation study (using only syntactic mask, only positional mask, both masks) to understand contribution of each component

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the exact mechanisms by which irrelevant information hinders the LRP process? Are there specific types of irrelevant information that are more detrimental than others?
- Basis in paper: [explicit] The paper states that irrelevant information can "obfuscate the LRP process" and lead to "biased explanation of input tokens."
- Why unresolved: The paper provides empirical evidence that irrelevant information is detrimental, but does not delve into the specific mechanisms or identify the most harmful types of irrelevant information.
- What evidence would resolve it: Detailed analysis of the LRP process with different types of irrelevant information, including ablation studies and visualization of the relevance propagation in the presence of irrelevant information.

### Open Question 2
- Question: How does the proposed method scale to larger models like GPT-3, LLAMA, and LLAMA-2? Are there specific challenges or limitations in applying the method to these models?
- Basis in paper: [explicit] The paper acknowledges that computational resource limitations prevent them from exploring the method's implications for Large Language Models (LLMs) like LLAMA and LLAMA-2.
- Why unresolved: The paper does not provide any empirical results or analysis of the method's performance on larger models.
- What evidence would resolve it: Experiments applying the method to larger models, including analysis of computational requirements, performance metrics, and any specific challenges encountered.

### Open Question 3
- Question: What are the potential applications of the proposed method beyond explanation? Can it be used for other tasks such as model debugging, fairness analysis, or model compression?
- Basis in paper: [inferred] The paper focuses on the explanation capabilities of the method, but does not explore other potential applications.
- Why unresolved: The paper does not provide any discussion or experiments related to other potential applications of the method.
- What evidence would resolve it: Experiments and analysis of the method's performance on other tasks such as model debugging, fairness analysis, or model compression.

## Limitations

- The method assumes attention head behavior is stable enough across examples for reliable statistical identification, which may not hold for all datasets or model architectures
- Evaluation focuses on attribution quality metrics rather than faithfulness to the model's actual internal reasoning process
- The choice of syntactic relations and threshold values appears somewhat arbitrary without thorough sensitivity analysis

## Confidence

**High Confidence**: The core claim that irrelevant attention heads can distort attribution scores and should be masked is well-supported by experimental evidence showing consistent improvements over eight baselines.

**Medium Confidence**: The statistical approach for identifying important attention heads is methodologically sound, but the assumption that attention head behavior is stable enough across examples for reliable mask generation may not hold for all datasets or model architectures.

**Low Confidence**: The paper's claim that the combination of LRP from important heads with attention gradient rollout produces superior explanations is less well-supported, as the ablation studies and component analysis are limited.

## Next Checks

1. **Cross-dataset stability analysis**: Evaluate the method's performance when trained and tested on different datasets to assess whether the attention head masks generalize beyond the specific datasets used in the original experiments.

2. **Threshold sensitivity study**: Systematically vary the 両synt and 両pos thresholds across a wide range (e.g., 0.01 to 0.5) and measure the impact on both explanation quality metrics and computational efficiency.

3. **Faithfulness vs. plausibility evaluation**: Design experiments that test whether the generated explanations actually reflect the model's internal decision-making process, not just produce high metric scores. This could involve perturbation studies where token attribution scores are compared to actual impact on model outputs when tokens are modified or removed.