---
ver: rpa2
title: Towards a Theoretical Understanding of the 'Reversal Curse' via Training Dynamics
arxiv_id: '2405.04669'
source_url: https://arxiv.org/abs/2405.04669
tags:
- training
- probability
- reversal
- token
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides theoretical analysis of the \"reversal curse\"\
  \ in auto-regressive language models through training dynamics. The authors examine\
  \ both a simplified bilinear model and one-layer transformers, showing that the\
  \ reversal curse arises due to asymmetry in model weights - training on \"A \u2192\
  \ B\" does not necessarily increase the probability of \"B \u2190 A\"."
---

# Towards a Theoretical Understanding of the 'Reversal Curse' via Training Dynamics

## Quick Facts
- arXiv ID: 2405.04669
- Source URL: https://arxiv.org/abs/2405.04669
- Authors: Hanlin Zhu; Baihe Huang; Shaolun Zhang; Michael Jordan; Jiantao Jiao; Yuandong Tian; Stuart Russell
- Reference count: 40
- Key outcome: This paper provides theoretical analysis of the "reversal curse" in auto-regressive language models through training dynamics. The authors examine both a simplified bilinear model and one-layer transformers, showing that the reversal curse arises due to asymmetry in model weights - training on "A → B" does not necessarily increase the probability of "B ← A". Their analysis reveals that the commonly used unconstrained optimization for cross-entropy loss leads to this weight asymmetry. The paper also extends this framework to analyze chain-of-thought reasoning, showing that intransitivity of model weights explains why models struggle to directly deduce "A ; C" from separately learned "A → B" and "B → C". Experiments on multi-layer transformers validate these theoretical findings, demonstrating that while models can perfectly learn training directions, they fail to generalize to unseen reverse directions.

## Executive Summary
This paper provides a theoretical framework for understanding the "reversal curse" - a phenomenon where auto-regressive language models fail to learn the reverse of relationships they were trained on. Through analysis of both simplified bilinear models and one-layer transformers, the authors demonstrate that the reversal curse arises from asymmetric weight updates during training. The core insight is that cross-entropy loss with unconstrained optimization leads to weight asymmetry, where training on "A → B" does not necessarily increase the weights for "B ← A". The paper extends this framework to analyze chain-of-thought reasoning, showing that intransitivity of model weights explains why models struggle with direct deductions from sequentially learned relationships.

## Method Summary
The authors analyze the reversal curse through training dynamics of auto-regressive models. They start with a simplified bilinear model and then extend to one-layer transformers, tracking how weight matrices evolve during training via gradient descent. The analysis focuses on how cross-entropy loss with unconstrained optimization creates asymmetric weight updates - when token A appears before token B in training sequences, the weights from A to B increase, but the weights from B to A may not change. This asymmetry persists through training and leads to failure on reverse directions at test time. The framework is extended to chain-of-thought reasoning by analyzing how separate training of "A → B" and "B → C" sequences doesn't create direct "A ; C" weights.

## Key Results
- Training on "A → B" does not necessarily increase the probability of "B ← A" due to asymmetric weight updates
- Cross-entropy loss with unconstrained optimization creates weight asymmetry in both bilinear models and one-layer transformers
- The reversal curse can be extended to explain chain-of-thought reasoning failures through intransitivity of model weights
- Experiments on multi-layer transformers validate theoretical findings - models can learn training directions perfectly but fail on unseen reverse directions
- In-context learning appears to mitigate the reversal curse, though theoretical explanation is needed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asymmetric model weights cause reversal curse
- Mechanism: Training on "A → B" increases weights for A→B but not necessarily B→A
- Core assumption: Cross-entropy loss and unconstrained optimization create weight asymmetry
- Evidence anchors:
  - [abstract] "the increase of weights from a token A to token B during training does not necessarily cause the increase of the weights from B to A"
  - [section 4.1] "the parameter matrix Θt is asymmetric. Consequently, the logits lΘt(y|x) = x⊤Θty and lΘt(x|y) = y⊤Θtx generally differ"
  - [corpus] Strong evidence - multiple papers directly address weight asymmetry as core mechanism
- Break condition: Constraint optimization or symmetric loss functions

### Mechanism 2
- Claim: Training dynamics under SGD create intransitivity
- Mechanism: Separate training of "A → B" and "B → C" doesn't create direct "A ; C" weights
- Core assumption: One-layer transformer dynamics follow the same weight asymmetry patterns
- Evidence anchors:
  - [section 4.2] "training the weights associated with A to B and B to C does not necessarily increase the weights associated with A to C"
  - [section 4.1] "Lemma 2 implies the asymmetry of the model weights Y (t): for two tokens x1, x3, when x1 appears as a contextual token and x3 serves as the next token in the same training sequence, the model weights Y (t)x1,x3 gets increased during training while Y (t)x3,x1 will not get increased"
  - [corpus] Moderate evidence - several papers discuss chain-of-thought as related phenomenon
- Break condition: Joint training of connected sequences

### Mechanism 3
- Claim: Attention score matrix Z(t) doesn't impact reversal curse
- Mechanism: Query token doesn't attend to itself, so attention scores don't create bidirectional dependencies
- Core assumption: Same framework as [14] where query token avoids self-attention
- Evidence anchors:
  - [section 4.3] "for a three-token training sequence, the weights b12 is always one since there is only one contextual token x1, no matter whether the value of the attention score is high or low"
  - [section 4] "we don't need to keep track of the dynamics of Z(t), which could greatly simplify the analysis"
  - [corpus] Weak evidence - this is more of a negative result that isn't widely discussed
- Break condition: Modified attention mechanisms with self-attention

## Foundational Learning

- Concept: Gradient flow dynamics
  - Why needed here: Understanding how weight updates propagate through training
  - Quick check question: What happens to gradient flow when loss surface is asymmetric?

- Concept: Reparameterization of transformer weights
  - Why needed here: Y and Z matrices represent different aspects of weight dynamics
  - Quick check question: How does Y matrix relate to token prediction probabilities?

- Concept: Cross-entropy loss properties
  - Why needed here: Explains why unconstrained optimization leads to asymmetry
  - Quick check question: What symmetry properties would a "symmetric loss" need to have?

## Architecture Onboarding

- Component map: Bilinear model → One-layer transformer → Multi-layer transformer
  - Bilinear: Simplified version with direct weight matrix
  - One-layer: Attention + reparameterization (Y, Z matrices)
  - Multi-layer: Stacked attention layers with residual connections

- Critical path: Weight asymmetry → Training dynamics → Generalization failure
  - Step 1: Identify asymmetric weight updates
  - Step 2: Track how asymmetry persists through training
  - Step 3: Measure impact on unseen reverse directions

- Design tradeoffs: Simplicity vs expressiveness
  - Bilinear model: Simple but limited expressivity
  - One-layer transformer: More realistic but requires assumptions
  - Multi-layer: Most realistic but harder to analyze theoretically

- Failure signatures: Perfect training accuracy, poor validation on reverse directions
  - Training loss approaches zero
  - Test loss remains high for unseen directions
  - Weight matrices show clear asymmetry patterns

- First 3 experiments:
  1. Train bilinear model on "A → B" pairs, test on "B → A" pairs
  2. Train one-layer transformer on chain-of-thought sequences, test direct deduction
  3. Visualize weight matrices to confirm asymmetry patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a unified framework for analyzing reversal curse, chain-of-thought, and other logical reasoning tasks?
- Basis in paper: explicit
- Why unresolved: The authors demonstrate their framework works for reversal curse and chain-of-thought, but explicitly state this as an open direction for future work.
- What evidence would resolve it: A theoretical framework that can be applied to multiple types of logical reasoning tasks, showing consistent patterns in model weight dynamics across different reasoning challenges.

### Open Question 2
- Question: What are the theoretical properties of multi-layer transformers regarding the reversal curse and chain-of-thought reasoning?
- Basis in paper: explicit
- Why unresolved: The authors analyze only bilinear models and one-layer transformers theoretically, while experiments show these phenomena persist in multi-layer transformers. They explicitly call for extending the analysis to multi-layer transformers.
- What evidence would resolve it: Mathematical proofs showing how model weight asymmetry and intransitivity manifest in multi-layer transformer architectures, with specific analysis of how depth affects these properties.

### Open Question 3
- Question: How does the reversal curse behave in in-context learning settings?
- Basis in paper: explicit
- Why unresolved: The authors mention in their experiments that the reversal curse does not happen in in-context learning settings, but do not provide theoretical analysis of why this occurs or under what conditions it might break down.
- What evidence would resolve it: Theoretical explanation of how in-context learning mechanisms differ from standard training in terms of weight dynamics, and empirical studies showing boundary conditions where ICL might still exhibit reversal curse behavior.

## Limitations

- The theoretical analysis relies on simplified architectures (bilinear models and one-layer transformers) that may not fully capture real-world language models
- Assumptions about token independence and attention mechanism behavior in deeper layers remain unverified for practical implementations
- The claim that unconstrained optimization is necessary for weight asymmetry doesn't account for regularization schemes used in practice

## Confidence

**High Confidence**: The core mechanism of weight asymmetry causing the reversal curse is well-supported by both theoretical analysis and experimental validation. The mathematical proofs showing how cross-entropy loss with unconstrained optimization leads to asymmetric weight updates are rigorous.

**Medium Confidence**: The extension to chain-of-thought reasoning and the explanation of intransitivity through weight dynamics is theoretically sound but relies on simplifying assumptions about how transformers process sequential information. The practical implications for real-world reasoning tasks need further validation.

**Low Confidence**: The claim that attention score matrix Z(t) doesn't impact reversal curse is primarily a negative result and lacks extensive experimental validation across different attention mechanisms and model depths.

## Next Checks

1. **Multi-layer Validation**: Test the theoretical predictions on 6-12 layer transformers to verify if weight asymmetry persists and causes similar reversal failures at scale. This would bridge the gap between theoretical analysis and practical implementations.

2. **Regularization Impact**: Experiment with different regularization schemes (L2, dropout, weight decay) to determine if they can reduce weight asymmetry and improve reverse direction learning, testing the claim that unconstrained optimization is necessary for the effect.

3. **Cross-model Comparison**: Apply the same analysis framework to different model architectures (RNNs, MLPs, and modern transformers) to determine if weight asymmetry is a fundamental property of auto-regressive training or specific to transformer architectures.