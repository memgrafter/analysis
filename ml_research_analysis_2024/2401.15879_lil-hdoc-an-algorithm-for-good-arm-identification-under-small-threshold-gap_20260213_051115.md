---
ver: rpa2
title: 'lil''HDoC: An Algorithm for Good Arm Identification under Small Threshold
  Gap'
arxiv_id: '2401.15879'
source_url: https://arxiv.org/abs/2401.15879
tags:
- hdoc
- arms
- algorithm
- good
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the good arm identification (GAI) problem in
  multi-armed bandits, focusing on scenarios with a small threshold gap. GAI aims
  to identify arms with expected rewards above a given threshold using as few samples
  as possible.
---

# lil'HDoC: An Algorithm for Good Arm Identification under Small Threshold Gap

## Quick Facts
- arXiv ID: 2401.15879
- Source URL: https://arxiv.org/abs/2401.15879
- Reference count: 14
- Solves good arm identification with small threshold gaps

## Executive Summary
This paper addresses the good arm identification (GAI) problem in multi-armed bandits, specifically focusing on scenarios with small threshold gaps. The authors propose lil'HDoC, an algorithm that improves upon the state-of-the-art HDoC algorithm by addressing the challenges posed by small threshold gaps. lil'HDoC achieves this by sampling each arm multiple times initially to gain more confidence in their goodness or badness, allowing for tighter confidence bounds in the identification process.

## Method Summary
lil'HDoC improves upon the HDoC algorithm by addressing the challenges of small threshold gaps in good arm identification. The core innovation is to sample each arm more than once at the beginning of the algorithm, which provides more confidence in their goodness or badness. This approach allows for tighter confidence bounds compared to HDoC. The algorithm leverages the Law of Iterated Logarithm to achieve these improvements, resulting in a more efficient identification process, especially when the threshold gap (∆) is small.

## Key Results
- lil'HDoC has the same sample complexity as HDoC for identifying the first λ good arms, except for a negligible term
- The total sample complexity of lil'HDoC is improved by reducing the 1/∆ log 1/∆ term to 1/∆ log log 1/∆
- Extensive experiments show lil'HDoC outperforms HDoC and LUCB-G in sample complexity, especially for arms requiring more samples to be identified

## Why This Works (Mechanism)
lil'HDoC works by addressing the fundamental challenge of small threshold gaps in good arm identification. The key insight is that initial multiple samplings of each arm provide more confidence in their quality assessments. This additional confidence allows for tighter confidence bounds in the identification process, which is crucial when the gap between good and bad arms is small. By leveraging the Law of Iterated Logarithm, the algorithm can make more efficient use of samples, particularly in scenarios where the threshold gap (∆) is small, resulting in the improved 1/∆ log log 1/∆ term in the sample complexity.

## Foundational Learning
- Law of Iterated Logarithm: Why needed - provides probabilistic bounds for sample means; Quick check - verify bounds on sample averages
- Multi-armed bandit theory: Why needed - foundation for understanding exploration-exploitation tradeoffs; Quick check - ensure proper arm elimination criteria
- Confidence bound analysis: Why needed - critical for determining when to stop sampling and identify good arms; Quick check - validate confidence bound tightness

## Architecture Onboarding

Component Map:
Multiple initial samplings -> Confidence bound calculation -> Arm elimination process -> Good arm identification

Critical Path:
1. Initial multiple samplings of each arm
2. Confidence bound calculation using Law of Iterated Logarithm
3. Iterative arm elimination based on confidence bounds
4. Identification of good arms when sufficient confidence is achieved

Design Tradeoffs:
- Increased initial sampling vs. tighter confidence bounds
- Computational overhead of multiple initial samplings vs. reduced total sample complexity
- Simplicity of implementation vs. potential for further optimization

Failure Signatures:
- Poor performance when threshold gap is large (not designed for this scenario)
- Potential inefficiency with extremely large number of arms due to initial multiple samplings
- Computational bottlenecks if confidence bound calculations are not optimized

First Experiments:
1. Compare sample complexity of lil'HDoC vs HDoC for various threshold gap sizes
2. Test algorithm performance on synthetic datasets with known good arm distributions
3. Evaluate runtime and memory usage compared to baseline algorithms

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on small threshold gaps may not generalize to all GAI scenarios
- Performance in extremely large or small-scale problems remains unexplored
- Computational overhead of initial multiple samplings not thoroughly analyzed

## Confidence

Theoretical claims:
- Sample complexity improvement (1/∆ log 1/∆ to 1/∆ log log 1/∆): High
- Same sample complexity as HDoC for first λ good arms: High

Experimental validation:
- Performance improvement over HDoC and LUCB-G: Medium

## Next Checks

1. Test the algorithm on datasets with extremely large numbers of arms to evaluate scalability
2. Conduct experiments varying the threshold gap magnitude to identify the threshold where the algorithm's advantages diminish
3. Compare the computational efficiency of lil'HDoC against HDoC and LUCB-G in terms of runtime and memory usage