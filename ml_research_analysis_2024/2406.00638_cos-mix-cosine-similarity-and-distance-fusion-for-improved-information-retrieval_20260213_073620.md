---
ver: rpa2
title: 'COS-Mix: Cosine Similarity and Distance Fusion for Improved Information Retrieval'
arxiv_id: '2406.00638'
source_url: https://arxiv.org/abs/2406.00638
tags:
- retrieval
- information
- distance
- https
- hybrid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces COS-Mix, a hybrid retrieval strategy for
  Retrieval-Augmented Generation (RAG) that combines cosine similarity and cosine
  distance measures to improve retrieval performance, particularly for sparse data.
  Traditional cosine similarity measures often yield arbitrary results in certain
  scenarios, which the proposed method addresses by incorporating cosine distance
  to quantify dissimilarity between vectors.
---

# COS-Mix: Cosine Similarity and Distance Fusion for Improved Information Retrieval

## Quick Facts
- arXiv ID: 2406.00638
- Source URL: https://arxiv.org/abs/2406.00638
- Authors: Kush Juvekar; Anupam Purwar
- Reference count: 23
- This study introduces COS-Mix, a hybrid retrieval strategy for Retrieval-Augmented Generation (RAG) that combines cosine similarity and cosine distance measures to improve retrieval performance, particularly for sparse data.

## Executive Summary
This study introduces COS-Mix, a hybrid retrieval strategy for Retrieval-Augmented Generation (RAG) that combines cosine similarity and cosine distance measures to improve retrieval performance, particularly for sparse data. Traditional cosine similarity measures often yield arbitrary results in certain scenarios, which the proposed method addresses by incorporating cosine distance to quantify dissimilarity between vectors. Experiments on proprietary data demonstrate enhanced retrieval performance, with improvements in classical metrics like precision (0.77), recall (0.63), and F-score (0.68), as well as nuanced metrics such as contextual precision (0.98) and answer relevancy (1.0). The method also reduces response time by efficiently handling sparse information. COS-Mix provides a promising solution for accurate and efficient information retrieval in knowledge-intensive applications.

## Method Summary
COS-Mix is a hybrid retrieval strategy for RAG that combines cosine similarity and cosine distance measures. The method uses a hybrid retriever composed of BM25 and traditional vector retriever, with an LLM validation prompt to determine when to switch between similarity-based and distance-based approaches. The corpus is pre-partitioned into sparse and non-sparse sets to reduce latency during inference. When hybrid retrieval fails, the system searches only the sparse subset using distance-based retrieval. The method was tested on proprietary HTML pages processed into text chunks and embedded using OpenAI's text-embedding-ada-002 model, with evaluation using both classical and nuanced metrics.

## Key Results
- Classical metrics show improvements: precision 0.77, recall 0.63, F-score 0.68, METEOR 0.66
- Nuanced metrics demonstrate high performance: contextual precision 0.98, contextual recall 0.86, answer relevancy 1.0, faithfulness 0.90
- The method reduces response time by efficiently handling sparse information through pre-partitioning and conditional distance search
- COS-Mix provides accurate and efficient information retrieval, particularly for sparse data scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: COS-Mix improves retrieval for sparse data by combining cosine similarity (captures semantic closeness) with cosine distance (captures semantic dissimilarity).
- Mechanism: The hybrid retrieval first uses a standard dense-sparse retriever (BM25 + vector). When validation fails, it re-ranks by explicitly measuring cosine distance to find chunks with sparse but relevant information that similarity alone misses.
- Core assumption: Cosine similarity alone can fail to retrieve sparse or outlier chunks because it treats all non-matching vectors as equally dissimilar; adding distance-based re-ranking surfaces these hidden relevant items.
- Evidence anchors:
  - [abstract] "traditional cosine similarity measure ... can yield arbitrary results in certain scenarios. To address this limitation, we incorporate cosine distance measures to provide a complementary perspective by quantifying the dissimilarity between vectors."
  - [section] "However, the usage of distance approach in information retrieval augments the classical RAG and LLM is able to respond to such questions with accurate answers every time"
  - [corpus] Weak - no direct mention of sparse-data COS-Mix experiments in cited neighbors.
- Break condition: If the corpus contains no sparse-relevant chunks, distance re-ranking will only add noise and degrade precision.

### Mechanism 2
- Claim: Pre-partitioning the corpus into sparse and non-sparse sets reduces latency during inference.
- Mechanism: Before inference, each chunk is tagged as "sparse" if it contains unique information not found in any other chunk. During retrieval, if hybrid retrieval fails, the system searches only the sparse subset S instead of the full corpus T, cutting search space.
- Core assumption: Sparse chunks are rare and identifiable offline; searching them separately is faster than brute-force distance computation on all chunks.
- Evidence anchors:
  - [section] "By creating a small subset out of large corpus T, we avoid latency involved in calculating distance between embedding vectors during inference time."
  - [algorithm] "Thus, one does not have to sequentially search all the text chunks to identify the most relevant chunks when the hybrid retrieval fails in retrieving relevant context."
  - [corpus] Missing - no neighbor papers discuss offline corpus partitioning for retrieval.
- Break condition: If the sparse-set S is large (e.g., >30% of T), the latency benefit disappears.

### Mechanism 3
- Claim: Validation prompt switches retrieval strategy only when necessary, preserving speed while improving accuracy.
- Mechanism: After initial hybrid retrieval, a lightweight LLM prompt checks whether the answer is satisfactory. If not, the system switches to distance-based retrieval on the sparse set. This conditional switch avoids the cost of distance computation on every query.
- Core assumption: The validation prompt reliably detects when hybrid retrieval has failed, so distance search is invoked only when it will improve the answer.
- Evidence anchors:
  - [section] "For RAG, we use a hybrid retriever composed of BM25 retriever coupled with traditional vector retriever and then the retrieved chunks were reordered [13]."
  - [section] "To facilitate the transition between the similarity-based and distance-based approaches, a validation prompt was utilized."
  - [corpus] Weak - no neighbor papers describe conditional strategy switching in RAG.
- Break condition: If the validation prompt is too permissive, many queries will skip the potentially useful distance step; if too strict, unnecessary distance searches increase latency.

## Foundational Learning

- Concept: Vector similarity vs. distance metrics in high-dimensional embedding spaces.
  - Why needed here: COS-Mix explicitly leverages both cosine similarity and cosine distance; understanding their geometric meaning is critical to grasp why one complements the other.
  - Quick check question: In a 300-dim embedding space, if two vectors have a cosine similarity of 0.1, what is their cosine distance?
    - Answer: 1 - 0.1 = 0.9.

- Concept: Sparse vs. dense retrieval trade-offs.
  - Why needed here: The hybrid retriever uses BM25 (sparse) + dense embeddings; knowing when each excels helps tune chunk size and top-k.
  - Quick check question: Which retrieval method is more robust to out-of-distribution queries: BM25 or dense embeddings?
    - Answer: BM25, because it relies on exact keyword overlap rather than semantic drift.

- Concept: Retrieval-augmented generation pipeline flow.
  - Why needed here: COS-Mix modifies the standard RAG loop by adding a fallback distance step; understanding the baseline flow clarifies where the changes occur.
  - Quick check question: In a standard RAG, what is the sequence from query to answer?
    - Answer: Query → retriever (fetch chunks) → context → LLM generation → output.

## Architecture Onboarding

- Component map:
  - Corpus → offline partitioner (sparse vs. non-sparse sets S, R)
  - Query → hybrid retriever (BM25 + dense)
  - Validation prompt (LLM judge)
  - Fallback distance retriever (searches S only)
  - LLM generator (produces final answer)

- Critical path:
  1. Hybrid retrieval on R
  2. LLM validation
  3. If fail → distance retrieval on S
  4. LLM generation on final context

- Design tradeoffs:
  - Precision vs. recall: distance retrieval boosts recall for sparse data but may pull in noisy chunks
  - Latency vs. accuracy: offline partitioning speeds up distance fallback but requires preprocessing
  - Complexity vs. maintainability: two distinct retrieval strategies increase code paths and failure modes

- Failure signatures:
  - Low contextual relevancy despite high precision → sparse chunks not being surfaced
  - High latency on certain queries → distance search on large S
  - Validation prompt false negatives → unnecessary distance searches

- First 3 experiments:
  1. Ablation: run COS-Mix with validation disabled; measure precision drop and latency gain
  2. Chunk-size sweep: vary chunk size; record impact on sparse-set S size and overall recall
  3. Corpus-size scaling: benchmark COS-Mix vs. pure hybrid on progressively larger corpora to confirm offline partitioning benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does COS-Mix perform compared to other state-of-the-art hybrid retrieval methods on open-source datasets?
- Basis in paper: [explicit] The paper states that COS-Mix was tested on proprietary data, unlike recent publications that used open-source datasets.
- Why unresolved: The paper does not provide a direct comparison with other hybrid retrieval methods on open-source datasets, which limits the generalizability of the results.
- What evidence would resolve it: Conducting experiments on open-source datasets and comparing COS-Mix's performance with other hybrid retrieval methods would provide a clearer understanding of its effectiveness.

### Open Question 2
- Question: What is the impact of varying the chunk size and top-k values on the performance of COS-Mix?
- Basis in paper: [explicit] The paper mentions that varying the chunk size and top-k values did not improve the performance of classical RAG in sparse information scenarios.
- Why unresolved: The paper does not explore the optimal chunk size and top-k values for COS-Mix, which could potentially enhance its performance.
- What evidence would resolve it: Experimenting with different chunk sizes and top-k values for COS-Mix and analyzing their impact on retrieval performance would help identify the optimal settings.

### Open Question 3
- Question: How does COS-Mix handle multilingual or domain-specific queries?
- Basis in paper: [inferred] The paper focuses on information retrieval for proprietary data, which suggests that COS-Mix may not have been tested on multilingual or domain-specific queries.
- Why unresolved: The paper does not discuss the performance of COS-Mix on multilingual or domain-specific queries, which limits its applicability in diverse scenarios.
- What evidence would resolve it: Evaluating COS-Mix on multilingual and domain-specific datasets and analyzing its performance would provide insights into its effectiveness in handling diverse query types.

## Limitations
- The experiments were conducted on proprietary data from a single source (i-venture.org), limiting generalizability to different domains or corpora
- The specific performance metrics cannot be independently verified due to the proprietary nature of the dataset and absence of comparison with established baselines
- The optimal chunk size and partitioning strategy for creating the sparse set S were not specified, leaving critical implementation details ambiguous

## Confidence
- **High Confidence**: The fundamental mechanism of combining cosine similarity and cosine distance is mathematically sound and the basic hybrid retrieval approach (BM25 + dense embeddings) is well-established in the literature
- **Medium Confidence**: The validation prompt strategy for conditional switching between retrieval methods is reasonable but lacks empirical validation in the provided information
- **Low Confidence**: The specific performance metrics (precision 0.77, recall 0.63, F-score 0.68) cannot be independently verified due to the proprietary nature of the dataset and the absence of comparison with established baselines

## Next Checks
1. Reproduce COS-Mix on a publicly available benchmark dataset (such as Natural Questions or MS MARCO) to verify performance claims across different domains and compare against established RAG baselines

2. Conduct ablation studies to measure the individual contributions of each component: hybrid retrieval alone, distance-based re-ranking alone, and the validation prompt mechanism, to isolate which elements drive the reported improvements

3. Measure and report end-to-end latency for COS-Mix compared to standard hybrid retrieval, particularly analyzing the trade-off between the preprocessing overhead for corpus partitioning and runtime performance benefits during inference