---
ver: rpa2
title: Natural Language Counterfactual Explanations for Graphs Using Large Language
  Models
arxiv_id: '2410.09295'
source_url: https://arxiv.org/abs/2410.09295
tags:
- counterfactual
- graph
- explanations
- node
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for generating natural language explanations
  for graph counterfactuals using large language models. The approach translates technical
  counterfactual examples from graph neural networks into accessible language by prompting
  open-source LLMs with factual and counterfactual graph instances.
---

# Natural Language Counterfactual Explanations for Graphs Using Large Language Models

## Quick Facts
- arXiv ID: 2410.09295
- Source URL: https://arxiv.org/abs/2410.09295
- Authors: Flavio Giorgi; Cesare Campagnano; Fabrizio Silvestri; Gabriele Tolomei
- Reference count: 2
- Primary result: Method generates natural language explanations for graph counterfactuals using LLMs, validated on Cora and CiteSeer datasets with performance improving with larger model sizes

## Executive Summary
This paper addresses the challenge of making graph counterfactual explanations interpretable for non-technical users by translating technical counterfactual examples from graph neural networks into natural language. The authors propose using large language models (LLMs) to generate explanations by prompting them with factual and counterfactual graph instances. They introduce novel evaluation metrics to assess the quality of these explanations and validate their approach through experiments with two graph counterfactual explainers across standard graph datasets.

## Method Summary
The approach involves prompting pre-trained LLMs with structured information about factual and counterfactual graph instances to generate natural language explanations. The method uses a predefined dictionary structure that guides the LLM to identify key elements like target nodes, classes, and features. The authors evaluate their approach using novel metrics (TNI, CCI, FTNF, CFTNF, CFTNN) that quantify the LLM's ability to identify target nodes, understand class changes, and recognize feature and neighbor changes in both factual and counterfactual graphs.

## Key Results
- Larger LLM parameter sizes (14B) achieve the highest scores across all evaluation metrics compared to smaller models (0.5B, 1.7B, 7B)
- The approach effectively produces accurate natural language representations of counterfactual instances across Cora and CiteSeer datasets
- Performance varies based on explainer type (CF-GNNExplainer vs CF-GNNFeatures) and dataset characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM generates accurate natural language explanations by mapping structural and feature-based transformations between factual and counterfactual graphs
- Mechanism: The LLM is prompted with both factual and counterfactual graph instances, along with a predefined dictionary structure that guides the model to identify key elements like target nodes, classes, and features. The LLM then generates explanations based on these structured inputs.
- Core assumption: The LLM can accurately interpret the structured graph information provided in the prompt and generate coherent natural language explanations.
- Evidence anchors:
  - [abstract] "Experiments across several graph datasets and counterfactual explainers show that our approach effectively produces accurate natural language representations of counterfactual instances, as demonstrated by key performance metrics."
  - [section 3.3] "By doing so, we aim to evaluate the language model’s ability to discern and convey the critical elements of the graph structure and its transformations, thereby assessing the model’s comprehension of how the changes in the graph influence the classification outcome for the target node."

### Mechanism 2
- Claim: Larger LLM parameter sizes lead to better performance in generating accurate explanations for graph counterfactuals
- Mechanism: As the number of parameters in the LLM increases, the model's capacity to understand and process complex graph structures and transformations improves, resulting in more accurate and coherent explanations.
- Core assumption: Model size is directly correlated with the ability to comprehend and articulate complex graph-based counterfactual explanations.
- Evidence anchors:
  - [section 4.5] "In particular, a clear trend emerges across all tables: as the number of parameters in the LLM increases, performance improves significantly across all metrics, as expected."
  - [table 1] Shows a clear trend of increasing performance metrics (TNI, CCI, FTNF, CFTNF, FTNN, CFTNN) as the number of parameters increases from 0.5B to 14B.

### Mechanism 3
- Claim: The proposed evaluation framework accurately assesses the quality of LLM-generated explanations for graph counterfactuals
- Mechanism: The framework introduces novel metrics (TNI, CCI, FTNF, CFTNF, CFTNN) that quantify the LLM's ability to identify target nodes, understand class changes, and recognize feature and neighbor changes in both factual and counterfactual graphs.
- Core assumption: These metrics provide a comprehensive and objective assessment of the LLM's performance in generating accurate explanations.
- Evidence anchors:
  - [section 3.3] "To address this gap and ensure a more systematic evaluation, we have developed a suite of novel metrics that provide a quantitative and objective assessment of the language model’s capability to articulate pairs of factual and counterfactual graphs into coherent and informative natural language explanations."

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: Understanding GNNs is crucial as the paper focuses on generating explanations for GNN-based counterfactuals in graph data.
  - Quick check question: What is the primary purpose of using GNNs in node classification tasks?

- Concept: Counterfactual Explanations
  - Why needed here: The paper's core contribution is translating counterfactual explanations for graphs into natural language using LLMs.
  - Quick check question: How do counterfactual explanations help in understanding the decision-making process of machine learning models?

- Concept: Large Language Models (LLMs)
  - Why needed here: LLMs are the primary tool used to generate natural language explanations from technical counterfactual examples.
  - Quick check question: What are the key capabilities of LLMs that make them suitable for translating complex technical information into natural language?

## Architecture Onboarding

- Component map: Factual/ counterfactual graph instances -> Structured prompt with dictionary -> LLM processing -> Natural language explanations -> Evaluation metrics (TNI, CCI, FTNF, CFTNF, CFTNN) and human judgment

- Critical path:
  1. Generate counterfactual examples using graph counterfactual explainers (CF-GNNExplainer or CF-GNNFeatures)
  2. Structure the prompt with factual and counterfactual graph information
  3. Feed the structured prompt to the LLM
  4. Evaluate the generated explanations using the proposed metrics and human judgment

- Design tradeoffs:
  - Model size vs. computational efficiency: Larger models perform better but require more resources
  - Granularity of graph information in prompts vs. LLM comprehension: More detailed information may improve explanations but could also overwhelm the LLM
  - Automated metrics vs. human judgment: Automated metrics provide objective assessment but may not capture all aspects of explanation quality

- Failure signatures:
  - Consistently low scores across all evaluation metrics
  - Explanations that do not align with the structural or feature-based changes in the graphs
  - LLM-generated explanations that are incoherent or irrelevant to the counterfactual examples

- First 3 experiments:
  1. Test the LLM with a simple graph structure and verify if it can accurately identify the target node and class change.
  2. Compare the performance of different LLM sizes on a fixed dataset and explainer to validate the model size impact.
  3. Evaluate the generated explanations using the proposed metrics and compare them with human judgment to assess the effectiveness of the evaluation framework.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on automated metrics that have not been extensively validated against human judgment across diverse real-world scenarios
- Study is constrained to academic graph datasets (Cora and CiteSeer) with synthetic counterfactual examples, limiting generalizability to complex real-world graph data
- Approach requires structured prompt engineering with predefined dictionaries, which may not scale well to diverse graph domains without significant manual effort

## Confidence

- **High Confidence**: The correlation between LLM parameter size and explanation quality is well-supported by empirical results across multiple experiments and metrics
- **Medium Confidence**: The effectiveness of the novel evaluation metrics is supported by experimental results but requires further validation against diverse human judgment criteria
- **Medium Confidence**: The overall approach of using LLMs to translate graph counterfactuals to natural language is validated on two datasets and two explainer types, but broader domain coverage is needed

## Next Checks

1. **Human Evaluation Validation**: Conduct a comprehensive human evaluation study with domain experts and non-technical users to validate whether the automated metrics (TNI, CCI, FTNF, CFTNF, CFTNN) align with human judgments of explanation quality and usefulness.

2. **Cross-Domain Generalization**: Test the approach on diverse real-world graph datasets from different domains (e.g., social networks, biological networks, financial transaction graphs) to assess generalization beyond academic citation networks.

3. **Real-time Performance Evaluation**: Measure the computational efficiency and latency of generating explanations with different LLM sizes in real-time applications, particularly for large-scale graphs with millions of nodes and edges.