---
ver: rpa2
title: Learning Elementary Cellular Automata with Transformers
arxiv_id: '2412.01417'
source_url: https://arxiv.org/abs/2412.01417
tags:
- state
- rule
- training
- steps
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether Transformers can learn to abstract
  and generalize the rules governing Elementary Cellular Automata (ECAs). The research
  focuses on training Transformers on state sequences generated with random initial
  conditions and local rules, aiming to assess their ability to infer and apply logical
  rules from data.
---

# Learning Elementary Cellular Automata with Transformers

## Quick Facts
- arXiv ID: 2412.01417
- Source URL: https://arxiv.org/abs/2412.01417
- Authors: Mikhail Burtsev
- Reference count: 36
- Primary result: Transformers can learn to abstract and generalize Boolean rules governing Elementary Cellular Automata from state sequences

## Executive Summary
This study investigates whether Transformers can learn to abstract and generalize the rules governing Elementary Cellular Automata (ECAs) by training on state sequences generated with random initial conditions and local rules. The research demonstrates that Transformers can achieve high accuracy in next-state prediction and generalize across different Boolean functions of fixed arity. The study explores four different tasks: predicting the next state, predicting multiple future states, predicting the next state and rule, and predicting the next state given a rule and orbit. Key findings show that including future states or rule prediction in training loss enhances the models' ability to form internal representations of the rules, leading to improved performance in longer planning horizons and autoregressive generation.

## Method Summary
The core method involves training Transformer encoders with full self-attention on four different tasks using a dataset of 9.5 Ã— 10^5 samples with lattice size W = 20 and neighborhood radius r = 2. The Transformer architecture uses 4 layers, 8 attention heads, and embedding dimension 512. Input encoding employs a vocabulary including binary states, separator, and mask tokens. The study evaluates per-bit accuracy averaged over free runs across tasks including next-state prediction (O-S), multi-future state prediction (O-O), next-state and rule prediction (O-SR), and next-state given rule and orbit (RO-S).

## Key Results
- Transformers achieve high accuracy (0.96) in next-state prediction and generalize across different Boolean functions of fixed arity
- Performance declines sharply in multi-step planning tasks without intermediate context
- Including future states or rule prediction in training loss enhances models' ability to form internal rule representations, improving performance in longer planning horizons
- Increasing model depth plays a crucial role in extended sequential computations required for complex reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers can abstract Boolean rules from state sequences without explicit symbolic input
- Mechanism: The model learns to encode local patterns across input orbits and generalizes to unseen rules by recognizing logical structure in state transitions
- Core assumption: State sequences contain sufficient information to infer the underlying rule
- Evidence anchors:
  - [abstract] "training Transformers on state sequences generated with random initial conditions and local rules, we show that they can generalize across different Boolean functions of fixed arity"
  - [section] "Successful learning of the O-S task demonstrate that the Transformer model is capable of generalizing not only over initial conditions for a particular function... but also across different Boolean functions of fixed arity"
  - [corpus] Weak evidence - related papers focus on CA classification but not rule abstraction from data
- Break condition: If rule complexity exceeds model capacity or training data lacks sufficient diversity in local patterns

### Mechanism 2
- Claim: Including future states or rule prediction in training loss improves long-horizon planning
- Mechanism: Additional training objectives force the model to develop explicit internal representations of the rules, which then generalize better across multiple steps
- Core assumption: Rule representation is more stable and generalizable than state-to-state transition mapping
- Evidence anchors:
  - [abstract] "including future states or rule prediction in the training loss enhances the models' ability to form internal representations of the rules"
  - [section] "for k = 2 and k = 3, the O-SR model outperformed the O-O model with accuracies of approximately 0.85 versus 0.75"
  - [corpus] No direct evidence - this is a novel contribution of the paper
- Break condition: If model depth is insufficient to support the additional computational requirements

### Mechanism 3
- Claim: Model depth scales linearly with planning horizon complexity
- Mechanism: Each additional planning step requires a proportional increase in computational layers to perform sequential rule applications
- Core assumption: Transformer layers can be composed sequentially to implement multi-step reasoning
- Evidence anchors:
  - [abstract] "increasing the model's depth plays a crucial role in extended sequential computations required for complex reasoning tasks"
  - [section] "Figure 3A shows that the O-O model begins to accurately predict the next state with 2 layers. Predicting two steps ahead requires at least 4 layers, three steps necessitate 7 layers, and four steps demand 10 layers"
  - [corpus] No direct evidence - this scaling relationship appears novel to this work
- Break condition: If depth scaling hits computational or optimization barriers before achieving target planning horizons

## Foundational Learning

- Concept: Boolean function abstraction
  - Why needed here: The core task requires learning logical rules governing state transitions, not just pattern matching
  - Quick check question: Can you explain how a 5-argument Boolean function maps neighborhood states to the next cell state?

- Concept: Sequential computation in transformers
  - Why needed here: Multi-step planning requires chaining multiple rule applications, testing the transformer's ability to perform extended sequential reasoning
  - Quick check question: How does the number of transformer layers relate to the number of sequential reasoning steps it can perform?

- Concept: Self-attention mechanism
  - Why needed here: The model must attend to relevant parts of the input orbit to infer rules and predict future states
  - Quick check question: What role does self-attention play in allowing the transformer to recognize patterns across the input sequence?

## Architecture Onboarding

- Component map: Input encoding -> self-attention layers -> final prediction head
- Critical path: Data flows through sequential transformer layers with full attention between all positions
- Design tradeoffs: Fixed depth vs. adaptive computation time; parallel vs. autoregressive prediction; explicit rule input vs. rule inference from states
- Failure signatures: Sharp accuracy drop for multi-step planning without intermediate context; performance plateaus after certain input length; depth scaling requirements
- First 3 experiments:
  1. Verify next-state prediction accuracy on O-S task with varying input lengths to find optimal Tmin
  2. Test planning accuracy for k=1,2,3 steps to establish baseline performance decay
  3. Compare O-SR vs O-O training objectives for 2-step planning to validate rule representation hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different architectural modifications (like recurrence, adaptive computation time, or increased model depth) impact the ability of Transformers to plan multiple steps ahead in cellular automata?
- Basis in paper: [explicit] The authors explicitly state that recurrence and adaptive computation time are promising directions for dynamic control of model depth, and they show that increasing depth helps with extended sequential computations.
- Why unresolved: The paper only tested fixed-depth Transformer encoders and didn't explore recurrent architectures or adaptive computation time, leaving open whether these modifications would improve multi-step planning.
- What evidence would resolve it: Experiments comparing Transformer encoders with recurrent variants (like Universal Transformers) or models with adaptive computation time, measuring planning accuracy across different look-ahead steps.

### Open Question 2
- Question: Does the performance drop in multi-step planning stem from limitations in the Transformer's attention mechanism or from insufficient training data?
- Basis in paper: [inferred] The authors suggest the decline might be due to the Transformer's inability to store intermediate states or limitations in sequential computation, but they don't test whether more training data or different attention mechanisms would help.
- Why unresolved: The study used a fixed dataset size and standard self-attention without exploring alternative attention mechanisms or data augmentation techniques that might improve planning performance.
- What evidence would resolve it: Experiments varying training data size, testing different attention mechanisms (like sparse attention or relative position embeddings), and measuring the impact on planning accuracy.

### Open Question 3
- Question: How does the performance of Transformers in learning cellular automata rules compare to other neural architectures like graph neural networks or convolutional networks?
- Basis in paper: [explicit] The authors note that transformers can abstract and generalize Boolean functions of fixed arity, but they don't compare their performance to other architectures commonly used for cellular automata.
- Why unresolved: The study only used Transformer encoders without benchmarking against other architectures like graph neural networks, convolutional networks, or dedicated cellular automata models.
- What evidence would resolve it: Direct comparisons of Transformer performance against graph neural networks, convolutional networks, and specialized cellular automata models on the same tasks and datasets.

## Limitations

- Performance degradation in multi-step planning without intermediate context suggests limitations in extended sequential reasoning
- Depth scaling relationship may hit computational or optimization barriers before achieving arbitrarily long planning horizons
- Study focuses on ECAs with fixed neighborhood radius r=2, leaving open questions about scalability to more complex rule sets

## Confidence

- **High Confidence**: The core finding that Transformers can learn to abstract Boolean rules from state sequences is well-supported by empirical results across multiple tasks and rule types
- **Medium Confidence**: The claim about including future states or rule prediction improving long-horizon planning is supported by comparative results, but the mechanism could benefit from further theoretical analysis
- **Medium Confidence**: The linear depth scaling relationship with planning horizon complexity is empirically validated but may not hold for more complex CA rules

## Next Checks

1. Evaluate model performance on ECAs with varying neighborhood radii (r > 2) to assess scalability and identify potential depth requirements for more complex rules

2. Test whether models trained on simpler ECAs can effectively transfer knowledge to more complex CA rules, measuring both accuracy and depth requirements for successful transfer

3. Compare Transformer performance against other sequence models (LSTMs, CNNs) on the same ECA tasks to isolate whether the observed capabilities are specific to self-attention mechanisms or general to sequence models