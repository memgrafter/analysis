---
ver: rpa2
title: 'Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed
  Program Sampling'
arxiv_id: '2402.19471'
source_url: https://arxiv.org/abs/2402.19471
tags:
- ship
- questions
- board
- blue
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a model for generating informative questions
  in a grounded environment using language models and program sampling. The model,
  called LIPS, samples questions from a language model prior, translates them into
  symbolic programs, and evaluates their expected information gain to choose the most
  informative one.
---

# Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling

## Quick Facts
- arXiv ID: 2402.19471
- Source URL: https://arxiv.org/abs/2402.19471
- Reference count: 40
- Primary result: LLM-based models approach human-level mean informativeness in generating Battleship questions

## Executive Summary
This paper presents LIPS (Language-Informed Program Sampling), a framework for generating informative questions in grounded environments using large language models. The approach samples questions from language models, translates them into symbolic programs, and evaluates their expected information gain to select the most informative one. Experiments on the Battleship task demonstrate that LIPS with CodeLlama and GPT-4 approaches human-level performance in question informativeness, outperforming traditional grammar-based baselines. The models achieve human-level mean performance with just 5-10 samples, though they still produce redundant questions and fall short of the best human questioners.

## Method Summary
LIPS combines language model priors with symbolic program sampling to generate informative questions in a grounded Battleship environment. The method first samples questions from a language model prior, then translates these questions into symbolic programs that can be evaluated against the game state. Each candidate question is scored based on its expected information gain using entropy reduction metrics. The system then selects the question with the highest expected informativeness. This approach leverages the language model's ability to generate diverse, human-like questions while using symbolic evaluation to ensure the questions are actually informative given the current game state. The framework bridges the gap between natural language generation and formal program-based reasoning about information gain.

## Key Results
- LLM-based models (CodeLlama and GPT-4) approach human-level mean informativeness in Battleship question generation
- With 5-10 samples, LLM models match the mean human informativeness score
- The models exhibit human-like question type distributions when provided few-shot examples
- LIPS outperforms hand-engineered grammar baselines by a significant margin

## Why This Works (Mechanism)
The LIPS framework works by combining the generative capabilities of large language models with the precision of symbolic program evaluation. Language models provide diverse, contextually appropriate question templates that capture human-like reasoning patterns. The symbolic program translation ensures these questions can be formally evaluated for information gain against the game state. This dual approach allows the system to generate questions that are both linguistically natural and strategically informative, leveraging the strengths of both neural language understanding and formal program semantics.

## Foundational Learning

**Language Model Priors** - Why needed: Provides diverse, human-like question generation patterns that capture linguistic variety and contextual appropriateness. Quick check: Sample 100 questions and verify linguistic diversity and contextual relevance.

**Symbolic Program Translation** - Why needed: Converts natural language questions into executable programs that can be formally evaluated against game state. Quick check: Test translation accuracy on benchmark question set and verify executable correctness.

**Information Gain Evaluation** - Why needed: Quantifies the expected reduction in uncertainty that a question provides, enabling selection of most informative questions. Quick check: Validate entropy calculations against known game states and verify information gain rankings.

## Architecture Onboarding

**Component Map**: Language Model -> Question Sampling -> Program Translation -> Information Gain Evaluation -> Question Selection

**Critical Path**: The critical path flows from language model sampling through program translation to information gain evaluation, with question selection as the final decision point. Each component must successfully complete before the next can begin.

**Design Tradeoffs**: The system trades computational efficiency for question quality by evaluating multiple sampled questions. While this increases runtime, it ensures selection of the most informative question rather than settling for the first reasonable candidate.

**Failure Signatures**: Model failures manifest as either syntactically invalid programs (translation failures) or semantically uninformative questions (evaluation failures). Translation failures typically indicate model hallucination, while evaluation failures suggest poor grounding or redundancy.

**3 First Experiments**:
1. Sample 100 questions from language model without evaluation to establish baseline diversity
2. Test program translation accuracy on a benchmark of human-generated questions
3. Evaluate information gain calculations on simplified game states with known optimal questions

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Models still produce many redundant or uninformative questions that don't consider board state
- Performance falls short of the best human questioners despite matching mean human informativeness
- The Battleship task represents a narrow domain that may not generalize to more complex question-asking scenarios

## Confidence
High confidence in the claim that LLM-based models approach human-level mean informativeness, based on quantitative results and clear statistical improvements over baselines.
Medium confidence in practical implications, as models match average performance but fall short of best human performance with high variance.
Medium confidence in grounding limitations, as the analysis of redundant questions is somewhat superficial.
Low confidence in generalizability to complex, open-ended scenarios due to the narrow scope of the Battleship task.

## Next Checks
1. Conduct detailed qualitative and quantitative analysis of redundant questions to identify systematic failure patterns in the language-informed program sampling approach.

2. Test model performance on novel board configurations not seen during training to assess generalization of question-asking strategies.

3. Implement user studies where human players interact with model-generated questions in actual gameplay to measure overall effectiveness beyond individual question informativeness.