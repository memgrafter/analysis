---
ver: rpa2
title: 'When Training-Free NAS Meets Vision Transformer: A Neural Tangent Kernel Perspective'
arxiv_id: '2405.04536'
source_url: https://arxiv.org/abs/2405.04536
tags:
- search
- neural
- space
- metrics
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying training-free neural
  architecture search (NAS) methods based on Neural Tangent Kernel (NTK) to Vision
  Transformers (ViTs). While NTK-based metrics effectively predict CNN performance,
  they show limited efficacy for ViTs.
---

# When Training-Free NAS Meets Vision Transformer: A Neural Tangent Kernel Perspective

## Quick Facts
- arXiv ID: 2405.04536
- Source URL: https://arxiv.org/abs/2405.04536
- Authors: Qiqi Zhou; Yichen Zhu
- Reference count: 0
- Primary result: ViNTK method significantly accelerates search costs (up to 30x) for Vision Transformers while maintaining similar performance

## Executive Summary
This paper addresses the challenge of applying training-free neural architecture search (NAS) methods based on Neural Tangent Kernel (NTK) to Vision Transformers (ViTs). While NTK-based metrics effectively predict CNN performance, they show limited efficacy for ViTs. The authors hypothesize that this stems from the conflict between ViT's low-frequency feature learning via self-attention and its reliance on high-frequency signals for image discrimination. They theoretically prove that NTK primarily estimates a network's ability to learn low-frequency signals, neglecting high-frequency components. To address this limitation, they propose ViNTK, which generalizes NTK to the high-frequency domain by integrating Fourier features from inputs.

## Method Summary
The proposed ViNTK method addresses NTK's limitations for ViT search by incorporating Fourier features into the NTK calculation. The approach leverages random Fourier features to approximate stationary kernel functions, transforming inputs into the Fourier domain. The ViNTK metric is then computed as the Hadamard product between the standard NTK matrix and the Fourier feature matrix. This integration allows the metric to better capture high-frequency signal learning capacity while maintaining the efficiency of training-free NAS. The method is evaluated on image classification and semantic segmentation tasks using both AutoFormer and NASViT search spaces, demonstrating significant acceleration in search costs while maintaining comparable performance.

## Key Results
- ViNTK achieves up to 30x search cost reduction compared to prior state-of-the-art NAS methods for ViTs
- The method maintains similar or slightly better accuracy than existing approaches on AutoFormer and NASViT search spaces
- Experiments on ImageNet-1K classification and Cityscapes/ADE20K semantic segmentation validate the effectiveness of ViNTK
- NTK-based metrics show stronger correlation with performance when only MSAs are involved in the search space, confirming NTK's focus on low-frequency learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NTK-based metrics work well for CNNs but fail for ViTs because NTK primarily estimates low-frequency learning capacity while ViTs rely heavily on high-frequency signals for image discrimination.
- Mechanism: The Neural Tangent Kernel approximates a network's ability to learn based on low-frequency signal components. ViTs use self-attention mechanisms that act as low-pass filters, but they still depend on high-frequency information for accurate image recognition. This creates a mismatch where NTK metrics cannot capture the full learning capacity of ViTs.
- Core assumption: NTK's predictive power is strongly correlated with the dimension of low-frequency signals a network can learn.
- Evidence anchors:
  - [abstract] "We both theoretically and empirically validate that NTK essentially estimates the ability of neural networks that learn low-frequency signals, completely ignoring the impact of high-frequency signals in feature learning."
  - [section] "Specifically, our analysis shows that NTK-based metrics are strongly correlated to the dimension of low-frequency signals. In other words, NTK can reasonably approximate the strength of low-frequency signals learned by neural networks."
  - [corpus] Weak - No direct evidence in corpus neighbors about NTK frequency limitations

### Mechanism 2
- Claim: The proposed ViNTK method improves NTK's effectiveness for ViT by integrating Fourier features to capture high-frequency signal learning capacity.
- Mechanism: By applying Fourier feature mapping to the input and combining it with NTK through Hadamard product, ViNTK balances the convergence rates across the Fourier spectrum. This enhancement allows the metric to better predict performance for architectures that rely on high-frequency signals.
- Core assumption: Random Fourier features can approximate an arbitrary stationary kernel function and effectively capture high-frequency components when combined with NTK.
- Evidence anchors:
  - [section] "We leverage random Fourier features [23], introduced to approximate an arbitrary stationary kernel function via Bochner's theorem. We use their method to feature input on the Fourier domain and combine it with NTK."
  - [section] "Experiments with multiple ViT search spaces on image classification and semantic segmentation tasks show that our method can significantly speed up search costs over prior state-of-the-art NAS for ViT while maintaining similar performance on searched architectures."
  - [corpus] Weak - No direct evidence in corpus neighbors about Fourier feature integration with NTK

### Mechanism 3
- Claim: NTK-based metrics show stronger correlation with performance when only MSAs are involved in the search space, confirming that NTK effectively estimates low-frequency learning capacity.
- Mechanism: By restricting the search space to only multi-head self-attention modules (which act as low-pass filters), the remaining architectural components that rely on high-frequency signals are eliminated. This creates a scenario where NTK metrics can effectively predict performance since they align with the low-frequency learning capacity of the network.
- Core assumption: MSAs are provable low-pass filters that primarily extract low-frequency features.
- Evidence anchors:
  - [section] "We again assess the NTK-based metrics on ViT search space, searching multi-head self-attention (MSAs) only. Surprisingly, the experimental results show that NTK can find promising ViT when only MSAs, provable low-pass filters, are involved in the search process."
  - [section] "Figure 2 showcases the correlation between NTK metrics and accuracy when only MSAs are considered. The correlation is significantly stronger than in a full search space."
  - [corpus] Weak - No direct evidence in corpus neighbors about MSA-specific NTK performance

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its role in training-free NAS
  - Why needed here: Understanding NTK is crucial because the paper's core contribution revolves around addressing NTK's limitations for ViT search. NTK serves as the foundation for training-free architecture evaluation.
  - Quick check question: What is the mathematical definition of NTK and how does it relate to network training dynamics?

- Concept: Fourier analysis and feature mapping in neural networks
  - Why needed here: The ViNTK method relies on Fourier features to capture high-frequency components that standard NTK misses. Understanding how Fourier analysis applies to neural networks is essential for grasping the proposed solution.
  - Quick check question: How do random Fourier features approximate stationary kernel functions and why is this useful for NTK generalization?

- Concept: Vision Transformer architecture and self-attention mechanisms
  - Why needed here: The paper specifically addresses ViTs, so understanding their unique components (MSAs, FFNs, patch embeddings) and how they differ from CNNs is necessary to appreciate why NTK-based methods fail for ViTs.
  - Quick check question: How do self-attention mechanisms in ViTs act as low-pass filters and what implications does this have for feature learning?

## Architecture Onboarding

- Component map:
  - NTK Calculator -> Fourier Feature Mapper -> ViNTK Combiner -> Correlation Evaluator -> Search Space Generator

- Critical path:
  1. Generate random ViT architecture
  2. Compute NTK matrix from random initialization
  3. Apply Fourier feature mapping to inputs
  4. Combine NTK and Fourier features to create ViNTK metric
  5. Evaluate correlation with ground truth performance
  6. Use metric for architecture search

- Design tradeoffs:
  - Computational cost vs. accuracy: ViNTK adds minimal computational overhead compared to standard NTK but provides significantly better correlation
  - Search space complexity: Restricting to MSAs-only improves NTK correlation but limits architectural diversity
  - Fourier basis selection: Full Fourier basis vs. partial basis affects high-frequency capture capability

- Failure signatures:
  - Low Kendall-Tau correlation between ViNTK and actual performance
  - Inconsistent performance across different search spaces
  - Computational cost approaching that of training-based NAS
  - Poor generalization from proxy metrics to actual architecture performance

- First 3 experiments:
  1. Implement standard NTK calculation on a simple CNN search space (like NAS-Bench-201) to establish baseline correlation
  2. Add Fourier feature mapping to the NTK calculation and measure the impact on correlation for the same CNN search space
  3. Apply the complete ViNTK method to a pure ViT search space and compare correlation with both standard NTK and training-based results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ViNTK's performance generalize to other types of architectures beyond ViT and hybrid CNN-ViT models?
- Basis in paper: [inferred] The paper primarily tests ViNTK on ViT and hybrid architectures, but doesn't explore its applicability to other types of models.
- Why unresolved: The paper's focus is on ViTs and doesn't extend the evaluation to other architectures, leaving the question of ViNTK's broader applicability open.
- What evidence would resolve it: Empirical results showing ViNTK's performance on a variety of architectures, including CNNs, RNNs, and other transformer-based models, would provide clarity on its generalization capabilities.

### Open Question 2
- Question: What is the impact of varying the number of Fourier features (p) on the performance of ViNTK?
- Basis in paper: [explicit] The paper mentions that p = 2 is used in experiments but doesn't explore the effects of different values of p.
- Why unresolved: The paper does not provide an analysis of how the choice of p affects ViNTK's performance, leaving this aspect unexplored.
- What evidence would resolve it: A study examining ViNTK's performance across a range of p values would elucidate the impact of Fourier features on its effectiveness.

### Open Question 3
- Question: How does ViNTK perform in terms of search cost and accuracy compared to training-based NAS methods when applied to tasks beyond image classification and semantic segmentation?
- Basis in paper: [explicit] The paper evaluates ViNTK on image classification and semantic segmentation tasks, but does not explore its effectiveness in other domains.
- Why unresolved: The paper's scope is limited to specific tasks, and it does not provide insights into ViNTK's performance on other tasks like object detection or video processing.
- What evidence would resolve it: Comparative studies of ViNTK against training-based NAS methods across a broader range of tasks would clarify its versatility and efficiency in different domains.

## Limitations

- The paper's focus on classification and segmentation tasks may not generalize to other vision tasks like object detection or video analysis
- The search spaces used (AutoFormer, NASViT) may not fully represent the architectural diversity of practical ViT applications
- The computational overhead of Fourier feature integration, while claimed to be minimal, is not thoroughly benchmarked against alternative approaches

## Confidence

- NTK frequency limitation claim: High confidence - Supported by theoretical analysis and experimental evidence, particularly MSA-only search space results
- ViNTK effectiveness claim: Medium confidence - Theoretically sound but lacks extensive empirical validation across diverse architectures
- Fourier feature integration overhead claim: Low confidence - Minimal details provided about computational cost comparisons

## Next Checks

1. **Cross-Task Validation**: Apply ViNTK to object detection and video analysis tasks to verify generalization beyond classification and segmentation.
2. **Search Space Diversity Test**: Evaluate ViNTK on additional ViT search spaces with different architectural constraints (e.g., Swin, DeiT variants) to assess robustness.
3. **Computational Overhead Analysis**: Quantify the exact computational cost of ViNTK compared to standard NTK and training-based NAS methods across multiple hardware configurations.