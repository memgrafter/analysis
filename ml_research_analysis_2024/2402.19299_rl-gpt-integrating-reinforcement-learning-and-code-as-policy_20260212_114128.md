---
ver: rpa2
title: 'RL-GPT: Integrating Reinforcement Learning and Code-as-policy'
arxiv_id: '2402.19299'
source_url: https://arxiv.org/abs/2402.19299
tags:
- agent
- arxiv
- actions
- tasks
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RL-GPT, a two-level hierarchical framework
  that combines Large Language Models (LLMs) and Reinforcement Learning (RL) to address
  complex embodied tasks. The framework decomposes tasks into high-level coding actions
  and low-level RL-based actions, leveraging the strengths of both approaches.
---

# RL-GPT: Integrating Reinforcement Learning and Code-as-policy

## Quick Facts
- arXiv ID: 2402.19299
- Source URL: https://arxiv.org/abs/2402.19299
- Reference count: 40
- Key outcome: RL-GPT outperforms traditional RL methods and existing GPT agents, achieving state-of-the-art performance across all designated MineDojo tasks, obtaining diamonds within a single day on an RTX3090 GPU

## Executive Summary
This paper introduces RL-GPT, a two-level hierarchical framework that combines Large Language Models (LLMs) and Reinforcement Learning (RL) to address complex embodied tasks in Minecraft. The framework decomposes tasks into high-level coding actions and low-level RL-based actions, leveraging the strengths of both approaches. By integrating high-level GPT-coded actions into the RL action space, RL-GPT demonstrates superior efficiency and sample efficiency compared to traditional RL methods and existing GPT agents.

## Method Summary
RL-GPT employs a two-level hierarchical framework with a slow agent (GPT-4) for task decomposition and action planning, a fast agent (GPT-4) for code generation and RL configuration, and a critic agent for optimization. The approach selectively applies RL versus code-as-policy by having the slow agent identify which sub-actions can be coded directly and which require RL learning. Coded actions are executed directly while challenging actions are integrated into the RL action space, reducing the RL search space and improving sample efficiency.

## Key Results
- Achieved state-of-the-art performance across all designated MineDojo tasks
- Obtained diamonds within a single day on an RTX3090 GPU in Minecraft
- Demonstrated superior efficiency by integrating high-level GPT-coded actions into the RL action space

## Why This Works (Mechanism)

### Mechanism 1
- Decomposing tasks into sub-actions and selectively applying RL vs. code-as-policy improves sample efficiency
- The slow agent identifies which sub-actions can be coded directly and which require RL learning
- Core assumption: LLMs can accurately identify which actions are too complex for direct coding

### Mechanism 2
- Integrating high-level coded actions into the RL action space improves RL sample efficiency
- By providing RL with high-level actions as additional options, the policy can learn to use these actions at appropriate times
- Core assumption: The RL agent can effectively learn when to use the provided high-level actions

### Mechanism 3
- Two-loop iteration with critic agent enables progressive refinement of both agents
- The slow agent iteratively refines its action decomposition based on feedback from the critic agent
- Core assumption: The critic agent can provide meaningful feedback to guide improvements

## Foundational Learning

- **Concept: Hierarchical task decomposition**
  - Why needed here: Breaking complex tasks into manageable sub-actions allows appropriate assignment to coding or RL
  - Quick check question: How does decomposing "harvest a log" into "find tree" and "cut log" improve learning efficiency?

- **Concept: Temporal abstraction in RL**
  - Why needed here: Integrating high-level coded actions into RL action space requires understanding how to represent temporal sequences
  - Quick check question: What is the difference between a primitive action and a temporally abstracted action in RL?

- **Concept: Reinforcement learning with sparse rewards**
  - Why needed here: Minecraft tasks have sparse rewards, requiring techniques like distance rewards to improve learning
  - Quick check question: How do distance rewards help in environments where the agent rarely receives task completion rewards?

## Architecture Onboarding

- **Component map**: Slow agent (GPT-4) -> Fast agent (GPT-4) -> RL training loop -> Critic agent (GPT-3.5/GPT-4) -> Slow agent feedback

- **Critical path**: 1. Slow agent decomposes task and identifies coded vs. RL actions 2. Fast agent generates code and RL configuration 3. RL training with integrated coded actions 4. Critic agent evaluates results and provides feedback 5. Iteration based on feedback

- **Design tradeoffs**: Using GPT-4 for both slow and fast agents provides capability but increases cost; two-loop iteration improves results but adds complexity and training time; integrating coded actions into RL action space requires careful design of action space interfaces

- **Failure signatures**: Low success rate indicates poor action decomposition or code generation; high dead loop ratio suggests RL agent fails to learn when to use high-level actions; inconsistent performance across tasks may indicate task-specific decomposition issues

- **First 3 experiments**: 1. Test single-task decomposition (e.g., "harvest a log") with both agents to validate basic functionality 2. Evaluate RL sample efficiency with and without integrated coded actions 3. Test iteration mechanism by comparing performance across 1, 2, and 3 iterations

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of RL-GPT scale with the complexity of tasks in Minecraft, and what is the upper limit of task complexity it can handle effectively?
- **Open Question 2**: What are the long-term effects of using RL-GPT on the capabilities and behavior of Large Language Models, and does it lead to any degradation in their general language understanding abilities?
- **Open Question 3**: How does the efficiency of RL-GPT compare to other hybrid approaches that integrate LLMs with RL, such as those that fine-tune LLMs directly for low-level control?
- **Open Question 4**: What are the limitations of RL-GPT in terms of the types of tasks it can effectively decompose and learn, and are there specific categories of tasks where it struggles?
- **Open Question 5**: How does the choice of the critic agent's model (e.g., GPT-3.5 vs. GPT-4) affect the performance and efficiency of the two-loop iteration process in RL-GPT?

## Limitations

- Requires access to GPT-4, which is expensive and may not be available to all researchers
- Specific prompts used for the slow and fast agents are not fully detailed, making it difficult to reproduce exact behavior
- Relies on effective task decomposition, which may be challenging for more complex or less structured tasks beyond Minecraft

## Confidence

- **High Confidence**: The core mechanism of integrating coded actions into the RL action space and the two-loop iteration approach
- **Medium Confidence**: The effectiveness of task decomposition and the specific implementation details of the GPT agents
- **Low Confidence**: The generalizability of the approach to tasks outside of Minecraft and scalability to more complex, real-world scenarios

## Next Checks

1. Implement the RL-GPT framework with provided details and attempt to reproduce results on MineDojo benchmark tasks
2. Conduct an ablation study to determine individual contributions of slow agent, fast agent, and critic agent to overall performance
3. Evaluate the RL-GPT approach on a different embodied task environment, such as a robotics simulation or different game, to assess generalizability beyond Minecraft