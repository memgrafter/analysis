---
ver: rpa2
title: What do MLLMs hear? Examining reasoning with text and sound components in Multimodal
  Large Language Models
arxiv_id: '2406.04615'
source_url: https://arxiv.org/abs/2406.04615
tags:
- audio
- reasoning
- arxiv
- mllms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether multimodal large language models
  (MLLMs) can leverage their reasoning capabilities across different modalities. The
  authors conduct two experiments to test if an audio MLLM can use its text-based
  reasoning for audio classification and concept understanding.
---

# What do MLLMs hear? Examining reasoning with text and sound components in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2406.04615
- Source URL: https://arxiv.org/abs/2406.04615
- Authors: Enis Berk Çoban; Michael I. Mandel; Johanna Devaney
- Reference count: 24
- Primary result: Audio MLLMs struggle to leverage text-based reasoning for audio classification and hierarchical reasoning tasks

## Executive Summary
This paper investigates whether multimodal large language models (MLLMs) can effectively leverage reasoning capabilities across text and audio modalities. Through two experiments, the authors demonstrate that while MLLMs excel at text-based reasoning, they struggle to transfer this capability to audio inputs. The study reveals that audio MLLMs do not fully integrate auditory and textual information, limiting their reasoning abilities across modalities. These findings highlight the need for better alignment between modalities to improve MLLMs' reasoning capabilities.

## Method Summary
The paper conducts two experiments using the LTU model (13B version) with an Audio Spectrogram Transformer for audio encoding. Experiment 1 tests in-context audio classification using the EDANSA bioacoustics dataset with 28 labels across 10,782 10-second samples. The model generates captions for audio inputs and scores them against labels using text-embedding-ada-002 similarity. Experiment 2 evaluates hierarchical reasoning by asking the model to identify semantic relationships (synonyms and hypernyms) between concept words using audio samples from both AudioSet and EDANSA. The experiments compare performance across different fine-tuning strategies, prompt modifications, and text versus audio conditions.

## Key Results
- Audio MLLMs show poor performance on in-context audio classification, particularly for complex sounds like grouse calls, despite providing detailed descriptions
- The model struggles with hierarchical reasoning on audio inputs, unable to effectively identify hypernyms and synonyms in audio-based queries
- Performance is significantly worse with out-of-distribution sound data (EDANSA) compared to training data (AudioSet), suggesting limited generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Audio MLLMs map specific audio tokens to associated words rather than leveraging general audio embeddings for reasoning.
- Mechanism: When presented with audio input, the model first processes it through the audio encoder to obtain embeddings. These embeddings are then tokenized and fed into the LLM. However, the LLM focuses on specific audio tokens that correspond to the related audio class, mapping these tokens to associated words, rather than utilizing the general properties encapsulated in the embeddings for reasoning.
- Core assumption: The LLM component in the audio MLLM is primarily focused on processing text-based information and has limited ability to directly reason with the general properties of audio embeddings.
- Evidence anchors:
  - [abstract]: "Previous work has demonstrated that when the LLM component in MLLMs is frozen, the audio or visual encoder serves to caption the sound or image input facilitating text-based reasoning with the LLM component."
  - [section]: "The performance improvement observed when transitioning from partial to full fine-tuning suggests that the LLM is not merely processing the general audio properties encapsulated in the embeddings. Instead, it appears to focus on specific audio tokens that correspond to the related audio class, indicating that the LLM is mapping specific sound events to their associated words."
  - [corpus]: The corpus contains papers that discuss the limitations of MLLMs in reasoning with audio inputs, supporting the claim that the LLM component primarily processes text-based information.

### Mechanism 2
- Claim: Audio MLLMs exhibit a lack of semantic grounding, limiting their ability to leverage textual relationships present in the LLM in the audio modality.
- Mechanism: The LLM component in the audio MLLM has learned semantic relationships between concepts from textual data. However, when presented with audio data, the model struggles to transfer this understanding to the audio modality. This is because the model does not have strong connections between audio and textual concepts, limiting its ability to perform hierarchical-related reasoning on audio input.
- Core assumption: The LLM component has learned semantic relationships between concepts from textual data, but these relationships are not effectively transferred to the audio modality.
- Evidence anchors:
  - [abstract]: "Previous work has demonstrated that when the LLM component in MLLMs is frozen, the audio or visual encoder serves to caption the sound or image input facilitating text-based reasoning with the LLM component."
  - [section]: "We also consider how this may be due to MLLMs separately representing auditory and textual information such that it severs the reasoning pathway from the LLM to the audio encoder."
  - [corpus]: The corpus contains papers that discuss the lack of semantic grounding in MLLMs, supporting the claim that the model struggles to transfer textual relationships to the audio modality.

### Mechanism 3
- Claim: Audio MLLMs have limited ability to process and understand out-of-distribution sound data.
- Mechanism: The model's performance is worse with EDANSA samples compared to AudioSet samples, indicating potential difficulties in processing and understanding out-of-distribution sound data. This is because the model has been primarily trained on AudioSet, and the EDANSA dataset contains different types of audio data that the model has not encountered during training.
- Core assumption: The model's performance is negatively impacted when presented with out-of-distribution sound data.
- Evidence anchors:
  - [section]: "We also observed that the model's performance is worse with EDANSA samples compared to AudioSet samples, indicating potential difficulties in processing and understanding out-of-distribution sound data."
  - [corpus]: The corpus contains papers that discuss the challenges of processing out-of-distribution data in MLLMs, supporting the claim that the model struggles with unfamiliar audio data.

## Foundational Learning

- Concept: Multimodal Large Language Models (MLLMs)
  - Why needed here: Understanding the architecture and functioning of MLLMs is crucial for analyzing their reasoning capabilities and limitations.
  - Quick check question: What are the main components of an MLLM, and how do they interact with each other?

- Concept: In-context learning
  - Why needed here: In-context learning is a key mechanism used in MLLMs to perform tasks without explicit training on those tasks. Understanding this concept is essential for evaluating the model's reasoning abilities.
  - Quick check question: How does in-context learning work in MLLMs, and what are its limitations?

- Concept: Semantic relationships
  - Why needed here: Semantic relationships, such as synonyms and hypernyms, play a crucial role in the model's ability to reason about concepts. Understanding these relationships is important for analyzing the model's performance.
  - Quick check question: What are synonyms and hypernyms, and how do they relate to the model's reasoning capabilities?

## Architecture Onboarding

- Component map: Audio input → Audio encoder → Tokenizer → LLM → Text encoder → Output
- Critical path: Audio input → Audio encoder → Tokenizer → LLM → Text encoder → Output
- Design tradeoffs:
  - Using a frozen LLM component allows for leveraging pre-trained language understanding but may limit the model's ability to adapt to audio-specific tasks.
  - Fine-tuning the LLM component can improve performance on audio tasks but may lead to catastrophic forgetting of previously learned language knowledge.
- Failure signatures:
  - Inability to correctly classify audio samples, especially for complex or out-of-distribution sounds.
  - Struggles with hierarchical reasoning tasks when presented with audio input.
  - Hallucinations or generation of incorrect information when prompted with audio-related questions.
- First 3 experiments:
  1. Evaluate the model's performance on in-context audio classification tasks using a diverse set of audio samples.
  2. Test the model's ability to perform hierarchical reasoning on audio inputs by asking questions about semantic relationships between concepts.
  3. Assess the model's performance on out-of-distribution sound data to identify potential limitations in processing unfamiliar audio.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can audio MLLMs effectively leverage hierarchical reasoning for audio classification tasks?
- Basis in paper: [explicit] The authors note that audio MLLMs struggle with hierarchical reasoning, particularly in Experiment 2 where they failed to correctly identify hypernyms in audio-based queries.
- Why unresolved: The paper only tests one specific audio MLLM (LTU) and uses a limited set of audio samples. It's unclear if this limitation is universal across all audio MLLMs or if it's specific to the LTU model or the particular dataset used.
- What evidence would resolve it: Testing multiple audio MLLMs on a broader range of hierarchical reasoning tasks and datasets would determine if this is a general limitation of audio MLLMs or specific to the tested model.

### Open Question 2
- Question: How can we improve the alignment between audio and textual representations in MLLMs to enhance reasoning capabilities?
- Basis in paper: [explicit] The authors suggest that the lack of semantic grounding and separate representation of auditory and textual information in MLLMs limits their reasoning abilities.
- Why unresolved: The paper proposes potential solutions like generating additional data pairs or finer-grained alignment but does not explore these in detail or evaluate their effectiveness.
- What evidence would resolve it: Developing and testing specific alignment techniques, such as contrastive learning or cross-modal attention mechanisms, and evaluating their impact on reasoning tasks would provide concrete solutions to this problem.

### Open Question 3
- Question: What are the underlying mechanisms that cause catastrophic forgetting in MLLMs when incorporating new modalities?
- Basis in paper: [explicit] The authors mention that MLLMs can experience catastrophic forgetting, which reduces their reasoning capabilities, particularly when non-linguistic prompts are used.
- Why unresolved: While the paper acknowledges this issue, it doesn't delve into the specific mechanisms or propose detailed strategies to mitigate catastrophic forgetting in MLLMs.
- What evidence would resolve it: Investigating the impact of different fine-tuning strategies, such as elastic weight consolidation or rehearsal methods, on MLLMs' ability to retain reasoning capabilities across modalities would shed light on the causes and potential solutions for catastrophic forgetting.

## Limitations

- The study focuses on a single MLLM architecture (LTU), limiting generalizability to other audio MLLM implementations
- Experiments use a limited set of audio samples and concept relationships, which may not capture the full complexity of cross-modal reasoning
- The paper does not explore potential solutions to the identified limitations, such as alternative training approaches or architectural modifications

## Confidence

**High Confidence**: The findings regarding poor performance on in-context audio classification, particularly for complex sounds like grouse calls, are well-supported by quantitative metrics (AUC and F1 scores) across multiple experimental conditions. The observation that performance drops significantly when actual audio is introduced versus text descriptions is consistently demonstrated.

**Medium Confidence**: The claim that audio MLLMs do not effectively leverage textual reasoning capabilities for audio inputs is supported by experimental evidence, but the exact mechanisms underlying this limitation require further investigation. The paper provides plausible explanations but does not definitively establish whether this represents a fundamental architectural limitation or a training data issue.

**Low Confidence**: The assertion that current MLLMs cannot achieve meaningful cross-modal reasoning capabilities is not fully established. While the experiments demonstrate current limitations, they do not explore alternative architectures, training approaches, or whether incremental improvements could overcome these barriers.

## Next Checks

1. **Architectural Ablation Study**: Test whether the observed limitations persist across multiple MLLM architectures (e.g., AudioPaLM, AudioGPT) using the same experimental protocols to determine if the findings are architecture-specific or represent broader challenges in the MLLM paradigm.

2. **Fine-tuning Impact Analysis**: Conduct a systematic study varying fine-tuning strategies (full vs partial, different learning rates, early stopping criteria) to quantify the relationship between model adaptation and cross-modal reasoning performance, particularly for out-of-distribution sounds.

3. **Cross-modal Transfer Experiment**: Design a controlled experiment where the model is explicitly trained on paired audio-text relationships for the specific concept hierarchies tested, then measure whether this targeted training improves hierarchical reasoning performance on audio inputs compared to the zero-shot results reported in the paper.