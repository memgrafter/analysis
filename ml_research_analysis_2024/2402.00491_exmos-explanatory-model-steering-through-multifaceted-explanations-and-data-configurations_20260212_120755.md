---
ver: rpa2
title: 'EXMOS: Explanatory Model Steering Through Multifaceted Explanations and Data
  Configurations'
arxiv_id: '2402.00491'
source_url: https://arxiv.org/abs/2402.00491
tags:
- data
- explanations
- system
- prediction
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examined the effectiveness of different types of global\
  \ explanations in interactive machine learning systems for healthcare experts to\
  \ improve prediction models. A prototype system with three versions of the explanation\
  \ dashboard\u2014data-centric, model-centric, and hybrid\u2014was developed."
---

# EXMOS: Explanatory Model Steering Through Multifaceted Explanations and Data Configurations

## Quick Facts
- arXiv ID: 2402.00491
- Source URL: https://arxiv.org/abs/2402.00491
- Reference count: 40
- Primary result: Hybrid explanations combining data-centric and model-centric views significantly improved prediction model performance compared to single-explanation approaches

## Executive Summary
This study evaluated how different types of global explanations affect healthcare experts' ability to improve prediction models through data configurations. A prototype system with three explanation dashboard versions (data-centric, model-centric, hybrid) was developed and tested with 70 participants. The hybrid version that combined both explanation types led to significantly better prediction accuracy improvements compared to the single-explanation approaches, though it also increased perceived task load. The qualitative study with 30 participants revealed that data-centric explanations were particularly valuable for understanding post-configuration system changes, while model-centric explanations alone were insufficient for effective model steering.

## Method Summary
The study employed a mixed-methods approach using a diabetes prediction task with the Pima Indians Diabetes dataset (768 patients, 8 predictors). Three prototype versions were created with different explanation dashboards (DCE, MCE, HYB) combined with manual and automated data configuration mechanisms. The quantitative study involved 70 participants randomly assigned to each prototype version, measuring prediction accuracy improvement, perceived task load (NASA-TLX), objective understanding (mental model questionnaire), perceived understandability, and perceived trust. A qualitative study with 30 participants explored user experiences and perceptions through semi-structured interviews and questionnaires.

## Key Results
- Hybrid explanation dashboards (combining data-centric and model-centric views) significantly improved prediction model accuracy compared to single-explanation approaches
- Manual data configurations were more effective and efficient than automated configurations across all dashboard versions
- Data-centric explanations proved particularly useful for understanding post-configuration system changes and training data quality issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining global data-centric and model-centric explanations leads to better prediction model improvement than using either type alone.
- Mechanism: Data-centric explanations provide insights into training data quality, distributions, and issues, helping users understand data flaws. Model-centric explanations show feature importance and decision rules, but are insufficient alone for guiding data configuration. Together, they provide both data understanding and model structure, enabling more effective manual configuration decisions.
- Core assumption: Domain experts need both data-level understanding and model-level understanding to make informed configuration decisions.
- Evidence anchors:
  - [abstract]: "Combining both types of explanations proved most effective"
  - [section]: "Findings also indicate the limitations of global model-centric explanations for guiding users during data configuration"
  - [corpus]: Weak - only 1/5 related papers directly address combining explanation types for model steering

### Mechanism 2
- Claim: Manual data configuration is more effective and efficient than automated configuration for model improvement.
- Mechanism: Manual configuration gives domain experts control to apply their knowledge about which features matter and which data ranges are valid, allowing them to make domain-specific decisions that automated algorithms might miss. This control leads to more targeted improvements.
- Core assumption: Domain experts have relevant knowledge about their domain that automated algorithms lack.
- Evidence anchors:
  - [section]: "participants across all three versions demonstrated more effective and efficient steering through manual configurations compared to the automated ones"
  - [section]: "The manual configurations also helped them to learn the impact of certain health measures on the risk of diabetes for specific patient groups"
  - [corpus]: Weak - corpus papers focus on data-centric approaches but don't specifically compare manual vs automated configurations

### Mechanism 3
- Claim: Global data-centric explanations improve understanding of post-configuration system changes.
- Mechanism: By showing how predictor variable distributions and data quality change after configuration, data-centric explanations help users understand the impact of their actions on the model, creating a clearer mental model of the system.
- Core assumption: Users need to see the direct relationship between their configuration actions and changes in data properties to understand system behavior.
- Evidence anchors:
  - [abstract]: "data-centric explanations were particularly useful for understanding post-configuration system changes"
  - [section]: "Data-centric explanations proved valuable for healthcare experts in performing better configurations by providing a better understanding of the training data"
  - [corpus]: Weak - corpus papers discuss data-centric explanations but not specifically their role in post-configuration understanding

## Foundational Learning

- Concept: Difference between local and global explanations
  - Why needed here: The paper focuses on global explanations but mentions local explanations as a future need. Understanding this distinction is crucial for grasping the current limitations.
  - Quick check question: What's the key difference between explaining a single prediction versus explaining the entire model?

- Concept: Model-centric vs data-centric explanations
  - Why needed here: The core contribution involves comparing these two types of explanations. Understanding their different focuses is essential.
  - Quick check question: Which type of explanation focuses on training data properties versus model parameters?

- Concept: Manual vs automated data configuration
  - Why needed here: The paper evaluates both approaches and finds manual more effective. Understanding the distinction and tradeoffs is important.
  - Quick check question: What's the main advantage of giving users manual control over data configuration?

## Architecture Onboarding

- Component map:
  - Explanation Dashboard (DCE/MCE/HYB versions)
  - Data Configuration Interface (Manual/Automated)
  - Prediction Model (Random Forest)
  - Backend API (FastAPI)
  - Database (MongoDB)
  - Frontend (React.js)

- Critical path:
  1. User configures training data (manual/automated)
  2. System retrains model with new data
  3. System updates explanations based on new model
  4. User sees updated explanations and accuracy

- Design tradeoffs:
  - Single dashboard with toggle vs. separate versions: Separate versions allowed cleaner between-subjects study but required more development
  - Manual vs automated configuration: Both included to accommodate different user preferences, though manual proved more effective
  - Real-time vs batch retraining: Real-time enabled immediate feedback but may impact performance

- Failure signatures:
  - Accuracy drops significantly after configuration
  - Explanations fail to update after model retraining
  - Configuration interface becomes unresponsive
  - Dashboard doesn't reflect current model state

- First 3 experiments:
  1. Verify that changing a single feature's range in manual configuration updates model and explanations
  2. Test that automated correction of class imbalance actually improves accuracy
  3. Confirm that switching between DCE, MCE, and HYB versions shows the correct explanation types only

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the trade-offs between global and local explanations in EXMOS systems for domain experts?
- Basis in paper: [explicit] The paper discusses both global and local explanations but focuses primarily on global explanations in the user studies. Participants expressed a need for local explanations to enhance the usefulness and actionability of global explanations.
- Why unresolved: The study did not include local explanations in the prototype, so their impact on model steering effectiveness and user trust/understanding remains unknown.
- What evidence would resolve it: Conducting user studies comparing the effectiveness of global explanations alone versus global explanations combined with local explanations in EXMOS systems.

### Open Question 2
- Question: How do different data collection process disclosures affect trust and understanding in EXMOS systems?
- Basis in paper: [explicit] Participants suggested that including information on the data collection process can boost transparency and trust. However, the prototype did not include this feature.
- Why unresolved: The impact of data collection process disclosure on user trust and understanding was not empirically tested in the study.
- What evidence would resolve it: Conducting user studies comparing versions of the EXMOS system with and without data collection process disclosure to measure differences in trust and understanding.

### Open Question 3
- Question: What are the long-term effects of model steering on prediction accuracy and user trust in EXMOS systems?
- Basis in paper: [inferred] The study only measured short-term effects of model steering (within a single session). It's unclear how continued use and model improvements over time affect user trust and system performance.
- Why unresolved: The study design did not include longitudinal measurements of user interactions and model performance over extended periods.
- What evidence would resolve it: Implementing a long-term study tracking user interactions, model improvements, and trust levels over multiple sessions or weeks of use.

## Limitations
- Small sample size (n=70 quantitative, n=30 qualitative) limits generalizability
- Single healthcare dataset (Pima Indians Diabetes) restricts external validity
- Higher task load in hybrid version raises questions about long-term usability

## Confidence
- Hybrid explanation effectiveness: High
- Manual configuration superiority: Medium (limited to participants with sufficient domain knowledge)
- Data-centric explanations for post-configuration understanding: Low-Medium (based on qualitative feedback only)

## Next Checks
1. Replicate findings with a larger, more diverse participant pool and multiple healthcare datasets to test generalizability.
2. Conduct a longitudinal study to assess whether the higher task load in hybrid versions affects sustained usage over time.
3. Systematically measure participants' domain expertise levels to determine if manual configuration benefits are conditional on expertise thresholds.