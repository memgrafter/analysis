---
ver: rpa2
title: Zero-Shot Offline Imitation Learning via Optimal Transport
arxiv_id: '2410.08751'
source_url: https://arxiv.org/abs/2410.08751
tags:
- learning
- goal
- imitation
- zero-shot
- zilot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ZILOT addresses myopic planning in zero-shot imitation learning
  by optimizing occupancy matching through Optimal Transport. The method lifts a goal-conditioned
  value function to a distance between occupancies, approximated via a learned world
  model, enabling non-myopic behavior from offline, suboptimal data.
---

# Zero-Shot Offline Imitation Learning via Optimal Transport

## Quick Facts
- arXiv ID: 2410.08751
- Source URL: https://arxiv.org/abs/2410.08751
- Reference count: 40
- Method achieves lower Wasserstein distances and higher goal completion rates than hierarchical decomposition and reward-based baselines in challenging manipulation and locomotion tasks

## Executive Summary
ZILOT addresses the limitation of myopic planning in zero-shot offline imitation learning by optimizing an occupancy matching objective via Optimal Transport (OT). The method learns a goal-conditioned value function and dynamics model from offline data, then uses MPC with a zero-order optimizer to minimize the Wasserstein-1 distance between the policy's state occupancy and the expert's goal occupancy. This approach enables non-myopic behavior without requiring reward engineering or additional online supervision, particularly excelling in tasks requiring long-term planning across 30 tasks in 5 environments.

## Method Summary
ZILOT learns three key components offline from suboptimal demonstrations: a dynamics model, a goal-conditioned value function V that estimates expected steps to reach goals, and a goal-to-goal step estimator W. During online planning, ZILOT uses MPC to roll out trajectories and optimize the OT objective, computing Wasserstein distances between empirical occupancies using V as the cost matrix via Sinkhorn iterations. The method truncates expert trajectories based on W to account for planning horizon limitations, enabling zero-shot imitation without further supervision.

## Key Results
- Achieves lower Wasserstein distances to expert trajectories (0.049 vs 0.109-0.233 for baselines in fetch_pick_and_place)
- Higher goal completion rates compared to baselines (0.88 vs 0.43-0.65)
- Particularly effective in challenging environments requiring long-term planning like fetch_slide tasks
- Consistent improvements across 30 tasks in 5 diverse environments (fetch manipulation, halfcheetah locomotion, pointmaze navigation)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ZILOT mitigates myopic planning by directly optimizing an occupancy matching objective via Optimal Transport.
- Mechanism: Instead of decomposing trajectories into goals solved myopically, ZILOT computes Wasserstein-1 distance between policy and expert occupancies using learned value function as cost.
- Core assumption: Learned dynamics and value function are accurate enough for planning within fixed horizon.
- Evidence anchors: [abstract] introduces OT-based occupancy matching; [section 4] describes minimizing Wasserstein-1 metric.

### Mechanism 2
- Claim: Using learned value function V as proxy for goal metric h yields more informative OT cost than using h directly.
- Mechanism: Distance d(s,g) = -V(s,g) estimates expected steps to reach goal, incorporating MDP geometry unlike abstract goal comparison.
- Core assumption: Learned V approximates true optimal value function well enough for relevant state-goal pairs.
- Evidence anchors: [section 4] proposes using -V* as proxy; [section 5.3] shows d captures MDP asymmetries.

### Mechanism 3
- Claim: Offline training of V and W enables zero-shot imitation without further supervision.
- Mechanism: V predicts steps to reach goals; W predicts steps between consecutive goals, used to truncate expert trajectory within planning horizon H.
- Core assumption: Offline datasets contain sufficient coverage of relevant state-goal and goal-goal transitions.
- Evidence anchors: [section 4] describes training W to estimate steps between goals and using it for trajectory truncation.

## Foundational Learning

- Concept: Optimal Transport (OT) and Wasserstein distances
  - Why needed here: OT compares distributions incorporating underlying space geometry, unlike f-divergences; key for lifting state-goal distance to occupancy distance.
  - Quick check question: What is the key difference between OT and f-divergences like KL divergence when comparing distributions?

- Concept: Goal-conditioned reinforcement learning (GC-RL) and value functions
  - Why needed here: GC-RL provides framework for learning V(s,g) estimating expected steps to reach goal g from state s, used as proxy for unknown goal metric h.
  - Quick check question: In GC-RL, what does the optimal goal-conditioned value function V*(s,g) represent in terms of expected steps?

- Concept: Model Predictive Control (MPC) with learned dynamics
  - Why needed here: MPC with learned dynamics enables planning over finite horizon H by rolling out trajectories and optimizing OT objective when true dynamics are unknown.
  - Quick check question: Why is it necessary to use a learned dynamics model for planning in ZILOT, rather than the true dynamics?

## Architecture Onboarding

- Component map:
  - Offline pretraining: Learn dynamics model → goal-conditioned value function V → goal-to-goal step estimator W
  - Online planning: For each state s_k: Use W to truncate expert trajectory → Use dynamics model to roll out H-step trajectories → Compute OT cost with V as cost matrix → Optimize with iCEM → Execute first action

- Critical path:
  1. Accurate pretraining of dynamics model, V, and W on offline data
  2. Correct truncation of expert trajectory using W at each planning step
  3. Efficient computation of OT cost (cost matrix + Sinkhorn iterations)
  4. Effective optimization of OT cost via iCEM

- Design tradeoffs:
  - Planning horizon H vs. model accuracy: Larger H enables longer-term planning but requires more accurate model
  - Entropy regularization in Sinkhorn: Balances computational efficiency with OT solution accuracy
  - Population size and iterations in iCEM: Affect optimization quality vs. computation time

- Failure signatures:
  - Poor performance in long-term planning tasks: Likely due to model inaccuracy or insufficient H
  - Degenerate solutions (stuck in one state): Likely due to uninformative OT cost (poor V) or insufficient exploration in iCEM
  - Failure to reach later goals: Likely due to incorrect truncation by W or myopic optimization

- First 3 experiments:
  1. Run ZILOT on fetch_push-L-dense and visualize learned V(s,g) to verify it captures expected steps to reach goals
  2. Compare OT cost computation with and without learned V (using simple metric h) to see impact on planning
  3. Vary planning horizon H and observe effect on performance in fetch_slide_large_2D requiring long-term planning

## Open Questions the Paper Calls Out
- How does ZILOT's performance scale with increasing planning horizon in environments with highly non-stationary dynamics?
- Can the OT-based occupancy matching objective be made differentiable end-to-end to enable policy gradient optimization?
- How robust is ZILOT to noise and uncertainty in expert demonstrations, such as measurement errors or time misalignment?

## Limitations
- Method's performance heavily depends on accuracy of learned dynamics model and value function, with no explicit analysis of model error propagation
- Choice of entropy regularization parameter in Sinkhorn algorithm is treated as hyperparameter but sensitivity is not explored
- Comparison to baselines assumes same dynamics model and value function architecture, but this is not verified

## Confidence
- **High**: OT-based occupancy matching objective directly addresses myopic planning, with consistent improvements in goal completion rate
- **Medium**: Learned value function V serves as effective proxy for unknown goal metric h, enabling informative OT costs
- **Low**: Zero-shot nature fully validated across all environments, particularly for challenging fetch_slide tasks

## Next Checks
1. Conduct ablation study on entropy regularization parameter in Sinkhorn to understand impact on planning performance and computational efficiency
2. Perform sensitivity analysis on planning horizon H to determine minimum horizon required for successful task completion across different environments
3. Compare ZILOT's performance using true dynamics model (if available in simulation) versus learned model to quantify impact of model error on planning quality