---
ver: rpa2
title: 'Quantity Matters: Towards Assessing and Mitigating Number Hallucination in
  Large Vision-Language Models'
arxiv_id: '2403.01373'
source_url: https://arxiv.org/abs/2403.01373
tags:
- number
- hallucination
- consistency
- prompt
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and quantifies number hallucination in large
  vision-language models (LVLMs), where models incorrectly identify object counts
  in images. The authors propose a consistency training method that combines direct
  counting, binary classification, and comparison tasks to improve model performance.
---

# Quantity Matters: Towards Assessing and Mitigating Number Hallucination in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2403.01373
- Source URL: https://arxiv.org/abs/2403.01373
- Reference count: 32
- Key outcome: Consistency training method achieves 8% improvement in F1 scores over direct fine-tuning for number hallucination mitigation

## Executive Summary
This paper addresses number hallucination in large vision-language models (LVLMs), where models incorrectly identify object counts in images. The authors propose a consistency training approach that combines direct counting, binary classification, and comparison tasks to improve model performance. Through comprehensive experiments across multiple LVLMs, the method demonstrates an 8% improvement in F1 scores compared to direct fine-tuning. The approach is model-agnostic and maintains computational efficiency by only fine-tuning alignment modules.

## Method Summary
The method introduces consistency training for LVLMs to mitigate number hallucination by training on multiple related tasks simultaneously. It constructs a training dataset combining primal counting tasks with binary classification (determining if object counts are above/below a threshold) and comparison tasks (comparing counts between objects). The approach fine-tunes only the alignment module between frozen vision encoders and LLMs, using the consistency principle to improve inner and outer consistency of model outputs. The method analyzes both inner consistency (within a single task's outputs) and outer consistency (between different task types) to reduce number hallucination.

## Key Results
- Consistency training achieves 8% enhancement in F1 scores compared to direct fine-tuning
- Combining binary classification and comparison tasks yields better performance than either alone
- The method is model-agnostic and effective across multiple LVLM architectures (LLaVA, InstructBLIP, MiniGPT-v2)
- Computational efficiency is maintained by only fine-tuning alignment modules while keeping vision encoders and LLMs frozen

## Why This Works (Mechanism)

### Mechanism 1
Training on consistency tasks improves inner consistency of model outputs. By forcing the model to simultaneously learn related tasks requiring consistent reasoning about object counts, it develops more coherent internal representations of numerical information. This works under the assumption that inconsistency between related tasks is a primary cause of number hallucination.

### Mechanism 2
Consistency training reduces outer inconsistency between direct counting responses and answers to related questions. By explicitly training the model to provide consistent answers across different task formats, it learns to maintain coherent reasoning about object quantities. This assumes outer inconsistency represents a significant portion of number hallucination errors.

### Mechanism 3
Combining multiple task perspectives during training improves overall counting ability more than direct fine-tuning alone. Exposure to multiple related tasks forces the model to develop more robust and comprehensive understanding of numerical concepts in visual contexts. This assumes single-task training is insufficient for complex counting reasoning.

## Foundational Learning

- Concept: Multi-task learning and consistency training
  - Why needed here: The method relies on training models across multiple related tasks simultaneously to improve consistency
  - Quick check question: How does training on binary classification and comparison tasks help improve direct counting performance?

- Concept: Vision-language model architecture (alignment modules)
  - Why needed here: Understanding how alignment modules work is crucial for implementing the fine-tuning approach
  - Quick check question: What components of LVLMs are typically frozen during fine-tuning for efficiency?

- Concept: Evaluation metrics for classification and regression tasks
  - Why needed here: The paper uses multiple metrics (macro-F1, weighted-F1, MAE) to comprehensively evaluate performance
  - Quick check question: Why is using only accuracy insufficient for evaluating counting tasks with non-uniform label distributions?

## Architecture Onboarding

- Component map: Vision Encoder -> Alignment Module -> LLM
- Critical path: Data construction → Consistency training dataset creation → Fine-tuning alignment module → Evaluation on counting task
- Design tradeoffs: Computational efficiency (freezing large components) vs. potential performance gains from full fine-tuning
- Failure signatures: Minimal improvement over direct fine-tuning, increased training instability, degradation in other LVLM capabilities
- First 3 experiments:
  1. Implement direct fine-tuning baseline on counting task only
  2. Add consistency training with binary classification questions
  3. Combine both consistency tasks (binary classification + comparison) and compare results

## Open Questions the Paper Calls Out

### Open Question 1
Does improving consistency between tasks fully mitigate number hallucination in LVLMs? The paper demonstrates 8% improvement in performance metrics but doesn't claim complete elimination of the problem. Further experiments on larger datasets and different architectures could determine if the method fully mitigates number hallucination.

### Open Question 2
Are there other forms of hallucination in LVLMs beyond object and number hallucination that require similar consistency-based mitigation approaches? While the paper focuses specifically on number hallucination, it suggests the methodological insight may be extendable to various tasks requiring LVLM finetuning. Systematic analysis of different hallucination types and their consistency properties could reveal broader applications.

### Open Question 3
How does the severity of number hallucination correlate with the model's overall performance on vision-language tasks? The paper mentions number hallucination as a critical issue even in powerful LVLMs like GPT-4V, but doesn't establish a clear relationship between hallucination severity and performance on other vision-language tasks. Correlation analysis across architectures could reveal important patterns.

## Limitations
- The approach's effectiveness across diverse visual domains beyond MSCOCO (everyday objects) is unknown
- The 8% improvement represents gains on a specific dataset and task configuration, limiting generalizability claims
- Computational efficiency claims assume frozen vision encoders and LLMs, but actual training time and resource requirements for different model sizes are not specified

## Confidence

**High Confidence:** The existence and measurement of number hallucination in LVLMs is well-established through the proposed evaluation methodology. The consistency training framework is technically sound and the experimental results show reproducible improvements across multiple model architectures.

**Medium Confidence:** The attribution of performance gains to consistency training rather than increased model capacity or training duration. While the method shows improvements, the paper doesn't fully isolate whether consistency training or simply additional training on counting-related tasks drives the gains.

**Low Confidence:** The generalizability of the approach to real-world applications with complex counting scenarios, varying object scales, and occlusions. The controlled experimental setup may not capture the full complexity of practical deployment scenarios.

## Next Checks

1. **Cross-domain robustness test:** Evaluate the trained models on counting tasks from diverse datasets (medical imaging, satellite imagery, industrial inspection) to assess domain generalization beyond MSCOCO.

2. **Ablation study on task combinations:** Systematically test each consistency task (binary classification, comparison) independently and in various combinations to quantify their individual contributions to performance improvements.

3. **Long-term stability assessment:** Monitor model performance across multiple training epochs and after extended inference periods to verify that consistency improvements are stable and not temporary artifacts of the fine-tuning process.