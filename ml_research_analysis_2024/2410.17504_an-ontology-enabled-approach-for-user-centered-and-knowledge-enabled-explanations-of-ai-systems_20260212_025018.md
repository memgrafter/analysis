---
ver: rpa2
title: An Ontology-Enabled Approach For User-Centered and Knowledge-Enabled Explanations
  of AI Systems
arxiv_id: '2410.17504'
source_url: https://arxiv.org/abs/2410.17504
tags:
- explanations
- explanation
- types
- questions
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This dissertation addresses the challenge of user-centered explainability
  in AI systems by developing three key contributions: an Explanation Ontology (EO)
  to formally represent fifteen literature-derived explanation types, a knowledge-augmented
  question-answering (QA) system to provide contextual explanations in clinical settings,
  and a MetaExplainer framework to combine multiple explanations from different AI
  methods and data modalities. The EO models explanation dependencies on system, user,
  and interface attributes, enabling the representation of user-centered explanations
  across diverse domains.'
---

# An Ontology-Enabled Approach For User-Centered and Knowledge-Enabled Explanations of AI Systems

## Quick Facts
- arXiv ID: 2410.17504
- Source URL: https://arxiv.org/abs/2410.17504
- Authors: Shruthi Chari
- Reference count: 40
- Key outcome: Develops Explanation Ontology, knowledge-augmented QA system, and MetaExplainer framework for user-centered AI explanations

## Executive Summary
This dissertation addresses the challenge of user-centered explainability in AI systems through three major contributions. The work introduces an Explanation Ontology (EO) to formally represent fifteen literature-derived explanation types and their dependencies on system, user, and interface attributes. A knowledge-augmented question-answering system leverages clinical guidelines and large language models to contextualize risk predictions in clinical settings. The MetaExplainer framework provides a three-stage approach to combine multiple explanations from different AI methods and data modalities through decomposition, delegation, and synthesis stages.

## Method Summary
The research develops three interconnected components for explainable AI: (1) An Explanation Ontology that models explanation types and their dependencies across seven use cases, (2) A knowledge-augmented QA system that uses clinical guidelines to contextualize AI explanations in healthcare, and (3) A MetaExplainer framework that orchestrates multiple explanation methods through a three-stage process. The QA system demonstrates improved performance through knowledge augmentation, achieving up to 82% MAP on guideline questions. The MetaExplainer provides a flexible architecture for generating natural-language explanations by identifying relevant explanation types, executing corresponding methods, and synthesizing results using retrieval-augmented generation.

## Key Results
- Explanation Ontology supports fifteen explanation types across seven use cases with competency question evaluation
- Knowledge-augmented QA system achieves up to 82% MAP on guideline questions with clinical knowledge augmentation
- MetaExplainer demonstrates reliable parsing and explanation generation on diabetes prediction datasets with F1 scores up to 81% for likelihood extraction

## Why This Works (Mechanism)
The approach succeeds by addressing three fundamental challenges in explainable AI: (1) formal representation of diverse explanation types through the Explanation Ontology, (2) contextualization of explanations using domain knowledge in clinical settings, and (3) integration of multiple explanation methods through the MetaExplainer framework. The ontology provides a structured way to capture explanation dependencies on user expertise and system attributes, while knowledge augmentation improves semantic coherence of explanations. The MetaExplainer's decomposition-delegation-synthesis pipeline enables flexible combination of different explanation methods and data modalities.

## Foundational Learning
- Explanation Ontology: A formal representation system for AI explanations and their dependencies on user/system attributes. Why needed: Provides structured vocabulary for diverse explanation types. Quick check: Can represent fifteen literature-derived explanation types.
- Knowledge Augmentation: Integration of domain-specific knowledge (e.g., clinical guidelines) into explanation generation. Why needed: Improves semantic coherence and contextual relevance. Quick check: Achieves 82% MAP improvement on guideline questions.
- MetaExplainer Framework: Three-stage architecture for combining multiple explanation methods. Why needed: Enables flexible integration of diverse AI explanation approaches. Quick check: Demonstrates reliable parsing with F1 scores up to 81%.

## Architecture Onboarding

**Component Map:**
Explanation Ontology -> Knowledge-Augmented QA System -> MetaExplainer Framework

**Critical Path:**
1. Identify user needs and system context
2. Retrieve relevant explanations using Explanation Ontology
3. Augment with domain knowledge (clinical guidelines)
4. Generate and synthesize explanations through MetaExplainer
5. Present to user in natural language

**Design Tradeoffs:**
- Ontology complexity vs. usability: Fifteen explanation types provide comprehensive coverage but may increase cognitive load
- Knowledge augmentation depth vs. computational efficiency: More detailed knowledge improves explanations but increases processing time
- MetaExplainer flexibility vs. performance: Supports diverse AI methods but may sacrifice optimization for specific approaches

**Failure Signatures:**
- Incomplete explanation coverage when user needs don't match ontology categories
- Semantic incoherence when knowledge augmentation fails to contextualize properly
- Poor synthesis quality when combining heterogeneous explanation methods

**First 3 Experiments:**
1. Evaluate Explanation Ontology competency questions across seven use cases
2. Test knowledge-augmented QA system on Diabetes100 dataset with clinical guidelines
3. Validate MetaExplainer parsing and explanation generation on diabetes prediction datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Explanation types remain literature-derived without empirical validation of user utility
- QA system evaluation limited to single diabetes dataset and guideline corpus
- MetaExplainer lacks user-centered evaluation to verify improved understanding and decision-making

## Confidence
- Explanation Ontology: High confidence in formal representation and competency evaluation
- Knowledge-Augmented QA: Medium confidence with promising results in specific clinical context
- MetaExplainer Framework: Medium confidence with demonstrated technical feasibility

## Next Checks
1. Conduct user studies across different expertise levels to evaluate whether the fifteen explanation types identified in the EO actually improve user understanding and decision-making
2. Test the knowledge-augmented QA system on multiple clinical domains and datasets to assess generalizability beyond diabetes prediction
3. Evaluate the MetaExplainer framework with diverse AI methods (beyond tree-based models) and data modalities (images, text, multimodal) to verify robustness across different prediction tasks