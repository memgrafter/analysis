---
ver: rpa2
title: Small Language Model Can Self-correct
arxiv_id: '2401.07301'
source_url: https://arxiv.org/abs/2401.07301
tags:
- answer
- self-correction
- data
- language
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Intrinsic Self-Correction (ISC) to improve the
  accuracy of small language models (6-13B parameters) by enabling them to self-correct
  their outputs. ISC trains models to generate an initial answer, self-verify its
  correctness, and modify it if needed, all in one step using a Self-Correction Prompt.
---

# Small Language Model Can Self-correct

## Quick Facts
- arXiv ID: 2401.07301
- Source URL: https://arxiv.org/abs/2401.07301
- Reference count: 36
- Small language models (6-13B parameters) can effectively self-correct their outputs using a single-step training approach

## Executive Summary
This paper introduces Intrinsic Self-Correction (ISC), a method that enables small language models to improve their accuracy by self-correcting their outputs. The approach trains models to generate initial answers, self-verify their correctness, and modify them if needed - all in a single step using a Self-Correction Prompt. By constructing self-correction training data and introducing Partial Answer Masking (PAM) to exclude incorrect answer parts from loss calculation, ISC demonstrates that small LMs can achieve up to 5.6% accuracy improvements on QA benchmarks without requiring additional inference steps.

## Method Summary
The Intrinsic Self-Correction approach works by training language models to perform a three-step process in a single forward pass: first generate an initial answer, then self-verify its correctness, and finally modify it if needed. The training data is constructed by taking model outputs, labeling them as correct or incorrect, and providing the necessary modifications. Partial Answer Masking (PAM) is introduced during fine-tuning to prevent the model from being penalized for correct parts of answers that need modification. The entire process is controlled through a Self-Correction Prompt that guides the model through all three steps without requiring multiple inference calls.

## Key Results
- Small language models (6-13B parameters) can achieve up to 5.6% accuracy improvements through self-correction
- ISC outperforms traditional approaches that require multiple inference steps for verification and correction
- The Partial Answer Masking technique effectively prevents incorrect answer parts from negatively impacting training

## Why This Works (Mechanism)
ISC works by leveraging the model's inherent ability to reason about its own outputs and make corrections in a single forward pass. By training the model to simultaneously generate, verify, and modify answers, it learns to integrate these cognitive steps rather than treating them as separate processes. The Self-Correction Prompt provides a structured framework that guides the model through this integrated process, while PAM ensures that the model is rewarded for correct reasoning even when parts of the answer need modification.

## Foundational Learning
- **Self-correction in language models**: The ability of models to evaluate and modify their own outputs is crucial for reducing errors and improving reliability. Quick check: Can the model identify when its answer is incorrect?
- **Single-step training vs. multi-step inference**: Training models to perform verification and correction in one pass reduces computational overhead during inference. Quick check: Does the model maintain performance while reducing inference steps?
- **Partial Answer Masking**: This technique allows models to learn from partially correct answers by focusing loss calculation only on incorrect components. Quick check: Does PAM improve learning efficiency compared to full answer masking?

## Architecture Onboarding

**Component Map**: Self-Correction Prompt -> Initial Answer Generation -> Self-Verification -> Answer Modification -> PAM Loss Calculation

**Critical Path**: The model follows a linear critical path where the Self-Correction Prompt triggers the generation of an initial answer, which then flows into self-verification and potential modification, with PAM applied during training to calculate loss only on incorrect components.

**Design Tradeoffs**: ISC trades increased training complexity (single-step multi-task learning) for reduced inference complexity (no additional verification steps). This approach requires careful prompt engineering and training data construction but eliminates the need for multiple forward passes during deployment.

**Failure Signatures**: Models may fail when self-verification is unreliable, leading to either unnecessary modifications of correct answers or failure to correct incorrect ones. PAM may also introduce challenges if the model struggles to distinguish between correct and incorrect answer components.

**3 First Experiments**:
1. Ablation study comparing single-step ISC with traditional multi-step verification approaches
2. Analysis of self-verification accuracy to understand when the model correctly identifies its own errors
3. Evaluation of PAM effectiveness by comparing with full answer masking during training

## Open Questions the Paper Calls Out
None

## Limitations
- Performance improvements are demonstrated only on two QA benchmarks (OpenBookQA and CommonsenseQA), raising questions about generalizability to other task types
- The approach does not address computational overhead during inference or scalability to more complex reasoning tasks
- Limited analysis of failure modes and the quality of corrections beyond simple accuracy metrics

## Confidence
- High confidence in the technical feasibility of the ISC approach and its implementation
- Medium confidence in the claimed performance improvements due to limited benchmark diversity
- Medium confidence in the scalability and robustness of the approach across different task types

## Next Checks
1. Test ISC on a broader range of benchmarks including complex reasoning tasks, code generation, and multi-step problem solving to evaluate generalizability
2. Conduct ablation studies to quantify the individual contributions of self-verification and answer modification components
3. Implement a human evaluation study to assess the quality and appropriateness of corrections made by the model beyond simple accuracy metrics