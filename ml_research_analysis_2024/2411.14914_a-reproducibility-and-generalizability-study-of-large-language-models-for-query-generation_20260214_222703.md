---
ver: rpa2
title: A Reproducibility and Generalizability Study of Large Language Models for Query
  Generation
arxiv_id: '2411.14914'
source_url: https://arxiv.org/abs/2411.14914
tags:
- query
- boolean
- queries
- systematic
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a reproducibility and generalizability study
  of large language models for generating Boolean queries in systematic literature
  reviews. The authors evaluate the reliability of ChatGPT and compare it with open-source
  alternatives (Mistral and Zephyr) using a pipeline that generates queries, retrieves
  documents from PubMed, and evaluates results.
---

# A Reproducibility and Generalizability Study of Large Language Models for Query Generation

## Quick Facts
- arXiv ID: 2411.14914
- Source URL: https://arxiv.org/abs/2411.14914
- Authors: Moritz Staudinger; Wojciech Kusa; Florina Piroi; Aldo Lipari; Allan Hanbury
- Reference count: 40
- Primary result: LLMs generally outperform baselines on precision for some datasets but are not suitable for high-recall retrieval tasks in systematic literature reviews.

## Executive Summary
This paper presents a reproducibility and generalizability study of large language models for generating Boolean queries in systematic literature reviews. The authors evaluate the reliability of ChatGPT and compare it with open-source alternatives (Mistral and Zephyr) using a pipeline that generates queries, retrieves documents from PubMed, and evaluates results. Their findings show that while LLMs generally outperform baselines on precision for some datasets, they are not suitable for high-recall retrieval tasks. The study also reveals significant variability in LLM-generated queries and identifies limitations such as incorrect formatting and parentheses errors.

## Method Summary
The study evaluates LLMs for generating Boolean queries for systematic literature reviews by translating review titles into PubMed search queries. The pipeline involves prompt engineering with zero-shot, one-shot, and guided approaches, LLM inference using OpenAI and open-source models, PubMed API execution, and evaluation against ground truth datasets. The authors test three commercial LLMs (GPT-3.5-1106, GPT-4, GPT-4-turbo) and three open-source models (Mistral-tiny, Mistral-7B, Zephyr-7B-beta) across eight systematic review datasets. Query quality is assessed using precision, recall, and F1-score metrics.

## Key Results
- GPT-3.5-1106 achieves higher precision than original baselines by generating more specific, domain-relevant Boolean query terms with PubMed search field syntax
- Open-source models (Mistral, Zephyr) perform competitively with GPT due to instruction tuning for task completion
- LLM-generated queries show significant variability with error rates ranging from 1.56% to 9.77%, particularly in parentheses formatting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3.5-1106 achieves higher precision than original baselines because it generates more specific, domain-relevant Boolean query terms.
- Mechanism: The zero-shot prompt provides the model with the SLR title and instructs it to structure output as JSON, leading to cleaner Boolean query construction with PubMed search field syntax (e.g., `[MeSH Terms]`, `[Title/Abstract]`).
- Core assumption: The model can correctly interpret medical domain terminology and translate it into structured PubMed query syntax without fine-tuning.
- Evidence anchors:
  - [abstract] states "LLMs generally outperform baselines on precision for some datasets"
  - [section] notes that GPT-3.5-1106 outputs "far fewer terms on average than the expert-crafted Boolean queries" but includes more PubMed search fields
  - [corpus] shows this is a trending topic in LLM-based query generation
- Break condition: If the model fails to generate valid PubMed syntax or produces overly generic terms, precision drops to baseline levels or below.

### Mechanism 2
- Claim: Prompting with high-quality examples (HQE) improves query generation because the model learns the structural pattern of effective Boolean queries.
- Mechanism: Including an example review's title and query in the prompt gives the model a template for structuring its own output, including term categorization and field usage.
- Core assumption: The model's attention mechanism can extract and generalize the structural features from the example without explicit instruction on Boolean logic.
- Evidence anchors:
  - [section] describes "two different one-shot prompts (q4 and q5)" using HQE
  - [section] shows that "usage of PubMed search fields increases for all models, when they are provided with valid examples"
  - [corpus] lists related work on example-driven query refinement
- Break condition: If the example is too dissimilar from the target topic, the model may generate irrelevant terms or fail to adapt the structure.

### Mechanism 3
- Claim: Open-source models (Mistral, Zephyr) perform competitively with GPT because they are instruction-tuned for task completion.
- Mechanism: Instruction tuning on diverse datasets equips these models with the ability to follow task-oriented prompts and generate syntactically correct Boolean queries without domain-specific fine-tuning.
- Core assumption: Instruction tuning captures enough general reasoning about Boolean logic and PubMed syntax to match commercial models on this task.
- Evidence anchors:
  - [section] states "open-source LLMs perform reasonably well compared to commercial GPT models"
  - [section] notes that Mistral models "outperform the original results on this dataset as well"
  - [corpus] includes related work on open-source LLM instruction tuning
- Break condition: If the task requires deep domain knowledge (e.g., medical MeSH terms), open-source models may fall back to surface-level term matching.

## Foundational Learning

- Concept: PubMed Boolean query syntax (MeSH fields, AND/OR/NOT operators, parentheses)
  - Why needed here: The entire evaluation pipeline depends on generating syntactically valid PubMed queries that can be executed via the API.
  - Quick check question: What is the difference between `[MeSH Terms]` and `[Title/Abstract]` search fields in PubMed?

- Concept: Systematic review methodology (SLR title interpretation, inclusion/exclusion criteria)
  - Why needed here: The model must translate abstract research questions into concrete search terms that balance precision and recall.
  - Quick check question: Why do systematic reviews typically prioritize recall over precision in query construction?

- Concept: LLM prompt engineering (system vs user roles, JSON output formatting)
  - Why needed here: Consistent output formatting is essential for automated parsing and evaluation; prompt structure affects model performance.
  - Quick check question: How does using the system role in an API call differ from including instructions in the user message?

## Architecture Onboarding

- Component map: Input: SLR title → LLM prompt (zero-shot/one-shot/guided) → LLM: OpenAI API / Mistral API / local model → Boolean query JSON → Execution: PubMed API → Document IDs → Evaluation: Precision/Recall/F1 from Seed/CLEF TAR ground truth

- Critical path: Prompt generation → LLM API call → Query parsing → PubMed retrieval → Metric calculation

- Design tradeoffs:
  - Precision vs recall: LLM queries tend to be more specific (higher precision) but miss relevant documents (lower recall)
  - Model size vs context: Larger models handle longer prompts better but cost more; smaller models may truncate important examples
  - API vs local: APIs provide consistent outputs but cost money and raise reproducibility concerns; local models are free but require setup

- Failure signatures:
  - Invalid PubMed syntax → PubMed API returns errors
  - Missing parentheses → Incorrect boolean logic, drastically different recall
  - Repeated terms (e.g., Zephyr) → Query timeout or excessive document count
  - No MeSH terms → Lower recall due to missing controlled vocabulary

- First 3 experiments:
  1. Run q1 prompt with GPT-3.5-1106 using JSON output mode; verify PubMed syntax and compare precision to baseline
  2. Run q4-HQE with Mistral-tiny; check if example inclusion increases PubMed field usage and precision
  3. Run guided prompt with Mistral-7B-local; measure error rate in parentheses and compare to API version

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning LLMs on domain-specific datasets affect the quality and reliability of generated Boolean queries for systematic literature reviews?
- Basis in paper: [explicit] The authors suggest future work on specialized training for LLMs.
- Why unresolved: The study only evaluates zero-shot generation performance of LLMs without fine-tuning, leaving the impact of domain-specific fine-tuning unexplored.
- What evidence would resolve it: Experiments comparing zero-shot generation with fine-tuned LLMs on systematic review datasets, measuring precision, recall, and F1-scores.

### Open Question 2
- Question: Can retrieval-augmented generation techniques improve the correctness and scope of LLM-generated Boolean queries for systematic literature reviews?
- Basis in paper: [explicit] The authors suggest future work on retrieval-augmented generation techniques.
- Why unresolved: The study does not investigate the use of external knowledge retrieval to enhance query generation.
- What evidence would resolve it: Experiments comparing standard LLM query generation with retrieval-augmented approaches, measuring improvements in query quality and retrieval performance.

### Open Question 3
- Question: What is the optimal prompt engineering strategy for generating accurate and complete Boolean queries using LLMs?
- Basis in paper: [inferred] The study shows variability in query quality based on different prompt types and LLM models.
- Why unresolved: The paper does not systematically explore the impact of prompt engineering techniques on query generation quality.
- What evidence would resolve it: A comprehensive study testing various prompt engineering strategies (e.g., few-shot learning, chain-of-thought prompting) and their effects on query accuracy and completeness.

## Limitations
- The evaluation relies on a small sample of systematic review datasets (n=8), which limits generalizability across different medical domains and query complexities.
- The absence of a direct comparison to human-curated queries makes it difficult to assess whether LLM performance represents genuine progress or simply shifts the baseline.
- The study does not account for the potential variability introduced by different PubMed API endpoints or search algorithms over time.

## Confidence

**High Confidence Claims:**
- GPT-3.5-1106 and Mistral models consistently achieve higher precision than original baselines on specific datasets (CLEF TAR, EarPiercing).
- LLM-generated queries show significant variability across runs, with Mistral-7B displaying the highest error rates in query formatting.
- All evaluated LLMs struggle with the high-recall requirements typical of systematic reviews.

**Medium Confidence Claims:**
- Open-source models perform competitively with commercial GPT models due to instruction tuning.
- Prompt engineering (particularly example-based approaches) improves PubMed field usage and query specificity.
- The proposed evaluation pipeline provides a reproducible framework for assessing LLM query generation.

**Low Confidence Claims:**
- The specific mechanisms by which LLMs achieve precision improvements (e.g., whether through better term selection vs. syntax understanding).
- Generalizability of findings to domains outside the evaluated medical topics.
- Long-term reliability of LLM-generated queries given potential API and model changes.

## Next Checks

1. **Human Comparison Study**: Recruit medical librarians to generate Boolean queries for the same review titles and compare precision/recall metrics directly against LLM outputs, controlling for query length and complexity.

2. **Cross-Domain Evaluation**: Test the evaluation pipeline on systematic reviews from non-medical domains (e.g., social sciences, engineering) to assess generalizability of LLM performance patterns and identify domain-specific failure modes.

3. **Fine-tuning Experiment**: Train Mistral-7B on a dataset of expert-curated Boolean queries with PubMed syntax annotations, then evaluate whether fine-tuning improves recall without sacrificing precision, comparing to zero-shot performance.