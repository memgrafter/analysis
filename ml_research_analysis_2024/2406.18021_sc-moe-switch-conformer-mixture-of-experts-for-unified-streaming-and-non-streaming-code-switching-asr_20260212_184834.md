---
ver: rpa2
title: 'SC-MoE: Switch Conformer Mixture of Experts for Unified Streaming and Non-streaming
  Code-Switching ASR'
arxiv_id: '2406.18021'
source_url: https://arxiv.org/abs/2406.18021
tags:
- layer
- streaming
- encoder
- speech
- decoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Switch-Conformer-based Mixture of Experts
  (SC-MoE) model for unified streaming and non-streaming code-switching (CS) speech
  recognition. The method uses a streaming MoE layer with three language experts (Mandarin,
  English, and blank) and a router with LID-CTC loss to enable real-time streaming.
---

# SC-MoE: Switch Conformer Mixture of Experts for Unified Streaming and Non-streaming Code-Switching ASR

## Quick Facts
- arXiv ID: 2406.18021
- Source URL: https://arxiv.org/abs/2406.18021
- Authors: Shuaishuai Ye; Shunfei Chen; Xinhui Hu; Xinkang Xu
- Reference count: 0
- Key outcome: SC-MoE model achieves MER reductions of 0.98% and 0.97% in streaming and non-streaming modes respectively on 1160-hour Mandarin-English dataset

## Executive Summary
This paper introduces a Switch-Conformer-based Mixture of Experts (SC-MoE) model for unified streaming and non-streaming code-switching (CS) speech recognition. The method uses a streaming MoE layer with three language experts (Mandarin, English, and blank) and a router with LID-CTC loss to enable real-time streaming. It also incorporates MoE layers into the decoder and introduces routers into every MoE layer of the encoder and decoder. Experiments on a 1160-hour Mandarin-English dataset show that the proposed SC-MoE significantly improves CS ASR performance over baseline systems with comparable computational efficiency.

## Method Summary
The SC-MoE model is based on the U2++ backbone with a 12-layer encoder (6 standard conformer + 6 switch conformer layers with streaming MoE and LID-CTC loss) and a 6-layer decoder (3-layer L2R + 3-layer R2L with switch transformer layers and LID-CE loss). The streaming MoE layer treats blank as a separate language expert, eliminating the need for the problematic blank replacement rule. MoE layers are incorporated into both encoder and decoder, with routers in every MoE layer to enable hierarchical error correction. The model is trained using Adam optimizer with dynamic chunk training for 120 epochs and evaluated using attention rescoring on streaming and non-streaming modes.

## Key Results
- SC-MoE achieves MER reductions of 0.98% and 0.97% in streaming and non-streaming modes respectively compared to baseline systems
- The model demonstrates comparable computational efficiency to baseline systems while improving code-switching ASR performance
- Streaming MoE layer with three language experts (Mandarin, English, blank) enables real-time processing without blank replacement rule

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The streaming MoE layer with three language experts (Mandarin, English, blank) enables real-time streaming by eliminating the need for the problematic blank replacement rule.
- Mechanism: By treating blank as a separate language expert, the model can make routing decisions immediately without waiting for complete sequences, thus supporting real-time processing.
- Core assumption: The blank expert can effectively handle silence or non-speech regions without degrading overall recognition performance.
- Evidence anchors:
  - [abstract] "we design a streaming MoE layer consisting of three language experts, which correspond to Mandarin, English, and blank, respectively"
  - [section] "we propose abandoning the BRR [13] and instead, consider blank as a separate language"
- Break condition: If the blank expert consistently routes non-blank speech frames, leading to incorrect language modeling.

### Mechanism 2
- Claim: Incorporating MoE layers into both encoder and decoder improves modeling of phonemic confusion by leveraging language-specific information from text.
- Mechanism: The decoder MoE layers use cross-entropy loss for language identification, enhancing the model's ability to differentiate phonemes across languages during decoding.
- Core assumption: Language-specific information in the decoder can be effectively learned from text data to complement acoustic information.
- Evidence anchors:
  - [abstract] "To further utilize the language information embedded in text, we also incorporate MoE layers into the decoder of SC-MoE"
  - [section] "we employ cross-entropy (CE) loss specifically tailored for the 2-class LID task (distinguishing Mandarin and English)"
- Break condition: If the decoder MoE layers introduce significant computational overhead without corresponding performance gains.

### Mechanism 3
- Claim: Using routers in every MoE layer (instead of sharing one router) mitigates the "one mistake, all mistakes" issue by allowing subsequent layers to correct errors.
- Mechanism: Each MoE layer has its own router, enabling hierarchical error correction where later layers can potentially rectify routing decisions made by earlier layers.
- Core assumption: Router errors are not highly correlated across layers, allowing for effective error correction.
- Evidence anchors:
  - [abstract] "we introduce routers into every MoE layer of the encoder and the decoder"
  - [section] "we incorporate LID networks equipped with CTC losses into each SC encoder layer"
- Break condition: If router errors are highly correlated across layers, making error correction ineffective.

## Foundational Learning

- Concept: Language identification (LID) in speech recognition
  - Why needed here: The model uses LID to route inputs to appropriate language experts, which is fundamental to handling code-switching scenarios.
  - Quick check question: How does the model differentiate between Mandarin and English speech segments at the frame level?

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: MoE allows the model to activate only relevant parameters for each input, enabling efficient handling of multiple languages with specialized experts.
  - Quick check question: What is the computational advantage of using MoE compared to a dense model with equivalent capacity?

- Concept: Streaming vs non-streaming ASR
  - Why needed here: The paper aims to create a unified model that can operate in both modes, requiring understanding of latency constraints and algorithmic differences.
  - Quick check question: What is the key architectural difference between streaming and non-streaming conformer layers?

## Architecture Onboarding

- Component map:
  - Input: 80-dimensional log-Mel filter-bank features
  - Encoder: 6 standard conformer layers + 6 switch conformer layers with streaming MoE
  - Decoder: 3-layer L2R + 3-layer R2L with ST decoder layers and MoE
  - CTC layer for joint CTC/Attention training
  - Routers with LID-CTC loss in encoder, LID-CE loss in decoder

- Critical path: Audio features → Encoder (with streaming MoE) → Decoder (with MoE) → Output

- Design tradeoffs:
  - MoE vs dense layers: Better parameter efficiency but added routing complexity
  - Streaming MoE with blank expert vs BRR: Real-time capability vs potential blank expert performance issues
  - Per-layer routers vs shared router: Better error correction vs increased parameter count

- Failure signatures:
  - High blank expert activation across all layers: Routing mechanism not learning language boundaries
  - Decoder performance significantly worse than encoder: MoE in decoder not helping or actively harming
  - Streaming mode performance much worse than non-streaming: Latency constraints impacting model effectiveness

- First 3 experiments:
  1. Verify that streaming MoE layer can correctly identify and route Mandarin, English, and blank frames by analyzing router gate values
  2. Compare performance with and without decoder MoE layers to quantify their contribution
  3. Test different router configurations (R1, R2, R3) to find optimal error correction strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SC-MoE model's performance scale with increasing numbers of language experts beyond the three (Mandarin, English, and blank) currently used?
- Basis in paper: [inferred] The paper mentions three language experts but does not explore scaling to more languages or experts.
- Why unresolved: The authors focus on Mandarin-English code-switching and do not experiment with additional languages or experts.
- What evidence would resolve it: Experiments testing the model with additional language experts and evaluating performance impacts on accuracy and computational efficiency.

### Open Question 2
- Question: What is the impact of different router architectures (e.g., transformer-based, CNN-based) on the streaming MoE layer's performance compared to the current linear router?
- Basis in paper: [explicit] The paper uses a linear router but does not compare it with other router architectures.
- Why unresolved: The authors only use a linear router and do not explore alternative router designs.
- What evidence would resolve it: Comparative experiments using different router architectures and analyzing their effects on accuracy, latency, and computational efficiency.

### Open Question 3
- Question: How does the SC-MoE model perform on code-switching scenarios with more than two languages, such as Mandarin-English-Spanish?
- Basis in paper: [inferred] The paper focuses on Mandarin-English code-switching and does not test multilingual scenarios.
- Why unresolved: The authors only test the model on Mandarin-English data and do not explore more complex multilingual code-switching scenarios.
- What evidence would resolve it: Experiments evaluating the model's performance on multilingual code-switching datasets and analyzing its ability to handle additional languages.

### Open Question 4
- Question: What is the optimal balance between the CTC and attention components in the joint training objective for different code-switching scenarios?
- Basis in paper: [explicit] The paper uses a joint CTC and attention training objective with fixed hyperparameters (λ=0.3, α=0.3 in training and λ=0.3, α=0.6 in decoding).
- Why unresolved: The authors use fixed hyperparameters without exploring the optimal balance for different scenarios.
- What evidence would resolve it: Experiments testing different combinations of λ and α values and analyzing their impact on recognition accuracy in various code-switching scenarios.

## Limitations

- The effectiveness of the SC-MoE architecture for other code-switching language pairs remains untested
- Computational efficiency claims lack detailed analysis of parameter counts and memory usage
- The streaming MoE layer's handling of the blank expert is theoretically sound but lacks empirical validation in diverse real-world scenarios

## Confidence

**High Confidence (9/10)**: The SC-MoE architecture's ability to improve code-switching ASR performance over baseline systems is well-supported by the experimental results showing MER reductions of 0.98% and 0.97% in streaming and non-streaming modes, respectively.

**Medium Confidence (6/10)**: The claim of comparable computational efficiency to baseline systems is based on inference time measurements but lacks detailed analysis of parameter counts and memory usage.

**Low Confidence (4/10)**: The generalization of these results to other language pairs and datasets is not established.

## Next Checks

1. **Router Performance Analysis**: Analyze the gate values of the streaming MoE routers across different test segments to verify that they correctly identify language boundaries and that the blank expert is appropriately activated during non-speech regions.

2. **Ablation Study on Router Configuration**: Conduct experiments comparing the performance of different router configurations (R1, R2, R3) to quantify the impact of per-layer routing versus shared routing on overall model accuracy.

3. **Cross-Lingual Generalization Test**: Implement and evaluate the SC-MoE architecture on a different code-switching language pair (e.g., Spanish-English or Hindi-English) using publicly available datasets to assess the model's generalizability beyond the Mandarin-English domain used in this study.