---
ver: rpa2
title: 'AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process
  Feedback'
arxiv_id: '2402.01469'
source_url: https://arxiv.org/abs/2402.01469
tags:
- amor
- feedback
- reasoning
- process
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AMOR is a modular agent framework for knowledge-intensive tasks
  that uses FSM-based reasoning logic and process feedback to adapt to specific domains.
  It consists of specialized modules for question decomposition, document retrieval,
  relevance judgment, passage retrieval, answer extraction, and task completion.
---

# AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback

## Quick Facts
- arXiv ID: 2402.01469
- Source URL: https://arxiv.org/abs/2402.01469
- Reference count: 40
- One-line primary result: AMOR achieves 30%-40% improvements over strong baselines like ReAct and LUMOS using FSM-based reasoning and process feedback

## Executive Summary
AMOR is a modular agent framework for knowledge-intensive tasks that uses FSM-based reasoning logic and process feedback to adapt to specific domains. It consists of specialized modules for question decomposition, document retrieval, relevance judgment, passage retrieval, answer extraction, and task completion. AMOR is built through two-stage fine-tuning: warm-up on diverse public datasets to generalize across knowledge environments, and adaptation to target domains using process feedback collected during autonomous exploration. Extensive experiments show AMOR outperforms strong baselines like ReAct and LUMOS, with 30%-40% improvements when using off-the-shelf LLMs and further gains after adaptation. The FSM-based reasoning and process feedback mechanism are key to its effectiveness.

## Method Summary
AMOR builds reasoning logic over a finite state machine (FSM) that solves problems through autonomous executions and transitions over disentangled modules. The framework uses a Mixture-of-Experts (MA-MoE) architecture where each LLM module has distinct FFN parameters activated via module index routing. The two-stage fine-tuning process first warm-ups on diverse public datasets to generalize across knowledge environments, then adapts to target domains using process feedback collected during autonomous exploration. Process feedback is more effective than outcome feedback in facilitating the adaptation of agents, as it provides granular signals for intermediate reasoning steps.

## Key Results
- AMOR achieves 30%-40% improvements over strong baselines like ReAct and LUMOS when using off-the-shelf LLMs
- The FSM-based reasoning logic and process feedback mechanism are key contributors to AMOR's effectiveness
- Adaptation using process feedback provides further gains beyond the initial warm-up stage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The FSM-based reasoning logic constrains and structures the agent's reasoning path, improving precision over free-form methods.
- Mechanism: States and modules define a narrow, navigable exploration space, with transitions guided by explicit branch tokens that select subsequent steps.
- Core assumption: Complex reasoning tasks can be decomposed into a finite set of atomic steps with clear dependencies.
- Evidence anchors:
  - [abstract]: "AMOR builds reasoning logic over a finite state machine (FSM) that solves problems through autonomous executions and transitions over disentangled modules."
  - [section]: "Defining reasoning logic as an FSM offers three advantages: (1) Structured Thinking... (2) Skill Disentanglement... (3) Intervenable Workflow."
- Break condition: If the task requires emergent, unstructured reasoning steps not capturable by predefined FSM states, the rigidity may hurt performance.

### Mechanism 2
- Claim: Process feedback enables targeted adaptation of intermediate reasoning steps, improving final performance more than outcome feedback alone.
- Mechanism: Human feedback is collected per LLM module output, then used to refine outputs and assign immediate rewards for fine-tuning via KTO optimization.
- Core assumption: Intermediate reasoning steps are more reliably judged and corrected than final outcomes alone.
- Evidence anchors:
  - [abstract]: "Based on this reasoning and feedback framework, we develop AMOR through two-stage fine-tuning: warm-up and adaptation. The latter tailors AMOR to specific domains using process feedback."
  - [section]: "Process feedback is more effective than outcome feedback in facilitating the adaptation of agents."
- Break condition: If feedback quality is poor or noisy, the fine-tuning signal may mislead the model rather than improve it.

### Mechanism 3
- Claim: Module-aware Mixture-of-Experts (MA-MoE) architecture allows efficient multi-task handling without interference between module-specific parameters.
- Mechanism: Each LLM module has distinct FFN parameters initialized from the base model, activated via module index routing during execution.
- Core assumption: Module outputs are largely independent, allowing specialization without catastrophic forgetting.
- Evidence anchors:
  - [section]: "We are inspired by the Mixture-of-Experts approach to learn distinct Feed-Forward Network (FFN) parameters in the final quarter of the Transformer blocks to balance the trade-off between performance and inference efficiency."
  - [section]: "The MA-MoE's module-specific awareness enables it to handle diverse tasks within the agent more adeptly."
- Break condition: If modules require heavy parameter sharing or interference, MA-MoE could degrade performance versus a unified model.

## Foundational Learning

- Concept: Finite State Machines (FSMs)
  - Why needed here: Provides a formal, tractable way to define structured reasoning logic for modular agents.
  - Quick check question: In an FSM, what determines the next state given the current state and an output?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Process feedback resembles RLHF but operates at intermediate reasoning steps, not just final outputs.
  - Quick check question: How does immediate reward assignment differ between outcome feedback and process feedback?

- Concept: Mixture-of-Experts (MoE) architectures
  - Why needed here: Enables efficient multi-task handling by routing to module-specific expert parameters.
  - Quick check question: What is the key difference between standard MoE and MA-MoE in AMOR?

## Architecture Onboarding

- Component map: LLM modules (Decompose, Judge, Answer, Complete) → Tool modules (SearchDoc, SearchPsg, NextDoc) → FSM state machine → Process feedback collector → MA-MoE parameter set
- Critical path: Question → Decompose → (Document Retrieval → Relevance Judgment → Passage Retrieval → Answer Extraction)* → Complete → Final Answer
- Design tradeoffs: FSM rigidity vs. flexibility; process feedback granularity vs. cost; MA-MoE complexity vs. inference efficiency
- Failure signatures: Incorrect state transitions → wrong module chain; poor feedback → degraded fine-tuning; routing errors → wrong module parameters
- First 3 experiments:
  1. Run AMOR w/o FT on a simple multi-hop QA task and inspect FSM state traces for correctness.
  2. Collect silver feedback on a small dataset and run adaptation; measure module-level accuracy changes.
  3. Compare MA-MoE vs. baseline MoE and Transformer on module-specific downstream tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the FSM-based reasoning logic in AMOR compare to other formal reasoning frameworks like Petri nets or production systems in terms of expressiveness and scalability?
- Basis in paper: [inferred] The paper discusses FSM-based reasoning but doesn't compare it to other formal reasoning frameworks.
- Why unresolved: The paper focuses on comparing AMOR to other agent frameworks but doesn't explore the theoretical properties of FSM-based reasoning compared to alternative formalisms.
- What evidence would resolve it: Comparative analysis of FSM, Petri nets, and production systems in terms of expressiveness, scalability, and suitability for knowledge-intensive tasks.

### Open Question 2
- Question: What is the impact of the maximum number of decomposed sub-queries on AMOR's performance and reasoning quality?
- Basis in paper: [explicit] The paper mentions setting this maximum to the maximum count of gold evidence passages but doesn't explore the impact of varying this parameter.
- Why unresolved: The paper uses a fixed setting without exploring how different values affect performance, reasoning quality, or computational efficiency.
- What evidence would resolve it: Systematic experiments varying the maximum number of sub-queries and measuring performance metrics, reasoning quality, and computational costs.

### Open Question 3
- Question: How does AMOR's performance degrade when dealing with noisy or incomplete knowledge bases compared to other agent frameworks?
- Basis in paper: [inferred] The paper doesn't explore AMOR's robustness to imperfect knowledge bases, which is a critical real-world scenario.
- Why unresolved: All experiments use clean, curated datasets without introducing noise or incompleteness in the knowledge bases.
- What evidence would resolve it: Experiments with artificially corrupted knowledge bases (missing passages, incorrect information) and comparison with baseline methods' robustness.

## Limitations

- The experimental evaluation relies heavily on synthetic or semi-synthetic feedback for the adaptation phase, which may not fully capture the complexity and noise of real human feedback.
- The comparison with ReAct and LUMOS baselines does not explore alternative modular architectures or different FSM structures that might achieve similar results.
- The paper does not investigate the computational overhead of maintaining separate module parameters in the MA-MoE architecture versus simpler alternatives.

## Confidence

- **High Confidence**: The FSM-based reasoning logic provides structured navigation and the MA-MoE architecture enables efficient multi-task handling. These claims are well-supported by the architectural description and ablation studies.
- **Medium Confidence**: Process feedback enables more effective adaptation than outcome feedback alone. While the mechanism is sound and supported by experimental results, the reliance on silver feedback introduces uncertainty about real-world effectiveness.
- **Low Confidence**: The 30%-40% improvement over strong baselines represents a definitive advance. Given the synthetic nature of some evaluation conditions and the potential for implementation-specific optimizations, these absolute numbers should be interpreted cautiously.

## Next Checks

1. **Human Feedback Validation**: Replace the silver feedback mechanism with actual human annotators to evaluate whether process feedback maintains its advantage when collected from real humans rather than synthetic sources.

2. **FSM Structure Ablation**: Systematically vary the FSM state definitions and transition rules to determine whether the specific structure used in AMOR is critical to performance, or whether simpler FSM designs could achieve comparable results.

3. **Computational Efficiency Analysis**: Measure the actual inference time and memory overhead of the MA-MoE architecture compared to both standard MoE implementations and fully shared-parameter approaches to quantify the claimed efficiency benefits.