---
ver: rpa2
title: Generalized Group Data Attribution
arxiv_id: '2410.09940'
source_url: https://arxiv.org/abs/2410.09940
tags:
- group
- data
- influence
- ggda
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Data attribution methods quantify the influence of individual training
  points on model predictions but are computationally expensive for large-scale models.
  This work introduces the Generalized Group Data Attribution (GGDA) framework, which
  simplifies DA by attributing to groups of training points instead of individuals.
---

# Generalized Group Data Attribution

## Quick Facts
- arXiv ID: 2410.09940
- Source URL: https://arxiv.org/abs/2410.09940
- Authors: Dan Ley; Suraj Srinivas; Shichang Zhang; Gili Rusak; Himabindu Lakkaraju
- Reference count: 40
- Primary result: Achieves 10x-50x speedups while maintaining attribution fidelity

## Executive Summary
This work introduces the Generalized Group Data Attribution (GGDA) framework, which addresses the computational expense of data attribution methods by attributing influence to groups of training points rather than individuals. GGDA generalizes existing attribution methods (Influence Functions, TracIn, TRAK) to operate on groups, achieving significant computational efficiency improvements while maintaining effectiveness for downstream tasks. By clustering training points and attributing to these groups, GGDA reduces computational requirements from scaling with the number of data points (n) to scaling with the number of groups (k).

## Method Summary
GGDA partitions training data into k groups and computes attribution scores for each group using modified versions of standard DA methods. The framework includes Grad-K-Means grouping, which clusters points based on their gradient similarity (gradients of loss w.r.t. penultimate-layer activations). The method is evaluated through retraining experiments (removing top k% important points and measuring accuracy drop) and noisy label detection (identifying mislabeled instances). The approach extends Influence Functions, TracIn, and TRAK to group-level computation while maintaining their theoretical properties.

## Key Results
- Achieves 10x-50x speedups compared to standard data attribution methods
- Maintains effectiveness for dataset pruning and noisy label identification tasks
- Grad-K-Means grouping outperforms random grouping by clustering points with similar influence patterns
- Computational requirements scale with number of groups (k) rather than data points (n)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Group attribution reduces computational complexity from O(n) to O(k) by batching gradients per group
- Mechanism: Computing gradients over groups rather than individual points scales with groups (k) instead of data points (n). The linearity of influence functions allows group influence to equal the sum of individual influences within that group.
- Core assumption: Similar data points affect models in similar ways, making group attribution meaningful
- Evidence anchors: [abstract]: "GGDA reduces computational requirements from scaling with the number of data points (n) to scaling with the number of groups (k)"; [section 3]: "the main advantage of GGDA methods is that they scale as the number of groups k, whereas DA methods scale with the number of data points n"
- Break condition: When groups contain heterogeneous data points with very different influences on the model, or when group sizes exceed hardware batch size constraints

### Mechanism 2
- Claim: Grad-K-Means grouping preserves attribution fidelity better than random grouping by clustering points with similar influence patterns
- Mechanism: Grouping data points based on their gradient representations clusters points that have similar effects on model learning, maintaining structural relationships between influential and non-influential points within groups.
- Core assumption: Points with similar gradient representations have similar influence on model performance
- Evidence anchors: [section 5.2]: "Gradient K-Means (Grad-K-Means) We compute the gradient of the loss with respect to the activations...and then apply K-means on the gradients"; [section 5.4.1]: "the most important Grad-K-Means group of size 64 consisted only of points with positive individual attributions"
- Break condition: When gradient representations don't capture true influence patterns, or when data points have non-local influence effects

### Mechanism 3
- Claim: GGDA maintains effectiveness for downstream tasks like dataset pruning and noisy label detection while providing computational speedups
- Mechanism: By preserving relative ordering of importance scores (despite some fidelity loss), GGDA enables effective identification of influential/non-influential points for practical applications, while computational savings enable these tasks at scale.
- Core assumption: Relative ranking of importance scores is sufficient for downstream tasks, not absolute values
- Evidence anchors: [abstract]: "For downstream applications such as dataset pruning and noisy label identification, we demonstrate that GGDA significantly improves computational efficiency and maintains effectiveness"; [section 5.4.2]: "GGDA significantly outperforms individual DA methods in this task, achieving higher test accuracies while requiring substantially less compute"
- Break condition: When downstream tasks require precise absolute attribution values rather than relative rankings, or when fidelity loss exceeds task-specific thresholds

## Foundational Learning

- Concept: Influence functions and their computational bottlenecks
  - Why needed here: Understanding why standard DA methods are computationally expensive is crucial for appreciating GGDA's efficiency gains
  - Quick check question: What are the two main computational bottlenecks in standard influence function computation?

- Concept: Empirical Fisher matrix approximation
  - Why needed here: GGDA extends to TRAK which uses empirical Fisher approximation, so understanding this concept is necessary for the complete framework
  - Quick check question: How does the empirical Fisher matrix differ from the true Fisher information matrix?

- Concept: Gradient-based clustering and representation learning
  - Why needed here: Grad-K-Means grouping relies on gradient representations, so understanding how to compute and interpret these is essential
  - Quick check question: Why might gradient representations of data points be useful for clustering in the context of attribution?

## Architecture Onboarding

- Component map:
  - Trained model + training dataset + grouping method + property function -> GGDA framework -> Group attribution scores for downstream tasks
  - Dependencies: Standard DA implementations (Influence Functions, TracIn, TRAK), clustering algorithms, evaluation metrics

- Critical path:
  1. Train base model on full dataset
  2. Generate groups using chosen grouping method (Grad-K-Means recommended)
  3. Apply GGDA to chosen DA method
  4. Use attribution scores for downstream task (pruning, noise detection, etc.)
  5. Evaluate performance vs standard DA

- Design tradeoffs:
  - Group size vs fidelity: Larger groups provide more speedup but potentially less accurate attributions
  - Grouping method choice: Grad-K-Means generally outperforms random grouping but requires computing gradients
  - Property function selection: Must balance between specific predictions and bulk model properties

- Failure signatures:
  - Poor downstream task performance despite computational speedup
  - Runtime not improving as expected (check if k < n/b constraint is violated)
  - Groups containing heterogeneous data points with conflicting influences

- First 3 experiments:
  1. Compare runtime and fidelity of GGDA vs standard DA on a small dataset (e.g., HELOC) with varying group sizes
  2. Evaluate Grad-K-Means vs random grouping on MNIST for noisy label detection
  3. Test dataset pruning effectiveness on CIFAR-10 with different grouping methods and group sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the fidelity of GGDA methods degrade as group sizes increase beyond the practical limit where points can no longer fit into a single batch?
- Basis in paper: [explicit] The paper mentions that when k â‰³ n/b (where b is the maximum batch size), GGDA influence provides a constant factor speedup of b, but doesn't provide experimental evidence on fidelity degradation beyond this point
- Why unresolved: The paper only tests group sizes up to 1024 and doesn't explore the regime where computational benefits are constant rather than scaling
- What evidence would resolve it: Experiments with group sizes approaching n/b and beyond, measuring both runtime and fidelity metrics like retraining score and noisy label detection AUC

### Open Question 2
- Question: How do GGDA methods perform on truly large-scale datasets with billions of training points, where the computational benefits would be most pronounced?
- Basis in paper: [inferred] The paper acknowledges this as a direction for future work, stating that evaluation on "genuinely large-scale, billion-data-point learning setting" could uncover remaining computational bottlenecks
- Why unresolved: All experiments in the paper use relatively small datasets (HELOC: ~10K points, MNIST: 60K points, CIFAR-10: 50K points, QNLI: 108K points)
- What evidence would resolve it: Experiments on datasets with millions or billions of points, measuring whether GGDA maintains its computational advantages and fidelity trade-offs at scale

### Open Question 3
- Question: What is the optimal grouping strategy for different types of downstream tasks (dataset pruning vs. noisy label detection vs. explainability)?
- Basis in paper: [explicit] The paper shows that Grad-K-Means outperforms other grouping methods overall, but doesn't systematically compare grouping strategies across different tasks
- Why unresolved: The paper uses Grad-K-Means as the default grouping method without exploring whether other methods (random, K-means on raw features, K-means on representations) might be superior for specific tasks
- What evidence would resolve it: Systematic comparison of grouping methods on each downstream task, identifying which grouping strategy maximizes task-specific performance while maintaining computational efficiency

## Limitations
- Computational speedup claims are primarily theoretical rather than comprehensively empirically validated
- Grad-K-Means effectiveness depends on gradient representations adequately capturing influence patterns
- Group attribution fidelity loss is not fully characterized - assumes relative rankings are sufficient

## Confidence
- High confidence: Mathematical framework and computational complexity claims are sound
- Medium confidence: Empirical results demonstrating effectiveness in downstream tasks, though sample sizes are limited
- Low confidence: Generalization to highly heterogeneous datasets or complex model architectures not thoroughly validated

## Next Checks
1. Conduct systematic ablation studies varying group sizes and composition to quantify the fidelity-speedup tradeoff curve
2. Test GGDA on additional model architectures (CNN variants, transformers) and dataset types (text, tabular, multimodal) to assess generalizability
3. Implement controlled experiments comparing absolute vs relative attribution accuracy in downstream tasks to determine when GGDA performance degrades significantly