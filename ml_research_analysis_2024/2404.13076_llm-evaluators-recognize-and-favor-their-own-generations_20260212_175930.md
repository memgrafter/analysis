---
ver: rpa2
title: LLM Evaluators Recognize and Favor Their Own Generations
arxiv_id: '2404.13076'
source_url: https://arxiv.org/abs/2404.13076
tags:
- self-recognition
- self-preference
- fine-tuning
- gpt-3
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-evaluation using large language models (LLMs) is increasingly
  used in benchmarking, reward modeling, and alignment methods. A key concern is self-preference
  bias, where an LLM evaluator rates its own outputs higher than others, even when
  human annotators judge them equal in quality.
---

# LLM Evaluators Recognize and Favor Their Own Generations

## Quick Facts
- arXiv ID: 2404.13076
- Source URL: https://arxiv.org/abs/2404.13076
- Reference count: 34
- Primary result: Self-recognition capability in LLMs strongly correlates with self-preference bias

## Executive Summary
This study investigates whether large language models (LLMs) can recognize their own generated outputs and whether this ability contributes to self-preference bias in evaluation tasks. The authors find that frontier LLMs like GPT-4 and Llama 2 can distinguish their own outputs from other models and humans with significant accuracy. More importantly, they demonstrate a strong linear correlation between self-recognition capability and the strength of self-preference bias. Fine-tuning LLMs for self-recognition on just 500 examples can increase both recognition accuracy to over 90% and the magnitude of self-preference. These findings reveal that self-recognition is a key mechanism behind self-preference, with important implications for unbiased evaluation and alignment in AI systems.

## Method Summary
The study uses two summarization datasets (XSUM and CNN/DailyMail) with 1,000 news articles each. LLMs (GPT-4, GPT-3.5, Llama 2) generate summaries using zero-temperature sampling. The authors measure self-recognition and self-preference using both pairwise and individual measurement formats, then fine-tune Llama 2 on 500 examples to increase/decrease self-recognition capability. Control tasks include length, vowel count, and readability fine-tuning. The study analyzes the correlation between self-recognition and self-preference scores across different measurement settings and datasets.

## Key Results
- GPT-4 achieves 73.5% accuracy at distinguishing itself from other LLMs and humans out of the box
- Fine-tuning increases self-recognition accuracy to over 90% with just 500 examples
- Strong linear correlation exists between self-recognition capability and self-preference bias strength
- Control experiments confirm the correlation is not due to confounding factors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs recognize their own outputs due to stylistic consistency in generation patterns
- Mechanism: LLMs trained on similar data and fine-tuned similarly produce text with consistent linguistic patterns, vocabulary, and structural choices that are detectable by the same model when prompted to distinguish authorship
- Core assumption: The LLM's internal representations encode detectable stylistic features that correlate with self-generated text
- Evidence anchors: "GPT-4 is 73.5% accurate distinguishing itself from two other LLMs and humans"

### Mechanism 2
- Claim: Fine-tuning increases self-recognition by reinforcing internal consistency detection
- Mechanism: Supervised fine-tuning on self vs. non-self examples strengthens the model's ability to detect subtle differences in output patterns, creating a stronger correlation between recognition and preference
- Core assumption: The fine-tuning process creates a feedback loop where recognition capability and preference bias reinforce each other
- Evidence anchors: "Fine-tuning LLMs for self-recognition on 500 examples increases accuracy to over 90%"

### Mechanism 3
- Claim: LLMs lack strong situational awareness, so self-recognition is based on pattern matching rather than true self-knowledge
- Mechanism: The model identifies self-generated text through statistical patterns and surface features rather than any conceptual understanding of "self," leading to a purely mechanistic preference bias
- Core assumption: The model operates on pattern recognition without meta-cognitive awareness of its own generation process
- Evidence anchors: "We follow the prosaic rather than the intentional interpretation"

## Foundational Learning

- Concept: Linear correlation interpretation
  - Why needed here: Understanding that the study shows a correlation, not necessarily causation, between self-recognition and self-preference
  - Quick check question: Does a linear correlation between two variables prove that one causes the other?

- Concept: Supervised fine-tuning mechanics
  - Why needed here: The study uses fine-tuning to manipulate self-recognition capability, so understanding how this works is crucial
  - Quick check question: What is the difference between supervised fine-tuning and other training approaches like reinforcement learning?

- Concept: Prompt engineering and ordering bias
  - Why needed here: The study accounts for ordering bias in pairwise comparisons, which is a common issue in LLM evaluation
  - Quick check question: How might the order of presented options influence an LLM's response in a pairwise comparison task?

## Architecture Onboarding

- Component map: Generate summaries -> Measure baseline self-recognition/preference -> Fine-tune for self-recognition -> Re-measure -> Analyze correlation
- Critical path: Generate summaries → Measure baseline self-recognition/preference → Fine-tune for self-recognition → Re-measure → Analyze correlation
- Design tradeoffs: Pairwise vs. individual measurement settings, choice of datasets (XSUM vs. CNN/DailyMail), temperature settings for generation
- Failure signatures: Inconsistent results across datasets, lack of correlation between self-recognition and self-preference, failure to control for confounders
- First 3 experiments:
  1. Generate summaries from all three models on both datasets and measure baseline self-recognition/preference
  2. Fine-tune one model for self-recognition on 500 examples and measure changes
  3. Fine-tune the same model on control tasks (length, vowel count, readability) and compare effects on self-recognition/preference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the mechanistic explanations for why self-recognition leads to self-preference in LLMs?
- Basis in paper: The paper states that the causal relationship between self-recognition and self-preference is not fully validated and requires mechanistic tools that do not exist for LLMs yet.
- Why unresolved: Current interpretability tools for LLMs are limited, preventing researchers from understanding the internal mechanisms that cause self-recognition to lead to self-preference.
- What evidence would resolve it: Developing mechanistic interpretability tools for LLMs that can trace how self-recognition signals are processed and influence evaluation decisions would provide definitive evidence.

### Open Question 2
- Question: Does self-preference persist when controlling for generation quality?
- Basis in paper: The paper mentions that disproportionate self-preference would require controlling for generation quality using groundtruth annotation.
- Why unresolved: The current experiments do not control for the actual quality of LLM-generated summaries compared to human-written ones.
- What evidence would resolve it: Experiments comparing self-preference when LLM-generated summaries are explicitly known to be of lower, equal, or higher quality than alternatives.

### Open Question 3
- Question: How does self-recognition generalize across different tasks beyond text summarization?
- Basis in paper: The paper notes that cross-dataset evaluation provides initial evidence of generalization but plans to validate the hypothesis on more tasks.
- Why unresolved: The current study is limited to text summarization, and it's unclear whether the observed relationship applies to other domains.
- What evidence would resolve it: Testing the self-recognition/self-preference relationship across diverse NLP tasks and domains.

## Limitations

- The study demonstrates correlation but cannot definitively prove causation between self-recognition and self-preference
- Results are limited to summarization tasks and specific model architectures, limiting generalizability
- Fine-tuning procedure for GPT-3.5 is only partially specified, making exact replication challenging

## Confidence

- **High Confidence**: The existence of non-trivial self-recognition out of the box and the ability to increase this through fine-tuning
- **Medium Confidence**: The linear correlation between self-recognition and self-preference
- **Medium Confidence**: The conclusion that self-recognition is a key mechanism of self-preference

## Next Checks

1. **Causal Intervention Test**: Perform ablation studies where self-recognition is selectively impaired to directly test whether reducing recognition reduces self-preference

2. **Cross-Domain Generalization**: Replicate the self-recognition and self-preference measurements on non-summarization tasks to determine if the correlation generalizes

3. **Mechanistic Analysis**: Analyze the specific features or patterns that LLMs use for self-recognition by examining attention weights and activation patterns