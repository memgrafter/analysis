---
ver: rpa2
title: 'Improving Classification Performance With Human Feedback: Label a few, we
  label the rest'
arxiv_id: '2401.09555'
source_url: https://arxiv.org/abs/2401.09555
tags:
- data
- spam
- labeling
- learning
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of obtaining labeled data for
  training supervised machine learning models, particularly in the context of unstructured
  data. The authors propose a method that combines human feedback with few-shot learning
  to improve classification performance.
---

# Improving Classification Performance With Human Feedback: Label a few, we label the rest

## Quick Facts
- **arXiv ID:** 2401.09555
- **Source URL:** https://arxiv.org/abs/2401.09555
- **Reference count:** 0
- **Primary result:** Combines human feedback with few-shot learning to improve classification performance using LLMs like GPT-3.5, BERT, and SetFit

## Executive Summary
This paper addresses the challenge of obtaining labeled data for supervised machine learning models, particularly for unstructured data. The authors propose an iterative approach that combines large language models with human feedback to improve classification performance. By identifying uncertain predictions and incorporating human corrections on edge cases, the method gradually increases labeled examples and enhances model accuracy, precision, and recall. The approach demonstrates effectiveness across multiple datasets including Financial Phrasebank, Banking, Craigslist, Trec, and Amazon Reviews.

## Method Summary
The proposed method uses large language models to generate initial predictions on unlabeled data, then iteratively refines these predictions by incorporating human feedback on edge cases where the model's confidence is low. The process begins with zero-shot predictions using LLMs, followed by entropy calculation to identify uncertain predictions. Human evaluators then label these high-entropy examples, which are used to fine-tune the model. This cycle repeats, gradually increasing the number of labeled examples and improving classification performance. The approach combines few-shot learning capabilities of LLMs with active learning principles to maximize the information gained from each human label.

## Key Results
- Achieved accuracy rates of 83.89%, 88.92%, and 92.65% on Financial Phrasebank, Banking, and Amazon datasets respectively
- Surpassed zero-shot LLM accuracy by labeling just a few examples (10-50) per dataset
- Demonstrated consistent improvement in precision and recall metrics across all tested datasets
- Showed decreasing entropy values over iterations, indicating increasing model confidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative refinement using human feedback on high-entropy predictions improves model accuracy
- Mechanism: The approach identifies uncertain predictions (high entropy) and obtains human labels for these edge cases. These corrections are then used to fine-tune the model, reducing entropy and increasing prediction confidence over successive iterations.
- Core assumption: Human feedback on the most uncertain predictions provides the most valuable learning signal for the model
- Evidence anchors:
  - [abstract] "This paper focuses on understanding how a continuous feedback loop can refine models, thereby enhancing their accuracy, recall, and precision through incremental human input."
  - [section] "Once this list is established, human evaluators can review and assign the appropriate labels. In Table 2, we can see that the actual label is not spam. By iterating through this process, the model is once again organized based on entropy and probability values."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.575" - The corpus shows related work exists but doesn't directly confirm this specific mechanism
- Break condition: If entropy doesn't decrease over iterations, or if human labeling doesn't improve model performance on the test set

### Mechanism 2
- Claim: Few-shot learning with LLMs can achieve comparable performance to models trained on much larger labeled datasets
- Mechanism: LLMs pre-trained on massive text corpora can generalize from very few labeled examples to perform well on specific classification tasks, reducing the need for extensive manual labeling.
- Core assumption: Pre-trained LLMs have learned general language representations that transfer well to specific classification tasks with minimal fine-tuning
- Evidence anchors:
  - [abstract] "We demonstrate that rather than needing to manually label millions of rows of data, we just need to label a few and the model can effectively predict the rest."
  - [section] "Few-shot learning is a technique that allows a model to utilize just a minimal number of examples to guide the model."
  - [corpus] "Text Classification in the LLM Era -- Where do we stand?" - This related paper title suggests ongoing evaluation of LLMs for classification, supporting the approach
- Break condition: If accuracy doesn't improve significantly beyond what zero-shot learning achieves, or if performance degrades with more fine-tuning iterations

### Mechanism 3
- Claim: Active learning with entropy-based sampling identifies the most informative examples for human labeling
- Mechanism: Instead of randomly selecting examples for human labeling, the system prioritizes examples where the model is least confident (highest entropy), maximizing the information gain from each human label.
- Core assumption: High-entropy predictions are more likely to be incorrect and therefore provide the most valuable learning opportunities for the model
- Evidence anchors:
  - [section] "The first step in active learning from human feedback is to generate a prioritized list of edge cases. These edge cases highlight instances where the model's predictions demonstrate uncertainty or reduced confidence."
  - [section] "Notice how over time, the entropy is decreasing as the model is becoming more stable as more human labels are added."
  - [corpus] "Efficient Few-shot Learning for Multi-label Classification of Scientific Documents with Many Classes" - Related work on efficient few-shot learning supports the approach
- Break condition: If entropy doesn't decrease systematically, or if model performance plateaus despite continued entropy reduction

## Foundational Learning

- Concept: Supervised machine learning with labeled training data
  - Why needed here: The paper's approach is fundamentally about improving supervised classification models with limited labeled data
  - Quick check question: What is the difference between supervised and unsupervised learning?

- Concept: Model evaluation metrics (accuracy, precision, recall)
  - Why needed here: The paper benchmarks its approach using these standard metrics to demonstrate improvement over baseline methods
  - Quick check question: How do precision and recall differ, and when would you prioritize one over the other?

- Concept: Entropy as a measure of uncertainty in probability distributions
  - Why needed here: The approach uses entropy to identify uncertain predictions that would benefit most from human feedback
  - Quick check question: What does it mean when a probability distribution has high entropy?

## Architecture Onboarding

- Component map:
  - Data ingestion layer -> LLM inference engine -> Entropy calculation module -> Active learning selector -> Human feedback interface -> Model fine-tuning pipeline -> Evaluation module

- Critical path: Data → Zero-shot predictions → Entropy calculation → Human feedback selection → Model fine-tuning → Performance evaluation

- Design tradeoffs:
  - Number of human labels vs. model performance: More labels generally improve performance but increase cost
  - Model selection: Different LLMs may perform better on different datasets
  - Entropy threshold: Too high and you miss learning opportunities; too low and you waste human effort

- Failure signatures:
  - Entropy not decreasing over iterations: Suggests feedback isn't being effectively incorporated
  - Performance plateauing early: May indicate insufficient model capacity or suboptimal fine-tuning
  - Significant variance between datasets: Could reveal dataset-specific challenges

- First 3 experiments:
  1. Reproduce the Amazon dataset results with 10, 20, 30, 40, and 50 human-labeled examples, measuring accuracy at each step
  2. Compare the three LLM approaches (GPT-3.5, BERT, SetFit) on the Financial Phrasebank dataset to identify which performs best
  3. Test the effect of different entropy thresholds for selecting human feedback examples on the Craigslist dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different large language model architectures (e.g., T5, Transformer-XL, newer BERT/GPT variants) perform compared to GPT-3.5, BERT, and SetFit in the few-shot learning and active learning framework?
- Basis in paper: [explicit] The paper mentions as a limitation that the study is focused on a limited number of models and suggests expanding to train a diverse array of models in future work.
- Why unresolved: The paper only evaluated GPT-3.5, BERT, and SetFit. Other models were not tested.
- What evidence would resolve it: Empirical results comparing accuracy, precision, and recall of different model architectures on the same datasets using the proposed few-shot learning and active learning approach.

### Open Question 2
- Question: How does the proposed method scale when applied to datasets with complex taxonomies and intricate label hierarchies, such as those found in medical, legal, or financial domains?
- Basis in paper: [inferred] The paper suggests as a next step to train models on datasets with complex taxonomies and intricate label hierarchies to challenge the model's ability to generalize.
- Why unresolved: The paper only tested the method on relatively simple datasets with a limited number of labels.
- What evidence would resolve it: Results showing the performance of the method on datasets with thousands of categories and subcategories, measuring accuracy and other metrics.

### Open Question 3
- Question: What is the optimal balance between the number of labeled examples provided by human feedback and the model's ability to make accurate predictions on the remaining unlabeled data?
- Basis in paper: [explicit] The paper iteratively increases the number of labeled examples by 10 in each iteration, but does not explore the optimal stopping point or the trade-off between labeling effort and model performance.
- Why unresolved: The paper does not provide a systematic analysis of how the number of labeled examples affects model performance and the point at which additional labeling becomes less beneficial.
- What evidence would resolve it: A detailed study varying the number of labeled examples and measuring the corresponding improvement in accuracy, precision, and recall, to determine the optimal number of labeled examples needed for different datasets and model architectures.

## Limitations

- The specific implementation details for entropy calculation and edge case selection remain underspecified, making exact reproduction challenging
- The human feedback component is described abstractly without addressing practical concerns about annotator consistency or bias
- Comparison with baseline models could be strengthened by including more contemporary few-shot learning approaches beyond zero-shot LLM baselines

## Confidence

- **High confidence:** The core premise that human feedback on uncertain predictions can improve classification performance is well-supported by the iterative refinement results shown across datasets
- **Medium confidence:** The claim that few-shot learning with LLMs can achieve performance comparable to models trained on larger labeled datasets is supported but could benefit from more rigorous baseline comparisons
- **Medium confidence:** The effectiveness of entropy-based active learning for selecting informative examples is demonstrated but the optimal entropy thresholds and selection criteria are not fully explored

## Next Checks

1. Implement a controlled reproduction study using the Amazon Reviews dataset with exactly 10, 20, 30, 40, and 50 human-labeled examples, measuring accuracy at each step to verify the claimed performance improvements

2. Conduct ablation studies comparing the full iterative approach against: (a) random selection of examples for human feedback instead of entropy-based selection, and (b) static few-shot learning without iterative refinement

3. Test annotator agreement and bias by having multiple human evaluators label the same edge cases and measuring inter-annotator agreement, then analyzing how this agreement affects model performance improvements