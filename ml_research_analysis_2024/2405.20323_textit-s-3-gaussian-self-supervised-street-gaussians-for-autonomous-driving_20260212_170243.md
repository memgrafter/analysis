---
ver: rpa2
title: '$\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving'
arxiv_id: '2405.20323'
source_url: https://arxiv.org/abs/2405.20323
tags:
- dynamic
- scene
- scenes
- gaussian
- static
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes S3Gaussian, a self-supervised method for decomposing
  dynamic and static elements in street scenes without requiring 3D annotations. The
  key idea is to represent each scene with 3D Gaussians and accompany them with a
  spatial-temporal field network to compactly model the 4D dynamics.
---

# $\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving

## Quick Facts
- arXiv ID: 2405.20323
- Source URL: https://arxiv.org/abs/2405.20323
- Authors: Nan Huang, Xiaobao Wei, Wenzhao Zheng, Pengju An, Ming Lu, Wei Zhan, Masayoshi Tomizuka, Kurt Keutzer, Shanghang Zhang
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on scene reconstruction (31.35 PSNR, 0.911 SSIM, 0.106 LPIPS) and novel view synthesis (27.44 PSNR, 0.857 SSIM, 0.137 LPIPS) on Waymo-Open dynamic32 dataset without requiring 3D annotations

## Executive Summary
S3Gaussian introduces a self-supervised method for decomposing dynamic and static elements in street scenes using 4D Gaussian representations. The method employs a multi-resolution Hexplane structure encoder to efficiently model 4D spatiotemporal features and a multi-head Gaussian decoder to predict deformations and appearance changes. Extensive experiments on the Waymo-Open dataset demonstrate significant improvements over state-of-the-art methods for both scene reconstruction and novel view synthesis tasks, achieving new benchmarks without requiring any 3D annotations.

## Method Summary
S3Gaussian represents street scenes using 3D Gaussians augmented with a spatial-temporal field network to model 4D dynamics. The method uses a multi-resolution Hexplane encoder that decomposes 4D spatiotemporal coordinates into six learnable feature planes (three spatial-only and three spatiotemporal), enabling efficient unsupervised static/dynamic decomposition. A multi-head Gaussian decoder predicts deformation offsets and appearance changes for each Gaussian, while LiDAR point cloud initialization and static-only warm-up provide stable geometric priors. The system is trained self-supervised using a combination of RGB, depth, feature, SSIM, total variation, and regularization losses without requiring any 3D annotations.

## Key Results
- Achieves 31.35 PSNR, 0.911 SSIM, and 0.106 LPIPS for scene reconstruction on Waymo-Open dynamic32 dataset
- Achieves 27.44 PSNR, 0.857 SSIM, and 0.137 LPIPS for novel view synthesis on the same dataset
- Outperforms state-of-the-art methods by significant margins without requiring 3D annotations
- Demonstrates effective static/dynamic decomposition through ablation studies and qualitative results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-resolution Hexplane encoder decomposes 4D spatiotemporal grid into separable spatial-only and spatiotemporal planes, enabling unsupervised static/dynamic decomposition
- Mechanism: Hexplane splits the 4D coordinate into six feature planes (Pxy, Pxz, Pyz for space-only; Pxt, Pyt, Pzt for spacetime). Bilinear interpolation on each plane, then Hadamard product merges them, yielding localized spatial-temporal features that naturally separate static and dynamic scene elements
- Core assumption: Adjacent Gaussians share similar spatial-temporal characteristics, so feature aggregation on planes can cleanly isolate motion-related components
- Evidence anchors: [abstract]: "introduces an efficient spatial-temporal decomposition network to automatically capture the deformation of 3D Gaussians"; [section]: "The HexPlane decomposes the 4D spatial-temporal grid into six multi-resolution learnable feature planes spanning each pair of coordinate axes, each endowed with an orthogonal axis"
- Break condition: If motion patterns are too complex or non-rigid, the plane separation fails and static/dynamic features become entangled

### Mechanism 2
- Claim: Multi-head Gaussian decoder transforms canonical Gaussians via predicted displacement and appearance offsets, allowing 4D dynamic reconstruction without explicit 3D annotations
- Mechanism: Decoder uses three MLPs: Dx predicts positional offset △X, DSH predicts spherical harmonic color offset △SH, Ds predicts semantic features fs. These offsets are applied to canonical Gaussians to model motion and appearance changes over time
- Core assumption: Most scene motion is rigid or smoothly deformable, so displacement fields can be predicted from learned spatial-temporal features
- Evidence anchors: [abstract]: "represent each scene with 3D Gaussians to preserve the explicitness and further accompany them with a spatial-temporal field network to compactly model the 4D dynamics"; [section]: "the multi-head Gaussian decoders calculate the deformation offsets from the canonical representations"
- Break condition: High-speed or highly non-rigid motion violates rigidity assumption, causing reconstruction errors

### Mechanism 3
- Claim: LiDAR point cloud initialization plus static 3D Gaussian warm-up provides stable geometric priors, reducing burden on spatial-temporal network to learn large-scale scene structure
- Mechanism: Initialize Gaussians from downsampled LiDAR point cloud, then perform 5k steps static-only training before activating spatiotemporal network. This ensures Gaussians have reasonable positions and colors before learning motion
- Core assumption: Static scene structure is largely consistent and can be learned before introducing dynamic complexity
- Evidence anchors: [abstract]: "To tackle the challenges in self-supervised street scene decomposition, our method consists of a Multi-resolution Hexplane Structure Encoder to encode 4D grid into feature planes and a multi-head Gaussian Decoder to decode them into deformed 4D Gaussians"; [section]: "We leverage the LiDAR point cloud captured by the vehicle instead of using the original SFM [44] point cloud to provide a better geometric structure"
- Break condition: If LiDAR initialization is poor or dynamic objects dominate early frames, warm-up may lock in wrong static structure

## Foundational Learning

- Concept: 3D Gaussian Splatting (3DGS) fundamentals
  - Why needed here: S3Gaussian extends 3DGS to 4D; understanding covariance matrix factorization, anisotropic Gaussians, and differentiable rendering is prerequisite
  - Quick check question: What is the difference between the scaling matrix S and rotation matrix R in Σ = RSS^T RT?

- Concept: Neural Radiance Fields (NeRF) and differentiable rendering
  - Why needed here: S3Gaussian borrows volume rendering concepts (alpha blending, depth estimation) from NeRF literature; familiarity with density/color field representation is important
  - Quick check question: How does alpha-blending in 3DGS differ from volumetric rendering in NeRF?

- Concept: Multi-resolution feature encoding and spatial-temporal modeling
  - Why needed here: Hexplane encoder and spatiotemporal decomposition rely on multi-scale hashing and plane-based feature aggregation; understanding these concepts is essential for extending to 4D
  - Quick check question: Why does Hexplane use six planes instead of a single 4D voxel grid?

## Architecture Onboarding

- Component map:
  Input (4D coordinates + camera intrinsics/extrinsics) -> Hexplane encoder (multi-resolution planes) -> Multi-head decoder (Dx, DSH, Ds) -> Deformed 4D Gaussians -> Renderer (differentiable splatting) -> Output image + depth + semantic map

- Critical path:
  1. Initialize Gaussians from LiDAR -> warm-up static training (5k steps)
  2. For each frame: Encode 4D coords -> decode offsets -> deform Gaussians -> render
  3. Compute losses -> backpropagate -> update encoder/decoder/Gaussians

- Design tradeoffs:
  - Explicit Gaussians vs. implicit NeRF: Faster rendering, but fixed point count; requires careful initialization
  - Hexplane planes vs. full 4D voxel grid: Memory efficient, but may lose some global 4D context
  - Semantic decoder vs. direct classification: Adds supervision signal for static/dynamic separation without labels

- Failure signatures:
  - Blurry or ghosted dynamic objects -> spatial-temporal separation failing
  - Noisy background -> insufficient static warm-up or overfitting to dynamic parts
  - Missing distant details -> inadequate resolution in Hexplane planes

- First 3 experiments:
  1. Ablation: Remove Hexplane encoder, use single MLP for 4D coordinates; compare PSNR/SSIM
  2. Ablation: Remove static warm-up, train spatiotemporal from scratch; measure convergence speed and quality
  3. Ablation: Remove semantic decoder Ds; evaluate impact on static/dynamic separation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does S3Gaussian handle the challenge of modeling high-speed objects, which the authors acknowledge as a limitation of their approach?
- Basis in paper: [explicit] The paper states that the scene encounters difficulty in modeling objects moving at high speeds, likely due to the deformation field's high variance
- Why unresolved: The authors do not provide a solution or workaround for this limitation, only suggesting it as a focus for future research
- What evidence would resolve it: Testing S3Gaussian on datasets with high-speed objects and comparing its performance to other methods, or developing a new method that specifically addresses this challenge

### Open Question 2
- Question: How does the performance of S3Gaussian compare to other methods when trained on datasets with a more balanced distribution of static and dynamic scenes?
- Basis in paper: [inferred] The authors mention that most public datasets with LiDAR data suffer from a severe imbalance, with simple scenes featuring few dynamic objects. They use the NOTR dataset, which they claim is more balanced, for their experiments
- Why unresolved: The authors do not provide a direct comparison of S3Gaussian's performance on imbalanced datasets versus balanced ones
- What evidence would resolve it: Conducting experiments on both imbalanced and balanced datasets and comparing S3Gaussian's performance on each

### Open Question 3
- Question: How does the choice of the basic resolution for the multi-resolution HexPlane encoder affect the performance of S3Gaussian?
- Basis in paper: [explicit] The authors set the basic resolution for their multi-resolution HexPlane encoder to 64 and upsample it by 2 and 4. They do not explore other resolutions or discuss the impact of this choice on performance
- Why unresolved: The authors do not provide any analysis or discussion on how different resolutions might affect the performance of S3Gaussian
- What evidence would resolve it: Experimenting with different basic resolutions and comparing the performance of S3Gaussian with each

## Limitations

- High-speed object modeling remains challenging due to deformation field variance limitations
- Performance generalization to datasets beyond Waymo-Open has not been tested
- Static warm-up strategy of 5,000 steps is empirically determined without theoretical justification for optimal training schedules

## Confidence

- **High confidence**: Claims about PSNR/SSIM/LPIPS improvements on Waymo-Open benchmarks (direct experimental evidence provided)
- **Medium confidence**: Claims about self-supervised dynamic-static decomposition mechanism (supported by ablation studies but not exhaustively validated)
- **Low confidence**: Claims about scalability to other autonomous driving datasets and real-time deployment scenarios (not tested or demonstrated)

## Next Checks

1. Implement ablation study removing Hexplane encoder to verify its contribution to spatiotemporal decomposition quality
2. Test model performance on alternative autonomous driving datasets (nuScenes, Argoverse) to assess generalizability
3. Conduct ablation of static warm-up phase (test with 0, 2.5k, 10k steps) to determine optimal training schedule