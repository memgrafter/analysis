---
ver: rpa2
title: 'From RAGs to rich parameters: Probing how language models utilize external
  knowledge over parametric information for factual queries'
arxiv_id: '2406.12824'
source_url: https://arxiv.org/abs/2406.12824
tags:
- token
- attention
- context
- subject
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper mechanistically analyzes how language models utilize
  external RAG context versus internal parametric knowledge for factual queries. Using
  causal mediation analysis, attention contributions, and knockouts, the authors find
  that models strongly prefer external context over internal memory when both are
  available.
---

# From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries

## Quick Facts
- **arXiv ID:** 2406.12824
- **Source URL:** https://arxiv.org/abs/2406.12824
- **Reference count:** 18
- **Key outcome:** Language models strongly prefer external RAG context over internal parametric knowledge when both are available for factual queries, with parametric memory (MLPs) being minimally used in the presence of RAG context.

## Executive Summary
This paper investigates how language models utilize external knowledge from retrieval-augmented generation (RAG) systems versus internal parametric knowledge when answering factual queries. Through mechanistic analysis using causal mediation analysis, attention contribution measurements, and knockouts, the authors demonstrate that models exhibit a strong preference for external context over their own internal memory. The study reveals that when RAG context is available, models essentially take a "shortcut" by relying on the external information rather than engaging their parametric memory, with attention contributions from subject tokens reduced by 5-7× compared to vanilla settings without context.

The findings have significant implications for understanding how language models process information and make decisions about which knowledge sources to trust. The research shows this behavior is consistent across both LLaMa-2 and Phi-2 architectures, suggesting it may be a general characteristic of transformer-based language models. The results challenge assumptions about the complementary relationship between external retrieval and internal knowledge, revealing instead that external context can effectively bypass the need for parametric memory retrieval in factual scenarios.

## Method Summary
The study employs a multi-method approach to analyze how language models utilize external RAG context versus internal parametric knowledge. The researchers use causal mediation analysis to trace information flow, attention contribution measurements to quantify the importance of different tokens, and knockout experiments to assess the impact of removing specific components. They compare model behavior in two conditions: vanilla settings without external context and RAG-enabled settings with structured context provision. The analysis focuses on factual queries where ground truth answers are available, allowing for validation of model responses. Both LLaMa-2 and Phi-2 architectures are examined to ensure findings are not model-specific.

## Key Results
- Models strongly prefer external RAG context over internal parametric knowledge when both are available for factual queries
- Parametric memory (MLPs) is minimally used with RAG context present, as models take a "shortcut" using external information
- RAG context reduces attention contributions from subject tokens by 5-7× compared to vanilla settings without context
- The last token residual stream receives more enriched information from context tokens than from subject tokens in the query

## Why This Works (Mechanism)
The mechanism underlying this behavior appears to be rooted in the transformer architecture's attention mechanism, which naturally prioritizes readily available information. When external context is provided, the model can directly access the relevant information through attention weights, bypassing the need to retrieve and process information from its internal parameters. This creates an efficient information processing pathway where the model effectively delegates factual recall to the external RAG system. The causal mediation analysis reveals that information flow is significantly enhanced for context tokens, while the attention contributions from the original query tokens (particularly subject tokens) are dramatically reduced when context is available.

## Foundational Learning
**Causal Mediation Analysis:** A statistical technique for understanding causal pathways by decomposing total effects into direct and indirect effects. Needed to trace how information flows through different components of the model when both internal and external knowledge are available. Quick check: Can be validated by comparing results with ablation studies and observing consistent patterns across different query types.

**Attention Contribution Measurements:** Quantitative assessment of how much each token contributes to the final prediction through the attention mechanism. Essential for understanding which information sources the model prioritizes. Quick check: Results should show consistent patterns across multiple model runs and query types.

**Knockout Experiments:** Method of systematically removing or disabling components to assess their importance. Critical for determining the actual impact of parametric memory versus external context. Quick check: Knockout results should align with causal mediation findings and show minimal degradation when context is available.

**Residual Stream Analysis:** Examination of information flow through the residual connections in transformer layers. Important for understanding how different token representations are combined and enriched. Quick check: The enrichment patterns should be observable across multiple layers and consistent with attention contribution findings.

## Architecture Onboarding

**Component Map:** Input Query -> Embedding Layer -> Transformer Blocks (Self-Attention + Feed-Forward MLPs) -> Output Layer. In RAG setting: External Context is injected into the system and integrated through attention mechanisms.

**Critical Path:** Query tokens -> Self-Attention Mechanism -> Context tokens -> Attention-weighted combination -> Residual Stream enrichment -> Output prediction. The critical insight is that with RAG context present, the path through internal parametric memory becomes largely bypassed.

**Design Tradeoffs:** The architecture prioritizes efficiency by using readily available external information rather than expending computational resources on internal retrieval. This creates a potential vulnerability where models may become overly dependent on external context quality and availability.

**Failure Signatures:** When external context is noisy, contradictory, or unavailable, models may struggle to fall back on their internal knowledge effectively. The reduced engagement with parametric memory could lead to performance degradation in contexts where external information is unreliable or absent.

**First Experiments:**
1. Test the same mechanistic analysis on non-factual queries (reasoning, creative tasks) to see if the preference for external context persists
2. Apply knockouts to specific attention heads to determine which components are most critical for the context-over-internal-knowledge preference
3. Compare attention patterns when external context contradicts internal knowledge to understand how models resolve conflicts

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on factual queries with structured context provision, which may not generalize to more complex reasoning tasks or open-ended generation scenarios
- The experimental design assumes ground truth availability for validation, but real-world RAG applications often deal with ambiguous or conflicting information sources
- The analysis is limited to specific model sizes and architectures, and the mechanistic methods used may not capture all aspects of information flow in these models

## Confidence
- High confidence in the core finding that models prefer external context over internal knowledge when both are available, supported by consistent results across multiple models and analytical approaches
- Medium confidence in the specific quantitative claims about attention reduction (5-7×) and the characterization of parametric memory (MLPs) as minimally used, as these measurements depend on the specific experimental setup and may vary with different query types or model configurations
- Medium confidence in the generalizability of findings to other model families or task types, given the limited scope to factual queries and two specific architectures

## Next Checks
1. Test the same mechanistic analysis pipeline on open-ended generation tasks and compare whether the context-over-internal-knowledge preference persists when queries are not strictly factual
2. Apply the analysis to larger model architectures (e.g., GPT-4, Claude) and smaller models (e.g., LLaMa-3 8B) to determine if the observed behavior scales consistently across the model size spectrum
3. Investigate scenarios where external context is noisy or contradictory to internal knowledge to examine how models balance these competing information sources in non-ideal conditions