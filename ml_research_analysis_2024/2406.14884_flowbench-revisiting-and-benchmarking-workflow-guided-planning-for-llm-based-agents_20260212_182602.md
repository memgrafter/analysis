---
ver: rpa2
title: 'FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based
  Agents'
arxiv_id: '2406.14884'
source_url: https://arxiv.org/abs/2406.14884
tags:
- user
- knowledge
- flight
- workflow
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FlowBench, the first comprehensive benchmark
  for workflow-guided planning in LLM-based agents. The authors formalize three formats
  of workflow knowledge (text, code, and flowchart) and construct a benchmark covering
  51 scenarios across 6 domains with 5,313 multi-turn sessions.
---

# FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents

## Quick Facts
- arXiv ID: 2406.14884
- Source URL: https://arxiv.org/abs/2406.14884
- Reference count: 36
- Key outcome: Current LLM agents achieve only 43.2% success rate on comprehensive workflow-guided planning benchmark, with flowchart formats showing the best performance

## Executive Summary
FlowBench introduces the first comprehensive benchmark for evaluating workflow-guided planning in LLM-based agents, addressing the challenge of planning hallucinations in knowledge-intensive tasks. The benchmark includes 51 scenarios across 6 domains with 5,313 multi-turn sessions, testing agents with three workflow knowledge formats: text, code, and flowchart. Results show that even the best-performing model (GPT-4o) struggles with satisfactory performance, while flowchart formats achieve the best trade-off between performance, adaptability, and user-friendliness. The study reveals that current LLM agents need considerable improvements for reliable planning in real-world applications.

## Method Summary
FlowBench constructs a comprehensive benchmark by first collecting task scenarios from 6 domains, then organizing them into workflow knowledge represented in three formats: text descriptions, Python code, and Mermaid flowcharts. The benchmark generates ground-truth sessions through automated construction and verification processes. Evaluation uses a two-tier framework: static turn-level evaluation compares predicted tool invocations against ground truth, while simulated session-level evaluation uses GPT-4 as a user simulator to assess end-to-end task completion. The study tests multiple LLM models (GPT-4o, GPT-4-Turbo, GPT-3.5-Turbo) across different knowledge format variants to assess performance impacts.

## Key Results
- Current LLM agents achieve only 43.2% and 40.9% success rates on the benchmark, indicating significant room for improvement
- Flowchart format outperforms text and code formats, achieving 6.4-6.5% higher F1 scores than other formats
- Cross-scenario settings show performance degradation due to increased tool availability and complexity
- GPT-4o performs significantly better than GPT-4-Turbo and GPT-3.5-Turbo across all knowledge formats

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Flowchart format provides the best trade-off between performance, adaptability, and user-friendliness
- Mechanism: Flowcharts integrate natural language expressiveness with structured symbolic representation, allowing LLMs to efficiently navigate workflow states while remaining accessible to users
- Core assumption: LLMs can effectively parse Mermaid markdown syntax and utilize flowchart structure for planning
- Evidence anchors:
  - [abstract]: "flowcharts struck the best trade-off among performance, adaptability, and user-friendliness"
  - [section 5.2]: "Overall, the flowchart format generally produces the best performance" with F1 score improvements of 6.4%, 6.5%, and 5.6% over other formats
  - [corpus]: No direct corpus evidence for this specific claim
- Break condition: If LLM fails to parse Mermaid syntax or if users find flowchart editing more difficult than expected

### Mechanism 2
- Claim: External workflow knowledge significantly improves LLM agent planning performance
- Mechanism: Providing task-specific workflow knowledge compensates for LLMs' limited parametric knowledge, reducing planning hallucinations in expertise-intensive domains
- Core assumption: Workflow knowledge is both complete and accurate enough to guide planning effectively
- Evidence anchors:
  - [abstract]: "incorporating external workflow-related knowledge" to address "planning hallucinations"
  - [section 5.2]: "incorporation of workflow knowledge in different formats can significantly facilitate agent planning, achieving evident improvements at both turn level and session level"
  - [corpus]: Weak corpus evidence - no direct citations supporting this specific mechanism
- Break condition: If workflow knowledge contains errors or if LLM cannot effectively utilize the provided knowledge

### Mechanism 3
- Claim: Multi-turn interactive setting better reflects real-world scenarios than single-turn planning
- Mechanism: Allowing users to incrementally refine requirements through ongoing conversations enables more realistic task completion and error correction
- Core assumption: User feedback can effectively guide the agent toward correct task completion
- Evidence anchors:
  - [section 3.1]: "we generalize this to a more realistic setting of multi-turn interactions, allowing users to incrementally refine and modify their requirements"
  - [section 4.3.2]: Simulated session-level evaluation framework that "simulates sequential planning"
  - [corpus]: No direct corpus evidence for this specific interactive paradigm
- Break condition: If user feedback becomes contradictory or if agent fails to maintain context across turns

## Foundational Learning

- Concept: Workflow knowledge representation formats
  - Why needed here: Understanding different formats (text, code, flowchart) is essential for both constructing the benchmark and evaluating LLM performance across formats
  - Quick check question: What are the three workflow knowledge formats evaluated in FlowBench and what are their key characteristics?

- Concept: LLM agent planning paradigms
  - Why needed here: Understanding how LLMs plan and execute actions in multi-turn settings is crucial for interpreting benchmark results and designing future agents
  - Quick check question: How does the multi-turn interaction setting differ from traditional single-turn agent planning approaches?

- Concept: Evaluation metrics for agent performance
  - Why needed here: Understanding tool invocation metrics (precision, recall, F1) and task completion metrics is essential for interpreting results and comparing agent performance
  - Quick check question: What constitutes a correct tool invocation in FlowBench's evaluation framework?

## Architecture Onboarding

- Component map:
  Data collection → Workflow organization → Session generation → Data verification → Static evaluation + Simulated evaluation

- Critical path:
  1. Construct workflow knowledge in all three formats
  2. Generate ground-truth sessions for evaluation
  3. Run evaluation with different LLM models and knowledge format combinations
  4. Analyze performance across single-scenario vs cross-scenario settings

- Design tradeoffs:
  - Knowledge format vs. LLM capability: Code format performs poorly on weaker LLMs
  - Single-scenario vs. cross-scenario: Cross-scenario provides more realistic evaluation but shows performance degradation
  - Turn-level vs. session-level: Turn-level provides granular analysis while session-level reflects real-world task completion

- Failure signatures:
  - Tool invocation failures: Incorrect tool names or missing parameters
  - Planning sequence errors: Missing steps or incorrect order
  - Transition errors: Incorrect recognition of user intent or transition conditions

- First 3 experiments:
  1. Run baseline evaluation with "None" variant (no workflow knowledge) to establish baseline performance
  2. Evaluate all three knowledge formats with GPT-4o to determine format effectiveness
  3. Compare single-scenario vs cross-scenario performance for each knowledge format to assess adaptability requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal formats for workflow knowledge that maximize both agent performance and user-friendliness?
- Basis in paper: [explicit] The authors compare text, code, and flowchart formats, finding flowcharts provide the best trade-off between performance, adaptability, and user-friendliness
- Why unresolved: The paper demonstrates flowcharts perform best but doesn't determine if there are even better formats or optimal ways to combine formats
- What evidence would resolve it: Experiments testing novel workflow representation formats or hybrid approaches that combine strengths of existing formats

### Open Question 2
- Question: How does workflow knowledge impact agent performance across different levels of LLM capability?
- Basis in paper: [explicit] The authors test GPT-4o, GPT-4-Turbo, and GPT-3.5-Turbo, showing performance varies significantly with model capability
- Why unresolved: While the paper shows varying effectiveness across models, it doesn't explore how workflow knowledge might differentially benefit weaker versus stronger models
- What evidence would resolve it: Systematic analysis of how workflow knowledge formats affect performance across a wider range of model capabilities

### Open Question 3
- Question: What are the most common failure modes in workflow-guided planning and how can they be addressed?
- Basis in paper: [explicit] The authors categorize failure reasons including missing steps, incorrect sequences, transition errors, and tool usage issues
- Why unresolved: The paper identifies failure types but doesn't provide solutions for preventing these failures or improving agent planning reliability
- What evidence would resolve it: Research developing techniques to prevent specific failure modes or improve agent planning accuracy

## Limitations

- The evaluation framework relies on simulated user interactions rather than real human feedback, potentially missing real-world complexity
- Benchmark construction may contain biases in task selection and workflow representation that affect generalizability
- Evaluation focuses primarily on planning accuracy without extensive assessment of final task outcome quality

## Confidence

- **High Confidence**: The finding that current LLM agents struggle with satisfactory performance across all knowledge formats (43.2% success rate) is well-supported by comprehensive experimental results across multiple models and domains
- **Medium Confidence**: The claim that flowchart format provides the best trade-off between performance, adaptability, and user-friendliness is supported by quantitative results but requires further validation with real user studies to confirm the user-friendliness aspect
- **Medium Confidence**: The assertion that external workflow knowledge significantly improves planning performance is demonstrated through comparative experiments, though the specific mechanisms by which different formats contribute to improvements need more detailed analysis

## Next Checks

1. Conduct user studies with real human participants to validate the user-friendliness claims of different workflow knowledge formats, particularly comparing flowchart usability against text and code formats

2. Test the benchmark's robustness by introducing noise and errors into workflow knowledge to evaluate how LLM agents handle incomplete or incorrect guidance, assessing the break conditions identified in the mechanisms section

3. Extend evaluation to cross-domain scenarios where workflow knowledge must be adapted or transferred between related but distinct task domains to better assess the practical utility of external workflow knowledge in real-world applications