---
ver: rpa2
title: 'Time Transfer: On Optimal Learning Rate and Batch Size In The Infinite Data
  Limit'
arxiv_id: '2410.05838'
source_url: https://arxiv.org/abs/2410.05838
tags:
- batch
- size
- learning
- rate
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how optimal learning rate and batch size scale
  jointly as model training progresses in the infinite data limit. Using a theoretical
  model based on Li et al.
---

# Time Transfer: On Optimal Learning Rate and Batch Size In The Infinite Data Limit

## Quick Facts
- arXiv ID: 2410.05838
- Source URL: https://arxiv.org/abs/2410.05838
- Reference count: 40
- Primary result: Optimal learning rate and batch size scale jointly as model training progresses in the infinite data limit, with critical batch size increasing linearly with pretraining token budget.

## Executive Summary
This study examines how optimal learning rate and batch size scale jointly as model training progresses in the infinite data limit. Using a theoretical model based on Li et al. (2024), the authors show that optimal learning rate depends on both batch size and pretraining token budget through a critical batch size parameter that evolves linearly with token budget (B_crit ∝ T). They demonstrate that optimal batch size increases with training time, and that naïvely fixing batch size while scaling learning rate becomes suboptimal. Surprisingly, they find that critical batch size evolution is preserved within the Maximal Update Parametrization (µP) family, challenging the conventional view that it depends solely on loss value. Additionally, learning rate sensitivity decreases with longer training and remains constant across µP width scaling. These findings suggest that optimal scaling requires joint consideration of both data and model size limits, pointing toward a unified theoretical framework.

## Method Summary
The authors conduct a comprehensive 5D grid search over learning rate, batch size, token budget, model width, and base model width using the MPT architecture trained on the C4 dataset. They implement Maximal Update Parametrization (µP) scaling and use Decoupled AdamW optimizer with warmup-stable learning rate schedules. For each configuration, they measure validation loss profiles across learning rates and fit a theoretical model to extract critical parameters (B_crit and η_crit). The analysis focuses on how these critical parameters evolve with increasing token budgets, examining the joint optimization landscape for learning rate and batch size scaling.

## Key Results
- Critical batch size increases linearly with pretraining token budget (B_crit ∝ T)
- Optimal batch size must increase alongside critical batch size, making fixed batch size strategies suboptimal
- Critical batch size evolution is preserved within µP family, challenging the view that it depends solely on loss value
- Learning rate sensitivity decreases with longer training and remains constant across µP width scaling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimal learning rate depends on both batch size and pretraining token budget through critical batch size evolution B_crit ∝ T.
- **Mechanism:** As token budget increases, the critical batch size increases proportionally. This shifts the optimal learning rate trajectory, creating a bell-shaped curve where learning rate must scale inversely with batch size beyond the critical point.
- **Core assumption:** The theoretical model from Li et al. (2024) accurately captures the relationship between η, B, and T through critical batch size.
- **Evidence anchors:** [abstract]: "we establish a dependence of the η∗ evolution with T on the batch size B and its relation to the critical batch size B_crit"; [section]: "we define B_crit as the corresponding parameter in Eq. 3.1, also describing the peak position of the bell-shaped curve"; [corpus]: Weak - related papers discuss critical batch size but don't establish T-dependence
- **Break condition:** If the power law exponent α_B deviates significantly from 1.0 ± 0.2, the linear scaling assumption fails.

### Mechanism 2
- **Claim:** Naïvely fixing batch size while scaling learning rate becomes suboptimal as token budget increases.
- **Mechanism:** As B_crit increases with T, keeping B fixed means the model moves from the left side of the bell curve (where η ∝ √B scaling applies) to the right side (where η ∝ 1/√B scaling applies), requiring joint optimization.
- **Core assumption:** The optimal batch size tracks the critical batch size evolution, creating a moving target.
- **Evidence anchors:** [abstract]: "Furthermore, we show that the optimal batch size is positively correlated with B_crit: keeping it fixed becomes suboptimal over time even if learning rate is scaled optimally"; [section]: "we observe a gradual increase of the optimal batch size B∗ with an increase of the pretraining token budget from B∗|T=230 = 2^18 to B∗|T=235 = 2^20"; [corpus]: Weak - related papers discuss batch size scaling but not joint optimization with learning rate
- **Break condition:** If optimal batch size evolution deviates from B_crit trajectory, fixed B strategy may remain viable.

### Mechanism 3
- **Claim:** Critical batch size evolution is preserved within µP family, challenging conventional view that it depends solely on loss value.
- **Mechanism:** Within a µP trajectory, models share the same critical batch size region despite different loss values, suggesting B_crit is determined by model parametrization rather than absolute performance.
- **Core assumption:** µP scaling preserves optimization dynamics that determine B_crit.
- **Evidence anchors:** [abstract]: "Surprisingly, our results demonstrate that the observed optimal η and B dynamics are preserved with µP model scaling, challenging the conventional view of B_crit dependence solely on loss value"; [section]: "we observe no significant change of the curves' shapes and peak positions across d_model values within the same µP trajectory, and also with the change of the base model"; [corpus]: Weak - related papers discuss µP but don't address critical batch size preservation
- **Break condition:** If different µP parametrizations alter the underlying optimization dynamics, B_crit preservation would break.

## Foundational Learning

- **Concept: Stochastic gradient descent dynamics**
  - Why needed here: Understanding how batch size affects gradient noise and convergence requires knowledge of SGD optimization principles
  - Quick check question: What is the relationship between batch size and gradient noise scale in SGD?

- **Concept: Power law scaling relationships**
  - Why needed here: The paper establishes relationships like B_crit ∝ T^α and η_crit ∝ T^β that require understanding of power law behavior
  - Quick check question: If B_crit = aT^α, what happens to B_crit when T increases by a factor of 8 and α = 1.0?

- **Concept: Critical phenomena in optimization**
  - Why needed here: Critical batch size represents a phase transition in optimization behavior that requires understanding of critical phenomena concepts
  - Quick check question: What characterizes a critical point in optimization dynamics?

## Architecture Onboarding

- **Component map:** Theoretical model (Li et al. framework) -> Experimental grid -> Fitting pipeline -> Validation loop
- **Critical path:**
  1. Define hyperparameter grid across all dimensions
  2. Train models and collect L_val(η) profiles
  3. Fit theoretical model to extract critical parameters
  4. Analyze B_crit(T) and η_crit(T) evolution
  5. Validate optimal batch size scaling
- **Design tradeoffs:** Grid resolution vs. computational cost - finer grids provide better parameter estimates but require more compute; Model width range vs. generality - broader range tests µP preservation but increases complexity; Token budget range vs. practical relevance - longer horizons show asymptotic behavior but may be computationally prohibitive
- **Failure signatures:** Poor model fit (low R²) indicates theoretical model inadequacy; Inconsistent B_crit across µP models suggests parametrization issues; Loss profile instability indicates training instability
- **First 3 experiments:**
  1. Train single model with d_model = d_base_model = 1024 at T = 230 tokens, sweep η for B ∈ {2^16, 2^18, 2^20} to establish baseline L_val(η) profiles
  2. Extend experiment 1 to T = 231, 232 tokens to observe critical batch size evolution
  3. Train µP-scaled models (d_model ∈ {256, 512, 1024}) at T = 230 tokens to test B_crit preservation across widths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the critical batch size B_crit evolve when training on datasets with different underlying data distributions?
- Basis in paper: [inferred] The paper notes B_crit ∝ T but focuses on a single dataset (C4). The authors speculate that B_crit might not depend exclusively on loss value, suggesting data distribution could play a role.
- Why unresolved: The experiments only used the C4 dataset. Different datasets may have different intrinsic noise scales or curvature properties that affect B_crit.
- What evidence would resolve it: Experiments measuring B_crit across multiple datasets (e.g., C4, Pile, RedPajama) with varying data quality, diversity, and domain. Comparing B_crit evolution curves and their relation to loss would reveal dataset-specific patterns.

### Open Question 2
- Question: What is the theoretical foundation for the observed η ∝ √T scaling in the high token budget regime?
- Basis in paper: [explicit] The paper observes this scaling empirically for large batch sizes (B = 2^24, 2^26) at high token budgets (T > 2^34) but derives it only from limiting cases of the Li et al. (2024) model.
- Why unresolved: The paper provides empirical observations and limiting-case derivations but lacks a first-principles theoretical explanation for why this specific scaling emerges.
- What evidence would resolve it: A theoretical derivation starting from optimization dynamics (e.g., SDE framework) that predicts η ∝ √T scaling specifically in the high token budget regime, potentially connecting to the noise scale evolution.

### Open Question 3
- Question: Does the joint optimal scaling of learning rate and batch size change when using different optimizer parametrizations beyond µP?
- Basis in paper: [explicit] The authors note their observations are "applicable to this parametrization as well" since µP reduces to SP for the base model, but acknowledge Everett et al. (2024) showed other parametrizations can induce hyperparameter transfer.
- Why unresolved: All experiments were conducted with µP. The paper speculates that other parametrizations might show different dynamics but provides no experimental evidence.
- What evidence would resolve it: Replicating the main experiments (critical batch size evolution, optimal η/B scaling) using alternative parametrizations like NTK, SP with weight decay, or other recent proposals. Comparing the scaling exponents and B_crit evolution across parametrizations would reveal if the phenomena are parametrization-dependent.

## Limitations

- Theoretical model fitting relies on a relatively coarse 5D grid search that may miss finer-grained optimal regions
- Study focuses primarily on transformer-based architectures with specific configurations, limiting generalizability
- Assumption of linear critical batch size scaling with token budget (B_crit ∝ T) is empirically supported but lacks theoretical justification

## Confidence

- **High confidence**: Critical batch size increases with token budget (B_crit ∝ T) - well-supported by empirical evidence across multiple experiments
- **Medium confidence**: Optimal batch size must track critical batch size evolution - supported by observed trends but theoretical justification is limited
- **Medium confidence**: µP preservation of critical batch size evolution - demonstrated within tested range but may not generalize

## Next Checks

1. **Theoretical justification of linear scaling**: Derive the B_crit ∝ T relationship from first principles of SGD optimization dynamics rather than relying solely on empirical fitting.

2. **Broader architectural validation**: Test the joint scaling framework across different model architectures (CNNs, RNNs, MLPs) and training tasks beyond language modeling.

3. **Finer-grained hyperparameter search**: Conduct experiments with higher resolution in the (η, B) space around critical regions to validate the precision of the fitted critical parameters.