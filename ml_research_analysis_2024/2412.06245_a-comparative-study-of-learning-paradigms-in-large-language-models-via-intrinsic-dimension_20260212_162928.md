---
ver: rpa2
title: A Comparative Study of Learning Paradigms in Large Language Models via Intrinsic
  Dimension
arxiv_id: '2412.06245'
source_url: https://arxiv.org/abs/2412.06245
tags:
- experiment
- intrinsic
- accuracy
- type
- dimension
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how supervised fine-tuning (SFT) and in-context
  learning (ICL) affect the intrinsic dimension (ID) of hidden representations in
  large language models. Using the TwoNN estimator, we measure ID across model layers
  for multiple datasets and models, analyzing how ID evolves during SFT and varies
  with the number of ICL demonstrations.
---

# A Comparative Study of Learning Paradigms in Large Language Models via Intrinsic Dimension

## Quick Facts
- arXiv ID: 2412.06245
- Source URL: https://arxiv.org/abs/2412.06245
- Authors: Saahith Janapati; Yangfeng Ji
- Reference count: 40
- Key outcome: ICL consistently induces higher intrinsic dimension than SFT, with ID increasing initially with more ICL demonstrations but plateauing or decreasing beyond peak accuracy.

## Executive Summary
This study investigates how different learning paradigms—supervised fine-tuning (SFT) and in-context learning (ICL)—affect the intrinsic dimension (ID) of hidden representations in large language models. Using the TwoNN estimator, the research measures ID across model layers for multiple datasets and model architectures, analyzing how ID evolves during SFT and varies with the number of ICL demonstrations. The findings reveal that ICL consistently produces higher ID than SFT, with representations occupying higher-dimensional manifolds. Interestingly, ID increases with more ICL demonstrations up to a point, then plateaus or decreases, coinciding with peak model accuracy. SFT with 1000 samples achieves the highest accuracy but with lower ID than ICL. These results suggest that ICL and SFT induce distinct geometric structures in the representation space, with ICL generating more complex embeddings.

## Method Summary
The study employs the TwoNN estimator to measure intrinsic dimension of hidden representations across different layers in large language models. Researchers conducted experiments using multiple datasets and model architectures, comparing ID evolution during supervised fine-tuning versus in-context learning. The analysis tracked how ID changes with varying numbers of ICL demonstrations and different sample sizes for SFT. The methodology focuses on measuring the dimensionality of the manifolds occupied by model representations rather than traditional performance metrics, providing geometric insights into how different learning paradigms shape the internal structure of neural networks.

## Key Results
- ICL consistently induces higher intrinsic dimension than SFT across all tested conditions and model architectures
- ID increases with the number of ICL demonstrations initially but plateaus or decreases beyond a certain point, aligning with peak accuracy
- SFT with 1000 samples achieves the highest accuracy, though with lower ID than ICL
- The geometric structure of representations differs fundamentally between ICL and SFT, with ICL generating more complex embeddings occupying higher-dimensional manifolds

## Why This Works (Mechanism)
The study demonstrates that different learning paradigms induce distinct geometric structures in the representation space of neural networks. The mechanism appears to involve how each learning method shapes the manifold structure of hidden representations. ICL, which relies on demonstrations within the context window, appears to encourage the model to maintain more complex, higher-dimensional representations that can accommodate multiple demonstration examples and generalize across diverse tasks. SFT, being more focused on specific labeled examples, may compress representations into lower-dimensional manifolds optimized for the specific task. The plateau or decrease in ID with excessive ICL demonstrations suggests an optimization process where the model balances representation complexity with task-specific efficiency.

## Foundational Learning
- **Intrinsic Dimension Estimation**: Measuring the minimum number of parameters needed to describe data manifolds; needed to quantify representation complexity beyond raw parameter count; quick check: verify ID estimates are consistent across multiple samples
- **Manifold Learning**: Understanding how high-dimensional data lies on lower-dimensional manifolds; needed to interpret what ID changes mean for model representations; quick check: visualize manifold structure in 2D projections
- **TwoNN Estimator**: A computationally efficient method for estimating intrinsic dimension from nearest neighbor distances; needed for practical ID measurement in large models; quick check: compare TwoNN estimates with ground truth on synthetic data
- **Representation Geometry**: How neural networks organize information in hidden layers; needed to connect ID changes to learning dynamics; quick check: analyze layer-wise ID patterns
- **Learning Paradigm Differences**: Understanding how SFT vs ICL shape model behavior; needed to contextualize ID differences; quick check: measure performance vs ID correlation
- **Neural Tangent Kernel**: Theoretical framework for understanding neural network training dynamics; provides context for why different learning methods produce different geometries; quick check: compare NTK evolution during SFT vs ICL

## Architecture Onboarding
**Component Map**: Data -> Model Architecture -> Hidden Representations -> TwoNN Estimator -> ID Measurements -> Learning Paradigm Analysis
**Critical Path**: Input data flows through model layers, producing hidden representations that are analyzed by TwoNN estimator to produce ID measurements, which are then compared across learning paradigms
**Design Tradeoffs**: Computational efficiency (TwoNN estimator) vs estimation accuracy, model diversity vs experimental complexity, quantitative ID measures vs qualitative geometric interpretation
**Failure Signatures**: Inconsistent ID estimates across similar conditions may indicate estimator instability; lack of ID differences between SFT and ICL may suggest model capacity limitations; ID increasing monotonically with demonstrations may indicate failure to capture optimal complexity
**First Experiments**: 1) Measure ID on random data to establish baseline dimensionality; 2) Compare ID across different model depths for the same learning paradigm; 3) Track ID evolution during early training stages to observe initial geometric changes

## Open Questions the Paper Calls Out
None

## Limitations
- Exclusive reliance on TwoNN estimator without validation using alternative ID estimation methods like MLE or packing numbers
- Limited dataset scope focusing on specific NLP domains, potentially limiting generalizability to other modalities
- Unclear causal relationship between ID changes and model performance, with correlations not establishing direct causation
- Focus on limited set of model sizes and architectures, potentially missing variations across different model families

## Confidence
- High Confidence: ICL induces higher ID than SFT across all tested conditions
- Medium Confidence: ID plateaus or decreases with excessive ICL demonstrations
- Medium Confidence: SFT with 1000 samples achieves highest accuracy

## Next Checks
1. Replicate ID measurements using alternative estimators (MLE, packing numbers) to verify consistency of ICL vs SFT ID differences
2. Conduct ablation studies varying model architectures (RNNs, CNNs, different transformer variants) to determine if ID patterns are architecture-dependent
3. Design controlled experiments manipulating specific linguistic features in input data to determine whether ID changes correlate with particular types of semantic or syntactic complexity in representations