---
ver: rpa2
title: 'CLIPArTT: Adaptation of CLIP to New Domains at Test Time'
arxiv_id: '2405.00754'
source_url: https://arxiv.org/abs/2405.00754
tags:
- clipartt
- adaptation
- clip
- cifar-10
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLIPArTT, a test-time adaptation method for
  CLIP that updates normalization-layer parameters using instance-specific multi-class
  prompts and transductive learning. The method constructs new text prompts from top-k
  predicted classes, combines visual and text similarities, and uses cross-entropy
  to refine predictions without requiring additional image transformations or trainable
  modules.
---

# CLIPArTT: Adaptation of CLIP to New Domains at Test Time

## Quick Facts
- arXiv ID: 2405.00754
- Source URL: https://arxiv.org/abs/2405.00754
- Reference count: 15
- Primary result: CLIPArTT improves CLIP's accuracy on corrupted datasets like CIFAR-100-C (+11.08%), ImageNet-C (+6.34%), and VisDA-C (+2.73%)

## Executive Summary
CLIPArTT introduces a test-time adaptation method for CLIP that updates normalization-layer parameters using instance-specific multi-class prompts and transductive learning. The method constructs new text prompts from top-k predicted classes, combines visual and text similarities, and uses cross-entropy to refine predictions without requiring additional image transformations or trainable modules. CLIPArTT demonstrates significant improvements on corrupted datasets while maintaining competitive performance on clean data, outperforming state-of-the-art methods like TENT and TPT in challenging domain shift scenarios.

## Method Summary
CLIPArTT adapts CLIP to new domains at test time by updating normalization-layer parameters in the visual encoder. The method generates instance-specific multi-class text prompts from the top-k predicted classes, computes image-to-image and text-to-text similarity matrices, and creates pseudo-labels by averaging these similarities. A cross-entropy loss between the pseudo-labels and predictions (computed using image-to-text similarity with multi-class prompts) is used to update the normalization parameters over multiple iterations, without requiring additional training modules or image transformations.

## Key Results
- Improves CLIP's accuracy on CIFAR-100-C by +11.08% compared to vanilla CLIP
- Achieves +6.34% improvement on ImageNet-C and +2.73% on VisDA-C
- Outperforms state-of-the-art methods like TENT and TPT on challenging domain shift scenarios
- Maintains competitive performance on clean datasets while adapting to corrupted data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIPArTT improves accuracy on corrupted datasets by adapting normalization-layer parameters at test time using instance-specific multi-class prompts.
- Mechanism: CLIPArTT updates batch normalization layer parameters in the visual encoder by minimizing cross-entropy loss between a pseudo-label Q (derived from average image-to-image and text-to-text similarities) and the prediction matrix ˆP (from image-to-text similarity with multi-class text prompts).
- Core assumption: The correct class for a given image is likely to be among the top-k predicted classes, and aggregating these into a new prompt improves the supervision signal.
- Evidence anchors:
  - [abstract]: "CLIPArTT improves CLIP’s accuracy on corrupted datasets like CIFAR-100-C (+11.08%), ImageNet-C (+6.34%), and VisDA-C (+2.73%)"
  - [section]: "Our method employs a unique, minimally invasive text prompt tuning process, wherein multiple predicted classes are aggregated into a single new text prompt, used as pseudo label to re-classify inputs in a transductive manner."
- Break condition: If the top-k predictions do not contain the correct class, the pseudo-label will be inaccurate and adaptation may degrade performance.

### Mechanism 2
- Claim: CLIPArTT leverages semantic relationships between samples within a batch to improve adaptation by using pairwise similarity matrices.
- Mechanism: CLIPArTT computes image-to-image similarity matrix Sv and text-to-text similarity matrix St using normalized embeddings. These matrices are combined to form pseudo-labels Q that capture both visual and semantic relationships between batch samples.
- Core assumption: Samples in a batch that are visually or semantically similar should have similar predictions, and this relationship can be used to regularize adaptation.
- Evidence anchors:
  - [section]: "We deploy these two pairwise similarity matrices to compute pseudo-labels... This matrix, along with the pairwise pseudo-labels we introduced in Eq. (3), yield our final TTA loss based on the cross-entropy"
  - [section]: "Our TTA loss in Eq. (5) ensures that the inter-modality (text-to-image) similarities of batch samples are aligned with their intra-modality ones (text-to-text and image-to-image)"
- Break condition: If batch samples are not semantically related or the similarity computation fails, the regularization may not help or could harm adaptation.

### Mechanism 3
- Claim: CLIPArTT's test-time adaptation loss can be interpreted as graph-Laplacian regularization over a bipartite graph.
- Mechanism: The cross-entropy loss in CLIPArTT (Eq. 5) is equivalent to Laplacian regularization that enforces similar embeddings for nodes with high connection weights in the bipartite graph of image and text embeddings.
- Core assumption: Well-trained models have maximum similarity between an image embedding and its corresponding text embedding, allowing the LogSumExp approximation.
- Evidence anchors:
  - [section]: "Proposition 1. The TTA loss in Eq. (5) can be expressed as a Laplacian regularization over a bipartite graph with one set of nodes for image embeddings and another for text embeddings"
  - [section]: "The modified Laplacian LQ + I enforces nodes with a high connection weight (qij) to have similar embeddings, while also avoiding embeddings to collapse into a single vector"
- Break condition: If the assumption about maximum similarity between corresponding embeddings fails, the Laplacian interpretation breaks down and the regularization may not work as intended.

## Foundational Learning

- Concept: Stochastic Neighbor Embedding (SNE)
  - Why needed here: Understanding SNE helps explain how CLIPArTT uses similarity matrices to capture relationships between samples and create pseudo-labels.
  - Quick check question: How does SNE estimate the probability of a point given another point, and how is this related to CLIPArTT's use of cosine similarity?

- Concept: Graph-Laplacian regularization
  - Why needed here: CLIPArTT's loss function can be interpreted as graph-Laplacian regularization, which is a technique used in semi-supervised learning to propagate label information through a graph.
  - Quick check question: What is the role of the Laplacian matrix in graph-Laplacian regularization, and how does CLIPArTT's modified Laplacian LQ + I differ from the standard formulation?

- Concept: Conformal learning and confidence intervals
  - Why needed here: CLIPArTT draws inspiration from conformal learning by creating a conformal set of class predictions to help adapt CLIP towards accurate top-1 predictions.
  - Quick check question: How does conformal prediction assign confidence intervals to predictions, and how does CLIPArTT's approach of using top-k predictions relate to this concept?

## Architecture Onboarding

- Component map:
  - Visual encoder (f_v) mapping images to D-dimensional features
  - Text encoder (f_t) mapping text prompts to D-dimensional features
  - Layer normalization layers in visual encoder (adaptation target)
  - Instance-specific multi-class prompt generator (top-k class aggregation)
  - Similarity matrix computation (image-to-image and text-to-text)
  - Pseudo-label computation (softmax over averaged similarities)
  - Prediction matrix computation (image-to-text similarity with multi-class prompts)
  - Cross-entropy loss computation (between pseudo-labels and predictions)

- Critical path:
  1. Compute zero-shot predictions using original text prompts
  2. Generate instance-specific multi-class text prompts from top-k predictions
  3. Compute image-to-image and text-to-text similarity matrices
  4. Compute pseudo-labels from averaged similarities
  5. Compute predictions using image-to-text similarity with multi-class prompts
  6. Compute cross-entropy loss between pseudo-labels and predictions
  7. Update normalization layer parameters in visual encoder using ADAM optimizer

- Design tradeoffs:
  - Using instance-specific multi-class prompts vs. simple class averaging: CLIPArTT uses natural language aggregation which better exploits CLIP's language understanding but may be more sensitive to prompt construction
  - Batch size selection: Larger batches provide more diverse samples for similarity computation but increase memory usage
  - Number of classes K in prompts: Higher K captures more uncertainty but may introduce noise if incorrect classes are included

- Failure signatures:
  - Performance degradation on clean datasets: May indicate overfitting to adaptation or incorrect pseudo-labels
  - No improvement on corrupted datasets: Could suggest similarity computation is not capturing useful relationships
  - Memory errors during similarity matrix computation: Likely due to batch size being too large for available GPU memory

- First 3 experiments:
  1. Verify that CLIPArTT improves accuracy on CIFAR-10-C with default hyperparameters (K=3, batch size=128, 10 iterations)
  2. Test the effect of different K values (1, 3, 4) on CIFAR-10-C accuracy to find optimal number of classes in prompts
  3. Compare CLIPArTT's adaptation time and memory usage against TENT and TPT on CIFAR-10-C to verify computational efficiency claims

## Open Questions the Paper Calls Out
None explicitly stated in the provided text.

## Limitations
- Effectiveness heavily depends on the quality of top-k predictions, which may be unreliable in highly domain-shifted scenarios
- Computational overhead during inference may be prohibitive for real-time applications or resource-constrained devices
- Specific implementation details of instance-specific prompt construction are not fully specified, affecting reproducibility
- Performance on datasets with different characteristics (e.g., fine-grained classification, multi-label) has not been extensively validated

## Confidence
- **High Confidence**: CLIPArTT improves accuracy on corrupted datasets compared to vanilla CLIP, as demonstrated by consistent improvements across multiple benchmarks (CIFAR-100-C, ImageNet-C, VisDA-C)
- **Medium Confidence**: CLIPArTT outperforms state-of-the-art methods like TENT and TPT on challenging domain shift scenarios, though the margin of improvement varies across datasets
- **Medium Confidence**: The method maintains competitive performance on clean data while adapting to corrupted data, though the exact tradeoff depends on hyperparameters and dataset characteristics

## Next Checks
1. Test CLIPArTT on extremely corrupted versions of CIFAR-10-C and ImageNet-C (e.g., severity 5) to evaluate performance when top-k predictions are highly unreliable
2. Evaluate CLIPArTT on fine-grained classification datasets (e.g., CUB-200, Stanford Cars) to assess effectiveness when visual differences between classes are subtle
3. Measure the exact inference time overhead of CLIPArTT compared to vanilla CLIP across different batch sizes and hardware configurations to quantify computational cost in practical scenarios