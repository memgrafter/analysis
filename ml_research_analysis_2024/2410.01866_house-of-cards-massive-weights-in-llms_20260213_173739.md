---
ver: rpa2
title: 'House of Cards: Massive Weights in LLMs'
arxiv_id: '2410.01866'
source_url: https://arxiv.org/abs/2410.01866
tags:
- massive
- weights
- activations
- llms
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates massive activations in large language models,
  where certain feature dimensions exhibit unexpectedly large magnitudes. The authors
  trace these activations to the intermediate state of feed-forward networks in early
  layers, then define "massive weights" as the rows of projection matrices (Wup and
  Wgate) contributing to these activations.
---

# House of Cards: Massive Weights in LLMs

## Quick Facts
- arXiv ID: 2410.01866
- Source URL: https://arxiv.org/abs/2410.01866
- Reference count: 40
- Key outcome: Massive weights in early feed-forward layers are crucial for LLM functionality, and a curriculum dropout method called MacDrop can improve parameter-efficient fine-tuning by selectively dropping these weights.

## Executive Summary
This paper identifies "massive weights" - specific rows in the projection matrices of feed-forward networks in large language models that produce unusually large activation magnitudes. The authors demonstrate that these massive weights are critical for model performance, as zeroing them causes severe degradation while retaining only these weights preserves functionality. Based on this discovery, they propose MacDrop, a curriculum dropout method that applies dropout to massive weights during fine-tuning, starting with high dropout probability that gradually decreases. MacDrop improves zero-shot performance on downstream tasks and enhances robustness against attacks, particularly when combined with other parameter-efficient fine-tuning methods like LoRA or DoRA.

## Method Summary
The authors first identify massive weights by analyzing activation magnitudes in feed-forward networks, defining them as rows in W_up and W_gate matrices that contribute to unusually large activations. They then develop MacDrop, a curriculum dropout approach that applies dropout to these massive weights during fine-tuning. The method starts with high dropout probability (0.9) that gradually decreases to zero over training steps. MacDrop is designed to work alongside existing parameter-efficient fine-tuning methods, applying dropout only to the massive weight rows while keeping other parameters frozen or tuned separately.

## Key Results
- Zeroing massive weights causes severe performance degradation, while retaining only massive weights preserves model functionality despite many other weights being zeroed
- MacDrop improves zero-shot performance on downstream tasks compared to baseline fine-tuning methods
- MacDrop enhances robustness against adversarial attacks and improves performance on long-context tasks when combined with LoRA or DoRA

## Why This Works (Mechanism)
The massive weights identified in this work appear to play a disproportionate role in the model's ability to represent and process information. By applying curriculum dropout that initially prevents these weights from dominating the learning process and gradually allows them to contribute, MacDrop helps the model develop more robust representations that don't overly rely on these massive weights. This approach is particularly effective when combined with other parameter-efficient fine-tuning methods because it addresses a different aspect of the optimization process - managing the influence of critical weights rather than simply reducing the number of trainable parameters.

## Foundational Learning
- **Massive activations**: Unusually large magnitude values in neural network activations, which can indicate important features or potential numerical instability. Understanding these is crucial for diagnosing model behavior and designing training strategies.
- **Curriculum learning**: A training strategy where the model is gradually exposed to more complex tasks or data. Quick check: Does the learning curve show improved convergence when using curriculum approaches?
- **Parameter-efficient fine-tuning**: Methods that update only a small subset of model parameters during adaptation to new tasks. Quick check: Compare performance and parameter count between full fine-tuning and PEFT methods.
- **Feed-forward networks in transformers**: The MLP layers that process the output of self-attention. Quick check: Verify that these layers are where most parameters reside in transformer architectures.
- **Dropout**: A regularization technique that randomly sets activations to zero during training. Quick check: Does dropout rate affect model generalization on held-out data?
- **Activation magnitude analysis**: Examining the distribution and scale of activation values to understand model behavior. Quick check: Plot activation histograms across different layers to identify patterns.

## Architecture Onboarding

**Component Map**: Input -> Embedding -> Encoder Layers (Self-Attention + Feed-Forward) -> Output Projection
- Massive weights are found specifically in the W_up and W_gate matrices of feed-forward networks in early layers

**Critical Path**: Token embedding → Self-attention → Feed-forward (where massive weights reside) → Output projection
- The feed-forward networks in early layers are identified as the source of massive activations

**Design Tradeoffs**: The paper trades off immediate performance for long-term robustness by initially dropping massive weights during training, which prevents the model from becoming overly dependent on these weights while still allowing them to contribute as training progresses.

**Failure Signatures**: 
- Severe performance degradation when massive weights are zeroed
- Reduced robustness to adversarial attacks when massive weights are not properly managed during fine-tuning
- Poor long-context performance when massive weights are not preserved

**3 First Experiments**:
1. Measure activation magnitudes across different layers to identify where massive activations occur
2. Zero out massive weights and evaluate performance degradation
3. Retain only massive weights and test if model functionality is preserved

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness is conditional on the presence of massive activation phenomena, which may not exist in all model architectures
- Limited ablation studies on hyperparameters like dropout probability schedule and massive weight selection criteria
- Lack of theoretical grounding for why massive weights are crucial for model functionality

## Confidence
- High confidence: Identification of massive activations in early feed-forward layers, observation that zeroing massive weights severely degrades performance, and basic MacDrop methodology
- Medium confidence: Claim that retaining only massive weights preserves functionality, and effectiveness of MacDrop when combined with LoRA/DoRA
- Low confidence: Generalizability to models without massive activation phenomena, and exact mechanisms by which massive weights contribute to long-context performance and robustness

## Next Checks
1. Test MacDrop on a broader range of model architectures including those without massive activation phenomena to establish generalizability boundaries
2. Conduct ablation studies varying the dropout probability schedule and massive weight selection criteria to optimize MacDrop hyperparameters
3. Perform mechanistic interpretability analysis to understand why massive weights are critical for functionality and how they contribute to long-context understanding