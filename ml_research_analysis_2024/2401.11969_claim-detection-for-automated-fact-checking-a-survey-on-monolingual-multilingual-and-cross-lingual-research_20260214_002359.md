---
ver: rpa2
title: 'Claim Detection for Automated Fact-checking: A Survey on Monolingual, Multilingual
  and Cross-Lingual Research'
arxiv_id: '2401.11969'
source_url: https://arxiv.org/abs/2401.11969
tags:
- claim
- claims
- detection
- data
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive overview of multilingual
  claim detection research for automated fact-checking. It categorizes existing work
  into three key tasks: identifying verifiable claims, prioritizing claims, and matching
  or clustering similar claims across languages.'
---

# Claim Detection for Automated Fact-checking: A Survey on Monolingual, Multilingual and Cross-Lingual Research

## Quick Facts
- arXiv ID: 2401.11969
- Source URL: https://arxiv.org/abs/2401.11969
- Reference count: 21
- This survey provides a comprehensive overview of multilingual claim detection research for automated fact-checking.

## Executive Summary
This survey reviews research on multilingual claim detection for automated fact-checking, categorizing work into three tasks: identifying verifiable claims, prioritizing claims, and matching or clustering similar claims across languages. The review highlights the increasing reliance on transformer-based models, particularly multilingual variants like mBERT and XLM-R, for handling multiple languages. Despite progress, challenges remain due to limited multilingual datasets, the need for generalizable solutions across sources and modalities, and the lack of annotated data for claim clustering. The survey emphasizes the importance of developing robust, language-agnostic models and expanding multilingual datasets to advance the field.

## Method Summary
The survey synthesizes findings from existing research on multilingual claim detection, analyzing approaches across three key tasks: verifiable claim identification, claim prioritization, and claim matching/clustering. It examines the use of transformer models (mBERT, XLM-R, RoBERTa, DistilBERT) for multilingual settings, evaluates data augmentation techniques including back translation and synthetic data generation using large language models, and discusses challenges in cross-lingual transfer learning. The review is based on analysis of datasets and studies covering multiple languages including English, Arabic, Bulgarian, Dutch, Turkish, Hindi, Bengali, Malayalam, Tamil, Spanish, Catalan, Galician, and Basque, primarily from social media sources.

## Key Results
- Multilingual claim detection benefits from fine-tuning pre-trained multilingual transformer models (mBERT, XLM-R) on combined language datasets
- Claim matching tasks improve when integrating semantic similarity embeddings with ranking algorithms like BM25 in multilingual settings
- Large language models can generate synthetic data to address limited annotated training data, particularly for low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual claim detection benefits from fine-tuning pre-trained multilingual transformer models (mBERT, XLM-R) on combined language datasets.
- Mechanism: Cross-lingual fine-tuning leverages shared linguistic features across languages, allowing models to generalize claim detection to unseen languages without additional monolingual data.
- Core assumption: Multilingual transformers capture language-agnostic representations of factual claims, and combining datasets from multiple languages improves generalization.
- Evidence anchors:
  - [abstract] "The review highlights the increasing reliance on transformer-based models, particularly multilingual variants like mBERT and XLM-R, for handling multiple languages."
  - [section 3.2] "They observed similar or improved performance in cross-lingual settings when the models were trained using the combined dataset (dataset including all three languages under study)."
  - [corpus] Weak. Corpus neighbors do not directly address this mechanism but focus on bias and retrieval issues.
- Break condition: If multilingual datasets are too small or unbalanced across languages, the model may overfit to high-resource languages, leading to poor performance on low-resource ones.

### Mechanism 2
- Claim: Claim matching tasks improve when integrating semantic similarity embeddings with ranking algorithms (e.g., BM25) in multilingual settings.
- Mechanism: Using semantic embeddings (e.g., LaBSE) alongside traditional keyword-based ranking increases recall of relevant claims across languages, compensating for lexical mismatches.
- Core assumption: Semantic embeddings capture deeper semantic relationships between claims that keyword matching alone cannot, especially across languages.
- Evidence anchors:
  - [section 5.2] "The experiment results showed the BM25 algorithm is ineffective in handling multilingual environments and multilingual embedding representations such as LaBSE retrieve similar claims effectively."
  - [corpus] Weak. Corpus neighbors discuss bias and retrieval issues but do not confirm this specific combination approach.
- Break condition: If embeddings are poorly aligned or trained on non-overlapping domains, semantic similarity may be misleading and degrade retrieval quality.

### Mechanism 3
- Claim: Large language models (LLMs) can be used for synthetic data generation to address limited annotated training data in claim detection.
- Mechanism: LLMs generate plausible claim examples and labels, expanding training sets without manual annotation, enabling better model training especially for low-resource languages.
- Core assumption: LLMs can generate linguistically and factually consistent claims that mimic real data distribution, and synthetic examples retain enough realism for training.
- Evidence anchors:
  - [section 5.4] "The authors generated a large number of synthetic claims from fact-checked claims using text-to-text transfer transformer (T5) and Chat-GPT models."
  - [abstract] "The survey emphasizes the importance of developing robust, language-agnostic models and expanding multilingual datasets to advance the field."
  - [corpus] Weak. Corpus neighbors do not discuss synthetic data generation.
- Break condition: If synthetic claims introduce factual inaccuracies or bias, model performance may degrade or learn incorrect patterns.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: Enables models trained on one language to detect claims in others without extensive new annotations.
  - Quick check question: What happens if the source and target languages have very different syntactic structures? How does the model adapt?

- Concept: Semantic similarity vs keyword matching
  - Why needed here: Claim matching must recognize paraphrases and translations, not just exact lexical matches.
  - Quick check question: How does cosine similarity between sentence embeddings differ from BM25 score in capturing claim equivalence?

- Concept: Data augmentation via synthetic generation
  - Why needed here: Limited multilingual labeled data restricts model performance; synthetic examples can mitigate this.
  - Quick check question: What are the risks of hallucination or factual inconsistency when using LLMs to generate training claims?

## Architecture Onboarding

- Component map: Data ingestion -> Preprocessing (language detection, tokenization) -> Embedding generation (multilingual transformers) -> Classification/Ranking/Clustering -> Evaluation (precision, recall, F1, MAP@K)
- Critical path: Training multilingual transformer models on combined datasets, then applying to unseen language data for claim detection and matching
- Design tradeoffs: Balancing dataset size and language coverage vs. model complexity and training time; choosing between semantic similarity or classification for claim matching
- Failure signatures: Poor cross-lingual performance indicates insufficient shared representation; high false positives in matching suggest embedding misalignment; low precision in detection points to inadequate fine-tuning data
- First 3 experiments:
  1. Fine-tune mBERT on combined English, Arabic, and Bulgarian datasets for claim verifiability; evaluate cross-lingual transfer to Turkish
  2. Compare BM25 alone vs. BM25 + LaBSE embeddings for claim retrieval across languages; measure MAP@K and precision@K
  3. Generate synthetic claims using T5 for low-resource languages; retrain classifier and evaluate impact on F1 score

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal multilingual embedding representations for cross-lingual claim matching, and how do they compare to traditional ranking algorithms like BM25?
- Basis in paper: [explicit] The paper discusses the use of multilingual embeddings like LaBSE for claim matching and notes that BM25 performs poorly in multilingual settings unless combined with embeddings.
- Why unresolved: While some studies have shown the effectiveness of embeddings like LaBSE, a comprehensive comparison across various multilingual models and their combinations with traditional algorithms is lacking.
- What evidence would resolve it: A systematic evaluation comparing different multilingual embedding models (e.g., LaBSE, XLM-R, mBERT) both independently and in combination with BM25 or other ranking algorithms on a diverse set of multilingual claim matching datasets.

### Open Question 2
- Question: How can the temporal nature of claim validity and public interest be incorporated into automated fact-checking systems?
- Basis in paper: [explicit] The paper highlights the challenge of claims changing over time and the lack of research incorporating this temporal aspect.
- Why unresolved: Existing datasets and models do not adequately address the dynamic nature of claims, making it difficult to develop systems that can adapt to evolving information landscapes.
- What evidence would resolve it: Development and evaluation of models that incorporate temporal features (e.g., timestamps, evolving contexts) and their impact on claim detection, prioritization, and matching accuracy over time.

### Open Question 3
- Question: What are the most effective strategies for generating synthetic training data for claim detection tasks, particularly in low-resource languages?
- Basis in paper: [explicit] The paper mentions the use of large language models for synthetic data generation to address the lack of annotated data, but the effectiveness and generalizability of these approaches are not fully explored.
- Why unresolved: While synthetic data generation shows promise, there is a need for systematic evaluation of different generation techniques, their impact on model performance, and potential biases introduced.
- What evidence would resolve it: Comparative studies evaluating the performance of models trained on synthetic data generated by various techniques (e.g., GPT models, T5) against models trained on human-annotated data across multiple languages and claim detection tasks.

## Limitations
- The survey relies on reported results from existing studies, which may have inherent biases or limitations and may not be directly comparable
- The review does not provide empirical evidence for all mechanisms and approaches discussed, relying instead on findings from other studies
- The survey does not address the temporal nature of claim validity and public interest, which is a significant challenge in automated fact-checking

## Confidence
- **High Confidence**: The effectiveness of transformer-based models (BERT, mBERT, XLM-R) for multilingual claim detection is well-supported by multiple studies
- **Medium Confidence**: The benefits of cross-lingual fine-tuning and the use of semantic embeddings for claim matching are supported by evidence but may vary depending on specific implementations and datasets
- **Low Confidence**: The efficacy of synthetic data generation for low-resource languages is based on limited studies and may be sensitive to the quality of generated claims

## Next Checks
1. **Cross-Lingual Transfer Evaluation**: Conduct experiments to evaluate the cross-lingual transfer capabilities of mBERT and XLM-R by fine-tuning on a multilingual dataset and testing on an unseen language. Measure performance drop and identify factors influencing transfer success.
2. **Semantic Embedding vs. Keyword Matching**: Implement a controlled experiment comparing BM25 alone versus BM25 + LaBSE embeddings for multilingual claim retrieval. Use identical datasets and evaluation metrics to isolate the impact of semantic embeddings.
3. **Synthetic Data Quality Assessment**: Generate synthetic claims using T5 or ChatGPT for a low-resource language, then evaluate the factual consistency and linguistic quality of generated claims. Assess whether synthetic data improves model performance without introducing bias or errors.