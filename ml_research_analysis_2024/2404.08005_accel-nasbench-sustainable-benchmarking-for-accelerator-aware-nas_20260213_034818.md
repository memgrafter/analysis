---
ver: rpa2
title: 'Accel-NASBench: Sustainable Benchmarking for Accelerator-Aware NAS'
arxiv_id: '2404.08005'
source_url: https://arxiv.org/abs/2404.08005
tags:
- search
- training
- performance
- datasets
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Accel-NASBench, the first NAS benchmark for
  the ImageNet2012 dataset that incorporates on-device performance of accelerators
  including GPUs, TPUs, and FPGAs. The key contribution is a method to search for
  training proxies that reduce model training costs while preserving architecture
  rankings, enabling benchmark construction for large-scale datasets.
---

# Accel-NASBench: Sustainable Benchmarking for Accelerator-Aware NAS

## Quick Facts
- arXiv ID: 2404.08005
- Source URL: https://arxiv.org/abs/2404.08005
- Reference count: 20
- This paper introduces the first ImageNet2012-based NAS benchmark incorporating on-device accelerator performance, achieving R² > 0.98 and Kendall's Tau > 0.9 for surrogate predictions.

## Executive Summary
This paper addresses the challenge of constructing sustainable Neural Architecture Search (NAS) benchmarks for large-scale datasets like ImageNet2012 by introducing a training proxy search technique that reduces computational costs while preserving architecture rankings. The authors construct Accel-NASBench, a bi-objective benchmark that combines accuracy with accelerator-specific performance metrics across six hardware platforms. The benchmark enables zero-cost discovery of state-of-the-art hardware-aware models, achieving up to 1.8% higher accuracy and 55% better throughput compared to EfficientNet-B0 on FPGA platforms.

## Method Summary
The authors develop a training proxy search method that maximizes rank correlation between reference and proxy evaluations while minimizing training costs. This technique is applied to construct a bi-objective NAS benchmark for ImageNet2012 by collecting accuracy and on-device performance data for 5.2k architectures sampled from the MnasNet search space. The dataset is then used to train surrogate models (XGBoost, LGBoost, Random Forests, SVR) that predict both accuracy and hardware-specific performance metrics. The benchmark supports multi-objective hardware-aware NAS by providing realistic performance predictions for six different accelerator platforms including GPUs, TPUs, and FPGAs.

## Key Results
- Achieves R² > 0.98 and Kendall's Tau > 0.9 for both accuracy and throughput predictions
- Successfully constructs the first ImageNet2012-based NAS benchmark with accelerator-aware metrics
- Enables zero-cost discovery of hardware-aware models achieving up to 1.8% higher accuracy and 55% better throughput on FPGA platforms compared to EfficientNet-B0

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The training proxy search maximizes rank correlation between true and proxy evaluations, enabling cheaper but still effective model selection.
- Mechanism: By searching for a proxy training configuration that minimizes training time while maximizing Kendall's Tau rank correlation between proxy and reference evaluations, the method preserves the relative ordering of architectures. This allows the benchmark to use proxy-trained models for dataset collection without significantly altering the quality of selected architectures.
- Core assumption: Architecture ranking is more important than absolute performance accuracy for NAS benchmarks, and training proxies can preserve this ranking while reducing cost.
- Evidence anchors:
  - [abstract] "We present a technique that allows searching for training proxies that reduce the cost of benchmark construction by significant margins, making it possible to construct realistic NAS benchmarks for large-scale datasets."
  - [section 3.2] "We formulate our problem as an architecture rank-correlation maximization problem. We aim to maximize the rank-correlation between true evaluation (i.e., using a reference, high-fidelity training scheme) and proxified evaluation..."
- Break condition: If the proxy training scheme fails to maintain sufficient rank correlation (e.g., Kendall's Tau drops below 0.8), the benchmark would lose its ability to identify high-quality architectures accurately.

### Mechanism 2
- Claim: The method constructs the first ImageNet2012-based NAS benchmark by using a computationally efficient training proxy instead of dataset proxies or synthetic variants.
- Mechanism: Instead of using smaller proxy datasets like CIFAR-10 or downsampled ImageNet variants, the approach directly uses ImageNet2012 with a cost-reducing training proxy. This maintains the dataset's complexity and realism while making benchmark construction feasible.
- Core assumption: Direct search on large-scale datasets yields models with higher parameter efficiency than proxy-based search, but is computationally prohibitive without cost-reducing techniques.
- Evidence anchors:
  - [abstract] "Using this technique, we construct an open-source bi-objective NAS benchmark for the ImageNet2012 dataset..."
  - [section 2.2.1] "However, the construction of NAS benchmarks for large-scale datasets has remained elusive owing to the computational intractability of the evaluation of models on these datasets."
- Break condition: If the training proxy introduces biases specific to ImageNet2012 that don't generalize to other large-scale datasets, the approach may not scale to other complex datasets.

### Mechanism 3
- Claim: The benchmark accurately simulates real hardware performance by using end-to-end throughput measurements on actual accelerators rather than analytical approximations.
- Mechanism: Instead of estimating performance through analytical methods that sum up individual block performance, the method collects actual throughput and latency measurements on real hardware platforms (TPUv2/v3, GPUs, FPGAs) for 5.2k architectures. This provides realistic performance data for hardware-aware NAS.
- Core assumption: End-to-end hardware measurements capture device-specific factors (memory bandwidth, data reuse, off-chip memory access patterns) that analytical approximations miss.
- Evidence anchors:
  - [abstract] "we also explore performance benchmarks for high-performance hardware accelerators, including GPUs, TPUs, and FPGAs."
  - [section 3.3.2] "For training the performance surrogates, we perform on-device measurements of inference throughputs of the 5.2k randomly sampled architectures on 6 accelerator platforms..."
- Break condition: If hardware-specific factors change significantly between measurement time and deployment time (e.g., new hardware revisions or firmware updates), the benchmark's performance predictions may become outdated.

## Foundational Learning

- Concept: Kendall's Tau rank correlation
  - Why needed here: Used to measure how well the proxy training scheme preserves the relative ordering of architectures compared to the reference scheme. Critical for validating that the cost-reduction doesn't sacrifice benchmark quality.
  - Quick check question: What does a Kendall's Tau of 0.926 indicate about the relationship between proxy and reference architecture rankings?

- Concept: Zero-cost NAS benchmarking
  - Why needed here: The entire approach relies on creating surrogate models that can predict architecture performance without expensive training or hardware measurements, enabling efficient NAS evaluation.
  - Quick check question: How do surrogate benchmarks differ from traditional NAS evaluation in terms of computational requirements?

- Concept: Hardware-aware neural architecture search
  - Why needed here: The benchmark incorporates on-device performance metrics (throughput/latency) for multiple accelerator types, requiring understanding of how architecture design affects hardware-specific performance.
  - Quick check question: Why can't conventional metrics like FLOPs accurately predict on-device performance across different hardware platforms?

## Architecture Onboarding

- Component map: Training proxy search -> Dataset collection -> Surrogate fitting -> Benchmark evaluation
- Critical path: Training proxy search → Dataset collection → Surrogate fitting → Benchmark evaluation
  The training proxy search must complete first as it determines the cost-effective training scheme used throughout dataset collection.
- Design tradeoffs:
  - Training cost vs. accuracy: More expensive training proxies yield better rank correlation but increase benchmark construction cost
  - Hardware coverage vs. measurement cost: More accelerator types provide better hardware-aware search but increase data collection burden
  - Search space coverage vs. dataset size: Larger datasets improve surrogate quality but increase construction cost
- Failure signatures:
  - Low rank correlation (KTau < 0.8) between proxy and reference evaluations indicates the training proxy is inadequate
  - Poor surrogate predictive performance (R² < 0.9) suggests insufficient or unrepresentative training data
  - Benchmark predictions diverging from true evaluations indicates issues with surrogate models or data quality
- First 3 experiments:
  1. Run the training proxy search on a small subset of the search space to validate the methodology before full-scale implementation
  2. Collect accuracy data for 100-200 architectures using the identified proxy training scheme and train initial accuracy surrogate to check predictive performance
  3. Perform end-to-end throughput measurements on 10-20 architectures across one hardware platform to validate the measurement pipeline before full dataset collection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust are the training proxies to different search spaces and datasets beyond MnasNet/ImageNet2012?
- Basis in paper: [inferred] The paper demonstrates effectiveness on one specific search space (MnasNet) and dataset (ImageNet2012), but does not evaluate generalizability to other search spaces or datasets.
- Why unresolved: The paper only provides results for one search space and dataset combination, leaving open questions about whether the proxy search methodology would work equally well for other search spaces or datasets.
- What evidence would resolve it: Systematic evaluation of the proxy search methodology across multiple diverse search spaces (e.g., DARTS, ResNet variants) and datasets (e.g., CIFAR-100, COCO) showing consistent performance improvements and cost reductions.

### Open Question 2
- Question: What is the optimal trade-off between training proxy cost reduction and accuracy preservation across different hardware platforms?
- Basis in paper: [explicit] The paper mentions searching for training proxies with a constraint on training time, but doesn't systematically explore how different proxy configurations affect accuracy-throughput trade-offs on different hardware.
- Why unresolved: The paper uses a fixed training time constraint (3 GPU-hours) without exploring how varying this constraint affects the quality of discovered architectures across different hardware platforms.
- What evidence would resolve it: Comprehensive analysis showing how different proxy training time budgets affect the Pareto-optimal solutions discovered on each hardware platform, with systematic evaluation of the accuracy-throughput trade-off.

### Open Question 3
- Question: How does the quality of hardware performance benchmarks affect the final discovered architectures in multi-objective search?
- Basis in paper: [explicit] The paper uses on-device measurements for hardware performance but doesn't evaluate how measurement noise or approximation affects the final discovered models.
- Why unresolved: While the paper validates that discovered models achieve good accuracy-throughput trade-offs, it doesn't quantify how measurement errors in hardware performance benchmarks propagate through the search process.
- What evidence would resolve it: Controlled experiments injecting varying levels of noise into hardware performance measurements and measuring the impact on final discovered architectures' quality across different hardware platforms.

## Limitations
- The approach is validated only on the MnasNet search space, limiting generalizability to other architectural families
- The training proxy search introduces additional computational overhead that scales with search space size
- Performance predictions are tied to specific hardware configurations and may not transfer well to newer accelerator generations

## Confidence
- **High Confidence**: The core methodology of using training proxies to reduce benchmark construction costs while preserving architecture rankings is well-supported by the strong empirical results (R² > 0.98, Kendall's Tau > 0.9).
- **Medium Confidence**: The generalizability of the approach to other search spaces and datasets beyond ImageNet2012, while theoretically sound, requires further validation.
- **Medium Confidence**: The zero-cost discovery claims, while impressive on the tested hardware platforms, may face challenges when applied to different accelerator architectures or deployment scenarios.

## Next Checks
1. **Cross-dataset validation**: Test the training proxy methodology on a different large-scale dataset (e.g., JFT-300M or COCO) to verify generalizability beyond ImageNet2012.
2. **Search space generalization**: Apply the benchmark construction approach to a different search space (e.g., DARTS or MobileNetV3) to assess the method's flexibility across architectural families.
3. **Hardware transfer study**: Evaluate how well performance predictions from one hardware generation (e.g., TPUv3) transfer to newer hardware (e.g., TPUv4) to quantify the benchmark's temporal validity.