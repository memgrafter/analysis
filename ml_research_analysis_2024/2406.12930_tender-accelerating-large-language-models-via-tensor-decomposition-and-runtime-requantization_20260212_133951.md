---
ver: rpa2
title: 'Tender: Accelerating Large Language Models via Tensor Decomposition and Runtime
  Requantization'
arxiv_id: '2406.12930'
source_url: https://arxiv.org/abs/2406.12930
tags:
- tender
- quantization
- channels
- channel
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tender is an algorithm-hardware co-design technique for efficiently
  deploying LLM inference with low-precision integer quantization. It addresses the
  challenge of outlier values in LLM activations, which hinder low-precision quantization.
---

# Tender: Accelerating Large Language Models via Tensor Decomposition and Runtime Requantization

## Quick Facts
- arXiv ID: 2406.12930
- Source URL: https://arxiv.org/abs/2406.12930
- Authors: Jungi Lee; Wonbeom Lee; Jaewoong Sim
- Reference count: 40
- Key outcome: Tender achieves up to 2.63× speedup over outlier-aware accelerators while retaining comparable model performance to FP16 baseline with INT8 quantization.

## Executive Summary
Tender presents an algorithm-hardware co-design technique for efficient LLM inference with low-precision integer quantization. The key innovation addresses outlier values in LLM activations that hinder low-precision quantization by decomposing activation tensors along the channel dimension and isolating outlier channels. This allows for implicit runtime requantization using simple shifter logic in tensor compute units, avoiding costly explicit dequantization/quantization operations. The technique achieves higher accuracy and inference performance compared to state-of-the-art methods while being less intrusive to existing accelerators.

## Method Summary
Tender's core approach involves decomposing activation tensors along the channel dimension, isolating outlier channels, and applying a "power-of-2" channel decomposition rule with different scale factors for each group. The decomposed tensors are then requantized at runtime using simple shifter logic in the tensor compute units, avoiding explicit dequantization/quantization operations. This approach allows for efficient low-precision integer quantization (INT8 and INT4) while maintaining model accuracy. The technique is evaluated on various LLM models (OPT, LLaMA, Llama-2) and datasets (WikiText-2, Penn Treebank, GLUE benchmark), demonstrating superior performance compared to state-of-the-art quantization techniques.

## Key Results
- Tender retains comparable model performance to FP16 baseline with INT8 quantization
- Tender outperforms other outlier-aware PTQ techniques in INT4 quantization with up to 10988× lower perplexity
- Tender hardware achieves up to 2.63× speedup over outlier-aware accelerators

## Why This Works (Mechanism)
Tender works by addressing the fundamental challenge of outlier values in LLM activations that prevent efficient low-precision quantization. By decomposing activation tensors along the channel dimension and isolating outlier channels, Tender can apply different scale factors to different groups of channels. The runtime requantization using shifter logic in tensor compute units allows for efficient processing of these decomposed tensors without the overhead of explicit dequantization/quantization operations. This approach enables the use of low-precision integer quantization while maintaining model accuracy.

## Foundational Learning
- Tensor Decomposition: Why needed - to isolate outlier channels for efficient quantization; Quick check - verify decomposition preserves activation distribution
- Power-of-2 Channel Decomposition: Why needed - enables efficient runtime requantization using shifter logic; Quick check - confirm power-of-2 grouping doesn't degrade accuracy
- Runtime Requantization: Why needed - avoids costly explicit dequantization/quantization operations; Quick check - measure overhead of shifter-based requantization
- Shifter Logic Implementation: Why needed - enables efficient bit-shift operations for requantization; Quick check - verify shifter accuracy and latency
- Tensor Compute Units: Why needed - specialized hardware for tensor operations with requantization support; Quick check - validate TCUs can handle decomposed tensors efficiently
- Scale Factor Management: Why needed - different scale factors for different channel groups; Quick check - ensure proper scale factor application during inference

## Architecture Onboarding

Component Map: LLM Model -> Tensor Decomposition -> Runtime Requantization -> Shifter Logic -> Tensor Compute Units -> Output

Critical Path: The critical path involves tensor decomposition, runtime requantization using shifter logic, and tensor compute operations. The efficiency of this path determines overall inference performance.

Design Tradeoffs:
1. Precision vs. Performance: Lower precision (INT4) offers better performance but may impact accuracy
2. Decomposition Granularity: Finer decomposition may improve accuracy but increase hardware complexity
3. Scale Factor Optimization: More scale factors can better handle outliers but increase memory requirements

Failure Signatures:
- Accuracy degradation due to aggressive decomposition
- Performance bottlenecks in runtime requantization
- Hardware resource underutilization

First 3 Experiments:
1. Evaluate model accuracy degradation with increasing decomposition granularity
2. Measure runtime requantization overhead across different tensor sizes
3. Profile hardware resource utilization under various workload patterns

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the accuracy of Tender scale with larger and more complex LLM models beyond the evaluated sizes (6.7B to 70B parameters)?
- Basis in paper: [inferred] The paper evaluates Tender on OPT and Llama-2 models up to 70B parameters, but does not explore its performance on even larger models.
- Why unresolved: The paper focuses on demonstrating Tender's effectiveness on current state-of-the-art models, but the scalability to future, larger models remains untested.
- What evidence would resolve it: Comprehensive evaluation of Tender on LLM models with trillions of parameters or more, comparing its accuracy and efficiency to other quantization methods.

### Open Question 2
- Question: What is the impact of Tender's implicit runtime requantization on energy consumption and thermal management in large-scale LLM deployments?
- Basis in paper: [explicit] The paper mentions that Tender's runtime requantization is performed with minimal overhead, but does not provide detailed energy consumption analysis.
- Why unresolved: While the paper demonstrates performance benefits, the energy efficiency implications of the implicit requantization mechanism in real-world deployments are not explored.
- What evidence would resolve it: Detailed energy profiling of Tender-based accelerators in large-scale LLM deployments, comparing energy consumption and thermal characteristics to baseline accelerators.

### Open Question 3
- Question: How does Tender's performance and accuracy change when applied to LLM models with different architectural variations, such as sparse attention mechanisms or different layer normalization techniques?
- Basis in paper: [inferred] The paper evaluates Tender on standard Transformer-based LLMs, but does not investigate its performance on models with architectural variations.
- Why unresolved: The paper demonstrates Tender's effectiveness on a specific class of models, but its generalizability to other LLM architectures is untested.
- What evidence would resolve it: Evaluation of Tender on a diverse set of LLM architectures, including sparse attention models and models with alternative layer normalization techniques, comparing its accuracy and efficiency to other quantization methods.

## Limitations
- Lack of specific implementation details for runtime requantization logic in tensor compute units
- Unknown exact configuration parameters for Tender hardware components
- Evaluation methodology and experimental setup not fully described for comparison with other techniques

## Confidence
High: The core concept of decomposed quantization and runtime requantization using shifter logic is well-explained and theoretically sound.
Medium: Reported performance improvements and accuracy retention based on presented evaluation methodology, but lack of detailed implementation information reduces confidence.
Low: Specific hardware implementation details and configuration parameters not provided, making independent validation difficult.

## Next Checks
1. Implement a detailed cycle-level simulator for the Tender hardware, incorporating the runtime requantization logic and shifter units as described in the paper. Validate the simulator against reported performance metrics and compare with other state-of-the-art hardware designs.
2. Conduct an ablation study to evaluate the impact of different configuration parameters on the Tender hardware's performance and energy efficiency. This includes varying the number of tensor compute units, memory hierarchy, and shifter logic implementation.
3. Perform an extensive comparison of Tender with other low-precision quantization techniques, such as SmoothQuant, ANT, and OliVe, using the same LLM models and datasets. Ensure consistent evaluation methodology and experimental setup for fair comparison.