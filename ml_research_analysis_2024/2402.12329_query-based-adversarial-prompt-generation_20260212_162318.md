---
ver: rpa2
title: Query-Based Adversarial Prompt Generation
arxiv_id: '2402.12329'
source_url: https://arxiv.org/abs/2402.12329
tags:
- attack
- attacks
- adversarial
- arxiv
- success
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a query-based adversarial attack that forces
  language models to emit harmful strings with high probability, bypassing current
  transfer-only attacks. The method, Greedy Coordinate Query (GCQ), iteratively samples
  and evaluates token replacements using both a local proxy model and direct queries
  to the target model, maintaining a buffer of high-performing candidates.
---

# Query-Based Adversarial Prompt Generation

## Quick Facts
- arXiv ID: 2402.12329
- Source URL: https://arxiv.org/abs/2402.12329
- Authors: Jonathan Hayase; Ema Borevkovic; Nicholas Carlini; Florian Tramèr; Milad Nasr
- Reference count: 40
- Key outcome: Query-based attack causes GPT-3.5 to emit 86% of targeted harmful strings (100% with longer prompts) and evades safety classifiers nearly 100% of the time

## Executive Summary
This paper presents a query-based adversarial attack method called Greedy Coordinate Query (GCQ) that significantly outperforms previous transfer-based attacks at generating harmful strings from language models. The attack iteratively samples and evaluates token replacements using both a local proxy model and direct queries to the target model, maintaining a buffer of high-performing candidates. The method successfully causes GPT-3.5 to emit 86% of targeted harmful strings (with up to 100% success using longer prompts) and evades OpenAI's safety classifier nearly 100% of the time. The authors also demonstrate that query-only attacks without proxy models can still outperform prior methods, achieving 30% better success than the original GCG attack.

## Method Summary
The Greedy Coordinate Query (GCQ) algorithm maintains a buffer of the B best unexplored prompts and iteratively expands the best node by sampling candidate token replacements. At each iteration, it evaluates the top bq candidates using a proxy model's loss function, then queries the true loss on the target model for the best-performing candidates. The algorithm uses an initialization technique of repeating the target string truncated on the left. For content moderation evasion, it uses the sum of moderation scores as the loss function. The authors also develop a proxy-free variant that focuses search on the most promising position identified through initial sampling, achieving approximately 2× query efficiency compared to the original GCG attack.

## Key Results
- GCQ achieves 86% success rate in generating targeted harmful strings from GPT-3.5 (vs ~0% for transfer attacks)
- Success rate reaches 100% for longer prompts (200+ tokens) despite increased query costs
- Evades OpenAI's content moderation classifier in nearly 100% of cases
- Proxy-free variant outperforms original GCG attack by 30% with 2× query efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The query-based approach bypasses transferability limitations by directly optimizing prompts against the target model.
- Mechanism: Instead of relying on surrogate models, GCQ samples candidate token replacements and queries the actual target model to evaluate their effectiveness. This allows the attack to adapt to the specific behavior of the target model.
- Core assumption: Querying the target model provides more accurate gradient information than using a surrogate model.
- Evidence anchors:
  - [abstract] "We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples..."
  - [section 3.1] "Our main attack strategy is similar to GCG in that it makes greedy updates to an adversarial string. At each iteration of the algorithm, we perform an update based on the best adversarial string found so far..."
  - [corpus] Weak evidence - corpus focuses on defenses and related attacks rather than mechanism validation
- Break condition: If the target model rate-limits or blocks queries, or if query costs become prohibitive

### Mechanism 2
- Claim: Maintaining a buffer of high-performing candidates improves search efficiency.
- Mechanism: The algorithm maintains a buffer of the B best unexplored prompts. At each iteration, it expands the best node from the buffer by sampling neighbors and evaluating them against the target model.
- Core assumption: Exploring a diverse set of candidates in parallel is more efficient than a purely sequential search
- Evidence anchors:
  - [section 3.1] "Each 'node' corresponds to a given adversarial suffix. Our attack maintains a buffer of the B best unexplored nodes."
  - [section 3.2] "Because we maintain a buffer of the B best unexplored prompts seen so far, we know that any prompt with a loss greater than ℓ(bworst) will be discarded."
  - [corpus] No direct evidence - corpus neighbors discuss defenses but not search efficiency mechanisms
- Break condition: If the buffer becomes too large to manage efficiently, or if early pruning eliminates promising candidates

### Mechanism 3
- Claim: The proxy-free optimization variant reduces query costs by focusing search on a single position.
- Mechanism: Instead of trying token replacements in random positions, the algorithm identifies the position where a single replacement reduced loss the most, then focuses B' additional replacements on that position.
- Core assumption: Gradient information from surrogate models is weak, so focusing queries on the most promising position is more efficient
- Evidence anchors:
  - [section 3.3] "Instead of choosing this position at random as in AutoPrompt, we first try a single token replacement in each position, and then write down the position where this replacement reduced the loss the most."
  - [section 4.4] "Our optimized variant of GCG is approximately 2× more query-efficient than the original attack, when gradients are available."
  - [corpus] No evidence - corpus focuses on defenses rather than optimization variants
- Break condition: If the initial position selection is poor, or if multiple positions need simultaneous updates

## Foundational Learning

- Concept: Gradient-based optimization for discrete text spaces
  - Why needed here: The attack needs to navigate the discrete token space efficiently
  - Quick check question: How does the algorithm handle the discrete nature of text tokens when applying gradient-based updates?

- Concept: Transferability in adversarial examples
  - Why needed here: Understanding why transfer attacks fail helps explain the need for query-based approaches
  - Quick check question: Why do transfer attacks struggle with targeted harmful string generation?

- Concept: Logit-bias manipulation for probability inference
  - Why needed here: The attack needs to infer log-probabilities when direct access is unavailable
  - Quick check question: How does the logit-bias technique allow reconstruction of log-probabilities from top-k sampling?

## Architecture Onboarding

- Component map: Token replacement sampling -> Proxy loss evaluation -> Target model querying -> Buffer update -> Candidate selection
- Critical path: The algorithm maintains a buffer of B best unexplored prompts, expands the best node by sampling bp neighbors, evaluates bq best with proxy loss, then queries true loss on target model
- Design tradeoffs: Query efficiency vs. accuracy (using proxy vs. direct queries), buffer size vs. memory usage, prompt length vs. success rate
- Failure signatures: High API costs with low success rates, buffer stagnation, proxy model poor correlation with target
- First 3 experiments:
  1. Compare white-box vs. proxy-based attack success rates on open-source models
  2. Test buffer size optimization for search efficiency
  3. Evaluate logit-bias technique accuracy for log-probability inference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can query-based attacks be adapted to work under the stricter black-box setting where log-probabilities cannot be computed?
- Basis in paper: [explicit] The paper discusses the need for effective attacks that work when log-probabilities cannot be computed and mentions this as a promising direction for future work.
- Why unresolved: The paper does not provide a concrete method or solution for adapting query-based attacks to settings where log-probabilities are not available.
- What evidence would resolve it: Development and empirical validation of a new attack method that successfully generates adversarial examples without access to log-probabilities, showing comparable success rates to current methods.

### Open Question 2
- Question: What is the impact of OpenAI's API nondeterminism on the reliability and reproducibility of query-based adversarial attacks?
- Basis in paper: [explicit] The paper discusses the nondeterminism in GPT-3.5 Turbo and GPT-4, observing that log-probabilities of individual tokens are not stable over time.
- Why unresolved: While the paper acknowledges the issue, it does not explore the long-term implications or propose solutions to mitigate the effects of nondeterminism on attack reliability.
- What evidence would resolve it: A comprehensive study showing how nondeterminism affects attack success rates over time and the development of techniques to ensure reproducibility despite API changes.

### Open Question 3
- Question: Can the attack success rate be improved by developing more sophisticated initialization techniques beyond repeating the target string?
- Basis in paper: [inferred] The paper mentions that initializing with certain prompts can significantly increase attack success rates, but also notes that random initialization is much less effective.
- Why unresolved: The paper only explores one initialization method and does not investigate other potential strategies that might yield better results.
- What evidence would resolve it: Experimental comparison of various initialization techniques, demonstrating which methods lead to higher attack success rates and under what conditions.

## Limitations

- Query costs increase substantially with longer prompts, limiting practical applicability despite higher success rates
- Attack effectiveness against other model architectures and safety systems remains unproven beyond GPT-3.5
- Reliance on proxy models introduces potential failure modes if the proxy poorly represents target behavior

## Confidence

**High Confidence:** The core claim that query-based attacks outperform transfer-only attacks is well-supported by empirical results showing 86% success rate compared to near-zero for transfer methods.

**Medium Confidence:** The buffer-based search efficiency improvement claims are supported but could benefit from more comprehensive ablation studies across different buffer sizes and attack scenarios.

**Low Confidence:** The universal attack claims (section 4.3) are based on limited testing with only 20 training strings and 554 validation strings.

## Next Checks

1. **Cross-Model Generalization Test:** Validate the attack's effectiveness against a broader range of language models including open-source alternatives like Llama 2, Mistral, and Claude, measuring success rates and query efficiency across different architectures and safety implementations.

2. **Defense Evasion Robustness:** Test the attack against active defense mechanisms including query rate limiting, pattern detection, and adaptive safety classifiers to understand real-world feasibility and potential countermeasures.

3. **Cost-Benefit Analysis at Scale:** Conduct experiments with longer prompts (200+ tokens) to quantify the relationship between prompt length, success rate, and total query costs, determining practical limits for