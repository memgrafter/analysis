---
ver: rpa2
title: Long Context RAG Performance of Large Language Models
arxiv_id: '2411.03538'
source_url: https://arxiv.org/abs/2411.03538
tags:
- context
- answer
- tokens
- long
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks the RAG performance of 20 popular open source
  and commercial LLMs with varying context lengths up to 2 million tokens. Experiments
  were conducted on three domain-specific datasets (Databricks DocsQA, FinanceBench,
  Natural Questions) using a standard retrieval-augmented generation pipeline with
  FAISS and OpenAI text-embedding-3-large.
---

# Long Context RAG Performance of Large Language Models

## Quick Facts
- **arXiv ID:** 2411.03538
- **Source URL:** https://arxiv.org/abs/2411.03538
- **Reference count:** 40
- **Primary result:** Only a handful of state-of-the-art models maintain consistent RAG accuracy above 64k tokens

## Executive Summary
This study benchmarks the retrieval-augmented generation (RAG) performance of 20 popular open source and commercial large language models with varying context lengths up to 2 million tokens. Experiments were conducted on three domain-specific datasets using a standard retrieval-augmented generation pipeline with FAISS and OpenAI text-embedding-3-large. The key finding is that while longer contexts can improve RAG performance, most models show significant performance degradation beyond 16-32k tokens, with only a few state-of-the-art models maintaining accuracy at longer contexts.

## Method Summary
The researchers evaluated 20 open source and commercial LLMs across three domain-specific datasets (Databricks DocsQA, FinanceBench, Natural Questions) using a standard RAG pipeline. They employed FAISS for vector storage and retrieval, paired with OpenAI text-embedding-3-large for generating embeddings. Context lengths were tested from standard ranges up to 2 million tokens to assess performance degradation and failure modes at scale.

## Key Results
- Only a handful of state-of-the-art models (o1-mini/preview, GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro) maintain consistent RAG accuracy above 64k tokens
- The majority of models show performance degradation beyond 16-32k tokens
- Models exhibit distinct failure modes at long context lengths, including refusal due to perceived copyright concerns, safety filtering, and random/repeated content generation

## Why This Works (Mechanism)
None provided in the source material.

## Foundational Learning
- **Context window management**: Understanding how LLMs handle information within different context lengths is crucial for effective RAG implementation. Quick check: Test model performance at incremental context lengths to identify degradation thresholds.
- **Vector retrieval systems**: FAISS and similar systems enable efficient document retrieval for RAG pipelines. Quick check: Verify retrieval accuracy across different embedding models and index configurations.
- **RAG pipeline optimization**: The interplay between retrieval quality and generation performance determines overall system effectiveness. Quick check: Measure standalone retrieval performance versus end-to-end RAG accuracy.

## Architecture Onboarding

**Component map:** Documents -> Embedding Model -> Vector Store (FAISS) -> Retriever -> LLM (with context) -> Answer

**Critical path:** Retrieval accuracy and context window management are the critical factors determining RAG performance at long context lengths.

**Design tradeoffs:** The study reveals tradeoffs between context length capacity and sustained accuracy, suggesting that longer context windows don't automatically translate to better performance.

**Failure signatures:** Distinct failure modes observed include: refusal due to copyright concerns (Claude 3 Sonnet), safety filtering (Gemini 1.5 Pro), and random/repeated content generation (Mixtral-8x7B).

**First experiments:**
1. Test incremental context lengths (16k, 32k, 64k, 128k) to identify performance degradation thresholds
2. Compare retrieval-only performance against end-to-end RAG accuracy
3. Evaluate different embedding models while keeping the same retrieval and generation pipeline

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Evaluation focuses on domain-specific datasets which may not generalize to other domains or use cases
- Results may vary with different retrieval approaches or embedding models beyond the standard pipeline tested
- Observed failure modes could be influenced by prompt engineering choices and system-level configurations rather than inherent model limitations

## Confidence
- Long-context LLMs can potentially subsume traditional RAG workflows: High confidence
- Only a handful of state-of-the-art models maintain consistent accuracy above 64k tokens: Medium confidence
- Majority of models show performance degradation beyond 16-32k tokens: Medium confidence

## Next Checks
1. Evaluate the same models on additional domain-specific and general-purpose datasets to assess generalizability of the findings
2. Test alternative retrieval methods and embedding models to determine if the observed performance patterns are pipeline-dependent
3. Conduct ablation studies varying prompt engineering and system configurations to isolate model-specific versus configuration-specific failure modes