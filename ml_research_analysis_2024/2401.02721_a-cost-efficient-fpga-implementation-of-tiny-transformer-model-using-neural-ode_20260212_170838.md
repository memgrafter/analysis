---
ver: rpa2
title: A Cost-Efficient FPGA Implementation of Tiny Transformer Model using Neural
  ODE
arxiv_id: '2401.02721'
source_url: https://arxiv.org/abs/2401.02721
tags:
- proposed
- neural
- fpga
- accuracy
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of implementing Transformer-based
  models on resource-limited edge devices by proposing a lightweight hybrid model
  that combines Convolutional Neural Networks (CNNs) and Multi-Head Self-Attention
  (MHSA) using Neural Ordinary Differential Equations (Neural ODE). The core idea
  is to replace parts of ResNet with Neural ODE to reduce parameter size while maintaining
  accuracy.
---

# A Cost-Efficient FPGA Implementation of Tiny Transformer Model using Neural ODE

## Quick Facts
- arXiv ID: 2401.02721
- Source URL: https://arxiv.org/abs/2401.02721
- Authors: Ikumi Okubo; Keisuke Sugiura; Hiroki Matsutani
- Reference count: 40
- Achieves 79.68% top-1 accuracy on STL10 dataset with 95.1% parameter reduction

## Executive Summary
This paper addresses the challenge of implementing Transformer-based models on resource-limited edge devices by proposing a lightweight hybrid model that combines Convolutional Neural Networks (CNNs) and Multi-Head Self-Attention (MHSA) using Neural Ordinary Differential Equations (Neural ODE). The core innovation is replacing parts of ResNet with Neural ODE to reduce parameter size while maintaining accuracy, then quantizing the model using Quantization Aware Training (QAT) to minimize FPGA resource utilization. The FPGA implementation accelerates the backbone and MHSA parts by 34.01× and achieves an overall 9.85× speedup compared to an ARM Cortex-A53 CPU, with 7.10× better energy efficiency.

## Method Summary
The method combines Neural ODE with a CNN-Transformer hybrid architecture, replacing ResNet blocks with ODEBlocks to achieve C-fold parameter reduction through parameter reuse across iterations. The model is trained on the STL10 dataset using SGD with cosine annealing scheduler for 310 epochs, then quantized using QAT with learnable lookup tables (LLT) selectively applied to DSBlocks and MHSABlock. The quantized model is implemented on Xilinx ZCU104 FPGA using Vitis HLS 2023.1 and Vivado 2023.1, storing all weights on-chip in BRAM/URAM to eliminate memory transfer overhead. The design achieves 4-bit or 8-bit quantization for specific blocks while maintaining 79.68% accuracy.

## Key Results
- Achieves 79.68% top-1 accuracy on STL10 dataset
- Reduces parameter size by 95.1% compared to ResNet50
- FPGA implementation achieves 9.85× speedup over ARM Cortex-A53 CPU
- Energy efficiency improvement of 7.10× compared to CPU implementation
- Resource utilization: 17% BRAM, 27% DSP, 9% LUT on ZCU104

## Why This Works (Mechanism)

### Mechanism 1
Neural ODE reduces parameter size by reusing parameters across multiple iterations of the same building block. By reformulating ResNet forward propagation as a numerical solution of ODEs, a set of building blocks is replaced by a single ODEBlock that iteratively applies the same parameters, achieving C-fold parameter reduction. The continuous limit of ResNet blocks (as C → ∞) can be approximated by a single parameterized ODEBlock that maintains model accuracy.

### Mechanism 2
Quantization Aware Training (QAT) allows aggressive bit-width reduction while preserving accuracy better than Post Training Quantization (PTQ). QAT incorporates quantization during training by simulating quantization in forward passes, allowing the network to adapt its weights and activations to the quantized representation. The network can learn to compensate for quantization errors if quantization is part of the training loop rather than applied afterward.

### Mechanism 3
Storing all weights on-chip eliminates memory transfer overhead, making the FPGA implementation compute-bound and faster. By fitting all model parameters within BRAM and URAM, the accelerator avoids DRAM access latency, focusing FPGA resources on computation rather than data movement. The total parameter size after compression (Neural ODE + quantization) fits within the on-chip memory capacity of the target FPGA.

## Foundational Learning

- **Multi-Head Self-Attention (MHSA) mechanism and its computational complexity**
  - Why needed: MHSA is the core attention mechanism in the hybrid model; understanding its O(N²) complexity explains why it's computationally expensive and why hardware acceleration is beneficial.
  - Quick check: What is the computational complexity of standard self-attention with respect to input sequence length N, and why does this matter for hardware implementation?

- **Neural Ordinary Differential Equations (Neural ODEs) and their relation to ResNet**
  - Why needed: Neural ODE is the key technique for parameter reduction; knowing how it reformulates ResNet as ODEs is essential to understand the model compression.
  - Quick check: How does treating ResNet layers as discrete steps of an ODE lead to parameter reuse, and what is the role of the solver iterations?

- **Quantization Aware Training (QAT) vs Post Training Quantization (PTQ)**
  - Why needed: QAT is used to aggressively quantize the model while maintaining accuracy; understanding the difference from PTQ explains why QAT was chosen.
  - Quick check: Why does incorporating quantization during training (QAT) typically yield better accuracy than applying it after training (PTQ)?

## Architecture Onboarding

- **Component map**: ARM Cortex-A53 CPU (pre-processing) -> FPGA IP core (ODEBlock1, DSBlock1, ODEBlock2, DSBlock2, MHSABlock) -> ARM Cortex-A53 CPU (post-processing)
- **Critical path**: ODEBlock iterations → DSBlock downsampling → ODEBlock iterations → DSBlock downsampling → MHSABlock attention → Output to CPU
- **Design tradeoffs**: Parameter size vs. accuracy (more ODE iterations or less quantization improves accuracy but increases memory use), Bit-width vs. resource use (4-bit quantization saves more memory than 8-bit but risks accuracy loss), Parallelism vs. DSP use (higher parallelism speeds up computation but consumes more DSP slices)
- **Failure signatures**: Accuracy drop (likely from aggressive quantization or insufficient ODE iterations), Timing violation (overly aggressive parallelization or clock frequency too high), BRAM overflow (model size too large for on-chip memory after compression)
- **First 3 experiments**: 1) Run FPGA implementation with 8-bit quantization on DS+Block3 and measure accuracy vs. software baseline, 2) Vary number of ODE iterations (C) and measure impact on accuracy and execution time, 3) Test 4-bit vs. 8-bit quantization impact on both accuracy and FPGA resource utilization

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed lightweight Transformer model with Neural ODE compare to other state-of-the-art lightweight models (e.g., MobileViT, Mobile-Former) on larger datasets like ImageNet or COCO? The paper focuses on demonstrating feasibility on a modest-sized FPGA for edge computing and does not investigate scalability to larger datasets.

### Open Question 2
How does the proposed model perform in terms of robustness to adversarial attacks or domain shift compared to other models? The paper focuses on accuracy and resource efficiency but does not investigate robustness to real-world challenges like adversarial attacks or domain shift.

### Open Question 3
How does the proposed model perform in terms of energy efficiency and latency on other resource-constrained edge devices like microcontrollers or ASICs? The paper evaluates energy efficiency and latency on an FPGA but does not explore performance on other resource-constrained edge devices.

## Limitations

- Hardware verification lacks detailed breakdown of resource utilization scaling across different configurations
- Quantization specifics (exact K value and threshold ranges) are not fully specified, making exact reproduction difficult
- Solver parameters like integration step size and optimal iteration count are not thoroughly explored

## Confidence

**High Confidence**: The overall framework of combining Neural ODE with CNN-Transformer hybrid models is sound, and the reported STL10 accuracy of 79.68% is verifiable through standard evaluation procedures.

**Medium Confidence**: The claimed parameter reduction of 95.1% through Neural ODE compression appears technically feasible but depends heavily on implementation details not fully specified.

**Low Confidence**: The absolute resource utilization numbers (17% BRAM, 27% DSP) are difficult to verify without more detailed implementation specifications.

## Next Checks

1. Test the impact of different quantization granularities (K values) on accuracy for each block type (DSBlocks, MHSABlock) to determine the optimal tradeoff between compression and accuracy preservation.

2. Systematically vary the number of ODE solver iterations from 1 to 50 and measure the impact on both accuracy and execution time to identify the optimal iteration count that balances performance and precision.

3. Validate the on-chip memory requirements by calculating the exact memory footprint for 4-bit and 8-bit quantized versions of all model components, including intermediate activations, to confirm the claim that the design remains compute-bound rather than memory-bound.