---
ver: rpa2
title: STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive
  Progressions
arxiv_id: '2409.13843'
source_url: https://arxiv.org/abs/2409.13843
tags:
- bias
- sensitivity
- severity
- each
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces STOP, a dataset of 2,700 sentences across
  450 offensive progressions designed to evaluate bias in LLMs by tracking how bias
  escalates in narrative contexts. It covers 9 demographics and 46 sub-demographics
  with varying severity levels.
---

# STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions

## Quick Facts
- **arXiv ID:** 2409.13843
- **Source URL:** https://arxiv.org/abs/2409.13843
- **Reference count:** 17
- **Primary result:** Llama 3-70b shows strongest human alignment in sensitivity to offensive progressions; fine-tuning boosts bias benchmark answer rates by up to 191%

## Executive Summary
The STOP benchmark introduces a novel dataset of 2,700 sentences across 450 offensive progressions to evaluate bias in large language models. By tracking how bias escalates in narrative contexts across 9 demographics and 46 sub-demographics, STOP provides a fine-grained measure of model sensitivity. Evaluations reveal significant variation in model responses, with top models like Llama 2-70b reaching up to 69.8% sensitivity. Human alignment is strongest with Llama 3-70b, and fine-tuning on human judgments substantially improves model performance on bias benchmarks with minimal trade-offs.

## Method Summary
STOP is constructed as a dataset of offensive progressions, each consisting of sentences that incrementally increase in severity regarding bias against 9 demographic categories and 46 sub-demographics. Human annotators generate and rate sentences, enabling evaluation of LLM sensitivity to bias escalation. Models are tested on their ability to recognize and respond to these progressions. Fine-tuning is performed using human judgments to improve model alignment, and performance is measured by answer rates on bias benchmarks such as BBQ, StereoSet, and CrowS-Pairs.

## Key Results
- Llama 2-70b achieves up to 69.8% sensitivity to offensive progressions.
- Llama 3-70b shows the strongest human alignment in sensitivity evaluations.
- Fine-tuning Llama 3-70b on human judgments increases answer rates on bias benchmarks by up to 191%, with minimal performance loss.

## Why This Works (Mechanism)
The mechanism relies on tracking how bias escalates in narrative contexts. By presenting models with incremental offensive progressions, STOP captures nuanced changes in model sensitivity that traditional benchmarks may miss. Human-generated and rated progressions ensure realistic and varied scenarios, allowing for robust evaluation of bias recognition and response.

## Foundational Learning
- **Offensive progression:** A sequence of sentences with increasing severity of bias, used to evaluate model sensitivity. Needed to simulate realistic bias escalation. Quick check: Verify progression ratings are consistent across annotators.
- **Demographic categorization:** Grouping content by protected groups (e.g., race, gender) to assess bias. Needed for targeted bias detection. Quick check: Confirm demographic coverage is representative.
- **Human alignment:** Agreement between model responses and human judgments on bias sensitivity. Needed to validate model fairness. Quick check: Measure inter-annotator agreement rates.
- **Fine-tuning on judgments:** Training models using human-annotated data to improve bias recognition. Needed to adapt models to nuanced bias contexts. Quick check: Compare pre- and post-fine-tuning answer rates.
- **Answer rate metric:** The proportion of bias-related questions a model responds to, used as a performance indicator. Needed to quantify model engagement with bias content. Quick check: Ensure answer rates are not artificially inflated.
- **Bias benchmark:** Standardized datasets (e.g., BBQ, StereoSet) used to measure model bias. Needed for comparative evaluation. Quick check: Validate benchmark relevance to STOP scenarios.

## Architecture Onboarding

### Component Map
STOP Dataset -> LLM Evaluation -> Human Judgment Alignment -> Fine-tuning Pipeline -> Bias Benchmark Testing

### Critical Path
1. Generate offensive progressions with human annotators.
2. Evaluate LLM responses to progressions.
3. Align model outputs with human judgments.
4. Fine-tune models using aligned judgments.
5. Test fine-tuned models on bias benchmarks.

### Design Tradeoffs
- **Dataset size vs. annotation quality:** Larger datasets improve coverage but may reduce annotation consistency.
- **Answer rate vs. response quality:** Higher answer rates may come at the cost of nuanced, fair responses.
- **Model size vs. fine-tuning efficiency:** Larger models show better alignment but require more resources for fine-tuning.

### Failure Signatures
- Inconsistent progression ratings across annotators.
- Over-reliance on answer rates without assessing response fairness.
- Cultural or linguistic biases in the dataset limiting generalizability.

### First Experiments
1. Replicate sensitivity evaluation on a subset of offensive progressions.
2. Conduct inter-annotator reliability analysis for progression ratings.
3. Test fine-tuned models on a held-out set of progressions not seen during training.

## Open Questions the Paper Calls Out
None

## Limitations
- Inter-annotator agreement rates and annotator diversity are not reported, affecting reliability.
- Fine-tuning results focus on answer rates, not the quality or fairness of responses.
- Potential cultural or linguistic biases in the dataset may limit generalizability.

## Confidence
- **High:** LLMs vary in sensitivity to offensive progressions (consistently observed).
- **Medium:** STOP dataset and methodology are sound, but key validation details are missing.
- **Low:** Fine-tuning impact claims due to limited reporting on model behavior beyond answer rates.

## Next Checks
1. Conduct inter-annotator reliability analysis for the STOP dataset to quantify consistency in severity ratings.
2. Evaluate fine-tuned models not only by answer rates but also by the quality and fairness of responses to offensive progressions.
3. Test the STOP benchmark across diverse linguistic and cultural contexts to assess generalizability and uncover potential dataset biases.