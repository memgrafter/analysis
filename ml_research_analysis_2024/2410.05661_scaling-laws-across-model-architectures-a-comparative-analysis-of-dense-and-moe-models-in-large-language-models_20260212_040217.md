---
ver: rpa2
title: 'Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and
  MoE Models in Large Language Models'
arxiv_id: '2410.05661'
source_url: https://arxiv.org/abs/2410.05661
tags:
- training
- size
- scaling
- loss
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the transferability and discrepancies of
  scaling laws between Dense Models and Mixture of Experts (MoE) models. Through theoretical
  analysis and extensive experiments, it reveals that the power-law scaling framework
  applies to MoE Models, with both architectures demonstrating consistent scaling
  behaviors for loss, batch size, and learning rate.
---

# Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models

## Quick Facts
- arXiv ID: 2410.05661
- Source URL: https://arxiv.org/abs/2410.05661
- Authors: Siqi Wang; Zhengyu Chen; Bei Li; Keqing He; Min Zhang; Jingang Wang
- Reference count: 37
- Primary result: MoE models show approximately 16.37% better data efficiency than Dense models under the same compute budget

## Executive Summary
This paper investigates the transferability and discrepancies of scaling laws between Dense Models and Mixture of Experts (MoE) models. Through theoretical analysis and extensive experiments, it reveals that the power-law scaling framework applies to MoE Models, with both architectures demonstrating consistent scaling behaviors for loss, batch size, and learning rate. The study finds that MoE Models have lower gradient noise scales during training, enabling stable training with smaller batch sizes and larger learning rates.

The research demonstrates that MoE Models achieve comparable performance with fewer training tokens, showing approximately 16.37% better data efficiency than Dense Models under the same compute budget. The paper provides valuable insights into how different model architectures follow similar scaling principles while exhibiting unique characteristics in their training dynamics.

## Method Summary
The paper employs theoretical analysis combined with extensive experimental validation to investigate scaling laws across Dense and MoE architectures. The researchers conducted controlled experiments comparing model performance across different scales, analyzing loss scaling, batch size scaling, and learning rate scaling behaviors. They measured gradient noise scales during training to understand stability differences between architectures. The experiments were conducted on transformer-based models with varying sizes, up to 1.4B parameters, using standardized training protocols and evaluation metrics.

## Key Results
- MoE models exhibit power-law scaling behaviors consistent with Dense models across loss, batch size, and learning rate dimensions
- MoE models demonstrate approximately 16.37% better data efficiency than Dense models under identical compute budgets
- MoE models maintain lower gradient noise scales during training, enabling stable training with smaller batch sizes and larger learning rates

## Why This Works (Mechanism)
The consistent scaling behavior between Dense and MoE models stems from their shared fundamental architecture based on transformer principles. Both architectures optimize the same underlying loss functions and benefit from similar optimization dynamics, though MoE introduces additional routing mechanisms that affect gradient flow patterns. The improved data efficiency in MoE models arises from their ability to activate specialized expert parameters only when relevant to the input, effectively increasing the model's functional capacity without proportional increases in computational cost.

## Foundational Learning
- **Power-law scaling**: Describes how model performance scales with compute, data, and model size - needed to understand efficiency gains across architectures
- **Gradient noise scale**: Measures optimization stability during training - needed to explain training dynamics differences
- **Mixture of Experts routing**: Mechanism for activating specialized sub-networks - needed to understand MoE's efficiency advantage
- **Batch size scaling**: Relationship between batch size and training stability - needed to optimize training configurations
- **Learning rate scaling**: How optimal learning rates change with model and batch size - needed to maintain training stability

Quick check: Verify that scaling relationships follow power-law patterns with consistent exponents across architectures.

## Architecture Onboarding

Component map: Input -> Dense Layer or Expert Router -> Expert Modules (MoE only) -> Output Layer

Critical path: Data flow proceeds through embedding, attention layers, feed-forward networks, and output projection, with MoE adding a routing decision layer before expert selection.

Design tradeoffs: MoE trades computational efficiency during inference (when fewer experts are active) against increased model complexity and potential routing overhead. Dense models offer simpler optimization but require full computation for all parameters.

Failure signatures: MoE models may suffer from load imbalance among experts, while Dense models can face optimization instability at large scales.

3 first experiments:
1. Verify scaling law consistency by training models at 125M, 350M, and 1.4B parameters
2. Measure gradient noise scale across batch sizes to confirm stability differences
3. Compare data efficiency by training to equivalent performance with matched compute budgets

## Open Questions the Paper Calls Out
The paper acknowledges that the theoretical framework for understanding gradient noise scale differences between architectures is not fully developed, leaving open questions about when and why these differences occur. The study focuses on transformer-based architectures and does not address whether these scaling law relationships hold for other model families or emerging architectures.

## Limitations
- Analysis is limited to specific model sizes and configurations, not covering extremely large-scale models
- The 16.37% data efficiency claim is based on particular training setups and may not generalize across all MoE architectures
- Theoretical understanding of gradient noise scale differences between architectures remains incomplete

## Confidence
- Scaling law consistency between Dense and MoE models: High
- 16.37% data efficiency advantage: Medium (depends on specific training conditions)
- Gradient noise scale differences enabling stable training: Medium (theoretical understanding incomplete)
- Applicability to larger model scales: Low (limited by experimental scope)

## Next Checks
1. Test the scaling relationships on significantly larger model sizes (beyond 1.4B parameters) to verify if the power-law exponents remain consistent and the claimed efficiency advantages persist at scale.

2. Conduct ablation studies varying MoE architecture parameters (expert count, capacity factor, routing strategies) to isolate which design choices most strongly influence scaling behavior and gradient noise characteristics.

3. Validate the findings across multiple tasks and domains beyond the primary training setup to assess the generalizability of the scaling law relationships and efficiency claims across different use cases.