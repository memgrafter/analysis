---
ver: rpa2
title: Comparison of Reservoir Computing topologies using the Recurrent Kernel approach
arxiv_id: '2401.14557'
source_url: https://arxiv.org/abs/2401.14557
tags:
- reservoir
- computing
- recurrent
- kernel
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the recurrent kernel approach to analyze various
  reservoir computing (RC) topologies, including leaky RC, sparse RC, and deep RC.
  The authors define the recurrent kernel limit for each topology and conduct a convergence
  study using different activation functions and hyperparameters.
---

# Comparison of Reservoir Computing topologies using the Recurrent Kernel approach

## Quick Facts
- **arXiv ID**: 2401.14557
- **Source URL**: https://arxiv.org/abs/2401.14557
- **Reference count**: 23
- **Key outcome**: Sparse Reservoir Computing converges to the same recurrent kernel limit as vanilla RC, suggesting no expressiveness loss if sparsity level is above threshold.

## Executive Summary
This paper extends the recurrent kernel approach to analyze various Reservoir Computing (RC) topologies including leaky RC, sparse RC, and deep RC. The authors define appropriate recurrent kernel limits for each topology and conduct convergence studies using different activation functions and hyperparameters. Key findings include the equivalence of sparse and vanilla RC in the large reservoir limit, the optimal ordering of reservoir sizes in deep RC, and the characterization of leaky RC dynamics. The framework enables systematic comparison of RC architectures and reveals that structured reservoir computing can achieve computational savings without sacrificing expressiveness.

## Method Summary
The paper analyzes RC topologies by comparing their empirical Gram matrices with theoretical recurrent kernel limits. For each topology (vanilla, sparse, leaky, deep), the authors derive the appropriate RK limit and study convergence as reservoir size increases. They use three activation functions (erf, ReLU, sign) and systematically vary hyperparameters including sparsity levels, leak rates, and layer sizes. The primary metric is the Frobenius norm between Gram matrices, normalized by reservoir size. Numerical simulations validate theoretical predictions about convergence behavior across different RC variants.

## Key Results
- Sparse RC converges to the same recurrent kernel limit as vanilla RC, implying no expressiveness loss above a sparsity threshold
- Optimal sparsity level grows with reservoir size, enabling significant computational savings while maintaining convergence
- In deep RC, convergence is better achieved with reservoir layers of decreasing sizes, though the effect is small
- Leaky RC converges to a well-defined limit with modified update rule, but convergence region is smaller than non-leaky RC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse RC converges to the same recurrent kernel limit as vanilla RC, implying no expressiveness loss if sparsity level is above threshold
- Mechanism: When reservoir size N is large, expected scalar product between sparse reservoir states equals non-sparse limit due to concentration of measure; sparsity only increases variance of approximation but not mean
- Core assumption: Activation function outputs are not sparse (e.g., erf, ReLU, sign produce dense activations), so scalar product remains dominated by non-zero weights
- Evidence anchors: [abstract] "sparse RC is equivalent to the non-sparse case, as long as the sparsity rate"; [section] "Sparse RC converges to the same limit regardless of the level of sparsity"
- Break condition: If activation functions produce sparse outputs or sparsity level is below threshold for given reservoir size

### Mechanism 2
- Claim: In Deep RC, optimal performance is achieved when earlier reservoir layers are slightly larger than later ones, reducing noise propagation
- Mechanism: Larger initial layers produce more stable recurrent kernel approximations, so noise injected in early layers is averaged out before being fed to subsequent layers, improving convergence
- Core assumption: Noise from random weight initialization accumulates through layers and can be mitigated by allocating more neurons to earlier layers
- Evidence anchors: [abstract] "convergence is better achieved with successive reservoirs of decreasing sizes"; [section] "optimal shapes have first reservoir sizes that are larger than subsequent ones"
- Break condition: If later layers are larger, noise propagation degrades convergence significantly

### Mechanism 3
- Claim: Leaky RC converges to a well-defined recurrent kernel limit with modified update rule incorporating leak rate
- Mechanism: Leak rate introduces convex combination of previous state and new activation; this modifies iterative kernel formula but preserves iterability, leading to deterministic limit when N → ∞
- Core assumption: Activation function has zero mean under Gaussian input (satisfied by erf, sign, and zero-centered ReLU)
- Evidence anchors: [abstract] "define the appropriate Recurrent Kernels for each topology"; [section] "The Recurrent Kernel corresponding to Reservoir Computing with leak rate is defined by replacing the update equation"
- Break condition: If leak rate is too large/small or activation function has non-zero mean under Gaussian input

## Foundational Learning

- Concept: Recurrent Kernels and their iterative definition via scalar products over time
  - Why needed here: The paper builds its analysis framework by showing RC variants converge to specific Recurrent Kernel limits; understanding kernel recursion is essential to interpret results
  - Quick check question: Given two reservoir states x(t) and y(t), write expression for their scalar product at next time step in terms of current scalar product and new inputs

- Concept: Sparse random matrices and concentration of measure
  - Why needed here: Equivalence of sparse and dense RC relies on high-dimensional concentration; knowing how sparsity affects variance of scalar products is key to interpreting threshold behavior
  - Quick check question: If matrix has sparsity s, how does its spectral norm scale with s in large N limit?

- Concept: Deep network composition and error propagation
  - Why needed here: Deep RC convergence depends on how errors from each layer propagate; understanding this helps explain why earlier layers should be larger
  - Quick check question: If each layer introduces independent Gaussian noise with variance σ², what is total variance at output of 2-layer deep RC?

## Architecture Onboarding

- Component map: Input -> Reservoir weight matrix Wr (N×N) -> Activation function f -> Output; For deep RC: Input -> Layer 1 -> Layer 2 -> ... -> Output
- Critical path: 1) Initialize random weights (Gaussian or sparse Gaussian) 2) Iterate reservoir state update equations 3) Compute scalar products between reservoir states over time 4) Compare Gram matrix from RC with Gram matrix from corresponding RK 5) Analyze convergence behavior as N increases or hyperparameters vary
- Design tradeoffs: Sparsity vs. approximation accuracy (higher sparsity speeds computation but requires larger N); Leak rate vs. memory (smaller a slows dynamics and can improve stability but may reduce expressiveness); Layer size ordering in Deep RC (larger first layers improve convergence but increase computational cost)
- Failure signatures: Divergence for large σr/σi with ReLU activation (error accumulation); Poor convergence for sparsity below threshold (approximation error plateaus); Unstable dynamics for leaky RC with inappropriate leak rate
- First 3 experiments: 1) Reproduce convergence study in Fig. 2 for sparse RC with varying sparsity levels and reservoir sizes; verify threshold behavior 2) Test Deep RC with different layer size orderings (decreasing, equal, increasing) and measure Gram matrix error; confirm optimal ordering 3) Compare leaky RC convergence for erf vs. ReLU vs. sign activations across range of leak rates; observe which activation functions satisfy zero-mean condition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does optimal sparsity level scale with reservoir size for different activation functions and hyperparameters?
- Basis in paper: [explicit] Authors found optimal sparsity level grows with reservoir size but only investigated limited range of parameters and activation functions
- Why unresolved: Authors only studied optimal sparsity level for few activation functions and limited range of hyperparameters; relationship may depend on specific characteristics of activation function and hyperparameters
- What evidence would resolve it: Conducting more extensive numerical study varying activation functions and hyperparameters to determine how optimal sparsity level scales with reservoir size in different scenarios

### Open Question 2
- Question: How does convergence behavior of deep RC with decreasing reservoir sizes compare to other architectural choices?
- Basis in paper: [explicit] Authors found convergence is better with decreasing reservoir sizes but effect is small; suggest equal sizes should perform similarly in practice
- Why unresolved: Authors only studied convergence with decreasing reservoir sizes and fixed computational budget; unclear how other architectural choices would compare in terms of convergence and performance
- What evidence would resolve it: Conducting comprehensive numerical study comparing convergence behavior and performance of deep RC with different reservoir size configurations

### Open Question 3
- Question: How does choice of activation function impact convergence and performance of RC topologies?
- Basis in paper: [explicit] Authors studied convergence for three activation functions and found behavior varies depending on activation function and hyperparameters
- Why unresolved: Authors only investigated limited set of activation functions and did not explore relationship between activation functions and specific RC topologies in detail
- What evidence would resolve it: Conducting systematic study investigating impact of various activation functions on convergence and performance of different RC topologies

## Limitations

- Relies heavily on high-dimensional concentration of measure arguments which may not hold for small reservoir sizes or pathological activation functions
- Equivalence between sparse and dense RC assumes activation functions produce dense outputs, which may not hold for all activation choices
- Layer size ordering in Deep RC shows only small performance differences, making practical significance uncertain

## Confidence

- **High Confidence**: Sparse RC convergence to same recurrent kernel limit as vanilla RC (supported by multiple convergence studies and theoretical analysis)
- **Medium Confidence**: Optimal layer size ordering in Deep RC (evidence shows effect exists but is small, with wide region of near-optimal performance)
- **Medium Confidence**: Leaky RC convergence analysis (new framework defined but fewer empirical validations compared to other topologies)

## Next Checks

1. **Convergence threshold verification**: Systematically test sparse RC with varying sparsity levels and reservoir sizes to empirically identify threshold where sparse and dense RC become equivalent
2. **Activation function robustness**: Test leaky RC convergence across wider range of activation functions (including non-zero-mean activations) to validate zero-mean assumption
3. **Deep RC sensitivity analysis**: Quantify performance gap between optimal and suboptimal layer size orderings across different task complexities and noise levels