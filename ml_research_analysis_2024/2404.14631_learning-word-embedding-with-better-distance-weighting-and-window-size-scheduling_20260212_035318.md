---
ver: rpa2
title: Learning Word Embedding with Better Distance Weighting and Window Size Scheduling
arxiv_id: '2404.14631'
source_url: https://arxiv.org/abs/2404.14631
tags:
- word
- window
- words
- size
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the lack of distance awareness in Word2Vec''s
  Continuous Bag-of-Words (CBOW) and Continuous Skip-gram (Skip-gram) models. It proposes
  two methods to incorporate distance information: Learnable Formulated Weights (LFW)
  for CBOW and Epoch-based Dynamic Window Size (EDWS) for Skip-gram.'
---

# Learning Word Embedding with Better Distance Weighting and Window Size Scheduling

## Quick Facts
- arXiv ID: 2404.14631
- Source URL: https://arxiv.org/abs/2404.14631
- Reference count: 21
- Primary result: LFW improves CBOW accuracy by 15.3% and EDWS improves Skip-gram accuracy by 2.5% on analogical reasoning task

## Executive Summary
This paper addresses fundamental limitations in Word2Vec's CBOW and Skip-gram models by incorporating distance awareness into the training process. The authors propose two novel methods: Learnable Formulated Weights (LFW) for CBOW that replaces uniform averaging with distance-based weighting using learnable parameters, and Epoch-based Dynamic Window Size (EDWS) for Skip-gram that systematically increases window size over training epochs rather than using random selection. Experiments on the enwik9 dataset demonstrate significant improvements in analogical reasoning accuracy, with LFW achieving 15.3% gains for CBOW and EDWS showing 2.5% improvements for Skip-gram. The power-law decay formula with shared parameters on both sides of the center word emerged as the best-performing configuration for LFW.

## Method Summary
The paper introduces two complementary approaches to enhance Word2Vec's context modeling. For CBOW, Learnable Formulated Weights replaces the traditional uniform averaging of context word embeddings with a weighted sum where weights are determined by a distance-based formula containing learnable parameters. This allows closer context words to have greater influence on the prediction. For Skip-gram, Epoch-based Dynamic Window Size eliminates the randomness in window size selection by progressively expanding the window over training epochs, starting from small windows and gradually including more distant context words. Both methods maintain the core architecture of Word2Vec while addressing its distance-agnostic nature, with LFW focusing on asymmetric context weighting and EDWS on systematic context expansion.

## Key Results
- LFW improves CBOW model accuracy by 15.3% on analogical reasoning tasks
- EDWS improves Skip-gram model accuracy by 2.5% on analogical reasoning tasks
- Power-law decay formula with shared parameters on both sides of center word performs best
- Both methods show consistent improvements over baseline Word2Vec implementations

## Why This Works (Mechanism)
The improvements stem from better modeling of contextual relationships by incorporating distance information that Word2Vec traditionally ignores. LFW's learnable weights allow the model to automatically discover optimal weighting schemes for different distances, giving more influence to contextually relevant nearby words while still incorporating information from distant words. EDWS's systematic window expansion ensures comprehensive coverage of context at different granularities throughout training, rather than relying on random sampling that may miss important patterns. Together, these methods create a more nuanced representation of word relationships that better captures the hierarchical nature of linguistic context.

## Foundational Learning

Distance-weighted averaging: Why needed - captures the intuition that nearby words are more contextually relevant than distant ones. Quick check - verify weight decay patterns follow linguistic expectations.

Context window scheduling: Why needed - ensures systematic exposure to all relevant context sizes rather than random sampling. Quick check - confirm monotonic increase in effective context coverage over epochs.

Learnable parameterization: Why needed - allows automatic optimization of weighting schemes rather than hand-crafted heuristics. Quick check - validate parameter convergence and absence of overfitting.

## Architecture Onboarding

Component map: Input corpus -> Tokenizer -> Context extraction (CBOW/SG) -> Distance weighting (LFW/EDWS) -> Embedding update -> Output embeddings

Critical path: Context extraction → Weight calculation → Weighted sum/average → Prediction loss → Parameter update

Design tradeoffs: LFW adds learnable parameters with minimal computational overhead but requires careful initialization; EDWS increases training time but eliminates randomness in context selection

Failure signatures: LFW may overfit with too many parameters or poor initialization; EDWS may converge slowly if window expansion is too gradual

First experiments:
1. Ablation test comparing uniform vs weighted averaging in CBOW
2. Fixed vs dynamic window size comparison in Skip-gram
3. Different distance decay functions (exponential, linear, power-law) in LFW

## Open Questions the Paper Calls Out
None

## Limitations
- LFW's learnable parameters lack regularization, potentially leading to overfitting
- Experiments limited to enwik9 dataset without broader validation across domains
- No comparison with state-of-the-art neural language models or modern embedding techniques
- Computational complexity and training time implications not addressed

## Confidence
High confidence in mathematical formulation and implementation
Medium confidence in empirical improvements on enwik9 dataset
Low confidence in generalizability to other datasets and downstream tasks

## Next Checks
1. Test proposed methods on multiple diverse datasets including domain-specific corpora
2. Compare performance against modern embedding techniques like GloVe, fastText, and contextual embeddings
3. Conduct ablation studies to determine individual component contributions and hyperparameter sensitivity