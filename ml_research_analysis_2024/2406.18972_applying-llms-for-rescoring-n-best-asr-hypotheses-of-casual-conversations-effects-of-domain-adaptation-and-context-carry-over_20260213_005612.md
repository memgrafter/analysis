---
ver: rpa2
title: 'Applying LLMs for Rescoring N-best ASR Hypotheses of Casual Conversations:
  Effects of Domain Adaptation and Context Carry-over'
arxiv_id: '2406.18972'
source_url: https://arxiv.org/abs/2406.18972
tags:
- llama2
- proc
- context
- rescoring
- best
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the application of large language models
  (LLMs) for rescoring automatic speech recognition (ASR) hypotheses of highly casual
  conversations. The study focuses on the CHiME-7 distant ASR (DASR) task, which provides
  datasets of challenging casual conversations between multiple participants.
---

# Applying LLMs for Rescoring N-best ASR Hypotheses of Casual Conversations: Effects of Domain Adaptation and Context Carry-over

## Quick Facts
- arXiv ID: 2406.18972
- Source URL: https://arxiv.org/abs/2406.18972
- Reference count: 0
- Even without domain adaptation, Llama2 outperforms a standard-size domain-adapted Transformer-LM, especially when using a long context

## Executive Summary
This paper investigates the application of large language models (LLMs) for rescoring automatic speech recognition (ASR) hypotheses of highly casual conversations. The study focuses on the CHiME-7 distant ASR (DASR) task, which provides datasets of challenging casual conversations between multiple participants. The authors use Llama2, a representative LLM, and investigate the effects of domain adaptation and context carry-over when performing N-best rescoring. They find that even without domain adaptation, Llama2 outperforms a standard-size domain-adapted Transformer-LM, especially when using a long context. Domain adaptation shortens the context length needed with Llama2 to achieve its best performance, significantly reducing the computational cost.

## Method Summary
The authors employ Llama2-7B for N-best rescoring of ASR hypotheses, using context carry-over from previous utterances. They adapt Llama2 to the casual conversation domain using QLoRA with LoRA adapters, and compare performance against a downsized domain-adapted Transformer-LM (Slama2-70M). The system processes 32-best hypotheses from a Conformer-S4 E2E ASR model, rescoring them with varying context lengths (0-1024 tokens) while considering conversational ordering and period addition. The rescoring combines language model scores with hypothesis length penalties using weighted scoring.

## Key Results
- Non-adapted Llama2-7B outperforms domain-adapted Slama2-70M Transformer-LM in WER reduction
- Context length of 256 tokens achieves near-optimal performance for Llama2-7B
- Domain adaptation reduces required context length from 256 to 64 tokens while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM rescoring outperforms domain-adapted Transformer-LM even without domain adaptation when using long context
- Mechanism: LLMs capture conversational flow and linguistic coherence across longer token spans, compensating for lack of domain-specific training data
- Core assumption: Casual conversation patterns are sufficiently covered by general LLM pretraining to benefit from context carry-over
- Evidence anchors:
  - [abstract]: "even without domain adaptation, Llama2 outperforms a standard-size domain-adapted Transformer-LM, especially when using a long context"
  - [section]: "However, considering the past hypotheses sequence as the context is effective for rescoring the current hypotheses, especially for the conversational speech case"
  - [corpus]: Weak evidence - corpus neighbors focus on generative rescoring but not specifically on context carry-over effectiveness
- Break condition: If conversational domain contains highly specialized vocabulary or syntax patterns not present in general LLM pretraining data

### Mechanism 2
- Claim: Domain adaptation shortens the context length needed for optimal LLM performance
- Mechanism: Fine-tuning adapters to casual conversation domain makes shorter context spans sufficient for capturing relevant patterns
- Core assumption: Domain adaptation effectively transfers casual conversation patterns into the LLM parameters
- Evidence anchors:
  - [abstract]: "Domain adaptation shortens the context length needed with Llama2 to achieve its best performance, i.e., it reduces the computational cost of Llama2"
  - [section]: "We employ QLoRA [24] to adapt Llama2 to the target casual conversational domain with its memory efficient way"
  - [corpus]: No direct evidence - corpus neighbors focus on different adaptation approaches
- Break condition: If domain adaptation fails to capture casual conversation patterns or if longer context provides additional non-domain-specific benefits

### Mechanism 3
- Claim: Context carry-over improves rescoring by leveraging conversational history
- Mechanism: Past hypotheses provide contextual cues that disambiguate current utterance recognition
- Core assumption: Conversational speech has dependencies across utterance boundaries that can be captured in context
- Evidence anchors:
  - [abstract]: "We investigate the effects of domain adaptation of the LLM and context carry-over when performing N-best rescoring"
  - [section]: "Considering past and future contexts is useful for rescoring current ASR hypotheses, and some previous studies perform context carry-over"
  - [corpus]: Weak evidence - corpus neighbors mention context-aware decoding but not specifically for N-best rescoring
- Break condition: If conversation has minimal inter-utterance dependencies or if context introduces noise from previous topics

## Foundational Learning

- Concept: Large Language Models and their pretraining methodology
  - Why needed here: Understanding LLM capabilities and limitations is crucial for evaluating their effectiveness in ASR rescoring
  - Quick check question: What are the key differences between decoder-based LLMs like Llama2 and encoder-based models like BERT in terms of context handling?

- Concept: Automatic Speech Recognition and N-best hypothesis generation
  - Why needed here: The study builds on ASR output, so understanding how N-best lists are generated and their quality is essential
  - Quick check question: How does the quality of N-best hypotheses typically degrade as you move down the ranking list?

- Concept: Domain adaptation techniques for language models
  - Why needed here: The study investigates QLoRA-based domain adaptation, so understanding how parameter-efficient fine-tuning works is important
  - Quick check question: What are the trade-offs between full fine-tuning and parameter-efficient methods like LoRA for domain adaptation?

## Architecture Onboarding

- Component map:
  - ASR system (Conformer-S4 encoder-decoder) → N-best hypothesis generation → LLM-based rescoring with context carry-over → Final 1-best output
  - Domain adaptation pipeline: Llama2 → QLoRA fine-tuning with LoRA adapters → Adapted model for rescoring
  - Context management: Token sequence processing with conversational ordering and period addition

- Critical path:
  1. ASR decoding produces 32-best hypotheses
  2. Text preprocessing (conversational ordering, period addition)
  3. LLM rescoring with context carry-over (variable context length)
  4. Score combination (α·log P_lm + γ·length)
  5. Selection of highest scoring hypothesis

- Design tradeoffs:
  - Context length vs computational cost: Longer contexts improve performance but increase inference time linearly
  - Domain adaptation vs general knowledge: Adaptation reduces context needs but may lose some general linguistic patterns
  - Conversational ordering vs speaker-conditioned ordering: Affects context relevance and flow capture

- Failure signatures:
  - Performance degradation with very short context lengths
  - Minimal improvement from domain adaptation despite sufficient data
  - Unexpected performance drops when increasing context beyond certain thresholds
  - Inconsistent results across different conversation types

- First 3 experiments:
  1. Baseline comparison: Run ASR without rescoring vs with Llama2 rescoring (L=0) to establish performance gain
  2. Context length sweep: Test rescoring with L=0, 64, 256, 1024 tokens to find optimal tradeoff point
  3. Domain adaptation comparison: Compare domain-adapted vs non-adapted Llama2 at optimal context length from experiment 2

## Open Questions the Paper Calls Out
1. How do larger LLM variants (e.g., Llama2-13B, Llama2-70B) perform in N-best rescoring for casual conversations compared to Llama2-7B?
2. Does domain adaptation of LLMs for casual conversations require task-specific fine-tuning strategies beyond QLoRA?
3. How effective is backward language model integration in N-best rescoring for casual conversations?

## Limitations
- Context length sensitivity may vary across conversation types and domains
- Domain adaptation effectiveness limited by the quality and diversity of adaptation data
- Computational cost trade-offs depend on specific hardware and implementation constraints

## Confidence

**High Confidence** (4+ independent evidence anchors):
- LLM rescoring outperforms domain-adapted Transformer-LM with long context
- Domain adaptation reduces optimal context length requirements
- Context carry-over improves rescoring for conversational speech
- QLoRA provides effective parameter-efficient domain adaptation

**Medium Confidence** (2-3 evidence anchors):
- 256-token context length is near-optimal for Llama2-7B
- Non-adapted Llama2-7B outperforms downsized domain-adapted Slama2-70M
- Computational cost reduction is proportional to context length reduction

**Low Confidence** (0-1 evidence anchors):
- Adaptation effectiveness across diverse casual conversation domains
- Generalization to other LLM architectures beyond Llama2
- Long-term conversational dependencies captured beyond immediate context

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate the adapted Llama2-7B model on casual conversation datasets from different domains (e.g., social media transcripts, informal interviews) to assess how well the domain adaptation generalizes beyond the CHiME-6 + Mixer 6 training data.

2. **Context Length Robustness Analysis**: Systematically test the model's performance across varying context lengths (32, 64, 128, 256, 512, 1024 tokens) on multiple conversation types to identify whether the 256-token optimum is consistent or varies significantly with conversation characteristics.

3. **Alternative LLM Architecture Comparison**: Implement the same N-best rescoring pipeline using different LLM architectures (GPT-2 variants, OPT models, or domain-specific LLMs) to determine whether the observed performance advantages are specific to Llama2 or generalizable across transformer-based LLMs.