---
ver: rpa2
title: Efficient Long Video Tokenization via Coordinate-based Patch Reconstruction
arxiv_id: '2411.14762'
source_url: https://arxiv.org/abs/2411.14762
tags:
- video
- videos
- coordtok
- frame
- long
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoordTok, a scalable video tokenizer that
  learns to map coordinate-based representations to corresponding video patches. By
  encoding videos into factorized triplane representations and reconstructing randomly
  sampled (x, y, t) coordinates, CoordTok enables efficient training on long videos
  without excessive memory or computational requirements.
---

# Efficient Long Video Tokenization via Coordinate-based Patch Reconstruction

## Quick Facts
- arXiv ID: 2411.14762
- Source URL: https://arxiv.org/abs/2411.14762
- Authors: Huiwon Jang; Sihyun Yu; Jinwoo Shin; Pieter Abbeel; Younggyo Seo
- Reference count: 40
- Primary result: CoordTok encodes 128-frame, 128×128 videos into 1280 tokens with better reconstruction than baselines using 6144-8192 tokens

## Executive Summary
This paper introduces CoordTok, a scalable video tokenizer that learns to map coordinate-based representations to corresponding video patches. By encoding videos into factorized triplane representations and reconstructing randomly sampled (x, y, t) coordinates, CoordTok enables efficient training on long videos without excessive memory or computational requirements. The approach significantly reduces token count while maintaining or improving reconstruction quality compared to traditional patch-based methods.

The memory efficiency gains from CoordTok enable training diffusion transformers on longer video sequences. On the UCF-101 dataset, CoordTok demonstrates that 128-frame videos can be encoded into just 1280 tokens while achieving superior Fréchet Video Distance (FVD) compared to baselines requiring 6144-8192 tokens. This breakthrough addresses the computational bottleneck of video tokenization that has limited the development of video generation models.

## Method Summary
CoordTok employs a coordinate-based sampling approach to learn triplane representations for video tokenization. The method samples random coordinates (x, y, t) from video frames and reconstructs the corresponding patches through a learned mapping. This approach factorizes the video into three planes (xy, xt, yt) that capture spatial and temporal information separately. The tokenizer is trained to reconstruct video patches from these coordinate samples, enabling efficient encoding of long videos with significantly fewer tokens than traditional patch-based methods.

The learned triplane representation allows for flexible sampling and reconstruction, making it possible to encode and decode videos without processing all patches simultaneously. This coordinate-based approach reduces memory requirements and computational complexity while maintaining reconstruction quality. The method is particularly effective for long video sequences where traditional patch-based tokenization becomes prohibitively expensive.

## Key Results
- Encodes 128-frame, 128×128 resolution videos into 1280 tokens
- Achieves significantly better reconstruction quality than baselines using 6144-8192 tokens
- Enables memory-efficient training of diffusion transformers for 128-frame video generation
- Outperforms previous video generation models in terms of Fréchet Video Distance (FVD)

## Why This Works (Mechanism)
CoordTok leverages the inherent structure of video data by factorizing it into coordinate-based triplane representations. Instead of processing all video patches simultaneously, it learns to reconstruct patches from randomly sampled coordinates, which reduces computational complexity while preserving essential information. The factorized representation captures spatial and temporal relationships more efficiently than traditional patch-based methods, allowing for better compression without significant quality loss.

The coordinate-based sampling approach provides several advantages: it enables flexible resolution scaling, reduces memory requirements by avoiding full patch processing, and allows the model to focus on important regions of the video. By learning to map coordinates to patches, CoordTok can reconstruct high-quality videos from a fraction of the tokens required by conventional methods, making long video processing computationally feasible.

## Foundational Learning

1. **Triplane Representations** - Factorized 3D representations that decompose video data into three 2D planes (xy, xt, yt) to capture spatial-temporal relationships efficiently. Needed for reducing dimensionality while preserving video structure. Quick check: Verify that the three planes capture complementary information without redundancy.

2. **Coordinate-based Sampling** - Technique of randomly sampling (x, y, t) coordinates from video frames for reconstruction rather than processing all patches. Needed to reduce computational load while maintaining reconstruction quality. Quick check: Ensure sampling strategy covers the full video space uniformly.

3. **Diffusion Transformers for Video** - Sequence-to-sequence models that generate videos frame-by-frame through iterative denoising. Needed for leveraging the efficient tokenization in generative tasks. Quick check: Confirm that the transformer can handle the reduced token sequence effectively.

## Architecture Onboarding

**Component Map:** Video -> CoordTok Encoder -> Triplane Representation -> Coordinate Sampler -> Patch Reconstructor -> Video

**Critical Path:** The core processing pipeline involves video input → triplane factorization → coordinate sampling → patch reconstruction. The encoder learns to map videos to triplane representations, while the reconstructor learns to generate patches from sampled coordinates. The diffusion transformer then uses these efficient tokens for video generation.

**Design Tradeoffs:** The primary tradeoff is between token efficiency and reconstruction quality. CoordTok sacrifices some fine-grained detail by using coordinate-based sampling instead of full patch processing, but gains significant memory efficiency. The method also trades off some temporal continuity guarantees for computational scalability, as coordinate sampling may not preserve strict frame ordering.

**Failure Signatures:** Potential failure modes include incomplete coverage of video space during coordinate sampling, leading to missing information in reconstructed videos. Temporal artifacts may occur if coordinate sampling disrupts frame continuity. The method may also struggle with rapid motion or complex scenes where local patch relationships are crucial for reconstruction.

**3 First Experiments:**
1. Train CoordTok on UCF-101 with varying coordinate sampling densities to measure reconstruction quality trade-offs
2. Compare reconstruction quality of 1280-token CoordTok encoding against 6144-token patch-based baseline on fixed video sequences
3. Evaluate memory usage and training speed differences between CoordTok and traditional tokenization methods on 128-frame videos

## Open Questions the Paper Calls Out

None specified in the provided materials.

## Limitations

- The 128-frame encoding at 128×128 resolution may not generalize to higher resolutions or longer sequences
- The paper lacks detailed ablation studies comparing different coordinate sampling strategies
- Memory efficiency gains are promising but not independently verified across different hardware configurations
- Potential artifacts or temporal consistency issues from coordinate-based reconstruction are not fully addressed

## Confidence

- High confidence: The core methodology of using coordinate-based sampling for triplane reconstruction is sound and well-explained
- Medium confidence: The memory efficiency improvements and token reduction claims, pending independent verification
- Medium confidence: The FVD improvements over previous video generation models, though specific baseline comparisons could be more comprehensive

## Next Checks

1. Conduct ablation studies varying the number of coordinates sampled per triplane and measuring reconstruction quality trade-offs
2. Test the approach on longer videos (>128 frames) and higher resolutions to verify scalability claims
3. Perform user studies or additional metrics (beyond FVD) to evaluate temporal consistency and visual quality of generated videos