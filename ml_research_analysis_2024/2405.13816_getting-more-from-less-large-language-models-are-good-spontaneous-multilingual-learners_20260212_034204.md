---
ver: rpa2
title: 'Getting More from Less: Large Language Models are Good Spontaneous Multilingual
  Learners'
arxiv_id: '2405.13816'
source_url: https://arxiv.org/abs/2405.13816
tags:
- language
- llms
- languages
- multilingual
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) exhibit strong multilingual capabilities,
  but performance is unbalanced across languages, especially for low-resource ones.
  While existing approaches rely on instruction-tuning with annotated answers in multiple
  languages, this work investigates whether LLMs can spontaneously improve multilingual
  alignment through question translation data alone.
---

# Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners

## Quick Facts
- arXiv ID: 2405.13816
- Source URL: https://arxiv.org/abs/2405.13816
- Reference count: 37
- Large language models can improve multilingual alignment using only question translation data without annotated answers

## Executive Summary
Large language models demonstrate strong multilingual capabilities but show performance imbalance across languages, particularly for low-resource ones. This work investigates whether LLMs can improve multilingual alignment through question translation data alone, without requiring annotated answers in target languages. The method involves instruction-tuning models on parallel question translation data in a few languages and evaluating performance across many languages. Experiments show significant multilingual alignment improvements even for languages unseen during training, across different model types, parameter sizes, and task types.

## Method Summary
The approach involves instruction-tuning large language models using parallel question translation data (without answers) in a few languages. Models are then evaluated on multiple tasks including emotion classification, natural language inference, and paraphrase identification across many languages. The training data consists of translated questions in different languages, allowing the model to learn cross-lingual alignment patterns. Performance is measured on both seen and unseen languages to assess generalization. Mechanistic interpretability analysis using logit lens and PCA is applied to understand the alignment patterns.

## Key Results
- Models achieve significant multilingual alignment improvements across different languages
- Improvements occur even for languages not seen during training
- Results hold across different model types, parameter sizes, and three task types (emotion classification, NLI, paraphrase identification)

## Why This Works (Mechanism)
The mechanism relies on the model's ability to learn cross-lingual semantic alignments from parallel question structures, even without answer supervision. When exposed to translated questions across languages, the model identifies shared semantic patterns and alignments between language representations. This spontaneous learning occurs because question structures capture essential semantic content that transfers across languages. The logit lens and PCA analysis reveals latent alignment patterns in the model's internal representations, showing that the model develops shared semantic spaces across languages despite the absence of explicit alignment supervision.

## Foundational Learning
- Cross-lingual transfer learning: why needed - enables models to apply knowledge from high-resource to low-resource languages; quick check - evaluate performance on unseen languages
- Multilingual representation alignment: why needed - allows models to map concepts across languages; quick check - compare embedding similarities across languages
- Question structure semantics: why needed - questions capture core semantic content independent of language; quick check - test alignment improvements with different question types
- Zero-shot generalization: why needed - assesses model's ability to perform on languages without direct training; quick check - measure performance on completely unseen languages
- Mechanistic interpretability (logit lens, PCA): why needed - provides insights into how models achieve alignment; quick check - visualize representation spaces before and after training

## Architecture Onboarding
**Component Map**: Question Translation Data -> Instruction Tuning -> Multilingual Evaluation -> Mechanistic Analysis
**Critical Path**: Translation data preparation → Model instruction tuning → Cross-lingual evaluation → Interpretability analysis
**Design Tradeoffs**: Minimal supervision (translation only) vs comprehensive supervision (full annotations); efficiency vs coverage
**Failure Signatures**: Poor alignment when translation quality is low; limited generalization to unseen language families; task-specific alignment issues
**3 First Experiments**: 1) Test different numbers of translation pairs to find minimal effective set; 2) Evaluate alignment persistence under domain shift; 3) Compare performance across language families (e.g., Indo-European vs non-Indo-European)

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to three task types, limiting generalizability to other NLP tasks
- Performance gains show considerable variance across languages and tasks
- Does not investigate impact of translation quality or coverage, particularly for low-resource languages

## Confidence
- Core finding that LLMs can leverage question translation data for multilingual alignment: **High**
- Claim that this approach works "spontaneously" without any supervision: **Medium**
- Mechanistic interpretability conclusions: **Low**

## Next Checks
1. Evaluate the approach on a broader range of NLP tasks, including generation and retrieval, to assess generalizability
2. Test whether alignment improvements transfer to out-of-distribution data or domains not seen during training
3. Investigate the impact of translation quality and coverage on alignment performance, particularly for low-resource languages