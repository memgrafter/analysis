---
ver: rpa2
title: 'Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language
  Models'
arxiv_id: '2409.11136'
source_url: https://arxiv.org/abs/2409.11136
tags:
- query
- instruction
- relevant
- promptriever
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Promptriever, the first retrieval model capable
  of following natural language instructions like language models. The authors curate
  a large synthetic instruction dataset from MS MARCO, including both instruction-following
  examples and instruction-negatives.
---

# Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models

## Quick Facts
- arXiv ID: 2409.11136
- Source URL: https://arxiv.org/abs/2409.11136
- Reference count: 20
- Primary result: First instruction-following dense retriever achieving +14.3 p-MRR on FollowIR and +12.9 Robustness@10 on InstructIR

## Executive Summary
Promptriever introduces the first retrieval model capable of following natural language instructions like language models. The authors curate a large synthetic instruction dataset from MS MARCO, including both instruction-following examples and instruction-negatives. Experiments show Promptriever achieves state-of-the-art performance on instruction-following retrieval tasks (+14.3 p-MRR on FollowIR), improved robustness to query phrasing (+12.9 on InstructIR's Robustness@10), and reliable performance gains through prompting (+1.4 average improvement on BEIR). The work demonstrates that dense retrievers can be made instruction-following through appropriate training data, aligning retrieval models more closely with language model capabilities.

## Method Summary
Promptriever is a bi-encoder dense retriever trained on MS MARCO data augmented with a synthetic instruction dataset. The synthetic dataset is generated by prompting Llama-3-70B-Instruct to create per-query instructions and using GPT-4o to generate instruction-negative passages from MS MARCO data. The model uses a Llama-2-7B backbone and is trained for one epoch with a learning rate of 1e-4. Instruction negatives are mined using a FollowIR-7B filter model. The model is compared against RepLLaMA baseline and evaluated on FollowIR, InstructIR, and BEIR benchmarks.

## Key Results
- Achieves +14.3 p-MRR improvement over RepLLaMA on FollowIR instruction-following benchmark
- Demonstrates +12.9 Robustness@10 improvement on InstructIR's query phrasing robustness test
- Shows +1.4 average nDCG@10 improvement across BEIR datasets through instruction-aware prompting
- Proves instruction-following capability transfers across different instruction types and domains

## Why This Works (Mechanism)
Promptriever works by training dense retrievers to understand and follow natural language instructions through exposure to synthetic instruction data. The key mechanism is the inclusion of instruction-negative passages that are query-relevant but violate the instruction, forcing the model to learn instruction-conditioned relevance rather than just query-document matching. This instruction-awareness allows the model to adapt retrieval behavior based on per-query instructions, similar to how language models follow prompts. The synthetic instruction dataset provides diverse instruction types and scenarios that wouldn't naturally occur in standard retrieval datasets.

## Foundational Learning
- **Bi-encoder architecture**: Two separate encoders for query and document embeddings that can be pre-computed for efficient retrieval
  - Why needed: Enables scalable retrieval at inference time by avoiding expensive query-document re-encoding
  - Quick check: Model should produce fixed-size embeddings that can be compared via dot product

- **Instruction-conditioned relevance**: The model learns to weight document relevance based on instruction interpretation rather than just query matching
  - Why needed: Allows retrieval to adapt to different user intents and information needs beyond keyword matching
  - Quick check: Same query with different instructions should retrieve different documents

- **Synthetic instruction data generation**: LLM-generated instructions and instruction-negative passages create training diversity
  - Why needed: Real retrieval datasets lack instruction-following scenarios, requiring synthetic augmentation
  - Quick check: Generated instructions should cover diverse types (e.g., "find...," "give me...," "show me...")

- **Instruction-negative mining**: Identifying passages that satisfy the query but violate the instruction
  - Why needed: Forces the model to learn instruction-specific relevance rather than ignoring instructions
  - Quick check: Instruction negatives should score high on query relevance but low on instruction compliance

## Architecture Onboarding

**Component map**: User query -> Instruction parser -> Promptriever bi-encoder -> Document embeddings -> Top-k retrieval

**Critical path**: Query + Instruction → Promptriever encoders → Similarity scoring → Ranked document list

**Design tradeoffs**: 
- Uses bi-encoder for scalability vs. cross-encoder for accuracy
- Relies on synthetic data vs. limited real instruction-following data
- Instruction negatives increase training complexity but improve instruction compliance

**Failure signatures**:
- Model ignores instructions if not trained with instruction negatives
- Poor generalization if synthetic instruction distribution doesn't match real queries
- Performance degradation if instruction-negative mining is ineffective

**Three first experiments**:
1. Evaluate Promptriever on standard MS MARCO benchmarks to confirm baseline retrieval capability
2. Test same query with different instructions to verify instruction-following behavior
3. Remove instruction-negative training data to measure its contribution to instruction compliance

## Open Questions the Paper Calls Out
- Can instruction-following dense retrievers effectively utilize in-context learning (ICL) with few-shot examples rather than just imperative instructions?
- What specific components of the query-document connection are critical for understanding why prompts improve retrieval performance?
- How does the effectiveness of instruction negatives vary with different generation methods, and what is the optimal balance between instruction positives and negatives in training?

## Limitations
- Synthetic instruction dataset generation relies heavily on LLM-generated content, raising potential biases and generalization concerns
- FollowIR-7B filter model for instruction-negative mining is used without detailed transparency into its reliability
- Limited baseline comparisons, primarily against RepLLaMA with minimal ablation studies

## Confidence
- High confidence in core instruction-following capability (FollowIR results are robust and well-validated)
- Medium confidence in BEIR and InstructIR generalization claims
- Medium confidence in instruction-negative mining effectiveness due to black-box filter

## Next Checks
1. Conduct human evaluation of instruction-negative passages to verify they are truly query-positive but instruction-negative
2. Perform ablation studies removing instruction-negative training data to quantify its contribution
3. Test Promptriever on diverse real user queries with varying instruction types to assess practical generalization