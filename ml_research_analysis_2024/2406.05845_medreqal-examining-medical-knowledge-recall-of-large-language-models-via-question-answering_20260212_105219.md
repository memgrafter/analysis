---
ver: rpa2
title: 'MedREQAL: Examining Medical Knowledge Recall of Large Language Models via
  Question Answering'
arxiv_id: '2406.05845'
source_url: https://arxiv.org/abs/2406.05845
tags:
- question
- medical
- knowledge
- dataset
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of evaluating large language
  models' ability to recall medical knowledge by constructing a novel dataset from
  systematic reviews. The MedREQAL dataset consists of 2,786 question-answer pairs
  derived from Cochrane systematic reviews, with questions generated from review objectives
  and answers taken from authors' conclusions.
---

# MedREQAL: Examining Medical Knowledge Recall of Large Language Models via Question Answering

## Quick Facts
- arXiv ID: 2406.05845
- Source URL: https://arxiv.org/abs/2406.05845
- Authors: Juraj Vladika; Phillip Schneider; Florian Matthes
- Reference count: 14
- Key outcome: Mixtral achieved 62% accuracy and 34.8% F1 score on medical knowledge recall from systematic reviews

## Executive Summary
This study constructs a novel dataset (MedREQAL) from Cochrane systematic reviews to evaluate large language models' ability to recall medical knowledge. The dataset contains 2,786 question-answer pairs derived from review objectives and authors' conclusions, with three-way classification labels (SUPPORTED/REFUTED/NOT ENOUGH INFORMATION). Six LLMs were evaluated in zero-shot settings, revealing that while models can recall relevant medical information and reference systematic reviews, they struggle particularly with differentiating between refuted claims and cases with insufficient evidence.

## Method Summary
The researchers constructed MedREQAL by scraping Cochrane systematic review abstracts from PubMed (2018-2023), then used GPT-3.5 to generate questions from review objectives and extract answers/labels from authors' conclusions. Questions were classified into health categories using another GPT-3.5 prompt. Six LLMs (GPT-4, Mixtral, Mistral, ChatDoctor, MedAlpaca, PMC-LLaMa) were evaluated in zero-shot settings using their respective base prompts, with performance measured through classification accuracy, F1-score, ROUGE-L, and BERTScore.

## Key Results
- Mixtral achieved the highest classification accuracy of 62% and F1 score of 34.8%
- All models generally struggled with decisively concluding when evidence was insufficient (NEI class)
- Models demonstrated ability to recall relevant medical information and reference systematic reviews as authoritative sources
- Common failure mode: misclassifying REFUTED as NEI due to similar negative phrasing in conclusions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MedREQAL dataset construction leverages systematic reviews as evidence sources to create high-quality question-answer pairs for medical knowledge recall evaluation.
- Mechanism: Systematic reviews synthesize the best available evidence on specific medical questions, providing authoritative conclusions that serve as ground truth answers. By extracting questions from review objectives and answers from authors' conclusions, the dataset captures clinically relevant medical knowledge in a structured format.
- Core assumption: Systematic reviews from the Cochrane Collaboration provide reliable, evidence-based conclusions that can serve as ground truth for evaluating medical knowledge recall.
- Evidence anchors:
  - [abstract] "Through experiments on the new MedREQAL dataset, comprising question-answer pairs extracted from rigorous systematic reviews, we assess six LLMs, analyzing their classification and generation performance."
  - [section] "We deem that the focus on important healthcare interventions, wide domain coverage, standardized structure, and rigorous process followed by medical experts to construct these reviews are all factors that made them a highly suitable source for the construction of a novel biomedical question-answering dataset."
  - [corpus] Weak evidence - the corpus shows related work on medical QA datasets but none using systematic reviews as primary source, making this dataset construction approach novel.
- Break condition: If systematic reviews contain biases, errors, or outdated information that LLMs encode incorrectly, the ground truth quality would be compromised.

### Mechanism 2
- Claim: Zero-shot prompting without document context forces LLMs to rely on their internal knowledge representations for medical recall tasks.
- Mechanism: By providing only questions without the full review text, the evaluation isolates the model's ability to recall medical knowledge from training data rather than performing document-based comprehension. This setup tests the internalized knowledge rather than reading comprehension skills.
- Core assumption: LLMs retain and can recall relevant medical knowledge from their pre-training without needing to access external documents during inference.
- Evidence anchors:
  - [section] "Since the models were instruction-tuned with different templates, we used for each of them the base prompt reported by authors in technical reports... All the models were tested only in a zero-shot setting with just the question provided."
  - [section] "We are interested in medical knowledge recall, which is best evaluated with a zero-shot setup."
  - [corpus] Moderate evidence - related work mentions zero-shot performance evaluation but focuses more on instruction-following capabilities than pure knowledge recall.
- Break condition: If models perform well in zero-shot but fail when given relevant documents, this would indicate knowledge gaps rather than retrieval issues.

### Mechanism 3
- Claim: Classification into SUPPORTED/REFUTED/NEI labels reveals model limitations in distinguishing between refuted claims and insufficient evidence.
- Mechanism: The three-way classification task exposes whether models can differentiate between claims that have been tested and found ineffective versus claims that lack sufficient high-quality studies. This distinction is crucial for medical decision-making.
- Core assumption: Medical knowledge requires not just knowing what works, but also understanding when evidence is insufficient to draw conclusions.
- Evidence anchors:
  - [abstract] "The results show that Mixtral achieved the highest classification accuracy of 62% and F1 score of 34.8%, while the models generally struggled with decisively concluding when evidence was insufficient."
  - [section] "Another interesting finding is shown in Table 5... This clearly demonstrates the encoded internal medical knowledge and the ability of models to refer to systematic reviews as the highest type of clinical evidence to answer given questions."
  - [corpus] Weak evidence - the corpus shows related work on medical QA but none specifically examining the REFUTED vs NEI distinction in LLM knowledge recall.
- Break condition: If models consistently misclassify REFUTED as NEI (or vice versa), this indicates fundamental misunderstanding of medical evidence interpretation.

## Foundational Learning

- Concept: Systematic review methodology and evidence hierarchy
  - Why needed here: Understanding how systematic reviews synthesize evidence and their position at the top of evidence hierarchies is crucial for interpreting dataset construction and results.
  - Quick check question: What makes systematic reviews from the Cochrane Collaboration particularly reliable as evidence sources compared to individual studies?

- Concept: Zero-shot vs few-shot vs fine-tuning learning paradigms
  - Why needed here: The study uses zero-shot prompting to test pure knowledge recall, which requires understanding how this differs from other learning approaches in terms of what is being evaluated.
  - Quick check question: How would providing few-shot examples change the nature of what is being evaluated in this medical knowledge recall task?

- Concept: Precision-recall tradeoffs in classification metrics
  - Why needed here: The classification performance uses F1-score, which balances precision and recall - understanding this tradeoff is essential for interpreting model performance.
  - Quick check question: Why might a model with high accuracy still have low F1-score on a dataset with imbalanced class distribution like MedREQAL?

## Architecture Onboarding

- Component map: Dataset construction → LLM prompting → Zero-shot inference → Classification metrics (Accuracy, F1) + Generation metrics (ROUGE, BERTScore) → Analysis
- Critical path: Data preparation → Model evaluation → Metric computation → Result interpretation
- Design tradeoffs: Zero-shot evaluation tests knowledge recall but may underestimate performance compared to few-shot; using systematic reviews ensures quality but limits dataset size; classification simplifies evaluation but may lose nuance from full conclusions.
- Failure signatures: Consistently misclassifying NEI as SUPPORTED indicates over-agreement bias; poor ROUGE scores despite good classification suggest recall without proper comprehension; systematic review references without correct conclusions indicate surface-level knowledge.
- First 3 experiments:
  1. Evaluate a subset of questions with human annotators to verify label quality and identify systematic biases
  2. Test few-shot prompting to establish upper bounds on performance and compare with zero-shot results
  3. Analyze model predictions by medical domain to identify which areas show better/worse knowledge recall

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve LLMs' ability to differentiate between "refuted" and "not enough information" cases in medical knowledge recall?
- Basis in paper: [explicit] The paper states that models struggle to differentiate between these two classes, leading to common mislabeling even when correct clinical evidence is recalled.
- Why unresolved: The paper identifies this as a challenge but doesn't propose specific solutions or methodologies to address this differentiation problem.
- What evidence would resolve it: Development and testing of new prompting strategies, fine-tuning approaches, or architectural modifications that specifically target this classification challenge would provide evidence of improvement.

### Open Question 2
- Question: What is the optimal approach for updating outdated medical knowledge within LLMs?
- Basis in paper: [explicit] The paper mentions that models tend to cite studies that are sometimes rather old, which can lead to incorrect predictions and quoting outdated knowledge, and references knowledge editing as an ongoing challenge.
- Why unresolved: The paper acknowledges the challenge of updating outdated knowledge but doesn't explore specific techniques or methodologies for knowledge editing in medical contexts.
- What evidence would resolve it: Implementation and evaluation of knowledge editing techniques specifically designed for medical information, with measurable improvements in accuracy and timeliness of responses.

### Open Question 3
- Question: How does human evaluation of generated model responses compare to automated metrics like BERTScore and ROUGE-L in assessing medical knowledge recall quality?
- Basis in paper: [inferred] The paper states that the study "lacks human evaluation of generated model responses" and mentions this could have shed more light on qualitative performance and user-friendliness.
- Why unresolved: The paper relies solely on automated evaluation metrics and acknowledges the absence of human evaluation as a limitation.
- What evidence would resolve it: Conducting a human evaluation study comparing human assessments with automated metrics, identifying correlations and discrepancies between human and machine evaluation of medical response quality.

## Limitations
- Dataset size of 2,786 question-answer pairs may not fully capture the breadth of medical knowledge
- Zero-shot evaluation approach may underestimate model capabilities compared to few-shot or fine-tuning approaches
- Automated question generation and labeling using GPT-3.5 introduces potential quality control issues without full human validation

## Confidence
- High Confidence: The mechanism of using systematic reviews as authoritative sources for ground truth answers is well-established and the dataset construction approach is sound.
- Medium Confidence: The finding that models struggle with distinguishing REFUTED from NOT ENOUGH INFORMATION cases is supported by results but may vary with different prompting strategies or evaluation conditions.
- Low Confidence: The absolute performance numbers (62% accuracy, 34.8% F1) should be interpreted cautiously as they depend heavily on the specific dataset composition and evaluation methodology.

## Next Checks
1. Conduct human evaluation of a random subset of 100-200 generated question-answer pairs to verify the quality and accuracy of automated labeling, particularly for the NEI category.
2. Implement few-shot prompting experiments using the same dataset to establish performance upper bounds and compare with zero-shot results to understand the gap between knowledge recall and instruction-following capabilities.
3. Perform temporal analysis by testing model performance on questions where newer systematic reviews have superseded older ones, to assess how well models handle outdated medical knowledge.