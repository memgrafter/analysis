---
ver: rpa2
title: 'Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking
  in Large Language Models'
arxiv_id: '2402.03271'
source_url: https://arxiv.org/abs/2402.03271
tags:
- questions
- question
- reward
- information
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Uncertainty of Thoughts (UoT), a framework
  to enable large language models (LLMs) to actively seek information by asking effective
  questions. UoT combines an uncertainty-aware simulation approach, uncertainty-based
  rewards motivated by information gain, and a reward propagation scheme to select
  optimal questions that maximize expected reward.
---

# Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models

## Quick Facts
- **arXiv ID**: 2402.03271
- **Source URL**: https://arxiv.org/abs/2402.03271
- **Reference count**: 40
- **Key outcome**: UoT improves success rates by 38.1% on average across multiple LLMs compared to direct prompting, while also increasing efficiency (fewer questions needed)

## Executive Summary
This paper introduces Uncertainty of Thoughts (UoT), a framework that enables large language models to actively seek information by asking effective questions. UoT combines uncertainty-aware simulation of future scenarios, uncertainty-based rewards motivated by information gain, and a reward propagation scheme to select optimal questions. Experiments on medical diagnosis, troubleshooting, and the 20 Questions game demonstrate significant improvements over baseline methods, with UoT achieving 38.1% higher success rates on average across multiple LLMs.

## Method Summary
UoT generates candidate questions, simulates future conversation trees by branching based on affirmative/negative answers, and calculates uncertainty-based rewards using entropy and information gain formulas. The framework then propagates these rewards upward to compute expected rewards and selects the question with the highest expected reward. The method uses multistep simulation (default 3 steps with 3 questions per step) and has been tested across five datasets using multiple LLMs including Llama-3-70B-Instruct, Mistral-Large, and GPT-4.

## Key Results
- UoT achieves 38.1% higher success rates on average across multiple LLMs compared to direct prompting
- The framework reduces the number of questions needed, improving efficiency in successful cases
- UoT outperforms baselines including Chain-of-Thought, Tree-of-Thoughts, and Reflexion on all tested datasets

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty-Aware Simulation
UoT uses uncertainty-aware simulation to model possible future scenarios and their likelihoods by generating candidate questions and simulating future conversation trees. At each node, it computes possibility sets and their probabilities to capture the likelihood of different outcomes. This mechanism assumes the model can simulate plausible future scenarios and accurately estimate probabilities of different answers at each node.

### Mechanism 2: Information Gain-Based Rewards
UoT rewards questions based on information gain to incentivize seeking information that reduces uncertainty. The framework calculates rewards using entropy and information gain formulas, maximizing rewards when questions split the possibility set into equally likely subsets. This mechanism assumes information gain is a good proxy for the value of a question in reducing uncertainty about the unknown target.

### Mechanism 3: Reward Propagation for Optimal Selection
UoT uses reward propagation to select the optimal question by maximizing expected reward over the simulation tree. The framework computes accumulated rewards over multiple simulation steps and propagates them upward to calculate expected rewards, selecting the question with the highest expected reward. This mechanism assumes expected reward over multiple simulation steps is a good metric for selecting the optimal question.

## Foundational Learning

- **Concept: Entropy and Information Gain**
  - Why needed here: These concepts are used to quantify uncertainty and the value of information gained from asking questions.
  - Quick check question: What is the formula for entropy of a discrete random variable, and how does it relate to information gain?

- **Concept: Simulation and Tree Search**
  - Why needed here: UoT uses simulation to explore possible future conversation paths and select the optimal question.
  - Quick check question: How does UoT simulate future conversation trees, and what are the two types of nodes in the tree?

- **Concept: Reward Propagation**
  - Why needed here: UoT uses reward propagation to calculate expected rewards over multiple simulation steps.
  - Quick check question: How does UoT compute accumulated rewards and expected rewards, and how are they used to select the optimal question?

## Architecture Onboarding

- **Component map**: Question Generation -> Simulation -> Uncertainty-Based Rewards -> Reward Propagation -> Question Selection

- **Critical path**:
  1. Generate candidate questions based on history and current possibility set
  2. Simulate future conversation trees for each candidate question
  3. Calculate uncertainty-based rewards for each node in the simulation tree
  4. Compute accumulated rewards and propagate them upward to calculate expected rewards
  5. Select the question with the highest expected reward

- **Design tradeoffs**:
  - Simulation depth vs. computational cost: Deeper simulations provide more accurate expected rewards but are computationally expensive
  - Number of candidate questions vs. exploration: Generating more candidate questions increases the likelihood of finding a good question but also increases computational cost
  - Reward function design vs. task performance: The choice of reward function (e.g., information gain vs. other metrics) affects the quality of selected questions

- **Failure signatures**:
  - Questions become too specific or irrelevant to the task
  - The model fails to explore the possibility space effectively
  - Computational cost becomes prohibitive for real-time applications

- **First 3 experiments**:
  1. Test UoT on a simple 20 Questions game with a small possibility set to validate the basic mechanism
  2. Compare UoT with direct prompting and other baselines on a medical diagnosis task to measure performance improvement
  3. Analyze the effect of simulation depth on task performance and computational cost to find the optimal tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
How does UoT perform in scenarios with more than two answer categories (e.g., multiple-choice responses)? The paper focuses on binary (yes/no) responses for simplicity in computing uncertainty metrics, but mentions the framework can be extended to allow for a wider selection of answers. This remains unresolved as the paper does not provide experimental results or theoretical analysis for scenarios with more than two answer categories.

### Open Question 2
How does the performance of UoT scale with larger possibility spaces (e.g., thousands of diseases in medical diagnosis)? The paper uses datasets with a limited number of possibilities but mentions that UoT can be adapted to open-set scenarios. This remains unresolved as the paper does not provide experimental results or theoretical analysis for scenarios with significantly larger possibility spaces.

### Open Question 3
How sensitive is UoT to the choice of the hyperparameter λ in the uncertainty-based reward function? The paper mentions that λ is a hyperparameter that helps to sharpen the rewards but does not provide a detailed analysis of its impact on performance. This remains unresolved as the paper does not provide a sensitivity analysis or guidelines for choosing the optimal value of λ.

### Open Question 4
How does UoT compare to human performance in information-seeking tasks? The paper evaluates UoT's performance on various datasets but does not compare it to human performance. This remains unresolved as the paper does not provide any data or analysis on how well humans perform in the same information-seeking tasks.

## Limitations

- The framework's performance depends heavily on the LLM's ability to accurately simulate future scenarios and estimate probabilities
- Computational complexity grows exponentially with simulation depth, potentially limiting real-world applicability
- The effectiveness depends on the specific reward function design, which may not generalize well across all task types

## Confidence

**High Confidence:**
- The framework improves success rates compared to direct prompting across multiple benchmarks
- The information gain-based reward mechanism provides theoretical grounding for question selection
- The simulation-based approach enables exploration of multiple future conversation paths

**Medium Confidence:**
- The efficiency improvements (fewer questions needed) are consistent across all task types
- The optimal simulation depth of 3 steps generalizes well across different domains
- The reward propagation scheme effectively identifies the best questions

**Low Confidence:**
- The framework's performance with smaller or less capable LLMs
- The scalability of the approach to more complex, real-world scenarios
- The generalizability of the specific reward function parameters across different domains

## Next Checks

1. **Robustness Testing**: Validate the framework's performance with smaller LLMs (7B parameters) to assess the dependency on model size and capabilities.

2. **Domain Transferability**: Test the framework on tasks requiring domain-specific knowledge (e.g., legal reasoning, technical troubleshooting) to evaluate generalizability.

3. **Computational Efficiency Analysis**: Measure the actual computational overhead of the simulation process and assess its feasibility for real-time applications.