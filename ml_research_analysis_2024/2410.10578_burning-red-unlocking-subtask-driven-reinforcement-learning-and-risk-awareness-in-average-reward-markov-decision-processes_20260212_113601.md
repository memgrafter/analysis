---
ver: rpa2
title: 'Burning RED: Unlocking Subtask-Driven Reinforcement Learning and Risk-Awareness
  in Average-Reward Markov Decision Processes'
arxiv_id: '2410.10578'
source_url: https://arxiv.org/abs/2410.10578
tags:
- cvar
- subtask
- average-reward
- function
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Reward-Extended Differential (RED) reinforcement
  learning, a novel framework for solving multiple learning objectives simultaneously
  in average-reward Markov decision processes. The key innovation is the reward-extended
  temporal-difference (TD) error, which enables learning various subtasks alongside
  the primary objective using only TD error-based updates.
---

# Burning RED: Unlocking Subtask-Driven Reinforcement Learning and Risk-Awareness in Average-Reward Markov Decision Processes

## Quick Facts
- **arXiv ID**: 2410.10578
- **Source URL**: https://arxiv.org/abs/2410.10578
- **Reference count**: 24
- **Key outcome**: Reward-Extended Differential (RED) RL framework enables simultaneous learning of multiple subtasks and risk-aware decision making through CVaR optimization in average-reward MDPs

## Executive Summary
This paper introduces Reward-Extended Differential (RED) reinforcement learning, a novel framework for solving multiple learning objectives simultaneously in average-reward Markov decision processes. The key innovation is the reward-extended temporal-difference (TD) error, which enables learning various subtasks alongside the primary objective using only TD error-based updates. The authors apply this framework to optimize the conditional value-at-risk (CVaR) risk measure in a fully-online manner without requiring augmented state-spaces or explicit bi-level optimization schemes. Empirical results demonstrate successful CVaR optimization in both tabular and function approximation settings, including a two-state "red-pill blue-pill" environment and the inverted pendulum task. The approach enables risk-aware decision-making by learning policies that prioritize reward CVaR over average reward when appropriate.

## Method Summary
The RED framework extends standard average-reward RL algorithms by incorporating subtask learning through reward extensions. The core mechanism is the reward-extended TD error, which modifies the observed reward with linear or piecewise linear functions of subtasks. When the standard TD error converges to zero, all reward-extended TD errors also converge to zero, causing all subtask estimates to converge to their true values. The framework is applied to CVaR optimization by treating VaR as a subtask, enabling implicit bi-level optimization without explicit parameter search. Two algorithms are proposed: RED CVaR Q-learning for tabular settings and RED CVaR Actor-Critic for function approximation, both learning VaR and CVaR estimates online alongside the primary value function.

## Key Results
- RED CVaR Q-learning successfully learns optimal CVaR policies in the red-pill blue-pill environment, choosing the risk-aware "red pill" when appropriate
- RED CVaR Actor-Critic demonstrates risk-aware behavior in the inverted pendulum task using tile coding function approximation
- The framework maintains convergence properties of standard average-reward RL while adding subtask learning capabilities without explicit bi-level optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The reward-extended TD error allows simultaneous learning of multiple subtasks by converting them into modifications of the MDP's reward structure
- Mechanism: By extending the observed reward with linear or piecewise linear functions of the subtasks, the TD error naturally incorporates information about all subtasks simultaneously. When the regular TD error converges to zero, all reward-extended TD errors also converge to zero, causing all subtask estimates to converge to their true values
- Core assumption: The subtask function must be linear or piecewise linear with respect to the subtasks and independent of states/actions
- Evidence anchors:
  - [abstract]: "The key innovation is the reward-extended temporal-difference (TD) error, which enables learning various subtasks alongside the primary objective using only TD error-based updates"
  - [section 5.1]: "We will show that the reward-extended TD error satisfies the following property: Eπ[βi,t] → 0 ∀i = 1, 2, ..., n as Eπ[δt] → 0"
  - [corpus]: Weak - no direct citations found for this specific mechanism, though related average-reward RL work exists
- Break condition: If the subtask function violates the linearity independence requirement, the reward-extended TD error properties no longer hold and subtask learning fails

### Mechanism 2
- Claim: The RED framework converts bi-level optimization problems into implicit bi-level optimizations that can be solved in a fully-online manner
- Mechanism: By treating unknown optimal parameters (like VaR in CVaR optimization) as subtasks, the framework learns these parameters online while simultaneously optimizing the primary objective. This eliminates the need for explicit outer-loop optimization over parameter guesses
- Core assumption: The optimal parameter values exist as fixed points of the subtask learning process
- Evidence anchors:
  - [abstract]: "without the use of an explicit bi-level optimization scheme or an augmented state-space"
  - [section 4]: "what we do know is that if we know this optimal value, VaR, then optimizing for CVaR ultimately amounts to optimizing an average"
  - [section 6]: "utilize CVaR-specific versions of the RED algorithms... to optimize VaR and CVaR, such that CVaR corresponds to the primary control objective"
- Break condition: If the optimal parameter does not correspond to a stable fixed point of the learning dynamics, the online estimation will diverge or oscillate

### Mechanism 3
- Claim: The RED framework preserves the convergence properties of standard average-reward RL algorithms while adding subtask learning capabilities
- Mechanism: The reward-extended TD error is constructed such that when the standard TD error converges (as proven for differential algorithms), all subtask estimates also converge. This is achieved by ensuring the subtask updates are proportional to the TD error
- Core assumption: The step sizes for subtasks decrease at appropriate rates relative to value function and average-reward step sizes
- Evidence anchors:
  - [section 5.2]: "we now provide an equivalent theorem for our RED TD-learning algorithm, which also shows that Zi,t converges to zi,π ∀zi ∈ Z"
  - [section 5.2]: "The RED Q-learning algorithm (16) converges, almost surely, ¯Rt to ¯r∗, Zi,t to z∗i ∀zi ∈ Z"
  - [corpus]: Weak - related convergence results exist but specific RED framework convergence is novel
- Break condition: If step size schedules violate the multiple-timescales requirement, subtask estimates may fail to converge despite value function convergence

## Foundational Learning

- Concept: Average-reward MDPs and differential methods
  - Why needed here: The entire RED framework is built on the structural properties of average-reward MDPs that enable subtask learning through TD errors
  - Quick check question: What distinguishes average-reward MDPs from discounted MDPs in terms of the Bellman equations they satisfy?

- Concept: Risk measures (CVaR, VaR)
  - Why needed here: The case study demonstrates how RED enables risk-aware decision making by optimizing CVaR, which requires understanding both the measure and its computational challenges
  - Quick check question: How does CVaR differ from expected value as a risk measure, and why might one prefer CVaR in safety-critical applications?

- Concept: Temporal difference learning and TD errors
  - Why needed here: The reward-extended TD error is the core mechanism that enables subtask learning, building directly on standard TD error concepts
  - Quick check question: What is the relationship between the TD error and the Bellman equation in the context of prediction?

## Architecture Onboarding

- Component map:
  - MDP environment providing rewards and transitions
  - Value function estimator (V or Q table)
  - Average-reward estimator (scalar)
  - Subtask estimators (one per subtask, initialized arbitrarily)
  - Reward extension function combining observed reward with subtask estimates
  - TD error calculator using extended rewards
  - Subtask update mechanism using reward-extended TD errors

- Critical path:
  1. Observe reward and next state from environment
  2. Compute extended reward using current subtask estimates
  3. Calculate TD error using extended reward
  4. Update value function and average-reward estimates
  5. Calculate reward-extended TD errors for each subtask
  6. Update subtask estimates
  7. Repeat

- Design tradeoffs:
  - Linear vs piecewise linear subtask functions: Linear functions are simpler but less expressive; piecewise linear allows more complex relationships at the cost of implementation complexity
  - Step size schedules: Faster subtask learning vs stability; aggressive schedules may speed convergence but risk instability
  - Number of subtasks: More subtasks enable richer behavior but increase computational overhead and step size tuning complexity

- Failure signatures:
  - Value function and average-reward converge but subtask estimates diverge or oscillate: Indicates step size schedule violations
  - All estimates converge but to incorrect values: Suggests subtask function is misspecified or violates independence requirements
  - Slow convergence: May indicate inappropriate step sizes or overly complex subtask functions

- First 3 experiments:
  1. Implement tabular RED TD-learning on a simple MDP with one subtask (e.g., tracking a constant offset from rewards) to verify basic mechanism
  2. Test RED CVaR optimization on the red-pill blue-pill environment with varying τ values to validate risk-awareness capability
  3. Compare RED vs standard differential algorithms on inverted pendulum task to evaluate function approximation performance and verify optimal policy equivalence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the RED framework perform with multiple subtasks (more than 2) in terms of convergence speed and policy quality?
- Basis in paper: [explicit] The authors note that "it remains to be seen empirically how our framework performs when dealing with multiple subtasks" and only demonstrate single-subtask (VaR) optimization in the CVaR case study.
- Why unresolved: The current empirical evaluation focuses on single-subtask problems, leaving scalability and performance with multiple concurrent subtasks untested.
- What evidence would resolve it: Controlled experiments comparing RED algorithms with 2+ subtasks against baseline methods across diverse environments, measuring convergence rates and final policy performance.

### Open Question 2
- Question: Can the RED framework maintain performance when using nonlinear function approximation (e.g., neural networks) instead of linear methods like tile coding?
- Basis in paper: [explicit] The authors state "it remains to be seen empirically how our framework performs... when utilizing nonlinear function approximation" and only test linear function approximation in the inverted pendulum experiment.
- Why unresolved: The current experiments use linear function approximation, which may not capture complex state-action relationships present in more challenging domains.
- What evidence would resolve it: Empirical comparison of RED algorithms using neural networks versus linear methods in benchmark RL tasks, measuring learning efficiency and final performance.

### Open Question 3
- Question: What is the impact of violating the unichain or communicating assumptions on RED algorithm convergence and performance?
- Basis in paper: [inferred] The authors acknowledge that "by nature of operating in the average-reward setting, we are subject to the somewhat-strict assumptions made about the Markov chain" and these assumptions "could restrict the applicability of our framework."
- Why unresolved: The theoretical analysis assumes unichain/communicating MDPs, but real-world problems often violate these assumptions, potentially affecting algorithm behavior.
- What evidence would resolve it: Empirical testing of RED algorithms in MDPs with multiple recurrent classes or absorbing states, measuring convergence behavior and policy quality compared to environments satisfying the assumptions.

## Limitations
- The framework relies heavily on linearity assumptions for subtask functions, which may be too restrictive for complex real-world applications
- Theoretical convergence guarantees depend on strict step-size scheduling requirements that can be difficult to satisfy in practice
- While RED successfully demonstrates CVaR optimization, its performance with other risk measures and more complex subtask combinations remains untested

## Confidence
- **High Confidence**: The basic RED mechanism for simultaneous subtask learning is sound and well-supported by the theoretical analysis in the paper. The convergence proof for the tabular case appears rigorous.
- **Medium Confidence**: The RED CVaR optimization approach works as demonstrated in the experiments, but the general applicability to other risk measures and the robustness to hyperparameter choices warrant further investigation.
- **Low Confidence**: The function approximation results are promising but not thoroughly validated. The paper does not address potential stability issues when using non-linear function approximators with the RED framework.

## Next Checks
1. Test RED with non-linear subtask functions to evaluate the limits of the linearity assumption and identify scenarios where the framework breaks down.
2. Conduct extensive hyperparameter sensitivity analysis for the RED CVaR algorithms across multiple environments to understand robustness and provide practical guidance.
3. Compare RED CVaR performance against state-of-the-art risk-aware RL methods (like those using augmented state spaces) on challenging continuous control benchmarks to validate competitive performance.