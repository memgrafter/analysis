---
ver: rpa2
title: Standards for Belief Representations in LLMs
arxiv_id: '2405.21030'
source_url: https://arxiv.org/abs/2405.21030
tags:
- belief
- beliefs
- llms
- representation
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper
---

# Standards for Belief Representations in LLMs

## Quick Facts
- arXiv ID: 2405.21030
- Source URL: https://arxiv.org/abs/2405.21030
- Reference count: 6
- This paper proposes four criteria for identifying belief-like representations in LLMs: accuracy, coherence, uniformity, and use.

## Executive Summary
This paper addresses the challenge of identifying belief-like representations in large language models (LLMs) by proposing a framework of four criteria: accuracy, coherence, uniformity, and use. The authors argue that existing methods from decision theory and formal epistemology are inadequate for LLMs due to their unique epistemic situation (limited behavioral evidence but full internal access). The framework aims to be both philosophically rigorous and practice-informed, acknowledging the current limitations of interpretability techniques while providing a path forward for belief measurement in these models.

## Method Summary
The authors develop a four-criterion framework for identifying belief-like representations in LLMs by synthesizing philosophical accounts of belief with practical considerations from current interpretability research. They propose that a representation qualifies as belief-like only if it satisfies all four criteria simultaneously: accuracy (truth-tracking), coherence (logical consistency), uniformity (cross-domain applicability), and use (functional guidance of behavior). The framework is presented as a response to the inadequacy of single-criterion approaches and the unique challenges posed by LLMs' internal-only access.

## Key Results
- Proposes a comprehensive framework of four criteria for belief attribution in LLMs
- Identifies limitations of existing single-criterion approaches to belief measurement
- Acknowledges that current empirical work has not validated the framework's effectiveness
- Highlights the unique epistemic situation of having full internal access but limited behavioral evidence with LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The four criteria together provide a comprehensive framework for identifying belief-like representations in LLMs.
- Mechanism: Each criterion addresses a different aspect of what makes a representation functionally similar to belief: accuracy ensures truth-tracking, coherence ensures logical consistency, uniformity ensures cross-domain applicability, and use ensures the representation guides behavior.
- Core assumption: A representation satisfying all four criteria will be both theoretically sound and practically useful for understanding LLM cognition.
- Evidence anchors:
  - [abstract]: "Our proposed criteria include accuracy, coherence, uniformity, and use, which together help lay the groundwork for a comprehensive understanding of belief representation in LLMs."
  - [section]: "Our approach aims to be philosophically rigorous by ensuring that the requirements for belief-like representations align with our best philosophical accounts of belief."
  - [corpus]: Weak evidence - corpus neighbors don't directly address the four-criteria framework, but some papers touch on related concepts like representation quality and LLM evaluation.
- Break condition: If LLMs lack internal representations that satisfy even one of these criteria, the framework cannot identify belief-like representations.

### Mechanism 2
- Claim: The framework is practice-informed because it acknowledges the unique epistemic situation with LLMs (limited behavioral evidence, full internal access).
- Mechanism: By focusing on internal representations rather than behavioral proxies, the framework leverages the one advantage we have with LLMs - complete access to their internal states.
- Core assumption: Standard methods for belief attribution from decision theory and formal epistemology are inadequate for LLMs due to their unique characteristics.
- Evidence anchors:
  - [section]: "we don't think that we can apply off-the-shelf representation theorem methods from decision theory to extract beliefs from LLMs."
  - [abstract]: "computer scientists are developing methods to understand their cognitive processes, particularly concerning how (and if) LLMs internally represent their beliefs about the world."
  - [corpus]: Weak evidence - corpus neighbors don't directly address the epistemic differences between humans and LLMs.
- Break condition: If behavioral evidence becomes available or reliable, the framework might need revision to incorporate it.

### Mechanism 3
- Claim: The framework provides a path forward for belief measurement by identifying limitations of existing approaches.
- Mechanism: By showing that individual criteria (like accuracy alone) are insufficient, the framework points to the need for a multi-faceted approach to belief measurement.
- Core assumption: Existing belief measurement techniques fail because they focus on single criteria rather than the combination needed for true belief-like representations.
- Evidence anchors:
  - [section]: "we noticed that the claims in their datasets did not contain negations... accuracy dropped from over 80% to 40-60% depending on the layers used and the training method."
  - [abstract]: "We draw on empirical work showing the limitations of using various criteria in isolation to identify belief representations."
  - [corpus]: Weak evidence - corpus neighbors don't directly address limitations of existing belief measurement techniques.
- Break condition: If new measurement techniques emerge that successfully use single criteria, the framework's emphasis on multiple criteria might need revision.

## Foundational Learning

- Concept: Transformer architecture basics
  - Why needed here: The paper assumes understanding of how LLMs process information through layers, attention mechanisms, and embeddings.
  - Quick check question: What is the fundamental difference between how information flows in a transformer versus a recurrent neural network?

- Concept: Probing techniques
  - Why needed here: The paper discusses using interpretability methods to identify representations satisfying the proposed criteria.

- Concept: Philosophical theories of belief
  - Why needed here: The framework is grounded in philosophical accounts of what constitutes belief, requiring familiarity with key debates in philosophy of mind.

## Architecture Onboarding

### Component Map
Input representations -> Transformer layers -> Activation spaces -> Probing techniques -> Belief criteria evaluation

### Critical Path
1. Identify candidate representations in activation space
2. Evaluate against accuracy criterion
3. Evaluate against coherence criterion
4. Evaluate against uniformity criterion
5. Evaluate against use criterion
6. Determine if representation satisfies all criteria simultaneously

### Design Tradeoffs
- Internal vs. behavioral evidence: The framework prioritizes internal access over behavioral evidence, which may miss functionally important representations that don't satisfy formal criteria
- Simplicity vs. comprehensiveness: Four criteria provide theoretical rigor but may be practically difficult to apply simultaneously
- Current vs. future techniques: The uniformity criterion depends on available measurement methods, which may evolve

### Failure Signatures
- Single-criterion approaches failing to identify belief-like representations
- High accuracy but low coherence (or vice versa) in candidate representations
- Representations that satisfy some criteria but not others

### First Experiments
1. Test accuracy criterion alone on simple fact representations in GPT-2
2. Test coherence criterion by evaluating logical consistency in mathematical reasoning representations
3. Test uniformity criterion by comparing representations of similar concepts across different domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs possess internal representations that satisfy all four proposed criteria (accuracy, coherence, uniformity, and use) to be considered belief-like?
- Basis in paper: [explicit] The authors propose four criteria and acknowledge it may turn out LLMs have no representations satisfying them
- Why unresolved: Current empirical work has shown failures of individual criteria when used in isolation, and no comprehensive testing has been done to verify all criteria together
- What evidence would resolve it: Systematic testing of candidate belief representations against all four criteria simultaneously, ideally using interventional techniques to verify use

### Open Question 2
- Question: What diachronic criteria, if any, should be added to the proposed synchronic criteria for belief attribution in LLMs?
- Basis in paper: [explicit] The authors briefly discuss diachronic stability as a potential future direction
- Why unresolved: Current interpretability work focuses on synchronic analysis, and the unique training/inference dynamics of LLMs make traditional diachronic constraints potentially inappropriate
- What evidence would resolve it: Empirical studies of belief evolution during training and in-context learning, comparing different diachronic coherence requirements

### Open Question 3
- Question: How should the uniformity requirement be operationalized as belief measurement techniques become more sophisticated?
- Basis in paper: [explicit] The authors acknowledge uniformity is pragmatic and depends on available measurement techniques
- Why unresolved: Current techniques find simple directions in activation space, but future methods might extract more complex feature representations
- What evidence would resolve it: Development and testing of new feature extraction methods (like sparse autoencoders) to determine what constitutes uniformity in more complex representational spaces

## Limitations

- The framework's practical applicability remains untested empirically
- The relationship between the four criteria is not fully specified
- The framework assumes complete access to LLM internal states, which may overstate current interpretability capabilities

## Confidence

- Medium confidence in the theoretical framework itself - the philosophical grounding is sound and the criteria are well-justified individually, but the integration and practical utility are unproven
- Low confidence in the claim that this framework provides a practical path forward for belief measurement - this is asserted but not demonstrated
- Medium confidence in the critique of existing approaches - the limitations of single-criterion methods are plausible but the evidence is thin

## Next Checks

1. Apply the four-criterion framework to an actual LLM (e.g., LLaMA or GPT-2) and document whether any representations satisfy all criteria simultaneously
2. Systematically vary each criterion independently while holding others constant to determine whether they are truly orthogonal requirements or whether satisfying some criteria implies satisfying others
3. Apply the same four criteria to human belief attribution scenarios and compare results with standard philosophical accounts of belief to validate whether the framework captures what philosophers mean by "belief" in practice