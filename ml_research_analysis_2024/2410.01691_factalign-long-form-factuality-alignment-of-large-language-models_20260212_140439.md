---
ver: rpa2
title: 'FactAlign: Long-form Factuality Alignment of Large Language Models'
arxiv_id: '2410.01691'
source_url: https://arxiv.org/abs/2410.01691
tags:
- factuality
- alignment
- long-form
- arxiv
- factual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FactAlign, a novel framework for improving
  the long-form factuality of large language models (LLMs). FactAlign leverages a
  fine-grained alignment algorithm, fKTO, which extends Kahneman-Tversky Optimization
  (KTO) to the sentence level, enabling the model to use fine-grained factuality assessments
  provided by an automatic evaluator.
---

# FactAlign: Long-form Factuality Alignment of Large Language Models

## Quick Facts
- arXiv ID: 2410.01691
- Source URL: https://arxiv.org/abs/2410.01691
- Reference count: 36
- Key outcome: Introduces FactAlign framework that improves long-form factuality of LLMs through sentence-level alignment, outperforming proprietary models in factuality benchmarks

## Executive Summary
FactAlign addresses the challenge of long-form factuality in large language models by introducing a novel alignment framework that operates at the sentence level. The framework leverages an automatic factuality evaluator to provide fine-grained assessments of factual accuracy, enabling precise optimization through an extended Kahneman-Tversky Optimization (KTO) algorithm. By combining response-level and sentence-level alignment objectives in an iterative process, FactAlign significantly improves the factual accuracy of LLM responses while maintaining helpfulness. The method demonstrates superior performance compared to both proprietary and open-weight models on factuality benchmarks, while providing control over the precision-recall tradeoff in factual content generation.

## Method Summary
FactAlign introduces a fine-grained alignment algorithm (fKTO) that extends Kahneman-Tversky Optimization to the sentence level, enabling precise optimization based on automatic factuality assessments. The framework combines two alignment objectives: response-level alignment that measures overall response factuality, and sentence-level alignment that evaluates each sentence individually. These objectives are optimized iteratively, with the model alternating between response-level and sentence-level alignment steps. The approach leverages a knowledge corpus (Wikipedia) to ground responses in verifiable facts, and uses an automatic evaluator to provide fine-grained factuality assessments at the sentence level. This sentence-level granularity allows the model to learn from detailed feedback about which specific parts of its responses contain factual errors, leading to more accurate long-form generation.

## Key Results
- FactAlign significantly improves factual F1 score and precision compared to baseline models on open-domain prompts and information-seeking questions
- The framework enables control over the tradeoff between factual precision and recall through sentence-level alignment
- FactAlign outperforms both proprietary models (GPT-3.5) and open-weight models (LLaMA-3 8B, Phi3-Mini) in factuality benchmarks while maintaining helpfulness
- Response-level and sentence-level alignment objectives are both necessary, with ablation studies showing performance degradation when either is removed

## Why This Works (Mechanism)
FactAlign works by providing fine-grained supervision at the sentence level, allowing the model to precisely identify and correct factual errors in its responses. The fKTO algorithm extends traditional KTO by operating at the sentence level, which enables more targeted optimization compared to response-level approaches. The iterative process alternates between response-level and sentence-level alignment, allowing the model to progressively refine its factuality while maintaining coherence and helpfulness. The automatic evaluator provides consistent, scalable feedback that would be impractical to obtain through human annotation alone, enabling efficient training on large datasets of information-seeking prompts.

## Foundational Learning

**Knowledge Corpora**: The body of verifiable information used to ground LLM responses (e.g., Wikipedia, search engine results)
- Why needed: Provides factual basis for responses and enables automatic evaluation against verifiable sources
- Quick check: Verify the knowledge corpus covers the domain of the prompts and is regularly updated

**Automatic Factuality Evaluation**: Methods for assessing the factual accuracy of text without human intervention
- Why needed: Enables scalable training by providing consistent feedback on factual errors
- Quick check: Ensure the evaluator can detect both missing and incorrect information at the sentence level

**Kahneman-Tversky Optimization (KTO)**: A preference optimization algorithm that aligns model outputs with human preferences
- Why needed: Provides theoretical foundation for preference-based alignment at the sentence level
- Quick check: Verify the extension to sentence-level optimization preserves the mathematical properties of original KTO

## Architecture Onboarding

**Component Map**: Knowledge Corpus -> Prompt Encoder -> Response Generator -> Factuality Evaluator -> fKTO Optimizer -> Aligned Model

**Critical Path**: The iterative alignment process where model responses are evaluated, optimized, and regenerated through alternating response-level and sentence-level alignment steps

**Design Tradeoffs**: Sentence-level granularity provides precise feedback but increases computational complexity; automatic evaluation enables scalability but may miss nuanced errors

**Failure Signatures**: Degradation in response coherence when sentence-level alignment is too aggressive; loss of helpfulness when precision is maximized at the expense of recall

**First Experiments**: 1) Ablation study comparing response-level vs. sentence-level alignment effectiveness; 2) Evaluation of precision-recall tradeoff control across different prompt types; 3) Scalability test of iterative alignment process with increasing prompt complexity

## Open Questions the Paper Calls Out

**Open Question 1**: How does the performance of FactAlign vary when using different knowledge corpora (e.g., Wikipedia vs. commercial search engines like Google Search)?
- Basis in paper: The paper mentions using Wikipedia due to cost and controlled setting, but notes that commercial search engines have broader coverage
- Why unresolved: The paper does not provide experimental results comparing different knowledge sources
- What evidence would resolve it: Experiments comparing factual accuracy when using different knowledge corpora

**Open Question 2**: How does the performance of FactAlign scale with model size, particularly for larger models like GPT-4 or smaller models?
- Basis in paper: The paper evaluates on Gemma-2B, LLaMA-3 8B, and Phi3-Mini, but does not explore performance on larger or smaller models
- Why unresolved: The paper lacks information on how performance changes with model size
- What evidence would resolve it: Experiments evaluating FactAlign on a range of model sizes from very small to very large

**Open Question 3**: How does FactAlign perform on prompts outside the information-seeking question domain, such as creative writing or reasoning tasks?
- Basis in paper: The paper focuses on information-seeking questions and open-domain prompts
- Why unresolved: The paper does not provide evidence of effectiveness on other prompt types
- What evidence would resolve it: Experiments evaluating FactAlign on diverse prompt types including creative writing and reasoning

## Limitations
- The evaluation relies heavily on the FactScore metric, which may not capture all aspects of factuality
- The effectiveness depends on the quality of the automatic factuality evaluator, which could propagate errors
- The iterative alignment process lacks analysis of convergence properties and potential diminishing returns

## Confidence
- High confidence: Experimental methodology is sound with proper ablation studies and baseline comparisons
- Medium confidence: Long-term stability and generalizability across diverse domains remains uncertain
- Medium confidence: Scalability to extremely long responses (>2048 tokens) has not been fully validated

## Next Checks
1. Conduct human evaluation studies across multiple domains to validate the automatic FactScore metric and assess qualitative improvements
2. Test the aligned models on adversarial prompts designed to expose potential weaknesses in factuality alignment
3. Evaluate the computational efficiency and memory requirements of the iterative alignment process at scale for industrial applications