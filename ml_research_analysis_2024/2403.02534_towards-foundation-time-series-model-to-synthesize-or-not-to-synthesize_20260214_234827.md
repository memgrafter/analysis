---
ver: rpa2
title: 'Towards Foundation Time Series Model: To Synthesize Or Not To Synthesize?'
arxiv_id: '2403.02534'
source_url: https://arxiv.org/abs/2403.02534
tags:
- series
- time
- data
- dataset
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether synthetic data can effectively
  pretrain foundation models for time series forecasting, or if limited real data
  suffices. The authors propose a synthetic data generation framework based on Fourier
  coefficients and trend functions, then train a transformer model on this synthetic
  data for zero-shot forecasting.
---

# Towards Foundation Time Series Model: To Synthesize Or Not To Synthesize?

## Quick Facts
- arXiv ID: 2403.02534
- Source URL: https://arxiv.org/abs/2403.02534
- Reference count: 40
- Primary result: Real time series data outperforms synthetic data for foundation time series model pretraining in zero-shot settings

## Executive Summary
This paper investigates whether synthetic data can effectively pretrain foundation models for time series forecasting, or if limited real data suffices. The authors propose a synthetic data generation framework based on Fourier coefficients and trend functions, then train a transformer model on this synthetic data for zero-shot forecasting. Extensive experiments on multiple time series datasets show that using even a small amount of real data in a supervised setting consistently outperforms training on larger amounts of synthetic data for zero-shot forecasting. The choice of source dataset strongly impacts transferability performance, with synthetic data providing more consistent results across different source datasets.

## Method Summary
The authors develop a synthetic data generation framework for time series that generates synthetic time series based on Fourier coefficients and trend functions. They use a transformer-based architecture for forecasting and evaluate performance in a zero-shot setting where models are trained on synthetic data and tested on real data. The study compares this approach against supervised training on small amounts of real data, examining how the source dataset choice affects transferability and performance consistency.

## Key Results
- Synthetic data pretraining consistently underperforms supervised training on small amounts of real data for zero-shot forecasting
- Choice of source dataset strongly impacts transferability performance
- Synthetic data provides more consistent results across different source datasets
- Real time series data is preferable to synthetic data for foundation time series model pretraining in zero-shot settings

## Why This Works (Mechanism)
The paper's synthetic data generation approach uses Fourier coefficients and trend functions to create time series patterns that mimic real-world data structures. The transformer architecture is well-suited for capturing long-range dependencies in time series data. The zero-shot evaluation paradigm allows for testing the foundation model's ability to generalize to unseen data without fine-tuning.

## Foundational Learning
- Fourier analysis: Why needed - To decompose time series into periodic components; Quick check - Can the model capture seasonality patterns
- Transformer architecture: Why needed - To handle long-range dependencies in time series; Quick check - Does attention mechanism focus on relevant time steps
- Zero-shot learning: Why needed - To evaluate foundation model generalization; Quick check - Can model forecast unseen time series patterns
- Transfer learning: Why needed - To assess knowledge transfer across datasets; Quick check - Does performance improve with related source data
- Time series forecasting metrics: Why needed - To quantitatively evaluate forecasting performance; Quick check - Are MAE and RMSE values comparable across methods

## Architecture Onboarding
- Component map: Data generation -> Model training -> Zero-shot evaluation
- Critical path: Synthetic/real data generation -> Transformer pretraining -> Zero-shot forecasting evaluation
- Design tradeoffs: Synthetic data provides consistency but may lack real-world complexity; supervised learning uses limited real data but may overfit
- Failure signatures: Poor synthetic data quality leads to ineffective pretraining; insufficient real data causes overfitting in supervised setting
- First experiments:
  1. Generate synthetic data using Fourier coefficients and test basic pattern reproduction
  2. Train transformer on synthetic data and evaluate on simple real time series
  3. Compare supervised learning on small real dataset against synthetic pretraining

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data generation approach may not capture full complexity of real-world time series patterns
- Study focuses on specific dataset types and sizes, results might differ for other domains
- Zero-shot evaluation paradigm doesn't account for potential benefits of synthetic data in few-shot or transfer learning scenarios

## Confidence
- Main claim (real data outperforms synthetic data): High
- Broader implication (limited utility of synthetic data): Medium

## Next Checks
1. Evaluate the proposed synthetic data generation framework on domains with severe data scarcity to test if synthetic data provides value when real data is extremely limited
2. Test whether more sophisticated synthetic data generation methods (e.g., GANs, diffusion models) can outperform the Fourier-based approach and potentially challenge the conclusion
3. Investigate whether synthetic data pretraining followed by fine-tuning on small real datasets can outperform direct supervised training from scratch on the limited real data