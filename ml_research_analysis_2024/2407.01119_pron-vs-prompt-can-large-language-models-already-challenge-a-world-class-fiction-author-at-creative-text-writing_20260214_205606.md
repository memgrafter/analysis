---
ver: rpa2
title: 'Pron vs Prompt: Can Large Language Models already Challenge a World-Class
  Fiction Author at Creative Text Writing?'
arxiv_id: '2407.01119'
source_url: https://arxiv.org/abs/2407.01119
tags:
- creativity
- creative
- writing
- gpt-4
- style
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a creative writing contest between GPT-4 and
  acclaimed novelist Patricio Pron. The authors designed 60 writing tasks, where both
  participants wrote synopses for movie titles they proposed themselves or their opponent
  proposed.
---

# Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?
## Quick Facts
- arXiv ID: 2407.01119
- Source URL: https://arxiv.org/abs/2407.01119
- Reference count: 40
- Primary result: GPT-4 underperformed acclaimed novelist Patricio Pron in creative writing tasks across all evaluated dimensions

## Executive Summary
This paper presents a creative writing contest between GPT-4 and acclaimed novelist Patricio Pron. The authors designed 60 writing tasks where both participants wrote synopses for movie titles they proposed themselves or their opponent proposed. Expert literary critics and scholars evaluated the texts using a rubric based on Boden's definition of creativity. The results show that GPT-4 consistently underperformed compared to Pron across all evaluated dimensions, including attractiveness, originality, and overall creativity.

The study highlights that while GPT-4's performance improved when given titles proposed by Pron, it still fell short of matching the quality of the human author. The authors conclude that despite impressive advancements in AI language models, they are not yet ready to compete with world-class human authors in creative writing tasks. The study also emphasizes the importance of prompts in shaping AI-generated content and suggests that human-AI collaboration may be more promising than fully autonomous AI writing.

## Method Summary
The authors designed a creative writing contest between GPT-4 and acclaimed novelist Patricio Pron, consisting of 60 writing tasks. In each task, both participants wrote synopses for movie titles - either proposed by themselves or by their opponent. The texts were evaluated by expert literary critics and scholars using a rubric based on Boden's definition of creativity, which assesses dimensions such as attractiveness, originality, and overall creativity. The evaluation process included both English and Spanish language tasks to test language-specific performance.

## Key Results
- GPT-4 consistently underperformed compared to Pron across all evaluated dimensions including attractiveness, originality, and overall creativity
- GPT-4's performance improved when given titles proposed by Pron, but still fell short of human author quality
- GPT-4 demonstrated stronger creative writing abilities in English than in Spanish, with evaluators becoming more able to recognize AI writing style over time

## Why This Works (Mechanism)
The study demonstrates that current large language models, despite their impressive capabilities, struggle with the nuanced and creative aspects of fiction writing that human authors excel at. The mechanism behind this limitation appears to be rooted in the fundamental differences between how AI and human authors approach creative tasks - humans draw from lived experiences, emotional understanding, and cultural context, while AI relies on pattern recognition from training data. The study also reveals that prompt engineering plays a crucial role in shaping AI-generated content, suggesting that the gap between human and AI creativity may be partially bridgeable through better interaction design.

## Foundational Learning
- Boden's Creativity Theory: Framework for evaluating creativity across three dimensions (why needed: provides structured rubric for assessment; quick check: ensure rubric captures all dimensions of creative writing)
- Prompt Engineering: Techniques for optimizing AI responses through careful instruction design (why needed: reveals how human input shapes AI output quality; quick check: test different prompt variations)
- Cross-lingual Performance: Understanding how AI models perform across different languages (why needed: reveals potential biases in training data; quick check: compare results across multiple languages)

## Architecture Onboarding
Component Map: Human Evaluators -> Rubric Application -> GPT-4 Output -> Prompt Design -> Task Assignment -> Result Aggregation
Critical Path: Task Design -> Writing Generation -> Expert Evaluation -> Result Analysis
Design Tradeoffs: Controlled task environment vs. real-world creative writing complexity; Expert evaluation vs. automated assessment; English vs. Spanish language performance
Failure Signatures: Consistent underperformance in originality metrics; Language-dependent performance variations; Recognizable AI writing patterns
First Experiments:
1. Replicate with additional large language models to compare performance
2. Test with different creative writing genres beyond movie synopses
3. Implement blind evaluations to eliminate evaluator bias

## Open Questions the Paper Calls Out
None

## Limitations
- The sample size of 60 writing tasks may not be representative of all creative writing scenarios
- Evaluation methodology introduces subjectivity in assessing creativity
- Study only evaluated GPT-4, leaving uncertainty about other large language models' performance

## Confidence
Medium - Limited generalizability to other creative writing tasks; Potential evaluator bias from recognizing AI writing patterns; Results may be influenced by specific prompt engineering approach

## Next Checks
1. Replicate the study with a larger and more diverse set of writing tasks across different genres and styles to test generalizability
2. Conduct a similar evaluation with multiple large language models to understand if GPT-4's performance is representative of current AI capabilities
3. Implement blind evaluations where evaluators have no knowledge of which texts are AI-generated to eliminate potential bias from recognizing AI writing patterns