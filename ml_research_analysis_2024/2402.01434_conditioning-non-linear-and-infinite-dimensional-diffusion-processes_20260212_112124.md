---
ver: rpa2
title: Conditioning non-linear and infinite-dimensional diffusion processes
arxiv_id: '2402.01434'
source_url: https://arxiv.org/abs/2402.01434
tags:
- stochastic
- function
- process
- time
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for conditioning infinite-dimensional
  diffusion processes without first discretizing the model. The authors use an infinite-dimensional
  version of Girsanov's theorem to condition a function-valued stochastic process,
  leading to a stochastic differential equation (SDE) for the conditioned process
  involving the score.
---

# Conditioning non-linear and infinite-dimensional diffusion processes

## Quick Facts
- arXiv ID: 2402.01434
- Source URL: https://arxiv.org/abs/2402.01434
- Reference count: 40
- One-line primary result: Introduces a method for conditioning infinite-dimensional diffusion processes without first discretizing the model, applied to modeling butterfly shape evolution

## Executive Summary
This paper presents a novel approach to conditioning infinite-dimensional diffusion processes by applying an infinite-dimensional version of Girsanov's theorem and Doob's h-transform. The authors develop a method that avoids the typical discretization step, instead working directly in function space and discretizing only for computational implementation via truncated Fourier bases. The technique is applied to model the evolution of butterfly shapes over time, where the shape space is treated as an infinite-dimensional Hilbert space and the conditioned process satisfies a new stochastic differential equation involving the score function.

## Method Summary
The paper uses infinite-dimensional Doob's h-transform to condition function-valued stochastic processes by constructing a strictly positive martingale Z(t) = h(t, X(t)) that reweights the original probability measure. This transforms the original SDE into a new SDE with a drift term involving the score function ∇log h(t,ξ). The method is applied to butterfly shape evolution by modeling shapes as functions in a Hilbert space, discretizing via truncated Fourier basis, and learning the score function using score matching methods adapted from Heng et al. (2021). The approach allows conditioning Brownian motion between observed shapes and modeling evolutionary trajectories in shape space.

## Key Results
- The infinite-dimensional Doob's h-transform successfully conditions Brownian motion between observed butterfly shapes
- Discretization via truncated Fourier bases preserves conditioning properties for cylindrical sets
- The score learning method from Heng et al. (2021) can be adapted to learn the score function in infinite dimensions
- The method produces realistic evolutionary trajectories for butterfly shape evolution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The infinite-dimensional Doob's h-transform allows conditioning non-linear diffusion processes without prior discretization.
- Mechanism: The paper uses an infinite-dimensional version of Girsanov's theorem to construct a new probability measure via a strictly positive martingale Z(t) = h(t, X(t)), where h(t,ξ) = E[ψ(X(T-t,ξ))]. This martingale reweights the original probability measure to create the conditioned process.
- Core assumption: The function h(t,ξ) is twice Fréchet differentiable with respect to ξ and once differentiable with respect to t, with continuous derivatives.
- Evidence anchors:
  - [abstract] "To do so, we use an infinite-dimensional version of Girsanov's theorem to condition a function-valued stochastic process"
  - [section 5.1] "Theorem 5.1. Let h : [0,T] × H → R>0 be a continuous function twice Fréchet differentiable..."
- Break condition: If h(t,ξ) fails to satisfy the required differentiability conditions or if Z(t) is not a strictly positive martingale, the measure change becomes invalid.

### Mechanism 2
- Claim: The conditioned process satisfies a new SDE involving the score function ∇log h(t,ξ).
- Mechanism: By applying the infinite-dimensional Itô's lemma to h(t,X(t)) and using Girsanov's theorem, the original SDE is transformed into a new SDE where the drift term includes B(X(t))B*(X(t))∇log h(t,X(t)).
- Core assumption: The original SDE has a unique strong Markov solution that is twice Fréchet differentiable with respect to the initial value.
- Evidence anchors:
  - [section 4.2] "Doob's h-transform in finite dimensions is a useful theory for conditioning stochastic differential equations"
  - [section 5.1] "Theorem 5.1...we use the infinite-dimensional Girsanov's theorem, and rewrite the original SDE in terms of the resulting Wiener process"
- Break condition: If the solution lacks the required differentiability or if the infinite-dimensional Girsanov theorem conditions are not met, the SDE transformation fails.

### Mechanism 3
- Claim: Discretizing the conditioned process via truncated Fourier bases preserves the conditioning properties.
- Mechanism: The paper shows that conditioning on cylindrical sets in infinite dimensions (depending only on first N basis elements) is equivalent to conditioning the N-dimensional projection of the SDE onto the first N basis elements.
- Core assumption: The cylindrical sets ΓN satisfy Assumption 3.3 as long as the corresponding finite-dimensional sets Γi do.
- Evidence anchors:
  - [section 5.3] "Lemma 5.4. Let ΓN be as in Equation (16)...Then ⟨∇ log h(t, Y ), ei⟩ = [∇ log g(t, (Yi)N i=1)]i"
  - [section 5.2.1] "The proof that this h-transform does solve Problem 3.1 is similar to the finite-dimensional version"
- Break condition: If the basis expansion doesn't converge appropriately or if the conditioning sets don't preserve the necessary properties in the limit, the discretization breaks down.

## Foundational Learning

- Concept: Infinite-dimensional Itô's lemma and Girsanov's theorem
  - Why needed here: These are the fundamental tools that allow the construction of the conditioned process in infinite dimensions, analogous to how finite-dimensional versions work in standard SDE conditioning.
  - Quick check question: What are the key conditions required for the infinite-dimensional Girsanov theorem to apply?

- Concept: Fréchet differentiability in Hilbert spaces
  - Why needed here: The conditioning function h(t,ξ) must be twice Fréchet differentiable in the Hilbert space H to apply the infinite-dimensional Itô's lemma and construct the martingale Z(t).
  - Quick check question: How does Fréchet differentiability differ from standard differentiability when working with function-valued processes?

- Concept: Score matching and neural network approximation
  - Why needed here: Since the score function ∇log h(t,ξ) typically has no closed form, score matching methods are used to learn it from data, enabling practical sampling from the conditioned distribution.
  - Quick check question: What is the relationship between score matching and the denoising score matching loss used in Heng et al. [2021]?

## Architecture Onboarding

- Component map: Infinite-dimensional SDE model -> Conditioning function h(t,ξ) construction -> Score function ∇log h(t,ξ) approximation (neural network) -> Discretization module (Fourier basis truncation) -> Sampling module (Euler-Maruyama or similar) -> Evaluation module (comparison to ground truth)

- Critical path:
  1. Define the infinite-dimensional SDE model
  2. Construct appropriate conditioning function h(t,ξ)
  3. Approximate the score function using neural networks
  4. Discretize via truncated Fourier basis
  5. Sample from the conditioned process
  6. Evaluate against ground truth or alternative methods

- Design tradeoffs:
  - Number of Fourier basis elements vs. computational cost
  - Network architecture complexity vs. learning efficiency
  - Exact vs. inexact matching (whether to include observation noise)
  - Choice of kernel function for distance measurement

- Failure signatures:
  - Score function learning diverges or produces unrealistic trajectories
  - Conditioning fails to maintain shape constraints at endpoint
  - Numerical instability in discretization for higher dimensions
  - Training loss plateaus without convergence

- First 3 experiments:
  1. Verify the Brownian motion conditioning matches analytical solution for simple shapes
  2. Test conditioning performance as number of Fourier basis elements increases
  3. Compare exact vs. inexact matching for butterfly shape evolution with noisy observations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the conditioning method scale to higher-dimensional infinite-dimensional diffusion processes?
- Basis in paper: [inferred] The authors mention that the neural network architecture used for learning the score function did not scale well to higher-dimensional SDEs, and they had to use a different structure with a U-net architecture.
- Why unresolved: The paper does not provide experimental results or theoretical analysis of the method's performance as the dimensionality of the infinite-dimensional process increases.
- What evidence would resolve it: Experimental results demonstrating the method's performance on higher-dimensional infinite-dimensional diffusion processes, along with theoretical analysis of the computational complexity and convergence properties as the dimensionality increases.

### Open Question 2
- Question: How well does the learned time reversal approximate the forward bridge, or how can the forward bridge be learned directly?
- Basis in paper: [explicit] The authors state that they only do the first step in Heng et al. [2021] and learn the time reversal, and future work will consider how well the time reversal approximates the forward bridge or how to learn the forward bridge directly.
- Why unresolved: The paper does not provide experimental results or theoretical analysis of the accuracy of the learned time reversal in approximating the forward bridge, or alternative methods for learning the forward bridge directly.
- What evidence would resolve it: Experimental results comparing the accuracy of the learned time reversal in approximating the forward bridge, or theoretical analysis and experimental results of alternative methods for learning the forward bridge directly.

### Open Question 3
- Question: How can the infinite-dimensional bridges be extended to inference problems in phylogenetic inference for shapes of species?
- Basis in paper: [explicit] The authors mention that learning Doob's h-transform is only the first step in phylogenetic inference for shapes of species, and future work will consider how to expand the infinite-dimensional bridges to inference problems.
- Why unresolved: The paper does not provide a detailed discussion of how the infinite-dimensional bridges can be applied to phylogenetic inference problems, or experimental results demonstrating the method's performance in this context.
- What evidence would resolve it: A detailed discussion of how the infinite-dimensional bridges can be applied to phylogenetic inference problems, along with experimental results demonstrating the method's performance in this context.

## Limitations

- The method requires the conditioning function h(t,ξ) to satisfy strict differentiability conditions that may not hold for all diffusion processes
- Neural network architectures for learning the score function in high dimensions face scalability challenges
- The practical implementation details, particularly regarding network architecture choices, are not fully specified

## Confidence

- High: The theoretical framework of infinite-dimensional Doob's h-transform and its relationship to score-based conditioning
- Medium: The discretization approach via truncated Fourier bases, as the paper provides theoretical justification but limited empirical validation across different basis choices
- Low: The practical implementation details, particularly regarding neural network architecture choices and training procedures for high-dimensional score functions

## Next Checks

1. Test conditioning accuracy when varying the number of Fourier basis elements to identify the point where approximation error becomes significant.

2. Compare the proposed method against exact analytical solutions for simple conditioning problems (e.g., Brownian motion between two points) to validate implementation correctness.

3. Evaluate the method's robustness by adding varying levels of observation noise and measuring performance degradation.