---
ver: rpa2
title: Entropy-Regularized Token-Level Policy Optimization for Language Agent Reinforcement
arxiv_id: '2402.06700'
source_url: https://arxiv.org/abs/2402.06700
tags:
- policy
- etpo
- learning
- action
- train
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ETPO, an entropy-regularized reinforcement
  learning algorithm for optimizing language models at the token level. It addresses
  the challenge of aligning reinforcement learning objectives with language modeling
  and the instability caused by large action spaces.
---

# Entropy-Regularized Token-Level Policy Optimization for Language Agent Reinforcement

## Quick Facts
- arXiv ID: 2402.06700
- Source URL: https://arxiv.org/abs/2402.06700
- Authors: Muning Wen; Junwei Liao; Cheng Deng; Jun Wang; Weinan Zhang; Ying Wen
- Reference count: 26
- One-line primary result: ETPO achieves higher ROC AUC scores than PPO baseline on CodeLlama-7B for code generation while maintaining model capabilities

## Executive Summary
This paper introduces ETPO, an entropy-regularized reinforcement learning algorithm that optimizes language models at the token level. The method addresses the challenge of aligning RL objectives with language modeling by decomposing action-level optimization into token-level updates with theoretical consistency guarantees. ETPO is evaluated on a simulated data science code generation task, demonstrating improved performance over PPO baselines while maintaining the model's fundamental capabilities as measured by perplexity on benchmark datasets.

## Method Summary
ETPO is an entropy-regularized reinforcement learning algorithm that optimizes language models token-by-token rather than at the action level. The method uses per-token soft Bellman updates to learn Q-values for each token conditioned on previous tokens, and per-token policy updates that minimize KL divergence between the policy and the exponent of the learned soft Q-function. The algorithm incorporates a reference policy (set to the original language model) to constrain policy updates and maintain alignment with the original distribution. The approach is theoretically grounded with proof of optimization consistency when decomposing action-level updates into token-level updates.

## Key Results
- ETPO achieves higher ROC AUC scores on validation sets compared to PPO baseline for code generation tasks
- Model maintains fundamental capabilities with minor changes in perplexity on benchmark datasets (GitHub, Wikitext, arXiv)
- Training on 14 datasets (4 Kaggle + 10 OpenML) with CodeLlama-7B model shows stable performance over 500 environment steps

## Why This Works (Mechanism)

### Mechanism 1
Entropy regularization prevents the LLM policy from diverging too far from the original language model distribution. By incorporating a KL divergence term between the learned policy π and the reference policy ¯π (which is set to the original language model ρ), ETPO constrains the policy updates. This keeps the generated text aligned with the original language model's distribution while still allowing optimization for task-specific rewards. The core assumption is that the original language model distribution is a good reference point for maintaining linguistic coherence and diversity.

### Mechanism 2
Per-token soft Bellman updates decompose the Q-function from action-level to token-level, reducing the action space complexity and enabling fine-grained credit assignment. Instead of updating Q-values for entire actions, ETPO updates Q-values for each token conditioned on previous tokens. This reduces the action space from O(|V|^l) to O(|V| × l), making exploration more tractable. It also provides precise credit assignment by correlating each token generation directly with action performance.

### Mechanism 3
Per-token policy updates minimize the KL divergence between the policy and the exponent of the learned soft Q-function, leading to optimal token-level stochastic policies. After learning the token-level soft Q-function, the policy is updated toward minimizing the KL divergence between the policy and the optimal soft policy derived from the Q-function. This is done token-by-token, ensuring that each token generation is optimal with respect to the current Q-function.

## Foundational Learning

- **Concept**: Entropy-Regularized Reinforcement Learning (ERL)
  - Why needed here: ERL bridges the gap between reinforcement learning objectives (maximizing rewards) and language modeling objectives (modeling token distributions) by constraining policy updates with a KL divergence term.
  - Quick check question: How does the KL divergence term in the reward function of ERL prevent the policy from diverging too far from the original language model distribution?

- **Concept**: Soft Bellman Updates
  - Why needed here: Soft Bellman updates incorporate entropy regularization into the Q-function updates, ensuring that the policy maintains stochasticity and explores the action space effectively.
  - Quick check question: What is the difference between the standard Bellman update and the soft Bellman update in terms of the terms included in the update equation?

- **Concept**: Token-Level vs Action-Level Optimization
  - Why needed here: Language models generate actions as sequences of tokens, so optimizing at the token level is more natural and allows for finer-grained credit assignment compared to optimizing at the action level.
  - Quick check question: How does the action space complexity grow when considering actions as sequences of tokens versus single actions, and why is this a problem for reinforcement learning?

## Architecture Onboarding

- **Component map**: Interactive Environment -> LLM Policy -> Soft Q-Network -> Target Soft Q-Network -> Data Buffer -> Per-Token Soft Bellman Update -> Per-Token Policy Update

- **Critical path**: 1. LLM generates code token-by-token based on the current state. 2. Code is executed in the environment, yielding a reward and next state. 3. Trajectory is stored in the data buffer. 4. Sample a mini-batch from the buffer. 5. Update the soft Q-network using per-token soft Bellman updates. 6. Update the LLM policy using per-token policy updates. 7. Update the target soft Q-network with Polyak averaging. 8. Repeat steps 1-7 for a number of epochs.

- **Design tradeoffs**: Entropy regularization vs. pure reward maximization (maintains diversity and stability but may slow convergence); Token-level vs. action-level optimization (reduces complexity but requires more complex updates); Reference policy choice (maintains linguistic coherence but may limit exploration).

- **Failure signatures**: Training instability if policy diverges from original language model or Q-function estimates become inaccurate; Poor exploration if entropy regularization is too strong or reference policy too restrictive; Suboptimal policies if token-level updates fail to capture action-level dependencies or policy update step size is too large.

- **First 3 experiments**: 1. Run ETPO on a simple environment with known optimal policy (e.g., grid world) and verify it learns optimal policy while maintaining original language model distribution. 2. Compare ETPO with standard PPO baseline on data science code generation task and verify ETPO achieves higher ROC AUC scores and more stable training. 3. Analyze generated code by ETPO and verify it maintains linguistic coherence and diversity while achieving high rewards.

## Open Questions the Paper Calls Out

- Does ETPO's token-level optimization provide consistent improvements across different LLM architectures beyond CodeLlama-7B?
- What is the optimal balance between exploration and exploitation in ETPO's entropy regularization for different task complexities?
- How does ETPO's token-level credit assignment compare to hierarchical or multi-level credit assignment approaches in terms of sample efficiency?

## Limitations

- Experimental scope limited to single simulated data science code generation task using CodeLlama-7B model
- Missing implementation details including specific prompting templates and code execution error handling
- Limited hyperparameter sensitivity analysis and exploration of different reference policy choices

## Confidence

**High Confidence**:
- Entropy regularization in RL to maintain policy stability is well-established
- Decomposing complex action spaces into simpler components is a recognized strategy

**Medium Confidence**:
- Per-token soft Bellman updates application to language generation is theoretically sound but lacks extensive validation
- KL divergence minimization between policy and Q-function exponent is supported by SAC literature but needs domain verification

**Low Confidence**:
- Optimization consistency claim cannot be fully assessed without reviewing theoretical proof in appendix
- Assertion about maintaining model capabilities through minor perplexity changes needs more rigorous testing

## Next Checks

1. Review and validate the theoretical proof of optimization consistency when decomposing action-level Q-function updates into token-level updates.

2. Systematically vary key hyperparameters (entropy regularization coefficient, learning rate, KL divergence penalty coefficient) and measure their impact on training stability, final performance, and policy divergence from the original language model.

3. Implement ETPO on at least two additional language generation tasks (e.g., text summarization, dialogue generation) with different model architectures and compare performance against both PPO and direct supervised fine-tuning baselines.