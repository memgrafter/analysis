---
ver: rpa2
title: Infinite-Horizon Reinforcement Learning with Multinomial Logistic Function
  Approximation
arxiv_id: '2406.13633'
source_url: https://arxiv.org/abs/2406.13633
tags:
- lemma
- learning
- regret
- proceedings
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies reinforcement learning with multinomial logistic
  function approximation for both average-reward and discounted-reward infinite-horizon
  settings. The authors develop UCMNLK, a model-based algorithm that constructs confidence
  polytopes for the true transition function and runs discounted extended value iteration.
---

# Infinite-Horizon Reinforcement Learning with Multinomial Logistic Function Approximation

## Quick Facts
- arXiv ID: 2406.13633
- Source URL: https://arxiv.org/abs/2406.13633
- Reference count: 40
- This paper develops UCMNLK, a model-based algorithm achieving Õ(dD√T + κ⁻¹d²D) regret for average-reward MDPs and Õ(d(1-γ)⁻²√T + κ⁻¹d²(1-γ)⁻²) regret for discounted-reward MDPs using multinomial logistic function approximation.

## Executive Summary
This paper addresses reinforcement learning with multinomial logistic function approximation in infinite-horizon settings. The authors develop UCMNLK, a model-based algorithm that constructs confidence polytopes for transition probabilities and runs discounted extended value iteration. The key innovation is using confidence polytopes over probability distributions rather than parameter vectors, enabling efficient optimization despite the non-convexity of the multinomial logistic model. The algorithm achieves near-optimal regret bounds for both average-reward and discounted-reward MDPs, with regret scaling as Õ(d√T) plus terms dependent on the condition number κ.

## Method Summary
The paper proposes UCMNLK, a model-based reinforcement learning algorithm for MDPs with multinomial logistic function approximation. The algorithm maintains confidence polytopes over the true transition function and runs discounted extended value iteration to compute policies. For each round, it constructs confidence sets for transition probabilities using concentration inequalities, then solves a planning problem over these confidence sets to find an optimistic policy. The key technical innovation is the use of confidence polytopes over probability distributions rather than parameter vectors, which avoids the non-convex optimization problems that would arise from directly optimizing over the multinomial logistic parameters.

## Key Results
- Achieves Õ(dD√T + κ⁻¹d²D) regret for average-reward communicating MDPs with diameter D
- Achieves Õ(d(1-γ)⁻²√T + κ⁻¹d²(1-γ)⁻²) regret for discounted-reward MDPs with discount factor γ
- Provides lower bounds of Ω(d√DT) for average-reward MDPs and Ω(d(1-γ)³/²√T) for discounted-reward MDPs
- Introduces confidence polytopes over probability distributions as a novel approach to handle non-convexity in function approximation

## Why This Works (Mechanism)
The algorithm works by constructing confidence polytopes over transition probability distributions rather than optimizing directly over the multinomial logistic parameters. This approach sidesteps the non-convexity inherent in the multinomial logistic model while still capturing the uncertainty in the learned transition model. By using these confidence polytopes in conjunction with discounted extended value iteration, the algorithm can compute optimistic policies that balance exploration and exploitation. The regret bounds depend polynomially on the condition number κ, which captures the difficulty of learning the model parameters.

## Foundational Learning
- **Confidence polytopes**: Sets of probability distributions that contain the true transition function with high probability, constructed using concentration inequalities
  - *Why needed*: To capture uncertainty in learned transition models while maintaining computational tractability
  - *Quick check*: Verify that constructed polytopes contain true transitions with probability at least 1-δ

- **Multinomial logistic function approximation**: Parameterized model for transition probabilities that uses softmax over linear functions of features
  - *Why needed*: Provides expressive function approximation while maintaining structure amenable to theoretical analysis
  - *Quick check*: Confirm that model satisfies the required regularity conditions (e.g., condition number κ bounded)

- **Discounted extended value iteration**: Planning algorithm that computes value functions over confidence sets rather than point estimates
  - *Why needed*: Enables computation of optimistic policies that account for model uncertainty
  - *Quick check*: Verify convergence properties and that computed values are indeed optimistic

- **Communicating MDPs with bounded diameter**: MDPs where any state can reach any other state within D steps
  - *Why needed*: Ensures that exploration is possible and regret bounds can be expressed in terms of D
  - *Quick check*: Confirm that the diameter D is finite and known or estimable

## Architecture Onboarding

**Component Map**
Input features -> Multinomial logistic model -> Confidence polytope construction -> Discounted extended value iteration -> Optimistic policy computation -> Regret calculation

**Critical Path**
Feature extraction → Model learning → Confidence set construction → Planning → Policy execution

**Design Tradeoffs**
The main tradeoff is between model expressiveness (multinomial logistic allows rich representations) and computational complexity (confidence polytopes require solving optimization problems over sets). The paper chooses to prioritize theoretical guarantees over computational efficiency, which may limit practical applicability to large-scale problems.

**Failure Signatures**
- High computational cost due to maintaining and optimizing over confidence polytopes
- Poor performance when condition number κ is large (ill-conditioned models)
- Suboptimal exploration when diameter D is large
- Breakdown when multinomial logistic assumptions are violated

**First Experiments**
1. Compare UCMNLK against baselines on synthetic MDPs with known transition dynamics
2. Evaluate sensitivity to condition number κ by varying the difficulty of the logistic model
3. Test scalability by increasing state space dimension and measuring computational runtime

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Computational complexity may limit scalability to large-scale problems
- Requires access to a generative model or simulator, not applicable in pure online settings
- Regret bounds depend polynomially on diameter D, which could be prohibitive for large state spaces
- No empirical validation provided to verify practical performance

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical regret bounds for average-reward MDPs | High |
| Theoretical regret bounds for discounted-reward MDPs | High |
| Use of confidence polytopes as effective approach | High |
| Lower bounds matching upper bounds up to logarithmic factors | Medium |
| Practical applicability of model-based approach | Low |

## Next Checks
1. Implement UCMNLK and conduct empirical evaluations on benchmark MDPs to verify practical performance and computational feasibility
2. Extend the analysis to non-communicating MDPs and study how the regret bounds change with different structural assumptions
3. Investigate whether alternative function approximation methods (e.g., neural networks) can achieve similar theoretical guarantees while offering better practical scalability