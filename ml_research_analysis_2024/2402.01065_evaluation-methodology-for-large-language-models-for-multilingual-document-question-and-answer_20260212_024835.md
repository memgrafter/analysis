---
ver: rpa2
title: Evaluation Methodology for Large Language Models for Multilingual Document
  Question and Answer
arxiv_id: '2402.01065'
source_url: https://arxiv.org/abs/2402.01065
tags:
- language
- dataset
- translation
- multilingual
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates multilingual support in large language models
  for document question-answering tasks. The authors propose a methodology to test
  GPT-4 and GPT-3.5 models across multiple languages using datasets like XQuAD, SQuAD,
  ESG, and HeQ.
---

# Evaluation Methodology for Large Language Models for Multilingual Document Question and Answer

## Quick Facts
- arXiv ID: 2402.01065
- Source URL: https://arxiv.org/abs/2402.01065
- Reference count: 31
- Key outcome: GPT-4 outperforms GPT-3.5 in multilingual document QA, with partial translation (English context, translated question) yielding better results than full translation

## Executive Summary
This paper proposes a methodology to evaluate multilingual support in large language models for document question-answering tasks. The authors test GPT-4 and GPT-3.5 models across multiple languages using datasets like XQuAD, SQuAD, ESG, and HeQ. Their approach involves injecting translated context, questions, and answers into the pipeline, then verifying answers against ground truth. Key findings include GPT-4's superior performance across all languages, the effectiveness of partial translation strategies, and the benefit of translating documents to English before answering for non-English documents.

## Method Summary
The methodology involves evaluating GPT-4 and GPT-3.5 models for multilingual document QA by translating context, questions, and answers across multiple languages, then verifying answers against ground truth. The authors test various translation strategies including full translation (native language context, question, answer) and partial translation (English context, translated question). They use datasets XQuAD, SQuAD, ESG, and HeQ across languages including English, Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Dutch, and Hebrew. The evaluation flow uses prompt engineering with system prompts, translation services, and answer verification against ground truth.

## Key Results
- GPT-4 significantly outperforms GPT-3.5 across all tested languages
- Partial translation strategy (English context, translated question) yields better results than full translation
- Translating Hebrew documents to English before answering improves accuracy
- Operating in English with translation is preferable for multilingual scenarios despite added complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 outperforms GPT-3.5 in multilingual QA tasks
- Mechanism: GPT-4's larger model size and improved training allows better cross-lingual understanding and answer generation
- Core assumption: GPT-4's architecture enables superior multilingual performance compared to GPT-3.5
- Evidence anchors: [abstract] "Our preliminary results show that, translating the native language context, question and answer into a high resource language produced the best results."; [section 3] "We observe that there is a major leap in accuracy when using GPT-4."
- Break condition: If GPT-4's multilingual training data is insufficient for specific low-resource languages

### Mechanism 2
- Claim: Partial translation (English context, translated question) yields better results than full translation
- Mechanism: Reducing translation steps minimizes error accumulation and preserves semantic fidelity
- Core assumption: Each translation step introduces potential errors that compound in the QA pipeline
- Evidence anchors: [section 3] "We observe clearly for GPT-4 that partial translation works better than full translation, which advocated for the use of English."; [section 2.3] "We use a 'PDF to text' converter, which results in lots of long and noisy texts."
- Break condition: If source language context contains critical cultural/linguistic nuances lost in translation to English

### Mechanism 3
- Claim: Translating documents to English before answering improves accuracy for non-English documents
- Mechanism: English serves as a common semantic bridge, leveraging the model's strongest language capabilities
- Core assumption: The model's English training data provides the most robust semantic representations
- Evidence anchors: [section 3] "translating the data into English and performing the question answering in English yields even better results."; [section 2.3] "The translated text files are cleaner than the English ones, but may suffer from early-stopping of the LLM."
- Break condition: If source language contains idiomatic expressions or cultural references that don't translate well to English

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding how knowledge transfers between languages is crucial for evaluating multilingual QA performance
  - Quick check question: How does a model trained primarily on English data handle questions in other languages?

- Concept: Translation quality assessment
  - Why needed here: Evaluating the impact of translation quality on downstream QA performance requires understanding translation metrics
  - Quick check question: What metrics would you use to assess translation quality in a multilingual QA pipeline?

- Concept: Language resource availability
  - Why needed here: Understanding the disparity in available training data between high-resource and low-resource languages explains performance differences
  - Quick check question: How does the amount of available training data in a language correlate with LLM performance in that language?

## Architecture Onboarding

- Component map: LLM -> Translation Service -> Question Answering Module -> Answer Verification
- Critical path: Context -> Question -> Answer Generation -> Answer Verification
- Design tradeoffs: Translation accuracy vs. computational cost; model size vs. inference time
- Failure signatures: Inconsistent answers across languages; poor performance on low-resource languages; high variance in results
- First 3 experiments:
  1. Test GPT-4 vs GPT-3.5 on English QA to establish baseline performance
  2. Evaluate full translation vs partial translation for a single language pair
  3. Compare performance on native vs translated documents for a low-resource language

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4 compare to specialized multilingual models trained from scratch for low-resource languages?
- Basis in paper: [inferred] The paper shows GPT-4 performs decently on Hebrew but doesn't compare to specialized models
- Why unresolved: The study only evaluates GPT models against English benchmarks, not against other multilingual models
- What evidence would resolve it: Direct comparison studies testing GPT-4 against specialized multilingual models on the same low-resource language tasks

### Open Question 2
- Question: What is the impact of document length and complexity on translation quality and subsequent question-answering performance?
- Basis in paper: [explicit] The authors note that translated text files are cleaner but sometimes chopped, affecting performance
- Why unresolved: The paper observes this effect but doesn't systematically study how document characteristics affect outcomes
- What evidence would resolve it: Controlled experiments varying document length and complexity across languages while measuring both translation and QA accuracy

### Open Question 3
- Question: How does the performance of multilingual LLMs degrade over time as new languages and domains emerge?
- Basis in paper: [inferred] The authors note that most LLMs are trained on predominantly English data and may not keep up with evolving languages
- Why unresolved: The study is a snapshot evaluation without longitudinal tracking of model performance
- What evidence would resolve it: Regular re-evaluation of the same models and tasks over time with newly collected multilingual data

## Limitations

- Dataset Coverage Gaps: Relies on publicly available datasets that may not fully represent real-world multilingual document diversity
- Translation Quality Uncertainty: Assumes translation quality is sufficient but doesn't explicitly measure or report translation metrics
- Single-Model Focus: Results based only on GPT-4 and GPT-3.5 through Azure OpenAI service, without comparison to other LLMs

## Confidence

- High Confidence: GPT-4 consistently outperforms GPT-3.5 across all tested languages and scenarios
- Medium Confidence: Partial translation strategy yields better results than full translation, though effect size varies across languages
- Medium Confidence: Translating documents to English before answering improves accuracy, requires more rigorous testing to confirm mechanism

## Next Checks

1. Implement automated translation quality assessment (BLEU, COMET scores) to quantify how translation errors correlate with QA performance degradation across different language pairs
2. Test the same evaluation methodology on alternative LLMs (Claude, LLaMA variants) to determine if GPT-4's superiority is model-specific
3. Systematically vary document length, complexity, and noise levels in the ESG dataset to measure how these factors interact with translation strategy and affect QA accuracy across languages