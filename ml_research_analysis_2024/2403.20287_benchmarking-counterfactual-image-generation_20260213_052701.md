---
ver: rpa2
title: Benchmarking Counterfactual Image Generation
arxiv_id: '2403.20287'
source_url: https://arxiv.org/abs/2403.20287
tags:
- image
- counterfactual
- causal
- variables
- celeba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks counterfactual image generation methods using
  SCM-based approaches. The authors evaluate three model families (Normalising Flows,
  VAEs/HVAEs, and GANs) across three datasets (MorphoMNIST, CelebA, ADNI) using multiple
  metrics including composition, effectiveness, realism (FID), and minimality (CLD).
---

# Benchmarking Counterfactual Image Generation

## Quick Facts
- arXiv ID: 2403.20287
- Source URL: https://arxiv.org/abs/2403.20287
- Reference count: 40
- Primary result: HVAEs consistently outperform VAEs and GANs across most datasets and metrics for counterfactual image generation

## Executive Summary
This paper presents a comprehensive benchmark for counterfactual image generation methods using structural causal models (SCMs). The authors evaluate three model families - Normalizing Flows, VAEs/HVAEs, and GANs - across three datasets with varying complexity (MorphoMNIST, CelebA, ADNI). Using multiple evaluation metrics including composition, effectiveness, realism (FID), and minimality (CLD), the framework demonstrates that HVAEs consistently outperform other approaches, particularly excelling at maintaining composition and realism while implementing causal modifications. The work provides both theoretical foundations and practical implementation details, with the framework released as a user-friendly Python package for future extensions.

## Method Summary
The framework implements counterfactual image generation through an Abduction-Action-Prediction paradigm using SCMs. For each variable in the causal graph, separate models are trained: conditional Normalizing Flows for attribute mechanisms and conditional VAEs, HVAEs, or GANs for image mechanisms. The counterfactual inference process involves three steps - abducting the posterior noise from factual observations, intervening on specific variables while maintaining original noise, and predicting the counterfactual image. The framework evaluates generated counterfactuals using four metrics: composition (l1 distance and LPIPS), effectiveness (classification/regression metrics), realism (FID), and minimality (CLD). Models are trained independently for each variable and evaluated across three datasets with different causal graph complexities.

## Key Results
- HVAEs outperform VAEs and GANs across most datasets and metrics, particularly excelling at composition and realism
- VAEs and GANs show comparable performance, with VAEs producing blurry but structurally accurate reconstructions while GANs create sharp but sometimes structurally distorted images
- Diffusion models underperform compared to other approaches in their current implementation, suggesting architectural refinements are needed for counterfactual generation
- The framework demonstrates scalability across datasets ranging from synthetic (MorphoMNIST) to natural (CelebA) to medical (ADNI) images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HVAEs outperform other model families across most datasets and metrics due to their hierarchical latent structure
- Mechanism: The multiple stochastic layers in HVAEs allow for more optimal learning and maintenance of the prior of the latent variables from the data distribution, enabling better preservation of semantic information while implementing causal modifications
- Core assumption: The hierarchical structure aligns with the underlying causal graph and captures the complex relationships between high-level variables
- Evidence anchors:
  - [abstract]: "demonstrating the superiority of Hierarchical VAEs across most datasets and metrics"
  - [section 4.1]: "VAE and GAN were comparable, with VAE reconstructions maintaining structure but being too blurry and GAN changing image structure, especially in complex datasets. The ability of amortized explicit methods to retain pixel-level details can be attributed to the reparametrization they utilise"
  - [corpus]: Weak - corpus papers discuss counterfactual generation but don't specifically address hierarchical latent structures as the key mechanism
- Break condition: If the causal graph becomes too complex or the dataset contains variables with non-hierarchical relationships, the HVAE advantage may diminish

### Mechanism 2
- Claim: The Abduction-Action-Prediction paradigm enables accurate counterfactual inference by maintaining the original noise while intervening on specific variables
- Mechanism: By abducting the posterior noise from the factual observation and then intervening on specific variables while keeping the noise constant, the model generates counterfactuals that are both realistic and causally faithful
- Core assumption: The causal graph is known and there are no unobserved confounders (causal sufficiency)
- Evidence anchors:
  - [section 3.1]: "The variables x are observable, hence termed endogenous, while ϵ are unobservable, exogenous. Thanks to the causal interpretation of SCMs, we can compute interventional distributions"
  - [section 3.2]: "For each variable of the SCM examined, we train a model independently to perform noise abduction, considering conditional Normalising Flows for the parent attributes and conditional VAEs, HVAEs and GANs for the image"
  - [corpus]: Weak - corpus papers discuss counterfactual inference but don't specifically address the Abduction-Action-Prediction paradigm as the key mechanism
- Break condition: If there are unobserved confounders or the causal graph is incorrectly specified, the counterfactuals generated may not be causally faithful

### Mechanism 3
- Claim: The combination of multiple evaluation metrics provides a comprehensive assessment of counterfactual image generation quality
- Mechanism: By using metrics that evaluate composition, effectiveness, realism, and minimality, the framework captures different aspects of counterfactual quality, ensuring that generated images are not only realistic but also causally faithful and minimally changed
- Core assumption: No single metric can adequately capture all aspects of counterfactual quality
- Evidence anchors:
  - [abstract]: "We incorporate several metrics that assess diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism"
  - [section 3.3]: "The evaluation of counterfactual inference has been formalised through the axioms of (13; 15). Monteiro et al. (35) utilise such an axiomatic definition to introduce three metrics to evaluate image counterfactuals: Composition, Effectiveness, and Reversibility"
  - [corpus]: Weak - corpus papers discuss evaluation metrics but don't specifically address the combination of multiple metrics as the key mechanism
- Break condition: If the metrics are not properly aligned with the causal graph or if they introduce bias, the evaluation may not accurately reflect the quality of counterfactuals

## Foundational Learning

- Concept: Structural Causal Models (SCM) and causal graphs
  - Why needed here: Understanding how variables relate to each other causally is fundamental to generating counterfactuals that respect these relationships
  - Quick check question: Can you explain the difference between observational and interventional distributions in the context of SCMs?

- Concept: Variational Autoencoders (VAEs) and their extensions
  - Why needed here: VAEs and HVAEs are key model families used for counterfactual generation, and understanding their mechanisms is crucial for implementing and extending the framework
  - Quick check question: How does the reparameterization trick enable backpropagation through stochastic layers in VAEs?

- Concept: Normalizing Flows and their invertibility
  - Why needed here: Normalizing Flows are used for attribute mechanisms in the framework, and their invertibility is crucial for the Abduction-Action-Prediction paradigm
  - Quick check question: Why are Normalizing Flows particularly well-suited for modeling attribute mechanisms compared to other generative models?

## Architecture Onboarding

- Component map:
  Data preprocessing -> SCM definition -> Attribute mechanisms (Normalizing Flows) -> Image mechanisms (VAE/HVAE/GAN) -> Abduction-Action-Prediction pipeline -> Evaluation metrics -> Result aggregation

- Critical path:
  1. Load and preprocess dataset
  2. Define SCM and causal graph
  3. Train attribute mechanisms (Normalizing Flows)
  4. Train image mechanisms (VAE/HVAE/GAN)
  5. Perform counterfactual inference using Abduction-Action-Prediction
  6. Evaluate counterfactuals using multiple metrics
  7. Aggregate and analyze results

- Design tradeoffs:
  - Model complexity vs. training time: HVAEs are more expressive but require more training time
  - Evaluation metric comprehensiveness vs. computational cost: Using multiple metrics provides a thorough assessment but increases computational requirements
  - Dataset size vs. model performance: Larger datasets generally lead to better performance but require more resources

- Failure signatures:
  - Poor composition: Model fails to reconstruct original image during null-intervention
  - Low effectiveness: Interventions do not successfully change the targeted attributes
  - Unrealistic counterfactuals: Generated images have poor FID scores or obvious artifacts
  - Non-minimal changes: Counterfactuals differ significantly from the original image beyond the intervened attributes

- First 3 experiments:
  1. Run composition evaluation on MorphoMNIST with all three model families to assess baseline reconstruction quality
  2. Test effectiveness on CelebA (simple graph) by intervening on smiling attribute and measuring classifier performance
  3. Evaluate realism on ADNI by computing FID scores between counterfactuals and original dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications to diffusion models would enable them to perform competitively with HVAEs for counterfactual image generation in complex causal graphs?
- Basis in paper: [explicit] The authors state that diffusion models "under-perform compared to the other approaches" and suggest that "architectural and hyperparameter choices may need refinement for counterfactual image generation"
- Why unresolved: The paper only provides preliminary results using a cross-attention conditioning mechanism, which they acknowledge is a "naive approach" that doesn't enable fair comparison with other models
- What evidence would resolve it: Developing and testing novel diffusion model architectures specifically designed for multi-variable SCM conditioning, with systematic ablation studies on architectural choices and hyperparameters

### Open Question 2
- Question: How does the performance gap between HVAEs and other models scale with increasing causal graph complexity and dataset resolution?
- Basis in paper: [inferred] The authors observe that HVAEs "outperform other models across most datasets and metrics" and show particular advantage "as data complexity increases (CelebA, ADNI)" but their resolution experiments on CelebAHQ failed
- Why unresolved: The paper's scaling experiments were limited by HVAE's inability to handle 256x256 resolution, preventing systematic comparison across graph complexities
- What evidence would resolve it: Systematic benchmarking of all model families across a graduated series of causal graph complexities and resolutions, with performance metrics tracking the relative advantage of HVAEs

### Open Question 3
- Question: What is the optimal balance between realism and minimality in counterfactual image generation, and how should this trade-off be quantified?
- Basis in paper: [explicit] The authors identify that "introducing more attributes to condition the HVAE (complex graph on CelebA) affected image realism" while noting that GANs maintained good performance, suggesting a potential trade-off
- Why unresolved: The paper uses separate metrics (FID for realism, CLD for minimality) without addressing how these should be weighted or balanced against each other
- What evidence would resolve it: User studies evaluating human perception of counterfactual images, or development of composite metrics that capture the trade-off between realism and minimality in a principled way

## Limitations

- Assumes causal sufficiency - all confounders must be observed and included in the SCM
- Focuses on relatively simple causal graphs (depth ≤ 3), leaving uncertainty about performance on more complex structures
- Diffusion model implementation remains under-optimized, suggesting current results may not reflect the full potential of this architecture family

## Confidence

- **High** for the superiority of HVAEs on composition and realism metrics, supported by consistent results across three diverse datasets
- **Medium** for effectiveness claims, as classifier-based evaluation may be sensitive to dataset-specific factors
- **Low** for diffusion model comparisons, given the explicit acknowledgment that architectural choices and hyperparameters were not optimized for this task

## Next Checks

1. Test the framework on a more complex causal graph (depth > 3) to evaluate scalability limits of each model family
2. Implement optimized diffusion model architectures specifically designed for counterfactual generation to provide a fairer comparison
3. Conduct ablation studies on the conditional architectures to isolate which components contribute most to counterfactual quality