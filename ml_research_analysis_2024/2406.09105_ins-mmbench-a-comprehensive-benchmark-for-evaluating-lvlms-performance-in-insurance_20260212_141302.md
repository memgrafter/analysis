---
ver: rpa2
title: 'INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs'' Performance
  in Insurance'
arxiv_id: '2406.09105'
source_url: https://arxiv.org/abs/2406.09105
tags:
- insurance
- lvlms
- tasks
- arxiv
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces INS-MMBench, the first comprehensive benchmark\
  \ for evaluating Large Vision-Language Models (LVLMs) in the insurance domain. The\
  \ authors systematically review multimodal tasks across four types of insurance\u2014\
  auto, property, health, and agricultural\u2014and develop a three-level hierarchical\
  \ framework: fundamental tasks (recognizing individual visual elements), meta-tasks\
  \ (combining multiple elements), and scenario tasks (real-world use cases)."
---

# INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance in Insurance

## Quick Facts
- arXiv ID: 2406.09105
- Source URL: https://arxiv.org/abs/2406.09105
- Reference count: 40
- Primary result: First comprehensive benchmark evaluating LVLMs across 22 fundamental, 12 meta-, and 5 scenario insurance tasks using 12,052 images and 10,372 questions

## Executive Summary
This paper introduces INS-MMBench, the first comprehensive benchmark designed to evaluate Large Vision-Language Models (LVLMs) in the insurance domain. The benchmark systematically covers four insurance types—auto, property, health, and agricultural—through a three-level hierarchical framework: fundamental tasks (recognizing individual visual elements), meta-tasks (combining multiple elements), and scenario tasks (real-world use cases). When evaluating 11 leading LVLMs including GPT-4o and LLaVA, no model achieved over 70% accuracy on fundamental tasks, and performance on complex scenario tasks fell below 50% in most cases. The study reveals that LVLMs perform better in auto and health insurance tasks than in property and agricultural ones, while highlighting the narrowing performance gap between open-source and closed-source models.

## Method Summary
The benchmark construction follows a bottom-up hierarchical methodology using public datasets curated under expert guidance. The authors systematically review multimodal tasks across four insurance types, develop 22 fundamental tasks (object recognition and basic understanding), 12 meta-tasks (compositional understanding of multiple visual elements), and 5 scenario tasks (real-world insurance applications requiring multi-step reasoning). Public datasets are filtered and sampled to match real insurance scenarios, with expert-designed questions aligned to insurance workflows and distractors generated to increase task difficulty. Eleven LVLMs including GPT-4o, Qwen-VL-Max, Gemini 1.5 Flash, and LLaVA are evaluated using the VLMEvalKit toolkit with exact matching and LLM-based answer extraction.

## Key Results
- No LVLM achieved over 70% accuracy on fundamental tasks, with best performance at 66.7% by GPT-4o
- Scenario task accuracy fell below 50% for most models, with best performance at 48.2% by Qwen-VL-Max
- LVLMs performed better in auto and health insurance tasks than in property and agricultural insurance
- Open-source models showed performance approaching closed-source models in certain tasks
- Models achieved 64.2% accuracy on fundamental tasks but only 41.3% on meta-tasks and 41.7% on scenario tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The hierarchical task framework (fundamental → meta → scenario) enables progressive assessment of LVLMs by first isolating basic perception, then compositional reasoning, and finally multi-step real-world insurance workflows.
- **Mechanism:** Fundamental tasks test object recognition and basic visual understanding; meta-tasks combine multiple visual elements requiring integration; scenario tasks demand reasoning across multiple steps and insurance domain knowledge.
- **Core assumption:** Insurance tasks can be decomposed into these three layers without losing task fidelity.
- **Evidence anchors:**
  - [abstract] "structured into three levels: fundamental task, which focuses on the understanding of individual insurance-related visual elements; meta-task, which involves the compositional understanding of multiple insurance-related visual elements; and scenario task, which pertains to real-world insurance tasks requiring multi-step reasoning and decision-making."
  - [section] "This framework is developed using a bottom-up hierarchical methodology and is structured into three levels: Fundamental task focuses on recognizing and understanding an individual insurance-related visual element. Meta-task combines multiple related fundamental tasks to enable integrated understanding and reasoning of interconnected visual elements. Scenario task simulates real-world insurance application that requires multi-step reasoning and decision-making."
  - [corpus] Weak evidence; no directly comparable hierarchical insurance benchmark found.
- **Break condition:** If insurance workflows cannot be cleanly decomposed into these levels, or if task dependencies violate the assumed independence, the progressive assessment loses validity.

### Mechanism 2
- **Claim:** Domain-specific benchmark construction using publicly available datasets curated under expert guidance provides a realistic test bed for LVLMs in insurance tasks.
- **Mechanism:** Public datasets are filtered and sampled to match real insurance scenarios; experts design questions aligned to insurance workflows; distractors generated to increase task difficulty.
- **Core assumption:** Public datasets, when properly curated and filtered, can approximate real insurance scenarios sufficiently for benchmarking.
- **Evidence anchors:**
  - [abstract] "INS-MMBench includes a total of 12,052 images, 10,372 thoroughly designed questions (including multiple-choice visual questions and free-text visual questions), comprehensively covering 22 fundamental tasks, 12 meta-tasks, and 5 scenario tasks, aligned with key stages of the insurance process, including underwriting, risk monitoring, and claims processing."
  - [section] "We search for datasets relevant to each task in our framework by issuing task-specific keywords across several popular public sources... When multiple datasets are available for a task, we compare candidates using usage statistics and community feedback, and retain those that best match our needs."
  - [corpus] Weak evidence; no comparable insurance benchmark using public datasets was found.
- **Break condition:** If public datasets fail to capture domain-specific nuances, or if expert-guided question design introduces bias, the benchmark validity is compromised.

### Mechanism 3
- **Claim:** Evaluating both closed-source and open-source LVLMs on the same benchmark reveals performance gaps and narrowing differences, informing cost-effectiveness of open-source adoption.
- **Mechanism:** Comprehensive evaluation across model types (11 LVLMs) measures accuracy per task type; comparison identifies where open-source models approach closed-source performance.
- **Core assumption:** Direct comparison on the same benchmark fairly reflects model capabilities.
- **Evidence anchors:**
  - [abstract] "We benchmark 11 leading LVLMs, including closed-source models such as GPT-4o and open-source models like LLaVA."
  - [section] "Furthermore, we select 11 LVLMs for evaluation and conduct a comprehensive analysis of the results... (3) The gap between open-source and closed-source LVLMs is shrinking, with some open-source models now approaching or even surpassing the capabilities of closed-source models in some tasks."
  - [corpus] Weak evidence; no directly comparable study evaluating open vs closed LVLMs on insurance tasks was found.
- **Break condition:** If evaluation methodology introduces bias toward certain model types, or if the benchmark fails to reflect real-world deployment constraints, conclusions about cost-effectiveness may be invalid.

## Foundational Learning

- **Concept:** Multimodal task decomposition in specialized domains
  - **Why needed here:** Understanding how visual perception, reasoning, and domain knowledge combine is critical for designing effective insurance benchmarks.
  - **Quick check question:** Can you explain why insurance claim processing requires both object detection (damage severity) and domain reasoning (eligibility rules)?

- **Concept:** Benchmark construction pipeline (data collection → filtering → question design)
  - **Why needed here:** Building INS-MMBench required selecting, cleaning, and labeling multimodal datasets to create a realistic test bed.
  - **Quick check question:** What are the key steps in transforming raw public datasets into benchmark-ready insurance tasks?

- **Concept:** Error analysis methodology for multimodal models
  - **Why needed here:** Understanding why LVLMs fail (perception errors, lack of knowledge, refusal to answer) informs future model improvements.
  - **Quick check question:** How would you categorize a failure where the model recognizes a vehicle but misclassifies damage severity?

## Architecture Onboarding

- **Component map:** Data ingestion (public datasets) → Expert filtering → Question/answer generation → LVLM evaluation → Error analysis
- **Critical path:** Dataset selection → Expert question design → Benchmark assembly → Model evaluation → Result interpretation
- **Design tradeoffs:** Using public datasets trades privacy and realism for accessibility and reproducibility; expert-guided question design trades bias risk for domain alignment.
- **Failure signatures:** Low accuracy across all tasks suggests dataset mismatch or overly difficult tasks; high perception errors suggest visual encoder limitations; high knowledge errors suggest insufficient domain adaptation.
- **First 3 experiments:**
  1. Load and visualize a sample of images from each fundamental task to verify dataset relevance.
  2. Run a single LVLM on a small subset of tasks and manually inspect outputs to confirm evaluation pipeline correctness.
  3. Perform ablation by removing distractor generation and re-evaluating to check impact on model scores.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can domain-specific multimodal reasoning datasets be designed to improve LVLMs' performance on complex insurance tasks?
- **Basis in paper:** [explicit] The paper states that "Future improvements may come from constructing domain-specific multimodal reasoning datasets for the insurance field and applying reinforcement learning to address reasoning deficiencies."
- **Why unresolved:** While the paper identifies this as a potential direction, it doesn't provide specific methodologies or examples for designing such datasets.
- **What evidence would resolve it:** Published research demonstrating a novel methodology for creating domain-specific multimodal reasoning datasets, along with empirical results showing performance improvements in LVLMs on insurance tasks.

### Open Question 2
- **Question:** What is the optimal balance between open-source and closed-source LVLMs for different types of insurance applications?
- **Basis in paper:** [explicit] The paper notes that "The performance gap between open-source and closed-source LVLMs is narrowing" and that "fine-tuned open-source models may offer a cost-effective and data-secure alternative for insurance applications."
- **Why unresolved:** The paper doesn't provide a framework for determining when to use open-source versus closed-source models, or how to optimize their combination.
- **What evidence would resolve it:** A comprehensive study comparing cost-benefit analyses of open-source versus closed-source models across various insurance applications, with clear guidelines for selection.

### Open Question 3
- **Question:** How can Retrieval-Augmented Generation (RAG) be effectively integrated with LVLMs to enhance their reasoning capabilities in insurance tasks?
- **Basis in paper:** [inferred] The paper mentions that "Future work may explore fine-tuning LVLMs on domain-specific data or integrating Retrieval-Augmented Generation (RAG), which could enable models to incorporate external knowledge and enhance their reasoning abilities."
- **Why unresolved:** While RAG is suggested as a potential solution, the paper doesn't explore specific implementation strategies or evaluate its effectiveness in the insurance domain.
- **What evidence would resolve it:** Research demonstrating a successful integration of RAG with LVLMs for insurance tasks, showing measurable improvements in reasoning performance and practical applicability.

## Limitations
- Benchmark relies on public datasets curated under expert guidance, which may not fully capture real-world insurance domain nuances and could introduce bias in task representation.
- Evaluation focuses primarily on accuracy metrics without deeper analysis of model calibration or robustness to adversarial inputs.
- Error analysis framework identifies three failure types but lacks systematic investigation into root causes and potential mitigation strategies.

## Confidence
- **High Confidence:** The hierarchical framework structure (fundamental → meta → scenario tasks) and the comprehensive dataset collection (12,052 images, 10,372 questions) are well-documented and verifiable.
- **Medium Confidence:** Performance comparisons between open-source and closed-source models are valid, though the exact prompt engineering methodology for prompt augmentation is insufficiently detailed for exact replication.
- **Low Confidence:** Generalization of findings to real-world insurance applications is uncertain due to the benchmark's reliance on curated public datasets rather than actual insurance claims data.

## Next Checks
1. **Dataset Fidelity Check:** Compare the distribution of images and tasks in INS-MMBench against real insurance claim datasets to assess domain representation gaps.
2. **Error Root Cause Analysis:** Conduct systematic ablation studies on model failures to distinguish between perception limitations, knowledge gaps, and reasoning failures.
3. **Generalization Test:** Evaluate LVLMs on a small set of actual insurance claims (with appropriate privacy safeguards) to validate benchmark relevance to real-world applications.