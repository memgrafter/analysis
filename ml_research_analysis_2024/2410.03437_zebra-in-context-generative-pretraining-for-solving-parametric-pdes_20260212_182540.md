---
ver: rpa2
title: 'Zebra: In-Context Generative Pretraining for Solving Parametric PDEs'
arxiv_id: '2410.03437'
source_url: https://arxiv.org/abs/2410.03437
tags:
- time
- zebra
- example
- mean
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Zebra introduces a generative auto-regressive transformer for solving
  parametric PDEs through in-context learning, eliminating the need for gradient-based
  adaptation at inference. It leverages a VQ-VAE to encode physical observations into
  discrete tokens, then pretrains a transformer on next-token prediction using context
  trajectories with shared dynamics but different initial conditions.
---

# Zebra: In-Context Generative Pretraining for Solving Parametric PDEs

## Quick Facts
- arXiv ID: 2410.03437
- Source URL: https://arxiv.org/abs/2410.03437
- Authors: Louis Serrano; Armand Kassaï Koupaï; Thomas X Wang; Pierre Erbacher; Patrick Gallinari
- Reference count: 40
- Primary result: Generative auto-regressive transformer achieves 0.00794-0.119 relative L2 error on 1D/2D PDEs without gradient-based adaptation

## Executive Summary
Zebra introduces a generative auto-regressive transformer for solving parametric PDEs through in-context learning, eliminating the need for gradient-based adaptation at inference. It leverages a VQ-VAE to encode physical observations into discrete tokens, then pretrains a transformer on next-token prediction using context trajectories with shared dynamics but different initial conditions. The model dynamically adapts to new PDE instances by conditioning on example trajectories, enabling uncertainty quantification and trajectory generation. Evaluated across 1D and 2D PDE benchmarks, Zebra consistently outperforms gradient-based adaptation baselines (CODA, CAPE, ViT variants) in one-shot and out-of-distribution settings.

## Method Summary
Zebra combines a VQ-VAE for discrete encoding of physical states with a transformer trained on next-token prediction using context sequences. The VQ-VAE compresses continuous physical states into discrete tokens that preserve essential dynamics information. The transformer is pretrained on sequences combining multiple trajectories with shared underlying dynamics but different initial conditions. During inference, new queries are concatenated with context trajectories, and the model generates predictions conditioned on this combined sequence without gradient updates. An accelerated inference variant replaces token-wise autoregressive generation with a UNet surrogate conditioned on a context embedding, achieving up to 150× speedup while maintaining competitive accuracy.

## Key Results
- Relative L2 errors as low as 0.00794 (Advection) and 0.119 (Vorticity 2D) across 1D and 2D benchmarks
- Outperforms gradient-based adaptation baselines (CODA, CAPE, ViT variants) in one-shot and out-of-distribution settings
- Achieves uncertainty quantification through generative sampling, with strong calibration on Advection and Combined equations
- Accelerated inference variant maintains competitive accuracy while providing up to 150× speedup

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning allows the model to adapt to new PDE instances without gradient updates at inference.
- Mechanism: The transformer is pretrained on sequences that combine multiple trajectories with shared dynamics but different initial conditions, using a next-token prediction objective. During inference, new queries are concatenated with context trajectories, and the model generates predictions conditioned on this combined sequence.
- Core assumption: The context trajectories share the same underlying PDE dynamics as the query, so the model can infer the correct dynamics from the examples.
- Evidence anchors:
  - [abstract] "By leveraging in-context information during both pre-training and inference, Zebra dynamically adapts to new tasks by conditioning on input sequences that incorporate context example trajectories."
  - [section 3.2] "We design sequences that enable Zebra to perform in-context learning on trajectories that share underlying dynamics with different initial states."
  - [corpus] Weak - the corpus papers focus on generative auto-regressive methods but do not explicitly discuss in-context learning adaptation without gradient updates.
- Break condition: If the context trajectories do not share the same underlying dynamics as the query, the model will generate incorrect predictions.

### Mechanism 2
- Claim: Generative modeling with a VQ-VAE allows uncertainty quantification and trajectory generation.
- Mechanism: The VQ-VAE compresses physical states into discrete tokens, which the transformer models as a probability distribution. By sampling from this distribution at inference, the model can generate multiple plausible trajectories and compute statistics like mean and standard deviation.
- Core assumption: The discrete token space adequately captures the essential features of the physical dynamics for both reconstruction and generation.
- Evidence anchors:
  - [abstract] "As a generative model, Zebra can be used to generate new trajectories and allows quantifying the uncertainty of the predictions."
  - [section 3.4] "At inference, we adjust the temperature parameter τ of the classifier layer to calibrate the level of diversity of the next-token distributions."
  - [section 4.4] "Given a context example and an initial condition, Zebra can generate multiple trajectories thanks to the sampling operation at the classifier level."
- Break condition: If the VQ-VAE reconstruction quality is poor, the generated trajectories will not accurately represent the true dynamics.

### Mechanism 3
- Claim: The accelerated inference variant using a UNet surrogate achieves significant speedup while maintaining accuracy.
- Mechanism: Instead of token-wise autoregressive generation, a UNet conditioned on a context embedding predicts entire frames at once. The context embedding is extracted from the transformer's output using a special [DYN] token.
- Core assumption: The context embedding captures sufficient information about the underlying dynamics to condition the UNet effectively.
- Evidence anchors:
  - [abstract] "An accelerated inference variant using a UNet surrogate achieves up to 150× speedup while maintaining competitive accuracy."
  - [section 4.5] "Instead of token-wise autoregressive generation, we predict entire frames at once. This is achieved by replacing the token-wise autoregressive generation process with a frame-wise autoregressive surrogate, implemented as a UNet."
  - [section 4.5] "The UNet, conditioned on a context embedding output by the transformer, takes frame ut as input and predicts ˆut+∆t."
- Break condition: If the UNet cannot accurately predict frames from the context embedding, the accelerated inference will produce poor results.

## Foundational Learning

- Concept: Vector quantization and discrete representations
  - Why needed here: The transformer requires discrete inputs, and the VQ-VAE provides a way to convert continuous physical states into discrete tokens while preserving essential information.
  - Quick check question: Can you explain how the VQ-VAE encoder maps continuous physical states to discrete codes, and how the decoder reconstructs them?

- Concept: In-context learning and few-shot adaptation
  - Why needed here: The core innovation is adapting to new PDE instances using only example trajectories, without gradient updates at inference.
  - Quick check question: How does concatenating context trajectories with a query initial condition allow the model to adapt to new dynamics?

- Concept: Autoregressive generation and sampling
  - Why needed here: The model generates trajectories token by token, and sampling from the predicted distributions enables uncertainty quantification.
  - Quick check question: How does adjusting the temperature parameter τ affect the diversity and accuracy of the generated trajectories?

## Architecture Onboarding

- Component map: Physical states -> VQ-VAE encoder -> discrete tokens -> transformer -> predicted tokens -> VQ-VAE decoder -> reconstructed states
- Critical path:
  1. Tokenize context trajectories and query initial condition
  2. Concatenate sequences with special tokens
  3. Generate tokens autoregressively or use UNet for acceleration
  4. Detokenize generated tokens to physical space
- Design tradeoffs:
  - VQ-VAE codebook size vs reconstruction quality vs transformer learning
  - Transformer depth and width vs computational cost vs performance
  - Temperature parameter vs prediction accuracy vs uncertainty calibration
  - UNet conditioning complexity vs inference speedup
- Failure signatures:
  - Poor reconstruction → VQ-VAE issues or codebook size too small
  - Unstable training → Next-token objective not suitable for continuous latents
  - Slow inference → Token-by-token generation bottleneck
  - Inaccurate predictions → Context trajectories not representative or transformer underfitting
- First 3 experiments:
  1. Train VQ-VAE on 1D dataset and verify reconstruction quality on test set
  2. Train transformer with next-token objective on tokenized trajectories and evaluate next-token prediction accuracy
  3. Implement inference pipeline with context conditioning and test on one-shot adaptation task

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of Zebra scale with increasing numbers of environments and trajectories per environment in training data?
  - Basis in paper: [explicit] The paper notes that "Zebra requires a substantial amount of data to generalize effectively" and provides scaling analysis results showing performance plateaus between 100 and 1,000 trajectories for the Combined equation.
  - Why unresolved: The paper only tests up to 12,000 trajectories total. Scaling beyond this point could reveal whether performance continues improving, plateaus, or potentially degrades.
  - What evidence would resolve it: Systematic experiments training Zebra on datasets with 10×, 100×, and 1000× more trajectories than tested, measuring one-shot adaptation accuracy across different PDE types.

- **Open Question 2**: Can Zebra's uncertainty quantification capabilities be extended to provide well-calibrated probabilistic forecasts across all PDE types?
  - Basis in paper: [explicit] The paper demonstrates uncertainty quantification for Advection and Combined equations with strong calibration (high confidence levels), but shows poorer performance for Burgers and Heat.
  - Why unresolved: The paper only tests temperature calibration on four datasets and does not explore alternative uncertainty quantification methods beyond simple sampling statistics.
  - What evidence would resolve it: Testing alternative uncertainty quantification approaches (e.g., ensemble methods, Bayesian variants) across all PDE types, measuring CRPS and RMSCE consistently.

- **Open Question 3**: How would Zebra perform on irregularly sampled data or complex geometric domains?
  - Basis in paper: [inferred] The paper acknowledges this as a limitation: "Our current encoder and decoder rely on convolutional blocks, which limits the architecture to data defined on regular grids."
  - Why unresolved: The paper does not test Zebra on non-regular grids or complex geometries, leaving open whether the in-context learning capabilities transfer to these scenarios.
  - What evidence would resolve it: Experiments replacing convolutional encoders/decoders with graph neural networks or mesh-based architectures, testing one-shot adaptation on irregular grid data and complex geometries.

## Limitations
- Performance relies on synthetic datasets with controlled parametric variations, limiting validation on real-world PDEs with complex couplings and stochastic forcing
- Accelerated UNet variant introduces additional approximation layer with unclear long-term stability and accuracy guarantees for extended trajectories
- Uncertainty quantification shows strong calibration for some PDE types but poorer performance for others, requiring further validation across diverse scenarios

## Confidence
- **High Confidence**: The core mechanism of using in-context learning for PDE adaptation without gradient updates is well-supported by the experimental results, particularly the consistent outperformance over gradient-based baselines across all tested datasets.
- **Medium Confidence**: The uncertainty quantification capabilities through generative sampling are demonstrated, but the calibration and reliability of these uncertainty estimates for critical decision-making require further validation on more diverse and challenging PDE scenarios.
- **Medium Confidence**: The 150× speedup claim for accelerated inference is impressive but based on a single variant that may not generalize to all PDE types, particularly those with complex dynamics requiring fine-grained temporal resolution.

## Next Checks
1. **Robustness Test**: Evaluate Zebra on a heterogeneous dataset combining multiple PDE types with varying dimensionality to assess cross-domain generalization and identify potential failure modes when dynamics fundamentally differ between context and query trajectories.

2. **Uncertainty Calibration**: Perform extensive uncertainty calibration studies by varying the number and quality of context trajectories, measuring how prediction confidence correlates with actual error across different PDE regimes, and testing whether the model appropriately expresses uncertainty in out-of-distribution scenarios.

3. **Long-Horizon Stability**: Conduct systematic analysis of error accumulation during extended trajectory generation for the accelerated UNet variant, comparing its long-term stability against the autoregressive baseline across multiple time horizons to quantify the tradeoff between speed and accuracy degradation.