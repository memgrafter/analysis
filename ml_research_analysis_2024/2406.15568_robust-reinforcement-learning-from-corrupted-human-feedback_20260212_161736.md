---
ver: rpa2
title: Robust Reinforcement Learning from Corrupted Human Feedback
arxiv_id: '2406.15568'
source_url: https://arxiv.org/abs/2406.15568
tags:
- learning
- preference
- reward
- human
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of robustness in reinforcement\
  \ learning from human feedback (RLHF) when human annotators provide incorrect or\
  \ inconsistent preference labels. The authors propose a method called R3M (Robust\
  \ Reward Modeling for RLHF) that models corrupted preference labels as sparse outliers\
  \ and learns the reward function and perturbation factors simultaneously using \u2113\
  1-regularized maximum likelihood estimation."
---

# Robust Reinforcement Learning from Corrupted Human Feedback

## Quick Facts
- **arXiv ID**: 2406.15568
- **Source URL**: https://arxiv.org/abs/2406.15568
- **Reference count**: 40
- **Primary result**: R3M achieves robust reward learning in RLHF by identifying and mitigating corrupted preference labels through ℓ1-regularized maximum likelihood estimation with alternating optimization

## Executive Summary
This paper addresses the critical challenge of robustness in reinforcement learning from human feedback (RLHF) when preference labels contain errors or inconsistencies. The authors propose R3M (Robust Reward Modeling for RLHF), which models corrupted preference labels as sparse outliers and learns both the reward function and perturbation factors simultaneously. By formulating the problem as ℓ1-regularized maximum likelihood estimation, R3M can identify corrupted data points while maintaining theoretical guarantees and computational efficiency. The method demonstrates significant improvements over standard RLHF approaches on both robotic control tasks with synthetic noise and natural language generation tasks with real-world preference data.

## Method Summary
R3M addresses corrupted preference labels in RLHF by modeling the Bradley-Terry pairwise comparison model with additional perturbation factors that capture label corruption. The method uses ℓ1-regularized maximum likelihood estimation to simultaneously learn the reward function parameters and identify corrupted preference pairs. Computationally, R3M employs an alternating optimization algorithm where perturbation factors are updated via closed-form solutions (leveraging the log-likelihood's convexity) while reward parameters are updated through gradient descent. Theoretically, the authors prove consistency guarantees under sparsity assumptions about corrupted labels. The method is evaluated on robotic control tasks with synthetic noise and natural language generation using Llama-2 7B and Claude 3 as judge, showing improved robustness against various perturbation types.

## Key Results
- R3M successfully identifies corrupted preference pairs through learned perturbation factors, with sparsity promoting clean data recovery
- On robotic control tasks, R3M achieves higher normalized returns than baseline RLHF methods under all three noise models (stochastic, myopic, irrational)
- For natural language generation, R3M improves win rates and winning scores against reference models when evaluated by Claude 3 judge, demonstrating practical robustness benefits

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The ℓ1-regularized maximum likelihood formulation enables sparse identification of corrupted preference labels while maintaining convexity for efficient optimization.
- **Mechanism**: The ℓ1 penalty on perturbation factors δi induces sparsity, allowing the model to identify which preference pairs are likely corrupted by driving their corresponding δi to large positive values. The log-likelihood remains convex in δi for fixed reward parameters, enabling closed-form updates.
- **Core assumption**: The number of corrupted labels scales sublinearly with the total number of preference pairs (s = o(n)).
- **Evidence anchors**:
  - [abstract] "Accordingly, we formulate the robust reward learning as an ℓ1-regularized maximum likelihood estimation problem."
  - [section 3.2] "To encourage the sparsity of δ, we propose to minimize an ℓ1-regularized negative log-likelihood"
- **Break condition**: If corrupted labels scale superlinearly with n, the model cannot distinguish true rewards from noise, leading to inconsistent estimation.

### Mechanism 2
- **Claim**: Alternating optimization between reward parameters and perturbation factors provides computational efficiency while maintaining theoretical guarantees.
- **Mechanism**: The optimization alternates between (1) fixing reward parameters and computing closed-form updates for each δi using the log-likelihood's convexity, and (2) updating reward parameters via gradient descent with the current δ estimates. This decomposition makes the problem tractable.
- **Core assumption**: The log-likelihood is strictly convex and univariate in each δi, enabling closed-form solutions.
- **Evidence anchors**:
  - [section 3.3] "By examining the optimality condition of (3.3), we can obtain a closed-form solution"
  - [section 3.3] "The log-likelihood is strictly convex and univariate with respect to each perturbation factor"
- **Break condition**: If the log-likelihood loses convexity or becomes highly multi-modal in δi, closed-form updates become approximate and theoretical guarantees may fail.

### Mechanism 3
- **Claim**: Theoretical consistency guarantees hold even with corrupted data, provided sparsity conditions are met.
- **Mechanism**: Under Assumptions 4.2 and 4.3, the robust estimator achieves the same convergence rate as uncorrupted MLE (O(|S||A|/n)) when corrupted labels are sufficiently sparse. This is proven through strong convexity arguments and decomposition of the estimation error.
- **Core assumption**: The ground truth reward lies in a bounded set R_B and perturbations are bounded by C.
- **Evidence anchors**:
  - [section 4] "we prove that under proper regularity conditions, R3M can consistently learn the underlying ground truth reward"
  - [section 4] "Theorem 4.4. Suppose Assumptions 4.2 and 4.3 hold..."
- **Break condition**: If the reward function is not bounded or perturbation bounds are violated, the model identifiability and consistency guarantees fail.

## Foundational Learning

- **Concept**: Bradley-Terry model for pairwise comparison data
  - Why needed here: R3M builds upon the Bradley-Terry model as the base preference model, extending it with perturbation factors to handle corruption
  - Quick check question: What is the probability formula in the Bradley-Terry model for preferring z_w over z_ℓ given rewards r(z_w) and r(z_ℓ)?

- **Concept**: ℓ1 regularization and sparsity in high-dimensional statistics
  - Why needed here: The ℓ1 penalty is crucial for identifying which preference pairs are corrupted while maintaining convexity for efficient optimization
  - Quick check question: How does ℓ1 regularization promote sparsity compared to ℓ2 regularization in the context of outlier detection?

- **Concept**: Alternating optimization and coordinate descent methods
  - Why needed here: The computational efficiency of R3M relies on decomposing the joint optimization into alternating updates with closed-form solutions
  - Quick check question: What are the convergence properties of alternating optimization when each subproblem has a unique solution?

## Architecture Onboarding

- **Component map**: Preference data -> Reward modeling with R3M (reward network + perturbation factors) -> Alternating optimization (closed-form δ updates + gradient-based ϕ updates) -> PPO policy optimization -> Evaluation

- **Critical path**: Preference data → Reward modeling with R3M → Policy optimization via PPO → Evaluation
  The most critical component is the alternating optimization between reward and perturbation estimation, as this directly addresses the corruption problem.

- **Design tradeoffs**: 
  - Computational overhead vs. robustness: R3M adds negligible overhead (closed-form δ updates) while significantly improving robustness
  - Model complexity vs. interpretability: The perturbation factors provide interpretable identification of corrupted data points
  - Theoretical guarantees vs. practical flexibility: Strong assumptions enable proofs but may limit applicability to non-tabular settings

- **Failure signatures**: 
  - Poor reward recovery despite low corruption rates: Check if λ is too large, causing over-regularization
  - Slow convergence: Verify that closed-form δ updates are being computed correctly
  - Inconsistent policy performance: Examine if the learned δ values are identifying meaningful outliers

- **First 3 experiments**:
  1. Implement the closed-form δ update and verify it matches the log-likelihood gradient condition
  2. Test alternating optimization on synthetic data with known corruption patterns
  3. Compare policy performance with and without R3M on a simple control task with injected noise

## Open Questions the Paper Calls Out

- **Question**: How does R3M's performance degrade when the proportion of corrupted labels exceeds the sublinear scaling requirement stated in the theoretical analysis?
- **Basis in paper**: [explicit] The theoretical analysis assumes the number of outliers scales sublinearly with the sample size, and the experiments test corruption rates up to 2/3 of data.
- **Why unresolved**: The paper does not test scenarios with majority corrupted labels, which would violate the theoretical assumptions and provide insight into practical limitations.
- **What evidence would resolve it**: Experimental results showing R3M's performance with 80-90% corrupted labels would reveal the practical breaking point beyond theoretical guarantees.

## Limitations

- Theoretical guarantees rely heavily on sparsity assumptions (s = o(n)) and boundedness conditions that may not hold in practical RLHF settings
- Computational complexity scales with the number of preference pairs, potentially limiting applicability to large-scale RLHF datasets
- The method assumes corruption can be modeled as sparse outliers, which may not capture all real-world noise patterns

## Confidence

- **High Confidence**: The mechanism of ℓ1 regularization inducing sparsity for outlier detection and the alternating optimization approach for computational efficiency
- **Medium Confidence**: The theoretical consistency guarantees under stated assumptions, as they depend on specific conditions that may be challenging to verify in practice
- **Medium Confidence**: The empirical results showing robustness improvements, as they are demonstrated on synthetic corruption and may not capture all real-world noise patterns

## Next Checks

1. Test R3M on preference data with varying corruption patterns (bursty vs. uniform) to validate the sublinear sparsity assumption in practice
2. Evaluate the sensitivity of performance to the ℓ1 regularization parameter λ across different reward model architectures
3. Measure the computational overhead of R3M compared to standard RLHF methods on large-scale preference datasets