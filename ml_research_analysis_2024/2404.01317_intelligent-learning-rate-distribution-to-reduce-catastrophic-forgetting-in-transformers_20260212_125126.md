---
ver: rpa2
title: Intelligent Learning Rate Distribution to reduce Catastrophic Forgetting in
  Transformers
arxiv_id: '2404.01317'
source_url: https://arxiv.org/abs/2404.01317
tags:
- learning
- rate
- dataset
- bert
- shift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates catastrophic forgetting in transformer networks
  during sequential task learning. It proposes a method to automatically distribute
  different learning rates across layers to mitigate forgetting.
---

# Intelligent Learning Rate Distribution to reduce Catastrophic Forgetting in Transformers

## Quick Facts
- arXiv ID: 2404.01317
- Source URL: https://arxiv.org/abs/2404.01317
- Authors: Philip Kenneweg; Alexander Schulz; Sarah Schröder; Barbara Hammer
- Reference count: 16
- Primary result: Method reduces catastrophic forgetting in transformers during sequential task learning, outperforming flat learning rate and EWC baselines by ~5% on GLUE datasets

## Executive Summary
This paper addresses catastrophic forgetting in transformer networks during sequential task learning by proposing a method for intelligent learning rate distribution across layers. The authors hypothesize that different transformer layers capture different types of representations (task-specific vs. task-agnostic), and therefore should be adapted at different speeds during fine-tuning. Using Bayesian optimization with Hyperband scheduling, they find optimal learning rate distributions for layer-specific adaptation and combine them via weighted geometric mean to generalize across tasks. Experiments on GLUE datasets demonstrate that this approach reduces performance drops during dataset shifts, with the combined distribution outperforming flat learning rate and EWC baselines by approximately 5% on average.

## Method Summary
The method employs Bayesian optimization with Hyperband scheduling to automatically distribute different learning rates across transformer layers, mitigating catastrophic forgetting during sequential task learning. Learning rates are divided into 10 choices that affect specific layer groupings (e.g., choice 1 affects layer 1, choice 2 affects layers 2-3, etc.). The optimization searches for rates that balance performance on both original and shifted datasets. Multiple dataset pairs are optimized individually, then their distributions are combined using a weighted geometric mean to create a single fixed distribution that generalizes to unseen dataset pairs. This combined distribution is then applied to new sequential learning scenarios without requiring additional optimization.

## Key Results
- Proposed method outperforms flat learning rate and EWC baselines by ~5% on average across GLUE datasets
- Distribution shifts have minimal impact on transformer performance when using the intelligent learning rate distribution
- Combined learning rate distribution from multiple dataset pairs generalizes well to unseen shifts
- Method is robust and flexible across different transformer architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different layers in transformers capture task-specific vs. task-agnostic representations, so layer-wise learning rates prevent overwriting general knowledge during fine-tuning.
- Mechanism: Lower learning rates are assigned to earlier layers (task-agnostic), higher to later layers (task-specific), and a spike for the newly added dense layer.
- Core assumption: Earlier transformer layers encode general linguistic patterns while later layers specialize per task.
- Evidence anchors:
  - [abstract]: "We hypothesize that different layers of the transformer network represent different abstract concepts and, therefore, should be adapted with different speed when fine-tuning to a new task to reduce catastrophic forgetting."
  - [section 3.1]: "Since BERT has 12 encoder layers we divide the different layers equally into 8 of these (choice 1 affects layer 1, choice 2 affects layers 2,3, choice 3 affect layer 4, choice 4 affect layers 5,6 ...)."
  - [corpus]: Weak anchor—no direct comparison with multi-rate LSTM studies in this corpus.
- Break condition: If layer specialization is task-dependent rather than consistent, fixed distributions may not generalize.

### Mechanism 2
- Claim: Bayesian optimization with Hyperband efficiently searches the high-dimensional learning rate space and finds robust distributions that reduce forgetting.
- Mechanism: Hyperband prunes poor configurations early, Bayesian optimization models the reward landscape (ps + po), and the weighted geometric mean combines ranked distributions for generalization.
- Core assumption: The reward function (sum of both dataset performances) captures both adaptation and retention.
- Evidence anchors:
  - [section 3.1]: "The optimization strategy used is Bayesian Optimization [13] with Hyperband Scheduling [11], as a particularly promising technology in the domain of hyperparameter optimization."
  - [section 3.2]: "They are weighted by their performance ranking and combined with the weighted geometric mean (in correspondence to the exponential search of the learning rates)."
  - [corpus]: Weak anchor—no mention of Bayesian optimization in neighboring works.
- Break condition: If the reward landscape is noisy or multimodal, optimization may settle in local optima.

### Mechanism 3
- Claim: Combining layer-wise distributions from multiple dataset pairs yields a single fixed distribution that works well on unseen shifts.
- Mechanism: Weighted geometric mean across ranked learning rate sets smooths over task-specific quirks.
- Core assumption: Similar task pairs produce similar optimal distributions.
- Evidence anchors:
  - [section 3.2]: "In practice performing hyperparameter optimization for every dataset pair is infeasible... we combine the learning rate distributions lrsa found for a few dataset pairs a using the geometric mean."
  - [section 5.1]: "On average over the unseen data, BERT cL combined outperforms the flat learning rate, as well as BERT + EW C by about 5%."
  - [corpus]: Weak anchor—no other works in corpus combine distributions this way.
- Break condition: If task pairs are too dissimilar, the combined distribution may be suboptimal for all.

## Foundational Learning

- **Catastrophic forgetting**
  - Why needed here: Core problem being solved; understanding how fine-tuning can erase earlier knowledge is essential.
  - Quick check question: What happens to the model's accuracy on dataset A after training on dataset B without any forgetting mitigation?

- **Hyperparameter optimization (Bayesian + Hyperband)**
  - Why needed here: The method relies on automated search; knowing how Bayesian optimization models the reward and how Hyperband allocates resources is key to tuning or debugging.
  - Quick check question: In Hyperband, what happens to configurations with the lowest intermediate scores?

- **Geometric mean combination of ranked distributions**
  - Why needed here: Explains why the final "combined" distribution generalizes; important for modifying the combination strategy.
  - Quick check question: If one dataset pair yields a distribution with much higher ranks than others, how does that affect the combined result?

## Architecture Onboarding

- **Component map**:
  Pre-trained BERT (12 layers) -> Layer-wise learning rates -> Task-specific dense layer -> Adam optimizer -> Bayesian optimization + Hyperband -> Weighted geometric mean combiner -> Sequential training pipeline

- **Critical path**:
  1. Split data into original and shifted sets.
  2. Run Bayesian optimization per dataset pair to find per-layer rates.
  3. Rank configurations by ps + po.
  4. Combine ranked distributions via weighted geometric mean.
  5. Apply combined rates to new dataset pairs and evaluate forgetting.

- **Design tradeoffs**:
  - 10 learning rate slots vs. per-layer rates: coarser but faster search.
  - Geometric mean vs. arithmetic mean: geometric better for multiplicative spaces.
  - Fixed combined distribution vs. per-pair fine-tuning: simpler deployment but less optimal per task.

- **Failure signatures**:
  - No improvement over flat LR: likely layer specialization assumption invalid.
  - High variance in combined rates: dataset pairs too dissimilar.
  - Optimization unstable: reward function (ps + po) may be noisy or not correlated with actual forgetting.

- **First 3 experiments**:
  1. Baseline: BERT with flat LR on SST2→MRPC and measure forgetting.
  2. Layer grouping: Assign LR1 to layer 1, LR2 to layers 2-3, LR3 to layers 4-12; optimize only these three rates.
  3. Rank-sensitivity: Vary b in the geometric mean (b=1.2, 1.8, 2.5) and observe generalization on unseen pairs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact contribution of the layer-specific learning rate distribution compared to a flat learning rate in reducing catastrophic forgetting?
- Basis in paper: [explicit] The authors state that "It is difficult to provide meaningful intuition about what changed in a transformer based network during training" and that "Possible further research also includes other options for the hyperparameter optimization process, like different optimizer choices as well as learning rate schedules, for different parts of the network."
- Why unresolved: The paper does not provide a detailed analysis of how the layer-specific learning rates contribute to mitigating catastrophic forgetting compared to a flat learning rate.
- What evidence would resolve it: A detailed ablation study comparing the performance of BERT with different learning rate distributions (including flat, layer-specific, and other variations) on various NLP tasks would provide insights into the contribution of the layer-specific learning rates.

### Open Question 2
- Question: How does the proposed method of intelligent learning rate distribution generalize to other transformer architectures beyond BERT, such as RoBERTa, distilBERT, or GPT-2?
- Basis in paper: [explicit] The authors mention that "This approach can be applied to many common encoder or decoder models like BERT, RoBERTa, distilBERT or GPT-2."
- Why unresolved: The paper only validates the proposed method on BERT and does not provide evidence of its effectiveness on other transformer architectures.
- What evidence would resolve it: Conducting experiments on various transformer architectures (e.g., RoBERTa, distilBERT, GPT-2) using the proposed method and comparing the results with their respective baseline performances would demonstrate the generalization of the method.

### Open Question 3
- Question: What is the impact of the learning rate distribution on the generalization capabilities of the transformer model when dealing with distribution shifts, such as changes in sentence length or artificial clustering of data?
- Basis in paper: [explicit] The authors observe that "the transformer architecture is robust during distribution shifts, and can in many cases even improve performance, contrary to the catastrophic forgetting paradigm."
- Why unresolved: The paper does not investigate the role of the learning rate distribution in the model's robustness to distribution shifts.
- What evidence would resolve it: Analyzing the performance of the proposed method on distribution shift scenarios with varying learning rate distributions would provide insights into the impact of the learning rate distribution on the model's generalization capabilities.

## Limitations
- Sample size of dataset pairs is limited, potentially affecting generalization to other domains
- Does not compare against alternative hyperparameter search methods (e.g., random search, evolutionary algorithms)
- Assumes task pairs produce similar optimal distributions without extensive validation
- Claims about minimal impact of distribution shifts appear contradictory to the paper's core focus

## Confidence

**High Confidence**: The experimental methodology is sound, using standard GLUE benchmarks and appropriate evaluation metrics. The sequential training setup (original dataset followed by shifted dataset) is clearly specified and reproducible.

**Medium Confidence**: The claim that the proposed method outperforms flat learning rates and EWC baselines by ~5% on average is supported by the presented results, though the specific margin may vary with different task combinations or model variants.

**Low Confidence**: The assertion that distribution shifts have "minimal impact" on transformer performance is not well-supported in the abstract and appears to contradict the paper's core focus on catastrophic forgetting, which is fundamentally about performance degradation during distribution shifts.

## Next Checks

1. **Cross-domain generalization test**: Apply the combined learning rate distribution to a dataset pair from a different domain (e.g., biomedical or legal text) to verify the method's robustness beyond GLUE tasks.

2. **Alternative optimization comparison**: Implement the same layer-wise learning rate approach using random search or evolutionary algorithms for hyperparameter optimization, then compare the resulting performance and search efficiency against the Bayesian optimization + Hyperband approach.

3. **Single-task ablation study**: Test the method on individual GLUE tasks without sequential training to determine whether the performance gains are specific to the catastrophic forgetting scenario or represent general fine-tuning improvements.