---
ver: rpa2
title: Follow-Up Questions Improve Documents Generated by Large Language Models
arxiv_id: '2407.12017'
source_url: https://arxiv.org/abs/2407.12017
tags:
- document
- questions
- user
- users
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study demonstrates that Large Language Models (LLMs) can\
  \ improve document generation quality by asking follow-up questions to clarify user\
  \ requests. A novel web-based system, CQDG, was developed to prompt users for document\
  \ requests, generate clarifying questions, and produce paired documents\u2014one\
  \ using only the initial request and another incorporating user responses to questions."
---

# Follow-Up Questions Improve Documents Generated by Large Language Models

## Quick Facts
- arXiv ID: 2407.12017
- Source URL: https://arxiv.org/abs/2407.12017
- Authors: Bernadette J Tix
- Reference count: 32
- Primary result: Users preferred documents generated with follow-up questions (41%) over baseline documents (25%)

## Executive Summary
This study demonstrates that Large Language Models (LLMs) can improve document generation quality by asking follow-up questions to clarify user requests. A novel web-based system, CQDG, was developed to prompt users for document requests, generate clarifying questions, and produce paired documents—one using only the initial request and another incorporating user responses to questions. Users preferred documents generated with question-answering assistance, with 41% preferring QA documents over 25% for baseline documents. GPT-4 outperformed GPT-3.5 and Gemini in producing preferred QA documents. Users found the question-answering process engaging and valuable, with 74% reporting it made them think about their requests in new ways.

## Method Summary
The study developed a web-based system that generates follow-up questions to clarify user document requests before generating the requested content. Users submit document requests, and the system uses LLMs to identify ambiguities and generate three clarifying questions. After users answer these questions, the system generates two documents: one using only the initial request (baseline) and another incorporating the full Q&A context. Both documents are presented side-by-side for user comparison and preference rating. The study tested three different LLMs (GPT-3.5 Turbo, GPT-4 Turbo, Gemini Pro) and collected both quantitative preference data and qualitative feedback on the question-answering experience.

## Key Results
- Users preferred documents generated with follow-up questions (41%) over baseline documents (25%)
- GPT-4 significantly outperformed GPT-3.5 and Gemini in producing preferred QA documents
- 74% of users reported that the question-answering process made them think about their requests in new ways
- Users found more value in thought-provoking, open-ended questions that offered unique insights

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Follow-up questions improve document quality by resolving user request ambiguity
- Mechanism: When a user's initial prompt is ambiguous or incomplete, LLMs generate clarifying questions to gather missing context. The additional information from user responses enables the LLM to produce more targeted and relevant documents that better match user intent.
- Core assumption: The LLM's question generation capability is sufficiently sophisticated to identify meaningful gaps in user requests
- Evidence anchors:
  - [abstract] "The AI then generated follow-up questions to clarify the user’s needs or offer additional insights before generating the requested documents."
  - [section] "When developing new software, for example, an active and involved process of requirements gathering is typically necessary before the development team can begin their work [10]."
  - [corpus] Weak - corpus neighbors focus on question generation but not specifically on document generation improvement through clarification
- Break condition: If the LLM cannot generate meaningful follow-up questions or if user responses don't provide additional useful context

### Mechanism 2
- Claim: Follow-up questions increase user engagement and satisfaction with the AI interaction
- Mechanism: By asking questions, the LLM creates a collaborative dialogue that makes users feel more engaged with the problem-solving process. Users perceive the AI as more attentive to their needs and report the experience as more enjoyable and valuable.
- Core assumption: Users value interactive dialogue over simple request-response interactions
- Evidence anchors:
  - [abstract] "users found more value in questions which were thought-provoking, open-ended, or offered unique insights"
  - [section] "Most users agreed with the statements 'I felt like the AI was more engaged with my problem because it asked follow-up questions,'"
  - [corpus] Weak - corpus neighbors don't address user engagement with question-answering processes
- Break condition: If questions become annoying or perceived as unnecessary by users

### Mechanism 3
- Claim: Follow-up questions prompt users to think more deeply about their document requirements
- Mechanism: The act of answering clarifying questions forces users to consider aspects of their request they may not have initially considered, leading to better-defined requirements and more useful documents.
- Core assumption: Users benefit from being prompted to think more deeply about their requests
- Evidence anchors:
  - [abstract] "74% reporting it made them think about their requests in new ways"
  - [section] "users found more value in questions which were thought-provoking, open-ended, or offered unique insights"
  - [corpus] Weak - corpus neighbors focus on question generation but not on cognitive benefits to users
- Break condition: If questions are too simple or don't prompt meaningful reflection

## Foundational Learning

- Concept: Natural Language Processing and Understanding
  - Why needed here: The system needs to understand user prompts, identify ambiguities, and generate relevant follow-up questions
  - Quick check question: Can you explain how an LLM identifies ambiguity in a natural language request?

- Concept: Prompt Engineering
  - Why needed here: The quality of follow-up questions depends heavily on the prompt engineering used to guide the LLM
  - Quick check question: What are the key elements of an effective prompt for generating clarifying questions?

- Concept: Human-Computer Interaction Design
  - Why needed here: The system needs to present questions and documents in a way that maximizes user engagement and satisfaction
  - Quick check question: How would you design the interface to present two document options to users?

## Architecture Onboarding

- Component map: Frontend (HTML/JavaScript) -> Backend (LLM APIs: GPT-3.5, GPT-4, Gemini) -> Database (Azure Data Services) -> Azure Functions (serverless orchestration)

- Critical path: 1. User submits initial document request, 2. LLM generates follow-up questions, 3. User answers questions, 4. LLM generates QA document using full context, 5. LLM generates baseline document using only initial request, 6. User compares and rates both documents

- Design tradeoffs: Using multiple LLMs allows comparison but adds complexity; Random LLM selection prevents bias but makes results less predictable; Showing both documents enables comparison but requires more screen space

- Failure signatures: LLM fails to generate meaningful questions (producing simple fill-in-the-blank questions); Document generation fails or produces irrelevant content; User abandons the process due to frustration with questions; Technical issues with API calls or database operations

- First 3 experiments: 1. Test with simple, unambiguous document requests to establish baseline performance, 2. Test with intentionally ambiguous requests to evaluate question generation quality, 3. Test with different prompt engineering approaches for question generation to optimize quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the preference for QA documents over baseline documents persist when generating longer documents (e.g., 5+ pages) rather than 1-page documents?
- Basis in paper: [inferred] The study specifically examined 1-page documents, and the question of scalability to longer documents remains untested
- Why unresolved: The study was limited to short documents, and document complexity increases with length
- What evidence would resolve it: A follow-up study comparing QA vs baseline document preferences for multi-page documents of varying lengths

### Open Question 2
- Question: How does the quality of follow-up questions impact user preferences, and what specific question characteristics lead to the best outcomes?
- Basis in paper: [explicit] The paper states "users found more value in questions which were thought-provoking, open-ended, or offered unique insights" but doesn't quantify this effect
- Why unresolved: The study didn't systematically vary question types or measure their impact on outcomes
- What evidence would resolve it: A controlled experiment testing different question types (yes/no, open-ended, clarifying, etc.) and measuring their effect on document quality and user satisfaction

### Open Question 3
- Question: What is the optimal number of follow-up questions to ask for maximum benefit without causing user fatigue?
- Basis in paper: [inferred] The study used exactly three questions, but didn't explore whether this is optimal or explore the relationship between question quantity and user experience
- Why unresolved: The study used a fixed number of questions without testing alternative quantities
- What evidence would resolve it: A study varying the number of follow-up questions (e.g., 1, 2, 3, 5, 10) and measuring both document quality and user engagement/fatigue levels

## Limitations
- The study's evaluation relied entirely on user preferences without objective quality metrics
- The user sample (N=67) may not represent broader populations
- Critical implementation details like exact prompt templates and conversation history formatting are only described in general terms

## Confidence
**High Confidence**: The finding that GPT-4 outperformed GPT-3.5 and Gemini in producing preferred QA documents is well-supported by the statistical results (41% vs 25% preference rates). The observation that users found the question-answering process engaging and thought-provoking (74% reporting new insights) is also well-documented through both quantitative and qualitative data.

**Medium Confidence**: The claim that follow-up questions improve document quality through ambiguity resolution is supported by the preference data but relies on assumptions about the quality of question generation that aren't fully validated.

## Next Checks
1. **Prompt Engineering Validation**: Systematically test different prompt templates for question generation to identify which approaches produce the most meaningful clarifying questions across various document request types.

2. **Objective Quality Metrics**: Develop and apply automated quality metrics (coherence, relevance, completeness scores) to complement user preference data, providing more objective validation of document quality improvements.

3. **Scalability Testing**: Evaluate system performance and user satisfaction with longer, more complex documents (beyond 1-page length) and more specialized document types to assess generalizability of the approach.