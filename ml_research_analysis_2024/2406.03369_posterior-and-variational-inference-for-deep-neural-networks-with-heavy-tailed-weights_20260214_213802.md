---
ver: rpa2
title: Posterior and variational inference for deep neural networks with heavy-tailed
  weights
arxiv_id: '2406.03369'
source_url: https://arxiv.org/abs/2406.03369
tags:
- network
- such
- lemma
- weights
- heavy-tailed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a Bayesian deep learning framework with\
  \ heavy-tailed priors on network weights, achieving adaptive contraction rates in\
  \ nonparametric regression without requiring hyperparameter tuning. The method uses\
  \ ReLU neural networks with deterministic architecture, where weights are sampled\
  \ from a heavy-tailed distribution scaled by parameters \u03C3k that decay polynomially\
  \ with sample size."
---

# Posterior and variational inference for deep neural networks with heavy-tailed weights

## Quick Facts
- arXiv ID: 2406.03369
- Source URL: https://arxiv.org/abs/2406.03369
- Authors: Ismaël Castillo; Paul Egels
- Reference count: 12
- Primary result: Bayesian deep learning framework with heavy-tailed priors achieves adaptive contraction rates in nonparametric regression without hyperparameter tuning

## Executive Summary
This paper introduces a Bayesian deep learning framework with heavy-tailed priors on network weights, achieving adaptive contraction rates in nonparametric regression without requiring hyperparameter tuning. The method uses ReLU neural networks with deterministic architecture, where weights are sampled from a heavy-tailed distribution scaled by parameters σk that decay polynomially with sample size. The approach automatically adapts to both smoothness and intrinsic dimension of the target function, achieving near-optimal minimax rates up to logarithmic factors. Theoretical results show that the fractional posterior and its mean-field variational approximation concentrate around the true function at the optimal rate simultaneously across multiple settings including compositional classes, geometric data with low Minkowski dimension, and anisotropic Besov spaces.

## Method Summary
The framework uses ReLU neural networks with deterministic architecture (depth L = ⌈log1+δ n⌉, polynomial width) and heavy-tailed priors on weights θk ~ σkζk, where ζk follows a heavy-tailed distribution and σk decays polynomially. A fractional posterior (tempered by α < 1) is constructed, along with a mean-field variational approximation using product-form heavy-tailed distributions. The method automatically adapts to function complexity without requiring model selection or hyperparameter tuning, achieving near-optimal contraction rates in terms of integrated squared loss across compositional, Besov, and geometric function classes.

## Key Results
- Heavy-tailed priors achieve automatic adaptation to smoothness and intrinsic dimension without hyperparameter tuning
- Mean-field variational approximations retain the same adaptive convergence rates as the full posterior
- Overparameterized network architecture with heavy-tailed priors achieves optimal rates without model selection
- The approach works simultaneously across compositional classes, geometric data with low Minkowski dimension, and anisotropic Besov spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heavy-tailed priors enable automatic adaptation to both smoothness and intrinsic dimension without hyperparameter tuning.
- Mechanism: The heavy-tailed distribution allows large weights when needed while the polynomially decaying scaling factors prevent overfitting. This creates a "soft selection" where important weights can take large values under the posterior while less important ones remain small.
- Core assumption: The true function has either compositional structure or lies on a low-dimensional manifold, and the network architecture is sufficiently overparameterized.
- Evidence anchors:
  - [abstract]: "heavy-tailed prior distributions achieve automatic adaptation to smoothness"
  - [section]: "The best rate is automatically attained without assuming any knowledge of regularity parameters"

### Mechanism 2
- Claim: Mean-field variational approximations retain the same adaptive convergence rates as the full posterior.
- Mechanism: The product-form variational distribution approximates the tempered posterior effectively because the heavy-tailed components in the variational class match the prior structure. The optimization problem finds parameter values that capture the essential posterior mass.
- Core assumption: The variational class (product of heavy-tailed distributions) can represent the posterior well enough to preserve contraction rates.
- Evidence anchors:
  - [abstract]: "mean-field variational approximations still benefit from near-optimal theoretical support"
  - [section]: "Theorem 5 establishes contraction of the heavy-tailed mean-field variational approximation with the same adaptive rate"

### Mechanism 3
- Claim: Overparameterized network architecture combined with heavy-tailed priors achieves optimal rates without model selection.
- Mechanism: The logarithmic depth and polynomial width create enough capacity to approximate any function in the target classes. The heavy-tailed prior ensures that only necessary weights take significant values, while the tempering (α < 1) prevents overfitting despite the large capacity.
- Core assumption: The network width is polynomial in n with power at least 1/2, and the depth is logarithmic.
- Evidence anchors:
  - [section]: "any choice of widths rl of polynomial order rl ≍ nal, for arbitrary given al ≥ 1/2 is compatible with the obtained rates"
  - [section]: "we show that well-chosen heavy tailed weight distributions enable one to obtain statistical adaptation"

## Foundational Learning

- Concept: R´enyi divergence and its relationship to L2 contraction
  - Why needed here: The theoretical results are stated in terms of R´enyi divergence contraction, but practitioners often care about L2 error. Understanding the relationship helps interpret results.
  - Quick check question: If a posterior contracts in R´enyi divergence at rate εn, what is the corresponding L2 contraction rate under boundedness assumptions?

- Concept: Compositional function classes and effective dimensions
  - Why needed here: The main theoretical results assume the true function lies in a compositional class, where smoothness propagates through layers in a specific way. Understanding this helps design appropriate architectures.
  - Quick check question: For a composition f = gq ◦ ··· ◦ g0, how does the effective smoothness β*i relate to the individual smoothnesses βi and intrinsic dimensions ti?

- Concept: Heavy-tailed distributions and their tail behavior
  - Why needed here: The prior and variational distributions are heavy-tailed, which is crucial for achieving adaptation. Understanding tail properties helps in choosing appropriate distributions and analyzing their behavior.
  - Quick check question: What conditions on a density h ensure it satisfies (H2) with κ = 0, and which common distributions meet these criteria?

## Architecture Onboarding

- Component map:
  - Deterministic architecture selection: Depth L = ⌈log1+δ n⌉, width vector r with polynomial scaling
  - Heavy-tailed prior: Weights θk ~ σkζk with ζk from heavy-tailed distribution
  - Tempering: Likelihood raised to power α ∈ (0,1)
  - Variational approximation: Product of heavy-tailed distributions as variational class
  - Posterior sampling: MCMC or variational optimization methods

- Critical path:
  1. Choose architecture parameters (L, r) based on sample size
  2. Define scaling factors σk according to decay schedule
  3. Implement heavy-tailed prior for weight initialization
  4. Construct tempered posterior with α < 1
  5. Either sample from posterior (MCMC) or optimize variational approximation

- Design tradeoffs:
  - Width vs depth: Logarithmic depth with polynomial width balances approximation power and computational cost
  - Decay schedule: Faster decay (larger power in σk) provides more regularization but may hurt adaptation
  - Tempering parameter: Smaller α provides more regularization but slower convergence

- Failure signatures:
  - Underfitting: Posterior concentrates on simple functions, large bias
  - Overfitting: Posterior spreads widely, large variance despite tempering
  - Poor adaptation: Posterior doesn't automatically adjust to function complexity

- First 3 experiments:
  1. Implement constant scaling σk = exp(-log2(1+δ) n) with fixed architecture and verify basic posterior sampling
  2. Test adaptation by fitting functions with varying smoothness and checking if contraction rates match theory
  3. Compare mean-field variational approximation against full posterior sampling for computational efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the heavy-tailed prior approach maintain its adaptive properties when applied to standard (non-tempered) posterior distributions, rather than fractional posteriors?
- Basis in paper: The authors conjecture that their results should extend to standard posteriors but note that current techniques in Bayesian nonparametrics don't seem to allow for simple results with heavy-tailed priors.
- Why unresolved: The available techniques require constructing sieve sets that are not straightforward with heavy-tailed priors.
- What evidence would resolve it: A proof showing that standard posteriors with heavy-tailed priors achieve the same adaptive contraction rates, or empirical simulations demonstrating no visible phase transition when α increases from 0.5 to 1.

### Open Question 2
- Question: How does the performance of heavy-tailed priors compare to spike-and-slab priors when computational cost is factored in, particularly for moderate sample sizes?
- Basis in paper: The authors note that spike-and-slab priors require sampling from the posterior on the support points, which can have large computational cost, while their heavy-tailed approach avoids this.
- Why unresolved: No direct computational comparisons are provided between the two approaches.
- What evidence would resolve it: Empirical studies comparing computational efficiency and statistical performance of both methods on benchmark datasets with varying sample sizes.

### Open Question 3
- Question: Does the double descent phenomenon appear in the context of very wide networks with heavy-tailed priors, particularly for widths larger than those considered in the paper?
- Basis in paper: The authors suggest that their exchangeable prior with constant scaling factors might be particularly appropriate to test for double descent, as it doesn't penalize a high number of large coefficients.
- Why unresolved: The paper doesn't investigate very wide networks or explicitly test for double descent.
- What evidence would resolve it: Simulation studies with extremely wide networks (e.g., width n^3 or larger) showing whether the risk decreases for very overfitted models as width increases.

## Limitations
- The theoretical framework relies heavily on the compositional function class assumption, which may not hold for many real-world functions
- The heavy-tailed prior construction, while theoretically sound, requires careful implementation to ensure proper tail behavior and numerical stability
- Results primarily focus on the specific function classes (compositional, Besov, geometric) and may not generalize to arbitrary function spaces

## Confidence
- High confidence in the theoretical contraction rates for compositional functions under stated assumptions
- Medium confidence in the variational approximation results due to the strong mean-field assumption
- Medium confidence in practical implementation feasibility, pending empirical validation

## Next Checks
1. **Adaptation verification**: Test the framework on synthetic functions with varying smoothness and intrinsic dimension to empirically verify the automatic adaptation claims
2. **Numerical stability**: Implement the MCMC sampling algorithm and assess convergence properties for different heavy-tailed priors (Cauchy, Student-t, etc.)
3. **Approximation quality**: Compare the mean-field variational approximation against full posterior sampling on benchmark regression problems to validate computational efficiency claims