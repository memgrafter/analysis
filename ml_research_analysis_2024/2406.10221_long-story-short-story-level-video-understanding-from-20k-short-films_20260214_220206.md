---
ver: rpa2
title: 'Long Story Short: Story-level Video Understanding from 20K Short Films'
arxiv_id: '2406.10221'
source_url: https://arxiv.org/abs/2406.10221
tags:
- dataset
- question
- data
- video
- movie
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Short Film Dataset (SFD), a long-form
  video understanding benchmark featuring 1,078 amateur short films totaling 243 hours.
  It addresses the limitations of existing video datasets by offering story-oriented,
  publicly available content with minimal data leakage.
---

# Long Story Short: Story-level Video Understanding from 20K Short Films

## Quick Facts
- arXiv ID: 2406.10221
- Source URL: https://arxiv.org/abs/2406.10221
- Authors: Ridouane Ghermi; Xi Wang; Vicky Kalogeiton; Ivan Laptev
- Reference count: 40
- Primary result: Introduces Short Film Dataset (SFD) with 1,078 amateur short films, addressing data leakage in long-form video understanding through reduced internet exposure of source content.

## Executive Summary
This paper introduces the Short Film Dataset (SFD), a long-form video understanding benchmark featuring 1,078 amateur short films totaling 243 hours. It addresses limitations of existing video datasets by offering story-oriented, publicly available content with minimal data leakage. SFD includes two QA tasks—multiple-choice and open-ended—automatically generated and manually curated. Experiments show that language-only models achieve performance close to humans, while vision-only and multimodal methods lag significantly, highlighting the need for improved visual integration techniques. SFD provides a robust, accessible platform for advancing long-term video understanding.

## Method Summary
The method introduces a benchmark dataset of 1,078 amateur short films with minimal internet exposure to reduce data leakage. The dataset includes automatically generated and manually curated multiple-choice and open-ended QA pairs. Evaluation uses zero-shot performance of vision-language models and language-only models on video frames, subtitles, or both, measuring accuracy for multiple-choice QA and LLM-based similarity scoring for open-ended QA.

## Key Results
- Language-only models achieve performance levels comparable to humans (59.0% accuracy), while vision-based and multimodal methods lag behind (average accuracy 38.3%)
- SFD exhibits low data leakage with maximum accuracy of 36.0% when tested on movie metadata alone
- Broader temporal windows significantly enhance SFD task performance, especially in language-only settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using amateur short films with limited internet exposure reduces data leakage in long-form video understanding.
- **Mechanism:** Commercial movies are heavily documented online, making it easy for LLMs to memorize plots and details. Amateur films have less public information, so models must rely more on actual video content.
- **Core assumption:** LLMs have not been trained on extensive amateur film metadata.
- **Evidence anchors:**
  - [abstract] "With minimal exposure to LLMs, our dataset is less prone to data leakage issues plaguing other movie datasets"
  - [section 3.1] "SFD exhibits a maximum accuracy of 36.0%, indicating a low leakage issue"
- **Break condition:** If amateur films become widely documented online, leakage risk increases.

### Mechanism 2
- **Claim:** Long-term reasoning improves with larger temporal windows in video understanding.
- **Mechanism:** Processing more shots or scenes allows models to capture evolving storylines and interactions, which are critical for answering complex questions.
- **Core assumption:** Narrative coherence requires information from multiple temporal segments.
- **Evidence anchors:**
  - [section 3.2] "Broader temporal windows significantly enhance SFD task performance, especially in language-only settings"
  - [section 2.3] "Each movie in the dataset is accompanied by a title, a logline... and a synopsis of about 97 words"
- **Break condition:** If questions can be answered with short clips, long-term reasoning becomes unnecessary.

### Mechanism 3
- **Claim:** Language-only models outperform vision-only models on story-level video understanding.
- **Mechanism:** Subtitles and synopses provide dense narrative information, while visual features alone lack sufficient context for complex story comprehension.
- **Core assumption:** Textual information is more informative than raw visual data for narrative tasks.
- **Evidence anchors:**
  - [section 3.3] "Language-based methods achieve performance levels comparable to humans, while state-of-the-art vision-based and multimodal methods fall behind"
  - [section 3.3] "the gap in performance between modern methods (average accuracy 38.3%) and user study (59.0% accuracy) is still very high"
- **Break condition:** If visual models improve in capturing narrative context, the gap may narrow.

## Foundational Learning

- **Concept:** Long-form video understanding
  - Why needed here: The dataset contains movies averaging 13 minutes, requiring models to track storylines over extended periods.
  - Quick check question: Can the model answer questions about character motivations that develop across multiple scenes?

- **Concept:** Multimodal fusion
  - Why needed here: Effective integration of visual and language cues is critical for performance, but current methods lag behind language-only approaches.
  - Quick check question: Does combining video frames with subtitles improve accuracy over subtitles alone?

- **Concept:** Data leakage mitigation
  - Why needed here: Many existing movie datasets suffer from leakage due to pre-training exposure to movie metadata.
  - Quick check question: Can the model answer questions correctly using only the movie title, without watching the video?

## Architecture Onboarding

- **Component map:** Video ingestion pipeline → Shot segmentation → Annotation extraction (captions, faces, locations) → QA generation → Manual curation → Benchmark evaluation
- **Critical path:** Video preprocessing → Subtitle extraction → Question generation → Manual QA curation
- **Design tradeoffs:** Using amateur films limits leakage but may reduce narrative complexity compared to commercial movies. Longer videos improve reasoning but increase computational cost.
- **Failure signatures:** Low accuracy on vision-only tasks indicates weak visual feature extraction; high accuracy on title-only tasks suggests leakage.
- **First 3 experiments:**
  1. Test zero-shot accuracy of an LLM using only movie titles to confirm low leakage.
  2. Compare shot-level vs. movie-level accuracy to validate the importance of long-term reasoning.
  3. Evaluate language-only vs. multimodal performance to identify integration bottlenecks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of vision-only models on SFD compare to that of language-only models, and what specific architectural improvements could bridge this gap?
- Basis in paper: [explicit] The paper states that vision-only models perform significantly worse than language-only models on SFD, highlighting a large gap in performance.
- Why unresolved: The paper identifies the gap but does not provide specific architectural solutions or experiments to address it.
- What evidence would resolve it: Experimental results showing the performance of vision-only models after implementing proposed architectural improvements, such as better visual feature extraction or multimodal fusion techniques.

### Open Question 2
- Question: What are the long-term implications of using amateur short films for training models in terms of narrative complexity and diversity compared to traditional movie datasets?
- Basis in paper: [explicit] The paper suggests that amateur short films offer rich narratives and minimal data leakage, but does not explore the long-term implications for model training.
- Why unresolved: The paper does not discuss how the use of amateur films might affect the diversity and complexity of narratives that models can learn over time.
- What evidence would resolve it: Longitudinal studies comparing the narrative complexity and diversity learned by models trained on SFD versus those trained on traditional movie datasets.

### Open Question 3
- Question: How does the temporal window size affect the performance of multimodal models on SFD, and is there an optimal window size for maximizing accuracy?
- Basis in paper: [explicit] The paper conducts a temporal window study but does not specify an optimal window size for multimodal models.
- Why unresolved: While the study shows improvements with larger windows, it does not determine the point of diminishing returns or the optimal size.
- What evidence would resolve it: Detailed analysis and experiments identifying the optimal temporal window size that maximizes model performance on SFD tasks.

## Limitations

- The dataset's reliance on amateur short films may limit narrative complexity compared to commercial productions
- Manual QA curation introduces potential subjectivity in question selection and answer validation
- The benchmark focuses on English-language content, limiting applicability to multilingual video understanding

## Confidence

**High Confidence:**
- Language-only models significantly outperform vision-only models on story-level video understanding
- Amateur short films reduce data leakage compared to commercial movie datasets
- Long-term reasoning improves with larger temporal windows in video understanding

**Medium Confidence:**
- Current multimodal methods have substantial room for improvement in visual integration
- The dataset represents a meaningful advancement over existing long-form video benchmarks
- Manual QA curation is necessary for maintaining quality in automatically generated questions

**Low Confidence:**
- The performance gap between humans and models will narrow significantly with improved visual features
- The dataset's amateur film focus provides sufficient narrative complexity for real-world applications
- Current zero-shot evaluation methods adequately capture model capabilities for story understanding

## Next Checks

1. **Leakage Validation**: Conduct a comprehensive leakage analysis by testing whether LLMs can answer SFD questions using only movie metadata (titles, synopses) without viewing the videos, comparing performance to full-video access conditions.

2. **Narrative Complexity Assessment**: Evaluate the narrative complexity of SFD films against commercial movies using established film analysis metrics, determining whether the amateur focus limits the benchmark's applicability to real-world scenarios.

3. **Multilingual Extension**: Replicate the benchmark using machine-translated subtitles and QA pairs to assess whether the observed language dominance holds across different languages and cultural contexts.