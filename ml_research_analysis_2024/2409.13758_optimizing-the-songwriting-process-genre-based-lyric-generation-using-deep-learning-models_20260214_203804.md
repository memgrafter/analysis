---
ver: rpa2
title: 'Optimizing the Songwriting Process: Genre-Based Lyric Generation Using Deep
  Learning Models'
arxiv_id: '2409.13758'
source_url: https://arxiv.org/abs/2409.13758
tags:
- lyrics
- lstm
- learning
- genre
- songs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigated the use of deep learning to automate genre-specific
  lyric generation for songwriting. The authors developed two models: a Hugging Face
  pre-trained T5 seq2seq baseline and a custom three-layer LSTM model in PyTorch.'
---

# Optimizing the Songwriting Process: Genre-Based Lyric Generation Using Deep Learning Models

## Quick Facts
- arXiv ID: 2409.13758
- Source URL: https://arxiv.org/abs/2409.13758
- Authors: Tracy Cai; Wilson Liang; Donte Townes
- Reference count: 17
- Primary result: Deep learning models can generate coherent, genre-appropriate lyrics with promising qualitative results

## Executive Summary
This work investigates deep learning approaches for automated genre-specific lyric generation in songwriting. The authors developed and compared two models: a Hugging Face pre-trained T5 seq2seq baseline and a custom three-layer LSTM model in PyTorch. Using English lyrics from a Spotify dataset (~15K songs), both models processed verse-level sequences after tokenization. The study demonstrates that LSTM-based models can generate plausible, genre-appropriate lyrics, supporting the feasibility of automated lyric generation for songwriting applications.

## Method Summary
The authors developed two deep learning models for lyric generation: a Hugging Face pre-trained T5 seq2seq baseline and a custom three-layer LSTM model implemented in PyTorch. Both models were trained on English lyrics from a Spotify dataset containing approximately 15,000 songs. The lyrics were tokenized and processed at the verse level to create training sequences. The T5 baseline leveraged pre-trained transformer architecture, while the custom LSTM model was built from scratch. The models were evaluated across rock, pop, and R&B genres using both automated metrics (ROUGE, BLEU) and qualitative assessment of generated outputs.

## Key Results
- T5 baseline achieved higher ROUGE scores (2073.89–1235.07) indicating better recall of training words
- Both models showed extremely low BLEU scores (~10⁻²²⁷ to 10⁻²²⁹), reflecting limited overlap with training data
- LSTM model demonstrated more balanced ROUGE scores (~20) and comparable BLEU performance (~10⁻²²⁹) across all three genres
- Generated lyrics were largely comprehensible and stylistically coherent across rock, pop, and R&B genres

## Why This Works (Mechanism)
The success of both models in generating genre-appropriate lyrics stems from their ability to learn and reproduce patterns in song structure, vocabulary, and stylistic elements from the training data. The T5 baseline benefits from pre-trained transformer knowledge, allowing it to capture broader linguistic patterns and achieve higher recall of training vocabulary as evidenced by ROUGE scores. The LSTM model, while simpler, demonstrates sufficient capacity to model sequential dependencies in lyrics and maintain genre coherence through its recurrent architecture. Both approaches show that deep learning can effectively capture the statistical properties of lyrics across different musical genres.

## Foundational Learning
- **Sequence-to-sequence modeling**: Needed for mapping input prompts to lyric outputs; quick check: verify model architecture supports variable-length sequences
- **Genre-specific pattern recognition**: Essential for maintaining stylistic consistency; quick check: test genre classification accuracy on training data
- **Verse-level tokenization**: Critical for preserving song structure; quick check: confirm tokenization maintains meaningful lyrical units
- **Automated evaluation metrics**: ROUGE and BLEU provide quantitative assessment; quick check: validate metric implementation against standard benchmarks
- **Qualitative assessment**: Human evaluation needed for creative text generation; quick check: establish rubric for coherence and genre-appropriateness

## Architecture Onboarding

**Component Map:**
- Data Preprocessing -> Tokenization -> Model Training -> Generation -> Evaluation
- Input Text -> LSTM Layers/T5 Encoder -> Output Text -> ROUGE/BLEU Scoring -> Human Review

**Critical Path:**
Data preprocessing and tokenization directly impact model performance. The custom LSTM model's three-layer architecture must effectively capture sequential dependencies in lyrics. Both models require careful hyperparameter tuning for optimal generation quality.

**Design Tradeoffs:**
The T5 baseline trades computational efficiency for pre-trained knowledge, while the custom LSTM model offers more control but requires more training data. The choice between models depends on available resources and desired balance between performance and customization.

**Failure Signatures:**
- Extremely low BLEU scores indicate poor n-gram overlap
- High variance in ROUGE scores across genres suggests inconsistent pattern learning
- Generated lyrics lacking coherence point to insufficient training or poor tokenization

**First Experiments:**
1. Test genre classification accuracy on training data to verify pattern learning
2. Compare generated lyrics against human-written examples using blinded evaluation
3. Evaluate model performance on out-of-distribution prompts to assess generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Extremely low BLEU scores (10⁻²²⁷ to 10⁻²²⁹) raise concerns about metric validity or implementation issues
- Qualitative evaluation relies on subjective assessment rather than systematic human evaluation
- Limited cross-genre testing prevents assessment of model generalization capabilities

## Confidence
- **High confidence**: LSTM models can generate coherent, genre-appropriate lyrics
- **Medium confidence**: T5 baseline outperforms LSTM on ROUGE metrics
- **Low confidence**: The extremely low BLEU scores reflect meaningful limitations in model performance

## Next Checks
1. Conduct systematic human evaluation of generated lyrics across multiple dimensions (coherence, genre-appropriateness, creativity) with blind comparisons to human-written lyrics
2. Re-examine BLEU implementation and scoring methodology, potentially exploring alternative evaluation metrics better suited for creative text generation
3. Expand validation to include cross-genre testing and evaluation of model generalization beyond training data distributions