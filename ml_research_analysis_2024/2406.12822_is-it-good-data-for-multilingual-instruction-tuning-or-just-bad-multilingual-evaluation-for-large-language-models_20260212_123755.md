---
ver: rpa2
title: Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual
  Evaluation for Large Language Models?
arxiv_id: '2406.12822'
source_url: https://arxiv.org/abs/2406.12822
tags:
- native
- cohere
- google
- data
- qwen1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work systematically investigates the effects of using native
  and translated data in multilingual large language model instruction tuning and
  evaluation. It examines three languages (Spanish, Russian, Chinese) with native
  instruction data from the Aya dataset and machine-translated data from English,
  and evaluates on nine benchmarks covering native/generative and translated/structured
  tasks.
---

# Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?

## Quick Facts
- arXiv ID: 2406.12822
- Source URL: https://arxiv.org/abs/2406.12822
- Reference count: 15
- Three languages (Spanish, Russian, Chinese) with native and translated instruction data show native data outperforms translated on generative and native benchmarks when model performance is high

## Executive Summary
This work systematically investigates the effects of using native and translated data in multilingual large language model instruction tuning and evaluation. It examines three languages with native instruction data from the Aya dataset and machine-translated data from English, evaluating on nine benchmarks covering native/generative and translated/structured tasks. Empirical results show that native data leads to significantly better performance on native and generative benchmarks when model performance is high, while differences are minimal on translated structured tasks. Round-trip translation from native data outperforms single-pass translation from English, indicating missing language-specific knowledge is more detrimental than translation defects. Regularization via lower learning rates or multilingual instruction tuning can bridge the gap on structured but not generative tasks.

## Method Summary
The study uses three languages (Spanish, Russian, Chinese) with native instruction data from the Aya dataset and machine-translated data from English. Models are fine-tuned using LoRA with rank 8, alpha 16, dropout 0.05, and learning rates of 1e-4 and 1e-6. Monolingual instruction tuning is performed per language, with multilingual instruction tuning using 8 languages. Evaluation includes nine benchmarks covering TyDi QA, CMMLU, C-Eval, XQuAD, MGSM, MT/HT-MMLU, and open-ended QA tasks. The study compares performance across native vs translated instruction data and tests regularization techniques to bridge performance gaps.

## Key Results
- Native data significantly outperforms translated data on native and generative benchmarks when model performance is high
- Round-trip translation from native data outperforms single-pass translation from English, indicating missing language-specific knowledge is more detrimental than translation defects
- Regularization via lower learning rates or multilingual instruction tuning can bridge the native-translated performance gap on structured but not generative tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Native data outperforms translated data on generative and native benchmarks when model performance is high.
- Mechanism: When models achieve higher accuracy, the subtle translation biases and knowledge mismatches in translated instruction data become more pronounced, especially in tasks requiring generation or deep cultural knowledge.
- Core assumption: Instruction tuning quality correlates with model performance, and higher performance amplifies sensitivity to data imperfections.
- Evidence anchors:
  - [abstract] "native data leads to significantly better performance on native and generative benchmarks when model performance is high"
  - [section 4.2] "the correlation between âˆ†S and Snative is weak for structured tasks but very strong for tasks involving generation"
- Break condition: If model performance remains consistently low, the differences between native and translated data become negligible.

### Mechanism 2
- Claim: Missing language-specific knowledge is more detrimental than translation defects.
- Mechanism: Round-trip translation (translating native data via English and back) preserves language-specific knowledge while introducing translation artifacts, yet still outperforms single-pass translation from English, indicating knowledge gaps hurt performance more.
- Core assumption: Round-trip translation introduces more translation errors but retains native knowledge, isolating the effect of knowledge versus defects.
- Evidence anchors:
  - [abstract] "Round-trip translation from native data outperforms single-pass translation from English, indicating missing language-specific knowledge is more detrimental than translation defects"
  - [section 4.4] "models with RTT (test language-origin) are uniformly better than those with data translated from English"
- Break condition: If translation errors dominate the instruction content, defects could outweigh missing knowledge.

### Mechanism 3
- Claim: Regularization techniques (lower learning rate or multilingual instruction tuning) can bridge the native-translated performance gap on structured but not generative tasks.
- Mechanism: Lower learning rate reduces overfitting to translated artifacts, while multilingual tuning prevents overfitting to a single language's idiosyncrasies, helping on structured tasks but not generative ones.
- Core assumption: Structured tasks are less sensitive to subtle language nuances than generative tasks.
- Evidence anchors:
  - [abstract] "Regularization via lower learning rates or multilingual instruction tuning can bridge this gap on structured but not generative tasks"
  - [section 4.5] "for MGSM and MT-MMLU, the difference between using translated and native data is not clear under most conditions"
- Break condition: If the task requires deep cultural or language-specific understanding, regularization may fail to close the gap.

## Foundational Learning

- Concept: Instruction tuning and its role in aligning models to human preferences.
  - Why needed here: The study compares instruction tuning with native versus translated data, so understanding this process is essential to interpret results.
  - Quick check question: What is the main goal of instruction tuning in large language models?

- Concept: Translation artifacts and their impact on model evaluation.
  - Why needed here: The research highlights how translationese and errors can distort evaluation, especially in multilingual contexts.
  - Quick check question: How can translationese affect the quality of translated instruction data?

- Concept: Cross-lingual transfer and its limitations.
  - Why needed here: The findings suggest that generative tasks benefit more from cross-lingual transfer than classification tasks, informing evaluation design.
  - Quick check question: In what type of tasks is cross-lingual transfer more effective, according to the study?

## Architecture Onboarding

- Component map: Base LLM (e.g., Llama2, Gemma, Qwen) -> Low-Rank Adaptation (LoRA) -> Instruction Data (Native/Translated) -> Fine-tuning -> Evaluation Benchmarks (Native/Translated, Structured/Generative)
- Critical path: Fine-tune base model with instruction data -> evaluate on native/generative benchmarks -> assess native-translated gap -> apply regularization if needed -> re-evaluate
- Design tradeoffs: Balancing data quality (native vs translated) against cost and availability; choosing between structured and generative benchmarks for evaluation
- Failure signatures: No significant gap between native and translated data suggests either low model performance or ineffective evaluation; persistent gap despite regularization indicates generative task sensitivity
- First 3 experiments:
  1. Fine-tune a base model with native instruction data and evaluate on native/generative benchmarks to establish baseline performance.
  2. Repeat with translated instruction data and compare results to detect native-translated gaps.
  3. Apply lower learning rate regularization to translated data fine-tuning and re-evaluate to test gap closure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do native and translated instruction data perform across different language families (e.g., Romance vs. Slavic vs. Sino-Tibetan)?
- Basis in paper: [inferred] The paper compares three languages from different families but doesn't analyze performance differences between language families.
- Why unresolved: The study focuses on individual languages without examining cross-family patterns or generalizing findings across language families.
- What evidence would resolve it: Systematic testing of native vs. translated data across multiple languages from each major language family, comparing performance patterns.

### Open Question 2
- Question: What specific linguistic features or cultural knowledge are lost or distorted in machine translation that most impact LLM performance?
- Basis in paper: [explicit] The paper mentions translationese and knowledge mismatches but doesn't analyze specific linguistic/cultural features.
- Why unresolved: The study identifies that native knowledge matters but doesn't specify which features or types of knowledge are most critical.
- What evidence would resolve it: Detailed linguistic analysis of native vs. translated data, identifying specific features (e.g., idioms, cultural references, grammatical structures) that most affect performance.

### Open Question 3
- Question: How does the performance gap between native and translated data change as model size increases beyond those tested?
- Basis in paper: [explicit] The study tests models up to 14B parameters but notes that larger models are increasingly common.
- Why unresolved: The paper's largest model is 14B parameters, while models like GPT-4 are much larger, and scaling effects are unknown.
- What evidence would resolve it: Testing native vs. translated data performance on models significantly larger than 14B parameters, examining if gaps widen or narrow with scale.

### Open Question 4
- Question: Can targeted data augmentation or specialized training techniques fully bridge the native-translated performance gap for generative tasks?
- Basis in paper: [explicit] The paper shows regularization helps with structured tasks but not generative ones, leaving the gap unresolved for generation.
- Why unresolved: The study tests basic regularization techniques but doesn't explore more sophisticated approaches specifically for generative tasks.
- What evidence would resolve it: Testing advanced techniques like domain-specific fine-tuning, curriculum learning, or knowledge injection methods on generative benchmarks.

## Limitations
- The study uses only three languages and a limited set of evaluation benchmarks, which may not generalize to other linguistic families or tasks
- Translation quality and evaluation methodologies are not fully detailed, introducing potential variability
- The use of instruction tuning with only three languages in multilingual experiments may not capture full multilingual model behavior complexity

## Confidence
- High confidence: Native data consistently outperforms translated data on generative and native benchmarks when model performance is high; round-trip translation preserves language-specific knowledge better than single-pass translation
- Medium confidence: Regularization techniques (lower learning rates, multilingual tuning) can bridge the gap on structured but not generative tasks; translation artifacts and missing language-specific knowledge are significant factors
- Low confidence: Generalization to other languages, tasks, or model architectures; specific impact of different translation APIs or evaluation protocols

## Next Checks
1. Replicate the study with a broader set of languages (e.g., Arabic, Japanese, Swahili) and additional generative and structured benchmarks to test the robustness of the findings
2. Compare the effects of different translation APIs (e.g., DeepL, GPT-4) and translation strategies (e.g., back-translation, glossary use) on model performance and evaluation outcomes
3. Investigate the impact of alternative fine-tuning methods (e.g., full fine-tuning, adapter-based approaches) and model architectures (e.g., Mistral, LLaMA-3) on the native-translated performance gap across tasks