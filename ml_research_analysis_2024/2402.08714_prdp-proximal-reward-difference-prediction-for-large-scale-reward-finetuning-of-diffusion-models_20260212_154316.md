---
ver: rpa2
title: 'PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning
  of Diffusion Models'
arxiv_id: '2402.08714'
source_url: https://arxiv.org/abs/2402.08714
tags:
- reward
- diffusion
- prdp
- training
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PRDP addresses the instability of RL-based reward finetuning for
  diffusion models when scaling to large prompt datasets. The core method is a Reward
  Difference Prediction (RDP) objective that converts RLHF into a supervised regression
  task, enabling stable optimization without policy gradients.
---

# PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models

## Quick Facts
- arXiv ID: 2402.08714
- Source URL: https://arxiv.org/abs/2402.08714
- Authors: Fei Deng; Qifei Wang; Wei Wei; Matthias Grundmann; Tingbo Hou
- Reference count: 40
- Primary result: Stable black-box reward maximization for diffusion models on 100K+ prompts using supervised regression (RDP) instead of RL, achieving 0.3175 reward score vs 0.2464 for DDPO on unseen prompts

## Executive Summary
PRDP introduces a novel approach to reward finetuning of diffusion models that converts the reinforcement learning problem into a stable supervised regression task. The method addresses the critical instability issues that plague RL-based approaches when scaling to large prompt datasets. By predicting reward differences rather than absolute rewards and using proximal updates with online optimization, PRDP achieves stable training on datasets exceeding 100K prompts - a scale where traditional RL methods like DDPO completely fail.

The key innovation lies in the Reward Difference Prediction (RDP) objective, which transforms the non-differentiable black-box reward maximization problem into a differentiable supervised learning task. This allows PRDP to leverage the stability of gradient-based optimization while maintaining the ability to optimize arbitrary reward functions. The method demonstrates significant improvements in generation quality on complex, unseen prompts compared to existing approaches, while avoiding the reward hacking issues common in RL-based methods.

## Method Summary
PRDP addresses the instability of RL-based reward finetuning for diffusion models when scaling to large prompt datasets. The core method is a Reward Difference Prediction (RDP) objective that converts RLHF into a supervised regression task, enabling stable optimization without policy gradients. Using proximal updates and online optimization, PRDP achieves stable black-box reward maximization for the first time on over 100K prompts. On large-scale training, PRDP significantly improves generation quality on complex, unseen prompts, whereas RL-based methods fail completely. For example, on HPD v2 unseen prompts, PRDP achieves 0.3175 reward score versus 0.2464 for DDPO with HPSv2.

## Key Results
- PRDP achieves stable training on datasets with over 100K prompts, where RL methods like DDPO completely fail
- On HPD v2 unseen prompts, PRDP achieves 0.3175 reward score versus 0.2464 for DDPO with HPSv2
- The method successfully prevents reward hacking through KL regularization while maintaining high reward scores

## Why This Works (Mechanism)
PRDP works by converting the reinforcement learning problem into a supervised regression task through reward difference prediction. Instead of directly optimizing for high rewards (which creates unstable policy gradients), the model learns to predict the difference in rewards between two denoising steps. This difference prediction is a smooth, differentiable function that can be optimized using standard gradient descent. The proximal updates further stabilize training by constraining the magnitude of parameter updates, preventing the large, destabilizing gradients that plague RL methods. The online optimization framework allows the model to adapt to each prompt's specific characteristics while maintaining overall stability.

## Foundational Learning
- **Diffusion models**: Generative models that denoise random noise into images through a learned reverse diffusion process. Needed to understand the base architecture being finetuned. Quick check: Can explain the forward and reverse diffusion processes.
- **Reinforcement Learning from Human Feedback (RLHF)**: A framework where models are trained to maximize human-provided reward signals. Needed to understand the problem PRDP solves. Quick check: Can describe the policy gradient optimization in RLHF.
- **Reward hacking**: When models exploit reward function loopholes to achieve high scores without producing desired outputs. Needed to understand the failure mode PRDP prevents. Quick check: Can identify examples of reward hacking in previous diffusion model work.
- **Proximal updates**: Optimization technique that constrains parameter updates to prevent large, destabilizing changes. Needed to understand PRDP's stability mechanism. Quick check: Can explain how proximal updates differ from standard gradient descent.
- **Online optimization**: Sequential optimization approach that updates model parameters for each data point independently. Needed to understand PRDP's training procedure. Quick check: Can describe advantages of online vs batch optimization for large datasets.
- **Classifier-free guidance**: Technique that improves text-image alignment in diffusion models by interpolating between conditional and unconditional generations. Needed to understand the base model's capabilities. Quick check: Can explain how guidance scale affects generation quality.

## Architecture Onboarding

**Component Map**: Stable Diffusion v1.4 -> PRDP Finetuning Module -> Reward Models (HPSv2, PickScore) -> KL Regularization

**Critical Path**: Text prompts → Denoising process → PRDP loss computation → Proximal update → Image generation → Reward evaluation

**Design Tradeoffs**: 
- Supervised regression vs policy gradients: Sacrifices theoretical optimality for stability
- Online optimization vs batch training: Enables scalability at cost of increased computational complexity
- KL regularization: Balances reward maximization against preserving pretrained capabilities

**Failure Signatures**:
- Training instability manifests as exploding gradients and NaN losses
- Reward hacking appears as high reward scores but poor text-image alignment
- Over-regularization shows as low reward scores despite stable training

**First Experiments**:
1. Implement PRDP loss function on a small subset of prompts (100-1000) and verify stable training curves
2. Compare reward scores and generation quality between PRDP and DDPO on a validation set
3. Test KL regularization impact by training with different β values and measuring reward hacking incidence

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Performance gains are demonstrated only on Stable Diffusion v1.4 with specific reward models (HPSv2, PickScore)
- The method's effectiveness on other diffusion architectures or reward functions remains unverified
- Computational requirements for online optimization over large prompt datasets may limit practical deployment

## Confidence
- High confidence: PRDP enables stable reward finetuning where traditional RL methods fail completely
- Medium confidence: General applicability of PRDP to other diffusion models or reward functions
- Low confidence: Method's robustness to reward hacking without additional alignment checks

## Next Checks
1. Test PRDP on a different diffusion architecture (e.g., SDXL) to verify generalizability beyond Stable Diffusion v1.4
2. Implement automated reward hacking detection by evaluating text-image alignment on held-out reward models not used during training
3. Conduct ablation studies varying the proximal update coefficient to determine optimal balance between stability and performance