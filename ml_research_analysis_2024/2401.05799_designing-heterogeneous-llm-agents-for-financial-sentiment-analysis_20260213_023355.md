---
ver: rpa2
title: Designing Heterogeneous LLM Agents for Financial Sentiment Analysis
arxiv_id: '2401.05799'
source_url: https://arxiv.org/abs/2401.05799
tags:
- sentiment
- agents
- design
- negative
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores using large language models (LLMs) without
  fine-tuning for financial sentiment analysis (FSA). The proposed Heterogeneous multi-Agent
  Discussion (HAD) framework leverages Minsky's theory of mind and emotions to instantiate
  specialized LLM agents based on FSA error types.
---

# Designing Heterogeneous LLM Agents for Financial Sentiment Analysis

## Quick Facts
- arXiv ID: 2401.05799
- Source URL: https://arxiv.org/abs/2401.05799
- Authors: Frank Xing
- Reference count: 40
- Primary result: HAD framework improves FSA accuracy, fixing 25-35% of fine-tuning performance gap

## Executive Summary
This study introduces a Heterogeneous multi-Agent Discussion (HAD) framework that leverages Minsky's theory of mind to design specialized LLM agents for financial sentiment analysis (FSA) without fine-tuning. By instantiating agents based on FSA error types (mood, rhetoric, dependency, aspect, reference), the framework achieves significant performance improvements over naive prompting. The multi-agent discussion approach allows for granular error detection and correction, demonstrating that collaborative LLM agents can effectively address domain-specific sentiment analysis challenges.

## Method Summary
The HAD framework implements five heterogeneous LLM agents, each specialized in detecting and analyzing specific FSA error types: irrealis mood, rhetoric (sarcasm), dependency, aspect, and reference expressions. These agents receive the same input message but are prompted to focus on their designated error type, generating specialized sentiment analyses. The agent outputs are then concatenated into a summative prompt, which is fed to a consensus agent that produces the final sentiment label. The framework is evaluated across five FSA datasets using zero-shot prompting with GPT-3.5 and BLOOMZ models, comparing performance against naive prompting and fine-tuning baselines.

## Key Results
- HAD improves FSA accuracy across multiple datasets compared to naive prompting
- The framework fixes 25-35% of the performance gap between zero-shot prompting and fine-tuning
- Mood, rhetoric, and aspect agents contribute more significantly than reference and dependency agents
- Ablation analysis reveals the dependency agent is ineffective and can be removed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heterogeneous agents with error-type specialization improve FSA accuracy over naive prompting.
- Mechanism: Each agent focuses on a specific FSA error type, enabling granular error detection and correction during multi-agent discussion.
- Core assumption: FSA errors are discrete and identifiable for targeted agent specialization via prompt engineering.
- Evidence anchors: [abstract] "The framework instantiates specialized agents using prior domain knowledge of the types of FSA errors and reasons on the aggregated agent discussions."
- Break condition: If FSA errors overlap significantly, agent specialization becomes ambiguous and performance benefit diminishes.

### Mechanism 2
- Claim: Multi-agent discussion aggregation yields better consensus than single-agent output.
- Mechanism: Intermediary agent responses are concatenated into a summative prompt for the consensus agent to integrate multiple perspectives.
- Core assumption: LLM-generated outputs can be meaningfully combined to improve final classification accuracy.
- Evidence anchors: [abstract] "The framework instantiates specialized agents ... and reasons on the aggregated agent discussions."
- Break condition: If agent outputs are too divergent, aggregation may introduce noise and degrade accuracy.

### Mechanism 3
- Claim: Prompt-based specialization can approximate fine-tuning benefits without retraining.
- Mechanism: Error-type focused prompts guide agents to extract latent domain-specific capabilities from LLMs.
- Core assumption: In-context learning with carefully designed prompts can extract domain knowledge from LLMs.
- Evidence anchors: [abstract] "using LLMs without fine-tuning for FSA ... fixes 25-35% of the performance gap between zero-shot prompting and fine-tuning."
- Break condition: If LLM lacks sufficient pre-training on relevant domain knowledge, prompt-based specialization fails to achieve comparable performance.

## Foundational Learning

- Concept: Minsky's Society of Mind theory
  - Why needed here: Provides theoretical foundation for decomposing sentiment analysis into specialized "resources" (agents) activated by context
  - Quick check question: How does Society of Mind explain why multiple specialized agents can outperform a single general-purpose agent?

- Concept: Financial sentiment analysis error taxonomy
  - Why needed here: Guides agent specialization design by mapping FSA error types to agent functions
  - Quick check question: What are the six FSA error types used to instantiate the heterogeneous agents?

- Concept: In-context learning (ICL) vs. fine-tuning
  - Why needed here: Explains why zero-shot approach achieves competitive results without retraining and sets performance expectations
  - Quick check question: What percentage of the fine-tuning performance gap does HAD close on average?

## Architecture Onboarding

- Component map: Input message → 5 specialized agents (mood, rhetoric, dependency, aspect, reference) → Aggregated discussion → Consensus agent → Final sentiment label
- Critical path:
  1. Distribute original user message to all five agents
  2. Each agent generates focused sentiment analysis
  3. Concatenate agent outputs into summative prompt
  4. Final agent resolves to positive/negative/neutral
- Design tradeoffs:
  - More agents → higher accuracy but increased latency and cost
  - Simpler agent roles → faster but less error coverage
  - Aggregation method → concatenation is simple but may not optimally merge conflicting views
- Failure signatures:
  - Low agent agreement → consensus agent may default to majority or become indecisive
  - Agent outputs too generic → loss of specialization benefit
  - Prompt misalignment → agents may ignore error-type focus and revert to general sentiment analysis
- First 3 experiments:
  1. Baseline: Naive prompting on FPB dataset, measure accuracy and F1
  2. HAD with all five agents, measure accuracy and F1 improvement
  3. Ablation: Remove one agent at a time, measure contribution to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance gap between HAD and fine-tuning be further reduced, and what specific design modifications could achieve this?
- Basis in paper: [explicit] The paper states that HAD fixes around 25%-35% of the gap between ICL and fine-tuning.
- Why unresolved: The paper does not explore further optimizations to reduce the remaining gap.
- What evidence would resolve it: Experimental results comparing HAD with additional design modifications against fine-tuned models on FSA datasets.

### Open Question 2
- Question: What are the specific linguistic or contextual features that cause the remaining errors in HAD predictions, and how can these be addressed?
- Basis in paper: [inferred] The paper discusses error types in FSA but does not analyze the remaining errors after applying HAD.
- Why unresolved: The paper focuses on improving accuracy but does not investigate the nature of persistent errors.
- What evidence would resolve it: Detailed error analysis of HAD predictions, identifying specific linguistic or contextual features leading to mistakes, and proposing targeted solutions.

### Open Question 3
- Question: How does HAD performance generalize to other domain-specific sentiment analysis tasks beyond finance, and what adaptations are necessary?
- Basis in paper: [inferred] The paper evaluates HAD on multiple FSA datasets but does not explore its applicability to other domains.
- Why unresolved: The study is limited to FSA, and generalizability to other tasks is unknown.
- What evidence would resolve it: Empirical evaluation of HAD on sentiment analysis tasks in different domains with appropriate adaptations to agent design and prompts.

## Limitations
- Framework effectiveness depends on prompt engineering quality and assumes FSA errors are sufficiently discrete for specialization
- Performance improvement (25-35% of fine-tuning gap) is meaningful but leaves substantial room for improvement
- Simple concatenation aggregation may not optimally resolve conflicts between agent outputs

## Confidence

- **High confidence**: The mechanism of using heterogeneous agents for FSA is novel and technically sound; performance improvements over naive prompting are demonstrated
- **Medium confidence**: The specific error-type specialization approach is theoretically justified but lacks direct empirical validation beyond the proposed framework
- **Medium confidence**: The 25-35% performance gap closure is supported by the study but depends on dataset characteristics and baseline comparisons

## Next Checks

1. Test HAD framework on FSA datasets with different error distributions to assess generalization of agent specialization
2. Implement and compare alternative aggregation methods (weighted voting, attention-based merging) to evaluate if concatenation is optimal
3. Conduct controlled experiments varying the number of agent discussion rounds to determine the point of diminishing returns for multi-round deliberation