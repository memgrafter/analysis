---
ver: rpa2
title: On the Limited Generalization Capability of the Implicit Reward Model Induced
  by Direct Preference Optimization
arxiv_id: '2409.03650'
source_url: https://arxiv.org/abs/2409.03650
tags:
- reward
- dporm
- exrm
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the generalization capability of the implicit
  reward model induced by Direct Preference Optimization (DPO) compared to explicit
  reward models trained via RLHF. The authors conduct extensive experiments across
  five out-of-distribution settings using three model series (Gemma-2B, Gemma-7B,
  Mistral-7B) and six training datasets.
---

# On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization

## Quick Facts
- **arXiv ID:** 2409.03650
- **Source URL:** https://arxiv.org/abs/2409.03650
- **Reference count:** 40
- **Primary result:** DPORM underperforms EXRM on out-of-distribution data with mean accuracy drop of 3% and maximum of 7%

## Executive Summary
This paper investigates the generalization capability of the implicit reward model induced by Direct Preference Optimization (DPO) compared to explicit reward models trained via RLHF. The authors conduct extensive experiments across five out-of-distribution settings using three model series and six training datasets. Their key finding is that while DPO's implicit reward model (DPORM) achieves comparable in-distribution accuracy to explicit reward models (EXRM), it consistently underperforms EXRM on out-of-distribution data, with a mean drop in accuracy of 3% and maximum drop of 7%. This performance gap persists across different distribution shifts including prompt shifts and response shifts.

## Method Summary
The authors compare EXRM and DPORM by fine-tuning both on the same preference datasets and evaluating their accuracy at distinguishing preferred from rejected responses. EXRM uses a linear layer on top of a language model trained with negative log-likelihood, while DPORM optimizes the language model directly with the DPO objective. They evaluate across three model series (Gemma-2B, Gemma-7B, Mistral-7B), six training datasets (HH-RLHF, Arena, UltraFeedBack, Nectar, Summarization, WebGPT), and five out-of-distribution settings. The training uses 1 epoch for EXRM with LR=5e-6 and 2 epochs for DPO with LR=1e-6 and β=0.03, with evaluation on both in-distribution and out-of-distribution test sets.

## Key Results
- DPORM achieves comparable in-distribution accuracy to EXRM but underperforms on out-of-distribution data
- DPORM shows mean accuracy drop of 3% and maximum drop of 7% across five OOD settings
- Performance gap persists across different distribution shifts (prompt shifts and response shifts)
- Results suggest EXRM has better generalization ability than DPORM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPO's implicit reward model (DPORM) can approximate an explicit reward model (EXRM) in the limit under certain assumptions.
- Mechanism: DPO implicitly represents the reward function through the language model policy by optimizing the negative log-likelihood of preference data, bypassing explicit reward model training.
- Core assumption: The Bradley-Terry model holds for human preferences, and the reference policy is well-specified.
- Evidence anchors: [abstract] "Prior work has shown that the implicit reward model of DPO... can approximate an EXRM in the limit"; [section 2] "Under these assumptions, an aligned LLM πdpo can be optimized without explicitly training a reward model."
- Break condition: When the Bradley-Terry model assumption is violated or the reference policy is mis-specified.

### Mechanism 2
- Claim: EXRM has better generalization ability than DPORM, especially under distribution shifts.
- Mechanism: EXRM learns a discriminative function between preferred and rejected responses, which generalizes better to out-of-distribution data compared to DPORM's generative approach.
- Core assumption: The feature representation learned by EXRM is more robust to distribution shifts than the generative representation in DPORM.
- Evidence anchors: [abstract] "Our findings indicate that even though DPORM fits the training dataset comparably, it generalizes less effectively than EXRM, especially when the validation datasets contain distribution shifts"; [section 3.1] "DPORM has a mean drop in accuracy of 3% and a maximum drop of 7% across five out-of-distribution settings."
- Break condition: When the distribution shift is minimal or when the generative model in DPORM happens to align well with the new distribution.

### Mechanism 3
- Claim: The quality of DPORM is conditioned on the generative power of the language model, leading to potential issues with mis-specified representation features.
- Mechanism: DPORM parameterizes the reward function through the language model, so if the model's representation features are not well-suited for the task or distribution, the reward model quality suffers.
- Core assumption: Different training datasets or architectures can lead to mis-specified representation features.
- Evidence anchors: [section 2] "Because DPO parameterizes its reward function through the language model, the quality of the reward model is conditioned on the generative power of the language model"; [section 3.1] "DPORM has limited generalization ability and substantiates the integration of an explicit reward model in iterative DPO approaches."
- Break condition: When the language model's representation features align well with the task or when explicit reward models also suffer from representation issues.

## Foundational Learning

- Concept: Distribution shift in machine learning
  - Why needed here: The paper investigates how well reward models generalize when the test data distribution differs from the training distribution.
  - Quick check question: What are the two types of distribution shifts studied in this paper?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is the broader framework within which DPO operates, and understanding its components helps contextualize the comparison between EXRM and DPORM.
  - Quick check question: What are the two main approaches for learning a reward model in RLHF mentioned in the abstract?

- Concept: Bradley-Terry model for pairwise comparisons
  - Why needed here: The Bradley-Terry model assumption underlies the theoretical justification for DPO's approach to learning preferences.
  - Quick check question: According to the Bradley-Terry model, what is the probability that response y_w is preferred over y_l given a prompt x?

## Architecture Onboarding

- Component map: Preference dataset -> EXRM (LM + linear layer) / DPORM (LM with DPO objective) -> Evaluation on in-distribution and out-of-distribution test sets

- Critical path:
  1. Collect preference dataset
  2. Train EXRM and DPORM on the same dataset
  3. Evaluate both models on in-distribution and out-of-distribution data
  4. Compare accuracy metrics

- Design tradeoffs:
  - EXRM requires training an additional reward model but may generalize better
  - DPORM is simpler and more stable but may have limited generalization
  - Using ensembles of reward models could improve robustness but adds complexity

- Failure signatures:
  - DPORM consistently underperforms EXRM on out-of-distribution data
  - Large accuracy drops when evaluating on data from different sources or with response shifts
  - DPORM may perform better on in-distribution data but worse on OOD data

- First 3 experiments:
  1. Train EXRM and DPORM on UltraFeedback dataset, evaluate on in-distribution UltraFeedback validation set and out-of-distribution HH-RLHF dataset
  2. Train on Arena dataset, evaluate on in-distribution Arena validation and out-of-distribution UltraFeedback dataset
  3. Train on HH-RLHF dataset, evaluate on in-distribution HH-RLHF validation and out-of-distribution Nectar dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the generalization gap between DPORM and EXRM vary with model scale beyond 7B parameters?
- Basis in paper: [inferred] The authors use 2B and 7B models but note that varying model sizes may impact results differently, suggesting the need to study larger models.
- Why unresolved: The paper focuses on 2B-7B models due to common benchmarking practices, but doesn't explore how results scale to larger models like 70B+ parameters which are increasingly common.
- What evidence would resolve it: Systematic experiments comparing DPORM vs EXRM generalization across a wider range of model scales (e.g., 7B, 13B, 30B, 70B) on the same tasks and datasets.

### Open Question 2
- Question: What specific architectural differences in EXRM versus DPORM contribute to the generalization gap?
- Basis in paper: [explicit] The authors note that DPORM parameterizes its reward function through the language model, which can face issues when representation features are mis-specified, suggesting architectural factors may be important.
- Why unresolved: While the paper identifies that DPORM underperforms EXRM, it doesn't investigate which specific architectural components (e.g., separate reward head vs language model parameterization) drive this difference.
- What evidence would resolve it: Ablation studies comparing different reward model architectures while controlling for other factors, such as testing variants of DPORM with explicit reward heads or EXRM with language model parameterization.

### Open Question 3
- Question: How do different types of distribution shifts (prompt vs response) interact with each other and compound the generalization gap?
- Basis in paper: [explicit] The authors identify two types of distribution shifts (prompt shift and response shift) and find DPORM underperforms EXRM in both cases, but don't study their interaction.
- Why unresolved: The experiments study prompt shift and response shift separately, but real-world scenarios often involve combinations of both types of shifts occurring simultaneously.
- What evidence would resolve it: Controlled experiments introducing combined distribution shifts (e.g., both prompt and response shifts simultaneously) and measuring how the generalization gap between DPORM and EXRM changes compared to single-shift scenarios.

## Limitations

- The study focuses on pairwise preference classification accuracy rather than actual response quality in real-world applications
- Results are based on specific model families (Gemma and Mistral) and may not generalize to other architectures
- The paper doesn't explore alternative DPO variants or more complex distribution shift scenarios

## Confidence

**High Confidence:** The experimental methodology is sound, with systematic evaluation across multiple model series, training datasets, and distribution shift scenarios. The results showing DPORM's underperformance on out-of-distribution data are consistent and statistically significant.

**Medium Confidence:** The claim that DPORM approximates EXRM in the limit is supported by theoretical analysis, but the experimental evidence is limited to specific conditions. The mechanism by which DPORM's generative approach leads to worse generalization is plausible but not definitively proven.

**Low Confidence:** The extent to which these findings generalize to other model families, dataset types, or preference learning objectives remains uncertain.

## Next Checks

1. **Dataset Diversity Test:** Evaluate EXRM and DPORM on a broader range of preference datasets, including those with different collection methodologies (e.g., single-answer vs. pairwise preferences) and domain-specific content to assess the robustness of the generalization gap.

2. **Model Architecture Scaling:** Repeat the experiments with larger model architectures (e.g., LLaMA, GPT-3.5) and smaller ones (e.g., Phi-2, DistilBERT) to determine if the generalization difference scales with model size or is specific to the Gemma and Mistral families used in this study.

3. **Real-World Task Performance:** Beyond pairwise accuracy, evaluate the actual quality of responses generated by models fine-tuned with DPO versus RLHF with EXRM on downstream tasks such as summarization, question answering, and dialogue generation to assess practical implications of the findings.