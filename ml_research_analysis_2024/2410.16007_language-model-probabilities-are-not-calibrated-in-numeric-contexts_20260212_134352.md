---
ver: rpa2
title: Language Model Probabilities are Not Calibrated in Numeric Contexts
arxiv_id: '2410.16007'
source_url: https://arxiv.org/abs/2410.16007
tags:
- blue
- gold
- pink
- white
- black
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) exhibit poor calibration when generating
  probabilities in numeric contexts, often producing outputs that deviate significantly
  from context-defined likelihoods. This work introduces three templated datasets
  (colors, wordproblems, distributions) to evaluate if model probabilities align with
  numeric information in text.
---

# Language Model Probabilities are Not Calibrated in Numeric Contexts

## Quick Facts
- arXiv ID: 2410.16007
- Source URL: https://arxiv.org/abs/2410.16007
- Reference count: 40
- Large language models exhibit poor calibration when generating probabilities in numeric contexts

## Executive Summary
Large language models (LLMs) exhibit poor calibration when generating probabilities in numeric contexts, often producing outputs that deviate significantly from context-defined likelihoods. This work introduces three templated datasets (colors, wordproblems, distributions) to evaluate if model probabilities align with numeric information in text. Using metrics like Wasserstein Distance and Relative Entropy, experiments across open-source and proprietary models reveal systematic biases based on word identity, order, and frequency. Instruction-tuning improves probability mass allocation but fails to ensure proper calibration, with models showing mode collapse and preference for certain numbers or options regardless of context. Frequency effects in training data further exacerbate miscalibration. The findings highlight significant risks in relying on LLMs for probabilistic decision-making, as models consistently exhibit uncalibrated behavior even in simple scenarios.

## Method Summary
The study evaluates whether language models produce probabilities calibrated to numeric contexts using three templated datasets: colors (165K problems), wordproblems (33.6K problems), and distributions (4.5K problems). The evaluation uses three metrics: Probability Mass (PM), Wasserstein Distance (WD), and Relative Entropy (RE). The experiments test base and instruction-tuned versions of multiple open-source and proprietary models, with instruction-tuned models prompted to calibrate outputs to numbers in context. Model logits are analyzed to compute probabilities for relevant tokens, and performance is compared against baselines like Pick Higher and Random.

## Key Results
- LLMs exhibit systematic biases based on word identity, order, and frequency when generating probabilities
- Instruction-tuning improves probability mass allocation but causes mode collapse and reduces entropy
- Frequency effects of number tokens in training data correlate with calibration scores, with larger frequency gaps leading to worse calibration for base models but improved calibration for instruction-tuned models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models' probability distributions are poorly calibrated to numeric contexts due to systematic biases in token selection.
- Mechanism: When generating text, LMs exhibit biases based on word identity and order, often selecting options based on position or frequency in training data rather than the numeric probabilities implied by the context.
- Core assumption: The training data contains frequency imbalances for different numbers and words, leading to systematic biases in model behavior.
- Evidence anchors:
  - [abstract] "artifacts like word identity, word order, and word frequency all impact calibration."
  - [section] "We find that many LMs are poorly calibrated in numeric contexts. Moreover, they fail in systematic ways."
  - [corpus] "Language model (LM) probability is not a reliable quality estimator, as natural language is ambiguous. When multiple output options are valid, the model's probability distribution is spread across them, which can misleadingly indicate low output quality."

### Mechanism 2
- Claim: Instruction-tuning improves probability mass allocation but fails to ensure proper calibration, often causing mode collapse.
- Mechanism: While instruction-tuning increases the total probability mass on valid options, it reduces the entropy of the output distribution, causing the model to over-allocate probability to a single option rather than distributing it according to the context-defined probabilities.
- Core assumption: The instruction-tuning process optimizes for certainty and correctness, which inadvertently reduces the model's ability to express uncertainty when the context implies multiple valid options.
- Evidence anchors:
  - [abstract] "Instruction-tuning improves probability mass allocation but fails to ensure proper calibration, with models showing mode collapse and preference for certain numbers or options regardless of context."
  - [section] "Instruction-tuning appears to cause this mode collapse. Across all three datasets, there is a drastic reduction in entropy between the base and chat model versions."
  - [corpus] "Calibrated Large Language Models for Binary Question Answering" suggests that calibration remains a challenge even after fine-tuning.

### Mechanism 3
- Claim: Frequency effects of number tokens in training data impact calibration, with larger frequency gaps correlating with worse calibration for base models but improved calibration for instruction-tuned models.
- Mechanism: The frequency of number tokens in training data creates biases where models are more likely to select options associated with more frequent numbers, regardless of the context-defined probabilities.
- Core assumption: The training data distribution of numbers is not uniform, and this distribution affects the model's learned biases.
- Evidence anchors:
  - [abstract] "Frequency effects of the number tokens in the training data impact calibration."
  - [section] "Across almost all models, there is a high and significant correlation between number frequencies and average calibration scores."
  - [corpus] "Language model (LM) probability is not a reliable quality estimator, as natural language is ambiguous."

## Foundational Learning

- Concept: Probability theory and calibration
  - Why needed here: Understanding how probability distributions should align with context-defined likelihoods is fundamental to evaluating whether LMs are calibrated.
  - Quick check question: If a fair coin is flipped, what should be the probability distribution over "Heads" and "Tails" in a calibrated model's output?

- Concept: Entropy and information theory
  - Why needed here: Entropy measures the uncertainty in a probability distribution, and mode collapse is characterized by reduced entropy in the model's output.
  - Quick check question: If a model outputs 90% probability for one option and 10% for another, what is the entropy of this distribution compared to a uniform distribution?

- Concept: Frequency analysis and bias detection
  - Why needed here: Understanding how frequency imbalances in training data can create systematic biases in model behavior is crucial for interpreting the results.
  - Quick check question: If the number "7" appears 10 times more frequently than "8" in training data, how might this affect a model's preference for these numbers when making probabilistic choices?

## Architecture Onboarding

- Component map: Data generation -> Model interfaces -> Evaluation metrics -> Analysis tools -> Human experiments
- Critical path: Generate templated datasets with controlled numeric contexts -> Run models on datasets and collect output probabilities -> Calculate evaluation metrics (PM, WD, RE) -> Analyze results for systematic biases and frequency effects -> Interpret findings in terms of calibration and model behavior
- Design tradeoffs:
  - Using templated datasets provides controlled conditions but may not capture all real-world scenarios
  - Focusing on simple numeric contexts allows clear evaluation but may not reflect complex probabilistic reasoning
  - Measuring calibration through output probabilities rather than sampling introduces approximation but enables direct comparison
- Failure signatures:
  - Low probability mass on valid options suggests model doesn't understand the task
  - High Wasserstein distance indicates poor calibration
  - Negative relative entropy indicates mode collapse
  - Significant correlation between frequency gaps and calibration scores suggests frequency bias
- First 3 experiments:
  1. Run a simple "fair coin flip" test with equal numbers of options to check basic calibration
  2. Test models with options that have large frequency gaps to observe frequency bias effects
  3. Compare base vs. instruction-tuned versions on the same tasks to isolate the effect of instruction-tuning on calibration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do language models perform on calibration tasks involving mathematical concepts like Bayes' theorem or De Morgan's laws as the models' capabilities improve?
- Basis in paper: [explicit] The paper states: "As models improve, it would be exciting if datasets targeted concepts like Bayes' theorem, De Morgan's laws, event independence, etc."
- Why unresolved: The current study only tests basic ratios and probability, leaving the performance on more complex mathematical concepts unexplored.
- What evidence would resolve it: Creating and testing datasets with problems involving Bayes' theorem, De Morgan's laws, and other advanced mathematical concepts would provide insights into model performance as their capabilities advance.

### Open Question 2
- Question: To what extent does the frequency of number tokens in training data explain differences in calibration scores, and how does this vary across different models and datasets?
- Basis in paper: [explicit] The paper investigates frequency effects using infini-gram over Dolma and finds correlations between number frequencies and calibration scores.
- Why unresolved: While the paper finds clear frequency effects, it acknowledges that experiments examining a wider range of numbers and frequencies would better study this problem.
- What evidence would resolve it: Conducting experiments with a broader range of numbers and analyzing frequency effects across different models and datasets would provide a more comprehensive understanding of this issue.

### Open Question 3
- Question: What are the systematic biases exhibited by different models when making probabilistic choices, and how do these biases impact real-world applications?
- Basis in paper: [explicit] The paper finds that models exhibit systematic biases based on word identity and order, and discusses the risks of using uncalibrated models in probabilistic scenarios.
- Why unresolved: While the paper identifies these biases, it does not fully explore their implications for real-world applications or provide solutions to mitigate them.
- What evidence would resolve it: Analyzing the impact of these biases on specific real-world applications and developing methods to reduce or account for them would help address this question.

## Limitations
- The study uses templated datasets with controlled numeric contexts that may not fully capture real-world probabilistic reasoning scenarios
- The frequency analysis assumes a direct relationship between training data statistics and model behavior, which may be oversimplified
- The extent to which findings generalize to more naturalistic language tasks and complex probabilistic reasoning remains unclear

## Confidence

- **High Confidence**: The identification of systematic biases based on word identity, order, and frequency is well-supported by the evidence. The correlation between frequency gaps and calibration scores across multiple models and datasets is robust.
- **Medium Confidence**: The claim that instruction-tuning causes mode collapse is supported by the entropy reduction observed, but the underlying mechanism could involve other factors beyond the instruction-tuning process itself.
- **Medium Confidence**: The assertion that LMs are poorly calibrated in numeric contexts is strongly supported for the tested scenarios, but confidence decreases when extrapolating to broader contexts and tasks.

## Next Checks
1. Test model calibration on unstructured, real-world text containing implicit numeric information (e.g., weather forecasts, sports statistics, financial reports) to assess generalization beyond templated datasets.

2. Conduct a comprehensive analysis of number token frequencies in actual training corpora to establish the empirical relationship between training data statistics and model biases, potentially using methods like influence functions.

3. Experiment with fine-tuning approaches that explicitly preserve or encourage appropriate uncertainty expression (e.g., using calibration-aware loss functions or entropy regularization) to determine if instruction-tuning's mode collapse can be mitigated.