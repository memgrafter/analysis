---
ver: rpa2
title: 'Mixture of Nested Experts: Adaptive Processing of Visual Tokens'
arxiv_id: '2407.19985'
source_url: https://arxiv.org/abs/2407.19985
tags:
- nested
- mone
- tokens
- experts
- router
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of Vision Transformers (ViTs)
  and Video ViTs (ViViTs) which process all tokens equally, ignoring inherent redundancy
  and leading to high computational costs. The authors propose Mixture of Nested Experts
  (MoNE), a framework that dynamically routes tokens to nested expert models of varying
  computational capacity.
---

# Mixture of Nested Experts: Adaptive Processing of Visual Tokens

## Quick Facts
- arXiv ID: 2407.19985
- Source URL: https://arxiv.org/abs/2407.19985
- Authors: Gagan Jain; Nidhi Hegde; Aditya Kusupati; Arsha Nagrani; Shyamal Buch; Prateek Jain; Anurag Arnab; Sujoy Paul
- Reference count: 40
- Primary result: Achieves over 2x reduction in inference-time compute while maintaining equivalent performance on ImageNet-21K, Kinetics-400, and Something-Something-v2

## Executive Summary
Vision Transformers (ViTs) and Video ViTs (ViViTs) process all tokens equally, ignoring inherent redundancy and leading to high computational costs. Mixture of Nested Experts (MoNE) addresses this by dynamically routing tokens to nested expert models of varying computational capacity. More informative tokens are processed by larger models while redundant tokens use cheaper nested models. MoNE achieves over 2x reduction in inference-time compute compared to baseline models while maintaining equivalent performance, and demonstrates strong adaptability across different compute budgets using a single trained model.

## Method Summary
MoNE builds on ViT/ViViT architecture by adding dynamic routing to nested experts. The framework uses a single router at the first layer to produce probability distributions over E nested expert blocks with exponentially-spaced dimensions. Expert Preferred Routing (EPR) algorithm greedily assigns tokens to experts under capacity constraints based on router predictions. The nested model structure maintains parameter count while varying compute requirements through structured slices of the parameter space. This enables efficient computation scaling by extracting smaller models from the full model without increasing total parameters.

## Key Results
- Achieves over 2x reduction in inference-time compute compared to baseline models
- Maintains equivalent performance on ImageNet-21K, Kinetics-400, and Something-Something-v2
- Demonstrates strong adaptability across different compute budgets using a single trained model
- Reduces average token compute by 2.4x on Kinetics-400 with 0.2% accuracy drop

## Why This Works (Mechanism)

### Mechanism 1
MoNE achieves over 2x reduction in inference-time compute by dynamically routing tokens to nested experts of varying capacity. The framework uses a learned router to assign tokens to different nested expert models based on their information content, where more informative tokens are processed by larger (more computationally expensive) models while redundant tokens use cheaper nested models. Core assumption: Visual tokens exhibit inherent redundancy that can be exploited for efficiency without sacrificing performance.

### Mechanism 2
The nested expert structure maintains parameter count while varying compute requirements. By using structured slices of the parameter space (partial projections W[:D/m][:D/m]), smaller models can be extracted from the full model without increasing total parameters, enabling efficient computation scaling. Core assumption: Information can be encoded hierarchically in feature dimensions, with the first k dimensions containing the most relevant information.

### Mechanism 3
Expert Preferred Routing (EPR) algorithm ensures optimal compute-performance tradeoff by prioritizing larger models for more important tokens. EPR greedily assigns tokens to experts under capacity constraints based on router predictions, giving higher preference to larger nested models to identify the most important tokens first. Core assumption: Router predictions accurately reflect token importance, and capacity constraints can be optimally distributed across experts.

## Foundational Learning

- Concept: Vision Transformer (ViT) architecture and tokenization process
  - Why needed here: MoNE builds directly on ViT by adding dynamic routing to nested experts, so understanding the base architecture is essential
  - Quick check question: What is the dimensionality of token embeddings after ViT tokenization and how does patch size affect the number of tokens?

- Concept: Mixture of Experts (MoE) routing mechanisms
  - Why needed here: MoNE extends MoE concepts but with nested experts instead of equal-sized experts, requiring understanding of both traditional MoE and the novel nesting approach
  - Quick check question: How does expert-choice routing differ from token-choice routing in traditional MoE, and what are the trade-offs?

- Concept: Nested model extraction and parameter slicing
  - Why needed here: The core efficiency mechanism relies on extracting smaller models from the full model through partial projections, which is the foundation of MoNE's compute savings
  - Quick check question: Given a weight matrix W of dimension DxD, how would you extract the first D/4 dimensions to create a nested model, and what information might be lost?

## Architecture Onboarding

- Component map: Tokenization -> Router network -> EPR assignment -> Nested expert blocks (E blocks with [D/8, D/4, D/2, D] dimensions) -> Self-attention interaction -> Output features
- Critical path: Tokenization → Router prediction → EPR assignment → Nested expert processing → Self-attention interaction → Output features
- Design tradeoffs:
  - Single router vs. multiple routers (simplicity vs. potentially better feature utilization)
  - Exponential spacing of expert dimensions vs. linear spacing (efficiency vs. granularity)
  - Router placement at first layer vs. later layers (early compute savings vs. richer features)
- Failure signatures:
  - Router collapse (all tokens routed to same expert) - check router loss and capacity distribution
  - Load imbalance across experts - monitor EPR assignments and adjust capacity distribution
  - Performance degradation with compute reduction - verify token redundancy assumptions and router accuracy
- First 3 experiments:
  1. Verify router predictions correlate with visual importance by visualizing tokens routed to largest expert on sample images
  2. Test capacity distribution optimization by running with different (β, δ) parameters and measuring accuracy-efficiency trade-offs
  3. Evaluate adaptation capability by training at one compute budget and testing at multiple different budgets to verify single-model flexibility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MoNE's performance scale with larger numbers of nested experts (E > 4) beyond the tested configurations?
- Basis in paper: [explicit] The paper tests with E=4 experts but mentions "we explore various design choices" without extensive scaling analysis
- Why unresolved: The paper only tests with E=4 experts and doesn't explore how performance changes with more expert levels
- What evidence would resolve it: Empirical results comparing MoNE performance with E=2, 4, 8, 16 experts across various model sizes and datasets

### Open Question 2
- Question: What is the impact of router placement on MoNE's performance when using multiple routers versus a single router at the first layer?
- Basis in paper: [explicit] Section 6 discusses router placement and number of routers, showing performance degradation with multiple routers
- Why unresolved: While the paper tests router placement at different layers, it doesn't explore optimal router distribution or the trade-off between router depth and computational savings
- What evidence would resolve it: Systematic experiments varying router positions and quantities, measuring both performance and computational efficiency trade-offs

### Open Question 3
- Question: How does MoNE's token routing strategy compare to adaptive token pruning approaches in terms of information retention and final accuracy?
- Basis in paper: [inferred] The paper mentions that unlike MoD which "completely skipping tokens," MoNE "makes fuzzy decisions to choose intermediate-sized models" and discusses information retention
- Why unresolved: The paper doesn't provide direct comparisons with token pruning methods or quantitative analysis of information retention between the two approaches
- What evidence would resolve it: Head-to-head comparisons between MoNE and token pruning methods on the same tasks, measuring both final accuracy and information retention metrics

## Limitations
- Capacity distribution optimization sensitivity not thoroughly explored across different model scales and datasets
- Performance characteristics at extreme model scales (very small and very large) remain untested
- Computational overhead of router network itself across different input resolutions and batch sizes not quantified

## Confidence
- High Confidence: The fundamental efficiency mechanism of nested expert routing and the claimed 2x compute reduction appear well-supported by experimental results across multiple datasets
- Medium Confidence: The adaptation capability claims are demonstrated but with limited exploration of the full range of possible budgets
- Low Confidence: The claim about maintaining equivalent performance while reducing compute relies heavily on the assumption of token redundancy, which is stated but not directly validated through ablation studies

## Next Checks
1. Conduct ablation studies varying the capacity distribution parameters (β, δ) across a wider range to understand the sensitivity of MoNE's performance to these critical hyperparameters, particularly on smaller datasets where compute budgets are more constrained.
2. Implement a controlled experiment comparing MoNE against a baseline where tokens are randomly routed to nested experts (rather than using learned router predictions) to directly validate the importance of the router's learned token importance assessment.
3. Profile the computational overhead of the router network itself across different input resolutions and batch sizes to quantify the true compute savings, particularly for scenarios with high-resolution inputs where tokenization produces many tokens.