---
ver: rpa2
title: 'IndicSentEval: How Effectively do Multilingual Transformer Models encode Linguistic
  Properties for Indic Languages?'
arxiv_id: '2410.02611'
source_url: https://arxiv.org/abs/2410.02611
tags:
- languages
- language
- probing
- tasks
- indic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INDICSENTEVAL, a multilingual benchmark dataset
  of ~47K sentences in six Indic languages, to probe and assess robustness of 9 multilingual
  Transformer models across 8 linguistic properties. Probing tasks target surface,
  syntactic, and semantic features, with 13 text perturbations testing model robustness.
---

# IndicSentEval: How Effectively do Multilingual Transformer Models encode Linguistic Properties for Indic Languages?

## Quick Facts
- arXiv ID: 2410.02611
- Source URL: https://arxiv.org/abs/2410.02611
- Reference count: 40
- Key outcome: Indic-specific models (MuRIL, IndicBERT) best capture linguistic properties, while universal models (InfoXLM, BLOOM, mGPT, mT5, XGLM) exhibit superior robustness to perturbations.

## Executive Summary
This paper introduces INDICSENTEVAL, a multilingual benchmark dataset of ~47K sentences in six Indic languages, to probe and assess robustness of 9 multilingual Transformer models across 8 linguistic properties. Probing tasks target surface, syntactic, and semantic features, with 13 text perturbations testing model robustness. Results show Indic-specific models best capture linguistic properties, while universal models exhibit superior robustness to perturbations. Models trained with larger Indic language corpora demonstrate higher proficiency. Notably, English is encoded consistently across models, unlike Indic languages which show mixed results. The study reveals that robustness to perturbations and linguistic structure encoding vary significantly across languages and models, emphasizing the need for robust, language-agnostic architectures.

## Method Summary
The study evaluates 9 multilingual Transformer models on 8 linguistic probing tasks across 6 Indic languages using the INDICSENTEVAL dataset. Probing tasks cover surface (SentLen), syntactic (BShift, TreeDepth), and semantic (SubjNum, ObjNum, VerbGen, VerbNum, VerbPer) properties. Models are assessed using logistic regression classifiers with frozen base representations, mean-pooling for sentence representations, and stratified 5-fold cross-validation. Robustness is evaluated by comparing clean and perturbed test sets across 13 text perturbation types. Layer-wise analysis examines how different linguistic properties are encoded across model layers.

## Key Results
- Indic-specific models (MuRIL, IndicBERT) best capture linguistic properties but are less robust to perturbations
- Universal models (InfoXLM, BLOOM, mGPT, mT5, XGLM) show superior robustness to perturbations, especially for noun/verb dropping
- Models trained with larger Indic language corpora demonstrate higher proficiency on linguistic tasks
- English is encoded consistently across all models, unlike Indic languages which show mixed results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual models encode linguistic properties better when trained with larger Indic language corpora.
- Mechanism: Models trained on larger, more diverse Indic datasets develop richer, language-specific representations that capture surface, syntactic, and semantic nuances more effectively.
- Core assumption: Training data quantity and linguistic diversity directly improve model capability to encode language-specific properties.
- Evidence anchors:
  - [abstract]: "Models trained with larger Indic language corpora demonstrate higher proficiency."
  - [section]: "Indic-specific models (MuRIL, IndicBERT) best capture linguistic properties...trained on Indic language data only."
- Break condition: If training data quality is poor or unbalanced across languages, even larger datasets may not improve performance.

### Mechanism 2
- Claim: Universal models show better robustness to perturbations than Indic-specific models.
- Mechanism: Universal models, trained on diverse, multi-language data, learn more generalized representations that are less sensitive to word-level noise and structural perturbations.
- Core assumption: Broader linguistic exposure during pretraining leads to more robust, language-agnostic representations.
- Evidence anchors:
  - [abstract]: "Universal models broadly exhibit better robustness compared to Indic-specific models, particularly under perturbations such as dropping both nouns and verbs..."
  - [section]: "Text perturbation analysis reveals that universal models such as InfoXLM, BLOOM, mGPT, XGLM and mT5 demonstrate higher resilience to perturbations..."
- Break condition: If perturbation types are language-specific and not well-represented in pretraining, robustness may degrade.

### Mechanism 3
- Claim: Linguistic property encoding and robustness vary significantly across layers in multilingual models.
- Mechanism: Different layers capture different linguistic abstractions; surface features are encoded in early layers, while deeper layers handle syntactic and semantic complexity, with perturbation sensitivity also layer-dependent.
- Core assumption: Layer-wise specialization exists for linguistic property encoding, and perturbation impact is not uniform across layers.
- Evidence anchors:
  - [section]: "Surface-level tasks...higher accuracy in the early (or lower) layers...TreeDepth...higher probing accuracy in the middle layers...For the verb-related tasks...last layer is the most predictive..."
  - [section]: "Text perturbation analysis reveals that...perturbation impact for every pair of (model, language), (model, probing task), (perturbation, model) and (language, probing task)..."
- Break condition: If models lack sufficient depth or have uniform attention mechanisms, layer-wise differences may not emerge.

## Foundational Learning

- Concept: Linguistic probing tasks (surface, syntactic, semantic)
  - Why needed here: Probing tasks are the primary method to evaluate what linguistic properties multilingual models encode; understanding their design and interpretation is critical for replicating or extending this study.
  - Quick check question: What is the difference between a surface-level probing task (like SentLen) and a semantic probing task (like VerbGen)?

- Concept: Text perturbation techniques and robustness evaluation
  - Why needed here: Perturbations simulate real-world noise and help assess model robustness; knowing how to construct and analyze perturbations is essential for robustness testing.
  - Quick check question: Which perturbation types are most likely to degrade semantic task performance, and why?

- Concept: Multilingual model architectures (BERT-like vs. decoder-based)
  - Why needed here: Different architectures (e.g., mBERT, BLOOM, mT5) have distinct strengths and weaknesses in encoding and robustness; understanding these differences is key to interpreting results.
  - Quick check question: How do the architectural differences between encoder-only and decoder-based models influence their layer-wise probing performance?

## Architecture Onboarding

- Component map: Dataset curation -> Probing task design -> Text perturbation generation -> Model representation extraction -> Logistic regression classification -> Layer-wise analysis -> Robustness scoring
- Critical path: Curate dataset → Design probing tasks → Generate perturbations → Extract model representations → Train logistic regression classifiers → Evaluate accuracy and robustness → Analyze layer-wise trends and model comparisons
- Design tradeoffs: (a) Using a simple logistic regression classifier keeps the probe lightweight but may miss complex interactions; (b) focusing on 6 Indic languages provides depth but limits generalizability; (c) using 13 perturbations provides robustness insights but increases experimental complexity
- Failure signatures: Low probing accuracy may indicate poor linguistic encoding; large accuracy drops after perturbations suggest brittleness; inconsistent layer-wise trends may reveal architectural limitations or data quality issues
- First 3 experiments:
  1. Run SentLen probing across all models and languages to establish baseline surface-level encoding
  2. Apply a simple perturbation (e.g., DropNV) and compare robustness scores across models
  3. Analyze layer-wise probing accuracy for one syntactic task (e.g., TreeDepth) to observe hierarchical encoding patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Indic language-specific models like MuRIL and IndicBERT compare to universal models like mBERT in terms of robustness to syntactic perturbations across different Indic languages?
- Basis in paper: [explicit] The paper explicitly compares the robustness of Indic-specific models (MuRIL, IndicBERT) and universal models (mBERT) to various perturbations, showing that universal models like InfoXLM, BLOOM, mT5, and mGPT exhibit higher resilience to perturbations than BERT-like models (mBERT, IndicBERT, MuRIL), particularly under perturbations such as dropping both nouns and verbs, dropping only verbs, or keeping only nouns.
- Why unresolved: While the paper highlights that universal models are generally more robust, it does not provide a detailed, model-by-model comparison of robustness to syntactic perturbations across all Indic languages, leaving gaps in understanding the specific strengths and weaknesses of each model.
- What evidence would resolve it: Detailed experimental results comparing the robustness of each model to syntactic perturbations across all six Indic languages, including statistical significance tests and visualizations of performance trends.

### Open Question 2
- Question: To what extent does the size of the pretraining dataset influence the ability of multilingual models to capture semantic properties in low-resource Indic languages?
- Basis in paper: [inferred] The paper suggests that models with larger pretraining datasets, such as mT5 and MuRIL, perform better on semantic tasks in Marathi and Telugu, implying a correlation between dataset size and semantic understanding. However, it does not explicitly analyze the relationship between dataset size and semantic property capture across all languages.
- Why unresolved: The paper does not provide a comprehensive analysis of how pretraining dataset size affects semantic property capture in low-resource languages, leaving uncertainty about the generalizability of this trend.
- What evidence would resolve it: A systematic study varying the size of pretraining datasets for each language and measuring the impact on semantic property capture, along with a correlation analysis between dataset size and semantic performance.

### Open Question 3
- Question: How do the layer-wise trends in probing accuracy for syntactic tasks differ between encoder-based and decoder-based universal models, and what architectural factors contribute to these differences?
- Basis in paper: [explicit] The paper notes that encoder-based universal models (e.g., mBERT, XLM-R) and Indic models show higher accuracy for TreeDepth in middle layers, while decoder-based models (e.g., BLOOM, mGPT) exhibit less clear trends. It also mentions that decoder-based models might excel in sequential processing tasks due to unidirectional attention.
- Why unresolved: The paper does not provide a detailed explanation of the architectural factors that lead to these differences in layer-wise trends, nor does it explore how these factors interact with the specific linguistic features of Indic languages.
- What evidence would resolve it: An in-depth analysis of the architectural differences between encoder-based and decoder-based models, including attention mechanisms and layer interactions, combined with probing experiments that isolate the effects of these factors on syntactic task performance.

## Limitations

- The study focuses on only 6 of India's 22 official languages, limiting generalizability across broader linguistic diversity
- Evaluation relies on a single logistic regression probe per task, which may not capture complex linguistic relationships
- Perturbation analysis uses synthetic noise that may not fully reflect real-world text degradation patterns
- Comparison between Indic-specific and universal models may be influenced by differences in training objectives and data quality

## Confidence

- **High Confidence**: Indic-specific models outperform universal models on linguistic property encoding for their target languages
- **Medium Confidence**: Universal models show superior robustness to perturbations, though magnitude varies significantly
- **Medium Confidence**: Layer-wise patterns align with existing literature, though specific patterns may vary by language and task

## Next Checks

1. Replicate the layer-wise probing analysis for a subset of tasks (SentLen, BShift, VerbGen) across all models to verify reported patterns
2. Conduct ablation studies on perturbation types by systematically removing individual categories to isolate impact on semantic vs. syntactic tasks
3. Extend analysis to include probing with a more expressive classifier (e.g., MLP) to determine if patterns persist with increased probe capacity