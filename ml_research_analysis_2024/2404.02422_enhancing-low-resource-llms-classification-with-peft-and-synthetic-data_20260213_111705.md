---
ver: rpa2
title: Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data
arxiv_id: '2404.02422'
source_url: https://arxiv.org/abs/2404.02422
tags:
- data
- examples
- synthetic
- peft
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to improve the performance of Large
  Language Models (LLMs) in low-resource text classification settings. The proposed
  approach involves generating synthetic data using the LLM itself, filtering the
  generated data to remove label-inconsistent examples, and then fine-tuning the LLM
  using Parameter-Efficient Fine-Tuning (PEFT) techniques.
---

# Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data

## Quick Facts
- arXiv ID: 2404.02422
- Source URL: https://arxiv.org/abs/2404.02422
- Reference count: 0
- Primary result: Improves low-resource LLM classification by generating, filtering, and fine-tuning on synthetic data, achieving accuracy comparable to or better than In-Context Learning

## Executive Summary
This paper addresses the challenge of low-resource text classification by proposing a method that generates synthetic training data using the LLM itself, filters out label-inconsistent examples, and fine-tunes the model using Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA. The approach aims to make LLMs as efficient as 0-shot classifiers while achieving accuracy comparable to or better than In-Context Learning (ICL). Experimental results on three text classification datasets demonstrate that the proposed method leads to competitive results, outperforming ICL in some cases and achieving accuracy levels similar to training LoRA with more real examples.

## Method Summary
The proposed method involves a three-step process: first, generating synthetic examples per class using In-Context Learning (ICL) with few real examples; second, filtering out label-inconsistent synthetic examples using ICL classification; and third, fine-tuning the LLM using LoRA on the combined real and filtered synthetic data. The approach targets low-resource settings with only 4 examples per class, leveraging the LLM's generation capabilities to overcome data scarcity. The same LLM is used throughout all steps to maintain consistency and avoid external biases.

## Key Results
- Achieves 92.03% accuracy on SST2 dataset compared to 90.68% with vanilla LoRA on real data alone
- Outperforms ICL in some cases while maintaining similar accuracy levels to training with more real examples
- Demonstrates effectiveness across three text classification datasets (SST2, TREC, AG News)

## Why This Works (Mechanism)

### Mechanism 1
Synthetic data generation via ICL allows the LLM to leverage its generation capabilities to produce more examples, overcoming the scarcity of real data. The LLM generates new examples for each class by conditioning on a few real examples, then filters out label-inconsistent generations using ICL classification. This works because the LLM already has latent knowledge of the classification task and can generate valid examples that match the intended class.

### Mechanism 2
PEFT with LoRA becomes effective when trained on a larger dataset (synthetic + real), preventing underfitting/overfitting in low-resource settings. By combining few real examples with a large set of filtered synthetic examples, the LoRA adaptation learns stable task-relevant parameters instead of overfitting to noise. This assumes the synthetic data distribution is similar enough to the real data that it helps the model learn the task rather than just memorizing noise.

### Mechanism 3
Using the same LLM for all three steps (generation, filtering, training) avoids introducing external biases and leverages the model's internal consistency. The LLM's self-generated and self-filtered data maintains coherence with the model's learned representations, improving downstream fine-tuning stability. This assumes the LLM's ICL classification of its own generations is accurate enough to filter out label-inconsistent examples.

## Foundational Learning

- **In-Context Learning (ICL)**: Mechanism by which the LLM generates and classifies examples without parameter updates, enabling zero-shot data augmentation. Why needed: Enables synthetic data generation and filtering without additional training. Quick check: How does ICL differ from standard fine-tuning in terms of parameter usage and inference cost?

- **Parameter-Efficient Fine-Tuning (PEFT) / LoRA**: Updates only a small subset of parameters, making fine-tuning feasible on limited data and hardware while preserving the base LLM's knowledge. Why needed: Makes fine-tuning practical in low-resource settings. Quick check: What is the rank parameter in LoRA and how does it affect the number of trainable parameters?

- **Data filtering and consistency checking**: Removes label-inconsistent generations that could mislead the fine-tuning process, ensuring synthetic data quality. Why needed: Ensures synthetic data is accurate and useful for training. Quick check: What are common heuristics for detecting label-inconsistent generations in a classification task?

## Architecture Onboarding

- **Component map**: Vicuna LLM -> Synthetic Data Generation -> Filtering -> LoRA Fine-tuning -> Evaluation
- **Critical path**: 1) Generate synthetic examples per class using ICL, 2) Filter out label-inconsistent examples via ICL classification, 3) Combine filtered synthetic data with real data, 4) Fine-tune LLM with LoRA on combined dataset, 5) Evaluate on test sets
- **Design tradeoffs**: Using same LLM for all steps avoids external bias but risks compounding errors; temperature and decoding settings affect synthetic data diversity vs. quality; LoRA rank and alpha balance adaptation capacity vs. overfitting risk
- **Failure signatures**: Low synthetic data diversity → overfitting on limited patterns; high label-inconsistency in filtered data → poor fine-tuning performance; slow generation/filtration → increased training time
- **First 3 experiments**: 1) Run ICL baseline with 4 examples per class to establish reference accuracy, 2) Generate synthetic data with temperature=1.0, top_k=50, num_beams=1, then filter and fine-tune, 3) Vary synthetic data size (e.g., 5, 10, 20 examples per class) to find optimal augmentation level

## Open Questions the Paper Calls Out

### Open Question 1
How does the diversity of synthetic data impact the performance of the fine-tuned model compared to the diversity of real data? The paper mentions that increasing the size of synthetic data does not always provide a clear benefit and that the lack of diversity in the synthetic data might be a reason. It also shows that the diversity of real data increases faster as the data size increases compared to synthetic data. This remains unresolved because the paper does not provide a detailed analysis of how synthetic data diversity affects model performance or how it compares to real data diversity in terms of generalization.

### Open Question 2
How do different filtering strategies affect the quality and performance of the synthetic data used for fine-tuning? The paper mentions that a basic filtering step is applied to remove duplicates and malformed generations, and that ICL is used to classify the generated data and remove label-inconsistent examples. It also notes that the filtering step improves the accuracy of the model. This remains unresolved because the paper does not explore or compare different filtering strategies or their effectiveness in improving synthetic data quality.

### Open Question 3
How do the proposed method's performance and efficiency compare to other parameter-efficient fine-tuning techniques in low-resource settings? The paper mentions that the proposed method is comparable to or better than ICL and vanilla LoRA in low-resource settings. However, it does not provide a direct comparison with other parameter-efficient fine-tuning techniques such as Prefix-Tuning or P-Tuning. This remains unresolved because the paper does not explore or compare the performance and efficiency of the proposed method with other parameter-efficient fine-tuning techniques.

## Limitations

- The evaluation doesn't provide comprehensive ablation studies on the filtering step's effectiveness across different noise levels
- The claim that synthetic data diversity matches real data is questionable without statistical significance testing
- The temperature=1.0 setting for generation likely introduces substantial noise that isn't fully quantified in terms of its impact on downstream performance

## Confidence

- **High confidence**: The core observation that combining few real examples with filtered synthetic data improves LoRA fine-tuning performance compared to using real data alone
- **Medium confidence**: The claim that the same LLM performing all three steps avoids external bias
- **Low confidence**: The assertion that synthetic data generation fully overcomes the data scarcity problem

## Next Checks

1. **Synthetic Data Quality Analysis**: Conduct a systematic analysis of synthetic data quality by varying the filtering threshold and measuring how many examples are removed at each stage. Compare the semantic diversity and label consistency of synthetic data across different temperature settings (0.5, 1.0, 1.5) to establish the optimal balance between diversity and accuracy.

2. **Cross-Model Generalization Test**: Evaluate whether synthetic data generated by Vicuna can effectively fine-tune other LLMs (e.g., Llama, MPT) or whether the approach is specific to the model used for generation. This would test the hypothesis that the method leverages general task knowledge rather than model-specific biases.

3. **Long-Tail Performance Evaluation**: Analyze model performance on rare or difficult examples within each dataset by creating stratified test subsets. Measure whether synthetic data generation helps the model learn patterns that few-shot examples miss, particularly for underrepresented classes or complex linguistic phenomena.