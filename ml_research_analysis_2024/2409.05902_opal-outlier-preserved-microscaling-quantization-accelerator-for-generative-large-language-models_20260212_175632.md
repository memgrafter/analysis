---
ver: rpa2
title: 'OPAL: Outlier-Preserved Microscaling Quantization Accelerator for Generative
  Large Language Models'
arxiv_id: '2409.05902'
source_url: https://arxiv.org/abs/2409.05902
tags:
- opal
- mx-opal
- quantization
- outliers
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OPAL, a hardware accelerator for generative
  large language models (LLMs) that improves energy efficiency by quantizing both
  weights and activations. The key innovation is a microscaling data format (MX-OPAL)
  that preserves a few outliers per block while quantizing non-outliers to low bit-width
  (3/5-bit or 4/7-bit).
---

# OPAL: Outlier-Preserved Microscaling Quantization Accelerator for Generative Large Language Models

## Quick Facts
- arXiv ID: 2409.05902
- Source URL: https://arxiv.org/abs/2409.05902
- Authors: Jahyun Koo; Dahoon Park; Sangwoo Jung; Jaeha Kung
- Reference count: 16
- Key outcome: 1.6-2.2× energy efficiency improvement and 2.4-3.1× area reduction with <1 perplexity increase

## Executive Summary
OPAL is a hardware accelerator designed to improve energy efficiency for generative large language models (LLMs) through quantization. The key innovation is MX-OPAL, a microscaling data format that preserves a few outliers per block while quantizing non-outliers to low bit-width (3/5-bit or 4/7-bit). This is combined with mixed-precision quantization and a log2-based softmax approximation to maximize power efficiency. The accelerator achieves significant energy and area improvements while maintaining model accuracy.

## Method Summary
OPAL introduces MX-OPAL, a microscaling quantization format that preserves four outliers per 128-element block in bfloat16 while quantizing the remaining elements to low bit-width using a shared exponent. The hardware includes FP units for outlier computations and vectorized INT multipliers for non-outliers, with configurable low-low (3/5-bit) and low-high (4/7-bit) modes. A novel log2-based softmax approximation reduces hardware complexity by replacing floating-point dividers with shift-and-subtract operations. The design uses mixed-precision quantization, applying higher bit-width to sensitive layers while aggressively quantizing others.

## Key Results
- 1.6-2.2× energy efficiency improvement compared to baselines
- 2.4-3.1× area reduction while maintaining accuracy
- <1 perplexity increase across evaluated models and datasets
- 1.56× power savings from log2-based softmax approximation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preserving a small number of outliers per block while quantizing the rest to low-bit-width reduces overall quantization error without significantly increasing memory overhead.
- Mechanism: In MX-OPAL, four largest absolute values per 128-element block are kept in bfloat16, while the rest are quantized to 3/5-bit or 4/7-bit using a shared exponent per block.
- Core assumption: The memory overhead of storing outliers in 16-bit is small relative to the gains in accuracy, because only a tiny fraction (e.g., 4/128 = 3.125%) of elements are preserved in higher precision.
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 2
- Claim: The log2-based softmax approximation reduces hardware complexity and power consumption by replacing floating-point dividers with shift-and-subtract operations.
- Mechanism: The softmax output is approximated using log2 quantization: ⌈log2(2^(-⌈log2(x)⌉))⌉, implemented as a difference of exponents and mantissas followed by conditional rounding.
- Core assumption: The approximation error introduced by the log2-based softmax is small enough (<0.4 perplexity increase) to maintain model accuracy while achieving large power savings (1.56×).
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 3
- Claim: Using a shared exponent per block (microscaling) allows non-outlier elements to be represented with fewer bits while enabling efficient shift-based scaling, eliminating the need for dividers.
- Mechanism: All non-outlier elements in a block share one exponent (the second largest in the block after removing the top outliers). Each element's value is computed as (significand << (shared_exponent - element_exponent)), which is just a bit shift, not a division.
- Core assumption: The block size (128) is large enough that the shared exponent captures the typical range well, and the quantization error introduced by this coarse scaling is acceptable for non-outliers.
- Evidence anchors: [section], [abstract], [corpus]

## Foundational Learning

- Concept: Activation outlier dynamics in LLMs
  - Why needed here: Understanding why some activation channels consistently produce large values is key to designing effective outlier preservation strategies.
  - Quick check question: What percentage of activation elements in typical LLM layers are considered outliers that require special handling?

- Concept: Microscaling (block floating-point) data representation
  - Why needed here: The shared exponent per block enables aggressive bit-width reduction for non-outliers while maintaining dynamic range through shift operations.
  - Quick check question: How does the shared exponent selection (second largest after removing top outliers) impact quantization error compared to using the maximum exponent?

- Concept: Softmax approximation techniques
  - Why needed here: Efficient softmax computation is critical for attention mechanisms; understanding trade-offs between accuracy and hardware efficiency guides the log2-based approach.
  - Quick check question: What is the maximum acceptable perplexity increase when approximating softmax for LLM inference?

## Architecture Onboarding

- Component map: Data Distributor -> INT MUs / FP Units -> FP Adder Tree -> MX-OPAL Quantizer
- Critical path: Matrix-vector multiplication (MxV) for Q, K, V, and attention computation -> Log2-based softmax computation for attention map -> Accumulation and conversion back to MX-OPAL format
- Design tradeoffs: Low-bit vs high-bit modes (lower bits save memory but increase quantization error), number of preserved outliers (more outliers improve accuracy but increase memory and FP computation), block size (larger blocks improve memory efficiency but may reduce quantization accuracy for heterogeneous data)
- Failure signatures: Excessive perplexity increase (>1.0) indicates quantization error is too high, memory overhead significantly above 10% suggests outlier preservation is too aggressive, power consumption not improving as expected may indicate inefficient data routing
- First 3 experiments:
  1. Implement MX-OPAL quantizer with varying numbers of preserved outliers (1, 2, 4, 8) and measure perplexity impact on a small LLM layer
  2. Benchmark the log2-based softmax unit against a floating-point baseline to verify the 1.56× power savings claim
  3. Characterize the INT MU performance in low-low, low-high, and high-high modes to confirm the 4× throughput advantage of low-low mode

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does OPAL's performance scale with larger models (e.g., GPT-3 175B) and more complex tasks?
- Basis in paper: [explicit] The paper mentions the potential for scaling to larger models like GPT-3 175B and discusses the challenges in memory and energy consumption for larger models.
- Why unresolved: The paper focuses on Llama2 models up to 70B parameters. While the methodology is applicable to larger models, the actual performance and efficiency gains for models like GPT-3 175B are not evaluated.
- What evidence would resolve it: Benchmarking OPAL on larger models like GPT-3 175B and evaluating its performance and efficiency gains in terms of energy consumption, memory usage, and inference latency.

### Open Question 2
- Question: What is the impact of OPAL's log2-based softmax approximation on the accuracy of other LLM architectures beyond attention layers?
- Basis in paper: [explicit] The paper discusses the log2-based softmax approximation for attention layers and its impact on perplexity. However, it does not explore its applicability to other parts of LLM architectures that use softmax.
- Why unresolved: The paper only evaluates the log2-based softmax approximation in the context of attention layers. Its effectiveness and potential impact on accuracy for other softmax operations within LLM architectures remain unexplored.
- What evidence would resolve it: Evaluating the log2-based softmax approximation on other softmax operations within LLM architectures, such as in the final output layer, and measuring its impact on accuracy and perplexity.

### Open Question 3
- Question: How does OPAL handle dynamic changes in the number of outliers during inference, and what is the overhead associated with this adaptation?
- Basis in paper: [inferred] The paper mentions that OPAL preserves a fixed number of outliers per block (e.g., four out of 128 elements). However, it does not discuss how the hardware adapts to dynamic changes in the number of outliers during inference.
- Why unresolved: The paper does not provide details on how OPAL handles scenarios where the number of outliers varies significantly across different inputs or layers. The potential overhead associated with adapting to these changes is also not addressed.
- What evidence would resolve it: Analyzing the impact of dynamic changes in the number of outliers on OPAL's performance and evaluating the hardware overhead associated with adapting to these changes.

## Limitations
- Evaluation limited to specific LLM architectures (Llama2 and OPT families) and benchmarks (Wikitext-2, C4, ARC, PIQA)
- Microscaling approach assumes relatively stable outlier patterns across layers, which may not hold for all model architectures
- Log2-based softmax approximation error may accumulate in deeper transformer stacks or more complex attention patterns

## Confidence
- High confidence: Energy efficiency measurements and area reduction claims (measured on actual hardware)
- Medium confidence: Perplexity impact assessments (based on software simulations with quantization)
- Medium confidence: Mixed-precision layer sensitivity analysis (based on empirical testing on specific models)

## Next Checks
1. Test OPAL's quantization approach on transformer architectures outside the Llama2/OPT families, including encoder-only models and models with different attention mechanisms, to assess generalizability
2. Evaluate the log2-based softmax approximation on attention patterns with extreme sparsity or very large dynamic ranges to determine breaking points
3. Profile outlier distribution patterns across different training datasets and fine-tuning scenarios to verify the assumption that outlier rates remain stable at ~3%