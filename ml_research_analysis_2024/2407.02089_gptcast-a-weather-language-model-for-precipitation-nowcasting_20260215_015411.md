---
ver: rpa2
title: 'GPTCast: a weather language model for precipitation nowcasting'
arxiv_id: '2407.02089'
source_url: https://arxiv.org/abs/2407.02089
tags:
- precipitation
- nowcasting
- gptcast
- weather
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GPTCast, a novel generative deep-learning
  method for ensemble nowcasting of radar-based precipitation inspired by large language
  models (LLMs). The method employs a GPT model to learn spatiotemporal precipitation
  dynamics using tokenized radar images, where the tokenizer is based on a Quantized
  Variational Autoencoder (VQGAN) with a novel reconstruction loss tailored for the
  skewed distribution of precipitation.
---

# GPTCast: a weather language model for precipitation nowcasting

## Quick Facts
- **arXiv ID**: 2407.02089
- **Source URL**: https://arxiv.org/abs/2407.02089
- **Reference count**: 40
- **Primary result**: Novel GPT-based ensemble nowcasting method with superior performance to LINDA baseline

## Executive Summary
This paper introduces GPTCast, a generative deep learning method for ensemble nowcasting of radar-based precipitation inspired by large language models. The approach uses a two-stage architecture where a VQGAN tokenizer compresses radar precipitation maps into discrete tokens, which are then processed by a GPT model to learn spatiotemporal precipitation dynamics. The method produces realistic ensemble forecasts with accurate uncertainty estimation without requiring randomness during training or inference, unlike other generative deep learning models. GPTCast is trained and tested on a 6-year radar dataset over Northern Italy, showing superior results compared to state-of-the-art ensemble extrapolation methods.

## Method Summary
GPTCast employs a novel two-stage architecture for precipitation nowcasting. First, a VQGAN tokenizer compresses radar precipitation maps into discrete tokens using a new Magnitude Weighted Absolute Error (MWAE) loss function that improves reconstruction of high rainfall rates. Second, a GPT-2 model processes sequences of these tokens to learn spatiotemporal precipitation dynamics and generate ensemble forecasts by sampling from learned categorical distributions. The deterministic architecture enables ensemble generation without random inputs during training or inference, with all variability learned from the data itself. The model is trained on a 6-year radar dataset from the Emilia-Romagna region in Northern Italy.

## Key Results
- GPTCast outperforms the LINDA baseline in CRPS scores (0.24 vs 0.32) and produces more reliable ensemble forecasts with better rank histogram uniformity
- The novel MWAE loss function in VQGAN improves reconstruction of high rainfall rates compared to standard MAE loss, particularly for extreme precipitation events
- GPTCast generates realistic ensemble forecasts with accurate uncertainty estimation without requiring random inputs during training or inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Magnitude Weighted Absolute Error (MWAE) loss function improves reconstruction of high rainfall rates in the VQGAN tokenizer.
- Mechanism: By weighting the loss based on the magnitude of precipitation values (using a sigmoid function), the tokenizer is incentivized to allocate more learning capacity to accurately reconstruct extreme precipitation events, which are critical for nowcasting.
- Core assumption: Precipitation data has a skewed distribution with extreme values being rare but important for accurate forecasts.
- Evidence anchors:
  - [abstract]: "novel reconstruction loss tailored for the skewed distribution of precipitation that promotes faithful reconstruction of high rainfall rates"
  - [section]: "MW AE loss introduced in Section 2.1 is specifically built to improve the reconstruction performances of the tokenizer and reach a good level of data reconstruction while maintaining a high compression factor"
- Break condition: If the distribution of precipitation in the target region differs significantly from the training data, the MWAE loss may not provide the same benefit.

### Mechanism 2
- Claim: The GPT model learns spatiotemporal precipitation dynamics by processing token sequences representing fixed 3D contexts (time x height x width).
- Mechanism: By training on sequences of quantized precipitation fields, the GPT model learns the conditional probability distribution of the next token given the current spatiotemporal context, enabling it to generate realistic ensemble forecasts.
- Core assumption: The spatiotemporal evolution of precipitation can be modeled as a sequence of discrete tokens, and the GPT model can learn the underlying dynamics from these sequences.
- Evidence anchors:
  - [abstract]: "The approach produces realistic ensemble forecasts and provides probabilistic outputs with accurate uncertainty estimation"
  - [section]: "The GPT model is trained on token sequences to causally learn the evolutionary dynamics of precipitation over space and time"
- Break condition: If the spatiotemporal dynamics of precipitation in the target region are too complex or non-stationary, the GPT model may not capture them accurately.

### Mechanism 3
- Claim: The deterministic architecture of GPTCast allows for ensemble generation without random inputs during training or inference.
- Mechanism: By learning a discretized representation in the tokenizer and outputting categorical distributions over the vocabulary, the forecaster can generate diverse ensemble members by sampling from these distributions, without relying on random noise inputs.
- Core assumption: The variability in precipitation forecasts can be captured by learning the distribution of possible outcomes from the training data, rather than introducing randomness during the forecasting process.
- Evidence anchors:
  - [abstract]: "The model is trained without resorting to randomness, all variability is learned solely from the data and exposed by model at inference for ensemble generation"
  - [section]: "Unlike continuous variable regression, inherently enables probabilistic outputs. In contrast, all other generative deep learning models[ 21, 22, 20] require random input during training and inference to promote output variability and generate ensemble members"
- Break condition: If the training data does not adequately represent the full range of possible precipitation scenarios, the deterministic model may not generate sufficiently diverse ensemble members.

## Foundational Learning

- Concept: VQGAN (Vector Quantized Generative Adversarial Network)
  - Why needed here: VQGAN is used as the spatial tokenizer to compress and discretize radar precipitation maps into a finite set of tokens, which can then be processed by the GPT model.
  - Quick check question: How does the VQGAN architecture balance the trade-off between compression ratio and reconstruction quality?

- Concept: GPT (Generative Pre-trained Transformer)
  - Why needed here: GPT is used as the spatiotemporal forecaster to learn the conditional probability distribution of the next precipitation token given the current spatiotemporal context, enabling it to generate realistic ensemble forecasts.
  - Quick check question: How does the choice of context size (e.g., 8 timesteps x 256 x 256 pixels vs. 8 timesteps x 128 x 128 pixels) affect the model's ability to capture spatiotemporal dynamics?

- Concept: Precipitation Nowcasting
  - Why needed here: Precipitation nowcasting is the target application of GPTCast, requiring accurate short-term forecasts of rainfall intensity and location to support decision-making in various sectors.
  - Quick check question: What are the key challenges in precipitation nowcasting, and how do they differ from other weather forecasting tasks?

## Architecture Onboarding

- Component map:
  VQGAN tokenizer -> GPT forecaster -> Decoder

- Critical path:
  1. Preprocess radar data and extract contiguous precipitating sequences
  2. Train the VQGAN tokenizer to compress and discretize the precipitation fields
  3. Train the GPT forecaster on token sequences representing spatiotemporal contexts
  4. Generate ensemble forecasts by sampling from the GPT's output distributions
  5. Decode the token sequences back into precipitation fields

- Design tradeoffs:
  - Compression ratio vs. reconstruction quality in the VQGAN tokenizer
  - Context size vs. computational complexity in the GPT forecaster
  - Deterministic architecture vs. ensemble diversity

- Failure signatures:
  - Poor reconstruction of high rainfall rates in the VQGAN tokenizer
  - Under-dispersed or over-dispersed ensemble forecasts from the GPT forecaster
  - Inability to capture complex spatiotemporal dynamics in the precipitation evolution

- First 3 experiments:
  1. Train the VQGAN tokenizer with the MWAE loss function and compare its reconstruction performance to a baseline VQGAN with MAE loss on a held-out test set.
  2. Train the GPT forecaster with different context sizes (e.g., 8 timesteps x 256 x 256 pixels vs. 8 timesteps x 128 x 128 pixels) and compare their forecasting performance on a held-out test set.
  3. Generate ensemble forecasts using the trained GPTCast model and evaluate their skill and reliability using metrics such as CRPS and rank histograms.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal context size for GPTCast that balances computational complexity and forecasting performance?
- Basis in paper: [explicit] The paper mentions testing two configurations (GPTCast-16x16 and GPTCast-8x8) and discusses the trade-off between performance and computational demands.
- Why unresolved: The paper shows that GPTCast-16x16 outperforms GPTCast-8x8 but requires significantly more computational resources. The optimal balance point is not determined.
- What evidence would resolve it: Systematic testing of multiple context sizes to identify the point of diminishing returns where additional computational cost does not justify marginal performance gains.

### Open Question 2
- Question: How does GPTCast perform when trained on stratiform versus convective precipitation separately?
- Basis in paper: [inferred] The paper suggests that training separate models for stratiform (winter) and convective (summer) precipitation may result in better forecasts, but this hypothesis is not tested.
- Why unresolved: The current model is trained on a mixed dataset of both precipitation types, and the paper does not explore whether specialized training improves performance.
- What evidence would resolve it: Training and evaluating separate models for stratiform and convective precipitation, then comparing their performance against the combined model.

### Open Question 3
- Question: Can GPTCast's interpretability be leveraged for tasks beyond nowcasting, such as weather generation or observation correction?
- Basis in paper: [explicit] The paper discusses the potential for using GPTCast's interpretability to guide the generative process for tasks like seamless forecasting, what-if scenarios, and observation correction.
- Why unresolved: While the potential is mentioned, no experiments or implementations of these applications are presented.
- What evidence would resolve it: Demonstrations of GPTCast's performance on these extended tasks, showing how its interpretability can be practically applied to achieve results in weather generation or observation correction.

## Limitations

- The model is trained and tested exclusively on radar data from the Emilia-Romagna region of Northern Italy, limiting generalizability to other geographical regions.
- The study assumes sufficient data quality after basic preprocessing, but radar data can contain artifacts, calibration issues, or coverage gaps that may affect model performance.
- The sensitivity of results to critical hyperparameters (VQGAN codebook size, downsampling factors, GPT context lengths) is not explored, and optimal values may vary across different datasets or regions.

## Confidence

**High Confidence**: The claim that GPTCast produces realistic ensemble forecasts with accurate uncertainty estimation is supported by multiple quantitative metrics (CRPS, rank histograms) and qualitative visual comparisons.

**Medium Confidence**: The superiority of GPTCast compared to LINDA baseline is demonstrated on the specific test dataset, but the limited comparison to only one baseline method and the single geographical region reduce confidence in generalizability.

**Low Confidence**: The specific contribution of the MWAE loss function to improved reconstruction of high rainfall rates is demonstrated through ablation studies, but the magnitude of improvement and whether this translates to better forecasting performance is not conclusively established.

## Next Checks

1. **Cross-Regional Validation**: Test GPTCast on radar datasets from geographically and climatologically distinct regions (e.g., tropical vs. mid-latitude, mountainous vs. flat terrain) to assess generalizability. Compare performance metrics across regions to identify potential limitations.

2. **Baseline Method Expansion**: Implement and compare against additional state-of-the-art ensemble nowcasting methods (e.g., PySTEPS, Deep Generative Models) on the same dataset to provide a more comprehensive performance evaluation.

3. **Ablation Study on VQGAN Components**: Conduct a systematic ablation study varying the VQGAN codebook size, downsampling factors, and loss function components (MWAE vs. MAE) to quantify their individual contributions to overall model performance and identify potential overfitting or underfitting regimes.