---
ver: rpa2
title: Acquiring Bidirectionality via Large and Small Language Models
arxiv_id: '2408.09640'
source_url: https://arxiv.org/abs/2408.09640
tags:
- proposed
- unilms
- backward
- tasks
- forward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to acquire bidirectionality for unidirectional
  language models (UniLMs) by concatenating their representations with those of a
  newly trained backward UniLM. The approach is evaluated on token-classification
  tasks including chunking, POS tagging, and named entity recognition (NER).
---

# Acquiring Bidirectionality via Large and Small Language Models

## Quick Facts
- arXiv ID: 2408.09640
- Source URL: https://arxiv.org/abs/2408.09640
- Authors: Takumi Goto; Hiroyoshi Nagao; Yuta Koreeda
- Reference count: 3
- Primary result: Proposed method improves UniLM performance on token-classification tasks by over 10 points in CoNLL2003-NER

## Executive Summary
This paper proposes a method to acquire bidirectionality for unidirectional language models (UniLMs) by concatenating their representations with those of a newly trained backward UniLM. The approach is evaluated on token-classification tasks including chunking, POS tagging, and named entity recognition (NER). Results show that the proposed method consistently improves performance over forward-only UniLMs, achieving over 10 points improvement in CoNLL2003-NER. The method is particularly effective in few-shot learning settings and for rare domains. The authors also demonstrate that larger forward LMs are more effective when targeting rare domains. The proposed method provides a simple and effective way to improve the performance of UniLMs in token-level classification tasks.

## Method Summary
The proposed method involves training a small backward language model (LM) to complement an existing forward UniLM. The backward LM is trained to predict tokens in reverse order, providing context from the future. Representations from both forward and backward LMs are concatenated and used as input for token-level classification tasks. The forward and backward LMs share the same vocabulary but are otherwise separate models. Classification layers are trained on top of the concatenated representations while the LMs remain fixed. The method is evaluated on CoNLL2003 and Few-NERD datasets for tasks including chunking, POS tagging, and NER.

## Key Results
- Proposed method achieves over 10 points improvement in CoNLL2003-NER F1 score compared to forward-only UniLMs
- Method is particularly effective for entities at sentence beginnings and conjoined entities
- Shows significant advantages in few-shot learning settings (less than 16 shots per entity type)
- Larger forward LMs (7B parameters) demonstrate better performance on rare domains compared to smaller models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concatenating forward and backward LM representations provides bidirectional context for token-level classification.
- Mechanism: The forward LM computes token representations using only preceding context, while the backward LM computes representations using only following context. Concatenating these two representations gives the classification layer access to both preceding and following context, mimicking bidirectional attention.
- Core assumption: The backward LM can learn to effectively encode future context when trained with reversed text.
- Evidence anchors:
  - [abstract] "To that end, we propose to newly train a small backward LM and concatenate its representations to those of an existing LM for downstream tasks."
  - [section 2.2] "← −h i = ← − −U niLMθ′(xi|x>i). The final representation for the i-th token considers both the forward and backward contexts by concatenating − →hi and ← −hi, denoted as hi = Concat[− →hi, ← −hi]."
  - [corpus] Weak - corpus contains no direct evidence about backward LM training or effectiveness.
- Break condition: If the backward LM fails to learn meaningful future context representations, the concatenated representations will not provide genuine bidirectional information.

### Mechanism 2
- Claim: The proposed method improves performance particularly for entities at sentence beginnings and conjoined entities.
- Mechanism: Forward-only LMs struggle with entities at sentence beginnings because they lack preceding context. The backward LM provides context from the sentence end, enabling correct entity type prediction even when entities appear early in the sentence.
- Core assumption: The backward LM's context from the sentence end is sufficient to disambiguate entity types for beginning-of-sentence entities.
- Evidence anchors:
  - [section 3.5] "The forward UniLM could not capture any context because the entity appears at the beginning of the sentence. In contrast, the proposed method was able to predict the entity using the context from the end."
  - [section 3.5] "We also found that the proposed method could accurately estimate the leading entity in phrases where entities are conjoined by 'and.'"
  - [corpus] Missing - corpus doesn't contain case study examples or discussion of this specific phenomenon.
- Break condition: If entities require long-range context that spans beyond the sentence boundaries, the backward LM may not capture sufficient context for correct prediction.

### Mechanism 3
- Claim: The proposed method is especially effective in few-shot learning settings and for rare domains.
- Mechanism: Large forward LMs contain extensive knowledge that can be leveraged when training data is limited. The backward LM provides additional context that helps the model generalize better from few examples, particularly for rare or difficult entity types.
- Core assumption: The knowledge embedded in large forward LMs can be effectively combined with backward context to improve generalization in low-data regimes.
- Evidence anchors:
  - [abstract] "Furthermore, we show that the proposed method is especially effective for rare domains and in few-shot learning settings."
  - [section 3.4.2] "Particularly, when the training data is limited (less than 16 shots for each entity) the proposed method is more effective than BERT."
  - [corpus] Weak - corpus contains no direct evidence about few-shot performance or rare domain effectiveness.
- Break condition: If the backward LM introduces noise or if the forward LM's knowledge is not relevant to the specific rare domain, performance gains may not materialize in few-shot settings.

## Foundational Learning

- Concept: Bidirectional vs unidirectional language models
  - Why needed here: The paper's core innovation is addressing the bidirectionality limitation of unidirectional LMs by combining forward and backward representations.
  - Quick check question: What is the key difference between how BERT (bidirectional) and GPT-2 (unidirectional) compute token representations?
  - Answer: BERT uses context from both directions (left and right), while GPT-2 uses only preceding context.

- Concept: Token-level classification tasks
  - Why needed here: The proposed method is evaluated on chunking, POS tagging, and NER, which are token-level classification tasks requiring context from both directions.
  - Quick check question: Why do token-level classification tasks benefit from bidirectional context while language generation tasks do not?
  - Answer: Token classification needs to consider both preceding and following context to make accurate predictions, whereas generation only needs preceding context to predict the next token.

- Concept: Few-shot learning and rare domain adaptation
  - Why needed here: The paper demonstrates that the proposed method is particularly effective when training data is limited or when targeting rare domains.
  - Quick check question: What makes few-shot learning and rare domain scenarios challenging for standard language models?
  - Answer: Limited training examples provide insufficient signal for the model to learn robust patterns, and rare domains may contain entities or patterns not well-represented in the pre-training data.

## Architecture Onboarding

- Component map:
  Forward LM (e.g., GPT-2 or Llama2) -> Backward LM (newly trained) -> Concatenation layer -> Classification layer

- Critical path:
  1. Input sequence is processed by forward LM to get − →hi
  2. Same input sequence is processed by backward LM to get ← −hi
  3. Representations are concatenated: hi = Concat[− →hi, ← −hi]
  4. Concatenated representations are fed to classification layer
  5. Classification layer outputs token-level predictions

- Design tradeoffs:
  - Parameter efficiency vs performance: Using a small backward LM (124M parameters) with a large forward LM (7B parameters) balances computational cost with performance gains
  - Vocabulary sharing requirement: Forward and backward LMs must share vocabulary for concatenation to work
  - Fixed backbone constraint: The forward LM cannot be fine-tuned, limiting adaptation possibilities

- Failure signatures:
  - No performance improvement over forward-only baseline: Indicates backward LM failed to learn useful future context
  - Performance degradation: Suggests backward LM is introducing noise or the concatenation is harming the representation quality
  - Memory issues during training: Results from storing both forward and backward representations simultaneously

- First 3 experiments:
  1. Verify backward LM training: Test the backward LM on a simple next-token prediction task with reversed text to ensure it learns to predict from the end of the sequence
  2. Ablation study on concatenation: Compare performance of forward-only, backward-only, and concatenated representations on a simple token classification task
  3. Few-shot learning validation: Test the method on CoNLL-2003 NER with K-shot setting (e.g., 4-shot per entity type) to verify the claimed effectiveness in low-data regimes

## Open Questions the Paper Calls Out
- How does the proposed method perform on languages other than English, especially low-resource languages?
- What is the impact of using larger unidirectional language models (e.g., beyond 7B parameters) on the performance of the proposed method?
- How does the proposed method compare to other bidirectionalization techniques like LLM2Vec in terms of computational efficiency and performance?
- Can the proposed method be effectively applied to text classification tasks by pooling token-level representations?
- What are the environmental impacts of training the backward language model, and how can they be mitigated?

## Limitations
- Limited empirical validation of backward LM quality - no direct evaluation of whether the backward LM independently learns meaningful future context representations
- Potential overfitting in few-shot settings - evaluation protocol for K-shot experiments is not fully specified
- Limited scope of evaluated tasks - only tested on three token-classification tasks that may not capture all scenarios requiring bidirectional context

## Confidence
- **High confidence** in the core methodology: The approach of concatenating forward and backward representations is technically sound and the implementation details are clearly specified.
- **Medium confidence** in performance claims: The reported improvements over forward-only baselines are substantial and consistent across multiple tasks, but the absence of certain experimental details prevents high confidence in the robustness of these results.
- **Low confidence** in claims about rare domain effectiveness: The Few-NERD evaluation provides limited evidence for this claim and doesn't demonstrate whether the method helps with genuinely rare entity types.

## Next Checks
1. **Backward LM standalone evaluation**: Implement a direct evaluation of the backward LM's ability to predict next tokens in reversed sequences on a held-out validation set to establish whether it's actually learning meaningful future context.
2. **Multiple random seeds for few-shot experiments**: Re-run the few-shot learning experiments with 5-10 different random seeds for both data selection and model initialization, reporting mean and standard deviation of F1 scores across runs.
3. **Ablation study on backward LM size**: Systematically vary the size of the backward LM while keeping the forward LM fixed, and measure the impact on downstream task performance to determine the optimal size tradeoff.