---
ver: rpa2
title: 'BOND: Aligning LLMs with Best-of-N Distillation'
arxiv_id: '2407.14622'
source_url: https://arxiv.org/abs/2407.14622
tags:
- best-of-n
- reward
- bond
- policy
- anchor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BOND is a novel RLHF algorithm that distills the Best-of-N sampling
  strategy into a policy without the high computational cost of sampling N times at
  inference. It models Best-of-N as a distribution matching problem using the Jeffreys
  divergence, balancing mode-covering and mode-seeking behaviors.
---

# BOND: Aligning LLMs with Best-of-N Distillation

## Quick Facts
- arXiv ID: 2407.14622
- Source URL: https://arxiv.org/abs/2407.14622
- Reference count: 23
- Primary result: BOND is a novel RLHF algorithm that distills the Best-of-N sampling strategy into a policy without the high computational cost of sampling N times at inference.

## Executive Summary
BOND is a novel reinforcement learning from human feedback (RLHF) algorithm that distills the Best-of-N sampling strategy into a policy without the high computational cost of sampling N times at inference. It models Best-of-N as a distribution matching problem using the Jeffreys divergence, balancing mode-covering and mode-seeking behaviors. The approach includes Monte-Carlo quantile estimation and an iterative formulation with a moving anchor for efficiency. Experiments on abstractive summarization and Gemma models show that BOND improves the KL-reward Pareto front and outperforms standard RLHF algorithms on academic benchmarks and side-by-side comparisons.

## Method Summary
BOND addresses the computational inefficiency of Best-of-N sampling in RLHF by treating alignment as a distribution matching problem. It uses the Jeffreys divergence (a linear combination of forward and backward KL divergences) to minimize the difference between the policy's generation distribution and the analytically derived Best-of-N distribution. The algorithm estimates reward quantiles using Monte Carlo sampling from the reference policy and employs an iterative approach with a moving anchor policy to scale to large N values efficiently.

## Key Results
- BOND improves the KL-reward Pareto front compared to standard RLHF algorithms
- Outperforms other RLHF algorithms on academic benchmarks and side-by-side comparisons
- Successfully distills Best-of-N behavior into policies without the N-fold computational cost at inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BOND achieves the same reward distribution as Best-of-N sampling but without the N-fold computational cost at inference time.
- Mechanism: BOND treats alignment as a distribution matching problem, using the Jeffreys divergence to minimize the difference between the policy's generation distribution and the analytically derived Best-of-N distribution.
- Core assumption: The Best-of-N distribution can be expressed analytically in terms of quantiles of the reference policy, and this distribution can be approximated through Monte Carlo sampling.
- Evidence anchors:
  - [abstract]: "BOND is a distribution matching algorithm that forces the distribution of generations from the policy to get closer to the Best-of-N distribution."
  - [section]: "Theorem 1 provides an intuitive explanation on the behavior of Best-of-N sampling: it essentially reweights the original sampling distribution ðœ‹ref..."
  - [corpus]: Weak - related papers discuss similar best-of-N alignment approaches but don't explicitly verify the analytical Best-of-N distribution formulation.
- Break condition: If the quantile estimation becomes highly inaccurate (e.g., with sparse rewards or very large N), the distribution matching objective may fail to properly emulate Best-of-N behavior.

### Mechanism 2
- Claim: The Jeffreys divergence balances mode-covering and mode-seeking behaviors, producing better aligned policies than using only forward or backward KL.
- Mechanism: Jeffreys divergence is a weighted average of forward KL (encourages mode-covering) and backward KL (encourages mode-seeking), allowing the policy to both explore diverse high-reward modes and concentrate probability mass on the best ones.
- Core assumption: The optimal alignment policy requires both exploring diverse high-reward modes and concentrating on the best ones, rather than just one behavior.
- Evidence anchors:
  - [section]: "We use the Jeffreys divergence (a linear combination of forward and backward KL) to balance between mode-covering and mode-seeking behavior..."
  - [section]: "The Jeffreys divergence (Jeffreys, 1946) between two distributions is defined as... Notably, when fine-tuning policy ð‘, the forward KL... encourages a mode-covering behavior. Instead, the reverse KL... is well-known to have a mode-seeking effect..."
  - [corpus]: Moderate - related papers mention Jeffreys divergence but don't provide direct experimental comparison showing its superiority over pure forward/backward KL in the best-of-N context.
- Break condition: If the weighting parameter Î² is set too close to 0 or 1, the algorithm reverts to pure mode-covering or mode-seeking behavior, potentially losing the benefits of the balanced approach.

### Mechanism 3
- Claim: Iterative BOND with a moving anchor allows exponential scaling to arbitrary large N while maintaining computational efficiency and stability.
- Mechanism: Instead of sampling N times from the reference policy, iterative BOND repeatedly applies Best-of-2 distillation using a moving anchor policy, effectively achieving Best-of-N^M behavior without explicitly setting large N.
- Core assumption: Best-of-N sampling from a Best-of-N distribution is equivalent to Best-of-N^2 sampling from the original distribution, allowing recursive application of the distillation process.
- Evidence anchors:
  - [section]: "We can then runBOND against ðœ‹anchor... and, after a given number of distillation steps, updateðœ‹anchor to be the current training policy ðœ‹ð‘¡."
  - [section]: "if we know how to distill the Best-of-N distribution (i.e., viaBOND), then we can applyBOND recursively (say ð‘€ times), equivalently to distilling a Best-of-Nð‘€ of the initial distribution ðœ‹ref."
  - [corpus]: Weak - related papers discuss iterative best-of-N approaches but don't explicitly verify the mathematical equivalence between recursive Best-of-2 and single-step Best-of-N^M.
- Break condition: If the anchor update frequency is too slow, the policy may not improve rapidly enough; if too fast, training stability may be compromised.

## Foundational Learning

- Concept: KL divergence and its variants (forward KL, backward KL, Jeffreys divergence)
  - Why needed here: BOND relies on minimizing divergence between the policy distribution and the Best-of-N distribution, and Jeffreys divergence is specifically used as the objective function.
  - Quick check question: What is the key difference between forward KL and backward KL, and how does Jeffreys divergence combine them?

- Concept: Quantile estimation and its Monte Carlo approximation
  - Why needed here: The Best-of-N distribution depends on reward quantiles, which must be estimated from samples to implement the distribution matching objective.
  - Quick check question: How would you estimate the probability that a generation has reward better than 90% of reference samples using Monte Carlo sampling?

- Concept: Reinforcement learning from human feedback (RLHF) and policy gradient methods
  - Why needed here: BOND builds on RLHF concepts and uses policy gradient-style updates, even though it frames the problem as distribution matching rather than direct reward maximization.
  - Quick check question: What is the role of the KL regularization term in standard RLHF, and how does BOND's approach differ conceptually?

## Architecture Onboarding

- Component map:
  Reference policy (ðœ‹ref) -> Reward model (ð‘Ÿ) -> Policy (ðœ‹) -> Anchor policy (ðœ‹anchor) -> Quantile estimator -> Divergence calculator -> Optimizer

- Critical path:
  1. Sample 1 generation from current policy and 2 from anchor policy
  2. Compute rewards for all samples
  3. Select Best-of-2 from anchor samples
  4. Estimate quantiles using Monte Carlo samples from reference policy
  5. Compute Jeffreys divergence loss (forward KL + backward KL)
  6. Update policy weights with gradient descent
  7. Update anchor policy with EMA
  8. Repeat for each batch

- Design tradeoffs:
  - Using 2 anchor samples vs. more: Lower computational cost but noisier divergence estimates
  - EMA anchor vs. periodic updates: Better stability but slower policy evolution
  - Jeffreys divergence weighting (Î²): Balances exploration vs. exploitation
  - Number of Monte Carlo samples: Accuracy vs. computational cost

- Failure signatures:
  - Policy KL divergence from reference policy increases too rapidly: May indicate Î² too low or Î³ too high
  - Reward plateaus early: May indicate anchor update frequency too slow or N too small in iterative setting
  - High variance in training: May indicate insufficient Monte Carlo samples or inappropriate Î² setting
  - Policy collapse to few modes: May indicate backward KL dominance (Î² too high)

- First 3 experiments:
  1. Verify analytical Best-of-N distribution: Sample many generations from reference policy, compute actual Best-of-N distribution, compare to analytical formula
  2. Test Jeffreys divergence weighting: Run BOND with Î² âˆˆ {0, 0.5, 1} and compare reward/KL trade-offs
  3. Validate iterative approach: Compare single-step BOND with N=8 vs. iterative BOND with n=2 and M=3 steps (should achieve similar results)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of learned quantile models compare to Monte Carlo estimation across different domains and reward model qualities?
- Basis in paper: [explicit] The paper explores using a learned quantile model as an alternative to Monte Carlo sampling for estimating reward quantiles, showing comparable results on abstractive summarization.
- Why unresolved: The paper only tests this approach on one task (abstractive summarization). The computational benefits and potential limitations of learned quantile models across diverse domains and with varying reward model qualities remain unexplored.
- What evidence would resolve it: Systematic experiments comparing learned quantile models to Monte Carlo sampling across multiple domains (e.g., dialogue, code generation), with reward models of varying quality and complexity. Analysis of computational trade-offs and sensitivity to reward model noise.

### Open Question 2
- Question: What is the optimal strategy for setting the Jeffreys divergence weight (Î²) and anchor update frequency in J-BOND for different model scales and task complexities?
- Basis in paper: [explicit] The paper uses Î² = 0.5 and explores different anchor update frequencies, but doesn't provide a systematic method for choosing these hyperparameters.
- Why unresolved: The paper demonstrates that Î² = 0.5 works well in their experiments, but doesn't explore the sensitivity to this choice or provide guidance for selecting optimal values. The impact of anchor update frequency on different model scales and task complexities is also not fully characterized.
- What evidence would resolve it: A comprehensive study examining the effects of varying Î² and anchor update frequency across different model scales (e.g., 1B, 7B, 70B parameters) and task complexities (e.g., summarization, dialogue, reasoning). This should include analysis of reward/KL trade-offs, stability, and final performance.

### Open Question 3
- Question: How does BOND's approach to reward hacking prevention compare to other methods in terms of both effectiveness and potential side effects?
- Basis in paper: [inferred] The paper suggests that BOND's use of log reward quantiles and mode-seeking behavior makes it more robust to reward hacking compared to standard RLHF, but doesn't provide a direct comparison.
- Why unresolved: While the paper provides theoretical intuition about why BOND might be more robust to reward hacking, it doesn't empirically compare its effectiveness to other reward hacking prevention methods (e.g., KL regularization, conservative Q-learning, or adversarial training). The potential side effects of BOND's approach, such as overly conservative policies, are also not explored.
- What evidence would resolve it: Comparative experiments testing BOND against other reward hacking prevention methods in environments known to induce reward hacking. Analysis should include not only the presence/absence of reward hacking but also the quality and diversity of generated outputs.

## Limitations
- The analytical formulation of the Best-of-N distribution relies on accurate quantile estimation, which may be problematic when rewards are sparse or the reference policy has limited diversity.
- The iterative BOND formulation assumes mathematical equivalence between recursive Best-of-2 and single-step Best-of-N^M, but this equivalence is not explicitly verified through experiments.
- The paper does not address potential mode collapse when the anchor policy becomes too similar to the current policy, nor does it provide guarantees on convergence speed or stability across different reward landscapes.

## Confidence

**High Confidence**: The core mechanism of treating Best-of-N as a distribution matching problem using Jeffreys divergence is theoretically sound and well-grounded in information theory. The experimental results showing improved KL-reward Pareto fronts on academic benchmarks are compelling.

**Medium Confidence**: The iterative BOND approach with moving anchor appears promising for scaling to large N, but the mathematical claims about equivalence to single-step Best-of-N^M are not fully verified. The quantile estimation method using Monte Carlo sampling is standard practice but may face challenges with high-dimensional reward spaces.

**Low Confidence**: The paper does not provide sufficient analysis of failure modes, particularly regarding hyperparameter sensitivity (Î², anchor update frequency) and the impact of reward sparsity on quantile estimation accuracy. The assumption that the reference policy can always provide sufficient diverse samples for reliable quantile estimation is not validated.

## Next Checks
1. **Quantile Estimation Robustness**: Conduct experiments varying the number of Monte Carlo samples and measuring the impact on Best-of-N distribution accuracy. Test on reward landscapes with different levels of sparsity and multimodality to identify failure conditions.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary Î² (Jeffreys divergence weighting) and anchor update frequency across multiple tasks. Measure the resulting reward/KL trade-offs to identify robust default settings and understand when the balanced approach outperforms pure forward/backward KL.

3. **Iterative vs. Direct Comparison**: For the same effective N value, compare iterative BOND with moving anchor against direct BOND sampling N times from the reference policy. Measure both final performance and computational efficiency, particularly for large N scenarios.