---
ver: rpa2
title: Open Domain Question Answering with Conflicting Contexts
arxiv_id: '2410.12311'
source_url: https://arxiv.org/abs/2410.12311
tags:
- contexts
- question
- conflicting
- questions
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QACC, a human-annotated dataset for open
  domain question answering with conflicting contexts. The authors find that 25% of
  unambiguous, open domain questions retrieved from Google contain conflicting evidence.
---

# Open Domain Question Answering with Conflicting Contexts

## Quick Facts
- arXiv ID: 2410.12311
- Source URL: https://arxiv.org/abs/2410.12311
- Authors: Siyi Liu, Qiang Ning, Kishaloy Halder, Wei Xiao, Zheng Qi, Phu Mon Htut, Yi Zhang, Neha Anna John, Bonan Min, Yassine Benajiba, Dan Roth
- Reference count: 10
- Key outcome: 25% of unambiguous open domain questions contain conflicting evidence; finetuning with human explanations improves LLM performance on conflict resolution

## Executive Summary
This paper introduces QACC, a human-annotated dataset for open domain question answering with conflicting contexts, revealing that 25% of unambiguous questions contain contradictory evidence. The authors benchmark three powerful LLMs (GPT-4o, Claude-3, and Phi-3) and demonstrate significant performance degradation when conflicting contexts are present. To address this limitation, they fine-tune models with human-written explanations, introducing richer reasoning information that guides conflict resolution. Their results show that finetuning with explanations improves Phi-3's performance on QACC and generalizes to a perturbed NQ-Open dataset.

## Method Summary
The authors collected QACC by retrieving top-10 Google Search results for 2,000+ unambiguous open domain questions from AmbigQA, then having human annotators identify conflicts, select correct answers, and write explanations for their reasoning. They fine-tuned Phi-3-Medium-Instruct using LoRA with explanation data from QACC, optimizing with language modeling loss. The training involved instruction tuning where the model learned to both answer questions and generate explanations for its reasoning process when faced with conflicting contexts.

## Key Results
- All evaluated LLMs (GPT-4o, Claude-3, Phi-3) experience significant performance degradation on conflicting contexts compared to non-conflicting ones
- Finetuning with human explanations improves Phi-3's performance on QACC by introducing richer reasoning information
- The explanation-based finetuning approach generalizes to a perturbed NQ-Open dataset
- Humans primarily resolve conflicts through majority voting, choosing answers supported by the most sources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs experience significant performance degradation when exposed to conflicting contexts
- Mechanism: Conflicting contexts introduce multiple plausible answers, causing the model to struggle with source credibility and answer selection
- Core assumption: Performance degradation is directly caused by conflicting information rather than other factors
- Evidence anchors: Abstract mentions limitations in addressing conflicting information; section 4.4 shows worse performance on EM-C vs EM-NC; limited related corpus (25 papers)

### Mechanism 2
- Claim: Finetuning LLMs with human-written explanations improves their ability to handle conflicting contexts
- Mechanism: Human explanations provide explicit reasoning strategies that guide the model through conflict resolution processes
- Core assumption: Reasoning patterns in explanations are generalizable across domains
- Evidence anchors: Abstract discusses introducing richer information through explanations; section 4.4 shows Phi-3 improvement; weak corpus support (25 papers)

### Mechanism 3
- Claim: Humans primarily resolve conflicts through majority voting when multiple contexts provide different answers
- Mechanism: Humans gravitate toward answers supported by the most sources, treating frequency as a proxy for credibility
- Core assumption: Majority answer is more likely to be correct than minority answers
- Evidence anchors: Section 3.5 states humans favor popular answers; figure 3 shows "Majority" as most selected reason; limited corpus on human conflict resolution strategies

## Foundational Learning

- Concept: Conflict detection in retrieved contexts
  - Why needed here: System must identify when contexts contain conflicting information before applying resolution strategies
  - Quick check question: Given three contexts with answers "2011", "2003", and "2010s" to the same question, can you programmatically determine these represent conflicting information?

- Concept: Source credibility assessment
  - Why needed here: Different sources have varying reliability, and the model needs to weigh evidence appropriately when resolving conflicts
  - Quick check question: If one context comes from Wikipedia and another from an unknown blog, how would you encode this credibility difference for the model?

- Concept: Explanation-based reasoning transfer
  - Why needed here: Finetuning approach relies on transferring human reasoning patterns from explanations to model behavior
  - Quick check question: Given a human explanation "I chose 2011 because most sources agree and Wikipedia confirms it", how would you parse this into training signals for the model?

## Architecture Onboarding

- Component map: Retriever → Conflict Detector → Answer Generator → Explanation Generator
- Critical path: Retriever → Conflict Detector → Answer Generator
- Design tradeoffs: Using human explanations for finetuning adds training complexity but provides better conflict resolution than simple majority voting rules
- Failure signatures: Performance drops when conflicts exist (EM-C << EM-NC), model generates long answers instead of concise ones, or model fails to identify conflicts that humans easily detect
- First 3 experiments:
  1. Run the retriever on unambiguous questions and measure conflict frequency to validate the 25% finding
  2. Implement simple majority voting baseline and compare against vanilla LLM performance on conflicting questions
  3. Finetune with explanation data and evaluate on both QACC and perturbed NQ-Open to test generalization

## Open Questions the Paper Calls Out

None

## Limitations

- Dataset size (1,984 examples) is relatively small for fine-tuning studies, particularly given task complexity
- Evaluation focuses on only three LLM families with limited parameter scale ranges, leaving uncertainty about generalizability
- Paper doesn't systematically explore correlation between conflict detection performance and answer accuracy

## Confidence

- **High confidence**: The empirical finding that 25% of unambiguous open-domain questions contain conflicting evidence, and observation that all evaluated LLMs perform worse on conflicting versus non-conflicting contexts
- **Medium confidence**: Claim that fine-tuning with human explanations improves conflict resolution performance, limited by dataset size and single model evaluation
- **Medium confidence**: Characterization of human conflict resolution strategies (majority voting, trusted sources), based on AMT annotations that may reflect annotation artifacts

## Next Checks

1. **Dataset quality validation**: Manually sample 50 examples from QACC to verify annotator disagreements are properly resolved and conflict annotations are consistent across annotator pools

2. **Generalization stress test**: Evaluate the fine-tuned model on a completely held-out domain (e.g., scientific questions or legal questions) to test whether explanation-based training transfers beyond web-retrieved context domain

3. **Conflict detection ablation**: Systematically remove the conflict detection component and measure degradation in answer accuracy to quantify how much performance loss is due to conflict identification versus conflict resolution