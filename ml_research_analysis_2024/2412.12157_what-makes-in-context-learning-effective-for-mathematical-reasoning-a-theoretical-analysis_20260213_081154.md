---
ver: rpa2
title: 'What Makes In-context Learning Effective for Mathematical Reasoning: A Theoretical
  Analysis'
arxiv_id: '2412.12157'
source_url: https://arxiv.org/abs/2412.12157
tags:
- uni00000013
- htest
- uni00000011
- learning
- demonstration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper theoretically analyzes how in-context demonstrations
  affect large language models' mathematical reasoning performance. The authors prove
  that reasoning efficacy can be bounded by a LLM-oriented semantic similarity and
  an inference stability of demonstrations, establishing conditions under which demonstrations
  improve or harm performance.
---

# What Makes In-context Learning Effective for Mathematical Reasoning: A Theoretical Analysis

## Quick Facts
- arXiv ID: 2412.12157
- Source URL: https://arxiv.org/abs/2412.12157
- Authors: Jiayu Liu; Zhenya Huang; Chaokun Wang; Xunpeng Huang; Chengxiang Zhai; Enhong Chen
- Reference count: 10
- This paper theoretically analyzes how in-context demonstrations affect large language models' mathematical reasoning performance.

## Executive Summary
This paper presents a theoretical framework for understanding when in-context demonstrations improve mathematical reasoning performance in large language models. The authors prove that reasoning efficacy can be bounded by a LLM-oriented semantic similarity and an inference stability of demonstrations, establishing conditions under which demonstrations improve or harm performance. Based on this theory, they propose LMS3, a demonstration selection method that balances semantic similarity and inference stability while incorporating a rejection mechanism to avoid negative examples. Tested across three mathematical benchmarks with two LLM backbones and multiple few-shot settings, LMS3 consistently outperformed existing methods, achieving significant accuracy improvements over both zero-shot and other in-context learning approaches.

## Method Summary
The method selects demonstrations that are semantically similar to the test problem and have stable inference predictions, reducing the empirical prediction loss bound. It uses the inference LLM's own representations to measure similarity, capturing the model's understanding of problem relationships rather than just surface-level features. The approach includes a novel demonstration rejection mechanism that identifies when demonstrations are likely to harm reasoning performance, defaulting to zero-shot when necessary. The method demonstrates superior generalization with O(M+N) complexity compared to more expensive alternatives.

## Key Results
- LMS3 consistently outperformed existing methods across three mathematical benchmarks (MA WPS, GSM8K, MATH)
- The method achieved significant accuracy improvements over both zero-shot and other in-context learning approaches
- Demonstrated strong interpretability through case studies while maintaining O(M+N) complexity compared to more expensive alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Demonstrations improve mathematical reasoning when they satisfy both semantic similarity and inference stability conditions.
- Mechanism: The method selects demonstrations that are semantically similar to the test problem and have stable inference predictions, reducing the empirical prediction loss bound.
- Core assumption: The LLM's attention mechanism can be approximated as linear attention, allowing gradient-based analysis of demonstration impact.
- Evidence anchors:
  - [abstract] "We prove that the reasoning efficacy (measured by empirical prediction loss) can be bounded by a LLM-oriented semantic similarity and an inference stability of demonstrations"
  - [section] "Based on this finding, we propose a straightforward, generalizable, and low-complexity demonstration selection method named LMS3"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.499, average citations=0.0.
- Break condition: When the demonstration's semantic similarity is too large relative to the inference stability, causing the loss bound to increase rather than decrease.

### Mechanism 2
- Claim: The LLM-oriented semantic similarity is more effective than traditional similarity measures because it incorporates the inference LLM's encoding characteristics.
- Mechanism: By using the inference LLM's own representations to measure similarity, the method captures the model's understanding of problem relationships rather than just surface-level features.
- Core assumption: The inference LLM's parameter space captures meaningful semantic relationships between mathematical problems.
- Evidence anchors:
  - [abstract] "The former goes beyond traditional methods that rely solely on the semantic similarity between demonstrations and test samples"
  - [section] "It goes beyond traditional methods by taking into account 1) the whole reasoning path of demonstrations and 2) the characteristics of the inference LLM itself"
  - [corpus] Weak corpus evidence - no direct citations found for this specific mechanism.
- Break condition: When the inference LLM's encoding fails to capture the relevant semantic relationships between problems.

### Mechanism 3
- Claim: The demonstration rejection mechanism prevents negative performance by identifying when demonstrations are likely to harm reasoning performance.
- Mechanism: The method ranks demonstrations by their semantic similarity and rejects those that rank too low relative to their score, defaulting to zero-shot when necessary.
- Core assumption: When demonstrations are too dissimilar from the test problem, their negative impact outweighs any potential benefits.
- Evidence anchors:
  - [abstract] "it includes a novel demonstration rejection mechanism to automatically filter out samples that are unsuitable for few-shot learning"
  - [section] "we introduce an innovative demonstration rejection mechanism that can adaptively identify when few-shot learning should not be used"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.499, average citations=0.0.
- Break condition: When the rejection threshold is set too conservatively, potentially rejecting useful demonstrations.

## Foundational Learning

- Concept: Linear approximation of attention mechanisms
  - Why needed here: The theoretical analysis relies on approximating the transformer attention as linear attention to enable gradient-based analysis of demonstration impact
  - Quick check question: What mathematical operation does the linear attention approximation replace in the original attention mechanism?

- Concept: Influence functions in machine learning
  - Why needed here: The theoretical analysis uses influence functions to measure how demonstrations affect the test loss, drawing from established statistical learning theory
  - Quick check question: What does the influence function measure in the context of in-context learning?

- Concept: Lipschitz continuity
  - Why needed here: The theoretical bounds assume the gradient of the loss is Lipschitz continuous, which is a standard assumption in optimization theory
  - Quick check question: What property of a function is guaranteed when its gradient is Lipschitz continuous?

## Architecture Onboarding

- Component map: Demonstration pool -> Inference LLM for semantic representation -> Scoring function (semantic similarity × inference stability) -> Rejection mechanism (based on similarity ranking) -> Output module (select top-k or zero-shot)
- Critical path: 1) Compute semantic representations of all demonstrations and test problem, 2) Calculate semantic similarity and inference stability for each demonstration, 3) Score demonstrations using the product of similarity and stability, 4) Apply rejection mechanism, 5) Select top-k demonstrations or use zero-shot
- Design tradeoffs: The method trades off computational complexity (O(M+N) vs more expensive alternatives) for theoretical guarantees and interpretability. It also balances the need for demonstration similarity against the risk of negative examples.
- Failure signatures: The method may fail when demonstrations are too similar but have poor inference stability, or when the rejection mechanism is too aggressive and rejects useful examples. Performance may also degrade on datasets where zero-shot performance is already strong.
- First 3 experiments:
  1. Run the method with different values of the rejection threshold λ to observe its effect on performance
  2. Compare the method's performance with and without the demonstration rejection mechanism on datasets where zero-shot is strong
  3. Test the method's generalization by using demonstrations selected for one LLM on different LLMs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LMS3 scale when applied to more complex mathematical domains beyond the current benchmarks, such as higher-level mathematics or interdisciplinary problems?
- Basis in paper: [explicit] The paper tests LMS3 on three benchmarks (MAWPS, GSM8K, MATH) covering elementary to high school-level mathematics, but does not explore more advanced domains.
- Why unresolved: The theoretical framework and empirical results are limited to specific benchmark datasets, leaving open the question of generalizability to more complex or diverse mathematical reasoning tasks.
- What evidence would resolve it: Testing LMS3 on higher-level mathematics datasets or interdisciplinary problems, such as those involving physics or engineering, would provide insights into its scalability and robustness.

### Open Question 2
- Question: What is the impact of LMS3's performance when the inference LLM's encoding capacity is significantly different from the one used during the demonstration selection process?
- Basis in paper: [explicit] The paper emphasizes that LMS3's effectiveness depends on the inference LLM's encoding capacity, but does not explore scenarios where the encoding capacity of the LLM used for selection differs from the one used for inference.
- Why unresolved: The theoretical analysis assumes consistency in the inference LLM's encoding capacity, but real-world applications may involve varying LLM models with different encoding capabilities.
- What evidence would resolve it: Experiments comparing LMS3's performance across different LLM models with varying encoding capacities would clarify its robustness to such variations.

### Open Question 3
- Question: How does the rejection mechanism in LMS3 perform when the dataset contains ambiguous or poorly defined problems that are difficult for any LLM to reason about?
- Basis in paper: [inferred] The rejection mechanism is designed to filter out unsuitable demonstrations, but the paper does not explicitly test its effectiveness on ambiguous or poorly defined problems.
- Why unresolved: The rejection mechanism's performance in handling edge cases, such as ambiguous problems, is not evaluated, leaving uncertainty about its reliability in such scenarios.
- What evidence would resolve it: Testing LMS3 on datasets with intentionally ambiguous or poorly defined problems would reveal the effectiveness and limitations of the rejection mechanism in such cases.

## Limitations

- The theoretical analysis relies on a linear attention approximation that may not capture the full complexity of transformer attention mechanisms, particularly for larger models.
- The influence function analysis assumes infinitesimal perturbations to demonstrations, which may not reflect the discrete nature of actual in-context learning.
- The semantic similarity measure depends heavily on the quality of the inference LLM's embeddings, which may not be optimal for all mathematical domains.

## Confidence

- **High Confidence**: The empirical results showing LMS3 outperforming baselines across multiple benchmarks and LLM backbones. The O(M+N) computational complexity claim is well-supported by the algorithm description.
- **Medium Confidence**: The theoretical bounds connecting semantic similarity and inference stability to prediction loss. While mathematically rigorous, the practical tightness of these bounds depends on how well the linear attention approximation holds.
- **Low Confidence**: The claim that LLM-oriented semantic similarity is inherently superior to traditional similarity measures. The paper provides theoretical justification but limited empirical comparison with alternative similarity metrics.

## Next Checks

1. Test the method's performance on mathematical reasoning problems that require symbolic manipulation rather than numerical computation to assess generalization across mathematical domains.
2. Conduct ablation studies systematically varying the relative weights of semantic similarity versus inference stability in the scoring function to quantify their individual contributions.
3. Apply the method to non-mathematical reasoning tasks (such as commonsense reasoning or code generation) to evaluate whether the theoretical insights generalize beyond mathematical problems.