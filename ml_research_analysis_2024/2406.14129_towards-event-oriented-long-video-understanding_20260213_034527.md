---
ver: rpa2
title: Towards Event-oriented Long Video Understanding
arxiv_id: '2406.14129'
source_url: https://arxiv.org/abs/2406.14129
tags:
- video
- understanding
- videos
- question
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Event-Bench, an event-oriented long video
  understanding benchmark designed to evaluate video Multimodal Large Language Models
  (MLLMs). Unlike existing datasets, Event-Bench addresses the "shortcut bias" by
  focusing on videos with rich, complex events that require full viewing to answer
  questions.
---

# Towards Event-oriented Long Video Understanding

## Quick Facts
- arXiv ID: 2406.14129
- Source URL: https://arxiv.org/abs/2406.14129
- Reference count: 36
- Primary result: Introduces Event-Bench benchmark and Video Instruction Merging (VIM) method, achieving 53.33% accuracy with GPT-4o on event-oriented long video understanding tasks

## Executive Summary
This paper introduces Event-Bench, a benchmark designed to evaluate video Multimodal Large Language Models (MLLMs) on event-oriented long video understanding tasks. The benchmark addresses the "shortcut bias" found in existing datasets by focusing on videos with rich, complex events that require full viewing to answer questions. The authors also propose Video Instruction Merging (VIM), a cost-effective method that merges similar video instructions into more complex, event-rich ones. Experiments demonstrate that VIM significantly improves model performance, with GPT-4o achieving 53.33% accuracy and outperforming open-source models by 41.42%.

## Method Summary
The paper introduces Event-Bench, an event-oriented long video understanding benchmark with six tasks: atomic event description, temporal reasoning, causal reasoning, contextual reasoning, episodic reasoning, and counter-intuitive reasoning. The benchmark includes 2,190 test instances across these tasks. To address the scarcity of event-intensive video data, the authors propose Video Instruction Merging (VIM), which merges similar video instructions to create more complex training data. The method uses BGE embeddings to find semantically similar videos, merges them temporally, and generates new questions/answers using GPT-4. An adaptive model architecture handles both image and video inputs, allowing integration of high-quality image instruction data during training.

## Key Results
- GPT-4o achieves 53.33% accuracy on Event-Bench, outperforming open-source models by 41.42%
- VIM method significantly improves performance compared to state-of-the-art open-source models and GPT-4V
- Increasing frame count from 8 to 16 improves performance, but open-source models don't consistently benefit from additional frames
- Removing image instructions from training data leads to performance decrease across all tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Merging similar video instructions creates more complex event narratives that require full video viewing to answer.
- Mechanism: By selecting videos with similar semantic content and merging them, the resulting instruction combines multiple event sequences into a single coherent narrative. This forces models to track multiple events across time rather than relying on single-frame shortcuts.
- Core assumption: Videos with high semantic similarity can be meaningfully merged without creating contradictory or incoherent narratives.
- Evidence anchors:
  - [abstract] "we propose Video Instruction Merging (VIM), a cost-effective method that enhances video MLLMs using merged, event-intensive video instructions"
  - [section 4.1] "we first find the most similar ones and then merge them into a new sample"
  - [corpus] Weak evidence - related papers focus on different merging strategies but don't directly validate this specific approach
- Break condition: If merged videos contain contradictory events or incompatible timelines, the resulting instruction becomes incoherent and cannot be answered reliably.

### Mechanism 2
- Claim: Adaptive model architecture that handles both image and video inputs enables effective use of image instruction data.
- Mechanism: The model treats videos as sequences of images with shared encoding pathways, allowing seamless integration of high-quality image instruction data during training. This provides additional training signal without requiring expensive video annotation.
- Core assumption: Image understanding capabilities transfer effectively to video understanding when videos are processed as image sequences.
- Evidence anchors:
  - [section 4.2] "our model can flexibly handle inputs of varying sequence lengths (e.g., a single image, short videos, or long videos)"
  - [section 5.3] "removing image instructions from our training data leads to a performance decrease on all the tasks"
  - [corpus] Weak evidence - related work discusses unified architectures but doesn't specifically validate this image-video integration approach
- Break condition: If video-specific temporal reasoning cannot be learned from static image data, performance will plateau regardless of image instruction quantity.

### Mechanism 3
- Claim: Increasing frame count improves performance by providing more temporal context for event understanding.
- Mechanism: More frames allow the model to observe event progression and relationships that aren't visible in single frames or short clips, particularly important for composite and overall understanding tasks.
- Core assumption: The relationship between frame count and performance follows a predictable pattern where more frames consistently improve understanding.
- Evidence anchors:
  - [section 5.3] "more input frames lead to better performance for GPT-4o" and "performance of GPT-4o in the temporal reasoning task is boosted from 47.50 to 56.75 when the number of input frames increases from 8 to 32"
  - [section 5.3] "open-source models do not consistently benefit from additional frames" - showing this mechanism doesn't work universally
  - [corpus] No direct evidence - related work focuses on different aspects of frame processing
- Break condition: If the model's context window becomes saturated or if frame redundancy provides no additional information, increasing frame count will yield diminishing returns.

## Foundational Learning

- Concept: Event-oriented understanding vs frame-level understanding
  - Why needed here: The benchmark specifically targets event comprehension, which requires tracking changes and relationships over time rather than recognizing static features
  - Quick check question: Can you distinguish between answering questions that require watching the entire video versus questions answerable from a single frame?

- Concept: Instruction merging as data augmentation
  - Why needed here: The scarcity of event-intensive video data makes traditional annotation approaches too expensive, requiring creative synthesis methods
  - Quick check question: How does merging similar instructions create more complex reasoning requirements than the original individual instructions?

- Concept: Multimodal instruction tuning
  - Why needed here: Video MLLMs require large-scale training data across multiple modalities to develop robust understanding capabilities
  - Quick check question: What advantages does incorporating image instructions provide for video understanding models?

## Architecture Onboarding

- Component map:
  - Image encoder (EV A-CLIP) -> Q-Former -> LLM (Vicuna) -> Video processing pipeline

- Critical path: Frame sampling → Image encoding → Q-Former processing → Text embedding concatenation → LLM inference
- Design tradeoffs: Larger frame counts improve understanding but increase computational cost; more image instruction data helps but may not fully substitute for video data
- Failure signatures: Performance degradation with more frames suggests context window issues; poor performance on episodic reasoning indicates insufficient training on complex narratives
- First 3 experiments:
  1. Test frame count sensitivity by running with {8, 16, 24, 32} frames and measuring performance on temporal reasoning task
  2. Evaluate instruction merging effectiveness by comparing performance with k=1 (no merge) vs k=2,3,4
  3. Assess image instruction contribution by training with and without image data and measuring overall task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of frames (k) to sample from videos for video MLLMs to balance computational efficiency and performance on event understanding tasks?
- Basis in paper: Inferred from the analysis showing that increasing frame count from 8 to 16 improves performance, but increasing to 32 frames leads to performance degradation for open-source models.
- Why unresolved: The paper shows mixed results across different models and tasks, with proprietary models like GPT-4o continuing to improve with more frames while open-source models plateau or decline.
- What evidence would resolve it: Systematic ablation studies testing multiple frame counts (e.g., 8, 12, 16, 20, 24, 28, 32) across all baseline models and tasks to identify optimal sampling strategies.

### Open Question 2
- Question: How does the inclusion of multimodal data (audio, text, speech) beyond visual frames impact performance on event-oriented long video understanding tasks?
- Basis in paper: Explicitly mentioned in the limitations section as an acknowledged weakness where "events are not only represented by visual modality, but also by other modalities in the real world."
- Why unresolved: The Event-Bench benchmark and VIM methodology are currently limited to visual information only, with the authors explicitly noting plans to add other modalities in the future.
- What evidence would resolve it: Experiments comparing models trained on Event-Bench with and without audio/text information, measuring performance differences on event understanding tasks.

### Open Question 3
- Question: What is the relationship between instruction merging diversity and the resulting model's ability to understand complex events in videos?
- Basis in paper: Inferred from the Video Instruction Merging methodology, where similar video instructions are merged to create more complex training data, but the paper notes that "their diversity and complexity are still lower than those of human-annotated ones."
- Why unresolved: The paper shows that merging improves performance compared to single videos, but doesn't systematically explore how varying the diversity or complexity of merged instructions affects learning outcomes.
- What evidence would resolve it: Controlled experiments varying the semantic similarity thresholds and number of videos merged, measuring downstream performance on event understanding tasks.

### Open Question 4
- Question: How does the proposed Video Instruction Merging (VIM) method scale with larger model sizes and more extensive training datasets?
- Basis in paper: Explicitly mentioned in the limitations section, stating that "experimental results show that including more high-quality video and image instructions positively impacts model performance" but computational constraints limited training to 500K instructions.
- Why unresolved: The current VIM implementation and evaluation used relatively modest model sizes (7B-13B parameters) and training data, with the authors noting plans to scale up in the future.
- What evidence would resolve it: Training VIM with larger models (e.g., 34B+ parameters) and significantly more instruction data (10M+ instructions), measuring performance improvements on Event-Bench.

### Open Question 5
- Question: What is the impact of the hierarchical task taxonomy design on the benchmark's ability to diagnose specific weaknesses in video MLLMs?
- Basis in paper: Explicitly described in the methodology section where Event-Bench organizes tasks into three categories (atomic, composite, overall understanding) with six sub-tasks to evaluate different event understanding capabilities.
- Why unresolved: While the paper demonstrates that overall understanding tasks are more challenging than atomic/composite ones, it doesn't provide detailed analysis of which model architectures or training strategies specifically improve performance on different task categories.
- What evidence would resolve it: Detailed per-task performance analysis showing which models excel at which types of reasoning, and ablation studies testing whether specialized architectures for different task categories improve overall performance.

## Limitations
- Reliance on instruction merging as data synthesis technique may not capture naturally occurring event sequences
- Benchmark focuses exclusively on accuracy metrics without examining model robustness to temporal noise or varying video quality
- Evaluation methodology limited to 2,190 test instances, potentially insufficient for detecting subtle performance differences

## Confidence
- High Confidence: Event-Bench provides more rigorous evaluation of event-oriented video understanding compared to existing benchmarks
- Medium Confidence: VIM significantly improves model performance, but generalizability to other video MLLMs requires further validation
- Low Confidence: Increasing frame count consistently improves performance is contradicted by observation that open-source models don't benefit from additional frames

## Next Checks
1. **Temporal Coherence Validation**: Select 50 merged video instruction pairs and conduct human evaluation to assess whether the merged instructions maintain temporal coherence and logical event progression. Calculate inter-rater agreement and compare against baseline (non-merged) instructions.

2. **Cross-Domain Transfer Test**: Apply the trained VIM-enhanced model to a completely different video domain (e.g., surveillance footage or medical procedure videos) and measure performance degradation. This tests whether the event-intensive training generalizes beyond the benchmark's source domains.

3. **Frame Redundancy Analysis**: For each task type, systematically vary frame sampling density (e.g., 1 fps, 2 fps, 5 fps, 10 fps) and plot performance curves to identify the optimal sampling rate. Determine whether the observed performance gains from increased frame count are due to temporal resolution or simple frame redundancy.