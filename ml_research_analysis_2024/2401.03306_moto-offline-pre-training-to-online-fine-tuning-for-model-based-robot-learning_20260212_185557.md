---
ver: rpa2
title: 'MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning'
arxiv_id: '2401.03306'
source_url: https://arxiv.org/abs/2401.03306
tags:
- learning
- offline
- model-based
- arxiv
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies offline-to-online fine-tuning for reinforcement
  learning from high-dimensional observations, focusing on realistic robot tasks.
  Existing model-based offline RL methods are not suitable for this setting due to
  issues with distribution shifts, off-dynamics data, and non-stationary rewards.
---

# MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning

## Quick Facts
- arXiv ID: 2401.03306
- Source URL: https://arxiv.org/abs/2401.03306
- Reference count: 40
- Primary result: MOTO solves Franka Kitchen tasks from pixels with 100% and 90.5% success rates, outperforming baselines on 9/10 MetaWorld tasks

## Executive Summary
This work introduces MOTO, a model-based actor-critic algorithm designed for offline pre-training and online fine-tuning in robot learning from high-dimensional observations. MOTO addresses key challenges in offline-to-online RL, including distribution shifts, off-dynamics data, and non-stationary rewards, by leveraging model-based value expansion and policy regularization while controlling epistemic uncertainty. Evaluated on 10 MetaWorld tasks and 2 Franka Kitchen environments from pixels, MOTO achieves state-of-the-art performance, notably solving Franka Kitchen from images—a first according to the authors.

## Method Summary
MOTO is a model-based actor-critic algorithm that reuses prior data through model-based value expansion and policy regularization while controlling epistemic uncertainty to prevent model exploitation. The method uses real trajectories as starting points for model-generated rollouts, applies Monte Carlo returns of varying horizons to estimate value targets, and trains both actor and critic directly from the model. An ensemble of latent dynamics models provides epistemic uncertainty estimates via disagreement, which penalizes rewards to encourage conservative exploration. Policy regularization based on offline data behavior further stabilizes learning, especially early on when the model is inaccurate.

## Key Results
- MOTO outperforms baselines on 9/10 MetaWorld tasks.
- MOTO solves both Franka Kitchen tasks from pixels (100% and 90.5% success rates).
- MOTO is the first method to solve Franka Kitchen environment from pixels, to the best of the authors' knowledge.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-based value expansion removes the need for large latent replay buffers while still enabling effective use of offline data.
- Mechanism: Real trajectories serve as starting points for model-generated rollouts, and Monte Carlo returns of varying horizons estimate value targets, allowing training directly from the model without storing latent states.
- Core assumption: The offline dataset provides sufficient state coverage for model rollouts to remain relevant.
- Evidence anchors:
  - [abstract] "MOTO uses model-based value expansion, which removes the need for large replay buffers, mitigates issues related to distribution shifts..."
  - [section] "At each agent training step, we infer latent states s0 1:T from real data. We then use the true data as starting points for model-generated rollouts..."
- Break condition: If the offline dataset lacks sufficient coverage, model rollouts will drift into low-data regions, making value targets unreliable.

### Mechanism 2
- Claim: Epistemic uncertainty estimation via ensemble disagreement penalizes rewards, preventing model exploitation during offline training.
- Mechanism: An ensemble of latent dynamics models is trained, and their logit disagreement is used as an uncertainty penalty added to rewards, encouraging the agent to prefer actions with lower model uncertainty.
- Core assumption: Ensemble disagreement is a valid proxy for epistemic uncertainty in the learned latent dynamics model.
- Evidence anchors:
  - [abstract] "preventing model exploitation by controlling epistemic uncertainty."
  - [section] "To mitigate issues with model exploitation, similar to [19, 11], we train a latent dynamics model ensemble... and implement model conservatism by penalizing rewards via dynamics model disagreement..."
- Break condition: If ensemble models converge too closely or the latent space is too coarse, disagreement may not capture true uncertainty, allowing exploitation.

### Mechanism 3
- Claim: Behavior-regularized policy optimization stabilizes early learning when the dynamics model is inaccurate.
- Mechanism: The policy is regularized to follow the behavior distribution in the offline data, but only on trajectory snippets where the model's estimated advantage is positive, preventing drift into unsafe behaviors while allowing model-based optimization where the model is confident.
- Core assumption: The offline dataset contains at least some high-return trajectories that can serve as effective behavior priors.
- Evidence anchors:
  - [section] "To avoid additional complexity from modeling the behaviour distribution, we follow an approach similar to [47] which deploys a regularization term of the form..."
  - [abstract] "Our method utilizes model-based value expansion, which allows us to take advantage of the efficiency of model-based training, while also using high-quality offline data for critic supervision."
- Break condition: If the offline dataset is of uniformly low quality or too narrow, the regularization term may suppress necessary exploration or model-based improvement.

## Foundational Learning

- Concept: Variational latent dynamics models (RSSM-style) for learning state representations from pixels.
  - Why needed here: The method operates directly on high-dimensional image observations, so it must learn a compact latent state that captures both visual information and dynamics.
  - Quick check question: Can you explain how the RSSM model separates stochastic and deterministic components in the latent state, and why both are useful?

- Concept: Model-based value expansion and its use of multi-step Monte Carlo returns.
  - Why needed here: This technique allows the critic to be trained on synthetic data without needing a replay buffer of latent states, avoiding distribution shift issues.
  - Quick check question: How does mixing returns at different horizons (H-step returns) in the value expansion improve stability compared to using only 1-step or only full rollout returns?

- Concept: Ensemble-based uncertainty estimation in model-based RL.
  - Why needed here: Prevents the agent from exploiting inaccuracies in the learned dynamics model by penalizing actions in high-uncertainty regions.
  - Quick check question: Why might using only a single model's predictions be insufficient for estimating epistemic uncertainty in the dynamics?

## Architecture Onboarding

- Component map: Vision encoder (CNN) → latent belief state (RSSM) → ensemble of latent dynamics models → reward model → policy network (actor) + two Q-value networks (critics)
- Critical path: Real data → latent inference → model rollout → value targets → policy/critic gradients. Any failure in latent inference or model accuracy directly degrades policy learning.
- Design tradeoffs: On-policy model rollouts give stable representation use but require more computation per step vs. replay buffer methods; ensemble training increases robustness to model exploitation but adds compute.
- Failure signatures: If latent inference is poor, rollouts will diverge quickly; if ensemble disagreement is too low, the policy may exploit model errors; if behavior regularization is too strong, learning may stall.
- First 3 experiments:
  1. Train only the world model (RSSM + ensemble) on the offline dataset and evaluate latent state quality and model rollouts.
  2. Train actor/critic using only real data (no model rollouts) to establish a baseline.
  3. Enable full MOTO training and compare sample efficiency and final performance against baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MOTO perform when trained on larger-scale datasets or more complex environments, such as autonomous driving simulations?
- Basis in paper: [explicit] The authors mention that MOTO is well-suited for use with very large-scale dynamics models, such as those used in autonomous driving, and plan to explore this direction in future work.
- Why unresolved: The current experiments are limited to the MetaWorld and Franka Kitchen environments, which are not as complex or large-scale as real-world applications like autonomous driving.
- What evidence would resolve it: Evaluating MOTO on large-scale datasets or more complex environments, such as CARLA [50], and comparing its performance to other state-of-the-art methods in these settings.

### Open Question 2
- Question: How does the performance of MOTO scale with the size of the ensemble of latent transition models?
- Basis in paper: [explicit] The authors use an ensemble size of 7 models in their experiments, but it is not clear how the performance would change with a larger or smaller ensemble.
- Why unresolved: The current experiments only evaluate MOTO with a fixed ensemble size, and there is no analysis of how the ensemble size affects performance.
- What evidence would resolve it: Conducting experiments with different ensemble sizes and analyzing the trade-off between performance and computational cost.

### Open Question 3
- Question: Can MOTO be extended to handle non-sparse rewards or more complex reward structures?
- Basis in paper: [explicit] The authors mention that MOTO uses policy regularization based on task success, which may be difficult to adapt to tasks with more complex or non-sparse rewards.
- Why unresolved: The current experiments only evaluate MOTO on tasks with sparse rewards, and there is no analysis of its performance on tasks with more complex reward structures.
- What evidence would resolve it: Evaluating MOTO on tasks with non-sparse or more complex reward structures and analyzing its performance compared to other methods.

## Limitations

- The paper does not report how many random seeds were used, making the reported success rates harder to interpret statistically.
- The computational overhead of training an ensemble of dynamics models and running model-based rollouts is not quantified, limiting assessment of real-world applicability.
- The method's reliance on high-quality offline data is not thoroughly discussed; if the dataset is narrow or suboptimal, behavior regularization and model rollouts may not be effective.

## Confidence

- **High confidence**: The core algorithmic contributions (model-based value expansion, ensemble-based uncertainty estimation, behavior regularization) are well-motivated and grounded in prior literature. The claim that MOTO outperforms baselines on 9/10 MetaWorld tasks is supported by the experimental results, though seed count uncertainty affects confidence in statistical significance.
- **Medium confidence**: The claim that MOTO is the first to solve Franka Kitchen from pixels is likely true, but without public baselines or a clear experimental protocol for comparison, this is difficult to verify independently.
- **Low confidence**: The computational cost and scalability of MOTO for real-world deployment are not addressed, making it unclear whether the method is practical outside of controlled benchmark environments.

## Next Checks

1. **Statistical robustness**: Re-run the Franka Kitchen experiments with at least 5 random seeds and report mean and standard deviation of success rates to assess result stability.
2. **Dataset sensitivity**: Evaluate MOTO on a deliberately narrow or low-quality offline dataset to quantify how performance degrades and confirm the importance of dataset coverage.
3. **Computational overhead**: Measure and report wall-clock time and GPU/CPU usage for MOTO training compared to model-free baselines, including both offline pre-training and online fine-tuning phases.