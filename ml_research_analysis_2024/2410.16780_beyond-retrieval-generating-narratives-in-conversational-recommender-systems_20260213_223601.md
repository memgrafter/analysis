---
ver: rpa2
title: 'Beyond Retrieval: Generating Narratives in Conversational Recommender Systems'
arxiv_id: '2410.16780'
source_url: https://arxiv.org/abs/2410.16780
tags:
- user
- purchase
- language
- item
- history
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REGEN, a new dataset designed to advance
  research in conversational recommender systems by enhancing the Amazon Product Reviews
  dataset with richer user narratives, including purchase reasons, user summaries,
  and product endorsements. The authors propose a fusion architecture that combines
  collaborative filtering (CF) signals and semantic embeddings as input to a Large
  Language Model (LLM), demonstrating that this approach improves narrative generation
  quality by 4-12% in key language metrics compared to using either embedding type
  alone.
---

# Beyond Retrieval: Generating Narratives in Conversational Recommender Systems

## Quick Facts
- arXiv ID: 2410.16780
- Source URL: https://arxiv.org/abs/2410.16780
- Reference count: 26
- This paper introduces REGEN, a new dataset designed to advance research in conversational recommender systems by enhancing the Amazon Product Reviews dataset with richer user narratives, including purchase reasons, user summaries, and product endorsements.

## Executive Summary
This paper introduces REGEN, a new dataset designed to advance research in conversational recommender systems by enhancing the Amazon Product Reviews dataset with richer user narratives, including purchase reasons, user summaries, and product endorsements. The authors propose a fusion architecture that combines collaborative filtering (CF) signals and semantic embeddings as input to a Large Language Model (LLM), demonstrating that this approach improves narrative generation quality by 4-12% in key language metrics compared to using either embedding type alone. The model learns to effectively interpret user history through CF and content embeddings, generating personalized and contextually relevant recommendations. Analysis of soft token embeddings reveals that content-based signals have a more significant contribution than CF signals in the learned representations, though both are important for optimal performance.

## Method Summary
The proposed method uses a fusion architecture that combines collaborative filtering (CF) embeddings from WALS matrix factorization with semantic embeddings from a sentence encoder (Gecko). These embeddings are concatenated, normalized, and passed through an adapter layer (MLP or projection) to generate soft tokens that serve as input to an LLM (PaLM2 XXS). The model is trained to generate personalized narratives from user interaction sequences, with training and test sets containing different users to ensure generalization. The approach leverages both interaction-based and content-based signals about user preferences to produce rich, natural language narratives for conversational recommender systems.

## Key Results
- Combining CF and content embeddings leads to improvements of 4-12% in BLEU, ROUGE, and semantic similarity metrics compared to using either embedding type alone.
- Analysis of soft token embeddings reveals that content-based signals have a more significant contribution than CF signals in the learned representations, though both are important for optimal performance.
- The model demonstrates ability to generalize to new users through evaluation with distinct users in training and test sets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining collaborative filtering (CF) and semantic embeddings as input to an LLM improves narrative generation quality by 4-12% in key language metrics compared to using either embedding type alone.
- Mechanism: The fusion architecture concatenates item embeddings from WALS matrix factorization with semantic embeddings from Gecko, providing the LLM with both interaction-based and content-based signals about user preferences.
- Core assumption: Both CF and semantic signals contain complementary information that, when combined, enable more accurate and personalized narrative generation.
- Evidence anchors:
  - [abstract] "Our experiments show that combining CF and content embeddings leads to improvements of 4-12% in key language metrics compared to using either type of embedding individually."
  - [section] "Combining collaborative filtering (CF) and semantic embeddings leads to up to 12%, +8%, and +8% gains in BLEU, ROUGE, and Semantic Similarity metrics, respectively, compared to using the best result using either of these embeddings alone."
- Break condition: If the adapter layer fails to effectively combine the two embedding types, or if one signal type dominates and drowns out the other, the improvement would not materialize.

### Mechanism 2
- Claim: The model learns to construct rich in-context and aggregate narratives from history instead of simply memorizing using several examples.
- Mechanism: By training on different users for training and test sets, the model must learn to interpret user history from provided embeddings rather than memorizing specific user patterns.
- Core assumption: The fusion architecture with different users in training and test sets forces the model to generalize its understanding of user preferences rather than overfitting to individual users.
- Evidence anchors:
  - [section] "The training and test sets contain different users. This forces the model to learn how to interpret user history from the provided embeddings, rather than memorizing specific users."
  - [section] "We demonstrate the model's capabilities as follows: Evaluation with Distinct Users...This ensures the model can generalize to new users and accurately capture their preferences through the generated narratives."
- Break condition: If the dataset is too small or the embedding space is too constrained, the model might still memorize patterns rather than learning to generalize.

### Mechanism 3
- Claim: Content-based signals have a more significant contribution than CF signals in the learned representations, though both are important for optimal performance.
- Mechanism: Analysis of soft token embeddings reveals that the content-based components (W2es) contribute more to the norm of the final embeddings than the CF components (W1ei).
- Core assumption: In datasets where items appear in relatively few users (10-20 in Office Products), content signals provide richer information than CF signals.
- Evidence anchors:
  - [abstract] "Analysis of soft token embeddings reveals that content-based signals have a more significant contribution than CF signals in the learned representations, though both are important for optimal performance."
  - [section] "Figure 6 shows the distribution of relative contribution (in norm) of projected CF and content embeddings...This shows that the content based signals have more significant representation in the learnt soft tokens compared to the CF parts, though CF parts are not insignificant."
- Break condition: In datasets with richer interaction data (higher user-item density), the CF signals might become more dominant and the relative contribution could shift.

## Foundational Learning

- Concept: Matrix Factorization for Collaborative Filtering
  - Why needed here: WALS is used to generate user and item embeddings from the user-item interaction matrix, which form the CF component of the fused embeddings.
  - Quick check question: How does WALS differ from standard ALS, and why is it particularly suited for implicit feedback datasets like Amazon reviews?

- Concept: Sentence Embeddings and Semantic Representation
  - Why needed here: Gecko (or similar models) generates semantic embeddings for item metadata and reviews, capturing content-based signals about user preferences.
  - Quick check question: What properties should a good sentence embedding model have for this application, and how do these properties affect the quality of generated narratives?

- Concept: Soft Token Embeddings and Adapter Layers
  - Why needed here: The adapter layer (MLP or projection layer) transforms the concatenated CF and semantic embeddings into soft tokens that the LLM can process as input.
  - Quick check question: How does the adapter layer learn to balance the contributions of CF and semantic signals, and what happens if the layer is too simple or too complex?

## Architecture Onboarding

- Component map: Data Preprocessing -> CF Encoder -> Semantic Encoder -> Embedding Fusion -> Adapter Model -> LLM -> Narrative Output

- Critical path: Data Preprocessing → CF Encoder → Semantic Encoder → Embedding Fusion → Adapter Model → LLM → Narrative Output

- Design tradeoffs:
  - Simple concatenation vs. more complex fusion methods (cross-attention, co-training)
  - Separate encoders vs. joint training of CF and semantic representations
  - Fixed prompt vs. dynamic context incorporation
  - Trade-off between embedding dimensionality and model capacity

- Failure signatures:
  - Low BLEU/ROUGE scores indicate poor alignment with reference narratives
  - High variance in automated evaluator scores suggests instability in narrative quality
  - Soft token analysis showing one embedding type dominates completely indicates imbalance in fusion
  - Model generating generic narratives rather than personalized ones suggests failure to learn from embeddings

- First 3 experiments:
  1. Baseline with CF embeddings only → measure performance drop compared to fusion
  2. Baseline with semantic embeddings only → measure performance drop compared to fusion
  3. Ablation study with different adapter architectures (MLP vs. projection layer) → identify optimal adapter complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed fusion architecture generalize to datasets with item counts orders of magnitude larger (O(1M-1B)) than those typically used in research?
- Basis in paper: [explicit] The paper states that the modular approach is "highly practical for real-world datasets with item counts orders of magnitude larger (O(1M-1B)) than those typically used in research, enabling efficient encoding of diverse data scales."
- Why unresolved: The experiments only use datasets with item counts of 27K and 376K, which are significantly smaller than the stated practical range. No experiments or analysis are provided to validate the claimed scalability.
- What evidence would resolve it: Experiments demonstrating the model's performance on datasets with item counts in the O(1M-1B) range, along with analysis of computational costs and any degradation in performance.

### Open Question 2
- Question: Does co-training the encoders with the LLM or fusing embeddings on a recommendations task yield further improvements compared to the independent encoding strategy used in this paper?
- Basis in paper: [inferred] The paper mentions that "future work should explore whether jointly training the encoders with the LM or fusing these embeddings on a recommendations task could yield further improvements."
- Why unresolved: The paper uses an independent encoding strategy and does not explore joint training or fusing embeddings on a recommendations task. No experiments or analysis are provided to compare the performance of the proposed approach with these alternative strategies.
- What evidence would resolve it: Experiments comparing the performance of the proposed approach with co-trained encoders or fused embeddings on a recommendations task, using the same datasets and evaluation metrics.

### Open Question 3
- Question: Does pre-training recommender language models on datasets comparable in scale to those used for general-purpose language models change the behavior of the adapter layer and the learned soft token embeddings?
- Basis in paper: [inferred] The paper states that "it is an open question whether this behavior would persist if recommender language models were pre-trained on datasets comparable in scale to those used for general-purpose language models."
- Why unresolved: The paper only uses a small-scale LLM (PaLM2 XXS) for the experiments. No analysis or experiments are provided to investigate the impact of pre-training on larger datasets.
- What evidence would resolve it: Experiments using pre-trained recommender language models on large-scale datasets, along with analysis of the learned soft token embeddings and their comparison with the embeddings learned by the adapter layer in the proposed approach.

## Limitations
- Dataset Specificity: The REGEN dataset construction process relies heavily on LLM-generated narratives from the Amazon Product Reviews dataset, with no extensive validation of narrative quality or coverage.
- Evaluation Methodology: The automated rater evaluation using Gemini Pro is particularly dependent on the quality of the evaluation prompts and may introduce its own biases.
- Generalizability: The experiments are conducted on two specific Amazon verticals, and performance gains may not generalize to other domains with different characteristics.

## Confidence
- **High Confidence**: The fusion of CF and semantic embeddings leads to 4-12% improvements in BLEU, ROUGE, and semantic similarity metrics; the model demonstrates ability to generalize to new users; content-based signals contribute more significantly than CF signals.
- **Medium Confidence**: The REGEN dataset successfully captures rich user narratives; the adapter layer effectively learns to balance CF and semantic signal contributions; improvements in automated rater evaluation translate to meaningful real-world improvements.
- **Low Confidence**: The specific mechanism by which fusion architecture enables better narrative generation beyond simple concatenation; the optimal balance between CF and semantic signal contributions across different datasets; long-term stability of narrative quality across extended conversations.

## Next Checks
1. **Dataset Validation**: Conduct a human evaluation study to validate the quality, diversity, and representativeness of the narratives in the REGEN dataset across different user profiles and product categories.

2. **Cross-Domain Generalization**: Replicate the experiments on at least two additional Amazon verticals with different characteristics (e.g., High-interaction domains like Books vs. Low-interaction domains like Luxury Items) to assess robustness of observed improvements.

3. **Ablation Study on Fusion Mechanisms**: Perform systematic comparison of simple concatenation vs. cross-attention/co-training, separate vs. joint training of encoders, and different adapter architectures to isolate specific contributions to performance gains.