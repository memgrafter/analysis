---
ver: rpa2
title: 'German Text Simplification: Finetuning Large Language Models with Semi-Synthetic
  Data'
arxiv_id: '2402.10675'
source_url: https://arxiv.org/abs/2402.10675
tags:
- simplification
- text
- language
- data
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study pioneers using synthetically generated data for training
  generative models in document-level text simplification of German texts. Addressing
  the challenge of data scarcity in language simplification, the authors crawled professionally
  simplified German texts and synthesized a corpus using GPT-4.
---

# German Text Simplification: Finetuning Large Language Models with Semi-Synthetic Data

## Quick Facts
- **arXiv ID**: 2402.10675
- **Source URL**: https://arxiv.org/abs/2402.10675
- **Reference count**: 4
- **Primary result**: SARI score of 62.24 and human evaluation score of 2.68 (out of 3) on real-world German text simplification

## Executive Summary
This paper pioneers the use of synthetically generated data for document-level German text simplification. The authors address the challenge of data scarcity by crawling professionally simplified German texts and using GPT-4 to generate corresponding complex originals, creating a semi-synthetic parallel corpus. They finetune Large Language Models with up to 13 billion parameters on this data and evaluate performance using both automatic metrics and human evaluation. The results show that these models can significantly simplify real-world online German texts, demonstrating the potential of synthetic data to improve text simplification systems.

## Method Summary
The approach involves crawling professionally simplified German texts from various web sources, then using GPT-4 to generate synthetic source texts corresponding to these simplifications. This creates a semi-synthetic dataset where synthetic originals map to real simplifications. Large Language Models (up to 13 billion parameters) are finetuned on this dataset for 3 epochs. The models are evaluated using automatic metrics (BLEU, METEOR, SARI) and human evaluation on real-world German texts from the internet. Different decoding strategies (beam search, sampling) are compared to determine optimal generation approaches.

## Key Results
- The best model achieved a SARI score of 62.24 on real-world German texts
- Human evaluation scored the best model at 2.68 out of 3 on real-world data
- Model performance scales with parameter count up to 13 billion parameters, showing diminishing returns beyond 7 billion
- Beam search decoding outperformed sampling methods for the largest model

## Why This Works (Mechanism)

### Mechanism 1: Semi-synthetic Data Generation
The approach addresses data scarcity by leveraging GPT-4 to produce plausible complex German texts for crawled simplified content. By using existing simplified texts as targets and generating corresponding complex originals, the model creates a parallel corpus where synthetic originals map to real simplifications. This works under the assumption that GPT-4 can generate diverse, realistic complex German text that aligns semantically with the simplifications.

### Mechanism 2: Parameter Scaling
Larger model parameters yield better text simplification performance up to a saturation point. Increasing parameters improves the model's capacity to learn complex simplification patterns from the semi-synthetic data, reflected in higher BLEU, METEOR, and SARI scores. This assumes model performance scales with parameter count in the simplification task.

### Mechanism 3: Beam Search Decoding
Beam search decoding outperforms sampling-based methods for text simplification in this context. By exploring multiple candidate sequences and selecting the most probable one, beam search yields more coherent and accurate simplifications than stochastic sampling. This assumes that in text simplification, coherence and accuracy outweigh diversity, making deterministic decoding preferable.

## Foundational Learning

- **Tokenization and subword units in transformer models**: Models operate on tokenized sequences; understanding subword tokenization (e.g., GPT-2's BPE) is essential for interpreting model behavior and tuning generation parameters. *Quick check*: What is the difference between byte-pair encoding and wordpiece tokenization, and how does this affect model vocabulary size?

- **Parallel corpora structure for supervised sequence-to-sequence tasks**: Text simplification requires mapping complex inputs to simplified outputs; understanding parallel dataset alignment and evaluation metrics is critical for training and validation. *Quick check*: How does SARI differ from BLEU in evaluating text simplification, and why might SARI be more suitable for this task?

- **Decoding algorithms and their impact on output quality**: Different decoding strategies (greedy, beam, sampling, contrastive) yield varying trade-offs between coherence, diversity, and accuracy in generated simplifications. *Quick check*: In what scenarios would contrastive search be preferable to beam search, and why did beam search perform best here?

## Architecture Onboarding

- **Component map**: Web crawling → preprocessing → GPT-4 synthesis → dataset creation → model training → automatic evaluation → human evaluation → real-world testing
- **Critical path**: Crawling → synthetic generation → model training → real-world evaluation
- **Design tradeoffs**: Dataset size vs. quality (larger synthetic corpora improve training but risk introducing noise); Model size vs. efficiency (larger models perform better but require more compute); Decoding strategy vs. output style (beam search yields consistent quality; sampling introduces variability)
- **Failure signatures**: Low automatic metric scores (poor synthetic data alignment or insufficient training diversity); High repetition in outputs (decoding parameters need adjustment); Low human evaluation scores (model fails to preserve meaning or over-simplifies)
- **First 3 experiments**: 1) Train a small model (e.g., GPT-2-small) on a subset of synthetic data and evaluate with BLEU/METEOR/SARI; 2) Compare greedy vs. beam search decoding on the same trained model; 3) Fine-tune the 7B model and evaluate on real-world crawled texts

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the analysis, several remain unresolved: How does the complexity of synthetic data compare to real-world web content? How does model performance on semi-synthetic data compare to real-world data? How does semi-synthetic data training compare to other approaches like real parallel data or few-shot learning?

## Limitations
- No direct validation of synthetic text quality - the authors assume semantic alignment between synthetic originals and real simplifications without evaluation
- Beam search superiority only demonstrated for the largest model, not generalizable across model sizes
- Corpus analysis provides weak evidence for synthetic data quality, relying on indirect complexity metrics rather than direct semantic evaluation

## Confidence
- **High Confidence**: Dataset construction pipeline and general finding that larger models perform better up to 13B parameters
- **Medium Confidence**: Claim that semi-synthetic data significantly improves simplification quality (lacks validation for synthetic text quality)
- **Low Confidence**: Specific superiority of beam search decoding for all models (only demonstrated for largest model)

## Next Checks
1. **Synthetic Data Quality Audit**: Manually evaluate 100 synthetic-original pairs for semantic alignment and complexity appropriateness, calculating the percentage of pairs where GPT-4 generates text that appropriately matches the simplification level of the target text.

2. **Cross-Model Decoding Comparison**: Train the 7B model with both beam search and sampling-based decoding, then conduct human evaluations comparing output quality to validate whether beam search superiority extends beyond just the largest model.

3. **Real-World Performance Stress Test**: Apply the best-performing model to a diverse set of German online texts from domains not represented in the training data (e.g., medical, legal, technical), measuring both automatic metrics and human comprehension scores to assess generalization beyond the crawled dataset domains.