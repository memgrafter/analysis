---
ver: rpa2
title: 'PASS++: A Dual Bias Reduction Framework for Non-Exemplar Class-Incremental
  Learning'
arxiv_id: '2407.14029'
source_url: https://arxiv.org/abs/2407.14029
tags:
- learning
- classes
- incremental
- conference
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses catastrophic forgetting in class-incremental
  learning (CIL), where a model must learn new classes while retaining knowledge of
  old classes without access to old training data. The authors identify two main sources
  of forgetting: representation bias (where learned features are not reusable across
  tasks) and classifier bias (where decision boundaries become distorted).'
---

# PASS++

## Quick Facts
- arXiv ID: 2407.14029
- Source URL: https://arxiv.org/abs/2407.14029
- Authors: Fei Zhu; Xu-Yao Zhang; Zhen Cheng; Cheng-Lin Liu
- Reference count: 40
- Average accuracy on CIFAR-100: 69.12% (outperforming state-of-the-art methods)

## Executive Summary
PASS++ addresses catastrophic forgetting in class-incremental learning by tackling two key biases: representation bias (features not reusable across tasks) and classifier bias (distorted decision boundaries). The framework combines self-supervised transformation (SST) through rotation augmentation with prototype augmentation (protoAug) to maintain old class knowledge without storing exemplars. Experimental results show state-of-the-art performance on CIFAR-100, TinyImageNet, and ImageNet-Subset, with particular strength in long incremental phases and under distribution shifts.

## Method Summary
PASS++ implements a dual bias reduction framework that first applies rotation-based self-supervised transformation (SST) to learn generic, transferable representations from current task data. Simultaneously, it maintains class prototypes for old classes and augments them either explicitly (storing full covariance matrices) or implicitly (using radius-based approximations) to constrain classifier weights and preserve old class decision boundaries. The method uses hardness-aware augmentation by mixing nearest new class features with old prototypes to create challenging examples near decision boundaries, and employs multi-view ensemble at inference by averaging logits from all rotation angles.

## Key Results
- Achieves 69.12% average accuracy on CIFAR-100 with 10 incremental phases (50→100 classes)
- Outperforms exemplar-based methods like iCaRL and UCIR on ImageNet-Full with 25 phases
- Demonstrates superior robustness under 15 different corruption scenarios compared to DER and UCIR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SST learns diverse and transferable representations that can be reused across tasks.
- Mechanism: Rotation-based self-supervised transformation augments current classes with rotated versions, forcing the model to learn generic features invariant to orientation.
- Core assumption: Features useful for distinguishing rotated versions of a class will also be useful for distinguishing that class from other classes in future tasks.
- Evidence anchors:
  - [abstract] "SST alleviates the representation bias by learning generic and diverse representations that can transfer across different tasks."
  - [section] "Inspired by the natural connection between incremental learning and self-supervised learning, we propose to leverage the rotation based self-supervised transformation (SST) to reduce the representation bias"
  - [corpus] Weak evidence for rotation-based transfer learning mechanisms in CIL.
- Break condition: If rotated versions introduce task-irrelevant variation or if future tasks require features orthogonal to rotation invariance.

### Mechanism 2
- Claim: Prototype augmentation maintains decision boundaries for old classes.
- Mechanism: Memorizing one prototype per old class and augmenting it generates pseudo-features that constrain classifier weights to preserve old class boundaries.
- Core assumption: Class distributions in deep feature space can be approximated by Gaussian distributions centered at the prototype.
- Evidence anchors:
  - [abstract] "protoAug overcomes the classifier bias by explicitly or implicitly augmenting prototypes of old classes in the deep feature space"
  - [section] "When learning a new task, we generate pseudo-features of old classes by augmenting the memorized prototypes"
  - [corpus] Limited evidence for prototype-based distribution approximation in CIL.
- Break condition: If class distributions are multi-modal or if prototypes drift significantly during new task learning.

### Mechanism 3
- Claim: Hardness-aware augmentation improves boundary maintenance.
- Mechanism: Mixing nearest new class features with old prototypes creates challenging examples near decision boundaries.
- Core assumption: The nearest new class sample provides the most informative constraint for maintaining old class boundaries.
- Evidence anchors:
  - [section] "hardness-aware protoAug strategy to enhance the performance... generates informative and hard feature instances near the decision boundary"
  - [corpus] No direct corpus evidence for hardness-aware mixing strategies in CIL.
- Break condition: If nearest neighbor selection becomes noisy or if mixing ratio λ is poorly chosen.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why models lose old knowledge when learning new tasks is fundamental to addressing the problem
  - Quick check question: What causes catastrophic forgetting when fine-tuning on new classes without access to old data?

- Concept: Self-supervised learning through transformations
  - Why needed here: SST relies on learning from augmented views of data without labels
  - Quick check question: How does predicting rotation angles help learn features useful for classification?

- Concept: Prototype-based representation
  - Why needed here: protoAug assumes class distributions can be represented by their means in feature space
  - Quick check question: What assumptions about class distributions make prototype-based methods work?

## Architecture Onboarding

- Component map:
  Feature extractor (ResNet-18 or ViT with LoRA) -> Rotation augmentation module -> Prototype storage module -> Classifier with expanded output nodes -> Multi-view ensemble module

- Critical path: Input -> Rotation augmentation -> Feature extraction -> Prototype augmentation -> Classification -> Multi-view ensemble

- Design tradeoffs:
  - SST increases training time but improves transfer
  - Explicit vs implicit protoAug: memory vs computation tradeoff
  - Number of rotations affects representation diversity vs training cost

- Failure signatures:
  - Accuracy plateaus early -> SST not effective
  - Old class accuracy drops -> protoAug insufficient
  - New class accuracy drops -> Too much regularization

- First 3 experiments:
  1. CIFAR-10 with 4→6 class split: Validate SST improves new class accuracy
  2. MNIST incremental: Visualize protoAug maintains old class distributions
  3. CIFAR-100 with 10 phases: Compare explicit vs implicit protoAug memory usage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PASS++ compare to exemplar-based methods when integrated with pre-trained models on ImageNet-Full, particularly in long incremental phases (25 tasks)?
- Basis in paper: [explicit] The paper mentions that PASS++ achieves superior performance compared to popular exemplar-based methods like iCaRL, UCIR, and PODnet on ImageNet-Full, with the results being comparable to DER (w/o pruning) in many cases.
- Why unresolved: While the paper demonstrates that PASS++ performs well on ImageNet-Full, it does not provide a detailed comparison with exemplar-based methods in long incremental phases (25 tasks), which is a crucial aspect for real-world applications.
- What evidence would resolve it: Conducting experiments on ImageNet-Full with 25 incremental phases and comparing the performance of PASS++ with exemplar-based methods like DER, UCIR, and PODnet would provide the necessary evidence to answer this question.

### Open Question 2
- Question: How does the robustness of PASS++ under distribution shift compare to that of exemplar-based methods, and what are the underlying reasons for any differences?
- Basis in paper: [explicit] The paper discusses the evaluation of incremental learning models under distribution shift and provides results comparing PASS++ with exemplar-based methods like UCIR and DER under 15 different corruption scenarios.
- Why unresolved: While the paper presents results on the robustness of PASS++ under distribution shift, it does not delve into the reasons behind any differences in robustness compared to exemplar-based methods.
- What evidence would resolve it: Analyzing the underlying reasons for the differences in robustness between PASS++ and exemplar-based methods, such as the impact of different training strategies and model architectures, would provide insights into this question.

### Open Question 3
- Question: What is the impact of using different forms of covariance matrices (original, diagonal, or radius) for implicit prototype augmentation on the performance and memory cost of PASS++?
- Basis in paper: [explicit] The paper mentions that using the original covariance matrix is slightly better than the diagonal and spherical forms, but storing the original covariance matrix might be inefficient when the matrix dimension is large.
- Why unresolved: The paper does not provide a detailed comparison of the performance and memory cost of using different forms of covariance matrices for implicit prototype augmentation in PASS++.
- What evidence would resolve it: Conducting experiments to compare the performance and memory cost of using different forms of covariance matrices (original, diagonal, or radius) for implicit prototype augmentation in PASS++ would provide the necessary evidence to answer this question.

## Limitations
- Requires storing prototypes for all old classes, leading to linear growth in memory requirements
- Rotation augmentation effectiveness may be dataset-dependent and not universally applicable
- Nearest neighbor selection for hardness-aware augmentation can be noisy in high-dimensional spaces
- Multi-view ensemble at inference increases computational cost by a factor of 4

## Confidence
- High confidence in representation bias reduction mechanism (SST) due to clear empirical evidence and established self-supervised learning principles
- Medium confidence in classifier bias reduction (protoAug) as prototype-based methods show consistent but not universally optimal performance
- Medium confidence in hardness-aware augmentation effectiveness due to limited ablation studies and no comparison with alternative boundary maintenance strategies

## Next Checks
1. Test SST effectiveness on datasets where rotation invariance is less relevant (e.g., CUB-200 birds) to validate generalizability beyond natural scene datasets
2. Conduct memory efficiency analysis comparing explicit vs implicit protoAug under varying prototype storage constraints
3. Evaluate performance degradation when reducing the number of rotation angles in SST from 4 to 2 to assess the tradeoff between accuracy and computational cost