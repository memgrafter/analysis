---
ver: rpa2
title: 'Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation
  via Multi-Agent Collaboration'
arxiv_id: '2406.01014'
source_url: https://arxiv.org/abs/2406.01014
tags:
- operation
- agent
- arxiv
- operations
- mobile-agent-v2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Mobile-Agent-v2 addresses the challenge of effective navigation
  in long sequences of interleaved text and images during mobile device operation
  tasks. It introduces a multi-agent architecture with three specialized roles: planning
  agent, decision agent, and reflection agent.'
---

# Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration

## Quick Facts
- arXiv ID: 2406.01014
- Source URL: https://arxiv.org/abs/2406.01014
- Reference count: 31
- Primary result: Mobile-Agent-v2 achieves over 30% improvement in task completion rate compared to single-agent Mobile-Agent, reaching 55% success rate on advanced instructions

## Executive Summary
Mobile-Agent-v2 addresses the challenge of effective navigation in long sequences of interleaved text and images during mobile device operation tasks. The system introduces a three-agent architecture (planning, decision, and reflection) that collaboratively handles mobile device operations with improved navigation efficiency. By condensing history operations into task progress summaries and incorporating a memory unit for focus content retention, Mobile-Agent-v2 demonstrates significant performance improvements over single-agent approaches, particularly in multi-app scenarios.

## Method Summary
Mobile-Agent-v2 employs a multi-agent architecture with three specialized roles: a planning agent that generates task progress from history operations, a decision agent that performs operations using visual perception tools and a memory unit, and a reflection agent that corrects erroneous operations by observing screen changes. The system uses GPT-4 for planning, GPT-4V for decision and reflection, and integrates visual perception tools (OCR, object detection, icon description) to enhance operation accuracy. A memory unit stores focus content related to the current task, and the system operates within a fixed space of six operations (Open app, Tap, Swipe, Type, Home, Stop).

## Key Results
- Mobile-Agent-v2 achieves over 30% improvement in task completion rate compared to single-agent Mobile-Agent
- Success rate reaches 55% on advanced instructions with significant improvements across all instruction difficulties
- Demonstrates 37.5% success rate improvement and 44.2% completion rate improvement in multi-app scenarios involving two applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The planning agent reduces navigation burden by condensing interleaved image-text history into pure-text task progress summaries
- Mechanism: Instead of passing full operation sequences with screenshots to the decision agent, the planning agent extracts only task-relevant semantic progress, dramatically reducing context length and eliminating interleaved multimodal complexity
- Core assumption: Task progress can be accurately represented in text without losing critical navigation information needed for decision-making
- Evidence anchors:
  - [abstract] "The planning agent generates task progress from history operations" and "making the navigation of history operations more efficient"
  - [section 3.3] "planning agent to summarize the history operations and track task progress" with formula T Pt = P A(Ins, Ot−1, T Pt−1, F Ct−1)
  - [corpus] Weak evidence - related work focuses on GUI agents but doesn't directly validate text summarization approach

### Mechanism 2
- Claim: The memory unit preserves focus content across app transitions, enabling multi-app task completion
- Mechanism: Decision agent actively observes current screen for task-relevant content and updates a dedicated memory store, which persists across app switches and informs subsequent decisions
- Core assumption: Task-relevant information appears visually on screens and can be reliably identified and stored by the decision agent
- Evidence anchors:
  - [abstract] "To retain focus content, we design a memory unit that updates with task progress by decision agent"
  - [section 3.2] "We design a memory unit to store the focus content related to the current task from history screens"
  - [section 3.4] Formula F Ct = DA(Ins, F Ct−1, St, Pt) showing memory update mechanism

### Mechanism 3
- Claim: The reflection agent corrects erroneous and ineffective operations by observing screen state changes
- Mechanism: After each decision agent operation, the reflection agent compares pre/post screenshots and operation intent to detect mismatches, triggering corrective actions
- Core assumption: Screen state changes can be reliably detected and mapped to operation success/failure criteria
- Evidence anchors:
  - [abstract] "the reflection agent corrects erroneous or ineffective operations by observing screen changes"
  - [section 3.5] "reflection agent to observe the screens before and after the decision agent's operation to determine whether the current operation meets expectations"
  - [section 3.5] Formula Rt = RA(Ins, F Ct, Ot, St, Pt, St+1, Pt+1) with three reflection outcomes

## Foundational Learning

- Concept: Multi-agent collaboration patterns
  - Why needed here: Mobile device operation requires separating distinct cognitive tasks (planning, decision-making, error correction) that benefit from specialized agents
  - Quick check question: What are the three distinct agent roles in Mobile-Agent-v2 and what specific function does each serve?

- Concept: Visual perception tool integration with LLMs
  - Why needed here: MLLMs alone struggle with precise screen element localization and text recognition, requiring specialized tools for robust operation
  - Quick check question: How does the visual perception module complement the decision agent's capabilities?

- Concept: Memory management in sequential task execution
  - Why needed here: Multi-app operations require preserving task-relevant information across different application contexts
  - Quick check question: What triggers the memory unit update and what type of information gets stored?

## Architecture Onboarding

- Component map:
  Planning Agent (GPT-4) -> Decision Agent (GPT-4V) -> Reflection Agent (GPT-4V)
  Visual Perception Module (OCR + Object Detection + Icon Description) -> Decision Agent
  Memory Unit (Short-term storage) -> Decision Agent, Planning Agent, Reflection Agent
  User Instruction -> All Agents
  Device Operation Execution -> Decision Agent

- Critical path: User instruction → Planning agent (progress update) → Decision agent (operation + memory update) → Reflection agent (validation) → Device execution

- Design tradeoffs:
  - Single-agent vs multi-agent: Single-agent struggles with long interleaved contexts; multi-agent adds coordination complexity but improves navigation
  - End-to-end MLLM vs tool-assisted: End-to-end simpler but less accurate; tool-assisted more complex but handles localization better
  - Memory vs re-perception: Memory saves computation but risks stale data; re-perception ensures freshness but increases cost

- Failure signatures:
  - Planning agent produces vague task progress → Decision agent generates irrelevant operations
  - Memory unit misses critical focus content → Multi-app tasks fail
  - Reflection agent misclassifies operations → Loops or incorrect state transitions
  - Visual perception tools fail → Decision agent cannot locate targets

- First 3 experiments:
  1. Unit test planning agent with various operation histories to verify task progress generation quality
  2. Integration test decision agent with visual perception tools to verify operation localization accuracy
  3. End-to-end test with simple single-app instructions to verify complete agent pipeline functionality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Mobile-Agent-v2 scale with increasingly complex multi-app scenarios involving more than two applications?
- Basis in paper: [inferred] The paper mentions Mobile-Agent-v2's capability in multi-app scenarios with 2 apps, showing improvements of 37.5% in success rate and 44.2% in completion rate compared to Mobile-Agent.
- Why unresolved: The paper only evaluates multi-app tasks involving 2 apps. The performance in scenarios with 3 or more apps is not tested.
- What evidence would resolve it: Experimental results showing success rate, completion rate, and decision accuracy of Mobile-Agent-v2 on multi-app tasks involving 3 or more applications.

### Open Question 2
- Question: What is the impact of memory unit size and retention duration on Mobile-Agent-v2's performance?
- Basis in paper: [inferred] The paper describes the memory unit as a short-term memory module that updates with task progress, but does not specify its capacity or retention policies.
- Why unresolved: The paper mentions the memory unit exists and is updated by the decision agent, but doesn't detail its size limitations or how long information is retained before being forgotten.
- What evidence would resolve it: Experiments varying memory unit capacity (e.g., number of items stored) and retention duration, showing how these parameters affect success rate and completion rate across different task complexities.

### Open Question 3
- Question: How does Mobile-Agent-v2's performance degrade when facing screen recognition errors from the visual perception module?
- Basis in paper: [explicit] The paper mentions that even with the visual perception module, Mobile-Agent-v2 may still generate unexpected operations, and that MLLMs can produce severe hallucinations even in the most advanced models like GPT-4V.
- Why unresolved: While the paper acknowledges the reflection agent's role in handling errors, it doesn't quantify how often screen recognition errors occur or measure performance degradation when these errors happen.
- What evidence would resolve it: Analysis showing the frequency of screen recognition errors, correlation between these errors and operation failures, and performance metrics (success rate, completion rate) when the reflection agent successfully or unsuccessfully corrects these errors.

## Limitations

- The memory unit mechanism lacks detailed validation of its effectiveness across different task types and operating systems
- The reflection agent's ability to correctly identify and handle all types of operation failures is not thoroughly tested, particularly for complex UI state changes
- The visual perception module's accuracy in handling diverse UI layouts and languages across different applications is not comprehensively evaluated

## Confidence

- High Confidence: The 30% improvement claim over Mobile-Agent is well-supported by the experimental results presented
- Medium Confidence: The effectiveness of the multi-agent architecture is demonstrated, but the individual contributions of each component could be better isolated
- Low Confidence: The generalizability of the system to completely different mobile operating systems and applications beyond those tested

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of the planning agent, memory unit, and reflection agent to the overall performance improvement
2. Test the system on additional operating systems (iOS, Windows Mobile) and applications not included in the original benchmark to assess generalizability
3. Implement and measure the performance of a single-agent baseline that uses the same visual perception tools but without the multi-agent architecture to better isolate the architectural contribution