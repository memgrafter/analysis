---
ver: rpa2
title: Composing Reinforcement Learning Policies, with Formal Guarantees
arxiv_id: '2402.13785'
source_url: https://arxiv.org/abs/2402.13785
tags:
- reset
- room
- state
- latent
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a formal framework for synthesizing controllers
  in two-level environments by combining reinforcement learning with reactive synthesis.
  The key innovation is training low-level policies with latent models that provide
  PAC guarantees, then composing these into a high-level planner with formal correctness
  guarantees.
---

# Composing Reinforcement Learning Policies, with Formal Guarantees

## Quick Facts
- arXiv ID: 2402.13785
- Source URL: https://arxiv.org/abs/2402.13785
- Reference count: 40
- Key outcome: Framework for composing RL policies with formal guarantees via latent models and PAC bounds

## Executive Summary
This paper presents a novel framework for synthesizing controllers in two-level environments by combining reinforcement learning with reactive synthesis. The approach trains low-level policies on latent MDPs with PAC guarantees on abstraction quality, then composes these into a high-level planner with formal correctness guarantees. Experiments demonstrate successful training of latent policies where standard DQN fails, achieving high success rates while maintaining formal guarantees in challenging grid-world and vision-based environments.

## Method Summary
The framework trains low-level policies and latent models for each room-direction pair using WAE-DQN, which jointly optimizes a latent MDP's transition function and a DQN policy. A latent entrance function models distribution shifts between training and synthesis phases. The high-level planner is synthesized by constructing a succinct MDP where states represent room entry directions and transition probabilities encode expected low-level values. This reduction enables formal guarantees on the composed controller's performance through PAC bounds on the abstraction quality.

## Key Results
- WAE-DQN successfully trains latent policies in environments where standard DQN fails
- The composed two-level controller achieves high success rates while maintaining formal guarantees
- PAC bounds on abstraction quality effectively predict the difference between latent and true value functions
- Framework handles vision-based environments with unknown dynamics intractable for traditional formal methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning latent policies with PAC guarantees on abstraction quality enables formal synthesis of high-level planners.
- Mechanism: The framework trains low-level policies on latent MDPs where transition losses bound the difference between latent and true values. These guarantees are then lifted to the high-level planner by composing the latent models and policies into a succinct MDP whose transition probabilities encode expected low-level values.
- Core assumption: The initial distribution in each room provides sufficient state space coverage and the latent entrance function accurately models the distribution shift between training and synthesis.
- Evidence anchors:
  - [abstract] "We develop a reinforcement learning procedure to train low-level policies on latent structures...pair the policy with probably approximately correct guarantees on its performance and on the abstraction quality"
  - [section] "minimizing the loss function to learn an abstraction of each room independently...guarantees that the values obtained under the two-level controller in the abstraction closely match those obtained in the true two-level environment"
  - [corpus] Weak - neighboring papers discuss hierarchical RL and formal guarantees but don't directly address the PAC-bound lifting mechanism
- Break condition: If the initial distribution doesn't cover the state space adequately or the latent entrance function poorly models the distribution shift, the lifted guarantees become invalid.

### Mechanism 2
- Claim: The WAE-DQN algorithm simultaneously learns latent policies and latent models, avoiding the need for separate distillation steps.
- Mechanism: WAE-DQN alternates between optimizing the latent MDP's transition function and embedding function via WAE-MDP, and optimizing a DQN policy on the learned latent representation. This joint optimization ensures the latent space groups states with similar values and the policy prescribes similar actions for similar states.
- Core assumption: The Wasserstein distance between the latent and true transition distributions can be minimized through gradient descent on the latent MDP parameters.
- Evidence anchors:
  - [abstract] "develops a novel training procedure that trains a latent policy directly and circumvents the need for model distillation"
  - [section] "WAE-DQN ensures the following properties: (i) ùúô groups states with close values, supporting the learning of ùúã; (ii) ùúã prescribes the same actions for states with close behaviors"
  - [corpus] Weak - neighboring papers discuss distillation and verification but don't address the joint learning approach
- Break condition: If the latent MDP becomes too complex for gradient-based optimization or the DQN policy overfits to the learned latent space.

### Mechanism 3
- Claim: The two-level controller construction reduces to finding an optimal policy in a succinct MDP where states represent room entry directions and actions represent planner choices.
- Mechanism: The framework constructs M G
Œ† where states are ‚ü®ùë£, ùë¢‚ü© pairs indicating entering room ùë¢ from direction ‚ü®ùë£, ùë¢‚ü©. The transition probabilities encode the expected values achieved by low-level policies in each room. Finding an optimal policy in this MDP corresponds to finding an optimal planner for the original two-level model.
- Core assumption: The low-level policies provide accurate value estimates for their respective reach-avoid objectives, which can be composed multiplicatively in the high-level MDP.
- Evidence anchors:
  - [abstract] "we construct a 'planner' that selects which low-level policy to apply in each room"
  - [section] "finding a policy in MŒ† is equivalent to finding a policy in a succinct model M G
Œ† where...the transition probabilities to the values achieved by the latent policy chosen"
  - [corpus] Weak - neighboring papers discuss MDP reductions but not this specific construction for two-level environments
- Break condition: If the low-level value estimates are inaccurate or the composition of values doesn't preserve optimality due to non-Markovian effects.

## Foundational Learning

- Concept: Markov Decision Processes and their value functions
  - Why needed here: The entire framework is built on MDP theory - both the explicit two-level model and the latent MDPs use MDP concepts for policy optimization and value computation
  - Quick check question: Can you derive the Bellman equation for a discounted MDP and explain why it has a unique solution under ergodicity?

- Concept: Probably Approximately Correct (PAC) learning bounds
  - Why needed here: The framework provides PAC guarantees on the abstraction quality and policy performance, which are essential for formal synthesis and correctness claims
  - Quick check question: Given a bound on the transition loss between latent and true MDPs, can you derive the bound on the value difference using the contraction mapping argument?

- Concept: Reinforcement learning algorithms (DQN, WAE-MDP)
  - Why needed here: The framework uses DQN for policy learning and WAE-MDP for representation learning - understanding these algorithms is crucial for implementing and extending the approach
  - Quick check question: How does the WAE-MDP loss function differ from standard auto-encoder reconstruction loss, and why is this difference important for MDP abstraction?

## Architecture Onboarding

- Component map:
  - Room MDPs -> Latent MDPs -> Low-level policies ùúãùëÖ,ùëë -> Latent entrance functions -> Succinct MDP M G
Œ† -> Two-level controller ‚ü®ùúè, Œ† ‚ü©

- Critical path:
  1. Train latent policies and models for each room-direction using WAE-DQN
  2. Learn latent entrance functions via exploration and MADE training
  3. Construct M G
Œ† with transition probabilities from low-level values
  4. Synthesize optimal planner ùúè for high-level objective
  5. Validate controller performance and bound satisfaction

- Design tradeoffs:
  - Memory vs. optimality: Using |V|-memory planners vs. potentially smaller memory with approximate solutions
  - Abstraction quality vs. computational cost: Finer latent state spaces give better guarantees but require more computation
  - Parallel training vs. coordination: Independent room training is efficient but requires careful handling of distribution shifts

- Failure signatures:
  - High transition loss ùêøP: Latent MDP poorly approximates true dynamics
  - Large entrance loss ùêøI: Latent entrance function fails to model distribution shift
  - Poor high-level performance: Low-level value estimates inaccurate or composition fails
  - Training instability: WAE-DQN struggles to converge or latent space becomes degenerate

- First 3 experiments:
  1. Single room validation: Train WAE-DQN on a simple grid world room and verify PAC bounds match empirical value differences
  2. Two-room composition: Construct M G
Œ† for two connected rooms and verify planner finds optimal path
  3. Distribution shift test: Intentionally modify entrance distributions and measure impact on lifted guarantees

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust are the PAC bounds to variations in the initial distribution of each room during training and synthesis?
- Basis in paper: [explicit] The paper discusses initial distribution shifts between training and synthesis phases and proposes learning a latent entrance function to address this. Theorem 8 provides bounds on the transition loss that depend on the difference between training and synthesis distributions.
- Why unresolved: The paper provides theoretical bounds, but the practical impact of initial distribution shifts on the quality of the PAC guarantees is not empirically evaluated. The extent to which the bounds hold in practice when the initial distributions differ significantly is unclear.
- What evidence would resolve it: Empirical results comparing the theoretical PAC bounds with the observed performance when varying the initial distribution during synthesis would be needed. This could involve systematically modifying the entrance function during synthesis and measuring the degradation in performance relative to the predicted bounds.

### Open Question 2
- Question: How does the proposed framework handle environments where rooms have different structures or dynamics?
- Basis in paper: [inferred] The paper assumes that rooms can be modeled as MDPs with unknown dynamics and that a latent model can be learned for each room. However, the paper does not explicitly address the case where rooms have significantly different structures or dynamics.
- Why unresolved: The framework relies on learning a latent model for each room, but the effectiveness of this approach when rooms have very different characteristics is not explored. It is unclear how the framework would handle cases where the dynamics of one room are vastly different from those of another.
- What evidence would resolve it: Experiments evaluating the performance of the framework in environments with rooms of varying structures and dynamics would be needed. This could involve creating environments with rooms that have different layouts, adversary behaviors, or other properties, and measuring the impact on the learned policies and the overall performance of the controller.

### Open Question 3
- Question: Can the framework be extended to handle more complex specifications beyond reach-avoid objectives?
- Basis in paper: [explicit] The paper focuses on reach-avoid objectives at both the high and low levels. While the authors mention that reachability properties are building blocks for verifying more complex specifications, they do not explore this extension in the paper.
- Why unresolved: The paper provides a framework for synthesizing controllers for reach-avoid objectives, but it does not address the challenge of handling more complex specifications such as those expressible in temporal logics like LTL or PCTL.
- What evidence would resolve it: Extending the framework to handle more complex specifications would require developing new methods for translating these specifications into the two-level model and synthesizing controllers that satisfy them. This could involve exploring techniques from reactive synthesis or other formal methods for handling temporal logic specifications.

## Limitations
- The framework assumes sufficient state space coverage by initial distributions, which may not hold in complex environments with limited exploration
- WAE-DQN's convergence properties lack comprehensive theoretical guarantees, particularly regarding potential mode collapse in latent spaces
- The approach focuses on reach-avoid objectives and doesn't address more complex temporal logic specifications

## Confidence
- **Medium**: The formal guarantee lifting mechanism is theoretically sound but depends on assumptions about state space coverage that may not hold in practice
- **Low**: The WAE-DQN algorithm's joint optimization approach lacks comprehensive theoretical guarantees and convergence proofs
- **High**: The MDP reduction construction for the high-level planner is mathematically well-established, though implementation details remain unclear

## Next Checks
1. **PAC Bound Validation**: Conduct systematic experiments measuring the gap between theoretical PAC bounds and empirical value differences across varying latent state space granularities and room complexities.

2. **Distribution Shift Sensitivity**: Design controlled experiments that intentionally modify entrance distributions between training and synthesis phases to quantify the impact on lifted guarantees and identify threshold conditions for guarantee failure.

3. **WAE-DQN Stability Analysis**: Perform ablation studies on the Wasserstein distance components, regularization scales, and temperature parameters to identify optimal configurations and failure modes when latent spaces become degenerate.