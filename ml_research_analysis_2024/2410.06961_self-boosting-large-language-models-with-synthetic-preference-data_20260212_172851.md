---
ver: rpa2
title: Self-Boosting Large Language Models with Synthetic Preference Data
arxiv_id: '2410.06961'
source_url: https://arxiv.org/abs/2410.06961
tags:
- data
- synpo
- preference
- synthetic
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SynPO, a self-boosting method for large language
  models that uses synthetic preference data for alignment. The core idea is to iteratively
  generate synthetic prompts and refine model responses to create preference pairs
  for training, eliminating the need for human-annotated data.
---

# Self-Boosting Large Language Models with Synthetic Preference Data

## Quick Facts
- arXiv ID: 2410.06961
- Source URL: https://arxiv.org/abs/2410.06961
- Reference count: 29
- Key outcome: SynPO achieves over 22.1% win rate gains on AlpacaEval 2.0 and ArenaHard benchmarks while improving Open LLM leaderboard scores by 3.2-5.0 points

## Executive Summary
This paper introduces SynPO, a self-boosting method for large language models that uses synthetic preference data for alignment without requiring human annotation. The core innovation is an iterative mechanism where a self-prompt generator creates diverse prompts and a response improver refines model outputs to create synthetic preference pairs. Tested on Llama3-8B and Mistral-7B, SynPO significantly improves instruction-following abilities and general performance across multiple benchmarks, demonstrating that LLMs can autonomously learn generative rewards for their own outputs through synthetic data.

## Method Summary
SynPO employs an iterative self-boosting approach that generates synthetic prompts and refines model responses to create preference pairs for training. The method trains a self-prompt generator on seed data to create diverse prompts using keyword sampling, then uses a response improver to refine model outputs on these prompts. The pre- and post-refinement responses form synthetic preference pairs, which are filtered and used for preference optimization via SimPO loss. The process iterates 4 times, validating against small held-out seed data to prevent model deviation, requiring only a small SFT dataset initially.

## Key Results
- Achieved over 22.1% win rate gains on AlpacaEval 2.0 length-controlled and raw evaluations
- Improved ArenaHard win rates significantly compared to baseline models
- Enhanced general performance with 3.2 to 5.0 average score increases on Open LLM leaderboard across 6 tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SynPO iteratively improves model responses by training a response improver to refine policy model outputs, creating synthetic preference pairs for training.
- Mechanism: The response improver learns the gap between current model outputs and gold standard responses on seed data, then refines model responses to synthetic prompts, creating pre- and post-refinement pairs that serve as chosen and rejected responses.
- Core assumption: LLMs excel at identifying distribution gaps between texts and refining responses is easier than generating high-quality responses from scratch.
- Evidence anchors: Abstract states "This approach trains LLMs to autonomously learn the generative rewards for their own outputs"; response improver uses "only the LLM itself and three random keywords as input"; corpus neighbors discuss related preference optimization methods.

### Mechanism 2
- Claim: SynPO generates diverse synthetic prompts using a self-prompt generator trained on seed data, enabling effective preference learning across varied scenarios.
- Mechanism: The self-prompt generator is trained on seed data by extracting keywords from prompts and using corresponding completions as targets, enabling generation of unlimited diverse prompts controlled by keyword inputs.
- Core assumption: Keyword sampling from different paragraphs maintains inherent keyword relationships while ensuring overall diversity.
- Evidence anchors: Abstract mentions "self-prompt generator creates diverse prompts"; approach "requires only a small SFT data to enable the model to generate diverse prompts independently"; corpus neighbors discuss related prompt generation approaches.

### Mechanism 3
- Claim: SynPO uses iterative preference optimization on synthetic data while validating on seed data to prevent model deviation during training.
- Mechanism: Each iteration uses the previous model to generate synthetic preference data, which is then used to optimize the initial model, with small validation sets ensuring the model doesn't deviate from human-aligned responses.
- Core assumption: Synthetic preference data can effectively guide model improvement when validated against high-quality seed data.
- Evidence anchors: Abstract states "This approach"; iterative mechanism uses "small SFT data" for validation; synthetic data generation process described in methodology section.

## Foundational Learning

**Synthetic Preference Optimization**: Using artificially generated data pairs (preferred vs non-preferred responses) for model alignment instead of human annotations. Why needed: Eliminates costly human annotation while enabling continuous model improvement. Quick check: Verify that generated preference pairs maintain quality through filtering thresholds.

**Keyword-to-Text Generation**: Converting sampled keywords into coherent text prompts using trained generators. Why needed: Enables creation of diverse synthetic prompts without manual curation. Quick check: Ensure generated prompts have low inter-prompt similarity (<0.1) to confirm diversity.

**Response Refinement vs Generation**: Training models to improve existing outputs rather than creating from scratch. Why needed: More efficient than pure generation and leverages model's self-correction capabilities. Quick check: Measure improvement in response quality metrics between pre- and post-refinement outputs.

## Architecture Onboarding

**Component Map**: Seed Data → Self-Prompt Generator → Synthetic Prompts → Response Improver → Synthetic Preference Pairs → SimPO Optimization → Improved Model

**Critical Path**: The iterative loop of synthetic prompt generation → response refinement → preference pair creation → model optimization forms the core improvement cycle, with validation on seed data preventing deviation.

**Design Tradeoffs**: Uses only the LLM itself and minimal external inputs (3 random keywords) rather than powerful external models or extensive instruction examples, trading some generation quality for autonomy and scalability.

**Failure Signatures**: Model collapse on synthetic data (repetition patterns >50%), no improvement over iterations (stagnant win rates), degraded performance on general tasks (score decreases on Open LLM tasks).

**First Experiments**: 1) Validate synthetic prompt diversity by measuring inter-prompt similarity across generated sets, 2) Test response improver quality by comparing pre/post refinement outputs on held-out prompts, 3) Run ablation study removing data filtering to assess impact on final model quality.

## Open Questions the Paper Calls Out

**Open Question 1**: How does the quality of the response improver affect the overall performance of SynPO? The paper mentions the response improver's importance but doesn't analyze its specific contribution through ablation studies or quality metrics.

**Open Question 2**: Can SynPO be effectively applied to other types of language models, such as decoder-only models or multimodal models? The paper focuses on Llama3-8B and Mistral-7B decoder-only models without exploring applicability to encoder-only or multimodal architectures.

**Open Question 3**: How does the size of the seed SFT data affect the performance of SynPO? The paper uses a fixed 18k seed data amount but doesn't explore how different seed data sizes might impact final results or convergence behavior.

## Limitations

- The iterative optimization process may be susceptible to compounding errors if synthetic data quality degrades over time
- The method's effectiveness across different model scales and architectures remains untested beyond Llama3-8B and Mistral-7B
- Long-term stability of performance gains and robustness across different seed data distributions are not evaluated

## Confidence

**High Confidence Claims**: Measurable improvements on instruction-following benchmarks (AlpacaEval 2.0, ArenaHard), technical soundness of iterative synthetic data generation, statistically significant performance gains on Open LLM leaderboard.

**Medium Confidence Claims**: Effective capture of diverse prompt distributions by self-prompt generator, reliable creation of useful preference pairs by response improver, generalization across Mistral-7B and Llama3-8B architectures.

**Low Confidence Claims**: Long-term stability of performance gains over extended iterations, robustness across different seed data distributions, effectiveness for models substantially larger than tested sizes.

## Next Checks

1. **Data Quality Analysis**: Conduct detailed analysis of synthetic preference pair quality across iterations to identify degradation patterns and establish optimal iteration thresholds.

2. **Cross-Domain Generalization**: Test SynPO's effectiveness on non-instruction-following tasks like mathematical reasoning and code generation to validate general applicability beyond current scope.

3. **Scale Sensitivity Study**: Evaluate performance across different model scales (1B to 70B parameters) to determine if effectiveness is scale-dependent or maintains consistent relative improvements.