---
ver: rpa2
title: 'Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm'
arxiv_id: '2408.05834'
source_url: https://arxiv.org/abs/2408.05834
tags:
- dcpc
- predictive
- coding
- learning
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Divide-and-Conquer Predictive Coding (DCPC),
  a novel algorithm for structured Bayesian inference in the brain. DCPC addresses
  the limitations of existing predictive coding algorithms by respecting the correlation
  structure of the generative model and provably performing maximum-likelihood updates
  of model parameters.
---

# Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm

## Quick Facts
- arXiv ID: 2408.05834
- Source URL: https://arxiv.org/abs/2408.05834
- Reference count: 40
- Key outcome: DCPC achieves better numerical performance than competing algorithms and provides accurate inference in problems not previously addressed with predictive coding, e.g., on MNIST achieving negative log-likelihood of 102.5 with mean squared error of 0.01

## Executive Summary
This paper introduces Divide-and-Conquer Predictive Coding (DCPC), a novel algorithm for structured Bayesian inference in the brain. DCPC addresses the limitations of existing predictive coding algorithms by respecting the correlation structure of the generative model and provably performing maximum-likelihood updates of model parameters. The core method idea is to decompose the problem of sampling from structured targets into local coordinate updates to individual random variables, informed by prediction errors. DCPC combines these local updates with Sequential Monte Carlo to target any statically structured graphical model.

## Method Summary
DCPC is a structured Bayesian inference algorithm that recursively targets each variable's complete conditional density using local prediction errors. It combines coordinate updates with sequential Monte Carlo to approximate structured target distributions. The algorithm requires two hyperparameters: a learning rate and particle count, and is initialized via ancestor sampling. DCPC's updates are informed by prediction errors defined as score functions of the complete conditional density, which drive Langevin proposals. The method provably performs maximum-likelihood updates of model parameters using only local computations, making it biologically plausible.

## Key Results
- On MNIST, DCPC achieves negative log-likelihood of 102.5 with mean squared error of 0.01
- DCPC outperforms Monte Carlo Predictive Coding (144.6 NLL, 0.0829 MSE) on MNIST
- DCPC provides accurate inference in problems not previously addressed with predictive coding

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DCPC improves over classical predictive coding by respecting the correlation structure of the generative model.
- **Mechanism:** Instead of factorizing the posterior into independent marginals, DCPC targets complete conditional densities using local gradient updates informed by prediction errors. This allows correlated latents to be updated jointly within each coordinate step.
- **Core assumption:** The generative model has a known causal structure and the conditional densities are differentiable.
- **Evidence anchors:**
  - [abstract] "DCPC differs from other formulations of predictive coding, as it respects the correlation structure of the generative model..."
  - [section] "DCPC then requires two hyperparameters: a learning rate η ∈ R+, and particle count K ∈ N+, and is initialized (at t = 0) via a population of predictions by ancestor sampling defined as z0 ∼Q z∈z pθ(z0 | Pa(z0))."
  - [corpus] Weak evidence; corpus neighbors discuss structured variational inference but not predictive coding with Gibbs-style conditioning.
- **Break condition:** If the model structure is unknown or conditional densities are not differentiable, the gradient-based coordinate updates fail.

### Mechanism 2
- **Claim:** Prediction errors in DCPC correspond to score functions of the complete conditional density.
- **Mechanism:** Each prediction error is defined as the gradient of the log complete conditional, combining local prior and child likelihood terms. This gradient drives Langevin proposals toward the true conditional distribution.
- **Core assumption:** The complete conditional density is differentiable and locally accessible.
- **Evidence anchors:**
  - [section] "We therefore define εz in DCPC as the complete conditional’s score function εz := ∇z log γθ(z; z\z) = ∇z log pθ(z | Pa(z)) + X v∈Ch(z) ∇z log pθ(v | Pa(v))."
  - [abstract] "...all without sacrificing biological plausibility."
  - [corpus] No direct evidence; this is a novel contribution not reflected in corpus neighbors.
- **Break condition:** If the gradient of the complete conditional is intractable or noisy, the proposal distribution will be biased.

### Mechanism 3
- **Claim:** DCPC parameter updates require only local computations, making them biologically plausible.
- **Mechanism:** Theorem 2 shows that in a factorized generative model, the gradient of the free energy decomposes into a sum of local particle averages over individual variables and their parents, eliminating the need for global backpropagation.
- **Core assumption:** Model parameters factorize disjointly with respect to variables (θ = Q x∈x Θx × Q z∈z Θz).
- **Evidence anchors:**
  - [section] "Theorem 2 will demonstrate that given the 'factorization' above, DCPC’s model learning requires only local prediction errors."
  - [abstract] "...provably performs maximum-likelihood updates of model parameters, all without sacrificing biological plausibility."
  - [corpus] Weak; corpus neighbors discuss variational inference but not local parameter updates in factorized models.
- **Break condition:** If the parameter factorization assumption fails, the gradient decomposition no longer holds.

## Foundational Learning

- **Concept:** Variational inference
  - Why needed here: DCPC optimizes a variational free energy to approximate intractable posteriors in structured generative models.
  - Quick check question: What is the relationship between the variational free energy and the log model evidence?

- **Concept:** Gibbs sampling
  - Why needed here: DCPC approximates Gibbs sampling by updating one latent variable at a time conditioned on the rest, using gradient-informed proposals.
  - Quick check question: How does DCPC's coordinate update differ from a standard Gibbs step?

- **Concept:** Score functions and Langevin dynamics
  - Why needed here: Prediction errors are defined as score functions, and Langevin proposals use these gradients to explore the posterior.
  - Quick check question: Why is the score function of a Gaussian equivalent to the precision-weighted prediction error?

## Architecture Onboarding

- **Component map:**
  - Latents (z) -> Prediction errors (εz) -> Proposal distribution -> Importance weights -> Free energy -> Parameter θ

- **Critical path:**
  1. Initialize particles by ancestor sampling.
  2. For each latent variable:
     - Compute local and child prediction errors.
     - Propose new sample via Langevin update.
     - Compute importance weight and resample.
  3. Update model parameters using local gradient estimates.
  4. Repeat until convergence.

- **Design tradeoffs:**
  - *Particle count K vs. computational cost*: More particles improve posterior approximation but increase runtime.
  - *Learning rate η vs. stability*: Larger η speeds convergence but risks instability in high dimensions.
  - *Coordinate sweep order vs. mixing*: Fixed order may slow mixing; random order can help but adds overhead.

- **Failure signatures:**
  - *High variance in free energy estimates*: Likely due to insufficient particles or poor learning rate.
  - *Posterior collapse*: May indicate over-regularization or vanishing gradients in conditionals.
  - *Slow mixing*: Could result from poor coordinate ordering or high correlations not captured in updates.

- **First 3 experiments:**
  1. Run DCPC on a simple two-latent Gaussian model and compare posterior samples to analytic solution.
  2. Train a deep latent Gaussian model on MNIST and evaluate reconstruction error vs. MCPC.
  3. Test DCPC on a structured model (e.g., hierarchical mixture) where classical PC fails due to correlations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DCPC's biological plausibility be empirically validated through neuroimaging or electrophysiology studies?
- Basis in paper: [explicit] The paper discusses how DCPC aligns with the canonical cortical microcircuit hypothesis, suggesting specific neuronal oscillations (gamma, beta, theta) could implement different aspects of the algorithm.
- Why unresolved: The paper provides a theoretical framework for how DCPC could be implemented in the brain, but does not present empirical evidence from neuroscience experiments.
- What evidence would resolve it: Neuroimaging studies showing prediction error signals in specific cortical layers during tasks that would elicit such errors, or electrophysiology data demonstrating the proposed oscillatory patterns during inference.

### Open Question 2
- Question: How does DCPC scale to very high-dimensional latent spaces compared to other inference algorithms?
- Basis in paper: [inferred] While the paper demonstrates DCPC's performance on various datasets, it does not explicitly address scalability to extremely high-dimensional problems or compare scaling behavior with other algorithms.
- Why unresolved: The experiments shown are on moderate-sized problems, and the computational complexity of DCPC for very large models is not discussed.
- What evidence would resolve it: Systematic scaling experiments showing runtime and accuracy as latent space dimensionality increases, compared to other inference methods like Hamiltonian Monte Carlo or variational inference with normalizing flows.

### Open Question 3
- Question: What is the optimal balance between local prediction errors and global structure in the proposal distribution for DCPC?
- Basis in paper: [explicit] The paper emphasizes that DCPC uses local prediction errors for coordinate updates, but also employs a global resampling step to target the joint distribution.
- Why unresolved: The paper does not provide a theoretical analysis or empirical study of how the balance between local and global information affects inference quality or computational efficiency.
- What evidence would resolve it: Experiments varying the frequency and scope of global updates (e.g., different resampling strategies or adaptive proposals) and measuring their impact on convergence speed and final inference quality.

## Limitations
- The biological plausibility claim remains speculative without empirical validation from neuroscience experiments.
- The algorithm's performance advantages are demonstrated on moderate-sized problems but scalability to very high-dimensional latent spaces is untested.
- The experiments do not establish DCPC's advantages in complex, real-world scenarios with tight computational constraints.

## Confidence
- **High confidence**: The theoretical derivations of DCPC's update rules and the relationship between prediction errors and score functions are mathematically rigorous and well-supported by the text.
- **Medium confidence**: The experimental results showing improved performance over MCPC are convincing within the tested domains, but the generalization to more complex models is untested.
- **Low confidence**: The claim that DCPC is "biologically plausible" is primarily supported by the local computation argument, which is plausible but not directly verified against neural data.

## Next Checks
1. Test DCPC on a structured model with known posterior (e.g., Gaussian mixture) to verify that the algorithm recovers the true distribution more accurately than MCPC.
2. Evaluate DCPC's computational efficiency and scalability on larger, more complex models (e.g., deep hierarchical models with many layers) to assess practical limitations.
3. Compare DCPC's performance against other structured inference methods (e.g., structured variational inference) on a common benchmark to contextualize its advantages and disadvantages.