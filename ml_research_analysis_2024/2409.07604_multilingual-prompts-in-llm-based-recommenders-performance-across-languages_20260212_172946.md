---
ver: rpa2
title: 'Multilingual Prompts in LLM-Based Recommenders: Performance Across Languages'
arxiv_id: '2409.07604'
source_url: https://arxiv.org/abs/2409.07604
tags:
- prompts
- language
- english
- recommendation
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how non-English prompts affect LLM-based
  recommender performance, focusing on Spanish and Turkish. It uses OpenP5 to test
  pre-trained T5 models on ML1M, LastFM, and Amazon-Beauty datasets, comparing English,
  Spanish, and Turkish prompts under random and sequential indexing.
---

# Multilingual Prompts in LLM-Based Recommenders: Performance Across Languages

## Quick Facts
- arXiv ID: 2409.07604
- Source URL: https://arxiv.org/abs/2409.07604
- Reference count: 40
- Key outcome: Non-English prompts (Spanish, Turkish) reduce LLM-based recommender performance, especially for Turkish; multilingual fine-tuning improves cross-language balance but slightly reduces English performance.

## Executive Summary
This work investigates how non-English prompts affect LLM-based recommender performance, focusing on Spanish and Turkish. It uses OpenP5 to test pre-trained T5 models on ML1M, LastFM, and Amazon-Beauty datasets, comparing English, Spanish, and Turkish prompts under random and sequential indexing. Results show that non-English prompts generally reduce performance, especially for Turkish, which is linguistically distant from English. Retraining the model with multilingual prompts led to more balanced performance across languages, but slightly reduced English performance. This highlights the need for diverse language support in LLM-based recommenders and suggests future research on evaluation datasets, newer models, and additional languages.

## Method Summary
The study uses the OpenP5 platform with a pre-trained T5 model to evaluate LLM-based recommenders on ML1M, LastFM, and Amazon-Beauty datasets. English prompt templates are translated to Spanish and Turkish using GPT-3.5. The model is tested with both random and sequential indexing methods. Performance is measured using HitRate@5, HitRate@10, NDCG@5, and NDCG@10. The pre-trained model is then retrained with multilingual prompts to assess performance changes across languages.

## Key Results
- Non-English prompts (Spanish, Turkish) generally reduce recommender performance compared to English prompts.
- Turkish prompts perform worse than Spanish due to linguistic distance from English.
- Retraining with multilingual prompts improves cross-language balance but slightly reduces English performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning an LLM with multilingual prompts improves performance across languages, though English performance may slightly decline.
- Mechanism: By retraining the model on a diverse set of prompts in English, Spanish, and Turkish, the model learns to generalize better across languages, reducing the performance gap between high-resource and low-resource languages.
- Core assumption: The model can effectively learn cross-lingual representations when exposed to multilingual training data.
- Evidence anchors:
  - [abstract]: "Retraining with multilingual prompts resulted in more balanced performance across languages, but slightly reduced English performance."
  - [section]: "However, with the retrained model, the performance across all languages becomes closer."
- Break Condition: If the model overfits to English during retraining, the performance of non-English languages may not improve as expected.

### Mechanism 2
- Claim: Random indexing introduces artificial relationships between items due to tokenizer behavior, leading to inconsistent performance.
- Mechanism: Random indexing assigns item IDs randomly, which can cause items with similar numerical IDs to be tokenized into similar sub-chunks, creating false associations in the model's understanding.
- Core assumption: The tokenizer's handling of numerical IDs significantly impacts the model's ability to distinguish between unrelated items.
- Evidence anchors:
  - [section]: "As discussed in OpenP5 [19, 14], random indexing assigns item ids randomly and due to the way tokenizers used by LLMs, these ids might be split into sub-chunks."
- Break Condition: If sequential or collaborative indexing is used instead, the performance should become more stable and consistent across languages.

### Mechanism 3
- Claim: Linguistic similarity between the prompt language and English affects LLM performance, with more distant languages (e.g., Turkish) experiencing greater performance drops.
- Mechanism: The model's pre-training on English data leads to better performance on languages that share linguistic features with English, while languages with different structures (e.g., agglutinative languages like Turkish) are harder to process.
- Core assumption: The model's ability to transfer knowledge from English to other languages is influenced by linguistic proximity.
- Evidence anchors:
  - [abstract]: "Turkish, being less resourced, was anticipated to perform lower."
  - [section]: "Spanish and English belong to the Indo-European language family, whereas Turkish is part of the Altaic language family."
- Break Condition: If the model is specifically fine-tuned on a diverse set of languages, the impact of linguistic distance may be reduced.

## Foundational Learning

- Concept: Multilingual LLMs and their limitations
  - Why needed here: Understanding how LLMs handle different languages is crucial for evaluating their performance in multilingual recommender systems.
  - Quick check question: Why do multilingual LLMs tend to perform better on high-resource languages like English compared to low-resource languages?
- Concept: Fine-tuning vs. in-context learning
  - Why needed here: The study compares the effects of retraining the model with multilingual prompts versus using non-English prompts on a pre-trained model.
  - Quick check question: What is the difference between fine-tuning a model and using in-context learning for multilingual tasks?
- Concept: Indexing methods in recommender systems
  - Why needed here: The study evaluates the impact of random and sequential indexing on recommendation performance, highlighting the importance of choosing the right indexing method.
  - Quick check question: How does random indexing differ from sequential indexing in terms of item representation?

## Architecture Onboarding

- Component map:
  - OpenP5 platform -> T5 model -> Indexing methods (random/sequential) -> Datasets (ML1M, LastFM, Amazon-Beauty) -> Performance metrics (HitRate@N, NDCG@N)
- Critical path:
  1. Expand OpenP5's English prompt templates to include Spanish and Turkish.
  2. Evaluate the performance of the pre-trained T5 model using non-English prompts.
  3. Retrain the T5 model with multilingual prompts and compare performance across languages.
- Design tradeoffs:
  - Random indexing is simpler but can introduce artificial relationships between items, leading to inconsistent performance.
  - Sequential indexing is more stable but may not capture all item relationships as effectively as collaborative indexing.
- Failure signatures:
  - Significant performance drops for non-English prompts indicate the model's inability to generalize across languages.
  - Inconsistent performance with random indexing suggests issues with tokenizer behavior and item representation.
- First 3 experiments:
  1. Evaluate the pre-trained T5 model using English, Spanish, and Turkish prompts on the ML1M dataset with random indexing.
  2. Repeat the evaluation using sequential indexing to assess the impact of indexing method on performance.
  3. Retrain the T5 model with multilingual prompts and compare its performance to the pre-trained model across all languages and indexing methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-based recommenders vary across different non-English languages with varying linguistic distances from English (e.g., Romance vs. Turkic languages)?
- Basis in paper: [explicit] The paper compares Spanish and Turkish prompts, noting that Turkish (a Turkic language) shows significantly worse performance than Spanish (a Romance language), suggesting linguistic distance impacts performance.
- Why unresolved: The study only tests two non-English languages; a broader comparison across multiple language families would be needed to generalize findings.
- What evidence would resolve it: Testing LLM-based recommenders with prompts in multiple non-English languages spanning different linguistic families (e.g., Romance, Germanic, Slavic, Turkic, Sino-Tibetan) and comparing performance metrics would clarify the impact of linguistic distance.

### Open Question 2
- Question: What is the optimal balance between multilingual prompt training and maintaining high performance in English for LLM-based recommenders?
- Basis in paper: [explicit] The paper finds that retraining the model with multilingual prompts leads to more balanced performance across languages but slightly reduces English performance, highlighting a trade-off.
- Why unresolved: The study does not explore the extent to which multilingual training can be optimized to minimize performance loss in English while maximizing gains in other languages.
- What evidence would resolve it: Systematic experimentation with varying proportions of English and non-English prompts during training, along with performance evaluations, would identify the optimal balance.

### Open Question 3
- Question: How do newer multilingual LLM models (e.g., Llama 3, InternLM) perform compared to T5 in LLM-based recommenders when using non-English prompts?
- Basis in paper: [inferred] The paper suggests future research should consider newer multilingual models, implying their performance with non-English prompts is unexplored.
- Why unresolved: The study uses the T5 model; newer models with improved multilingual capabilities have not been evaluated in this context.
- What evidence would resolve it: Benchmarking LLM-based recommenders using newer multilingual models (e.g., Llama 3, InternLM) with non-English prompts and comparing their performance to T5 would provide insights into their effectiveness.

## Limitations
- The study only tests two non-English languages (Spanish, Turkish), limiting generalizability to other language families.
- GPT-3.5 was used for translation, introducing potential variability in prompt quality despite manual verification.
- The exact prompt templates and hyperparameter settings for retraining are not provided, affecting reproducibility.

## Confidence

- High confidence: The finding that random indexing introduces performance instability is well-supported by the evidence and mechanism described.
- Medium confidence: The claim that multilingual fine-tuning leads to more balanced performance is supported by the results but could benefit from additional hyperparameter details and cross-validation.
- Medium confidence: The observation that linguistic distance affects performance is plausible given the results, but the small language sample size limits definitive conclusions.

## Next Checks

1. Test additional languages from different language families (e.g., Asian languages, Slavic languages) to validate the linguistic distance hypothesis and assess generalizability.
2. Conduct ablation studies comparing different indexing methods (sequential, collaborative) to isolate the impact of random indexing on performance variability.
3. Implement manual translation verification for prompt templates to ensure semantic equivalence across languages, controlling for translation quality as a confounding factor.