---
ver: rpa2
title: 'OS-Copilot: Towards Generalist Computer Agents with Self-Improvement'
arxiv_id: '2402.07456'
source_url: https://arxiv.org/abs/2402.07456
tags:
- task
- friday
- code
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OS-Copilot is a framework for building generalist computer agents
  that can interface with the entire operating system. It provides a universal interface
  for app interactions in the heterogeneous OS ecosystem.
---

# OS-Copilot: Towards Generalist Computer Agents with Self-Improvement

## Quick Facts
- arXiv ID: 2402.07456
- Source URL: https://arxiv.org/abs/2402.07456
- Reference count: 40
- FRIDAY achieves 35% improvement on GAIA benchmark and 60% success rate on SheetCopilot dataset

## Executive Summary
OS-Copilot introduces a framework for building generalist computer agents that can interface with entire operating systems through a unified control layer. The framework provides a universal interface that consolidates multiple control methods (Python, bash, mouse/keyboard, API calls) to interact with diverse OS elements. FRIDAY, built on OS-Copilot, demonstrates self-improving capabilities by learning to control unfamiliar applications through task-based curriculum generation and a critic-executor-refiner loop for continuous improvement.

## Method Summary
OS-Copilot provides a unified interface for controlling diverse OS applications through four execution backends: Python code interpreter, bash terminal, mouse/keyboard control, and API calls. The framework uses a DAG-based planner to decompose tasks into subtasks, a configurator to manage working memory and generate tools, and an actor with critic-executor-refiner loop for execution and improvement. FRIDAY, built on this framework, employs self-directed learning where it generates progressively challenging tasks for unfamiliar applications, attempts solutions through trial-and-error, and refines its approach based on feedback.

## Key Results
- FRIDAY outperforms previous methods by 35% on the GAIA benchmark
- FRIDAY achieves 60% success rate on SheetCopilot dataset, surpassing state-of-the-art models designed specifically for spreadsheet control
- Demonstrates successful PowerPoint slide creation through self-directed learning

## Why This Works (Mechanism)

### Mechanism 1
OS-Copilot achieves broad OS compatibility through a unified interface that consolidates multiple control methods (Python code interpreter, bash terminal, mouse/keyboard control, API calls). By providing a single abstraction layer that maps high-level commands to multiple execution backends, the framework can interact with diverse OS elements without requiring separate implementations for each application.

### Mechanism 2
FRIDAY's self-directed learning capability enables it to acquire control over unfamiliar applications through task-based curriculum generation. The configurator proposes a stream of progressively challenging tasks related to an unfamiliar application, FRIDAY attempts to solve them, accumulates successful tools, and refines its approach through self-correction cycles.

### Mechanism 3
The critic-executor-refiner loop enables continuous improvement by evaluating execution outcomes and refining both actions and tools. After each execution, the critic assesses completion, provides error analysis and suggestions, and the refiner updates tools or subtasks based on this feedback. This creates a closed-loop improvement system.

## Foundational Learning

- Concept: Directed Acyclic Graph (DAG) planning
  - Why needed here: Enables parallel execution of independent subtasks, improving efficiency compared to sequential execution
  - Quick check question: How does a DAG planner differ from a linear planner in terms of task dependencies and execution order?

- Concept: Tool generation and refinement
  - Why needed here: Allows the agent to create new capabilities on-the-fly when encountering unfamiliar applications, rather than being limited to pre-defined tools
  - Quick check question: What information must be extracted from a task description to generate a generic, reusable tool?

- Concept: Self-directed learning curriculum
  - Why needed here: Provides a systematic approach for mastering new application domains without human supervision
  - Quick check question: What characteristics should tasks in a learning curriculum have to ensure progressive skill development?

## Architecture Onboarding

- Component map: User request → Planner → Configurator (retrieves/generates tools) → Executor → Critic → Refiner → repeat until completion
- Critical path: User request → Planner → Configurator (retrieves/generates tools) → Executor → Critic → Refiner → repeat until completion
- Design tradeoffs:
  - Generality vs performance: Supporting many applications through generic interfaces may be less efficient than specialized implementations
  - Autonomy vs control: Self-directed learning enables scalability but may produce unpredictable tool behaviors
  - Complexity vs maintainability: The multi-component architecture provides flexibility but increases system complexity
- Failure signatures:
  - Agent gets stuck in loops: Likely issues with the critic's evaluation or the refiner's ability to generate effective modifications
  - Tools fail to generalize: Indicates the tool generator isn't properly abstracting task requirements into generic parameters
  - Performance degradation over time: May indicate working memory isn't properly managing long-term memory updates
- First 3 experiments:
  1. Test basic tool execution: Verify the four core control methods (Python, bash, API, mouse/keyboard) work independently
  2. Test tool generation: Give the agent a simple novel task and verify it can generate a working tool
  3. Test self-correction: Provide a deliberately flawed tool and verify the refiner can fix it based on critic feedback

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of FRIDAY compare when using fine-tuning versus prompting for tool generation and task execution? The paper mentions that prompting may lead to performance changes when underlying LLMs change and suggests future research in reinforcement learning and fine-tuning. What evidence would resolve it: Experiments comparing FRIDAY's performance using prompting versus fine-tuning on the same set of tasks, measuring success rates and execution times.

### Open Question 2
What are the specific challenges in evaluating general computer agents like FRIDAY, and how can these challenges be addressed? The paper discusses the challenges in evaluating general computer agents, including the lack of ground truth for subtask completion and the need for comparing system states. What evidence would resolve it: Development and testing of new evaluation metrics or methodologies that can accurately assess the performance of general computer agents in open environments like GAIA.

### Open Question 3
How can OS-Copilot be extended to support closed-source applications and multimodal inputs for enhanced computer control? The paper mentions that controlling computers solely through code and language is infeasible due to closed-source software and suggests expanding OS-Copilot to support visual input and screenshot-to-action generation. What evidence would resolve it: Implementation and testing of OS-Copilot with support for closed-source applications and multimodal inputs, demonstrating improved performance on tasks that require these capabilities.

## Limitations

- Evaluation focuses primarily on benchmark tasks that may not fully represent real-world complexity
- The claim of "universal" OS compatibility is limited by the requirement that at least one of four supported control methods must work for each application
- Self-directed learning approach may produce unpredictable tool behaviors and requires careful monitoring

## Confidence

- **High confidence**: The basic architecture of OS-Copilot (DAG-based planner, configurator, actor with critic-executor-refiner loop) is well-defined and implemented
- **Medium confidence**: The 35% improvement on GAIA and 60% success rate on SheetCopilot are reproducible, though benchmark limitations exist
- **Medium confidence**: The self-directed learning approach enables control of unfamiliar applications, though the curriculum generation mechanism may have limitations

## Next Checks

1. **Generalization test**: Apply FRIDAY to completely novel applications outside the GAIA and SheetCopilot domains to assess true OS generality
2. **Scalability evaluation**: Measure performance degradation as the number of supported applications and tools grows, particularly focusing on working memory management
3. **Critic reliability assessment**: Systematically evaluate the accuracy of the critic's evaluations and the refiner's ability to address identified issues across diverse task types