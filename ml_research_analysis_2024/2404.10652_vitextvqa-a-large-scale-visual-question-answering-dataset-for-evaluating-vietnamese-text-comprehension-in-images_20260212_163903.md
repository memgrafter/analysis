---
ver: rpa2
title: 'ViTextVQA: A Large-Scale Visual Question Answering Dataset for Evaluating
  Vietnamese Text Comprehension in Images'
arxiv_id: '2404.10652'
source_url: https://arxiv.org/abs/2404.10652
tags:
- text
- image
- dataset
- question
- vietnamese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ViTextVQA, the first large-scale dataset
  for Vietnamese text-based visual question answering (VQA), containing over 16,000
  images and 50,342 question-answer pairs. The dataset focuses on scene text comprehension
  and includes detailed analysis of question/answer lengths, object frequency, and
  Vietnamese language characteristics.
---

# ViTextVQA: A Large-Scale Visual Question Answering Dataset for Evaluating Vietnamese Text Comprehension in Images

## Quick Facts
- arXiv ID: 2404.10652
- Source URL: https://arxiv.org/abs/2404.10652
- Reference count: 40
- ViTextBLIP-2 achieves state-of-the-art 53.95 F1-score and 25.48 EM on Vietnamese text-based VQA

## Executive Summary
This paper introduces ViTextVQA, the first large-scale dataset for Vietnamese text-based visual question answering containing over 16,000 images and 50,342 question-answer pairs. The dataset addresses scene text comprehension in Vietnamese, including detailed analysis of question/answer lengths, object frequency, and Vietnamese language characteristics. To efficiently tackle this task, the authors propose ViTextBLIP-2, which leverages frozen pre-trained vision and language models with a trainable Q-Former for multimodal feature fusion. The approach achieves state-of-the-art results, outperforming previous methods by over 9 points in F1-score.

## Method Summary
The authors propose ViTextBLIP-2, an efficient Vietnamese text-based VQA method that uses frozen pre-trained vision and language models with a trainable Q-Former module for multimodal feature fusion. The model employs a Vision Transformer for image encoding, SwinTextSpotter for OCR text extraction, and ViT5 as the language model, with only the Q-Former component being trained during optimization. The method is trained using Adam optimizer with learning rate 3.0e-5, dropout 0.2, batch size 16, and weight decay 1.0e-4 for 10 epochs. The approach specifically addresses Vietnamese text characteristics through careful preprocessing and arrangement of OCR tokens from top-left to bottom-right.

## Key Results
- ViTextBLIP-2 achieves state-of-the-art F1-score of 53.95 and EM of 25.48 on ViTextVQA
- Arranging OCR text from top-left to bottom-right improves performance across baseline models
- Removing Vietnamese diacritics significantly degrades model performance, highlighting their importance for comprehension
- GPT-4o performance (F1=48.22, EM=23.19) still lags behind human performance (F1=72.31, EM=48.31)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Arranging OCR text from top-left to bottom-right improves VQA performance on Vietnamese text.
- Mechanism: Vietnamese text follows a left-to-right, top-to-bottom reading order, so models benefit from OCR tokens in that spatial sequence.
- Core assumption: The model can leverage the spatial context of text when it is ordered in natural reading sequence.
- Evidence anchors:
  - [section] "This sequential arrangement seems to enhance the semantic context of OCR text, helping the models can easily understand and process information from text in the most natural way."
  - [section] "Arranging OCR text from top-left to bottom-right appears to be a beneficial approach for VQA models when dealing with Vietnamese text."
  - [corpus] Weak anchor; corpus does not discuss OCR ordering.
- Break condition: If Vietnamese text layout deviates significantly from left-to-right, top-to-bottom (e.g., right-to-left script or vertical layout), the benefit may vanish.

### Mechanism 2
- Claim: The Q-Former module in ViTextBLIP-2 enables efficient multimodal feature fusion by only training that component while freezing others.
- Mechanism: Q-Former learns to align frozen image and OCR embeddings with language embeddings, reducing training cost while maintaining strong performance.
- Core assumption: Frozen pre-trained vision and language models retain sufficient representational power for Vietnamese VQA when combined via a trained Q-Former.
- Evidence anchors:
  - [section] "To tackle this task efficiently, we propose ViTextBLIP-2, an novel multimodal feature fusion Method, which optimizes Vietnamese OCR-based VQA by integrating a frozen Vision Transformer, SwinTextSpotter OCR, and ViT5 LLM with a trainable Q-Former for multimodal feature fusion."
  - [section] "During training, only the Q-Former is optimized, while other components remain frozen to reduce computational costs."
  - [corpus] No corpus evidence on Q-Former specifically.
- Break condition: If pre-trained models are insufficiently tuned for Vietnamese linguistic and visual features, freezing them could limit accuracy gains.

### Mechanism 3
- Claim: Removing Vietnamese diacritics significantly degrades model performance.
- Mechanism: Diacritics encode tonal and phonetic distinctions crucial for Vietnamese word identity; their removal increases ambiguity and impairs comprehension.
- Core assumption: The VQA model relies on diacritic cues for distinguishing homographs and understanding question semantics.
- Evidence anchors:
  - [section] "The results of the experiments shown in Figure 21 show that removing diacritics significantly affects the ability of models to understand and process language."
  - [section] "In general, diacritics not only provide information about context and intonation but also help VQA models understand and interact with the language accurately and effectively."
  - [corpus] No corpus evidence on diacritic effects.
- Break condition: If the dataset contains few diacritic-sensitive word pairs or if the model learns robust contextual disambiguation, the impact might be reduced.

## Foundational Learning

- Concept: Multimodal transformer architectures (ViT + LLM)
  - Why needed here: ViTextBLIP-2 fuses vision and language via a transformer-based Q-Former; understanding this is essential to grasp the proposed method.
  - Quick check question: How does the Q-Former bridge vision and language embeddings without retraining the entire model?

- Concept: OCR token ordering and spatial context
  - Why needed here: The top-left to bottom-right sorting improvement relies on spatial context cues; engineers must understand why this matters for Vietnamese text.
  - Quick check question: What would happen to performance if OCR tokens were sorted randomly instead of spatially?

- Concept: Vietnamese language characteristics (diacritics, word segmentation)
  - Why needed here: The dataset and experiments explicitly analyze how these features affect VQA accuracy; engineers need to account for them in preprocessing.
  - Quick check question: How does removing diacritics change the number of unique tokens in a Vietnamese sentence?

## Architecture Onboarding

- Component map:
  - Vision Encoder (frozen ViT) → Image embeddings
  - OCR System (frozen SwinTextSpotter) → OCR tokens & embeddings
  - Captioning Model (frozen Qwen2-VL) → Automatic captions
  - Q-Former (trainable) → Multimodal fusion of image, OCR, caption features
  - Large Language Model (frozen ViT5) + Q-Former output → Answer generation

- Critical path: Image → Vision Encoder → Image embeddings → Q-Former; OCR tokens → OCR System → OCR embeddings → Q-Former; Q-Former → fused multimodal features → LLM → answer.

- Design tradeoffs:
  - Freezing pre-trained models reduces compute but may limit adaptation to Vietnamese specifics.
  - Top-left to bottom-right sorting simplifies OCR processing but may not reflect all layouts.
  - Relying on diacritics increases accuracy but reduces robustness to noisy OCR.

- Failure signatures:
  - Low F1/EM scores when diacritics are removed or OCR is unsorted.
  - Performance drop when Q-Former fails to align modalities properly.
  - Errors correlating with small OCR text area or high token overlap in OCR.

- First 3 experiments:
  1. Train baseline with unsorted OCR, measure F1/EM, then retrain with top-left to bottom-right sorting and compare.
  2. Remove diacritics at 0%, 50%, 100% rates; observe impact on F1/EM to quantify diacritic importance.
  3. Freeze/unfreeze Q-Former while keeping other components frozen; measure change in performance and training time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Vietnamese diacritics removal rate affect the performance of Vietnamese text-based VQA models?
- Basis in paper: [explicit] The paper conducted experiments on the ViTextVQA dataset with diacritics removal levels of 0%, 25%, 50%, 75%, and 100%, and analyzed the impact on model performance.
- Why unresolved: The paper provides initial results showing performance degradation with increasing diacritic removal, but does not establish a precise threshold or quantify the specific contribution of diacritics to model accuracy.
- What evidence would resolve it: A systematic study varying diacritic removal rates while measuring F1-score and EM metrics across multiple model architectures would establish the relationship between diacritic preservation and VQA performance.

### Open Question 2
- Question: What is the optimal OCR text arrangement strategy for Vietnamese text-based VQA models?
- Basis in paper: [explicit] The paper experimented with three OCR arrangement methods (no sorting, confidence score sorting, and top-left to bottom-right sorting) and found that top-left to bottom-right sorting improved performance for most models.
- Why unresolved: While the paper shows top-left to bottom-right sorting is generally better, it does not explore alternative Vietnamese-specific sorting strategies or determine if there are cases where other arrangements might be superior.
- What evidence would resolve it: Comparative experiments testing additional Vietnamese text arrangement strategies (such as phonetic ordering or semantic clustering) across diverse Vietnamese text scenarios would identify optimal approaches for different VQA contexts.

### Open Question 3
- Question: How do Vietnamese-specific linguistic features impact the generalization of text-based VQA models to other languages?
- Basis in paper: [inferred] The paper highlights unique Vietnamese characteristics including diacritics, word segmentation challenges, and text reading patterns, but does not investigate cross-linguistic transferability.
- Why unresolved: The paper focuses exclusively on Vietnamese but does not explore whether models trained on Vietnamese text-based VQA can transfer to other languages with different orthographic and linguistic properties.
- What evidence would resolve it: Cross-lingual experiments transferring models trained on ViTextVQA to other Southeast Asian languages with different script systems (Thai, Khmer) while measuring performance degradation would quantify the impact of Vietnamese-specific features on model generalization.

## Limitations

- The evaluation focuses exclusively on Vietnamese, making it unclear whether results generalize to other languages with different reading orders
- Reliance on frozen pre-trained models limits adaptation to domain-specific visual features unique to Vietnamese text
- The dataset size (50,342 QA pairs) is substantial for Vietnamese but smaller than many multilingual VQA datasets

## Confidence

**High Confidence (Likelihood >80%)**
- ViTextBLIP-2 achieves state-of-the-art F1-score of 53.95 and EM of 25.48 on ViTextVQA
- Arranging OCR text from top-left to bottom-right improves VQA performance
- Removing Vietnamese diacritics significantly degrades model performance
- GPT-4o's performance (F1=48.22, EM=23.19) still lags behind human performance (F1=72.31, EM=48.31)

**Medium Confidence (Likelihood 60-80%)**
- The Q-Former module enables efficient multimodal feature fusion by only training that component while freezing others
- Vietnamese language characteristics (diacritics, word segmentation) are crucial for accurate VQA performance
- The proposed method substantially advances Vietnamese text-based VQA capabilities

**Low Confidence (Likelihood <60%)**
- The specific architectural choices in ViTextBLIP-2 would generalize to other languages or domains
- The computational efficiency gains from freezing components outweigh potential accuracy losses

## Next Checks

1. **Cross-language generalization test**: Evaluate ViTextBLIP-2 on a multilingual text-based VQA dataset (e.g., TextVQA or ST-VQA) to determine if top-left to bottom-right OCR sorting benefits extend beyond Vietnamese.

2. **Ablation study on model components**: Systematically freeze/unfreeze each component (vision encoder, OCR system, Q-Former, LLM) while measuring performance and computational cost to quantify the contribution of each element.

3. **Diacritic sensitivity analysis**: Create controlled experiments with varying levels of diacritic removal (0%, 25%, 50%, 75%, 100%) and analyze which types of questions (counting, counting, color, etc.) are most affected to understand the mechanism of diacritic importance.