---
ver: rpa2
title: 'Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends'
arxiv_id: '2407.21489'
source_url: https://arxiv.org/abs/2407.21489
tags:
- mention
- coreference
- computational
- maverick
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Maverick challenges the trend of using large autoregressive generative
  models for Coreference Resolution by introducing an efficient encoder-only pipeline
  that achieves state-of-the-art performance with significantly fewer parameters.
  The framework employs a novel mention extraction method, end-of-sentence regularization,
  and mention pruning strategy to reduce computational overhead while maintaining
  high accuracy.
---

# Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends

## Quick Facts
- arXiv ID: 2407.21489
- Source URL: https://arxiv.org/abs/2407.21489
- Reference count: 26
- Primary result: Achieves SOTA CoNLL-F1 of 83.6 using only 500M parameters, 170x faster and 0.006x memory of models with 13B parameters

## Executive Summary
Maverick introduces an encoder-only pipeline for coreference resolution that challenges the trend of using large autoregressive generative models. The framework achieves state-of-the-art performance with significantly fewer parameters by employing a novel mention extraction method, end-of-sentence regularization, and mention pruning strategy. Three model variants leverage different clustering techniques, with Maverickincr showing superior performance in long-document and out-of-domain settings. Trained on OntoNotes, Maverick outperforms models with up to 13 billion parameters while using only 500 million parameters, demonstrating robustness across data-scarce, long-document, and out-of-domain scenarios.

## Method Summary
Maverick uses a DeBERTa-v3 encoder with a novel start-end mention extraction method that classifies each token as a potential mention start, then scores subsequent tokens as possible ends. This approach reduces candidate mentions by ~8x compared to traditional Coarse-to-Fine enumeration. End-of-sentence regularization restricts end prediction to tokens up to the nearest EOS, eliminating cross-sentence mentions. Three clustering variants are implemented: Mavericks2e (mention-antecedent classification), Maverickmes (linguistically motivated categories), and Maverickincr (incremental transformer clustering). The framework uses binary cross-entropy losses for start, end, and clustering predictions, trained jointly with teacher forcing.

## Key Results
- Achieves 83.6 CoNLL-F1 on OntoNotes, surpassing models with 13B parameters
- Uses only 500M parameters, 0.006x the memory and 170x faster than larger models
- Maverickincr shows +3.9 CoNLL-F1 improvement over previous incremental methods
- Demonstrates robustness across data-scarce, long-document, and out-of-domain settings

## Why This Works (Mechanism)

### Mechanism 1
The Maverick Pipeline's start-end mention extraction reduces candidate mentions by ~8x compared to traditional Coarse-to-Fine enumeration. Instead of scoring all possible spans (quadratic in length), Maverick first classifies each token as a possible mention start, then for each start token, scores only subsequent tokens as possible ends. This filtering via start probability drastically reduces candidate mentions before the expensive end scoring step.

### Mechanism 2
End-of-sentence (EOS) regularization eliminates long-mention bias without losing accuracy. Since annotated mentions never span across sentence boundaries, Maverick restricts end prediction to tokens up to the nearest EOS after each start. This avoids computing scores for spans that cannot be mentions.

### Mechanism 3
Maverick's incremental transformer architecture improves out-of-domain and long-document generalization over previous incremental methods. Instead of using a linear classifier over fixed-length cluster vectors, Maverick uses a lightweight transformer to attend to all previously built cluster mentions, allowing richer context and more flexible cluster representations.

## Foundational Learning

- Concept: Span scoring vs start-end mention extraction
  - Why needed here: Understanding the computational complexity difference (quadratic vs linear-then-quadratic) is key to seeing why Maverick is faster
  - Quick check question: In a 100-token document, how many span pairs are considered in traditional Coarse-to-Fine vs Maverick's start-end method?

- Concept: Teacher forcing in multitask training
  - Why needed here: Maverick trains start, end, and clustering losses jointly; knowing how teacher forcing works explains the stability and efficiency
  - Quick check question: What happens if Maverick used gold starts during training but predicted starts during inference for end scoring?

- Concept: Incremental vs global clustering trade-offs
  - Why needed here: Maverickincr's incremental transformer clustering is slower but more robust; understanding this trade-off helps choose the right model variant
  - Quick check question: Why might incremental clustering outperform global clustering on long documents?

## Architecture Onboarding

- Component map: DeBERTa-v3 encoder → start classifier → end classifier (conditioned on start) → mention pruning → clustering module (2e, mes, or incr)
- Critical path: Mention extraction (start → end) → mention pruning → clustering (mention-antecedent or incremental)
- Design tradeoffs:
  - Start-end extraction: Faster but may miss complex overlapping mentions
  - EOS regularization: Eliminates cross-sentence mentions; safe for current datasets but brittle if guidelines change
  - Incremental clustering: More robust out-of-domain but slower due to transformer steps
- Failure signatures:
  - Low recall: Start classifier too strict; consider lowering start threshold
  - High memory: EOS regularization off or too many mentions pass pruning
  - Slow inference: Using Maverickincr on short docs where global clustering would suffice
- First 3 experiments:
  1. Run Maverick with default start threshold (0.5) on OntoNotes dev; measure mention count and CoNLL-F1
  2. Toggle EOS regularization off; compare mention count and speed
  3. Switch clustering from Mavericks2e to Maverickincr; measure speed vs accuracy trade-off

## Open Questions the Paper Calls Out

### Open Question 1
How would Maverick perform if trained with larger document encoders beyond DeBERTa-v3, such as GPT-3 or PaLM, and what would be the trade-offs between performance gains and computational costs? The authors acknowledge hardware limitations prevented testing with larger models and could not run Sequence-to-Sequence models for proper comparison.

### Open Question 2
How would Maverick perform on multilingual coreference resolution tasks beyond English, and what modifications would be needed to adapt it for languages with different coreference annotation guidelines? The methodology is language agnostic but the paper focuses exclusively on English OntoNotes without exploring cross-lingual performance.

### Open Question 3
What is the impact of Maverick's mention extraction errors on downstream applications like Entity Linking and Relation Extraction, and how does this compare to the impact of similar errors from other coreference resolution systems? The paper shows mention extraction is a weak point but does not quantify this impact on downstream tasks or compare it to other systems.

## Limitations
- Scalability to extremely long documents remains untested despite efficiency gains
- EOS regularization assumes mentions never cross sentence boundaries, which may not hold for future datasets
- Incremental transformer clustering has unquantified computational complexity at scale

## Confidence
**High Confidence**: Maverick achieves SOTA performance on OntoNotes with significantly fewer parameters; start-end extraction reduces candidates by ~8-9x; EOS regularization safely eliminates cross-sentence mentions.

**Medium Confidence**: Efficiency gains scale predictably to other datasets; incremental transformer clustering provides consistent out-of-domain improvements; mention pruning maintains high recall.

**Low Confidence**: Approach generalizes equally well to all future datasets; efficiency benefits remain proportional as document length increases; no systematic mention types are lost.

## Next Checks
1. **Cross-Dataset Generalization Test**: Evaluate Maverick on multiple coreference resolution datasets (PreCo, LitBank) with varying mention annotation guidelines, specifically testing the EOS regularization assumption and measuring any recall loss for cross-sentence mentions.

2. **Long Document Scalability Analysis**: Test Maverick on artificially extended documents or datasets with longer average document lengths to quantify how the start-end extraction's quadratic component scales and whether efficiency gains persist.

3. **Mention Type Coverage Audit**: Conduct a systematic analysis of which mention types (pronouns, named entities, nominals) are affected by the start-end extraction constraints and mention pruning thresholds, measuring any systematic biases in mention extraction recall.