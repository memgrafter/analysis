---
ver: rpa2
title: 'Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with
  Human-VLM Collaboration'
arxiv_id: '2406.16469'
source_url: https://arxiv.org/abs/2406.16469
tags:
- cultural
- questions
- korean
- dataset
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a semi-automated framework for constructing
  culturally aware vision-language benchmarks through human-VLM collaboration, specifically
  targeting Korean culture with the K-Viscuit dataset. The framework generates multiple-choice
  QA pairs using VLMs guided by annotation guidelines and human-annotated examples,
  followed by native speaker verification to ensure cultural relevance.
---

# Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration

## Quick Facts
- arXiv ID: 2406.16469
- Source URL: https://arxiv.org/abs/2406.16469
- Reference count: 35
- Key outcome: Proposes semi-automated framework for culturally-aware vision-language benchmarks, achieving 89.5% accuracy for top proprietary models on Korean cultural understanding vs 68% for best open-source model

## Executive Summary
This paper introduces a semi-automated framework for constructing culturally-aware vision-language benchmarks through human-VLM collaboration, specifically targeting Korean culture with the K-Viscuit dataset. The framework generates multiple-choice QA pairs using VLMs guided by annotation guidelines and human-annotated examples, followed by native speaker verification to ensure cultural relevance. K-Viscuit contains 657 examples across 10 cultural concepts, revealing significant performance gaps between proprietary and open-source VLMs in understanding Korean culture. Human evaluation demonstrates that Korean participants score 80.2% versus 47% for non-Koreans, while VLM performance matches human-level understanding. The dataset shows that models perform better on cultural knowledge application questions than visual recognition ones.

## Method Summary
The paper presents a semi-automated framework combining VLM generation with human verification to create culturally relevant vision-language benchmarks. The process involves: (1) manual image selection based on specified cultural concepts, (2) VLM generation of multiple-choice questions using detailed annotation guidelines, demonstration examples, and image-specific knowledge, (3) native speaker verification to filter and select culturally appropriate questions, and (4) dataset compilation and evaluation. The framework was applied to create K-Viscuit, a benchmark containing 657 examples across 10 Korean cultural concepts. The evaluation included 12 VLMs (8 open-source, 4 proprietary) tested on overall accuracy, question type breakdown (visual recognition vs cultural knowledge), and per-concept category analysis. Retrieval-augmented generation experiments were conducted to assess the impact of external knowledge on performance.

## Key Results
- Proprietary VLMs (Claude-3-opus, GPT-4-Turbo, Gemini-1.5-Pro, GPT-4o) achieved 89.5% accuracy vs 68% for best open-source model on K-Viscuit
- Models perform better on TYPE 2 questions (cultural knowledge application) than TYPE 1 questions (visual recognition)
- Korean participants scored 80.2% accuracy vs 47% for non-Korean participants on the same dataset
- Retrieval-augmented generation improved performance on food-related questions when relevant Wikipedia documents were provided

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLMs can generate culturally relevant questions when guided by annotation guidelines, demonstration examples, and image-specific knowledge.
- Mechanism: The VLM receives structured prompts containing the target image, exemplar questions, detailed annotation guidelines, and contextual knowledge about the image. This combination provides sufficient context for the VLM to generate questions that align with cultural nuances.
- Core assumption: VLMs possess enough cultural knowledge to generate relevant questions when provided with appropriate context and examples.
- Evidence anchors:
  - [abstract]: "VLMs generate questions based on guidelines, a small set of annotated examples, and relevant knowledge, followed by a verification process by native speakers"
  - [section 3.1.3]: "The VLM receives: 1) the target image, 2) human-annotated demonstration examples, 3) detailed annotation guidelines, and 4) image-specific knowledge descriptions"
  - [corpus]: Weak evidence - no direct corpus support for VLM question generation capability, but the paper demonstrates this works in practice
- Break condition: If VLMs lack sufficient cultural knowledge in their training data, they cannot generate meaningful questions even with proper guidance.

### Mechanism 2
- Claim: Human verification of VLM-generated questions ensures cultural relevance and quality while reducing manual annotation burden.
- Mechanism: Native speakers review and filter VLM-generated questions, selecting those that best reflect intended cultural subtleties rather than simply correcting incorrect content. This process maintains high cultural accuracy while leveraging VLM efficiency.
- Core assumption: Human reviewers can effectively distinguish between culturally appropriate and inappropriate questions, even when VLM outputs are factually correct.
- Evidence anchors:
  - [abstract]: "followed by a verification process by native speakers"
  - [section 3.1.4]: "Our human verification process prioritizes selecting question-option sets that best reflect intended cultural subtleties"
  - [corpus]: Weak evidence - no direct corpus support for verification effectiveness, but the paper demonstrates this approach works
- Break condition: If human reviewers lack cultural expertise or become inconsistent in their evaluations, the quality control mechanism fails.

### Mechanism 3
- Claim: Retrieval-augmented generation improves VLM performance on culturally-specific questions by providing external knowledge.
- Mechanism: VLMs are provided with relevant documents retrieved from Wikipedia based on image captions and embeddings, enhancing their ability to answer culturally nuanced questions that require specific knowledge.
- Core assumption: VLMs can effectively utilize external knowledge when provided in a structured format alongside the question and image.
- Evidence anchors:
  - [section 5.2]: "We augmented the models with relevant documents for each test image from the FOOD concept" and "providing retrieved documents can enhance model performance"
  - [table 7]: Shows improved performance with retrieved documents across multiple models
  - [corpus]: Weak evidence - no direct corpus support for retrieval effectiveness in cultural VQA tasks
- Break condition: If retrieved documents are irrelevant or VLMs cannot effectively integrate external knowledge, performance gains disappear.

## Foundational Learning

- Concept: Semi-automated data annotation pipelines
  - Why needed here: The paper combines VLM generation with human verification to create culturally relevant benchmarks efficiently
  - Quick check question: What are the advantages and limitations of using VLMs for question generation compared to purely manual annotation?

- Concept: Cultural bias in vision-language models
  - Why needed here: The paper addresses the performance gap between Western-centric models and their ability to understand non-Western cultures
  - Quick check question: How does cultural bias manifest in VLM performance, and why is it particularly problematic for visual interpretation tasks?

- Concept: Retrieval-augmented generation for knowledge enhancement
  - Why needed here: The paper explores using external knowledge to improve VLM performance on culturally-specific questions
  - Quick check question: What factors determine the effectiveness of retrieval-augmented generation in VQA tasks?

## Architecture Onboarding

- Component map: Image selection → VLM question generation (with guidelines and examples) → Human verification → Dataset creation
- Critical path: The most time-consuming step is human verification, which ensures quality but requires cultural expertise
- Design tradeoffs: Fully automated generation vs. human verification - automation increases efficiency but requires quality control; human-only approaches ensure quality but are resource-intensive
- Failure signatures: Low human verification acceptance rate indicates VLM cannot generate culturally relevant questions; inconsistent human evaluations suggest unclear verification criteria
- First 3 experiments:
  1. Test VLM question generation quality with different annotation guideline formulations
  2. Measure human verification time and acceptance rates with varying numbers of demonstration examples
  3. Evaluate retrieval-augmented generation performance with different document retrieval strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the semi-automated framework be extended to support fully automated dataset generation without requiring manual image selection?
- Basis in paper: [inferred] The paper notes that the current framework requires manual image selection as a limitation, stating "Our current framework requires a manual selection of images that match specified concepts, preventing fully automated dataset generation."
- Why unresolved: The paper identifies this as a limitation but doesn't propose specific solutions for achieving fully automated generation. The authors suggest that multimodal retrieval modules could help, but this remains unexplored.
- What evidence would resolve it: A follow-up study demonstrating a working multimodal retrieval system that can automatically select culturally relevant images without human intervention, showing comparable quality to the current manually curated dataset.

### Open Question 2
- Question: What specific training objectives or architectural modifications could reduce the sensitivity of VLMs to answer option ordering in multiple-choice questions?
- Basis in paper: [explicit] The paper discusses that "VLMs often fail to produce consistent predictions when the positions of the answer options are altered" and suggests exploring "calibration techniques, such as contrastive decoding or uncertainty-aware selection strategies."
- Why unresolved: While the paper identifies the problem and suggests potential approaches, it doesn't implement or evaluate any specific solutions to address this positional bias.
- What evidence would resolve it: Implementation and evaluation of specific techniques (e.g., contrastive decoding, uncertainty-aware selection) showing reduced accuracy variance across different option orderings compared to baseline models.

### Open Question 3
- Question: How does the performance gap between TYPE 1 and TYPE 2 questions vary across different cultural contexts and what underlying factors contribute to this pattern?
- Basis in paper: [explicit] The paper observes that "most models show higher accuracy in TYPE 2 questions compared to TYPE 1 questions" and speculates that "visual recognition with diverse cultural contexts poses inherent challenges for VLMs."
- Why unresolved: The paper only analyzes this pattern for Korean culture and doesn't explore whether this phenomenon generalizes to other cultures or investigate the root causes of this performance difference.
- What evidence would resolve it: Cross-cultural experiments comparing TYPE 1 vs TYPE 2 performance across multiple cultural datasets, along with ablation studies identifying whether the gap stems from visual complexity, cultural familiarity, or other factors.

## Limitations
- VLM generation mechanism shows effectiveness but relies heavily on human verification, with only 50-65% acceptance rate for generated questions
- Cultural performance gap may be influenced by training data biases rather than inherent model capabilities
- Retrieval-augmented generation experiments are limited to the FOOD concept only, with manual oracle selection potentially inflating performance metrics

## Confidence

- VLM generation with guidelines: Medium - demonstrated in practice but limited by high rejection rates
- Human verification effectiveness: Medium - ensures quality but introduces subjectivity and resource requirements
- Retrieval-augmented performance gains: Low - limited to single concept with manual intervention

## Next Checks

1. **Cross-cultural validation**: Apply the framework to construct benchmarks for 2-3 additional non-Western cultures to test generalizability and identify culture-specific challenges in VLM generation.

2. **Automated quality assessment**: Develop and validate automated metrics to predict human verification acceptance rates, reducing the need for extensive manual review while maintaining cultural accuracy.

3. **Longitudinal performance tracking**: Retrain the evaluated models on culturally diverse datasets and measure changes in K-Viscuit performance over time to distinguish between memorized knowledge and genuine cultural understanding.