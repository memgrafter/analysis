---
ver: rpa2
title: 'From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons'
arxiv_id: '2412.08442'
source_url: https://arxiv.org/abs/2412.08442
tags:
- arxiv
- training
- data
- tasks
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a method to adapt multimodal large language
  models (MLLMs) into generalist embodied agents capable of controlling diverse embodiments
  across manipulation, navigation, video games, UI control, and planning. The core
  idea is to learn a multi-embodiment action tokenizer that maps diverse continuous
  and discrete action spaces into a shared discrete token space compatible with the
  LLM.
---

# From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons

## Quick Facts
- arXiv ID: 2412.08442
- Source URL: https://arxiv.org/abs/2412.08442
- Reference count: 40
- Primary result: A method to adapt multimodal LLMs into generalist embodied agents that achieve strong performance across manipulation, navigation, games, UI control, and planning.

## Executive Summary
This work presents a method to adapt multimodal large language models (MLLMs) into generalist embodied agents capable of controlling diverse embodiments across manipulation, navigation, video games, UI control, and planning. The core idea is to learn a multi-embodiment action tokenizer that maps diverse continuous and discrete action spaces into a shared discrete token space compatible with the LLM. The agent is trained in two stages: first via supervised finetuning (SFT) on a large dataset of expert trajectories, then via online reinforcement learning (RL) to improve robustness and error recovery. Experiments show the resulting GEA model achieves strong generalization to unseen tasks, outperforming prior generalist agents and closely matching or exceeding specialist systems across multiple benchmarks.

## Method Summary
The method adapts a pretrained MLLM into a generalist embodied agent (GEA) through a two-stage training process. First, a multi-embodiment action tokenizer (RVQ VAE) is trained to map diverse action spaces into shared discrete tokens. Then, the MLLM is fine-tuned via supervised learning on a large dataset of embodied experiences. Finally, online reinforcement learning (PPO) is applied to improve robustness and error recovery. The resulting agent can control various embodiments across multiple domains using a unified architecture.

## Key Results
- GEA achieves strong generalization to unseen tasks, outperforming prior generalist agents.
- Cross-domain SFT training provides significant performance gains over per-domain training.
- Online RL improves robustness and error recovery, enabling the agent to correct mistakes during execution.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-embodiment action tokenization enables a single LLM to control diverse action spaces.
- Mechanism: The learned Residual VQ-VAE tokenizer maps continuous and discrete action spaces from various embodiments into a unified discrete token space compatible with the LLM vocabulary.
- Core assumption: The tokenizer can represent the diversity of action spaces precisely enough that decoded actions can be executed in their respective environments.
- Evidence anchors:
  - [abstract]: "learn a multi-embodiment action tokenizer that maps diverse continuous and discrete action spaces into a shared discrete token space compatible with the LLM."
  - [section]: "we use a learned action tokenizer that maps each continuous action into a sequence of new tokens... This hierarchical encoding makes RVQ effective in precisely representing continuous actions with a minimal number of discrete tokens."
- Break condition: If the tokenizer cannot preserve fine-grained control precision, decoded actions will be too coarse or incorrect for precise tasks like manipulation.

### Mechanism 2
- Claim: Stage 1 SFT on cross-domain data provides cross-domain performance gains.
- Mechanism: Finetuning the MLLM on a large dataset spanning manipulation, navigation, games, UI, and planning tasks teaches the model to generalize task instructions and visual understanding across domains.
- Core assumption: The MLLM's pre-trained vision-language knowledge is rich enough that cross-domain SFT will not dilute it but instead enable transfer.
- Evidence anchors:
  - [abstract]: "training with cross-domain data...are key to achieving these gains."
  - [section]: "training on the combined data from a diverse set of domains for SFT provides a cross-domain performance boost over using only per-domain data."
- Break condition: If the data domains are too heterogeneous without sufficient shared structure, the model may fail to learn coherent cross-domain mappings.

### Mechanism 3
- Claim: Stage 2 online RL corrects covariate shift and enables robustness to non-expert behaviors.
- Mechanism: After SFT, online RL allows the agent to recover from mistakes and explore actions not seen in the expert dataset, correcting for the mismatch between training trajectories and the agent's actual execution paths.
- Core assumption: The agent's own data collection in simulation can generate high-quality corrective trajectories to complement expert demonstrations.
- Evidence anchors:
  - [abstract]: "incorporating online RL are key to achieving these gains...GEA greatly increases...despite the latter being trained on 50k successful Habitat Pick demonstrations."
  - [section]: "GEA-Base can suffer from an inherent lack of data, specifically data diversity, and rarely exhibits robustness to mistakes. We thus also train GEA with a second stage of online reinforcement learning (RL) training..."
- Break condition: If online RL exploration is too limited or unstable, it may not generate useful corrections, or could degrade performance.

## Foundational Learning

- Concept: Action tokenization (continuous → discrete)
  - Why needed here: MLLMs naturally output text tokens; to control robots or games with continuous actions, these must be discretized while preserving precision.
  - Quick check question: How does the Residual VQ-VAE represent continuous actions using discrete tokens, and why is the residual encoding helpful?

- Concept: Cross-entropy supervised finetuning for imitation learning
  - Why needed here: SFT on expert trajectories teaches the MLLM to map instructions + observations → expert actions, leveraging its pre-trained vision-language knowledge.
  - Quick check question: What is the difference between training on discrete vs continuous action tokens during SFT?

- Concept: Covariate shift in imitation learning
  - Why needed here: Agents trained only on expert data often fail when small errors cause observation distributions to drift; online RL helps correct this.
  - Quick check question: Why does training only on successful demonstrations lead to poor robustness, and how does online RL mitigate this?

## Architecture Onboarding

- Component map:
  Visual encoder (from base MLLM) -> LLM (fine-tuned) -> Multi-embodiment action detokenizer (trained RVQ) -> Environment
  PPO value function MLP (used only during RL training)

- Critical path:
  1. Prompt + instruction + history of observations/actions -> LLM input
  2. LLM generates action tokens (discrete for discrete tasks, learned tokens for continuous)
  3. Action detokenizer decodes tokens into executable action
  4. Action executed in simulator; observation returned

- Design tradeoffs:
  - Single learned tokenizer vs per-domain tokenizers: unified tokenizer reduces model size but may sacrifice some domain-specific precision.
  - Joint SFT + RL vs iterative SFT: joint training encourages robustness earlier but is more complex to tune.
  - Context length (3 steps) vs longer memory: shorter context is computationally cheaper but may limit performance in partially observable settings.

- Failure signatures:
  - Action detokenizer outputs invalid actions -> likely tokenizer underfit or decoding mismatch
  - Low success rate in manipulation tasks -> possibly insufficient precision in continuous action tokenization
  - Poor generalization to new instructions -> possibly SFT data not diverse enough or instruction conditioning weak

- First 3 experiments:
  1. **Tokenization sanity check**: Feed a continuous action vector into the trained RVQ tokenizer and detokenizer; verify decoded action ≈ original within tolerance.
  2. **SFT ablation**: Train GEA-Base on a single domain dataset vs full multi-domain dataset; compare performance on that domain to confirm cross-domain benefits.
  3. **RL vs SFT-only**: Train GEA-Base with and without the online RL stage on a representative task (e.g., Habitat Pick); measure success rate improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GEA change when trained on more diverse or specialized datasets beyond the ones used in this work?
- Basis in paper: [explicit] The paper mentions that GEA is trained on a large dataset of 2.2 million trajectories spanning various domains and collection methods, but also notes that the performance of GEA in some domains, such as Maniskill, Atari, and AndroidControl, is far from perfect.
- Why unresolved: The paper does not explore the impact of using different or additional datasets on the performance of GEA. It would be interesting to see how the model performs with more specialized data for specific tasks or more diverse data from a wider range of domains.
- What evidence would resolve it: Training and evaluating GEA on a wider variety of datasets, including those not used in this work, and comparing the performance to the results reported in the paper.

### Open Question 2
- Question: How does the choice of base MLLM affect the performance of GEA across different domains and tasks?
- Basis in paper: [explicit] The paper analyzes the impact of the base MLLM on GEA's performance, finding that increasing model capacity leads to stronger performance. However, it does not explore the effect of using different base MLLMs or the impact of the base MLLM's pre-training on specific domains or tasks.
- Why unresolved: The paper only evaluates GEA using LLaVA-OneVision and MM1.5 as base MLLMs. It would be valuable to understand how other base MLLMs, especially those pre-trained on domain-specific data, would affect GEA's performance.
- What evidence would resolve it: Training and evaluating GEA using different base MLLMs, including those pre-trained on domain-specific data, and comparing the performance across different domains and tasks.

### Open Question 3
- Question: What are the limitations of GEA in terms of generalization to entirely new domains or embodiments not seen during training?
- Basis in paper: [explicit] The paper states that GEA cannot control arbitrary embodiments and operate in arbitrary environments zero-shot, indicating limitations in its generalization capabilities.
- Why unresolved: The paper does not provide a detailed analysis of GEA's performance when faced with entirely new domains or embodiments. It would be insightful to understand the extent of GEA's generalization capabilities and the factors that limit its performance in such scenarios.
- What evidence would resolve it: Evaluating GEA's performance on a set of tasks and environments that are entirely new and unrelated to those seen during training, and analyzing the factors that contribute to its success or failure in these scenarios.

## Limitations

- The evaluation relies heavily on synthetic or controlled environments, which may not fully capture real-world robustness.
- Direct attribution of performance gains to specific mechanisms is challenging without independent ablation studies.
- The robustness improvements from online RL are asserted but not rigorously measured with detailed failure mode analysis.

## Confidence

- **High Confidence**: The two-stage training procedure (SFT + online RL) and the use of a learned action tokenizer are technically sound and well-supported by the literature.
- **Medium Confidence**: The cross-domain performance gains from combined SFT training are plausible but the specific contribution of each domain is not fully quantified.
- **Low Confidence**: The robustness improvements from online RL are asserted but not rigorously measured; the paper lacks detailed analysis of failure modes.

## Next Checks

1. **Ablation on SFT data diversity**: Train GEA-Base on per-domain SFT datasets versus the combined dataset, then evaluate on a held-out domain to quantify the exact contribution of cross-domain training.
2. **Tokenizer precision analysis**: Systematically measure the reconstruction error of the RVQ VAE on continuous action spaces and correlate this with task success rates in manipulation tasks to confirm the importance of fine-grained control.
3. **Online RL robustness test**: After SFT and RL stages, introduce controlled perturbations (e.g., slight actuator noise, partial observability) and measure the agent's success rate and recovery behavior to validate the robustness claims.