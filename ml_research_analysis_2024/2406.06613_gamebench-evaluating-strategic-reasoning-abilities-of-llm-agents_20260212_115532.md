---
ver: rpa2
title: 'GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents'
arxiv_id: '2406.06613'
source_url: https://arxiv.org/abs/2406.06613
tags:
- game
- games
- cards
- reasoning
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GAME BENCH, a cross-domain benchmark for
  evaluating strategic reasoning abilities of large language model (LLM) agents in
  game environments. The benchmark consists of 9 different games covering various
  types of reasoning skills such as abstract strategy, non-deterministic outcomes,
  hidden information, language communication, social deduction, and cooperation.
---

# GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents

## Quick Facts
- arXiv ID: 2406.06613
- Source URL: https://arxiv.org/abs/2406.06613
- Reference count: 23
- Primary result: LLM agents perform significantly worse than humans in strategic reasoning games, with GPT-4 sometimes performing worse than random action

## Executive Summary
This paper introduces GAME BENCH, a benchmark designed to evaluate the strategic reasoning capabilities of large language model agents across diverse game environments. The benchmark includes 9 different games requiring various reasoning skills, from abstract strategy to social deduction. The authors evaluate GPT-3, GPT-4, and two scaffolding techniques (Chain-of-Thought prompting and Reasoning Via Planning) by having them play against each other, a random baseline, and a human baseline. Results consistently show that none of the tested models match human performance, with GPT-4 performing worse than random action in some cases, though both CoT and RAP scaffolding improve scores.

## Method Summary
The benchmark consists of 9 game implementations where agents play against each other, random baselines, and human players. The evaluation uses GPT-3 (gpt-3.5-turbo-1106) and GPT-4 (gpt-4-1106-preview) models with two scaffolding techniques: Chain-of-Thought prompting and Reasoning Via Planning. Match outcomes are collected and agent performance is compared using the Bradley-Terry rating model, which provides robust comparisons across multiple games without requiring head-to-head matchups. Games were selected to minimize exposure to strategic patterns in pretraining data by excluding those with published strategy guides or dedicated online forums.

## Key Results
- None of the tested models match human performance in strategic reasoning games
- GPT-4 performs worse than random action in some games
- Chain-of-Thought and RAP scaffolding both improve scores but not to human levels
- Human baseline consistently outperforms all LLM agents across all games

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Thought prompting significantly improves strategic reasoning in LLMs by providing intermediate reasoning steps
- Core assumption: The LLM's reasoning capabilities are latent but can be activated through explicit prompting to "think step-by-step"
- Evidence anchors: CoT provided best median and upper quartile results; none of the models match human performance
- Break condition: If the model's base reasoning capabilities are too limited, even CoT cannot generate coherent intermediate steps

### Mechanism 2
- Claim: Bradley-Terry rating model provides more robust comparisons across multiple games than Elo
- Core assumption: Agent skill remains constant across different games
- Evidence anchors: Bradley-Terry assumes fixed agent abilities and doesn't need decentralized calculation
- Break condition: If agent abilities vary significantly across different game types

### Mechanism 3
- Claim: Out-of-distribution game selection prevents overfitting to common strategic patterns
- Core assumption: Games with limited online presence have minimal strategy documentation in pretraining data
- Evidence anchors: Games were filtered to exclude those with strategy guides or dedicated forums
- Break condition: If selected games still have significant strategy content in pretraining data

## Foundational Learning

- Concept: Bradley-Terry rating model
  - Why needed here: Provides statistical framework for comparing agent performance across multiple games
  - Quick check question: How does Bradley-Terry differ from Elo in handling agent comparisons across different games?

- Concept: Chain-of-Thought prompting
  - Why needed here: Enables LLMs to demonstrate reasoning by explicitly generating intermediate steps
  - Quick check question: What is the key difference between CoT prompting and standard prompting approaches?

- Concept: Out-of-distribution evaluation
  - Why needed here: Ensures performance reflects genuine reasoning rather than memorized patterns
  - Quick check question: Why is it important to evaluate LLMs on games without published strategy guides?

## Architecture Onboarding

- Component map: Game environment implementations -> Agent classes (random, human, GPT-3/4 base/scaffolded) -> Rating calculation module -> Data collection pipeline
- Critical path: Game environment initialization → Agent action selection → State update → Match completion → Rating calculation through bootstrapping and maximum likelihood estimation
- Design tradeoffs: Single-use agent instances (simplifies state management but prevents learning) vs persistent agents (enables learning but complicates comparison)
- Failure signatures: Poor performance in specific games indicating reasoning gaps; rating instability suggesting model limitations; API failures revealing implementation issues
- First 3 experiments:
  1. Run baseline random agent vs itself in one game to verify game mechanics and scoring
  2. Run base GPT-3 vs random agent in one game to establish baseline performance
  3. Run GPT-3 with CoT vs base GPT-3 in one game to measure scaffolding impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GPT-4 performance change when provided with strategy guides in-context?
- Basis in paper: [inferred] The importance of selecting games without published strategy guides suggests this could be a key factor
- Why unresolved: The paper does not include experiments where strategy guides are provided to the models
- What evidence would resolve it: Conduct experiments with strategy guides for both in-distribution and out-of-distribution games

### Open Question 2
- Question: What is the effect of increasing Monte Carlo tree search depth in GPT-4-RAP?
- Basis in paper: [explicit] GPT-4-RAP was run with depth limit of 2 due to resource constraints
- Why unresolved: The paper does not explore GPT-4-RAP performance at different depths
- What evidence would resolve it: Run experiments with GPT-4-RAP at various depths across different games

### Open Question 3
- Question: How do LLMs perform on slightly modified versions of common games (counterfactual games)?
- Basis in paper: [explicit] The paper suggests counterfactual games could reduce association with in-distribution counterparts
- Why unresolved: The paper does not implement or test counterfactual games
- What evidence would resolve it: Create counterfactual versions of common games and compare performance

## Limitations

- Human baseline performance data is not comprehensively detailed, making it difficult to assess the true gap between human and LLM capabilities
- Out-of-distribution game selection may not fully prevent exposure to strategic patterns in pretraining data
- Bradley-Terry rating system assumes fixed agent abilities that may not hold true across diverse game types

## Confidence

**High Confidence:** Experimental methodology for implementing and running game environments is well-specified; comparison between scaffolding techniques is methodologically sound

**Medium Confidence:** CoT prompting improves strategic reasoning performance (though magnitude varies); GPT-4 performing worse than random is supported by data but warrants further investigation

**Low Confidence:** Effectiveness of out-of-distribution selection in preventing pretraining data contamination is difficult to verify; human baseline comparison is less confident due to limited documentation

## Next Checks

1. Conduct ablation studies testing variations of CoT and RAP prompt templates to identify minimum viable scaffolding that produces consistent improvements across all games

2. Re-run human data collection with larger sample size and more detailed documentation of human decision-making processes to better understand performance gap

3. Perform systematic analysis of pretraining data to identify presence of strategy guides or discussions related to selected games, validating whether out-of-distribution assumption holds