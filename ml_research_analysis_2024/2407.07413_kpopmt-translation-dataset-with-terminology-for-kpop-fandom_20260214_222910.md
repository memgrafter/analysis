---
ver: rpa2
title: 'KpopMT: Translation Dataset with Terminology for Kpop Fandom'
arxiv_id: '2407.07413'
source_url: https://arxiv.org/abs/2407.07413
tags:
- translation
- language
- fandom
- data
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KpopMT, a terminology-tagged translation
  dataset designed to address the challenges of translating specialized language systems
  within social groups, specifically the Kpop fandom. The dataset comprises 1,000
  English translations of Korean posts and comments, each annotated with specific
  fandom-related terminologies.
---

# KpopMT: Translation Dataset with Terminology for Kpop Fandom

## Quick Facts
- arXiv ID: 2407.07413
- Source URL: https://arxiv.org/abs/2407.07413
- Authors: JiWoo Kim; Yunsu Kim; JinYeong Bak
- Reference count: 11
- Primary result: Human evaluators strongly prefer translations preserving fandom-specific terminology over standard translations

## Executive Summary
This paper introduces KpopMT, a terminology-tagged translation dataset designed to address the challenges of translating specialized language systems within social groups, specifically the Kpop fandom. The dataset comprises 1,000 English translations of Korean posts and comments, each annotated with specific fandom-related terminologies. Human evaluators, native English-speaking Kpop fans, strongly preferred translations with fandom-specific terms over standard language translations. Evaluations using existing translation systems, including GPT models, showed overall low performance scores, highlighting the difficulty of reflecting group-specific terminologies and styles in translation.

## Method Summary
The KpopMT dataset was constructed through a two-phase process: collecting Korean posts and comments from Kpop fan communities, then having expert translators annotate and translate these with terminology tags. The dataset includes three types of terminology annotations: Group-Lexicon (fandom-specific terms), Group-NE (named entities), and Slang. Translation systems were evaluated using both standard metrics (BLEU, COMET, chrF++) and terminology-focused metrics (EMA, 1-TERm). Models were fine-tuned on general Korean-English parallel data (800k pairs) plus fandom-specific data using back-translation and domain adaptation techniques.

## Key Results
- Human evaluators preferred fandom-specific terminology translations over standard translations at a 2:1 ratio
- Existing translation systems, including GPT models, showed overall low performance scores on the KpopMT dataset
- GPT models demonstrated higher scores than other systems but still struggled with Group-Lexicon terminology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Social group language systems differ from general language corpora because they develop unique terminologies that are context-dependent and socially constructed.
- Mechanism: By creating a terminology-tagged dataset that captures these unique terms and their usage patterns, translation systems can be trained to recognize and preserve social group-specific language.
- Core assumption: The unique terminology of social groups is consistent enough within the group to be captured and translated effectively.
- Evidence anchors:
  - [abstract] "humans have the unique capability to establish new language systems...This makes human form unique language systems within social groups."
  - [section] "Social groups often develop their unique linguistic systems, replete with specific terminologies and jargon"
  - [corpus] Weak - The corpus analysis shows low citation counts and few direct references to this mechanism in related work, suggesting this is a novel approach.
- Break condition: If the terminology within a social group is too fluid or context-dependent to be captured in a static dataset, or if the group's language evolves faster than the dataset can be updated.

### Mechanism 2
- Claim: Human evaluators who are native speakers of the social group's language strongly prefer translations that preserve group-specific terminology over standard translations.
- Mechanism: By involving native speakers of the social group in the evaluation process, the dataset ensures that translations are not just linguistically accurate but also socially appropriate and authentic.
- Core assumption: Native speakers of the social group have a shared understanding of what constitutes appropriate use of group-specific terminology.
- Evidence anchors:
  - [abstract] "Human evaluators, native English-speaking Kpop fans, strongly preferred translations with fandom-specific terms over standard language translations."
  - [section] "KpopMT is constructed in two phases...terminology information in the sentences"
  - [corpus] Weak - The corpus analysis does not provide evidence of this mechanism being validated in other contexts or with other social groups.
- Break condition: If the social group is too diverse or fragmented, such that even native speakers cannot agree on appropriate terminology usage.

### Mechanism 3
- Claim: Existing translation systems perform poorly on social group-specific terminology because they are trained on general language corpora that do not include these specialized terms.
- Mechanism: By evaluating existing translation systems on KpopMT and demonstrating their poor performance, the paper highlights the need for specialized translation models trained on social group-specific data.
- Core assumption: The poor performance of existing systems is due to their lack of exposure to social group-specific terminology, not due to fundamental limitations of translation technology.
- Evidence anchors:
  - [abstract] "Evaluations using existing translation systems, including GPT models, showed overall low performance scores"
  - [section] "GPT models demonstrate higher scores than other systems...However, Group-Lexicon type faces challenges"
  - [corpus] Weak - The corpus analysis does not provide evidence of this mechanism being tested with other types of specialized terminology or social groups.
- Break condition: If the poor performance is due to fundamental limitations of translation technology that cannot be overcome by training on specialized data.

## Foundational Learning

- Concept: Terminology translation and its importance in specialized domains
  - Why needed here: The paper focuses on translating specialized terminology used by social groups, which requires understanding how to capture and translate these terms accurately.
  - Quick check question: What is the difference between general language translation and terminology translation?

- Concept: Social dialects and language variation within communities
  - Why needed here: The paper argues that social groups develop their own language systems, which need to be considered in translation.
  - Quick check question: How do social dialects differ from standard language, and why is this important for translation?

- Concept: Evaluation metrics for machine translation, including BLEU, COMET, and chrF++
  - Why needed here: The paper evaluates translation systems using these metrics, which requires understanding how they work and what they measure.
  - Quick check question: What are the key differences between BLEU, COMET, and chrF++ as evaluation metrics for machine translation?

## Architecture Onboarding

- Component map: Data Collection -> Terminology Annotation -> Translation -> Evaluation
- Critical path: Data collection → Terminology annotation → Translation → Evaluation
- Design tradeoffs:
  - Using expert translators ensures high-quality annotations but is resource-intensive
  - Focusing on a single social group (Kpop fandom) allows for deep understanding but limits generalizability
  - Providing both the dataset and evaluation tools enables further research but requires significant upfront investment
- Failure signatures:
  - Low scores on terminology-focused metrics (EMA, 1-TERm) indicate poor handling of social group-specific terms
  - High scores on general translation metrics (BLEU, COMET) but low scores on terminology metrics suggest the model is fluent but not accurate in preserving social group language
- First 3 experiments:
  1. Evaluate existing translation systems (GPT, mBART, etc.) on KpopMT to establish baseline performance
  2. Fine-tune a translation model on KpopMT and evaluate its performance improvement
  3. Compare human evaluation results with automated metric scores to assess the validity of the metrics for this domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating fandom monolingual data into training consistently improve translation performance for specialized terminology?
- Basis in paper: [explicit] The paper shows that mBART w/ Fandom and Domain Adaptation, which use fandom monolingual data, do not consistently outperform baselines, suggesting noise in pseudo-parallel data generated through back-translation.
- Why unresolved: The exact nature and extent of noise in fandom monolingual data during back-translation is unclear, and how to mitigate this issue is not addressed.
- What evidence would resolve it: Comparative experiments showing the impact of noise reduction techniques on translation quality when using fandom monolingual data.

### Open Question 2
- Question: Is it possible to enhance overall translation quality while preserving the distinctive terminology of social groups?
- Basis in paper: [explicit] The paper highlights a tension between translation fluency and terminology accuracy, indicating a need to explore methods that improve both aspects simultaneously.
- Why unresolved: The paper does not provide solutions or experimental results that address this tension effectively.
- What evidence would resolve it: Experimental results demonstrating improved translation quality metrics alongside maintained or enhanced terminology accuracy.

### Open Question 3
- Question: How do different types of terminologies (Group-Lexicon, Group-NE, Slang) impact the performance of translation models?
- Basis in paper: [explicit] The paper evaluates terminology accuracy using EMA and 1-TERm scores, showing that GPT-4 struggles with Group-Lexicon terms, but does not provide a detailed analysis of how each type affects overall performance.
- Why unresolved: The paper does not break down the performance metrics by terminology type across all models, leaving the impact of each type unclear.
- What evidence would resolve it: Detailed performance breakdowns by terminology type for each translation model, highlighting which types are most challenging.

## Limitations
- The dataset size (1,000 sentence pairs) is relatively small compared to standard translation benchmarks
- The focus on a single social group (Kpop fandom) limits generalizability to other social groups
- The paper does not definitively establish whether poor performance is due to lack of specialized data or fundamental limitations of translation technology

## Confidence
**High Confidence**: The claim that human evaluators strongly prefer translations preserving social group terminology is well-supported by the human evaluation results showing a 2:1 preference ratio.

**Medium Confidence**: The claim that existing translation systems perform poorly on social group terminology is supported by the evaluation results, but the paper does not rule out other explanations for this poor performance.

**Low Confidence**: The generalizability of findings to other social groups or larger-scale datasets remains uncertain given the limited scope of the current study.

## Next Checks
1. **Cross-group validation**: Evaluate KpopMT-style datasets on 2-3 other social groups (e.g., gaming communities, professional fields) to test whether the observed translation challenges generalize beyond Kpop fandom.

2. **Scale sensitivity analysis**: Train models on incrementally larger subsets of KpopMT (100, 500, 1000 pairs) to determine the minimum dataset size required for effective social group terminology translation.

3. **Longitudinal stability test**: Collect and evaluate translations of the same social group content at different time points (e.g., 6-month intervals) to assess how quickly social group terminology evolves and whether static datasets become obsolete.