---
ver: rpa2
title: Stepwise Reasoning Error Disruption Attack of LLMs
arxiv_id: '2412.11934'
source_url: https://arxiv.org/abs/2412.11934
tags:
- reasoning
- attack
- answer
- step
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEED, a novel attack method targeting the
  reasoning processes of large language models (LLMs). The method works by subtly
  injecting errors into early reasoning steps, which then propagate through subsequent
  reasoning to produce incorrect final answers.
---

# Stepwise Reasoning Error Disruption Attack of LLMs

## Quick Facts
- arXiv ID: 2412.11934
- Source URL: https://arxiv.org/abs/2412.11934
- Reference count: 40
- One-line primary result: Introduces SEED, a novel attack method that injects subtle errors into early reasoning steps of LLMs, achieving high attack success rates (58.40% to 97.68%) while maintaining low detection rates (3.64% to 16.40%) across four datasets and four models.

## Executive Summary
This paper introduces SEED (Stepwise Reasoning Error Disruption), a novel attack method that targets the reasoning processes of large language models by subtly injecting errors into early reasoning steps. These injected errors propagate through subsequent reasoning steps, ultimately producing incorrect final answers. The attack is designed to be both effective and covert, maintaining the natural flow of reasoning while achieving high attack success rates. Extensive experiments demonstrate SEED's effectiveness across four datasets (MATH, GSM8K, CSQA, MATHQA) and four different LLMs (Llama3-8B, Qwen-2.5-7B, Mistral-v0.3-7B, GPT-4o), revealing significant vulnerabilities in LLMs to reasoning process disruptions.

## Method Summary
SEED works by injecting errors into the reasoning steps of LLMs, with two variations: SEED-S (Step Modification) that modifies individual reasoning steps, and SEED-P (Problem Modification) that alters the problem statement itself. The attack uses a hyperparameter σ to control the proportion of injected errors. The method is evaluated across zero-shot and few-shot settings, measuring accuracy, attack success rate, modification success rate, and detection rate. The attack maintains high stealth with low detection rates while achieving attack success rates ranging from 58.40% to 97.68% across different datasets.

## Key Results
- SEED achieves attack success rates ranging from 58.40% to 97.68% across different datasets and LLMs
- The attack maintains low detection rates of 3.64% to 16.40%, demonstrating high stealth
- SEED outperforms baseline methods in both zero-shot and few-shot settings
- Qwen-2.5-7B and GPT-4o show greater robustness to SEED attacks compared to Llama3-8B and Mistral-v0.3-7B

## Why This Works (Mechanism)
SEED exploits the sequential nature of LLM reasoning processes. By injecting subtle errors into early reasoning steps, the attack creates a cascade effect where subsequent reasoning steps build upon incorrect foundations. The method's effectiveness stems from the LLM's tendency to maintain consistency with its earlier reasoning rather than questioning foundational errors. The covert nature of the attack is achieved by making modifications that preserve the overall reasoning flow and appear natural within the context of the problem-solving process.

## Foundational Learning
- **LLM reasoning chain propagation**: Why needed - understanding how errors in early steps affect later conclusions; Quick check - trace error propagation through multi-step reasoning tasks
- **Stealth attack design**: Why needed - creating modifications that evade detection while maintaining effectiveness; Quick check - measure detection rates across different modification strategies
- **Hyperparameter control (σ)**: Why needed - balancing attack success rate against stealth requirements; Quick check - vary σ and observe changes in ASR and detection rate
- **Zero-shot vs few-shot settings**: Why needed - understanding attack performance across different prompt engineering approaches; Quick check - compare results across different few-shot examples
- **Model robustness variation**: Why needed - identifying which model architectures are more resistant to reasoning disruption; Quick check - test across diverse model families with different training approaches
- **Error injection techniques**: Why needed - determining optimal ways to introduce errors without raising suspicion; Quick check - compare different error injection methods on success rate and detection

## Architecture Onboarding

**Component map**: Datasets (MATH, GSM8K, CSQA, MATHQA) -> SEED attack implementation (SEED-S, SEED-P) -> LLM evaluation (Llama3-8B, Qwen-2.5-7B, Mistral-v0.3-7B, GPT-4o) -> Metric calculation (ACC, ASR, MSR, detection rate)

**Critical path**: Error injection in early reasoning steps → Propagation through subsequent reasoning → Final answer generation → Detection by judge model → Metric computation

**Design tradeoffs**: The paper balances attack effectiveness against stealthiness by controlling the proportion of injected errors through σ, accepting moderate ASR reductions to maintain low detection rates. The choice between step modification and problem modification represents another tradeoff between precision and scope of attack.

**Failure signatures**: Low attack success rates may indicate insufficient error injection or overly robust reasoning models. High detection rates suggest modifications are too obvious or unnatural. Inconsistent results across datasets may indicate task-specific vulnerabilities or limitations in the attack method.

**First experiments**:
1. Test SEED-S and SEED-P on a single dataset (e.g., MATH) with one model (e.g., Llama3-8B) to validate basic functionality
2. Vary σ values to find optimal balance between attack success rate and detection rate
3. Compare zero-shot and few-shot performance on the same dataset-model combination to establish baseline effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of SEED attacks vary when applied to LLMs with different levels of reasoning complexity or architectural designs? The paper tests SEED on four different LLMs and observes varying levels of robustness, but does not explore the relationship between architectural design, reasoning complexity, and vulnerability to SEED attacks. Testing SEED on a broader range of LLMs with diverse architectures and reasoning capabilities, followed by comparative analysis of attack success rates and detection rates, would clarify how model design impacts vulnerability.

### Open Question 2
Can SEED attacks be effectively mitigated through dynamic prompt-based defenses that adapt to the reasoning process in real-time? The paper evaluates a simple prompt-based self-review mitigation and finds only modest improvements, suggesting that static or basic prompt-based defenses are insufficient. Developing and testing dynamic prompt-based defenses that monitor and adapt to the reasoning process in real-time, followed by evaluating their effectiveness against SEED attacks, would determine whether such defenses can significantly reduce vulnerability.

### Open Question 3
How does the effectiveness of SEED attacks scale with the length and complexity of the reasoning chain? The paper introduces σ to control injected errors but does not systematically analyze how reasoning chain length or complexity affects attack success. Conducting experiments with reasoning tasks of varying lengths and complexities, while systematically varying σ and analyzing attack success rates, would reveal how the reasoning chain's structure influences vulnerability to SEED attacks.

## Limitations
- Sampling only 500 questions per dataset may not capture full dataset diversity and complexity
- Detection rate measurement depends on GPT-4o's judgment capabilities, introducing third-party model dependency
- Limited error analysis of why specific attacks succeed or fail on particular reasoning steps
- Does not explore attack effectiveness against specialized reasoning-focused models beyond general-purpose LLMs

## Confidence

**High confidence** in the attack methodology and implementation based on clear experimental procedures and reproducible results
**Medium confidence** in the stealthiness claims due to third-party judge dependency and limited error analysis
**Medium confidence** in generalizability across different reasoning tasks due to the sampling approach rather than full dataset evaluation

## Next Checks

1. Validate the attack's effectiveness using full datasets rather than the sampled 500 questions per dataset to assess robustness across broader problem distributions
2. Conduct ablation studies comparing different error injection strategies and their impact on both attack success rate and detection rate to understand the attack mechanism better
3. Test the attack against additional reasoning-focused models like DeepSeek-Coder or specialized math models to evaluate performance beyond general-purpose LLMs