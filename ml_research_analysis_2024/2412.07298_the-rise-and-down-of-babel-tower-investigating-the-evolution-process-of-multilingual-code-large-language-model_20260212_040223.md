---
ver: rpa2
title: 'The Rise and Down of Babel Tower: Investigating the Evolution Process of Multilingual
  Code Large Language Model'
arxiv_id: '2412.07298'
source_url: https://arxiv.org/abs/2412.07298
tags:
- language
- python
- pre-training
- languages
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the evolution of multilingual capabilities
  in large language models (LLMs) during pre-training. The authors propose the Babel
  Tower Hypothesis, which suggests that during pre-training, multiple languages initially
  share a single knowledge system dominated by a primary language and then gradually
  shift to developing multiple language-specific knowledge systems.
---

# The Rise and Down of Babel Tower: Investigating the Evolution Process of Multilingual Code Large Language Model

## Quick Facts
- arXiv ID: 2412.07298
- Source URL: https://arxiv.org/abs/2412.07298
- Reference count: 27
- Authors: Jiawei Chen, Wentao Chen, Jing Su, Jingjing Xu, Hongyu Lin, Mengjie Ren, Yaojie Lu, Xianpei Han, Le Sun
- Primary result: Proposes Babel Tower Hypothesis showing multilingual LLMs evolve from single-language dominance to multiple language-specific systems during pre-training

## Executive Summary
This paper investigates the evolution of multilingual capabilities in large language models during pre-training, proposing the Babel Tower Hypothesis. The hypothesis suggests that multiple languages initially share a single knowledge system dominated by a primary language, which then gradually shifts to developing multiple language-specific knowledge systems. To validate this hypothesis, the authors track internal states of LLMs by identifying working languages and language-transferring neurons, finding that internal state changes align with their proposed hypothesis. Based on these insights, they develop a novel method to construct an optimized pre-training corpus for multilingual code LLMs, achieving significant performance improvements over models trained on the original corpus.

## Method Summary
The authors track the internal states of multilingual code LLMs during pre-training to validate their Babel Tower Hypothesis. They identify working languages and language-transferring neurons to monitor how the model's multilingual capabilities evolve. The methodology involves analyzing neuron activations to detect when and how languages transition from sharing a single knowledge system to developing separate language-specific systems. Building on these insights, they construct an optimized pre-training corpus by adjusting the data distribution to better align with the identified evolution patterns, resulting in improved multilingual performance.

## Key Results
- Successfully validated the Babel Tower Hypothesis through tracking internal states of multilingual code LLMs
- Identified working languages and language-transferring neurons that demonstrate the evolution from single-language dominance to multiple language-specific systems
- Achieved significant performance improvements using the optimized pre-training corpus compared to models trained on the original corpus

## Why This Works (Mechanism)
The Babel Tower Hypothesis works because it captures the fundamental evolutionary pattern of how multilingual models develop during pre-training. As models process multilingual data, they initially rely on a primary language to bootstrap understanding across all languages, creating a unified knowledge representation. Over time, as the model encounters sufficient data from secondary languages, it develops separate, language-specific representations to better capture linguistic nuances. This transition is facilitated by specific neurons that act as bridges between languages during the transfer phase. By understanding this mechanism, the authors can optimize the pre-training corpus distribution to accelerate this natural evolution, leading to more efficient multilingual capability development.

## Foundational Learning

### Neuron Activation Analysis
- Why needed: Essential for identifying working languages and tracking language-transferring neurons
- Quick check: Verify neuron activation patterns correlate with language-specific task performance

### Multilingual Pre-training Dynamics
- Why needed: Understanding how models process and integrate multiple languages during training
- Quick check: Monitor training loss curves across different language groups

### Corpus Distribution Optimization
- Why needed: Critical for constructing effective pre-training data that accelerates multilingual development
- Quick check: Validate optimized corpus improves performance across all target languages

## Architecture Onboarding

### Component Map
Input Corpus -> Babel Tower Analysis -> Language-Transferring Neuron Identification -> Optimized Corpus Construction -> Pre-trained Multilingual LLM

### Critical Path
1. Corpus preparation and initial pre-training
2. Babel Tower Hypothesis validation through neuron tracking
3. Optimized corpus construction based on insights
4. Final pre-training with optimized data

### Design Tradeoffs
- Primary language dominance vs. balanced multilingual representation
- Computational cost of neuron tracking vs. performance gains
- Corpus diversity vs. targeted language-specific optimization

### Failure Signatures
- Inability to identify clear language-transferring neurons
- Persistent performance gaps between primary and secondary languages
- Overfitting to primary language at the expense of multilingual capability

### 3 First Experiments
1. Track neuron activations across different language samples during initial pre-training
2. Test optimized corpus construction by varying language distribution ratios
3. Compare performance of models trained with optimized vs. original corpus on multilingual benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- Findings are based on multilingual code LLMs rather than general-purpose LLMs, limiting generalizability
- Methodology for identifying working languages and language-transferring neurons may introduce selection bias
- Optimized corpus construction lacks systematic evaluation across diverse multilingual tasks beyond code generation

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Experimental results show improved performance with optimized corpus | High |
| Babel Tower Hypothesis characterization of evolution process | Medium |
| Methodology for tracking working languages and identifying neurons | Medium |

## Next Checks
1. Replicate core findings on general-purpose multilingual LLMs to test generalizability of the Babel Tower Hypothesis
2. Conduct ablation studies removing specific language-transferring neurons to verify their causal role
3. Test optimized corpus construction method on multiple diverse multilingual tasks beyond code generation