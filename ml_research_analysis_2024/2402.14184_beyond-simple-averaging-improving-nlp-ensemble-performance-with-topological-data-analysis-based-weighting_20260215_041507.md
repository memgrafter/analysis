---
ver: rpa2
title: 'Beyond Simple Averaging: Improving NLP Ensemble Performance with Topological-Data-Analysis-Based
  Weighting'
arxiv_id: '2402.14184'
source_url: https://arxiv.org/abs/2402.14184
tags:
- ensemble
- quality
- weights
- neural
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to improve ensemble performance in
  NLP by incorporating model similarity into the weighting process, rather than using
  equal weights. The authors leverage topological data analysis (TDA) features from
  attention layers to measure model similarity and optimize ensemble weights using
  quadratic programming.
---

# Beyond Simple Averaging: Improving NLP Ensemble Performance with Topological-Data-Analysis-Based Weighting

## Quick Facts
- arXiv ID: 2402.14184
- Source URL: https://arxiv.org/abs/2402.14184
- Authors: Polina Proskura; Alexey Zaytsev
- Reference count: 26
- Primary result: 1% accuracy improvement over equal-weight ensembles using TDA-based model similarity

## Executive Summary
This paper proposes a method to improve ensemble performance in NLP by incorporating model similarity into the weighting process, rather than using equal weights. The authors leverage topological data analysis (TDA) features from attention layers to measure model similarity and optimize ensemble weights using quadratic programming. Experiments on IMDB and CoLA datasets show that the proposed method achieves higher accuracy (up to 1% improvement) and better uncertainty estimation compared to equal-weight ensembles and ensembles weighted using output correlation. The authors also demonstrate that combining strong and weak models can further improve ensemble performance.

## Method Summary
The method combines individual model predictions using optimized weights that account for both model quality and similarity. Model similarity is measured using Representation Topology Divergence (RTD) features extracted from attention layer representations through topological data analysis. The similarity matrix, along with model error estimates, is used in a quadratic programming framework to optimize ensemble weights. The optimization minimizes a risk function that balances individual model strengths against their redundancy.

## Key Results
- 1% accuracy improvement over equal-weight ensembles on IMDB and CoLA datasets
- Better uncertainty estimation measured by lower Area Under the Rejection Curve (AURC)
- RTD-based similarity outperforms output correlation for ensemble weighting
- Combining strong and weak models achieves better performance than strong models alone

## Why This Works (Mechanism)

### Mechanism 1
Using TDA-based similarity metrics (RTD) to measure model divergence improves ensemble weighting by reducing the influence of redundant models. RTD captures topological differences between attention layer representations by analyzing how connected components merge under varying thresholds, providing a more nuanced similarity measure than output correlation alone. This works because models that share similar attention topology are more likely to make correlated errors, so their combined influence should be downweighted.

### Mechanism 2
Quadratic programming optimization of ensemble weights using both model quality (error) and similarity (RTD or output correlation) achieves lower ensemble risk than equal weighting. The optimization minimizes L(α) = αT(a - c) + αT Bα, where B contains similarity measures, balancing individual model strengths against their redundancy. This works because the validation error estimates and similarity measures are accurate enough for the quadratic optimization to find weights that generalize well.

### Mechanism 3
Combining strong and weak models in an ensemble with optimized weights can achieve better accuracy than using only strong models. Weak models provide complementary error patterns that, when properly weighted, can correct systematic errors of strong models. This works because weak models have sufficient diversity and are not simply inferior versions of strong models on the same error patterns.

## Foundational Learning

- **Topological Data Analysis (TDA) and persistent homology**: Understanding how RTD features capture topological differences between attention graphs is crucial for implementing and debugging the similarity measure. Quick check: How does the barcode representation encode the stability of graph features across different threshold values?

- **Quadratic programming and convex optimization**: The weight optimization problem is formulated as a quadratic program, so understanding constraints, objective functions, and solution methods is essential. Quick check: What are the Karush-Kuhn-Tucker (KKT) conditions for the weight optimization problem, and how do they ensure the solution satisfies both equality and inequality constraints?

- **Attention mechanism in transformer models**: The method relies on attention layer features, so understanding how attention weights encode token relationships is necessary for feature extraction and interpretation. Quick check: How does the attention weight matrix structure differ between layers, and what information does each layer capture about token relationships?

## Architecture Onboarding

- **Component map**: Individual model training -> Feature extraction from attention layers -> Similarity matrix computation using RTD or output correlation -> Quadratic optimization to find ensemble weights -> Ensemble prediction using weighted combination
- **Critical path**: Feature extraction → Similarity computation → Weight optimization → Ensemble prediction
- **Design tradeoffs**: TDA-based similarity provides more nuanced model relationships but is computationally expensive; output correlation is faster but may miss important topological differences; equal weighting is simplest but ignores model diversity
- **Failure signatures**: If ensemble weights are all nearly equal, similarity measures may not be capturing meaningful differences; if optimization produces extreme weights (close to 0 or 1), similarity or quality estimates may be unreliable
- **First 3 experiments**:
  1. Implement RTD feature extraction on attention layers and visualize barcodes for different model pairs to verify topological differences are captured
  2. Compute similarity matrices using both RTD and output correlation, then analyze correlation between the two measures to understand their relationship
  3. Run quadratic optimization with synthetic similarity and quality values to verify weights adjust appropriately based on redundancy and individual performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of specific topological features (beyond RTD) affect ensemble performance and uncertainty estimation? The authors use RTD features but mention other features like barcodes and R-Cross-Barcodes in the methodology section. This remains unresolved because the paper only tests one specific topological feature (RTD) and does not explore whether other topological features might perform better. Comparative experiments testing multiple topological features against RTD and traditional correlation methods would resolve this.

### Open Question 2
How do different ensemble weighting optimization algorithms (beyond quadratic programming) compare in terms of accuracy and uncertainty estimation? The authors mention Bayesian optimization as an alternative but only implement quadratic programming. This remains unresolved because the paper only explores one optimization method despite acknowledging others exist. Systematic comparison of multiple optimization algorithms on the same ensemble tasks would resolve this.

### Open Question 3
What is the impact of ensemble size on the effectiveness of topological similarity-based weighting? The authors test ensembles of size 5 and subsets of 2-4 models, but don't explore larger ensembles. This remains unresolved because the paper doesn't investigate whether the topological weighting approach scales effectively to larger ensembles. Experiments with varying ensemble sizes comparing topological weighting to equal weighting across different ensemble scales would resolve this.

## Limitations
- Experiments limited to two datasets (IMDB and CoLA) using only DistilBERT models, limiting generalizability
- RTD feature extraction implementation details are not fully specified, making exact reproduction challenging
- Computational complexity of TDA-based similarity measures may limit scalability to larger ensembles

## Confidence
- **High**: The quadratic programming framework for weight optimization is well-established and correctly applied
- **Medium**: The improvement over equal weighting and output correlation ensembles is demonstrated but may be dataset-specific
- **Medium**: The claim about combining strong and weak models requires more extensive ablation studies

## Next Checks
1. Implement the RTD feature extraction independently and verify that similarity measures capture meaningful topological differences between attention layers
2. Conduct experiments on additional datasets and model architectures to test generalizability of the TDA-based weighting approach
3. Perform ablation studies varying the number of models in the ensemble to understand scalability and diminishing returns