---
ver: rpa2
title: 'MCS-SQL: Leveraging Multiple Prompts and Multiple-Choice Selection For Text-to-SQL
  Generation'
arxiv_id: '2405.07467'
source_url: https://arxiv.org/abs/2405.07467
tags:
- question
- schema
- prompts
- molecule
- bond
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of improving the performance
  of large language models (LLMs) in text-to-SQL tasks, particularly on complex benchmarks
  like BIRD. The core method idea involves leveraging multiple prompts to generate
  diverse candidate SQL queries and then selecting the optimal query through a multiple-choice
  selection process.
---

# MCS-SQL: Leveraging Multiple Prompts and Multiple-Choice Selection For Text-to-SQL Generation

## Quick Facts
- arXiv ID: 2405.07467
- Source URL: https://arxiv.org/abs/2405.07467
- Authors: Dongjun Lee; Choongwon Park; Jaehyuk Kim; Heesoo Park
- Reference count: 24
- Primary result: Establishes new state-of-the-art performance on BIRD benchmark with 65.5% execution accuracy

## Executive Summary
This paper addresses the challenge of improving large language model (LLM) performance in text-to-SQL tasks, particularly on complex benchmarks like BIRD. The authors propose MCS-SQL, a method that leverages multiple prompts to generate diverse candidate SQL queries and employs multiple-choice selection to identify the optimal query. By mitigating LLM sensitivity to prompt variations and effectively aggregating diverse responses, the approach achieves significant improvements over existing in-context learning methods, establishing new state-of-the-art performance on challenging text-to-SQL benchmarks.

## Method Summary
The proposed MCS-SQL method operates in three stages: (1) robust schema linking using multiple prompts with shuffled table/column order and union aggregation of responses, (2) generation of multiple candidate SQL queries through dynamic few-shot example selection and high-temperature sampling across various prompts, and (3) filtering and selection of the best query using confidence scores based on execution result agreement combined with LLM-based multiple-choice selection. The approach specifically addresses the sensitivity of LLMs to prompt variations by exploring a broader search space and aggregating diverse responses.

## Key Results
- Achieved 65.5% execution accuracy on BIRD benchmark, establishing new state-of-the-art performance
- Achieved 89.6% execution accuracy on Spider benchmark, significantly outperforming previous in-context learning methods
- Multiple-choice selection with confidence-based filtering improved accuracy by 0.6% on BIRD and 0.3% on Spider compared to majority voting

## Why This Works (Mechanism)

### Mechanism 1
LLM sensitivity to prompt variations significantly impacts text-to-SQL generation quality. By using multiple prompts with shuffled schema presentation and varying few-shot examples, the method captures diverse SQL generation paths that a single prompt might miss. The union aggregation of responses ensures comprehensive coverage of necessary schema elements while reducing the impact of individual prompt sensitivity.

### Mechanism 2
Schema linking is critical for accurate SQL generation, and missing even a single necessary table or column prevents generation of the correct query. The multiple-prompt approach improves recall by generating diverse schema presentations and aggregating responses through union operations. This ensures all necessary schema elements are captured even if individual prompts miss some components.

### Mechanism 3
Confidence-based filtering combined with multiple-choice selection effectively discriminates between candidate SQL queries. Queries are filtered by execution result agreement frequencies, and the remaining candidates are ordered by confidence scores before being presented to an LLM for final selection. This approach outperforms simple majority voting by leveraging execution semantics to identify the most reliable queries.

## Foundational Learning

- **In-context learning (ICL) for text-to-SQL**: Understanding how LLMs generate SQL from examples without fine-tuning is fundamental since the paper builds on ICL-based approaches. Quick check: What distinguishes ICL-based text-to-SQL from fine-tuning approaches, and why did ICL show better performance in the cited work?

- **Schema linking in text-to-SQL**: Schema linking identifies relevant database elements and is a core component where the paper's multiple-prompt approach specifically addresses robustness. Quick check: How does schema linking impact SQL generation accuracy, and why is high recall more important than precision in this context?

- **Confidence scoring based on execution result agreement**: The paper uses this technique to filter candidate SQL queries before final selection, which is a novel approach not commonly seen in text-to-SQL literature. Quick check: How is confidence calculated in this approach, and why might queries with the same execution result have different confidence scores?

## Architecture Onboarding

- **Component map**: Schema Linking → Multiple SQL Generation → Selection
- **Critical path**: Each stage depends on successful completion of the previous stage; failure in schema linking propagates to later stages
- **Design tradeoffs**: Multiple prompts vs. computational cost (more prompts improve coverage but increase API calls), high sampling temperature vs. quality (increases diversity but may introduce low-quality candidates), confidence threshold selection (too high filters out correct queries, too low includes too many candidates)
- **Failure signatures**: Low schema linking recall (missing necessary tables/columns), poor MCS performance (confidence filtering doesn't improve accuracy), performance degradation on complex queries
- **First 3 experiments**:
  1. Compare single-prompt vs. multi-prompt schema linking recall on BIRD development set
  2. Test different confidence threshold values (0.1, 0.2, 0.3) on SQL selection accuracy
  3. Evaluate few-shot example selection strategies (question similarity vs. masked question similarity) on SQL generation quality

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important questions emerge from the research:

1. How does the performance of MCS-SQL change when using different LLMs, such as GPT-4 vs. Claude or open-source models?
2. What is the impact of varying the number of prompts (pt and pc) and responses (n) in the schema linking phase on the final SQL generation accuracy?
3. How does the proposed method handle ambiguous questions or those requiring complex reasoning beyond schema linking?

## Limitations

- Computational overhead is substantial due to multiple prompts and numerous candidate queries, though exact cost-benefit analysis is not provided
- Method effectiveness may diminish as LLM capabilities improve, particularly since GPT-4 already achieves high performance without schema linking on simpler benchmarks
- Approach appears particularly tailored to BIRD benchmark's complexity, raising questions about generalizability to other domains

## Confidence

**High Confidence**: The core observation that LLM outputs vary with prompt structure and that multiple prompts improve schema linking recall is well-supported by experimental evidence (Section 5.3). The effectiveness of confidence-based filtering in improving selection accuracy is demonstrated with statistically significant improvements on both BIRD and Spider benchmarks.

**Medium Confidence**: The overall approach's superiority on complex benchmarks like BIRD is convincing, but the specific contribution of each component (multi-prompt schema linking vs. MCS selection vs. confidence filtering) is not fully isolated through ablation studies. The choice of confidence threshold (0.2) appears somewhat arbitrary without sensitivity analysis.

**Low Confidence**: Claims about the method's scalability and applicability to extremely large databases (10,000+ tables) remain speculative, as only tested on benchmarks with limited schema sizes. The long-term robustness of the approach as LLM capabilities evolve is uncertain.

## Next Checks

1. **Component Ablation Study**: Systematically remove each major component (multi-prompt schema linking, confidence filtering, MCS selection) to quantify individual contributions to overall performance improvements.

2. **Scalability Testing**: Evaluate the method on synthetically generated benchmarks with progressively larger schema sizes (100, 1000, 10000+ tables) to identify performance degradation points and computational limits.

3. **Confidence Threshold Sensitivity**: Perform a grid search over confidence threshold values (0.1, 0.15, 0.2, 0.25, 0.3) and analyze the tradeoff between precision and recall in SQL selection across different query complexity levels.