---
ver: rpa2
title: Extracting Moore Machines from Transformers using Queries and Counterexamples
arxiv_id: '2410.06045'
source_url: https://arxiv.org/abs/2410.06045
tags:
- transformer
- state
- language
- moore
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method to extract Moore machines from transformers
  using an extension of the L algorithm. The approach involves querying a trained
  transformer and constructing a finite state automaton that simulates its behavior.
---

# Extracting Moore Machines from Transformers using Queries and Counterexamples

## Quick Facts
- arXiv ID: 2410.06045
- Source URL: https://arxiv.org/abs/2410.06045
- Reference count: 34
- The paper presents a method to extract Moore machines from transformers using an extension of the L* algorithm

## Executive Summary
This paper introduces a method to extract Moore machines from transformers trained on regular languages by extending the L* algorithm. The approach treats transformer pre-embedding activations as a continuous state space and uses queries and counterexamples to construct finite state automata that simulate transformer behavior. Experiments demonstrate that the extracted automata accurately model transformers trained on regular languages, achieving high prediction accuracy and generalizing beyond training lengths. The work reveals important insights about transformer learning behavior, including that transformers may achieve high sequence accuracy while learning incorrect languages when trained on only positive examples.

## Method Summary
The method extends the L* algorithm to extract Moore machines from transformers by treating pre-embedding activations as state representations. The process involves querying a trained transformer to populate an observation table, checking for consistency and closure, proposing a Moore machine, and verifying equivalence through activation space partitioning. The approach uses a decision tree with SVM nodes to discretize the continuous activation space and resolve equivalence queries. Transformers are trained on regular languages using a one-layer encoder-only architecture with soft attention, pre-norm layer normalization, and rotary positional encodings.

## Key Results
- Extracted automata accurately model transformers trained on regular languages with high prediction accuracy
- The method successfully generalizes to sequences beyond training lengths
- Transformers trained on only positive examples do not model garbage states, yet can achieve high sequence accuracy by learning incorrect languages
- The approach reveals that sequence accuracy can be high even when the transformer learns the wrong language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The L* algorithm can extract a Moore machine from a transformer by treating the transformer's pre-embedding activations as a state space.
- Mechanism: The transformer's hidden state at each position is used to represent automaton states. Queries to the transformer populate an observation table, which is closed and consistent to build a Moore machine. The equivalence query is resolved by checking against a discrete partition of the activation space.
- Core assumption: The transformer's pre-embedding activations form a continuous state space that simulates the generalized transition function of a Moore machine.
- Evidence anchors:
  - [abstract] "we construct finite state automata as high-level abstractions of transformers trained on regular languages using queries and counterexamples"
  - [section 4.1] "we propose using the activations just before unembedding"
  - [corpus] Weak: No direct evidence found about using pre-embedding activations for Moore machine extraction
- Break condition: If the transformer's attention mechanism or residual connections introduce state that cannot be captured by a static activation snapshot, the extraction fails.

### Mechanism 2
- Claim: Transformers trained on regular languages learn to simulate state machines even without explicit recurrent connections.
- Mechanism: The transformer's attention mechanism allows each position to attend to all previous positions, effectively modeling the generalized transition function by propagating state information through the sequence.
- Core assumption: The attention mechanism can simulate the effect of recurrent state transitions through direct token-to-token dependencies.
- Evidence anchors:
  - [section 4.1] "for a transformer, this cannot hold as there are no recurrent connections. Instead, it is more accurate to think of the transformer's forward projection from a sequence to the chosen state space as simulating the generalized transition function"
  - [section 6.2] "The other languages are learnt poorly... This is because of their garbage state"
  - [corpus] Weak: No direct evidence found about attention mechanisms simulating state transitions
- Break condition: If the transformer cannot model the required state dependencies through attention alone, the extraction will fail.

### Mechanism 3
- Claim: Sequence accuracy can be high even when the transformer learns the wrong language.
- Mechanism: In character prediction tasks, a sequence is considered accepted if all symbols are predicted valid and the end-of-sequence symbol is valid. This allows the transformer to accept all training sequences without modeling garbage states.
- Core assumption: The sequence accuracy metric only checks validity of symbols in training sequences, not whether the transformer correctly models rejection states.
- Evidence anchors:
  - [section 6.3] "transformers can achieve perfect sequence accuracy by correctly labelling non-garbage states without explicitly modelling the garbage state"
  - [section 6.3] "The dataset is generated by traversing the target DFA at random after removing the garbage state"
  - [corpus] No evidence found about sequence accuracy and garbage states
- Break condition: If the evaluation metric includes checking for correct rejection behavior, this mechanism breaks down.

## Foundational Learning

- Concept: Moore machines as state machines with output functions
  - Why needed here: The extraction method recovers Moore machines, not just DFAs, to handle different training tasks
  - Quick check question: What distinguishes a Moore machine from a DFA in terms of output?

- Concept: L* algorithm for learning DFAs/Moore machines
  - Why needed here: The extraction method extends L* to learn Moore machines from queries and counterexamples
  - Quick check question: How does the L* algorithm use membership and equivalence queries?

- Concept: Star-free regular languages
  - Why needed here: The experiments use star-free languages like First and Ones to test the extraction method
  - Quick check question: What makes a regular language star-free?

## Architecture Onboarding

- Component map:
  - Transformer -> Pre-embedding activations -> Observation table -> Moore machine
  - Partitioner -> Decision tree with SVM nodes -> Discretized activation space

- Critical path:
  1. Train transformer on target language
  2. Query transformer to populate observation table
  3. Check consistency and closure of table
  4. Propose Moore machine and check equivalence
  5. Refine partitioning if needed and repeat

- Design tradeoffs:
  - Using pre-embedding activations provides direct access to state space but may miss some attention-based state information
  - Partitioning activation space discretizes continuous state but introduces approximation error
  - Time limits on extraction prevent infinite loops but may return incomplete automata

- Failure signatures:
  - Large automata with high extraction time indicate the transformer hasn't generalized well
  - Multiple states mapping to same partition suggest poor state separation
  - Low accuracy on test sequences indicates the extracted automaton doesn't match transformer behavior

- First 3 experiments:
  1. Train transformer on Ones language and extract Moore machine to verify basic functionality
  2. Train on G2 language and check extraction accuracy and generalization
  3. Train on Parity language and observe extraction behavior for non-learnable languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different transformer architectures (e.g., multi-layer, different attention mechanisms) affect the learnability and extractability of Moore machines from transformers?
- Basis in paper: [inferred] The paper mentions that small changes in transformer architecture can significantly impact what languages can or cannot be expressed, and uses a specific subset of transformer configurations for experiments.
- Why unresolved: The experiments were limited to a specific subset of transformer configurations, and the paper does not explore how different architectures might affect the extraction process or the learned automata.
- What evidence would resolve it: Experiments comparing the extraction process and resulting automata across different transformer architectures (e.g., varying depth, attention mechanisms, positional encodings) on the same languages.

### Open Question 2
- Question: Can the extraction method be extended to handle non-regular languages or more complex formal languages?
- Basis in paper: [explicit] The paper focuses on regular languages and mentions that transformers can express more complex languages, but does not explore extraction beyond regular languages.
- Why unresolved: The L* algorithm and its extensions used in the paper are designed for regular languages, and it is unclear how they would generalize to more complex languages.
- What evidence would resolve it: Successful application of the extraction method to transformers trained on non-regular languages, along with analysis of the resulting automata or other abstractions.

### Open Question 3
- Question: How does the choice of initial split depth and other hyperparameters in the extraction algorithm affect the quality and efficiency of the extracted Moore machines?
- Basis in paper: [explicit] The paper mentions the use of an aggressive initial splitting strategy and the initial split depth as a hyperparameter, but does not explore the impact of different values.
- Why unresolved: The paper uses a fixed initial split depth of 10 and does not explore how different values might affect the extraction process or the resulting automata.
- What evidence would resolve it: Experiments varying the initial split depth and other hyperparameters, measuring their impact on extraction time, automaton size, and fidelity to the transformer's behavior.

### Open Question 4
- Question: Can the extraction method be used to identify specific patterns or biases in transformer behavior beyond just the learned language?
- Basis in paper: [explicit] The paper discusses how transformers do not model garbage states when trained on only positive examples, revealing a bias in transformer behavior.
- Why unresolved: The paper focuses on extracting the learned language, but does not explore whether the method can be used to identify other patterns or biases in transformer behavior.
- What evidence would resolve it: Application of the extraction method to transformers trained on different tasks or datasets, with analysis of the resulting automata to identify patterns or biases beyond the learned language.

## Limitations

- The method relies on the assumption that pre-embedding activations form a continuous state space, which may not hold for more complex transformer architectures
- Activation space partitioning introduces approximation errors that could affect accuracy, particularly for languages with subtle state distinctions
- Experiments are limited to simple regular languages and small transformer architectures, raising questions about scalability

## Confidence

**High Confidence:** The core claim that Moore machines can be extracted from transformers trained on regular languages is well-supported by the experimental results. The method successfully extracts automata that accurately model transformer behavior on test sequences beyond training lengths.

**Medium Confidence:** The mechanism by which transformers learn to simulate state machines through attention mechanisms is plausible but not definitively proven. The paper provides theoretical justification but lacks direct evidence of how attention weights encode state transitions.

**Medium Confidence:** The finding that transformers can achieve high sequence accuracy while learning the wrong language is compelling, but the analysis could be more rigorous. The paper shows this occurs for some languages but doesn't systematically explore when and why this happens.

## Next Checks

1. **Stress Test with Complex Languages:** Apply the extraction method to transformers trained on more complex regular languages (e.g., languages requiring many states) to evaluate whether the approach scales and maintains accuracy. This would test the limits of the activation space partitioning and equivalence query resolution.

2. **Multi-layer Transformer Analysis:** Extend the extraction to transformers with multiple layers to determine if pre-embedding activations still provide sufficient state information. Compare extracted automata with single-layer and multi-layer transformers trained on the same languages.

3. **Garbage State Learning Verification:** Systematically test whether transformers trained on balanced datasets (containing both positive and negative examples) learn garbage states correctly. Compare extracted automata from transformers trained with and without negative examples to verify the garbage state learning mechanism.