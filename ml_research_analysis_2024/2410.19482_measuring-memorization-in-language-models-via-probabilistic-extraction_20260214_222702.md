---
ver: rpa2
title: Measuring memorization in language models via probabilistic extraction
arxiv_id: '2410.19482'
source_url: https://arxiv.org/abs/2410.19482
tags:
- extraction
- sampling
- memorization
- discoverable
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a probabilistic relaxation of discoverable
  extraction to better measure memorization in large language models (LLMs). The method
  quantifies the probability of extracting a target sequence after multiple queries,
  using various sampling schemes beyond greedy sampling.
---

# Measuring memorization in language models via probabilistic extraction

## Quick Facts
- arXiv ID: 2410.19482
- Source URL: https://arxiv.org/abs/2410.19482
- Reference count: 40
- Key outcome: Probabilistic extraction reveals higher memorization rates than traditional methods, particularly for larger models and repeated data

## Executive Summary
This paper introduces a probabilistic relaxation of discoverable extraction to better measure memorization in large language models. Traditional discoverable extraction only checks if a target sequence can be extracted via greedy sampling in a single query, providing a binary yes/no answer. The authors propose a probabilistic measure that captures the likelihood of extracting a target sequence after multiple queries using various sampling schemes beyond greedy sampling. Experiments across different model sizes, sampling schemes, and training data repetitions show that this probabilistic measure reveals higher memorization rates than traditional discoverable extraction, particularly for larger models and repeated data. The approach provides more nuanced information about extraction risk and better aligns with realistic user interactions with LLMs.

## Method Summary
The paper relaxes the discoverable extraction definition by measuring the probability of extracting a target sequence through multiple queries using various sampling schemes. For a target sequence, the method first identifies the prefix that maximizes the probability of generating the target suffix. It then calculates the probability of generating the target suffix from this prefix under different sampling schemes (temperature-based, top-k, top-p). The extraction probability is computed as 1 - (1 - p_z)^n, where p_z is the single-query probability and n is the number of queries. The method evaluates this probability across different values of n and various sampling schemes to determine the parameters (n, p) at which extraction becomes feasible.

## Key Results
- Probabilistic extraction reveals significantly higher memorization rates than greedy sampling alone, with extraction probabilities often exceeding 0.5 after multiple queries
- Larger models show higher memorization rates under probabilistic extraction compared to smaller models, with the gap widening as model size increases
- Repeated data in training sets dramatically increases extraction probability, with even a single repetition substantially boosting memorization rates
- Temperature-based sampling with higher temperatures consistently yields higher extraction probabilities than greedy sampling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic extraction captures memorization missed by greedy sampling because some memorized sequences have high probability but are not the single most likely continuation.
- Mechanism: Under greedy sampling, only the highest probability token is selected at each step. If a memorized suffix has probability slightly below the maximum at any position, greedy sampling will diverge from the target. Probabilistic sampling with temperature > 0 allows selection of non-maximum tokens, enabling generation of the memorized sequence.
- Core assumption: The model assigns non-zero probability to memorized sequences even when they're not the single most likely continuation.
- Evidence anchors:
  - [abstract] "This definition yields a yes-or-no determination of whether extraction was successful with respect to a single query. Though efficient to compute, we show that this definition is unreliable because it does not account for non-determinism present in more realistic (non-greedy) sampling schemes"
  - [section 3] "In this example, all but one token in the target sequence is the most likely to be generated. At the 15th index (out of 50), the target suffix token is the second most likely causing greedy sampling to select the incorrect token"
- Break condition: If memorized sequences have probability distributions that are always sharply peaked on a single token sequence (very low entropy), probabilistic extraction provides no advantage over greedy sampling.

### Mechanism 2
- Claim: Multiple queries increase extraction probability for sequences that have non-zero probability but low single-query probability.
- Mechanism: For a target sequence with probability p_z per query, the probability of generating it at least once in n independent queries is 1 - (1 - p_z)^n. This compounds the extraction probability without changing the underlying model.
- Core assumption: Multiple independent queries are allowed and the model's probability distribution is stable across queries.
- Evidence anchors:
  - [section 3] "the probability of not generating z_k+1:2k in a single trial is 1 - p_z, and the probability of not generating z_k+1:2k in n independent draws from the sampling scheme is (1 - p_z)^n"
  - [section 5.1] "A user who extracts sensitive information after several tries could be just as successful at exploiting this information compared to if they'd gotten the correct sequence on the first try"
- Break condition: If queries are not independent (e.g., model has state that persists across queries) or if the model's probability distribution changes significantly between queries.

### Mechanism 3
- Claim: Probabilistic extraction better captures the actual risk faced by users who can choose their sampling scheme.
- Mechanism: Production LLMs allow users to select sampling temperature or scheme. The probabilistic measure evaluates extraction under various realistic sampling schemes that users might choose, rather than just greedy sampling which is often used for measurement but not in practice.
- Core assumption: Users in practice use non-greedy sampling schemes and can make multiple queries.
- Evidence anchors:
  - [abstract] "users may query the model multiple times. Extracting the secret even once out of multiple queries could be highly problematic"
  - [section 3] "users of production large language models are often free to decide which sampling scheme to use"
- Break condition: If all production users are constrained to use greedy sampling or if rate limiting prevents multiple queries.

## Foundational Learning

- Concept: Probability theory and independence of events
  - Why needed here: Understanding why multiple queries compound extraction probability requires understanding independent events and probability calculations
  - Quick check question: If a sequence has 10% chance of being generated in one query, what's the probability it appears in at least one of three independent queries?

- Concept: Sampling algorithms and their hyperparameters (temperature, top-k, top-p)
  - Why needed here: The method evaluates extraction under different sampling schemes, requiring understanding of how these schemes affect token selection
  - Quick check question: How does increasing temperature affect the probability distribution over tokens?

- Concept: Memorization vs learning in neural networks
  - Why needed here: The work addresses the specific problem of measuring memorization, which requires understanding what memorization means in the context of LLMs
  - Quick check question: What's the difference between a model that has learned a general pattern versus one that has memorized a specific training example?

## Architecture Onboarding

- Component map: Prefix ‚Üí Sampling scheme ‚Üí Sequence generation ‚Üí Probability calculation ‚Üí (ùëõ, ùëù) determination
- Critical path: Prefix ‚Üí Sampling scheme ‚Üí Sequence generation ‚Üí Probability calculation ‚Üí (ùëõ, ùëù) determination
- Design tradeoffs: Tradeoff between computational cost (more queries = more computation) and measurement accuracy; tradeoff between permissive parameters (higher risk detection) and practical constraints
- Failure signatures: Overestimation of memorization when p_z is misestimated; underestimation when queries are not truly independent; failure when sampling scheme implementation has bugs
- First 3 experiments:
  1. Verify the probability calculation: Generate n sequences with a known target probability and check if empirical extraction rate matches theoretical calculation
  2. Compare greedy vs probabilistic extraction on a small dataset with known memorization
  3. Test the effect of sampling temperature on extraction rates for a fixed target sequence

## Open Questions the Paper Calls Out
None

## Limitations
- The probabilistic extraction method assumes independence between queries, which may not hold in practice when models maintain internal state across queries
- The choice of sampling schemes and parameters significantly affects extraction probabilities, but the paper doesn't systematically explore parameter sensitivity
- The method measures extraction probability but doesn't quantify the practical harm or utility of the extracted information

## Confidence

**High Confidence:** The mathematical framework for probabilistic extraction (Section 3) is sound and well-justified. The probability calculations for multiple independent queries are correct, and the conceptual distinction between greedy and probabilistic sampling is clearly articulated.

**Medium Confidence:** The empirical findings showing higher memorization rates with probabilistic extraction compared to greedy extraction. While the experimental methodology is reasonable, the specific results depend on dataset characteristics, model implementations, and parameter choices that may not generalize across all scenarios.

**Low Confidence:** The claim that probabilistic extraction better captures "actual risk faced by users" in production systems. This assertion requires more evidence about real-world usage patterns, user behavior, and the practical implications of different memorization detection methods.

## Next Checks

1. **Independence validation experiment:** Test whether the independence assumption holds by measuring extraction probabilities when queries are performed with and without context preservation. Compare the compound probability formula against empirical results under both conditions to quantify the impact of stateful interactions.

2. **Parameter sensitivity analysis:** Systematically vary sampling parameters (temperature, top-k, top-p) across a wide range and measure how extraction probabilities change for the same target sequences. Identify which parameter ranges are most effective at detecting memorization and whether optimal parameters differ across model sizes or data types.

3. **Real-world exploitation simulation:** Design an experiment where human evaluators or automated systems attempt to recognize and extract useful information from model outputs generated using the probabilistic extraction method. Measure the actual utility of extracted sequences rather than just their probability of generation to validate whether higher extraction probability correlates with practical exploitability.