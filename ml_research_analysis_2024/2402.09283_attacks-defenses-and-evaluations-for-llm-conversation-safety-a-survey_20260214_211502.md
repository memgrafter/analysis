---
ver: rpa2
title: 'Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey'
arxiv_id: '2402.09283'
source_url: https://arxiv.org/abs/2402.09283
tags:
- language
- attacks
- safety
- llms
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively categorizes recent research on LLM
  conversation safety into attacks, defenses, and evaluations. It covers inference-time
  attacks using adversarial prompts (red-team attacks, template-based attacks with
  heuristic and optimization-based methods, and neural prompt-to-prompt attacks) and
  training-time attacks that modify model weights through data poisoning or backdoor
  attacks.
---

# Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey

## Quick Facts
- arXiv ID: 2402.09283
- Source URL: https://arxiv.org/abs/2402.09283
- Authors: Zhichen Dong; Zhanhui Zhou; Chao Yang; Jing Shao; Yu Qiao
- Reference count: 18
- This survey comprehensively categorizes recent research on LLM conversation safety into attacks, defenses, and evaluations

## Executive Summary
This survey provides a systematic overview of research on large language model (LLM) conversation safety, organizing the literature into three main categories: attacks, defenses, and evaluations. The authors present a comprehensive taxonomy of attack methods including red-team attacks, template-based attacks with heuristic and optimization-based approaches, and neural prompt-to-prompt attacks, as well as training-time attacks through data poisoning and backdoor techniques. The defense mechanisms are structured hierarchically into safety alignment, inference guidance, and input/output filters. The survey also reviews evaluation methods including safety datasets and metrics for measuring attack success rate, robustness, and efficiency, while highlighting vulnerabilities in both open-source and fine-tunable proprietary models.

## Method Summary
The survey synthesizes existing research on LLM conversation safety by systematically categorizing attacks, defenses, and evaluations from the current literature. The authors organized attacks into inference-time attacks (using adversarial prompts) and training-time attacks (modifying model weights), defenses into hierarchical layers of safety alignment, inference guidance, and filtering mechanisms, and evaluations into datasets and metrics for assessing safety. The methodology involves comprehensive literature review and classification of research approaches without conducting original experiments.

## Key Results
- LLM safety research is categorized into inference-time attacks (red-team, template-based, neural prompt-to-prompt) and training-time attacks (data poisoning, backdoor)
- Defenses are structured hierarchically: safety alignment (SFT, RLHF), inference guidance (system prompts, token selection), and input/output filters (rule-based, model-based)
- Evaluations include safety datasets covering toxicity, discrimination, privacy, and misinformation, with metrics like attack success rate and robustness

## Why This Works (Mechanism)
The survey works by providing a comprehensive framework that organizes the rapidly evolving field of LLM safety research into clear categories and subcategories. By systematically classifying attacks, defenses, and evaluations, the authors create a structured understanding of how adversaries can compromise LLM safety and how these vulnerabilities can be addressed. The hierarchical organization of defenses reflects the multi-layered nature of safety protection, while the categorization of attacks helps identify specific vulnerability vectors. This systematic approach enables researchers to understand the landscape, identify gaps, and develop more effective safety mechanisms.

## Foundational Learning
- **Adversarial Prompt Engineering**: Creating prompts that bypass safety filters - needed to understand how attackers manipulate LLMs to generate harmful content; quick check: test prompt variations against safety filters
- **Safety Alignment Techniques**: Methods like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) - needed to understand how models learn to avoid harmful outputs; quick check: evaluate alignment effectiveness across different attack types
- **Multi-Objective Reinforcement Learning**: Balancing safety with other objectives like helpfulness - needed to prevent over-conservative responses while maintaining safety; quick check: measure trade-off between safety and utility
- **Backdoor Attacks**: Inserting hidden triggers during training that cause specific behaviors - needed to understand persistent vulnerabilities in model weights; quick check: test model responses to known backdoor triggers
- **Evaluation Metrics**: Attack Success Rate (ASR), robustness, and efficiency measurements - needed to quantify safety performance and compare different approaches; quick check: calculate ASR across different attack categories

## Architecture Onboarding

### Component Map
Training Pipeline -> Model Weights -> Inference Layer -> Output Filter -> Safety Evaluation

### Critical Path
Training (data poisoning/backdoor) -> Model weights modification -> Inference-time attack execution -> Safety filter bypass -> Harmful output generation

### Design Tradeoffs
Safety vs. helpfulness trade-off: stricter safety measures may reduce model utility and conversational quality; computational overhead vs. safety coverage: more comprehensive filtering increases latency; open-source vulnerability vs. control: proprietary models offer better control but reduce transparency

### Failure Signatures
Successful attacks show: bypass of multiple safety layers, generation of harmful content in multi-turn contexts, persistence across different prompt formulations; defense failures show: false positives blocking benign content, evasion by adaptive adversaries, performance degradation in normal use

### Three First Experiments
1. Test red-team attack effectiveness against different safety alignment methods (SFT vs RLHF vs multi-objective RLHF)
2. Evaluate inference guidance techniques by measuring token selection changes and their impact on attack success rates
3. Compare rule-based versus model-based filtering performance across different attack categories and safety dataset types

## Open Questions the Paper Calls Out
Major uncertainties include potential biases in the categorization of attacks, defenses, and evaluations as the field rapidly evolves with new attack vectors emerging frequently. The survey may not capture the most recent developments or novel attack strategies. Additionally, the effectiveness of defenses may be overstated as many proposed methods have not been thoroughly tested against adaptive adversaries who can modify their attacks in response to defensive measures. The evaluation metrics discussed may not fully capture real-world impact of safety violations, particularly in complex, multi-turn conversations where context and intent are harder to assess.

## Limitations
- Potential biases in categorization due to rapidly evolving attack landscape
- Effectiveness of defenses may be overstated without testing against adaptive adversaries
- Evaluation metrics may not fully capture real-world safety violations in complex conversational contexts

## Confidence
- High confidence in taxonomy of attack types (red-team, template-based, neural prompt-to-prompt) as these are well-established in literature
- Medium confidence in effectiveness of defense mechanisms, particularly safety alignment and inference guidance, as real-world performance against adaptive attacks remains uncertain
- Medium confidence in evaluation metrics, as there is ongoing debate about which metrics best capture LLM safety in practical scenarios

## Next Checks
1. Test the robustness of proposed defense mechanisms against adaptive adversaries who can modify their attack strategies based on the specific defenses deployed
2. Conduct longitudinal studies to assess the effectiveness of safety measures over extended periods of use, particularly in multi-turn conversations where safety violations may accumulate
3. Develop and validate evaluation frameworks that better capture the nuanced nature of safety violations in real-world conversational contexts, including measures of context awareness and intent recognition