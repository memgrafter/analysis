---
ver: rpa2
title: 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM
  Agents'
arxiv_id: '2402.03610'
source_url: https://arxiv.org/abs/2402.03610
tags:
- action
- agents
- memory
- task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of leveraging past experiences
  for decision-making in complex environments using Large Language Models (LLMs) as
  agents. The proposed method, Retrieval-Augmented Planning (RAP), dynamically retrieves
  relevant past experiences based on the current context and situation to enhance
  agents' planning capabilities.
---

# RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents

## Quick Facts
- arXiv ID: 2402.03610
- Source URL: https://arxiv.org/abs/2402.03610
- Reference count: 35
- Key outcome: RAP achieves 33.6%, 13.0%, 18.2%, and 12.7% gain over ReAct on ALFWorld, Webshop, Franka Kitchen, and Meta World benchmarks respectively

## Executive Summary
This paper addresses the challenge of leveraging past experiences for decision-making in complex environments using Large Language Models as agents. The proposed Retrieval-Augmented Planning (RAP) framework dynamically retrieves relevant past experiences based on current context and situation to enhance agents' planning capabilities. RAP demonstrates versatility by excelling in both text-only and multimodal environments, achieving state-of-the-art performance in textual scenarios and notably enhancing multimodal LLM agents' performance for embodied tasks.

## Method Summary
RAP implements a memory-augmented planning framework that stores past experiences containing task information, overall plans, and agent trajectories. When faced with a new task, RAP retrieves the most relevant experiences based on similarity scores that adapt to environment type and retrieval key type. The Executor then generates actions through in-context learning using the retrieved experiences as examples. The framework is designed to work across both textual and multimodal environments by adjusting the retrieval key and similarity calculation mechanisms accordingly.

## Key Results
- Achieves 33.6% gain over ReAct on ALFWorld benchmark
- Achieves 13.0% gain over ReAct on Webshop benchmark
- Achieves 18.2% and 12.7% gains over ReAct on Franka Kitchen and Meta World benchmarks respectively

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RAP enhances decision-making by retrieving past experiences most similar to the current situation and using them as in-context examples for the LLM.
- **Mechanism:** The Retriever calculates a weighted similarity score combining task, plan, and retrieval key similarities. The most similar experiences are passed to the Executor, which generates actions via in-context learning.
- **Core assumption:** LLMs can effectively leverage analogies from retrieved past trajectories to inform current decision-making.
- **Evidence anchors:**
  - [abstract] "Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities."
  - [section 3.4] "The Retriever is designed to extract the most relevant memory logs to guide the agent's subsequent actions to complete the current task."
  - [corpus] Weak - related work focuses on retrieval-augmented generation but not specifically on planning with past experiences.

### Mechanism 2
- **Claim:** RAP is versatile and works across both text-only and multimodal environments by adapting the retrieval key and similarity calculation.
- **Mechanism:** In multimodal environments, the retrieval key corresponds to the agent's current visual observation. Similarity is calculated between current and logged visual trajectory observations. In textual environments, the retrieval key type determines whether to use action or observation similarity.
- **Core assumption:** The same framework can be adapted to different modalities by changing the retrieval key and similarity calculation.
- **Evidence anchors:**
  - [abstract] "RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks."
  - [section 3.4] "The similarity score between the retrieval key and the log trajectory is adaptive based on environment type and retrieval key type."
  - [corpus] Weak - related work focuses on retrieval-augmented generation but not specifically on multimodal environments.

### Mechanism 3
- **Claim:** RAP improves performance by storing both context and action-observation trajectories for each experience.
- **Mechanism:** The Memory stores logs containing task information, overall plan, and agent trajectory (plans, actions, observations). This comprehensive information allows for more effective retrieval and utilization of past experiences.
- **Core assumption:** Storing more detailed information about past experiences enables better retrieval and utilization.
- **Evidence anchors:**
  - [section 3.2] "For each log Li completing a task Ti in Hi steps, we record the task information Ti, the overall plan pi, the trajectory of the agent τLi including plans, actions, and observations sequences."
  - [section 4.1.1] "In RAPtrain, we use 1000 tasks from the provided training set, and run recursively with memory from successful tasks both from the training set and previous trials."
  - [corpus] Weak - related work focuses on retrieval-augmented generation but not specifically on storing comprehensive past experiences.

## Foundational Learning

- **Concept:** In-context learning
  - Why needed here: RAP relies on the LLM's ability to learn from examples provided in the prompt, which is the core mechanism for utilizing retrieved experiences.
  - Quick check question: Can the LLM effectively generate actions based on a few examples of past experiences provided in the prompt?

- **Concept:** Retrieval-augmented generation
  - Why needed here: RAP builds upon retrieval-augmented generation by adapting it for planning tasks, requiring an understanding of how to integrate retrieved information into the generation process.
  - Quick check question: Can the Retriever effectively identify and retrieve the most relevant past experiences based on the current context?

- **Concept:** Multimodal learning
  - Why needed here: RAP works in both text-only and multimodal environments, requiring an understanding of how to process and integrate information from different modalities.
  - Quick check question: Can the framework effectively adapt to different modalities by changing the retrieval key and similarity calculation?

## Architecture Onboarding

- **Component map:** Memory -> Reasoner -> Retriever -> Executor
- **Critical path:** Reasoner → Retriever → Executor (in a loop until task completion)
- **Design tradeoffs:**
  - Storing more detailed information in Memory improves retrieval but increases storage overhead
  - Adapting to different modalities increases versatility but may introduce complexity
  - Using in-context learning simplifies the architecture but may limit the amount of information that can be effectively utilized
- **Failure signatures:**
  - Low success rate in both text-only and multimodal environments
  - High variance in performance across different tasks or environments
  - Excessive storage or computation requirements
- **First 3 experiments:**
  1. Evaluate RAP on a simple text-based task (e.g., ALFWorld) and compare performance to a baseline (e.g., ReAct)
  2. Evaluate RAP on a multimodal task (e.g., Franka Kitchen) and compare performance to a baseline
  3. Analyze the impact of different similarity calculation methods on retrieval performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RAP handle situations where past experiences are not directly applicable to the current task?
- Basis in paper: [inferred] from the discussion on RAP's ability to flexibly utilize multimodal information and extract relevant components from experiences for the current task.
- Why unresolved: The paper does not provide specific details on how RAP deals with the absence of directly applicable past experiences or how it generalizes from less relevant experiences.
- What evidence would resolve it: Experimental results or theoretical analysis demonstrating RAP's performance when past experiences are only partially relevant or not directly applicable to the current task.

### Open Question 2
- Question: What is the impact of the number of stored experiences on RAP's performance, and is there an optimal size for the memory database?
- Basis in paper: [inferred] from the discussion on RAP storing past experiences in memory and retrieving them based on similarity to the current context.
- Why unresolved: The paper does not explore the relationship between the size of the memory database and the performance of RAP, nor does it suggest an optimal size for the memory database.
- What evidence would resolve it: Empirical studies varying the number of stored experiences and analyzing the impact on RAP's performance to identify an optimal memory size.

### Open Question 3
- Question: How does RAP's performance compare to other memory-augmented planning methods that use different retrieval strategies or memory representations?
- Basis in paper: [explicit] from the comparison of RAP's performance to previous methods like ReAct, Reflexion, and ADaPT.
- Why unresolved: While RAP is shown to outperform these methods, the paper does not compare RAP to other memory-augmented planning methods with different retrieval strategies or memory representations.
- What evidence would resolve it: Comparative studies between RAP and other memory-augmented planning methods with different retrieval strategies or memory representations, measuring performance across various benchmarks.

### Open Question 4
- Question: Can RAP be extended to handle continuous action spaces, or is it limited to discrete action spaces?
- Basis in paper: [inferred] from the discussion on RAP's application in textual and multimodal environments, which often involve discrete actions.
- Why unresolved: The paper does not address the potential extension of RAP to continuous action spaces, which are common in many real-world applications.
- What evidence would resolve it: Demonstrations of RAP's effectiveness in environments with continuous action spaces or theoretical analysis of its applicability to such scenarios.

### Open Question 5
- Question: How does RAP's memory retrieval mechanism scale with the complexity and diversity of tasks in more realistic, unstructured environments?
- Basis in paper: [inferred] from RAP's demonstrated effectiveness in structured benchmarks like ALFWorld, Webshop, Franka Kitchen, and Meta-World.
- Why unresolved: The paper does not explore RAP's performance in more complex, unstructured, or open-ended environments, which would better represent real-world scenarios.
- What evidence would resolve it: Empirical studies evaluating RAP's performance in more complex, unstructured, or open-ended environments, assessing its ability to scale with task complexity and diversity.

## Limitations

- The paper provides limited details on how similarity scores between current contexts and stored experiences are computed and validated
- The evaluation focuses on only two environments of each type (text-only and multimodal), which may not be representative of broader application domains
- The reliance on in-context learning introduces potential scalability limitations due to context window constraints of underlying LLMs

## Confidence

- **High Confidence:** The core architecture of RAP (Memory-Reasoner-Retriever-Executor) is clearly defined and the implementation details are sufficiently specified for reproduction. The reported performance gains over ReAct are well-documented with specific numerical improvements.
- **Medium Confidence:** The claims about RAP's versatility across different modalities are supported by experimental results, but the evaluation could be more comprehensive. The effectiveness of the similarity calculation mechanism is demonstrated but not extensively validated across diverse scenarios.
- **Low Confidence:** The scalability of RAP to larger, more complex environments and the long-term impact of memory accumulation are not adequately addressed in the paper.

## Next Checks

1. **Similarity Calculation Validation:** Conduct ablation studies to quantify the impact of different similarity calculation methods on retrieval performance, including testing with corrupted or irrelevant experiences to establish robustness.

2. **Memory Scaling Analysis:** Evaluate RAP's performance as memory size increases to identify potential saturation points or degradation in retrieval quality, and measure the computational overhead introduced by larger memory stores.

3. **Cross-Domain Transferability:** Test RAP's performance when trained on one domain and evaluated on a substantially different domain to assess its ability to generalize beyond the specific environments used in the reported experiments.