---
ver: rpa2
title: Are Anomaly Scores Telling the Whole Story? A Benchmark for Multilevel Anomaly
  Detection
arxiv_id: '2411.14515'
source_url: https://arxiv.org/abs/2411.14515
tags:
- anomaly
- level
- detection
- severity
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multilevel Anomaly Detection (MAD), a novel
  framework addressing the gap between anomaly detection and practical severity assessment.
  Unlike traditional binary AD, MAD incorporates severity levels, enabling more nuanced
  decision-making in real-world applications like medical diagnostics and industrial
  inspection.
---

# Are Anomaly Scores Telling the Whole Story? A Benchmark for Multilevel Anomaly Detection

## Quick Facts
- **arXiv ID:** 2411.14515
- **Source URL:** https://arxiv.org/abs/2411.14515
- **Reference count:** 40
- **Primary result:** MLLM-based models outperform conventional methods in multilevel anomaly detection with higher C-index and Kendall's Tau-b scores

## Executive Summary
This paper introduces Multilevel Anomaly Detection (MAD), a novel framework that extends traditional binary anomaly detection by incorporating severity levels. MAD addresses the gap between anomaly detection and practical severity assessment, enabling more nuanced decision-making in real-world applications like medical diagnostics and industrial inspection. The authors present MAD-Bench, a comprehensive benchmark designed to evaluate both anomaly detection performance and severity-aligned scoring across diverse domains. Experiments demonstrate that MLLM-based models significantly outperform conventional methods in severity alignment while maintaining strong detection capabilities.

## Method Summary
The MAD framework categorizes anomalies into multiple severity levels (L0 to Ln) and evaluates models on their ability to detect anomalies and assign severity-aligned scores. MAD-Bench includes six datasets spanning industrial inspection, medical imaging, and general anomaly detection. The evaluation compares conventional AD methods (reconstruction-based, knowledge distillation, memory-bank approaches) with MLLM-based models using few-shot prompting. Performance is measured using AUROC for binary detection, C-index for severity alignment, and Kendall's Tau-b for severity ranking.

## Key Results
- MLLM-based models consistently outperform conventional methods in severity alignment (C-index and Kendall's Tau-b)
- Conventional methods show strong binary detection performance but struggle with severity differentiation, especially in medical datasets
- The benchmark reveals domain-specific challenges, with medical imaging requiring more sophisticated reasoning than industrial inspection
- Area-based severity levels show stronger correlation with model performance than risk-based severity levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLLM-based models outperform conventional anomaly detection methods in aligning anomaly scores with severity levels due to their ability to leverage domain knowledge and multimodal reasoning.
- Mechanism: MLLMs integrate visual and textual understanding, allowing them to interpret the context of anomalies beyond simple feature deviation. This enables them to assign scores that reflect practical severity rather than just anomaly magnitude.
- Core assumption: The severity of an anomaly in real-world applications correlates with contextual and semantic understanding, not just pixel-level differences.
- Evidence anchors:
  - [abstract]: "MLLM-based models outperform conventional methods, achieving higher C-index and Kendall's Tau-b scores, particularly in severity alignment."
  - [section 5.2]: "Most MLLM-based models demonstrated consistently better performance than conventional methods across all datasets... they excelled in severity differentiation, as reflected in higher C and Ken values."
  - [corpus]: "Weak - the corpus papers do not directly address multimodal reasoning or domain knowledge integration in MAD."

### Mechanism 2
- Claim: The proposed MAD setting improves decision-making in real-world applications by providing a nuanced severity spectrum rather than a binary classification.
- Mechanism: By categorizing anomalies into multiple severity levels, MAD allows for prioritization of responses based on the potential impact or risk. This aligns with how experts assess anomalies in domains like medical diagnostics and industrial inspection.
- Core assumption: Real-world anomaly handling requires prioritization based on severity, not just detection.
- Evidence anchors:
  - [abstract]: "MAD incorporates severity levels, enabling more nuanced decision-making in real-world applications like medical diagnostics and industrial inspection."
  - [section 1]: "In a practical context, severity refers to the degree of potential impact or risk an anomaly poses to the system... the ability to differentiate between these levels of severity is important for effective decision-making."
  - [section 3.3]: "Multilevel anomaly detection has significant implications across real-world domains where distinguishing anomalies based on severity is essential for effective decision-making."

### Mechanism 3
- Claim: The MAD-Bench benchmark provides a comprehensive evaluation framework that assesses both anomaly detection performance and severity alignment, which is critical for developing practical severity-aware AD systems.
- Mechanism: MAD-Bench includes diverse datasets and evaluation metrics (AUROC, C-index, Kendall's Tau-b) that measure a model's ability to detect anomalies and align scores with severity levels. This comprehensive evaluation is necessary to ensure models are effective in real-world scenarios.
- Core assumption: Evaluating anomaly detection models requires metrics that capture both detection accuracy and severity alignment.
- Evidence anchors:
  - [abstract]: "MAD-Bench, a benchmark designed to evaluate both anomaly detection performance and severity-aligned scoring across diverse domains."
  - [section 4]: "Building on the MAD setting, we introduce a new benchmark named MAD-Bench, designed to evaluate models on both their anomaly detection capabilities and their effectiveness in assigning severity-aligned anomaly scores."

## Foundational Learning

- Concept: Anomaly detection fundamentals
  - Why needed here: Understanding the basics of anomaly detection is essential to grasp the significance of the MAD setting, which builds upon traditional binary AD.
  - Quick check question: What is the primary goal of traditional anomaly detection, and how does MAD extend this goal?

- Concept: Severity assessment in real-world applications
  - Why needed here: Comprehending how severity is determined and prioritized in practical scenarios (e.g., medical diagnostics, industrial inspection) is crucial for appreciating the value of MAD.
  - Quick check question: How does the concept of severity influence decision-making in industrial inspection, and why is this important for MAD?

- Concept: Multimodal learning and reasoning
  - Why needed here: Understanding how models integrate and reason with multiple data modalities (e.g., visual and textual) is key to grasping why MLLMs excel in MAD.
  - Quick check question: What advantage does multimodal reasoning provide in assigning severity-aligned anomaly scores, and how do MLLMs leverage this capability?

## Architecture Onboarding

- Component map:
  - MAD setting -> Defines problem space with multiple severity levels
  - MAD-Bench -> Benchmark framework for evaluation
  - Conventional baselines -> Traditional anomaly detection methods
  - MLLM-based baselines -> Multimodal models leveraging domain knowledge
  - Evaluation metrics -> AUROC, C-index, Kendall's Tau-b

- Critical path:
  1. Define severity levels for each dataset
  2. Implement anomaly detection models
  3. Assign severity-aligned scores using models
  4. Evaluate performance using MAD-Bench metrics

- Design tradeoffs:
  - Binary vs. multilevel detection: Binary is simpler but less nuanced; multilevel provides more information but is more complex
  - Conventional vs. MLLM models: Conventional models are faster but less accurate in severity alignment; MLLMs are more accurate but computationally intensive

- Failure signatures:
  - Poor severity alignment: Model assigns similar scores to anomalies of different severity
  - Over-reliance on spatial features: Model is biased toward larger anomalies
  - Sensitivity to input corruption: Model performance degrades significantly with noise or brightness changes

- First 3 experiments:
  1. Compare binary and multilevel detection performance on a simple dataset to understand the impact of severity levels
  2. Evaluate conventional and MLLM-based models on a dataset with well-defined severity levels to assess severity alignment
  3. Test model robustness to input corruption (e.g., brightness adjustment, noise addition) to identify failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a hybrid approach that combines the strengths of MLLMs and conventional AD methods to improve both detection accuracy and severity alignment?
- Basis in paper: [explicit] The paper identifies that MLLMs excel at aligning anomaly scores with severity levels through domain knowledge and contextual reasoning, while conventional methods perform better in binary detection, particularly for industrial inspection. The authors suggest that a hybrid approach, such as a two-stage framework where conventional models detect anomalies and MLLMs assign severity scores, could offer a robust and comprehensive solution.
- Why unresolved: While the paper suggests this direction, it does not explore or validate such hybrid approaches. The performance trade-offs and integration challenges between these methods remain unexplored.
- What evidence would resolve it: Experimental results comparing hybrid models against standalone MLLMs and conventional methods across diverse datasets, demonstrating improved performance in both binary detection and severity alignment.

### Open Question 2
- Question: What are the specific factors that make medical imaging datasets (e.g., DRD-MAD, Covid19-MAD) more challenging for conventional AD models compared to industrial inspection or novelty detection datasets?
- Basis in paper: [explicit] The paper notes that conventional models struggle significantly on the two medical datasets, achieving C scores around 65%, while performing reasonably well on other datasets with C scores above 80%. This suggests unique challenges in medical imaging that are not present in other domains.
- Why unresolved: The paper does not investigate the underlying reasons for this performance gap. Potential factors could include the complexity of medical images, the need for expert-level knowledge, or the subtle differences between severity levels in medical conditions.
- What evidence would resolve it: Detailed analysis comparing feature representations, severity differentiation capabilities, and domain knowledge requirements across medical and non-medical datasets to identify specific challenges.

### Open Question 3
- Question: How does the area of anomalous regions correlate with practical severity across different application domains, and can this correlation be leveraged to improve AD models?
- Basis in paper: [explicit] The paper finds that conventional models show a strong bias toward larger anomalous areas, with C values for area-based severity levels consistently exceeding those for risk-based severity levels. However, this bias may underestimate the severity of small but impactful anomalies.
- Why unresolved: While the paper identifies this bias, it does not explore whether this correlation between area and severity is domain-specific or how it could be used to improve model performance. The trade-offs between area-based and risk-based severity assessment are not fully explored.
- What evidence would resolve it: Comparative analysis of area-severity correlations across multiple domains, experiments with models that incorporate both area and risk-based severity assessments, and evaluation of model performance when area-based biases are corrected.

## Limitations
- MAD-Bench datasets may not fully represent real-world complexity and severity distributions
- MLLM performance heavily depends on prompt engineering quality, affecting reproducibility
- Computational cost and efficiency trade-offs between MLLMs and conventional methods are not thoroughly analyzed

## Confidence
- High confidence: MLLM models outperform conventional methods in severity alignment (C-index and Kendall's Tau-b)
- Medium confidence: MAD provides more practical value than binary AD for real-world applications
- Medium confidence: Benchmark effectively captures detection and severity alignment, though generalizability to new domains is uncertain

## Next Checks
1. Conduct ablation studies removing multimodal reasoning components from MLLMs to quantify their specific contribution to severity alignment performance
2. Test model performance across varying severity level distributions to assess robustness when real-world data doesn't follow benchmark patterns
3. Evaluate computational efficiency trade-offs by measuring inference time and resource requirements for both MLLM and conventional approaches across different hardware configurations