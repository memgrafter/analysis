---
ver: rpa2
title: 'Semformer: Transformer Language Models with Semantic Planning'
arxiv_id: '2409.11143'
source_url: https://arxiv.org/abs/2409.11143
tags:
- tokens
- uni00000013
- language
- semformer
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Semformer, a Transformer language model that
  explicitly incorporates semantic planning to address the Clever Hans cheat phenomenon
  in next-token prediction, where models rely on exposed ground-truth prefixes to
  spuriously fit future tokens rather than genuinely planning. Semformer appends trainable
  planning tokens to the input prefix and guides them to predict latent semantic representations
  of the target sequence, induced by an autoencoder, thereby encouraging lookahead
  and abstract reasoning.
---

# Semformer: Transformer Language Models with Semantic Planning

## Quick Facts
- arXiv ID: 2409.11143
- Source URL: https://arxiv.org/abs/2409.11143
- Authors: Yongjing Yin; Junran Ding; Kai Song; Yue Zhang
- Reference count: 22
- Key outcome: Near-perfect accuracy (>99%) on graph path-finding task, outperforming standard models by addressing Clever Hans cheat through semantic planning

## Executive Summary
Semformer is a Transformer language model that addresses the Clever Hans cheat phenomenon in next-token prediction by explicitly incorporating semantic planning. The model appends trainable planning tokens to the input prefix and guides them to predict latent semantic representations of the target sequence, induced by an autoencoder. This approach forces the model to look ahead and plan rather than relying solely on ground-truth prefixes. Semformer demonstrates significant improvements across multiple tasks including near-perfect accuracy on graph path-finding, better perplexity on language modeling, improved in-context learning, and enhanced summarization performance compared to standard Transformers.

## Method Summary
Semformer modifies the standard Transformer decoder by appending trainable planning tokens to the input prefix. These planning tokens are guided by an autoencoder to predict latent semantic representations of the subsequent target sequence. The autoencoder uses a bottleneck layer to create compressed representations of the target, which the planning tokens must predict through a representation prediction head. The model is trained with a combined loss function that includes next-token prediction, autoencoder reconstruction, and representation prediction. This forces the model to learn both better representations of the target sequence and the ability to look ahead and plan, addressing the shortcut learning that occurs when models rely on exposed ground-truth prefixes during training.

## Key Results
- Achieves near-perfect accuracy (>99%) on graph path-finding task, effectively mitigating shortcut learning
- Improves perplexity on language modeling tasks when pretrained from scratch on OpenWebText
- Shows enhanced performance on in-context learning tasks (SST-2, MRPC) and abstractive summarization (XSum, SAMSum, DialogSum)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semformer addresses Clever Hans cheat by forcing the model to predict latent semantic representations of future tokens before generating them.
- Mechanism: The model appends trainable planning tokens to the input prefix. These tokens are guided by an autoencoder to predict latent semantic representations of the subsequent target sequence. This forces the model to look ahead and plan rather than relying solely on the ground-truth prefix.
- Core assumption: The latent semantic representations induced by the autoencoder capture meaningful abstract information about the target sequence that can guide planning.
- Evidence anchors:
  - [abstract] "guiding the planning token representations to predict the latent semantic representations of the response, which are induced by an autoencoder"
  - [section] "The semantics of finding the response path is predicted by internal computation, with token output guided by the intended semantics"
- Break condition: If the autoencoder fails to produce meaningful latent representations that capture the essential semantic information of the target sequence, the planning tokens won't receive useful guidance and the model may revert to Clever Hans behavior.

### Mechanism 2
- Claim: The representation prediction loss creates a more efficient supervisory signal than standard next-token prediction for learning planning abilities.
- Mechanism: By requiring the model to predict compressed latent representations of the entire future sequence, Semformer provides a more global and abstract form of supervision compared to predicting individual tokens sequentially. This encourages the model to understand the overall structure and semantics of the target.
- Core assumption: Predicting compressed representations of future sequences is a more efficient way to teach planning than predicting individual tokens, because it forces the model to consider the global structure.
- Evidence anchors:
  - [abstract] "incorporates a sequence of planning tokens into the prefix, guiding the planning token representations to predict the latent semantic representations of the response"
  - [section] "We provide the plan tokens with generic supervision information, enabling them to serve as the function to compute a future plan before response generation"
- Break condition: If the representation prediction loss becomes too difficult relative to the model's capacity, it might not converge properly or might learn degenerate solutions that don't actually encode useful planning information.

### Mechanism 3
- Claim: Semformer's performance advantage comes from both improved representation learning and explicit lookahead capability.
- Mechanism: The combination of the autoencoder's compressed representations and the planning tokens creates a dual benefit: the model learns better representations of the target sequence (as evidenced by improved perplexity) while simultaneously developing the ability to look ahead and plan (as evidenced by the graph path-finding task).
- Core assumption: Better representation learning and lookahead capability are both necessary for improved performance on reasoning and planning tasks.
- Evidence anchors:
  - [abstract] "In a minimal planning task (i.e., graph path-finding), our model exhibits near-perfect performance and effectively mitigates shortcut learning"
  - [section] "Semformer results in improvements on perplexity evaluation, in-context learning, and fine-tuning on abstractive summarization"
- Break condition: If the model overfits to the representation prediction task without actually learning to use that information for planning, it may show good performance on representation metrics but fail on actual planning tasks.

## Foundational Learning

- Concept: Teacher forcing and Clever Hans cheat
  - Why needed here: Understanding the fundamental problem Semformer is trying to solve - the shortcut learning that occurs when models rely on ground-truth prefixes during training
  - Quick check question: What is the Clever Hans cheat phenomenon in next-token prediction, and why does it lead to poor generalization?

- Concept: Autoencoder latent representations
  - Why needed here: The autoencoder is central to Semformer's mechanism, providing compressed semantic representations that guide planning
  - Quick check question: How does an autoencoder with a bottleneck layer create useful latent representations for guiding future token prediction?

- Concept: Transformer attention mechanisms
  - Why needed here: Understanding how the planning tokens interact with the rest of the model through self-attention is crucial for understanding how the planning mechanism works
  - Quick check question: How do the planning tokens interact with other tokens through self-attention, and what role does this play in the planning process?

## Architecture Onboarding

- Component map:
  - Base language model (decoder-only Transformer)
  - Trainable planning tokens appended to input
  - Autoencoder with bottleneck layer for latent representation generation
  - Representation prediction head that maps language model outputs to predicted latents
  - Combined loss function (next-token prediction + autoencoder reconstruction + representation prediction)

- Critical path:
  1. Input sequence split into prefix and target
  2. Planning tokens appended to prefix
  3. Full sequence passed through language model
  4. Target sequence passed through autoencoder to generate target latents
  5. Language model representations of planning tokens passed through prediction head to generate predicted latents
  6. L2 loss computed between predicted and target latents
  7. Combined loss backpropagated through both language model and autoencoder

- Design tradeoffs:
  - Number of planning tokens vs. computational cost
  - Latent dimension size vs. expressiveness vs. overfitting
  - Coefficient α for representation prediction loss vs. balance between next-token and representation prediction
  - Sharing vs. separate parameters between encoder and language model

- Failure signatures:
  - Poor convergence on representation prediction loss indicates issues with autoencoder or representation prediction head
  - High perplexity with good representation prediction suggests overfitting to representation task
  - Low accuracy on planning tasks with good perplexity suggests representation prediction isn't effectively guiding planning
  - Planning tokens not attending to relevant parts of input indicates poor integration with attention mechanism

- First 3 experiments:
  1. Verify representation prediction works: Train with fixed random planning tokens and check if L2 loss decreases and if predicted latents correlate with target latents
  2. Test planning token integration: Visualize attention weights to see if planning tokens attend to relevant parts of input and if other tokens attend to planning tokens
  3. Ablation on α coefficient: Run with different α values (0.1, 1.0, 10.0) on graph path-finding task to find optimal balance between next-token and representation prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Semformer's performance scale with model size when moving from 125M to larger models (e.g., 1B+ parameters)?
- Basis in paper: [inferred] The paper acknowledges this as a future research direction, noting they only pretrained a 125M parameter model due to computational constraints.
- Why unresolved: The paper explicitly states that validation with larger models and corpora is needed to determine if Semformer maintains its advantages at scale.
- What evidence would resolve it: Experiments showing perplexity, in-context learning, and reasoning task performance comparisons between Semformer and standard Transformers across various model sizes (125M, 1B, 10B+ parameters) trained on the same data.

### Open Question 2
- Question: What is the theoretical relationship between semantic planning tokens and the reduction of Clever Hans cheat in teacher forcing?
- Basis in paper: [explicit] The paper mentions they "do not provide theoretical analysis to prove that the method can mitigate the bias in teacher forcing" as a limitation.
- Why unresolved: While empirical results show improvement, there's no formal proof or theoretical framework explaining why predicting latent semantic representations reduces shortcut learning.
- What evidence would resolve it: A mathematical analysis demonstrating how semantic planning tokens change the loss landscape or gradient flow to reduce reliance on ground-truth prefixes during training.

### Open Question 3
- Question: How does the performance of Semformer change when the coefficient α is tuned differently for different task types?
- Basis in paper: [explicit] The paper notes that while α=0.5 worked well for pretraining, "a larger coefficient of 0.5 is found inferior to a smaller one, i.e., 0.1" for SST-2 classification, suggesting task-dependent optimization is needed.
- Why unresolved: The paper only explored a limited range of α values and didn't systematically study how optimal α varies across different NLP tasks and dataset characteristics.
- What evidence would resolve it: Ablation studies across diverse task types (classification, generation, reasoning) showing optimal α ranges and whether a universal value exists.

## Limitations
- Lack of comprehensive ablation studies to isolate individual contributions of components
- Uncertainty about scalability to larger models and more diverse pretraining corpora
- Limited analysis of learned planning behavior and what planning tokens actually capture

## Confidence
**High Confidence Claims:**
- The Clever Hans cheat phenomenon is a real issue in next-token prediction that can lead to spurious correlations and poor generalization
- The representation prediction loss can improve perplexity on standard language modeling tasks
- Semformer achieves near-perfect accuracy on the graph path-finding task compared to baseline models

**Medium Confidence Claims:**
- The semantic planning mechanism is the primary driver of performance improvements across all tasks
- The autoencoder's latent representations are capturing meaningful semantic information that guides planning
- The improvements on in-context learning and summarization tasks are directly attributable to the semantic planning approach

**Low Confidence Claims:**
- The approach will scale effectively to larger models and more diverse pretraining corpora
- The learned planning behavior is genuinely abstract and generalizable across different types of planning tasks
- The method is superior to alternative approaches for addressing shortcut learning in language models

## Next Checks
1. **Ablation study on component contributions**: Systematically remove or modify individual components (planning tokens, autoencoder, representation prediction loss) to isolate their individual contributions to performance. This should include testing with random planning tokens, removing the representation prediction loss while keeping planning tokens, and using the autoencoder without planning tokens to predict representations of future tokens.

2. **Scaling experiment with varying model sizes**: Train Semformer with different parameter counts (125M, 350M, 760M, 1.3B) on the same pretraining corpus to evaluate how performance gains scale with model size. This will reveal whether the semantic planning mechanism provides proportional benefits across different scales and whether there are diminishing returns.

3. **Analysis of planning token behavior**: Conduct a detailed investigation of what the planning tokens learn during training, including visualization of their attention patterns, analysis of their activation distributions, and probing experiments to determine what semantic information they capture. This should include comparing planning tokens trained with and without the representation prediction objective to understand what each component contributes to their behavior.