---
ver: rpa2
title: 'Learning Syntax Without Planting Trees: Understanding Hierarchical Generalization
  in Transformers'
arxiv_id: '2404.16367'
source_url: https://arxiv.org/abs/2404.16367
tags:
- generalization
- hierarchical
- training
- language
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates why transformer language models trained
  on natural language data exhibit hierarchical generalization, even without explicit
  structural bias. The researchers explore whether training objectives influence this
  behavior and find that only the language modeling objective consistently leads to
  hierarchical generalization across five different tasks.
---

# Learning Syntax Without Planting Trees: Understanding Hierarchical Generalization in Transformers

## Quick Facts
- **arXiv ID**: 2404.16367
- **Source URL**: https://arxiv.org/abs/2404.16367
- **Reference count**: 40
- **Primary result**: Language modeling objective consistently leads to hierarchical generalization in transformers across five syntactic tasks, unlike other training objectives.

## Executive Summary
This study investigates why transformers trained on natural language data exhibit hierarchical generalization despite lacking explicit structural bias. The researchers find that only the language modeling objective consistently produces hierarchical generalization across five syntactic tasks, while other objectives like seq2seq and classification do not. They discover that transformers contain subnetworks implementing both hierarchical and linear generalization rules that coexist during training, and that hierarchical grammars have higher posterior probabilities than regular grammars when explaining diverse language data according to a Bayesian framework.

## Method Summary
The study uses synthetic datasets with controlled syntactic structures (question formation, tense reinflection, passivization, simple agreement, German question formation) to test five different training objectives on transformers. Models are trained from scratch with 8 attention heads and 512 embedding dimension, using Adam optimizer with learning rate 0.0001, batch size 8, for 300k steps. The researchers employ pruning experiments to discover subnetworks with different generalization behaviors and use a Bayesian framework with probabilistic grammars to compare hierarchical and regular grammars. They measure both in-distribution accuracy and generalization accuracy on test sets with ambiguous syntactic structures.

## Key Results
- Language modeling objective is the only training objective that consistently leads to hierarchical generalization across all five syntactic tasks
- Transformers contain subnetworks implementing both hierarchical and linear generalization rules that coexist during training
- Hierarchical grammars have higher posterior probabilities than regular grammars when explaining diverse language data according to Bayesian analysis

## Why This Works (Mechanism)

### Mechanism 1
Language modeling training objective acts as a source of inductive bias for hierarchical generalization in transformers. The full sequence generation task inherently benefits from understanding hierarchical relationships between words, making hierarchical grammars simpler explanations than linear ones for this task. Evidence shows only language modeling objective consistently produces hierarchical generalization across tasks.

### Mechanism 2
Transformers contain subnetworks implementing different generalization behaviors (hierarchical and linear rules) that coexist during training. Pruning experiments reveal these subnetworks persist even as the overall model shifts toward hierarchical behavior. The ambiguous training data creates conditions where multiple generalization strategies can be encoded simultaneously.

### Mechanism 3
Hierarchical grammars have higher posterior probability than regular grammars when explaining diverse language data. The Bayesian framework shows hierarchical grammars achieve better balance between goodness of fit and simplicity for diverse datasets, making them preferred explanations according to Occam's razor.

## Foundational Learning

- **Context-Free Grammars (CFGs) and Regular Grammars**: The study constructs these grammars to model hierarchical and linear rules respectively, and uses them to explain why transformers prefer hierarchical generalization. *Quick check*: Can you explain the key difference between CFG production rules (A → BC or A → a) and regular grammar production rules (A → bC or A → a)?

- **Bayesian Occam's Razor and Posterior Probability**: The study uses this framework to quantify the tradeoff between simplicity and goodness of fit for competing grammars, explaining transformers' preference for hierarchical generalization. *Quick check*: How does the posterior probability p(G|D) ∝ p(D|G) · p(G) balance between likelihood and prior probability?

- **Subnetwork Discovery through Pruning**: The study uses attention head pruning to reveal subnetworks implementing different generalization behaviors within the trained transformer. *Quick check*: What does it mean when a subnetwork achieves 100% generalization accuracy but the full network only achieves 30%?

## Architecture Onboarding

- **Component map**: Data generation → Model training with language modeling objective → Pruning experiments to find subnetworks → Bayesian grammar analysis → Correlation analysis between transformer behavior and grammar posteriors
- **Critical path**: Generate synthetic datasets → Train transformers from scratch → Evaluate on generalization sets → Apply pruning to discover subnetworks → Compute grammar posteriors → Analyze correlation
- **Design tradeoffs**: Language modeling objective requires more computation per training step (predicting all tokens) but provides stronger hierarchical bias. Seq2seq objectives are more efficient for tasks with clear input-output pairs but may not capture full sequence dependencies.
- **Failure signatures**: If transformers fail to show hierarchical generalization, check: (1) training objective is not language modeling, (2) training data lacks diversity, (3) model depth is too shallow, or (4) pruning hyperparameters are incorrect.
- **First 3 experiments**:
  1. Train transformer with language modeling objective on simple agreement task and measure main-verb accuracy on generalization set
  2. Apply Train-prune, Gen-prune, and Train\Gen-prune to discover subnetworks with different generalization behaviors
  3. Construct CFG and regular grammars for the dataset, compute posteriors, and compare with transformer generalization performance

## Open Questions the Paper Calls Out

### Open Question 1
Why does hierarchical generalization in transformers emerge only after achieving perfect in-distribution accuracy? The paper notes transformers typically obtain 100% in-distribution accuracy much earlier than achieving high generalization performance, but does not explain this delay mechanistically.

### Open Question 2
What specific architectural or algorithmic features of transformers enable hierarchical generalization, given that RNNs with the same training objective do not exhibit this behavior? The paper finds transformers with language modeling objective consistently generalize hierarchically while RNNs do not, suggesting unique transformer properties that remain unexplored.

### Open Question 3
How does the Bayesian framework for simplicity bias apply to more complex linguistic tasks beyond simple agreement? The paper applies the framework to simple agreement but notes the need for more complex frameworks like synchronous grammars for sentence transformation tasks.

## Limitations

- The synthetic nature of datasets may not fully capture natural language complexity
- Pruning-based subnetwork discovery relies on specific hyperparameters that may not reveal all mechanisms
- Bayesian grammar comparison assumes specific prior distributions that may not perfectly represent possible language structures

## Confidence

- **High Confidence**: Language modeling objective consistently produces hierarchical generalization across all five tasks
- **Medium Confidence**: Subnetwork coexistence mechanism supported by pruning experiments but depends on specific configurations
- **Medium Confidence**: Bayesian framework correlation methodologically sound but depends on grammar construction choices

## Next Checks

1. Test whether observed hierarchical generalization behavior transfers to more complex, naturally-occurring syntactic phenomena beyond controlled synthetic datasets

2. Validate subnetwork discovery results using different pruning approaches (magnitude-based, structured, iterative) to ensure findings are not artifacts of specific pruning hyperparameters

3. Systematically vary prior distributions and grammar parameterizations in Bayesian analysis to assess robustness of correlation between transformer behavior and hierarchical grammar posteriors