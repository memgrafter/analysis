---
ver: rpa2
title: Evaluating Pre-Training Bias on Severe Acute Respiratory Syndrome Dataset
arxiv_id: '2408.15398'
source_url: https://arxiv.org/abs/2408.15398
tags:
- bias
- data
- metrics
- region
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates pre-training bias in machine learning models
  using a severe acute respiratory syndrome (SRAG) dataset from Brazil's OpenDataSUS.
  The research evaluates three pre-training bias metrics (Class Imbalance, Kullback-Leibler
  Divergence, and Kolmogorov-Smirnov) across Brazil's five regions, focusing on protected
  attributes of sex and race.
---

# Evaluating Pre-Training Bias on Severe Acute Respiratory Syndrome Dataset

## Quick Facts
- arXiv ID: 2408.15398
- Source URL: https://arxiv.org/abs/2408.15398
- Reference count: 7
- Pre-training metrics showed low bias risk across Brazil's regions, with model performance remaining consistent across protected attribute classes despite regional demographic differences.

## Executive Summary
This study evaluates pre-training bias in machine learning models using a Severe Acute Respiratory Syndrome (SRAG) dataset from Brazil's OpenDataSUS system. The research investigates three pre-training bias metrics (Class Imbalance, Kullback-Leibler Divergence, and Kolmogorov-Smirnov) across Brazil's five regions, focusing on protected attributes of sex and race. A random forest model was trained on each region and tested across all regions to assess performance disparities. Results showed low bias risk according to most metrics, with consistent model performance across protected attribute classes. The study highlights the importance of using multiple metrics for bias assessment, as the Class Imbalance metric indicated potential bias not reflected in actual model predictions.

## Method Summary
The study uses SRAG datasets from 2021-2023 from OpenDataSUS, containing medical data with protected attributes (sex, race) and outcomes (ICU admission or vaccination status). Data is preprocessed by removing irrelevant features and split by Brazil's five regions. Three pre-training bias metrics (Class Imbalance, KL Divergence, Kolmogorov-Smirnov) are calculated for each regional split. Random Forest models with 300 estimators using Gini index are trained on each region separately. Models are then tested across all regions in a cross-regional evaluation, and performance metrics (accuracy, F1-score) are analyzed, particularly focusing on false positive/negative rates across protected attribute classes.

## Key Results
- All three pre-training bias metrics showed low bias risk across Brazil's regions
- Model performance remained consistent across protected attribute classes despite regional demographic differences
- Class Imbalance metric indicated potential bias not reflected in actual model predictions, demonstrating the importance of multi-metric assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training bias metrics can detect dataset-level demographic disparities before model training, allowing visualization-based early bias detection.
- Mechanism: The metrics quantify distributional differences between protected attribute groups in the dataset. By computing these values per region, researchers can identify where one demographic is underrepresented or has label distribution divergence, potentially signaling downstream model bias risk.
- Core assumption: Dataset-level distributional differences correlate with model performance disparities on protected groups.
- Evidence anchors:
  - [abstract] "evaluating pre-training metrics applied to the datasets before training, which might contribute to identifying potential harm before any effort is put into training"
  - [section 2.2] Definitions of CI, KL, KS metrics with formulas showing how they measure group disparities
  - [corpus] Weak corpus support: neighbor papers focus on SARS-CoV-2 and respiratory modeling, not bias metrics.

### Mechanism 2
- Claim: Cross-regional model testing exposes whether models generalize across demographic distributions.
- Mechanism: By training Random Forest models per region and evaluating them across all regions, the study tests whether regional demographic differences affect model performance uniformly across protected attribute classes.
- Core assumption: Regional demographic differences in Brazil create distinct data distributions that could impact model generalization.
- Evidence anchors:
  - [section 4.2] "We then apply the five models to the five regions, resulting in 25 values for each performance metric"
  - [section 5] Results showing consistent performance across protected attributes despite regional demographic differences
  - [corpus] No direct corpus support for cross-regional testing methodology.

### Mechanism 3
- Claim: Using multiple bias metrics together provides more robust bias detection than any single metric alone.
- Mechanism: Class Imbalance indicated potential bias not reflected in actual model predictions, while KL and KS showed low bias risk. This demonstrates that different metrics capture different aspects of bias, and using them together prevents false positives/negatives.
- Core assumption: Different bias metrics measure complementary aspects of dataset-level disparities.
- Evidence anchors:
  - [section 5] "For the Class Imbalance, it does not agree with the other two metrics, indicating a risk of bias that is not confirmed by the other metrics"
  - [section 6] "It does not mean, however, that if all the metrics values were high, we would see a discrepancy in the predicted values"
  - [corpus] No corpus evidence for multi-metric bias assessment.

## Foundational Learning

- Concept: Bias in machine learning
  - Why needed here: The entire study is predicated on understanding how demographic attributes in training data can lead to unfair model predictions
  - Quick check question: What is the difference between bias in data versus bias in algorithms, and which does this study focus on?

- Concept: Pre-training bias metrics
  - Why needed here: The study uses three specific metrics (Class Imbalance, KL Divergence, KS) to quantify bias risk before model training
  - Quick check question: How does each metric (CI, KL, KS) measure different aspects of demographic disparity in the dataset?

- Concept: Cross-validation across regions
  - Why needed here: The methodology involves training models on one region and testing on others to assess generalization across demographic distributions
  - Quick check question: Why might training a model on data from one Brazilian region and testing it on another region be useful for bias assessment?

## Architecture Onboarding

- Component map: Data collection -> Pre-processing -> Regional split -> Metric evaluation -> Model training (Random Forest) -> Cross-regional testing -> Performance analysis
- Critical path: Data collection -> Pre-processing -> Regional split -> Metric evaluation -> Model training -> Performance analysis
- Design tradeoffs: Using Random Forest (interpretable, robust) vs. more complex models that might better capture patterns; evaluating multiple metrics vs. focusing on one comprehensive metric
- Failure signatures: High metric values but no performance disparity (false positive bias detection); low metric values but actual bias in predictions (false negative bias detection); inconsistent performance across regions despite similar metrics
- First 3 experiments:
  1. Run all three bias metrics (CI, KL, KS) on each regional split to establish baseline bias scores
  2. Train Random Forest models on each region and evaluate performance within-region to establish baseline accuracy
  3. Perform cross-regional testing by applying each regional model to all other regions and compare performance across protected attribute classes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the Class Imbalance metric show potential bias that is not reflected in actual model predictions, and under what conditions might this discrepancy occur?
- Basis in paper: [explicit] The paper explicitly states that the Class Imbalance metric indicated potential bias that was not reflected in actual model predictions, highlighting the importance of using multiple metrics for bias assessment.
- Why unresolved: The paper observes this discrepancy but does not investigate the underlying reasons for why CI shows potential bias while other metrics and actual model performance do not reflect this.
- What evidence would resolve it: Comparative analysis of datasets with varying CI values but similar model performance, along with investigation of the mathematical relationship between CI and other metrics.

### Open Question 2
- Question: How does the effectiveness of Brazil's public health system in data collection across demographics contribute to the observed model performance stability across regions?
- Basis in paper: [explicit] The paper suggests that Brazil's public health system effectively collected data across different demographics, which may explain the stable model performance across protected attributes.
- Why unresolved: While the paper hypothesizes this explanation, it does not empirically validate whether the public health system's data collection practices directly influence model performance consistency.
- What evidence would resolve it: Comparative analysis of model performance using datasets from countries with varying public health system effectiveness, or detailed examination of data collection procedures across Brazilian regions.

### Open Question 3
- Question: What are the specific mechanisms by which pre-training metrics can fail to predict actual model bias in real-world scenarios?
- Basis in paper: [inferred] The paper demonstrates that high metric values do not necessarily translate to model bias in predictions, suggesting a disconnect between pre-training metrics and actual model behavior.
- Why unresolved: The paper identifies this limitation but does not explore the theoretical or practical reasons why pre-training metrics might not correlate with post-training model bias.
- What evidence would resolve it: Systematic testing of various pre-training metrics against post-training bias measures across multiple domains and datasets, identifying patterns of correlation or lack thereof.

## Limitations

- Pre-training metrics may not capture algorithmic bias that emerges during model training or inference
- Cross-regional testing assumes regional demographic differences are the primary source of bias, potentially overlooking other confounding factors
- The study uses only Random Forest models with fixed hyperparameters, limiting generalizability to other model architectures

## Confidence

- High Confidence: The methodology for calculating pre-training bias metrics (CI, KL Divergence, KS) is clearly specified and technically sound.
- Medium Confidence: The conclusion that Brazil's public health system effectively collected data across different demographics, based on stable model performance despite regional differences.
- Low Confidence: The assertion that using multiple metrics together provides robust bias detection, given that Class Imbalance produced a false positive not validated against actual model performance.

## Next Checks

1. **Ground truth validation**: Compare metric-based bias predictions against actual model performance disparities on held-out demographic subgroups to establish metric accuracy rates.
2. **Model architecture sensitivity**: Repeat the analysis using different model types (e.g., gradient boosting, neural networks) to determine if Random Forest's robustness masks biases that other architectures might reveal.
3. **Temporal bias assessment**: Extend the analysis to include pre-2021 data to evaluate whether bias patterns have changed over time, particularly given the pandemic context of the SRAG data.