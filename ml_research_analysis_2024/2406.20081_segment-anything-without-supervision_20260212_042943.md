---
ver: rpa2
title: Segment Anything without Supervision
arxiv_id: '2406.20081'
source_url: https://arxiv.org/abs/2406.20081
tags:
- unsam
- segmentation
- masks
- image
- sa-1b
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UnSAM enables segmentation of images without human supervision
  by using a divide-and-conquer strategy that leverages top-down clustering to extract
  initial instance/semantic-level masks, followed by bottom-up iterative merging to
  create hierarchical fine-grained pseudo masks. These unsupervised multi-granular
  masks are then used to train a promptable segmentation model.
---

# Segment Anything without Supervision

## Quick Facts
- arXiv ID: 2406.20081
- Source URL: https://arxiv.org/abs/2406.20081
- Reference count: 40
- UnSAM achieves 11% AR gain over state-of-the-art unsupervised segmentation and 6.7% AR gain on SA-1B when integrated with supervised data

## Executive Summary
UnSAM presents an unsupervised approach to image segmentation that leverages a divide-and-conquer strategy to discover hierarchical structures in visual scenes without human supervision. The method generates multi-granular pseudo masks through top-down clustering and bottom-up iterative merging, then trains a promptable segmentation model on these pseudo labels. Evaluated across seven popular datasets, UnSAM achieves competitive results with the fully-supervised Segment Anything Model (SAM), advancing unsupervised segmentation by 11% in average recall.

## Method Summary
UnSAM uses a two-stage divide-and-conquer pipeline: first, top-down clustering with CutLER extracts initial semantic and instance-level masks from raw images; second, bottom-up iterative merging refines these masks into hierarchical fine-grained segments by progressively merging semantically similar patches at multiple similarity thresholds. The resulting unsupervised pseudo masks train a promptable segmentation model (Mask2Former or Semantic-SAM), with optional self-training refinement and integration with supervised datasets (UnSAM+) to further improve performance.

## Key Results
- Achieves 11% improvement in average recall over state-of-the-art unsupervised segmentation methods
- Outperforms SAM by 6.7% in average recall and 3.9% in average precision when integrated with SA-1B (UnSAM+)
- Zero-shot performance on seven datasets (COCO, LVIS, SA-1B, ADE, Entity, PartImageNet, PACO) is competitive with fully-supervised SAM

## Why This Works (Mechanism)

### Mechanism 1
Divide-and-conquer strategy generates multi-granular hierarchical masks without supervision. Top-down clustering (CutLER/MaskCut) extracts initial semantic/instance-level masks, then bottom-up iterative merging refines these into finer-grained hierarchical segments by merging semantically similar patches at multiple similarity thresholds. Core assumption: visual scenes have natural hierarchical structure that can be uncovered by clustering at different granularity levels.

### Mechanism 2
Self-training with pseudo-masks improves model performance by cleaning noisy labels. UnSAM trains on pseudo-masks, then performs self-training by merging high-confidence mask predictions (above τself-train) back into the training set as new ground truth, iteratively refining mask quality. Core assumption: model predictions on noisy pseudo-masks will gradually converge to cleaner, more accurate masks through iterative refinement.

### Mechanism 3
Unsupervised pseudo-masks can supplement supervised datasets by capturing entities overlooked by human annotators. UnSAM+ merges SA-1B ground truth with unsupervised pseudo-masks using IoU thresholding, creating a richer dataset that includes both human-annotated and model-discovered entities. Core assumption: human annotators systematically miss certain entity types (small objects, fine-grained parts) that unsupervised methods can discover.

## Foundational Learning

- Concept: Normalized Cuts and spectral clustering for image segmentation
  - Why needed here: Forms the basis of the top-down clustering stage (MaskCut) that extracts initial instance/semantic masks from raw images
  - Quick check question: How does normalized cuts use the second smallest eigenvector to partition an image into foreground/background?

- Concept: Iterative merging and hierarchical clustering
  - Why needed here: Enables bottom-up refinement of coarse masks into multi-level hierarchical structures by merging similar patches at decreasing similarity thresholds
  - Quick check question: What happens to cluster features when two clusters are merged in the iterative process?

- Concept: Self-training and pseudo-label refinement
  - Why needed here: Improves model performance by iteratively incorporating high-confidence predictions as training data, cleaning up noisy pseudo-masks
  - Quick check question: Why is a confidence threshold (τself-train) necessary when incorporating model predictions back into training?

## Architecture Onboarding

- Component map: Image → DINO ViT encoder → Patch features → CutLER/MaskCut → Top-down clustering → Coarse semantic/instance masks → Iterative merging pipeline → Bottom-up refinement → Hierarchical fine-grained masks → Mask2Former/CascadePSP → Model training → UnSAM → (Optional: Merge with SA-1B ground truth → UnSAM+)

- Critical path:
  1. Image preprocessing and feature extraction
  2. Top-down clustering to generate initial masks
  3. Bottom-up iterative merging for hierarchical refinement
  4. Model training with pseudo-masks
  5. (Optional) Self-training refinement
  6. (Optional) Integration with supervised dataset

- Design tradeoffs:
  - Granularity vs. noise: More merging iterations produce finer masks but risk creating noisy, meaningless segments
  - Computation vs. quality: Larger patch sizes reduce computation but may miss fine details
  - Self-training iterations vs. overfitting: Too many iterations may cause model to memorize pseudo-mask noise

- Failure signatures:
  - Excessive mask fragmentation indicating too aggressive merging thresholds
  - Systematic omission of certain object types suggesting bias in clustering approach
  - Degraded performance after self-training indicating overfitting to noisy pseudo-labels

- First 3 experiments:
  1. Run divide-and-conquer pipeline on a single image and visualize masks at different hierarchy levels to verify hierarchical structure
  2. Train UnSAM on 100 images and evaluate AR on a small validation set to confirm basic functionality
  3. Test UnSAM+ by merging pseudo-masks with ground truth on 10 images and verify that new masks are correctly incorporated

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of UnSAM's pseudo-masks compare to human annotations in terms of capturing fine-grained details, and can this be quantitatively measured? The paper mentions qualitative improvements but lacks quantitative comparison of fine-grained details captured by UnSAM versus human annotations.

### Open Question 2
What is the impact of using different backbones (e.g., ViT vs. ResNet) on the performance of UnSAM, and how does this affect the trade-off between model size and segmentation accuracy? The paper uses ResNet-50 but doesn't provide detailed comparison of different backbone architectures.

### Open Question 3
How does UnSAM's performance generalize to other domains beyond natural images, such as medical imaging or satellite imagery? The paper evaluates primarily on natural images without exploring applicability to other domains.

### Open Question 4
What are the limitations of UnSAM in handling complex scenes with overlapping objects or occlusions, and how can these be addressed? The paper mentions some limitations but doesn't provide detailed analysis of handling overlapping objects or occlusions.

## Limitations
- Divide-and-conquer approach depends heavily on quality of initial top-down clustering masks, with unclear behavior when CutLER/MaskCut fails
- Iterative merging may create excessive fragmentation in complex scenes or overly coarse masks in uniform regions
- Self-training mechanism could potentially amplify errors if model becomes overconfident on incorrect predictions

## Confidence
- High confidence: The core divide-and-conquer pipeline and its implementation on SA-1B data
- Medium confidence: The quantitative improvements over baselines (11% AR gain, 6.7% AR gain on SA-1B)
- Medium confidence: The claim that UnSAM+ discovers entities missed by SAM, though qualitative evidence supports this
- Low confidence: The exact behavior of the iterative merging algorithm and its sensitivity to parameter choices

## Next Checks
1. Implement the iterative merging pipeline on 5-10 diverse images and systematically vary the similarity thresholds to quantify the trade-off between mask granularity and quality
2. Run UnSAM with self-training for 3-5 iterations and track AR/MaxIoU/OracleIoU on COCO validation to detect any degradation or overfitting patterns
3. Select 50 images from SA-1B where UnSAM+ discovers new entities not in ground truth, manually annotate these entities, and measure their frequency/size distribution to validate claims about systematic omission in human annotations