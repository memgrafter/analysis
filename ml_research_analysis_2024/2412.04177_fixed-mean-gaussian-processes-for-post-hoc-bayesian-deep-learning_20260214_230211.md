---
ver: rpa2
title: Fixed-Mean Gaussian Processes for Post-hoc Bayesian Deep Learning
arxiv_id: '2412.04177'
source_url: https://arxiv.org/abs/2412.04177
tags: []
core_contribution: This paper introduces Fixed-Mean Gaussian Processes (FMGPs), a
  novel family of sparse variational Gaussian processes that convert pre-trained deep
  neural networks into Bayesian models without compromising their accuracy. The key
  innovation is fixing the posterior mean of the GP to the output of the pre-trained
  DNN, allowing the model to learn only the predictive variances while preserving
  the original predictions.
---

# Fixed-Mean Gaussian Processes for Post-hoc Bayesian Deep Learning

## Quick Facts
- arXiv ID: 2412.04177
- Source URL: https://arxiv.org/abs/2412.04177
- Reference count: 40
- One-line primary result: FMGP achieves lower NLL (1.248 vs 1.248 for ELLA) and ECE (0.015 vs 0.025 for ELLA) on ImageNet while maintaining competitive training times

## Executive Summary
This paper introduces Fixed-Mean Gaussian Processes (FMGPs), a novel family of sparse variational Gaussian processes that convert pre-trained deep neural networks into Bayesian models without compromising their accuracy. The key innovation is fixing the posterior mean of the GP to the output of the pre-trained DNN, allowing the model to learn only the predictive variances while preserving the original predictions. The method uses variational inference with a decoupled basis to optimize inducing points and kernel hyperparameters, making it scalable to large networks and datasets.

## Method Summary
FMGP converts pre-trained DNNs into Bayesian models by fixing the GP's posterior mean to the DNN's outputs while learning only the predictive variances through variational inference. The method employs a decoupled basis parameterization that allows separate optimization of mean and variance parameters using different inducing points. A regularization technique with an extra variational Gaussian measure prevents overfitting by encouraging the model to account for training data under two predictive means. The approach scales to large datasets through sparse variational GP techniques while maintaining the pre-trained DNN's accuracy.

## Key Results
- On ImageNet with ResNet architectures, FMGP achieves NLL of 1.248 and ECE of 0.015, outperforming ELLA (NLL: 1.248, ECE: 0.025)
- On CIFAR10, FMGP provides better uncertainty estimates with NLL of 0.183 and ECE of 0.0096 compared to other post-hoc Bayesian methods
- In regression tasks on QM9 molecular property prediction, FMGP achieves NLL of -1.85, slightly outperforming linearized Laplace approximation (-1.78)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The fixed-mean GP (FMGP) preserves the pre-trained DNN's predictions while learning only the uncertainty.
- Mechanism: FMGP fixes the posterior mean of the GP to the output of the pre-trained DNN, allowing the model to learn only the predictive variances. This is achieved by using a universal kernel that can approximate any continuous function, ensuring the GP's mean matches the DNN's output.
- Core assumption: The pre-trained DNN's output is a continuous function that can be approximated by the GP's mean with arbitrary precision.
- Evidence anchors: [abstract]: "Specifically, we fix the mean of this GP to the output of the pre-trained DNN, allowing our approach to effectively fit the GP's predictive variances to estimate the DNN prediction uncertainty." [section III.D]: "By Proposition 5, it is clear that for any g ∈ C(Z), it is verified that Qg Z,ϵ ̸= ∅ and its corresponding set of FMGPs exists."

### Mechanism 2
- Claim: The use of a decoupled basis in the GP allows for separate optimization of the mean and variance parameters.
- Mechanism: The decoupled basis in the GP allows for the mean and variance to be parameterized using different sets of inducing points. This separation enables independent optimization of the mean and variance parameters, improving the efficiency and effectiveness of the learning process.
- Core assumption: The decoupled basis provides a valid parameterization of the GP that allows for separate optimization of the mean and variance.
- Evidence anchors: [section III.B]: "This decoupled parameterization is a clear generalization from standard SVGPs and cannot be obtained using the approach of [48] unless Zα = Zβ." [section III.C]: "As a result, given an error rate ϵ > 0, we can set the posterior mean of a decoupled GP to any continuous function in any compact set of the input space."

### Mechanism 3
- Claim: The regularization technique using an extra variational Gaussian measure prevents overfitting in the FMGP.
- Mechanism: The regularization technique introduces an extra variational Gaussian measure that shares parameters with the original measure but also incorporates additional parameters for its predictive mean. This encourages the model to account for training data under two predictive means, preventing overfitting.
- Core assumption: The regularization technique effectively balances the fit of the model to the training data with the complexity of the model.
- Evidence anchors: [section III.E]: "To address this, we introduce a regularization technique using an extra variational Gaussian measure." [section III.E]: "This loss function implies that the predictive variances must account for training data under two predictive means: the pre-trained one, g(·), and the one defined by a."

## Foundational Learning

- Concept: Universal Kernels
  - Why needed here: Universal kernels are essential for the FMGP to approximate any continuous function, ensuring the GP's mean matches the DNN's output.
  - Quick check question: What is the definition of a universal kernel, and how does it relate to the FMGP's ability to fix the mean to the DNN's output?

- Concept: Variational Inference
  - Why needed here: Variational inference is used to optimize the GP's predictive variances and kernel hyperparameters, making the FMGP scalable to large datasets.
  - Quick check question: How does variational inference enable efficient optimization of the FMGP's parameters, and what are the key steps in this process?

- Concept: Decoupled Basis
  - Why needed here: The decoupled basis allows for separate optimization of the mean and variance parameters, improving the efficiency and effectiveness of the FMGP.
  - Quick check question: What is the role of the decoupled basis in the FMGP, and how does it enable independent optimization of the mean and variance?

## Architecture Onboarding

- Component map: Pre-trained DNN -> Universal Kernel -> Decoupled Basis -> Variational Inference -> Regularization Technique -> FMGP
- Critical path:
  1. Pre-train the DNN on the target dataset.
  2. Choose a universal kernel and initialize the GP's parameters.
  3. Perform variational inference to optimize the GP's predictive variances and kernel hyperparameters.
  4. Use the FMGP to estimate uncertainty in the DNN's predictions.
- Design tradeoffs:
  - Kernel choice: More sophisticated kernels may improve performance but increase computational cost.
  - Number of inducing points: More inducing points may improve accuracy but increase training time.
  - Regularization strength: Stronger regularization may prevent overfitting but may also underfit the data.
- Failure signatures:
  - Poor uncertainty estimates: May indicate issues with the kernel choice or optimization process.
  - Overfitting: May indicate insufficient regularization or too many inducing points.
  - Underfitting: May indicate too strong regularization or too few inducing points.
- First 3 experiments:
  1. Validate the FMGP's ability to preserve the DNN's predictions on a simple regression task.
  2. Test the FMGP's performance on a classification task with a small dataset.
  3. Evaluate the FMGP's scalability on a large dataset with millions of instances.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section discusses areas for future work including systematic exploration of different kernel choices and extension to streaming/online learning scenarios.

## Limitations
- Limited empirical runtime comparisons against baselines, with scalability claims based primarily on theoretical O(N) complexity
- Regularization hyperparameter tuning requirements not extensively explored, potentially affecting reproducibility
- Improvement margin in regression tasks is relatively small (NLL -1.85 vs -1.78), suggesting domain-specific effectiveness

## Confidence
- High confidence in theoretical claims about universal kernel approximation and decoupled basis parameterization
- Medium-High confidence in ImageNet and CIFAR10 experimental results based on clear improvements over multiple baselines
- Medium confidence in regression results due to small improvement margins and limited comparison methods

## Next Checks
1. **Runtime Scaling Analysis**: Conduct systematic experiments measuring training time and memory usage as dataset size increases from 10K to 1M+ instances, comparing against exact scalability predictions.

2. **Kernel Sensitivity Study**: Evaluate FMGP performance across different kernel families (RBF, Matérn, spectral mixture) to quantify sensitivity to kernel choice and identify optimal configurations for different data types.

3. **Out-of-Distribution Robustness**: Test FMGP uncertainty estimates on intentionally corrupted datasets (Gaussian noise, adversarial examples) to validate the claimed superior OOD detection capabilities beyond the reported AUC metrics.