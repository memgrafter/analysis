---
ver: rpa2
title: Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features
arxiv_id: '2410.03558'
source_url: https://arxiv.org/abs/2410.03558
tags:
- diffusion
- activations
- should
- information
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies a significant gap in evaluating diffusion\
  \ model activations for discriminative tasks. While prior work focused on a limited\
  \ set of inter-module activations, this study reveals that many potentially useful\
  \ activations\u2014especially those within embedded ViT modules\u2014have been overlooked."
---

# Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features

## Quick Facts
- arXiv ID: 2410.03558
- Source URL: https://arxiv.org/abs/2410.03558
- Authors: Benyuan Meng; Qianqian Xu; Zitai Wang; Xiaochun Cao; Qingming Huang
- Reference count: 40
- Primary result: Achieves state-of-the-art 83.90 PCK@0.1, 45.71 mIoU, and 66.3 mIoU on semantic correspondence, semantic segmentation, and label-scarce segmentation respectively

## Executive Summary
This paper addresses a critical gap in evaluating diffusion model activations for discriminative tasks by systematically exploring activations within embedded ViT modules that have been overlooked in prior work. The authors identify three key properties of diffusion U-Nets—asymmetric diffusion noises, in-resolution granularity changes, and locality without positional embeddings—and develop a qualitative filtering approach to select high-quality activations. Through comprehensive experiments on three discriminative tasks, the proposed method demonstrates significant performance improvements over existing approaches while revealing important insights about the architecture and noise characteristics of modern diffusion models.

## Method Summary
The paper develops a feature selection methodology that extracts activations from both inter-module and embedded ViT module locations in diffusion U-Nets, then applies qualitative filtering based on three identified properties: asymmetric diffusion noise patterns, granularity changes within resolutions, and locality effects in self-attention. The approach focuses on SDv1.5 and SDXL models, extracting features from specific activation indices and training downstream models for semantic correspondence, semantic segmentation, and label-scarce segmentation tasks. The filtering process reduces the candidate pool while maintaining performance, with the final method achieving state-of-the-art results across all three tasks.

## Key Results
- Achieves 83.90 PCK@0.1 on semantic correspondence (SPair-71k), outperforming Legacy-v1.5 by 8.1 points
- Reaches 45.71 mIoU on semantic segmentation (ADE20K), exceeding Legacy-XL by 4.6 points
- Obtains 66.3 mIoU on label-scarce segmentation (Horse-21 subset of LSUN), surpassing prior methods by significant margins
- Reduces computational time by more than one week per (model, dataset) pair through effective qualitative filtering

## Why This Works (Mechanism)

### Mechanism 1
Selecting ViT module activations improves discriminative performance by capturing high-resolution spatial information absent from coarser inter-module features. Modern diffusion U-Nets have fewer resolutions but larger per-resolution modules, making granularity changes within resolutions significant. Embedded ViTs introduce detailed spatial activations that provide richer discriminative features for downstream tasks.

### Mechanism 2
Diffusion noises asymmetrically affect different parts of the U-Net, creating opportunities for quality-aware feature selection. The diffusion process introduces noise that impacts both high-frequency signals and uniquely low-frequency signals near inputs/outputs. This asymmetric pattern allows filtering out low-quality activations from noisy regions while preserving useful features from cleaner areas.

### Mechanism 3
Locality in self-attention activations can be exploited to suppress diffusion noises in high-resolution features. Self-attention locality focuses on spatial structures and can suppress severe diffusion noises, making noisy high-resolution activations usable when locality effects are present. This property enables extracting additional features from resolutions that would otherwise be too noisy.

## Foundational Learning

- Concept: Diffusion model forward and reverse processes
  - Why needed here: Understanding how noise is gradually added and removed is crucial for recognizing why diffusion noises appear in activations
  - Quick check question: What distinguishes the noise posterior q(xt|x0) from the predicted noise ϵθ(xt,t,c) in the denoising step?

- Concept: Residual connections and increment activations
  - Why needed here: The paper filters out increment activations because they introduce high-frequency noises; understanding this architecture is essential
  - Quick check question: In a residual block, what is the relationship between the residual activation and the increment activation?

- Concept: Vision Transformer attention mechanisms
  - Why needed here: The paper focuses on queries, keys, and values from self-attention and cross-attention layers; understanding these components is necessary
  - Quick check question: How do queries, keys, and values interact in a self-attention mechanism to produce output features?

## Architecture Onboarding

- Component map: Input → Down-stage (ResModules, ViTs, downsamplers) → Mid-stage → Up-stage (ResModules, ViTs, upsamplers) → Output
- Critical path: Input → Down-stage (resolution reduction) → Mid-stage → Up-stage (resolution expansion) → Output
- Design tradeoffs: Fewer resolutions with larger per-resolution modules vs. many resolutions with smaller modules; embedded ViTs vs. pure convolutional architectures
- Failure signatures: Poor discriminative performance when using noisy activations; missed performance gains when ignoring embedded ViT activations
- First 3 experiments:
  1. Extract and visualize activations from different U-Net components to observe noise patterns and granularity changes
  2. Implement qualitative filtering based on diffusion noise patterns and locality effects
  3. Conduct quantitative comparison of filtered activation subsets on a simple discriminative task (e.g., semantic correspondence on SPair-71k)

## Open Questions the Paper Calls Out

### Open Question 1
How do the identified properties of diffusion U-Nets generalize to newer diffusion model architectures like DiT models? The paper explicitly states uncertainty about whether observations can generalize to DiT models due to their markedly different architecture from U-Net-based diffusion models.

### Open Question 2
What is the optimal balance between feature selection comprehensiveness and computational efficiency for diffusion feature extraction? While the paper mentions significant time reductions through qualitative filtering, it doesn't systematically investigate how different filtering thresholds affect both performance and computational cost.

### Open Question 3
How do diffusion features perform on long-tail and out-of-distribution recognition tasks? The paper mentions this as a good future direction, noting that validation was conducted on standard datasets without testing on imbalanced or out-of-distribution scenarios.

## Limitations

- Theoretical claims about diffusion noise patterns and locality effects are supported by qualitative observations rather than rigorous mathematical proofs
- The exact contribution of each proposed filtering mechanism to final performance gains is not quantified through ablation studies
- Computational overhead of extracting and processing multiple activation types from embedded ViTs may limit practical applicability

## Confidence

- **High**: Empirical performance improvements on downstream tasks (83.90 PCK@0.1, 45.71 mIoU, 66.3 mIoU)
- **Medium**: Architectural insights about embedded ViT modules and granularity changes
- **Low**: Theoretical claims about diffusion noise patterns and locality-based noise suppression

## Next Checks

1. Test the feature selection approach on additional diffusion model architectures (e.g., non-U-Net variants) to verify generalizability of noise pattern observations
2. Conduct ablation studies isolating contributions of diffusion noise filtering versus locality-based selection to quantify individual impacts
3. Measure computational overhead and inference latency of the proposed method compared to baseline feature extraction approaches