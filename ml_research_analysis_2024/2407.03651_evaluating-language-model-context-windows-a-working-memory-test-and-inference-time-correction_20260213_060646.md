---
ver: rpa2
title: 'Evaluating Language Model Context Windows: A "Working Memory" Test and Inference-time
  Correction'
arxiv_id: '2407.03651'
source_url: https://arxiv.org/abs/2407.03651
tags:
- context
- documents
- long
- document
- effect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SWiM, a framework for evaluating long-context
  language models on real-world document processing tasks. It addresses limitations
  in existing benchmarks by testing models on user-specific documents and tasks, revealing
  the "lost-in-the-middle" effect where models struggle with information in the middle
  of long contexts.
---

# Evaluating Language Model Context Windows: A "Working Memory" Test and Inference-time Correction

## Quick Facts
- **arXiv ID**: 2407.03651
- **Source URL**: https://arxiv.org/abs/2407.03651
- **Reference count**: 13
- **Primary result**: Introduces SWiM framework revealing "lost-in-the-middle" effect and medoid voting achieves up to 24% accuracy improvement

## Executive Summary
This paper introduces SWiM, a framework for evaluating long-context language models on real-world document processing tasks. It addresses limitations in existing benchmarks by testing models on user-specific documents and tasks, revealing the "lost-in-the-middle" effect where models struggle with information in the middle of long contexts. The authors propose medoid voting—a training-free method that generates multiple responses with shuffled document order and selects the medoid answer—to mitigate this issue. Tested across eight models, SWiM shows significant performance degradation for middle-positioned documents, with medoid voting achieving up to 24% accuracy improvement on single-document QA tasks.

## Method Summary
The SWiM framework consists of four steps: task generation using LLMs to create QA pairs from user documents, task validation through human-in-the-loop verification, task completion testing position and context size effects, and evaluation using LLM-as-a-judge. The framework systematically tests how document position affects model performance by placing answer-containing documents at different positions (0%, 25%, 50%, 75%, 100%) within the context window. Medoid voting is then applied by generating multiple responses with randomly permuted document orders and selecting the response with minimum dissimilarity to all other responses in embedding space.

## Key Results
- Language models exhibit significant performance degradation when retrieving information from the middle of long contexts (25%-75% depth)
- Medoid voting achieves up to 24% accuracy improvement on single-document QA tasks
- Position-dependent performance effects are not uniform across models, with Gemini-1.5-Pro showing superior long-context handling
- The "lost-in-the-middle" effect varies depending on benchmarks and models tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models exhibit degraded performance when retrieving information from the middle of long contexts due to attention mechanisms diluting across thousands of tokens.
- Mechanism: The transformer attention mechanism assigns lower attention weights to tokens in the middle of long sequences as the model processes information from both ends, creating a "lost-in-the-middle" effect.
- Core assumption: Attention weights follow a monotonic pattern where tokens near the start and end receive higher weights than those in the middle.
- Evidence anchors:
  - [abstract] "degrade in performance when information is present in the middle of the context window (lost-in-the-middle effect)"
  - [section] "Most long context models are ineffective at retrieving information in the middle of the context window (confirming "lost-in-the-middle" effect)"
  - [corpus] Weak - no direct corpus evidence for this specific attention pattern claim
- Break condition: If attention mechanisms were re-weighted or normalized differently, or if the model used different positional encoding schemes that don't create this gradient.

### Mechanism 2
- Claim: Medoid voting improves performance by selecting the most representative response from multiple permutations of document order.
- Mechanism: By generating multiple responses with randomly permuted document orders and selecting the medoid (most central) response in embedding space, the method effectively averages out position-dependent biases.
- Core assumption: Responses from different document permutations will cluster around the correct answer in embedding space, with the medoid being closest to the true answer.
- Evidence anchors:
  - [abstract] "medoid voting—a simple, but effective training-free method that generates multiple responses with shuffled document order and selects the medoid answer"
  - [section] "We consider the medoid response (response with the least dissimilarity to all other responses) in the embedding space as our selection criteria"
  - [corpus] Weak - no direct corpus evidence for medoid voting effectiveness
- Break condition: If the model's responses are too diverse or if the embedding space doesn't capture semantic similarity well enough for medoid selection.

### Mechanism 3
- Claim: Position-dependent performance degradation is not uniform across all models, with some models handling long contexts better than others.
- Mechanism: Different architectural choices (attention mechanisms, positional encodings, model size) lead to varying degrees of position sensitivity in long-context processing.
- Core assumption: Model architecture fundamentally affects how position influences attention and information retrieval.
- Evidence anchors:
  - [section] "But this degradation is not uniform across models. Among models with a long context window (1Mtokens), Gemini-1.5-Pro does extremely well to handle noise"
  - [section] "Most models exhibited a degradation in performance at the 25%~75% depth and showed their best performance when the answer is located at 0% depth or 100% depth"
  - [corpus] Weak - no direct corpus evidence comparing model architectures
- Break condition: If all models showed similar position-dependent patterns regardless of architecture.

## Foundational Learning

- **Attention mechanisms in transformers**
  - Why needed here: Understanding how attention weights are distributed across long sequences explains the "lost-in-the-middle" effect
  - Quick check question: How do attention weights typically behave for tokens at different positions in a long sequence?

- **Embedding similarity and clustering**
  - Why needed here: Medoid voting relies on finding the most central response in embedding space, requiring understanding of how semantic similarity is captured
  - Quick check question: What properties make a response the "medoid" in a set of embeddings?

- **Position encoding schemes**
  - Why needed here: Different models use different positional encoding methods, which affect how position influences attention patterns
  - Quick check question: How do sinusoidal positional encodings differ from learned positional embeddings in transformers?

## Architecture Onboarding

- **Component map**: Document QA task generator (LLM-based) -> Task validator (human-in-the-loop) -> Context position tester (systematic positioning) -> Response evaluator (LLM-as-a-judge) -> Medoid voting module (embedding-based selection)

- **Critical path**:
  1. Generate QA pairs from user documents
  2. Validate generated tasks
  3. Systematically test model performance at different document positions
  4. Apply medoid voting if position effects are detected
  5. Evaluate final performance

- **Design tradeoffs**:
  - LLM-based task generation vs. manual creation: automation vs. accuracy
  - Human validation vs. full automation: quality vs. scalability
  - Single vs. multiple permutations for medoid voting: performance vs. computational cost

- **Failure signatures**:
  - Inconsistent LLM judge responses indicating evaluation reliability issues
  - No performance improvement with medoid voting suggesting embedding space issues
  - Position effects not detected despite long context suggesting model architecture differences

- **First 3 experiments**:
  1. Run the SWiM framework on a small document set with a single model to verify the "lost-in-the-middle" effect detection
  2. Test medoid voting with 3-5 permutations on a position-sensitive model to validate the improvement mechanism
  3. Compare performance across different embedding models for medoid selection to optimize the voting process

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the lost-in-the-middle effect vary across different types of documents and domains?
- Basis in paper: [explicit] The authors mention testing on narrative content and note that "the effect varies depending on benchmarks and models"
- Why unresolved: The experiments were conducted on a synthetic story forum dataset, which may not represent the full diversity of real-world documents like financial reports, legal documents, or technical documentation
- What evidence would resolve it: Testing the framework across multiple domains with different document structures, formality levels, and information density patterns

### Open Question 2
- Question: What is the optimal number of permutations for medoid voting to achieve maximum performance gains?
- Basis in paper: [inferred] The authors tested medoid voting but did not explore how performance scales with the number of permutations beyond "as few as 3 runs"
- Why unresolved: The paper only demonstrates effectiveness with 3 runs and does not provide an analysis of the trade-off between computational cost and accuracy improvement
- What evidence would resolve it: Systematic testing of medoid voting with varying numbers of permutations (e.g., 2, 3, 5, 10, 20) across different models and tasks

### Open Question 3
- Question: How does medoid voting perform on tasks requiring multi-document reasoning or source citations?
- Basis in paper: [explicit] The authors state that "Future development of the SWiM framework should address these complex scenarios with more fine grained evaluations"
- Why unresolved: The current evaluation only tested single-document QA tasks, leaving multi-document reasoning and citation requirements unexplored
- What evidence would resolve it: Testing medoid voting on multi-hop question answering, document summarization requiring cross-document synthesis, and tasks requiring explicit source attribution

## Limitations
- Claims about attention mechanism patterns causing the "lost-in-the-middle" effect lack direct supporting evidence
- Medoid voting effectiveness assumes but doesn't prove semantic clustering in embedding space
- Position-dependent performance differences across models are observational rather than causally explained

## Confidence
- **High Confidence**: The existence of position-dependent performance degradation across multiple models is well-established through systematic testing. The SWiM framework methodology for detecting this effect is clearly specified and reproducible.
- **Medium Confidence**: The effectiveness of medoid voting for mitigating position effects is demonstrated empirically, but the theoretical justification for why this specific method works remains weak. The embedding space assumptions require further validation.
- **Low Confidence**: Claims about specific attention mechanism patterns causing the "lost-in-the-middle" effect lack direct supporting evidence. The paper assumes but does not prove that attention weight distributions follow the proposed pattern.

## Next Checks
1. **Attention Weight Analysis**: Extract and visualize attention weight distributions from tested models when processing documents at different positions to directly verify the proposed "lost-in-the-middle" mechanism rather than assuming it based on performance patterns.

2. **Embedding Space Validation**: Conduct controlled experiments testing medoid voting effectiveness across different embedding models (semantic, semantic+syntactic, and contrastive embeddings) to determine which embedding properties are necessary and sufficient for the voting mechanism to work.

3. **Architectural Ablation Study**: Compare position-dependent performance patterns across models with systematically varied architectural features (positional encoding schemes, attention mechanisms, model depth) to identify which components most strongly influence position sensitivity.