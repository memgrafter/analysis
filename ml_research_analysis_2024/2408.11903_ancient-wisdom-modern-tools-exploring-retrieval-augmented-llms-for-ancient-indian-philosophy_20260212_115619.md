---
ver: rpa2
title: 'Ancient Wisdom, Modern Tools: Exploring Retrieval-Augmented LLMs for Ancient
  Indian Philosophy'
arxiv_id: '2408.11903'
source_url: https://arxiv.org/abs/2408.11903
tags:
- knowledge
- language
- retrieval
- evaluation
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the integration of retrieval-augmented large
  language models (RAGs) with ancient Indian philosophy, specifically Advaita Vedanta.
  A custom dataset, VedantaNY-10M, was created from 750+ hours of public discourses,
  ensuring domain-specific, novel, and authentic content.
---

# Ancient Wisdom, Modern Tools: Exploring Retrieval-Augmented LLMs for Ancient Indian Philosophy

## Quick Facts
- arXiv ID: 2408.11903
- Source URL: https://arxiv.org/abs/2408.11903
- Authors: Priyanka Mandikal
- Reference count: 15
- Key outcome: RAG models with keyword-based hybrid retrievers significantly outperform non-RAG models on Advaita Vedanta domain, achieving 81% preference rate in human evaluation.

## Executive Summary
This work explores the integration of retrieval-augmented large language models (RAGs) with ancient Indian philosophy, specifically Advaita Vedanta. A custom dataset, VedantaNY-10M, was created from 750+ hours of public discourses, ensuring domain-specific, novel, and authentic content. A keyword-based hybrid retriever was proposed to enhance retrieval in niche domains with unique terminology, using both deep and sparse embeddings. Human evaluation by computational linguists and domain experts showed that RAG models significantly outperformed non-RAG models across factuality, completeness, and specificity, with an 81% preference rate. The keyword-based RAG further improved performance, particularly in reducing hallucinations.

## Method Summary
The study created the VedantaNY-10M dataset from 750+ hours of Advaita Vedanta discourses, then implemented a RAG system with a keyword-based hybrid retriever combining dense and sparse embeddings. The retriever uses keyword extraction via ensemble models (OpenKP, KeyBERT, SpanMarker) and context refinement to adjust passage length. Mixtral-8x7B-Instruct-v0.1 serves as the LLM for generation. Evaluation combines automatic metrics (GPT-2 perplexity, self-BLEU, RankGen, QAFactEval) with human assessment by domain experts and computational linguists across factuality, completeness, and specificity dimensions.

## Key Results
- RAG models significantly outperform standard non-RAG LLMs across factuality, completeness, and specificity dimensions with an 81% preference rate
- Keyword-based hybrid retrievers improve performance in niche domains by emphasizing low-frequency, domain-specific terminology
- Context refinement using keyword-based expansion/trimming produces more coherent responses by optimizing passage length

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: RAG models significantly outperform non-RAG models in niche domains like Advaita Vedanta.
- **Mechanism**: RAG models incorporate external knowledge retrieval into the generation process, reducing hallucinations by grounding responses in verifiable source material.
- **Core assumption**: The niche domain has sufficient authentic textual data that can be effectively retrieved and used for augmentation.
- **Evidence anchors**:
  - [abstract] "Human evaluations by computational linguists and domain experts show that the RAG model significantly outperforms the standard model in producing factual and comprehensive responses having fewer hallucinations, with an 81% preference rate."
  - [section] "RAG models significantly outperform standard non-RAG LLMs along all axes, offering more factual, comprehensive, and specific responses while minimizing hallucinations, with an 81% preference rate."
  - [corpus] Weak - corpus neighbors are mostly unrelated to the philosophy domain, suggesting the dataset is quite specialized.
- **Break condition**: If the retrieval system cannot find relevant passages, the RAG model falls back to generation without grounding, potentially hallucinating.

### Mechanism 2
- **Claim**: Keyword-based hybrid retrievers outperform standard dense retrievers in niche domains with unique terminology.
- **Mechanism**: The hybrid approach combines dense semantic embeddings with sparse keyword matching, ensuring domain-specific terms (like Sanskrit words) are not missed by semantic-only models.
- **Core assumption**: Niche domains have low-frequency, domain-specific terms that are semantically distant from general language but critical for meaning.
- **Evidence anchors**:
  - [abstract] "In addition, a keyword-based hybrid retriever that emphasizes unique low-frequency terms further improves results."
  - [section] "Traditional sparse retrievers have a unique advantage over dense retrievers in niche domains having specific terminology—Sanskrit terms in our case."
  - [corpus] Weak - corpus doesn't contain examples of retrieval failure or success, only neighbor titles.
- **Break condition**: If keywords are ambiguous or too common, the sparse component may introduce noise rather than signal.

### Mechanism 3
- **Claim**: Context refinement improves retrieval quality by adjusting passage length based on keyword presence.
- **Mechanism**: The context refiner expands or contracts retrieved passages by including surrounding context around extracted keywords, ensuring completeness without overwhelming the generator.
- **Core assumption**: Relevant information for answering a query is clustered around key terms, and surrounding context can be algorithmically determined.
- **Evidence anchors**:
  - [abstract] "a keyword-based hybrid retriever that emphasizes unique low-frequency terms further improves results."
  - [section] "Furthermore, we refine our retrieved passages by leveraging the extracted keywords using a heuristic-based refinement operation to produce P′ = Ref(P, κ)."
  - [corpus] Weak - corpus doesn't show examples of context refinement.
- **Break condition**: If keywords are too sparse or the context is highly fragmented, the heuristic may fail to produce coherent passages.

## Foundational Learning

- **Concept**: Retrieval-augmented generation (RAG) fundamentals
  - Why needed here: The paper builds on RAG architecture as the core innovation, so understanding how retrieval and generation interact is essential.
  - Quick check question: What are the two main components of a RAG system, and how do they interact during inference?

- **Concept**: Dense vs sparse embeddings
  - Why needed here: The paper contrasts dense semantic embeddings with sparse keyword-based retrieval, so understanding their strengths and weaknesses is crucial.
  - Quick check question: When would a sparse retriever outperform a dense retriever, and why?

- **Concept**: Keyword extraction and named entity recognition
  - Why needed here: The keyword-based retriever relies on extracting important terms from queries, so familiarity with these techniques is necessary.
  - Quick check question: What is the difference between keyword extraction and named entity recognition, and how might each be used in retrieval?

## Architecture Onboarding

- **Component map**: Retriever (dense + sparse hybrid) → Context Refiner (keyword-based) → LLM (prompted with query + refined passages) → Answer
- **Critical path**: Query → Keyword extraction → Hybrid retrieval → Context refinement → LLM generation → Response
- **Design tradeoffs**: Hybrid retriever adds complexity but improves recall of niche terms; context refiner adds preprocessing time but improves answer coherence; keyword extraction adds another failure point.
- **Failure signatures**: Irrelevant retrievals (dense retriever confusion), incomplete context (refiner too aggressive), hallucinated answers (keyword extraction misses key terms), slow inference (multiple processing steps).
- **First 3 experiments**:
  1. Compare dense-only vs hybrid retriever retrieval accuracy on a small set of queries with known answers.
  2. Test context refiner on passages with varying keyword distributions to find optimal expansion parameters.
  3. Evaluate answer quality with and without context refinement on a diverse set of query types.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do retrieval models that summarize longer contexts before input to the LLM impact the quality and coherence of generated responses?
- Basis in paper: [explicit] The paper discusses that retrieved passages can sometimes be too short or too long, leading to incomplete or confusing responses. It suggests that future work could explore retrieval models capable of handling longer contexts and summarizing them effectively.
- Why unresolved: The paper only mentions this as a potential solution but does not implement or evaluate it. It remains an open question how such summarization would impact the quality of responses.
- What evidence would resolve it: Implementing a summarization step in the retrieval pipeline and evaluating the resulting responses against the current approach using the same human evaluation metrics (factuality, completeness, specificity, etc.) would provide evidence on the effectiveness of this approach.

### Open Question 2
- Question: How does the performance of retrieval-augmented LLMs compare to fine-tuned LLMs on the Advaita Vedanta dataset?
- Basis in paper: [inferred] The paper mentions that while it experiments with RAG models, fine-tuning the language models themselves on philosophy datasets is an interesting future direction. This implies that a comparison between RAG and fine-tuned models has not been conducted.
- Why unresolved: The paper focuses on RAG models and does not explore the performance of fine-tuned models on the same dataset. This leaves open the question of whether RAG or fine-tuning is more effective for this domain.
- What evidence would resolve it: Training a fine-tuned LLM on the VedantaNY-10M dataset and comparing its performance to the RAG models using the same evaluation metrics (automatic and human) would provide evidence on which approach is more effective.

### Open Question 3
- Question: How does the keyword-based hybrid retriever perform on other niche domains with unique terminology?
- Basis in paper: [explicit] The paper proposes a keyword-based hybrid retriever that combines sparse and dense embeddings to emphasize low-frequency or domain-specific terms. It shows improved performance on the Advaita Vedanta dataset.
- Why unresolved: The paper only evaluates the keyword-based retriever on the Advaita Vedanta domain. It remains unclear how well this approach generalizes to other niche domains with unique terminology.
- What evidence would resolve it: Applying the keyword-based hybrid retriever to other niche datasets (e.g., other philosophical traditions, specialized scientific fields) and comparing its performance to standard dense retrievers using relevant evaluation metrics would provide evidence on its generalizability.

## Limitations
- Reliance on human evaluation may introduce subjectivity despite expert involvement
- Performance improvements demonstrated through preference rates rather than absolute metric improvements
- Keyword-based hybrid retriever effectiveness depends on quality of keyword extraction, which may fail with ambiguous or polysemous terms

## Confidence

- **High confidence**: RAG models outperform non-RAG models in factuality and hallucination reduction (supported by 81% preference rate from human evaluators)
- **Medium confidence**: Keyword-based hybrid retrievers improve performance in niche domains (supported by theoretical arguments and limited empirical comparison)
- **Medium confidence**: Context refinement improves answer quality (mechanism described but not extensively validated)

## Next Checks
1. Test the keyword-based hybrid retriever on another niche domain with unique terminology to verify generalizability beyond Advaita Vedanta
2. Conduct A/B testing with different keyword extraction methods to measure impact on retrieval quality and downstream answer accuracy
3. Implement automated hallucination detection metrics and compare results with human evaluation to validate consistency and identify potential bias