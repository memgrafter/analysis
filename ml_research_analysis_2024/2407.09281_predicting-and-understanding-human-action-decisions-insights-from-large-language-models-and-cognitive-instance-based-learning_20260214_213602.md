---
ver: rpa2
title: 'Predicting and Understanding Human Action Decisions: Insights from Large Language
  Models and Cognitive Instance-Based Learning'
arxiv_id: '2407.09281'
source_url: https://arxiv.org/abs/2407.09281
tags:
- human
- llms
- cognitive
- learning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether Large Language Models (LLMs) and
  cognitive Instance-Based Learning (IBL) models can predict human action strategies
  in sequential decision-making tasks. The study uses two open-source LLMs (Mistral-7B
  and Llama-3 70B) and compares them with a cognitive IBL model across two experiments
  involving goal-directed decision-making in gridworld environments with varying levels
  of information presentation.
---

# Predicting and Understanding Human Action Decisions: Insights from Large Language Models and Cognitive Instance-Based Learning

## Quick Facts
- arXiv ID: 2407.09281
- Source URL: https://arxiv.org/abs/2407.09281
- Authors: Thuy Ngoc Nguyen; Kasturi Jamale; Cleotilde Gonzalez
- Reference count: 15
- Primary result: Mistral-7B LLM outperforms both Llama-3 70B and cognitive IBL model in predicting human decision strategies in gridworld tasks

## Executive Summary
This study investigates whether Large Language Models (LLMs) and cognitive Instance-Based Learning (IBL) models can predict human action strategies in sequential decision-making tasks. Using two open-source LLMs (Mistral-7B and Llama-3 70B) and comparing them with a cognitive IBL model across two experiments in gridworld environments, the research demonstrates that Mistral-7B outperforms other approaches in predicting human strategies. The study also reveals that the cognitive IBL model better captures human exploratory behavior with limited samples and effectively models loss aversion bias, suggesting potential benefits of integrating LLMs with cognitive architectures for understanding complex human decision-making patterns.

## Method Summary
The study compared two LLM-based models (Mistral-7B and Llama-3 70B) with a cognitive Instance-Based Learning (IBL) model to predict human action strategies in gridworld environments. Human behavioral data was collected from 400 participants across two experiments using 10×10 gridworld mazes with varying decision complexity and information presentation levels. Models were evaluated using trajectory divergence (KL divergence), prediction accuracy, and exploration entropy difference. The LLM models used schema-based and demonstration-based prompts with in-context learning, while the IBL model used past trajectory-based predictions with specified parameter settings (d=0.25, σ=0.5, default utility=1.0).

## Key Results
- Mistral-7B outperformed both Llama-3 70B and cognitive IBL model in predicting human strategies across all metrics
- Cognitive IBL model better accounted for human exploratory behavior with limited samples and captured loss aversion bias
- LLMs excelled at rapidly incorporating feedback to enhance prediction accuracy
- Integration of LLMs with cognitive architectures could enhance modeling of complex human decision-making patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mistral-7B outperforms both Llama-3 70B and the cognitive IBL model in predicting human strategies due to its ability to rapidly incorporate feedback and improve prediction accuracy.
- Mechanism: Mistral-7B leverages its training on vast amounts of human behavior data to generate trajectories that closely match human decision-making patterns. The model's lightweight architecture allows for efficient processing and adaptation to new information.
- Core assumption: The pre-training data of Mistral-7B contains sufficient examples of human decision-making in sequential tasks to enable accurate predictions.
- Evidence anchors:
  - [abstract] "Our findings indicate that LLMs excel at rapidly incorporating feedback to enhance prediction accuracy."
  - [section] "Our results from comparing the predicted behaviors of the models and humans demonstrate that the lightweight Mistral-7B model outperforms both Llama-3 70B and the cognitive IBL model in predicting human strategies."
- Break condition: If the pre-training data lacks diversity in human decision-making scenarios or if the task complexity exceeds the model's ability to generalize from its training data.

### Mechanism 2
- Claim: The cognitive IBL model better accounts for human exploratory behavior with limited samples and captures loss aversion bias.
- Mechanism: The IBL model simulates human experiential decision-making by incorporating mechanisms and limitations from the ACT-R cognitive architecture. It uses past experiences to predict the estimated utility of actions, allowing it to model human tendencies towards risk-averse behavior and satisficing.
- Core assumption: Human decision-making in sequential tasks involves reliance on past experiences and a tendency to avoid high-risk options, even if they offer higher rewards.
- Evidence anchors:
  - [abstract] "the cognitive IBL model better accounts for human exploratory behaviors and effectively captures loss aversion bias—the tendency to choose a sub-optimal goal with fewer step-cost penalties rather than exploring to find the optimal choice, even with limited experience."
  - [section] "We observed that the cognitive IBL model more accurately accounted for human exploratory behavior with few samples and aligned closely with human exploratory strategies under limited information, which reflects the tendency towards risk-averse or 'satisficing' behavior."
- Break condition: If human decision-making does not follow the assumed experiential learning patterns or if the task structure does not align with the IBL model's assumptions about utility and exploration.

### Mechanism 3
- Claim: Integrating LLMs with cognitive architectures could enhance the modeling and understanding of complex human decision-making patterns.
- Mechanism: Combining the rapid learning and adaptation capabilities of LLMs with the human-like decision-making mechanisms of cognitive models can create a more comprehensive and accurate representation of human behavior in sequential tasks.
- Core assumption: The strengths of LLMs (rapid feedback incorporation, large-scale pattern recognition) complement the strengths of cognitive models (human-like decision-making, experiential learning) to create a synergistic effect.
- Evidence anchors:
  - [abstract] "The results highlight the benefits of integrating LLMs with cognitive architectures, suggesting that this synergy could enhance the modeling and understanding of complex human decision-making patterns."
  - [section] "By leveraging these open-source models, we can create trustworthy LLM-powered systems that are more accurate, explainable, and aligned with human expectations, thereby enhancing their acceptance and effectiveness in real-world applications."
- Break condition: If the integration of LLMs and cognitive models introduces conflicting assumptions or if the combined model becomes too complex to effectively capture human decision-making patterns.

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: The task scenario is modeled as a POMDP, which requires understanding the agent's ability to make decisions based on incomplete information about the environment.
  - Quick check question: What are the key components of a POMDP, and how do they differ from a standard MDP?

- Concept: Instance-Based Learning Theory (IBLT)
  - Why needed here: The cognitive IBL model used for comparison is based on IBLT, which simulates human decision-making by incorporating past experiences and their associated utilities.
  - Quick check question: How does the IBL model use past experiences to predict the utility of actions in a given state?

- Concept: Kullback-Leibler (KL) Divergence
  - Why needed here: KL divergence is used to measure the difference between the trajectory distribution of human subjects and that predicted by the models, providing a quantitative assessment of model performance.
  - Quick check question: What does a low KL divergence between the predicted and human trajectories indicate about the model's performance?

## Architecture Onboarding

- Component map: Task environment (Gridworld mazes) -> Models (Mistral-7B, Llama-3 70B, cognitive IBL) -> Prediction mechanism (Schema-based/demonstration prompts for LLMs, past trajectory-based for IBL) -> Evaluation metrics (KL divergence, prediction accuracy, exploration entropy difference)

- Critical path: 1. Define task scenario and model the environment as a POMDP 2. Prepare task instructions and demonstration-based prompts for LLMs 3. Implement the cognitive IBL model with past trajectory-based predictions 4. Run experiments with human subjects and collect trajectory data 5. Compare model predictions with human trajectories using evaluation metrics 6. Analyze results and draw conclusions about model performance and human decision-making patterns

- Design tradeoffs:
  - Model complexity vs. prediction accuracy: More complex models may capture human behavior more accurately but require more computational resources and training data
  - Generalizability vs. task-specific performance: Models trained on specific tasks may perform better on those tasks but may not generalize well to other decision-making scenarios
  - Interpretability vs. predictive power: More interpretable models may provide insights into human decision-making but may sacrifice some predictive accuracy

- Failure signatures:
  - High KL divergence between predicted and human trajectories indicates poor model performance
  - Low prediction accuracy suggests the model fails to capture key aspects of human decision-making
  - Entropy differences between model and human exploration patterns highlight discrepancies in how the model and humans approach exploration and risk-taking

- First 3 experiments:
  1. Compare the prediction accuracy of Mistral-7B, Llama-3 70B, and the cognitive IBL model on a simple gridworld task with full information
  2. Evaluate the models' performance on a complex gridworld task with restricted information to assess their ability to handle incomplete information
  3. Analyze the exploration patterns of the models and humans in the first 10 episodes to identify differences in risk-taking and satisficing behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can open-source LLMs, when fine-tuned on human decision-making data, achieve better prediction accuracy than the vanilla versions used in this study?
- Basis in paper: [explicit] The paper mentions that the vanilla versions of the models were used without fine-tuning their parameters to human data.
- Why unresolved: The study used pre-trained, off-the-shelf versions of the models without any customization or fine-tuning to the specific decision-making tasks or human behavior patterns.
- What evidence would resolve it: A follow-up study comparing the prediction accuracy of fine-tuned LLMs against the vanilla versions on the same decision-making tasks.

### Open Question 2
- Question: How would integrating LLMs with cognitive architectures, such as ACT-R, impact the prediction of human decision-making strategies compared to using either model alone?
- Basis in paper: [explicit] The paper suggests that integrating LLMs with cognitive architectures could enhance the modeling and understanding of complex human decision-making patterns.
- Why unresolved: The study only compared the standalone performance of LLMs and cognitive IBL models, without exploring their integration or synergy.
- What evidence would resolve it: An experiment that integrates LLMs with cognitive architectures and compares their performance against standalone models in predicting human decision-making strategies.

### Open Question 3
- Question: How do different levels of environment presentation (full grid information vs. restricted grid information) affect the ability of LLMs and cognitive models to predict human decision strategies in real-time interactive systems?
- Basis in paper: [explicit] The paper explored how different levels of environment presentation affected model performance in static experiments.
- Why unresolved: The study did not investigate the impact of environment presentation on real-time prediction in interactive systems.
- What evidence would resolve it: A study that tests the models' prediction accuracy in real-time interactive systems with varying levels of environment presentation.

## Limitations

- The study's findings are limited to specific gridworld environments and may not generalize to more complex real-world decision-making scenarios
- The evaluation metrics may not fully capture the nuances of human decision-making processes, such as reasoning or external factor influences
- The integration of LLMs with cognitive architectures is presented as a theoretical possibility but lacks empirical evidence or implementation details

## Confidence

- **High Confidence**: The comparison of Mistral-7B, Llama-3 70B, and the cognitive IBL model on the specific gridworld tasks
- **Medium Confidence**: The interpretation of the cognitive IBL model's ability to capture human exploratory behavior and loss aversion bias
- **Low Confidence**: The claim that integrating LLMs with cognitive architectures will enhance the modeling and understanding of complex human decision-making patterns

## Next Checks

1. **Cross-Domain Validation**: Test the Mistral-7B model's performance on a variety of decision-making tasks beyond gridworld environments, such as real-world business decisions or complex social interactions, to assess the generalizability of its predictive capabilities.

2. **Behavioral Process Analysis**: Conduct a detailed analysis of the decision-making processes of both human participants and the models, including eye-tracking studies or think-aloud protocols, to better understand the cognitive mechanisms underlying the observed behaviors and improve the models' ability to capture these processes.

3. **Integrated Model Implementation**: Develop and test a prototype of an integrated LLM-cognitive architecture model, as suggested in the paper, and compare its performance with the standalone models on a challenging decision-making task. This would provide empirical evidence for the potential benefits of such integration.