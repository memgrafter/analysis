---
ver: rpa2
title: Faithfulness and the Notion of Adversarial Sensitivity in NLP Explanations
arxiv_id: '2409.17774'
source_url: https://arxiv.org/abs/2409.17774
tags:
- arxiv
- adversarial
- faithfulness
- explainers
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adversarial Sensitivity, a novel approach
  to faithfulness evaluation in NLP explainers. The core idea is that faithful explainers
  should reflect changes in model reasoning when the model is under adversarial attack.
---

# Faithfulness and the Notion of Adversarial Sensitivity in NLP Explanations

## Quick Facts
- arXiv ID: 2409.17774
- Source URL: https://arxiv.org/abs/2409.17774
- Reference count: 35
- Primary result: Introduces Adversarial Sensitivity, a novel faithfulness evaluation method measuring how much explanation weights change when models are under adversarial attack

## Executive Summary
This paper introduces Adversarial Sensitivity, a novel approach to faithfulness evaluation in NLP explainers that measures how much an explainer's token weights change when the underlying model is fooled by adversarial examples. The method generates adversarial examples under word-level, character-level, and behavioral invariance constraints, then computes distances between explanation weights before and after attacks. Experiments on six state-of-the-art explainers across three datasets show that perturbation-based methods (LIME, SHAP) and gradient-input variants consistently perform well, while vanilla gradient methods perform poorly, demonstrating that adversarial sensitivity provides complementary insights to traditional erasure-based faithfulness metrics.

## Method Summary
The method generates adversarial examples for text inputs using black-box attacks under three constraint types (word-level, character-level, behavioral invariance). For each original-adversarial pair, it computes explanation weights using six different explainers, then measures the distance between these weight vectors using the ˆτx correlation coefficient, which handles tokenization discrepancies better than traditional metrics. The framework assumes that faithful explainers should reflect changes in model reasoning when the model is deceived by adversarial examples, making the explanation weights before and after attacks dissimilar.

## Key Results
- Perturbation-based explainers (LIME, SHAP) and gradient-input variants consistently outperform vanilla gradient methods in adversarial sensitivity
- Results differ from traditional erasure-based faithfulness metrics, showing complementary evaluation value
- All six explainers achieve comparable performance in comprehensiveness and sufficiency metrics
- LIME, SHAP, and Integrated Gradient×Input show the most consistent performance across all three attack strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial Sensitivity measures faithfulness by quantifying how much an explainer's token weights change when the underlying model is fooled by adversarial examples
- Mechanism: The method generates adversarial examples under three constraint types, computes explanation weights for both original and adversarial inputs, then measures the distance between these weight vectors
- Core assumption: Faithful explainers must reflect changes in model reasoning when the model is deceived by adversarial examples
- Evidence anchors:
  - [abstract] "Our method measures the distance between explanation weights before and after generating adversarial examples"
  - [section] "Given x′ is an AE of x for f, if I is 'faithful' to f, then Wx,f and Wx′,f should be dissimilar"
- Break condition: If the model's reasoning doesn't actually change when fooled by adversarial examples

### Mechanism 2
- Claim: Using black-box attacks avoids bias toward gradient-based explainers
- Mechanism: The framework uses model-agnostic attacks that perturb inputs based on token importance rankings, without requiring gradient access
- Core assumption: Gradient-based attacks would unfairly disadvantage gradient-based explainers while unfairly advantaging perturbation-based ones
- Evidence anchors:
  - [section] "we do not consider investigation on white-box attacks for adversarial sensitivity and adhere to a more practical, model-agnostic, and transferable black-box attacking framework"
- Break condition: If black-box attacks fail to generate meaningful adversarial examples

### Mechanism 3
- Claim: ˆτx correlation coefficient handles tokenization discrepancies better than traditional metrics
- Mechanism: ˆτ is a non-parametric generalization of Kendall τ designed specifically for incomplete and non-strict rankings, making it robust to tokenization differences between original and adversarial examples
- Core assumption: Traditional correlation measures fail when tokenization produces different token sequences for similar inputs
- Evidence anchors:
  - [section] "Correlation measures like Pearson, Kendall, and Spearman cannot handle disjoint and unequal ranked lists"
- Break condition: If tokenization produces consistent token sequences

## Foundational Learning

- Concept: Adversarial examples and attacks
  - Why needed here: The method fundamentally relies on generating and using adversarial examples to test explainer sensitivity
  - Quick check question: Can you explain the difference between white-box and black-box adversarial attacks and why this paper uses black-box?

- Concept: Post-hoc explainability methods
  - Why needed here: The paper evaluates six different explainers (LIME, SHAP, Gradient variants), requiring understanding of their mechanisms
  - Quick check question: What distinguishes perturbation-based explainers from gradient-based ones, and why might this matter for adversarial sensitivity?

- Concept: Faithfulness evaluation metrics
  - Why needed here: The paper contrasts adversarial sensitivity with existing erasure-based metrics like comprehensiveness and sufficiency
  - Quick check question: How do erasure-based metrics differ from adversarial sensitivity in their assumptions about what makes an explainer faithful?

## Architecture Onboarding

- Component map:
  - Attack generator -> Explainer wrapper -> Distance calculator -> Aggregation layer -> Comparison module

- Critical path:
  1. Load model and dataset
  2. Generate adversarial examples for each input
  3. Compute explanations for original and adversarial pairs
  4. Calculate distances between explanation vectors
  5. Aggregate and rank explainer performance
  6. Compare with erasure-based metrics

- Design tradeoffs:
  - Black-box vs white-box attacks: Black-box avoids bias but may be less effective
  - Distance metric choice: ˆτx handles tokenization issues but may be less interpretable
  - Attack constraint selection: More constraints provide better coverage but increase computational cost

- Failure signatures:
  - No adversarial examples generated: Attack parameters may be too restrictive
  - All explainers score similarly: Distance metric may lack sensitivity
  - Results inconsistent across datasets: Evaluation may not be robust to domain differences

- First 3 experiments:
  1. Run adversarial sensitivity on a simple model (e.g., logistic regression) with synthetic data to verify basic functionality
  2. Compare results with a single explainer (e.g., LIME) across all three attack types on one dataset
  3. Validate aggregation method by checking if Kemeny-Young produces consistent rankings across different parameter settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different attack strategies (beyond the three used in this study) affect the measurement of adversarial sensitivity for explainers?
- Basis in paper: [explicit] The authors mention that "Whether methods like back-translation, paraphrasing, or hybrid attacks etc (Zhang et al., 2020) maintain semantic and structural similarity while generating AEs, and suitability for faithfulness evaluation are kept for further study."
- Why unresolved: The study focuses on three specific attack strategies and acknowledges that other methods exist that could potentially generate adversarial examples while maintaining semantic and structural similarity.
- What evidence would resolve it: Comparative studies using a broader range of attack strategies to measure their impact on adversarial sensitivity scores across different explainers and datasets.

### Open Question 2
- Question: Does the choice of distance measure significantly impact the ranking of explainers based on adversarial sensitivity?
- Basis in paper: [explicit] The authors discuss the selection of their distance measure (τx) in detail, noting that "We have extensively investigated selecting the similarity measures in previous works, but none of the works has tackled the problem of unequal and/or disjoint rank lists from an axiomatic perspective that will be adequate for our setting."
- Why unresolved: While the authors provide a rationale for choosing τx, they do not empirically compare its performance against other distance measures in terms of explainer ranking consistency.
- What evidence would resolve it: Systematic comparison of explainer rankings using different distance measures (e.g., Kendall tau, Spearman correlation) across multiple datasets and attack strategies.

### Open Question 3
- Question: How does the proposed adversarial sensitivity metric perform in multilingual and low-resource language settings?
- Basis in paper: [explicit] The authors state in their conclusion that "Future work will explore adversarial sensitivity for multilingual datasets, low-resource languages, and advanced lms."
- Why unresolved: The current study is limited to English text classification tasks, and the generalizability of adversarial sensitivity to other languages and resource-constrained scenarios remains unexplored.
- What evidence would resolve it: Experiments applying adversarial sensitivity to explainers in multilingual datasets and low-resource language tasks, comparing the results with those obtained in the English setting.

## Limitations

- The framework only evaluates faithfulness, not plausibility, leaving a gap in assessing whether explanations are both accurate and interpretable
- Black-box attacks may miss important attack vectors that gradient-based methods could reveal
- The ˆτx correlation coefficient, while addressing tokenization issues, may not fully capture semantic similarity between explanation weight distributions

## Confidence

- High confidence: The core methodology of measuring explanation distance before and after adversarial attacks is theoretically sound
- Medium confidence: Empirical results are rigorous but may be affected by attack success rates and tokenization inconsistencies
- Low confidence: Generalizability to other languages and the assumption that adversarial sensitivity directly correlates with faithfulness haven't been validated

## Next Checks

1. **Attack Success Rate Analysis**: Systematically vary attack parameters and measure how adversarial sensitivity scores change with attack success rates to validate metric robustness

2. **Cross-Dataset Robustness Test**: Apply the framework to additional datasets spanning different domains to assess whether observed explainer rankings are consistent or dataset-dependent

3. **Human Evaluation Correlation**: Conduct a small-scale human study where participants assess explanation quality for original vs adversarial examples, then correlate these judgments with adversarial sensitivity scores to validate metric alignment with human notions of faithfulness