---
ver: rpa2
title: Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions
  of Clean Data
arxiv_id: '2406.03736'
source_url: https://arxiv.org/abs/2406.03736
tags:
- diffusion
- time
- discrete
- sampling
- absorbing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents RADD, a reparameterized absorbing discrete
  diffusion model that simplifies the parameterization by removing time-conditioning
  and models conditional distributions of clean data directly. The key theoretical
  insight reveals that the concrete score in absorbing diffusion can be expressed
  as time-independent conditional probabilities multiplied by an analytic time-dependent
  scalar, which explains the effectiveness of the scaling trick in prior work and
  motivates the reparameterization.
---

# Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data

## Quick Facts
- arXiv ID: 2406.03736
- Source URL: https://arxiv.org/abs/2406.03736
- Authors: Jingyang Ou; Shen Nie; Kaiwen Xue; Fengqi Zhu; Jiacheng Sun; Zhenguo Li; Chongxuan Li
- Reference count: 40
- Primary result: RADD achieves state-of-the-art performance among diffusion models on five zero-shot language modeling benchmarks at the GPT-2 scale

## Executive Summary
This paper introduces RADD (Reparameterized Absorbing Discrete Diffusion), a novel framework that reparameterizes absorbing discrete diffusion models by removing time conditioning and directly modeling conditional distributions of clean data. The key theoretical insight reveals that the concrete score in absorbing diffusion can be factored into a time-independent conditional probability multiplied by an analytic time-dependent scalar. This allows RADD to simplify training, enable efficient caching during sampling, and achieve state-of-the-art zero-shot language modeling performance.

## Method Summary
RADD reparameterizes absorbing discrete diffusion by modeling time-independent conditional probabilities directly on clean data, removing the need for time conditioning. The model employs a caching strategy during sampling that stores outputs when input sequences remain unchanged, reducing the number of function evaluations. Training can be performed using various loss functions including denoising score entropy, t-denoising cross-entropy, λ-denoising cross-entropy, or any-order autoregressive loss. The framework is evaluated on zero-shot language modeling benchmarks using the OpenWebText dataset for training.

## Key Results
- RADD achieves state-of-the-art perplexity among diffusion models on five zero-shot language modeling benchmarks at GPT-2 scale
- Theoretical unification shows absorbing discrete diffusion and any-order autoregressive models have equivalent training objectives
- Expected number of function evaluations can be reduced through caching strategy when input sequences remain unchanged

## Why This Works (Mechanism)

### Mechanism 1
The concrete score in absorbing diffusion can be factored into a time-independent conditional distribution of clean data multiplied by an analytic time-dependent scalar. This factorization works because the absorbing transition matrix has zero values for transitions between unmasked states, allowing the concrete score to be rewritten using Bayes' rule as a product of a time-independent conditional probability and a time-dependent scalar. This breaks down if the diffusion process has dependencies between dimensions or uses a non-absorbing transition matrix.

### Mechanism 2
Removing time conditioning from the score network simplifies the model and enables efficient caching during sampling. By parameterizing the model to directly output time-independent conditional probabilities on clean data, the network no longer needs to condition on time. During sampling, when the input sequence doesn't change, the cached output can be reused, reducing the number of function evaluations. This fails if the model cannot learn accurate conditional distributions without time conditioning, or if the caching strategy fails due to implementation issues.

### Mechanism 3
The training objectives for absorbing discrete diffusion and any-order autoregressive models are equivalent when the final noise level approaches infinity. Through a change of variable from time to the probability of a token being masked, the absorbing diffusion loss can be transformed into an expected negative log-likelihood over factorial orderings, which is the AO-ARM objective. This equivalence breaks down if the final noise level is not sufficiently large, or if the model cannot approximate the true conditional distributions accurately.

## Foundational Learning

- **Concept**: Continuous-time discrete Markov chains with absorbing states
  - **Why needed here**: The entire framework relies on understanding how absorbing Markov chains work, particularly the transition rate matrix and the forward Kolmogorov equation
  - **Quick check question**: What is the difference between the transition probability matrix Pt|s and the transition rate matrix Qt in a continuous-time Markov chain?

- **Concept**: Concrete score and denoising score matching
  - **Why needed here**: The concrete score is the key quantity being estimated in discrete diffusion models, and understanding how it relates to denoising score matching is crucial for grasping the training objective
  - **Quick check question**: How does the concrete score in discrete diffusion differ from the score in continuous diffusion models?

- **Concept**: Any-order autoregressive models and their training objective
  - **Why needed here**: The equivalence between absorbing diffusion and AO-ARMs is a central theoretical contribution, requiring understanding of how AO-ARMs factorize the joint distribution over all possible orderings
  - **Quick check question**: What is the main difference between standard autoregressive models and any-order autoregressive models in terms of how they model the joint distribution?

## Architecture Onboarding

- **Component map**: Input -> Transformer encoder -> Softmax -> Conditional probabilities -> Sampling decision (cache or update)

- **Critical path**: Input → Transformer encoder → Softmax → Conditional probabilities → Sampling decision (cache or update)

- **Design tradeoffs**:
  - Time conditioning removal vs. model expressiveness
  - Caching efficiency vs. memory usage
  - Single-step sampling vs. multi-step sampling

- **Failure signatures**:
  - Poor perplexity: Model cannot learn accurate conditional distributions
  - High NFEs: Caching strategy not working or model still dependent on time
  - Training instability: Loss functions not equivalent in practice despite theoretical equivalence

- **First 3 experiments**:
  1. Train RADD with time conditioning removed vs. SEDD with time conditioning, compare perplexity
  2. Implement caching strategy and measure NFE reduction on sample sequences
  3. Train RADD with different loss functions (DSE, t-DCE, λ-DCE, AO) and compare performance

## Open Questions the Paper Calls Out

### Open Question 1
Does the time-independent parameterization of RADD improve training efficiency compared to time-conditioned models, and by what magnitude? While the paper demonstrates faster convergence empirically, it does not provide a detailed quantitative comparison of training efficiency metrics such as wall-clock time per epoch or total training time to reach comparable performance. Direct comparisons of training time to reach specific performance benchmarks would resolve this.

### Open Question 2
How does the caching strategy's effectiveness scale with sequence length and batch size in practical implementations? The paper provides theoretical analysis but lacks empirical validation across diverse sequence lengths and batch sizes, particularly regarding GPU utilization and memory overhead. Empirical studies measuring NFEs, sampling speed, and memory usage across varying sequence lengths and batch sizes would resolve this.

### Open Question 3
Can the unification of absorbing discrete diffusion and AO-ARMs lead to novel hybrid architectures or training methodologies? While the theoretical equivalence is established, the paper does not explore practical implications such as whether combining elements of both approaches could yield architectural or methodological innovations. Development and evaluation of hybrid models that integrate components from both frameworks would resolve this.

## Limitations

- The theoretical equivalence between absorbing diffusion and AO-ARMs relies on the assumption that the final noise level approaches infinity, which may not hold in practical implementations
- Empirical validation of the NFE reduction through caching is limited to theoretical analysis rather than comprehensive experimental verification
- The claim that time-independent parameterization works as well as time-dependent approaches needs more rigorous ablation studies

## Confidence

- **High confidence**: The theoretical derivation of the concrete score factorization is mathematically rigorous and well-supported by the proof in Theorem 1
- **Medium confidence**: The practical effectiveness of the reparameterization and caching strategy is demonstrated but lacks comprehensive ablation studies
- **Medium confidence**: The theoretical equivalence to AO-ARMs is proven but the practical implications for model performance are not fully explored

## Next Checks

1. Conduct ablation studies comparing RADD with time-independent parameterization against SEDD with time-dependent parameterization, controlling for all other variables
2. Measure actual NFE reduction on sample sequences with varying lengths and masking patterns to validate the theoretical E-NFE analysis
3. Test the robustness of RADD's performance when the final noise level is finite rather than approaching infinity, as assumed in the theoretical analysis