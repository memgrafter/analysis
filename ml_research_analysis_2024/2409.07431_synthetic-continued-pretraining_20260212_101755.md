---
ver: rpa2
title: Synthetic continued pretraining
arxiv_id: '2409.07431'
source_url: https://arxiv.org/abs/2409.07431
tags:
- synthetic
- entigraph
- https
- knowledge
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Synthetic continued pretraining converts a small, niche corpus
  into a larger synthetic one using knowledge graphs and entity relations, enabling
  data-efficient learning. EntiGraph, the proposed method, extracts entities from
  documents and uses language models to generate diverse text describing their relationships.
---

# Synthetic continued pretraining

## Quick Facts
- arXiv ID: 2409.07431
- Source URL: https://arxiv.org/abs/2409.07431
- Reference count: 40
- EntiGraph converts niche corpora into synthetic data, achieving 80% of performance with source documents available at inference

## Executive Summary
This paper addresses data scarcity in continued pretraining by proposing EntiGraph, a synthetic data augmentation method that converts small, niche corpora into larger synthetic ones using knowledge graphs and entity relations. The approach extracts salient entities from source documents and generates diverse text by drawing connections between these entities, enabling more data-efficient learning than simple paraphrasing. Evaluated on the QuALITY dataset (1.3M tokens), EntiGraph achieves log-linear improvement in QA accuracy up to 455M synthetic tokens, demonstrating that synthetic data generation can effectively bridge data scarcity in continued pretraining.

## Method Summary
EntiGraph is a synthetic data augmentation algorithm that first breaks down a text corpus into a list of entities, then uses a language model to generate text descriptions about relations among the extracted entities. The method constructs a knowledge graph over these entities and generates synthetic text describing relationships between entity pairs and triplets. This graph-based augmentation enforces diversity through entity-based generation, creating more diverse knowledge representations than simple paraphrasing. The synthetic data is then used for continued pretraining with a replay mechanism to prevent catastrophic forgetting, with the approach being compatible with both QA fine-tuning and instruction tuning for summarization.

## Key Results
- EntiGraph achieves log-linear scaling in QA accuracy up to 455M synthetic tokens on the QuALITY dataset
- The method achieves 80% of the performance of having source documents available at inference
- EntiGraph outperforms simple paraphrasing baselines, likely due to enforced diversity through entity-based generation
- The knowledge acquired through EntiGraph complements retrieval-augmented generation and is compatible with instruction tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic continued pretraining improves data efficiency by converting sparse knowledge representations into dense ones.
- Mechanism: EntiGraph extracts entities from source documents and generates synthetic text describing their relationships, creating diverse representations of the same knowledge.
- Core assumption: Language models can effectively learn from diverse synthetic representations of knowledge that go beyond simple paraphrasing.
- Evidence anchors:
  - [abstract] "EntiGraph, a synthetic data augmentation algorithm that extracts salient entities from the source documents and then generates diverse text by drawing connections between the sampled entities."
  - [section 2.2] "EntiGraph first breaks down a text corpus into a list of entities and then uses a language model to generate text descriptions about relations among the extracted entities"
  - [corpus] Evidence for this mechanism is strong - the paper demonstrates log-linear scaling in QA accuracy up to 455M synthetic tokens.
- Break condition: The mechanism breaks if the language model used for synthesis (LMaug) cannot generate faithful representations of the source knowledge, or if the synthetic data becomes too hallucinated to be useful.

### Mechanism 2
- Claim: Knowledge graph-based synthetic data generation creates more diverse knowledge representations than simple paraphrasing.
- Mechanism: Instead of just rewriting source documents, EntiGraph creates a knowledge graph over extracted entities and generates text describing relationships between entity pairs and triplets.
- Core assumption: The combinatorial relationships in a knowledge graph provide sufficient diversity to enable more data-efficient learning than simple paraphrases.
- Evidence anchors:
  - [abstract] "we find that our graph-based augmentation algorithm outperforms it, likely because our approach enforces diversity through entity-based generation."
  - [section 4.2] "Rephrase CPT scales poorly compared with EntiGraph...suggesting that for synthetic CPT to scale, the synthetic data must be sufficiently diverse."
  - [corpus] The evidence is mixed - while EntiGraph outperforms paraphrasing baselines, we don't have direct evidence that the knowledge graph approach is what causes the improvement versus other forms of diversity injection.
- Break condition: This mechanism breaks if the entity extraction step fails to capture the most important knowledge from source documents, or if the relationship generation step cannot create meaningful connections between entities.

### Mechanism 3
- Claim: Synthetic data generation can "rearrange" knowledge into formats more amenable to learning through next-token prediction.
- Mechanism: By generating synthetic text that explicitly describes relationships between entities, EntiGraph creates data that helps the model learn connections that might be implicit in the original documents.
- Core assumption: The next-token prediction objective struggles to learn certain knowledge patterns that are present in source documents but not explicitly stated in ways the model can easily capture.
- Evidence anchors:
  - [abstract] "we build a simple mathematical model of EntiGraph, and show how synthetic data augmentation can 'rearrange' knowledge to enable more data-efficient learning."
  - [section 6] Mathematical model showing mixture-of-exponential scaling behavior
  - [corpus] The mathematical model provides theoretical support, but we would need more direct evidence showing that specific knowledge patterns are learned better from synthetic data versus source documents.
- Break condition: This mechanism breaks if the synthetic data generation process doesn't actually create representations that are more learnable, or if the model can learn equally well from the original source documents with sufficient training.

## Foundational Learning

- Concept: Knowledge graph construction and traversal
  - Why needed here: Understanding how EntiGraph uses knowledge graphs to generate diverse synthetic data
  - Quick check question: How does the breadth-first search in the mathematical model relate to EntiGraph's entity relationship generation?

- Concept: Catastrophic forgetting in continual pretraining
  - Why needed here: Understanding why replay with RedPajama data is used and how it affects knowledge acquisition
  - Quick check question: What role does the replay rate play in balancing new knowledge acquisition with preserving pretrained knowledge?

- Concept: Mixture-of-exponential scaling behavior
  - Why needed here: Understanding the mathematical model that explains EntiGraph's scaling properties
  - Quick check question: How does the mixture-of-exponential formula predict the three distinct phases (linear, log-linear, plateau) observed in EntiGraph scaling?

## Architecture Onboarding

- Component map:
  - Source document corpus (Dsource)
  - Entity extraction module
  - Knowledge graph construction
  - Synthetic data generation (LMaug)
  - Continued pretraining pipeline
  - Evaluation framework (QA and instruction following)

- Critical path:
  1. Extract entities from source documents
  2. Generate synthetic data describing entity relationships
  3. Continue pretraining on synthetic corpus with replay
  4. Evaluate knowledge acquisition through QA and instruction following

- Design tradeoffs:
  - Entity extraction granularity vs. computational cost
  - Synthetic data diversity vs. faithfulness to source knowledge
  - Continued pretraining duration vs. risk of catastrophic forgetting
  - Evaluation comprehensiveness vs. computational expense

- Failure signatures:
  - Poor QA accuracy despite large synthetic corpus indicates issues with synthetic data quality or entity extraction
  - High false claims in summarization indicates hallucination in synthetic data generation
  - Flat scaling curves indicate lack of diversity in synthetic data

- First 3 experiments:
  1. Verify entity extraction correctly identifies key concepts from sample documents
  2. Test synthetic data generation on small entity pairs before scaling to full corpus
  3. Run continued pretraining on small synthetic corpus (e.g., 1M tokens) to verify basic pipeline functionality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can EntiGraph scaling trends be extrapolated beyond 455M synthetic tokens, and what accuracy ceiling might be reached?
- Basis in paper: [explicit] Authors observe log-linear scaling up to 455M tokens but do not test beyond this point.
- Why unresolved: Limited compute budget prevented scaling experiments past 455M tokens, leaving uncertainty about whether the log-linear trend continues or plateaus earlier.
- What evidence would resolve it: Continued pretraining experiments with synthetic token counts beyond 455M, measuring QA accuracy to determine if scaling plateaus or continues log-linearly.

### Open Question 2
- Question: How does EntiGraph's knowledge acquisition compare to retrieval-augmented generation (RAG) when retrieval quality is imperfect (e.g., low recall or precision)?
- Basis in paper: [explicit] Authors show EntiGraph knowledge complements RAG in high-recall settings but do not test imperfect retrieval.
- Why unresolved: Experiments used near-perfect retrieval (recall@8 ≈ 99.6%), so the complementarity of parametric vs. non-parametric knowledge under realistic, noisy retrieval conditions is unknown.
- What evidence would resolve it: RAG experiments with lower retrieval recall/precision, comparing EntiGraph CPT + RAG vs. base model + RAG to measure if parametric knowledge helps compensate for retrieval errors.

### Open Question 3
- Question: Can EntiGraph be applied to highly technical or specialized domains (e.g., research papers) without increased hallucination risk?
- Basis in paper: [inferred] Authors acknowledge hallucination risk increases with more challenging content and note manual fact-checking was only done on QuALITY books.
- Why unresolved: Experiments used general-interest articles; no evaluation on complex, domain-specific texts where LMaug might struggle more with factual accuracy.
- What evidence would resolve it: Applying EntiGraph to technical corpora (e.g., arXiv papers) and measuring hallucination rates via automated or human evaluation, comparing against baseline continued pretraining.

### Open Question 4
- Question: Is it possible to bootstrap EntiGraph without relying on a stronger prompted LM (e.g., gpt-4-turbo) for synthetic data generation?
- Basis in paper: [explicit] Authors state that their approach does not yet enable bootstrapping—using a model to generate its own synthetic data—and view this as future work.
- Why unresolved: Experiments required gpt-4-turbo as LMaug; no ablation or alternative LM experiments were conducted to test if weaker models could achieve similar results.
- What evidence would resolve it: Replicating EntiGraph experiments using smaller or open-source LMs (e.g., Llama 3 8B) as LMaug, measuring QA and instruction-following performance to determine if bootstrapping is feasible.

## Limitations
- Hallucination risk increases with more challenging content and hasn't been systematically evaluated
- The approach depends on a strong language model (gpt-4-turbo) for synthetic data generation, limiting bootstrapping potential
- The mathematical model explaining scaling behavior is validated on one dataset and may not generalize to different knowledge graph structures

## Confidence
- High confidence: The core empirical finding that synthetic continued pretraining with EntiGraph achieves log-linear scaling in QA accuracy up to 455M synthetic tokens
- Medium confidence: The claim that EntiGraph outperforms simple paraphrasing approaches due to enforced diversity through entity-based generation
- Low confidence: The mathematical model's predictions about scaling behavior across different knowledge graph structures and coverage patterns

## Next Checks
1. **Hallucination Impact Study**: Systematically evaluate how hallucination rates in synthetic data correlate with downstream task performance across multiple domains, comparing EntiGraph's hallucination patterns with those of baseline approaches.

2. **Coverage Sensitivity Analysis**: Test EntiGraph on datasets with varying knowledge graph coverage patterns (dense vs. sparse, hierarchical vs. flat) to validate the mathematical model's predictions about how coverage affects scaling behavior.

3. **Replay Rate Optimization**: Conduct a comprehensive study of replay rate effects on knowledge acquisition and preservation across different synthetic corpus sizes to determine optimal replay rates that vary with synthetic data volume.