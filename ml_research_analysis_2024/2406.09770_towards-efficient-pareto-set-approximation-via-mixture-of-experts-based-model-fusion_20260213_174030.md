---
ver: rpa2
title: Towards Efficient Pareto Set Approximation via Mixture of Experts Based Model
  Fusion
arxiv_id: '2406.09770'
source_url: https://arxiv.org/abs/2406.09770
tags:
- pareto
- learning
- task
- tasks
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently approximating
  the entire Pareto front of large deep neural networks for multi-objective optimization
  problems. The authors propose a novel approach using a Pareto weight-ensembling
  mixture of experts (PWE MoE) structure, which leverages model fusion to capture
  trade-offs between multiple objectives.
---

# Towards Efficient Pareto Set Approximation via Mixture of Experts Based Model Fusion

## Quick Facts
- arXiv ID: 2406.09770
- Source URL: https://arxiv.org/abs/2406.09770
- Reference count: 40
- The paper proposes a Pareto weight-ensembling MoE structure to efficiently approximate the entire Pareto front of large deep neural networks for multi-objective optimization problems.

## Executive Summary
This paper addresses the challenge of efficiently approximating the entire Pareto front of large deep neural networks for multi-objective optimization problems. The authors propose a novel approach using a Pareto weight-ensembling mixture of experts (PWE MoE) structure, which leverages model fusion to capture trade-offs between multiple objectives. The method involves upscaling task-specific models into an MoE model, fine-tuning the routers based on preference vectors, and unloading the MoE module for inference. Extensive experiments on vision and language tasks using large-scale models like CLIP-ViT and GPT-2 demonstrate the effectiveness of the approach, showing that it can efficiently approximate the entire Pareto front while reducing computational burden compared to existing methods. The method is scalable to both the number of objectives and the size of the model.

## Method Summary
The paper proposes a two-stage approach for Pareto set approximation. First, task-specific models are upscaled with MoE modules and merged using task arithmetic to create an initial MoE model. Second, the MoE routers are fine-tuned using preference vectors sampled from the standard simplex, with either linear scalarization or exact Pareto optimal search as the objective. The routing weights depend only on the preference vector, allowing the MoE module to be unloaded after training, eliminating additional computational cost during inference. The approach can efficiently approximate the entire Pareto set by interpolating between task-specific models in the parameter space.

## Key Results
- The proposed PWE MoE method can efficiently approximate the entire Pareto front of large neural networks while reducing computational burden compared to existing methods.
- Extensive experiments on vision and language tasks using CLIP-ViT and GPT-2 models demonstrate the effectiveness of the approach.
- The method is scalable to both the number of objectives and the size of the model.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weight-ensembling MoE captures trade-offs between objectives by interpolating between task-specific models in parameter space
- Mechanism: The MoE router learns a nonlinear mapping from preference vectors to routing weights, which are then used to generate parameters for the realization function. This allows the model to approximate the Pareto set by interpolating between task-specific models in the parameter space.
- Core assumption: The Pareto front is convex and can be approximated by a linear combination of task vectors centered at pre-trained weights
- Evidence anchors:
  - [abstract]: "By ensembling the weights of specialized single-task models, the MoE module can effectively capture the trade-offs between multiple objectives and closely approximate the entire Pareto set of large neural networks."
  - [section 4.1]: "The proposed PWE MoE module searches a subspace of the parameter space that is the span of the task vectors centered at pre-trained weights, rather than the entire parameter space."
  - [corpus]: Weak evidence; related papers focus on MoE efficiency rather than Pareto set approximation.
- Break condition: If the Pareto front is highly non-convex or the task vectors are not well-distributed, the approximation may be poor.

### Mechanism 2
- Claim: MoE modules can be unloaded after routing is learned, eliminating additional computational cost during inference
- Mechanism: Once the routers are learned and a preference vector is set, the MoE module can be substituted with the fixed parameters generated by the parameter decoder network, and no additional computation is needed during inference.
- Core assumption: The routing weights depend only on the preference vector and not on the input data
- Evidence anchors:
  - [abstract]: "Once the routers are learned and a preference vector is set, the MoE module can be unloaded, thus no additional computational cost is introduced during inference."
  - [section 4.1]: "Because the routing weight vector w depends only on the preference vector r, the MoE module can be unloaded by substituting the MoE module with F (ϕ∗) after setting a preference vector, and no additional computational cost is introduced during inference."
  - [corpus]: Weak evidence; related papers focus on MoE efficiency rather than inference-time unloading.
- Break condition: If the routing weights depend on the input data, the MoE module cannot be unloaded.

### Mechanism 3
- Claim: Up-scaling only MLP modules is more effective than up-scaling both MLP and Attention blocks for CLIP models
- Mechanism: Attention blocks are more similar to each other than MLP modules in parameter space, as shown by the L2 norm of the delta parameters. This suggests that up-scaling only the MLP modules may be more beneficial, as the MLPs are more diverse and thus task-specific.
- Core assumption: The similarity of parameters in the same block across tasks indicates the diversity of that block for different tasks
- Evidence anchors:
  - [section 4.1]: "Figure 2 shows that Attention blocks are more similar to each other than MLPs, suggesting that up-scaling the MLPs may be more beneficial, as the MLPs are more diverse and thus task-specific."
  - [section 5.1]: "For CLIP-ViT-B/32, up-scaling both the MLP and Attention leads to consistently better performance. However, for CLIP-ViT-L/14, gains from scaling both components are similar, showing that the benefits of up-scaling Attention are less significant in larger models."
  - [corpus]: Weak evidence; related papers focus on MoE efficiency rather than block-specific up-scaling strategies.
- Break condition: If the similarity pattern does not hold for other model architectures or tasks, the assumption may not be valid.

## Foundational Learning

- Concept: Pareto optimality and Pareto set/front
  - Why needed here: Understanding the concept of Pareto optimality is crucial for formulating and solving multi-objective optimization problems, which is the core of this work.
  - Quick check question: What is the difference between a Pareto optimal solution and a Pareto dominated solution?
- Concept: Multi-objective optimization problem (MOOP)
  - Why needed here: The paper addresses the challenge of efficiently approximating the Pareto front of large deep neural networks for MOOPs.
  - Quick check question: How does linear scalarization convert a MOOP into a single-objective optimization problem?
- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: The proposed approach uses a Pareto weight-ensembling MoE structure to approximate the Pareto set of large neural networks.
  - Quick check question: What is the role of the router network in a standard MoE architecture?

## Architecture Onboarding

- Component map: Preference vector → Router network → Routing weights → Parameter decoder network → Realization function → Output
- Critical path: Preference vector → Router network → Routing weights → Parameter decoder network → Realization function → Output
- Design tradeoffs:
  - Up-scaling only MLP modules vs. up-scaling both MLP and Attention blocks: The former may be more effective for CLIP models, but the latter may provide better performance for larger models.
  - Linear scalarization vs. exact Pareto optimal search for router fine-tuning: Linear scalarization is simpler and faster, while exact Pareto optimal search can find more accurate Pareto optimal solutions.
- Failure signatures:
  - Poor approximation of the Pareto front: If the MoE module fails to capture the trade-offs between objectives, the approximated Pareto front may be inaccurate.
  - High computational cost during inference: If the MoE module cannot be unloaded after routing is learned, the additional computational cost may negate the benefits of the approach.
- First 3 experiments:
  1. Implement a simple MoE module with a single MLP expert and a linear router network, and verify that it can approximate a known Pareto front.
  2. Compare the performance of up-scaling only MLP modules vs. up-scaling both MLP and Attention blocks for a small CLIP model on a two-task image classification problem.
  3. Implement the router fine-tuning algorithm with linear scalarization and exact Pareto optimal search, and compare their convergence speed and accuracy on a simple MOOP.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method scale to more than 8 objectives in practice, and what are the computational and memory constraints at that scale?
- Basis in paper: [inferred] The paper mentions scalability to both the number of objectives and model size, but only experiments with up to 8 objectives. Table 8 shows memory usage and training time increasing with the number of tasks.
- Why unresolved: The paper does not provide experiments or analysis for more than 8 objectives, leaving the practical limits of the method unexplored.
- What evidence would resolve it: Experiments with 10+ objectives showing memory usage, training time, and Pareto front approximation quality would clarify the practical scalability limits.

### Open Question 2
- Question: What is the impact of using different router architectures (e.g., deeper networks, attention mechanisms) on the quality of the Pareto front approximation?
- Basis in paper: [explicit] The paper uses a simple MLP-based router and mentions exploring more sophisticated router architectures as future work. Figure 8 shows router weights but does not explore architectural variations.
- Why unresolved: The paper only uses a basic MLP router without comparing to alternative architectures, so the potential benefits of more complex routers remain unknown.
- What evidence would resolve it: Comparative experiments using different router architectures (e.g., deeper MLPs, attention-based routers) on the same tasks would show the impact on Pareto front quality.

### Open Question 3
- Question: How sensitive is the method to the choice of upscaling strategy (e.g., MLP-only vs. Attention+MLP) across different model architectures and task domains?
- Basis in paper: [explicit] The paper discusses two upscaling strategies and shows CLIP-ViT results, noting that upscaling both MLP and Attention generally improves performance but is less significant in larger models.
- Why unresolved: The analysis is limited to CLIP-ViT models, and the paper does not explore how the upscaling strategy affects other architectures (e.g., CNNs, RNNs) or task domains (e.g., NLP vs. vision).
- What evidence would resolve it: Experiments applying the method to diverse model architectures and task domains with both upscaling strategies would reveal the sensitivity and generalizability of the approach.

## Limitations

- The method assumes convex Pareto fronts and relies on task arithmetic for parameter space interpolation, which may not hold for all problem domains.
- The effectiveness of unloading MoE modules after routing training needs empirical validation across diverse task sets and model architectures.
- The computational benefits claimed are primarily theoretical, as actual inference-time measurements are not provided.

## Confidence

**High Confidence:**
- The mathematical formulation of the PWE MoE architecture is sound
- The two-stage approach (model mixing followed by router fine-tuning) is implementable
- Experimental results demonstrate improved Pareto front approximation compared to baseline methods

**Medium Confidence:**
- Claims about unloading MoE modules eliminating inference-time computational cost
- The effectiveness of MLP-only upscaling for CLIP models
- The scalability of the approach to larger model sizes and task numbers

**Low Confidence:**
- The general applicability of the method to highly non-convex Pareto fronts
- Performance guarantees when extending to more than two objectives
- The robustness of the approach across different model architectures beyond CLIP and GPT-2

## Next Checks

1. **Inference-time Cost Validation**: Measure actual inference latency and memory usage with and without MoE unloading on representative hardware platforms to verify the claimed computational benefits.

2. **Non-Convex Pareto Front Testing**: Apply the method to synthetic optimization problems with known non-convex Pareto fronts to assess approximation quality beyond the convex cases tested in the paper.

3. **Multi-Objective Scalability**: Extend experiments to three or more objectives using established multi-objective optimization benchmarks to validate scalability claims and identify potential limitations.