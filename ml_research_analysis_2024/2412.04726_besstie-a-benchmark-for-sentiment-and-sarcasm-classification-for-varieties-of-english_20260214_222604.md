---
ver: rpa2
title: 'BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for Varieties
  of English'
arxiv_id: '2412.04726'
source_url: https://arxiv.org/abs/2412.04726
tags:
- varieties
- language
- sentiment
- sarcasm
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of benchmark datasets for sentiment
  and sarcasm classification in non-standard varieties of English, specifically Australian
  (en-AU), Indian (en-IN), and British (en-UK). The authors introduce BESSTIE, a benchmark
  dataset collected from Google Places reviews and Reddit comments using location-based
  and topic-based filtering methods.
---

# BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for Varieties of English

## Quick Facts
- arXiv ID: 2412.04726
- Source URL: https://arxiv.org/abs/2412.04726
- Authors: Dipankar Srirag; Aditya Joshi; Jordan Painter; Diptesh Kanojia
- Reference count: 40
- One-line primary result: Language models perform significantly better on inner-circle varieties (Australian and British English) than outer-circle varieties (Indian English), especially for sarcasm detection.

## Executive Summary
This paper addresses the lack of benchmark datasets for sentiment and sarcasm classification in non-standard varieties of English, specifically Australian (en-AU), Indian (en-IN), and British (en-UK). The authors introduce BESSTIE, a benchmark dataset collected from Google Places reviews and Reddit comments using location-based and topic-based filtering methods. Native speakers manually annotate the data for sentiment and sarcasm labels. The evaluation involves fine-tuning nine large language models on the dataset and assessing their performance on sentiment and sarcasm classification tasks, revealing significant performance drops in sarcasm detection and cross-variety generalization challenges.

## Method Summary
The study collects Google Places reviews and Reddit comments using location-based filtering for Google data and topic-based filtering for Reddit data to create variety-specific datasets. Native speakers of each variety manually annotate the data for sentiment and sarcasm labels. Nine LLMs (BERT, RoBERTa, ALBERT, MBERT, MDISTIL, XLM-R, GEMMA, MISTRAL, QWEN) are fine-tuned using cross-entropy loss for encoders and zero-shot instruction fine-tuning for decoders. Models are evaluated using macro-averaged F-score on in-variety, cross-variety, and cross-domain test sets.

## Key Results
- Models consistently perform better on inner-circle varieties (en-AU and en-UK) compared to the outer-circle variety (en-IN)
- Sarcasm classification shows significant performance drops across all varieties, with F-SCORE scores typically 15-20 points lower than sentiment classification
- Encoder models outperform decoder models across different tasks, with encoder architecture being inherently better suited for sequence classification
- Cross-variety performance reveals that en-AU and en-UK models can generalize to each other but struggle with en-IN, highlighting the need for variety-specific datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Location-based filtering combined with topic-based filtering improves variety-specific data quality
- Mechanism: Reviews are filtered by location (cities in target countries) and subreddits by topical relevance to each variety, increasing the likelihood that contributors use the appropriate language variety
- Core assumption: Content posted from or about specific locations reflects the linguistic patterns of that region's speakers
- Evidence anchors:
  - [abstract]: "collect datasets for these language varieties using two methods: location-based for Google Places reviews, and topic-based filtering for Reddit comments"
  - [section]: "To create the REDDIT subset, we employ topic-based filtering and choose up to four subreddits per variety... We select these subreddits based on the understanding that they feature popular discussions specific to a variety"
  - [corpus]: Weak evidence - corpus only shows related papers, no direct evidence of filtering effectiveness
- Break condition: If contributors are tourists or non-native speakers posting from target locations, the filtering would fail to capture authentic variety-specific language

### Mechanism 2
- Claim: Native speaker annotation provides more accurate sentiment and sarcasm labels for language varieties
- Mechanism: Annotators who are native speakers of each variety can better understand cultural pragmatics and localized expressions that indicate sentiment or sarcasm
- Core assumption: Language variety includes cultural pragmatics and contextual understanding that non-native speakers would miss
- Evidence anchors:
  - [abstract]: "Native speakers of the language varieties manually annotate the datasets with sentiment and sarcasm labels"
  - [section]: "We hire three annotators each for the three language varieties. The annotators assign the processed reviews and posts two labels: sentiment and sarcasm"
  - [corpus]: Weak evidence - corpus shows related work but no direct evidence of annotation quality improvement
- Break condition: If annotators are not truly representative of the variety or lack cultural familiarity, annotation accuracy would suffer

### Mechanism 3
- Claim: Encoder models outperform decoder models on sequence classification tasks across language varieties
- Mechanism: Encoder architecture is inherently better suited for sequence classification tasks, while decoder architecture is optimized for text generation
- Core assumption: Model architecture properties directly impact task performance regardless of language variety
- Evidence anchors:
  - [section]: "Table 7 shows a consistent trend where encoder models report higher performance than decoder models across different tasks"
  - [section]: "This performance gap is expected as encoder architecture is inherently better suited for sequence classification tasks, while decoder architecture is optimised for text generation tasks"
  - [corpus]: No direct evidence in corpus
- Break condition: If task requirements change to favor generation over classification, decoder models might outperform encoders

## Foundational Learning

- Concept: Language variety vs. dialect distinction
  - Why needed here: The paper specifically addresses "varieties of English" including national varieties (en-AU, en-IN, en-UK) rather than just regional dialects
  - Quick check question: What is the key difference between how "variety" and "dialect" are used in this context, and why does this distinction matter for model evaluation?

- Concept: Inner-circle vs. outer-circle varieties
  - Why needed here: The paper shows significant performance differences between inner-circle (en-AU, en-UK) and outer-circle (en-IN) varieties
  - Quick check question: How does the inner/outer circle distinction relate to the performance gaps observed in the experiments, and what sociolinguistic factors might explain this?

- Concept: Cross-domain vs. cross-variety evaluation
  - Why needed here: The paper distinguishes between generalization across domains (GOOGLE vs REDDIT) and across language varieties
  - Quick check question: What are the key differences in how models perform on cross-domain versus cross-variety tasks, and why might these patterns differ?

## Architecture Onboarding

- Component map: Data collection (Google Places API + Reddit API) -> Annotation (Native speaker annotation) -> Model training (9 LLMs with cross-entropy loss and zero-shot instruction fine-tuning) -> Evaluation (In-variety, cross-variety, and cross-domain testing with F-SCORE metrics) -> Analysis (Performance comparison and error analysis)
- Critical path: Data collection -> Annotation -> Model training -> Evaluation
  - Each step depends on successful completion of the previous one
  - Annotation quality directly impacts model training outcomes
- Design tradeoffs: Location-based filtering vs. topic-based filtering
  - Location-based provides geographic authenticity but may include tourist content
  - Topic-based ensures topical relevance but may miss variety-specific expressions
- Failure signatures: Performance degradation patterns
  - Cross-variety failure: Significant drops when fine-tuning on one variety and testing on another
  - Cross-domain failure: Lower performance when training and testing on different data sources
  - Sarcasm classification failure: Consistently lower F-SCORE across all varieties
- First 3 experiments:
  1. Train BERT on en-AU variety and test on all three varieties to establish baseline cross-variety performance
  2. Train Mistral on en-IN variety and evaluate cross-domain performance (GOOGLE â†’ REDDIT)
  3. Fine-tune a multilingual model on mixed variety data and test individual variety performance to assess generalization capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does sarcasm detection performance vary across different English varieties when using larger, more diverse datasets beyond Google Places reviews and Reddit comments?
- Basis in paper: [explicit] The authors note that sarcasm classification performance is consistently low across all varieties and highlight the need for language variety-specific datasets.
- Why unresolved: The study used a specific dataset from two domains (Google Places reviews and Reddit comments), which may not fully capture the linguistic diversity and cultural nuances of sarcasm across all English varieties.
- What evidence would resolve it: Comparative studies using larger, more diverse datasets from multiple domains and regions to assess sarcasm detection performance across different English varieties.

### Open Question 2
- Question: What are the specific linguistic and cultural factors that contribute to the lower performance of sarcasm detection in outer-circle varieties of English, such as Indian English?
- Basis in paper: [explicit] The authors observe that models perform worse on en-IN, an outer-circle variety, particularly for sarcasm classification.
- Why unresolved: The paper does not delve into the specific linguistic and cultural factors that might influence sarcasm detection performance in outer-circle varieties.
- What evidence would resolve it: Detailed linguistic and cultural analysis of sarcasm expressions in different English varieties, including inner-circle and outer-circle varieties, to identify specific factors affecting detection performance.

### Open Question 3
- Question: How does the performance of multilingual models compare to monolingual models when trained on mixed-domain data across different English varieties?
- Basis in paper: [inferred] The authors note that monolingual models marginally outperform multilingual models, but this performance gap is prominent on the GOOGLE subset, while multilingual models perform better on tasks involving REDDIT subset.
- Why unresolved: The paper does not provide a comprehensive comparison of multilingual and monolingual models when trained on mixed-domain data across different English varieties.
- What evidence would resolve it: Experimental studies comparing the performance of multilingual and monolingual models on mixed-domain data across various English varieties to determine which model type is more effective.

## Limitations
- Dataset construction relies on location-based filtering which may include non-native speakers or tourists posting from target locations, potentially diluting variety-specific linguistic patterns
- The study uses a single model architecture per family (BERT, RoBERTa, etc.) which limits generalizability across different model variants within each family
- Cross-variety performance drops are observed but the specific linguistic features causing these drops are not analyzed in depth

## Confidence

**High Confidence:** The mechanism that encoder architectures outperform decoder architectures on sequence classification tasks is well-established and consistently observed across all experiments in the paper. The claim that native speaker annotation provides more accurate labels is supported by the annotation methodology described.

**Medium Confidence:** The claim about location-based filtering improving variety-specific data quality is supported by the methodology but could be affected by confounding factors like tourist contributions. The performance differences between inner-circle and outer-circle varieties are clearly demonstrated but the exact linguistic or cultural factors causing these differences are not fully explored.

**Low Confidence:** The specific impact of topic-based filtering on capturing variety-specific expressions is not directly validated. The generalizability of results across different model variants within each architecture family remains uncertain.

## Next Checks

1. Conduct a linguistic analysis of misclassified examples to identify specific variety-specific features (idioms, spellings, cultural references) that cause model confusion.

2. Test additional models from each architecture family to verify that performance patterns hold across different model sizes and training regimes.

3. Implement speaker verification for location-based data collection to ensure contributors are authentic native speakers of the target variety rather than tourists or non-native speakers posting from those locations.