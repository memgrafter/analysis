---
ver: rpa2
title: 'Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models
  for Privacy Leakage'
arxiv_id: '2408.17354'
source_url: https://arxiv.org/abs/2408.17354
tags:
- data
- attacks
- dataset
- fine-tuning
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel model poisoning technique that leverages
  machine unlearning to enhance privacy leakage during the fine-tuning of large language
  models. The method increases both membership inference and data extraction attack
  success rates while preserving model utility by inducing overfitting through controlled
  loss maximization on noisy versions of fine-tuning data.
---

# Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage

## Quick Facts
- arXiv ID: 2408.17354
- Source URL: https://arxiv.org/abs/2408.17354
- Reference count: 12
- Primary result: Novel model poisoning technique using unlearning to increase membership inference and data extraction attack success rates by 7.5-18% AUC and up to 100% NSR while preserving model utility

## Executive Summary
This paper introduces a novel model poisoning technique that leverages machine unlearning to enhance privacy leakage during the fine-tuning of large language models. The method increases both membership inference and data extraction attack success rates while preserving model utility by inducing overfitting through controlled loss maximization on noisy versions of fine-tuning data. Experimental results show significant improvements over baselines across various models, datasets, and fine-tuning methods. The attacks remain effective even against differential privacy defenses, though at the cost of reduced model utility.

## Method Summary
The method involves poisoning pre-trained language models through bounded unlearning using gradient ascent to maximize loss on noisy versions of the fine-tuning data while constraining loss on a reference dataset to preserve utility. This induced overfitting amplifies the loss difference between member and non-member data during subsequent fine-tuning, enhancing membership inference attacks. The same mechanism increases verbatim memorization, benefiting data extraction attacks. The poisoning occurs before fine-tuning, and attacks are evaluated after fine-tuning is complete.

## Key Results
- Membership inference attack AUC increases by 7.5-18% compared to baselines
- Data extraction attack success rates improve by up to 100% in terms of successful reconstructions
- The method remains effective against differential privacy defenses, though with reduced attack success
- Model utility is preserved as measured by validation perplexity, with minimal degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Poisoning the pre-trained model via unlearning