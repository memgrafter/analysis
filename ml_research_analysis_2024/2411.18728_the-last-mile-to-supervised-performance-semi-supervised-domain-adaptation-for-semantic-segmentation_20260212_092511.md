---
ver: rpa2
title: 'The Last Mile to Supervised Performance: Semi-Supervised Domain Adaptation
  for Semantic Segmentation'
arxiv_id: '2411.18728'
source_url: https://arxiv.org/abs/2411.18728
tags:
- ssda
- target
- labels
- domain
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of reducing annotation cost in semantic
  segmentation by studying the Semi-Supervised Domain Adaptation (SSDA) setting. SSDA
  combines unlabeled target data, a few target labels, and source data with labels
  to train models.
---

# The Last Mile to Supervised Performance: Semi-Supervised Domain Adaptation for Semantic Segmentation

## Quick Facts
- **arXiv ID**: 2411.18728
- **Source URL**: https://arxiv.org/abs/2411.18728
- **Reference count**: 35
- **Primary result**: Achieves near-supervised performance (64.3 mIoU) with only 50 target labels in GTA→Cityscapes

## Executive Summary
This paper addresses the challenge of reducing annotation costs in semantic segmentation by studying Semi-Supervised Domain Adaptation (SSDA). The authors propose a simple yet effective framework that combines consistency regularization, pixel contrastive learning, and iterative self-training. By leveraging unlabeled target data alongside a few target labels and source data with labels, the method significantly outperforms prior SSDA approaches and achieves near-supervised performance with minimal annotation. The framework demonstrates strong generalization across multiple dataset pairs without requiring hyperparameter tuning.

## Method Summary
The proposed SSDA framework trains a student model using supervised loss on labeled data, consistency regularization with a mean teacher, and pixel contrastive learning on target labels only. The method employs iterative self-training where high-confidence pseudolabels generated by the teacher model are added to the target labeled set in each round. Key design choices include mixing source and target batches for domain robustness, using class weights to mitigate imbalance in contrastive learning, and setting a high confidence threshold (τ=0.9) for pseudolabel selection. The framework is trained using SGD with learning rate 1e-3, batch size 6, and 40k initial steps followed by 80k self-training steps.

## Key Results
- SSDA substantially outperforms UDA even with few labels (+6.9 mIoU improvement)
- Achieves near-supervised performance with only 50 target labels (64.3 mIoU vs 67.0 mIoU fully supervised)
- The proposed framework effectively leverages source data to improve SSL performance (+9.0 mIoU at 50 labels)
- Generalizes well to multiple dataset pairs (Synthia→Cityscapes, GTA→BDD, Synthia→BDD) without hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consistency regularization (CR) is the most important component for SSDA performance.
- Mechanism: CR encourages compact clustering of target representations by promoting consistent predictions between different augmentations of an unlabeled image. This aligns with the cluster assumption in semi-supervised learning.
- Core assumption: The model learns invariant representations that cluster similar pixels together in the latent space, and decision boundaries lie in low-density regions between clusters.
- Evidence anchors:
  - [section] "we find that consistency regularization is by far the most important element, as removingLCR results in−8 mIoU" (Tab. 5).
  - [abstract] "We propose a simple SSDA framework that combines consistency regularization, pixel contrastive learning, and self-training to effectively utilize a few target-domain labels."
- Break condition: If the augmentations used for CR do not sufficiently perturb the input space, the consistency constraint may be too weak to enforce cluster separation.

### Mechanism 2
- Claim: Supervised pixel contrastive learning (PCL) improves clusterability by pulling together positive pairs of pixels from the same class while pushing apart negative pairs from different classes.
- Mechanism: PCL creates tighter clusters for each class in the embedding space, making the classification task easier. It only uses labeled target data to avoid noise from incorrect pseudo-labels.
- Core assumption: Ground-truth labels are available and accurate enough to form meaningful positive and negative pairs for contrastive learning.
- Evidence anchors:
  - [section] "We find it important to use class weights to mitigate class imbalance (−2.3 mIoU), and to mix source and target data in the same batch inLsup (−0.8 mIoU), which encourages domain mixing (Chen et al., 2021a) and helps learn a more domain-robust segmentor."
  - [abstract] "Our method outperforms prior art in the popular GTA→Cityscapes benchmark and shows that as little as 50 target labels can suffice to achieve near-supervised performance."
- Break condition: If the number of labels per class is too low, the contrastive pairs may be insufficient to form meaningful clusters, leading to noisy gradients.

### Mechanism 3
- Claim: Iterative self-training increases the diversity of target labeled samples, which is particularly beneficial when very few target labels are available.
- Mechanism: In each self-training round, the model generates pseudolabels for unlabeled images, which are then added to the target labeled set. This increases the pool of labeled samples and improves training efficiency.
- Core assumption: The pseudolabels generated by the model have sufficient quality (confidence above threshold τ=0.9) to be useful for training.
- Evidence anchors:
  - [section] "We hypothesize that the main benefit of using pseudolabels is increasing diversity in target samples, which becomes more valuable at low labeling ratios, explaining the larger benefit at 50 target labels."
  - [section] "In Fig. 4 we break down the impact of self-training. The first round of self-training (betweenM0 and M1) is the most effective, while the second round offers marginal to no improvement, indicating convergence of the self-training algorithm."
- Break condition: If the pseudolabels have low confidence or contain significant noise, they may introduce bias or degrade the model's performance, especially in later self-training rounds.

## Foundational Learning

- Concept: Cluster assumption in semi-supervised learning
  - Why needed here: The method relies on the idea that similar pixels should be clustered together in the latent space, and decision boundaries should lie in low-density regions. This is the foundation for both consistency regularization and pixel contrastive learning.
  - Quick check question: Why does consistency regularization encourage compact clusters of representations?

- Concept: Domain adaptation and domain alignment
  - Why needed here: The method uses source labeled data and aims to learn domain-robust representations that generalize to both source and target domains. This is crucial for leveraging the additional source data effectively.
  - Quick check question: How does mixing source and target batches in the supervised loss help with domain alignment?

- Concept: Self-training and pseudolabel generation
  - Why needed here: The method uses iterative self-training to increase the diversity of target labeled samples. This is particularly important when very few target labels are available, as it reduces the risk of overfitting to the limited annotations.
  - Quick check question: Why is it important to use a confidence threshold when generating pseudolabels for self-training?

## Architecture Onboarding

- Component map:
  - Student model (fθ) -> Teacher model (fξ) -> Projection head (fproj) -> Self-training module

- Critical path:
  1. Initialize student and teacher models.
  2. Train student model on source labeled data, target labeled data, and target unlabeled data using supervised loss, consistency regularization, and pixel contrastive learning.
  3. Generate pseudolabels for unlabeled images using the teacher model.
  4. Add high-confidence pseudolabels to the target labeled set.
  5. Repeat steps 2-4 for a fixed number of self-training rounds.
  6. Ensemble the final models from each round for inference.

- Design tradeoffs:
  - Using a mean teacher vs. a single model: The mean teacher provides more stable pseudo-targets but adds computational overhead.
  - Supervised vs. unsupervised pixel contrastive learning: Supervised PCL relies on ground-truth labels and avoids noise from incorrect pseudo-labels, but requires more labeled data.
  - Self-training rounds: More rounds can improve performance but also increase computational cost and risk of accumulating noise in pseudolabels.

- Failure signatures:
  - Low performance on rare classes: May indicate overfitting to common classes or insufficient diversity in the labeled set.
  - Unstable training or divergence: May be caused by noisy pseudolabels, incorrect hyperparameters, or insufficient regularization.
  - Poor generalization to target domain: May indicate insufficient domain alignment or overfitting to source domain.

- First 3 experiments:
  1. Train the model with only supervised loss on source and target labeled data. This establishes a baseline for the fully supervised performance.
  2. Add consistency regularization to the training and compare the performance. This isolates the impact of CR on the model's ability to leverage unlabeled target data.
  3. Add pixel contrastive learning to the training and compare the performance. This isolates the impact of PCL on the model's ability to form compact clusters of representations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed SSDA framework perform on Transformer-based architectures like DAFormer, and what specific modifications are needed to adapt it effectively?
- Basis in paper: [explicit] The paper mentions that their framework underperforms on Transformer architectures in the UDA setting due to overfitting to common classes, and suggests future work could address designing an SSDA method tailored to Transformers.
- Why unresolved: The current framework is designed for DeepLabv2 with ResNet-101, and the authors do not explore its application to Transformer-based architectures in detail.
- What evidence would resolve it: Experimental results comparing the proposed SSDA framework on Transformer architectures (e.g., DAFormer) with and without specific modifications, such as additional regularization terms or architectural changes to prevent overfitting to common classes.

### Open Question 2
- Question: What is the impact of using high-resolution images in SSDA, and how does it affect the performance of the proposed framework compared to using downscaled images?
- Basis in paper: [explicit] The paper discusses the trend of using high-resolution images in UDA and mentions that training on high-resolution images can boost performance due to more detailed images, but also notes that it requires more computational resources and careful design to avoid overfitting.
- Why unresolved: The paper only provides preliminary results on high-resolution images in the UDA setting, and does not explore its impact in the SSDA setting or compare it to using downscaled images in detail.
- What evidence would resolve it: Experimental results comparing the proposed SSDA framework on high-resolution and downscaled images, including analysis of performance gains, computational requirements, and any necessary modifications to the framework.

### Open Question 3
- Question: How does the proposed SSDA framework compare to other semi-supervised learning methods in terms of performance and efficiency, especially when using different numbers of target labels?
- Basis in paper: [explicit] The paper compares the proposed SSDA framework to SSL methods and demonstrates that SSDA outperforms SSL, particularly when very few target labels are available. However, the paper does not provide a comprehensive comparison to a wide range of SSL methods or analyze the efficiency of the framework in detail.
- Why unresolved: The paper focuses on comparing the proposed framework to a specific SSL method (Alonso et al., 2021) and does not explore its performance and efficiency relative to other SSL methods or analyze the impact of different numbers of target labels in detail.
- What evidence would resolve it: A comprehensive comparison of the proposed SSDA framework to various SSL methods on multiple datasets, including analysis of performance, efficiency, and the impact of different numbers of target labels on the framework's effectiveness.

## Limitations

- Limited analysis of pseudolabel quality and error propagation across self-training rounds
- Incomplete understanding of domain alignment mechanisms, particularly when mixing source and target data
- No exploration of the framework's performance on Transformer-based architectures

## Confidence

- **High confidence**: The claim that SSDA outperforms UDA with few labels (+6.9 mIoU) is well-supported by experimental results across multiple benchmarks.
- **Medium confidence**: The claim that CR is the most important component is supported by ablation studies but could benefit from more granular analysis of specific design choices.
- **Medium confidence**: The claim that the framework generalizes well to other datasets without hyperparameter tuning is supported by results but would benefit from additional datasets for stronger validation.

## Next Checks

1. **Pseudolabel quality analysis**: Conduct a detailed analysis of pseudolabel quality (precision/recall) across self-training rounds to understand error propagation and identify optimal stopping criteria.

2. **Domain alignment ablation**: Perform controlled experiments isolating the impact of source-target batch mixing in the supervised loss to better understand domain alignment mechanisms.

3. **Architecture sensitivity**: Test the framework with different backbone architectures (e.g., Swin Transformer vs. ResNet) to assess robustness to architectural changes and identify potential overfitting patterns.