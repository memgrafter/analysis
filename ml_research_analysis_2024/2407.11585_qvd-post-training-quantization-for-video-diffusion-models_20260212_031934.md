---
ver: rpa2
title: 'QVD: Post-training Quantization for Video Diffusion Models'
arxiv_id: '2407.11585'
source_url: https://arxiv.org/abs/2407.11585
tags:
- quantization
- diffusion
- video
- temporal
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of post-training quantization
  (PTQ) for video diffusion models (VDMs), which suffer from high latency and extensive
  memory consumption due to processing multiple frame features concurrently. The authors
  identify two key issues: pronounced skewness in temporal features and significant
  inter-channel disparities in activation, leading to low quantization level coverage.'
---

# QVD: Post-training Quantization for Video Diffusion Models

## Quick Facts
- arXiv ID: 2407.11585
- Source URL: https://arxiv.org/abs/2407.11585
- Reference count: 40
- QVD achieves near-lossless 8-bit quantization with 205.32 decrease in FVD compared to existing methods

## Executive Summary
Video diffusion models (VDMs) suffer from high latency and memory consumption due to processing multiple frame features concurrently. This paper addresses post-training quantization (PTQ) for VDMs, which existing methods struggle with due to pronounced skewness in temporal features and significant inter-channel disparities in activations. The authors propose QVD, a PTQ framework that includes High Temporal Discriminability Quantization (HTDQ) to preserve temporal feature discriminability and Scattered Channel Range Integration (SCRI) to improve quantization level coverage across channels. Experimental results on various models and datasets demonstrate QVD's effectiveness, achieving near-lossless performance with 8-bit quantization.

## Method Summary
QVD introduces two key components: HTDQ and SCRI. HTDQ uses a modified logarithmic quantizer with a relaxation coefficient to preserve temporal feature discriminability by allocating more quantization levels to values near zero. SCRI improves quantization level coverage by integrating activations through per-channel scaling that amplifies individual channel ranges relative to the overall activation range. The framework is applied to MagicAnimate and AnimateDiff models using calibration data from TED-talks, FS-COCO, and COCO Captions datasets. The optimization process uses a composite metric balancing L2 quantization loss against temporal discriminability measured by TDScore.

## Key Results
- QVD achieves near-lossless 8-bit quantization (W8A8) with only 205.32 decrease in FVD
- Maintains high performance across FID-VID, CLIP metrics, and motion smoothness measures
- Reduces bit operations per video diffusion model while preserving generation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HTDQ preserves temporal feature discriminability by using a logarithmic quantizer with a relaxation coefficient to allocate more quantization levels to values near zero
- Core assumption: Preserving discriminability of small temporal feature values is critical for maintaining video generation quality
- Evidence: [abstract] "temporal features...exhibit pronounced skewness"; [section 4.1] "outliers often exceed thousands of times the magnitude of regular values"; [section 4.1.2] "allocates more quantization levels to values concentrated around zero"
- Break condition: Poor relaxation coefficient tuning could fail to reduce data concentration or distort temporal feature distribution

### Mechanism 2
- Claim: SCRI improves quantization level coverage across individual channels by integrating activations through per-channel scaling
- Core assumption: Increasing individual channel ranges while maintaining relative activation patterns will improve quantization quality
- Evidence: [abstract] "significant inter-channel disparities...resulting in low coverage of quantization levels"; [section 4.2] "activations...exhibit discrete and asymmetric characteristics"; [section 4.2] "SCRI significantly enhances the coverage of quantization levels by individual channels"
- Break condition: Incorrect per-channel scaling factors could fail to improve coverage or distort activation distribution

### Mechanism 3
- Claim: TDScore provides a metric to evaluate and optimize temporal feature discriminability during quantization
- Core assumption: A quantitative measure of temporal feature similarity can effectively guide quantization parameter optimization
- Evidence: [section 4.1.1] "evaluates the similarity between the current temporal feature and several adjacent temporal features"; [section 4.1.2] "composite metric K...incorporates both TDScore and L2 loss"
- Break condition: Poor correlation between TDScore and actual video generation quality could lead to suboptimal quantization parameters

## Foundational Learning

- Concept: Diffusion models and the forward/reverse process
  - Why needed here: Understanding how video diffusion models work is essential to grasp why temporal features are critical
  - Quick check: What is the main difference between the forward and reverse processes in diffusion models?

- Concept: Quantization techniques (uniform vs logarithmic)
  - Why needed here: The paper specifically contrasts uniform quantization with logarithmic quantization
  - Quick check: How does logarithmic quantization differ from uniform quantization in terms of interval spacing?

- Concept: Channel-wise operations and activation distributions
  - Why needed here: SCRI specifically addresses inter-channel disparities in activation distributions
  - Quick check: Why would having narrow activation ranges in individual channels lead to poor quantization coverage?

## Architecture Onboarding

- Component map: HTDQ (HiDi-TQ quantizer + TDScore metric) → Temporal feature fusion → SCRI (per-channel scaling) → Temporal attention module → Denoising process
- Critical path: Temporal feature quantization → Frame feature fusion → Temporal attention module → SCRI scaling → Denoising process
- Design tradeoffs:
  - Precision vs. discriminability: HTDQ prioritizes small value preservation over large value accuracy
  - Computational overhead: SCRI adds minimal overhead through equivalent transformations
  - Calibration complexity: Requires grid search for optimal parameters
- Failure signatures:
  - Model collapse or artifacts when temporal features lose discriminability
  - Inconsistent video generation quality across different frame positions
  - Increased quantization error when inter-channel variations aren't properly addressed
- First 3 experiments:
  1. Compare HTDQ vs. uniform quantization on temporal features while keeping SCRI constant
  2. Test SCRI with different scaling factors on a simplified temporal attention module
  3. Evaluate the impact of TDScore optimization on overall video generation quality across different bit-width settings

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations and scope of the work, several areas remain unexplored:
- Generalization of QVD to other generative model architectures beyond video diffusion
- Impact of varying calibration time steps on quantization performance and efficiency
- Performance on video diffusion models with alternative temporal attention mechanisms

## Limitations
- Focuses primarily on W8A8 quantization without extensive exploration of other bit-width configurations
- Limited computational overhead analysis beyond BOPs without detailed hardware latency measurements
- Evaluation relies mainly on standard metrics without qualitative analysis of generated video quality or edge cases

## Confidence
- High Confidence: Identification of skewness and inter-channel disparities as key challenges is well-supported; experimental results are robust across models and datasets
- Medium Confidence: Theoretical soundness of HTDQ and SCRI mechanisms; need for more ablation studies to isolate individual contributions
- Low Confidence: Generalization to other video diffusion architectures remains unproven; calibration process may not capture full diversity of real-world scenarios

## Next Checks
1. Conduct ablation studies isolating HTDQ and SCRI contributions by testing each component independently and in combination across different video generation tasks
2. Validate TDScore's correlation with actual video quality through user studies comparing videos generated with different TDScore-optimized quantization parameters
3. Test QVD's generalization to other video diffusion architectures (e.g., SVD, Video Diffusion Models) and evaluate performance degradation on out-of-distribution video content