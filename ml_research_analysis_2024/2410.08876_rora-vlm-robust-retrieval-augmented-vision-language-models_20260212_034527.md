---
ver: rpa2
title: 'RoRA-VLM: Robust Retrieval-Augmented Vision Language Models'
arxiv_id: '2410.08876'
source_url: https://arxiv.org/abs/2410.08876
tags:
- visual
- knowledge
- image
- query
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents RORA-VLM, a robust retrieval-augmented vision-language
  model designed to improve performance on knowledge-intensive tasks. The key innovations
  include: (1) a 2-stage retrieval process with image-anchored textual-query expansion
  that synergistically combines visual and textual information to retrieve relevant
  multimodal knowledge snippets; and (2) a robust retrieval augmentation method that
  enhances resilience to irrelevant information through adversarial noise injection
  during training and query-oriented visual token refinement to filter extraneous
  visual content.'
---

# RoRA-VLM: Robust Retrieval-Augmented Vision Language Models

## Quick Facts
- arXiv ID: 2410.08876
- Source URL: https://arxiv.org/abs/2410.08876
- Authors: Jingyuan Qi; Zhiyang Xu; Rulin Shao; Yang Chen; Jin Di; Yu Cheng; Qifan Wang; Lifu Huang
- Reference count: 16
- Key outcome: 7B parameter model outperforms 55B parameter state-of-the-art models on knowledge-intensive vision-language tasks

## Executive Summary
This paper presents RoRA-VLM, a robust retrieval-augmented vision-language model designed to improve performance on knowledge-intensive tasks. The key innovations include a 2-stage retrieval process with image-anchored textual-query expansion that synergistically combines visual and textual information, and a robust retrieval augmentation method that enhances resilience to irrelevant information through adversarial noise injection during training and query-oriented visual token refinement. The proposed framework is evaluated on three widely adopted benchmarks: OVEN, InfoSeek, and Enc-VQA. Results demonstrate that RoRA-VLM, even with only 7B parameters and fine-tuned on less than 10,000 instances per dataset, significantly outperforms state-of-the-art retrieval-augmented VLMs with much larger model sizes (up to 55B parameters) and substantially more training instances (up to 1 million).

## Method Summary
RoRA-VLM addresses the challenges of modality discrepancy and irrelevant information in knowledge-intensive vision-language tasks through a novel two-stage retrieval process and robust augmentation method. The two-stage retrieval uses image-anchored textual-query expansion to retrieve multimodal knowledge snippets, while the robust augmentation employs adversarial noise injection during training and query-oriented visual token refinement to filter extraneous visual information. The framework is built on LLaVA-v1.5-7B and fine-tuned on less than 10,000 instances per dataset, achieving state-of-the-art performance across three benchmarks while demonstrating strong zero-shot domain transfer capability.

## Key Results
- Achieves up to 14.36% accuracy improvement over state-of-the-art retrieval-augmented VLMs
- Outperforms Wiki-LLaVA across all benchmarks despite using only 7B parameters vs 55B parameters
- Demonstrates strong zero-shot domain transfer capability to unseen knowledge-intensive tasks
- Shows less sensitivity to number of retrieved knowledge snippets compared to single-stage retrieval approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage retrieval process with image-anchored textual-query expansion effectively addresses the modality discrepancy problem by leveraging visual information to disambiguate ambiguous textual queries.
- Mechanism: The first stage uses the query image as an anchor to retrieve visually similar images from a large database, extracting their associated entity names and descriptions. These are then used to expand the original textual query, resolving anaphoric references and generic terms. The expanded query is used in the second stage to retrieve relevant textual knowledge snippets from a textual knowledge base.
- Core assumption: Visual similarity between the query image and retrieved images correlates with semantic similarity in terms of the entities depicted, allowing entity names and descriptions to serve as effective query expansions.
- Evidence anchors:
  - [abstract] "a 2-stage retrieval process with image-anchored textual-query expansion that synergistically combines visual and textual information to retrieve relevant multimodal knowledge snippets"
  - [section] "Stage-1: Image-anchored Entity Retrieval...Stage-2: Query-expanded Text Retrieval"
  - [corpus] Weak - the related papers mention multimodal retrieval but do not specifically address the two-stage approach with image anchoring
- Break condition: If the visual similarity metric (CLIP embeddings) fails to capture semantic similarity between images, or if the retrieved images do not contain relevant entities that can disambiguate the query, the expansion will not improve retrieval accuracy.

### Mechanism 2
- Claim: The adversarial noise injection during training enables the model to learn resilience to irrelevant information in retrieved multimodal knowledge snippets by forcing it to selectively utilize relevant knowledge.
- Mechanism: During training, irrelevant knowledge snippets are intentionally introduced alongside relevant ones. The model is trained to predict correct answers despite this noise, implicitly learning to compare visual appearances between the query image and retrieved images to discard irrelevant textual knowledge.
- Core assumption: The model can learn to associate visual features with textual relevance through exposure to noisy training examples, enabling it to filter out irrelevant information during inference.
- Evidence anchors:
  - [abstract] "a robust retrieval augmentation method that enhances resilience to irrelevant information through adversarial noise injection during training"
  - [section] "Adversarial Noise Injection for Robust Augmentation...forces the model to become resilient to noises"
  - [corpus] Weak - related papers mention retrieval augmentation but do not specifically discuss adversarial noise injection for robustness
- Break condition: If the model fails to learn the visual-textual relevance association from noisy examples, or if the irrelevant information is too dominant, the model may still be misled by noise during inference.

### Mechanism 3
- Claim: The query-oriented visual token refinement strategy filters out extraneous visual information by selecting only the most relevant visual tokens to the text query, improving the model's focus on key entities.
- Mechanism: For both the query image and retrieved images, visual tokens are ranked by their similarity to the text query embedding. Only the top-ranked tokens are retained, filtering out background objects and unrelated entities.
- Core assumption: Visual tokens corresponding to the key entities in an image will have higher similarity to the text query than tokens corresponding to background or unrelated objects.
- Evidence anchors:
  - [abstract] "filters out extraneous visual information, such as unrelated entities presented in images, via a query-oriented visual token refinement strategy"
  - [section] "VLMs typically encode each input image into a sequence of n visual tokens...We refine the visual tokens of the query image by only keeping M tokens that are most related to the text query"
  - [corpus] Weak - related papers mention visual token processing but do not specifically address query-oriented refinement for filtering extraneous information
- Break condition: If the CLIP embedding similarity does not accurately reflect semantic relevance between visual tokens and text queries, important visual information may be filtered out while irrelevant information remains.

## Foundational Learning

- Concept: Multimodal embedding spaces and cross-modal retrieval
  - Why needed here: The method relies on CLIP embeddings to measure visual similarity and align visual and textual information. Understanding how these embeddings capture semantic relationships is crucial for the two-stage retrieval process.
  - Quick check question: How does CLIP's contrastive training objective create a shared embedding space for images and text, and what are its limitations for cross-modal retrieval tasks?

- Concept: Adversarial training and robust model development
  - Why needed here: The adversarial noise injection technique requires understanding how models learn to be robust to noise through exposure to corrupted training examples. This is fundamental to the noise-resilient augmentation method.
  - Quick check question: What is the theoretical basis for why training with adversarial noise can improve a model's ability to handle real-world noise during inference?

- Concept: Visual attention and token selection in transformer-based vision models
  - Why needed here: The query-oriented visual token refinement relies on selecting relevant visual tokens based on their similarity to text queries. Understanding how visual tokens represent image regions and how attention mechanisms work is essential.
  - Quick check question: How do visual tokens in CLIP-based models correspond to image patches, and what determines their relevance to a given text query?

## Architecture Onboarding

- Component map: Input -> Stage 1 Retrieval -> Stage 2 Retrieval -> Token Refinement -> VLM Processing -> Answer Generation
- Critical path: Input → Stage 1 Retrieval → Stage 2 Retrieval → Token Refinement → VLM Processing → Answer Generation
- Design tradeoffs:
  - Retrieval quality vs. computational cost: More retrieved snippets improve coverage but increase processing time
  - Token refinement aggressiveness vs. information loss: More aggressive filtering reduces noise but may remove relevant details
  - Noise injection level vs. training stability: More noise improves robustness but may slow convergence
- Failure signatures:
  - Poor retrieval precision: Check CLIP embedding quality and database indexing
  - Model still confused by noise: Verify noise injection implementation and token refinement effectiveness
  - Slow inference: Profile retrieval stages and consider caching strategies
- First 3 experiments:
  1. Baseline retrieval precision test: Measure entity matching accuracy for both retrieval stages on a held-out validation set
  2. Ablation study for token refinement: Compare performance with different numbers of retained tokens (m values)
  3. Noise injection sensitivity: Test model performance with varying levels of noise injection during training to find optimal robustness-accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the two-stage retrieval process compare to a single-stage retrieval process in terms of retrieval precision and model performance?
- Basis in paper: [explicit] The paper states that the two-stage retrieval process with image-anchored textual-query expansion improves retrieval precision by up to 11.52% compared to the general single-stage retrieval approach.
- Why unresolved: While the paper reports an improvement in retrieval precision, it does not provide a detailed comparison of the two approaches in terms of model performance on knowledge-intensive tasks. A comprehensive evaluation is needed to understand the full impact of the two-stage retrieval process.
- What evidence would resolve it: A detailed comparison of model performance on knowledge-intensive tasks using both the two-stage and single-stage retrieval processes would provide insights into the effectiveness of the two-stage approach.

### Open Question 2
- Question: What is the impact of the number of retrieved knowledge snippets on model performance?
- Basis in paper: [explicit] The paper mentions that expanding the retrieval from top-4 to top-8 snippets results in marginal improvements, demonstrating the less sensitivity of the two-stage retrieval strategy on the number of retrieved knowledge snippets.
- Why unresolved: The paper does not explore the impact of using more than 8 snippets or fewer than 4 snippets on model performance. It is unclear whether there is an optimal number of snippets that maximizes performance.
- What evidence would resolve it: An experiment evaluating model performance using different numbers of retrieved knowledge snippets (e.g., 2, 4, 8, 16) would provide insights into the optimal number of snippets for knowledge-intensive tasks.

### Open Question 3
- Question: How does the knowledge-intensive pre-training on WikiWeb2M compare to pre-training on other datasets, such as ShareGPT4V, in terms of model performance on knowledge-intensive tasks?
- Basis in paper: [explicit] The paper compares pre-training on WikiWeb2M to pre-training on ShareGPT4V and finds that the former leads to better performance on knowledge-intensive tasks.
- Why unresolved: The paper does not explore the impact of pre-training on other datasets or combinations of datasets. It is unclear whether there are other datasets that could lead to even better performance.
- What evidence would resolve it: An experiment evaluating model performance on knowledge-intensive tasks after pre-training on different datasets or combinations of datasets would provide insights into the best pre-training approach.

### Open Question 4
- Question: How does the query-oriented visual token refinement strategy impact model performance on knowledge-intensive tasks?
- Basis in paper: [explicit] The paper describes the query-oriented visual token refinement strategy and its purpose of filtering out irrelevant visual information. It reports that removing this strategy leads to a drop in performance on the InfoSeek dataset.
- Why unresolved: The paper does not provide a detailed analysis of how the query-oriented visual token refinement strategy impacts model performance on different types of knowledge-intensive tasks or with different types of visual information.
- What evidence would resolve it: An experiment evaluating model performance on various knowledge-intensive tasks with and without the query-oriented visual token refinement strategy would provide insights into its effectiveness and generalizability.

### Open Question 5
- Question: How does the adversarial noise injection for robust augmentation impact model performance on knowledge-intensive tasks?
- Basis in paper: [explicit] The paper describes the adversarial noise injection for robust augmentation and its purpose of making the model resilient to noise in the retrieved knowledge. It reports that removing this strategy leads to a drop in performance on the InfoSeek dataset.
- Why unresolved: The paper does not provide a detailed analysis of how the adversarial noise injection for robust augmentation impacts model performance on different types of knowledge-intensive tasks or with different levels of noise in the retrieved knowledge.
- What evidence would resolve it: An experiment evaluating model performance on various knowledge-intensive tasks with and without the adversarial noise injection for robust augmentation, and with different levels of noise in the retrieved knowledge, would provide insights into its effectiveness and generalizability.

## Limitations

- Knowledge database construction quality and scalability to other domains/languages is not fully explored
- Specific implementation details of adversarial noise injection are not provided, affecting reproducibility
- Sensitivity of visual token refinement to number of retained tokens and CLIP embedding accuracy is not thoroughly investigated

## Confidence

**High Confidence (8/10):**
- The two-stage retrieval process with image-anchored textual-query expansion effectively improves retrieval precision for knowledge-intensive tasks
- The framework achieves state-of-the-art performance on the tested benchmarks (OVEN, InfoSeek, Enc-VQA)
- The model demonstrates strong zero-shot domain transfer capability

**Medium Confidence (6/10):**
- The adversarial noise injection method significantly improves robustness to irrelevant information
- The query-oriented visual token refinement consistently filters extraneous visual content without losing relevant information
- The knowledge-intensive pre-training approach effectively aligns visual and textual information

**Low Confidence (4/10):**
- The performance gains would generalize to datasets beyond the three tested benchmarks
- The framework would maintain its effectiveness with substantially larger or different knowledge databases
- The computational efficiency would scale proportionally with model size increases

## Next Checks

1. **Ablation Study on Knowledge Database Quality**: Systematically vary the size, domain coverage, and quality of the external knowledge database to quantify its impact on retrieval precision and downstream task performance. This would validate the framework's dependency on knowledge database quality.

2. **Cross-Domain Transfer Evaluation**: Evaluate the zero-shot domain transfer capability on at least 3-5 additional knowledge-intensive visual question answering datasets from diverse domains (e.g., scientific, historical, technical) not seen during training to assess generalizability beyond the tested benchmarks.

3. **Noise Injection Robustness Analysis**: Conduct controlled experiments varying the proportion, type, and distribution of injected irrelevant knowledge snippets during training. Measure the resulting model robustness and identify the optimal noise injection strategy that maximizes resilience without compromising accuracy on clean examples.