---
ver: rpa2
title: 'SCAPE: Searching Conceptual Architecture Prompts using Evolution'
arxiv_id: '2402.00089'
source_url: https://arxiv.org/abs/2402.00089
tags:
- scape
- images
- prompt
- architecture
- dall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SCAPE is a tool that combines evolutionary algorithms with generative
  AI to improve conceptual architecture design. It uses GPT-4 to evolve text prompts
  for DALL-E 3, enabling exploration of novel architectural ideas through a simple
  point-and-click interface.
---

# SCAPE: Searching Conceptual Architecture Prompts using Evolution

## Quick Facts
- arXiv ID: 2402.00089
- Source URL: https://arxiv.org/abs/2402.00089
- Reference count: 0
- SCAPE achieves 67% improvement in image novelty compared to vanilla DALL-E 3

## Executive Summary
SCAPE is a tool that combines evolutionary algorithms with generative AI to improve conceptual architecture design. It uses GPT-4 to evolve text prompts for DALL-E 3, enabling exploration of novel architectural ideas through a simple point-and-click interface. The system automatically extracts architectural attributes from user input and uses evolution to generate diverse, high-quality designs. Testing with 20+ independent architects showed positive feedback, with attribute ratings indicating successful optimization and exploration over multiple iterations.

## Method Summary
SCAPE uses an evolutionary algorithm where GPT-4 generates and evolves text prompts that DALL-E 3 converts into architectural images. The system extracts architectural attributes from user input using GPT-4, then applies mutation and crossover operations to create new prompt variations. Users guide the evolutionary process by selecting preferred images and rating attributes, which directs subsequent generations of prompts. The web-based interface allows architects to explore creative designs without requiring prompt engineering expertise.

## Key Results
- 67% improvement in image novelty compared to vanilla DALL-E 3
- More architecturally plausible results than standard DALL-E 3 output
- Makes exploration easier without requiring prompt engineering expertise
- Positive feedback from 20+ independent architects in testing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's language understanding enables coherent crossover and mutation in prompt space.
- Mechanism: SCAPE uses GPT-4 to generate new attribute values by combining ideas from parent prompts, and to create novel alternatives when mutation is required.
- Core assumption: GPT-4 can generate syntactically valid and semantically coherent architectural attribute values.
- Evidence anchors: [abstract] "making use of the built-in language skills of GPT-4 to vary prompts via text-based mutation and crossover"

### Mechanism 2
- Claim: Human-in-the-loop evaluation directs evolutionary search toward user-desired architectural outcomes.
- Mechanism: Users select preferred parent images and optionally rate attributes as good or bad, which informs crossover and mutation operations.
- Core assumption: User selections and ratings effectively encode preferences that can be translated into genetic operations.
- Evidence anchors: [abstract] "enabling users to explore creative and good quality designs inspired by their initial input through a simple point and click interface"

### Mechanism 3
- Claim: Evolutionary search in prompt space provides controlled randomness that generative AI alone cannot achieve.
- Mechanism: By treating prompts as genomes and applying mutation and crossover operations, SCAPE explores architectural concept space systematically.
- Core assumption: Evolutionary algorithms can effectively navigate the high-dimensional prompt space to find novel architectural concepts.
- Evidence anchors: [abstract] "SCAPE injects randomness into generative AI, and enables memory"

## Foundational Learning

- Concept: Genetic algorithms and evolutionary computation
  - Why needed here: The system treats prompts as genomes and applies evolutionary operators to search for novel architectural concepts
  - Quick check question: What are the key components of a genetic algorithm and how do they apply to evolving text prompts?

- Concept: Prompt engineering and natural language processing
  - Why needed here: The system relies on GPT-4's ability to understand and generate coherent architectural descriptions
  - Quick check question: How does GPT-4's language understanding enable the extraction and generation of architectural attributes from text prompts?

- Concept: Human-computer interaction and user-centered design
  - Why needed here: The system is designed for architects who may not be technical experts, requiring an intuitive interface
  - Quick check question: What design principles ensure that non-technical users can effectively guide the evolutionary process through simple selection and rating?

## Architecture Onboarding

- Component map: User input -> GPT-4 attribute extraction -> DALL-E 3 image generation -> User selection/rating -> GPT-4 crossover/mutation -> Repeat
- Critical path: User input → GPT-4 attribute generation → DALL-E 3 image generation → User selection/rating → GPT-4 crossover/mutation → Repeat until user satisfaction
- Design tradeoffs:
  - Using GPT-4 for all genetic operations provides semantic coherence but may introduce bias or reduce diversity
  - Simple point-and-click interface sacrifices fine-grained control for accessibility
  - Evolutionary approach requires multiple iterations, which may be slower than direct generation
- Failure signatures:
  - GPT-4 generates incoherent or repetitive attribute values
  - DALL-E 3 produces architecturally implausible images despite valid prompts
  - Users cannot effectively communicate preferences through selection and rating
  - System becomes too slow due to API rate limits or computational overhead
- First 3 experiments:
  1. Test single iteration pipeline: User input → GPT-4 attribute generation → DALL-E 3 image generation → verify output quality and relevance
  2. Test crossover operation: Generate two parent attribute sets → use GPT-4 to combine → verify semantic coherence of child attributes
  3. Test full evolutionary loop: Run 2-3 iterations with mock user selections → verify that output evolves meaningfully based on selections

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of SCAPE compare to generative AI systems with specialized architectural training data?
- Basis in paper: [explicit] The paper notes that vanilla DALL-E 3 often produces architecturally unrealistic outputs and suggests that finetuning with specialized architectural training data could resolve such issues.
- Why unresolved: This remains untested as the current study used off-the-shelf DALL-E 3 without architectural fine-tuning.
- What evidence would resolve it: A direct comparison between SCAPE and a fine-tuned generative AI system specifically trained on architectural datasets.

### Open Question 2
- Question: How does the effectiveness of SCAPE's evolutionary prompt optimization scale with more iterations beyond the three tested?
- Basis in paper: [explicit] The study found improvements in optimization and exploration over three iterations, but notes that few users continued beyond this point due to rate limits.
- Why unresolved: The rate limit of five images per minute restricted extended testing.
- What evidence would resolve it: Longitudinal user studies with unlimited generation rates tracking novelty and user satisfaction across 10+ iterations.

### Open Question 3
- Question: How would SCAPE perform when integrated with multi-modal input (e.g., combining sketches with text prompts)?
- Basis in paper: [explicit] User feedback suggested adding sketch-to-CGI functionality, noting that architects "do more drawings than writing."
- Why unresolved: The current SCAPE implementation only accepts text input.
- What evidence would resolve it: Comparative studies testing SCAPE with text-only, sketch-only, and combined text-sketch inputs.

## Limitations

- The entire evolutionary mechanism relies on GPT-4's language understanding capabilities, creating a single point of dependency
- Evaluation methodology relies heavily on subjective user feedback rather than rigorous quantitative validation
- Scalability and generalizability remain uncertain, with no analysis of performance across different architectural styles or cultural contexts

## Confidence

**High Confidence**: The core architectural framework and implementation details are well-specified, including the evolutionary algorithm structure, the use of GPT-4 for prompt evolution, and DALL-E 3 for image generation.

**Medium Confidence**: The claimed performance improvements (67% novelty enhancement, better architectural plausibility) are supported by user testing but lack detailed quantitative metrics or comparative studies against alternative approaches.

**Low Confidence**: The scalability and generalizability of the approach remain uncertain, with no exploration of long-term sustainability or potential for user fatigue over multiple iterations.

## Next Checks

1. **Quantitative Performance Validation**: Conduct a controlled experiment comparing SCAPE-generated images against baseline DALL-E 3 outputs using objective metrics for novelty (e.g., CLIP-based similarity scores) and architectural plausibility (expert ratings on a standardized scale), with statistical significance testing across multiple architectural styles and user groups.

2. **Model Robustness Testing**: Systematically test the evolutionary algorithm with different language models (e.g., Claude, Gemini) and varying GPT-4 versions to assess the dependency on specific model capabilities, and evaluate how prompt engineering variations affect the quality and diversity of evolved outputs.

3. **Longitudinal User Study**: Implement a multi-session study tracking architect users over extended periods (2-4 weeks) to measure: a) learning curves and efficiency gains, b) evolution of user preferences and satisfaction, c) cognitive load and fatigue factors, and d) comparative creative output quality between SCAPE and traditional design workflows.