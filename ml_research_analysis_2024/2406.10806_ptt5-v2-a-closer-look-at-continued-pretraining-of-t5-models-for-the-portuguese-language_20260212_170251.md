---
ver: rpa2
title: 'ptt5-v2: A Closer Look at Continued Pretraining of T5 Models for the Portuguese
  Language'
arxiv_id: '2406.10806'
source_url: https://arxiv.org/abs/2406.10806
tags:
- pretraining
- portuguese
- language
- dataset
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ptt5-v2, which continues pretraining T5
  models for Portuguese using a larger corpus (mC4-pt) and a Portuguese-specific tokenizer.
  The authors pretrain models ranging from 60M to 3B parameters and evaluate them
  on three downstream tasks: ASSIN2 RTE, ASSIN2 STS, and TweetSentBR.'
---

# ptt5-v2: A Closer Look at Continued Pretraining of T5 Models for the Portuguese Language

## Quick Facts
- arXiv ID: 2406.10806
- Source URL: https://arxiv.org/abs/2406.10806
- Authors: Marcos Piau; Roberto Lotufo; Rodrigo Nogueira
- Reference count: 40
- Primary result: ptt5-v2 models achieve state-of-the-art results on ASSIN2 RTE and TweetSentBR, with performance gaps narrowing as model capacity increases

## Executive Summary
This paper introduces ptt5-v2, a family of T5 models continued pretrained for the Portuguese language. The authors pretrain models ranging from 60M to 3B parameters using the mC4-pt corpus and a Portuguese-specific tokenizer. The models are evaluated on three downstream tasks: ASSIN2 RTE, ASSIN2 STS, and TweetSentBR, achieving state-of-the-art results on ASSIN2 RTE and TweetSentBR. Additionally, the authors develop MonoPTT5 rerankers based on ptt5-v2 checkpoints, which set new SOTA on the mMARCO-pt and mRobust-pt retrieval datasets. Through ablation studies, the authors find that while pretraining data quality, optimization strategies, and multi-epoch pretraining can provide incremental improvements, their overall impact is subtle compared to the baseline.

## Method Summary
The authors continue pretraining T5 models on the mC4-pt corpus (524 GB, 169M documents) using a Portuguese-specific tokenizer. They pretrain models ranging from 60M to 3B parameters with a span corruption objective, Adafactor optimizer, constant learning rate of 0.001, batch size of 128, and sequence length of 512 tokens. The pretrained models are then finetuned on three downstream tasks: ASSIN2 RTE, ASSIN2 STS, and TweetSentBR, using the same hyperparameters as pretraining but for 100 epochs. Additionally, the authors develop MonoPTT5 rerankers based on ptt5-v2 checkpoints, which are trained using supervised text-to-text training with the same hyperparameters as finetuning but for 100k steps.

## Key Results
- ptt5-v2 models achieve state-of-the-art results on ASSIN2 RTE and TweetSentBR
- The performance gap favoring monolingual models over English-focused and multilingual models narrows as model capacity increases
- MonoPTT5 rerankers based on ptt5-v2 checkpoints set new SOTA on the mMARCO-pt and mRobust-pt retrieval datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specific tokenization significantly improves performance on Portuguese downstream tasks.
- Mechanism: The Portuguese tokenizer reduces text splitting into fewer tokens, leading to lower latency and the potential to accommodate more text within the same maximum token context window. This results in more efficient model processing and potentially better representation of Portuguese language nuances.
- Core assumption: The Portuguese tokenizer captures language-specific patterns and token boundaries more effectively than a general or English tokenizer.
- Evidence anchors:
  - [abstract]: "Additionally, the authors develop MonoPTT5 rerankers based on ptt5-v2 checkpoints, which set new SOTA on the mMARCO-pt and mRobust-pt retrieval datasets."
  - [section]: "Our findings also suggest that while continued pretraining enhances model capabilities, the increments in performance diminish as model size increases."
  - [corpus]: Weak - The corpus evidence doesn't directly address tokenization, but the focus on Portuguese-specific data supports the importance of language-specific processing.
- Break condition: If the Portuguese tokenizer doesn't capture language-specific patterns effectively, or if the performance gains are not observed across different model sizes.

### Mechanism 2
- Claim: Continued pretraining on language-specific corpora provides a practical solution for adapting models to other languages.
- Mechanism: By further pretraining on Portuguese texts, the models acquire a general-purpose Portuguese language representation, enhancing their performance on Portuguese downstream tasks. This approach uses significantly less data and computational resources than training from scratch.
- Core assumption: The pretraining data is representative of the Portuguese language and contains sufficient linguistic diversity.
- Evidence anchors:
  - [abstract]: "Continued pretraining on language-specific corpora provides a practical solution for adapting models to other languages."
  - [section]: "This method involves further pretraining on language-specific corpora, which has been shown to substantially enhance model performance on downstream tasks in the target language."
  - [corpus]: Weak - The corpus evidence shows the use of mC4-pt, but doesn't directly address the effectiveness of continued pretraining on language-specific corpora.
- Break condition: If the pretraining data quality is poor or if the pretraining process doesn't effectively capture the linguistic features of the Portuguese language.

### Mechanism 3
- Claim: The performance gap favoring monolingual models over English-focused and multilingual models narrows as model capacity increases.
- Mechanism: As the model size increases, the benefits of language-specific pretraining become less pronounced, suggesting that larger models can effectively learn language-specific features from multilingual data or English-focused data.
- Core assumption: The model's capacity to learn language-specific features is related to its size, and larger models can generalize better across languages.
- Evidence anchors:
  - [abstract]: "The ptt5-v2 models demonstrate a performance gap favoring monolingual models over English-focused and multilingual models, which narrows as model capacity increases."
  - [section]: "Our findings also suggest that while continued pretraining enhances model capabilities, the increments in performance diminish as model size increases."
  - [corpus]: Weak - The corpus evidence doesn't directly address the relationship between model size and performance gap, but the use of models up to 3B parameters supports the exploration of this mechanism.
- Break condition: If the performance gap doesn't narrow as model size increases, or if the performance of monolingual models doesn't improve with continued pretraining.

## Foundational Learning

- Concept: Text-to-text format
  - Why needed here: The T5 models use a text-to-text format, casting all tasks into a unified approach. This format is crucial for the model's adaptability across various NLP tasks.
  - Quick check question: Can you explain how the text-to-text format differs from other model architectures like encoder-only or decoder-only models?

- Concept: Pretraining objectives
  - Why needed here: The choice of pretraining objective (span corruption) and optimization strategies (Adafactor, learning rate) significantly impact the model's performance on downstream tasks.
  - Quick check question: How does the span corruption objective differ from other pretraining objectives like denoising or masked language modeling?

- Concept: Downstream task evaluation
  - Why needed here: The evaluation of the models on downstream tasks (ASSIN2 RTE, ASSIN2 STS, TweetSentBR) provides insights into the effectiveness of the pretraining process and the model's capabilities.
  - Quick check question: Can you describe the metrics used to evaluate the models on each downstream task and their significance?

## Architecture Onboarding

- Component map:
  Portuguese tokenizer -> T5 model architecture (encoder-decoder) -> mC4-pt pretraining data -> Adafactor optimizer -> Span corruption objective -> Downstream tasks (ASSIN2 RTE, ASSIN2 STS, TweetSentBR)

- Critical path:
  1. Pretrain T5 models on Portuguese corpus using Portuguese tokenizer.
  2. Finetune pretrained models on downstream tasks.
  3. Evaluate models on downstream tasks and retrieval tasks.
  4. Analyze the impact of different pretraining settings and model sizes.

- Design tradeoffs:
  - Language-specific tokenizer vs. general tokenizer: Language-specific tokenizer provides better performance but may limit the model's ability to generalize to other languages.
  - Model size vs. computational resources: Larger models provide better performance but require more computational resources for pretraining and finetuning.
  - Pretraining data quality vs. quantity: High-quality data may lead to better performance, but limited availability of such data may require compromises.

- Failure signatures:
  - Poor performance on downstream tasks despite successful pretraining.
  - Overfitting to pretraining data, resulting in poor generalization to downstream tasks.
  - Unstable training or optimization issues during pretraining or finetuning.

- First 3 experiments:
  1. Pretrain T5-small model on mC4-pt using Portuguese tokenizer and evaluate on downstream tasks.
  2. Pretrain T5-large model on mC4-pt using Portuguese tokenizer and evaluate on downstream tasks.
  3. Pretrain T5-3B model on mC4-pt using Portuguese tokenizer and evaluate on downstream tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of pretraining data quality (beyond token count) most significantly impact downstream task performance for Portuguese language models?
- Basis in paper: [explicit] The authors note that pretraining data quality improvements (MassiveText filters, BrWac dataset) showed an upward trend in performance, but the maximum difference in NPM was only approximately 2 points. They state: "Given the same compute budget, pretraining on a small dataset with high-quality data or a larger dataset with lower-quality data yields models of similar performance on downstream tasks."
- Why unresolved: The study only examined broad quality improvements and didn't isolate specific quality factors (e.g., document length, topic diversity, linguistic features). The authors acknowledge that the maximum performance difference was minimal despite significant data quality variations.
- What evidence would resolve it: Systematic ablation studies varying specific data quality metrics (document length distributions, topical diversity, linguistic complexity) while keeping total token count constant would identify which factors drive performance.

### Open Question 2
- Question: At what model size does continued pretraining on Portuguese-specific corpora provide diminishing returns compared to multilingual or English-focused models?
- Basis in paper: [explicit] The authors observe that "a performance gap favoring monolingual models over English-focused and multilingual models, which narrows as model capacity increases." They also note that "increments in performance diminish as model size increases."
- Why unresolved: The paper doesn't establish a precise inflection point where monolingual pretraining advantages plateau. The trend is observed but not quantified, and the study only examined up to 3B parameters.
- What evidence would resolve it: Systematic scaling experiments with Portuguese models ranging from 1B to 10B+ parameters, compared against equivalent-sized English/multilingual models on the same tasks, would identify the size threshold where pretraining advantages converge.

### Open Question 3
- Question: How does the choice of pretraining objective (span corruption vs denoising) affect downstream performance when continuing pretraining on Portuguese corpora?
- Basis in paper: [explicit] The authors note that "ptt5-v1 employed models... with a slightly different pretraining objective (denoising, where some input tokens are masked and the model is trained to predict the original text, rather than span corruption)." However, they didn't conduct direct comparisons between these objectives.
- Why unresolved: The comparison between ptt5-v1 and ptt5-v2 conflates multiple variables (dataset size, tokenizer, vocabulary) making it impossible to isolate the effect of the pretraining objective.
- What evidence would resolve it: Direct controlled experiments training Portuguese models from scratch or continuing pretraining using both span corruption and denoising objectives on identical datasets with identical tokenization would isolate the objective's impact.

## Limitations

- The paper doesn't provide a thorough validation of the quality and representativeness of the pretraining datasets for Portuguese language modeling.
- The generalizability of the findings to other languages remains untested, as the study focuses primarily on Portuguese tasks.
- The ablation studies don't explore other potentially important factors like learning rate schedules, regularization techniques, or different pretraining objectives that could significantly impact results.

## Confidence

**High Confidence Claims**:
- ptt5-v2 models achieve state-of-the-art results on ASSIN2 RTE and TweetSentBR (supported by direct comparisons with published results)
- The ptt5-v2-3B model achieves competitive performance on retrieval tasks (mMARCO-pt, mRobust-pt) with specific MRR and nDCG metrics reported
- The use of Portuguese-specific tokenizer improves performance compared to general tokenizers (shown through consistent improvements across model sizes)

**Medium Confidence Claims**:
- The performance gap between monolingual and multilingual models narrows with increased model capacity (observed trend but could be influenced by other factors)
- Continued pretraining provides incremental improvements over base T5 models (supported by ablation studies but with relatively small effect sizes)
- Pretraining data quality has a significant impact on downstream performance (suggested by ablation studies but not comprehensively explored)

**Low Confidence Claims**:
- The claim that continued pretraining provides a general solution for language adaptation (not tested beyond Portuguese)
- The assertion that model size is the primary factor in narrowing performance gaps (correlation doesn't prove causation)

## Next Checks

1. **Reproducibility Study**: Attempt to reproduce the pretraining and finetuning pipeline using the publicly released checkpoints and detailed hyperparameters. Focus on verifying the performance claims on ASSIN2 RTE and TweetSentBR, and confirm the effectiveness of the Portuguese tokenizer by comparing with a standard T5 tokenizer.

2. **Cross-Lingual Generalization Test**: Pretrain a T5 model for another language (e.g., Spanish or French) using the same methodology to test whether continued pretraining on language-specific corpora consistently improves performance across different languages.

3. **Comprehensive Ablation Analysis**: Conduct a more extensive ablation study that varies additional hyperparameters including learning rate schedules, regularization techniques, and different pretraining objectives (e.g., denoising vs. span corruption) to better understand which factors contribute most to performance improvements.