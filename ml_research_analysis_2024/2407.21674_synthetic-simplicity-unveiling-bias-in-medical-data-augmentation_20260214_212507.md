---
ver: rpa2
title: 'Synthetic Simplicity: Unveiling Bias in Medical Data Augmentation'
arxiv_id: '2407.21674'
source_url: https://arxiv.org/abs/2407.21674
tags:
- synthetic
- data
- real
- images
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the risks of simplicity bias when using
  synthetic data in medical imaging. The authors show that neural networks can exploit
  the spurious correlation between data source (real vs synthetic) and task labels,
  leading to poor generalization when this correlation is absent.
---

# Synthetic Simplicity: Unveiling Bias in Medical Data Augmentation

## Quick Facts
- **arXiv ID**: 2407.21674
- **Source URL**: https://arxiv.org/abs/2407.21674
- **Reference count**: 17
- **Primary result**: Neural networks can exploit spurious correlations between synthetic/real data source and task labels, leading to poor generalization when this correlation is absent.

## Executive Summary
This study investigates the risks of simplicity bias when using synthetic data in medical imaging. The authors demonstrate that neural networks can learn spurious correlations between data source (real vs synthetic) and task labels, resulting in poor generalization when this correlation is absent. Through experiments on digit classification (MNIST) and echocardiogram view classification (CAMUS), they show that extreme mixing ratios (α = 0 or 1) lead to near-zero accuracy on mismatched real/synthetic samples, while balanced mixing (α ≈ 0.5) mitigates this bias. The effect is more pronounced in medical imaging, suggesting higher complexity tasks are more vulnerable to synthetic simplicity bias.

## Method Summary
The study uses diffusion models to generate synthetic data for two tasks: MNIST digit classification and CAMUS echocardiogram view classification. For each task, they train CNN and ResNet-18 models respectively on mixed datasets containing real and synthetic samples. The mixing ratio α controls the proportion of real versus synthetic data for each class. They systematically vary α from 0 to 1 in steps of 0.25, training models on each configuration. Test accuracy is evaluated separately on real and synthetic test sets to identify when models exploit the spurious correlation between data source and labels rather than learning genuine task features.

## Key Results
- Models trained with extreme mixing ratios (α = 0 or 1) show near-zero accuracy on mismatched real/synthetic test samples
- Balanced mixing (α ≈ 0.5) effectively mitigates synthetic simplicity bias
- Medical imaging tasks (CAMUS) show steeper accuracy drops compared to digit classification (MNIST), suggesting higher complexity tasks are more vulnerable
- Simplicity bias persists across different network architectures including CNN and ResNet-18

## Why This Works (Mechanism)

### Mechanism 1
Neural networks learn spurious correlations between synthetic/real data source and task labels when mixing is unbalanced. During training, the model detects that real and synthetic data are correlated with different class labels, creating a shortcut where the model uses "data source" as a proxy feature instead of learning true task-relevant features. This spurious correlation is minimized when α is balanced (α ≈ 0.5), forcing the model to rely on genuine task features.

### Mechanism 2
Simplicity bias is more pronounced in medical imaging due to higher task complexity compared to natural images. Medical imaging tasks have more complex feature sets than digit classification. When synthetic data provides an easier-to-discriminate feature, the model preferentially learns this spurious correlation rather than the harder task-relevant features. The complexity difference between medical imaging and natural image tasks affects the strength of simplicity bias.

### Mechanism 3
Different downstream network architectures still exhibit synthetic simplicity bias when synthetic data is used for augmentation. Even complex models like ResNet-18, with deeper layers and more capacity, still exploit the simpler synthetic/real data distinction rather than learning task-relevant features when spurious correlations exist. Network architecture depth and complexity do not prevent exploitation of spurious correlations if they provide a simpler learning path.

## Foundational Learning

- **Spurious correlation and shortcut learning**: Understanding how models can learn non-causal relationships between features and labels is central to grasping why synthetic simplicity bias occurs.
  - Quick check: If a model learns to classify images based on background color rather than the object of interest, what type of learning problem is this?

- **Data augmentation and mixing ratios**: The α parameter controls the mixing ratio of real and synthetic data, which directly affects the correlation between data source and task labels.
  - Quick check: What happens to the correlation between data source and task labels when α = 0 or α = 1 versus when α = 0.5?

- **Diffusion models and synthetic data generation**: The study uses diffusion models to generate synthetic data, and understanding their characteristics helps explain why they might introduce spurious correlations.
  - Quick check: What is the key difference between how diffusion models and other generative models (like GANs) create synthetic samples?

## Architecture Onboarding

- **Component map**: Data generation module -> Mixing controller (α parameter) -> Base model (CNN/ResNet-18) -> Evaluation pipeline
- **Critical path**: 1) Generate synthetic data using appropriate diffusion model, 2) Mix real and synthetic data according to α parameter, 3) Train model on mixed dataset, 4) Evaluate on test set with balanced real/synthetic samples for each class, 5) Analyze performance across different α values
- **Design tradeoffs**: High α (mostly real data) provides better performance but less data augmentation benefit; low α (mostly synthetic data) offers more augmentation but higher risk of synthetic simplicity bias; balanced α (≈0.5) mitigates bias but may not fully exploit synthetic data potential
- **Failure signatures**: Large performance gap between real and synthetic test samples, sudden drops in accuracy when α changes from balanced to extreme values, model consistently performing better on the easier-to-discriminate data type
- **First 3 experiments**: 1) Replicate digit classification experiment with MNIST, varying α from 0 to 1 in steps of 0.25, 2) Test same framework on different medical imaging dataset (e.g., X-ray classification), 3) Compare performance across different network architectures (CNN, ResNet, Vision Transformer) with same synthetic data

## Open Questions the Paper Calls Out

### Open Question 1
Does the severity of simplicity bias in synthetic data augmentation vary across different medical imaging modalities (e.g., X-ray, MRI, ultrasound) and why? The authors observe that CAMUS echocardiogram classification shows significantly steeper accuracy drops than MNIST digit classification, suggesting higher complexity tasks are more vulnerable to simplicity bias. This remains unresolved as the paper only examines one medical imaging task and one general task, without testing across different modalities or providing quantitative measures of task complexity.

### Open Question 2
What specific statistical signatures in synthetic data make them distinguishable from real data, and how do these signatures vary across different generative models? The authors mention that models can exploit "statistical signatures" specific to their generation algorithms but do not analyze what specific features or statistical properties make synthetic data distinguishable from real data, nor do they compare different generative approaches systematically.

### Open Question 3
What are effective architectural or training modifications that can mitigate simplicity bias when using synthetic data in medical imaging? While the paper identifies the problem and suggests balanced mixing as a mitigation strategy, it does not explore other architectural or training modifications, regularization techniques, or curriculum learning approaches that might address this issue.

## Limitations
- Results may not generalize to all medical imaging tasks or synthetic data generation methods
- Study uses simplified binary classification tasks rather than multi-class, multi-label scenarios common in clinical practice
- Does not explore the full spectrum of potential mitigation strategies beyond balanced mixing ratios

## Confidence
- **High Confidence**: The existence of synthetic simplicity bias in controlled experimental settings, the mechanism of spurious correlation between data source and labels, and the effectiveness of balanced mixing (α ≈ 0.5) in mitigating this bias.
- **Medium Confidence**: The claim that medical imaging tasks are more vulnerable to this bias due to higher complexity, as this is based on comparison with a single digit classification task.
- **Low Confidence**: The broader implications for real-world medical AI deployment, as the study uses simplified tasks rather than complex clinical scenarios.

## Next Checks
1. **Cross-Dataset Validation**: Test the synthetic simplicity framework on additional medical imaging datasets (e.g., chest X-rays, histopathology) to assess generalizability across different imaging modalities and diagnostic tasks.
2. **Real-World Deployment Impact**: Evaluate whether models exhibiting synthetic simplicity bias maintain diagnostic utility when deployed in clinical settings, including assessment of false negative rates and clinical decision support accuracy.
3. **Alternative Mitigation Strategies**: Investigate whether techniques beyond balanced mixing (such as domain adversarial training, feature disentanglement, or curriculum learning approaches) can more effectively address synthetic simplicity bias while preserving the benefits of data augmentation.