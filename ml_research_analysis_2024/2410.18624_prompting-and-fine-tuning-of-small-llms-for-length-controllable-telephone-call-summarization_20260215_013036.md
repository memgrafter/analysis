---
ver: rpa2
title: Prompting and Fine-Tuning of Small LLMs for Length-Controllable Telephone Call
  Summarization
arxiv_id: '2410.18624'
source_url: https://arxiv.org/abs/2410.18624
tags:
- call
- data
- summarization
- transcript
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates the development of a high-quality telephone
  call summarization system by fine-tuning small language models with synthetic data
  generated by GPT-4. The key innovation is creating diverse, length-controllable
  synthetic summaries using carefully designed prompts, which are then used to fine-tune
  a Llama-2-7B model.
---

# Prompting and Fine-Tuning of Small LLMs for Length-Controllable Telephone Call Summarization

## Quick Facts
- arXiv ID: 2410.18624
- Source URL: https://arxiv.org/abs/2410.18624
- Authors: David Thulke; Yingbo Gao; Rricha Jalota; Christian Dugast; Hermann Ney
- Reference count: 29
- This paper demonstrates that fine-tuning small LLMs with synthetic data from GPT-4 achieves near frontier model quality for telephone call summarization with length control.

## Executive Summary
This paper presents a method for creating high-quality telephone call summarization systems by fine-tuning small language models with synthetic data generated by GPT-4. The key innovation is using carefully designed prompts to generate diverse, length-controllable synthetic summaries, which are then used to fine-tune a Llama-2-7B model. The resulting system achieves performance on par with GPT-4 in factual accuracy, completeness, and conciseness while enabling length control capabilities. The approach significantly outperforms general-domain instruction tuning and highlights the importance of task-specific data diversity for achieving high performance.

## Method Summary
The method involves generating synthetic summaries of simulated telephone call transcripts using GPT-4 with 20 different prompt variants across three categories (default, general, and length-oriented). These synthetic summaries are then used to fine-tune a Llama-2-7B model using a Megatron-LM fork with specific hyperparameters including a cosine learning rate schedule (peak 10⁻⁵, warmup 100 steps), batch size 64, and full sequence length 4096. The fine-tuning process combines general-domain instruction data with summarization-specific instruction data. The resulting model is evaluated using LLM-as-a-judge frameworks (Prometheus-Eval and FineSurE) for factual accuracy, completeness, and conciseness, with particular attention to length adherence capabilities.

## Key Results
- Fine-tuned Llama-2-7B achieves performance on par with GPT-4 in factual accuracy, completeness, and conciseness
- Length control capability is effectively restored by incorporating length-specific instructions during synthetic data generation
- Task-specific training data diversity is crucial for achieving high performance compared to general-domain instruction tuning
- The system enables length-controllable summarization while maintaining high summary quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data generated by GPT-4 can effectively teach a smaller LLM (Llama-2-7B) to perform telephone call summarization at near frontier model quality.
- Mechanism: The synthetic data acts as a distilled representation of the knowledge and capabilities of the stronger model, transferring task-specific understanding without requiring the same computational resources.
- Core assumption: GPT-4 can generate high-quality, diverse summaries that capture the essential characteristics of telephone call summarization across different scenarios and lengths.
- Evidence anchors: [abstract] "creation of a tailored synthetic training dataset utilizing stronger frontier models"; [section III] "we opted to generate synthetic summaries of call transcripts using GPT-4 [1], which is a strong external model capable of producing high-quality summaries"
- Break condition: If GPT-4's synthetic summaries contain biases, errors, or fail to represent the diversity of real telephone conversations, the smaller model will inherit these limitations.

### Mechanism 2
- Claim: Length control capability can be restored in the fine-tuned model by incorporating length-specific instructions during synthetic data generation.
- Mechanism: Exposing the model to prompts with explicit length constraints during training teaches it to follow such instructions during inference, creating a "soft" mechanism for length control.
- Core assumption: The model can learn to generalize length control from synthetic examples to unseen prompts during testing.
- Evidence anchors: [abstract] "Our findings show that fine-tuned Llama-2-7B-based summarization model performs on-par with GPT-4 in terms of factual accuracy, completeness, and conciseness"; [section V.B] "incorporating length-specific instructions into synthetic data generation can help restore this capability"
- Break condition: If the model overfits to specific length patterns in the synthetic data, it may fail to generalize to arbitrary length constraints during inference.

### Mechanism 3
- Claim: Task-specific training data diversity is crucial for achieving high performance, as evidenced by the significant performance gap between models trained on general vs. summarization-specific data.
- Mechanism: Diverse prompts covering different aspects of summarization (content, sentiment, next steps, etc.) teach the model to handle various summarization requirements, preventing overfitting to a narrow task definition.
- Core assumption: The diversity of prompts in the synthetic data correlates with the model's ability to generalize across different summarization scenarios.
- Evidence anchors: [abstract] "our findings demonstrate the potential for quickly bootstrapping a practical and efficient call summarization system"; [section IV.B] "We then fine-tune Llama-2-7B using various combinations of this data, resulting in seven different summarization-specific IFT model variants"
- Break condition: If the prompt diversity in synthetic data doesn't match the diversity of real-world summarization needs, the model may perform poorly on edge cases.

## Foundational Learning

- Concept: Synthetic data generation and fine-tuning methodology
  - Why needed here: The entire approach relies on generating synthetic training data and fine-tuning a smaller model, which requires understanding the relationship between data quality, model architecture, and performance outcomes
  - Quick check question: What is the primary advantage of using GPT-4 to generate synthetic training data rather than manually annotating real call transcripts?

- Concept: Length control mechanisms in language models
  - Why needed here: The paper demonstrates that length control can be achieved through specific prompting strategies during both training and inference, which requires understanding how models interpret and follow length constraints
  - Quick check question: How does incorporating length-specific instructions during synthetic data generation help restore length control capability in the fine-tuned model?

- Concept: LLM-as-a-judge evaluation frameworks
  - Why needed here: The paper uses two different LLM-as-a-judge approaches (Prometheus-Eval and FineSurE) to evaluate summary quality, requiring understanding of how these frameworks work and their respective strengths and limitations
  - Quick check question: What are the key differences between Prometheus-Eval and FineSurE in terms of how they assess summary quality?

## Architecture Onboarding

- Component map: Call transcript → Speaker tag processing → GPT-4 synthetic summary generation → Llama-2-7B fine-tuning → Summary generation → LLM-as-a-judge evaluation
- Critical path: Call transcript → Speaker tag processing → GPT-4 synthetic summary generation → Llama-2-7B fine-tuning → Summary generation → LLM-as-a-judge evaluation
- Design tradeoffs:
  - Using synthetic data vs. human-annotated data: Faster and cheaper but may contain model biases
  - Truncating context vs. using full transcripts: Necessary for model capacity but may impact summary quality
  - Fine-tuning vs. prompting only: Fine-tuning achieves better performance but requires more resources
  - Length-specific vs. general prompts: Length-specific helps control but reduces instruction-following ability
- Failure signatures:
  - Model produces summaries that are factually incorrect or incomplete (low factual validity/honesty scores)
  - Model fails to adhere to length constraints (low length adherence percentages)
  - Model performance degrades significantly compared to baseline models
  - Model overfits to synthetic data patterns and fails to generalize
- First 3 experiments:
  1. Generate synthetic summaries using only the default prompt category and fine-tune a model, then evaluate against models using more diverse prompts to measure impact of prompt diversity
  2. Train a model with length-specific prompts vs. without, then test both on length-constrained prompts to measure restoration of length control capability
  3. Compare model performance when trained only on general-domain data vs. summarization-specific data to validate the importance of task-specific training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do speech recognition errors impact the quality of generated summaries in the proposed telephone call summarization system?
- Basis in paper: [inferred] The authors mention that "Future research should focus on addressing several important areas that were not fully explored in this study. These include the impact of speech recognition errors" in the conclusion.
- Why unresolved: The paper focuses on text-based summarization using pre-existing transcripts without investigating how transcription errors might propagate through to the summaries.
- What evidence would resolve it: A controlled study comparing summary quality using both clean transcripts and transcripts with varying levels of speech recognition errors would provide evidence of the impact.

### Open Question 2
- Question: What is the optimal balance between general-domain instruction data and summarization-specific instruction data for fine-tuning small LLMs?
- Basis in paper: [inferred] The authors state that "Our results show that the diversity of the prompts in the synthetically generated data is crucial to improving performance" and explore different combinations of data types, but do not determine the optimal balance.
- Why unresolved: While the paper demonstrates the importance of diverse prompts and task-specific data, it does not systematically explore the optimal ratio between general and task-specific instruction data.
- What evidence would resolve it: An ablation study varying the proportion of general-domain versus summarization-specific instruction data in the fine-tuning process, measuring the impact on various evaluation metrics.

### Open Question 3
- Question: How does the performance of the fine-tuned Llama-2-7B model compare to larger frontier models like GPT-4 when controlling for instruction-following capabilities?
- Basis in paper: [explicit] The authors state that their fine-tuned Llama-2-7B model "performs on-par with GPT-4 in terms of factual accuracy, completeness, and conciseness" but do not isolate the instruction-following component.
- Why unresolved: While overall performance is compared, the paper does not specifically measure and compare instruction-following capabilities as a distinct metric.
- What evidence would resolve it: A targeted evaluation using prompts with varying levels of complexity and specificity to measure instruction-following accuracy across different model sizes.

## Limitations

- The quality of the fine-tuned model is fundamentally bounded by GPT-4's capabilities in generating synthetic summaries, meaning any biases or systematic errors will propagate to the downstream model.
- The simulated call corpus, though diverse in accents, remains limited to 2,331 examples and may not capture the full complexity of real-world telephone conversations.
- The truncation of context to 4096 tokens for model compatibility may result in loss of important conversational details, potentially impacting summary quality for longer calls.

## Confidence

**High Confidence**: The core finding that task-specific synthetic data fine-tuning outperforms general-domain instruction tuning is well-supported by the experimental results, with clear performance gaps demonstrated across multiple evaluation metrics.

**Medium Confidence**: The claim of achieving "near frontier model quality" (on-par with GPT-4) is supported by the evaluation metrics but relies on the assumption that the LLM-as-a-judge frameworks accurately capture human judgment of summary quality.

**Low Confidence**: The generalizability of the approach to other domains or languages remains untested, and the paper does not address potential degradation in performance when applied to longer, more complex calls that exceed the 4096 token context window.

## Next Checks

1. **Human Evaluation Validation**: Conduct a human evaluation study comparing summaries generated by the fine-tuned Llama-2-7B model against GPT-4 and other baseline models. This would validate whether the LLM-as-a-judge metrics accurately reflect human perceptions of summary quality, particularly for factual accuracy and completeness.

2. **Out-of-Domain Testing**: Test the fine-tuned model on real-world telephone call transcripts from different domains (e.g., medical, legal, customer service) and languages to assess generalizability. This would reveal whether the synthetic training data sufficiently covers the diversity needed for real-world deployment.

3. **Ablation Study on Prompt Diversity**: Systematically remove individual prompt categories (default, general, length-oriented) from the synthetic training data to quantify the specific contribution of each type to overall model performance. This would provide clearer insight into which aspects of prompt diversity are most critical for achieving high-quality summarization.