---
ver: rpa2
title: 'SpeGCL: Self-supervised Graph Spectrum Contrastive Learning without Positive
  Samples'
arxiv_id: '2410.10365'
source_url: https://arxiv.org/abs/2410.10365
tags:
- graph
- learning
- information
- contrastive
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of effectively utilizing high-frequency
  information in graph contrastive learning (GCL) and the computational burden of
  sampling both positive and negative pairs. The proposed SpeGCL framework leverages
  Fourier transforms to extract and leverage both low-frequency and high-frequency
  components of node features within a Fourier space.
---

# SpeGCL: Self-supervised Graph Spectrum Contrastive Learning without Positive Samples

## Quick Facts
- arXiv ID: 2410.10365
- Source URL: https://arxiv.org/abs/2410.10365
- Authors: Yuntao Shou; Xiangyong Cao; Deyu Meng
- Reference count: 40
- Outperforms state-of-the-art GCL methods on multiple graph classification tasks

## Executive Summary
This paper introduces SpeGCL, a self-supervised graph spectrum contrastive learning framework that eliminates the need for positive sample pairs while effectively capturing high-frequency information. The key innovation lies in using Fourier transforms to extract both low-frequency and high-frequency components of node features, then performing contrastive learning solely with negative samples. The framework demonstrates superior performance across unsupervised, transfer, and semi-supervised graph classification tasks, particularly on datasets with complex structures or high noise levels.

## Method Summary
SpeGCL addresses the limitations of traditional graph contrastive learning by leveraging Fourier transforms to extract high-frequency and low-frequency components of node features in the frequency domain. The framework constructs augmented views by separating these components and performs contrastive learning using only negative samples, eliminating the need for positive pairs. Theoretical analysis proves that the model can converge using negative samples alone. The approach employs a Fourier Graph Convolutional Network (FourierGCN) that uses the convolution theorem to achieve computational efficiency while preserving high-frequency information critical for effective representation learning.

## Key Results
- Outperforms state-of-the-art GCL methods on unsupervised, transfer, and semi-supervised graph classification tasks
- Ablation studies confirm high-frequency information is more important than low-frequency information for contrastive learning
- Negative-only sampling strategy achieves comparable or better performance than traditional positive-negative pairs
- Demonstrates particular effectiveness on datasets with complex structures or high noise levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SpeGCL achieves effective graph representation learning without positive sample pairs
- Mechanism: Uses Fourier transforms to extract high-frequency and low-frequency components, then performs contrastive learning solely on negative samples
- Core assumption: High-frequency differences between augmented graphs are greater than low-frequency differences, making negative samples sufficient
- Evidence: Abstract finding that high-frequency differences exceed low-frequency differences; theoretical analysis showing negative pairs alone enable convergence
- Break condition: Minimal high-frequency differences between augmented graphs or overly homogeneous graph structures

### Mechanism 2
- Claim: Fourier transform improves computational efficiency and representation quality in GNNs
- Mechanism: Converts graph convolution to frequency domain where convolution becomes element-wise multiplication via convolution theorem
- Core assumption: Graph convolution can be equivalently represented in frequency domain
- Evidence: Design of efficient FourierGCN based on convolution theorem; rewriting graph convolution as F(X)F(κ) = F((X ∗ κ))
- Break condition: Graph structures with patterns poorly preserved in frequency domain or significant inverse transform information loss

### Mechanism 3
- Claim: High-frequency information is more important than low-frequency information for graph contrastive learning
- Mechanism: Constructs augmented views separating high-pass and low-pass components, focusing contrastive learning on high-frequency differences
- Core assumption: High-frequency components represent rapid changes more discriminative for contrastive learning
- Evidence: Abstract observation of greater high-frequency differences; empirical observation of significant high-frequency changes
- Break condition: Predominantly smooth graph data with minimal high-frequency variations or overwhelming high-frequency noise

## Foundational Learning

- Concept: Fourier Transform and its properties (especially DFT/FFT and convolution theorem)
  - Why needed here: Entire framework relies on frequency domain conversion for efficiency and high-frequency capture
  - Quick check: What is computational complexity difference between standard convolution and frequency domain convolution using FFT?

- Concept: Graph Neural Networks and their limitations with high-frequency information
  - Why needed here: Understanding why existing GNNs fail to capture high-frequency information is crucial for appreciating SpeGCL's innovation
  - Quick check: Why do traditional GNNs struggle to capture high-frequency information in graph data?

- Concept: Contrastive Learning objectives (InfoNCE loss and its properties)
  - Why needed here: SpeGCL modifies standard contrastive learning by removing positive samples and proving convergence with negative samples only
  - Quick check: What are key properties of InfoNCE loss that make it suitable for contrastive learning?

## Architecture Onboarding

- Component map: Data Augmentation (node masking and edge perturbation) -> Fourier Transform (DFT/FFT) -> Fourier Graph Convolutional Network -> Frequency component separation -> Negative-only contrastive loss -> Graph embedding -> Classification

- Critical path: Input graph → Data augmentation → Fourier Transform → Frequency domain convolution → Frequency component separation → Negative-only contrastive loss → Graph embedding → Classification

- Design tradeoffs:
  - Computational efficiency vs. information preservation (FFT reduces complexity but may lose structural information)
  - High-frequency focus vs. low-frequency stability (emphasizing high-frequency may amplify noise)
  - Negative-only sampling vs. traditional positive-negative pairs (simplifies training but requires theoretical justification)

- Failure signatures:
  - Poor performance on graphs with minimal high-frequency variation
  - Degraded accuracy when inverse Fourier transform introduces significant distortion
  - Convergence issues if negative sampling becomes too sparse or uninformative

- First 3 experiments:
  1. Implement basic Fourier transform on node features and visualize high-pass vs low-pass components on simple graph dataset
  2. Compare standard GNN vs FourierGNN performance on graph classification task with identical architectures except convolution operation
  3. Test negative-only contrastive learning vs traditional positive-negative pairs on small graph dataset, measuring convergence speed and final accuracy

## Open Questions the Paper Calls Out

- Question: Does performance gap between positive/negative pairs and negative-only samples remain consistent across different graph types?
- Basis: Ablation study shows relatively small accuracy gaps, sometimes negative-only surpasses paired sampling
- Why unresolved: Ablation study results presented for multiple datasets but not analyzed systematically across graph types or domains
- What evidence would resolve it: Detailed analysis of ablation results broken down by graph type/domain with statistical significance testing

- Question: What is impact of different graph augmentation strategies on SpeGCL performance with negative-only sampling?
- Basis: Paper discusses node masking and edge perturbation but doesn't explore impact of different augmentation strategies
- Why unresolved: Focus on Fourier transform and negative-only sampling without investigating augmentation strategy interactions
- What evidence would resolve it: Experiments comparing SpeGCL with different augmentation strategies (node dropping, edge adding, subgraph sampling) using negative-only sampling

- Question: How does choice of low-frequency threshold (DL) in high-pass/low-pass augmentation affect SpeGCL performance?
- Basis: Paper describes using low-frequency threshold (DL) to separate components but doesn't discuss impact of different threshold choices
- Why unresolved: No discussion of how different low-frequency threshold choices affect performance
- What evidence would resolve it: Experiments varying low-frequency threshold (DL) and measuring impact across datasets and graph types

## Limitations
- Performance gains most pronounced on complex/noisy datasets, suggesting potential dataset-specific effects
- Limited empirical validation of computational efficiency gains across different graph sizes
- Theoretical convergence proof for negative-only sampling needs broader validation on diverse graph structures

## Confidence

- High: Fourier transform application for frequency domain conversion, basic GCL framework implementation
- Medium: Convergence proof for negative-only sampling, high-frequency information superiority claim, computational efficiency gains
- Low: Universal applicability across all graph types, noise robustness in real-world scenarios

## Next Checks

1. **Ablation Study Extension**: Conduct controlled experiments systematically varying ratio of high-frequency to low-frequency components across diverse graph datasets to quantify precise contribution of each frequency band.

2. **Convergence Analysis**: Perform empirical convergence rate comparison between SpeGCL and traditional positive-negative contrastive methods on graphs with varying levels of structural homogeneity to test negative-only sampling robustness.

3. **Computational Benchmarking**: Measure actual wall-clock time and memory usage for FourierGCN versus standard GCN on graphs ranging from small (100 nodes) to large (10K+ nodes) to validate claimed efficiency improvements across scales.