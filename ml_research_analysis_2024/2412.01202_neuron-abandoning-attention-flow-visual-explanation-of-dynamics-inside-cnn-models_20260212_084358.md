---
ver: rpa2
title: 'Neuron Abandoning Attention Flow: Visual Explanation of Dynamics inside CNN
  Models'
arxiv_id: '2412.01202'
source_url: https://arxiv.org/abs/2412.01202
tags:
- layer
- relu
- attention
- layers
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of visualizing attention evolution
  inside CNN models during classification by introducing a Neuron Abandoning Attention
  Flow (NAFlow) method. The core idea is to design a cascading neuron abandoning back-propagation
  algorithm that traces neurons involved in predictions and excludes those not contributing
  to decision-making.
---

# Neuron Abandoning Attention Flow: Visual Explanation of Dynamics inside CNN Models

## Quick Facts
- arXiv ID: 2412.01202
- Source URL: https://arxiv.org/abs/2412.01202
- Reference count: 33
- Primary result: Introduces NAFlow method for visualizing attention evolution across all CNN layers using cascading neuron abandoning back-propagation and Jacobian importance coefficients

## Executive Summary
This paper addresses the challenge of visualizing attention evolution inside CNN models during classification by introducing the Neuron Abandoning Attention Flow (NAFlow) method. Unlike existing methods limited to final layers, NAFlow traces decision-making neurons across all layers through a cascading neuron abandoning back-propagation algorithm. The method generates Back-Propagated Feature Maps (BPFM) via inverse operations of intermediate layers, combined with importance coefficients calculated through Jacobian Matrix computations, providing comprehensive visual explanations of CNN decision-making processes.

## Method Summary
NAFlow employs a cascading neuron abandoning back-propagation algorithm that traces neurons involved in predictions while excluding non-contributing neurons. The method generates Back-Propagated Feature Maps (BPFM) by applying inverse functions to intermediate layers, then calculates importance coefficients via Jacobian Matrix to measure each neuron's contribution to the final classification score. For similarity metric-based models, a channel contribution weight method extends NAFlow by decomposing similarity scores into channel-wise contributions. Experiments on nine CNN models across four tasks demonstrate consistent effectiveness in visualizing attention flows and providing deeper insights into model decision-making.

## Key Results
- Successfully visualizes attention flows across all layers of CNN models, not just final layers
- Demonstrates consistent effectiveness across nine different CNN models for general image classification, contrastive learning, few-shot classification, and image retrieval
- Provides deeper insights into model decision-making processes and potential for analyzing internal components
- Extends visualization capability to similarity metric-based CNN models through channel contribution weights

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The cascading neuron abandoning back-propagation algorithm correctly identifies neurons that contribute to the final classification decision.
- Mechanism: Uses inverse functions of intermediate layers to propagate the output feature map backward, removing neurons not involved in decision-making at each step.
- Core assumption: Neurons not selected by max pooling or not contributing to convolution operations are not involved in the final decision.
- Evidence anchors:
  - [abstract] "A novel cascading neuron abandoning back-propagation algorithm is designed to trace neurons in all layers of a CNN that involve in making its prediction to address the problem of significant interference from abandoned neurons."
  - [section III.A] "During this process, all the abandoned neurons are removed."
- Break condition: If inverse function approximations are inaccurate or Jacobian matrix computations fail to capture neuron importance correctly.

### Mechanism 2
- Claim: Importance coefficients calculated via Jacobian Matrix accurately measure neuron contribution to final classification score.
- Mechanism: Jacobian Matrix computes partial derivatives between output and input neurons, providing quantitative measure of neuron importance linearly combined with back-propagated feature maps.
- Core assumption: Jacobian Matrix accurately captures relationships between neurons across layers and can be computed reliably for various layer types.
- Evidence anchors:
  - [section III.A.1] "wij denotes the weights between yi and xj calculated by using Jacobian Matrix ∂(y1,...,yq)/∂(x1,...,xp)"
  - [section III.B] "the importance coefficients ΛN for BPFM N of the last layer gN of the CNN is calculated by ΛN = [λN1, ..., λNp] = ∂yc/∂[xN1, ..., xNp]"
- Break condition: If Jacobian Matrix becomes singular or ill-conditioned, leading to inaccurate importance coefficient calculations.

### Mechanism 3
- Claim: Channel contribution weight method extends NAFlow to similarity metric-based CNN models without classification scores.
- Mechanism: For models using similarity metrics like cosine similarity, calculates channel importance based on contribution of each channel to similarity score between query and support feature vectors.
- Core assumption: Similarity metric can be decomposed into channel-wise contributions reflecting importance to classification decision.
- Evidence anchors:
  - [abstract] "to be able to visualize attention flow for similarity metric-based CNN models, a new channel contribution weights module is proposed to calculate the importance coefficients via Jacobian Matrix"
  - [section III.C] "The proposed channel contribution weight can be applied to any similarity metric. Different similarity metrics require different ways to calculate channel contribution weight."
- Break condition: If similarity metric cannot be meaningfully decomposed into channel-wise contributions or decomposition loses critical information.

## Foundational Learning

- Concept: Back-propagation and inverse operations
  - Why needed here: NAFlow requires understanding how to propagate information backward through neural network layers using inverse operations to identify decision-making neurons.
  - Quick check question: How would you implement the inverse operation for a ReLU layer, and what would you do with neurons that were zeroed out during forward propagation?

- Concept: Jacobian Matrix computation and interpretation
  - Why needed here: The method relies on computing Jacobian matrices to measure neuron importance across layers, which requires understanding partial derivatives and their computation.
  - Quick check question: Given a simple 2-layer network with specific weights, can you compute the Jacobian matrix between the output and input neurons?

- Concept: Channel-wise importance and similarity metrics
  - Why needed here: For similarity-based models, understanding how to decompose similarity scores into channel contributions is essential for the channel contribution weight method.
  - Quick check question: For cosine similarity between two vectors, how would you compute the contribution of each channel to the overall similarity score?

## Architecture Onboarding

- Component map: NAFlow consists of three main components: (1) Neuron Abandoning Back-Propagation (NA-BP) for generating back-propagated feature maps, (2) Importance Coefficients calculation via Jacobian Matrix, and (3) Channel Contribution Weight for similarity-based models.
- Critical path: The critical path involves forward propagation to get the output, then backward propagation through NA-BP modules to generate BPFMs and importance coefficients, and finally linear combination to form the attention flow.
- Design tradeoffs: The method trades computational complexity (multiple backward passes) for accuracy in identifying decision-making neurons, and requires careful handling of different layer types.
- Failure signatures: Incorrect attention maps with random scatter patterns, failure to propagate through certain layer types, or NaN values in Jacobian computations indicate problems.
- First 3 experiments:
  1. Implement NAFlow on a simple 2-layer CNN with known decision-making neurons to verify correct identification.
  2. Compare attention maps generated by NAFlow vs Grad-CAM on intermediate layers to observe the difference in handling abandoned neurons.
  3. Apply NAFlow to a similarity-based model like few-shot classification to verify the channel contribution weight method works correctly.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper does not provide quantitative metrics for evaluating the quality of attention maps, making it difficult to objectively assess the superiority of NAFlow over existing methods
- The computational complexity of multiple backward passes through the network for generating attention maps at all layers is not discussed or benchmarked
- Limited ablation studies are provided to isolate the contribution of each component (NA-BP, Jacobian importance coefficients, channel contribution weights)

## Confidence

- **High confidence**: The core methodology of using inverse operations for back-propagation and Jacobian matrices for importance coefficients is clearly described and theoretically sound
- **Medium confidence**: The extension to similarity metric-based models through channel contribution weights is plausible but lacks detailed implementation specifics
- **Low confidence**: The claim that NAFlow consistently provides "deeper insights" across all tested model types is not substantiated with specific examples or quantitative comparisons

## Next Checks
1. Implement quantitative metrics (e.g., pointing game accuracy, deletion/insertion metrics) to evaluate NAFlow attention maps against Grad-CAM and other baselines on the same test sets
2. Benchmark the computational overhead of NAFlow compared to single-layer attention methods, measuring both memory usage and inference time
3. Conduct targeted ablation studies where individual components (NA-BP, Jacobian computation, channel weights) are disabled to quantify their contribution to final attention map quality