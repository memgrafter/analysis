---
ver: rpa2
title: Residual Connections and Normalization Can Provably Prevent Oversmoothing in
  GNNs
arxiv_id: '2406.02997'
source_url: https://arxiv.org/abs/2406.02997
tags:
- graph
- normalization
- gnns
- such
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a theoretical analysis of residual connections
  and normalization layers in Graph Neural Networks (GNNs) to address the oversmoothing
  problem. The key findings are: Residual connections prevent complete rank collapse
  by incorporating initial features at each layer, determining the subspace of possible
  node representations.'
---

# Residual Connections and Normalization Can Provably Prevent Oversmoothing in GNNs

## Quick Facts
- arXiv ID: 2406.02997
- Source URL: https://arxiv.org/abs/2406.02997
- Reference count: 40
- Key outcome: Theoretical analysis shows residual connections and normalization layers prevent rank collapse in GNNs, with proposed GraphNormv2 achieving top-2 performance across multiple tasks.

## Executive Summary
This paper provides a theoretical analysis of how residual connections and normalization layers prevent oversmoothing in Graph Neural Networks (GNNs). The authors prove that residual connections prevent complete rank collapse by incorporating initial features at each layer, while batch normalization prevents output embedding collapse through individual column rescaling. Based on these insights, they propose GraphNormv2, a novel normalization layer with learnable centering that preserves the integrity of the original graph signal while achieving superior performance across various GNN architectures and tasks.

## Method Summary
The authors develop a theoretical framework for analyzing oversmoothing in linear GNNs with specific operator assumptions. They prove that residual connections prevent complete rank collapse by incorporating initial features at each layer, while batch normalization prevents output embedding collapse through individual column rescaling. Based on these insights, they propose GraphNormv2, which modifies the centering step to be learnable, allowing the model to preserve important graph signal information while maintaining the benefits of normalization. The method is evaluated across multiple GNN architectures (GCN, GIN, GAT, JK-Net) and tasks including node classification on Cora, Citeseer, Pubmed, OGB products, and heterophilic graphs like Texas, Cornell, and Wisconsin.

## Key Results
- Residual connections prevent complete rank collapse by incorporating initial features at each layer, determining the subspace of possible node representations
- Batch normalization prevents complete collapse of output embedding space to one-dimensional subspace through individual column rescaling
- GraphNormv2 achieves best or second-best results in most experiments, particularly effective in deep architectures and heterophilic graphs

## Why This Works (Mechanism)
The mechanism works through two complementary effects: residual connections maintain diversity in the representation space by preserving initial features throughout the network depth, while normalization layers (particularly with learnable centering) prevent pathological contraction of the embedding space. The residual connection ensures that node representations never collapse completely to a single dimension by providing a direct path for initial features, while the modified normalization prevents the centering operation from distorting important graph signal information.

## Foundational Learning
1. **Graph Signal Processing**: Understanding how signals propagate on graphs through message passing operators
   - Why needed: Essential for analyzing how features evolve across GNN layers
   - Quick check: Can you explain how the graph Laplacian affects signal propagation?

2. **Linear Algebra of Rank Collapse**: Understanding conditions under which matrix products lead to rank deficiency
   - Why needed: Core to analyzing when and how oversmoothing occurs
- Quick check: Can you derive when a product of matrices collapses to rank 1?

3. **Eigenspace Analysis**: Understanding how operators converge to specific eigenspaces
   - Why needed: Critical for understanding the theoretical convergence properties
   - Quick check: Can you explain what determines which eigenspace an operator converges to?

4. **Normalization Layer Mechanics**: Understanding batch normalization and its effects on feature distributions
   - Why needed: Key to understanding how normalization prevents pathological behavior
   - Quick check: Can you describe how centering and scaling operations affect feature variance?

## Architecture Onboarding

Component Map:
Input Features -> Residual Connection -> Normalization (GraphNormv2) -> Message Passing -> Output Layer

Critical Path:
The critical path involves the interplay between residual connections (maintaining feature diversity) and the learnable centering normalization (preventing pathological contraction while preserving signal integrity). The message passing operator acts on the normalized, residual-connected features.

Design Tradeoffs:
- Fixed centering (standard normalization) vs learnable centering (GraphNormv2): fixed centering can distort important graph signals, while learnable centering allows the model to adapt to preserve meaningful information
- Depth vs oversmoothing: deeper networks benefit more from residual connections and proper normalization
- Computational overhead: GraphNormv2 adds learnable parameters but provides significant performance gains

Failure Signatures:
- Rank collapse: all node representations converge to the same value (detected through singular value analysis)
- Signal distortion: important structural information is lost due to aggressive centering
- Performance degradation: accuracy drops significantly in deeper architectures without proper normalization

First Experiments:
1. Implement basic GCN with residual connections and compare rank collapse behavior with and without residuals
2. Compare standard batch normalization vs GraphNormv2 on a simple node classification task
3. Analyze the learned centering parameters in GraphNormv2 to understand what signal information is being preserved

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis limited to linear GNNs with specific operator assumptions
- Claims about eigenspace convergence may not directly translate to non-linear or adaptive architectures
- Learnable parameters in GraphNormv2 could overfit on smaller datasets

## Confidence
- Theoretical claims: Medium (due to idealized assumptions about linear operators)
- Empirical results: High (consistent performance across multiple tasks and architectures)
- Generalization to non-linear GNNs: Low (not directly addressed in theoretical framework)

## Next Checks
1. Extend theoretical framework to analyze non-linear GNN variants and adaptive architectures
2. Evaluate GraphNormv2 on large-scale dynamic graph datasets to test scalability and temporal adaptation
3. Conduct ablation studies to isolate effects of learnable centering versus other architectural components