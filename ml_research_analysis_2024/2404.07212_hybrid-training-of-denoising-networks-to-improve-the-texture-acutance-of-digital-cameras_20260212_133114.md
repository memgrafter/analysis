---
ver: rpa2
title: Hybrid Training of Denoising Networks to Improve the Texture Acutance of Digital
  Cameras
arxiv_id: '2404.07212'
source_url: https://arxiv.org/abs/2404.07212
tags:
- image
- images
- acutance
- dead
- leaves
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hybrid training approach for image denoising
  networks that incorporates both natural and synthetic dead leaves images to improve
  texture acutance metrics used in camera evaluation. The key idea is to use a perceptual
  loss based on texture acutance during training, which measures the camera's ability
  to preserve textures by analyzing its frequential response to dead leaves images.
---

# Hybrid Training of Denoising Networks to Improve the Texture Acutance of Digital Cameras

## Quick Facts
- arXiv ID: 2404.07212
- Source URL: https://arxiv.org/abs/2404.07212
- Reference count: 34
- Primary result: Hybrid training with natural and synthetic dead leaves images improves texture acutance scores without compromising standard fidelity metrics

## Executive Summary
This paper proposes a hybrid training approach for image denoising networks that incorporates both natural and synthetic dead leaves images to improve texture acutance metrics used in camera evaluation. The key idea is to use a perceptual loss based on texture acutance during training, which measures the camera's ability to preserve textures by analyzing its frequential response to dead leaves images. By training denoising networks with this acutance loss, the authors demonstrate significant improvements in texture acutance scores without compromising standard fidelity metrics like PSNR and SSIM on natural images. The approach is validated on both RGB image denoising with FFDNet and RAW image denoising with a U-Net architecture, showing improved acutance scores while maintaining competitive denoising performance.

## Method Summary
The method trains denoising networks using a hybrid dataset containing both natural images and synthetic dead leaves images. The loss function combines standard L2 fidelity loss with an acutance loss computed from the cross-power spectral density between denoised and target images. The acutance loss is only applied to dead leaves images in each minibatch, allowing the network to optimize texture preservation without affecting spatial fidelity on natural images. This approach is implemented for both RGB denoising with FFDNet and RAW denoising with a U-Net architecture.

## Key Results
- FFDNet with acutance loss achieves 0.01-0.02 higher texture acutance scores on Kodak24 while maintaining competitive PSNR/SSIM
- RAW U-Net with acutance loss shows improved acutance on both generated and real RAW images from SIDD benchmark
- Perceptual quality (PieAPP) is slightly enhanced for high noise levels when using acutance loss
- The hybrid training approach successfully preserves high-frequency texture details without introducing artifacts

## Why This Works (Mechanism)

### Mechanism 1
The acutance loss optimizes the Fourier spectrum preservation of texture without affecting spatial fidelity. The acutance loss is computed from the cross-power density between the denoised output and the target, focusing only on the amplitude spectrum. This is insensitive to spatial organization and avoids penalizing texture hallucination.

### Mechanism 2
Mixing natural and synthetic dead leaves images allows joint optimization of fidelity and texture preservation. The minibatch contains both natural and dead leaves images. The L2 loss is computed on all, but the acutance loss only on dead leaves, enabling decoupled training of spatial and spectral fidelity.

### Mechanism 3
The cross-spectrum formulation (MTFcross) avoids artifacts from noise and nonlinear processing that plague simpler MTF estimators. By using complex cross-spectrum and auto-spectrum, the method retains phase information and subtracts noise contribution implicitly, yielding a more accurate MTF.

## Foundational Learning

- **Dead leaves model and its statistical properties**: The synthetic training set relies on this model; understanding its invariance and spectrum is key to grasping why it works. Quick check: What spectral property of dead leaves images makes them suitable for texture acutance evaluation?
- **Cross-power spectral density and MTF computation**: The acutance loss is derived from this metric; knowing how it differs from simple power spectrum ratios is essential. Quick check: How does MTFcross differ from a naive power spectrum ratio in handling noise?
- **Perceptual contrast sensitivity function (CSF)**: The acutance metric weights frequencies by CSF to match human visual sensitivity. Quick check: Why does the CSF decay rapidly at high frequencies, and how does this affect the acutance loss?

## Architecture Onboarding

- **Component map**: Input (noisy images) -> Network (FFDNet/U-Net) -> Loss (L2 + λ * acutance loss) -> Output (denoised images)
- **Critical path**: 1) Generate synthetic dead leaves training set 2) Build mixed minibatches (natural + dead leaves) 3) Forward pass through network 4) Compute L2 loss on all images 5) Compute acutance loss only on dead leaves 6) Backpropagate combined loss
- **Design tradeoffs**: Larger patches (100x100) improve MTF estimation but increase memory; batch size reduction (64→32) reduces memory but may slow convergence; λ tuning: low λ preserves fidelity, high λ boosts acutance but risks PSNR drop
- **Failure signatures**: Overfitting to dead leaves: high acutance but poor PSNR on Kodak24; Phase corruption: unstable MTF estimates, noisy acutance loss; Memory overflow: patch size or batch size too large
- **First 3 experiments**: 1) Train FFDNet with λ=0 (baseline) and λ=10 on mixed set; compare PSNR/Kodak24 and acutance 2) Visualize MTF curves for λ=0 vs λ=10 to confirm spectral preservation 3) Test RAW U-Net with λ=0, 10, 100; evaluate RAW and RGB acutance

## Open Questions the Paper Calls Out

### Open Question 1
How can the acutance loss function be further optimized to preserve high-frequency details without compromising low-frequency performance? The paper mentions that the gap between MTF values for high frequencies is smaller, possibly due to the profile of the CSF function, which quickly decreases for high frequency. The paper suggests that considering alternative CSF functions could further improve the preservation of details, but does not explore this possibility.

### Open Question 2
What is the impact of using the acutance loss on the perceptual quality of images with different noise levels? The paper reports that the perceptual evaluation with the PieAPP metric suggests that, for high noise values, the perceptual image quality is slightly enhanced by the addition of the acutance loss. The paper only provides a general observation without detailed analysis of how the acutance loss affects perceptual quality across different noise levels.

### Open Question 3
How does the proposed mixed training approach perform on other image restoration tasks beyond denoising and RAW image development? The paper demonstrates the effectiveness of the acutance loss on denoising and RAW image development but does not explore its applicability to other restoration tasks. The paper focuses on specific tasks and does not provide evidence of the approach's generalizability to other image restoration challenges.

## Limitations
- Limited empirical validation that dead leaves model captures full diversity of real-world textures
- Phase preservation assumption in MTFcross computation not experimentally validated
- Mixing strategy for minibatches not optimized or tested for sensitivity

## Confidence

- **High confidence**: The hybrid training framework is well-defined and reproducible; improvement in texture acutance metrics on Kodak24 is demonstrated
- **Medium confidence**: Theoretical basis for using dead leaves images and MTFcross is sound, but empirical validation is limited to synthetic tests
- **Low confidence**: Long-term generalization to diverse real-world scenarios and robustness to different camera sensor characteristics are not evaluated

## Next Checks

1. Apply the hybrid training approach to a different denoising architecture (e.g., DnCNN or UNet++) and verify texture acutance improvements on the same benchmarks
2. Evaluate the denoised images on a diverse set of real-world textures (e.g., fabrics, foliage, urban scenes) using human perceptual studies or established texture similarity metrics beyond acutance
3. Systematically vary the ratio of natural to dead leaves images in training and measure the impact on both texture acutance and standard fidelity metrics to find the optimal balance