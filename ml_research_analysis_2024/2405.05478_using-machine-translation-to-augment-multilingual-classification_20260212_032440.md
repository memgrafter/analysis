---
ver: rpa2
title: Using Machine Translation to Augment Multilingual Classification
arxiv_id: '2405.05478'
source_url: https://arxiv.org/abs/2405.05478
tags:
- data
- language
- translated
- original
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of multilingual text classification
  by investigating whether machine-translated training data can effectively train
  multilingual models. The core method involves fine-tuning a multilingual DistilBERT
  model using translated reviews from a multilingual Amazon reviews dataset, comparing
  models trained with and without original-translated contrastive (OTC) loss to improve
  semantic similarity between translated pairs.
---

# Using Machine Translation to Augment Multilingual Classification

## Quick Facts
- arXiv ID: 2405.05478
- Source URL: https://arxiv.org/abs/2405.05478
- Reference count: 4
- Primary result: Models trained on translated data outperform zero-shot baselines, with OTC loss providing consistent F1-micro improvements across languages.

## Executive Summary
This paper investigates whether machine-translated training data can effectively train multilingual models for text classification. The study fine-tunes a multilingual DistilBERT model using translated Amazon reviews, comparing models with and without Original-Translated Contrastive (OTC) loss to improve semantic similarity between translated pairs. Results show that translated data significantly improves performance over zero-shot transfer baselines, though still underperforming original data. OTC loss consistently improves F1-micro scores across languages with statistical significance (coefficient = 0.036, p = 0.035), demonstrating its effectiveness in mitigating translation noise.

## Method Summary
The method involves fine-tuning a multilingual DistilBERT model on Amazon product reviews translated across six languages using M2M100. For each target language, training sets contain original data for that language plus translated data from the other five languages. Minibatches use a 1:1 ratio of original to translated examples. The model is trained with categorical cross-entropy loss, optionally including OTC loss weighted by 0.4. Evaluation uses F1-micro scores on held-out test sets. OTC loss encourages semantic alignment between original and translated pairs in the embedding space.

## Key Results
- Models trained on translated data outperformed zero-shot baselines across all six languages
- OTC loss consistently improved F1-micro scores with statistically significant gains (coefficient = 0.036, p = 0.035)
- Translated-only models still underperformed compared to models trained on original data (gap of .07-.12 F1-micro)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translated training data enables multilingual models to perform better than relying on zero-shot cross-lingual transfer alone.
- Mechanism: The model benefits from fine-tuning on semantically similar examples in the target language, even if translated, because the shared multilingual representation space in the transformer allows for cross-lingual knowledge transfer.
- Core assumption: Translated data preserves enough semantic fidelity to support effective fine-tuning without severe degradation in task performance.
- Evidence anchors:
  - [abstract]: "models trained on translated data outperformed baselines without exposure to the target language"
  - [section]: "for all languages, models trained using only translated data did show a noticeable improvement (.02-.11), though for each language, models trained with only translated data did underperform models trained with the full set of original, untranslated training examples (.07-.12)"
- Break condition: Translation quality degrades to the point where semantic content is no longer preserved, leading to noisy or misleading training signals.

### Mechanism 2
- Claim: OTC loss improves semantic alignment between original and translated pairs in the shared embedding space, mitigating translation noise.
- Mechanism: By encouraging similar embeddings for semantically equivalent sentences across languages, OTC loss reduces the model's reliance on translation artifacts and focuses on the underlying semantic content, improving robustness.
- Core assumption: The embedding space in multilingual models can meaningfully represent semantic equivalence across languages, even with translation noise.
- Evidence anchors:
  - [abstract]: "the inclusion of OTC loss consistently improved F1-micro scores across all languages, with statistically significant gains (coefficient = 0.036, p = 0.035)"
  - [section]: "models trained using OTC loss saw an improvement over models trained without for all languages except Chinese"
- Break condition: If the translation noise is too severe, forcing semantic alignment may reinforce incorrect mappings rather than mitigate noise.

### Mechanism 3
- Claim: Multilingual models generalize better when fine-tuned on a mix of original and translated data because they learn more robust cross-lingual representations.
- Mechanism: Fine-tuning on translated data exposes the model to diverse linguistic structures and expressions, forcing it to learn more general representations that transfer well to unseen languages.
- Core assumption: The model's ability to learn cross-lingual representations is enhanced by exposure to multiple linguistic variations during fine-tuning.
- Evidence anchors:
  - [abstract]: "models trained on translated data outperformed baselines without exposure to the target language"
  - [section]: "models trained with data for a European language showed higher performance on other European languages, compared to Japanese or Chinese"
- Break condition: If the model overfits to translation artifacts rather than learning genuine cross-lingual patterns.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: The study relies on multilingual models' ability to transfer knowledge from one language to another, which is fundamental to understanding why translated data can improve performance.
  - Quick check question: Why does training on English data sometimes improve performance on Spanish, even without seeing Spanish examples?

- Concept: Contrastive learning
  - Why needed here: OTC loss is a form of contrastive learning that pulls semantically similar examples closer in embedding space, which is crucial for understanding how it mitigates translation noise.
  - Quick check question: How does contrastive loss differ from standard classification loss, and why is it useful for aligning embeddings?

- Concept: Embedding space alignment
  - Why needed here: The effectiveness of OTC loss depends on the model's ability to align embeddings of semantically equivalent sentences across languages, which is central to the mechanism.
  - Quick check question: What properties must an embedding space have to support effective cross-lingual semantic alignment?

## Architecture Onboarding

- Component map:
  Multilingual DistilBERT model -> Translation pipeline (M2M100) -> OTC loss module (optional) -> Batch sampling logic -> Evaluation framework

- Critical path:
  1. Translate training data from source language to target languages
  2. Fine-tune multilingual model on original + translated data
  3. Apply OTC loss (if enabled) during training
  4. Evaluate on original test data in all languages

- Design tradeoffs:
  - Using translated data vs. collecting original annotations: cost vs. performance
  - 1:1 original:translated ratio vs. higher translated ratios: exposure vs. computational cost
  - OTC loss inclusion: potential gains vs. added complexity

- Failure signatures:
  - Performance worse than zero-shot transfer: translation quality too low
  - No improvement with OTC loss: embeddings not meaningfully aligned
  - High variance across runs: batch sampling or initialization issues

- First 3 experiments:
  1. Fine-tune on original data only vs. original + translated data (baseline comparison)
  2. Fine-tune with and without OTC loss (OTC effect isolation)
  3. Vary the original:translated ratio (tradeoff analysis)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ratio of original to translated data affect model performance in multilingual classification tasks?
- Basis in paper: [inferred] The paper mentions that models trained with translated data saw a sizable boost in performance despite seeing far fewer examples of each translated language (roughly one-fifth) compared to the original language. This suggests that the 1:1 original-to-translated ratio might not be optimal.
- Why unresolved: The paper did not explore varying the original-to-translated data ratio. It only used a fixed 1:1 ratio for all experiments.
- What evidence would resolve it: Systematic experiments varying the ratio of original to translated data (e.g., 1:2, 1:5, 2:1) and measuring the resulting F1-micro scores for each language would show how the ratio impacts performance.

### Open Question 2
- Question: Does the inclusion of Original-Translated Contrastive (OTC) loss mitigate structural differences between languages, particularly for linguistically dissimilar languages like Chinese?
- Basis in paper: [explicit] The paper states that Chinese, which is linguistically dissimilar from the majority of languages in the dataset, showed a mixed ability to benefit from training with other languages but a clearer improvement using OTC loss.
- Why unresolved: While the paper observed that OTC loss improved performance for Chinese, it did not investigate the underlying mechanism of how OTC loss affects individual examples or mitigates structural differences between languages.
- What evidence would resolve it: Detailed analysis of how OTC loss affects the embeddings of translated examples compared to their original counterparts, particularly for structurally different languages, would reveal the mechanism behind its effectiveness.

### Open Question 3
- Question: How does the quality of machine translation affect the performance of multilingual classifiers trained on translated data?
- Basis in paper: [explicit] The paper mentions that the potential benefits of using MT to train a multilingual model are affected by the quality of translation, with even the best translations losing some information from the original language, which can lead to dropped performance.
- Why unresolved: The paper used a single multilingual translation model (M2M100) and did not investigate how different translation qualities or models would affect the final classifier performance.
- What evidence would resolve it: Training multilingual classifiers on datasets translated by different machine translation models of varying quality and comparing the resulting F1-micro scores would demonstrate the impact of translation quality on model performance.

## Limitations

- Translation quality variability across language pairs is not explicitly quantified, making it difficult to assess the impact of translation noise on model performance.
- The study focuses on a single multilingual model (DistilBERT) and dataset (Amazon reviews), raising questions about the robustness of the results to different architectures and domains.
- The fixed 1:1 original:translated ratio in training batches is not justified through ablation studies, leaving open the question of whether this ratio is optimal.

## Confidence

- High: Models trained on translated data outperform zero-shot baselines
- Medium: OTC loss consistently improves F1-micro scores with statistical significance
- Low: Translated data can fully substitute for original annotations

## Next Checks

1. Evaluate the impact of translation quality on model performance by training models with varying levels of translation fidelity (e.g., using human-translated vs. machine-translated data) and measuring the degradation in F1 scores.
2. Test the robustness of OTC loss by applying it to a different multilingual model (e.g., XLM-R) and a different text classification task (e.g., sentiment analysis on social media data) to assess whether the improvements generalize beyond the current setup.
3. Conduct an ablation study to determine the optimal original:translated ratio for training, testing ratios such as 1:2, 1:3, and 2:1 to identify whether the fixed 1:1 ratio is indeed optimal or if other ratios yield better performance for specific languages.