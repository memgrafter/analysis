---
ver: rpa2
title: 'Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance
  of Large Language Models'
arxiv_id: '2402.14848'
source_url: https://arxiv.org/abs/2402.14848
tags:
- input
- length
- reasoning
- paragraphs
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how input length affects reasoning performance
  in large language models (LLMs). The authors create a novel dataset, FLenQA, consisting
  of three question-answering tasks requiring multi-step reasoning over two key facts
  embedded within longer irrelevant texts.
---

# Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models

## Quick Facts
- arXiv ID: 2402.14848
- Source URL: https://arxiv.org/abs/2402.14848
- Reference count: 22
- Primary result: LLMs show significant reasoning accuracy degradation at input lengths far shorter than their technical maximum

## Executive Summary
This paper investigates how input length affects reasoning performance in large language models (LLMs) using a novel dataset called FLenQA. The authors systematically evaluate five LLMs across input lengths from 250 to 3000 tokens, finding that reasoning accuracy degrades significantly (from 0.92 to 0.68 on average) well before reaching technical maximum input lengths. Chain-of-thought prompting improves performance but doesn't mitigate length-related drops in most models. The study also reveals a negative correlation between next-word prediction accuracy and reasoning performance on long inputs, identifying key failure modes including instruction-following difficulties and answer-first reasoning patterns.

## Method Summary
The authors create FLenQA, a novel dataset consisting of three question-answering tasks requiring multi-step reasoning over two key facts embedded within longer irrelevant texts. They systematically vary input length (250-3000 tokens), padding type (duplicate, similar, different), and key fact location (first, middle, last, random) while evaluating five LLMs (GPT-4, GPT-3.5, Gemini Pro, Mistral Medium, Mixtral 8x7B) with and without Chain-of-Thought prompting. Performance is measured through accuracy in answering True/False questions, with additional analysis of failure modes and next-word prediction accuracy.

## Key Results
- LLMs show significant accuracy degradation (from 0.92 to 0.68 on average) well before reaching technical maximum input lengths
- Chain-of-thought prompting improves performance but doesn't mitigate length-related drops in most models
- Next-word prediction accuracy correlates negatively with reasoning performance on long inputs
- Performance degrades regardless of padding type or key fact positioning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs degrade reasoning performance at input lengths far shorter than their technical maximum
- Mechanism: When input length increases, the model's ability to maintain coherent reasoning across multiple facts deteriorates, even if those facts are embedded in irrelevant padding
- Core assumption: The degradation is primarily due to input length rather than task complexity or data contamination
- Evidence anchors:
  - [abstract] "Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum"
  - [section] "Results show significant accuracy degradation (from 0.92 to 0.68 on average) well before reaching technical maximum input lengths"
  - [corpus] Weak - related work mentions long-context evaluation benchmarks but doesn't specifically address the pre-maximum degradation phenomenon
- Break condition: The degradation stops if padding is perfectly relevant (duplicate key paragraphs) - but even then some degradation occurs

### Mechanism 2
- Claim: Next-word prediction accuracy correlates negatively with reasoning performance on long inputs
- Mechanism: As models become better at predicting the next word in longer sequences, they allocate more attention to local coherence rather than maintaining global reasoning across multiple facts
- Core assumption: Next-word prediction accuracy and reasoning performance are measuring different capabilities that can trade off against each other
- Evidence anchors:
  - [abstract] "Additionally, our study reveals that the traditional metric of next word prediction correlates negatively with performance of LLMs' on our reasoning dataset"
  - [section] "Our method finds similar trends on the next word prediction task to those shown in other works... however... next word accuracy correlates negatively with reasoning"
  - [corpus] Weak - corpus mentions perplexity studies but doesn't specifically address the negative correlation with reasoning tasks
- Break condition: If the reasoning task becomes purely local (no need to connect distant facts), the correlation might disappear

### Mechanism 3
- Claim: Chain-of-thought prompting improves performance but doesn't mitigate length-related drops
- Mechanism: CoT provides structure for reasoning but doesn't solve the underlying attention and context window limitations that cause degradation with length
- Core assumption: The improvement from CoT is consistent across lengths rather than scaling with input size
- Evidence anchors:
  - [abstract] "Chain-of-thought prompting improves performance but doesn't mitigate length-related drops in most models"
  - [section] "We also show that while Chain-of-Thought (CoT) prompting... increases performance in short inputs, in most models it does not mitigate the degradation of performance when inputs are longer"
  - [corpus] Moderate - related work mentions CoT techniques but doesn't specifically analyze their interaction with input length
- Break condition: If the model's architecture fundamentally changes to better handle long contexts, CoT might then scale with length

## Foundational Learning

- Concept: Attention mechanisms in transformer models
  - Why needed here: Understanding how transformers attend to different parts of long inputs is crucial for explaining why performance degrades
  - Quick check question: How does the attention mechanism in transformers handle tokens that are far apart in the input sequence?

- Concept: Next-word prediction vs. reasoning tasks
  - Why needed here: The paper shows these two metrics can be negatively correlated, indicating they measure different aspects of model capability
  - Quick check question: What are the key differences between next-word prediction tasks and multi-step reasoning tasks that might explain their different performance patterns?

- Concept: Prompt engineering and Chain-of-Thought
  - Why needed here: The paper evaluates whether CoT can mitigate length-related degradation, requiring understanding of how different prompting strategies work
  - Quick check question: How does Chain-of-Thought prompting change the way a model processes information compared to direct question answering?

## Architecture Onboarding

- Component map: FLenQA dataset generation with controlled padding and key fact placement -> Model evaluation across multiple input lengths (250-3000 tokens) -> Accuracy measurement with both direct and CoT prompting -> Failure mode analysis -> Conclusions about length effects
- Critical path: Generate dataset → Evaluate models at each length → Measure accuracy → Analyze failure modes → Draw conclusions about length effects
- Design tradeoffs: The dataset uses novel generated data to avoid contamination, which means results may not generalize to all reasoning tasks; using duplicate padding tests the extreme case but may not reflect natural usage
- Failure signatures: Look for accuracy dropping below 80% at lengths under 1000 tokens, negative correlation between next-word accuracy and reasoning accuracy, CoT failing to close the performance gap as length increases
- First 3 experiments:
  1. Run baseline evaluation on minimal text (250 tokens) to establish performance ceiling
  2. Test with duplicate padding to isolate length effect from relevance effect
  3. Evaluate with non-adjacent key paragraphs and different padding types to test most realistic scenario

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the degradation of reasoning performance with input length vary across different reasoning task complexities?
- Basis in paper: [inferred] The paper suggests focusing on a subset of reasoning task types and mentions that more complex tasks (e.g., involving 5 key paragraphs) might exhibit different performance degradation patterns.
- Why unresolved: The study focused on tasks with two key paragraphs and did not explore the impact of task complexity on performance degradation.
- What evidence would resolve it: Experiments with a wider range of reasoning task complexities, including those with more key paragraphs or more complex reasoning steps, to compare degradation patterns across task types.

### Open Question 2
- Question: What are the underlying mechanisms causing the negative correlation between next-word prediction accuracy and reasoning performance on long inputs?
- Basis in paper: [explicit] The paper found that next-word prediction accuracy correlates negatively with reasoning accuracy on FlenQA and suggests that measuring next-word prediction or perplexity cannot substitute downstream task evaluation on long inputs.
- Why unresolved: The paper identifies the correlation but does not explore the reasons behind it or propose solutions to address this issue.
- What evidence would resolve it: Further analysis of model behavior during next-word prediction and reasoning tasks, potentially through ablation studies or examination of attention patterns, to identify the mechanisms causing this negative correlation.

### Open Question 3
- Question: How can Chain-of-Thought prompting be optimized to mitigate performance degradation on long inputs across different LLMs?
- Basis in paper: [explicit] The paper found that while CoT improves performance, it does not mitigate degradation due to length in most models, except for GPT-4 where the gap between CoT and normal prompting increases with input length.
- Why unresolved: The study used a standard CoT prompt and did not explore variations in prompt engineering or model-specific optimizations to enhance CoT's effectiveness on long inputs.
- What evidence would resolve it: Systematic experiments with different CoT prompt structures, model-specific fine-tuning, or hybrid prompting strategies to determine the optimal approach for maintaining performance on long inputs.

## Limitations

- Task specificity: The FLenQA dataset uses only three reasoning tasks, limiting generalizability to other types of reasoning tasks
- Dataset generation: The synthetic nature of the dataset may not capture the full complexity of real-world long-form reasoning scenarios
- Model architecture differences: Comparing different model architectures makes it difficult to isolate input length effects from architectural differences

## Confidence

**High Confidence**: The core finding that LLMs show significant reasoning performance degradation at input lengths far shorter than their technical maximum is well-supported by systematic experiments across multiple models and conditions.

**Medium Confidence**: The conclusion that Chain-of-Thought prompting doesn't mitigate length-related degradation is supported, but the underlying mechanism remains less certain.

**Low Confidence**: The generalizability of these findings to all reasoning tasks and real-world applications remains uncertain due to the limited task diversity and synthetic dataset construction.

## Next Checks

1. **Cross-task validation**: Evaluate the same length degradation patterns on diverse reasoning benchmarks (e.g., GSM8K, StrategyQA, and naturally long documents from sources like QuALITY) to test generalizability beyond the three FLenQA tasks.

2. **Architecture-specific analysis**: Conduct ablation studies comparing models with similar architectures but different context window implementations (e.g., different attention mechanisms or positional encoding schemes) to isolate architectural factors from input length effects.

3. **Real-world application test**: Implement a prototype system using the longest input length that maintains 95% of maximum accuracy for each model, then evaluate this threshold in a realistic use case (e.g., multi-document question answering or long-form document summarization) to assess practical implications.