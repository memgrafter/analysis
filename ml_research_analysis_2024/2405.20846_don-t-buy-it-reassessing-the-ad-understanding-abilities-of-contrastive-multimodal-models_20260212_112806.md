---
ver: rpa2
title: Don't Buy it! Reassessing the Ad Understanding Abilities of Contrastive Multimodal
  Models
arxiv_id: '2405.20846'
source_url: https://arxiv.org/abs/2405.20846
tags:
- trade
- should
- text
- because
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reveals that zero-shot performance of contrastive vision-and-language
  models on an ad understanding task is driven by grounding heuristics rather than
  genuine multimodal reasoning. By introducing TRADE, a new evaluation set with adversarially
  constructed, visually and textually grounded but implausible explanations, the authors
  show that four leading models (CLIP, ALIGN, LiT, ALBEF) perform at chance level,
  in stark contrast to human performance (94% accuracy).
---

# Don't Buy it! Reassessing the Ad Understanding Abilities of Contrastive Multimodal Models

## Quick Facts
- arXiv ID: 2405.20846
- Source URL: https://arxiv.org/abs/2405.20846
- Reference count: 40
- Zero-shot contrastive vision-and-language models rely on grounding heuristics rather than genuine multimodal reasoning for ad understanding

## Executive Summary
This study investigates whether contrastive vision-and-language models (VLMs) like CLIP truly understand advertisements or simply exploit grounding heuristics. The authors evaluate four leading models on an ad understanding task where models must select the correct explanation for an advertisement from three candidates. By introducing TRADE, a new evaluation set with adversarially constructed explanations that are both visually and textually grounded but implausible, the researchers demonstrate that these models perform at chance level (33% accuracy) compared to human performance (94% accuracy). The findings reveal that the original evaluation setup allowed models to succeed by exploiting correlations between grounding scores and alignment scores rather than understanding the complex visual and linguistic elements in advertisements.

## Method Summary
The study evaluates zero-shot performance of CLIP, ALIGN, LiT, and ALBEF on an ad understanding task using the Pitt Ads dataset. For each ad, models select the correct explanation from three candidates by computing image-text alignment scores. The authors introduce TRADE, a new evaluation set where adversarial negatives are created to be both visually and textually grounded but implausible, controlling for the grounding heuristic. Grounding scores (text overlap, text similarity, object mention, caption similarity) are computed to analyze the relationship between grounding and model performance. Human accuracy is measured through crowdsourcing.

## Key Results
- CLIP and other contrastive VLMs achieve 98% accuracy on the original task but only 33% on TRADE (chance level)
- Humans maintain 94% accuracy on both the original task and TRADE
- Positive and negative explanations in the original setup show significant grounding score gaps, while TRADE equalizes these scores
- Strong positive correlations exist between grounding scores and CLIP's alignment scores in the original setup

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP and similar contrastive VLMs solve ad understanding by leveraging grounding heuristics rather than genuine multimodal reasoning.
- Mechanism: The models use visual and textual grounding scores (text overlap, text similarity, object mention, caption similarity) to align ads with explanations, exploiting correlations between grounding scores and alignment scores.
- Core assumption: The evaluation setup allows models to succeed by exploiting grounding heuristics without truly understanding the ad's persuasive message.
- Evidence anchors:
  - [abstract] "This study reveals that zero-shot performance of contrastive vision-and-language models on an ad understanding task is driven by grounding heuristics rather than genuine multimodal reasoning."
  - [section] "We observe a positive correlation between all our grounding scores and CLIP's alignment score... the matching explanations are significantly more grounded than the non-matching explanations for each ad."
  - [corpus] Weak: No direct corpus evidence of grounding heuristics; inference from experimental setup and results.
- Break condition: If adversarial negatives are created that are both visually and textually grounded but clearly implausible, models will fail at chance level.

### Mechanism 2
- Claim: The original evaluation setup has a grounding gap between positive and negative explanations, which models exploit.
- Mechanism: In the original setup, matching explanations have significantly higher grounding scores (text overlap, text similarity, object mention, caption similarity) compared to non-matching explanations, allowing models to discriminate based on grounding rather than understanding.
- Core assumption: The grounding gap is a major factor in model performance on the original task.
- Evidence anchors:
  - [abstract] "we introduce TRADE, a new evaluation test set with adversarial grounded explanations... they 'fool' four different contrastive VLMs."
  - [section] "we find that in the original setup the matching explanations are significantly more grounded than the non-matching explanations for each ad."
  - [corpus] Weak: No direct corpus evidence of grounding gap; inference from experimental setup and results.
- Break condition: If grounding scores for positive and negative explanations are equalized, model performance will drop to chance level.

### Mechanism 3
- Claim: TRADE eliminates the grounding gap, exposing the limitations of VLMs in multimodal reasoning.
- Mechanism: By creating adversarial negatives that are textually and visually grounded but implausible, TRADE controls for the grounding heuristic, forcing models to rely on genuine understanding rather than grounding.
- Core assumption: If models truly understood ads, they would perform well on TRADE despite the equalized grounding scores.
- Evidence anchors:
  - [abstract] "we introduce TRADE... controls for this confound... show that several contrastive models tested zero-shot... perform at chance level on TRADE, while humans excel at the task."
  - [section] "In TRADE the gap between positive and negative explanations is radically reduced compared to the original setup... we confirm that humans are not affected by the high level of grounding of both positive and negative examples."
  - [corpus] Weak: No direct corpus evidence of TRADE's effectiveness; inference from experimental setup and results.
- Break condition: If models develop genuine multimodal reasoning abilities, they will perform well on TRADE despite the equalized grounding scores.

## Foundational Learning

- Concept: Grounding heuristics in vision-and-language models
  - Why needed here: Understanding how models exploit visual and textual grounding to solve tasks without genuine reasoning is crucial for interpreting their performance.
  - Quick check question: Can you explain how grounding heuristics might allow a model to solve an ad understanding task without truly understanding the ad's message?

- Concept: Evaluation setup design and its impact on model performance
  - Why needed here: Recognizing the importance of task operationalization and the potential for models to exploit flaws in evaluation setups is essential for reliable model assessment.
  - Quick check question: How might the design of an evaluation setup influence a model's ability to demonstrate genuine understanding versus exploiting heuristics?

- Concept: Adversarial evaluation and its role in uncovering model limitations
  - Why needed here: Understanding how to create adversarial examples that control for specific confounds is key to developing more robust and reliable evaluation methods.
  - Quick check question: What are the benefits and challenges of using adversarial evaluation to test a model's reasoning abilities?

## Architecture Onboarding

- Component map:
  Input -> Grounding Score Calculators -> CLIP Model -> Alignment Score Computation -> Model Predictions -> Performance Metrics

- Critical path:
  1. Load ad images and candidate explanations
  2. Compute grounding scores for each candidate explanation
  3. Compute CLIP alignment scores for each ad-explanation pair
  4. Select the explanation with the highest alignment score as the model's prediction
  5. Calculate performance metrics (accuracy, grounding score gaps)

- Design tradeoffs:
  - Balancing the need for realistic adversarial negatives with the need to control for grounding heuristics
  - Choosing appropriate grounding score metrics that capture relevant aspects of visual and textual grounding
  - Ensuring the evaluation setup is challenging enough to expose model limitations without being impossible

- Failure signatures:
  - High performance on the original task but chance-level performance on TRADE
  - Strong correlations between grounding scores and alignment scores
  - Significant grounding score gaps between positive and negative explanations in the original setup

- First 3 experiments:
  1. Replicate the original task setup and compute grounding scores to confirm the grounding gap
  2. Create TRADE by designing adversarial negatives that are grounded but implausible
  3. Evaluate CLIP and other models on TRADE and compare their performance to human accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do models trained or fine-tuned on the Pitt Ads dataset perform on TRADE compared to zero-shot contrastive models?
- Basis in paper: [inferred] The paper states this remains an open question for future research, noting that their study focuses on zero-shot evaluation of contrastive models.
- Why unresolved: The authors explicitly state they haven't investigated models trained/fine-tuned on Pitt Ads dataset, limiting their scope to zero-shot evaluation.
- What evidence would resolve it: Training/fine-tuning various models (including those tested zero-shot) on Pitt Ads dataset and evaluating their performance on TRADE would directly compare trained vs. zero-shot approaches.

### Open Question 2
- Question: Would formulating ad understanding as a generative task rather than retrieval-based improve VLM evaluation?
- Basis in paper: [explicit] The authors discuss this as a future research direction, noting that generative tasks would solve some retrieval limitations but introduce new challenges like finding effective prompts and evaluation protocols.
- Why unresolved: The paper identifies this as an interesting direction but doesn't explore it experimentally, leaving the effectiveness of generative approaches unknown.
- What evidence would resolve it: Implementing ad understanding as a generative task with various prompt engineering approaches and developing evaluation metrics for generated explanations would provide empirical evidence.

### Open Question 3
- Question: How do personal and cultural factors influence ad interpretation, and how can evaluation protocols reflect this diversity?
- Basis in paper: [explicit] The authors note their study doesn't account for personal/cultural factors in ad perception and express hope that future research will adopt protocols reflecting diverse interpretations.
- Why unresolved: The current study uses expert annotators for creating adversarial examples and crowdsourcing for human accuracy, but doesn't systematically investigate cultural/personal variation in ad understanding.
- What evidence would resolve it: Conducting cross-cultural studies with diverse participant groups and developing evaluation metrics that capture multiple valid interpretations would address this limitation.

## Limitations

- The study relies on a single adversarial dataset (TRADE) to demonstrate model failures, raising questions about generalizability
- The analysis assumes human performance represents genuine multimodal understanding without alternative explanations
- The findings are limited to four specific contrastive models and may not generalize to other vision-and-language architectures

## Confidence

High confidence: The experimental demonstration that CLIP and related models achieve near-chance performance on TRADE while humans maintain high accuracy provides strong evidence for the grounding heuristic hypothesis.

Medium confidence: The correlation analysis between grounding scores and alignment scores suggests a mechanism for how models exploit the original evaluation setup, though causal relationships require further validation.

Low confidence: The generalizability of these findings to other vision-and-language tasks and model architectures beyond the four contrastive models tested.

## Next Checks

1. Test additional vision-and-language model architectures (including non-contrastive approaches) on TRADE to determine whether grounding heuristics are a universal limitation or specific to contrastive training.

2. Conduct ablation studies systematically removing individual grounding features (text overlap, object mentions, etc.) to isolate which heuristics are most exploited by current models.

3. Develop a second adversarial dataset with different properties (e.g., focusing on other types of misleading but grounded explanations) to verify that the TRADE findings are not dataset-specific artifacts.