---
ver: rpa2
title: Scalable iterative pruning of large language and vision models using block
  coordinate descent
arxiv_id: '2411.17796'
source_url: https://arxiv.org/abs/2411.17796
tags:
- pruning
- weights
- optimization
- problem
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an iterative pruning method called iCBS that
  scales to very large models like LLMs by solving small optimization problems over
  blocks of weights. The method starts with an initial pruning solution and iteratively
  improves it by solving a series of combinatorial optimization problems using block
  coordinate descent.
---

# Scalable iterative pruning of large language and vision models using block coordinate descent

## Quick Facts
- arXiv ID: 2411.17796
- Source URL: https://arxiv.org/abs/2411.17796
- Reference count: 0
- One-line primary result: iCBS achieves higher accuracy at the same density levels compared to Wanda on Mistral and DeiT models

## Executive Summary
This paper presents iCBS, an iterative pruning method that scales to very large language and vision models by solving small optimization problems over blocks of weights. The method starts with an initial pruning solution and iteratively improves it using block coordinate descent, achieving higher accuracy at the same density levels compared to existing methods. The key innovation is optimizing over only a small fraction of weights at a time, making the approach tractable for models with billions of parameters.

## Method Summary
iCBS is an iterative pruning algorithm that uses block coordinate descent to solve small combinatorial optimization problems over subsets of network weights. The method begins with an initial pruning solution obtained through a weight-scoring method, then iteratively refines this solution by solving a series of quadratic binary optimization problems (QCBO/QUBO) over blocks of weights. During each iteration, the algorithm estimates gradients and Hessians for the selected block, constructs an optimization problem to minimize loss change while maintaining target density, and solves it using a tabu search algorithm. The approach includes weight fixing strategies for extreme weights and tabu lists to avoid cycling through the same weights.

## Key Results
- iCBS achieves higher accuracy at the same density levels compared to Wanda on Mistral and DeiT models
- The method solves optimization problems over only a small fraction of weights, making it scalable to very large models
- iCBS allows for a quality-time tradeoff by adjusting block size and number of iterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Solving small block-level optimization problems instead of one global problem makes the pruning tractable for very large models.
- Mechanism: By dividing the pruning problem into blocks, each block's Hessian and gradient can be computed and stored in memory, and a combinatorial optimization solver can find the optimal subset of weights to prune within that block.
- Core assumption: The Taylor approximation of the loss change due to pruning remains valid over small blocks of weights.
- Evidence anchors:
  - [abstract] "solves an optimization problem over a subset of the network weights in an iterative, block-wise manner using block coordinate descent"
  - [section IV.A] "we maintain a vector wc with the current state of the weights, given the pruning decisions made in the steps completed so far"
  - [corpus] Weak: No corpus paper explicitly validates the assumption that block-wise Taylor approximation remains valid; this is assumed from the original CBS paper context.
- Break condition: If the block size is too large, the Hessian and gradient cannot be stored in memory; if too small, the per-block optimization overhead dominates.

### Mechanism 2
- Claim: Fixing extreme weights early reduces the search space and focuses optimization on uncertain weights.
- Mechanism: After initial pruning, weights with the lowest and highest scores (by a weight-scoring method) are fixed to be pruned or kept, respectively. This removes obvious decisions from the per-block optimization.
- Core assumption: Weights with extreme initial scores are unlikely to need rebalancing with other weights to minimize loss change.
- Evidence anchors:
  - [section IV.B] "We fix a fraction of the weights with the most extreme scores (high or low)"
  - [section IV.B] "extreme weight scores indicate that those weights should be pruned/kept with high confidence"
  - [corpus] Weak: No corpus paper explicitly measures the impact of fixing extreme weights; this is described as a design choice in the paper.
- Break condition: If too many weights are fixed, the remaining search space may not contain enough weights to achieve the target density.

### Mechanism 3
- Claim: Using a tabu list avoids cycling and improves exploration of the solution space.
- Mechanism: Weights selected in a block are added to a tabu list for that layer, preventing them from being selected again for a number of steps proportional to the layer size.
- Core assumption: Cycling through the same weights repeatedly is likely to get stuck in local optima; exploring new weights helps escape them.
- Evidence anchors:
  - [section IV.B] "we implement a tabu list for each layer [9]. In each step, when the n weights for the block are selected, the weights in the respective tabu list are excluded"
  - [section IV.B] "After the selection, the n weights that are selected are added to the tabu list for the respective layer"
  - [corpus] Moderate: The reference [9] is a classic tabu search paper, but no corpus paper explicitly validates its use in block pruning.
- Break condition: If the tabu list is too long relative to the number of weights, many weights may be unavailable for selection, slowing convergence.

## Foundational Learning

- Concept: Combinatorial optimization (QCBO/QUBO)
  - Why needed here: The per-block pruning problem is formulated as a binary quadratic optimization problem, which is solved using a QCBO solver.
  - Quick check question: What is the difference between a QUBO and a QCBO? (Answer: QUBO has no constraints; QCBO has constraints like the number of weights to prune.)

- Concept: Taylor approximation of loss function
  - Why needed here: The pruning decision is based on minimizing the change in the loss function, approximated by a second-order Taylor expansion around the current weights.
  - Quick check question: What terms are included in the Taylor approximation of the loss change in this work? (Answer: The gradient term, Hessian term, and a ridge term for stability.)

- Concept: Block coordinate descent
  - Why needed here: The overall pruning problem is decomposed into smaller block problems, each solved iteratively while keeping other blocks fixed.
  - Quick check question: What is the main benefit of using block coordinate descent in this pruning context? (Answer: It makes the problem tractable for very large models by limiting memory and computational requirements per step.)

## Architecture Onboarding

- Component map:
  Data loader -> Initial pruning -> Weight fixing -> Per-block optimizer -> Tabu manager -> Model updater -> Evaluator

- Critical path:
  1. Initial pruning → weight fixing
  2. For each epoch:
     a. For each step:
        i. Select block → estimate gradient/Hessian → construct QCBO → solve → update model
        ii. Update tabu lists
     b. Evaluate on validation data

- Design tradeoffs:
  - Block size n: Larger → more accurate per-step updates but higher memory/compute cost
  - Number of restarts: More → better solution quality but longer solve time
  - Tabu fraction: Larger → more exploration but slower convergence

- Failure signatures:
  - If accuracy plateaus early: Block size may be too small or too many weights are fixed
  - If memory error: Block size or batch size too large for available GPU memory
  - If very slow: QCBO solver may be inefficient; consider hardware accelerator or simpler solver

- First 3 experiments:
  1. Run iCBS on Garment Classifier with block_size=256, num_epochs=5, num_steps=100; observe accuracy vs density curve
  2. Vary block_size (128, 512, 1024) on same model; compare final accuracy and runtime
  3. Replace tabu list with random selection; compare convergence speed and final accuracy

## Open Questions the Paper Calls Out
The paper explicitly identifies several open questions and future directions:
- Extending the formulation to structured pruning (pruning entire nodes, layers, or attention heads rather than individual weights)
- Applying the method to pruning activation functions
- Exploring quantum computing hardware accelerators for solving the block optimization problems
- Investigating the relationship between block size and solution quality

## Limitations
- The performance claims are primarily based on comparisons with a single baseline (Wanda) on specific models and datasets
- The mechanism relies on assumptions about Taylor approximation validity over small blocks that are not explicitly validated
- The individual contributions of weight fixing and tabu lists to overall performance gains are not empirically isolated

## Confidence

**High Confidence:** The core algorithmic framework (block coordinate descent for iterative pruning) is clearly defined and implementable.

**Medium Confidence:** The general performance improvement over Wanda is demonstrated, but the extent of improvement across different models and tasks remains uncertain.

**Low Confidence:** The individual contributions of weight fixing, tabu lists, and block size selection to the overall performance gains are not empirically isolated.

## Next Checks
1. Conduct ablation studies to measure the individual impact of weight fixing and tabu lists on convergence speed and final accuracy
2. Apply iCBS to larger language models (e.g., LLaMA, GPT-2) and vision models (e.g., ResNet, ViT) on diverse datasets to assess generalizability
3. Analyze the error introduced by the Taylor approximation in blocks of varying sizes by comparing pruned models with exact re-training or fine-tuning