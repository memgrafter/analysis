---
ver: rpa2
title: 'HC-GLAD: Dual Hyperbolic Contrastive Learning for Unsupervised Graph-Level
  Anomaly Detection'
arxiv_id: '2407.02057'
source_url: https://arxiv.org/abs/2407.02057
tags:
- graph
- hyperbolic
- learning
- hypergraph
- anomaly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HC-GLAD, a dual hyperbolic contrastive learning
  framework for unsupervised graph-level anomaly detection. It addresses the limitations
  of traditional GNNs that rely on pairwise relationships and Euclidean space by incorporating
  hypergraph learning and hyperbolic geometry.
---

# HC-GLAD: Dual Hyperbolic Contrastive Learning for Unsupervised Graph-Level Anomaly Detection

## Quick Facts
- arXiv ID: 2407.02057
- Source URL: https://arxiv.org/abs/2407.02057
- Reference count: 40
- Key outcome: Achieves best average rank among 10 methods with max AUC of 99.51% on 13 real-world datasets

## Executive Summary
HC-GLAD introduces a dual hyperbolic contrastive learning framework for unsupervised graph-level anomaly detection that addresses limitations of traditional GNNs. The method combines hypergraph learning based on gold motifs with hyperbolic geometry to capture high-order node group information and preserve hierarchical structures. Extensive experiments demonstrate superior performance across 13 datasets, with ablation studies confirming the necessity of both components.

## Method Summary
HC-GLAD constructs hypergraphs using gold motifs (triangular relationships) to capture high-order node group information, then performs embedding learning in hyperbolic space to preserve hierarchical structures. The framework uses dual contrastive learning at both node-level and graph-level across two augmented views of each graph, implemented through both graph and hypergraph channels. The model outputs anomaly scores computed from the total loss, combining contrastive objectives with hyperbolic geometry operations.

## Key Results
- Achieves best average rank among all compared methods on 13 real-world datasets
- Maximum AUC of 99.51% demonstrates strong anomaly detection performance
- Ablation study confirms both hypergraph and hyperbolic components are necessary, with hyperbolic learning showing more pronounced impact

## Why This Works (Mechanism)

### Mechanism 1
Hypergraph construction based on gold motifs captures high-order node group information that pairwise GNNs miss. Triangular motifs create hyperedges connecting multiple nodes simultaneously, enabling message passing across broader node groups rather than just edges. This works because anomalous graphs exhibit distinctive high-order structures that can be captured through motif-based hyperedges.

### Mechanism 2
Hyperbolic geometry preserves hierarchical structures in graph data better than Euclidean space. The exponential volume growth in hyperbolic space matches tree-like structures common in real graphs, allowing better separation of hierarchical levels during embedding. This is effective because real-world graph datasets exhibit tree-like hierarchical structures with power-law degree distributions that benefit from hyperbolic embedding.

### Mechanism 3
Dual contrastive learning (node-level + graph-level) maximizes mutual information across views to improve anomaly detection. Two augmented views are created for each graph, then contrastive losses are computed in hyperbolic space to ensure consistent embeddings across views while preserving anomalous patterns. This works because normal graphs should produce consistent embeddings across augmented views while anomalous graphs will show inconsistencies.

## Foundational Learning

- **Graph Neural Networks (GNNs) and message passing**: Why needed here - Understanding why traditional GNNs with pairwise edges are insufficient for capturing complex anomaly patterns. Quick check question: What limitation do traditional GNNs have when detecting anomalies involving multi-node interactions?

- **Hyperbolic geometry and Riemannian manifolds**: Why needed here - The hyperboloid model and exponential/logarithmic maps are central to the embedding process. Quick check question: Why does hyperbolic space better preserve hierarchical structures compared to Euclidean space?

- **Hypergraphs and high-order relationships**: Why needed here - Hyperedges can connect multiple nodes simultaneously, capturing group interactions that regular edges cannot. Quick check question: How does a hyperedge differ from a regular edge in terms of node connectivity?

## Architecture Onboarding

- **Component map**: Input graphs -> Graph augmentation + Hypergraph construction -> Graph channel (GNN in hyperbolic space) + Hypergraph channel (HGCN in hyperbolic space) -> Multi-level contrastive losses -> Anomaly scores

- **Critical path**: 1. Preprocess input graphs (augmentation + hypergraph construction) 2. Perform hyperbolic graph aggregation on both views 3. Perform hyperbolic hypergraph aggregation on both views 4. Compute multi-level contrastive losses 5. Calculate anomaly scores from total loss

- **Design tradeoffs**: Computational complexity vs performance - adding hypergraph and hyperbolic components increases complexity but improves detection accuracy. Motif selection - triangle motifs work well but may miss other high-order patterns. Dimension choice - higher dimensions provide more capacity but increase computational cost.

- **Failure signatures**: Poor performance on datasets with low hyperbolicity values (non-hierarchical structures). Degradation when number of encoder layers is too high (over-smoothing). Suboptimal performance if augmentation doesn't create sufficiently different views.

- **First 3 experiments**: 1. Verify hypergraph construction works by checking hyperedge count and structure on a small test graph 2. Test hyperbolic aggregation by comparing embeddings in hyperbolic vs Euclidean space on a simple dataset 3. Validate contrastive learning by checking if normal graphs produce consistent embeddings across views while anomalous graphs don't

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of motif structure (e.g., triangles vs. other high-order relationships) impact the performance of hypergraph construction for anomaly detection? The paper only tests one alternative motif structure (triangle variant), leaving open whether other motif structures might be more effective for different types of graph data or anomalies.

### Open Question 2
What is the optimal balance between hyperbolic and hypergraph learning components for different types of graph anomalies? The ablation study shows both components are necessary, but doesn't explore how this balance should vary by anomaly type.

### Open Question 3
How does the performance of HC-GLAD scale with graph size and complexity? The paper tests on datasets with varying numbers of nodes and edges, but doesn't explicitly analyze how performance changes with increasing graph complexity or computational costs.

## Limitations
- Reliance on gold motif selection (triangles) which may not generalize to all anomaly types
- Lack of hyperparameter details that could affect reproducibility
- Assumption that all datasets exhibit hierarchical structures suitable for hyperbolic embedding

## Confidence
- High confidence: The dual contrastive learning framework combining node-level and graph-level objectives
- Medium confidence: The superiority of hyperbolic space over Euclidean space for hierarchical datasets
- Medium confidence: The effectiveness of gold motif-based hypergraph construction for anomaly detection

## Next Checks
1. Test HC-GLAD's performance on synthetic datasets with known hyperbolicity values to verify hyperbolic geometry benefits only hierarchical datasets
2. Conduct ablation studies on different motif types to assess whether gold motif selection is critical to performance
3. Measure and report the computational overhead of the dual hyperbolic contrastive learning approach compared to simpler baselines