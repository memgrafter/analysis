---
ver: rpa2
title: What Else Would I Like? A User Simulator using Alternatives for Improved Evaluation
  of Fashion Conversational Recommendation Systems
arxiv_id: '2401.05783'
source_url: https://arxiv.org/abs/2401.05783
tags:
- user
- target
- systems
- items
- simulator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of evaluating conversational recommendation
  systems (CRS) in fashion by introducing an improved user simulator that considers
  alternative target items. The core method involves creating extended datasets with
  labeled alternatives for existing CRS datasets (Shoes and FashionIQ Dresses) and
  developing a meta-user simulator that allows simulated users to consider alternatives
  when their patience runs out.
---

# What Else Would I Like? A User Simulator using Alternatives for Improved Evaluation of Fashion Conversational Recommendation Systems

## Quick Facts
- arXiv ID: 2401.05783
- Source URL: https://arxiv.org/abs/2401.05783
- Reference count: 40
- One-line primary result: Alternative-based user simulator significantly improves evaluation of fashion conversational recommendation systems, showing 14-140% improvements in MRR@10 and Success Rate metrics

## Executive Summary
This paper addresses a critical limitation in conversational recommendation system (CRS) evaluation by introducing an improved user simulator that considers alternative target items. The authors demonstrate that traditional single-target evaluation underestimates CRS effectiveness by failing to capture natural user behavior when exploring alternatives. Through crowdsourcing alternative judgments for existing fashion CRS datasets and implementing a meta-user simulator that switches to alternative targets when patience thresholds are exceeded, the study shows substantial improvements in evaluation metrics across multiple CRS models and datasets.

## Method Summary
The method involves creating extended datasets with labeled alternatives through crowdsourcing, then implementing a meta-user simulator that wraps existing simulators to allow users to switch targets when their patience runs out. The simulator selects the closest alternative to the current item using image similarity and continues the conversation with this new target. The approach is evaluated on three CRS models (GRU with reinforcement learning, GRU with supervised learning, and EGE) using Success Rate @ 1, NDCG@10, and MRR@10 metrics at conversation turns 3, 5, and 10.

## Key Results
- Alternative-based evaluation improved MRR@10 and Success Rate metrics by 14-140% compared to single-target evaluation
- The meta-user simulator successfully captured exploratory user behavior across both Shoes and FashionIQ Dresses datasets
- Improvements were consistent across all three CRS models tested (GRU-RL, GRU-SL, and EGE)
- The approach demonstrated that single-target evaluation artificially constrains user satisfaction and underestimates CRS effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alternative-based user simulator improves CRS evaluation by capturing exploratory user behavior
- Mechanism: When simulated user's patience threshold is exceeded, the simulator selects an alternative item similar to the current recommendation and continues the conversation, allowing for multiple relevant targets
- Core assumption: Users naturally explore alternatives when their initial target becomes difficult to find, and this behavior is not captured by single-target evaluation
- Evidence anchors:
  - [abstract] "when simulated users are allowed to instead consider alternatives, the system can rapidly respond to more quickly satisfy the user"
  - [section 3] "Our intuitions for a user that considers alternative items are that (I1) their patience critiquing for a single target item may run out after number of turns"
  - [corpus] Weak - related papers discuss user simulators but don't specifically address alternative-based evaluation mechanisms
- Break condition: If the similarity function fails to identify meaningful alternatives, or if users don't actually explore alternatives in real scenarios

### Mechanism 2
- Claim: Meta-user simulator addresses the incompleteness problem in CRS evaluation
- Mechanism: By extending datasets with alternative judgements through crowdsourcing, the simulator creates more complete test collections similar to pooling techniques in information retrieval
- Core assumption: The presence of alternatives in fashion datasets allows for more reliable evaluation similar to how pooling improves document retrieval assessment
- Evidence anchors:
  - [section 2.3] "we argue that it is possible to ask a 3rd party assessor to consider what other items they may have considered"
  - [section 4.2] "We use Amazon Mechanical Turk to gain assessments on alternative items"
  - [corpus] Weak - corpus mentions evaluation but doesn't discuss pooling techniques for alternatives
- Break condition: If assessor agreement is low (kappa < 0.6) or if alternatives don't meaningfully represent user preferences

### Mechanism 3
- Claim: Alternative-based evaluation reveals underestimation of CRS effectiveness in single-target settings
- Mechanism: By allowing simulated users to switch targets when patience runs out, the evaluation shows that CRS models can satisfy users faster than previously measured
- Core assumption: Single-target evaluation artificially constrains user satisfaction by forcing persistence on one item
- Evidence anchors:
  - [abstract] "specifically that the existing single-target evaluation underestimates their effectiveness"
  - [section 5.3] "we observe improved performance on all three evaluation metrics and both Shoes and Dresses"
  - [corpus] Weak - related papers discuss simulator limitations but don't quantify underestimation effects
- Break condition: If improvements are not statistically significant across multiple runs, or if alternative selection frequency is very low

## Foundational Learning

- Concept: Reinforcement Learning in Conversational Recommendation Systems
  - Why needed here: Understanding how CRS models like GRU-RL are trained and evaluated is crucial for interpreting why alternative-based evaluation affects their measured performance
  - Quick check question: How does the reward structure in reinforcement learning differ between short-term and long-term optimization in CRS?

- Concept: Query Performance Prediction in Information Retrieval
  - Why needed here: The paper uses QPP techniques to select diverse target items for assessment, and understanding these techniques helps explain the sampling methodology
  - Quick check question: What score-based predictors are commonly used in QPP to estimate query difficulty?

- Concept: User Simulation Techniques in Interactive Systems
  - Why needed here: The meta-user simulator builds on existing user simulation frameworks, and understanding agenda-based simulation helps explain how the alternative mechanism integrates with existing approaches
- Quick check question: How do push and pull operations work in agenda-based user simulation frameworks?

## Architecture Onboarding

- Component map: Meta-user simulator -> Check patience threshold -> Select alternative -> Base simulator critique -> CRS response
- Critical path: Meta-user simulator → Check patience threshold → Select alternative → Base simulator critique → CRS response
- Design tradeoffs:
  - Tolerance threshold vs. evaluation completeness
  - Similarity function accuracy vs. computational cost
  - Number of alternatives per target vs. assessment effort
  - Real-time evaluation vs. offline analysis capabilities
- Failure signatures:
  - Low alternative selection frequency (suggests single-target assumption is valid)
  - Decreased performance when alternatives are introduced (suggests poor similarity function)
  - High variance in results across tolerance levels (suggests patience parameter instability)
  - Poor assessor agreement (kappa < 0.6)
- First 3 experiments:
  1. Compare MRR@10 performance between single-target and alternative-based evaluation on Shoes dataset with tolerance=2
  2. Measure alternative selection frequency across different tolerance levels (1-4) for all three CRS models
  3. Test sensitivity of results to similarity function by comparing different embedding approaches (cosine vs. Euclidean distance)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of alternative items in user simulators affect the long-term satisfaction of users compared to traditional single-target simulators?
- Basis in paper: [explicit] The paper discusses how the alternative-based simulator allows users to change their minds and consider alternatives, potentially leading to faster satisfaction.
- Why unresolved: While the paper shows improved evaluation metrics, it does not directly measure long-term user satisfaction in real-world scenarios.
- What evidence would resolve it: User studies comparing satisfaction levels in systems using alternative-based versus single-target simulators over extended periods.

### Open Question 2
- Question: What are the optimal patience thresholds for user simulators across different fashion categories, and how do they vary with user demographics?
- Basis in paper: [explicit] The paper experiments with different patience levels but does not explore optimal thresholds or demographic variations.
- Why unresolved: The paper provides a general framework but does not tailor patience settings to specific user groups or fashion categories.
- What evidence would resolve it: A/B testing with diverse user groups to determine the most effective patience settings for each demographic and category.

### Open Question 3
- Question: How does the inclusion of alternative items impact the computational efficiency and scalability of conversational recommendation systems?
- Basis in paper: [inferred] The paper introduces a meta-user simulator that considers alternatives, but does not discuss computational implications.
- Why unresolved: The focus is on evaluation effectiveness, not on the practical aspects of implementing alternative-based systems at scale.
- What evidence would resolve it: Performance benchmarks comparing the computational load of systems using alternative-based simulators versus traditional ones.

## Limitations
- The approach relies heavily on crowdsourced alternative judgments, which may not capture the full diversity of user preferences
- Image similarity may not fully capture semantic or functional similarities that users consider when exploring alternatives
- The method is currently limited to fashion domains where alternatives are visually similar, limiting generalizability to other recommendation contexts
- The meta-user simulator's effectiveness depends on the quality of the underlying base simulator and CRS models

## Confidence

**High Confidence**: The observation that single-target evaluation underestimates CRS effectiveness is well-supported by the experimental results showing consistent improvements across all three CRS models and evaluation metrics when alternatives are considered.

**Medium Confidence**: The mechanism by which alternative-based evaluation captures exploratory user behavior is plausible but not fully validated. While the paper provides intuitive reasoning, there's limited empirical evidence that real users actually explore alternatives in the manner simulated.

**Low Confidence**: The claim that this approach addresses the incompleteness problem in CRS evaluation is weakly supported. The connection between alternative-based evaluation and pooling techniques in information retrieval is asserted but not rigorously demonstrated.

## Next Checks

1. **Cross-domain validation**: Test the alternative-based simulator on non-fashion CRS datasets (e.g., movie or restaurant recommendations) to verify that the approach generalizes beyond visually similar alternatives and captures different types of exploratory behavior.

2. **Real user behavior comparison**: Conduct user studies comparing real user satisfaction and alternative exploration patterns with simulated results to validate whether the meta-user simulator accurately captures actual user behavior when patience thresholds are exceeded.

3. **Ablation study on similarity functions**: Systematically test different alternative selection methods (content-based, collaborative filtering, hybrid approaches) to determine whether image similarity is optimal or if other similarity measures better capture meaningful alternatives that improve CRS evaluation.