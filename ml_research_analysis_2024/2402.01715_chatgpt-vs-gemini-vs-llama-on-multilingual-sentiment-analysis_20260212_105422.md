---
ver: rpa2
title: ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis
arxiv_id: '2402.01715'
source_url: https://arxiv.org/abs/2402.01715
tags:
- sentiment
- language
- chatgpt
- gemini
- scenarios
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of ChatGPT, Gemini, and LLaMA
  models in multilingual sentiment analysis, focusing on ambiguous and ironic scenarios.
  The authors constructed 20 nuanced scenarios translated into 10 languages and compared
  model outputs against human responses.
---

# ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis

## Quick Facts
- **arXiv ID**: 2402.01715
- **Source URL**: https://arxiv.org/abs/2402.01715
- **Reference count**: 40
- **Primary result**: Evaluated ChatGPT 3.5, 4, Gemini Pro, and LLaMA2 on 20 ambiguous/ironic scenarios in 10 languages, revealing language-specific biases and the importance of human validation

## Executive Summary
This study systematically evaluates the performance of major LLM models (ChatGPT 3.5, ChatGPT 4, Gemini Pro, and LLaMA2) in multilingual sentiment analysis, focusing on ambiguous and ironic scenarios. The authors constructed 20 nuanced scenarios translated into 10 languages and compared model outputs against human responses. The results reveal significant biases and inconsistent performance across models and languages, with LLaMA2 showing a consistent positive bias and Gemini's safety filters impacting results. The study provides a standardized methodology for evaluating automated sentiment analysis and emphasizes the critical role of post-hoc human validation in assessing model reliability.

## Method Summary
The study evaluated four LLMs (ChatGPT 3.5, ChatGPT 4, Gemini Pro, and LLaMA2 7b) on 20 carefully crafted ambiguous scenarios translated into 10 languages. Each model processed all scenarios in all languages using default settings, with multiple iterations per scenario/language to account for variability. Human respondents from each language group rated the same scenarios via questionnaires, providing ground truth validation. The authors analyzed mean ratings, normalized mean rate scores per language, and compared model outputs against human responses while also examining Gemini Pro's censorship behavior.

## Key Results
- ChatGPT and Gemini generally performed well but showed biases and inconsistencies across languages and models
- LLaMA2 exhibited a consistent positive bias, scoring all scenarios positively regardless of context
- Gemini's safety filters introduced censorship biases that skewed sentiment analysis results
- Human validation revealed significant discrepancies between model interpretations and human sentiment understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual sentiment analysis using LLMs can reveal language-specific biases in model outputs
- Mechanism: By translating ambiguous scenarios into multiple languages and analyzing LLM outputs across languages, systematic variations in sentiment scores are exposed
- Core assumption: LLMs trained on multilingual data process sentiment consistently across languages without hidden biases
- Evidence anchors:
  - [abstract] "significant biases and inconsistent performance across models and evaluated human languages"
  - [section] "the choice of a specific language has an impact on the evaluation provided by the models"
  - [corpus] "Average neighbor FMR=0.459" (indicates moderate similarity in multilingual focus)
- Break condition: If bias patterns are not statistically significant or vary randomly across languages, the mechanism fails

### Mechanism 2
- Claim: Post-hoc human validation is essential for interpreting LLM sentiment scores in ambiguous scenarios
- Mechanism: Human respondents rate the same ambiguous scenarios, providing ground truth against which LLM outputs are compared
- Core assumption: Human sentiment interpretation is more reliable than automated models in nuanced, ambiguous cases
- Evidence anchors:
  - [abstract] "results are validated against post-hoc human responses"
  - [section] "By means of questionnaires, submitted to native speakers... we obtained sentiment profiles"
  - [corpus] "Zero-shot Sentiment Analysis in Low-Resource Languages Using a Multilingual Sentiment Lexicon" (suggests human-in-the-loop approaches)
- Break condition: If human responses are too inconsistent or biased themselves, validation becomes unreliable

### Mechanism 3
- Claim: Default safety filters in LLMs can introduce censorship biases that skew sentiment analysis results
- Mechanism: Safety filters may block certain language-specific content, leading to incomplete or biased sentiment assessments
- Core assumption: Safety filters operate consistently across languages and scenarios
- Evidence anchors:
  - [section] "Gemini Pro is equipped with safety filters... If specific keywords are detected, the model responds by generating an error message"
  - [section] "it does not consistently employ them for identical language and scenarios"
  - [corpus] "English Please: Evaluating Machine Translation with Large Language Models for Multilingual Bug Reports" (suggests language-dependent behavior)
- Break condition: If safety filter behavior is random or inconsistent across languages, bias analysis becomes unreliable

## Foundational Learning

- **Concept**: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how LLMs process language is crucial for interpreting their sentiment analysis capabilities
  - Quick check question: How does the multi-head self-attention mechanism in transformers enable contextual understanding of sentiment?

- **Concept**: Cross-lingual transfer learning
  - Why needed here: Models are trained on multiple languages, and understanding transfer learning helps explain performance variations
  - Quick check question: What factors influence how well a model trained on one language performs on another language?

- **Concept**: Sarcasm and irony detection in NLP
  - Why needed here: The study focuses on ambiguous scenarios where sarcasm and irony are key challenges
  - Quick check question: Why are sarcasm and irony particularly difficult for automated sentiment analysis systems?

## Architecture Onboarding

- **Component map**: Scenario creation -> Translation pipeline (10 languages) -> LLM API integration (ChatGPT 3.5, 4, Gemini Pro, LLaMA2) -> Human questionnaire system -> Statistical analysis module -> Visualization and reporting tools

- **Critical path**: 1. Create ambiguous scenarios 2. Translate scenarios into 10 languages 3. Run LLMs on all scenarios in all languages 4. Collect human responses via questionnaires 5. Perform statistical analysis comparing LLM and human results 6. Generate visualizations and reports

- **Design tradeoffs**: Using default LLM settings vs. tuning for specific tasks; Automated vs. manual translation for scenario localization; Sample size of human respondents vs. statistical power; Number of scenarios vs. depth of ambiguity testing

- **Failure signatures**: LLM outputs are highly inconsistent across languages; Human responses are too scattered to provide reliable validation; Safety filters block too many responses, skewing results; Statistical analysis shows no significant differences or patterns

- **First 3 experiments**: 1. Run a single scenario through all four LLMs in one language, compare outputs 2. Translate one scenario into all 10 languages, run through one LLM, analyze cross-lingual variation 3. Pilot questionnaire with 10 respondents to test human response variability and refine survey design

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different training data distributions and proportions on the sentiment analysis performance of LLMs across various languages?
- Basis in paper: [explicit] The paper highlights that "noteworthy differences exist when evaluating various human languages and different ChatGPT versions" and that "Gemini shows significant rating difference across languages."
- Why unresolved: The study does not investigate the specific reasons behind the observed language-dependent biases in LLM performance
- What evidence would resolve it: Comparative analysis of training data distributions and proportions used for different languages in LLM development

### Open Question 2
- Question: How do linguistic and cultural nuances influence the sentiment analysis performance of LLMs, and can these be mitigated through improved training methodologies?
- Basis in paper: [explicit] The authors note that "language is intrinsically linked to culture, and the expression of sentiment often carries cultural nuances" and that their multilingual approach "allows us to explore how well the LLMs can adapt to and account for these cultural variations in sentiment interpretation."
- Why unresolved: The study does not delve into the specific cultural factors that may contribute to LLM biases and does not propose solutions for mitigating these biases
- What evidence would resolve it: Investigation of cultural factors influencing sentiment expression and development of training methodologies that incorporate cultural context

### Open Question 3
- Question: What are the underlying reasons for the observed "optimistic bias" in LLaMA2's sentiment analysis predictions, and how can this bias be addressed?
- Basis in paper: [explicit] The authors observe that LLaMA2 "repeatedly scores all scenarios as positive" and suggest that "this consistency in LLaMA2's performance largely stems from its tendency to generate consistently positive ratings."
- Why unresolved: The study does not investigate the specific reasons behind LLaMA2's positive bias and does not propose solutions for mitigating this bias
- What evidence would resolve it: Analysis of LLaMA2's training data and methodologies to identify potential sources of positive bias and development of strategies to address this bias

## Limitations
- Human response variability may introduce uncertainty in ground truth validation
- Fixed set of 20 scenarios may not capture full spectrum of multilingual sentiment nuances
- Study does not account for potential temporal variations in model behavior

## Confidence
- **High Confidence**: Identification of language-specific biases in model outputs is well-supported by systematic cross-lingual testing
- **Medium Confidence**: Gemini Pro's safety filters introducing bias is moderately supported but needs more granular analysis
- **Low Confidence**: Human validation being inherently more reliable than automated sentiment analysis needs stronger empirical support

## Next Checks
1. **Replication with Expanded Scenario Set**: Test the same methodology with 50-100 scenarios across the same 10 languages to verify if observed biases persist with larger sample sizes

2. **Temporal Consistency Analysis**: Run the complete evaluation pipeline at three-month intervals to assess model stability and detect systematic drift in sentiment analysis performance

3. **Cross-Cultural Validation Study**: Recruit human respondents from multiple cultural backgrounds within each language group to test whether cultural factors influence sentiment interpretation patterns beyond linguistic ones