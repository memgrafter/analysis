---
ver: rpa2
title: 'ESQA: Event Sequences Question Answering'
arxiv_id: '2407.12833'
source_url: https://arxiv.org/abs/2407.12833
tags:
- event
- esqa
- sequences
- tasks
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ESQA, a novel approach for adapting large\
  \ language models (LLMs) to event sequence data, a common format in domains like\
  \ finance, healthcare, and retail. ESQA addresses challenges in encoding structured\
  \ data, processing long sequences, and handling time and numeric features by introducing\
  \ trainable embeddings for event features, an event sequence encoder based on a\
  \ transformer decoder, and a connector layer using Q-Former to align event representations\
  \ with the LLM\u2019s embedding space."
---

# ESQA: Event Sequences Question Answering

## Quick Facts
- arXiv ID: 2407.12833
- Source URL: https://arxiv.org/abs/2407.12833
- Authors: Irina Abdullaeva; Andrei Filatov; Mikhail Orlov; Ivan Karpukhin; Viacheslav Vasilev; Denis Dimitrov; Andrey Kuznetsov; Ivan Kireev; Andrey Savchenko
- Reference count: 20
- One-line primary result: ESQA achieves state-of-the-art results on several event sequence tasks, including next-event prediction and multi-class classification.

## Executive Summary
This paper introduces ESQA, a novel approach for adapting large language models (LLMs) to event sequence data, a common format in domains like finance, healthcare, and retail. ESQA addresses challenges in encoding structured data, processing long sequences, and handling time and numeric features by introducing trainable embeddings for event features, an event sequence encoder based on a transformer decoder, and a connector layer using Q-Former to align event representations with the LLM's embedding space. Experiments on five real-world datasets show that ESQA achieves state-of-the-art results on several tasks, including next-event prediction and multi-class classification, and demonstrates strong generalization capabilities in zero-shot and few-shot settings.

## Method Summary
ESQA (Event Sequences Question Answering) adapts LLMs to handle event sequence data by introducing trainable embeddings for each event feature, avoiding text tokenization overhead. An event sequence tensor is processed autoregressively by a transformer decoder (e.g., Whisper-small), and a Q-Former connector layer extracts salient representations via cross-attention and projects them to the LLM's embedding space. This architecture enables effective modeling of temporal dependencies and structured event features, allowing ESQA to outperform baseline methods on downstream tasks such as next-event prediction and multi-class classification.

## Key Results
- ESQA achieves state-of-the-art results on next-event prediction and multi-class classification tasks across five real-world datasets.
- Demonstrates strong generalization in zero-shot and few-shot settings, particularly on predicting categorical attributes and temporal patterns.
- Outperforms baseline methods on imbalanced classification tasks, though performance is sensitive to class distribution.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ESQA improves event sequence processing by replacing text-based embeddings with trainable embeddings for each event feature, reducing sequence length and preserving structure.
- Mechanism: Each event feature value is encoded as a trainable embedding vector, concatenated into a single tensor per event, then stacked into a sequence tensor for the encoder. This avoids tokenization overhead and quadratic complexity.
- Core assumption: Direct embedding injection into the LLM embedding space maintains semantic richness while being computationally efficient.
- Evidence anchors:
  - [abstract] "ESQA addresses challenges in encoding structured data, processing long sequences, and handling time and numeric features by introducing trainable embeddings for event features..."
  - [section] "ESQA represents all event features as trainable embeddings... This index uniquely identifies the embedding embk of a feature value in the embedding matrix We."
- Break condition: If the embedding dimension grows too large for high-cardinality categorical features, memory usage or training instability could degrade performance.

### Mechanism 2
- Claim: ESQA leverages a transformer decoder as an event sequence encoder to capture autoregressive temporal dependencies, improving prediction of future events.
- Mechanism: The sequence tensor of event embeddings is processed autoregressively by a transformer decoder (e.g., Whisper-small), producing contextualized event representations suitable for downstream tasks.
- Core assumption: Autoregressive modeling aligns with the temporal nature of event sequences and is effective for next-event prediction.
- Evidence anchors:
  - [abstract] "...processing long sequences... We solve the problem of working with long sequences..."
  - [section] "After the initial layer of input data embeddings, vectorized event sequences are fed into a specialized encoder model... This module, based on the architecture of the Transformer decoder, processes sequences of events in an autoregressive manner..."
- Break condition: If the sequence length exceeds the model's context window, or if temporal dependencies are too sparse, autoregressive modeling may fail to capture useful patterns.

### Mechanism 3
- Claim: ESQA uses a Q-Former connector to extract salient event sequence representations and align them with the LLM embedding space, enabling efficient multimodal fusion.
- Mechanism: Q-Former takes a fixed set of query embeddings, performs cross-attention with the event encoder output, and produces query vectors projected to the LLM's embedding dimension, which are appended to the question tokens.
- Core assumption: Q-Former's cross-attention can effectively summarize variable-length event sequences into fixed-size representations without significant information loss.
- Evidence anchors:
  - [abstract] "...a connector layer using Q-Former to align event representations with the LLM's embedding space."
  - [section] "To achieve this, we propose an intermediate connection layer between the event sequence encoder and the LLM. We suggest using the Query Transformer model, or Q-Former..."
- Break condition: If the number of queries is insufficient to capture sequence complexity, or if the projection layer is poorly tuned, the alignment may be inadequate.

## Foundational Learning

- Concept: Embedding injection for structured data
  - Why needed here: Traditional text tokenization inflates sequence length for event features; embedding injection preserves structure and reduces computation.
  - Quick check question: What is the main advantage of encoding event feature values directly as embeddings instead of as text tokens?

- Concept: Transformer decoder for autoregressive modeling
  - Why needed here: Event sequences are ordered temporally; an autoregressive decoder naturally models dependencies between past and future events.
  - Quick check question: Why is a decoder-only transformer architecture more suitable for event sequence encoding than an encoder-only or encoder-decoder architecture?

- Concept: Cross-attention in multimodal fusion
  - Why needed here: Q-Former uses cross-attention to summarize variable-length event sequence representations into fixed-size vectors compatible with LLM embeddings.
  - Quick check question: How does Q-Former's cross-attention mechanism help align event sequence representations with the LLM's embedding space?

## Architecture Onboarding

- Component map: Event features → trainable embeddings → event sequence tensor → transformer decoder encoder → Q-Former connector → LLM backbone → question-answering output
- Critical path: Event embedding generation → sequence encoding → connector projection → LLM inference
- Design tradeoffs:
  - Direct embeddings vs. text tokenization: faster, shorter sequences vs. potential loss of interpretability
  - Decoder-only vs. encoder-only: better for autoregressive tasks vs. less bidirectional context
  - Q-Former vs. linear projection: richer summarization vs. fewer parameters
- Failure signatures:
  - Exploding or vanishing gradients in embedding matrix if cardinality is too high
  - Poor sequence representation if Q-Former queries don't capture salient patterns
  - LLM misalignment if connector projection dimension mismatches
- First 3 experiments:
  1. Compare trainable embeddings vs. text tokenization on sequence length and task accuracy
  2. Swap Whisper-tiny vs. Whisper-small vs. GPT2-base as encoder and measure downstream performance
  3. Replace Q-Former with a simple linear projection layer and observe impact on classification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of discretization intervals for numerical features impact the performance of ESQA across different datasets?
- Basis in paper: [explicit] The paper mentions using Doane's formula to determine the number of intervals for discretization and states that the number of intervals significantly influences discretization error.
- Why unresolved: The paper does not provide a systematic analysis of how varying the number of intervals affects model performance. It only mentions that coefficients λ = 1.6 and µ = 0.56 were chosen empirically without detailing the sensitivity of the model to these parameters.
- What evidence would resolve it: Conducting experiments that vary the number of discretization intervals and measuring the impact on model performance metrics such as AUC-ROC and accuracy would provide insights into the optimal number of intervals for different types of numerical features.

### Open Question 2
- Question: To what extent does the imbalance in class distribution affect ESQA's performance, and what strategies could mitigate this issue?
- Basis in paper: [explicit] The paper notes that ESQA is significantly affected by the imbalance of the target variable, particularly in the AlfaBattle and Gender datasets, and attributes this to the nature of LLMs.
- Why unresolved: The paper does not explore specific strategies or modifications to the ESQA architecture or training process that could address class imbalance. It only identifies the problem without proposing solutions.
- What evidence would resolve it: Implementing and testing techniques such as class weighting, oversampling, or using different loss functions during training, followed by evaluating their effectiveness in improving ESQA's performance on imbalanced datasets.

### Open Question 3
- Question: How does ESQA's performance in zero-shot and few-shot settings compare to models specifically fine-tuned for each task?
- Basis in paper: [explicit] The paper highlights ESQA's generalization capabilities in zero-shot and few-shot settings but does not provide a direct comparison with models that are specifically fine-tuned for each task.
- Why unresolved: While the paper demonstrates ESQA's versatility, it does not benchmark its performance against task-specific models that have undergone extensive fine-tuning, leaving a gap in understanding its relative effectiveness.
- What evidence would resolve it: Conducting experiments that compare ESQA's performance in zero-shot and few-shot settings with models that are fine-tuned on the same tasks, using metrics such as accuracy and F1-score, would clarify the trade-offs between generalization and task-specific optimization.

## Limitations
- Limited ablation studies to isolate contribution of each architectural component.
- No detailed error analysis for failure cases, especially with imbalanced classes or high-precision numerical features.
- Scalability analysis for extremely long sequences or high-cardinality categorical features is limited.

## Confidence
- Mechanism 1 (Trainable embeddings for event features): Medium confidence
- Mechanism 2 (Transformer decoder for autoregressive modeling): High confidence
- Mechanism 3 (Q-Former connector for multimodal fusion): Medium confidence
- Overall performance claims: Medium confidence

## Next Checks
1. Conduct ablation study on architectural components by disabling each component (trainable embeddings, transformer decoder, Q-Former connector) and measuring impact on performance across all five datasets.
2. Generate synthetic event sequences with controlled properties (varying sequence lengths up to 10,000 events, high-cardinality categorical features with 10,000+ categories, and numerical features with high precision) to test claimed scalability limitations.
3. Implement and evaluate simpler alternatives to Q-Former (e.g., mean pooling, attention-based pooling with learned weights) to determine whether added complexity provides meaningful performance gains relative to computational cost.