---
ver: rpa2
title: 'PGD-Imp: Rethinking and Unleashing Potential of Classic PGD with Dual Strategies
  for Imperceptible Adversarial Attacks'
arxiv_id: '2412.11168'
source_url: https://arxiv.org/abs/2412.11168
tags:
- adversarial
- attacks
- attack
- pgd-imp
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper rethinks the essence of imperceptible adversarial attacks
  and proposes a simple yet effective method, PGD-Imp, which achieves state-of-the-art
  results. The key insight is that imperceptible attacks should focus on minimizing
  the attack cost to cross the decision boundary, rather than relying on complex external
  modules.
---

# PGD-Imp: Rethinking and Unleashing Potential of Classic PGD with Dual Strategies for Imperceptible Adversarial Attacks

## Quick Facts
- arXiv ID: 2412.11168
- Source URL: https://arxiv.org/abs/2412.11168
- Authors: Jin Li; Zitong Yu; Ziqiang He; Z. Jane Wang; Xiangui Kang
- Reference count: 38
- Primary result: Achieves 100% attack success rate with 0.89 l2 distance, 52.93 PSNR, and 0.9988 SSIM against ResNet-50 on ImageNet

## Executive Summary
This paper rethinks imperceptible adversarial attacks by focusing on minimizing the attack cost to cross the decision boundary rather than relying on complex external modules. The authors propose PGD-Imp, a simple yet effective method that introduces two strategies: Dynamic Step Size to allocate perturbation budget more effectively across iterations, and Adaptive Early Stop to halt the attack as soon as the adversarial example successfully misleads the classifier. Experiments on ImageNet show that PGD-Imp significantly outperforms existing methods, achieving state-of-the-art results while reducing running time by 371 seconds.

## Method Summary
The PGD-Imp method builds upon the classic Projected Gradient Descent (PGD) attack by introducing two key strategies: Dynamic Step Size and Adaptive Early Stop. The Dynamic Step Size allocates a varying step size (ηt · β) where ηt follows a linear schedule from 0 to 1, allowing the attack to start with smaller perturbations and gradually increase them. The Adaptive Early Stop strategy monitors whether the current adversarial example has successfully fooled the classifier and halts the optimization process immediately if successful. Together, these strategies minimize the total perturbation cost more effectively than traditional PGD approaches.

## Key Results
- Achieves 100% attack success rate against ResNet-50 on ImageNet
- Maintains 0.89 l2 distance while achieving 52.93 PSNR and 0.9988 SSIM
- Reduces running time by 371 seconds compared to previous methods
- Outperforms existing state-of-the-art imperceptible attack methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic step size allocation based on linear coefficient sequence reduces the total perturbation cost needed to cross the decision boundary.
- Mechanism: Instead of using a fixed step size in each iteration, the method allocates a varying step size (ηt · β) where ηt follows a linear schedule from 0 to 1. This allows the attack to start with smaller perturbations and gradually increase them, effectively "fine-tuning" the trajectory toward the decision boundary with minimal total cost.
- Core assumption: A linear increase in step size from 0 to 1 is optimal for balancing exploration and exploitation in the adversarial space.
- Evidence anchors: [abstract] "Rather than relying on externally hiding or constraining adversarial perturbations, we argue that the essence of imperceptible adversarial attacks lies in pushing the image across the decision boundary of the attacked model with minimal cost." [section] "Specifically, the Dynamic Step Size is introduced to find the optimal solution with minimal attack cost towards the decision boundary of the attacked model"
- Break condition: If the linear schedule is suboptimal for a particular model or dataset, the attack may require more iterations or fail to achieve the minimal cost.

### Mechanism 2
- Claim: Adaptive early stopping prevents overshooting the decision boundary, keeping perturbations minimal and imperceptibility high.
- Mechanism: The attack monitors whether the current adversarial example has successfully fooled the classifier. If successful, the optimization process halts immediately in later iterations. This prevents the adversarial example from crossing the decision boundary too far, which would require unnecessary additional perturbations.
- Core assumption: Adversarial examples that barely cross the decision boundary are inherently more imperceptible than those that cross it by a larger margin.
- Evidence anchors: [abstract] "the Adaptive Early Stop strategy is adopted to reduce the redundant strength of adversarial perturbations to the minimum level" [section] "the Adaptive Early Stop strategy assesses whether the current result has successfully attacked the classifier at each step in the later iterations"
- Break condition: If the stopping criterion is too aggressive, the attack may terminate before reaching the decision boundary, resulting in failure.

### Mechanism 3
- Claim: The combination of dynamic step size and adaptive early stopping creates a synergistic effect that outperforms either strategy alone.
- Mechanism: Dynamic step size refines the optimization trajectory toward the decision boundary, while adaptive early stopping ensures the adversarial example stops near the boundary without overshooting. Together, they minimize the total perturbation cost more effectively than either strategy could achieve independently.
- Core assumption: The two strategies are complementary and their combined effect is greater than the sum of their individual effects.
- Evidence anchors: [section] "Working synergistically with the dynamic step sizes, the attack with Adaptive Early Stop starts with the refined optimization process in the early iterations to confirm the direction of optimization, then progresses along this direction and finally stops near the decision boundary" [section] "the final PGD-Imp achieves a more significant improvement benefiting from the cooperation and mutually reinforcing effects of the proposed DSS and AES strategies"
- Break condition: If the strategies conflict (e.g., dynamic step size pushes too far while early stopping tries to stop too soon), the combined effect may be suboptimal.

## Foundational Learning

- Concept: Projected Gradient Descent (PGD) attack
  - Why needed here: The paper builds upon PGD as the base attack algorithm, so understanding its mechanics is essential for implementing and extending the proposed improvements.
  - Quick check question: How does PGD ensure that adversarial perturbations stay within the allowed l∞ norm constraint?

- Concept: Learning rate schedules in optimization
  - Why needed here: The dynamic step size strategy draws inspiration from learning rate schedules used in training neural networks, so familiarity with these concepts is necessary to understand the rationale behind the design choice.
  - Quick check question: What is the difference between a constant learning rate and a schedule that decreases over time?

- Concept: Early stopping in machine learning
  - Why needed here: The adaptive early stop strategy is analogous to early stopping in model training, so understanding this concept helps in grasping the intuition behind the approach.
  - Quick check question: How does early stopping prevent overfitting in model training?

## Architecture Onboarding

- Component map: Input -> Dynamic Step Size Allocation -> PGD Update -> Adaptive Early Stop Check -> Output
- Critical path: 
  1. Initialize x1 = x (no random initialization)
  2. For each iteration t:
     a. Compute dynamic step size αt = ηt · β
     b. Update xt+1 using gradient sign and dynamic step size
     c. Round xt+1 to get xnow
     d. Check if xnow successfully attacks the model
     e. If successful and in later iterations, break and return xnow
  3. Return final adversarial example
- Design tradeoffs:
  - Using a linear schedule for ηt vs. other schedules (cosine, constant)
  - Starting with no random initialization vs. adding noise to escape local minima
  - Checking attack success only in later iterations vs. every iteration
- Failure signatures:
  - Attack success rate (ASR) significantly lower than expected
  - l2 distance or PSNR values worse than baseline methods
  - Adversarial examples still perceptible despite claims of imperceptibility
- First 3 experiments:
  1. Implement basic PGD attack with fixed step size and compare to proposed method on a small dataset
  2. Test different schedules for ηt (linear, cosine, constant) to verify linear schedule performs best
  3. Evaluate the effect of checking attack success in every iteration vs. only later iterations

## Open Questions the Paper Calls Out

- Question: How does the Dynamic Step Size strategy in PGD-Imp compare to other advanced optimization techniques (e.g., Nesterov accelerated gradient) in terms of attack success rate and imperceptibility?
- Basis in paper: [inferred] The paper mentions that the Dynamic Step Size strategy is inspired by learning rate schedules in optimization, but does not compare it to other advanced optimization techniques.
- Why unresolved: The paper focuses on comparing PGD-Imp with other imperceptible attack methods, but does not explore how its optimization strategy compares to other advanced techniques.
- What evidence would resolve it: Experimental results comparing PGD-Imp with other optimization-based attack methods using different learning rate schedules or advanced optimization techniques.

- Question: What is the impact of the Adaptive Early Stop strategy on the transferability of adversarial examples generated by PGD-Imp?
- Basis in paper: [inferred] The paper mentions that the Adaptive Early Stop strategy prevents overfitting to the decision boundary, but does not discuss its impact on transferability.
- Why unresolved: The paper focuses on the effectiveness of PGD-Imp in terms of imperceptibility and attack success rate, but does not explore how the Adaptive Early Stop strategy affects the transferability of adversarial examples.
- What evidence would resolve it: Experimental results evaluating the transferability of adversarial examples generated by PGD-Imp with and without the Adaptive Early Stop strategy.

- Question: How does the performance of PGD-Imp vary across different datasets and attack scenarios (e.g., physical-world attacks)?
- Basis in paper: [inferred] The paper evaluates PGD-Imp on the ImageNet-compatible dataset and discusses its performance in both untargeted and targeted attacks, but does not explore its performance on other datasets or attack scenarios.
- Why unresolved: The paper focuses on the performance of PGD-Imp on a specific dataset and attack scenarios, but does not explore its generalization to other datasets or attack scenarios.
- What evidence would resolve it: Experimental results evaluating the performance of PGD-Imp on different datasets and attack scenarios, such as physical-world attacks or attacks on different types of models.

## Limitations
- The optimal linear schedule for Dynamic Step Size lacks direct comparison to alternative schedules (cosine, exponential, adaptive)
- The Adaptive Early Stop threshold for "later iterations" is not rigorously justified
- The synergistic effect claim between DSS and AES lacks ablation studies showing individual contributions

## Confidence

- **High**: Basic algorithm implementation and ImageNet experiment setup
- **Medium**: Attack success rates and imperceptibility metrics compared to baselines
- **Low**: Specific claims about optimal linear schedule and synergistic effects

## Next Checks

1. Conduct ablation studies comparing linear step-size schedule against cosine and exponential alternatives across multiple datasets and models
2. Systematically vary the early-stop threshold (iterations when checking begins) to identify optimal values for different model architectures
3. Test the combined DSS+AES approach against their individual implementations on the same threat model to verify claimed synergistic improvements