---
ver: rpa2
title: 'Divide and not forget: Ensemble of selectively trained experts in Continual
  Learning'
arxiv_id: '2401.10191'
source_url: https://arxiv.org/abs/2401.10191
tags:
- seed
- task
- experts
- learning
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses class-incremental learning (CIL) without exemplars,
  where a model must learn new classes over time while retaining knowledge of previously
  seen classes. The key challenge is catastrophic forgetting, especially when the
  first task has limited data, making a strong feature extractor hard to obtain.
---

# Divide and not forget: Ensemble of selectively trained experts in Continual Learning

## Quick Facts
- arXiv ID: 2401.10191
- Source URL: https://arxiv.org/abs/2401.10191
- Reference count: 19
- Primary result: SEED outperforms state-of-the-art CIL methods, achieving up to 14.7 percentage points higher average incremental accuracy without exemplars

## Executive Summary
This paper addresses the challenge of catastrophic forgetting in class-incremental learning (CIL) without exemplars, where models must learn new classes over time while retaining knowledge of previously seen classes. The authors propose SEED, an ensemble method that selectively trains a single expert per new task, using data from that task to fine-tune only that chosen expert. The optimal expert is selected based on the overlap of Gaussian class distributions in each expert's latent space, which encourages diversity among experts while mitigating forgetting.

SEED demonstrates superior performance across multiple datasets and scenarios, achieving significant improvements over state-of-the-art CIL methods. The approach requires no extra computation during training and shows strong stability-plasticity trade-offs. The method can also be adapted for task-aware settings with fewer parameters, making it a versatile solution for continual learning challenges.

## Method Summary
SEED addresses catastrophic forgetting in class-incremental learning by maintaining an ensemble of selectively trained experts. For each new task, the method evaluates all available experts and selects the one whose latent space shows minimal overlap with the new task's data distribution, as measured by Gaussian distribution overlap. Only the selected expert is fine-tuned on the new task data, while others remain frozen. This selective approach prevents interference between tasks and maintains diversity among experts. The method requires no additional computation during training and can be adapted for both task-agnostic and task-aware scenarios.

## Key Results
- SEED achieves up to 14.7 percentage points higher average incremental accuracy compared to state-of-the-art CIL methods
- The method demonstrates superior stability-plasticity trade-offs across multiple datasets
- SEED maintains strong performance without requiring exemplar storage, addressing a key limitation of many CIL approaches

## Why This Works (Mechanism)
The effectiveness of SEED stems from its selective training approach that maintains diversity among experts while preventing catastrophic forgetting. By choosing the expert with minimal Gaussian distribution overlap for each new task, the method ensures that experts develop complementary capabilities rather than overlapping knowledge. This prevents interference between tasks and allows the ensemble to collectively cover the full task space. The Gaussian distribution overlap criterion provides a principled way to measure task similarity and guide expert selection, while the selective fine-tuning approach ensures that each expert specializes in specific task regions without being disrupted by unrelated data.

## Foundational Learning
**Catastrophic Forgetting**: The tendency of neural networks to rapidly lose previously learned information when trained on new tasks. This is the core problem SEED addresses in continual learning scenarios.

**Class-Incremental Learning (CIL)**: A continual learning setting where new classes are introduced over time while maintaining performance on previously learned classes. SEED operates in the more challenging CIL-without-exemplars setting.

**Gaussian Distribution Overlap**: A metric used to measure similarity between class distributions in latent space. SEED uses this to determine which expert is most appropriate for fine-tuning on new tasks, ensuring minimal interference with existing knowledge.

**Ensemble Methods**: Approaches that combine multiple models to achieve better performance than individual models. SEED uses an ensemble of experts, each specializing in different task regions.

**Latent Space**: The compressed representation space where data is embedded. SEED analyzes class distributions in this space to make decisions about expert selection and specialization.

## Architecture Onboarding

**Component Map**: Input Data -> Feature Extractor -> Multiple Experts -> Gaussian Distribution Analysis -> Expert Selection -> Selective Fine-tuning

**Critical Path**: The key sequence involves analyzing Gaussian distribution overlap between experts' latent spaces and new task data, selecting the optimal expert, and performing selective fine-tuning on only that expert.

**Design Tradeoffs**: SEED trades increased model parameters (multiple experts) for improved continual learning performance and reduced catastrophic forgetting. The selective training approach minimizes computational overhead during training while maintaining ensemble benefits.

**Failure Signatures**: Poor expert selection due to non-Gaussian class distributions could lead to interference between tasks. Limited diversity among experts might result in inadequate coverage of the task space.

**First Experiments**:
1. Evaluate SEED on a simple sequence of classification tasks with clearly separable class distributions
2. Test the Gaussian distribution overlap criterion on synthetic data with known distribution properties
3. Compare selective fine-tuning versus full ensemble fine-tuning on a small benchmark dataset

## Open Questions the Paper Calls Out
None

## Limitations
- The Gaussian distribution overlap selection criterion may not generalize well to non-Gaussian or multimodal class distributions
- The method's performance on non-image datasets remains unexplored, limiting generalizability claims
- Computational overhead of maintaining multiple experts could be prohibitive in resource-constrained deployment scenarios

## Confidence
**High Confidence**: Claims regarding SEED's performance improvements over state-of-the-art methods are well-supported by experimental results across multiple datasets.

**Medium Confidence**: The theoretical justification for Gaussian distribution overlap selection shows promise but requires further validation across diverse data distributions.

**Low Confidence**: Adaptability claims for task-aware settings with fewer parameters need more extensive validation across different task configurations.

## Next Checks
1. Test SEED's performance on non-image datasets (e.g., text, time series, or graph data) to validate general applicability
2. Evaluate the method's robustness to domain shifts and non-Gaussian class distributions to verify the Gaussian overlap criterion's reliability
3. Assess computational overhead and memory requirements for maintaining multiple experts in resource-constrained deployment scenarios