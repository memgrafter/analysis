---
ver: rpa2
title: 'Pragmatic Feature Preferences: Learning Reward-Relevant Preferences from Human
  Input'
arxiv_id: '2405.14769'
source_url: https://arxiv.org/abs/2405.14769
tags:
- reward
- preferences
- learning
- features
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces pragmatic feature preferences, a method for
  learning reward functions from human preference data by leveraging both example-level
  and feature-level comparisons. The approach enriches standard preference queries
  by asking users to identify which features drive their preferences, enabling more
  efficient learning of accurate reward models.
---

# Pragmatic Feature Preferences: Learning Reward-Relevant Preferences from Human Input

## Quick Facts
- arXiv ID: 2405.14769
- Source URL: https://arxiv.org/abs/2405.14769
- Reference count: 13
- Key outcome: Pragmatic feature preferences method significantly outperforms traditional RLHF baselines in learning reward functions from human preference data by leveraging both example-level and feature-level comparisons with pragmatic data augmentation.

## Executive Summary
This paper introduces pragmatic feature preferences, a method for learning reward functions from human preference data by leveraging both example-level and feature-level comparisons. The approach enriches standard preference queries by asking users to identify which features drive their preferences, enabling more efficient learning of accurate reward models. The key innovation is pragmatic data augmentation, which synthesizes new preference data by swapping non-relevant features based on user descriptions. Evaluated in vision-based (mushroom foraging) and language-based (flight booking) domains, the method significantly outperforms traditional reinforcement learning from human feedback (RLHF) baselines, especially with sparse reward functions. A user study confirms that pragmatic feature preferences do not significantly increase user effort while improving learning efficiency.

## Method Summary
The method combines traditional pairwise preference learning with feature-level preference queries and pragmatic data augmentation. Users provide pairwise preferences over actions and identify which features drive their preferences through linguistic descriptions. The model learns a reward function using a joint loss that combines RLHF loss with feature-level loss. Pragmatic data augmentation synthesizes new preference data by swapping non-relevant features identified in user descriptions. The approach is evaluated in linear contextual bandit settings with vision-based and language-based domains, using probability of the ground truth best example as the primary metric.

## Key Results
- Pragmatic feature preferences significantly outperforms RLHF baselines in sparse reward environments
- The method maintains comparable user effort while improving learning efficiency
- Pragmatic data augmentation effectively enriches the preference dataset for better reward model learning

## Why This Works (Mechanism)
The approach works by leveraging the principle of pragmatic communication - humans naturally provide informative feedback that is relevant to the decision context. By asking users to identify which features drive their preferences and synthesizing new preference data based on these descriptions, the method captures richer information about reward-relevant features than standard pairwise preferences alone. This enables more efficient learning of accurate reward models, particularly in sparse reward environments where traditional methods struggle.

## Foundational Learning
- **Linear contextual bandits**: Why needed - forms the theoretical foundation for the preference learning setting; Quick check - verify understanding of the bandit problem formulation
- **Reward function learning from preferences**: Why needed - core problem being addressed; Quick check - understand how pairwise preferences can be used to infer reward functions
- **Pragmatic communication theory**: Why needed - theoretical basis for why users provide informative feature-level feedback; Quick check - understand how pragmatic principles apply to preference elicitation
- **Data augmentation techniques**: Why needed - key innovation for enriching preference datasets; Quick check - understand how feature swapping creates synthetic preferences
- **Joint loss optimization**: Why needed - method for combining different types of preference information; Quick check - understand the mathematical formulation of the combined loss
- **Grounding linguistic descriptions**: Why needed - critical for converting user utterances to structured features; Quick check - understand the challenges of mapping natural language to feature representations

## Architecture Onboarding

**Component Map:**
User Interface -> Preference Collection -> Feature Identification -> Linguistic Processing -> Pragmatic Data Augmentation -> Reward Model Training -> Evaluation

**Critical Path:**
User provides pairwise preferences → User identifies reward-relevant features → Linguistic descriptions processed → Pragmatic data augmentation generates synthetic preferences → Joint loss optimizes reward model → Model evaluated on ground truth best example probability

**Design Tradeoffs:**
- Granularity of feature-level queries vs. user cognitive burden
- Accuracy of linguistic grounding vs. complexity of processing
- Amount of synthetic data vs. risk of introducing noise
- Joint loss weighting vs. stability of optimization

**Failure Signatures:**
- Poor performance on sparse rewards indicates inadequate feature-level preference queries
- Significant user effort increase suggests overly complex feature identification process
- Ground truth best example probability plateauing indicates reward model convergence issues
- Large variance in results suggests instability in pragmatic data augmentation

**First Experiments:**
1. Implement the linear contextual bandit environment with specified reward functions and feature representations
2. Collect or simulate pairwise preference data, feature-level preferences, and linguistic descriptions for training
3. Train reward models using the joint loss and pragmatic data augmentation, evaluating with the ground truth best example probability metric

## Open Questions the Paper Calls Out
- **Grounding error robustness**: How robust is the pragmatic data augmentation approach to linguistic grounding errors when converting user descriptions into structured feature representations? The paper acknowledges this challenge but does not evaluate its impact on method effectiveness.
- **Comparison to alternative feedback methods**: How does the approach compare to other methods of incorporating human feedback, such as active learning or preference learning with different types of queries? The paper focuses on RLHF comparisons but doesn't explore broader feedback incorporation methods.
- **Scalability to high-dimensional features**: How does the approach scale to domains with a large number of features or high-dimensional feature spaces? The paper evaluates on moderate feature sets but doesn't analyze computational complexity or scalability.

## Limitations
- Specific implementation details of pragmatic data augmentation functions (mask, feat-combos) are not fully specified
- User study methodology lacks detailed analysis of potential cognitive burden from feature-level queries
- Evaluation limited to synthetic environments without testing on complex real-world scenarios with higher-dimensional feature spaces

## Confidence
- **High confidence**: The core theoretical framework combining pairwise preferences with feature-level queries is well-founded and the general methodology is clearly described
- **Medium confidence**: The performance improvements over RLHF baselines are demonstrated, but the specific contribution of pragmatic data augmentation versus feature-level queries alone is not fully isolated
- **Medium confidence**: The user study results showing comparable user effort are presented, but the sample size and experimental design details are insufficient for strong generalization claims

## Next Checks
1. **Implementation validation**: Reproduce the pragmatic data augmentation pipeline with the exact mask and feat-combos functions to verify the claimed performance improvements
2. **User study replication**: Conduct a larger-scale user study with more participants and detailed cognitive load measurements to confirm that feature-level queries do not significantly increase user burden
3. **Generalization test**: Evaluate the method on a real-world task with higher-dimensional features and continuous actions to assess robustness beyond the synthetic environments used in the paper