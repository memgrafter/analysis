---
ver: rpa2
title: Understanding with toy surrogate models in machine learning
arxiv_id: '2410.05675'
source_url: https://arxiv.org/abs/2410.05675
tags:
- understanding
- they
- surrogate
- target
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the nature and epistemic role of toy surrogate
  models (TSMs) in machine learning, comparing them to scientific toy models. Unlike
  scientific toy models that represent phenomena, TSMs are models of models, aiming
  to provide global understanding of opaque ML systems.
---

# Understanding with toy surrogate models in machine learning

## Quick Facts
- arXiv ID: 2410.05675
- Source URL: https://arxiv.org/abs/2410.05675
- Reference count: 0
- Primary result: Toy surrogate models provide pragmatic understanding by enabling users to use and manipulate opaque ML models, distinct from scientific toy models that represent phenomena

## Executive Summary
This paper explores the epistemic role of toy surrogate models (TSMs) in machine learning, comparing them to scientific toy models. Unlike scientific toy models that represent phenomena, TSMs are models of models, aiming to provide global understanding of opaque ML systems. The author argues that TSMs are not Galilean or Aristotelian idealizations and that their epistemic value stems from their role as epistemic tools enabling successful use and manipulation of target models. Through case studies like interpretable decision lists and trees, the paper shows how TSMs offer pragmatic understanding by helping users reason counterfactually and interact with ML models effectively.

## Method Summary
The paper employs philosophical analysis comparing TSMs to scientific toy models across multiple dimensions including idealization type, representational relation, and epistemic function. Three case studies are examined: decision lists, weighted checklists, and decision trees, with a focus on how these interpretable models provide understanding of their black-box counterparts. The analysis considers the Rashomon Effect and argues that TSMs cannot be evaluated based on structural alignment alone, necessitating a pragmatic approach that includes user context and goals.

## Key Results
- TSMs are models of models, not models of phenomena, serving epistemic rather than representational purposes
- The epistemic value of TSMs stems from enabling successful use and manipulation of target models
- Objectual understanding via TSMs should precede local interpretability methods for genuine counterfactual reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TSMs provide pragmatic understanding by enabling successful use and manipulation of target models.
- Mechanism: TSMs simplify complex ML models into interpretable structures that allow users to reason counterfactually about the model's behavior and make actionable decisions.
- Core assumption: Users can achieve understanding through their ability to use and manipulate systems.
- Evidence anchors: Abstract, section 5.1
- Break condition: If users cannot translate TSM outputs into actionable insights.

### Mechanism 2
- Claim: TSMs provide objectual understanding distinct from local post-hoc interpretability methods.
- Mechanism: While local methods explain individual predictions, TSMs offer global understanding of the model's general functioning.
- Core assumption: Understanding specific predictions requires prior objectual understanding of the model as a whole.
- Evidence anchors: Abstract, section 5.2
- Break condition: If users can reason counterfactually about individual predictions without global understanding.

### Mechanism 3
- Claim: TSMs' epistemic value cannot be explained through informational/structural alignment alone.
- Mechanism: The Rashomon Effect and lack of ground truth prevent evaluation based solely on structural similarity.
- Core assumption: Multiplicity of equally accurate TSMs indicates epistemic value beyond structural similarity.
- Evidence anchors: Section 4
- Break condition: If ground truth for ML model behavior could be established.

## Foundational Learning

- Concept: Difference between objectual understanding and understanding-why
  - Why needed here: The paper distinguishes between these two types of understanding and argues TSMs provide objectual understanding while local methods provide understanding-why.
  - Quick check question: Can you explain why understanding a single prediction requires prior objectual understanding of the model?

- Concept: Rashomon Effect in machine learning
  - Why needed here: The paper uses this concept to argue that TSMs cannot be evaluated based on structural alignment with target models alone.
  - Quick check question: What problem does the Rashomon Effect create for evaluating the epistemic value of TSMs?

- Concept: Pragmatic theories of scientific representation
  - Why needed here: The paper adopts this framework to explain how TSMs can provide understanding despite lacking structural similarity to their targets.
  - Quick check question: How does a pragmatic approach to representation differ from approaches based on isomorphism or similarity?

## Architecture Onboarding

- Component map:
  - Target ML model (black box) → Surrogate model extraction process → Interpretable TSM (decision tree, rule list, etc.) → User interaction and understanding assessment
  - Stakeholder ecosystem (creators, operators, executors, decision subjects, examiners) with different epistemic needs
  - Evaluation framework including fidelity metrics and user studies

- Critical path:
  1. Identify target ML model and its opaque nature
  2. Extract surrogate model using appropriate technique
  3. Validate fidelity between surrogate and target model
  4. Conduct user studies to assess interpretability and usefulness
  5. Deploy TSM for intended user group with specific epistemic goals

- Design tradeoffs:
  - Simplicity vs. accuracy: More interpretable models may sacrifice predictive performance
  - Generality vs. specificity: Global TSMs may miss important local patterns
  - Standardization vs. customization: One-size-fits-all TSMs may not meet specific user needs
  - Computational cost vs. benefit: Complex extraction processes may outweigh benefits for simple models

- Failure signatures:
  - Low fidelity between surrogate and target model
  - User confusion despite interpretable structure
  - Inability to support counterfactual reasoning about the target model
  - Stakeholder-specific misalignment

- First 3 experiments:
  1. Extract a decision tree from a random forest model on a diabetes risk dataset and validate its fidelity
  2. Conduct user study with medical professionals comparing decision tree vs. rule list TSMs
  3. Test whether users can perform counterfactual reasoning about individual predictions after TSM training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific minimal criteria that distinguish toy surrogate models (TSMs) from regular surrogate models in ML, given the lack of empirical studies on their relative interpretability?
- Basis in paper: Explicit discussion of the tradeoff between precision and interpretability and the urgent need for empirical testing
- Why unresolved: Very limited evidence about comparative understandability of different interpretable methods
- What evidence would resolve it: Systematic empirical studies comparing interpretability and performance across user groups

### Open Question 2
- Question: How can we empirically validate that TSMs provide genuine counterfactual reasoning capabilities to non-expert users, beyond just showing correlations?
- Basis in paper: Argument that local interpretability methods alone cannot provide understanding and that TSMs enable counterfactual reasoning
- Why unresolved: Lack of empirical evidence that TSMs actually enable this capability for users
- What evidence would resolve it: Controlled user studies demonstrating correct counterfactual reasoning with TSMs

### Open Question 3
- Question: What is the optimal level of complexity for TSMs across different stakeholder groups in the ML ecosystem, and how does this vary by domain and application?
- Basis in paper: Discussion of different stakeholders with different epistemic goals
- Why unresolved: Acknowledgment that different users have different needs but no framework for determining optimal complexity
- What evidence would resolve it: Empirical studies mapping stakeholder needs to TSM complexity requirements across domains

## Limitations
- The philosophical framework lacks empirical validation through user studies or quantitative metrics
- The distinction between objectual understanding and local interpretability methods remains largely theoretical
- The claim that TSMs are fundamentally different from scientific toy models requires further defense against potential counterexamples

## Confidence
- High confidence: TSMs are models of models (not phenomena), serving epistemic rather than representational purposes
- Medium confidence: The pragmatic value of TSMs stems from their role as epistemic tools rather than structural similarity to targets
- Low confidence: The claim that objectual understanding must precede local interpretability methods in practice

## Next Checks
1. Conduct user studies comparing understanding outcomes between TSM-based training versus traditional local interpretability methods
2. Develop quantitative metrics for "pragmatic understanding" that can be empirically measured through user interaction and counterfactual reasoning tasks
3. Test whether the Rashomon Effect truly prevents structural evaluation by attempting to establish ground truth benchmarks for specific ML domains where expert knowledge exists