---
ver: rpa2
title: 'CoAM: Corpus of All-Type Multiword Expressions'
arxiv_id: '2412.18151'
source_url: https://arxiv.org/abs/2412.18151
tags:
- mwes
- coam
- annotation
- words
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoAM, a dataset of 1.3K sentences for multiword
  expression (MWE) identification. MWEs are idiomatic word sequences whose meaning
  cannot be derived from their components.
---

# CoAM: Corpus of All-Type Multiword Expressions
## Quick Facts
- arXiv ID: 2412.18151
- Source URL: https://arxiv.org/abs/2412.18151
- Reference count: 33
- Introduces CoAM dataset with 1.3K sentences for MWE identification, achieving best F1 score of 29.4% with fine-tuned Qwen-72B

## Executive Summary
This paper introduces CoAM, a dataset of 1.3K sentences for multiword expression (MWE) identification. MWEs are idiomatic word sequences whose meaning cannot be derived from their components. Existing MWE datasets are limited in size, annotation quality, or coverage. CoAM addresses these issues through a multi-step construction process involving human annotation, review, and automated consistency checking. The dataset includes MWEs tagged with types (e.g., NOUN, VERB) for fine-grained analysis.

Experiments compare two MWE identification approaches: MWEasWSD (combining rule-based and trainable models) and LLM fine-tuning using Llama and Qwen models. Fine-tuned Qwen-72B achieves the best F1 score (29.4%), outperforming the previous state-of-the-art. Analysis reveals VERB MWEs are easier to identify than NOUN MWEs, and MWEs not in WordNet are particularly challenging. The work demonstrates LLM fine-tuning's effectiveness for MWE identification while highlighting ongoing challenges in recall and unseen MWE detection.

## Method Summary
The paper presents CoAM, a dataset constructed through a multi-step process involving human annotation, review, and automated consistency checking. The dataset contains 1.3K sentences with multiword expressions (MWEs) tagged with types such as NOUN and VERB. Two approaches are evaluated for MWE identification: MWEasWSD (combining rule-based and trainable models) and LLM fine-tuning using Llama and Qwen models. The fine-tuned Qwen-72B model achieves the best F1 score of 29.4%, outperforming previous state-of-the-art methods.

## Key Results
- CoAM dataset constructed with 1.3K sentences through multi-step annotation process
- Fine-tuned Qwen-72B achieves best F1 score of 29.4% for MWE identification
- VERB MWEs show higher identification accuracy than NOUN MWEs
- MWEs not present in WordNet demonstrate particularly challenging identification

## Why This Works (Mechanism)
The paper demonstrates that LLM fine-tuning effectively leverages contextual understanding for MWE identification, outperforming traditional rule-based and trainable models. The multi-step annotation process with automated consistency checking ensures high-quality labels, while type-specific tagging enables fine-grained analysis of model performance across different MWE categories.

## Foundational Learning
- Multiword Expression Identification: Understanding idiomatic phrases whose meaning isn't compositional
  - Why needed: MWEs represent a fundamental challenge in NLP due to their non-compositional semantics
  - Quick check: Can identify at least 5 common English MWEs and explain why their meanings aren't compositional

- Dataset Construction Methodology: Multi-step process involving annotation, review, and automated consistency checking
  - Why needed: Ensures high-quality, consistent annotations across the dataset
  - Quick check: Can outline the key steps in the annotation process and explain the role of automated consistency checking

- LLM Fine-tuning for MWEs: Adapting large language models to recognize specific linguistic phenomena
  - Why needed: Demonstrates how general-purpose LLMs can be specialized for domain-specific tasks
  - Quick check: Can explain how fine-tuning differs from prompt engineering and when each approach is appropriate

## Architecture Onboarding
**Component Map:** Data Collection -> Annotation -> Review -> Automated Consistency Checking -> Dataset Publication -> Model Training (MWEasWSD/LLM) -> Evaluation

**Critical Path:** The annotation and review process is critical as it directly determines dataset quality, which in turn affects all downstream model performance.

**Design Tradeoffs:** Smaller dataset size (1.3K) vs. higher annotation quality through rigorous multi-step process. LLM fine-tuning vs. traditional methods for balancing performance with computational cost.

**Failure Signatures:** Low inter-annotator agreement, inconsistent MWE boundaries, models failing on out-of-vocabulary MWEs, poor generalization to unseen MWE types.

**First Experiments:**
1. Test basic MWE identification on a small validation set to establish baseline performance
2. Compare rule-based vs. trainable components of MWEasWSD approach
3. Evaluate LLM fine-tuning with different model sizes to identify optimal scale

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the low overall F1 score (29.4%) and challenges with NOUN MWEs and unseen MWEs suggest several implicit areas for future research.

## Limitations
- Dataset size of 1.3K sentences is relatively small compared to other NLP benchmarks
- Best model achieves only 29.4% F1 score, indicating the task remains challenging
- Limited exploration of alternative MWE identification approaches beyond MWEasWSD and LLM fine-tuning

## Confidence
- CoAM dataset construction methodology: Medium confidence - The process is described but lacks detailed quality metrics
- LLM fine-tuning effectiveness: Medium confidence - Results show improvement but absolute performance remains low
- VERB MWEs are easier to identify than NOUN MWEs: Medium confidence - Based on aggregate statistics without deeper analysis
- MWEs not in WordNet are particularly challenging: Medium confidence - The relationship between WordNet coverage and difficulty needs more rigorous testing

## Next Checks
1. Test model performance on an external MWE dataset to assess generalizability beyond CoAM
2. Conduct ablation studies to determine which components of the annotation process contribute most to dataset quality
3. Evaluate models on MWEs of varying lengths and syntactic structures to understand performance patterns across different MWE types