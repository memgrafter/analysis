---
ver: rpa2
title: 'FineSurE: Fine-grained Summarization Evaluation using LLMs'
arxiv_id: '2407.00908'
source_url: https://arxiv.org/abs/2407.00908
tags:
- summary
- evaluation
- error
- sentence
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes FineSurE, a fine-grained evaluation framework
  for text summarization using LLMs. It addresses the limitations of current LLM-based
  evaluators by providing sentence-level and keyfact-level assessments, enabling multi-dimensional
  evaluation of faithfulness, completeness, and conciseness.
---

# FineSurE: Fine-grained Summarization Evaluation using LLMs

## Quick Facts
- arXiv ID: 2407.00908
- Source URL: https://arxiv.org/abs/2407.00908
- Reference count: 7
- Key outcome: Sentence-level and keyfact-level evaluation framework achieving 86.4% balanced accuracy for faithfulness

## Executive Summary
FineSurE addresses the limitations of current LLM-based evaluators for text summarization by providing fine-grained, multi-dimensional assessment at the sentence and keyfact levels. The framework overcomes the instability of traditional Likert-scale evaluations by computing precise percentage scores for faithfulness, completeness, and conciseness. Through systematic experiments on FRANK and REALSumm datasets, FineSurE demonstrates superior performance compared to existing methods, particularly for faithfulness evaluation where it achieves 86.4% balanced accuracy at the sentence level.

## Method Summary
FineSurE employs a two-pronged approach: fact checking and keyfact alignment. The fact checking pipeline evaluates each summary sentence for nine predefined factuality error categories using structured LLM prompts that require JSON-formatted outputs for parsing. Keyfact alignment matches extracted keyfacts to summary sentences, constructing a bipartite graph to compute completeness (percentage of keyfacts covered) and conciseness (percentage of summary sentences aligned with keyfacts). The framework uses both human-provided and LLM-extracted keyfacts, with prompts designed to ensure reliable LLM responses through structured formatting and clear instructions.

## Key Results
- Achieves 86.4% balanced accuracy for faithfulness at sentence-level on FRANK dataset
- Outperforms existing methods in completeness and conciseness evaluation with Pearson correlation up to 0.45
- Demonstrates consistent performance across both open-source and proprietary LLMs with stable evaluation scores across multiple runs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained sentence-level error classification improves faithfulness evaluation accuracy.
- Mechanism: Instead of summary-level Likert scores, each summary sentence is evaluated for factuality errors using nine predefined categories (no error, out-of-context, entity, predicate, circumstantial, grammatical, coreference, discourse link, other). This allows precise identification of error types and enables sentence-level balanced accuracy computation.
- Core assumption: LLMs can reliably categorize factuality errors when given structured prompts with clear categories and reasoning instructions.
- Evidence anchors:
  - [abstract] "achieving balanced accuracy of 86.4% for faithfulness at the sentence-level"
  - [section] "Table 2 unveils the capability of LLMs for factuality error localization, demonstrating accuracy as the probability that the predicted error category matches the correct answer given by humans"
  - [corpus] Weak evidence - no direct citations found in corpus about sentence-level error classification accuracy
- Break condition: If LLMs fail to follow prompt instructions consistently, resulting in low success ratios or incorrect JSON outputs, the classification accuracy would degrade significantly.

### Mechanism 2
- Claim: Keyfact alignment enables precise completeness and conciseness evaluation.
- Mechanism: By matching each keyfact to summary sentences, completeness is calculated as the percentage of keyfacts covered, and conciseness as the percentage of summary sentences that align with keyfacts. This replaces vague Likert-scale assessments with precise percentage scores.
- Core assumption: LLMs can accurately determine whether keyfacts are inferred from summary sentences and identify the specific sentences that support each keyfact.
- Evidence anchors:
  - [abstract] "employing fact-checking and keyfact alignment tasks to compute precise percentage scores"
  - [section] "Through the keyfact alignment, we construct a bipartite graph M = (K, S, E), where the edge set E = {(k, s) : k → s | k ∈ K ∧ s ∈ S}"
  - [corpus] Weak evidence - no direct citations found in corpus about keyfact alignment accuracy
- Break condition: If keyfact extraction fails to capture essential information or alignment becomes ambiguous, completeness and conciseness scores would not accurately reflect summary quality.

### Mechanism 3
- Claim: Fine-grained evaluation framework reduces evaluation result instability compared to Likert-scale methods.
- Mechanism: By breaking evaluation into discrete sentence-level and keyfact-level assessments, the framework produces more stable percentage scores rather than subjective Likert ratings that vary across runs.
- Core assumption: Discrete classification tasks (error/no error, keyfact matched/not matched) are more stable than continuous rating scales when using LLMs.
- Evidence anchors:
  - [abstract] "FineSurE maintains much higher agreement in summary-level evaluation scores across three distinct runs"
  - [section] "Table 4 demonstrates that FineSurE (GPT-4) maintains much higher agreement in summary-level evaluation scores across three distinct runs"
  - [corpus] Weak evidence - no direct citations found in corpus about evaluation stability
- Break condition: If prompt-following success ratio drops significantly for certain LLMs, the stability advantage would diminish.

## Foundational Learning

- Concept: Factuality error categorization
  - Why needed here: Enables precise identification of different types of factual inconsistencies in summaries
  - Quick check question: What are the nine categories of factuality errors used in FineSurE?

- Concept: Keyfact matching
  - Why needed here: Provides the basis for calculating completeness and conciseness scores
  - Quick check question: How is the completeness score calculated in FineSurE?

- Concept: Balanced accuracy
  - Why needed here: Appropriate metric for sentence-level faithfulness evaluation when classes are imbalanced
  - Quick check question: Why is balanced accuracy preferred over simple accuracy for faithfulness evaluation?

## Architecture Onboarding

- Component map:
  Input processing -> Fact checking pipeline -> Keyfact extraction -> Keyfact alignment -> Scoring computation -> Output formatting

- Critical path:
  1. Receive document and generated summary
  2. Perform fact checking on each summary sentence
  3. Extract or receive keyfacts
  4. Align keyfacts with summary sentences
  5. Compute percentage scores
  6. Return detailed evaluation results

- Design tradeoffs:
  - Human keyfacts vs. automatic keyfact extraction: Accuracy vs. automation
  - Prompt complexity vs. LLM success ratio: Detailed instructions vs. reliable execution
  - Sentence-level vs. summary-level evaluation: Granularity vs. computational cost

- Failure signatures:
  - Low success ratio in prompt-following indicates prompt engineering issues
  - High error localization accuracy but low balanced accuracy suggests classification threshold problems
  - Inconsistent scores across runs indicates instability in LLM responses

- First 3 experiments:
  1. Compare balanced accuracy of FineSurE vs. traditional NLI-based methods on FRANK dataset
  2. Test keyfact alignment accuracy using human-annotated keyfacts vs. LLM-generated keyfacts
  3. Evaluate stability of FineSurE scores across multiple runs with different LLM temperature settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FineSurE's keyfact extraction and alignment capabilities generalize to domains beyond news summarization, such as medical or legal texts?
- Basis in paper: [inferred] The paper mentions that keyfacts depend on domain priorities and that human keyfacts are ideal but not always available. It also notes that prompts may need tuning for different domains.
- Why unresolved: The evaluation is primarily conducted on news domain datasets (CNNDM and XSUM), and the paper does not test FineSurE on other domains.
- What evidence would resolve it: Experiments applying FineSurE to medical, legal, or other specialized domains with appropriate keyfact definitions and prompts.

### Open Question 2
- Question: How does FineSurE's performance scale with increasing input text length, especially given the token limitations of open-source LLMs?
- Basis in paper: [explicit] The paper notes that open-source LLMs have shorter maximum token lengths compared to proprietary LLMs, leading to prompt truncation for lengthy input texts.
- Why unresolved: The experiments do not explicitly test FineSurE's performance on long documents or analyze how token limitations affect accuracy.
- What evidence would resolve it: Systematic evaluation of FineSurE on increasingly long documents, comparing performance across different LLM models with varying context windows.

### Open Question 3
- Question: What is the optimal balance between using human-generated versus LLM-generated keyfacts for different evaluation scenarios?
- Basis in paper: [explicit] The paper compares FineSurE using human keyfacts versus LLM-extracted keyfacts, showing both approaches work but with different correlation levels to human judgment.
- Why unresolved: The paper doesn't explore when human keyfacts are necessary versus when LLM-generated ones suffice, or how to determine this balance.
- What evidence would resolve it: A study varying the complexity and domain of texts to determine thresholds where human keyfacts become necessary for accurate evaluation.

## Limitations
- Framework performance depends heavily on quality of human-provided keyfacts, which may not capture all essential information
- Open-source LLMs struggle with prompt-following, achieving lower success ratios compared to proprietary models
- Framework has not been tested across diverse domains beyond news summarization, limiting generalizability claims

## Confidence
- High confidence: Sentence-level faithfulness evaluation achieving 86.4% balanced accuracy - This claim is directly supported by experimental results on the FRANK dataset with human annotations for verification.
- Medium confidence: Superior performance across all LLMs (open-source and proprietary) - While results show consistent improvements, the specific performance varies significantly between models, and some open-source LLMs struggle with prompt-following.
- Medium confidence: Framework stability and reduced evaluation result instability - The claim is supported by cross-run agreement metrics, but testing across a wider range of conditions and LLM configurations would strengthen this assertion.

## Next Checks
1. Implement and evaluate automatic keyfact extraction using LLMs and measure the impact on completeness and conciseness scores compared to human-provided keyfacts
2. Apply FineSurE to summarization datasets from diverse domains (medical, legal, technical) to evaluate whether the nine-category error classification system maintains accuracy across different content types
3. Conduct a detailed analysis of how error localization accuracy translates to final faithfulness scores by examining cases where the system correctly identifies errors but fails to compute accurate percentage scores, or vice versa