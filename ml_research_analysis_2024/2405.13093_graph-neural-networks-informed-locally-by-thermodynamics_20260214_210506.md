---
ver: rpa2
title: Graph neural networks informed locally by thermodynamics
arxiv_id: '2405.13093'
source_url: https://arxiv.org/abs/2405.13093
tags:
- networks
- neural
- graph
- learning
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a local implementation of thermodynamics-informed
  graph neural networks (GNNs) to address the computational inefficiencies of global
  matrix assembly in metriplectic formalism. By reformulating the GENERIC structure
  at the nodal level, the method preserves the local, vertex-based architecture of
  GNNs while enforcing thermodynamic constraints like energy conservation and entropy
  inequality.
---

# Graph neural networks informed locally by thermodynamics

## Quick Facts
- arXiv ID: 2405.13093
- Source URL: https://arxiv.org/abs/2405.13093
- Authors: Alicia Tierz; Iciar Alfaro; David González; Francisco Chinesta; Elías Cueto
- Reference count: 40
- Key outcome: Local GENERIC formulation in GNNs achieves 1-2 orders of magnitude higher accuracy while preserving thermodynamic consistency and eliminating memory bottlenecks

## Executive Summary
This paper addresses the computational inefficiencies of traditional physics-informed graph neural networks (GNNs) by reformulating the GENERIC structure at the nodal level. The authors introduce a local implementation that preserves the vertex-based architecture of GNNs while enforcing thermodynamic constraints like energy conservation and entropy inequality. Through three validation examples—3D and 2D viscoelastic beam bending and liquid sloshing—the method demonstrates significantly higher accuracy than vanilla GNNs while maintaining strong generalization capabilities and eliminating memory bottlenecks associated with global matrix assembly.

## Method Summary
The method reformulates the GENERIC formalism using a local port-metriplectic approach where each node represents a portion of the bulk material and exchanges energy with neighbors through graph edges. The architecture follows an encode-process-decode scheme where node and edge features are transformed into latent space, information is exchanged through message passing, and the GENERIC energy and entropy gradients are predicted. Critically, the method reshapes flattened operators into lower-triangular matrices at each node, avoiding global matrix assembly. The network is trained using a combined loss function incorporating data loss and degeneracy loss, with forward Euler integration and Adam optimization.

## Key Results
- Local implementation achieves 1-2 orders of magnitude higher accuracy compared to vanilla GNNs on viscoelastic beam bending and liquid sloshing problems
- Strong generalization demonstrated on unseen geometries and boundary conditions, including beams clamped at both ends
- Eliminates memory bottlenecks by avoiding global matrix assembly, enabling efficient training on large-scale problems
- Successfully preserves thermodynamic consistency through local enforcement of degeneracy conditions

## Why This Works (Mechanism)

### Mechanism 1
The nodal GENERIC formulation preserves the local, vertex-based architecture of graph neural networks by replacing global Poisson and dissipation matrices with node-local operators. Instead of assembling a global matrix of size (nv × ndof) × (nv × ndof), each node i maintains its own local energy and entropy gradients and interacts with neighbors through edge-based operators. This allows the graph structure to remain intact while enforcing thermodynamic constraints.

### Mechanism 2
The local implementation achieves one to two orders of magnitude higher accuracy compared to vanilla GNNs by incorporating thermodynamic inductive biases. By enforcing the degeneracy conditions (L ∂S/∂z = 0 and M ∂E/∂z = 0) at the node level, the network learns to preserve energy conservation and entropy inequality, which are fundamental to physical systems.

### Mechanism 3
The local implementation eliminates memory bottlenecks, enabling efficient training and inference even on large-scale problems. By avoiding the assembly of global matrices, the memory requirement scales linearly with the number of nodes and edges, rather than quadratically. This allows the method to handle systems with tens of thousands of nodes.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GNNs provide a natural way to represent and learn from graph-structured data, which is common in physical systems discretized into meshes.
  - Quick check question: What is the key advantage of GNNs over traditional neural networks when dealing with graph-structured data?

- Concept: Thermodynamics and the GENERIC formalism
  - Why needed here: The GENERIC formalism provides a framework for modeling non-equilibrium systems while ensuring thermodynamic consistency, which is crucial for accurate physical simulations.
  - Quick check question: What are the two key principles of thermodynamics that the GENERIC formalism enforces?

- Concept: Port-metriplectic formalism
  - Why needed here: The port-metriplectic formalism allows for the decomposition of the GENERIC structure into bulk and boundary contributions, enabling the local implementation of thermodynamic biases.
  - Quick check question: How does the port-metriplectic formalism differ from the standard GENERIC formalism in terms of energy exchange between nodes?

## Architecture Onboarding

- Component map:
  - Encoder: Transforms node and edge features into higher-dimensional latent space
  - Processor: Exchanges information between nodes and edges through message passing
  - Decoder: Predicts GENERIC energy and entropy gradients and flattened operators
  - Reparametrization: Reshapes flattened operators into lower-triangular matrices
  - Integrator: Constructs thermodynamically-sound integrator using nodal GENERIC formalism

- Critical path:
  Node and edge features are encoded into latent space, information is exchanged between nodes and edges through message passing, the GENERIC energy and entropy gradients and flattened operators are predicted, the flattened operators are reshaped into lower-triangular matrices, and the thermodynamically-sound integrator is constructed using the nodal GENERIC formalism.

- Design tradeoffs:
  Accuracy vs. computational efficiency: The local implementation trades some of the accuracy of the global implementation for improved computational efficiency. Expressivity vs. physical constraints: The incorporation of thermodynamic biases may limit the expressivity of the network but ensures physical consistency.

- Failure signatures:
  If the network fails to converge during training, it may indicate that the thermodynamic biases are too restrictive or that the network architecture is insufficient. If the predictions are physically inconsistent (e.g., energy is not conserved), it may indicate that the degeneracy conditions are not being properly enforced.

- First 3 experiments:
  1. 3D Viscoelastic Beam Bending: Evaluate the accuracy of the local implementation on a 3D viscoelastic cantilever beam subjected to bending forces.
  2. 2D Viscoelastic Beam Bending: Evaluate the accuracy of the local implementation on a 2D viscoelastic cantilever beam with varying geometry.
  3. Liquid Sloshing: Evaluate the accuracy of the local implementation on a fluid sloshing problem, demonstrating its ability to handle dissipative systems.

## Open Questions the Paper Calls Out

### Open Question 1
How does the local GENERIC formulation affect the scalability and efficiency of graph neural networks when dealing with systems of varying sizes and complexities? While the paper demonstrates improvements in memory efficiency and computational speed, it does not provide a comprehensive analysis of how the local formulation scales with different system sizes or complexities.

### Open Question 2
What are the limitations of the current implementation of thermodynamics-informed graph neural networks in handling non-Newtonian fluids and other complex material behaviors? The paper mentions the use of GENERIC formalism for modeling general non-conservative dynamics but does not explore its application to non-Newtonian fluids or other complex materials.

### Open Question 3
How does the incorporation of external forces and boundary conditions in the local GENERIC formulation impact the accuracy and generalization of predictions in diverse physical scenarios? While the paper shows improved accuracy and generalization, it does not specifically analyze how different external forces and boundary conditions affect these outcomes.

## Limitations

- Exact architectural hyperparameters (number of layers, hidden dimensions) are not fully specified beyond general ranges
- Validation primarily focuses on relative L2 errors with limited discussion of absolute error scales or computational efficiency comparisons
- Computational overhead of thermodynamic constraints is not explicitly quantified or compared to alternative approaches

## Confidence

- **High confidence**: The theoretical framework of local GENERIC implementation and its thermodynamic consistency
- **Medium confidence**: The claimed one to two orders of magnitude accuracy improvement
- **Medium confidence**: The elimination of memory bottlenecks

## Next Checks

1. Conduct ablation studies removing the thermodynamic constraints to quantify the exact contribution of the GENERIC formalism to accuracy improvements
2. Measure and compare wall-clock training and inference times between the local implementation and traditional global matrix approaches across varying system sizes
3. Test the method on problems with strong non-linearities or chaotic dynamics to evaluate robustness beyond the presented viscoelastic and fluid examples