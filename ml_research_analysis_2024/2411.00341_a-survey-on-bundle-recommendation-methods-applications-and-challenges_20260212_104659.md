---
ver: rpa2
title: 'A Survey on Bundle Recommendation: Methods, Applications, and Challenges'
arxiv_id: '2411.00341'
source_url: https://arxiv.org/abs/2411.00341
tags:
- bundle
- recommendation
- user
- learning
- bundles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews bundle recommendation systems,
  which enhance user experience and sales by recommending sets of items rather than
  individual items. The paper introduces a taxonomy classifying bundle recommendation
  into discriminative (recommending existing bundles) and generative (creating new
  bundles) approaches.
---

# A Survey on Bundle Recommendation: Methods, Applications, and Challenges

## Quick Facts
- arXiv ID: 2411.00341
- Source URL: https://arxiv.org/abs/2411.00341
- Reference count: 40
- Primary result: Comprehensive survey of bundle recommendation systems covering discriminative and generative approaches, taxonomy, resources, and reproducibility experiments

## Executive Summary
This survey provides a comprehensive review of bundle recommendation systems, which recommend sets of items rather than individual items. The paper introduces a taxonomy classifying bundle recommendation into discriminative (recommending existing bundles) and generative (creating new bundles) approaches. It systematically reviews representation learning strategies, interaction modeling techniques, and generation methods while providing resources including 30 datasets across 11 application domains. The survey also presents reproducibility experiments comparing 10 mainstream models, demonstrating that cooperative learning strategies and graph-based models outperform other approaches.

## Method Summary
The survey synthesizes research from 40+ papers to provide a comprehensive taxonomy of bundle recommendation methods. It categorizes discriminative approaches by representation learning strategies (unified, separate, cooperative) and interaction modeling techniques (inner product, distance modeling, neural networks). For generative approaches, it reviews representation learning from item level and bundle generation methods (search-based and model-based). The survey provides empirical validation through reproducibility experiments on three datasets (Youshu, NetEase, iFashion) comparing 10 mainstream models, demonstrating the effectiveness of cooperative learning strategies and graph-based models.

## Key Results
- Cooperative learning strategies across bundle and item views outperform unified and separate learning approaches
- Graph-based models show superior performance compared to traditional factorization-based methods
- The survey identifies 30 datasets across 11 application domains for future research
- Emerging challenges include representation learning in non-Euclidean spaces, data sparsity, and responsible recommendation systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discriminative bundle recommendation improves performance by employing cooperative learning strategies across bundle and item views
- Mechanism: Cross-view contrastive learning, knowledge distillation, and mutual learning techniques are used to align and enhance representations from different perspectives (bundle view and item view), addressing data sparsity and improving prediction accuracy
- Core assumption: The bundle view and item view contain complementary information that can be mutually reinforced through advanced learning techniques
- Evidence anchors:
  - [abstract]: "reproduces 6 mainstream graph-based models and 4 CF-based models, and compares their performance... cooperative learning across bundle view and item view is effective"
  - [section]: "Some studies have applied KD to RS... DGMAE uses knowledge distillation to transfer the knowledge from bundle view to item view for more fine-grained learning of user's bundle preferences"
  - [corpus]: "Recent advances in product bundling have leveraged multimodal information through sophisticated encoders, but remain constrained by limited semantic understanding... employ In-context Learning (ICL) to explore the potential of large language models (LLMs)"

### Mechanism 2
- Claim: Generative bundle recommendation can produce more relevant and intelligible bundles by capturing user intents
- Mechanism: Large Language Models (LLMs) and intent-aware learning paradigms are employed to infer user preferences from sessions and generate personalized bundles that reflect consistent user needs
- Core assumption: User interactions and session data contain latent intent signals that can be extracted and used to guide bundle generation
- Evidence anchors:
  - [abstract]: "Early bundle generation methods usually relied on specifying hard constraints... With the advancement of technology, deep learning-based approaches have gradually been introduced to capture latent associations between items for bundle generation"
  - [section]: "Recognizing the limitations of these approaches, Chen et al. introduced POG, an encoder-decoder model that harnesses the power of Transformer architecture to connect user preferences across individual items and outfits"
  - [corpus]: "Fine-tuning Multimodal Large Language Models for Product Bundling... Recent advances in product bundling have leveraged multimodal information through sophisticated encoders, but remain constrained by limited semantic understanding"

### Mechanism 3
- Claim: Non-Euclidean representation learning in hyperbolic space can better capture the hierarchical structure of bundle interactions
- Mechanism: Mapping Euclidean embeddings to hyperbolic space and performing graph convolution operations within hyperbolic space to reduce distortion and improve representation quality for tree-like data structures
- Core assumption: Bundle interaction data exhibits underlying tree-like structures that are better represented in hyperbolic space with negative curvature
- Evidence anchors:
  - [abstract]: "Exploring representation learning in non-Euclidean spaces is not only challenging but also a very attractive research direction"
  - [section]: "In the field of bundle recommendation, interaction data often exhibit an underlying tree-like structure, which is more suitable for learning representations in the hyperbolic space with negative curvature"
  - [corpus]: "Dynamic Bundling with Large Language Models for Zero-Shot Inference on Text-Attributed Graphs... adoption of LLMs faces two major challenges: limited information on graph structure"

## Foundational Learning

- Concept: Graph neural networks (GNNs) for representation learning
  - Why needed here: GNNs are essential for capturing complex relationships and dependencies in bundle recommendation, where items within bundles and relationships between bundles need to be modeled effectively
  - Quick check question: How do GNNs propagate information between nodes in a user-bundle-item graph?

- Concept: Contrastive learning for representation alignment
  - Why needed here: Contrastive learning helps align representations from different views (bundle and item) and enhances their quality by pulling similar representations closer and pushing dissimilar ones apart
  - Quick check question: What is the difference between cross-view contrastive learning and self-supervised contrastive learning?

- Concept: Transformer architecture for sequence modeling
  - Why needed here: Transformers are used in generative bundle recommendation to capture long-range dependencies and generate coherent sequences of items that form meaningful bundles
  - Quick check question: How does the self-attention mechanism in Transformers help in understanding item relationships within a bundle?

## Architecture Onboarding

- Component map:
  Data Layer: User-bundle interactions, user-item interactions, bundle-item affiliations, side information
  Representation Layer: Embedding models (MF, DNN, GNN), feature engineering techniques
  Learning Strategy Layer: Unified, separate, or cooperative representation learning strategies
  Interaction Modeling Layer: Inner product, distance modeling, neural networks for prediction
  Generation Layer (for generative BR): Search-based or model-based bundle generation methods
  Evaluation Layer: Accuracy, ranking quality, diversity, exposure metrics

- Critical path:
  For discriminative BR: Data → Representation Learning → Interaction Modeling → Prediction
  For generative BR: Data → Representation Learning → Bundle Generation → Recommendation

- Design tradeoffs:
  - Unified vs. Separate vs. Cooperative learning: Unified is simpler but may miss view-specific nuances; separate allows specialization but may lose cross-view benefits; cooperative balances both but is more complex
  - Search-based vs. Model-based generation: Search is interpretable and controllable but computationally expensive; model-based is efficient but may lack interpretability

- Failure signatures:
  - Poor performance on sparse datasets: May indicate insufficient representation learning or ineffective interaction modeling
  - Mode collapse in generative models: May suggest inadequate diversity in generation or poor intent capture
  - Overfitting to popular items: May indicate bias in training data or insufficient regularization

- First 3 experiments:
  1. Implement a simple MF-based discriminative BR model and evaluate on Youshu dataset using Recall@20
  2. Compare unified vs. separate learning strategies on NetEase dataset using NDCG@40
  3. Implement a greedy search-based generative BR model and evaluate on Flickr dataset using F1 score

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can representation learning be performed entirely within hyperbolic space without relying on tangent space mapping for bundle recommendation?
- Basis in paper: [explicit] The paper states that "Since there is no corresponding graph convolution operation in hyperbolic space, to perform equivalent graph convolution operations as in Euclidean space, one needs to map the data to the tangent space, complete the graph convolution operations, and then map the results back to hyperbolic space. This mapping and inverse mapping process introduces unnecessary noise, resulting the deduction of the accuracy of the embeddings"
- Why unresolved: The paper acknowledges this limitation but does not provide solutions for performing graph convolutions directly in hyperbolic space
- What evidence would resolve it: A computational method enabling graph convolution operations entirely within hyperbolic space without tangent space mapping, validated through empirical comparison showing improved performance over current approaches

### Open Question 2
- Question: What is the optimal way to dynamically adjust curvature spaces for bundle recommendation as user interactions evolve over time?
- Basis in paper: [explicit] The paper suggests that "user interactions will continue to generate, requiring a dynamically changing curvature space to represent learning for users, items, and bundles" and notes that "considering that the curvature of the graph may change over time, incorporating the variation of graph curvature into the bundle recommendation model can make it more flexible to adapt to environmental changes"
- Why unresolved: While the paper identifies the need for dynamic curvature adjustment, it does not propose specific mechanisms or models for achieving this
- What evidence would resolve it: A framework that dynamically adjusts curvature parameters based on temporal user interaction patterns, with demonstrated improvements in recommendation quality over static curvature models

### Open Question 3
- Question: How can multimodal information (images, descriptions) be effectively integrated with large language models to improve cold-start bundle recommendation performance?
- Basis in paper: [explicit] The paper states that "A promising future direction is to introduce multimodal information (e.g., images, descriptions of items or bundles, etc.) to model user preferences harnessing LLMs" and references studies that have "began to introduce multimodal information and attempt to utilize LLMs to boost recommendation performance"
- Why unresolved: The paper identifies this as a future direction but does not provide concrete methodologies for integrating multimodal information with LLMs for bundle recommendation
- What evidence would resolve it: A multimodal-LLM framework specifically designed for bundle recommendation that shows significant performance gains on cold-start datasets compared to unimodal approaches

## Limitations
- The evaluation experiments focus on only three datasets despite surveying 30 available datasets, potentially limiting generalizability
- The survey doesn't address computational complexity trade-offs between different methods for resource-constrained real-world deployments
- The paper doesn't explore user acceptance and usability aspects of bundle recommendations, which are crucial for practical implementation

## Confidence
- **High confidence**: The taxonomy of discriminative vs. generative bundle recommendation methods is well-established and clearly articulated
- **Medium confidence**: The effectiveness of cooperative learning strategies is supported by reproduction experiments but limited to specific datasets
- **Medium confidence**: The discussion of emerging challenges (non-Euclidean spaces, data sparsity, user intent) is theoretically sound but lacks extensive empirical validation
- **Low confidence**: The practical deployment challenges and user acceptance aspects of bundle recommendation systems

## Next Checks
1. **Cross-dataset validation**: Implement and test the top-performing cooperative learning models (e.g., DGMAE, CoBR) across at least 5 additional datasets from different domains to verify generalizability
2. **Ablation study**: Conduct controlled experiments to isolate the contribution of each component in cooperative learning strategies (cross-view contrastive learning, knowledge distillation, mutual learning) to understand which mechanisms drive performance improvements
3. **Scalability assessment**: Measure training and inference times across different model architectures and dataset sizes to identify computational bottlenecks and scalability limitations for real-world deployment