---
ver: rpa2
title: 'Provenance: A Light-weight Fact-checker for Retrieval Augmented LLM Generation
  Output'
arxiv_id: '2411.01022'
source_url: https://arxiv.org/abs/2411.01022
tags:
- context
- datasets
- provenance
- scores
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Provenance, a lightweight fact-checking system
  for retrieval-augmented generation (RAG) outputs that uses compact cross-encoder
  models instead of large language models. The system identifies relevant context
  sources, generates claims from queries and answers, and evaluates factuality through
  natural language inference scoring with weighted aggregation.
---

# Provenance: A Light-weight Fact-checker for Retrieval Augmented LLM Generation Output

## Quick Facts
- arXiv ID: 2411.01022
- Source URL: https://arxiv.org/abs/2411.01022
- Authors: Hithesh Sankararaman; Mohammed Nasheed Yasin; Tanner Sorensen; Alessandro Di Bari; Andreas Stolcke
- Reference count: 9
- Primary result: 300M parameter cross-encoder models achieve competitive ROC AUC scores on multiple RAG factuality datasets, outperforming larger models on summarization tasks

## Executive Summary
This paper introduces Provenance, a lightweight fact-checking system for retrieval-augmented generation (RAG) outputs that uses compact cross-encoder models instead of large language models. The system identifies relevant context sources, generates claims from queries and answers, and evaluates factuality through natural language inference scoring with weighted aggregation. Experiments on multiple open-source datasets show competitive ROC AUC scores, with the 300M parameter model achieving 0.94 AUC on PAWS and 0.92 AUC on DialFact, while outperforming larger models on specific tasks like summarization. The approach offers improved accessibility, lower latency, and interpretability compared to LLM-based fact-checkers.

## Method Summary
Provenance uses two cross-encoder models: one for relevance scoring between queries and context items, and another for factuality checking between prepared claims and selected sources. The system splits context paragraphs into sentences, scores their relevance to the query, selects top sources using either TopK or TopP strategies, generates claim prompts from the query and answer, and computes factuality scores using a specialized hallucination detection model. Scores are then aggregated using source weights based on relevance. The approach avoids the need for LLM fine-tuning and provides low-latency, low-cost inference.

## Key Results
- 300M parameter cross-encoder achieves 0.94 ROC AUC on PAWS dataset and 0.92 ROC AUC on DialFact dataset
- Outperforms larger 11B parameter models on summarization tasks while maintaining competitive performance on other RAG datasets
- Demonstrates effective factuality detection across multiple task types including dialogue, summarization, and fact verification
- Shows significant latency and cost advantages compared to LLM-based factuality checkers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-encoders provide lower latency than LLMs for fact-checking
- Mechanism: Cross-encoders compute relevance and factuality scores through parallel encoding of query-context/answer pairs, avoiding autoregressive decoding steps required by LLMs
- Core assumption: Cross-encoders can achieve comparable accuracy to LLMs for relevance and factuality tasks
- Evidence anchors: Abstract states "compact, open-source natural language inference (NLI) models that yield a freely accessible solution with low latency and low cost"
- Break condition: If cross-encoder accuracy drops significantly below LLM accuracy, latency benefit may not justify the trade-off

### Mechanism 2
- Claim: Weighted aggregation improves detection accuracy
- Mechanism: Factuality scores are combined using weights based on source relevance, allowing more relevant sources to have greater influence
- Core assumption: Relevance scores correlate with informativeness for factuality determination
- Evidence anchors: Scoring function uses F Score = nli-model(S, C) where S is sources and C is prepared claim prompt
- Break condition: If relevance doesn't correlate with informativeness, weighting could reduce accuracy

### Mechanism 3
- Claim: Sentence-level chunking enables effective processing of long contexts
- Mechanism: Contexts are split into individual sentences for relevance scoring and factuality checking, enabling processing of contexts longer than model input limits
- Core assumption: Sentence-level chunking preserves sufficient context for accurate relevance and factuality determination
- Evidence anchors: MSMarco and HotpotQA datasets contain 10 sources per question, all split into individual sentences
- Break condition: If important context spans multiple sentences, chunking could lose critical information

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: NLI models determine whether claims are supported by, contradicted by, or neutral with respect to premises, which is central to factuality checking
  - Quick check question: Can you explain the difference between entailment, contradiction, and neutral in NLI terminology?

- Concept: Cross-encoder architecture
  - Why needed here: Cross-encoders encode two texts together as a pair, enabling better performance on comparison tasks compared to bi-encoders
  - Quick check question: What is the key architectural difference between cross-encoders and bi-encoders, and why does this matter for factuality checking?

- Concept: ROC AUC and threshold-invariant evaluation
  - Why needed here: ROC AUC evaluates performance across different classification thresholds, appropriate when optimal threshold may vary by application
  - Quick check question: Why might ROC AUC be preferred over accuracy when evaluating a factuality checker with potentially imbalanced classes?

## Architecture Onboarding

- Component map: Query → Relevancy Scorer → Context Item Selector → Fact Checker → Aggregator → Threshold → Binary output
- Critical path: Query flows through relevance scoring, context selection, factuality checking, aggregation, and thresholding to produce binary output
- Design tradeoffs:
  - Model size vs. accuracy: 300M parameter models vs. 11B parameter models from baseline
  - Top-k vs. Top-p selection: Deterministic vs. probabilistic source selection
  - Aggregation method: Min, max, or weighted average of factuality scores
  - Chunking granularity: Sentence-level vs. larger context units
- Failure signatures:
  - Low AUC scores: Model may not be capturing relevant patterns
  - High false positive rate: Threshold may be too low or model may be overly conservative
  - High false negative rate: Threshold may be too high or model may miss subtle hallucinations
  - Performance degradation on long contexts: Sentence-level chunking may lose important context
- First 3 experiments:
  1. Run on TRUE dataset with Top-k=5 and max aggregation; verify ROC AUC meets expectations
  2. Test different aggregation methods (min, max, weighted average) on the same dataset
  3. Evaluate performance difference between Top-k and Top-p selection strategies

## Open Questions the Paper Calls Out

- Question: What is the optimal chunk length for context items that balances model performance and computational efficiency?
  - Basis in paper: [inferred] Paper discusses challenges with chunking long contexts but doesn't systematically explore optimal chunk sizes
  - Why unresolved: Paper uses sentence-level chunking but doesn't evaluate whether different chunk sizes would improve performance
  - What evidence would resolve it: Comparative experiments testing different chunk sizes across all evaluated datasets

- Question: How does the system perform when generating and fact-checking long answers versus short answers?
  - Basis in paper: [explicit] Paper explicitly states it focuses on short context/short answer and long context/short answer scenarios, excluding long context/long answer cases
  - Why unresolved: Paper acknowledges this limitation but doesn't provide preliminary results or analysis for long answers
  - What evidence would resolve it: Experiments testing the system on datasets with long answers or synthetic extensions of current datasets

- Question: What is the optimal combination of TopK and TopP values for different types of tasks?
  - Basis in paper: [explicit] Paper mentions using TopP=0.9 and TopK=5 but states "We have not carried out a systematic optimization of top_k and top_p values"
  - Why unresolved: Paper uses fixed hyperparameters without exploring the parameter space
  - What evidence would resolve it: Grid search or Bayesian optimization experiments varying TopK and TopP across different task types

## Limitations
- Sentence-level chunking may lose important multi-sentence context critical for factuality determination
- Limited evaluation on long answers, excluding a common real-world RAG scenario
- No systematic ablation studies to isolate contributions of individual components
- Performance generalizability to complex document structures and noisier retrieval outputs remains uncertain

## Confidence
- **Medium** for overall performance claims due to limited ablation studies on key design choices
- **Low** for generalizability beyond tested datasets with structured contexts
- **Medium** for latency and cost claims without direct latency comparisons provided

## Next Checks
1. Conduct ablation study on aggregation strategies to quantify contribution of weighting mechanism
2. Evaluate impact of different context chunking strategies on factuality detection accuracy
3. Test cross-dataset transferability by training on one dataset and evaluating on unseen datasets