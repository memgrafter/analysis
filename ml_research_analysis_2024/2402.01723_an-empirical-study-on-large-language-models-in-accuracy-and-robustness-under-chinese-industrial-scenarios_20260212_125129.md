---
ver: rpa2
title: An Empirical Study on Large Language Models in Accuracy and Robustness under
  Chinese Industrial Scenarios
arxiv_id: '2402.01723'
source_url: https://arxiv.org/abs/2402.01723
tags:
- llms
- robustness
- industrial
- arxiv
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the accuracy and robustness of large language
  models (LLMs) in Chinese industrial scenarios. The authors manually collect 1,200
  domain-specific problems from 8 different industrial sectors and design a metamorphic
  testing framework with 13,631 variants to assess robustness.
---

# An Empirical Study on Large Language Models in Accuracy and Robustness under Chinese Industrial Scenarios

## Quick Facts
- arXiv ID: 2402.01723
- Source URL: https://arxiv.org/abs/2402.01723
- Reference count: 40
- Primary result: Current LLMs exhibit low accuracy (<0.6) and varying robustness in Chinese industrial contexts

## Executive Summary
This paper evaluates the accuracy and robustness of large language models (LLMs) in Chinese industrial scenarios through a comprehensive study using 1,200 domain-specific problems from 8 industrial sectors. The authors develop a metamorphic testing framework with 13,631 variants to systematically assess how well LLMs handle transformations of industrial questions. They compare 9 Chinese local LLMs against 4 global LLMs, finding that while all models struggle with industrial accuracy, global LLMs show better overall robustness, particularly in logical reasoning tasks.

The study reveals important insights about the limitations of current LLMs in specialized domains, demonstrating that industrial terminology and context pose significant challenges for even advanced models. The authors identify key differences in how local and global LLMs handle various metamorphic relations, with local models showing strength in understanding Chinese industrial terminology while global models excel at logical reasoning. These findings provide valuable guidance for both LLM developers and industrial enterprises seeking to deploy AI solutions in specialized contexts.

## Method Summary
The authors manually collected 1,200 domain-specific problems from 8 different Chinese industrial sectors including electronic equipment manufacturing, mining, power, and petrochemicals. They designed a metamorphic testing framework that generates 13,631 variants of these questions across 4 stability categories and 8 abilities. The framework applies metamorphic relations to create question variants that test different transformation types. They then evaluate 9 Chinese local LLMs and 4 global LLMs on both original questions and their variants, measuring accuracy (correct answer rate) and robustness (consistency of answers to variants).

## Key Results
- Current LLMs exhibit low accuracy in Chinese industrial contexts, with all models scoring less than 0.6
- Local LLMs overall perform worse than global ones, with robustness scores varying across industrial sectors
- LLM robustness differs significantly across abilities: global LLMs are more robust under logical-related variants, while advanced local LLMs perform better on problems related to understanding Chinese industrial terminology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Industrial accuracy is limited by domain specificity mismatch between general LLM training data and specialized industrial terminology.
- Mechanism: LLMs trained primarily on general internet text lack sufficient exposure to industry-specific vocabulary and structured technical documents, resulting in lower accuracy on domain-specific questions.
- Core assumption: The industrial sectors evaluated have unique terminology and structured knowledge not well-represented in general LLM training corpora.
- Evidence anchors:
  - [abstract] "Current LLMs exhibit low accuracy in Chinese industrial contexts, with all models scoring less than 0.6."
  - [section] "Global LLMs excel in reasoning and open-ended tasks, outperforming local LLMs that are better at understanding Chinese terminology."
  - [corpus] Weak evidence - corpus contains related Chinese NLP work but not specifically industrial domain accuracy data.
- Break condition: If industrial sectors' terminology overlaps significantly with common internet text, accuracy would not be limited.

### Mechanism 2
- Claim: Robustness varies by ability due to different cognitive demands of metamorphic relations.
- Mechanism: Abilities like "Logic" require complex reasoning and understanding of equivalent or opposite meanings, while "Synonyms" only need basic word substitution comprehension, leading to varying robustness scores.
- Core assumption: Different metamorphic relations impose different cognitive loads on LLMs, affecting their robustness.
- Evidence anchors:
  - [section] "Logic capability is more complex, demanding comprehension of industrial questions along with diverse expressions of equivalent or opposite meanings."
  - [section] "Synonyms capability is basic, requiring only the understanding of question word synonyms."
  - [corpus] No direct corpus evidence for this specific mechanism.
- Break condition: If all metamorphic relations are processed equally by LLMs regardless of complexity, robustness would not vary by ability.

### Mechanism 3
- Claim: Local vs global LLM performance differences stem from training data quantity and quality differences for Chinese industrial contexts.
- Mechanism: Global LLMs like GPT-4 trained on massive internet corpora have broader knowledge, while local LLMs have more Chinese-specific data but potentially less industrial domain coverage.
- Core assumption: The amount and relevance of training data significantly impacts LLM performance in specific domains.
- Evidence anchors:
  - [section] "The disparity between the performance is likely due to differences in the quantity and quality of collected data."
  - [section] "Local LLMs overall perform worse than global ones."
  - [corpus] Weak evidence - corpus contains related Chinese NLP work but not specifically local vs global LLM data quality analysis.
- Break condition: If local LLMs have equivalent or superior industrial domain data quality compared to global LLMs, performance differences would not be explained by this mechanism.

## Foundational Learning

- Concept: Metamorphic testing
  - Why needed here: Used to systematically generate variants of industrial questions to assess LLM robustness to different transformations.
  - Quick check question: What is the key advantage of using metamorphic testing over traditional testing for LLM robustness evaluation?

- Concept: Domain-specific benchmarking
  - Why needed here: Standard LLM benchmarks don't cover industrial applications, requiring custom data collection from real industrial sources.
  - Quick check question: Why is it important to use real industrial documents rather than general internet text for this study?

- Concept: Accuracy vs robustness distinction
  - Why needed here: Accuracy measures correct answers to original questions, while robustness measures consistency of answers to question variants.
  - Quick check question: How would you define the difference between correct robustness and incorrect robustness in this context?

## Architecture Onboarding

- Component map:
  Data collection pipeline (industrial sources → question formatting) → Metamorphic testing framework (relations → variants) → LLM evaluation system (prompts → answers → scoring) → Analysis dashboard (metrics → insights)

- Critical path:
  1. Collect industry-specific questions from authoritative sources
  2. Apply metamorphic relations to generate variants
  3. Evaluate LLMs on original questions and variants
  4. Analyze accuracy and robustness across sectors and abilities

- Design tradeoffs:
  - Determinism vs diversity: Using temperature=0 for reproducibility but potentially missing diverse responses
  - Coverage vs depth: 8 industrial sectors provide breadth but may miss sector-specific nuances
  - Automation vs manual evaluation: Automated metrics insufficient for open-ended questions requiring human judgment

- Failure signatures:
  - Low accuracy across all LLMs indicates fundamental domain mismatch
  - Large variance in robustness scores suggests inconsistent handling of transformations
  - Local LLMs underperforming global ones points to training data quality issues

- First 3 experiments:
  1. Evaluate a single LLM on one industrial sector to establish baseline accuracy and robustness
  2. Compare LLM performance on a question and its variant to verify metamorphic testing works
  3. Analyze failure cases to identify common patterns in LLM industrial knowledge gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be effectively trained on industrial-specific data to improve accuracy and robustness?
- Basis in paper: [explicit] The authors note that current LLMs have low accuracy in Chinese industrial contexts and that local LLMs perform worse than global ones, likely due to differences in the quantity and quality of collected data.
- Why unresolved: The paper identifies the issue but does not propose specific solutions for training LLMs on industrial-specific data.
- What evidence would resolve it: A study that trains LLMs on a large, high-quality industrial dataset and demonstrates improved accuracy and robustness compared to existing models.

### Open Question 2
- Question: How can the robustness of LLMs be improved across different abilities and industrial sectors?
- Basis in paper: [explicit] The authors find that LLM robustness varies across abilities and industrial sectors, with local LLMs performing better on tasks requiring understanding of Chinese industrial terminology and global LLMs excelling in logical reasoning.
- Why unresolved: The paper identifies the differences in robustness but does not propose methods to improve robustness across all abilities and sectors.
- What evidence would resolve it: A study that develops and evaluates techniques to improve LLM robustness across all abilities and industrial sectors, such as fine-tuning on diverse datasets or incorporating domain-specific knowledge.

### Open Question 3
- Question: How can the efficiency, privacy, and security of LLMs be ensured for industrial applications?
- Basis in paper: [explicit] The authors mention that future work should investigate the efficiency, privacy, and security of LLMs in industrial applications, but do not provide specific solutions.
- Why unresolved: The paper highlights the importance of these factors but does not propose methods to address them.
- What evidence would resolve it: A study that develops and evaluates techniques to improve the efficiency, privacy, and security of LLMs for industrial applications, such as using federated learning, differential privacy, or homomorphic encryption.

## Limitations

- Specific questions and variants used for evaluation are not disclosed, making exact reproduction impossible
- Corpus evidence for key mechanisms is weak, with related papers focusing on general Chinese NLP rather than industrial domain evaluation
- No information provided about how "correctness" was determined for open-ended questions requiring human judgment

## Confidence

- **High confidence**: The methodology of using metamorphic testing for LLM robustness evaluation is sound and well-established
- **Medium confidence**: The finding that LLMs perform poorly on industrial domain questions is supported by the empirical results, though the exact questions remain unknown
- **Low confidence**: The specific mechanisms explaining why local LLMs underperform global ones (training data quality differences) lack direct supporting evidence in the corpus

## Next Checks

1. Replicate with public industrial datasets: Use publicly available Chinese industrial documentation to create a smaller test set and verify the general trend of low accuracy in industrial domains
2. Analyze LLM training data overlap: Investigate the training data sources of evaluated LLMs to quantify industrial domain coverage and validate the proposed mechanism about data quantity/quality differences
3. Test robustness on simplified variants: Create simplified metamorphic relations (e.g., basic synonym substitution) to verify that LLMs can handle simple transformations before testing complex logical variants