---
ver: rpa2
title: On the Convergence of Continual Federated Learning Using Incrementally Aggregated
  Gradients
arxiv_id: '2411.07959'
source_url: https://arxiv.org/abs/2411.07959
tags:
- learning
- forgetting
- data
- client
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes C-FLAG, a continual federated learning method\
  \ that combines memory-based replay with aggregated gradients to mitigate catastrophic\
  \ forgetting. It uses edge-based gradient updates on memory and aggregated gradients\
  \ on current data, achieving O(1/\u221AT) convergence rate."
---

# On the Convergence of Continual Federated Learning Using Incrementally Aggregated Gradients

## Quick Facts
- arXiv ID: 2411.07959
- Source URL: https://arxiv.org/abs/2411.07959
- Reference count: 40
- Key outcome: C-FLAG achieves O(1/√T) convergence with adaptive learning rates, outperforming baselines on accuracy and forgetting in both task and class-incremental settings

## Executive Summary
This paper introduces C-FLAG, a continual federated learning method that addresses catastrophic forgetting through memory-based replay combined with incrementally aggregated gradients. The approach achieves O(1/√T) convergence while maintaining strong performance across heterogeneous client data distributions. By adapting learning rates based on transference and interference metrics, C-FLAG effectively balances learning new tasks with retaining knowledge of previous ones, demonstrating significant improvements over state-of-the-art baselines in both task and class-incremental learning scenarios.

## Method Summary
C-FLAG is a continual federated learning algorithm that combines memory-based replay with incrementally aggregated gradients (IAG) to mitigate catastrophic forgetting. Each client maintains a local memory buffer storing representative samples from past tasks, enabling selective rehearsal while preserving privacy. The method uses edge-based gradient updates on memory data and aggregated gradients on current data, with adaptive learning rates computed per-client based on transference/interference metrics. The convergence analysis establishes O(1/√T) bounds under L-smoothness assumptions, and extensive experiments demonstrate superior performance across CIFAR-10, CIFAR-100, and TinyImageNet datasets under both IID and non-IID conditions.

## Key Results
- Achieves O(1/√T) convergence rate while mitigating catastrophic forgetting in federated settings
- Outperforms state-of-the-art baselines (Finetune-FL, EWC-FL, NCCL-FL, Erg-FL, FedTrack) on both task and class-incremental settings
- Demonstrates consistent performance improvements across varying data heterogeneity, client numbers, and memory sizes
- Successfully balances learning and forgetting through adaptive learning rates based on transference/interference metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: C-FLAG achieves O(1/√T) convergence by combining delayed gradient updates with adaptive learning rates
- Mechanism: The algorithm uses incrementally aggregated gradients (IAG) to reduce client drift and applies a single memory gradient step per epoch to guide learning, balancing forgetting and plasticity. Adaptive learning rates are computed per-client based on transference/interference metrics
- Core assumption: The gradient estimation error from IAG is bounded and the memory buffer provides representative samples of past tasks
- Evidence anchors:
  - [abstract] "We provide convergence analysis of the C-FLAG approach which addresses forgetting and bias while converging at a rate of O(1/√T) over T communication rounds"
  - [section] "Theorem 2... the sequence{xt}T generated by algorithm 1 with αt = α = 1/L(m+1) ∀t∈ {0,1,· · ·, T−1}... satisfies min_t E[∥∇f(x t)∥2] ≤ 2L(1+m)/T[F + T−1Σ_t=0 E[Γ(t)]]"
  - [corpus] Weak - no direct citations of IAG in CFL context; needs verification
- Break condition: If the memory buffer becomes too small or unrepresentative, the bias assumption fails and convergence degrades

### Mechanism 2
- Claim: Adaptive learning rates minimize catastrophic forgetting by trading off plasticity and stability
- Mechanism: Learning rates αt,i and βt,i are adjusted per-client based on Λt,i, a metric indicating transference (>0) or interference (≤0). This minimizes the forgetting term Γ(t) in the convergence bound
- Core assumption: The transference/interference metric Λt,i accurately captures the client's contribution to global forgetting
- Evidence anchors:
  - [abstract] "We formulate an optimization sub-problem that minimizes catastrophic forgetting, translating CFL into an iterative algorithm with adaptive learning rates that ensure seamless learning across tasks"
  - [section] "We derive adaptive rates by analyzing the average and the worst case individually... adaptive rates obtained after tackling the interference and transference are presented in Table 1"
  - [corpus] Weak - similar adaptive rate strategies exist in centralized CL but not in federated settings
- Break condition: If Λt,i is noisy or unstable across rounds, the adaptive rates may oscillate and hurt convergence

### Mechanism 3
- Claim: The replay memory buffer enables selective rehearsal without violating privacy constraints
- Mechanism: Each client stores a fixed-size subset of past task data locally. Training on this memory alongside current data provides gradient signals for past tasks while keeping raw data private
- Core assumption: Uniform sampling from past data yields unbiased gradient estimates on average
- Evidence anchors:
  - [abstract] "C-FLAG, a novel replay-memory based federated strategy consisting of edge-based gradient updates on memory and aggregated gradients on the current data"
  - [section] "Episodic memory Mi consisting of a fixed-size buffer of size at most m0 stores a subset of the data that arrives prior to the start of the current task (t = 0) at the i-th client"
  - [corpus] Weak - limited literature on federated replay memory; most works focus on centralized replay
- Break condition: If memory size is too small relative to task complexity, the buffer becomes unrepresentative and bias increases

## Foundational Learning

- Concept: L-smoothness and non-convex optimization
  - Why needed here: The convergence analysis relies on L-smoothness to bound gradient differences and ensure stable updates in the non-convex setting
  - Quick check question: What is the maximum ratio between ∥∇f(x₁) - ∇f(x₂)∥ and ∥x₁ - x₂∥ under L-smoothness?

- Concept: Catastrophic forgetting and transference/interference
  - Why needed here: Understanding these concepts is essential to grasp how adaptive rates mitigate forgetting by balancing learning from new tasks and retaining old knowledge
  - Quick check question: What distinguishes transference from interference in the context of continual learning?

- Concept: Federated learning with heterogeneous data
  - Why needed here: The method operates in a non-IID federated setting, so understanding data heterogeneity and its impact on convergence is critical
  - Quick check question: How does Dirichlet partitioning with ζ control the degree of non-IIDness among clients?

## Architecture Onboarding

- Component map: Edge clients -> Central server -> Edge clients
- Critical path:
  1. Server broadcasts current model x_t to all clients
  2. Each client computes ∇gi(xt) and ∇f†i(xt) and sends to server
  3. Server computes and broadcasts ∇g(xt) and ∇f†(xt)
  4. Each client performs E local updates using IAG and memory gradients
  5. Each client applies adaptive rate logic and sends update to server
  6. Server aggregates and updates global model x_{t+1}
- Design tradeoffs:
  - Memory size vs. forgetting: Larger memory reduces forgetting but increases storage cost
  - E (local epochs) vs. convergence speed: Larger E reduces communication rounds but may increase client drift
  - Adaptive rate computation vs. privacy: Requires sending Λt,i metrics; could be approximated locally
- Failure signatures:
  - Accuracy plateaus early: Memory buffer too small or unrepresentative
  - Accuracy degrades over tasks: Adaptive rates not suppressing Γ(t) sufficiently
  - High variance in client updates: IAG gradient estimation error too large
- First 3 experiments:
  1. Run C-FLAG on Split-CIFAR10 with IID data, compare accuracy and forgetting against FedAvg baseline
  2. Vary memory size (m0) on non-IID Split-CIFAR10, measure impact on average accuracy and forgetting
  3. Enable/disable adaptive learning rates, compare forgetting term Γ(t) evolution across tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed adaptive learning rate mechanism (AdapLR) behave in extreme heterogeneity scenarios, such as highly imbalanced data distribution across clients?
- Basis in paper: Explicit discussion of data heterogeneity effects in Section 7 and analysis of learning rate adaptation in Section 5
- Why unresolved: The paper provides ablation studies on varying heterogeneity levels but doesn't specifically analyze extreme cases where data imbalance could affect the adaptive rates' effectiveness
- What evidence would resolve it: Experiments showing performance degradation or adaptation failure in highly imbalanced scenarios, or theoretical analysis of AdapLR's stability bounds under extreme heterogeneity

### Open Question 2
- Question: What is the impact of using different memory sampling strategies (e.g., random vs. exemplar-based) on the convergence rate and forgetting mitigation in C-FLAG?
- Basis in paper: Explicit mention of memory buffer usage and sampling in Section 2, but no comparison of different sampling strategies
- Why unresolved: The paper assumes a fixed sampling approach without exploring alternatives that might improve performance
- What evidence would resolve it: Comparative experiments using different sampling strategies (e.g., reservoir sampling, k-center greedy) and their effects on convergence speed and forgetting metrics

### Open Question 3
- Question: How does the performance of C-FLAG scale with increasing number of tasks and classes beyond the datasets used in the experiments?
- Basis in paper: Explicit mention of scalability in Section 6 and experiments on Split-CIFAR10, Split-CIFAR100, and Split-TinyImageNet
- Why unresolved: The paper doesn't test the method on datasets with significantly more tasks or classes to evaluate scalability limits
- What evidence would resolve it: Experiments on larger-scale datasets (e.g., Split-ImageNet, Split-CIFAR-10M) showing performance trends as task/class count increases

## Limitations

- Convergence analysis relies on idealized assumptions about gradient smoothness and bounded memory gradient error that may not hold in practice
- Memory buffer approach increases client storage requirements and may not scale to larger task sequences or more complex models
- Adaptive learning rate mechanism depends on the accuracy of transference/interference metric Λt,i, which could be unstable in dynamic federated environments

## Confidence

- Convergence analysis (O(1/√T)): Medium - Proof provided but relies on idealized assumptions
- Adaptive learning rates reducing forgetting: Medium - Theoretically justified but metric sensitivity unclear
- Empirical superiority over baselines: High - Extensive experiments across multiple datasets and settings
- Practical scalability: Low - Memory requirements and communication overhead not thoroughly evaluated

## Next Checks

1. Test C-FLAG's convergence under extreme non-IID conditions (ζ→0) to verify the robustness of the O(1/√T) rate
2. Conduct ablation studies varying memory buffer sizes and sampling strategies to quantify their impact on forgetting vs. storage cost
3. Evaluate the sensitivity of adaptive learning rates to noisy Λt,i measurements by introducing client heterogeneity in data quality