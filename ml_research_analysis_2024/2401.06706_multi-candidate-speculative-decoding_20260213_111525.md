---
ver: rpa2
title: Multi-Candidate Speculative Decoding
arxiv_id: '2401.06706'
source_url: https://arxiv.org/abs/2401.06706
tags:
- draft
- acceptance
- sampling
- target
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multi-Candidate Speculative Decoding (MCSD)
  to accelerate large language model inference. The method samples multiple candidate
  tokens from a draft model at each decoding step and organizes them in batches for
  parallel verification by the target model.
---

# Multi-Candidate Speculative Decoding

## Quick Facts
- arXiv ID: 2401.06706
- Source URL: https://arxiv.org/abs/2401.06706
- Authors: Sen Yang; Shujian Huang; Xinyu Dai; Jiajun Chen
- Reference count: 21
- Key outcome: MCSD improves acceptance rates and wall-clock speedups compared to standard speculative decoding

## Executive Summary
This paper introduces Multi-Candidate Speculative Decoding (MCSD) to accelerate large language model inference by sampling multiple candidate tokens from a draft model at each decoding step and verifying them in parallel using the target model. The approach significantly improves acceptance rates while maintaining the target model's output distribution through carefully designed verification algorithms. Tree Attention is employed to reduce communication overhead by allowing candidates to share cached keys and values. Experiments demonstrate substantial performance improvements across multiple datasets and model configurations.

## Method Summary
MCSD extends standard speculative decoding by generating k candidate tokens at each position rather than a single token. These candidates are organized in batches for parallel verification by the target model using Tree Attention to share cached computations. The method includes two variants: with and without replacement sampling. The verification algorithms maintain the target model's output distribution while improving acceptance rates. The approach requires no changes to the target model and works with existing speculative decoding infrastructure.

## Key Results
- MCSD achieves significantly higher acceptance rates than standard speculative decoding across all tested configurations
- Wall-clock speedups improve proportionally with acceptance rate gains, demonstrating practical efficiency benefits
- Tree Attention effectively reduces communication overhead, enabling efficient parallel verification of multiple candidates
- The method works across different model sizes (LLaMA-13B, 33B) and datasets (Alpaca, WMT)

## Why This Works (Mechanism)

### Mechanism 1
Sampling multiple candidate tokens per step increases the probability of matching the target model's preferred token, thereby improving acceptance rates. The draft model generates k independent tokens at each position, and the target model evaluates all k tokens in parallel. By increasing k, the likelihood that at least one token matches the target model's top choice increases, even if the draft model's overall distribution doesn't perfectly align with the target. Core assumption: The draft model's output distribution overlaps sufficiently with the target model's distribution that multiple samples increase the probability of a match.

### Mechanism 2
The multi-candidate speculative sampling algorithm preserves the target model's output distribution while allowing multiple candidates to be verified efficiently. Instead of naive sampling where each candidate is independently accepted with probability min(1, p/q), the algorithm maintains the residual distribution after each rejection, ensuring that the final output distribution matches the target model's p(x). Core assumption: The sequential verification process with residual distribution updates maintains the correct marginal distribution.

### Mechanism 3
Tree Attention reduces communication overhead by allowing multiple candidates to share cached keys and values from the prefix tokens. Instead of duplicating cached keys and values for each candidate sequence, Tree Attention arranges candidates in a single sequence with a special attention mask that preserves causal relationships while allowing candidates to share prefix computations. Core assumption: The attention mask correctly isolates candidates while allowing them to share prefix computations, and the additional computation overhead is negligible compared to communication savings.

## Foundational Learning

- Concept: Speculative decoding and its acceptance rate formula
  - Why needed here: Understanding how standard speculative decoding works and why acceptance rate is the key metric is essential for grasping why multi-candidate approaches improve performance.
  - Quick check question: What is the acceptance rate formula for standard speculative decoding, and how does it relate to the agreement between draft and target model distributions?

- Concept: Probability theory and distribution preservation
  - Why needed here: The proof that multi-candidate sampling preserves the target distribution relies on understanding how sequential probability updates work and how residual distributions are calculated.
  - Quick check question: How does the residual distribution update in Algorithm 1 ensure that the final output distribution matches p(x)?

- Concept: Transformer attention mechanism and caching
  - Why needed here: Tree Attention's effectiveness depends on understanding how cached keys and values work in autoregressive generation and why communication overhead becomes a bottleneck.
  - Quick check question: Why can cached keys and values be reused across multiple candidate sequences in the same decoding step?

## Architecture Onboarding

- Component map: Draft model -> Multi-candidate sampling algorithm -> Tree Attention module -> Target model verification -> Output distribution

- Critical path:
  1. Draft model generates k tokens at current position
  2. All k tokens passed to target model with shared prefix cache
  3. Tree Attention computes attention for all candidates efficiently
  4. Multi-candidate speculative sampling algorithm verifies candidates sequentially
  5. If one candidate accepted, move to next position; if all rejected, sample from residual distribution

- Design tradeoffs:
  - Larger k improves acceptance rate but increases target model computation and memory usage
  - Without replacement sampling reduces collisions but adds complexity to the algorithm
  - Tree Attention saves communication but adds computation overhead proportional to sequence length
  - Balance between draft model size, k value, and target model batch size for optimal performance

- Failure signatures:
  - Low acceptance rates despite high k values suggest poor draft-target alignment
  - Memory overflow errors indicate k is too large for available GPU memory
  - Slow performance despite high acceptance rates suggests Tree Attention implementation issues or communication overhead not properly optimized
  - Distribution mismatch detected through statistical tests on generated outputs

- First 3 experiments:
  1. Run standard speculative decoding vs k=2 multi-candidate version on Alpaca dataset with LLaMA-13B target and LLaMA-68M draft; measure acceptance rate and wall-clock speedup
  2. Test Tree Attention implementation by comparing performance with and without Tree Attention for k=4 candidates; verify communication overhead reduction
  3. Evaluate without replacement sampling by comparing acceptance rates for k=8 with replacement vs without replacement on WMT dataset; measure any computational overhead differences

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important questions arise from the work:

1. How does the effectiveness of MCSD vary across different model architectures beyond the Transformer, such as State Space Models or Mamba?

2. What is the optimal strategy for dynamically adjusting k and Î³ during inference based on the complexity of the input or the agreement between draft and target models?

3. What is the theoretical upper bound on the acceptance rate improvement achievable by MCSD as the number of candidates (k) approaches infinity?

## Limitations

- The implementation details of the multi-candidate verification algorithms are not fully specified in the main text, requiring reference to the appendix
- The computational overhead of Tree Attention is claimed to be minimal but not quantitatively characterized
- The paper focuses on acceptance rate and wall-clock speedup without detailed analysis of memory usage patterns or energy efficiency
- Performance gains may not generalize to all use cases or model architectures beyond Transformers

## Confidence

**High Confidence:** The core mechanism of sampling multiple candidates to improve acceptance rates is well-established in the literature and mathematically sound.

**Medium Confidence:** The claim that the multi-candidate verification algorithms preserve the target model's output distribution appears correct based on the provided proofs, but practical implementation details could affect this guarantee.

**Low Confidence:** The practical performance gains across different model sizes and datasets are presented but may not generalize to all use cases without further validation.

## Next Checks

1. Implement a statistical test to verify that the output distribution from MCSD with k=8 matches the target model's distribution for a held-out validation set. Compare against standard speculative decoding using KL divergence or other distributional similarity metrics.

2. Measure GPU memory consumption for MCSD with different k values (2, 4, 8, 16) on the same hardware setup described in the paper. Identify the memory scaling pattern and determine the practical upper limit for k based on hardware constraints.

3. Profile the actual communication costs saved by Tree Attention by comparing with a baseline implementation that uses separate attention computations for each candidate. Measure the reduction in inter-GPU communication time and verify it scales with prefix length as claimed.