---
ver: rpa2
title: American Sign Language to Text Translation using Transformer and Seq2Seq with
  LSTM
arxiv_id: '2409.10874'
source_url: https://arxiv.org/abs/2409.10874
tags:
- transformer
- sign
- language
- translation
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares Transformer and Seq2Seq with LSTM models for
  translating American Sign Language to text. A dataset of ASL landmarks and corresponding
  phrases was used, with models trained using deep learning architectures.
---

# American Sign Language to Text Translation using Transformer and Seq2Seq with LSTM

## Quick Facts
- arXiv ID: 2409.10874
- Source URL: https://arxiv.org/abs/2409.10874
- Reference count: 37
- Transformer achieved 28.14% higher BLEU score (0.8501 vs 0.5687) than Seq2Seq with LSTM

## Executive Summary
This study compares Transformer and Seq2Seq with LSTM models for translating American Sign Language to text. Using a dataset of ASL landmarks and corresponding phrases, the researchers trained deep learning architectures to perform character-level translation. The Transformer model significantly outperformed Seq2Seq, achieving a 28.14% higher BLEU score (0.8501 vs 0.5687) and a lower word error rate (37.62% vs 74.56%). The study also found that adding Residual LSTM to Transformer degraded performance by 23.37% in BLEU score, suggesting this enhancement may not benefit ASL-to-text translation. Results highlight Transformer's effectiveness while identifying opportunities for improvement in handling longer and low-frequency phrases.

## Method Summary
The study uses the American Sign Language Recognition Dataset containing 52,958 samples with face, right-hand, and left-hand landmarks extracted via MediaPipe. Data is split 80% training, 20% validation, with 2,000 validation samples reserved for testing. Three models are implemented: Seq2Seq with LSTM (PyTorch), Transformer (TensorFlow), and Transformer with Residual LSTM. All models use character-level tokenization, categorical cross-entropy loss, Adam optimizer with learning rate 0.001, batch size 64, and 50 epochs of training on a P100 GPU. Performance is evaluated using BLEU Score and Word Error Rate (WER).

## Key Results
- Transformer model achieved BLEU score of 0.8501 and WER of 37.62%
- Seq2Seq with LSTM achieved BLEU score of 0.5687 and WER of 74.56%
- Adding Residual LSTM to Transformer degraded performance by 23.37% in BLEU score

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer architecture captures long-range dependencies in ASL landmark sequences better than Seq2Seq+LSTM
- Mechanism: Self-attention in Transformer allows each landmark frame to directly attend to all other frames, enabling more flexible context modeling across long sequences
- Core assumption: The ASL landmark sequence contains important contextual information spread across distant frames that sequential models struggle to capture
- Evidence anchors:
  - [abstract] "Transformer model outperformed Seq2Seq, achieving a 28.14% higher BLEU score (0.8501 vs 0.5687)"
  - [section] "The Transformer assigns different levels of importance to each input using a self-attention mechanism"
- Break condition: If attention weights become too uniform or if the sequence length exceeds the model's maximum positional encoding capacity

### Mechanism 2
- Claim: Residual connections in ResidualLSTM prevent gradient vanishing in deep architectures
- Mechanism: Identity skip connections allow gradients to flow directly through the network, maintaining signal strength in deeper layers
- Core assumption: Deep LSTM networks suffer from gradient degradation that impairs learning in later layers
- Evidence anchors:
  - [abstract] "Adding Residual LSTM to Transformer degraded performance by 23.37% in BLEU score, suggesting it may not benefit this task"
  - [section] "Residual LSTM overcomes the challenge of gradient loss in deep LSTM by providing information through two paths"
- Break condition: If the added complexity from ResidualLSTM outweighs its benefits for this specific task, or if the original architecture already handles gradients adequately

### Mechanism 3
- Claim: Character-level tokenization improves handling of out-of-vocabulary words in ASL-to-text translation
- Mechanism: Breaking words into characters creates a fixed vocabulary size regardless of the target language vocabulary, preventing unknown word issues
- Core assumption: ASL fingerspelling represents proper nouns and unknown concepts that may not appear in training data
- Evidence anchors:
  - [abstract] "the translation model uses the character level to overcome the out-of-vocabulary problem"
  - [section] "The model evaluation results show that the Transformer model is better than the other two"
- Break condition: If character-level modeling increases sequence length too much or if word-level semantics are critical for translation quality

## Foundational Learning

- Concept: Self-attention mechanism
  - Why needed here: Enables the model to weigh the importance of different landmark frames when translating ASL to text
  - Quick check question: How does self-attention differ from traditional sequential processing in RNNs?

- Concept: Residual connections
  - Why needed here: Allows training of deeper networks by preventing gradient vanishing, though in this case it may not improve performance
  - Quick check question: What problem do residual connections solve in deep neural networks?

- Concept: BLEU score calculation
  - Why needed here: Provides the primary evaluation metric for comparing translation quality between models
  - Quick check question: What components make up the BLEU score formula?

## Architecture Onboarding

- Component map:
  Input: ASL landmark sequences (face, right-hand, left-hand coordinates)
  Landmark Embedding: CNN layers to extract spatial features from MediaPipe landmarks
  Transformer Encoder: Multi-head self-attention with positional encoding
  Transformer Decoder: Causal attention mask with cross-attention to encoder
  Output: Character-level predictions with start/end/pad tokens

- Critical path: Landmark embedding → Transformer encoder → Transformer decoder → Character prediction

- Design tradeoffs:
  Character vs word-level tokenization: Character level handles OOV but increases sequence length
  Encoder depth: 5 layers chosen for balance between capacity and training stability
  ResidualLSTM addition: Experimented but degraded performance, suggesting self-attention is sufficient

- Failure signatures:
  Low BLEU scores with high WER indicate poor translation quality
  Training loss plateau suggests optimization issues or capacity problems
  Attention weight patterns showing uniform distribution indicate ineffective self-attention

- First 3 experiments:
  1. Compare Transformer with different numbers of encoder layers (3, 5, 7) to find optimal depth
  2. Test word-level vs character-level tokenization on a subset of the data
  3. Evaluate different positional encoding methods (sinusoidal vs learned) for the Transformer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does adding Residual LSTM to the Transformer model improve performance for longer sign language sequences (over 37 characters)?
- Basis in paper: [explicit] The paper states that the Transformer model excels for phrases of 21-37 characters, but long-sequence translations still pose challenges, and Residual LSTM addition degraded performance by 23.37% in BLEU score.
- Why unresolved: The study did not specifically test Residual LSTM's impact on longer sequences, only showing overall degradation.
- What evidence would resolve it: Testing Residual LSTM with Transformer on sign language sequences longer than 37 characters and comparing performance metrics.

### Open Question 2
- Question: Would incorporating word-level embedding instead of character-level embedding improve the model's ability to recognize complete words in sign language translation?
- Basis in paper: [inferred] The paper mentions that the translation model uses character-level embedding to overcome the out-of-vocabulary problem, but this prevents recognizing complete words, leading to translation errors.
- Why unresolved: The study only tested character-level embedding and did not explore word-level alternatives.
- What evidence would resolve it: Implementing and testing a word-level embedding approach and comparing translation accuracy with the current character-level model.

### Open Question 3
- Question: How would the Transformer model perform on unstructured sign languages like BISINDO compared to structured languages like ASL?
- Basis in paper: [explicit] The paper concludes by suggesting that Transformer could be tested for unstructured sign languages at the word level and other sign languages like BISINDO.
- Why unresolved: The study only tested the model on ASL and did not explore other sign languages.
- What evidence would resolve it: Training and testing the Transformer model on BISINDO or other unstructured sign languages and comparing performance with ASL results.

## Limitations

- Dataset composition and speaker diversity are not fully detailed, limiting generalizability of results
- Character-level tokenization may obscure word-level semantic understanding important for translation quality
- Comparison uses different frameworks (PyTorch vs TensorFlow) which may introduce confounding factors

## Confidence

- High Confidence: Transformer architecture outperforms Seq2Seq+LSTM for ASL-to-text translation (28.14% BLEU improvement, 37.62% vs 74.56% WER)
- Medium Confidence: Character-level tokenization effectively handles out-of-vocabulary words in ASL translation
- Low Confidence: ResidualLSTM connections are detrimental to performance; the specific mechanism and whether this generalizes to other architectures remains unclear

## Next Checks

1. **Ablation Study on Transformer Components**: Systematically remove or modify individual Transformer components (self-attention, positional encoding, feed-forward networks) to quantify their individual contributions to the performance improvement over Seq2Seq.

2. **Cross-Validation Across Diverse ASL Datasets**: Test the best-performing Transformer model on multiple ASL datasets with different signers, signing styles, and phrase distributions to assess generalizability beyond the single dataset used in this study.

3. **Hybrid Tokenization Evaluation**: Implement a hybrid word-character tokenization approach that uses word-level tokens when vocabulary coverage is sufficient but falls back to character-level for out-of-vocabulary words, comparing performance against pure character-level and word-level approaches.