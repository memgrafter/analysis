---
ver: rpa2
title: 'Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation'
arxiv_id: '2401.06591'
source_url: https://arxiv.org/abs/2401.06591
tags:
- score
- image
- prometheus
- response
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Prometheus-Vision, a vision-language model
  evaluator that uses customized rubrics to assess responses based on fine-grained
  criteria. The authors constructed the Perception Collection, a dataset of 15K detailed
  rubrics, to train the evaluator.
---

# Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation

## Quick Facts
- **arXiv ID**: 2401.06591
- **Source URL**: https://arxiv.org/abs/2401.06591
- **Reference count**: 40
- **Primary result**: Prometheus-Vision achieves high correlation with human evaluators and GPT-4V for VLM assessment using customized rubrics

## Executive Summary
Prometheus-Vision introduces a vision-language model evaluator trained on customized rubrics to assess responses based on fine-grained criteria. The authors constructed the Perception Collection, a dataset of 15K detailed rubrics, to train the evaluator. Prometheus-Vision demonstrates strong correlation with human evaluators and GPT-4V, excelling at pinpointing deficiencies in responses and explaining reasoning behind scores. This open-source approach offers an accessible alternative for evaluating vision-language models based on user-defined criteria.

## Method Summary
The method involves fine-tuning LLaVA-1.5 (7B & 13B) using the Perception Collection dataset to create Prometheus-Vision. The model is trained to generate detailed language feedback explaining strengths and weaknesses before producing a score (1-5). Evaluation uses benchmarks including LLaVA-Bench, Visit-Bench, Perception-Bench, OKVQA, VQA v2, TextVQA, COCO-Captions, and No Caps. Performance is measured through Pearson, Kendall-Tau, and Spearman correlations between human evaluators and evaluator VLMs, as well as Pairwise Preference Win-rate for feedback quality comparison.

## Key Results
- Prometheus-Vision shows highest Pearson correlation with human evaluators and GPT-4V among open-source models
- The model excels at pinpointing differences between parody artwork and original masterpieces
- Human evaluators determine Prometheus-Vision's feedback is better or as good as GPT-4V's feedback 57.78% of the time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prometheus-Vision's high correlation with human evaluators stems from its fine-tuning on Perception Collection, which includes 15K customized rubrics that mirror real-world evaluation criteria.
- Mechanism: The model learns to map specific rubric descriptions to scoring decisions through supervised learning on diverse, fine-grained evaluation criteria.
- Core assumption: The Perception Collection rubrics accurately represent what humans care about when evaluating VLM responses.
- Evidence anchors:
  - [abstract] "Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models"
  - [section] "We construct a comprehensive multi-modal feedback dataset called the Perception Collection"
  - [corpus] Weak evidence - corpus neighbors don't discuss evaluation rubrics specifically
- Break condition: If real-world evaluation criteria diverge significantly from those in Perception Collection, the model's human correlation would drop.

### Mechanism 2
- Claim: The Chain-of-Thought fine-tuning approach enables Prometheus-Vision to generate interpretable feedback before producing scores.
- Mechanism: The model is trained to first generate detailed language feedback explaining strengths and weaknesses, then append a score decision, creating an interpretable evaluation pipeline.
- Core assumption: Generating feedback before the score helps the model reason about evaluation more systematically.
- Evidence anchors:
  - [abstract] "Prometheus-Vision could evaluate based on the given criteria, pinpointing the differences between the parody artwork and the original masterpiece"
  - [section] "Training on the Perception Collection is analogous to Chain-of-Thought fine-tuning which requires generating a rationale (which is the feedback in our case) and then the score in a sequential manner"
  - [corpus] No direct corpus evidence for Chain-of-Thought in VLM evaluation
- Break condition: If the model learns to generate generic feedback that doesn't connect to the specific rubric, the interpretability benefit would be lost.

### Mechanism 3
- Claim: Prometheus-Vision avoids length bias by being trained in an absolute grading setting rather than a comparative setting.
- Mechanism: The training data includes responses of similar lengths across all score categories, preventing the model from learning to favor longer responses.
- Core assumption: Training data diversity in response length per score category prevents length bias during evaluation.
- Evidence anchors:
  - [abstract] "we ensure that there is no length bias (i.e., giving a higher score for longer responses)"
  - [section] "To nullify the tendency of recent LMs to give higher scores to longer responses... we aim to maintain similar length of responses across the score range"
  - [corpus] No corpus evidence about length bias in evaluation models
- Break condition: If evaluation scenarios involve significantly different response length distributions than training data, length bias could re-emerge.

## Foundational Learning

- Concept: Vision-Language Model (VLM) architecture and training
  - Why needed here: Understanding how VLMs process both visual and textual inputs is crucial for grasping why Prometheus-Vision needs specialized training
  - Quick check question: How does a VLM differ from separately trained vision and language models?

- Concept: Fine-grained evaluation criteria and rubrics
  - Why needed here: The Perception Collection uses detailed rubrics rather than coarse-grained metrics, which is central to the approach
  - Quick check question: What distinguishes fine-grained evaluation criteria from traditional metrics like BLEU or ROUGE?

- Concept: Chain-of-Thought reasoning in model training
  - Why needed here: Prometheus-Vision uses a similar approach to generate feedback before scoring
  - Quick check question: How does generating intermediate reasoning steps (like feedback) before final outputs improve model performance?

## Architecture Onboarding

- Component map: Image + instruction + response + rubric + reference answer → visual processing → text processing → alignment network → feedback generation → score decision
- Critical path: Image + instruction + response + rubric → visual processing → text processing → alignment network → feedback generation → score decision
- Design tradeoffs:
  - Using frozen backbone vs. full fine-tuning (faster training, but limited architecture changes)
  - Absolute scoring vs. comparative evaluation (simpler but may miss relative quality)
  - Including reference answers in input vs. not (helps calibration but adds complexity)
- Failure signatures:
  - Low correlation with human evaluators despite high training performance (overfitting to rubric patterns)
  - Consistent bias toward longer responses (length bias)
  - Generic feedback that doesn't connect to specific rubric criteria (feedback generation failure)
- First 3 experiments:
  1. Evaluate Prometheus-Vision on a held-out subset of Perception Collection with known human scores to verify correlation
  2. Test for length bias by comparing scores across responses of varying lengths with identical content quality
  3. Ablation study: Remove feedback generation step to see if Chain-of-Thought approach actually improves scoring accuracy

## Open Questions the Paper Calls Out
The paper identifies several limitations and calls out specific areas for future work:
1. The work does not consider cases when images generated by image generation models are given as input.
2. The model's performance on evaluating responses generated from AI-generated images versus real-world images is unknown.
3. The impact of adding more diversity to the training data, particularly including more text-rich images, on performance is unexplored.

## Limitations
- Prometheus-Vision's performance depends heavily on the quality and representativeness of the Perception Collection dataset
- The model's correlation with human evaluators doesn't necessarily translate to optimal evaluation for all downstream VLM applications
- The approach may introduce systematic biases through hand-crafted seed rubrics and expansion process

## Confidence

- **High Confidence**: The claim that Prometheus-Vision achieves high correlation with human evaluators (Pearson correlation metrics provided)
- **Medium Confidence**: The assertion that Chain-of-Thought fine-tuning improves interpretability and scoring accuracy (mechanism described but not extensively validated)
- **Low Confidence**: The claim about avoiding length bias through absolute grading (limited evidence of effectiveness across diverse response distributions)

## Next Checks

1. **Cross-Domain Validation**: Test Prometheus-Vision's performance on domain-specific evaluation tasks outside the Perception Collection (e.g., medical imaging, scientific visualization) to assess generalizability of the rubric-based approach.

2. **Longitudinal Stability**: Evaluate whether Prometheus-Vision maintains consistent scoring patterns over time as VLMs evolve and response styles change, particularly for criteria that may become outdated.

3. **Human-Machine Agreement Analysis**: Conduct detailed error analysis comparing Prometheus-Vision disagreements with human evaluators to identify systematic biases or blind spots in the rubric-based approach.