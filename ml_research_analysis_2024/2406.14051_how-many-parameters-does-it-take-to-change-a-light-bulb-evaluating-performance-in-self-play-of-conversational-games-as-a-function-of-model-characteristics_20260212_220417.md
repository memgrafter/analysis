---
ver: rpa2
title: How Many Parameters Does it Take to Change a Light Bulb? Evaluating Performance
  in Self-Play of Conversational Games as a Function of Model Characteristics
arxiv_id: '2406.14051'
source_url: https://arxiv.org/abs/2406.14051
tags:
- performance
- huggingface
- https
- corr
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates what model characteristics drive performance
  on a conversational game benchmark (clembench) that evaluates goal-directed agentive
  capabilities via self-play. It finds that while model size (number of parameters)
  shows a clear positive correlation with performance, there is significant variance
  within size brackets that can be attributed to training factors such as instruction-tuning
  data quality and method.
---

# How Many Parameters Does it Take to Change a Light Bulb? Evaluating Performance in Self-Play of Conversational Games as a Function of Model Characteristics

## Quick Facts
- arXiv ID: 2406.14051
- Source URL: https://arxiv.org/abs/2406.14051
- Authors: Nidhir Bhavsar; Jonathan Jordan; Sherzod Hakimov; David Schlangen
- Reference count: 12
- One-line primary result: Model size correlates with performance on conversational game benchmarks, but fine-tuning data quality and inference implementation details create significant variance within size brackets.

## Executive Summary
This paper investigates the relationship between model characteristics and performance on conversational game benchmarks using the clembench framework. Through systematic evaluation of diverse models, it demonstrates that while larger models generally perform better, the quality of instruction-tuning data and inference implementation details significantly impact results. The study reveals that even small models can achieve high scores with appropriate fine-tuning, and that quantization to 8-bit has negligible performance impact, making it a practical option for deployment.

## Method Summary
The study evaluates performance on the clembench benchmark through self-play of conversational games, measuring both format compliance and game play quality. Models are tested across different access methods (APIs vs local inference), quantization levels, and fine-tuning approaches. Performance is analyzed as a function of model size, training data composition, and implementation details, with statistical analysis of the relationships between these factors and benchmark scores.

## Key Results
- Model size shows a clear positive correlation with performance (R² = 0.57, p < 0.000)
- 8-bit quantization has negligible impact on performance across all tested models
- Different access methods (APIs vs local inference) yield ~5 point performance differences due to unexposed sampling parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model size directly correlates with performance on conversational game benchmarks.
- Mechanism: Larger models have more parameters that can capture complex patterns and reasoning capabilities needed for goal-directed tasks.
- Core assumption: The relationship between parameters and performance follows a power law as established in scaling laws literature.
- Evidence anchors:
  - [abstract]: "while there is a clear relationship between number of parameters and performance"
  - [section]: "Figure 3 plots (aggregated) score on clembench versus size of the model... a linear(-log) regression model finds a significant regression equation (F (1, 18) = 23 .56, p < 0.000), with an R2 of .57)"
  - [corpus]: Weak - no direct evidence about parameter-size correlation in corpus
- Break condition: When the relationship plateaus or shows diminishing returns at extremely large sizes.

### Mechanism 2
- Claim: Instruction-tuning data quality and method significantly impact performance beyond model size.
- Mechanism: Fine-tuning with appropriate data mixes (chat + reasoning) enables models to better follow instructions and perform agentive tasks.
- Core assumption: Different fine-tuning approaches (PPO, C-RLFT, DPO) have distinct effects on capability development.
- Evidence anchors:
  - [abstract]: "there is still a wide spread of performance points within a given size bracket, which is to be accounted for by training parameters such as fine-tuning data quality and method"
  - [section]: "we find that the quality of the instruction-tuning data mix also appears to profoundly impact performance, as models of the same magnitude show a wide spread of performance scores"
  - [corpus]: Weak - no direct evidence about instruction-tuning effects in corpus
- Break condition: When data quality becomes uniformly high across all models.

### Mechanism 3
- Claim: Inference implementation details affect benchmark performance through unexposed sampling parameters.
- Mechanism: Different access methods (APIs vs local inference) use different default sampling parameters that influence output quality.
- Core assumption: Sampling parameters like temperature and repetition penalties significantly affect conversational agent performance.
- Evidence anchors:
  - [abstract]: "different access methods (APIs vs local inference) yield performance differences of around 5 points, likely due to unexposed sampling parameters"
  - [section]: "not all sampling parameters are exposed by these APIs, and hence, the actually used settings might differ"
  - [corpus]: Weak - no direct evidence about inference parameter effects in corpus
- Break condition: When all sampling parameters become fully exposed and controllable.

## Foundational Learning

- Concept: Markov Decision Process formulation of dialogue games
  - Why needed here: Understanding the decision problem framework helps analyze what capabilities models need to succeed
  - Quick check question: What are the four components (S, A, P, R) of a Markov Decision Process in the context of conversational games?

- Concept: Self-play evaluation methodology
  - Why needed here: The benchmark measures performance through automated agent interactions, not human evaluation
  - Quick check question: How does clembench measure both format instruction following and game play quality separately?

- Concept: Quantization impact on model performance
  - Why needed here: Understanding weight compression effects is crucial for practical deployment decisions
  - Quick check question: What quantization level shows negligible performance impact according to the results?

## Architecture Onboarding

- Component map: Model interface (d, Θ) → clembench evaluation → score vector (format compliance, quality, combined)
- Critical path: Model weights → inference implementation → output formatting → automated scoring
- Design tradeoffs: Model size vs. inference cost vs. quantization quality
- Failure signatures: 
  - Format compliance failures: 0% played score
  - Quality failures: Low quality score despite high played percentage
  - Implementation issues: Inconsistent scores across access methods
- First 3 experiments:
  1. Compare same model across different inference backends (API vs local)
  2. Test quantization impact on your target model size
  3. Evaluate different instruction-tuning data mixes on a base model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the quality of fine-tuning data have a more significant impact on performance than model size for agentive tasks?
- Basis in paper: [explicit] The paper states "even small models can achieve comparatively high scores...provided that the right combination is found" and "the quality of the instruction-tuning data mix also appears to profoundly impact performance."
- Why unresolved: The paper only provides correlational evidence between data quality and performance, without controlled experiments varying data quality independently of model size.
- What evidence would resolve it: Controlled experiments training identical base models on varying qualities of fine-tuning data while keeping other factors constant.

### Open Question 2
- Question: How do different inference implementation details (sampling parameters, turn-taking mechanisms) affect performance on interactive benchmarks like clembench?
- Basis in paper: [explicit] The paper finds "a certain degree of unpredictability about performance across access methods, possible due to unexposed sampling parameters" and notes differences in end-of-turn token handling.
- Why unresolved: Many inference parameters are not exposed by commercial APIs, making it impossible to systematically test their effects.
- What evidence would resolve it: Access to full inference parameter control across all model access methods, allowing systematic variation of sampling and generation parameters.

### Open Question 3
- Question: Is there an optimal ratio of "chat" to "reasoning" data in fine-tuning mixtures for maximizing performance on agentive tasks?
- Basis in paper: [explicit] The paper finds "a good mix between 'chat' and 'reasoning' data in the fine-tuning data to be beneficial" and notes performance differences across models with different data mixes.
- Why unresolved: The paper only examines a few specific data mixes and doesn't systematically explore the parameter space of different ratios.
- What evidence would resolve it: Systematic experiments training models on fine-tuning data with varying ratios of chat to reasoning data while measuring performance on agentive benchmarks.

## Limitations

- Proprietary training data for many models prevents full attribution of performance differences
- API vs local inference comparison is confounded by unexposed sampling parameters
- Benchmark results may not generalize beyond conversational game tasks

## Confidence

**High Confidence**:
- Model size shows significant positive correlation with performance (R² = 0.57, p < 0.000)
- Quantization to 8-bit shows negligible performance impact across all tested models
- Format compliance (percentage played) is strongly bimodal, separating models that follow instructions from those that don't

**Medium Confidence**:
- Instruction-tuning data quality significantly impacts performance beyond model size
- Different fine-tuning methods (PPO, C-RLFT, DPO) produce distinct capability profiles
- API access methods show consistent ~5 point performance differences compared to local inference

**Low Confidence**:
- Specific data mix compositions (chat vs reasoning) are optimal for different model sizes
- Diminishing returns on model size follow a power law relationship
- Inference parameter differences fully explain API vs local performance gaps

## Next Checks

1. **Controlled Fine-tuning Experiment**: Take a base model (e.g., mistral-7b-v0.1) and fine-tune it with systematically varied data mixes (chat-only, reasoning-only, combined) while keeping all other training parameters constant. This would isolate the effect of instruction-tuning data composition on benchmark performance.

2. **Sampling Parameter Exposure Test**: Conduct a factorial experiment varying all available sampling parameters (temperature, top-k, top-p, repetition penalty) across multiple inference backends to quantify their individual and combined effects on performance. This would validate whether unexposed parameters fully explain API vs local differences.

3. **Cross-Benchmark Validation**: Evaluate the same model set across multiple agentive task benchmarks beyond clembench to test whether the observed size-performance relationships and data quality effects generalize to other goal-directed tasks. This would assess the external validity of the findings.