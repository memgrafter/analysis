---
ver: rpa2
title: 'SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF'
arxiv_id: '2411.01798'
source_url: https://arxiv.org/abs/2411.01798
tags:
- salsa
- reward
- arxiv
- soup
- reference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of limited exploration in RLHF\
  \ due to KL divergence constraints that restrict policy optimization to a narrow\
  \ region around the initial model. The proposed SALSA method overcomes this by using\
  \ a model soup\u2014weight-space averaging of two independently fine-tuned models\u2014\
  as the reference in KL regularization."
---

# SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF

## Quick Facts
- arXiv ID: 2411.01798
- Source URL: https://arxiv.org/abs/2411.01798
- Authors: Atoosa Chegini; Hamid Kazemi; Iman Mirzadeh; Dong Yin; Maxwell Horton; Moin Nabi; Mehrdad Farajtabar; Keivan Alizadeh
- Reference count: 20
- Primary result: SALSA achieves 54.01% win rate vs 45.99% on Arena-Hard for Llama2-7B, demonstrating superior performance through model soup-based KL regularization

## Executive Summary
SALSA addresses the exploration limitations in RLHF caused by KL divergence constraints that restrict policy optimization to narrow regions around initial models. The method introduces model soup - weight-space averaging of two independently fine-tuned models - as the reference for KL regularization, enabling larger deviations while maintaining stability. Experiments on Llama2-7B, Mistral-7B, and Gemma-2B demonstrate consistent improvements over standard PPO, with win rates exceeding 54% across all benchmarks. SALSA also shows superior out-of-distribution robustness and higher average rewards across multiple evaluation datasets.

## Method Summary
SALSA modifies standard RLHF by using a model soup as the reference point for KL divergence regularization. The method trains two independently fine-tuned models on UltraChat-200k, creates a weight-averaged soup model with α=0.5, and uses this soup as the KL reference during PPO optimization. The reward model is trained on UltraFeedback, and the policy is initialized from one of the reference SFT models. This approach allows for larger exploration steps while maintaining stability, as the soup represents a more robust starting point in the parameter space.

## Key Results
- Llama2-7B: SALSA achieves 54.01% win rate vs 45.99% on Arena-Hard
- Mistral-7B: SALSA achieves 54.40% win rate vs 45.60% on Arena-Hard
- Gemma-2B: SALSA achieves 53.7% win rate vs 46.3% on Arena-Hard
- SALSA demonstrates superior out-of-distribution robustness and higher average rewards across MT-Bench, Arena-Hard, and UltraFeedback datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model soup serves as a better reference point for KL divergence because it resides in a region of the parameter space with inherently higher rewards than a single SFT model.
- Mechanism: Weight-space averaging of multiple independently fine-tuned models creates a reference that is located in a higher-reward region of the solution space, allowing the policy to explore more promising areas while maintaining stability.
- Core assumption: The reward landscape around the model soup is superior to that around individual SFT models, and this reward advantage translates to better alignment performance.
- Evidence anchors:
  - [abstract] "This model soup allows for larger deviation in KL divergence and exploring a promising region of the solution space without sacrificing stability."
  - [section] "We hypothesize that πsoup resides in a region of the parameter space associated with generally higher rewards, suggesting that models explored in this vicinity could generate responses with increased reward values"
  - [corpus] Weak - no direct evidence found in corpus about reward advantages of model soups

### Mechanism 2
- Claim: Using model soup as reference allows for larger KL divergence deviation, enabling broader exploration of the solution space.
- Mechanism: The KL divergence constraint is less restrictive when using a model soup reference because the soup represents a more robust and diverse starting point, allowing the policy to explore further from any single model.
- Core assumption: The KL divergence penalty can be reduced while maintaining model quality because the soup reference is more stable and less prone to producing nonsensical outputs.
- Evidence anchors:
  - [abstract] "This model soup allows for larger deviation in KL divergence and exploring a promising region of the solution space without sacrificing stability."
  - [section] "We further show having model soups as reference point of RLHF results in higher reward outcomes."
  - [corpus] Weak - no direct evidence found in corpus about KL divergence advantages

### Mechanism 3
- Claim: Model averaging leverages the complementary strengths of diverse models, reducing variance and improving robustness while maintaining computational efficiency.
- Mechanism: Independent SFT models trained with different random seeds converge to different but equally good solutions in the loss landscape. Averaging their weights creates a model that combines their strengths without requiring extra inference time.
- Core assumption: Fine-tuned models from the same pre-trained initialization often reside in a shared low-error basin in the loss landscape, enabling effective weight interpolation.
- Evidence anchors:
  - [abstract] "This method leverages the principle that fine-tuned models from the same pre-trained initialization often reside in a shared low-error basin in the loss landscape, enabling effective weight interpolation without compromising accuracy."
  - [section] "A model soup is constructed by performing weight-space averaging of multiple independently supervised fine-tuned models that demonstrate comparable performance."
  - [corpus] Strong - "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time" [Wortsman et al., 2022]

## Foundational Learning

- Concept: KL divergence regularization in RLHF
  - Why needed here: Understanding how KL divergence constraints limit exploration in standard PPO and how replacing the reference model with a soup changes this dynamic
  - Quick check question: What happens to policy optimization if you completely remove the KL divergence term from the PPO loss?

- Concept: Weight-space model averaging
  - Why needed here: The core innovation relies on creating a better reference model through averaging independently trained models
  - Quick check question: Why might averaging weights of models trained with different random seeds be more effective than simply training one model longer?

- Concept: Loss landscape geometry in neural networks
  - Why needed here: The method assumes that fine-tuned models lie in a shared low-error basin where weight interpolation is effective
  - Quick check question: What would happen to model soup performance if fine-tuned models were in completely different loss basins?

## Architecture Onboarding

- Component map:
  SFT models (two independently trained) -> Model soup creation (weight averaging with α=0.5) -> Reward model (UltraFeedback) -> Policy model (initialized from SFT) -> SALSA optimizer (modified PPO with soup-based KL reference) -> Evaluation pipeline (MT-Bench, Arena-Hard, UltraFeedback)

- Critical path:
  1. Train two independent SFT models on UltraChat-200k
  2. Create model soup by averaging weights
  3. Train reward model on UltraFeedback
  4. Initialize policy from reference SFT model
  5. Run SALSA optimization using soup as KL reference
  6. Evaluate on out-of-distribution benchmarks

- Design tradeoffs:
  - Computational cost of training two SFT models vs single model
  - KL coefficient tuning (smaller for SALSA vs PPO)
  - Choice of α for weight averaging (0.5 found optimal)
  - Memory requirements for maintaining multiple model checkpoints

- Failure signatures:
  - Gibberish outputs when KL coefficient is too small
  - No improvement over PPO when using wrong α value
  - Performance degradation if models are not in the same loss basin
  - Instability if averaging coefficient deviates significantly from 0.5

- First 3 experiments:
  1. Compare reward landscapes of single SFT models vs model soup to verify higher-reward hypothesis
  2. Test KL divergence behavior with SALSA vs PPO at different KL coefficients
  3. Validate that weight averaging with α=0.5 produces superior results compared to other α values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of SFT models to include in the model soup for SALSA?
- Basis in paper: [explicit] The authors mention that increasing the number of SFT models in the soup model improves win rates, but they limited the number to two due to computational constraints. They also state that investigating soups of more than three elements and finding the optimal one is left as future work.
- Why unresolved: The authors did not explore soups with more than two SFT models due to computational constraints, leaving the optimal number of SFT models unknown.
- What evidence would resolve it: Experiments systematically varying the number of SFT models in the soup and measuring the resulting win rates and performance metrics would determine the optimal number.

### Open Question 2
- Question: How does SALSA perform compared to other RLHF methods like DPO or SimPO?
- Basis in paper: [inferred] The authors focus on PPO and SALSA, but they mention that DPO struggles with out-of-distribution data and SimPO eliminates the need for a reference model. A direct comparison of SALSA to these methods is not provided.
- Why unresolved: The paper does not include experiments comparing SALSA to other RLHF methods like DPO or SimPO.
- What evidence would resolve it: Experiments comparing SALSA to other RLHF methods like DPO and SimPO on the same benchmarks would determine if SALSA outperforms these methods.

### Open Question 3
- Question: Can the KL-Hack issue in SALSA be resolved or mitigated?
- Basis in paper: [explicit] The authors mention that high KL coefficients cannot be applied when using SALSA because the response length tends to converge to zero, a phenomenon they refer to as the KL-Hack.
- Why unresolved: The authors acknowledge the KL-Hack issue but do not provide a solution or mitigation strategy.
- What evidence would resolve it: Experiments exploring different KL coefficient values, KL divergence formulations, or regularization techniques to prevent the response length from converging to zero would determine if the KL-Hack can be resolved or mitigated.

## Limitations

- Computational overhead of training two independent SFT models before RLHF could be prohibitive for larger models
- Results may not generalize to models significantly larger than 7B parameters
- Performance on specialized domains (mathematical reasoning, code generation) remains uncertain

## Confidence

**High Confidence**: The mechanism of weight-space averaging for model soups is well-established in prior literature and the empirical results demonstrating SALSA's performance improvements are robust across multiple model architectures and evaluation datasets.

**Medium Confidence**: The hypothesis that model soups reside in higher-reward regions of the parameter space is supported by empirical evidence but lacks direct theoretical justification. The claim about reduced KL divergence constraints enabling better exploration is plausible but not rigorously proven.

**Low Confidence**: The generalizability of results to models significantly larger than 7B parameters, the stability of SALSA across different reward model architectures, and the performance on specialized domains remain uncertain.

## Next Checks

1. **Reward Landscape Analysis**: Conduct a systematic comparison of reward landscapes around single SFT models versus the model soup to verify that the soup consistently occupies higher-reward regions across different initializations and datasets.

2. **KL Divergence Behavior Testing**: Systematically vary the KL coefficient in both SALSA and standard PPO to identify the precise threshold where model quality degrades, and compare this behavior across different model scales and reward functions.

3. **Cross-Domain Generalization Study**: Evaluate SALSA's performance on specialized tasks (mathematical reasoning, code generation, multi-turn dialogue) that were not included in the original training data to assess its out-of-distribution capabilities beyond the reported benchmarks.