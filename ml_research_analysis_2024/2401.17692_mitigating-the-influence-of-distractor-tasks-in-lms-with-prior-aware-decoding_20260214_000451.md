---
ver: rpa2
title: Mitigating the Influence of Distractor Tasks in LMs with Prior-Aware Decoding
arxiv_id: '2401.17692'
source_url: https://arxiv.org/abs/2401.17692
tags:
- prompt
- task
- tasks
- language
- strong
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of distractor tasks in language
  models, where unintended secondary tasks inferred from prompts lead to unwanted
  outputs, sometimes even worsening with model scaling (inverse scaling). The authors
  propose a theoretical framework interpreting language models as products of experts
  combining multiple data generation processes, and introduce Prior-Aware Decoding
  (PAD) - a contrastive inference method to reduce distractor task influence.
---

# Mitigating the Influence of Distractor Tasks in LMs with Prior-Aware Decoding

## Quick Facts
- arXiv ID: 2401.17692
- Source URL: https://arxiv.org/abs/2401.17692
- Authors: Raymond Douglas; Andis Draguns; Tomáš Gavenčiak
- Reference count: 27
- Key outcome: PAD improves task completion in 41 of 44 task-model combinations with median 40% increase in task completion proportion

## Executive Summary
This paper addresses distractor tasks in language models, where unintended secondary tasks inferred from prompts lead to unwanted outputs, sometimes even worsening with model scaling (inverse scaling). The authors propose Prior-Aware Decoding (PAD), a contrastive inference method that reduces distractor task influence by contrasting outputs from full prompts against weakened prompts lacking the intended task component. Testing across 11 models and 4 datasets shows PAD outperforms baseline in most cases, with a 30 percentage point mean completion improvement at α = 2, demonstrating robust improvements across various model architectures and tasks.

## Method Summary
Prior-Aware Decoding (PAD) is an inference-time technique that mitigates distractor task influence by computing a linear combination of logits from two model queries: one with the full prompt and one with a weakened prompt lacking the intended task component. The method uses the formula L = LO + α(LO - LW), where LO represents original logits and LW represents weakened logits, with α controlling the amplification of intended task signal. This contrastive approach selectively amplifies the relative weight of intended task logits over distractor task logits, improving task completion rates without requiring model retraining.

## Key Results
- PAD outperforms baseline in 41 of 44 task-model combinations tested
- Median 40% increase in task completion proportion across all experiments
- 30 percentage point mean completion improvement at α = 2 compared to α = 0 baseline
- PAD produces greater gains than doubling the number of parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method works by creating a mixture of experts where distractor task influence is selectively suppressed
- Mechanism: The technique contrasts logits from full prompts against weakened prompts lacking the intended task component, then extrapolates logits to favor intended task-consistent outputs
- Core assumption: Language models can be modeled as a product of experts combining multiple data generation processes, where each expert represents a different task or context
- Evidence anchors:
  - [abstract] "we present a theoretical framework that interprets LMs as a product of experts that combine multiple data generation processes"
  - [section 4] "We assume that pM can be modeled as a geometric mixture of pC and pL, also known as a Product of Experts"
  - [corpus] No direct corpus evidence available for this specific mechanism
- Break condition: The method would fail if the prompt cannot be meaningfully decomposed into task and data components, or if the distractor task is not primarily driven by the data portion of the prompt

### Mechanism 2
- Claim: PAD improves task completion by amplifying the relative weight of intended task logits over distractor task logits
- Mechanism: The technique computes a linear combination of logits: LO + α(LO - LW), where LO represents original logits and LW represents weakened logits, effectively increasing the weight of intended task predictions
- Core assumption: The difference between original and weakened logits (LO - LW) contains signal about the intended task that can be amplified
- Evidence anchors:
  - [section 5] "We compute a linear combination of these logits: L = LO + α(LO - LW)"
  - [section 6] "extrapolating with α = 2 outperforms the baseline in 41/44 cases, with a 30 percentage point (%pt) mean completion improvement"
  - [corpus] No direct corpus evidence available for this specific mechanism
- Break condition: The method would fail if the weakened prompt does not adequately represent the distractor task, or if the relationship between (LO - LW) and intended task signal is not consistent

### Mechanism 3
- Claim: The technique works because language models exhibit "strong priors" where certain prompt components disproportionately influence output
- Mechanism: By removing task components to create weakened prompts, the method amplifies the influence of distractor tasks, making the contrast between full and weakened prompts more informative about the intended task signal
- Core assumption: Language models have a property where "a part of the input disproportionately affects the output, substantially diminishing the impact of other parts of the input"
- Evidence anchors:
  - [section 3] "Generative models can exhibit a phenomenon where a part of the input disproportionately affects the output"
  - [section 6] "PAD produces greater gains than doubling the number of parameters"
  - [corpus] No direct corpus evidence available for this specific mechanism
- Break condition: The method would fail if the strong prior effect is not primarily localized to the data portion of the prompt, or if the model does not exhibit this disproportionate influence pattern

## Foundational Learning

- Concept: Product of Experts (geometric mixture) models
  - Why needed here: The paper's theoretical framework interprets language models as products of experts, which is fundamental to understanding how PAD works
  - Quick check question: What is the mathematical relationship between a product of experts and the individual expert distributions?

- Concept: Contrastive decoding techniques
  - Why needed here: PAD is a form of contrastive decoding that compares outputs from different prompt variations
  - Quick check question: How does contrastive decoding differ from standard decoding in terms of the sampling strategy?

- Concept: Logit space manipulation
  - Why needed here: PAD operates by computing linear combinations of logits from different prompt variations
  - Quick check question: Why is working in logit space (rather than probability space) advantageous for this type of manipulation?

## Architecture Onboarding

- Component map: Prompt processor -> Logit generator (full) -> Logit generator (weakened) -> Linear combiner -> Sampler -> Evaluator
- Critical path: Prompt → Logit generation (full) → Logit generation (weakened) → Linear combination → Sampling → Evaluation
- Design tradeoffs:
  - α parameter tuning vs. performance: Higher α values generally improve performance but may introduce noise
  - Prompt weakening strategy: Different weakening approaches may work better for different tasks
  - Computational cost: Requires two model queries instead of one, doubling inference time
- Failure signatures:
  - No improvement or degradation when α > 0: May indicate weak distractor task signal or inappropriate weakening strategy
  - Inconsistent performance across tasks: May indicate that the product of experts model doesn't apply well to certain task types
  - High variance in results: May indicate noise amplification from high α values
- First 3 experiments:
  1. Implement basic PAD with α = 1.0 on a simple inverse scaling task (e.g., pattern matching suppression)
  2. Compare performance across different α values (0.0, 1.0, 2.0) on the same task
  3. Test different weakening strategies (truncated prompt vs. system prompt) on a single task to identify which works better for that specific case

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out specific open questions, but based on the discussion and limitations section, several directions for future research emerge:

- How does PAD perform on more complex distractor tasks beyond those tested, such as multi-step reasoning or long-form generation?
- Can PAD be combined with other inference-time techniques (e.g., classifier-free guidance) to further improve performance?
- How does the choice of weakening strategy (e.g., truncated prompt vs. system prompt) affect PAD's performance on different task types?
- What is the relationship between the degree of inverse scaling in a task and PAD's effectiveness?
- How does PAD perform on tasks that don't exhibit inverse scaling or strong priors?

## Limitations

- The paper demonstrates PAD's effectiveness but doesn't fully explore the underlying mechanisms beyond the product-of-experts framework
- The method requires two inference calls per generation, doubling computational cost
- The paper doesn't extensively explore alternative weakening strategies or investigate whether certain task types benefit more than others
- Results are based on a limited set of 4 datasets, which may not capture the full range of distractor task scenarios

## Confidence

- **High Confidence**: The empirical results showing PAD's effectiveness across 44 task-model combinations, with consistent improvements at α = 2. The paper's methodology for evaluating task completion is clearly defined and reproducible.
- **Medium Confidence**: The theoretical framework of modeling language models as products of experts. While this provides elegant justification, the paper acknowledges it's an assumption rather than proven fact, and results hold even when this assumption is violated.
- **Medium Confidence**: The generalizability of PAD across different task types and model architectures. The results are robust across the tested models and tasks, but the sample size (4 datasets) limits broader claims about universal applicability.

## Next Checks

1. **Ablation Study on Weakening Strategies**: Systematically compare PAD performance using different weakening approaches (truncated prompts, system prompt removal, data-only prompts) across all four task types to identify which strategy works best for which task category.

2. **Cross-Domain Transfer**: Test PAD on additional task types not in the Inverse Scaling Dataset (e.g., commonsense reasoning, code generation, or instruction following) to validate generalizability beyond the current scope.

3. **Resource Efficiency Analysis**: Quantify the practical tradeoff between PAD's doubled inference cost and its performance gains by measuring wall-clock time and comparing to alternative approaches like model retraining or prompt engineering.