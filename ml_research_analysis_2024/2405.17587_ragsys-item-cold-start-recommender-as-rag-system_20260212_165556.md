---
ver: rpa2
title: 'RAGSys: Item-Cold-Start Recommender as RAG System'
arxiv_id: '2405.17587'
source_url: https://arxiv.org/abs/2405.17587
tags:
- retrieval
- diversity
- learning
- information
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of information retrieval for In-Context
  Learning (ICL) in few-shot learning with Large Language Models (LLMs). The authors
  frame ICL retrieval as an item-cold-start recommender system problem, prioritizing
  discovery and maximizing information gain over strict relevance.
---

# RAGSys: Item-Cold-Start Recommender as RAG System

## Quick Facts
- arXiv ID: 2405.17587
- Source URL: https://arxiv.org/abs/2405.17587
- Authors: Emile Contal; Garrin McGoldrick
- Reference count: 40
- This paper explores using information retrieval for In-Context Learning (ICL) by framing it as an item-cold-start recommender system problem.

## Executive Summary
This paper proposes a novel approach to In-Context Learning (ICL) retrieval by treating it as an item-cold-start recommender system problem. The authors introduce a retrieval framework that combines semantic similarity, diversity, and quality bias to optimize demonstrations for LLM learning. They propose a new evaluation method using Direct Preference Optimization (DPO) to measure how retrieved context impacts LLM performance. Experiments on the TruthfulQA dataset demonstrate that their method significantly outperforms pure relevance-based retrieval across multiple LLMs and metrics, highlighting the critical role of diversity and quality in effective ICL.

## Method Summary
The paper frames ICL retrieval as an item-cold-start recommendation problem, treating demonstrations as items to be recommended to the LLM. The retrieval algorithm uses BERT embeddings for semantic similarity, Maximal Marginal Relevance (MMR) for diversity, and quality bias based on log perplexity of demonstration answers. The DPO metric evaluates retrieval quality by measuring how much retrieved context increases the probability of correct LLM outputs. The approach combines relevance, diversity, and quality bias through weighted parameters (λd = 0.75, λb = 0.95) to optimize demonstrations for few-shot learning.

## Key Results
- The Rel+Div+Bias retrieval strategy (combining relevance, diversity, and quality bias) significantly outperforms pure relevance-based retrieval across multiple LLMs and metrics
- Diversity plays a critical role in ICL effectiveness, with substantial performance improvements observed when diversity is incorporated
- The proposed DPO evaluation method provides a more direct assessment of retrieval quality than traditional retrieval metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL retrieval can be modeled as an item-cold-start recommendation problem where the query is an unseen item and the goal is to maximize collective information gain.
- Mechanism: The retrieval system treats demonstrations as items to be recommended, optimizing for diversity and quality rather than just relevance to maximize the LLM's learning signal.
- Core assumption: The LLM's ability to generate correct answers improves when presented with diverse, high-quality demonstrations rather than just the most similar ones.
- Evidence anchors:
  - [abstract]: "ICL retrieval in this context resembles item-cold-start recommender systems, prioritizing discovery and maximizing information gain over strict relevance."
  - [section 3.1]: "We propose to frame the ICL problem as an item-cold-start recommendation problem..."
- Break condition: If the LLM fails to benefit from diverse demonstrations or if quality scoring doesn't correlate with actual demonstration effectiveness.

### Mechanism 2
- Claim: Direct evaluation of retrieved demonstrations through LLM performance metrics (DPO) provides more accurate assessment than traditional retrieval metrics.
- Mechanism: Instead of measuring recall/precision, the system measures how much the retrieved context increases the probability of correct LLM outputs using a direct preference optimization loss.
- Core assumption: The probability ratio of correct vs incorrect answers under different retrieval conditions accurately reflects retrieval quality.
- Evidence anchors:
  - [abstract]: "We propose a novel evaluation method that measures the LLM's subsequent performance on NLP tasks..."
  - [section 3.2.2]: "We define the DPO metric as the negative of the Direct Preference Optimization loss..."
- Break condition: If DPO metric doesn't correlate with actual task performance or if the metric is too sensitive to token-level variations.

### Mechanism 3
- Claim: Quality bias based on LLM perplexity improves retrieval by prioritizing demonstrations the model finds more predictable.
- Mechanism: Demonstrations are scored using log perplexity of their answers given their questions, with lower perplexity indicating higher quality.
- Core assumption: Demonstrations with lower perplexity are more likely to provide useful information to the LLM during few-shot learning.
- Evidence anchors:
  - [section 3.3.3]: "This score is computed using the log perplexity of the demonstration answer given the demonstration question."
- Break condition: If perplexity doesn't correlate with demonstration quality or if the bias leads to overfitting to common patterns.

## Foundational Learning

- Concept: Dense embeddings and semantic similarity
  - Why needed here: The retrieval system uses BERT embeddings and cosine similarity to measure query-demonstration relevance
  - Quick check question: How does cosine similarity between embeddings differ from exact keyword matching in retrieval tasks?

- Concept: Recommender system diversity techniques
  - Why needed here: The system adapts Maximal Marginal Relevance (MMR) from recommender systems to ensure diverse demonstrations
  - Quick check question: What is the trade-off between relevance and diversity in MMR, and how is it controlled?

- Concept: Perplexity as a quality measure
  - Why needed here: The system uses perplexity to score demonstration quality, assuming lower perplexity indicates better demonstrations
  - Quick check question: Why would a demonstration with lower perplexity be considered higher quality for ICL purposes?

## Architecture Onboarding

- Component map: Query → Embedding → MMR retrieval with quality bias → Context assembly → LLM evaluation
- Critical path: Query → Embedding → MMR retrieval with quality bias → Context assembly → LLM evaluation
- Design tradeoffs:
  - Quality bias vs diversity: Higher quality bias may reduce diversity
  - Retrieval speed vs accuracy: Approximate kNN vs exact search
  - LLM latency vs demonstration quality: More demonstrations may improve quality but increase latency
- Failure signatures:
  - Low DPO scores despite high relevance scores
  - Retrieval becomes too focused on a small subset of demonstrations
  - Quality bias leads to repetitive or overly common demonstrations
- First 3 experiments:
  1. Compare pure relevance retrieval vs MMR-based retrieval on DPO metric
  2. Test different quality bias weights (λb) on LLM performance
  3. Measure diversity impact by varying diversity weight (λd) and observing DPO changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal balance between relevance, diversity, and quality bias vary across different LLM architectures and sizes?
- Basis in paper: [explicit] The paper observes contrasting results for the quality bias term across Mistral and Llama-3 models, suggesting LLM-dependent hyperparameter tuning of the quality bias may be necessary.
- Why unresolved: The paper does not perform systematic experiments to determine optimal parameter values for different LLMs, leaving the relationship between LLM architecture and optimal retrieval strategy unclear.
- What evidence would resolve it: Experiments systematically varying the quality bias parameter across multiple LLM architectures and sizes to identify optimal values and patterns.

### Open Question 2
- Question: What is the relationship between the amount of diversity in retrieved demonstrations and the subsequent performance improvement of the LLM?
- Basis in paper: [explicit] The paper mentions the difficulty of calibrating diversity without their proposed DPO metric and demonstrates a non-monotonous relationship between diversity and DPO in Figure 1.
- Why unresolved: The paper only provides a single example of this relationship for one specific LLM and does not explore the general patterns or optimal diversity levels across different tasks or models.
- What evidence would resolve it: Systematic experiments varying diversity levels across multiple LLMs, tasks, and evaluation metrics to characterize the relationship and identify optimal diversity ranges.

### Open Question 3
- Question: How do recommendation system techniques specifically improve ICL retrieval performance compared to traditional information retrieval methods?
- Basis in paper: [explicit] The paper frames ICL retrieval as an item-cold-start recommendation problem and suggests that recommendation engines offer a better solution than semantic search engines, but does not provide direct empirical comparisons.
- Why unresolved: The paper does not conduct controlled experiments comparing recommendation system techniques to traditional IR methods in the ICL context, leaving the specific benefits unclear.
- What evidence would resolve it: Controlled experiments implementing and comparing recommendation system techniques (e.g., collaborative filtering, diversity-aware ranking) with traditional IR methods (e.g., BM25, semantic similarity) for ICL retrieval across multiple datasets and LLMs.

## Limitations
- Dataset dependency: Experiments rely solely on the TruthfulQA dataset, raising questions about generalizability to other NLP tasks
- LLM-specific optimizations: The study suggests optimal quality bias hyperparameters may be LLM-dependent but only tests four models
- Evaluation metric validity: While DPO provides task-specific evaluation, it's unclear whether this metric fully captures demonstration quality nuances

## Confidence
**High Confidence Claims:**
- The proposed retrieval framework combining relevance, diversity, and quality bias consistently outperforms pure relevance-based retrieval across multiple metrics and LLMs
- Diversity plays a critical role in ICL effectiveness, with the Rel+Div+Bias approach showing significant improvements
- The DPO evaluation method provides a more direct assessment of retrieval quality than traditional metrics

**Medium Confidence Claims:**
- The item-cold-start recommender system framing accurately captures the ICL retrieval problem
- Quality bias based on perplexity is an effective way to prioritize demonstrations
- The specific parameter values (λd = 0.75, λb = 0.95) are optimal or near-optimal

**Low Confidence Claims:**
- The framework will generalize to all NLP tasks and domains
- The specific hyperparameter settings will work well for any LLM without adjustment
- The DPO metric is superior to all other possible evaluation methods

## Next Checks
1. **Cross-Dataset Validation**: Test the retrieval framework on multiple diverse NLP datasets (beyond TruthfulQA) including question answering, summarization, and reasoning tasks to assess generalizability.

2. **Ablation Study on Quality Bias**: Systematically vary the quality bias weight (λb) across a wider range of values and more LLM models to determine if the claimed LLM-dependence holds and to identify more robust optimization strategies.

3. **Alternative Evaluation Metrics**: Implement and compare multiple evaluation approaches (including human evaluation) to validate whether DPO consistently captures retrieval quality improvements and to identify potential blind spots in the current evaluation methodology.