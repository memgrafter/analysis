---
ver: rpa2
title: Large-scale cloze evaluation reveals that token prediction tasks are neither
  lexically nor semantically aligned
arxiv_id: '2410.12057'
source_url: https://arxiv.org/abs/2410.12057
tags:
- human
- responses
- language
- cloze
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work compares human and language model behavior on single-word
  completion tasks, revealing significant differences in both probability estimates
  and semantic spaces. While larger models trained longer show improved correlations
  with human responses, they consistently underestimate human response probabilities,
  over-rank rare responses, and under-rank top responses.
---

# Large-scale cloze evaluation reveals that token prediction tasks are neither lexically nor semantically aligned

## Quick Facts
- arXiv ID: 2410.12057
- Source URL: https://arxiv.org/abs/2410.12057
- Authors: Cassandra L. Jacobs; Loïc Grobol; Alvin Tsang
- Reference count: 12
- This work compares human and language model behavior on single-word completion tasks, revealing significant differences in both probability estimates and semantic spaces.

## Executive Summary
This study compares human and language model behavior on single-word cloze completion tasks, revealing systematic differences in both probability estimates and semantic representations. Using the Peelle et al. (2020) dataset with 3,085 English sentences and at least 100 human responses per sentence, the authors evaluate Pythia, GPT-2, and RoBERTa models across various sizes and training stages. Despite improvements with larger models and longer training, language models consistently underestimate human response probabilities, over-rank rare responses, and under-rank top responses. The semantic clustering of human and model-generated responses shows poor alignment, with shared responses forming distinct clusters separate from either human-only or model-only responses.

## Method Summary
The study evaluates single-word predictions from language models using the Peelle et al. (2020) cloze dataset. Models generate top predictions for each sentence, with probabilities extracted for word-initial subwords. Spearman rank correlations compare LM and human response ranks, while Bayesian Gaussian Mixture Models (BGMM) cluster middle-layer embeddings to analyze semantic space alignment. Clustering Mutual Information (CMI) scores quantify the semantic similarity between human and model-generated clusters. The analysis spans multiple model sizes including Pythia-160M, Pythia-2.8B, GPT-2, and RoBERTa, examining both initial and final training checkpoints.

## Key Results
- Language models consistently underestimate human cloze probabilities and show systematic rank misordering (under-ranking top responses, over-ranking rare responses)
- Semantic clustering analysis reveals poor alignment between human and LM-generated semantic spaces, with shared responses forming distinct clusters
- Even with model capacity increases and extended training, correlation between human and LM ranks remains below 0.5
- Models trained on deduplicated text show better performance on cloze tasks compared to those trained on repeated data

## Why This Works (Mechanism)
None

## Foundational Learning
- **Cloze task**: A psychological test where participants complete sentence fragments, used here to evaluate language model predictions against human responses
  - Why needed: Provides standardized human response data for comparing model predictions
  - Quick check: Verify dataset contains multiple human responses per sentence context
- **Spearman rank correlation**: Statistical measure comparing ranked variables rather than raw values
  - Why needed: Evaluates whether models rank responses similarly to humans, regardless of probability calibration
  - Quick check: Confirm correlation values between -1 and 1, with values below 0.5 indicating poor alignment
- **Bayesian Gaussian Mixture Models (BGMM)**: Clustering algorithm that probabilistically assigns data points to multiple clusters
  - Why needed: Groups semantically similar responses to analyze alignment between human and model semantic spaces
  - Quick check: Ensure clusters are well-separated in visualization and show meaningful semantic groupings
- **Clustering Mutual Information (CMI)**: Metric measuring shared information between two clusterings
  - Why needed: Quantifies semantic alignment between human and model response clusters
  - Quick check: Verify CMI scores reflect expected similarity levels between clusterings
- **UMAP (Uniform Manifold Approximation and Projection)**: Dimensionality reduction technique for visualizing high-dimensional data
  - Why needed: Visualizes clustering results in 2D space to assess semantic separation
  - Quick check: Confirm clusters are visually distinct and separated in UMAP plots
- **Subword tokenization**: Breaking words into smaller units for language model processing
  - Why needed: Explains potential mismatches between human responses and model vocabularies
  - Quick check: Verify tokenization doesn't create systematic biases in probability estimates

## Architecture Onboarding

**Component Map**: Data Preprocessing -> Model Evaluation -> Semantic Clustering -> Correlation Analysis -> Visualization

**Critical Path**: Sentence context extraction → Model prediction generation → Probability computation → Rank correlation calculation → BGMM clustering → CMI scoring → UMAP visualization

**Design Tradeoffs**: The study prioritizes comprehensive evaluation across multiple model sizes and training stages over exhaustive analysis of any single model configuration. Using single-word responses simplifies comparison but may miss nuances in multi-word completions. BGMM clustering provides probabilistic assignments but may be sensitive to hyperparameter choices.

**Failure Signatures**: Poor rank correlations (<0.5) indicate fundamental misalignment between model and human response distributions. UMAP visualizations showing overlapping or poorly separated clusters suggest semantic misalignment. Systematic probability underestimation by models reveals calibration issues.

**First Experiments**:
1. Load and preprocess Peelle et al. dataset, extracting sentence contexts and human responses
2. Run Pythia-160M on sample sentences to generate predictions and compute initial rank correlations
3. Perform BGMM clustering on embeddings of human and model responses from a single sentence to verify clustering methodology

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic or semantic properties of human-generated responses are most consistently misunderstood by language models, even when the models' overall predictive performance improves?
- Basis in paper: [explicit] The paper shows that language models systematically underestimate probabilities of human responses, over-rank rare responses, under-rank top responses, and produce semantic spaces that poorly align with human responses
- Why unresolved: While the paper identifies these patterns, it does not investigate which specific linguistic features (e.g., concreteness, familiarity, semantic relatedness) or response types are most responsible for these systematic errors
- What evidence would resolve it: Detailed analysis of which types of human responses (e.g., by semantic category, syntactic complexity, concreteness) are most poorly predicted by language models, and which features of these responses cause the largest prediction errors

### Open Question 2
- Question: At what point in language model training does the divergence from human-like probability distributions become most pronounced, and is this related to specific architectural changes or training data characteristics?
- Basis in paper: [explicit] The paper shows that language models improve in correlation with human responses during training but plateau at moderate levels (ρ < 0.5), with diminishing returns after certain training steps and model sizes
- Why unresolved: The paper observes this plateau effect but does not investigate whether specific architectural developments (like attention mechanism refinement) or data characteristics (like increased repetition in training data) cause the models to diverge from human-like distributions
- What evidence would resolve it: Analysis of when during training specific types of prediction errors first emerge, and whether these errors correlate with architectural developments or changes in training data composition

### Open Question 3
- Question: Can adjusting language model architectures or training objectives to better capture human-like semantic clustering improve their ability to predict human responses in cloze tasks?
- Basis in paper: [inferred] The paper shows that human responses form distinct semantic clusters that are poorly captured by language models, and that models trained on deduplicated text perform better at cloze tasks
- Why unresolved: While the paper demonstrates the semantic clustering mismatch, it does not explore whether modifying model architectures or training objectives specifically to capture human-like semantic structures would improve cloze task performance
- What evidence would resolve it: Experiments with language models trained with objectives that explicitly optimize for semantic clustering similarity to human responses, and whether this improves cloze task prediction accuracy

## Limitations
- Evaluation focuses exclusively on English single-word cloze completions, limiting generalizability to other languages or multi-word responses
- Dataset demographic composition and cultural context may influence human response distributions in ways not captured by the models
- Interpretation of BGMM clusterings and CMI metrics for semantic alignment remains somewhat qualitative

## Confidence
- **High confidence**: Models consistently underestimate human cloze probabilities and show systematic rank misordering; correlation between human and model ranks remaining below 0.5 is robust
- **Medium confidence**: Semantic space misalignment findings due to potential sensitivity to hyperparameter choices and visualization methods
- **Medium confidence**: Claim that increased model capacity and training duration do not substantially improve alignment, as analysis only covers specific model families

## Next Checks
1. **Cross-linguistic validation**: Replicate the analysis using multilingual cloze datasets to determine whether alignment failures are language-specific or universal
2. **Multi-word response analysis**: Extend the evaluation framework to handle multi-word human responses, investigating whether models show better alignment with phrase completions
3. **Alternative semantic comparison methods**: Apply additional semantic similarity measures (e.g., sentence transformers, BERTScore) to triangulate clustering findings and assess robustness across different evaluation approaches