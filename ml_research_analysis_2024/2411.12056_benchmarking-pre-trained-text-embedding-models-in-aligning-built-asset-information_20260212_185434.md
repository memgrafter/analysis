---
ver: rpa2
title: Benchmarking pre-trained text embedding models in aligning built asset information
arxiv_id: '2411.12056'
source_url: https://arxiv.org/abs/2411.12056
tags:
- data
- built
- text
- product
- asset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive benchmark of state-of-the-art
  text embedding models for aligning built asset information with domain-specific
  technical concepts. The authors developed six task-specific datasets derived from
  two renowned built asset data classification dictionaries, covering clustering,
  retrieval, and reranking tasks.
---

# Benchmarking pre-trained text embedding models in aligning built asset information

## Quick Facts
- arXiv ID: 2411.12056
- Source URL: https://arxiv.org/abs/2411.12056
- Authors: Mehrzad Shahinmoghadam; Ali Motamedi
- Reference count: 7
- This paper presents a comprehensive benchmark of state-of-the-art text embedding models for aligning built asset information with domain-specific technical concepts.

## Executive Summary
This paper presents a comprehensive benchmark of state-of-the-art text embedding models for aligning built asset information with domain-specific technical concepts. The authors developed six task-specific datasets derived from two renowned built asset data classification dictionaries, covering clustering, retrieval, and reranking tasks. They evaluated 24 text embedding models, including both open-source and proprietary models, on their ability to capture the semantics of built asset terminology.

The results showed variability in model performance across tasks, with instruction-tuned models generally performing better. The study found that model size alone is not a reliable predictor of performance, and that general-purpose benchmarks have limited transferability to the specialized domain of built asset information management. The authors highlight the need for tailored benchmarking datasets and domain adaptation techniques to improve automated data mapping in this field.

## Method Summary
The authors developed six task-specific datasets derived from two renowned built asset data classification dictionaries (IFC and Uniclass), covering clustering, retrieval, and reranking tasks. They evaluated 24 text embedding models, including both open-source and proprietary models, on their ability to capture the semantics of built asset terminology using cosine similarity and standard benchmarking metrics. The benchmark covers three main tasks: clustering (V-measure), retrieval (nDCG@10), and reranking (MAP) across sentence-to-sentence, paragraph-to-paragraph, and sentence-to-paragraph categories.

## Key Results
- Model performance varies significantly across clustering, retrieval, and reranking tasks
- Instruction-tuned models generally outperform non-instruction-tuned models in built asset domain
- Model size alone is not a reliable predictor of performance in this specialized domain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark effectively evaluates model performance in the built asset domain because it uses domain-specific datasets derived from established classification dictionaries (IFC and Uniclass).
- Mechanism: By grounding the evaluation in real-world classification systems, the benchmark captures the complex semantic relationships and technical terminology unique to built asset information, which general-purpose benchmarks miss.
- Core assumption: The semantic complexity of built asset terminology cannot be adequately captured by general-purpose benchmarks, necessitating domain-specific evaluation datasets.
- Evidence anchors:
  - [abstract]: "The results showed variability in model performance across tasks, with instruction-tuned models generally performing better."
  - [section]: "Our proposed datasets are derived from two renowned built asset data classification dictionaries."
  - [corpus]: Weak - corpus analysis shows low citation counts and no direct evidence of built asset domain expertise among neighboring papers, suggesting this may be an underexplored area.

### Mechanism 2
- Claim: The benchmarking methodology captures task-specific performance variations by evaluating models across clustering, retrieval, and reranking tasks.
- Mechanism: Different tasks stress different aspects of text embedding capabilities - clustering tests semantic grouping, retrieval tests relevance ranking, and reranking tests fine-grained discrimination between similar items.
- Core assumption: Model performance varies significantly across different semantic understanding tasks, and a single aggregate metric would mask these variations.
- Evidence anchors:
  - [abstract]: "The results showed variability in model performance across tasks, with instruction-tuned models generally performing better."
  - [section]: "Our proposed benchmark covers three main tasks: clustering, retrieval, and reranking."
  - [corpus]: Weak - no corpus evidence directly supports task-specific performance variations.

### Mechanism 3
- Claim: The semantic diversity sampling method improves benchmark quality by ensuring diverse and challenging evaluation sets.
- Mechanism: By selecting samples that are semantically dissimilar within categories and prioritizing hard negatives, the benchmark forces models to distinguish between closely related concepts rather than relying on simple keyword matching.
- Core assumption: Models that perform well on easily distinguishable samples may fail on harder, more nuanced distinctions that are common in built asset classification.
- Evidence anchors:
  - [section]: "To ensure a robust entity selection when creating task-specific datasets, we implemented the following sampling strategies: For positive sampling, we adopt a semantic diversity approach."
  - [corpus]: Weak - no corpus evidence directly supports the effectiveness of this sampling method.

## Foundational Learning

- Concept: Text embedding fundamentals (vector representations of semantic meaning)
  - Why needed here: Understanding how text embeddings work is crucial for interpreting benchmark results and for implementing domain adaptation techniques.
  - Quick check question: What is the difference between static embeddings (like word2vec) and contextual embeddings (like BERT)?

- Concept: Domain adaptation techniques for language models
  - Why needed here: The paper highlights the need for domain adaptation to improve performance on built asset terminology, making this a critical concept for extending the work.
  - Quick check question: What are the main approaches to domain adaptation for pre-trained language models?

- Concept: Evaluation metrics for text embeddings (V-measure, nDCG, MAP)
  - Why needed here: Understanding these metrics is essential for interpreting benchmark results and for comparing model performance across different tasks.
  - Quick check question: How does nDCG differ from MAP in evaluating ranked retrieval results?

## Architecture Onboarding

- Component map:
  - Data extraction layer (IFC and Uniclass parsing)
  - Data augmentation pipeline (description synthesis and curation)
  - Sampling module (semantic diversity and hard negative selection)
  - Task-specific dataset generators (clustering, retrieval, reranking)
  - Benchmark runner (model evaluation across tasks)
  - Result aggregation and analysis tools

- Critical path:
  1. Extract and preprocess data from IFC and Uniclass sources
  2. Generate and curate synthetic descriptions
  3. Apply sampling strategies to create task-specific datasets
  4. Run benchmark across all models and tasks
  5. Analyze and compare results

- Design tradeoffs:
  - Dataset size vs. quality (rigorous curation vs. comprehensive coverage)
  - Computational cost vs. model coverage (limited parameter range due to constraints)
  - Task difficulty vs. model discriminability (hard negatives vs. baseline performance)

- Failure signatures:
  - Poor baseline scores (0.8 V-measure or 1/N) indicating task is too easy or hard
  - High variance in model performance across tasks suggesting task design issues
  - Low thematic similarity with MTEB datasets indicating domain mismatch

- First 3 experiments:
  1. Run all models on S2S clustering task only to establish baseline performance
  2. Compare proprietary vs. open-source model performance on retrieval tasks
  3. Test instruction-tuned vs. non-instruction-tuned models on reranking tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much does instruction-tuning help smaller models adapt to the specialized domain of the built environment?
- Basis in paper: [explicit] The authors explicitly note that instruction-tuned models showed higher performance in the majority of their benchmark tasks and raise this as an important question for future research.
- Why unresolved: The paper only tested a limited number of instruction-tuned models and did not conduct ablation studies to isolate the effect of instruction-tuning from other factors like model size.
- What evidence would resolve it: Controlled experiments comparing performance of similarly-sized models with and without instruction-tuning on built environment tasks.

### Open Question 2
- Question: To what extent do general-purpose benchmarks transfer to the specialized domain of built asset information management?
- Basis in paper: [explicit] The authors explicitly state that their experiments indicate general-purpose benchmarks remain inadequate in capturing the unique semantic complexity of built asset text, even with high thematic similarity.
- Why unresolved: While the paper demonstrates limited transferability, it does not quantify the gap or identify specific factors causing the discrepancy.
- What evidence would resolve it: Systematic comparison of model performance across general and domain-specific benchmarks, identifying which aspects of built asset text are most poorly captured by general models.

### Open Question 3
- Question: What factors beyond model size influence the effectiveness of text embedding models for built asset information alignment?
- Basis in paper: [inferred] The authors note that model effectiveness is not strongly correlated with model size and observe performance gaps between models pre-trained with and without instruction tuning, suggesting other factors are at play.
- Why unresolved: The paper only tested a limited set of models and did not systematically vary other potential factors like training data composition or architectural choices.
- What evidence would resolve it: Controlled experiments varying individual factors (e.g., training data, architecture, tokenization) while holding model size constant to isolate their effects on built environment task performance.

## Limitations

- The benchmark's narrow domain focus using IFC and Uniclass dictionaries may not generalize to other built asset classification systems
- Evaluation was conducted using cosine similarity only, without exploring alternative distance metrics or fine-tuning approaches
- Limited parameter range tested for proprietary models due to computational constraints

## Confidence

**High Confidence**: The benchmark successfully identifies performance variability across different tasks (clustering, retrieval, reranking) and demonstrates that instruction-tuned models generally outperform non-instruction-tuned variants. The methodology for creating task-specific datasets from established classification dictionaries is sound and reproducible.

**Medium Confidence**: The finding that model size alone is not a reliable predictor of performance in this domain is supported by the results, though the limited parameter range tested prevents definitive conclusions about scaling relationships. The claim about limited transferability of general-purpose benchmarks to this specialized domain is plausible but requires additional validation across more benchmark suites.

**Low Confidence**: The effectiveness of the semantic diversity sampling method in creating challenging evaluation sets is asserted but not empirically validated against alternative sampling approaches. The corpus analysis reveals low citation counts and no direct evidence of built asset domain expertise among neighboring papers, suggesting this may be an underexplored area where validation is difficult.

## Next Checks

1. **Cross-benchmark comparison**: Evaluate the same models on general-purpose benchmarks (MTEB) and compare performance correlations to quantify the actual transferability gap between general and domain-specific evaluations.

2. **Alternative similarity metrics**: Re-run the benchmark using Euclidean distance, dot product, and learned similarity metrics to determine if cosine similarity is optimal for built asset terminology matching.

3. **Fine-tuning validation**: Select the top-performing models and conduct limited fine-tuning on domain-specific data to establish whether the performance gap between general and instruction-tuned models can be closed through adaptation.