---
ver: rpa2
title: Intent Detection in the Age of LLMs
arxiv_id: '2410.01627'
source_url: https://arxiv.org/abs/2410.01627
tags:
- intent
- detection
- llms
- datasets
- setfit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates the performance of 7 state-of-the-art LLMs
  for intent detection in task-oriented dialogue systems, comparing them against fine-tuned
  sentence transformer models. A hybrid system combining both approaches is proposed,
  achieving performance within 2% of native LLM accuracy with 50% less latency.
---

# Intent Detection in the Age of LLMs

## Quick Facts
- arXiv ID: 2410.01627
- Source URL: https://arxiv.org/abs/2410.01627
- Reference count: 14
- Primary result: Hybrid LLM/sentence transformer system achieves within 2% of native LLM accuracy with 50% less latency

## Executive Summary
This paper evaluates seven state-of-the-art LLMs for intent detection in task-oriented dialogue systems, comparing them against fine-tuned sentence transformer models. The authors propose a hybrid system combining both approaches that achieves near-LLM accuracy with significantly reduced latency. Through controlled experiments, they reveal that LLM out-of-scope detection capability is significantly influenced by intent label scope and label space size. A two-step methodology utilizing LLM internal representations demonstrates over 5% improvement in out-of-scope detection accuracy and F1-score for the Mistral-7B model.

## Method Summary
The paper evaluates LLM-based intent detection using adaptive in-context learning with chain-of-thought prompting against fine-tuned sentence transformer models. A hybrid system combines both approaches using uncertainty-based routing, where Monte Carlo dropout from SetFit determines when to route queries to the LLM. The authors introduce negative data augmentation to improve SetFit's decision boundaries and a two-step methodology using LLM internal representations for enhanced out-of-scope detection. Experiments span six datasets with varying intent distributions, label counts, and multi-label support.

## Key Results
- Hybrid system achieves within 2% of native LLM accuracy with 50% less latency
- Negative data augmentation improves SetFit performance by >5% across datasets
- Two-step methodology using LLM internal representations improves OOS detection accuracy and F1-score by >5% for Mistral-7B
- LLM OOS detection performance degrades significantly with larger label spaces and more ambiguous intent scopes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hybrid system achieves near-LLM accuracy with 50% less latency by conditionally routing queries based on SetFit's uncertainty
- **Mechanism:** SetFit's Monte Carlo dropout-based uncertainty estimation identifies when the model is uncertain about a query's intent. Uncertain queries are routed to the LLM for accurate detection, while confident queries are handled by the faster SetFit model.
- **Core assumption:** Variance in predictions across Monte Carlo samples reliably indicates true uncertainty
- **Evidence anchors:** Abstract states hybrid system achieves "within 2% of native LLM accuracy with 50% less latency"; section describes using MC dropout variance as uncertainty estimate
- **Break condition:** If SetFit's uncertainty estimation fails to capture true uncertainty, the hybrid system may route queries incorrectly

### Mechanism 2
- **Claim:** Negative data augmentation improves SetFit performance by >5% by helping learn better decision boundaries
- **Mechanism:** Training data is augmented by modifying keywords - removing them or replacing with random strings - treating these as OOS during training to help SetFit distinguish in-scope from OOS queries
- **Core assumption:** Augmented OOS sentences with similar lexical patterns to in-scope examples effectively help the model learn robust decision boundaries
- **Evidence anchors:** Section describes augmenting training data by modifying keywords and treating modified sentences as OOS; abstract states augmentation improves performance by >5%
- **Break condition:** If augmentation creates too many noisy examples or fails to maintain semantic similarity, it may confuse the model

### Mechanism 3
- **Claim:** Two-step methodology using LLM internal representations improves OOS detection accuracy by >5% for Mistral-7B
- **Mechanism:** Step 1: LLM predicts in-scope label without considering OOS. Step 2: Query representation is compared with training instances of predicted in-scope label using cosine similarity, reducing label space for OOS detection
- **Core assumption:** LLM's internal representations capture semantic similarity effectively, and comparison with training instances provides reliable OOS detection signal
- **Evidence anchors:** Abstract states methodology shows "empirical gains in OOS detection accuracy and F1-score by >5% across datasets for Mistral-7B"; section describes comparing query representation with training instances
- **Break condition:** If LLM's internal representations don't capture semantic similarity or comparison fails to distinguish OOS queries, method may not improve OOS detection

## Foundational Learning

- **Concept:** Monte Carlo dropout for uncertainty estimation
  - **Why needed here:** To determine when SetFit is uncertain about a query's intent, enabling conditional routing to LLM
  - **Quick check question:** How does Monte Carlo dropout estimate uncertainty in a neural network's predictions?

- **Concept:** In-context learning (ICL) and chain-of-thought (CoT) prompting
  - **Why needed here:** To adapt LLMs for intent detection using few-shot examples and explicit reasoning without fine-tuning
  - **Quick check question:** What is the difference between in-context learning and fine-tuning, and when would you choose one over the other?

- **Concept:** Sentence transformer models and contrastive learning
  - **Why needed here:** To understand how SetFit models are trained and how they encode semantic similarity between queries and intents
  - **Quick check question:** How do sentence transformers learn to map semantically similar sentences close together in embedding space?

## Architecture Onboarding

- **Component map:** Query -> SetFit model (with MC dropout) -> Uncertainty estimation -> Conditional routing to LLM -> Internal representation comparison -> Intent detection

- **Critical path:**
  1. Incoming query encoded and compared with training examples
  2. Top-k similar examples retrieved for ICL
  3. SetFit predicts intent with uncertainty estimation
  4. If uncertain, query routed to LLM for intent detection
  5. LLM's internal representations used for OOS detection if needed

- **Design tradeoffs:**
  - SetFit vs. LLM: Speed vs. accuracy tradeoff
  - Number of MC samples: Higher samples provide better uncertainty estimation but increase latency
  - ICL examples: More examples improve accuracy but increase prompt size and latency
  - Negative augmentation: More augmentation improves robustness but may introduce noise

- **Failure signatures:**
  - High latency: Too many queries routed to LLM or too many MC samples
  - Low accuracy: SetFit uncertainty estimation failing to capture true uncertainty or LLM's OOS detection failing
  - Model degradation: Negative augmentation introducing too much noise or ICL examples not being representative

- **First 3 experiments:**
  1. Evaluate SetFit's uncertainty estimation on held-out dataset to ensure it captures true uncertainty
  2. Test hybrid system's routing strategy with varying MC sample sizes to find optimal accuracy-latency balance
  3. Evaluate two-step OOS detection method with different similarity thresholds to find optimal balance between in-scope and OOS performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does performance scale when applied to multilingual datasets, particularly for low-resource languages?
- **Basis in paper:** [inferred] Paper mentions current evaluation focuses on English datasets and plans to generalize to multilingual settings
- **Why unresolved:** Paper doesn't provide empirical results or analysis on multilingual datasets
- **What evidence would resolve it:** Conducting experiments on diverse multilingual datasets including low-resource languages and comparing performance metrics across languages

### Open Question 2
- **Question:** What alternative hybrid strategies could be explored to improve balance between performance and latency?
- **Basis in paper:** [explicit] Paper suggests exploring alternative utility functions and comparing with model distillation strategies
- **Why unresolved:** Current work employs cascade routing strategy, but potential benefits of other strategies remain unexplored
- **What evidence would resolve it:** Implementing and evaluating alternative hybrid strategies such as information gain-based routing or model distillation, comparing performance and latency with current approach

### Open Question 3
- **Question:** How can interactive intent design be leveraged to enhance adaptability and accuracy of intent detection systems?
- **Basis in paper:** [explicit] Paper highlights potential for leveraging LLMs for interactive class design process but doesn't explore this approach
- **Why unresolved:** Current work assumes static intent design process, missing opportunities for dynamic adaptation and refinement
- **What evidence would resolve it:** Developing framework for interactive intent design using LLMs and evaluating its impact on system accuracy and adaptability

## Limitations

- Performance improvements are primarily demonstrated on Mistral-7B model, with limited generalization across different LLM architectures
- The two-step OOS detection methodology's effectiveness depends on the LLM correctly identifying the in-scope label in Step 1, which isn't addressed if this fails
- Controlled experiments on label space effects use synthetic modifications that may not reflect real-world intent distribution patterns

## Confidence

- **High Confidence:** Core finding that LLM OOS detection is sensitive to intent label scope and label space size - well-supported by controlled experiments and aligns with known scaling behaviors
- **Medium Confidence:** Hybrid system's performance claims are supported by methodology description, but exact implementation details make exact replication challenging
- **Low Confidence:** Specific numerical improvements (>5%) for two-step methodology are reported only for Mistral-7B and lack comparison across different LLM architectures

## Next Checks

1. **Uncertainty Estimation Validation:** Run controlled experiments testing SetFit's Monte Carlo dropout uncertainty estimates against ground truth uncertainty across different dataset characteristics to verify hybrid system's routing strategy reliability

2. **Generalization Across LLM Architectures:** Replicate two-step OOS detection methodology with different LLM families (GPT, Claude, open-source alternatives) to verify >5% improvement isn't specific to Mistral-7B's internal representation characteristics

3. **Label Space Sensitivity Analysis:** Conduct ablation studies systematically varying both S (intent scope) and L (label count) parameters to identify precise thresholds where OOS detection performance degrades, and test whether two-step methodology mitigates these effects consistently