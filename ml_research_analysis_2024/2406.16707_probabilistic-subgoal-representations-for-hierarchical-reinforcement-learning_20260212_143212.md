---
ver: rpa2
title: Probabilistic Subgoal Representations for Hierarchical Reinforcement learning
arxiv_id: '2406.16707'
source_url: https://arxiv.org/abs/2406.16707
tags:
- subgoal
- learning
- space
- representation
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HLPS, the first method to employ Gaussian Processes
  for probabilistic subgoal representations in hierarchical reinforcement learning.
  HLPS addresses the limitations of deterministic subgoal representations by modeling
  a posterior distribution over latent subgoal functions, enabling adaptation to stochastic
  uncertainties and novel states.
---

# Probabilistic Subgoal Representations for Hierarchical Reinforcement learning

## Quick Facts
- arXiv ID: 2406.16707
- Source URL: https://arxiv.org/abs/2406.16707
- Reference count: 40
- Primary result: First method using Gaussian Processes for probabilistic subgoal representations in HRL, outperforming baselines in sample efficiency, robustness, and transferability.

## Executive Summary
This paper introduces HLPS, a novel hierarchical reinforcement learning method that employs Gaussian Processes to model probabilistic subgoal representations. Unlike deterministic approaches, HLPS captures the uncertainty in subgoal transitions, enabling better adaptation to stochastic environments and novel states. The method leverages a learnable kernel to exploit long-range correlations in the state space and introduces a novel learning objective for joint optimization of representations and policies. Experimental results on continuous control tasks demonstrate significant improvements in sample efficiency, robustness, and transferability compared to state-of-the-art baselines.

## Method Summary
HLPS uses a two-level hierarchical policy framework with Gaussian Process (GP) based probabilistic subgoal representations. The method models subgoal representations as distributions rather than fixed values, allowing it to handle uncertainty and novel states. A learnable kernel in the GP layer exploits long-range correlations in the state space, providing a form of memory for subgoal information. The learning objective jointly optimizes GP hyperparameters and policy parameters by minimizing distances between low-level state transitions while maximizing distances for high-level transitions. This approach aims to improve stationarity of both high-level transitions and low-level reward functions.

## Key Results
- HLPS significantly outperforms deterministic baselines (LESSON, HESS, HRAC, TD3) in terms of sample efficiency and asymptotic performance on continuous control tasks
- The method demonstrates improved robustness to environmental stochasticity by modeling probabilistic subgoal representations
- HLPS shows promising transferability of learned low-level policies across tasks, with improved sample efficiency when initializing target task networks with weights from source tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HLPS improves sample efficiency and robustness by modeling a posterior distribution over subgoal representation functions rather than using a fixed deterministic mapping.
- Mechanism: Gaussian Processes (GPs) with learnable kernels encode a probabilistic prior over the latent subgoal space, allowing the model to adaptively exploit long-range correlations in the state space. This enables integration of subgoal information from previous planning steps, providing a form of memory that copes with stochastic uncertainties.
- Core assumption: The state space exhibits smooth and continuous latent subgoal structure that can be captured by GP priors with Matérn kernels, and that these priors can be effectively learned from finite support points.
- Evidence anchors:
  - [abstract] "utilizes Gaussian Processes (GPs) for the first probabilistic subgoal representation" and "exploiting the long-range correlation in the state space through learnable kernels"
  - [section 3.2] "We model the intermediate latent space representation f as a noise-corrupted version of the true latent subgoal space representation z, and the inference can be stated as the following GP regression model"
  - [corpus] Weak evidence; corpus papers focus on deterministic or graph-based subgoal representations, not probabilistic GP-based ones
- Break condition: If the state space lacks smooth, continuous structure or if the GP kernel hyperparameters cannot be reliably learned, the posterior distribution will not accurately capture the subgoal space, leading to poor performance.

### Mechanism 2
- Claim: The proposed learning objective jointly optimizes probabilistic subgoal representations and hierarchical policies within a unified framework, improving stationarity of both high-level transitions and low-level reward functions.
- Mechanism: The learning objective minimizes distances between low-level state transitions in latent space while maximizing distances for high-level transitions, using a softplus function to provide continuous gradients. This encourages smooth yet discriminative subgoal representations.
- Core assumption: The ratio of distances between low-level and high-level transitions in the latent space is a meaningful measure of feature discrimination that can guide effective representation learning.
- Evidence anchors:
  - [section 3.3] "Our insight is that the long-range correlation in the state space could be exploited through learnable kernel function" and the description of the learning objective minimizing and maximizing distances
  - [section 5.2] "HLPS consistently learns stable subgoal representations throughout training, in contrast to HESS and LESSON" and the observation that subgoals align with low-level trajectories
  - [corpus] Weak evidence; corpus papers focus on different learning objectives (e.g., slowness, novelty) rather than this specific contrastive approach
- Break condition: If the distance ratios do not correlate with meaningful task structure, or if the softplus function fails to provide adequate gradient signals, the learning objective may not converge to useful representations.

### Mechanism 3
- Claim: HLPS enables transferability of learned subgoal representations and low-level policies between tasks of the same agent, improving sample efficiency in new tasks.
- Mechanism: The GP-based probabilistic representation learns hyperparameters that generalize across tasks, allowing initialization of target task networks with weights from source tasks. This provides a head start in learning the new task.
- Core assumption: The subgoal space structure is similar enough across tasks of the same agent that learned hyperparameters can be transferred effectively.
- Evidence anchors:
  - [section 5.2] "The generality of our GP based subgoal representation learning framework underpins transferable subgoal space as well as the low-level policy between different tasks" and the experimental results showing improved sample efficiency when transferring from Ant Fall to Ant Push
  - [section 5.2] "In the tasks with image input, the benefit of probabilistic subgoal representation of our method is more substantial, since learning the subgoal representation in a higher dimensional state space is more challenging"
  - [corpus] Weak evidence; corpus papers focus on hierarchical learning but do not emphasize cross-task policy transfer using probabilistic representations
- Break condition: If the subgoal space structure differs significantly between tasks, or if the transferred weights are not well-suited to the target task, performance may degrade compared to training from scratch.

## Foundational Learning

- Concept: Gaussian Processes and their use in modeling distributions over functions
  - Why needed here: GPs provide a principled way to model uncertainty in the subgoal representation, allowing the model to adapt to novel states and stochastic environments
  - Quick check question: How does a GP differ from a neural network in terms of the type of function it models, and why is this difference important for handling uncertainty?

- Concept: Hierarchical Reinforcement Learning and the role of subgoal representations
  - Why needed here: Understanding how subgoals bridge the gap between high-level planning and low-level execution is crucial for grasping why the representation function matters
  - Quick check question: In goal-conditioned HRL, what are the two main functions of the subgoal representation, and how do they relate to stationarity?

- Concept: Contrastive learning and its application to representation learning
  - Why needed here: The learning objective uses a contrastive approach to encourage discriminative subgoal representations, which is key to understanding how the model learns
  - Quick check question: What is the purpose of using a contrastive loss in representation learning, and how does the softplus function contribute to this in HLPS?

## Architecture Onboarding

- Component map: State -> Encoding layer -> Intermediate latent representation -> GP layer -> Probabilistic subgoal representation -> High-level policy -> Next subgoal; Subgoal + state -> Low-level policy -> Action

- Critical path:
  1. State → encoding layer → intermediate latent representation
  2. Intermediate representation → GP layer → probabilistic subgoal representation
  3. Subgoal → high-level policy → next subgoal
  4. Subgoal + state → low-level policy → action
  5. Update GP hyperparameters and policies using the learning objective

- Design tradeoffs:
  - Deterministic vs. probabilistic subgoal representations: Probabilistic allows handling uncertainty but adds computational complexity
  - Fixed vs. learnable kernel hyperparameters: Learnable kernels can adapt to task structure but require more data to train
  - Batch vs. online inference: Batch inference can use more data but is computationally expensive; online inference is efficient but may be less accurate

- Failure signatures:
  - Unstable subgoal representations during training: May indicate issues with the learning objective or insufficient data
  - Poor transfer performance: May suggest that the subgoal space structure is not similar enough across tasks
  - Degraded performance with environmental stochasticity: May indicate that the GP model is not capturing the true uncertainty

- First 3 experiments:
  1. Reproduce the deterministic baseline (e.g., LESSON) to establish a performance benchmark
  2. Train HLPS on a simple deterministic task (e.g., Ant Maze) to verify that the GP layer and learning objective work as expected
  3. Introduce environmental stochasticity (e.g., Gaussian noise) to test the robustness of the probabilistic representation

## Open Questions the Paper Calls Out
- None explicitly stated in the paper.

## Limitations
- Limited theoretical analysis of convergence and generalization properties of the GP-based representation learning
- Dependence on learnable kernel hyperparameters that may not generalize well across all environments
- Empirical validation primarily on continuous control tasks, with unclear applicability to other domains

## Confidence
- **High confidence** in the architectural novelty and mathematical formulation of the GP-based probabilistic representation
- **Medium confidence** in the claimed performance improvements and robustness, as these are based on specific benchmark tasks
- **Low confidence** in the transferability claims, as only one cross-task transfer experiment is presented

## Next Checks
1. Ablation studies varying the GP kernel hyperparameters and latent dimension size to understand their impact on performance and generalization
2. Theoretical analysis of the convergence properties of the joint optimization objective for GP hyperparameters and policy parameters
3. Extended transfer experiments across multiple task pairs and agent types to validate the generalizability of the subgoal representation transfer capability