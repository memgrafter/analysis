---
ver: rpa2
title: 'ScaleNet: Scale Invariance Learning in Directed Graphs'
arxiv_id: '2411.08758'
source_url: https://arxiv.org/abs/2411.08758
tags:
- graph
- graphs
- scalenet
- node
- scaled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving node classification
  in graph neural networks (GNNs) by introducing scale invariance learning. The core
  idea is to extend the concept of scale invariance, well-known in image classification,
  to graphs by defining "scaled ego-graphs." These generalize traditional ego-graphs
  by replacing single undirected edges with ordered sequences of multiple directed
  edges, enabling the capture of multi-scale features.
---

# ScaleNet: Scale Invariance Learning in Directed Graphs

## Quick Facts
- arXiv ID: 2411.08758
- Source URL: https://arxiv.org/abs/2411.08758
- Reference count: 40
- Primary result: State-of-the-art performance on node classification in directed graphs, achieving top accuracy on 5 out of 7 benchmark datasets

## Executive Summary
This paper introduces ScaleNet, a novel approach to node classification in graph neural networks that leverages scale invariance through multi-scale ego-graphs. By replacing traditional undirected edges with ordered sequences of directed edges, ScaleNet captures hierarchical neighborhood information at different scales while preserving essential structural features for classification. The architecture outperforms existing models like Digraph Inception Networks on both homophilic and heterophilic graphs, demonstrating robustness across diverse graph structures through adaptive bidirectional aggregation strategies.

## Method Summary
ScaleNet extends traditional GNN architectures by introducing multi-scale ego-graphs that capture hierarchical neighborhood information through ordered sequences of directed edges. The model constructs multiple scaled adjacency matrices and uses bidirectional aggregation blocks to combine information from opposite directional matrices, controlled by parameter α. A layer-wise combination strategy (COMB1) and output combination (COMB2) aggregate information across scales and layers, with optional components including self-loops, batch normalization, and dropout. The architecture achieves scale invariance by preserving essential structural information for node classification across different scales of the ego-graph.

## Key Results
- Achieves state-of-the-art performance on 5 out of 7 benchmark datasets
- Outperforms Digraph Inception Networks across both homophilic and heterophilic graph settings
- Demonstrates robust performance through adaptive bidirectional aggregation strategies
- Shows that simplified constant edge weights (typically 1) outperform computationally expensive eigenvalue decomposition methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-scale ego-graphs preserve essential structural information for node classification, enabling scale invariance.
- Mechanism: By replacing undirected single-edges with ordered sequences of multiple directed edges ("scaled-edges"), the model captures hierarchical neighborhood information at different scales without losing classification-relevant features.
- Core assumption: The classification of a node remains invariant across different scaled ego-graphs, meaning essential structural information is preserved across scales.
- Evidence anchors:
  - [abstract] "Scaled ego-graphs generalize traditional ego-graphs by replacing undirected single-edges with 'scaled-edges', which are ordered sequences of multiple directed edges."
  - [section] "This property implies that the essential structural information for node classification is preserved across different scales of the ego-graph."
  - [corpus] No direct evidence found in related papers; this appears to be the novel theoretical contribution of the paper.
- Break condition: If higher-scale graphs introduce irrelevant structural noise or if the invariance property doesn't hold for specific graph types, classification performance would degrade.

### Mechanism 2
- Claim: Combining scaled graphs through bidirectional aggregation improves robustness, especially for heterophilic graphs.
- Mechanism: The AGG-Bα function combines information from both in-neighbors and out-neighbors using a weighted sum controlled by parameter α, allowing the model to adaptively focus on relevant directional information.
- Core assumption: Different graph types benefit from different combinations of directional information, and the model can learn optimal parameters through grid search.
- Evidence anchors:
  - [section] "ScaleNet is designed to adapt to the unique characteristics of each dataset, offering optimal performance for both homophilic and heterophilic graphs."
  - [section] "During tuning with a grid search of hyperparameters, we observed the following findings: Homophilic Graphs: performance is improved by adding self-loops and using both scaled graphs based on opposite directed scaled edges... Heterophilic Graphs: performance benefits from removing self-loops and utilizing scaled graphs with preferred directional scaled edges."
  - [corpus] No direct evidence in related papers; this appears to be a novel architectural contribution.
- Break condition: If the grid search cannot find optimal parameters for a given dataset, or if the bidirectional aggregation introduces conflicting information that confuses the model.

### Mechanism 3
- Claim: Simplified inception models without complex edge weight calculations outperform computationally expensive alternatives.
- Mechanism: Instead of using resource-intensive eigenvalue decomposition or Markov process-based edge weights, ScaleNet assigns constant weights (typically 1) to all scaled edges, reducing computational complexity while maintaining or improving performance.
- Core assumption: Complex edge weight calculations are not necessary for capturing meaningful multi-scale features in graph learning.
- Evidence anchors:
  - [section] "Our inception models achieve better performance by simply assigning a value of 1 to all scaled edges, making them simpler, faster, and more accurate."
  - [section] "We further demonstrate that the heavily computed edge weights used by DiGCN are not always desirable, as they may result in worse performance compared to even randomly assigned weights."
  - [section] "The results, shown in the 'Symmetric Type' section of Table 4 indicate that 1iG generally outperforms DiG across all datasets."
- Break condition: If certain graph structures require carefully weighted edges to capture important relationships, the constant-weight approach might miss critical information.

## Foundational Learning

- Concept: Graph Neural Networks and their limitations with directed graphs
  - Why needed here: The paper builds on existing GNN architectures but extends them to handle directed graphs and scale invariance, so understanding GNN fundamentals is essential.
  - Quick check question: What is the key difference between spatial and spectral methods for GNNs, and why do spectral methods struggle with directed graphs?

- Concept: Scale invariance in image classification and its extension to graphs
  - Why needed here: The core innovation involves adapting scale invariance from image processing to graph structures, requiring understanding of how scale invariance works in different domains.
  - Quick check question: How does scale invariance in image classification differ from the proposed scale invariance in graphs, and what makes the graph extension non-trivial?

- Concept: Ego-graphs and neighborhood aggregation in GNNs
  - Why needed here: The paper relies on ego-graphs as the fundamental unit of analysis for node classification, so understanding how information flows through neighborhoods is crucial.
  - Quick check question: What is an ego-graph, and how does aggregating information from different depths of an ego-graph affect node classification performance?

## Architecture Onboarding

- Component map: Input layer -> Scaled graph generation -> Bidirectional aggregation blocks (AGG-B) -> Layer-wise combination (COMB1) -> Multi-layer stacking -> Output combination (COMB2)
- Critical path: Graph → Scaled adjacency matrices → Bidirectional aggregation → Layer-wise combination → Multi-layer stacking → Output
- Design tradeoffs:
  - Computational efficiency vs. performance: Constant edge weights (faster) vs. complex weight calculations (potentially more accurate)
  - Model complexity vs. generalization: More scales and layers can capture more information but may overfit
  - Directional information vs. simplicity: Bidirectional aggregation captures more information but increases parameter space
- Failure signatures:
  - Poor performance on heterophilic graphs: May indicate incorrect α parameter settings or need to remove self-loops
  - Memory issues on large graphs: May require reducing number of scales or layers
  - Overfitting on small datasets: May need stronger regularization or fewer model parameters
  - Slow training: May benefit from reducing number of scales or using more efficient aggregation methods
- First 3 experiments:
  1. Compare single-scale vs. multi-scale performance on a homophilic dataset (e.g., Cora-ML) to verify scale invariance benefits
  2. Test different α parameter settings on a heterophilic dataset (e.g., Chameleon) to find optimal directional aggregation
  3. Evaluate the impact of self-loops on performance across different graph types to understand when they help vs. hurt

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ScaleNet's performance scale with larger graphs, and what are the computational bottlenecks?
- Basis in paper: [inferred] The paper mentions that existing methods like DiGCN and SymDiGCN are computationally expensive and have scalability issues, but ScaleNet's scalability is not explicitly tested.
- Why unresolved: The paper only evaluates ScaleNet on relatively small datasets and does not provide a thorough analysis of its performance on larger graphs or identify potential computational bottlenecks.
- What evidence would resolve it: Benchmarking ScaleNet on larger graph datasets and profiling its computational performance to identify bottlenecks.

### Open Question 2
- Question: How does ScaleNet's performance compare to other graph neural network architectures that also handle heterophilic graphs, such as Signed Graph Neural Networks (SGNNs) or Directional Graph Networks (DGNs)?
- Basis in paper: [inferred] The paper mentions that ScaleNet performs well on heterophilic graphs, but does not compare its performance to other specialized methods for handling heterophily.
- Why unresolved: The paper only compares ScaleNet to a limited set of baselines and does not explore the broader landscape of GNN architectures designed for heterophilic graphs.
- What evidence would resolve it: Conducting experiments comparing ScaleNet's performance to other state-of-the-art GNN architectures on heterophilic graph datasets.

### Open Question 3
- Question: Can ScaleNet's scale invariance property be extended to other graph learning tasks beyond node classification, such as graph classification or link prediction?
- Basis in paper: [inferred] The paper focuses on scale invariance in the context of node classification, but does not explore its applicability to other graph learning tasks.
- Why unresolved: The paper only demonstrates the benefits of scale invariance for node classification and does not investigate whether this property can be leveraged for other graph learning tasks.
- What evidence would resolve it: Applying ScaleNet or a similar scale-invariant approach to graph classification and link prediction tasks and evaluating its performance.

## Limitations

- Computational complexity of constructing multiple scaled adjacency matrices is not thoroughly analyzed
- Scalability to larger graphs remains unclear with only small datasets evaluated
- Comparison with baseline models could benefit from more extensive ablation studies to isolate individual component contributions

## Confidence

- Mechanism 1 (Scale invariance through multi-scale ego-graphs): **Medium** - The theoretical framework is well-defined, but empirical validation across diverse graph types is limited
- Mechanism 2 (Bidirectional aggregation for heterophilic graphs): **Medium-High** - Supported by grid search results, but the parameter optimization process could be more automated
- Mechanism 3 (Simplified edge weights outperforming complex calculations): **High** - Strong empirical evidence with multiple datasets showing consistent performance gains

## Next Checks

1. **Ablation study on aggregation strategies**: Systematically remove the bidirectional aggregation (AGG-B) component and test whether single-directional aggregation can achieve comparable performance, particularly on heterophilic datasets

2. **Scalability benchmark**: Evaluate ScaleNet on larger graph datasets (e.g., OGB datasets) to assess memory usage and training time as graph size increases, comparing against DiGCN and other baseline methods

3. **Parameter sensitivity analysis**: Conduct a more exhaustive grid search across a wider range of α, β, and γ values to determine if the current parameter settings are truly optimal or if better configurations exist that were not explored