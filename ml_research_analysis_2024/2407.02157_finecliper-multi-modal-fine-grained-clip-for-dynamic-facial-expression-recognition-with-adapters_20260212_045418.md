---
ver: rpa2
title: 'FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition
  with AdaptERs'
arxiv_id: '2407.02157'
source_url: https://arxiv.org/abs/2407.02157
tags:
- facial
- expression
- finecliper
- recognition
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FineCLIPER, a multi-modal framework for dynamic
  facial expression recognition (DFER) that addresses challenges of semantic ambiguity
  and insufficient facial dynamics modeling. The core idea is to extend class labels
  into positive-negative textual descriptors and mine hierarchical features from videos
  using face parsing, landmarks, and MLLM-generated fine-grained descriptions.
---

# FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs

## Quick Facts
- arXiv ID: 2407.02157
- Source URL: https://arxiv.org/abs/2407.02157
- Reference count: 40
- Primary result: Achieves SOTA performance on DFEW (65.98% UAR), FERV39k (45.22% UAR), and MAFW (45.01% UAR) with minimal tunable parameters (13-20M)

## Executive Summary
FineCLIPER addresses the challenges of semantic ambiguity and insufficient facial dynamics modeling in Dynamic Facial Expression Recognition (DFER) by extending class labels into positive-negative textual descriptors and mining hierarchical features from videos. The framework leverages face parsing, landmarks, and MLLM-generated fine-grained descriptions to capture multi-level visual information. By using parameter-efficient fine-tuning (PEFT) of pre-trained models like CLIP, FineCLIPER achieves state-of-the-art performance on three major DFER datasets while maintaining computational efficiency.

## Method Summary
FineCLIPER extends class labels into positive-negative textual descriptors to address semantic ambiguity in facial expression recognition. The framework extracts hierarchical features from videos: low-level features from video frames, middle-level features from face parsing and landmarks, and high-level features from MLLM-generated fine-grained textual descriptions. These features are integrated using weighted similarity-based scores and processed through adapter-based parameter-efficient fine-tuning of CLIP. The model is trained for 30 epochs with SGD optimizer, achieving superior performance with minimal tunable parameters (13-20M) across supervised and zero-shot settings.

## Key Results
- Achieves 65.98% UAR on DFEW, outperforming existing methods
- Achieves 45.22% UAR on FERV39k, setting new SOTA benchmark
- Achieves 45.01% UAR on MAFW with minimal trainable parameters (13-20M)

## Why This Works (Mechanism)
FineCLIPER addresses two core challenges in DFER: semantic ambiguity and insufficient facial dynamics modeling. By extending labels to positive-negative textual descriptors, the model learns more discriminative features for each expression class. The hierarchical feature extraction captures multi-level visual information - from basic frame-level dynamics to fine-grained semantic descriptions - providing comprehensive representation of facial expressions. The parameter-efficient fine-tuning approach enables effective adaptation of large pre-trained models while maintaining computational efficiency.

## Foundational Learning
- **Parameter-efficient fine-tuning (PEFT)**: Adapts large pre-trained models with minimal parameter updates, crucial for maintaining performance while reducing computational cost. Quick check: Verify adapter configurations match QLoRA/Lora specifications.
- **Multi-modal feature integration**: Combines visual and textual information through weighted similarity scores, essential for capturing comprehensive expression representations. Quick check: Validate weighted integration formula implementation.
- **Hierarchical feature extraction**: Extracts features at multiple semantic levels (low, middle, high) to capture complete expression information. Quick check: Confirm feature extraction pipeline from video to fine-grained text.
- **Positive-negative descriptor extension**: Expands class labels to include both target and non-target expressions, improving semantic discrimination. Quick check: Verify "A person with an expression of {Label}" and "A person with an expression of no {Label}" formatting.

## Architecture Onboarding

**Component Map**: Video frames -> Face parsing/landmarks -> MLLM fine-grained text -> Adapter-based PEFT -> Weighted feature integration -> Classification

**Critical Path**: The primary inference flow follows: video input → multi-modal feature extraction (frames, parsing, landmarks, MLLM) → adapter-based encoding → weighted feature integration → final classification. The most compute-intensive step is the multi-modal feature extraction and adapter processing.

**Design Tradeoffs**: The framework trades model size for performance by using parameter-efficient fine-tuning instead of full model fine-tuning. This reduces trainable parameters from millions to 13-20M while maintaining or improving accuracy. The hierarchical feature approach adds complexity but captures more comprehensive expression information compared to single-modality methods.

**Failure Signatures**: Poor performance on "Disgust" category indicates issues with negative descriptor implementation or semantic ambiguity handling. Inconsistent results across runs suggest problems with random seed initialization or adapter configuration mismatches.

**First Experiments**: 1) Test basic CLIP-ViT-B/16 baseline on single dataset to verify fundamental functionality. 2) Implement and validate positive-negative descriptor generation with manual inspection. 3) Verify hierarchical feature extraction pipeline with intermediate output visualization.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Performance heavily depends on the quality of MLLM-generated fine-grained descriptions, but evaluation of description quality is not thoroughly presented
- The exact prompt template for Video-LLaVA generation is only partially specified, affecting reproducibility
- Implementation details of weighted feature integration and specific adapter configurations are not fully specified

## Confidence
- **High confidence** in the overall framework design and reported performance improvements over baseline methods
- **Medium confidence** in the implementation details of the multi-modal feature integration and adapter configurations
- **Low confidence** in the exact reproducibility of results without the complete prompt template and adapter specifications

## Next Checks
1. Verify the complete prompt template for Video-LLaVA generation by testing different prompt variations and evaluating their impact on downstream performance
2. Implement and test the exact adapter configurations (QLoRA/Lora) to confirm the reported parameter counts (13-20M) match the actual implementation
3. Conduct ablation studies to quantify the individual contributions of each modality (video, parsing, landmarks, fine-grained text) to overall performance