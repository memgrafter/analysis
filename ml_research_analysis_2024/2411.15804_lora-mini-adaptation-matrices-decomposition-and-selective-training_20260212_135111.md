---
ver: rpa2
title: 'LoRA-Mini : Adaptation Matrices Decomposition and Selective Training'
arxiv_id: '2411.15804'
source_url: https://arxiv.org/abs/2411.15804
tags:
- lora
- parameters
- language
- fine-tuning
- matrices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LoRA-Mini addresses the high storage overhead of LoRA by decomposing
  its low-rank adaptation matrices into four parts, with only the two inner matrices
  being trainable. This selective training reduces the number of trainable parameters
  by up to 20x compared to standard LoRA.
---

# LoRA-Mini : Adaptation Matrices Decomposition and Selective Training

## Quick Facts
- arXiv ID: 2411.15804
- Source URL: https://arxiv.org/abs/2411.15804
- Reference count: 40
- Reduces LoRA trainable parameters by up to 20x while maintaining performance

## Executive Summary
LoRA-Mini addresses the high storage overhead of LoRA by decomposing its low-rank adaptation matrices into four parts, with only the two inner matrices being trainable. This selective training reduces the number of trainable parameters by up to 20x compared to standard LoRA. Evaluated on GLUE and WMT16 benchmarks across BERT, RoBERTa, and T5 models, LoRA-Mini achieves performance comparable to full fine-tuning and LoRA while significantly improving memory efficiency.

## Method Summary
LoRA-Mini decomposes each LoRA matrix (A and B) into four components: two frozen auxiliary matrices (Aaux, Baux) and two trainable inner matrices (Atrain, Btrain). Only Atrain and Btrain are updated during training, while Aaux and Baux remain frozen. The method uses Kaiming initialization with coefficient √5 for all matrices to ensure stable gradient propagation. The weight update follows the formula ΔW = Aaux·Atrain·Btrain·Baux, operating within a controlled subspace to maintain model stability while enabling efficient parameter optimization.

## Key Results
- Achieves performance comparable to full fine-tuning and standard LoRA across GLUE and WMT16 benchmarks
- Reduces trainable parameters by up to 20x compared to standard LoRA
- Tested on BERT-base, RoBERTa-base, T5-small, and T5-base models with various rank configurations (8, 16, 32)

## Why This Works (Mechanism)

### Mechanism 1
Selective freezing of outer matrices in LoRA decomposition reduces trainable parameters while preserving performance. By decomposing each LoRA matrix into auxiliary (frozen) and trainable parts, only the inner matrices (Atrain and Btrain) are updated during fine-tuning, reducing trainable parameters from r×(d+k) to r×(a+b).

### Mechanism 2
Kaiming initialization with coefficient √5 ensures stable gradient propagation in the decomposed LoRA-Mini architecture. The Kaiming initialization method scales the weight variance appropriately for the decomposed matrices, preventing gradient explosion or vanishing during training.

### Mechanism 3
Constraining weight updates to a controlled subspace through selective training maintains model stability while enabling efficient adaptation. By limiting updates to only the inner trainable matrices, the weight update operates within a controlled subspace, preventing large deviations from pre-trained weights while still allowing task-specific adaptation.

## Foundational Learning

- Concept: Low-Rank Matrix Decomposition
  - Why needed here: Understanding how LoRA reduces parameter count by decomposing weight updates into low-rank matrices is fundamental to grasping LoRA-Mini's extension.
  - Quick check question: Why does decomposing a d×k matrix into d×r and r×k matrices (where r << min(d,k)) reduce parameters from d×k to r×(d+k)?

- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: LoRA-Mini is a PEFT method, so understanding the broader landscape of PEFT approaches helps contextualize its contributions.
  - Quick check question: How does LoRA-Mini's parameter reduction compare to other PEFT methods like adapters or prefix-tuning?

- Concept: Matrix Initialization Techniques
  - Why needed here: The Kaiming initialization used in LoRA-Mini is critical for training stability with reduced parameters.
  - Quick check question: What is the purpose of using Kaiming initialization with coefficient √5 in neural network weight initialization?

## Architecture Onboarding

- Component map: Pre-trained model weights (frozen) -> Outer auxiliary matrices (Aaux, Baux) (frozen) -> Inner trainable matrices (Atrain, Btrain) (trainable) -> LoRA-Mini layer

- Critical path:
  1. Forward pass through pre-trained model
  2. Apply LoRA-Mini adaptation by computing (W + Aaux·Atrain·Btrain·Baux)·x
  3. Compute loss and gradients
  4. Backpropagate only through trainable inner matrices
  5. Update only Atrain and Btrain

- Design tradeoffs:
  - Memory efficiency vs. expressivity: Fewer trainable parameters reduce memory but may limit adaptation capacity
  - Stability vs. flexibility: Frozen outer matrices provide stability but may constrain necessary adaptations
  - Implementation complexity vs. performance gain: Additional decomposition adds complexity but yields significant parameter reduction

- Failure signatures:
  - Training instability: Could indicate poor initialization or insufficient parameter count
  - Performance degradation: May suggest frozen outer matrices are too restrictive
  - Memory usage not reducing as expected: Could indicate implementation errors in matrix decomposition

- First 3 experiments:
  1. Ablation study: Compare performance with different combinations of frozen vs. trainable matrices (e.g., only A frozen, only B frozen, both frozen)
  2. Rank sensitivity: Test how performance varies with different values of r (the rank of inner matrices) while keeping a and b constant
  3. Task-specific analysis: Evaluate whether certain tasks benefit more from LoRA-Mini than others, identifying when the approach is most effective

## Open Questions the Paper Calls Out

### Open Question 1
How does LoRA-Mini perform on larger models and more extensive benchmarks compared to standard LoRA and full fine-tuning? The authors explicitly state that due to computational constraints, they were unable to evaluate their approach on larger models such as LLaMA3.1-8B and Mistral-7B, or on more extensive benchmarks like MMLU, Math10k, and COMMONSENSE170K.

### Open Question 2
What is the impact of different matrix decomposition methods (e.g., QR decomposition, SVD) on the performance and efficiency of LoRA-Mini? The authors suggest that future research could explore alternative matrix decomposition methods, such as QR decomposition or singular value decomposition (SVD), to initialize the matrices.

### Open Question 3
How does selectively freezing different combinations of matrices within the LoRA framework affect model performance and parameter efficiency? The authors mention that future research could explore techniques like freezing randomly selected matrices to increase parameter efficiency.

## Limitations
- Lacks detailed training hyperparameters (learning rate, batch size, optimizer settings) critical for reproduction
- Limited ablation studies that would isolate the contribution of each component
- No empirical validation of the controlled subspace stability argument

## Confidence
- High confidence: Mathematical validity of decomposition mechanism and parameter reduction claims
- Medium confidence: Performance parity claims given limited ablation studies and lack of sensitivity analysis
- Low confidence: Stability argument for controlled subspace updates without empirical validation

## Next Checks
1. **Ablation study**: Systematically test different combinations of frozen vs. trainable matrices to isolate the contribution of each component to overall performance
2. **Rank sensitivity analysis**: Evaluate how performance scales with different rank values (r) and inner matrix dimensions (a, b) to identify optimal configurations
3. **Task transferability**: Test whether LoRA-Mini's effectiveness transfers across diverse task types (classification, generation, translation) to validate its general applicability