---
ver: rpa2
title: Efficient Adaptation of Pre-trained Vision Transformer via Householder Transformation
arxiv_id: '2410.22952'
source_url: https://arxiv.org/abs/2410.22952
tags:
- matrix
- adaptation
- householder
- matrices
- projection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a parameter-efficient fine-tuning approach for
  Vision Transformers using Householder transformations. The method decomposes the
  adaptation matrix via SVD, replacing unitary matrices with Householder transformations
  derived from learnable vectors.
---

# Efficient Adaptation of Pre-trained Vision Transformer via Householder Transformation

## Quick Facts
- arXiv ID: 2410.22952
- Source URL: https://arxiv.org/abs/2410.22952
- Authors: Wei Dong; Yuan Sun; Yiting Yang; Xing Zhang; Zhijun Lin; Qingsen Yan; Haokui Zhang; Peng Wang; Yang Yang; Hengtao Shen
- Reference count: 40
- Key outcome: HTA achieves 1.2% improvement over state-of-the-art on VTAB-1k while using half the parameters

## Executive Summary
This paper introduces Householder Transformation-based Adaptor (HTA), a parameter-efficient fine-tuning method for Vision Transformers that decomposes adaptation matrices using Householder transformations. By replacing unitary matrices in SVD decomposition with Householder transformations derived from learnable vectors, HTA achieves competitive performance with significantly fewer parameters than existing methods. The approach includes layer-wise learned diagonal scaling parameters and an optional low-rank component for enhanced robustness.

## Method Summary
HTA adapts pre-trained ViT weights using Householder transformations to construct orthogonal matrices efficiently. The method decomposes the adaptation matrix via SVD, replacing unitary matrices with Householder matrices H = I - v⃗v⃗⊤ constructed from single learnable vectors. Diagonal scaling parameters are learned layer-wise to capture unique properties of each layer, with an optional low-rank adaptation matrix added for robustness. The approach demonstrates significant parameter efficiency while maintaining competitive accuracy on FGVC and VTAB-1k benchmarks.

## Key Results
- On VTAB-1k benchmark, HTA achieves 1.2% improvement over state-of-the-art methods while using half the parameters
- Outperforms previous best methods on FGVC datasets including CUB-200-2011, NABirds, Oxford Flowers, Stanford Dogs, and Stanford Cars
- Demonstrates flexible adaptation capacity with layer-wise diagonal scaling parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Householder transformations provide a parameter-efficient way to construct orthogonal matrices for low-rank adaptation
- Mechanism: The Householder matrix H = I - v⃗v⃗⊤ is constructed from a single vector v⃗, maintaining orthogonality properties while requiring fewer parameters than standard unitary matrices
- Core assumption: Orthogonal matrices can be effectively replaced by Householder transformations without significant loss in representation capacity
- Evidence anchors:
  - [abstract] "We utilize Householder transformations to construct orthogonal matrices that efficiently mimic the unitary matrices, requiring only a vector."
  - [section 3.3] "Since the Householder transformation matrix is derived from a single vector, its transformation capacity can be limited and sensitive to the vector learned to derive it."
  - [corpus] Weak evidence - corpus contains related work on orthogonal adaptations but no direct validation of Householder parameter efficiency
- Break condition: When the transformation capacity of Householder matrices becomes insufficient for the task, requiring more general orthogonal bases

### Mechanism 2
- Claim: Layer-wise adaptive diagonal scaling provides flexibility to handle layer-specific properties
- Mechanism: The diagonal matrix D(l) contains learnable scaling values that can vary across layers, allowing different adaptation ranks for each layer
- Core assumption: Different layers in ViT have distinct properties that benefit from layer-specific adaptation capacity
- Evidence anchors:
  - [abstract] "The diagonal values are learned in a layer-wise manner, allowing them to flexibly capture the unique properties of each layer."
  - [section 3.3] "We learn the diagonal matrix adaptively for each layer to accommodate layer-wise properties."
  - [section 3.2] "However, these methods usually set the bottleneck dimensionality empirically to balance adaptation performance and parameter size."
- Break condition: When layer-wise variation is minimal and a fixed-rank approach would suffice

### Mechanism 3
- Claim: Combining Householder transformations with low-rank adaptation matrices improves robustness
- Mechanism: The final HTA adaptation matrix is W(l)HTA = (I - v⃗(l)left⃗v⃗(l)⊤left)D(l)H(I - v⃗(l)right⃗v⃗(l)⊤right) + W(l)downW(l)up, adding a rank-1 adaptation matrix to enhance transformation capacity
- Core assumption: Pure Householder-based adaptation may have limited capacity, requiring additional low-rank components for robustness
- Evidence anchors:
  - [section 3.3] "To enhance the robustness of the adaptation matrix, we incorporate an additional low-rank adaptation matrix, resulting in the ultimate HTA."
  - [section 4.3] "Without the addition of low-rank adaptation, HTA experiences an obvious performance drop."
  - [corpus] Moderate evidence - related work on combining adaptation strategies exists in the literature
- Break condition: When the additional low-rank component becomes unnecessary due to sufficient Householder transformation capacity

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and its role in matrix factorization
  - Why needed here: The method is inspired by SVD's decomposition into left unitary, diagonal, and right unitary matrices
  - Quick check question: What are the three components of SVD decomposition and what role does each play in the transformation?

- Concept: Householder transformations and orthogonal matrix construction
  - Why needed here: Householder matrices are used to replace unitary matrices in the adaptation matrix construction
  - Quick check question: How is a Householder matrix constructed from a single vector, and why is it orthogonal?

- Concept: Low-rank matrix decomposition and its parameter efficiency
  - Why needed here: Understanding why decomposing matrices into low-rank products reduces parameters is crucial for grasping the method's efficiency
  - Quick check question: How does decomposing a matrix W into W = WdownWup reduce the number of learnable parameters?

## Architecture Onboarding

- Component map:
  - Input: Pre-trained ViT weights (frozen)
  - Householder components: Learnable vectors v⃗left, v⃗right, and diagonal scaling parameters d⃗
  - Low-rank components: Optional Wdown, Wup matrices (typically rank-1)
  - Output: Adapted weight matrices for downstream tasks

- Critical path: Pre-trained weights → Householder transformations → Diagonal scaling → (Optional) Low-rank addition → Fine-tuned weights

- Design tradeoffs:
  - Parameter efficiency vs. adaptation capacity: Pure Householder (very efficient) vs. Householder + low-rank (more capacity)
  - Layer-wise flexibility vs. parameter count: More diagonal parameters allow layer-specific adaptation but increase total parameters
  - Orthogonal structure vs. general representation: Householder matrices are orthogonal but may have limited span compared to general bases

- Failure signatures:
  - Performance plateaus despite increased parameters (Householder transformation capacity saturated)
  - Layer-specific degradation patterns (diagonal scaling not capturing layer properties)
  - Training instability with certain vector initializations (Householder reflection causing numerical issues)

- First 3 experiments:
  1. Baseline comparison: Replace HTA with pure LoRA on a single layer to measure parameter efficiency gains
  2. Ablation study: Test HTA with and without the low-rank addition component to quantify robustness benefits
  3. Layer-wise analysis: Apply different diagonal ranks to different layers to identify optimal layer-specific configurations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Householder transformation-based adaptation method compare in performance to traditional low-rank adaptation methods when applied to larger and more complex vision transformer architectures beyond ViT-Base and ViT-Large?
- Basis in paper: [explicit] The paper mentions that experiments were conducted on ViT-Base, ViT-Large, and Swin Transformer architectures, but it does not explore the scalability of the method to even larger models or different types of architectures.
- Why unresolved: The paper's experiments are limited to specific vision transformer models, and there is no exploration of how the method performs with more complex or diverse architectures.
- What evidence would resolve it: Conducting experiments on a wider range of vision transformer architectures, including those with more layers or different design principles, would provide evidence of the method's scalability and effectiveness.

### Open Question 2
- Question: What are the theoretical limits of the Householder transformation-based adaptation method in terms of the number of parameters it can effectively handle without significant performance degradation?
- Basis in paper: [inferred] The paper discusses the parameter efficiency of the method but does not provide a theoretical analysis of its limits or how it scales with increasing model complexity.
- Why unresolved: While the paper demonstrates practical effectiveness, it lacks a theoretical framework for understanding the method's limitations and scalability.
- What evidence would resolve it: A theoretical analysis or empirical study that explores the performance of the method as the number of parameters increases would help establish its limits.

### Open Question 3
- Question: How does the Householder transformation-based adaptation method perform on non-vision transformer architectures, such as those used in natural language processing or other domains?
- Basis in paper: [inferred] The paper focuses on vision transformers and does not explore the applicability of the method to other types of transformer architectures.
- Why unresolved: The method's effectiveness is demonstrated only within the context of vision transformers, leaving its potential for other domains unexplored.
- What evidence would resolve it: Testing the method on transformer architectures from other domains, such as NLP or audio processing, would provide insights into its versatility and generalizability.

## Limitations
- Limited exploration of scalability to larger and more complex vision transformer architectures
- Lack of theoretical analysis of the method's parameter efficiency limits
- No investigation of performance on non-vision transformer architectures

## Confidence
- High confidence: The theoretical foundation of Householder transformations for orthogonal matrix construction is well-established in linear algebra
- Medium confidence: The experimental results showing competitive performance on FGVC and VTAB-1k benchmarks are reproducible given proper implementation of the HTA mechanism
- Low confidence: The claim of "1.2% improvement over state-of-the-art with half the parameters" requires careful verification due to architectural differences in the comparison

## Next Checks
1. **Implementation Verification**: Implement HTA and conduct controlled experiments comparing pure Householder adaptation against standard LoRA on identical architectures to isolate the parameter efficiency gains
2. **Ablation Analysis**: Systematically test HTA variants with different diagonal rank configurations across layers to validate the layer-wise adaptation hypothesis and identify optimal configurations
3. **Generalization Testing**: Apply HTA to non-ViT architectures (e.g., ConvNets) to determine whether the benefits stem from the Householder mechanism itself or are specific to ViT's attention-based structure