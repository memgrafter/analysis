---
ver: rpa2
title: 'Quamba: A Post-Training Quantization Recipe for Selective State Space Models'
arxiv_id: '2410.13229'
source_url: https://arxiv.org/abs/2410.13229
tags:
- quantization
- quamba
- accuracy
- input
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We study quantization methods for selective State Space Models\
  \ and propose Quamba, an 8-bit static per-tensor quantization method that leverages\
  \ hardware acceleration to achieve 1.72\xD7 speedup in generation latency on an\
  \ Nvidia Orin Nano 8G, with only a 0.9% drop in average accuracy on zero-shot tasks.\
  \ The method addresses the unique challenges of SSMs, including highly sensitive\
  \ feature maps and massive outliers in output activations, by suppressing maximum\
  \ values in input activations and quantizing output activations in an outlier-free\
  \ space using Hadamard transform."
---

# Quamba: A Post-Training Quantization Recipe for Selective State Space Models

## Quick Facts
- arXiv ID: 2410.13229
- Source URL: https://arxiv.org/abs/2410.13229
- Reference count: 40
- Achieves 1.72× speedup on Nvidia Orin Nano 8G with 0.9% accuracy drop

## Executive Summary
Quamba is a post-training quantization method specifically designed for selective state space models (SSMs). The approach addresses unique challenges in SSM quantization including highly sensitive feature maps and massive outliers in output activations. By suppressing maximum values in input activations and applying Hadamard transform to quantize output activations in an outlier-free space, Quamba achieves efficient 8-bit static per-tensor quantization that leverages hardware acceleration.

The method demonstrates Pareto-optimality across both cloud and edge platforms, achieving significant speedups while maintaining model accuracy. Experimental results show 1.72× generation latency improvement on Nvidia Orin Nano 8G with only 0.9% average accuracy reduction on zero-shot tasks, outperforming existing quantization techniques for SSMs.

## Method Summary
Quamba implements an 8-bit static per-tensor quantization approach tailored for selective state space models. The method tackles two primary challenges: sensitive feature maps that amplify quantization errors and massive outliers in output activations that complicate fixed-point representation. The solution employs a two-pronged strategy - first suppressing maximum values in input activations to reduce sensitivity, then applying Hadamard transform to map output activations into an outlier-free space before quantization. This enables effective hardware acceleration while preserving model accuracy.

## Key Results
- Achieves 1.72× speedup in generation latency on Nvidia Orin Nano 8G
- Maintains model accuracy with only 0.9% drop on zero-shot tasks
- Demonstrates Pareto-optimality across cloud and edge platforms

## Why This Works (Mechanism)
The method's effectiveness stems from addressing the fundamental characteristics of SSMs. By suppressing maximum input activation values, the approach reduces the amplification of quantization errors that typically occurs in sensitive feature maps. The Hadamard transform operation creates a mathematical transformation that maps output activations into a space where outliers are minimized, enabling more effective 8-bit quantization without significant accuracy loss.

## Foundational Learning

**Selective State Space Models**: Neural architectures that use selective mechanisms to control information flow through recurrent state updates. Why needed: Understanding SSM structure is crucial for identifying quantization challenges. Quick check: Verify the model contains recurrent state transitions and selective gating mechanisms.

**Post-Training Quantization**: Technique for reducing model precision after training without retraining. Why needed: Enables deployment optimization without expensive fine-tuning cycles. Quick check: Confirm model weights can be represented in reduced precision without catastrophic accuracy loss.

**Hadamard Transform**: Orthogonal transform that maps data into frequency domain with bounded coefficients. Why needed: Provides outlier-free representation space for quantization. Quick check: Verify transform preserves information while reducing dynamic range extremes.

**Per-Tensor Quantization**: Quantization scheme where scaling factors are computed per tensor rather than per channel. Why needed: Simpler implementation that leverages hardware acceleration. Quick check: Confirm hardware supports per-tensor operations efficiently.

## Architecture Onboarding

Component map: Input → Activation Suppression → Hadamard Transform → Quantization → Output

Critical path: The quantization pipeline processes activations through suppression, transformation, and fixed-point conversion. Performance bottlenecks occur primarily in the Hadamard transform computation and quantization mapping operations.

Design tradeoffs: The method prioritizes hardware acceleration and simplicity over maximum possible accuracy preservation. Per-tensor quantization sacrifices some precision compared to per-channel approaches but gains significant speedups through hardware support.

Failure signatures: Accuracy degradation occurs when outlier suppression is too aggressive, removing meaningful signal information. Quantization errors amplify in highly sensitive feature maps when transform parameters are improperly tuned.

First experiments:
1. Baseline quantization without activation suppression or transform
2. Activation suppression only, measuring sensitivity to suppression ratio
3. Hadamard transform application, evaluating outlier reduction effectiveness

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Hardware evaluation limited to Nvidia Orin Nano 8G platform only
- Accuracy assessment restricted to zero-shot tasks without fine-tuned model evaluation
- Scaling claims for "all sizes" of SSM models not fully validated experimentally

## Confidence

High confidence:
- Hardware acceleration achieving 1.72× speedup on Nvidia Orin Nano 8G (empirical measurement with specific hardware)

Medium confidence:
- Pareto-optimality claim across cloud and edge platforms (based on limited comparative experiments)

Low confidence:
- Effectiveness for "all sizes" of SSM-based models (scaling claims not fully validated)

## Next Checks

1. Test the quantization method on additional hardware platforms (AMD, Intel, different Nvidia architectures) to verify cross-platform performance claims
2. Evaluate accuracy impact on fine-tuned models and supervised learning tasks beyond zero-shot scenarios
3. Conduct ablation studies to isolate the contribution of each component (Hadamard transform, outlier suppression) to the overall performance gains