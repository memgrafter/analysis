---
ver: rpa2
title: 'HyQE: Ranking Contexts with Hypothetical Query Embeddings'
arxiv_id: '2410.15262'
source_url: https://arxiv.org/abs/2410.15262
tags:
- queries
- contexts
- query
- context
- hypothetical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a context ranking framework that uses LLMs
  to generate hypothetical queries based on retrieved contexts, then ranks contexts
  by comparing the similarity between these hypothetical queries and the original
  user query. This approach addresses the limitations of traditional embedding-based
  ranking methods, which often fail to capture true relevance, and avoids the scalability
  and fine-tuning issues of LLM-based re-rankers.
---

# HyQE: Ranking Contexts with Hypothetical Query Embeddings

## Quick Facts
- arXiv ID: 2410.15262
- Source URL: https://arxiv.org/abs/2410.15262
- Authors: Weichao Zhou, Jiaxin Zhang, Hilaf Hasson, Anu Singh, Wenchao Li
- Reference count: 34
- Primary result: Improves NDCG@10 by up to 11.2% on Touche2020 dataset

## Executive Summary
This paper introduces HyQE, a context ranking framework that addresses limitations in traditional embedding-based ranking methods. HyQE uses a pre-trained LLM to generate hypothetical queries based on retrieved contexts, then ranks contexts by comparing the similarity between these hypothetical queries and the original user query. The approach avoids the scalability and fine-tuning issues of LLM-based re-rankers while improving relevance assessment. By confining LLM generation to context information, HyQE mitigates hallucination risks while leveraging the LLM's ability to generate semantically rich queries that better capture context relevance.

## Method Summary
HyQE ranks contexts by generating hypothetical queries from each context using a pre-trained LLM, then computing a ranking score that combines context-query similarity with the maximum hypothetical query-query similarity. The framework uses a pre-trained LLM to hypothesize probable queries that each context can address, ranks contexts based on the similarity between these hypothesized queries and the user query, and requires no fine-tuning. HyQE is compatible with various embedding models and retrieval methods, and its efficiency is achieved by amortizing LLM query generation costs across multiple user queries through pre-computation and storage of hypothetical queries.

## Key Results
- Improves NDCG@10 across multiple benchmarks: COVID, NEWS, Touche2020, DL19, and DL20
- Achieves up to 11.2% relative improvement on Touche2020 dataset
- Outperforms traditional embedding-based ranking and existing re-ranking methods
- Shows consistent improvements of 0.5-2.2% NDCG@10 on most datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HyQE improves ranking by generating hypothetical queries that are semantically closer to the user query than the original context embedding alone.
- Mechanism: Instead of relying solely on context-query similarity, HyQE uses an LLM to generate hypothetical queries from each context. The ranking score is then computed as the sum of the context-query similarity and the maximum hypothetical query-query similarity, scaled by a hyperparameter λ.
- Core assumption: The LLM can generate hypothetical queries that accurately reflect the semantic scope of the context without introducing external knowledge or hallucination.
- Evidence anchors:
  - [abstract] "Our framework uses a pre-trained LLM to hypothesize the user query based on the retrieved contexts and ranks the context based on the similarity between the hypothesized queries and the user query."
  - [section 4] "For each context c ∈ C, we hypothesize the probable queries that the context c can address or the topics it discusses."
- Break condition: If the LLM consistently generates irrelevant or hallucinated queries, the ranking performance will degrade. This is especially likely if the context is ambiguous or lacks sufficient information.

### Mechanism 2
- Claim: HyQE mitigates the issue of spurious causality in traditional similarity-based ranking by confining the LLM's generation to the context's information space.
- Mechanism: By generating hypothetical queries only from the context, HyQE avoids the LLM's prior knowledge influencing the generation, which can introduce outdated or irrelevant information. This confines the generation to the context's information scope.
- Core assumption: The LLM's generation is constrained by the context, preventing it from accessing or relying on external knowledge.
- Evidence anchors:
  - [section 5.1] "In contrast, HyQE, as shown in Fig.3(c), introduces a hypothetical query ˆq as a latent variable and employs a generative model to simulate p(ˆq|c) without involving the prior knowledge of the LLM."
  - [section 4] "Our method does not require the LLM to have prior knowledge about the query or the context, the hallucination of the LLM is restrained since a context has limited information and can only provide answers to a certain range of queries."
- Break condition: If the context is too short or lacks sufficient information, the LLM may still generate queries that are too broad or irrelevant, leading to spurious correlations.

### Mechanism 3
- Claim: HyQE's efficiency and scalability are achieved by amortizing the LLM query generation cost across multiple user queries.
- Mechanism: Since the hypothetical queries generated from a context are independent of the input user query, they can be stored and reused for future queries involving the same context. This eliminates the need to generate hypothetical queries for each new query.
- Core assumption: The same context is likely to be retrieved for multiple user queries, making the pre-computation and storage of hypothetical queries beneficial.
- Evidence anchors:
  - [section 4] "Since the hypothetical queries H(c) are independent of the input q, once H(c) and the corresponding embeddings are obtained, they can be stored and reused for future queries that involve the same context c."
  - [section 4] "When a previously seen context c is retrieved for some new input query q′, we can quickly retrieve the stored H(c) and embeddings of the queries in H(c)."
- Break condition: If the dataset contains a large number of unique contexts that are rarely reused, the storage and pre-computation overhead may outweigh the benefits.

## Foundational Learning

- Concept: Cosine similarity as a measure of semantic similarity between text embeddings.
  - Why needed here: HyQE uses cosine similarity to compare both the context-query and hypothetical query-query embeddings. Understanding how cosine similarity works is crucial for interpreting the ranking scores.
  - Quick check question: If two text embeddings have a cosine similarity of 1, what does that imply about their semantic relationship?

- Concept: Variational inference and its application to probabilistic modeling.
  - Why needed here: HyQE is grounded in a variational inference framework that models the probability of a context answering a query. Understanding variational inference is key to grasping the theoretical underpinnings of the method.
  - Quick check question: In variational inference, what is the goal of minimizing the KL-divergence between the approximate and true posterior distributions?

- Concept: Embedding models and their role in capturing semantic information.
  - Why needed here: HyQE relies on embedding models to represent both contexts and queries as dense vectors. Understanding how embedding models work and their limitations is important for evaluating HyQE's performance.
  - Quick check question: What are the potential limitations of using embedding models for capturing semantic similarity, especially in cases where lexical overlap is low?

## Architecture Onboarding

- Component map: Retriever -> Hypothetical Query Generator (LLM) -> Embedding Model -> Ranker -> Storage
- Critical path:
  1. User query is received
  2. Retriever fetches K candidate contexts
  3. For each context, check if hypothetical queries are already stored
     - If yes, retrieve them; if no, generate them using the LLM and store them
  4. Compute embeddings for the user query and the contexts
  5. For each context, compute the ranking score as the sum of context-query similarity and the maximum hypothetical query-query similarity
  6. Sort the contexts based on the ranking scores
  7. Return the top-ranked contexts to the user
- Design tradeoffs:
  - LLM choice: Using a larger LLM may generate more diverse and relevant hypothetical queries but will increase the computational cost and storage requirements
  - Number of hypothetical queries per context: Generating more hypothetical queries may improve the ranking accuracy but will increase the storage and computation overhead
  - Hyperparameter λ: A higher value of λ places more emphasis on the hypothetical query-query similarity, which may be beneficial if the context-query similarity is not a reliable indicator of relevance
- Failure signatures:
  - Poor ranking performance: If the LLM consistently generates irrelevant or hallucinated queries, or if the embedding model fails to capture the semantic similarity between queries and contexts
  - High storage and computational overhead: If the number of unique contexts is very large, leading to excessive storage and computation requirements for generating and storing hypothetical queries
  - Scalability issues: If the retrieval and ranking process becomes too slow for real-time applications, especially when dealing with a large number of candidate contexts
- First 3 experiments:
  1. Baseline comparison: Compare HyQE's ranking performance with traditional similarity-based ranking methods (e.g., cosine similarity) on a small dataset to verify the improvement
  2. LLM ablation: Evaluate the impact of using different LLMs (e.g., Mistral-7b, GPT-3.5-turbo, GPT-4o) on the ranking performance to identify the optimal LLM for the task
  3. Hyperparameter tuning: Experiment with different values of λ to find the optimal balance between context-query and hypothetical query-query similarity for the given dataset and embedding model

## Open Questions the Paper Calls Out

The paper mentions in its Limitations section that documents may be segmented with different chunk sizes depending on the requirement, but does not provide experimental validation of HyQE's performance in such scenarios. This represents an important open question about how the method performs when applied to document retrieval tasks where contexts are created by segmenting documents into chunks of varying sizes.

## Limitations

- The paper does not thoroughly investigate the impact of LLM choice on performance, only briefly mentioning that smaller models like Mistral-7b can be used but may produce less diverse queries
- The exceptional performance on Touche2020 (11.2% improvement) is not well explained and may not generalize to other datasets
- The claimed compatibility with "any embedding model and retrieval method" lacks comprehensive validation across different embedding architectures and retrieval approaches

## Confidence

**High Confidence**: The core claim that combining context-query similarity with hypothetical query-query similarity improves ranking performance is well-supported by experimental results across five different datasets. The efficiency argument for pre-computing and storing hypothetical queries is logically sound and directly supported by the framework description.

**Medium Confidence**: The claim that HyQE effectively constrains LLM hallucination by limiting generation to context information is supported by theoretical arguments but would benefit from more empirical validation, particularly in cases where contexts are short or ambiguous.

**Low Confidence**: The assertion that HyQE is "compatible with any embedding model and retrieval method" lacks comprehensive validation across different embedding architectures and retrieval approaches beyond the few tested in the experiments.

## Next Checks

1. **Ablation Study on LLM Size**: Systematically evaluate how different LLM sizes (Mistral-7b, LLaMA-2-13b, GPT-3.5-turbo, GPT-4) impact the diversity and quality of generated hypothetical queries, and correlate these differences with ranking performance to determine the optimal tradeoff between quality and efficiency.

2. **Context Length Sensitivity Analysis**: Test HyQE's performance on contexts of varying lengths (short paragraphs vs. long documents) to empirically validate the claim that the method works well even with limited context information, and identify at what point context brevity begins to degrade performance.

3. **Embedding Model Interoperability Test**: Evaluate HyQE with multiple embedding architectures beyond the sentence-transformers models used in the paper (e.g., contrastive models like CLIP, multilingual models) to rigorously test the claimed compatibility with any embedding model and identify any limitations or requirements.