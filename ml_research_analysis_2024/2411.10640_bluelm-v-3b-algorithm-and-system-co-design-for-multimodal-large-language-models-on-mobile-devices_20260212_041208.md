---
ver: rpa2
title: 'BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language
  Models on Mobile Devices'
arxiv_id: '2411.10640'
source_url: https://arxiv.org/abs/2411.10640
tags:
- arxiv
- image
- preprint
- language
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BlueLM-V-3B, a multimodal large language
  model designed for efficient deployment on mobile devices through algorithm and
  system co-design. To address the challenges of memory constraints and computational
  limitations on mobile phones, the authors redesigned the dynamic resolution scheme
  to reduce image token generation and implemented hardware-aware system optimizations,
  including batched image encoding, pipeline parallelism, and chunked token processing.
---

# BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices

## Quick Facts
- arXiv ID: 2411.10640
- Source URL: https://arxiv.org/abs/2411.10640
- Reference count: 40
- Primary result: BlueLM-V-3B achieves 24.4 tokens/s generation speed with 2.2GB memory usage on MediaTek Dimensity 9300, outperforming larger models on OpenCompass benchmark

## Executive Summary
BlueLM-V-3B introduces a multimodal large language model specifically optimized for mobile deployment through algorithm-system co-design. The authors address the fundamental challenges of memory constraints and computational limitations on mobile devices by redesigning the dynamic resolution scheme and implementing hardware-aware system optimizations. The resulting model achieves state-of-the-art performance among models with ≤4B parameters on the OpenCompass benchmark while maintaining efficient operation on mobile processors.

## Method Summary
The authors developed BlueLM-V-3B through a comprehensive co-design approach that combines algorithmic innovations with system-level optimizations. The key algorithmic change involves redesigning the dynamic resolution scheme to reduce image token generation, while the system optimizations include batched image encoding, pipeline parallelism, and chunked token processing. The model architecture consists of a 2.7B parameter language model paired with a 400M parameter vision encoder, specifically sized to balance performance and resource constraints on mobile devices.

## Key Results
- Achieves 24.4 tokens/s generation speed on MediaTek Dimensity 9300 with 4-bit quantization
- Uses only 2.2GB memory while maintaining high performance
- Attains highest average score of 66.1 on OpenCompass benchmark among models with ≤4B parameters
- Outperforms larger models including MiniCPM-V-2.6 and InternVL2-8B despite smaller parameter count

## Why This Works (Mechanism)
The performance gains stem from the synergistic combination of algorithmic and system optimizations that specifically target mobile hardware constraints. By reducing image token generation through the redesigned dynamic resolution scheme, the model decreases computational load while maintaining visual understanding capabilities. The hardware-aware system optimizations leverage mobile processor characteristics such as memory hierarchy and parallel processing capabilities to maximize throughput within the tight resource constraints of mobile devices.

## Foundational Learning
- **Dynamic Resolution Scheme**: Adjusts image processing based on content complexity to reduce unnecessary token generation; needed to minimize computational overhead while preserving important visual information; quick check: measure token reduction rate vs accuracy impact
- **Hardware-Aware Optimization**: Tailors model execution to specific mobile processor characteristics; needed to extract maximum performance from constrained mobile hardware; quick check: compare performance across different mobile processors
- **4-bit Quantization**: Reduces model precision to decrease memory footprint; needed to fit large models within mobile memory constraints; quick check: verify memory savings vs performance degradation

## Architecture Onboarding
- **Component Map**: Vision Encoder (400M) -> Image Token Generator -> Dynamic Resolution Module -> Language Model (2.7B) -> Output Generation
- **Critical Path**: Image encoding and tokenization occur first, followed by dynamic resolution processing, then language model inference for token generation
- **Design Tradeoffs**: Smaller vision encoder reduces memory usage but may impact visual understanding; aggressive quantization saves memory but could affect generation quality; batched processing improves throughput but increases latency
- **Failure Signatures**: Memory overflow during image encoding, generation stalls due to insufficient parallel processing, accuracy degradation from excessive quantization
- **First Experiments**: 1) Measure baseline performance without system optimizations, 2) Test individual optimization components in isolation, 3) Validate memory usage breakdown across different processing stages

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims based solely on MediaTek Dimensity 9300 processor, limiting generalizability across different mobile hardware architectures
- Lack of ablation studies to isolate contributions of individual system optimizations versus algorithmic changes
- Missing detailed memory allocation breakdown across model components and processing stages

## Confidence
- **High confidence**: The reported generation speed of 24.4 tokens/s and memory usage of 2.2GB on the specified hardware configuration
- **Medium confidence**: The claim of highest average score on OpenCompass benchmark among models with ≤4B parameters
- **Low confidence**: Generalization of performance improvements across different mobile processors and real-world applications

## Next Checks
1. Test BlueLM-V-3B across multiple mobile processor architectures (Qualcomm, Apple, Samsung) to verify hardware generalization of the system optimizations
2. Conduct ablation studies measuring individual contributions of batched image encoding, pipeline parallelism, and chunked token processing to overall performance
3. Evaluate model performance on diverse real-world multimodal tasks beyond benchmark datasets to assess practical deployment readiness