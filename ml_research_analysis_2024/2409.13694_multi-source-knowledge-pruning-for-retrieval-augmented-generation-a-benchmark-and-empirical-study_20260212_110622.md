---
ver: rpa2
title: 'Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A Benchmark
  and Empirical Study'
arxiv_id: '2409.13694'
source_url: https://arxiv.org/abs/2409.13694
tags:
- knowledge
- pruning
- retrieval
- arxiv
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces PruningRAG, a plug-and-play framework for
  retrieval-augmented generation (RAG) that addresses the challenge of integrating
  multiple heterogeneous knowledge sources while reducing hallucinations. The key
  idea is multi-granularity pruning: coarse-grained filtering to select relevant sources
  and fine-grained pruning to extract precise context.'
---

# Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A Benchmark and Empirical Study

## Quick Facts
- arXiv ID: 2409.13694
- Source URL: https://arxiv.org/abs/2409.13694
- Authors: Shuo Yu; Mingyue Cheng; Qi Liu; Daoyu Wang; Jiqian Yang; Jie Ouyang; Yucong Luo; Chenyi Lei; Enhong Chen
- Reference count: 40
- Primary result: PruningRAG improves accuracy by up to 11.48% and reduces hallucination by up to 20.87% for multi-source RAG

## Executive Summary
This paper introduces PruningRAG, a plug-and-play framework that addresses the challenge of integrating multiple heterogeneous knowledge sources in retrieval-augmented generation. The framework employs multi-granularity pruning: coarse-grained filtering to select relevant sources and fine-grained pruning to extract precise context. Using a standardized benchmark combining structured (API) and unstructured (web pages) sources, PruningRAG consistently improves accuracy and reduces hallucinations across various RAG variants. The approach is validated through extensive experiments showing that naive multi-source integration often degrades performance, while the pruning strategy enhances both accuracy and reliability.

## Method Summary
PruningRAG implements a two-stage pruning pipeline for multi-source RAG. The coarse-grained stage uses a fine-tuned LLM to dynamically select relevant knowledge sources from the available pool, filtering out misleading or irrelevant information. The fine-grained stage employs hierarchical retrieval: BM25 sparse retrieval followed by BGE-M3 dense retrieval for web sources, and NER-based entity routing with rule-based conversion for API sources. The framework integrates pruned context with query using prompt engineering strategies (CoT/ICL) and positions the query after the context to mitigate the lost-in-the-middle phenomenon. The approach is evaluated on a standardized dataset of 4,409 QA pairs spanning finance, sports, and entertainment domains.

## Key Results
- PruningRAG increases accuracy by up to 11.48% and reduces hallucination by up to 20.87% when using web pages plus API sources
- Hierarchical retrieval (BM25 → dense) significantly reduces latency while improving accuracy within the candidate set
- Query positioning after context consistently outperforms other strategies, achieving highest accuracy and lowest hallucination rates
- Chunk size of 200-500 tokens with 25-50 token overlap provides optimal balance between context richness and relevance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coarse-grained pruning dynamically selects relevant knowledge sources to maximize utility while minimizing misleading context.
- Mechanism: A fine-tuned LLM takes the query and available knowledge sources as input, and outputs a subset of sources that are most relevant for answering the query. This reduces the search space and filters out sources that are likely to introduce conflicting or misleading information.
- Core assumption: A fine-tuned LLM can accurately distinguish between relevant and irrelevant knowledge sources for a given query.
- Evidence anchors:
  - [abstract] "Coarse-grained pruning effectively removes misleading information from inappropriate sources, thereby mitigating hallucinations."
  - [section 6.1] "The experimental results indicate that directly relying on multiple knowledge sources simultaneously often introduces conflicting information, resulting in performance degradation compared to using a single source."
  - [corpus] Weak evidence - only 1 related paper mentions "multi-source" pruning explicitly, others focus on retrieval-augmented generation generally.

### Mechanism 2
- Claim: Fine-grained pruning uses hierarchical retrieval (sparse followed by dense) to extract semantically relevant context from selected sources.
- Mechanism: For web sources, BM25 is applied first to narrow the document set, then dense passage retrieval (DPR) extracts high-relevance chunks. For API sources, NER extracts key entities to guide API queries, followed by rule-based conversion to natural language.
- Core assumption: Sparse retrieval can serve as an effective pre-filtering step that significantly reduces the computational cost of dense retrieval while preserving relevant candidates.
- Evidence anchors:
  - [section 6.2] "The results show that this sparse retrieval stage significantly reduces latency, especially when handling large volumes of external knowledge."
  - [section 6.2] "Within this reduced candidate set, dense retrieval outperforms both sparse and hybrid approaches."
  - [corpus] Weak evidence - no related papers explicitly discuss hierarchical retrieval for multi-source pruning.

### Mechanism 3
- Claim: Query positioning after context in prompts mitigates the "lost-in-the-middle" phenomenon and improves answer relevance.
- Mechanism: The prompt places the retrieved context before the query, allowing the LLM to absorb all relevant information before formulating an answer, rather than losing focus when the query appears early in a long prompt.
- Core assumption: LLMs have attention mechanisms that can degrade when important elements (like the query) appear in the middle of long contexts.
- Evidence anchors:
  - [section 6.3.3] "Among the three strategies, Query-Append consistently yields the best performance, achieving the highest accuracy and lowest hallucination and missing rates."
  - [section 6.3.3] "This degradation may stem from a lost-in-the-middle effect [26], where placing the query before a long context causes large language models to lose attention focus."
  - [corpus] No direct evidence - the lost-in-the-middle effect is cited from external literature, not demonstrated in this corpus.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The paper builds upon RAG as the foundational framework, extending it to handle multiple heterogeneous knowledge sources through pruning strategies.
  - Quick check question: What are the two main stages in a standard RAG pipeline, and how does PruningRAG modify this pipeline?

- Concept: Fine-tuning for task-specific adaptation
  - Why needed here: The coarse-grained pruning mechanism relies on a fine-tuned LLM to accurately select relevant knowledge sources, demonstrating the importance of adapting general-purpose models to specific tasks.
  - Quick check question: Why might a fine-tuned LLM perform better than an untuned one for source selection in multi-source RAG scenarios?

- Concept: Hierarchical retrieval (sparse + dense)
  - Why needed here: The fine-grained pruning mechanism employs this approach to balance computational efficiency with semantic accuracy when processing large volumes of web-based knowledge.
  - Quick check question: What are the complementary strengths of sparse retrieval (like BM25) and dense retrieval (like DPR) that make them suitable for a two-stage approach?

## Architecture Onboarding

- Component map: Query + Knowledge Sources → Coarse-grained pruning → Fine-grained pruning → Prompt construction → LLM generation → Evaluation
- Critical path: Query → Coarse-grained pruning → Fine-grained pruning → Prompt construction → LLM generation → Evaluation
- Design tradeoffs:
  - Source selection granularity vs. computational cost: Coarse-grained pruning reduces computation but risks losing useful sources
  - Retrieval accuracy vs. hallucination control: Dense retrieval improves accuracy but may introduce more hallucinations compared to sparse methods
  - Context richness vs. relevance: Larger chunk sizes provide more context but dilute relevance, increasing hallucination risk
- Failure signatures:
  - Over-pruning: LLM selects too few sources, missing critical information → accuracy drops, answers become incomplete
  - Under-pruning: LLM retains irrelevant sources → hallucination increases, answers become misleading
  - Poor source selection: Fine-tuned LLM fails to distinguish relevant sources → similar to under-pruning
  - Retrieval mismatch: BM25 filters out semantically relevant documents → DPR cannot recover them, causing information gaps
- First 3 experiments:
  1. Compare PruningRAG with naive RAG on a single query type to verify the coarse-grained pruning mechanism improves accuracy while reducing hallucination
  2. Test different chunk sizes (50, 200, 500, 1000) to find the optimal balance between context richness and relevance for fine-grained pruning
  3. Evaluate query positioning strategies (prepend, append, surround) on a subset of queries to confirm the "lost-in-the-middle" effect and optimal placement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of multi-granularity pruning vary across different query types (e.g., simple factual, conditional, multi-hop) and knowledge source combinations?
- Basis in paper: [explicit] The paper shows that PruningRAG consistently improves performance across various RAG variants and knowledge sources, but the specific impact on different query types is not explored in detail.
- Why unresolved: The paper focuses on overall performance improvements but does not analyze the nuanced impact of pruning strategies on different query types or their interaction with specific knowledge sources.
- What evidence would resolve it: Detailed experimental results showing performance breakdowns by query type and knowledge source combination, including accuracy, hallucination, and efficiency metrics for each scenario.

### Open Question 2
- Question: What is the optimal balance between chunk size and overlap for maximizing retrieval effectiveness while minimizing hallucination in multi-source RAG systems?
- Basis in paper: [inferred] The sensitivity analysis shows that chunk size and overlap impact performance, but the optimal configuration is not definitively established.
- Why unresolved: The paper demonstrates that these parameters affect performance but does not provide clear guidance on how to balance them for different use cases or knowledge sources.
- What evidence would resolve it: Systematic experiments across diverse datasets and query types to identify optimal chunk size and overlap combinations, validated through ablation studies and cross-domain testing.

### Open Question 3
- Question: How can confidence evaluation strategies be effectively integrated into multi-source RAG frameworks to further reduce hallucination while maintaining high accuracy?
- Basis in paper: [explicit] The paper evaluates several confidence strategies including context sufficiency checks and entropy-based uncertainty estimation, but notes that combining strategies yields overly cautious responses.
- Why unresolved: While the paper demonstrates the potential of confidence strategies, it does not explore how to balance caution with accuracy or how to adapt these strategies dynamically based on query characteristics.
- What evidence would resolve it: Development and validation of adaptive confidence evaluation frameworks that can dynamically adjust their thresholds based on query complexity, knowledge source reliability, and user requirements.

## Limitations

- The coarse-grained pruning mechanism relies on a fine-tuned LLM whose training process and evaluation are not fully specified, raising concerns about generalizability
- Evaluation uses GPT-3.5 Turbo for both semantic assessment and hallucination detection, creating potential circularity in the validation approach
- The framework's effectiveness depends heavily on the quality of the initial knowledge sources and their conversion to compatible formats

## Confidence

- High confidence: The observation that multiple knowledge sources can introduce conflicting information and degrade performance (supported by experimental results showing up to 11.48% accuracy decrease when using web pages plus API)
- Medium confidence: The effectiveness of hierarchical retrieval (BM25 → dense retrieval) for fine-grained pruning, as the results show consistent improvements but the evidence base is limited
- Medium confidence: The query positioning strategy to mitigate lost-in-the-middle effects, though this relies on citing external literature rather than demonstrating the effect within this corpus

## Next Checks

1. Conduct ablation studies isolating the coarse-grained pruning mechanism to determine whether improvements stem from source selection or other confounding factors in the pipeline
2. Evaluate the fine-tuned source selection model on held-out queries and sources to verify its ability to generalize beyond the benchmark dataset
3. Implement independent hallucination detection using human evaluation or alternative models to validate the semantic assessment methodology currently relying on GPT-3.5 Turbo