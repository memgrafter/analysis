---
ver: rpa2
title: Simulating Field Experiments with Large Language Models
arxiv_id: '2408.09682'
source_url: https://arxiv.org/abs/2408.09682
tags:
- field
- experiments
- experiment
- mode
- participant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores the use of large language models (LLMs) to
  simulate field experiments. The authors propose two prompting strategies: observer
  mode, where the LLM directly predicts main conclusions, and participant mode, where
  the LLM simulates participant responses.'
---

# Simulating Field Experiments with Large Language Models

## Quick Facts
- arXiv ID: 2408.09682
- Source URL: https://arxiv.org/abs/2408.09682
- Reference count: 0
- Observer mode achieved 66% accuracy rate in predicting experimental conclusions

## Executive Summary
This paper explores the use of large language models (LLMs) to simulate field experiments, extending previous work focused on lab environments. The authors propose two prompting strategies: observer mode, where the LLM directly predicts main conclusions, and participant mode, where the LLM simulates participant responses. They apply their framework to 15 field experiments from marketing and information systems literature, finding that observer mode successfully replicates over 66% of experiments while participant mode achieves 47.9% accuracy. The study reveals that LLMs perform well for certain topics but struggle significantly with gender differences and social norms-related research.

## Method Summary
The framework extracts experimental settings from academic papers using Elicit.org, then applies standardized prompting strategies to simulate experiments and compare results with actual findings. The process involves three steps: selecting papers, extracting experimental settings, and running simulations using GPT-4-turbo API with observer or participant mode prompts. Observer mode uses the LLM's reasoning ability to predict main conclusions based on experimental settings, while participant mode generates synthetic participant data distributions through role-play.

## Key Results
- Observer mode achieved 66% accuracy rate in predicting experimental conclusions
- Participant mode reached only 47.9% accuracy due to prompt generation issues
- LLMs show significant topic sensitivity, performing poorly on gender differences and social norms research
- Automated workflow enables systematic examination of LLM's emergent abilities in field experiment simulation

## Why This Works (Mechanism)

### Mechanism 1
LLMs can simulate field experiments by predicting outcomes and replicating participant responses. The observer mode uses the LLM's reasoning ability to predict main conclusions based on experimental settings, while participant mode uses role-play to generate synthetic participant data distributions. Core assumption: LLMs have sufficient knowledge and reasoning capabilities to understand and simulate complex real-world experimental contexts.

### Mechanism 2
Automated workflow enables systematic examination of LLM's emergent abilities in simulating field experiments. The framework extracts experimental settings from academic papers using Elicit.org, then applies standardized prompting strategies to simulate experiments and compare results with actual findings. Core assumption: Extracted experimental settings are complete and accurate enough for the LLM to generate meaningful simulations.

### Mechanism 3
LLM performance varies significantly based on topic sensitivity, particularly struggling with gender differences and social norms. The LLM's training data and reasoning capabilities are insufficient for understanding complex social dynamics and gender-related phenomena. Core assumption: LLM training data contains biases and lacks nuanced understanding of social norms and gender differences.

## Foundational Learning

- Concept: Field experiments vs. lab experiments
  - Why needed here: The paper contrasts field experiments with lab experiments to highlight the novelty and complexity of simulating field experiments with LLMs
  - Quick check question: What are the key advantages of field experiments over lab experiments that make them more challenging to simulate with LLMs?

- Concept: Prompt engineering for LLMs
  - Why needed here: The paper introduces two prompting strategies (observer mode and participant mode) that are critical to the LLM's ability to simulate field experiments
  - Quick check question: How do the observer mode and participant mode prompting strategies differ in their approach to simulating field experiments?

- Concept: Automated data extraction from academic papers
  - Why needed here: The framework relies on Elicit.org to extract experimental settings from academic papers, which is a key step in the automated workflow
  - Quick check question: What types of information are extracted from academic papers to characterize field experiments for LLM simulation?

## Architecture Onboarding

- Component map: Data extraction (Elicit.org) -> Prompt generation (templates) -> LLM simulation (GPT-4-turbo API) -> Result analysis (comparison with actual findings)

- Critical path: 1. Extract experimental settings from academic papers 2. Generate prompts using standardized templates 3. Run simulations using GPT-4-turbo API 4. Analyze results and calculate accuracy

- Design tradeoffs: Observer mode prioritizes prediction accuracy but may miss nuanced participant responses; Participant mode captures participant responses but has lower accuracy due to prompt generation issues; Automated data extraction speeds up the process but may miss important experimental details

- Failure signatures: Low accuracy in observer mode indicates issues with prompt templates or LLM understanding; Participant mode failures suggest problems with prompt generation or LLM's ability to role-play; Topic sensitivity issues reveal LLM limitations in understanding social norms or gender differences

- First 3 experiments: 1. Run observer mode on a simple field experiment with clear conclusions to verify basic functionality 2. Test participant mode on an experiment with straightforward participant responses to check prompt generation 3. Simulate an experiment related to social norms to identify topic sensitivity issues

## Open Questions the Paper Calls Out

### Open Question 1
What is the precise boundary between feasible and infeasible experimental scenarios for LLM-based simulation? The authors note that while they empirically conclude certain scenarios are infeasible, the boundary remains unclear and impairs the approach's fidelity as pilot testing. A systematic classification framework or decision tree that identifies the characteristics of field experiments that make them suitable or unsuitable for LLM simulation would resolve this.

### Open Question 2
How can participant mode prompting be improved to achieve accuracy comparable to observer mode? The authors state that participant mode results are notably less favorable due to flaws in automated prompt generation and the need for substantial human intervention. A refined prompt generation algorithm that maintains experimental fidelity while automating the conversion process, demonstrated through improved accuracy metrics, would resolve this.

### Open Question 3
Can LLM simulation accuracy be improved for topics involving social norms and gender differences? The authors identify that LLMs perform poorly on gender difference and social norms-related research, attributing this to limitations in social norm awareness. Experimental results showing improved accuracy on gender and social norms topics after implementing specific training protocols or prompt engineering techniques would resolve this.

## Limitations

- Manual curation of experimental settings may introduce bias and reduce reproducibility
- Reliance on Elicit.org for automated extraction represents an intermediate solution that balances speed with accuracy
- Topic sensitivity findings suggest LLM performance may vary significantly across different research domains
- Complete prompt templates are not provided, making exact replication difficult

## Confidence

**High Confidence**: The observer mode's 66% accuracy rate and its superiority over participant mode are well-supported by the experimental results. The automated workflow using Elicit.org for data extraction is clearly described and replicable.

**Medium Confidence**: The participant mode's lower accuracy (47.9%) is documented, but the specific causes (prompt generation issues vs. LLM limitations) are not fully isolated. The topic sensitivity findings are supported but require further