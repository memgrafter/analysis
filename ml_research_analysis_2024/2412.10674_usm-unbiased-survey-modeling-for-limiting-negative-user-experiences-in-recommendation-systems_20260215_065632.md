---
ver: rpa2
title: 'USM: Unbiased Survey Modeling for Limiting Negative User Experiences in Recommendation
  Systems'
arxiv_id: '2412.10674'
source_url: https://arxiv.org/abs/2412.10674
tags:
- survey
- user
- users
- negative
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the problem of reducing negative user experiences
  in recommendation systems, which is crucial for platform success. They propose a
  novel approach that combines in-feed surveys with advanced modeling techniques to
  capture valuable negative feedback.
---

# USM: Unbiased Survey Modeling for Limiting Negative User Experiences in Recommendation Systems

## Quick Facts
- arXiv ID: 2412.10674
- Source URL: https://arxiv.org/abs/2412.10674
- Reference count: 20
- Authors: Chenghui Yu, Peiyi Li, Haoze Wu, Yiri Wen, Bingfeng Deng, Hongyu Xiong
- One-line result: Reduces negative feedback metrics (reports, dislikes, survey inappropriate rate) by 1.75-2.57% through bias-corrected survey modeling

## Executive Summary
The paper addresses the challenge of reducing negative user experiences in recommendation systems by introducing a novel approach that combines in-feed surveys with advanced modeling techniques. The authors propose a comprehensive framework that captures negative feedback through surveys, corrects for response and exposure biases, and integrates the refined predictions into the recommendation pipeline. Their method leverages inverse propensity weighting to address response bias and pseudo label distillation to overcome exposure bias, resulting in significant improvements in user experience metrics. The approach is validated through both offline experiments and online A/B testing, demonstrating its effectiveness in reducing negative feedback while maintaining or improving user retention.

## Method Summary
The authors propose a survey-based approach to reduce negative user experiences in recommendation systems by modeling negative feedback and integrating predictions into the recommendation pipeline. The method involves distributing in-feed surveys to users, building a multi-head neural network to predict survey outcomes (satisfaction, sexual content, inappropriate content), and correcting for two types of bias: response bias and exposure bias. To address response bias, they train a survey-submit model to estimate user submission propensities and apply inverse propensity weighting during training. For exposure bias, they implement a pseudo label distillation framework where a teacher model generates labels for all exposed items using posterior feedback features, and a student model is trained on these pseudo labels combined with true survey data. The approach is deployed online and achieves significant reductions in negative feedback metrics.

## Key Results
- The response debiased model achieves a 3.9% reduction in survey sexual rate and a 3.53% reduction in survey inappropriate rate
- The pseudo label distillation approach reduces negative feedback metrics by up to 2.27% for dislikes and 1.47% for reports
- After full deployment, the model achieves 1.75% reduction in reports, 2.57% reduction in dislikes, and 2.06% reduction in survey inappropriate rate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Response bias occurs when users have different likelihoods of submitting survey responses, leading to training data that overrepresents certain user types.
- Mechanism: The survey-submit model estimates each user's propensity to submit a survey, and inverse propensity weighting (IPW) is applied during training to give higher weights to responses from users who are less likely to submit.
- Core assumption: The user's likelihood to submit a survey is independent of their actual survey response given the observed features.
- Evidence anchors:
  - [abstract] "We strive to resolve the problem of response Bias by applying a survey-submit model"
  - [section] "We noticed varying levels of willingness among users to submit their responses... These two groups of users are distinct, and we need to address these differences in our client base to ensure the accuracy of our model."
  - [corpus] Weak - corpus papers focus on negative sampling and multi-interest recommendation, not survey response bias specifically.
- Break condition: If the submission propensity is correlated with the survey response even after conditioning on features, the IPW correction would be insufficient.

### Mechanism 2
- Claim: Exposure bias occurs because the model only sees survey responses for a small fraction of all video exposures, leading to poor generalization.
- Mechanism: A pseudo label distillation framework is used where a teacher model trained on survey responses generates pseudo labels for all exposed videos. These pseudo labels are filtered by confidence thresholds and used to train a student model.
- Core assumption: The teacher model can generate reasonably accurate pseudo labels for unexposed items based on available features and posterior feedback signals.
- Evidence anchors:
  - [abstract] "We address the Exposure Bias by adopting a pseudo label distillation training framework"
  - [section] "Surveys are randomly distributed to users at a low frequency... each user receives only one survey sample within a short time frame, further limiting the granularity of personalization"
  - [corpus] Weak - corpus papers discuss negative sampling and graph-based recommendations, not pseudo label distillation for exposure bias.
- Break condition: If the teacher model generates too many incorrect pseudo labels, the student model will learn from noise rather than improving generalization.

### Mechanism 3
- Claim: Combining response debiasing with exposure debiasing through pseudo label distillation yields better performance than either method alone.
- Mechanism: The survey-submit model's predicted scores are used to weight the pseudo labels in the student model training, addressing both types of bias simultaneously.
- Core assumption: Both response bias and exposure bias independently degrade model performance, and correcting both provides additive benefits.
- Evidence anchors:
  - [abstract] "After we launched the survey model based our approach on our platform, the model is able to bring reductions of 1.75%, 2.57%, 2.06% on reports, dislikes, survey inappropriate rate, respectively."
  - [section] "We combined pseudo label distillation with the survey-submit model... the online UAUC was higher than that of the model that relied solely on the pseudo label distillation method"
  - [corpus] Weak - no corpus evidence directly supports the combined approach.
- Break condition: If one bias correction method interferes with the other, or if the combined model becomes too complex to train effectively.

## Foundational Learning

- Concept: Inverse Propensity Weighting (IPW)
  - Why needed here: To correct for response bias where some users are more likely to submit surveys than others
  - Quick check question: If user A has a 0.1 probability of submitting a survey and user B has 0.5, what weight should be applied to each response to make them equally influential?

- Concept: Pseudo Label Distillation
  - Why needed here: To address exposure bias by generating training labels for all video exposures, not just those where surveys were distributed
  - Quick check question: What is the purpose of applying confidence thresholds to teacher model predictions when generating pseudo labels?

- Concept: Multi-head Neural Architecture
  - Why needed here: To model different survey questions (satisfaction, sexual content, inappropriate content) simultaneously while sharing feature representations
  - Quick check question: How does a multi-head architecture differ from training separate models for each survey question?

## Architecture Onboarding

- Component map: User watches video -> Survey is shown (low frequency) -> User responds or not -> Data flows to survey-submit model and in-feed survey model -> Teacher model generates pseudo labels -> Student model is trained -> Model predictions integrated into ranking stage -> Improved recommendations shown to users

- Critical path: User watches video → Survey is shown (low frequency) → User responds or not → Data flows to survey-submit model and in-feed survey model → Teacher model generates pseudo labels → Student model is trained → Model predictions integrated into ranking stage → Improved recommendations shown to users

- Design tradeoffs:
  - Frequency of survey distribution vs. data quantity (lower frequency reduces user burden but increases exposure bias)
  - Complexity of teacher model vs. inference latency (more complex teacher can generate better pseudo labels but may slow training)
  - Threshold strictness for pseudo labels vs. training data quality (higher thresholds reduce noise but also reduce training data volume)

- Failure signatures:
  - Model performance degrades when deployed if offline/online data distributions differ significantly
  - Calibration values drift far from 1.0 indicating systematic prediction bias
  - UAUC metrics show no improvement or degrade when adding pseudo labels
  - Response debiasing creates unexpected negative impacts on certain user segments

- First 3 experiments:
  1. Compare in-feed survey model performance with and without survey-submit model weighting on a validation set
  2. Test different confidence thresholds for pseudo label selection and measure impact on student model UAUC
  3. A/B test the combined approach against baseline to verify improvements in negative feedback metrics (reports, dislikes) and positive metrics (retention)

## Open Questions the Paper Calls Out
- How does the proposed pseudo label distillation framework perform when applied to other types of user feedback beyond negative signals, such as neutral or mixed sentiment?
- What is the impact of the survey-submit model on user engagement and satisfaction in the long term, beyond the initial improvements observed in the study?
- How can the model be adapted to handle random survey responses, ensuring that the feedback collected is a true reflection of user intentions?

## Limitations
- Limited corpus validation with weak supporting evidence for the specific bias correction mechanisms
- Missing technical specifications including feature sets, embedding dimensions, and hyperparameter details
- Deployment context dependence with results potentially not generalizable across different platforms and user populations

## Confidence
- High confidence: The general problem formulation and use of multi-head architectures for multi-task prediction
- Medium confidence: The survey-submit model with inverse propensity weighting and pseudo label distillation framework
- Low confidence: The combined approach's superiority and specific architectural contributions without independent verification

## Next Checks
1. Create a validation framework that simulates different response propensity distributions to measure how well the survey-submit model's IPW correction maintains calibration and AUC across user segments
2. Systematically vary the confidence thresholds, teacher model complexity, and pseudo label filtering strategies to identify which components most significantly impact the student model's performance
3. Apply the methodology to a different recommendation dataset (e.g., MovieLens with added simulated survey responses) to verify whether the 1.75-2.57% improvement range holds across platforms