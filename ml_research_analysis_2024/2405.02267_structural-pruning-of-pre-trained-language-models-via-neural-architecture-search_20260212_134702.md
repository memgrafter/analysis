---
ver: rpa2
title: Structural Pruning of Pre-trained Language Models via Neural Architecture Search
arxiv_id: '2405.02267'
source_url: https://arxiv.org/abs/2405.02267
tags:
- search
- hypervolume
- space
- layer
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores neural architecture search (NAS) for structural
  pruning of pre-trained language models (PLMs) to find sub-networks that optimally
  trade off efficiency and generalization performance. The authors propose a multi-objective
  approach that identifies the Pareto optimal set of sub-networks, allowing for a
  more flexible and automated compression process compared to traditional pruning
  methods with fixed thresholds.
---

# Structural Pruning of Pre-trained Language Models via Neural Architecture Search

## Quick Facts
- arXiv ID: 2405.02267
- Source URL: https://arxiv.org/abs/2405.02267
- Authors: Aaron Klein; Jacek Golebiowski; Xingchen Ma; Valerio Perrone; Cedric Archambeau
- Reference count: 40
- Primary result: NAS methods achieve competitive performance to structural pruning methods, especially for higher pruning ratios, with weight-sharing NAS substantially reducing runtime

## Executive Summary
This paper explores neural architecture search (NAS) for structural pruning of pre-trained language models (PLMs) to find sub-networks that optimally trade off efficiency and generalization performance. The authors propose a multi-objective approach that identifies the Pareto optimal set of sub-networks, allowing for a more flexible and automated compression process compared to traditional pruning methods with fixed thresholds. They evaluate four search spaces with varying complexity for pruning transformer-based architectures and show how to apply recently developed two-stage weight-sharing NAS approaches to accelerate the search process.

## Method Summary
The method involves fine-tuning a pre-trained PLM on a downstream task, defining a search space of sub-networks that can be created by pruning heads and neurons, and then applying either standard NAS or weight-sharing NAS to find the Pareto-optimal set of sub-networks balancing parameter count and validation error. The weight-sharing approach trains a single super-network containing all possible sub-networks, then evaluates sampled sub-networks using shared weights. The SMALL search space (pruning heads and neurons per layer) was found to work best through ablation studies.

## Key Results
- Weight-sharing NAS achieves competitive performance to structural pruning methods, especially for higher pruning ratios
- Weight-sharing NAS substantially reduces overall runtime by fine-tuning only a single super-network
- The SMALL search space outperforms larger search spaces within reasonable compute budgets
- NAS methods provide a practical use-case for PLM compression with better flexibility than fixed-threshold pruning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-objective NAS finds Pareto-optimal sub-networks balancing model size and performance, unlike fixed-threshold pruning.
- Mechanism: By treating pruning as a multi-objective optimization over parameter count and validation error, the method identifies non-dominated solutions across the entire efficiency-accuracy trade-off curve.
- Core assumption: The relationship between parameter count and model performance is non-linear and task-dependent, making fixed thresholds suboptimal.
- Evidence anchors:
  - [abstract] "Unlike traditional pruning methods with fixed thresholds, we propose to adopt a multi-objective approach that identifies the Pareto optimal set of sub-networks"
  - [section] "NAS offers a distinct advantage over other pruning strategies by enabling a multi-objective approach to identify the Pareto optimal set of sub-networks"
  - [corpus] "Multi-Objective Neural Architecture Search by Learning Search Space Partitions" (related work showing multi-objective NAS effectiveness)

### Mechanism 2
- Claim: Weight-sharing NAS substantially reduces runtime by fine-tuning a single super-network instead of multiple sub-networks.
- Mechanism: The pre-trained model serves as a super-network containing all possible sub-networks. Fine-tuning this single network and then evaluating sub-networks using shared weights avoids repeated fine-tuning.
- Core assumption: Sub-networks can be evaluated accurately using shared weights without additional fine-tuning.
- Evidence anchors:
  - [abstract] "We also show how we can utilize more recently developed two-stage weight-sharing NAS approaches in this setting to accelerate the search process"
  - [section] "Weight-sharing NAS (Pham et al., 2018; Liu et al., 2019a) addresses the cost issue by training a single super-network consisting of all architectures in the search space"
  - [corpus] "SuperSAM: Crafting a SAM Supernetwork via Structured Pruning and Unstructured Parameter Prioritization" (related work on supernetworks)

### Mechanism 3
- Claim: Smaller, more constrained search spaces can outperform larger, more expressive ones within reasonable compute budgets.
- Mechanism: The SMALL search space, despite being less expressive than LARGE, provides better exploration efficiency and leads to superior Pareto fronts when evaluated with limited computational resources.
- Core assumption: High-dimensional search spaces are too difficult to explore effectively with random sampling and limited budget.
- Evidence anchors:
  - [section] "Interestingly, even though the MEDIUM search space allows for a more fine-grained per layer pruning, it leads to worse results. We attribute this to the non-uniform distribution of parameter count"
  - [section] "The LARGE search space, which is a superset of the other search spaces, seems infeasible to explore with random sampling over so few observations"
  - [corpus] "Neural Network Compression for Reinforcement Learning Tasks" (related work showing constrained search spaces can be effective)

## Foundational Learning

- Concept: Multi-objective optimization and Pareto optimality
  - Why needed here: The core contribution is framing PLM pruning as a multi-objective problem rather than single-objective with fixed thresholds
  - Quick check question: What distinguishes a Pareto-optimal solution from other solutions in a multi-objective optimization problem?

- Concept: Weight-sharing in neural architecture search
  - Why needed here: The weight-sharing approach is central to reducing the computational cost of NAS for PLM pruning
  - Quick check question: How does weight-sharing in NAS differ from standard NAS where each architecture is trained independently?

- Concept: Transformer architecture components (MHA, FFN layers, residual connections)
  - Why needed here: The search spaces are defined over specific components of transformer architectures that can be pruned
  - Quick check question: What are the two main types of layers in each transformer block that the pruning approach targets?

## Architecture Onboarding

- Component map: Pre-trained model (BERT-base or RoBERTa-base) -> Binary masks for heads and neurons -> Search space definitions -> Multi-objective optimization framework -> Weight-sharing training protocol -> Evaluation pipeline
- Critical path: 1. Load pre-trained model and define search space, 2. Fine-tune super-network using weight-sharing approach, 3. Sample sub-networks from search space, 4. Evaluate sub-networks using shared weights, 5. Perform multi-objective optimization to find Pareto front, 6. Select optimal sub-network based on requirements
- Design tradeoffs: Search space expressiveness vs exploration efficiency, Computational cost of fine-tuning vs evaluation accuracy, Fixed vs flexible pruning ratios, Memory footprint vs latency optimization
- Failure signatures: High variance in validation error across sub-networks (noisy fine-tuning), Pareto front dominated by a single point (poor exploration), Sub-networks performing worse than fixed-threshold methods, Extremely long training times for super-network
- First 3 experiments: 1. Implement SMALL search space with random sampling to verify it outperforms larger search spaces on a single dataset, 2. Compare weight-sharing NAS vs standard NAS on a single dataset to measure runtime reduction, 3. Test different super-network training strategies (random, sandwich, kd) to find optimal approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of weight-sharing NAS for structural pruning generalize to larger PLM architectures (e.g., BERT-large, GPT-3) and more complex downstream tasks?
- Basis in paper: [inferred] The authors benchmark their approach on BERT-base and RoBERTa-base models for text classification tasks, but do not explore larger architectures or more complex tasks.
- Why unresolved: Scaling up to larger models and more complex tasks may introduce new challenges in terms of search space complexity, training stability, and computational requirements that are not addressed in the current study.
- What evidence would resolve it: Conducting experiments with larger PLM architectures and more complex downstream tasks (e.g., question answering, summarization) and comparing the performance of weight-sharing NAS against other pruning methods would provide insights into the scalability and generalizability of the approach.

### Open Question 2
- Question: How does the proposed NAS approach for structural pruning compare to other compression techniques such as knowledge distillation or low-rank factorization in terms of final model performance and efficiency?
- Basis in paper: [explicit] The authors compare their NAS approach to existing structural pruning methods (e.g., head-pruning, layer dropping) but do not directly compare against other compression techniques like knowledge distillation or low-rank factorization.
- Why unresolved: Knowledge distillation and low-rank factorization are popular alternatives for compressing PLMs, and their relative performance compared to NAS-based pruning remains unclear.
- What evidence would resolve it: Conducting a comprehensive comparison of NAS-based pruning against knowledge distillation and low-rank factorization on a range of PLM architectures and downstream tasks would provide insights into the relative strengths and weaknesses of each approach.

### Open Question 3
- Question: Can the proposed NAS approach be extended to optimize other efficiency metrics beyond parameter count and latency, such as energy consumption or memory usage?
- Basis in paper: [explicit] The authors focus on optimizing the trade-off between model size (parameter count) and validation error using a multi-objective approach, but do not explore other efficiency metrics like energy consumption or memory usage.
- Why unresolved: Optimizing for additional efficiency metrics beyond parameter count and latency could lead to more practical and sustainable PLM deployments, especially in resource-constrained environments.
- What evidence would resolve it: Extending the NAS approach to incorporate energy consumption or memory usage as additional objectives and evaluating the performance of the resulting models on real-world hardware platforms would provide insights into the feasibility and effectiveness of this extension.

### Open Question 4
- Question: How does the choice of search space (e.g., SMALL, MEDIUM, LARGE) impact the final pruned model's performance and efficiency, and can this choice be automated or made task-specific?
- Basis in paper: [explicit] The authors propose and evaluate four different search spaces with varying complexity for pruning transformer-based architectures, but do not provide a systematic analysis of how the choice of search space impacts the final model's performance and efficiency.
- Why unresolved: The choice of search space can significantly impact the search process and the resulting pruned models, but it remains unclear how to select the most appropriate search space for a given task or dataset.
- What evidence would resolve it: Conducting a comprehensive study on the impact of search space choice on final model performance and efficiency, and developing methods for automating or making the search space selection task-specific, would provide valuable insights for practitioners.

## Limitations

- Heavy reliance on the weight-sharing assumption, which may not hold for all PLM architectures or downstream tasks
- Focus exclusively on text classification tasks, leaving open questions about effectiveness for generation or summarization
- Random sampling strategy may miss promising sub-networks in high-dimensional search spaces

## Confidence

- Mechanism 1 (Multi-objective optimization): High confidence - the theoretical foundation is well-established and the empirical results consistently show improved Pareto fronts
- Mechanism 2 (Weight-sharing NAS): Medium confidence - while runtime reductions are substantial, the assumption that shared weights accurately reflect fine-tuned sub-network performance needs further validation
- Mechanism 3 (Small search spaces): Medium confidence - the results show clear trends but may be task-dependent, and larger search spaces might outperform with increased computational budgets

## Next Checks

1. Test the weight-sharing assumption by fine-tuning a subset of the top-k sub-networks identified by the NAS approach and comparing their performance to the weight-sharing predictions
2. Replicate the experiments on additional PLM architectures (e.g., GPT-2, T5) and task types (e.g., question answering, summarization) to assess generalizability
3. Implement a more sophisticated search strategy (e.g., evolutionary algorithms or Bayesian optimization) to compare against random sampling and evaluate whether better sub-networks can be discovered within similar computational budgets