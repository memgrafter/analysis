---
ver: rpa2
title: Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task
  Model Merging
arxiv_id: '2410.21804'
source_url: https://arxiv.org/abs/2410.21804
tags:
- uni00000013
- merging
- task
- layer
- wemoe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Weight-Ensembling Mixture of Experts (WEMoE)
  and its efficient variant E-WEMoE for multi-task model merging. The method identifies
  critical MLP modules in ViT architectures through parameter variation analysis,
  then dynamically merges them via a mixture-of-experts structure while statically
  merging non-critical modules.
---

# Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging

## Quick Facts
- arXiv ID: 2410.21804
- Source URL: https://arxiv.org/abs/2410.21804
- Reference count: 40
- Key outcome: WEMoE and E-WEMoE outperform state-of-the-art merging methods with 12-24× fewer trainable parameters and 4× fewer total parameters while maintaining comparable performance

## Executive Summary
This paper introduces Weight-Ensembling Mixture of Experts (WEMoE) and its efficient variant E-WEMoE for merging multiple fine-tuned vision transformer models into a single multi-task model. The method identifies MLP modules as more task-specific through parameter variation analysis, then dynamically merges them via a mixture-of-experts structure while statically merging non-critical modules. E-WEMoE further improves efficiency by pruning non-essential elements in MLP task vectors and sharing routing across transformer blocks. Experiments across three ViT architectures and eight tasks show WEMoE and E-WEMoE achieve superior multi-task performance, generalization, and robustness compared to existing methods.

## Method Summary
The method first fine-tunes pre-trained CLIP ViT models on each downstream task, then analyzes parameter variations to identify critical MLP modules. Non-critical modules are merged using static task arithmetic, while critical MLPs are upgraded to a Weight-Ensembling Mixture of Experts structure with task-specific experts and a router that assigns merging weights based on input features. E-WEMoE introduces sparse task vectors (90% sparsity) and shared cross-layer routing to reduce parameters. The routers are fine-tuned using test-time adaptation with entropy loss on unlabeled test data, enabling dynamic adaptation during inference without access to original training data.

## Key Results
- WEMoE and E-WEMoE outperform state-of-the-art merging methods across average accuracy, generalization to unseen tasks, and robustness to corrupted images
- E-WEMoE reduces trainable parameters by 12-24× and total parameters by 4× compared to WEMoE while maintaining comparable performance
- The 90% sparsity level in E-WEMoE preserves most task-specific information while dramatically reducing memory requirements
- Dynamic routing enables task-specific adaptation at inference time without requiring original training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic routing in WEMoE allows task-specific adaptation at inference time without requiring access to original training data.
- Mechanism: The router takes input features from each transformer block and computes merging weights for task-specific MLP experts, enabling context-aware merging rather than static parameter averaging.
- Core assumption: Task-specific knowledge can be encoded as task vectors and effectively selected via routing based on input features.
- Evidence anchors: [abstract] "expert modules in the MoE are dynamically merged based on input samples, enabling a more flexible and adaptive merging approach"; [section] "The role of the router, Rl : Rd → Rn, is to generate the merged weights λl = {λ1,l, λ2,l, ..., λn,l} in the l-th block by combining the pre-trained weights and the expert task vector based on the input features"; [corpus] Weak - no direct evidence of this specific routing mechanism in related works

### Mechanism 2
- Claim: MLP modules contain more task-specific information than attention modules, making them better candidates for dynamic expert upgrading.
- Mechanism: Parameter variation analysis shows MLP modules undergo larger changes during fine-tuning, indicating they capture more task-specific knowledge that conflicts during static merging.
- Core assumption: Larger parameter changes indicate higher task specificity and conflict potential.
- Evidence anchors: [section] "we analyze the core modules... the MLP module exhibits more significant parameter changes after fine-tuning on downstream tasks, whereas the Att module shows less variation"; [section] "Lmlp2 = ||θmlp l,i − θmlp l,0 ||2 2, Latt2 = ||θatt l,i − θatt l,0 ||2 2" showing quantification of parameter changes; [corpus] Weak - related works focus on general model merging without this specific module-level analysis

### Mechanism 3
- Claim: Sparsifying MLP task vectors preserves most task-specific information while dramatically reducing memory requirements.
- Mechanism: Statistical analysis reveals most elements in MLP task vectors have very small magnitudes, allowing 90-99% pruning without significant performance loss.
- Core assumption: Small-magnitude elements contribute negligibly to task-specific knowledge.
- Evidence anchors: [section] "most of the values in τ mlp t,l are very small, approaching zero... the absolute values of over 75% of the elements are less than 0.0005"; [section] "E-WEMoE-90% results in a 4× reduction in the total number of parameters for ViT-B/32, ViT-B/16, and ViT-L/14"; [corpus] Weak - related works mention parameter efficiency but not this specific magnitude-based pruning approach

## Foundational Learning

- Concept: Transformer architecture and ViT specifics
  - Why needed here: Understanding which modules (MLP vs Attention) to upgrade and how to implement routing requires deep knowledge of transformer internals
  - Quick check question: What are the two main components of a transformer block and how do they differ in function?

- Concept: Multi-task learning interference patterns
  - Why needed here: The motivation for dynamic merging stems from understanding when and why task conflicts occur during model merging
  - Quick check question: Why does simple parameter averaging often fail in multi-task settings?

- Concept: Mixture of Experts routing mechanisms
  - Why needed here: Implementing effective routing requires understanding how experts are selected and how routing decisions impact model performance
  - Quick check question: What is the difference between static routing and dynamic routing in MoE architectures?

## Architecture Onboarding

- Component map:
  Pre-trained ViT model (shared across tasks) -> Task-specific MLP experts (upgraded from fine-tuned MLPs) -> Router network (layer-wise or shared across layers) -> Task vector dictionary (sparse representation of task-specific modifications) -> Test-time adaptation module (entropy loss minimization)

- Critical path:
  1. Load pre-trained model and task-specific fine-tuned models
  2. Compute task vectors (parameter differences)
  3. Identify critical vs non-critical modules
  4. Build WEMoE/E-WEMoE structure with routers and sparse task vectors
  5. Implement test-time adaptation training
  6. Deploy with dynamic routing at inference

- Design tradeoffs:
  - Router depth (0 vs 1 vs 2 layers) - affects parameter count vs performance
  - Sparsity level (70% to 99%) - affects memory vs task coverage
  - Layer-wise vs shared routing - affects parameter efficiency vs task specificity
  - Static vs dynamic merging - affects runtime efficiency vs adaptability

- Failure signatures:
  - Router consistently assigns same weights regardless of input → routing not learning task distinctions
  - Performance drops after pruning task vectors → critical information in small-magnitude elements
  - Test-time adaptation fails to converge → routing initialization or loss function issues
  - Memory usage exceeds expectations → task vector sparsity not as effective as anticipated

- First 3 experiments:
  1. Verify parameter variation analysis by computing L2 distances between pre-trained and fine-tuned MLP vs attention modules across tasks
  2. Test routing effectiveness by feeding inputs from different tasks and visualizing routing weight distributions
  3. Validate sparsity assumption by progressively pruning task vectors and measuring performance degradation

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The effectiveness of dynamic weight ensembling relies on specific parameter variation patterns that may not generalize to other model architectures or task types
- The sparsity-based pruning approach assumes uniform distribution of task information across task vector elements, which could be violated in certain domains
- Test-time adaptation requires unlabeled test data, limiting applicability in scenarios where such data is unavailable

## Confidence
- High confidence in the parameter variation analysis methodology and its identification of MLP modules as more task-specific
- Medium confidence in the routing mechanism's ability to capture task-specific patterns from input features
- Low confidence in the universal applicability of the 90-99% sparsity threshold across diverse task types

## Next Checks
1. Test the parameter variation analysis on additional model architectures (e.g., ResNet, ConvNeXt) and task types (regression, detection) to verify the generalization of MLP vs attention module differences.

2. Conduct ablation studies removing the router and comparing performance to static merging, then visualize routing weight distributions across different input types to confirm task-specific selection.

3. Systematically vary sparsity levels (50% to 99%) across different task combinations and measure the performance trade-off curve to identify optimal sparsity for various scenarios.