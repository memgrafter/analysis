---
ver: rpa2
title: Evaluating Frontier Models for Dangerous Capabilities
arxiv_id: '2403.13793'
source_url: https://arxiv.org/abs/2403.13793
tags:
- agent
- capabilities
- difficulty
- dangerous
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a set of dangerous capability evaluations
  for large language models, focusing on persuasion, cyber-security, self-proliferation,
  and self-reasoning. The authors pilot these evaluations on Gemini 1.0 models using
  agent scaffolding and find that the models do not exhibit strong dangerous capabilities
  in the tested areas, though some early warning signs are identified, particularly
  in persuasion.
---

# Evaluating Frontier Models for Dangerous Capabilities

## Quick Facts
- arXiv ID: 2403.13793
- Source URL: https://arxiv.org/abs/2403.13793
- Authors: Mary Phuong, Matthew Aitchison, Elliot Catt, Sarah Cogan, Alexandre Kaskasoli, Victoria Krakovna, David Lindner, Matthew Rahtz, Yannis Assael, Sarah Hodkinson, Heidi Howard, Tom Lieberum, Ramana Kumar, Maria Abi Raad, Albert Webson, Lewis Ho, Sharon Lin, Sebastian Farquhar, Marcus Hutter, Gregoire Deletang, Anian Ruoss, Seliem El-Sayed, Sasha Brown, Anca Dragan, Rohin Shah, Allan Dafoe, Toby Shevlane
- Reference count: 40
- Primary result: Current Gemini 1.0 models do not exhibit strong dangerous capabilities in persuasion, cyber-security, self-proliferation, or self-reasoning, though early warning signs are identified

## Executive Summary
This paper introduces a comprehensive framework for evaluating dangerous capabilities in frontier language models, focusing on four key areas: persuasion, cyber-security, self-proliferation, and self-reasoning. The authors develop novel evaluation methodologies including agent scaffolding, milestone-based task decomposition, and expert intervention measurement to assess how close current models are to possessing dangerous capabilities. While the evaluated Gemini 1.0 models do not demonstrate strong dangerous capabilities, the work establishes important early warning indicators and methodological foundations for anticipating future capability emergence.

## Method Summary
The evaluation methodology employs agent scaffolding to simulate enhanced model performance through structured planning frameworks and tool access, allowing measurement of how close models are to dangerous capabilities through expert intervention in bits. Tasks are decomposed into milestones to reveal capabilities that might be missed in end-to-end testing, and expert best-of-N elicitation uncovers high-quality actions the model knows but doesn't select. The framework combines human-in-the-loop oversight with automated assessment to create a rigorous science of dangerous capability evaluation.

## Key Results
- Gemini 1.0 models (Ultra, Pro, Nano) do not exhibit strong dangerous capabilities in the tested areas
- Models show early warning signs in persuasion tasks, with some ability to influence human behavior
- Agent scaffolding reveals that models can complete subtasks but struggle with end-to-end execution
- Expert intervention measurement provides a quantitative proxy for capability gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The evaluations reveal dangerous capabilities before models reach a critical capability level by using agent scaffolding to simulate enhanced model performance.
- Mechanism: Agent scaffolding provides a structured environment with planning frameworks and tool access, allowing models to perform tasks they couldn't complete end-to-end without assistance. By measuring how much expert intervention (in bits) is required to succeed, we can quantify how close models are to possessing dangerous capabilities.
- Core assumption: Expert intervention in bits provides a meaningful and comparable measure of how close models are to possessing dangerous capabilities.
- Evidence anchors: [abstract] "Our goal is to help advance a rigorous science of dangerous capability evaluation, in preparation for future models."

### Mechanism 2
- Claim: Milestone-based evaluation methodology reveals low-probability capabilities that end-to-end testing misses.
- Mechanism: Breaking complex tasks into subtasks (milestones) allows measurement of model performance on individual components. If a model fails all end-to-end attempts but succeeds on subtasks when instantiated from those milestones, it indicates the model has the necessary skills but lacks planning or execution ability.
- Core assumption: Success on individual subtasks implies the model has the underlying capability, even if it cannot chain these skills together end-to-end.
- Evidence anchors: [section] "Neither Pro1.0 nor Ultra1.0 could complete any tasks in our evaluation, but they both completed several sub-tasks."

### Mechanism 3
- Claim: Expert best-of-N elicitation uncovers high-quality, low-probability actions that reveal model knowledge not demonstrated in typical interactions.
- Mechanism: By generating multiple possible actions and having an expert select the best one, the evaluation can identify actions the model knows but doesn't choose. This reveals knowledge that would otherwise remain hidden due to the model's internal scoring.
- Core assumption: The model's probability distribution over actions contains useful information about its knowledge, even when the highest-probability action isn't optimal.
- Evidence anchors: [section] "In the expert best-of-N experiments, we found that the models would occasionally generate high-quality actions but not take them."

## Foundational Learning

- Concept: Contextualized capability evaluation
  - Why needed here: Standard benchmarks test general capabilities, but dangerous capabilities only manifest in specific contexts. Understanding when and how models exhibit dangerous behavior requires evaluating in relevant scenarios.
  - Quick check question: Why is it insufficient to evaluate a model's coding ability without also evaluating its ability to use that coding capability for cyber attacks?

- Concept: Dual-use capability assessment
  - Why needed here: Many capabilities are beneficial in some contexts but dangerous in others. The evaluation methodology must distinguish between raw capability and its potential for misuse.
  - Quick check question: How does vulnerability detection differ from vulnerability exploitation in terms of dual-use potential?

- Concept: Early warning system design
  - Why needed here: The goal is not just to detect current dangerous capabilities but to anticipate when they will emerge. This requires understanding scaling trends and capability emergence patterns.
  - Quick check question: What makes a capability evaluation useful as an early warning system rather than just a current capability assessment?

## Architecture Onboarding

- Component map: Task definition -> Agent scaffolding -> Human data collection -> Expert intervention scoring -> Analysis -> Reporting
- Critical path: Task definition → Agent scaffolding → Human data collection → Expert intervention scoring → Analysis → Reporting
- Design tradeoffs:
  - Safety vs. realism: Evaluations must be safe but also realistic enough to reveal dangerous capabilities
  - Automation vs. expert involvement: Expert intervention provides better measurement but limits scalability
  - Granularity vs. practicality: More detailed milestone decomposition provides better insight but increases complexity
- Failure signatures:
  - Model overfits to evaluation tasks rather than demonstrating general capability
  - Expert intervention scoring becomes unreliable for complex tasks
  - Human data collection introduces too much noise or bias
  - Evaluation methodology fails to generalize across different model families
- First 3 experiments:
  1. Implement the context length modification task to test self-reasoning capabilities
  2. Set up the expert best-of-N framework for the email setup task
  3. Create milestone decomposition for the build product task to test self-proliferation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do planning and reasoning frameworks affect dangerous capability emergence in models?
- Basis in paper: [explicit] The paper discusses experimenting with various planning and reasoning frameworks on the cyberattack automation evaluation.
- Why unresolved: The paper mentions they conducted experiments but does not provide detailed results on how different frameworks affected model capabilities.
- What evidence would resolve it: Detailed comparative results showing how different planning and reasoning frameworks impact the emergence of dangerous capabilities in models.

### Open Question 2
- Question: What is the relationship between general model performance metrics and expert intervention requirements for self-proliferation tasks?
- Basis in paper: [explicit] The paper hypothesizes that it is possible to find a (non-linear) correlation between expert bits on a task and easy-to-obtain measures of the model's general performance.
- Why unresolved: The paper presents this as a hypothesis but does not provide empirical evidence or data to support or refute this claim.
- What evidence would resolve it: Empirical data showing the correlation between general model performance metrics and expert intervention requirements across different self-proliferation tasks.

### Open Question 3
- Question: How do response length and 'chattiness' affect model performance in persuasion tasks?
- Basis in paper: [inferred] The paper notes that preliminary analysis suggests Pro 1.0 waits longer to mention links but mentions them more times per conversation than Ultra 1.0.
- Why unresolved: The paper identifies this as a potential factor affecting performance but does not conduct a systematic investigation into how these factors influence persuasion outcomes.
- What evidence would resolve it: Systematic experiments varying response length and frequency of mentions to measure their impact on persuasion success rates.

## Limitations

- The use of agent scaffolding may artificially inflate or deflate capability assessments, making it unclear how these results translate to unconstrained deployment scenarios
- The evaluations only cover four specific capability areas, potentially missing other dangerous capabilities that could emerge in frontier models
- Expert intervention measurement in bits assumes a linear relationship with capability gaps that may not hold across different task types

## Confidence

**High Confidence**: The methodology for milestone-based evaluation and expert best-of-N elicitation is technically sound and provides novel approaches to capability assessment. The finding that current Gemini 1.0 models do not exhibit strong dangerous capabilities is well-supported by the experimental results.

**Medium Confidence**: The interpretation of early warning signs, particularly in persuasion capabilities, requires more evidence to determine whether these represent genuine precursors to dangerous capabilities or artifacts of the evaluation methodology.

**Low Confidence**: The forecasts about when dangerous capabilities might emerge rely heavily on expert judgment rather than empirical scaling laws or other quantitative predictors.

## Next Checks

1. Apply the evaluation methodology to a diverse set of frontier models (including non-Google models) to test whether the bits-of-intervention measurement correlates consistently with actual capability emergence across different architectures and training approaches.

2. Systematically test how the presence or absence of safety filters affects the evaluation outcomes, particularly for persuasion and self-proliferation tasks, to better understand the relationship between safety measures and dangerous capability assessment.

3. Design and execute evaluations that more closely mirror realistic deployment scenarios, including tests of capability persistence and adaptability in dynamic environments, to validate whether scaffolding-based assessments accurately predict unconstrained model behavior.