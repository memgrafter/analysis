---
ver: rpa2
title: 'Deep Learning for Multivariate Time Series Imputation: A Survey'
arxiv_id: '2402.04059'
source_url: https://arxiv.org/abs/2402.04059
tags:
- imputation
- data
- time
- series
- missing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews deep learning methods for multivariate
  time series imputation (MTSI), addressing the challenge of missing values in time
  series data that hinder accurate analysis and downstream applications. The paper
  proposes a novel taxonomy categorizing methods based on imputation uncertainty (predictive
  vs.
---

# Deep Learning for Multivariate Time Series Imputation: A Survey

## Quick Facts
- arXiv ID: 2402.04059
- Source URL: https://arxiv.org/abs/2402.04059
- Reference count: 2
- Primary result: Comprehensive survey of deep learning methods for multivariate time series imputation (MTSI), proposing taxonomy and extensive empirical evaluation

## Executive Summary
This survey provides a comprehensive review of deep learning methods for multivariate time series imputation, addressing the critical challenge of missing values in time series data. The authors propose a novel taxonomy categorizing methods based on imputation uncertainty (predictive vs. generative) and neural network architecture (RNN, CNN, GNN, attention, VAE, GAN, and diffusion-based). Through extensive empirical studies using the PyPOTS toolkit, the survey evaluates various deep learning models across real-world datasets, demonstrating significant performance improvements over traditional approaches, especially in high-missingness scenarios. The work identifies key future research directions including handling non-ignorable missing mechanisms, improving scalability, and exploring integration with large language models.

## Method Summary
The survey systematically categorizes deep learning imputation methods through extensive literature review and empirical validation. The methodology involves implementing and comparing eight deep learning imputation models across three real-world datasets (Air, PhysioNet2012, and ETTm1) using the PyPOTS toolkit. The evaluation includes both imputation accuracy metrics (MAE, MSE) and downstream task performance (PR-AUC, ROC-AUC for classification on PhysioNet2012). The survey also proposes a novel taxonomy that classifies methods based on imputation uncertainty and neural network architecture, providing a structured framework for understanding the diverse approaches in this field.

## Key Results
- Deep learning methods significantly outperform traditional approaches, especially in high-missingness scenarios like PhysioNet2012 (80% missing rate)
- Attention-based models generally show superior performance due to their ability to handle long-range dependencies
- Generative methods like diffusion models demonstrate strong capability in capturing complex data distributions but come with higher computational costs
- Improved imputation quality directly enhances downstream task performance, with SAITS achieving 5% and 1% gains over naive methods in classification tasks on PhysioNet2012

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention-based models excel at handling long-range dependencies in multivariate time series imputation tasks, leading to superior performance over RNN and CNN-based methods.
- Mechanism: Self-attention allows each time point to directly attend to all other time points, capturing global temporal dependencies without the sequential bottleneck of RNNs or the limited receptive field of CNNs.
- Core assumption: The missing data patterns in multivariate time series exhibit long-range temporal dependencies that can be effectively captured by attention mechanisms.
- Evidence anchors:
  - [abstract] "Attention-based models generally show superior performance due to their ability to handle long-range dependencies"
  - [section 3.5] "Due to the attention mechanism, attention-based models generally outperform RNN-based and CNN-based methods in imputation tasks due to their superior ability to handle long-range dependencies and parallel processing capabilities."
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: When the temporal dependencies in the data are short-range and local patterns dominate, RNNs or CNNs with appropriate receptive fields may perform comparably or better.

### Mechanism 2
- Claim: Generative imputation methods can capture complex data distributions and quantify imputation uncertainty through sampling, making them suitable for highly sparse and irregular missing patterns.
- Mechanism: Models like VAEs, GANs, and diffusion models learn the joint distribution of observed and missing data, then generate multiple plausible imputations by sampling from this learned distribution, effectively capturing imputation uncertainty.
- Core assumption: The true data distribution can be approximated by a generative model, and this learned distribution generalizes well to missing regions.
- Evidence anchors:
  - [abstract] "Generative methods like diffusion models demonstrate strong capability in capturing complex data distributions but come with higher computational costs"
  - [section 4.2] "VAEs employ an encoder-decoder structure to approximate the true data distribution by maximizing the Evidence Lower Bound (ELBO)"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: When the generative model cannot accurately capture the true data distribution, or when computational constraints make sampling from the generative model impractical.

### Mechanism 3
- Claim: Diffusion-based models achieve state-of-the-art performance in MTSI by gradually denoising corrupted data through a learned Markov chain, effectively handling the complex spatiotemporal dependencies in time series data.
- Mechanism: Diffusion models learn to reverse a noising process by predicting the noise added at each timestep, allowing them to iteratively reconstruct the original data from increasingly corrupted versions, capturing complex dependencies in the process.
- Core assumption: The complex spatiotemporal dependencies in multivariate time series can be effectively modeled as a denoising process that can be reversed through learned steps.
- Evidence anchors:
  - [abstract] "Generative methods like diffusion models demonstrate strong capability in capturing complex data distributions"
  - [section 4.4] "As an emerging and potent category of generative models, diffusion models are adept at capturing complex data distributions by progressively adding and then reversing noise through a Markov chain of diffusion steps."
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: When the computational cost of the iterative denoising process becomes prohibitive, or when the Markov assumption underlying the diffusion process does not hold for the specific time series data.

## Foundational Learning

- Concept: Missing data mechanisms (MCAR, MAR, MNAR)
  - Why needed here: Understanding the missing data mechanism is crucial for selecting appropriate imputation methods and interpreting their results, as different mechanisms require different modeling approaches.
  - Quick check question: If the probability of data being missing depends only on observed values, what missing mechanism is this? (Answer: MAR)

- Concept: Deep learning architectures for sequential data (RNN, CNN, Attention, GNN)
  - Why needed here: Different deep learning architectures have distinct strengths and weaknesses in modeling temporal dependencies, and understanding these differences is essential for selecting the right architecture for a given MTSI problem.
  - Quick check question: Which architecture allows direct connections between all time points, enabling efficient modeling of long-range dependencies? (Answer: Attention)

- Concept: Generative vs. Predictive modeling
  - Why needed here: The choice between generative and predictive approaches affects how imputation uncertainty is handled and what types of downstream analyses are supported, making it a fundamental design decision in MTSI.
  - Quick check question: Which approach generates multiple plausible imputations by sampling from a learned distribution, allowing quantification of imputation uncertainty? (Answer: Generative)

## Architecture Onboarding

- Component map: Data Preprocessing -> Model Architecture Selection (RNN, CNN, Attention, GNN, VAE, GAN, or diffusion) -> Training Objective (reconstruction or likelihood-based) -> Evaluation (imputation accuracy and downstream task performance) -> Downstream Task
- Critical path: Data → Preprocessing → Model → Training → Evaluation → Downstream Task
- Design tradeoffs: Accuracy vs. computational cost (generative methods are more accurate but slower), model complexity vs. interpretability (RNNs are simpler but may miss complex dependencies), uncertainty quantification vs. deterministic output (generative methods provide uncertainty but may be overkill for some applications)
- Failure signatures: Poor imputation accuracy on highly missing data (may indicate inadequate model capacity or wrong architecture choice), unstable training (may indicate issues with the learning objective or data preprocessing), poor downstream task performance (may indicate that imputation quality does not translate to task improvement)
- First 3 experiments:
  1. Implement and compare a simple RNN-based imputer (like BRITS) vs. an attention-based imputer (like Transformer) on a small, highly missing dataset to verify the attention advantage in handling long-range dependencies.
  2. Implement a VAE-based imputer and evaluate its ability to generate multiple plausible imputations for the same missing pattern, demonstrating uncertainty quantification.
  3. Implement a diffusion-based imputer and measure its imputation accuracy vs. computational cost on a medium-sized dataset, verifying the tradeoff between accuracy and efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do imputation methods perform under MNAR (Missing Not At Random) mechanisms compared to MCAR/MAR assumptions?
- Basis in paper: [explicit] The paper explicitly identifies MNAR as a key future research direction, noting that real-world data often follows MNAR mechanisms and that existing methods predominantly assume MCAR or MAR.
- Why unresolved: Current deep learning imputation methods are primarily designed for MCAR/MAR scenarios. MNAR involves a distributional shift between observed and true data that current methods don't adequately address, potentially leading to biased parameter estimates.
- What evidence would resolve it: Comparative studies testing deep learning imputation methods on datasets with known MNAR mechanisms, measuring performance degradation and proposing modifications to handle MNAR scenarios.

### Open Question 2
- Question: What is the optimal paradigm for handling partially-observed time series data: "impute and predict" two-stage or "encode and predict" end-to-end?
- Basis in paper: [explicit] The paper discusses both paradigms and notes that while the optimal approach remains an open question, end-to-end methods appear more promising when missing patterns contain information relevant to downstream tasks.
- Why unresolved: Both approaches have demonstrated effectiveness in different scenarios, and there's limited research directly comparing their performance across diverse downstream tasks and missingness patterns.
- What evidence would resolve it: Systematic empirical studies comparing both paradigms across multiple downstream tasks (classification, forecasting, etc.) while varying missingness patterns and data characteristics.

### Open Question 3
- Question: How can deep learning imputation models be made scalable for large-scale datasets while maintaining or improving performance?
- Basis in paper: [explicit] The paper identifies scalability as a key challenge, noting that deep learning methods have higher computational costs than traditional approaches and become less feasible for large-scale datasets.
- Why unresolved: Existing deep learning imputation methods require significant computational resources, and while some approaches use parallel/distributed computing, there's no clear consensus on best practices for scaling these methods effectively.
- What evidence would resolve it: Development and benchmarking of scalable deep learning imputation architectures using distributed computing frameworks, demonstrating performance improvements on large-scale datasets compared to traditional methods.

## Limitations
- Empirical claims rely heavily on PyPOTS toolkit results without providing detailed dataset statistics or hyperparameter configurations
- Comparison between predictive and generative methods does not fully account for computational constraints in real-world deployment scenarios
- Limited independent verification possible due to lack of open data/code access

## Confidence

**High Confidence**: The taxonomy classification of imputation methods (predictive vs. generative, architecture-based) is well-grounded and aligns with established literature. The observation that attention-based models generally outperform RNN/CNN approaches in handling long-range dependencies is consistent with broader deep learning trends.

**Medium Confidence**: Claims about generative methods' superior uncertainty quantification and diffusion models' state-of-the-art performance are supported by recent literature but may be dataset-dependent. The specific performance gains reported (5% and 1% on PhysioNet2012) appear reasonable but require independent verification.

**Low Confidence**: Future research directions, particularly regarding MNAR mechanisms and LLM integration, are largely speculative given the current state of research.

## Next Checks
1. Replicate the comparative analysis using publicly available implementations of BRITS, Transformer, and SAITS on the PhysioNet2012 dataset to verify the claimed performance improvements.

2. Conduct ablation studies on attention mechanisms by systematically removing attention components from top-performing models to quantify their exact contribution to imputation accuracy.

3. Test the scalability claims by evaluating the same models on increasingly larger time series datasets (e.g., MIMIC-III) to verify computational cost assertions, particularly for diffusion-based methods.