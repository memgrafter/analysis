---
ver: rpa2
title: 'V-STaR: Training Verifiers for Self-Taught Reasoners'
arxiv_id: '2402.06457'
source_url: https://arxiv.org/abs/2402.06457
tags:
- verifier
- solutions
- v-star
- training
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: V-STaR addresses the limitation of existing self-improvement methods
  for LLMs, such as STaR, which discard incorrect solutions generated during the iterative
  fine-tuning process. V-STaR utilizes both correct and incorrect solutions to train
  a verifier using Direct Preference Optimization (DPO), in addition to training the
  generator on correct solutions.
---

# V-STaR: Training Verifiers for Self-Taught Reasoners

## Quick Facts
- arXiv ID: 2402.06457
- Source URL: https://arxiv.org/abs/2402.06457
- Reference count: 13
- 4-17% absolute improvement in test accuracy over existing methods on math reasoning and code generation benchmarks

## Executive Summary
V-STaR addresses a key limitation of self-improvement methods for large language models (LLMs) by utilizing both correct and incorrect solutions during the training process. Unlike previous approaches that discard incorrect solutions, V-STaR trains a verifier using Direct Preference Optimization (DPO) on both types of solutions, while training the generator only on correct ones. The verifier ranks multiple candidate solutions at inference time, selecting the best one. This iterative process progressively improves both the generator and verifier over multiple rounds.

## Method Summary
V-STaR iteratively fine-tunes both a generator and verifier for LLMs. The generator is fine-tuned on correct solutions, while the verifier is trained using DPO on preference pairs consisting of correct and incorrect solutions. The iterative data collection process ensures that each iteration provides higher quality training data for both components. At inference, the generator produces multiple candidate solutions which the verifier ranks and selects from.

## Key Results
- Achieves 4-17% absolute improvement in test accuracy over existing self-improvement and verification-based methods
- Shows consistent gains across math reasoning (GSM8K, MATH) and code generation (MBPP, HumanEval) benchmarks
- Demonstrates progressive improvement over multiple iterations of training

## Why This Works (Mechanism)

### Mechanism 1
Iterative training with both correct and incorrect solutions progressively improves both the generator and verifier. In each iteration, the generator is fine-tuned on correct solutions, which become more accurate over time. Incorrect solutions provide challenging negative examples for the verifier, improving its ability to discriminate correct from incorrect reasoning. This creates a feedback loop where better generators produce higher quality solutions and more informative negative examples for the verifier.

### Mechanism 2
Direct Preference Optimization (DPO) is more effective than traditional ORM-style verification for training verifiers. DPO directly optimizes the relative log probability of correct versus incorrect solutions, unifying the language modeling and classification objectives. This creates a more sample-efficient training process compared to the separate language modeling and classification objectives used in ORM-style verification.

### Mechanism 3
The iterative data collection process produces higher quality verifier training data than non-iterative approaches. Each iteration uses a better generator to produce solutions, which means the verifier training data improves in quality over time. This creates a more challenging and informative training distribution for the verifier compared to using data from a single generator.

## Foundational Learning

- **Concept**: Supervised fine-tuning (SFT) on original training data
  - **Why needed here**: Provides the initial reference policy for DPO and establishes a baseline generator
  - **Quick check question**: What is the objective function used in SFT for language models?

- **Concept**: Rejection sampling and self-improvement
  - **Why needed here**: Forms the basis of how V-STaR iteratively improves the generator using correctness feedback
  - **Quick check question**: How does STaR differ from RFT in terms of data collection?

- **Concept**: Preference learning and reward modeling
  - **Why needed here**: Underlies the DPO approach used to train the verifier
  - **Quick check question**: What is the key difference between DPO and traditional reward modeling approaches?

## Architecture Onboarding

- **Component map**: Generator (LLM fine-tuned on correct solutions) → Verifier (DPO-trained on preference pairs) → Inference (generator produces candidates, verifier ranks them)
- **Critical path**: Training generator → Generating solutions → Labeling correctness → Training verifier → Inference ranking
- **Design tradeoffs**: Iterative vs non-iterative training (quality vs compute), DPO vs ORM (sample efficiency vs implementation complexity), generator-only vs verification-based (simplicity vs performance)
- **Failure signatures**: Generator produces mostly incorrect solutions, verifier fails to improve over iterations, Verifier@k performance plateaus or degrades
- **First 3 experiments**:
  1. Implement SFT baseline on GSM8K to establish reference performance
  2. Implement V-STaR[1 Iter] with non-iterative data collection to compare against full iterative approach
  3. Implement verification-only baseline (SFT + verifier) to isolate verification benefits

## Open Questions the Paper Calls Out

### Open Question 1
Does including the verifier in the training loop (filtering correct solutions) provide benefits beyond the simpler approach of not using it during training? The paper only experimented with the MBPP task and a specific sampling strategy, making it unclear if the verifier in the loop would be beneficial for other tasks or with different sampling strategies.

### Open Question 2
What is the optimal number of candidate solutions to generate per problem to maximize the performance of V-STaR? The paper only provides results for a limited range of k values (up to 64) and for specific model sizes, leaving uncertainty about the optimal number for other model sizes or tasks.

### Open Question 3
How does the performance of V-STaR compare to other self-improvement methods that utilize incorrect solutions, such as those that use contrastive learning or reinforcement learning? The paper only compares V-STaR to a limited set of baselines and does not include other methods that utilize incorrect solutions in different ways.

## Limitations
- Limited empirical validation to specific model architectures (LLaMA2, CodeLLaMA) and narrow testbeds (GSM8K, MATH, MBPP, HumanEval)
- Insufficient detail on DPO implementation and hyperparameters for faithful reproduction
- Ambiguous procedure for generating and labeling candidate solutions for iterative training

## Confidence
- **High confidence**: The core mechanism of using incorrect solutions for verifier training is theoretically sound
- **Medium confidence**: The empirical claims of 4-17% improvements are supported but may not generalize broadly
- **Low confidence**: The comparative advantage of DPO over ORM-style verification lacks thorough validation

## Next Checks
1. **DPO Implementation Validation**: Reproduce the verifier training using the described DPO approach with standardized hyperparameters, comparing against a baseline ORM implementation on identical data.

2. **Iterative vs Non-Iterative Comparison**: Implement V-STaR with non-iterative data collection to empirically demonstrate whether the iterative data collection process provides meaningful improvements.

3. **Generalization Study**: Evaluate V-STaR on additional reasoning tasks beyond math and code, including logical reasoning and commonsense reasoning benchmarks.