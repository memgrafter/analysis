---
ver: rpa2
title: 'Dissecting Language Models: Machine Unlearning via Selective Pruning'
arxiv_id: '2403.01267'
source_url: https://arxiv.org/abs/2403.01267
tags:
- pruning
- neurons
- code
- attention
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce selective pruning, a compute- and data-efficient method
  for machine unlearning in large language models (LLMs). Our method prunes neurons
  based on their relative importance to a targeted capability compared to overall
  network performance, enabling selective removal of specific behaviors.
---

# Dissecting Language Models: Machine Unlearning via Selective Pruning

## Quick Facts
- arXiv ID: 2403.01267
- Source URL: https://arxiv.org/abs/2403.01267
- Reference count: 40
- One-line primary result: Selective pruning achieves effective capability removal while maintaining overall model performance through neuron specialization.

## Executive Summary
This paper introduces selective pruning, a method for machine unlearning in large language models that removes specific capabilities by pruning neurons based on their relative importance to targeted behaviors. The approach exploits neuron specialization, where certain neurons are more crucial for specific tasks than others. By comparing neuron importance across forget and retain datasets, the method achieves selective removal of capabilities like coding ability while preserving general language understanding. The technique is compute- and data-efficient, requiring only forward passes on sample data to identify neurons for pruning, making it a practical baseline for machine unlearning.

## Method Summary
The selective pruning method works by first collecting neuron activation data from pre-trained models on both forget and retain datasets using 100,000 next-token predictions. Importance metrics including frequency, absolute activation, RMS, and standard deviation are calculated for each neuron relative to both datasets. A scoring function based on relative importance is then computed to identify neurons that are significantly more important for the forget task than the retain task. The algorithm iteratively prunes 2% of neurons with the highest scores, recalculating importances after each pruning step. The process continues until all neurons are pruned or performance degrades excessively, with separate treatment for feed-forward and attention neurons.

## Key Results
- Selective pruning achieves effective capability removal while maintaining overall model performance
- Pruning feed-forward neurons is more effective than pruning attention neurons for unlearning
- Dropout during training increases neuron specialization, improving selectivity
- Models trained without dropout (like Pythia) show poor selectivity due to lack of neuron specialization

## Why This Works (Mechanism)
The method exploits neuron specialization in neural networks, where certain neurons develop task-specific importance during training. By calculating the relative importance of neurons to targeted capabilities compared to overall network performance, the approach identifies neurons that are crucial for specific behaviors but less important for general capabilities. This differential importance allows for selective removal of targeted behaviors while preserving general functionality.

## Foundational Learning
- **Neuron Specialization**: Neurons develop task-specific importance during training - needed to understand why selective pruning works, check by comparing neuron importance distributions across tasks
- **Relative Importance Metrics**: Using frequency, activation magnitude, and variance to measure neuron importance - needed for identifying pruning candidates, check by verifying metrics capture task-specific relevance
- **Iterative Pruning**: Gradual removal of neurons with recalculation of importances - needed to avoid catastrophic forgetting, check by monitoring performance degradation curves
- **Feed-forward vs Attention Specialization**: Different neuron types have varying degrees of task-specific importance - needed to optimize pruning strategy, check by comparing selectivity between neuron types
- **Dropout's Role**: Regularization technique that increases neuron specialization - needed to understand model characteristics affecting selectivity, check by comparing models with/without dropout
- **Relative vs Absolute Importance**: Comparing neuron importance across datasets rather than absolute values - needed for selective capability removal, check by validating scoring function discriminates between tasks

## Architecture Onboarding

**Component Map**
Pre-trained Model -> Activation Collection -> Importance Metric Calculation -> Scoring Function -> Iterative Pruning -> Pruned Model

**Critical Path**
The critical path is the iterative pruning loop where neuron importances are recalculated after each pruning step, as this directly affects which neurons are selected for removal in subsequent iterations.

**Design Tradeoffs**
- **Granularity vs Efficiency**: Pruning individual neurons vs entire attention heads - individual neurons provide more precise control but require more computation
- **Relative vs Absolute Importance**: Using relative importance between tasks vs absolute importance for pruning - relative importance enables selectivity but may be noisier
- **Feed-forward vs Attention Neurons**: Prioritizing one neuron type over another for pruning - feed-forward neurons show better selectivity but attention neurons may be more fundamental

**Failure Signatures**
- Exponential performance degradation on retain dataset indicates loss of general capabilities beyond targeted behavior
- Similar performance drops on forget and retain datasets indicate lack of neuron specialization, particularly in models trained without dropout
- Inability to achieve high selectivity despite multiple pruning iterations suggests the targeted capability is too distributed across neurons

**3 First Experiments**
1. Apply selective pruning to a small pre-trained transformer on a synthetic task with known neuron importance to verify basic functionality
2. Compare selectivity when pruning feed-forward neurons vs attention neurons on the same model to confirm differential effectiveness
3. Test the method on models with and without dropout to verify the importance of regularization for neuron specialization

## Open Questions the Paper Calls Out
None

## Limitations
- Attention head pruning implementation details are not fully specified, particularly regarding value vs pre-out neurons and bias handling
- Method effectiveness varies significantly across model architectures, with high selectivity for OPT but poor results for Pythia and RoBERTa
- Stopping criteria beyond "all neurons pruned" is unclear, potentially leading to over-pruning

## Confidence
- **High confidence**: The core selective pruning methodology and its theoretical foundation based on neuron specialization
- **Medium confidence**: The reported selectivity metrics and their interpretation across different model-dataset pairs
- **Low confidence**: The exact implementation details for attention head pruning and stopping criteria

## Next Checks
1. Implement and test the pruning algorithm on a small transformer model (e.g., BERT-base) to verify the basic functionality and selectivity measurement
2. Compare the selectivity results when pruning feed-forward neurons vs attention neurons on the same model to verify the reported difference in effectiveness
3. Test the method on a model trained without dropout (e.g., Pythia) to confirm the reduced selectivity and verify the importance of dropout for neuron specialization