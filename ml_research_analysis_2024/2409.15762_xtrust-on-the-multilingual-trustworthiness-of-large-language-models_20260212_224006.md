---
ver: rpa2
title: 'XTRUST: On the Multilingual Trustworthiness of Large Language Models'
arxiv_id: '2409.15762'
source_url: https://arxiv.org/abs/2409.15762
tags:
- llms
- evaluation
- arxiv
- languages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: XTRUST is the first multilingual benchmark for evaluating the trustworthiness
  of large language models across 10 languages, covering 10 trustworthiness dimensions
  such as illegal activities, hallucination, out-of-distribution robustness, mental
  and physical health, toxicity, fairness, misinformation, privacy, and machine ethics.
  The benchmark includes 2,359 instances, expanded to 23,590 when translated into
  10 languages.
---

# XTRUST: On the Multilingual Trustworthiness of Large Language Models

## Quick Facts
- **arXiv ID:** 2409.15762
- **Source URL:** https://arxiv.org/abs/2409.15762
- **Authors:** Yahan Li; Yi Wang; Yi Chang; Yuan Wu
- **Reference count:** 40
- **One-line primary result:** First multilingual benchmark for evaluating LLM trustworthiness across 10 languages and 10 dimensions, revealing significant performance gaps in low-resource languages.

## Executive Summary
XTRUST introduces the first comprehensive multilingual benchmark for evaluating the trustworthiness of large language models across 10 languages (Arabic, Chinese, French, German, Hindi, Italian, Korean, Portuguese, Russian, Spanish) and 10 trustworthiness dimensions including illegal activities, hallucination, out-of-distribution robustness, mental health, physical health, toxicity, fairness, misinformation, privacy, and machine ethics. The benchmark comprises 2,359 original instances expanded to 23,590 through translation. Evaluation of five widely used LLMs reveals that while GPT-4 generally outperforms others in multilingual trustworthiness, all models struggle significantly with certain low-resource languages like Arabic and Russian, with average accuracies below 70% on critical tasks such as hallucination detection and out-of-distribution robustness. These findings highlight substantial room for improvement in the multilingual trustworthiness of current language models.

## Method Summary
The XTRUST benchmark was constructed through a systematic process of collecting 2,359 instances across 10 trustworthiness dimensions, then translating them into 10 languages using Google Translate. The evaluation framework employed zero-shot settings for most tasks, with five-shot evaluation for machine ethics tasks, using temperature settings of 0 for classification and 1 for text generation. Five LLMs (GPT-4, ChatGPT, Text-Davinci-002, Gemini Pro, Baichuan) were evaluated using API-based access with task-specific prompts including both benign and adversarial variants. Performance metrics varied by task type: accuracy for classification tasks, toxicity scores and disagreement rates for text generation, and noResponseIndex for misinformation detection.

## Key Results
- GPT-4 emerged as the most consistent performer across languages, achieving 58.5% accuracy in Portuguese among Romance languages
- All models struggled with low-resource languages Arabic and Russian, with average accuracies below 70% on hallucination and OOD robustness tasks
- Models generally performed better in Chinese and Indic languages, while exhibiting weaker performance in Korean, Russian, German, and Arabic
- Significant performance gaps were observed across trustworthiness dimensions, with toxicity detection showing particularly variable results across languages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** XTRUST improves multilingual trustworthiness evaluation by expanding beyond single-language benchmarks and including diverse trustworthiness dimensions.
- **Mechanism:** The benchmark uses a comprehensive set of 2,359 instances covering 10 categories translated into 10 languages, exposing LLMs to varied linguistic and cultural contexts.
- **Core assumption:** LLMs can be reliably evaluated on trustworthiness across languages using translated data without introducing significant bias.
- **Evidence anchors:**
  - [abstract]: "XTRUST is the first multilingual benchmark for evaluating the trustworthiness of large language models across 10 languages, covering 10 trustworthiness dimensions..."
  - [section]: "We constructed a comprehensive evaluation dataset covering all assessment dimensions through carefully designed procedures."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.564. Related to multilingual LLM evaluation and benchmarking.
- **Break condition:** If translations introduce systematic errors or cultural misalignments, evaluation results may not reflect true trustworthiness capabilities.

### Mechanism 2
- **Claim:** GPT-4 generally outperforms other LLMs in multilingual trustworthiness due to superior model scale and alignment efforts.
- **Mechanism:** GPT-4's larger parameter count and targeted fine-tuning for safety and alignment enable better handling of trustworthiness tasks across languages.
- **Core assumption:** Model scale and safety alignment during training directly translate to better trustworthiness performance.
- **Evidence anchors:**
  - [abstract]: "Evaluation of five widely used LLMs shows that while GPT-4 generally outperforms others in multilingual trustworthiness..."
  - [section]: "GPT-4 emerges as the most consistent performer, particularly in Romance languages where it achieves 58.5% accuracy in Portuguese."
  - [corpus]: "Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners" - suggests LLMs can learn multilingual capabilities effectively.
- **Break condition:** If GPT-4's training data or alignment was not representative of all target languages, performance gaps may appear.

### Mechanism 3
- **Claim:** Low-resource languages (Arabic, Russian) present significant trustworthiness evaluation challenges due to data scarcity and model bias.
- **Mechanism:** Models struggle with Arabic and Russian because these languages have less training data and fewer online resources in trustworthiness-related domains, leading to lower accuracy.
- **Core assumption:** Model performance correlates with availability of high-quality training data in each language.
- **Evidence anchors:**
  - [abstract]: "all models struggle with certain low-resource languages like Arabic and Russian, with average accuracies below 70%..."
  - [section]: "Models generally perform better in Chinese and Indic languages... However, in Korean, Russian, German, and Arabic, they exhibit weaker performance..."
  - [corpus]: "MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages" - addresses multilingual LLM evaluation gaps.
- **Break condition:** If models can generalize from high-resource to low-resource languages without specific training, this mechanism may not fully explain performance gaps.

## Foundational Learning

- **Concept:** Multilingual benchmark construction
  - **Why needed here:** Ensures fair and comprehensive evaluation of LLMs across languages and trustworthiness dimensions.
  - **Quick check question:** How does translating a benchmark into multiple languages affect the validity of evaluation results?

- **Concept:** Trustworthiness dimensions in LLMs
  - **Why needed here:** Defines what aspects of model behavior are being evaluated (e.g., toxicity, fairness, privacy).
  - **Quick check question:** Why is it important to evaluate multiple trustworthiness dimensions rather than a single metric?

- **Concept:** Zero-shot and few-shot evaluation settings
  - **Why needed here:** Determines how models are tested without or with minimal task-specific training.
  - **Quick check question:** What is the difference between zero-shot and few-shot evaluation in LLM benchmarking?

## Architecture Onboarding

- **Component map:** Benchmark construction (data collection, translation) -> Model evaluation (5 LLMs) -> Result analysis (accuracy metrics, language comparisons) -> Tooling (GitHub repository)
- **Critical path:** 1) Define trustworthiness dimensions → 2) Collect and translate data → 3) Design evaluation prompts → 4) Run model evaluations → 5) Analyze results by language and dimension
- **Design tradeoffs:** Translation via Google Translate balances speed and quality but may introduce bias; focusing on 10 languages covers diversity but misses some language families
- **Failure signatures:** Consistently low accuracy in certain languages suggests data scarcity or model bias; high toxicity scores indicate safety alignment failures
- **First 3 experiments:**
  1. Run XTRUST evaluation on a single LLM (e.g., GPT-4) across all 10 languages and dimensions to establish baseline performance
  2. Compare model performance in high-resource vs low-resource languages to identify data scarcity effects
  3. Test different prompt engineering strategies (zero-shot vs few-shot) to see their impact on evaluation outcomes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do specific architectural differences between LLMs (e.g., transformer depth, attention mechanisms) influence their performance on multilingual trustworthiness tasks, particularly for low-resource languages like Arabic and Russian?
- **Basis in paper:** [inferred] The paper notes that GPT-4 outperforms other models but does not explore the underlying architectural reasons for this difference, nor does it analyze why models struggle with specific languages.
- **Why unresolved:** The study focuses on benchmarking rather than architectural analysis, leaving the connection between model design and multilingual trustworthiness unexplored.
- **What evidence would resolve it:** Comparative ablation studies isolating architectural components (e.g., attention heads, layer depth) and their impact on performance across languages.

### Open Question 2
- **Question:** What are the long-term effects of fine-tuning LLMs on trustworthiness-specific datasets in non-English languages, and how does this impact their generalization to unseen tasks or domains?
- **Basis in paper:** [explicit] The paper highlights the need for improvement in multilingual trustworthiness but does not investigate the effects of fine-tuning on non-English datasets.
- **Why unresolved:** The study evaluates pre-trained models without exploring the potential benefits or drawbacks of fine-tuning on trustworthiness-specific data.
- **What evidence would resolve it:** Longitudinal studies comparing pre-trained and fine-tuned models across multiple trustworthiness tasks and languages.

### Open Question 3
- **Question:** How do cultural and linguistic nuances in low-resource languages affect the interpretation and generation of trustworthy responses by LLMs, and what strategies can mitigate these challenges?
- **Basis in paper:** [inferred] The paper identifies performance disparities in low-resource languages but does not delve into the cultural or linguistic factors contributing to these gaps.
- **Why unresolved:** The study focuses on quantitative benchmarking without qualitative analysis of cultural or linguistic influences.
- **What evidence would resolve it:** Cross-cultural studies involving native speakers to assess LLM outputs and identify culturally sensitive improvements.

## Limitations

- The benchmark's reliance on Google Translate for multilingual expansion introduces potential translation artifacts that could affect evaluation validity, particularly for complex trustworthiness tasks requiring nuanced understanding
- The paper does not provide detailed validation of translation quality across all 10 languages
- While the benchmark covers 10 trustworthiness dimensions, the relative weighting and importance of each dimension in overall trustworthiness assessment remains unclear

## Confidence

- **High confidence:** Core finding that multilingual trustworthiness evaluation reveals significant performance gaps in low-resource languages (Arabic, Russian)
- **Medium confidence:** Claim that GPT-4 generally outperforms other models across languages, as this depends on specific evaluation conditions and prompt engineering choices
- **Medium confidence:** Benchmark's ability to comprehensively capture trustworthiness across all 10 dimensions, given the translation-based methodology

## Next Checks

1. Conduct human evaluation of translated benchmark instances to verify semantic preservation across all 10 languages, focusing on tasks requiring nuanced understanding (fairness, machine ethics, mental health)
2. Perform ablation studies testing different translation methods (human translation vs machine translation) to quantify the impact of translation quality on model performance metrics
3. Expand evaluation to include additional low-resource languages not covered in the benchmark to test whether observed performance gaps are systematic across language families