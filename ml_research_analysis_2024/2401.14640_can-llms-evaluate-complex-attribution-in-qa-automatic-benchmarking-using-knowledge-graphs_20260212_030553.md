---
ver: rpa2
title: Can LLMs Evaluate Complex Attribution in QA? Automatic Benchmarking using Knowledge
  Graphs
arxiv_id: '2401.14640'
source_url: https://arxiv.org/abs/2401.14640
tags:
- answer
- attribution
- supportive
- question
- evaluators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CAQA, a large-scale benchmark for attributed
  question answering that addresses limitations in existing benchmarks by providing
  comprehensive attribution categories and considering complex reasoning scenarios.
  CAQA is automatically generated using knowledge graphs and KGQA datasets, producing
  161K samples with four attribution categories (supportive, partially supportive,
  contradictory, irrelevant) and four complexity levels (single, union, intersection,
  concatenation).
---

# Can LLMs Evaluate Complex Attribution in QA? Automatic Benchmarking using Knowledge Graphs

## Quick Facts
- arXiv ID: 2401.14640
- Source URL: https://arxiv.org/abs/2401.14640
- Reference count: 40
- Key outcome: Introduces CAQA, a large-scale benchmark with 161K samples for attributed QA, revealing that current LLMs struggle with nuanced attribution categories, especially partially supportive cases, but can be significantly improved through fine-tuning.

## Executive Summary
This paper introduces CAQA, a large-scale benchmark for attributed question answering that addresses limitations in existing benchmarks by providing comprehensive attribution categories and considering complex reasoning scenarios. CAQA is automatically generated using knowledge graphs and KGQA datasets, producing 161K samples with four attribution categories (supportive, partially supportive, contradictory, irrelevant) and four complexity levels (single, union, intersection, concatenation). The evaluation of 25 automatic attribution evaluators on CAQA reveals that all struggle to identify nuanced negative attribution categories under zero-shot and few-shot settings, particularly partially supportive cases. Fine-tuning significantly improves performance across all evaluators, with most achieving over 90% F1 scores. The benchmark also demonstrates reliability through high consistency with human annotations and enables fine-grained evaluation of partially supportive attributions using FACTSCORES. CAQA proves effective for developing attribution evaluators, as demonstrated by strong performance of models fine-tuned on CAQA when tested on out-of-domain datasets.

## Method Summary
The CAQA benchmark is automatically generated using knowledge graphs and KGQA datasets. The method extends KGQA logical queries using union and intersection operators to increase reasoning complexity. These extended queries are grounded in KGs to extract subgraphs, which are then edited probabilistically to create subgraphs representing different attribution categories (supportive, partially supportive, contradictory, irrelevant). Natural language citations are generated from these subgraphs. The benchmark contains 161K samples with four attribution categories and four complexity levels. The evaluation involves testing 25 automatic attribution evaluators (2 specialized, 23 LLM-based) across zero-shot, few-shot, and fine-tuning settings, measuring performance using F1 scores, micro-F1 for complexity levels, and FACTSCORES for fine-grained evaluation of partially supportive cases.

## Key Results
- All 25 automatic attribution evaluators struggle with nuanced negative attribution categories, particularly partially supportive cases, in zero-shot and few-shot settings
- Fine-tuning on CAQA significantly improves performance across all evaluator types, with most achieving over 90% F1 scores
- CAQA demonstrates high consistency with human annotations and enables fine-grained evaluation of partially supportive attributions using FACTSCORES metric
- Models fine-tuned on CAQA show strong performance on out-of-domain datasets, proving the benchmark's effectiveness for developing attribution evaluators

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Automatic generation of CAQA using KGQA datasets and logical query extensions reliably produces attribution categories and complexity labels.
- **Mechanism:** The method extends KGQA logical queries using union and intersection operators to increase reasoning complexity. These extended queries are grounded in KGs to extract subgraphs, which are then edited probabilistically to create subgraphs representing different attribution categories (supportive, partially supportive, contradictory, irrelevant). Natural language citations are generated from these subgraphs.
- **Core assumption:** KGQA datasets contain high-quality question-answer pairs with corresponding KG logical queries that can be extended and grounded to produce relevant subgraphs.
- **Evidence anchors:**
  - [section] "The selection of KGs and KGQA datasets is primarily motivated by two observations: (1) KGQA is a well-established task with a wealth of open resources, as evidenced by 25 KGQA datasets for 5 KGs reported in (Jiang and Usbeck, 2022); (2) existing KGQA datasets contain high-quality question-answer pairs and corresponding KG logical queries, often expressed in SPARQL, which are capable of deriving the correct answers and can be leveraged to generate evidence."
  - [abstract] "We introduce Complex Attributed Question Answering (CAQA), a large-scale benchmark containing comprehensive attribution categories, automatically generated using Knowledge Graphs (KGs), and complex attribution scenarios."
- **Break condition:** If the KGQA datasets lack diversity in query types or the logical operators fail to produce meaningful extended queries, the automatic generation would fail to create realistic attribution scenarios.

### Mechanism 2
- **Claim:** Fine-tuning on CAQA significantly improves attribution evaluator performance across all evaluator types.
- **Mechanism:** Attribution evaluators (both LLM-based and specially developed) are trained on the CAQA training set, which contains 161K samples with four attribution categories and four complexity levels. This training enables the evaluators to learn the nuanced distinctions between attribution categories, particularly the subtle differences in partially supportive cases.
- **Core assumption:** The CAQA training data is representative enough to teach evaluators the distinctions between attribution categories and reasoning complexities.
- **Evidence anchors:**
  - [abstract] "Fine-tuning significantly improves performance across all evaluators, with most achieving over 90% F1 scores."
  - [section] "Fine-tuning effectively enhances evaluator performance, while few-shot prompting is particularly beneficial for large-scale evaluators. All fine-tuned evaluators trained on our dataset achieve similar performance, exceeding 90%, except for Mistral-v0.2."
- **Break condition:** If the CAQA training data contains systematic biases or the evaluators overfit to CAQA's specific patterns, their performance may degrade on out-of-domain datasets.

### Mechanism 3
- **Claim:** CAQA enables fine-grained evaluation of partially supportive attributions using FACTSCORES metric.
- **Mechanism:** CAQA's automatic generation method creates partially supportive cases by probabilistically deleting triples from supportive subgraphs. This allows for automated computation of FACTSCORES by comparing the initial subgraph with the edited subgraph. The benchmark also enables human annotation of supported sub-facts for validation.
- **Core assumption:** The probabilistic deletion of triples from supportive subgraphs creates realistic partially supportive cases that reflect genuine gaps in evidence.
- **Evidence anchors:**
  - [abstract] "The benchmark also demonstrates reliability through high consistency with human annotations and enables fine-grained evaluation of partially supportive attributions using FACTSCORES."
  - [section] "Our CAQA benchmark enables a more fine-grained evaluation of attribution, particularly for partially supportive cases, compared to existing benchmarks. To achieve this, we employ the metric FACTSCORES (Min et al., 2023), which is the proportion of sub-facts in the answer that are supported by the citations."
- **Break condition:** If the probabilistic deletion strategy doesn't create meaningful gaps in evidence or if the sub-fact extraction is unreliable, the FACTSCORES metric would not accurately reflect attribution quality.

## Foundational Learning

- **Concept:** Knowledge Graphs (KGs) and their role in QA
  - **Why needed here:** CAQA is constructed using KGs and KGQA datasets. Understanding KGs is essential to grasp how the benchmark is generated and why it's effective.
  - **Quick check question:** What is the basic structure of a KG triple, and how does it differ from unstructured text?

- **Concept:** Logical query operations (union, intersection) in KGQA
  - **Why needed here:** The CAQA generation method relies on extending KGQA queries using union and intersection operations to create different complexity levels. Understanding these operations is crucial for understanding the benchmark's structure.
  - **Quick check question:** How do union and intersection operations differ when extending KGQA queries, and what types of reasoning complexity do they create?

- **Concept:** Attribution categories and their distinctions
  - **Why needed here:** CAQA introduces four attribution categories (supportive, partially supportive, contradictory, irrelevant) that are more fine-grained than previous benchmarks. Understanding these distinctions is essential for evaluating and developing attribution systems.
  - **Quick check question:** What is the key difference between a partially supportive attribution and an irrelevant attribution, and how would you distinguish them programmatically?

## Architecture Onboarding

- **Component map:** KGQA Datasets (GrailQA, WebQuestionsSP) → Logical Query Extension → KG Subgraph Extraction → Subgraph Editing → Natural Language Citation Generation → CAQA Benchmark
- **Critical path:** Query Collection → Query Extension → Structured Attribution Generation → Data Generation → Benchmark Evaluation → Evaluator Development
- **Design tradeoffs:**
  - Automatic generation vs. manual annotation: CAQA trades potential quality control issues for scalability and cost-effectiveness
  - Fine-grained categories vs. simplicity: The four attribution categories provide more nuanced evaluation but increase complexity
  - Complexity levels vs. evaluator capability: Higher complexity levels may be beyond current evaluator capabilities
- **Failure signatures:**
  - Poor performance on partially supportive category: Indicates evaluators struggle with subtle attribution differences
  - Degradation on complex reasoning scenarios: Suggests limitations in logical reasoning capabilities
  - Inconsistency with human annotations: Reveals issues with the automatic generation process or evaluator understanding
- **First 3 experiments:**
  1. Evaluate a simple LLM evaluator (like GPT-3.5) on CAQA in zero-shot setting to establish baseline performance
  2. Fine-tune the same evaluator on CAQA and re-evaluate to measure improvement
  3. Test the fine-tuned evaluator on an out-of-domain dataset (like ALCE-FineGrained) to assess generalization capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can attribution evaluators be improved to better handle complex reasoning scenarios like concatenation, intersection, and union?
- Basis in paper: Explicit
- Why unresolved: The paper demonstrates that evaluators struggle with complex attribution scenarios requiring multiple pieces of evidence, particularly with logical operations like concatenation, intersection, and union. While the authors identify this as a key limitation, they do not propose specific solutions for improving evaluators' ability to handle such complex reasoning.
- What evidence would resolve it: Comparative experiments testing evaluators with enhanced logical reasoning capabilities on complex attribution scenarios, showing improved performance on concatenation, intersection, and union operations.

### Open Question 2
- Question: What is the optimal approach for handling partially supportive attribution cases in automatic evaluation?
- Basis in paper: Explicit
- Why unresolved: The paper identifies that partially supportive cases are the most challenging for evaluators, with significant performance gaps between automated and human annotations. However, it doesn't explore alternative methods or frameworks specifically designed to better capture the nuances of partial support.
- What evidence would resolve it: Experimental results comparing different evaluation frameworks or fine-tuning strategies specifically targeting partially supportive cases, showing improved accuracy in distinguishing partial support from other attribution categories.

### Open Question 3
- Question: How does the quality of automatically generated natural language citations impact attribution evaluation performance?
- Basis in paper: Inferred
- Why unresolved: While the paper discusses the generation of natural language citations from knowledge graph subgraphs, it doesn't systematically investigate how variations in citation quality (e.g., clarity, completeness, coherence) affect evaluator performance. The current benchmark assumes high-quality generation without validating this assumption.
- What evidence would resolve it: Controlled experiments varying citation quality parameters and measuring their impact on evaluator accuracy, identifying which aspects of citation generation most strongly influence attribution evaluation.

## Limitations
- The automatic generation process may introduce systematic biases that aren't fully captured in the evaluation
- The benchmark's reliance on knowledge graphs may limit generalizability to domains without structured knowledge sources
- The evaluation primarily focuses on English language data, potentially limiting applicability to other languages

## Confidence
- **High**: The CAQA benchmark successfully identifies limitations in existing attribution evaluators (supported by consistent poor performance on partially supportive categories across multiple evaluators)
- **Medium**: Fine-tuning significantly improves performance across all evaluator types (though the exact magnitude varies and some evaluators show more improvement than others)
- **Medium**: CAQA enables fine-grained evaluation of partially supportive attributions using FACTSCORES (the method is validated but relies on the accuracy of sub-fact extraction)

## Next Checks
1. Test the best-performing fine-tuned models from CAQA on an entirely different attribution task (like LAQuer) to verify cross-task generalization
2. Conduct ablation studies removing different components of the automatic generation process to identify which aspects most contribute to evaluator performance
3. Evaluate human annotators on CAQA samples to quantify the upper bound of attribution accuracy and compare against automated systems