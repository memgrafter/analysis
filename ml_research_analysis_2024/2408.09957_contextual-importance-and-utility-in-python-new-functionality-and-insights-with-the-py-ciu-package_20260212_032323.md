---
ver: rpa2
title: 'Contextual Importance and Utility in Python: New Functionality and Insights
  with the py-ciu Package'
arxiv_id: '2408.09957'
source_url: https://arxiv.org/abs/2408.09957
tags:
- value
- values
- contextual
- amling
- influence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents py-ciu, a Python implementation of the Contextual
  Importance and Utility (CIU) method for explaining machine learning predictions.
  CIU differs from popular methods like LIME and SHAP by distinguishing between feature
  importance and influence, enabling novel explanations such as potential influence
  plots.
---

# Contextual Importance and Utility in Python: New Functionality and Insights with the py-ciu Package

## Quick Facts
- arXiv ID: 2408.09957
- Source URL: https://arxiv.org/abs/2408.09957
- Authors: Kary FrÃ¤mling
- Reference count: 6
- Key outcome: py-ciu implements CIU method that distinguishes between feature importance and influence, enabling novel explanations like potential influence plots

## Executive Summary
This paper introduces py-ciu, a Python implementation of the Contextual Importance and Utility (CIU) method for explaining machine learning predictions. CIU differs from popular methods like LIME and SHAP by explicitly distinguishing between the importance of features and their influence on predictions, which enables unique capabilities such as potential influence plots and hierarchical explanations through intermediate concepts. The implementation provides comparable functionality to state-of-the-art XAI methods while offering novel features including contrastive explanations and various visualization options.

The paper demonstrates py-ciu's capabilities using the Titanic and Ames housing datasets, showing how CIU can generate diverse visualizations and textual explanations that address different types of user questions about model predictions. The method's ability to separate importance from influence allows for more nuanced explanations that can capture both the relevance of features and their directional impact on outcomes, potentially offering advantages over traditional feature attribution methods.

## Method Summary
The py-ciu package implements the Contextual Importance and Utility method, which calculates feature importance by measuring how much a feature's values affect the prediction outcome within specific contexts. Unlike other XAI methods, CIU explicitly separates importance (how relevant a feature is) from utility (the directional influence of feature values), allowing for more nuanced explanations. The implementation supports generating various visualization types including standard importance plots, potential influence plots that show what could have happened with different feature values, and hierarchical explanations through intermediate concepts. The package is designed to work with different types of ML models and provides both graphical and textual explanations of predictions.

## Key Results
- py-ciu provides functionality comparable to state-of-the-art XAI methods while enabling novel explanations through its distinction between importance and influence
- The implementation supports hierarchical explanations using intermediate concepts, allowing for more interpretable explanations of complex models
- CIU can generate contrastive explanations and potential influence plots that show how predictions could change with different feature values

## Why This Works (Mechanism)
The CIU method works by explicitly modeling the relationship between feature values and prediction outcomes through two separate calculations: contextual importance (how much a feature matters in a given context) and contextual utility (the directional influence of specific feature values). This separation allows the method to capture both the relevance of features and their specific impact on predictions, providing more comprehensive explanations than methods that only measure feature attribution. The mathematical framework ensures that importance and utility are computed independently, enabling novel visualizations like potential influence plots that show what-if scenarios.

## Foundational Learning
- Contextual Importance: Measures how much a feature affects predictions in a specific context (why needed: to identify which features matter most; quick check: compare importance scores across different prediction instances)
- Contextual Utility: Captures the directional influence of feature values on predictions (why needed: to understand how specific values push predictions up or down; quick check: verify utility signs match prediction directions)
- Potential Influence: Shows how predictions could change with different feature values (why needed: to answer "what if" questions; quick check: ensure plots are bounded by feature value ranges)
- Intermediate Concepts: Allows hierarchical explanations by grouping features (why needed: to make complex models more interpretable; quick check: verify concept hierarchies preserve important relationships)
- Contrastive Explanations: Compares predictions to alternatives (why needed: to understand decision boundaries; quick check: ensure contrasts highlight meaningful differences)

## Architecture Onboarding

Component map:
User input -> CIU Engine -> Visualization/Text Output
Input data -> Feature preprocessing -> Importance/Utility calculation
Prediction model -> CIU integration -> Explanation generation

Critical path:
1. User provides input data and prediction model
2. CIU engine calculates contextual importance and utility
3. Results are processed into visualizations or textual explanations
4. Output is presented to user

Design tradeoffs:
- Flexibility vs performance: Supporting various model types may impact computational efficiency
- Granularity vs interpretability: More detailed explanations may be harder to understand
- Visualization richness vs simplicity: Multiple plot types provide more insight but increase complexity

Failure signatures:
- Importance scores that don't correlate with feature relevance
- Utility values that contradict prediction directions
- Visualizations that are cluttered or misleading
- Explanations that are too generic or not context-specific

First experiments:
1. Run CIU on a simple linear regression model with known coefficients to verify importance and utility calculations
2. Generate potential influence plots on the Titanic dataset to validate what-if explanations
3. Test hierarchical explanations by grouping features into concepts and comparing results to individual feature explanations

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Limited evaluation scope primarily focused on synthetic examples and standard datasets
- No rigorous comparison to existing XAI methods on real-world datasets with known ground truth
- Performance and scalability on large-scale ML models and datasets not thoroughly evaluated

## Confidence
- py-ciu's general functionality and compatibility with existing XAI methods: High
- Novelty of CIU's distinction between importance and influence: Medium
- py-ciu's practical utility for real-world applications: Low

## Next Checks
1. Comparative analysis against LIME and SHAP on real-world datasets with known ground truth feature importance
2. Evaluation of py-ciu's performance and scalability on large-scale ML models and datasets
3. User study to assess whether CIU-specific explanations (like potential influence plots) actually improve user understanding compared to traditional XAI methods