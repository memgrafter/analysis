---
ver: rpa2
title: 'ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition
  Models'
arxiv_id: '2403.06586'
source_url: https://arxiv.org/abs/2403.06586
tags: []
core_contribution: This paper introduces ContextGPT, a novel prompt engineering approach
  that leverages Large Language Models (LLMs) to retrieve common-sense knowledge about
  the relationships between human activities and the contexts in which they are performed.
  Unlike existing Neuro-Symbolic (NeSy) methods for context-aware Human Activity Recognition
  (HAR) that rely on manually constructed ontologies, ContextGPT automatically generates
  context-specific prompts to query LLMs and obtain activity consistency vectors.
---

# ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models

## Quick Facts
- arXiv ID: 2403.06586
- Source URL: https://arxiv.org/abs/2403.06586
- Authors: Luca Arrotta; Claudio Bettini; Gabriele Civitarese; Michele Fiori
- Reference count: 40
- Introduces ContextGPT, a prompt engineering approach using LLMs for context-aware HAR

## Executive Summary
This paper presents ContextGPT, a novel approach that leverages Large Language Models to automatically generate context-specific prompts for retrieving common-sense knowledge about human activities and their contexts. Unlike traditional Neuro-Symbolic (NeSy) methods that rely on manually constructed ontologies, ContextGPT uses LLM-generated activity consistency vectors to enhance context-aware Human Activity Recognition (HAR) models. The method addresses the challenge of labeled data scarcity while significantly reducing the human effort required for ontology development. Experiments on two public datasets demonstrate that ContextGPT achieves recognition rates comparable to or better than ontology-based approaches.

## Method Summary
ContextGPT combines neuro-symbolic modeling with LLM-based knowledge infusion for context-aware HAR. The approach automatically generates prompts containing examples of activities and their associated contexts, which are then used to query LLMs and obtain activity consistency vectors. These vectors capture the semantic relationships between activities and contextual features. The consistency vectors are subsequently infused into a NeSy HAR model to improve recognition accuracy, particularly in scenarios with limited labeled data. The method eliminates the need for manual ontology construction while maintaining or improving recognition performance compared to traditional NeSy approaches.

## Key Results
- ContextGPT achieves recognition rates similar to or better than ontology-based NeSy approaches
- Significantly reduces human effort compared to manual ontology construction
- Outperforms purely data-driven baselines, especially in data scarcity scenarios
- Performance improves with more examples in prompts, with higher numbers of context variables and activities requiring more examples for optimal results

## Why This Works (Mechanism)
ContextGPT leverages the vast knowledge encoded in LLMs to automatically generate activity-context relationships that would traditionally require manual ontology development. By using carefully crafted prompts with examples of activities and their contexts, the LLM can generate activity consistency vectors that capture the semantic relationships between different activities and environmental or temporal contexts. These vectors provide the neuro-symbolic model with rich contextual information that helps disambiguate activities and improve recognition accuracy, particularly when labeled training data is limited.

## Foundational Learning

Context-Aware HAR
Why needed: Understanding how contextual information improves activity recognition accuracy
Quick check: Can the model distinguish between similar activities in different contexts (e.g., sitting at work vs. sitting at home)?

Neuro-Symbolic Integration
Why needed: Combines the learning capabilities of neural networks with symbolic reasoning for better interpretability
Quick check: Does the model maintain logical consistency while achieving high recognition rates?

Prompt Engineering for LLMs
Why needed: Effective prompts are crucial for extracting relevant knowledge from LLMs
Quick check: How does prompt structure and example selection affect the quality of generated consistency vectors?

Ontology vs. LLM-based Knowledge
Why needed: Comparing traditional manual knowledge representation with automated LLM approaches
Quick check: What types of knowledge gaps exist in LLM-generated vs. manually constructed ontologies?

## Architecture Onboarding

Component Map:
Raw Sensor Data -> NeSy HAR Model <- LLM Consistency Vectors <- Prompt Generator <- Activity-Context Examples

Critical Path:
Sensor Data → NeSy Model → Activity Recognition ← Consistency Vectors ← LLM Query ← Prompt Generation

Design Tradeoffs:
- Manual ontology construction vs. automated prompt engineering
- Prompt complexity vs. LLM response quality
- Number of examples vs. computational cost
- Symbolic reasoning vs. neural network flexibility

Failure Signatures:
- Poor recognition when contexts are ambiguous or overlapping
- Degradation in performance with insufficient prompt examples
- Inconsistent results across different LLM providers or versions
- Sensitivity to prompt phrasing variations

3 First Experiments:
1. Baseline NeSy model without consistency vectors on DOMINO dataset
2. Ontology-based NeSy approach on ExtraSensory dataset for comparison
3. ContextGPT with varying numbers of examples in prompts to identify optimal configuration

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations

- Evaluation limited to only two public datasets (DOMINO and ExtraSensory), which may not generalize to more complex scenarios
- Performance comparisons lack detailed benchmarks against state-of-the-art deep learning approaches
- Claims about reduced human effort lack quantitative support and metrics
- No systematic exploration of optimal prompt design strategies or guidance for practitioners

## Confidence

| Claim | Confidence |
|-------|------------|
| Recognition rates comparable to ontology-based approaches | Medium |
| Significant reduction in human effort | Low (no quantitative support) |
| Outperformance of data-driven baselines | Medium |
| Performance improvement with more prompt examples | Medium |

## Next Checks

1. Evaluate ContextGPT on additional HAR datasets with different sensor modalities and activity complexities to test generalizability

2. Conduct a systematic study comparing human effort required for ontology construction versus prompt engineering across multiple activity recognition scenarios

3. Perform ablation studies varying prompt structure, example selection strategies, and the number of context variables to identify optimal prompt engineering practices