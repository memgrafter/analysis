---
ver: rpa2
title: Selective Preference Optimization via Token-Level Reward Function Estimation
arxiv_id: '2408.13518'
source_url: https://arxiv.org/abs/2408.13518
tags:
- tokens
- sepo
- oracle
- reward
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Selective Preference Optimization (SePO), a
  novel selective alignment strategy for large language model (LLM) alignment that
  centers on efficient key token selection. SePO trains an oracle model with Direct
  Preference Optimization (DPO) on the target data to estimate a token-level reward
  function, then uses this function to score and select only the most effective tokens
  (typically 30%) for training the target policy model with a reference model-free
  contrastive objective.
---

# Selective Preference Optimization via Token-Level Reward Function Estimation

## Quick Facts
- arXiv ID: 2408.13518
- Source URL: https://arxiv.org/abs/2408.13518
- Reference count: 40
- Primary result: Selective Preference Optimization (SePO) achieves superior LLM alignment performance while training on only 30% of tokens, reducing GPU training hours by up to 60%

## Executive Summary
Selective Preference Optimization (SePO) introduces a novel selective alignment strategy that centers on efficient key token selection for large language model (LLM) alignment. The method trains an oracle model with Direct Preference Optimization (DPO) to estimate a token-level reward function, then uses this function to score and select only the most effective tokens (typically 30%) for training the target policy model. Extensive experiments on three public benchmarks demonstrate that SePO significantly outperforms competitive baseline methods while achieving substantial computational efficiency gains. The approach also shows strong promise in weak-to-strong generalization scenarios and can effectively select useful supervision signals from out-of-distribution data.

## Method Summary
SePO operates through a three-stage pipeline: first, a reference model is trained via supervised fine-tuning on UltraChat-200K; second, an oracle model is trained via DPO on UltraFeedback to estimate token-level reward functions; and third, the target policy model is trained using a contrastive objective on the selected key tokens. The key innovation is the selective token selection mechanism that identifies the most impactful tokens for training, reducing computational costs while maintaining or improving performance. The method achieves this through cross-vocabulary token selection that enables transfer of key tokens across different tokenizers, making it applicable to various model architectures.

## Key Results
- SePO outperforms competitive baseline methods by optimizing on only 30% key tokens
- Achieves up to 60% reduction in GPU training hours while maintaining superior performance
- Successfully selects useful supervision signals from out-of-distribution data, alleviating over-optimization problems
- Shows strong performance in weak-to-strong generalization scenarios with up to 16.8Ã— more parameters

## Why This Works (Mechanism)
SePO works by recognizing that not all tokens contribute equally to model performance during alignment. By training an oracle model to estimate token-level rewards, the method can identify which tokens are most critical for capturing the desired behaviors and preferences. The selective training approach focuses computational resources on these high-impact tokens while ignoring less informative ones, achieving both efficiency gains and performance improvements. The contrastive objective used in training the target policy model further reinforces the selection by emphasizing the differences between chosen and rejected responses at the token level.

## Foundational Learning
- **Token-level reward function estimation**: Needed to identify which tokens most strongly influence model behavior; quick check: verify oracle model can accurately rank tokens by importance
- **Cross-vocabulary token selection**: Required for transferring key tokens across different tokenizer architectures; quick check: test token selection consistency across different model pairs
- **Contrastive objective training**: Essential for reinforcing selected tokens while de-emphasizing others; quick check: monitor training loss convergence with selective vs. full token sets
- **Weak-to-strong generalization**: Enables smaller models to effectively supervise larger ones; quick check: measure performance gains when increasing policy model size relative to oracle
- **Out-of-distribution data handling**: Allows effective training when target data differs from training distribution; quick check: test performance on deliberately mismatched datasets
- **Selective alignment strategy**: Core concept of focusing on most informative training examples; quick check: compare performance with random vs. selective token selection

## Architecture Onboarding
**Component Map**: UltraChat-200K -> SFT Reference Model -> UltraFeedback -> DPO Oracle Model -> Token-Level Reward Function -> Selective Token Selection -> Contrastive Objective -> Target Policy Model -> Benchmark Evaluation

**Critical Path**: The most critical components are the oracle model training (which estimates token-level rewards) and the token selection mechanism (which identifies the 30% most important tokens). Without accurate token-level reward estimation, the selective training approach fails to identify the right tokens to focus on.

**Design Tradeoffs**: The method trades computational efficiency (training on 30% of tokens) against potential information loss from ignoring other tokens. The 30% threshold was empirically determined but may not be optimal for all tasks. Using smaller oracle models for token selection reduces computational cost but may sacrifice selection quality.

**Failure Signatures**: Poor oracle model training leads to ineffective token selection and degraded policy model performance. Random token selection (SePO-rand baseline) significantly underperforms SePO, confirming that token selection quality is crucial. Over-reliance on oracle models with insufficient data proportion results in poor LC win rates on AlpacaEval 2.0.

**First Experiments**:
1. Test cross-vocabulary token selection accuracy between different tokenizer pairs
2. Compare SePO performance with varying oracle model sizes (1B, 7B, 12B)
3. Validate token selection effectiveness by comparing SePO with SePO-rand baseline on benchmark datasets

## Open Questions the Paper Calls Out
- **Open Question 1**: What is the optimal token selection ratio for different types of tasks (e.g., instruction following vs. mathematical reasoning)? The paper only tested a limited set of ratios and found 30% worked best for instruction following, but different task types may require different optimal ratios.

- **Open Question 2**: How does SePO performance scale with oracle model size beyond 12B parameters, and what is the point of diminishing returns? The paper tested up to 12B parameters but computational constraints prevented testing larger models.

- **Open Question 3**: How can token selection ratios be determined adaptively during training rather than being fixed empirically? The paper acknowledges that current ratios are empirically determined and that developing dynamic adjustment mechanisms is an open problem.

## Limitations
- Token selection mechanism relies heavily on oracle model quality, which may not generalize well across different domains
- Computational efficiency claims are based on specific hardware configurations and may vary in different environments
- Method requires response-level supervision signals, limiting applicability to certain types of alignment data
- 30% token selection threshold was empirically determined but may not be optimal for all tasks

## Confidence
- **High Confidence**: Experimental results showing SePO's superior performance on benchmark datasets and effectiveness in weak-to-strong generalization scenarios
- **Medium Confidence**: Claims about token selection efficiency and the specific 30% threshold, which may be sensitive to dataset characteristics
- **Low Confidence**: Exact implementation details of cross-vocabulary token selection algorithm and GPT-4 validation prompting templates

## Next Checks
1. **Cross-Vocabulary Token Selection Validation**: Implement and validate the cross-vocabulary token selection algorithm by testing token transfer accuracy between different tokenizer pairs and measuring selection consistency across model combinations.

2. **Oracle Model Data Proportion Sensitivity**: Systematically vary the proportion of training data used for oracle model training (10%, 30%, 50%, 100%) and measure impact on token selection quality and final policy model performance.

3. **Out-of-Distribution Data Robustness**: Test SePO's performance on datasets significantly different from UltraChat-200K and UltraFeedback to validate claims about effective supervision signal selection from OOD data.