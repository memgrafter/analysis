---
ver: rpa2
title: Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis
arxiv_id: '2411.19655'
source_url: https://arxiv.org/abs/2411.19655
tags:
- text
- claim
- claims
- factual
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-Oasis is a large-scale dataset for training end-to-end factuality
  evaluators. It is constructed by extracting claims from Wikipedia, falsifying a
  subset of these claims, and generating pairs of factual and unfactual texts.
---

# Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis

## Quick Facts
- arXiv ID: 2411.19655
- Source URL: https://arxiv.org/abs/2411.19655
- Reference count: 40
- Primary result: Introduces LLM-Oasis dataset for end-to-end factuality evaluation with modular pipeline achieving up to 60% accuracy

## Executive Summary
LLM-Oasis is a large-scale dataset designed to train end-to-end factuality evaluation systems. The dataset is constructed by extracting claims from Wikipedia, falsifying a subset of these claims, and generating pairs of factual and unfactual texts. This enables two key tasks: end-to-end factuality evaluation and evidence-based claim verification. The modular pipeline approach decomposes factuality evaluation into claim extraction, evidence retrieval, and claim verification, allowing smaller specialized models to achieve competitive performance.

## Method Summary
The methodology involves constructing LLM-Oasis by extracting atomic claims from Wikipedia passages, falsifying some claims, and generating paraphrased versions of both factual and altered texts. The factuality evaluation pipeline is decomposed into three stages: a T5-base model extracts claims, an E5-base retriever finds relevant evidence passages, and a DeBERTa-v3-large model performs NLI-style verification. The system is trained on the constructed dataset and evaluated on a gold standard test set with human validation.

## Key Results
- GPT-4o achieves up to 60% accuracy on the end-to-end factuality evaluation task
- Smaller specialized models trained on LLM-Oasis achieve competitive performance
- Evidence-based claim verification benefits from the retriever-verifier architecture
- Human annotators validate dataset quality and create gold standard benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset enables training of end-to-end factuality evaluators by pairing paraphrased factual and unfactual texts.
- Mechanism: The process extracts atomic claims from Wikipedia passages, falsifies one claim, and generates both a paraphrased factual text and an altered unfactual version. This creates controlled input-output pairs where the only difference is the factual error, allowing models to learn to detect subtle inconsistencies.
- Core assumption: LLMs can reliably generate paraphrases that preserve meaning while altering surface form, and can introduce subtle factual errors that are detectable but not obvious.
- Evidence anchors:
  - [abstract] "LLM-Oasis is constructed by extracting claims from Wikipedia, falsifying a subset of these claims, and generating pairs of factual and unfactual texts"
  - [section] "we leverage paraphrase generation... produces texts that convey the same meaning as the original ones but with different surface forms"
- Break condition: If the LLM generates paraphrases that introduce new factual errors beyond the intended falsification, or if the falsified claims are too subtle or too obvious to be useful for training.

### Mechanism 2
- Claim: The evidence retrieval module improves factuality evaluation by providing relevant supporting passages for claim verification.
- Mechanism: The retriever is trained on pairs of claims and passages (both factual and unfactual texts) to learn dense representations that map claims to their supporting evidence. This enables the claim verifier to compare claims against retrieved evidence rather than relying solely on model knowledge.
- Core assumption: The retrieved passages contain sufficient information to verify or refute the claims, and the retriever can accurately match claims to relevant evidence.
- Evidence anchors:
  - [abstract] "Experiments show that GPT-4o achieves up to 60% accuracy... with smaller specialized models trained on LLM-Oasis achieving competitive performance"
  - [section] "we use the dot product vc · vp to rank all the passages in D and, finally, extract the top k among these"
- Break condition: If the retriever fails to find relevant evidence (low recall), or if the retrieved passages contain conflicting information that confuses the verifier.

### Mechanism 3
- Claim: Decomposing factuality evaluation into claim extraction, evidence retrieval, and claim verification enables smaller models to achieve competitive performance.
- Mechanism: Instead of asking a large model to verify entire texts directly, the task is broken down into simpler subtasks. A T5-base model extracts claims, an E5-base retriever finds evidence, and a DeBERTa-v3-large model performs NLI-style verification. This specialization allows smaller models to handle complex reasoning through composition.
- Core assumption: Breaking down the task preserves the essential reasoning needed for factuality evaluation, and the composition of these components can match or exceed end-to-end approaches.
- Evidence anchors:
  - [abstract] "smaller specialized models trained on LLM-Oasis achieving competitive performance"
  - [section] "we decompose the task of evaluating the factuality of a given text into three simpler tasks, namely, Claim Extraction, Evidence Retrieval and Claim Verification"
- Break condition: If errors compound through the pipeline (e.g., poor claim extraction leads to wrong evidence retrieval), or if the decomposition loses critical context needed for verification.

## Foundational Learning

- Concept: Natural Language Inference (NLI) and claim verification
  - Why needed here: The claim verification module formalizes factuality checking as an NLI problem, determining if claims are entailed, contradicted, or neutral with respect to evidence passages.
  - Quick check question: Given premise "The cat sat on the mat" and hypothesis "A feline rested on a rug", what NLI label would this receive?

- Concept: Dense passage retrieval and vector embeddings
  - Why needed here: The evidence retriever uses dense vector representations to match claims with relevant passages, enabling efficient retrieval from large corpora.
  - Quick check question: What is the primary difference between sparse retrieval (like BM25) and dense retrieval approaches?

- Concept: Paraphrase generation and semantic preservation
  - Why needed here: The dataset relies on generating paraphrased versions of original texts to create challenging evaluation instances where surface forms differ but meanings should be preserved.
  - Quick check question: If a model paraphrases "The quick brown fox jumps over the lazy dog" as "A swift russet fox leaps above the idle canine", has it successfully preserved the meaning?

## Architecture Onboarding

- Component map: T5-base claim extractor → E5-base evidence retriever → DeBERTa-v3-large NLI verifier → aggregation layer
- Critical path: Text input → claim extraction → evidence retrieval (top-k passages) → claim verification → aggregation of claim-level verdicts → final text-level factuality prediction
- Design tradeoffs: Using smaller specialized models instead of large end-to-end models trades off some potential performance for efficiency and interpretability; using Wikipedia as source provides breadth but may not cover all domains
- Failure signatures: Low recall in evidence retrieval (claims lack supporting passages), high error rates in claim extraction (missing or incorrect claims), NLI model confusion between neutral and contradictory evidence
- First 3 experiments:
  1. Evaluate claim extractor on held-out Wikipedia passages to measure precision and recall of extracted claims
  2. Test evidence retriever's R@30 on validation set to ensure it finds relevant passages
  3. Run end-to-end pipeline on small validation set to identify where errors occur in the pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of multiple claims in a single text affect the performance of factuality evaluation models compared to single-claim verification tasks?
- Basis in paper: [explicit] The paper discusses the challenge of verifying factual accuracy in texts containing multiple claims, contrasting it with simpler claim verification tasks.
- Why unresolved: The paper provides experimental results showing that end-to-end factuality evaluation is more challenging than single-claim verification, but it does not explore in detail how the number of claims in a text impacts model performance.
- What evidence would resolve it: Conducting experiments that vary the number of claims in test texts and measuring the accuracy of factuality evaluation models could provide insights into how claim density affects performance.

### Open Question 2
- Question: What are the specific types of factual errors introduced by the model during the claim falsification process, and how do these errors impact the difficulty of the factuality evaluation task?
- Basis in paper: [explicit] The paper describes the process of claim falsification, where the model introduces subtle factual errors, and notes that some errors are more challenging to detect than others.
- Why unresolved: While the paper mentions different types of errors, such as lexical variations and hyponym substitutions, it does not provide a detailed analysis of how these specific error types affect the overall difficulty of the task.
- What evidence would resolve it: Analyzing the performance of factuality evaluation models on texts with different types of factual errors could reveal which error types are most challenging and why.

### Open Question 3
- Question: How does the performance of end-to-end factuality evaluation systems vary across different domains and languages, given that the current dataset is primarily based on Wikipedia in English?
- Basis in paper: [inferred] The paper mentions that the methodology can be adapted to any corpus in any domain or language, but the experiments are conducted using Wikipedia data.
- Why unresolved: The paper does not provide experimental results on the generalizability of the proposed factuality evaluation systems to other domains or languages.
- What evidence would resolve it: Extending the dataset to include texts from various domains and languages, and evaluating the performance of the factuality evaluation systems on these datasets, would demonstrate the models' generalizability.

## Limitations

- Dataset construction relies heavily on LLM-generated falsifications without extensive human verification, raising concerns about data quality and error types
- 60% accuracy on end-to-end task suggests significant difficulty, but it's unclear whether this reflects task complexity or dataset limitations
- Generalization beyond Wikipedia-derived claims remains unproven, with no experiments on non-encyclopedic domains or languages

## Confidence

- **High confidence**: The modular pipeline architecture (claim extraction → evidence retrieval → claim verification) is well-specified and follows established NLP patterns. The use of T5-base, E5-base, and DeBERTa-v3-large models for these subtasks is technically sound and the training objectives are clearly defined.

- **Medium confidence**: The dataset construction methodology is logically coherent, but the reliance on LLM-generated falsifications without extensive human verification introduces uncertainty about data quality. The claim that specialized smaller models can achieve "competitive performance" is supported by the experimental setup but lacks comparative benchmarks against larger end-to-end models.

- **Low confidence**: The scalability and generalization of LLM-Oasis beyond Wikipedia-derived claims remains unproven. The paper doesn't address whether the dataset captures the full spectrum of factual errors found in real-world generation scenarios, particularly for domains outside encyclopedic knowledge.

## Next Checks

1. **Data Quality Audit**: Randomly sample 100 ⟨factual, unfactual⟩ pairs from the dataset and conduct a blind human evaluation to assess whether the falsified claims are genuinely subtle and challenging versus obvious errors. This would validate the core assumption that the dataset provides useful training signals for factuality detection.

2. **Error Analysis on Pipeline Components**: Run the three-stage pipeline on a small validation set and categorize errors at each stage (claim extraction misses, evidence retrieval failures, NLI confusion). This would identify whether the modular approach compounds errors or if specific components are bottlenecks.

3. **Cross-Domain Generalization Test**: Evaluate the trained models on factuality assessment tasks using non-Wikipedia sources (news articles, scientific papers, social media content). This would test whether the Wikipedia-centric dataset generalizes to the broader factuality evaluation challenge.