---
ver: rpa2
title: Liquid Ensemble Selection for Continual Learning
arxiv_id: '2405.07327'
source_url: https://arxiv.org/abs/2405.07327
tags:
- learning
- delegation
- ensemble
- each
- classifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel ensemble-based approach for continual
  learning that leverages liquid democracy principles. The core idea is to dynamically
  select which models within an ensemble should learn on new data versus predict,
  based on recent performance metrics and delegation mechanisms.
---

# Liquid Ensemble Selection for Continual Learning

## Quick Facts
- arXiv ID: 2405.07327
- Source URL: https://arxiv.org/abs/2405.07327
- Reference count: 28
- Key result: Achieves 43.27% accuracy in class-incremental MNIST vs 19.44% for single learners

## Executive Summary
This paper introduces k-Best-Accuracy-Trend (k-BAT), a novel ensemble-based approach for continual learning that leverages liquid democracy principles. The method dynamically selects which models within an ensemble should learn on new data versus predict, based on recent performance metrics and delegation mechanisms. By allowing only the most performant classifiers to learn while others delegate their predictions, the approach significantly reduces catastrophic forgetting compared to traditional single-model approaches.

## Method Summary
The k-BAT method implements a liquid democracy-inspired delegation mechanism where classifiers (voters) delegate their learning and prediction responsibilities based on recent performance trends. The algorithm maintains a sliding window of performance metrics for each classifier, calculates regression slopes to identify performance trends, and selects k classifiers as "gurus" to actively learn on new batches. Non-guru classifiers delegate their predictions to gurus based on probability functions derived from their performance metrics, creating an adaptive ensemble that dynamically reallocates learning and prediction tasks as data distributions shift.

## Key Results
- In class-incremental learning on MNIST, k-BAT achieved 43.27% accuracy versus 19.44% for single learners
- In domain-incremental learning on MNIST, k-BAT achieved 74.78% accuracy versus 62.29% for single learners
- The Student-Expert variant, which leverages ordered test streams, showed further performance improvements over standard k-BAT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Delegation mechanisms allow the ensemble to dynamically allocate learning tasks to classifiers most suited to the current data distribution, reducing catastrophic forgetting.
- Mechanism: Classifiers are assigned to be "gurus" based on their recent performance trends (slopes of learning curves). Non-gurus delegate to gurus, allowing only the most performant classifiers to learn on the current batch while others contribute to prediction.
- Core assumption: The slope of a classifier's performance over recent batches is a reliable proxy for its suitability to the current data distribution.
- Evidence anchors: [abstract] "By drawing on work from delegative voting we develop an algorithm for using delegation to dynamically select which models in an ensemble are active." [section] "The set of all classifiers with a higher slope than ğ‘£ğ‘– is ğ‘ + (ğ‘£ğ‘– ) = {ğ‘£ ğ‘— âˆˆ ğ‘‰ |ğ‘ ğ‘—,ğ‘¤ > ğ‘ğ‘–,ğ‘¤ }... Those showing more recent improvement in accuracy."
- Break condition: If performance slopes do not correlate with actual distributional shifts, delegation becomes suboptimal and may worsen forgetting.

### Mechanism 2
- Claim: Limiting the number of active learners (gurus) forces specialization, reducing interference between contexts and mitigating forgetting.
- Mechanism: By selecting only ğ‘˜ classifiers as gurus, the ensemble ensures that each context is primarily learned by a subset of classifiers, preventing all classifiers from being exposed to every context.
- Core assumption: Specialization leads to better retention of context-specific knowledge compared to full ensemble learning.
- Evidence anchors: [abstract] "Such shifting distributions can be broken into disjoint subsets of related examples; by training each member of an ensemble on a different subset it is possible for the ensemble as a whole to achieve much higher accuracy with less forgetting than a naive model." [section] "Classifiers that have learned on fewer samples are usually more plastic... and thus, have a steeper learning curve, making them more likely to receive a delegation."
- Break condition: If contexts share significant feature overlap, specialization may hinder cross-context knowledge transfer and hurt performance.

### Mechanism 3
- Claim: Delegation probability functions allow fine-grained control over which classifiers delegate to which gurus, optimizing ensemble composition for prediction accuracy.
- Mechanism: Delegation probabilities are assigned based on metrics such as regression slope, weight, or diversity, guiding non-gurus to delegate to the most suitable gurus.
- Core assumption: The choice of delegation probability function significantly impacts ensemble prediction performance.
- Evidence anchors: [section] "We explore a variety of delegation methods drawn from existing research on liquid democracy and several measures of classifier performance." [section] "Definition 4.2. The Proportional Better Delegation Probability Function... assigns delegation probabilities such that each classifier with a higher regression slope than the delegator is assigned a probability proportional to their regression slope value."
- Break condition: If delegation probabilities do not align with actual prediction utility, ensemble performance degrades.

## Foundational Learning

- Concept: Continual Learning and Catastrophic Forgetting
  - Why needed here: The paper addresses how to enable models to learn from shifting data distributions without forgetting previously learned information.
  - Quick check question: What is catastrophic forgetting, and why does it occur in standard continual learning approaches?

- Concept: Ensemble Methods and Dynamic Selection
  - Why needed here: The approach leverages ensembles where different classifiers learn on different data subsets, requiring dynamic selection mechanisms.
  - Quick check question: How does dynamic ensemble selection differ from static ensemble methods in terms of adaptability to data shifts?

- Concept: Liquid Democracy and Delegation Mechanisms
  - Why needed here: The paper draws on liquid democracy principles to dynamically assign learning and prediction roles within the ensemble.
  - Quick check question: What is liquid democracy, and how is it adapted from voting systems to machine learning ensemble selection?

## Architecture Onboarding

- Component map:
  - Ensemble of classifiers (voters) -> Delegation function (d: V Ã— T â†’ V) -> Weight vector (W_t) based on delegation counts -> Performance tracking matrix (A) and slope calculation (Q) -> Delegation probability functions -> Data stream with contexts and batches

- Critical path:
  1. Initialize classifiers and set default self-delegation.
  2. For each batch: collect predictions, update learning for gurus, compute performance metrics.
  3. Calculate performance slopes over recent batches.
  4. Select ğ‘˜ gurus based on best slopes.
  5. Assign delegations using chosen probability function.
  6. Aggregate predictions from gurus weighted by delegation counts.

- Design tradeoffs:
  - Number of gurus (ğ‘˜) vs. specialization vs. redundancy.
  - Window size (ğ‘¤) for slope calculation vs. responsiveness to shifts.
  - Delegation probability function choice vs. ensemble adaptability.
  - Memory usage for performance tracking vs. accuracy of delegation decisions.

- Failure signatures:
  - Delegation instability: frequent changes in gurus indicate poor slope reliability.
  - Performance degradation: if slopes do not reflect actual accuracy, delegation misallocates learning.
  - Overfitting to recent batches: small window sizes may cause overfitting to noise.

- First 3 experiments:
  1. Validate delegation behavior on a simple class-incremental task with clear context shifts.
  2. Test impact of different ğ‘˜ values on catastrophic forgetting in class-incremental learning.
  3. Compare delegation probability functions in domain-incremental learning for prediction accuracy.

## Open Questions the Paper Calls Out
None

## Limitations

- The method's performance heavily depends on hyperparameter choices (ğ‘˜, ğ‘¤, delegation function), yet systematic sensitivity analysis is absent.
- The experimental results are limited to MNIST datasets, which may not generalize to more complex, real-world data distributions.
- The assumption that slope-based delegation reliably captures distributional shifts across all data types lacks empirical validation beyond the presented experiments.

## Confidence

- High confidence: The liquid democracy framework is well-established and correctly adapted for ensemble selection. The mathematical formulation and delegation mechanisms are rigorously defined.
- Medium confidence: The experimental results on MNIST datasets are compelling, but the domain-specific nature of these benchmarks limits generalizability.
- Low confidence: The assumption that slope-based delegation reliably captures distributional shifts across all data types lacks empirical validation beyond the presented MNIST experiments.

## Next Checks

1. Test k-BAT on more complex, real-world datasets (e.g., CIFAR, ImageNet) to assess scalability and robustness to noise and class imbalance.
2. Conduct ablation studies varying ğ‘˜, window size ğ‘¤, and delegation functions systematically to identify optimal configurations and understand sensitivity to hyperparameters.
3. Compare delegation slope-based selection against alternative metrics (e.g., prediction uncertainty, Fisher information) to validate slope as the most reliable proxy for distributional shifts.