---
ver: rpa2
title: 'Semantic Component Analysis: Introducing Multi-Topic Distributions to Clustering-Based
  Topic Modeling'
arxiv_id: '2410.21054'
source_url: https://arxiv.org/abs/2410.21054
tags:
- components
- topic
- bertopic
- component
- found
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Semantic Component Analysis (SCA) addresses the problem of topic
  modeling in large, noisy datasets of short texts where documents often contain multiple
  topics, a limitation in existing approaches like BERTopic that assume single topics
  per document and typically assign many samples to no topic (high noise rate). SCA
  introduces a novel decomposition step into the clustering-based topic modeling framework.
---

# Semantic Component Analysis: Introducing Multi-Topic Distributions to Clustering-Based Topic Modeling

## Quick Facts
- **arXiv ID**: 2410.21054
- **Source URL**: https://arxiv.org/abs/2410.21054
- **Reference count**: 40
- **Key outcome**: SCA discovers multiple semantic components per document, achieving near-zero noise rate and doubling topic count compared to BERTopic while maintaining competitive coherence and diversity

## Executive Summary
Semantic Component Analysis (SCA) addresses the fundamental limitation in clustering-based topic modeling where documents are typically assigned to single topics, despite often containing multiple semantic themes. By introducing an iterative decomposition step that projects embeddings along cluster centroids and computes residuals, SCA uncovers multiple nuanced semantic components per document while maintaining the efficiency and interpretability of clustering-based approaches. Evaluated across English, Chinese, and Hausa Twitter datasets, SCA consistently doubles topic discovery (182 vs 55 topics for Trump dataset) while reducing noise from 27-57% to near zero, with components representing distinct semantic patterns verified through low token overlap scores.

## Method Summary
SCA extends clustering-based topic modeling by adding an iterative decomposition mechanism. After initial clustering with UMAP and HDBSCAN, it computes cluster centroids and iteratively decomposes embeddings by removing projections along these centroids (controlled by hyperparameter μ), creating residual embeddings for subsequent clustering. Components are represented using c-TF-IDF and merged if token overlap exceeds threshold θ to prevent duplicates. The method uses cosine similarity for clustering, maintains near-zero noise rate through the decomposition approach, and preserves interpretability through token-based component representations.

## Key Results
- **Topic discovery**: At least doubles topics/components compared to BERTopic (182 vs 55 for Trump, 331 vs 102 for Hausa, 594 vs 212 for Chinese)
- **Noise reduction**: Near-zero noise rate achieved versus 27-57% for BERTopic across all datasets
- **Quality maintenance**: Competitive topic coherence and diversity matching BERTopic with distinct semantic patterns (average max token overlap 1.39-1.85 out of 10)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SCA discovers multiple semantic components per document by iteratively decomposing embeddings along cluster centroids.
- Mechanism: After initial clustering, SCA represents each cluster with a normalized centroid vector. It then decomposes each embedding by removing the projection along this centroid vector (scaled by hyperparameter μ), creating residual embeddings. This process repeats on residuals, uncovering additional components in subsequent iterations.
- Core assumption: Document embeddings contain multiple semantically meaningful linear components that can be isolated through sequential projection removal.
- Evidence anchors:
  - [abstract]: "discovering multiple topics per sample by introducing a decomposition step to the clustering-based topic modeling framework"
  - [section]: "we introduce a decomposition step and iterate over the residual embeddings. This decomposition enables us to represent a sample's content independently of the semantic components found in previous iterations"
- Break condition: If residual embeddings after decomposition contain no meaningful information (low variance) or clustering yields no new clusters, iterations terminate.

### Mechanism 2
- Claim: Linear decomposition captures distinct semantic patterns not found by BERTopic's single-topic assumption.
- Mechanism: BERTopic clusters once and assigns one topic per document. SCA's iterative decomposition allows documents to be represented across multiple components by progressively removing dominant semantic directions, revealing sub-patterns and cross-topic themes.
- Core assumption: Short texts often contain multiple topics, and linear components along embedding directions can represent these distinct semantic aspects.
- Evidence anchors:
  - [abstract]: "discovers multiple, nuanced semantic components beyond a single topic in short texts"
  - [section]: "the additional components relate to patterns distinct from the topics found by BERTopic, which is evident from the low average maximum token overlap scores"
- Break condition: If token overlap between SCA components and BERTopic topics remains consistently high, indicating no new semantic information is being captured.

### Mechanism 3
- Claim: Component merging prevents duplicate components while maintaining semantic granularity.
- Mechanism: After decomposition, components are represented using c-TF-IDF token weighting. Components with token overlap above threshold θ are merged, combining similar semantic directions while avoiding redundancy from multi-dimensional cluster spread.
- Core assumption: Similar semantic patterns produce similar token distributions, allowing merging without losing distinct semantic information.
- Evidence anchors:
  - [section]: "We introduce a merging step that computes the token overlap between two components as the fraction of tokens their representations share"
  - [section]: "This is necessary to avoid duplicate components that arise for large clusters spread across a non-linear or multi-dimensional part of the embedding space"
- Break condition: If merging threshold θ is too low, meaningful distinct components may be incorrectly merged; if too high, duplicates persist.

## Foundational Learning

- Concept: Linear algebra - vector projection and residual computation
  - Why needed here: Core SCA mechanism relies on projecting embeddings onto centroid vectors and computing residuals to isolate semantic components
  - Quick check question: Given embedding x and centroid v, what is the formula for the residual after removing the projection along v with scaling μ?

- Concept: Clustering algorithms and distance metrics
  - Why needed here: SCA uses UMAP with cosine distance for dimensionality reduction and HDBSCAN for clustering; understanding these is crucial for implementation
  - Quick check question: Why does SCA use cosine distance in UMAP rather than Euclidean distance?

- Concept: Topic coherence metrics (NPMI, CV)
  - Why needed here: Evaluating whether SCA maintains topic quality while finding more components requires understanding these metrics
  - Quick check question: What does NPMI coherence measure, and why is it important for evaluating topic modeling methods?

## Architecture Onboarding

- Component map: Input documents → Sentence embeddings (SBERT) → UMAP dimensionality reduction → HDBSCAN clustering → Centroid representation → Linear decomposition (residual computation) → Iterative clustering on residuals → c-TF-IDF component representation → Token overlap-based merging → Output components with medoids
- Critical path: Embedding generation → First clustering iteration → Decomposition step → Component representation → Merging step
- Design tradeoffs: Linear decomposition vs. non-linear methods (SCA is efficient but may miss non-linear patterns); higher component count vs. interpretability (more components = more nuanced but potentially redundant)
- Failure signatures: High token overlap between components (indicates poor decomposition); low component count despite high μ (indicates decomposition removing too much information); high noise rate (clustering not capturing meaningful structure)
- First 3 experiments:
  1. Run SCA with μ=1.0, α=0.0 on small dataset to verify it matches BERTopic on first iteration
  2. Vary α from 0.0 to 0.9 with μ=1.0 to observe effect on component count and overlap
  3. Test merging threshold θ=0.5 vs θ=0.7 to see impact on duplicate prevention while preserving distinct components

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SCA perform on longer documents compared to short texts, and what modifications might be needed for optimal performance on multi-topic longer documents?
- Basis in paper: [inferred] The paper acknowledges that longer documents could be split into smaller parts for SCA application, but notes this as a limitation and suggests it requires further investigation.
- Why unresolved: The paper only evaluates SCA on Twitter datasets (short texts). The performance on longer documents with inherent multi-topic content remains untested.
- What evidence would resolve it: Comparative experiments applying SCA to both split longer documents and continuous longer texts, measuring topic coherence, diversity, and noise rate against single-topic baselines.

### Open Question 2
- Question: What is the computational complexity of SCA compared to BERTopic and TopicGPT for large-scale datasets, and how does it scale with increasing dataset size and dimensionality?
- Basis in paper: [explicit] The paper mentions that BERTopic typically has high noise rates and that SCA maintains near-zero noise rate, but doesn't provide detailed runtime or memory complexity analysis. The authors note GPU implementation could improve UMAP performance but found inconsistent results.
- Why unresolved: While runtime data is provided for specific datasets, there's no systematic complexity analysis or scaling study. The GPU vs CPU implementation trade-offs remain unclear.
- What evidence would resolve it: Empirical studies measuring runtime, memory usage, and scaling behavior across datasets of varying sizes (10K to 10M samples) and dimensionalities, comparing SCA, BERTopic, and TopicGPT implementations.

### Open Question 3
- Question: How robust is SCA to different embedding models and dimensionality reduction techniques, and what impact do these choices have on topic quality and interpretability?
- Basis in paper: [explicit] The paper uses a modular approach allowing replacement of embedding, dimensionality reduction, clustering, and representation methods, and notes that SBERT and UMAP choices make first iteration equivalent to BERTopic.
- Why unresolved: The paper uses specific choices (UMAP, HDBSCAN, SBERT) but doesn't systematically evaluate alternative combinations or their impact on performance metrics.
- What evidence would resolve it: Ablation studies testing various embedding models (Doc2Vec, LaBSE, custom models), dimensionality reduction methods (t-SNE, PCA), and clustering algorithms (K-means, spectral clustering) while measuring changes in coherence, diversity, noise rate, and downstream task performance.

## Limitations

- **Short text focus**: SCA is evaluated primarily on Twitter datasets; performance on longer documents or non-social-media domains remains untested
- **Hyperparameter sensitivity**: Optimal values for α, μ, and θ are not fully explored, and performance may vary significantly with dataset characteristics
- **Linear decomposition assumption**: The method assumes semantic components can be captured through linear projections, potentially missing non-linear semantic relationships

## Confidence

- **Core performance claims**: Medium-High (doubling topics while maintaining coherence is well-supported across three datasets)
- **Near-zero noise rate**: Medium-High (consistent across all three languages with appropriate parameter tuning)
- **Generalization across languages**: Medium (Hausa results positive but based on smaller dataset; English/Chinese provide stronger evidence)
- **Decomposition mechanism attribution**: Medium (token overlap analysis supports the claim, but controlled ablation studies would strengthen causal attribution)

## Next Checks

1. **Ablation Study**: Run SCA variants with μ=0 (no decomposition, matching BERTopic baseline) and μ=1.0 (full decomposition) on the same datasets to quantify the decomposition step's contribution to topic discovery versus other factors like clustering parameters.

2. **Cross-Domain Testing**: Apply SCA to a non-short-text domain such as academic papers or news articles to evaluate whether the multi-topic discovery mechanism generalizes beyond social media content and whether new hyperparameters are needed for longer documents.

3. **Parameter Sensitivity Analysis**: Systematically vary α ∈ [0, 0.5, 0.9] and θ ∈ [0.3, 0.5, 0.7] across all three datasets to map the performance landscape and identify whether there are consistent optimal settings or significant dataset-dependent variations.