---
ver: rpa2
title: A Unified Optimal Transport Framework for Cross-Modal Retrieval with Noisy
  Labels
arxiv_id: '2403.13480'
source_url: https://arxiv.org/abs/2403.13480
tags:
- noisy
- labels
- learning
- cross-modal
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes UOT-RCL, a unified optimal transport framework
  for robust cross-modal retrieval under noisy labels. The method addresses two key
  challenges: enforcing multimodal samples to align incorrect semantics and widening
  the heterogeneous gap caused by noisy labels.'
---

# A Unified Optimal Transport Framework for Cross-Modal Retrieval with Noisy Labels

## Quick Facts
- **arXiv ID:** 2403.13480
- **Source URL:** https://arxiv.org/abs/2403.13480
- **Reference count:** 40
- **Primary result:** UOT-RCL framework significantly outperforms state-of-the-art approaches for cross-modal retrieval with noisy labels, achieving up to 7.4% improvement in mean average precision

## Executive Summary
This paper introduces UOT-RCL, a unified optimal transport framework designed to address cross-modal retrieval challenges in the presence of noisy labels. The framework tackles two fundamental issues: enforcing alignment of multimodal samples with incorrect semantics and managing the increased heterogeneity gap caused by label noise. By leveraging partial optimal transport for progressive label correction and incorporating OT-based relation alignment for semantic matching, UOT-RCL demonstrates robust performance across multiple benchmark datasets.

## Method Summary
UOT-RCL employs a two-pronged approach to cross-modal retrieval with noisy labels. First, it utilizes partial optimal transport to progressively correct noisy labels through a cross-modal consistent cost function that captures semantic relationships between modalities. Second, it implements an OT-based relation alignment mechanism to infer semantic-level cross-modal matching, effectively narrowing the modality gap. The framework is designed to be robust across various noise patterns and label corruption rates, with experiments demonstrating significant improvements over existing state-of-the-art methods.

## Key Results
- UOT-RCL achieves up to 7.4% improvement in mean average precision under various noise settings
- The framework demonstrates robust performance across three widely-used cross-modal retrieval datasets
- Significant improvements over state-of-the-art approaches for cross-modal retrieval with noisy labels

## Why This Works (Mechanism)
The effectiveness of UOT-RCL stems from its dual optimization strategy that simultaneously addresses label noise and modality gap challenges. The partial optimal transport component enables progressive refinement of noisy labels by leveraging cross-modal consistency, while the relation alignment component ensures semantic-level matching between heterogeneous data types. This unified approach allows the model to maintain retrieval accuracy even as label quality degrades.

## Foundational Learning
- **Optimal Transport Theory**: Needed for establishing mathematical foundations of cross-modal alignment; quick check: verify Wasserstein distance calculations
- **Partial Optimal Transport**: Required for handling incomplete or noisy correspondence between modalities; quick check: confirm partial transport constraints are properly implemented
- **Cross-Modal Consistency Metrics**: Essential for measuring alignment quality between different data types; quick check: validate consistency scoring functions
- **Semantic-Level Matching**: Critical for bridging the modality gap beyond surface-level features; quick check: examine semantic embedding spaces
- **Noise Pattern Analysis**: Important for understanding label corruption effects; quick check: verify noise injection mechanisms

## Architecture Onboarding
**Component Map:** Cross-Modal Data -> Partial OT Module -> Label Correction -> OT Relation Alignment -> Retrieval Output

**Critical Path:** Input data undergoes initial encoding → Partial OT module progressively corrects labels → OT relation alignment establishes semantic matching → Final retrieval predictions generated

**Design Tradeoffs:** The framework prioritizes label noise robustness over computational efficiency, requiring multiple optimal transport computations that increase training time but improve accuracy under noisy conditions.

**Failure Signatures:** Poor performance on datasets with highly domain-specific terminology, degradation when noise patterns deviate significantly from training assumptions, potential overfitting to specific noise types.

**First Experiments:**
1. Baseline retrieval performance on clean labels without noise
2. Performance comparison under uniform vs. class-conditional label noise
3. Ablation study isolating partial OT and relation alignment contributions

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains may vary significantly across different noise patterns and dataset characteristics
- Limited evaluation scope with only three datasets, potentially missing diverse real-world scenarios
- Insufficient ablation studies to isolate individual component contributions

## Confidence
- **High confidence:** Overall framework design and mathematical formulation are sound
- **Medium confidence:** Experimental results demonstrating performance improvements
- **Medium confidence:** Claims about robustness to various noise settings

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of semantic alignment and OT-based relation alignment components
2. Test the framework on additional cross-modal datasets with different characteristics (e.g., medical imaging-text pairs, multimodal social media data)
3. Evaluate performance under different noise injection strategies and corruption rates beyond what was reported in the paper