---
ver: rpa2
title: 'Optimizing Time Series Forecasting Architectures: A Hierarchical Neural Architecture
  Search Approach'
arxiv_id: '2406.05088'
source_url: https://arxiv.org/abs/2406.05088
tags:
- forecasting
- search
- architecture
- series
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces DARTS-TS, a novel hierarchical neural architecture
  search approach for time series forecasting. The method designs a search space that
  incorporates various architecture types and enables efficient combination of different
  forecasting modules.
---

# Optimizing Time Series Forecasting Architectures: A Hierarchical Neural Architecture Search Approach

## Quick Facts
- **arXiv ID**: 2406.05088
- **Source URL**: https://arxiv.org/abs/2406.05088
- **Reference count**: 34
- **Primary result**: DARTS-TS achieves comparable or better performance than handcrafted state-of-the-art models while requiring significantly fewer computational resources.

## Executive Summary
This work introduces DARTS-TS, a novel hierarchical neural architecture search approach for time series forecasting. The method designs a search space that incorporates various architecture types and enables efficient combination of different forecasting modules. Experiments on long-term time series forecasting tasks show that DARTS-TS can identify lightweight, high-performing architectures across different forecasting tasks.

## Method Summary
DARTS-TS is a neural architecture search method that optimizes time series forecasting architectures through a hierarchical search space with three levels: operation level (defining candidate operations), micro network level (structuring Flat Net and Seq Net families), and macro architecture level (combining Flat and Seq networks). The approach trains a supernet with joint optimization of weights and architecture parameters, then prunes operations and edges using a perturbation-based importance measure. The final architecture is evaluated on target datasets to assess forecasting performance.

## Key Results
- DARTS-TS achieves comparable or better performance than handcrafted state-of-the-art models while requiring significantly fewer computational resources
- The approach shows effectiveness on multiple datasets, including ECL, ETT, Traffic, and Weather
- Experiments demonstrate that DARTS-TS often outperforms baseline methods in both accuracy and efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The hierarchical search space enables efficient exploration of diverse forecasting architectures by combining operations from different families in both encoder and decoder components.
- **Mechanism**: The search space is designed at three levels: operation level (defining candidate operations), micro network level (structuring Flat Net and Seq Net families), and macro architecture level (combining Flat and Seq networks). This allows the search algorithm to find architectures that blend strengths of different operation families, such as combining local dependency capture (TCN) with global pattern modeling (Transformer).
- **Core assumption**: Different forecasting tasks benefit from different combinations of operation families, and a flexible hierarchical space can capture these combinations better than homogeneous search spaces.
- **Evidence anchors**: [abstract]: "We design a hierarchical search space that contains most forecasting architecture design decisions and allows any sort of architecture layers to be combined to form new architectures." [section 4.1]: "Our search space encompasses operations from different architectures...the operations contained in DARTS-TS come from different families."

### Mechanism 2
- **Claim**: DARTS-TS can identify lightweight, high-performing architectures by pruning unnecessary operations and edges after joint training of weights and architecture parameters.
- **Mechanism**: After training the supernet with gradient descent, a perturbation-based pruning approach removes less important operations and edges. The hierarchical pruning starts from the lowest granularity (operations) and progresses to edges, ensuring that dependent operations are pruned after related encoder operations.
- **Core assumption**: The perturbation-based importance measure reliably identifies suboptimal operations and edges without requiring retraining from scratch.
- **Evidence anchors**: [section 5.1]: "We propose pruning our network from the lowest granularity level and gradually increasing the granularity level until we prune the operations in our search space."

### Mechanism 3
- **Claim**: DARTS-TS achieves competitive accuracy with significantly lower computational cost by leveraging a single forward pass instead of multiple passes for multi-variate series.
- **Mechanism**: Unlike PatchTST, which decomposes a multi-variate series into multiple uni-variate series and requires separate forward passes, DARTS-TS processes the entire series in one pass. The combination of diverse operations also allows capturing receptive fields more efficiently than stacking many similar layers.
- **Core assumption**: A single-pass architecture can match or exceed the performance of decomposed multi-pass approaches while reducing latency and memory usage.
- **Evidence anchors**: [section 6.3]: "DARTS-TS only performs one forward pass to get the forecasting results, which could significantly reduce the required computational power and the memory requirements."

## Foundational Learning

- **Concept: Neural Architecture Search (NAS)**
  - Why needed here: DARTS-TS is a NAS method that automates the design of time series forecasting architectures. Understanding NAS principles (supernet training, differentiable relaxation, architecture parameter optimization) is essential to grasp how DARTS-TS works.
  - Quick check question: What is the difference between a one-shot NAS approach and traditional NAS methods that train each architecture from scratch?

- **Concept: Time Series Forecasting Families (Seq Net vs Flat Net)**
  - Why needed here: DARTS-TS operates on two main architecture families—Seq Net (maintaining sequence structure) and Flat Net (decomposing variables). Knowing their differences explains why a hierarchical search space is necessary.
  - Quick check question: How does a Seq Net handle multi-variate time series differently from a Flat Net?

- **Concept: Hierarchical Pruning in NAS**
  - Why needed here: DARTS-TS uses hierarchical pruning to remove unnecessary operations and edges. Understanding pruning strategies (e.g., operation-level vs edge-level) is critical for interpreting the search process and results.
  - Quick check question: Why might it be important to prune dependent operations (e.g., decoder choice) after related encoder operations?

## Architecture Onboarding

- **Component map**:
  - Operation Level: Defines candidate operations (e.g., TSMixer, LSTM, GRU, Transformer, TCN, SepTCN, skip connections for Seq Net; Linear, N-BEATS variants, skip connections for Flat Net)
  - Micro Network Level: Structures Flat Net (stacking Flat cells) and Seq Net (encoder-decoder pairs with various decoder types)
  - Macro Architecture Level: Combines Flat Net and Seq Net outputs with weighted sum
  - Forecasting Heads: Maps encoder/decoder outputs to predictions using MSE, MAE, or quantile loss

- **Critical path**:
  1. Define hierarchical search space (operations → micro networks → macro combination)
  2. Train supernet with joint optimization of weights and architecture parameters
  3. Prune operations and edges using perturbation-based importance
  4. Evaluate final architecture on target dataset

- **Design tradeoffs**:
  - Flexibility vs Complexity: Hierarchical search space allows diverse architectures but increases search space size and computational cost
  - Single Pass vs Multiple Passes: DARTS-TS uses single pass for efficiency but may limit cross-variable dependency modeling compared to multi-pass methods like PatchTST
  - Operation Diversity vs Specialization: Including many operation families increases chances of finding optimal combinations but may introduce instability or redundancy

- **Failure signatures**:
  - High validation loss with low training loss: Overfitting to training set; consider stronger regularization or early stopping
  - Dominance of skip connections in pruned architecture: Instability in search process; try robust DARTS variants or adjust perturbation thresholds
  - Poor performance on specific datasets despite good average performance: Search space may lack task-specific inductive biases; consider dataset-specific operation additions

- **First 3 experiments**:
  1. Search on ECL dataset with forecasting horizon 96: Validate that DARTS-TS can find architectures competitive with handcrafted models on a standard benchmark
  2. Ablation on RevIN: Compare DARTS-TS with and without reversible instance normalization to assess its impact on accuracy and stability
  3. Window size sensitivity: Search with different input window sizes (96, 192, 336, 720) on ECL-96 and evaluate transfer performance to assess robustness to window size choice

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can zero-cost proxies be effectively generalized to compare performance across different operation types in complex search spaces?
- **Basis in paper**: The paper shows that nearly all existing zero-cost proxies fail when applied to the heterogeneous search space containing operations from different architecture families.
- **Why unresolved**: The search space encompasses operations from different architecture types (e.g., transformers, LSTMs, CNNs), making performance differences difficult to capture with simple proxies that work well for homogeneous spaces.
- **What evidence would resolve it**: Developing and validating new zero-cost proxies that can effectively rank architectures containing mixed operation types, or proving theoretically why such proxies cannot exist for heterogeneous spaces.

### Open Question 2
- **Question**: How does the optimal architecture transfer across different forecasting horizons and window sizes?
- **Basis in paper**: The paper shows that models searched with different window sizes and forecasting horizons can transfer to some extent, but performance varies depending on the similarity between search and evaluation settings.
- **Why unresolved**: While some transfer is possible, the paper demonstrates that the optimal architecture may vary depending on the specific forecasting horizon and window size used during search and evaluation.
- **What evidence would resolve it**: Systematic experiments evaluating the performance of architectures searched with different combinations of window sizes and forecasting horizons across all possible evaluation settings.

### Open Question 3
- **Question**: What is the impact of different training-validation split strategies on the generalization ability of searched architectures?
- **Basis in paper**: The paper compares a holdout strategy with 5-fold cross-validation during the search phase, showing some performance differences across datasets.
- **Why unresolved**: The paper only compares two specific strategies (holdout vs. 5-fold cross-validation) and does not explore other potential strategies or their impact on generalization to unseen data.
- **What evidence would resolve it**: Experiments comparing multiple training-validation split strategies (e.g., different cross-validation schemes, time-series specific splits) and their impact on test set performance across diverse datasets.

## Limitations
- The hierarchical search space design, while comprehensive, lacks direct empirical validation that combining diverse operation families provides consistent benefits across all forecasting tasks
- The perturbation-based pruning mechanism has limited evidence for reliability across different dataset characteristics
- Efficiency claims comparing single-pass DARTS-TS to multi-pass methods need more rigorous benchmarking, particularly for datasets with complex cross-variable dependencies

## Confidence
- **High confidence**: The core claim that DARTS-TS can identify lightweight, high-performing architectures is well-supported by experimental results showing competitive performance with reduced computational cost across multiple benchmarks
- **Medium confidence**: The hierarchical search space design effectively enables diverse architecture combinations, though evidence is primarily from ablation studies rather than direct comparisons with alternative search space designs
- **Low confidence**: The perturbation-based pruning mechanism reliably identifies optimal operations and edges, as the paper provides limited evidence on pruning stability and the risk of removing useful components

## Next Checks
1. **Pruning stability test**: Run the search process multiple times with different random seeds and analyze the consistency of pruned architectures. Compare architectures from multiple runs to assess whether the perturbation-based importance measure produces stable results.

2. **Cross-dataset transfer evaluation**: Take architectures discovered on one dataset (e.g., ECL) and evaluate their performance on unseen datasets (e.g., Traffic, Weather) without retraining. This would validate whether DARTS-TS discovers generalizable architectural patterns or dataset-specific solutions.

3. **Single-pass vs multi-pass comparison**: Implement a controlled experiment comparing DARTS-TS (single-pass) against an equivalent multi-pass architecture that processes each variable separately. Measure both accuracy and computational efficiency across datasets with varying numbers of variables and dependency structures.