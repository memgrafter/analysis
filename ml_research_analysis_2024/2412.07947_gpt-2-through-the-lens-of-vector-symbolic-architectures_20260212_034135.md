---
ver: rpa2
title: GPT-2 Through the Lens of Vector Symbolic Architectures
arxiv_id: '2412.07947'
source_url: https://arxiv.org/abs/2412.07947
tags:
- vectors
- vector
- orthogonal
- concept
- nearly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how GPT-2 might use vector symbolic architectures
  (VSA) principles, specifically nearly orthogonal vector bundling and binding operations,
  for computation and communication between layers. The authors propose that the residual
  stream in transformers could represent distributed codes composed of nearly orthogonal
  concept vectors, and that operations like attention and MLP layers might perform
  bundling and binding operations similar to VSA.
---

# GPT-2 Through the Lens of Vector Symbolic Architectures

## Quick Facts
- arXiv ID: 2412.07947
- Source URL: https://arxiv.org/abs/2412.07947
- Authors: Johannes Knittel; Tushaar Gangavarapu; Hendrik Strobelt; Hanspeter Pfister
- Reference count: 34
- Primary result: GPT-2 uses nearly orthogonal vector bundling and binding operations similar to VSA principles

## Executive Summary
This paper proposes that GPT-2 employs vector symbolic architecture (VSA) principles for computation and communication between layers. The authors suggest that the residual stream contains bundled concept vectors composed of nearly orthogonal word embeddings, and that attention and MLP layers perform unbinding, selection, and rebinding operations similar to VSA. Through experiments on GPT-2 small, they find evidence supporting this framework by demonstrating near orthogonality in word embeddings, attention matrices, and MLP projections, as well as identifying neurons that can be explained by sums of token embeddings.

## Method Summary
The authors analyze GPT-2 (small) by extracting weights and activations using the TransformerLens library. They compute matrix products (M^T * M) for word embeddings, attention matrices, and MLP output projections to check for near orthogonality. For MLP neurons, they greedily assemble bundled vectors from token embeddings and compute cosine similarity with neural weights to identify neurons explained by sums of token embeddings. The FAISS library is used for efficient dot product nearest neighbor search.

## Key Results
- Word embeddings, attention matrices, and MLP output projections show strong diagonal dominance in M^T * M matrices, indicating near orthogonality
- More than 80% of neurons in the first MLP layer can be explained by sums of token embeddings with cosine similarity of 0.5 or higher
- Identified circuits where neural weights can be explained by simple bundled vectors, providing evidence for matrix binding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-2 uses nearly orthogonal vector bundling to represent and process concepts in the residual stream
- Mechanism: The residual stream contains bundled concept vectors created by summing nearly orthogonal word embeddings and neural outputs. These bundled vectors maintain directional similarity to their constituent vectors, enabling OR-like logical operations through simple dot products with ReLU activations.
- Core assumption: Word embeddings and neural weight vectors are composed of nearly orthogonal vectors that can be bundled without significant loss of information
- Evidence anchors:
  - [abstract] "GPT-2 uses mechanisms involving nearly orthogonal vector bundling and binding operations similar to VSA"
  - [section 2.1] "The word embeddings of GPT-2 are nearly orthogonal to each other"
  - [section 3.1] "We computed the matrix product M⊺M for word embeddings... we observed strong diagonals with minimal artifacts"
- Break condition: If word embeddings or neural weights contain significant non-orthogonal components, bundling would lead to information loss and incorrect concept representation

### Mechanism 2
- Claim: Attention mechanisms perform unbinding, selection, and rebinding operations on concept vectors
- Mechanism: Attention computes query, key, and value vectors through nearly orthogonal weight matrices. The softmax operation selects relevant value vectors based on dot product similarity, and the output projection rebinds the selected vectors into new concept vectors. This sequence enables copying and repackaging concepts from other tokens.
- Core assumption: Attention matrices (WQ, WK, WV, WO) are composed of nearly orthogonal vectors that preserve dot product relationships through transformations
- Evidence anchors:
  - [abstract] "GPT-2 uses mechanisms involving nearly orthogonal vector bundling and binding operations"
  - [section 2.2] "the attention head focuses on a specific package of possibly bundled vectors by rotating the input space appropriately"
  - [section 3.1] "we computed the matrix product M⊺M for... the attention matrices... we observed strong diagonals with minimal artifacts"
- Break condition: If attention matrices lose orthogonality through training, the unbinding/rebinding interpretation breaks down and attention becomes less interpretable

### Mechanism 3
- Claim: MLP layers perform boolean operations on concept vectors using ReLU activations and nearly orthogonal weight vectors
- Mechanism: Individual neurons in MLP layers perform weighted summations using weights resembling concept vectors. ReLU nonlinearity enables OR and AND conjunctions through bias selection. The second MLP layer binds results into new concept vectors. This allows complex boolean logic on bundled concepts.
- Core assumption: MLP weights can be decomposed into sums of nearly orthogonal concept vectors, enabling interpretable boolean operations
- Evidence anchors:
  - [abstract] "some neurons in the first MLP layer can be explained by sums of token embeddings, suggesting that these neurons perform boolean operations"
  - [section 2.4] "A single neuron in the first MLP layer could perform a simple check of whether the current stream contains a specific concept"
  - [section 3.2] "we achieve a similarity of 0.5 or higher for more than 80% of the neurons in the first layer"
- Break condition: If MLP weights cannot be explained by simple bundled vectors, the boolean operation interpretation fails and neurons may represent more complex, distributed functions

## Foundational Learning

- Concept: Vector symbolic architectures and high-dimensional computing
  - Why needed here: The entire framework relies on understanding how nearly orthogonal vectors can represent concepts and perform operations through bundling and binding
  - Quick check question: How many nearly orthogonal vectors can exist in a 1000-dimensional space compared to the number of strictly orthogonal vectors?

- Concept: Dot product geometry and cosine similarity
  - Why needed here: The paper's interpretation depends on understanding how dot products between bundled vectors relate to the presence/absence of constituent concepts
  - Quick check question: If vector c1 has dot product 0.8 with bundled vector c1+c2, what does this tell you about c2's relationship to c1?

- Concept: Matrix transformations and orthogonality preservation
  - Why needed here: The binding operations require understanding how matrix multiplication affects vector relationships and dot products
  - Quick check question: What happens to the dot product between two vectors after transformation by a nearly orthogonal matrix?

## Architecture Onboarding

- Component map: Word embeddings -> Attention layers -> MLP layers -> Unembedding prediction
- Critical path: Embedding → Attention processing → MLP processing → Unembedding prediction
  - Each layer adds nearly orthogonal concept vectors to the residual stream
  - Attention copies and repackages concepts from previous tokens
  - MLP performs boolean logic and creates new concepts
  - Final unembedding selects the strongest concept direction
- Design tradeoffs:
  - Nearly orthogonal vectors enable many concepts but may sacrifice semantic smoothness
  - Simple bundling/binding vs. complex distributed representations
  - Interpretability through VSA lens vs. traditional neural network interpretation
  - Computational efficiency of simple operations vs. expressive power
- Failure signatures:
  - Strong off-diagonal elements in M⊺M matrices indicate loss of orthogonality
  - MLP weights that cannot be explained by bundled vectors suggest different computation
  - Low cosine similarity between attention output and next token embeddings
  - Mean of embeddings significantly different from zero indicating LayerNorm issues
- First 3 experiments:
  1. Compute M⊺M for word embeddings and attention matrices to verify near-orthogonality
  2. Explain MLP neuron weights using greedy sums of token embeddings to test boolean operation hypothesis
  3. Trace concept vectors through attention heads to verify unbinding/rebinding interpretation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do other transformer architectures beyond GPT-2 (small) exhibit similar bundling and binding operations as proposed in the VSA framework?
- Basis in paper: [inferred] The authors note that their findings are preliminary and only performed experiments on GPT-2 small, stating that "further experiments are needed to confirm that these behaviors also exist in related and more recent models."
- Why unresolved: The paper only investigates GPT-2 small, and it's unclear if the observed phenomena generalize to other transformer architectures or larger models.
- What evidence would resolve it: Conducting similar experiments on other transformer architectures (e.g., BERT, RoBERTa, GPT-3) and comparing the results would provide evidence for or against the generalizability of the VSA framework.

### Open Question 2
- Question: How do the bundling and binding operations in transformers relate to the training process and the emergence of these mechanisms?
- Basis in paper: [explicit] The authors state that "it may help to understand which architectural components promote or hinder the learning of bundling and binding operations, possibly leading to better model architectures."
- Why unresolved: The paper focuses on analyzing the final trained model but doesn't investigate how these operations emerge during training or which architectural choices facilitate their development.
- What evidence would resolve it: Studying the training dynamics of transformers and identifying the stages at which bundling and binding operations emerge, as well as experimenting with architectural modifications, could shed light on their relationship with the training process.

### Open Question 3
- Question: How do the bundling and binding operations in transformers contribute to the model's overall performance and task-specific capabilities?
- Basis in paper: [inferred] The authors mention that a better understanding of the inner workings "may also inform future architectural decisions" and help hypothesize "for which type of tasks GPT-like models work better or worse."
- Why unresolved: While the paper demonstrates the existence of these operations, it doesn't investigate their impact on the model's performance or how they contribute to specific capabilities.
- What evidence would resolve it: Conducting ablation studies to remove or modify bundling and binding operations and measuring their impact on the model's performance across various tasks would provide insights into their contribution to the model's capabilities.

## Limitations
- The paper only investigates GPT-2 small, limiting generalizability to other transformer architectures
- Statistical significance and confidence intervals for similarity scores are not provided
- The framework doesn't establish whether the observed VSA-like operations are intentional computational strategies or emergent properties

## Confidence
- High Confidence: The observation that word embeddings, attention matrices, and MLP projections are composed of nearly orthogonal vectors
- Medium Confidence: The interpretation that MLP neurons perform boolean operations on bundled concepts
- Low Confidence: The specific unbinding/rebinding interpretation of attention mechanisms

## Next Checks
1. Systematically vary the threshold for "near orthogonality" and measure how this affects the ability to explain neural weights
2. Apply the same analysis to other transformer models (GPT-2 medium/large, BERT, etc.) to determine if VSA patterns are general
3. Modify the near-orthogonality of word embeddings and observe effects on model performance and neural weight interpretability