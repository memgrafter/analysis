---
ver: rpa2
title: 'Hard to Explain: On the Computational Hardness of In-Distribution Model Interpretation'
arxiv_id: '2408.03915'
source_url: https://arxiv.org/abs/2408.03915
tags:
- explanations
- complexity
- input
- proc
- conf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the computational complexity of interpreting
  ML models when the explanations must align with a specific data distribution to
  avoid socially misaligned outputs. The authors formalize the problem by introducing
  a context indicator that filters out-of-distribution (OOD) inputs, and show that
  many common explanation forms (sufficient reasons, contrastive reasons, Shapley
  values) can be unified under a single second-order logic framework.
---

# Hard to Explain: On the Computational Hardness of In-Distribution Model Interpretation

## Quick Facts
- arXiv ID: 2408.03915
- Source URL: https://arxiv.org/abs/2408.03915
- Authors: Guy Amir; Shahaf Bassan; Guy Katz
- Reference count: 40
- Key outcome: Distribution alignment significantly impacts computational complexity of model interpretability, with unified second-order logic framework showing interpretation can be as hard as interpreting the OOD detector itself

## Executive Summary
This paper establishes that generating interpretable explanations for machine learning models that are aligned with a specific data distribution is computationally hard. The authors formalize this by introducing context indicators that filter out-of-distribution inputs and prove that common explanation forms (sufficient reasons, contrastive reasons, Shapley values) can be unified under second-order logic. They demonstrate that distribution-aware interpretation is at least as hard as interpreting the OOD detector itself, and introduce the concept of "self-alignment" where certain model classes can inherently produce distribution-aligned explanations without external detectors.

## Method Summary
The authors develop a theoretical framework that unifies various explanation forms under second-order logic with a context indicator representing distribution alignment. They prove computational hardness results by showing reductions from OOD detection problems to explanation problems, establishing that distribution-aware interpretation inherits the complexity of OOD detection. The framework introduces "self-alignment" as a property where a model class can generate distribution-aligned explanations intrinsically. They prove decision trees and neural networks are self-aligned while linear classifiers are not, using formal proofs based on model structure and behavior under distribution shifts.

## Key Results
- Distribution-aware interpretation is at least as hard as interpreting the OOD detector itself
- Common explanation forms (sufficient reasons, contrastive reasons, Shapley values) can be unified under second-order logic framework
- Decision trees and neural networks are self-aligned (can produce distribution-aligned explanations intrinsically), while linear classifiers are not
- The computational complexity of interpretation increases significantly when distribution alignment constraints are imposed

## Why This Works (Mechanism)
The theoretical framework works by establishing a precise connection between explanation generation and OOD detection through second-order logic. By formalizing the context indicator that captures distribution alignment, the authors can prove that generating valid explanations requires solving problems at least as hard as OOD detection. The self-alignment property emerges from the structural characteristics of certain model classes that inherently encode distribution information in their decision boundaries.

## Foundational Learning

**Second-order logic with context indicators**: A formal system extending first-order logic with quantification over predicates, where context indicators filter inputs by distribution. Needed to formalize distribution-aligned explanations. Quick check: Can you express a sufficient reason with a context constraint using this logic?

**Computational hardness and reductions**: Proof technique showing one problem is at least as hard as another by reducing one to the other. Needed to establish lower bounds on explanation complexity. Quick check: Can you trace through a reduction from OOD detection to explanation generation?

**Self-alignment property**: A model class characteristic where explanations are inherently distribution-aligned without external OOD detection. Needed to identify model classes that avoid the computational hardness of distribution-aware interpretation. Quick check: Can you explain why linear classifiers fail the self-alignment test?

## Architecture Onboarding

**Component map**: OOD Detector -> Context Indicator -> Explanation Generator -> Model Output. The OOD detector provides context filtering, the context indicator constrains the explanation space, and the explanation generator produces valid explanations for in-distribution inputs.

**Critical path**: OOD detection (bottleneck) → Context filtering → Explanation generation. The computational complexity is dominated by OOD detection since interpretation inherits this hardness.

**Design tradeoffs**: Perfect OOD detection vs. explanation tractability (trade perfect accuracy for computational feasibility), Self-alignment vs. model expressiveness (choose models that are self-aligned but may have limited capacity), Distribution specificity vs. generalization (tighter distribution constraints make explanations harder).

**Failure signatures**: When OOD detector is imperfect, explanations may be invalid or misleading; when model is not self-aligned, distribution-aware interpretation becomes computationally intractable; when context constraints are too tight, no valid explanations exist.

**First experiments**:
1. Prove that a given model class is not self-aligned by constructing a distribution shift where explanations fail
2. Demonstrate computational hardness by implementing an exponential-time algorithm for distribution-aware explanation generation
3. Show that removing the context indicator reduces the problem to standard (tractable) explanation generation

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Analysis is primarily theoretical with no empirical validation on real datasets
- Assumes access to a perfect OOD detector, which is rarely achievable in practice
- Self-alignment characterizations may not translate to complex, continuous distribution shifts in real-world scenarios
- Does not address approximate or heuristic methods for distribution-aware interpretation

## Confidence
- High confidence in theoretical complexity results and unification framework
- Medium confidence in self-alignment characterizations for specific model classes
- Low confidence in direct practical implications without empirical validation

## Next Checks
1. Empirical validation on real datasets showing how theoretical complexity bounds manifest in practice when generating distribution-aligned explanations
2. Analysis of approximate OOD detectors and how their imperfections affect complexity of interpretation tasks
3. Extension of self-alignment framework to more complex model classes beyond decision trees and neural networks, particularly ensemble methods and transformer-based architectures