---
ver: rpa2
title: Explaining Deep Neural Networks by Leveraging Intrinsic Methods
arxiv_id: '2407.12243'
source_url: https://arxiv.org/abs/2407.12243
tags:
- memory
- explanations
- neural
- networks
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis advances explainable deep learning by developing intrinsic
  techniques to enhance the interpretability of deep neural networks. The core contributions
  include: (1) Prototype-based Interpretable Graph Neural Networks (PIGNN) for graph
  data, preserving black-box performance while improving interpretability through
  prototype-based explanations; (2) Self-Explainable Memory-Augmented Deep Neural
  Networks that augment black-box models with memory modules to provide various types
  of explanations (feature attribution, examples, counterfactuals) while maintaining
  or improving performance; (3) Graph Concept Whitening (GCW), which aligns latent
  representations with molecular properties in the chemical domain, improving disentanglement
  and interpretability; (4) Clustered Compositional Explanations, a post-training
  algorithm that analyzes neuron activations across multiple ranges to extract richer
  explanations and discover novel activation phenomena; and (5) a comprehensive analysis
  of visual analytics systems for explaining deep learning, identifying strengths,
  weaknesses, and opportunities for better integration between XAI and VA fields.'
---

# Explaining Deep Neural Networks by Leveraging Intrinsic Methods

## Quick Facts
- arXiv ID: 2407.12243
- Source URL: https://arxiv.org/abs/2407.12243
- Authors: Biagio La Rosa
- Reference count: 0
- This thesis advances explainable deep learning by developing intrinsic techniques to enhance interpretability of deep neural networks.

## Executive Summary
This thesis addresses the critical need for explainable AI by developing intrinsic methods that leverage deep neural networks' inner workings to provide interpretable explanations while preserving or improving performance. The research introduces five major contributions: prototype-based interpretable graph neural networks (PIGNN), memory-augmented self-explainable DNNs, Graph Concept Whitening for molecular property alignment, Clustered Compositional Explanations for analyzing neuron activations, and a comprehensive analysis of visual analytics systems. These methods demonstrate that intrinsic approaches can provide explanations as good as or better than extrinsic methods across multiple domains including graphs, images, and text.

## Method Summary
The thesis develops intrinsic XAI methods that enhance DNN interpretability by inserting novel components after feature extractors or modifying existing layers. Key approaches include prototype-based layers that compute similarity scores between inputs and learned prototypes, memory modules with sparse content-based attention for example-based explanations, Graph Concept Whitening that aligns latent representations with molecular properties through normalization layer modification, and post-training analysis algorithms that cluster neuron activations across multiple ranges. The methods are validated across graph neural networks, computer vision models, and sequential models, demonstrating consistent improvements in interpretability metrics while maintaining or improving task performance.

## Key Results
- Prototype-based Interpretable Graph Neural Networks (PIGNN) preserve black-box performance while improving interpretability through prototype-based explanations
- Memory-augmented networks provide explanations by examples and counterfactuals while maintaining or improving model performance
- Graph Concept Whitening improves disentanglement and interpretability of molecular property representations in chemical domain
- Clustered Compositional Explanations algorithm extracts richer explanations and discovers novel activation phenomena through multi-range neuron analysis
- Comprehensive analysis of visual analytics systems identifies opportunities for better integration between XAI and VA fields

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The thesis proposes intrinsic methods that leverage DNN inner workings for explaining DL, rather than relying solely on external approximation methods.
- **Mechanism**: The methods enhance interpretability by inserting novel components into DNNs without disrupting their structure, preserving most of their representation power. Specifically, they place novel layers after the feature extractor or modify normalization layers to improve interpretability of learned knowledge.
- **Core assumption**: Intrinsic methods can achieve explanations as good as or better than extrinsic methods while preserving or improving model performance, and that leveraging inner workings directly is more faithful to model behavior than external approximations.
- **Evidence anchors**:
  - [abstract]: "This thesis addresses this issue by contributing to the field of eXplainable AI, focusing on enhancing the interpretability of deep neural networks. The core contributions lie in introducing novel techniques aimed at making these networks more interpretable by leveraging an analysis of their inner workings."
  - [section]: "The core contributions lie in introducing novel techniques aimed at making these networks more interpretable by leveraging an analysis of their inner workings. Specifically, the contributions are threefold."
  - [corpus]: **Weak evidence** - The corpus contains papers about intrinsic methods like "Self-Explaining Hypergraph Neural Networks" and "Self-Explaining Reinforcement Learning" but none directly discuss the specific thesis contributions or the comparative advantage of intrinsic over extrinsic methods.
- **Break condition**: If the intrinsic modifications significantly degrade model performance or if the explanations fail to capture the true decision process of the DNN, the core assumption would be invalidated.

### Mechanism 2
- **Claim**: Prototype-based layers for GNNs can preserve black-box performance while improving interpretability through explanations by examples and feature attributions.
- **Mechanism**: The PIGNN design introduces a prototype-based layer that learns node embeddings representing prototypes. It computes similarity scores between input nodes and prototypes, using these scores for classification. The prototypes are projected to real training points during training to maintain interpretability.
- **Core assumption**: Prototypes can effectively capture the semantics of node embeddings in graphs, and the similarity-based classification can maintain or improve performance compared to standard GNN approaches.
- **Evidence anchors**:
  - [abstract]: "Specifically, the contributions are threefold. Firstly, the thesis introduces designs for self-explanatory deep neural networks, such as the integration of external memory for interpretability purposes and the usage of prototype and constraint-based layers across several domains."
  - [section]: "As a first step, this chapter proposes a self-explainable prototype-based layer for Graph Neural Networks (GNNs) inspired by the design of prototype-based architectures [33] commonly used for image classification."
  - [corpus]: **Missing evidence** - The corpus does not contain papers specifically about prototype-based GNNs or PIGNN, making it difficult to verify this mechanism against related work.
- **Break condition**: If the projection phase fails to maintain prototype interpretability or if the similarity computation cannot effectively capture relevant graph patterns, the mechanism would break down.

### Mechanism 3
- **Claim**: Memory modules can enhance interpretability of existing neural networks while preserving their performance by providing explanations by examples and counterfactuals.
- **Mechanism**: Memory Wrap and SDNC designs augment black-box DNNs with memory modules. They use sparse content-based attention to select similar samples from memory, providing explanations by examples (samples with same prediction) and counterfactuals (samples with different prediction). The memory tracking mechanism monitors reading/writing operations to derive explanations.
- **Core assumption**: The memory can effectively store and retrieve semantically meaningful samples that help explain predictions, and the sparse attention mechanism can distinguish between relevant and irrelevant samples.
- **Evidence anchors**:
  - [abstract]: "Secondly, this research delves into novel investigations on neurons within trained deep neural networks, shedding light on overlooked phenomena related to their activation values."
  - [section]: "Starting from this analysis, here we further develop the employment of memory modules for enhancing the interpretability of DNNs."
  - [corpus]: **Weak evidence** - The corpus contains papers about memory-augmented networks like "Self-Explaining Reinforcement Learning" but none specifically discuss memory modules for interpretability or the Memory Wrap/SDNC designs.
- **Break condition**: If the memory selection becomes too noisy to provide meaningful explanations or if the counterfactual identification fails to capture true decision boundaries, the mechanism would fail.

## Foundational Learning

- **Concept**: Graph Neural Networks (GNNs) and their message-passing paradigm
  - Why needed here: The thesis extensively works with GNNs in Chapters 4 and 6, requiring understanding of how GNNs process graph-structured data through node representations and message passing.
  - Quick check question: How does a GNN layer update a node's representation based on its neighbors?

- **Concept**: Prototype-based learning and its application to neural networks
  - Why needed here: Chapter 4 introduces prototype-based layers for GNNs, building on prototype-based architectures from computer vision. Understanding how prototypes capture semantic meaning is crucial.
  - Quick check question: What is the difference between computing prototypes as averages of embeddings versus as real training samples?

- **Concept**: Memory-augmented neural networks (MANNs) and their components
  - Why needed here: Chapters 5 introduces memory-based self-explainable DNNs, requiring understanding of controller, memory, read/write heads, and how they interact in MANNs.
  - Quick check question: How does dynamic memory allocation in DNCs differ from the memory operations in Memory Wrap?

## Architecture Onboarding

- **Component map**: Feature extractor → Prototype layer → Classifier (PIGNN); Feature extractor → Memory module → Classifier (Memory Wrap/SDNC); BatchNorm layers → GCW layers → Classifier (GCW)
- **Critical path**: For prototype-based: Feature extractor → Prototype layer (similarity computation, max pooling) → Classifier. For memory-based: Feature extractor → Memory module (content addressing, sparsemax) → Classifier. For GCW: BatchNorm layers → GCW layers (whitening, rotation) → Classifier.
- **Design tradeoffs**: Prototype layers require custom training recipes and careful prototype number selection. Memory modules add computational overhead and memory footprint. GCW requires concept datasets and alignment process. All approaches must balance interpretability gains against performance preservation.
- **Failure signatures**: Prototype duplication despite orthogonality loss, memory overwriting due to insufficient size, concept leakage in GCW, or clustering algorithm failures in Clustered Compositional Explanations.
- **First 3 experiments**:
  1. Implement PIGNN on BA-shapes or BBBP datasets using GCN/GAT/GIN backbones with 10 prototypes per class and joint training.
  2. Add Memory Wrap to a pre-trained CNN and test explanation quality on SVHN using input non-representativeness metrics.
  3. Apply GCW to a GAT model on BBBP dataset by replacing batch normalization layers and fine-tuning with concept alignment loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design a unified framework for self-explainable deep neural networks that can generate different types of explanations (feature attribution, examples, counterfactuals) across multiple domains and problem types?
- Basis in paper: [explicit] The thesis discusses the need for a unified framework in the long-term vision section (9.3), noting that such a design could have an impact similar to Transformers for the deep learning field.
- Why unresolved: While the thesis presents several approaches for different types of explanations and domains, it does not propose a single, unified framework that can handle all these cases.
- What evidence would resolve it: A proposed unified framework architecture with demonstrated effectiveness across multiple domains (vision, text, graphs) and problem types (classification, regression, reinforcement learning), showing comparable or better performance and interpretability than current specialized approaches.

### Open Question 2
- Question: How can we systematically evaluate and compare different intrinsic XAI methods across multiple interpretability metrics and real-world applications?
- Basis in paper: [implicit] The thesis presents various intrinsic methods but lacks a comprehensive evaluation framework that could be applied consistently across different approaches and domains.
- Why unresolved: Current evaluation relies on domain-specific metrics and case studies rather than a standardized framework for comparing intrinsic XAI methods.
- What evidence would resolve it: Development of a standardized evaluation protocol with benchmark datasets, multiple interpretability metrics, and systematic comparison methodology that can fairly assess different intrinsic XAI approaches.

## Limitations

- The evaluation focuses primarily on graph and image data, with limited testing on sequential or tabular data types
- Many mechanisms rely on specialized training procedures and architecture modifications that may not generalize across all DNN types
- The interpretability-accuracy tradeoff analysis is primarily empirical rather than theoretically grounded

## Confidence

- **High confidence**: The prototype-based GNN approach (PIGNN) and memory-augmented methods (Memory Wrap, SDNC) are well-validated with multiple datasets and consistent performance improvements
- **Medium confidence**: The Graph Concept Whitening technique shows promise but requires careful hyperparameter tuning and may not generalize to all molecular properties or domains
- **Medium confidence**: The Clustered Compositional Explanations algorithm provides novel insights but depends heavily on clustering quality and heuristic parameters that may not transfer well across datasets

## Next Checks

1. Implement ablation studies on PIGNN to quantify the contribution of prototype interpretability versus black-box performance, testing on at least three different GNN backbones (GCN, GAT, GIN) and two graph datasets
2. Conduct systematic hyperparameter sensitivity analysis for GCW across different concept datasets, measuring the impact of concept number, alignment iterations, and learning rates on both interpretability metrics and downstream task performance
3. Validate the Clustered Compositional Explanations approach on a pre-trained BERT model, testing whether the discovered activation patterns generalize across multiple text classification tasks and comparing against established post-hoc explanation methods