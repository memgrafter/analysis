---
ver: rpa2
title: Efficient In-Domain Question Answering for Resource-Constrained Environments
arxiv_id: '2409.17648'
source_url: https://arxiv.org/abs/2409.17648
tags:
- fine
- tuning
- raft
- lora
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CRAFT, a method that combines Retrieval Augmented
  Fine Tuning (RAFT) with Low-Rank Adaptation (LoRA) to create a more efficient approach
  for in-domain question answering in resource-constrained environments. The method
  addresses the challenge of deploying high-performance QA models in settings with
  limited computational resources and restricted internet access.
---

# Efficient In-Domain Question Answering for Resource-Constrained Environments

## Quick Facts
- arXiv ID: 2409.17648
- Source URL: https://arxiv.org/abs/2409.17648
- Reference count: 5
- Achieves 13.41% average F1 score improvement over baseline RAG models

## Executive Summary
This paper presents CRAFT (CoT-enhanced RAFT), a method for efficient in-domain question answering in resource-constrained environments. The approach combines Retrieval Augmented Fine Tuning (RAFT) with Low-Rank Adaptation (LoRA) to create a parameter-efficient QA system. CRAFT fine-tunes a 7-8B parameter LLM using LoRA on data generated via RAFT, which involves creating question-answer pairs from target domain documents using a large LLM. The method achieves significant performance improvements over baseline RAG models while reducing training requirements and computational resources.

## Method Summary
CRAFT fine-tunes a smaller LLM (7-8B parameters) on domain-specific QA pairs generated by a larger LLM (70B parameters) using the RAFT approach, combined with LoRA parameter-efficient fine-tuning. The method involves chunking target domain documents, generating question-answer pairs using the large LLM, and training LoRA adapters on this generated data. The approach maintains high performance while requiring only 2% of trainable parameters compared to full fine-tuning, reducing GPU memory usage by nearly 35%, and achieving 7.5x faster training.

## Key Results
- Achieves average F1 score improvement of 13.41% over baseline RAG models
- Reduces trainable parameters to 2% of full fine-tuning requirements
- Cuts GPU memory usage by nearly 35% and achieves 7.5x training speedup

## Why This Works (Mechanism)

### Mechanism 1
CRAFT achieves significant performance gains by fine-tuning a smaller LLM on domain-specific QA pairs generated via a larger LLM, combined with LoRA parameter efficiency. The large LLM generates question-answer pairs from domain documents (RAFT approach), which are then used to fine-tune a smaller 7-8B parameter LLM using LoRA. This allows the smaller model to learn domain-specific knowledge while maintaining computational efficiency. Core assumption: The generated question-answer pairs accurately represent the domain knowledge needed for effective QA. Evidence anchors: RAFT applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5; RAFT is a technique that combines fine tuning with information retrieval, allowing the LLM to more effectively answer questions using relevant content from the retrieved data.

### Mechanism 2
LoRA adapters provide a parameter-efficient alternative to full fine-tuning, reducing training requirements while maintaining performance. LoRA introduces low-rank matrices that approximate weight updates, requiring only a small fraction of parameters to be trained. This significantly reduces GPU memory usage and training time compared to full fine-tuning. Core assumption: The low-rank approximation is sufficient to capture the necessary model adaptations for domain-specific QA. Evidence anchors: LoRA consists of a set of pairs of low-rank matrices; using CRAFT reduces the number of trainable parameters to just 2% of those required by SFT.

### Mechanism 3
The combination of RAFT and LoRA creates a synergistic effect that improves both performance and efficiency in resource-constrained environments. RAFT provides the domain adaptation through generated training data, while LoRA ensures this adaptation happens efficiently. The resulting model is both performant and suitable for deployment in environments with limited computational resources. Core assumption: The efficiency gains from LoRA do not compromise the effectiveness of the RAFT-based domain adaptation. Evidence anchors: The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution; By combining RAFT and LoRA methods, we aim to create a framework that addresses the specific needs of resource-constrained environments without compromising on the quality of the QA system.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Understanding how RAG works is fundamental to grasping why RAFT and CRAFT improve upon it
  - Quick check question: What is the main limitation of standard RAG that RAFT attempts to address?

- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: LoRA is a PEFT technique, and understanding PEFT is crucial for understanding how CRAFT achieves its efficiency gains
  - Quick check question: How does LoRA differ from traditional fine-tuning in terms of parameter updates?

- Concept: Low-Rank Approximation
  - Why needed here: LoRA relies on low-rank matrix decomposition to achieve parameter efficiency
  - Quick check question: What is the mathematical basis for why low-rank approximations can effectively approximate weight updates?

## Architecture Onboarding

- Component map: Large LLM (70B) -> Chunking mechanism -> LoRA adapter training system -> Smaller 7-8B LLM -> RAG pipeline

- Critical path: Chunk domain documents into manageable pieces -> Use large LLM to generate QA pairs for each chunk -> Train LoRA adapter on generated data -> Deploy adapted model in RAG pipeline

- Design tradeoffs: Model size vs. performance (larger models generally perform better but are less suitable for resource-constrained environments); Rank of LoRA adapters (higher ranks may capture more complex adaptations but require more parameters); Amount of generated training data (more data may improve performance but increases computational requirements)

- Failure signatures: Poor QA performance despite successful training (may indicate issues with quality of generated training data); Excessive memory usage during training (could suggest suboptimal LoRA configuration); Slow inference times (might indicate inefficient RAG pipeline or suboptimal model architecture)

- First 3 experiments: Train CRAFT on a single dataset (e.g., HotPotQA) and compare performance against baseline RAG model; Vary the rank of LoRA adapters to find optimal balance between performance and efficiency; Test adapter swapping capability by training multiple adapters on different datasets and measuring swap time and performance impact

## Open Questions the Paper Calls Out
- How does CRAFT perform when deployed with quantized versions of the large LLM used for generating questions and CoT answers, compared to the full-precision version?
- How does the performance of CRAFT scale with different values of the LoRA rank parameter (r) beyond those tested in the paper?
- How does CRAFT compare to other parameter-efficient fine-tuning (PEFT) techniques, such as AdapterDrop or BitFit, in terms of performance and resource efficiency?

## Limitations
- Evaluation limited to English language datasets with no assessment of multilingual capabilities
- Focus exclusively on open-book QA tasks, leaving other question-answering formats unexplored
- Does not address computational requirements for initial RAFT data generation phase

## Confidence
**High Confidence:** LoRA reduces trainable parameters to 2% compared to full fine-tuning; CRAFT achieves average 13.41% F1 improvement over baseline RAG; GPU memory reduction of nearly 35% and 7.5x training speedup are achievable; The framework is functional and demonstrates measurable performance gains.

**Medium Confidence:** LoRA adapters with ranks [4, 16, 64] provide optimal trade-offs; Adapter swapping functionality works as described; The combination of RAFT and LoRA creates synergistic benefits; The approach generalizes across diverse QA datasets.

**Low Confidence:** CRAFT is the optimal solution for all resource-constrained QA scenarios; The specific parameter choices (rank values, distractor counts) are universally optimal; The 7-8B model size represents the ideal balance point for all use cases.

## Next Checks
1. Measure total computational resources required for RAFT data generation phase to determine true feasibility in resource-constrained environments
2. Evaluate CRAFT's performance across broader range of question types and domains, including adversarial questions and multi-hop reasoning
3. Test adapter swapping capability across significantly different domains to measure knowledge transfer and potential catastrophic forgetting