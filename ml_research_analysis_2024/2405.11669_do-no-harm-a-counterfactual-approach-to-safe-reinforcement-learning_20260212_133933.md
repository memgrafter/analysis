---
ver: rpa2
title: 'Do No Harm: A Counterfactual Approach to Safe Reinforcement Learning'
arxiv_id: '2405.11669'
source_url: https://arxiv.org/abs/2405.11669
tags:
- harm
- constraint
- policy
- learning
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a counterfactual approach to safe reinforcement
  learning that formulates constraints on counterfactual harm, addressing challenges
  in penalizing agents for inevitable constraint violations. The key idea is to compare
  the outcome of a learned policy to a default safe policy using counterfactual inference,
  penalizing the agent only for constraint violations it causes.
---

# Do No Harm: A Counterfactual Approach to Safe Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.11669
- Source URL: https://arxiv.org/abs/2405.11669
- Authors: Sean Vaskov; Wilko Schwarting; Chris L. Baker
- Reference count: 28
- Key outcome: Counterfactual harm constraints enable safer and more performant RL policies by penalizing only agent-caused constraint violations

## Executive Summary
This paper proposes a counterfactual approach to safe reinforcement learning that addresses the challenge of penalizing agents for inevitable constraint violations. The key innovation is formulating constraints on counterfactual harm, which compares the outcome of a learned policy to a default safe policy using counterfactual inference. By penalizing the agent only for constraint violations it causes, the approach maintains feasibility while learning safer policies. Experiments in rover navigation and tractor-trailer parking environments demonstrate that the counterfactual harm constraint enables agents to learn safer and more performant policies than alternative formulations.

## Method Summary
The method formulates safe RL as a constrained MDP where constraints are defined on counterfactual harm - the difference between the constraint violation under the learned policy and what would have occurred under a default safe policy. The approach uses an actor-critic framework with separate critics for reward, default constraint, learner constraint, and counterfactual harm. At each state, counterfactual inference estimates the outcome of the default policy to compute the harm constraint. The actor maximizes the advantage of the unconstrained objective while satisfying the counterfactual harm constraint using Lagrange multipliers.

## Key Results
- Counterfactual harm constraint enables agents to learn safer policies with higher success rates and lower constraint violation probabilities
- Performance improves over baseline methods (DBS, IC, MC, CC, CCATE) in both rover and tractor-trailer environments
- Additional computation for counterfactual inference is modest in these experimental settings
- Counterfactual harm is more robust to dynamics uncertainty than CCATE by comparing infinite horizon outcomes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Counterfactual harm constraints ensure agents are only penalized for violations they actually cause, not for inevitable violations.
- **Mechanism:** By comparing the outcome of a learned policy to a default safe policy using counterfactual inference, the agent is only penalized when its actions increase the probability or severity of constraint violation beyond what would have occurred with the default policy.
- **Core assumption:** A default safe policy exists that can satisfy constraints or minimize violation severity when the agent exits the viability kernel.
- **Evidence anchors:**
  - [abstract] "penalizing the agent only for constraint violations it causes"
  - [section] "only penalizes the learner for constraint violations that it caused; in a practical sense it maintains feasibility of the optimal control problem"
- **Break condition:** If the default policy is not sufficiently safe or cannot minimize violation severity when the agent exits the viability kernel.

### Mechanism 2
- **Claim:** The clipped conditional average treatment effect (CCATE) reduces conservatism while preserving safety by enforcing the learner policy to be safer than the default policy in expectation at each state.
- **Mechanism:** CCATE compares the expected constraint violation of the learner policy to the clipped expected violation of the default policy at each state, ensuring the learner only needs to violate constraints less than the default policy.
- **Core assumption:** The default policy's expected constraint violation can be precomputed and approximated.
- **Evidence anchors:**
  - [section] "enforces the learner policy to be safer than the default policy in expectation"
  - [section] "The clipping ensures that the learner policy is not required to satisfy the constraints more than the default policy, only to violate them less"
- **Break condition:** If the precomputation of the default policy's expected constraint violation is inaccurate or the approximation is poor.

### Mechanism 3
- **Claim:** The counterfactual harm constraint is more robust to dynamics uncertainty than CCATE because it compares outcomes over an infinite time horizon and conditions on the observed trajectory.
- **Mechanism:** Counterfactual harm uses the exogenous variables conditioned on the observed trajectory and estimates the infinite time horizon outcome using a learned approximation of the default policy's constraint value function and TD(λ) methods.
- **Core assumption:** The exogenous variables can be modeled and their distribution conditioned on the observed trajectory.
- **Evidence anchors:**
  - [section] "there is an additional clipping inside the expectation, which enforces the constraint for all realizations of uncertainty"
  - [section] "Defining pξ is an active area of research"
- **Break condition:** If the exogenous variables cannot be accurately modeled or their distribution conditioned on the observed trajectory.

## Foundational Learning

- **Concept:** Markov Decision Processes (MDPs) and Constrained MDPs (CMDPs)
  - **Why needed here:** The safe RL problem is formulated as a CMDP with constraints on constraint violation.
  - **Quick check question:** What is the difference between an MDP and a CMDP?

- **Concept:** Counterfactual inference and causal models
  - **Why needed here:** The counterfactual harm constraint uses counterfactual inference to compare the outcome of the learned policy to a default policy.
  - **Quick check question:** What are the three steps of counterfactual inference?

- **Concept:** Viability theory and the viability kernel
  - **Why needed here:** The viability kernel defines the set of initial states from which the constraints can be satisfied, and the default policy can be used to minimize violation severity when the agent exits the viability kernel.
  - **Quick check question:** How is the viability kernel defined?

## Architecture Onboarding

- **Component map:** Actor -> Reward critic, Default constraint critic, Learner constraint critic, Counterfactual harm critic -> Counterfactual inference module
- **Critical path:** The counterfactual inference step, performed at each state to estimate the counterfactual outcome of the default policy and compute the counterfactual harm
- **Design tradeoffs:** Safety versus performance - counterfactual harm ensures safety by only penalizing agent-caused violations but requires additional computation for counterfactual inference
- **Failure signatures:** Agent still violates constraints when initialized within viability kernel, or constraint violations are severe
- **First 3 experiments:**
  1. Implement counterfactual harm constraint in simple grid world with default safe policy moving agent away from obstacles
  2. Compare counterfactual harm to CCATE constraint performance in grid world environment
  3. Test counterfactual harm in rover environment with uncertain road friction versus baseline methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the counterfactual harm constraint change when applied to environments with stochastic dynamics that are not fully observable or when the exogenous variables are not directly observable?
- Basis in paper: [inferred] The paper discusses application in environments where simulation model is accessible and exogenous variables are observable, mentioning defining exogenous variable distribution is active research.
- Why unresolved: Paper lacks experimental results or theoretical analysis for environments with unobservable exogenous variables or non-fully observable stochastic dynamics.
- What evidence would resolve it: Experimental results comparing performance in environments with fully observable versus unobservable exogenous variables, along with theoretical analysis of observability impact.

### Open Question 2
- Question: What is the impact of using a learned default policy versus a hand-designed default policy on the performance of the counterfactual harm constraint?
- Basis in paper: [explicit] Paper mentions default policy can be hand-designed, computed using reachability analysis, learned with behavior cloning, or combination thereof, and future work will explore learned default policies.
- Why unresolved: Paper does not provide experimental results comparing hand-designed versus learned default policies.
- What evidence would resolve it: Experimental results comparing performance when using hand-designed and learned default policies across various environments.

### Open Question 3
- Question: How does the counterfactual harm constraint perform in multi-agent environments, and what modifications are necessary to adapt it to such settings?
- Basis in paper: [explicit] Paper mentions experiments conducted in single-agent environments and future work will study value in multi-agent settings.
- Why unresolved: Paper lacks experimental results or theoretical analysis of counterfactual harm constraint in multi-agent environments.
- What evidence would resolve it: Experimental results demonstrating performance in multi-agent environments, along with theoretical analysis of necessary modifications.

## Limitations

- Default safe policy requirement may be difficult to establish in complex environments
- Accurate counterfactual inference depends on modeling exogenous variables, an active research challenge
- Additional computation for counterfactual inference may become prohibitive in high-dimensional state spaces

## Confidence

- **High Confidence:** Core mechanism of using counterfactual inference to isolate agent-caused harm from inevitable violations is well-grounded
- **Medium Confidence:** Empirical results showing improved safety and performance are promising but limited to two simple environments
- **Medium Confidence:** Claim that counterfactual harm is more robust to dynamics uncertainty than CCATE is supported by theoretical arguments but needs empirical validation

## Next Checks

1. Test counterfactual harm constraint in environments with higher-dimensional state spaces and more complex dynamics to evaluate scalability
2. Evaluate approach when default safe policy is suboptimal or fails to minimize constraint violation severity
3. Compare performance and safety of counterfactual harm to other safe RL methods in environments with varying levels of uncertainty