---
ver: rpa2
title: 'QueryBuilder: Human-in-the-Loop Query Development for Information Retrieval'
arxiv_id: '2409.04667'
source_url: https://arxiv.org/abs/2409.04667
tags:
- query
- user
- system
- information
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QueryBuilder is an interactive system that enables novice users
  to develop fine-grained cross-lingual queries with minimal effort. The system combines
  a fast probabilistic IR model with a BERT-based neural IR model, allowing users
  to iteratively refine queries through relevance feedback.
---

# QueryBuilder: Human-in-the-Loop Query Development for Information Retrieval

## Quick Facts
- arXiv ID: 2409.04667
- Source URL: https://arxiv.org/abs/2409.04667
- Reference count: 14
- Key outcome: Novice users can develop fine-grained cross-lingual queries in under 10 minutes per sub-topic, achieving 12% better nDCG performance than using only the overarching task

## Executive Summary
QueryBuilder is an interactive system designed to help novice users develop effective cross-lingual queries through a human-in-the-loop approach. The system combines a fast probabilistic IR model for initial exploration with a BERT-based neural IR model for semantic enrichment, allowing users to iteratively refine queries through relevance feedback. By marking relevant sentences from retrieved documents, users can expand and refine their queries with weighted terms and event features. Experiments on IARPA BETTER IR datasets demonstrate that QueryBuilder enables novice users to create useful queries quickly, improving CLIR performance across multiple languages.

## Method Summary
QueryBuilder employs a two-stage query development process. First, users enter search terms to explore an English corpus using a fast probabilistic IR model that combines lexical matching with event features. The system retrieves relevant sentences, which users review and mark as relevant to their information needs. These marked sentences are then used to refine the query by weighting terms and optionally incorporating event-related features. In the second stage, a BERT-based neural IR model retrieves semantically similar sentences to further enrich the query. Users can iterate this process up to 6 times per query, progressively building a more comprehensive and targeted query for cross-lingual information retrieval.

## Key Results
- Novice users can develop fine-grained queries in under 10 minutes per sub-topic
- Achieved 12% better nDCG performance compared to using only the overarching task
- Supports query combination to improve CLIR performance across multiple languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining fast probabilistic IR with BERT-based neural IR improves query quality by leveraging both lexical matching and semantic meaning.
- Mechanism: Initial exploration uses probabilistic IR for rapid retrieval; subsequent enrichment uses neural IR to find semantically similar sentences.
- Core assumption: Lexical and semantic matching are complementary and can be effectively combined in sequence.
- Evidence anchors:
  - [abstract] "QueryBuilder performs near real-time retrieval of documents based on user-entered search terms; the user looks through the retrieved documents and marks sentences as relevant to the information needed. The marked sentences are used by the system as additional information in query formation and refinement: query terms (and, optionally, event features, which capture event 'triggers' (indicator terms) and agent/patient roles) are appropriately weighted, and a neural-based system, which better captures textual meaning, retrieves other relevant content."
  - [section] "For the initial query creation step, we use a fast probabilistic IR model that is mostly lexical. In addition to lexical information, the probabilistic IR system can also use neural network lexical translation and word embeddings, and can also leverage event extraction features... For the subsequent query enrichment step, we instead use a BERT-based neural IR model that works in a 'query by example' setting, thanks to its ability to capture high-level semantic features."

### Mechanism 2
- Claim: Relevance feedback at the sentence level is more efficient than document-level feedback while providing richer information than sub-sentence feedback.
- Mechanism: Users mark sentences as relevant, which are then used to expand and refine the query with weighted terms and event features.
- Core assumption: Sentence-level relevance judgments provide a good balance between information richness and annotation effort.
- Evidence anchors:
  - [section] "In terms of finding relevant snippets, we focus on the sentence level, which is more cost-effective than doing relevance feedback at the document level, while providing more complete information than the sub-sentence level."
  - [section] "QueryBuilder performs near real-time retrieval of documents based on user-entered search terms; the user looks through the retrieved documents and marks sentences as relevant to the information needed."

### Mechanism 3
- Claim: Novice users can develop fine-grained queries in under 10 minutes per sub-topic, achieving better CLIR performance than using only the overarching task.
- Mechanism: Iterative query refinement through exploration and relevance feedback allows rapid development of detailed queries.
- Core assumption: Novice users can effectively use the system interface to refine queries without extensive training.
- Evidence anchors:
  - [abstract] "Experiments on IARPA BETTER IR datasets show that novice users can create useful queries in under 10 minutes per sub-topic, achieving 12% better nDCG performance than using only the overarching task."
  - [section] "The QueryBuilder system is highly effective: novice users are able to develop fine-grained queries in at most 10 minutes per query."

## Foundational Learning

- Concept: Information Retrieval (IR) basics
  - Why needed here: Understanding how IR systems work is fundamental to grasping how QueryBuilder improves query development.
  - Quick check question: What is the difference between lexical matching and semantic matching in IR?

- Concept: Relevance feedback
  - Why needed here: QueryBuilder relies on user feedback to refine queries iteratively.
  - Quick check question: How does relevance feedback improve the performance of an IR system?

- Concept: Cross-lingual Information Retrieval (CLIR)
  - Why needed here: QueryBuilder's end goal is to generate queries for CLIR systems.
  - Quick check question: What challenges arise in CLIR that do not exist in monolingual IR?

## Architecture Onboarding

- Component map:
  User Interface -> Probabilistic IR Engine -> BERT-based Neural IR Engine -> Query Refinement Logic -> CLIR Integration

- Critical path:
  1. User enters search terms
  2. Probabilistic IR retrieves sentences
  3. User marks relevant sentences
  4. Query is refined with weighted terms and event features
  5. Neural IR retrieves semantically similar sentences
  6. User continues marking and refining
  7. Final query is output for CLIR

- Design tradeoffs:
  - Speed vs. accuracy: Probabilistic IR is faster but less accurate than neural IR.
  - Granularity vs. effort: Sentence-level feedback is more granular but requires more user effort than document-level feedback.
  - Complexity vs. usability: Adding event features increases complexity but can improve precision.

- Failure signatures:
  - Probabilistic IR returns irrelevant sentences: Check term weighting and event feature extraction.
  - Neural IR fails to find semantically similar sentences: Verify model training and sentence embeddings.
  - Users cannot refine queries effectively: Review UI design and feedback mechanisms.

- First 3 experiments:
  1. Test probabilistic IR retrieval with known relevant queries to ensure baseline functionality.
  2. Validate neural IR semantic similarity with manually labeled sentence pairs.
  3. Conduct a small user study to test the end-to-end query development process and gather feedback.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions, but several remain based on the content:

### Open Question 1
- Question: How does the performance of QueryBuilder scale with the size of the development corpus?
- Basis in paper: [inferred] The paper mentions using an English development corpus of about 750K news articles, but does not explore the impact of corpus size on performance.
- Why unresolved: The paper focuses on demonstrating the effectiveness of QueryBuilder with a specific corpus size, but does not investigate how performance changes with larger or smaller corpora.
- What evidence would resolve it: Experiments comparing QueryBuilder's performance on development corpora of varying sizes, showing how nDCG and query development time are affected.

### Open Question 2
- Question: How does QueryBuilder's performance compare to other human-in-the-loop query development systems?
- Basis in paper: [inferred] The paper presents QueryBuilder as a novel system but does not compare it to other similar systems in the literature.
- Why unresolved: While the paper demonstrates QueryBuilder's effectiveness, it does not provide a direct comparison to other human-in-the-loop query development approaches.
- What evidence would resolve it: A comparative study between QueryBuilder and other human-in-the-loop query development systems, evaluating their respective performance in terms of query quality and development time.

### Open Question 3
- Question: How does the choice of neural IR model (e.g., BERT vs. XLM-R) affect QueryBuilder's performance in cross-lingual information retrieval?
- Basis in paper: [explicit] The paper mentions using a BERT-based neural IR model but also references XLM-R as a cross-lingual alternative in the related work section.
- Why unresolved: The paper does not investigate the impact of different neural IR models on QueryBuilder's performance in cross-lingual scenarios.
- What evidence would resolve it: Experiments comparing QueryBuilder's performance using different neural IR models (e.g., BERT, XLM-R) in cross-lingual information retrieval tasks, showing how each model affects nDCG and query development time.

## Limitations
- The effectiveness metrics are based on a single dataset (IARPA BETTER) and limited to 8 overarching tasks, which may not generalize to other domains or languages.
- The user study involved only 5 participants, raising questions about statistical significance and potential bias in the results.
- The paper lacks detailed implementation specifics for both the probabilistic IR system and the BERT-based neural IR model, making exact reproduction challenging.

## Confidence
- Claim about 12% nDCG improvement: Medium - Results are from a single dataset with limited participants
- Claim about novice users creating queries in under 10 minutes: Medium - Based on small user study without detailed timing methodology
- Claim about combining lexical and semantic matching improving results: High - Well-supported by established IR principles and the described two-stage architecture

## Next Checks
1. Test QueryBuilder on a different CLIR dataset (e.g., CLEF collections) to verify generalizability beyond the IARPA BETTER corpus
2. Conduct a larger user study (15+ participants) with randomized task assignment to assess statistical significance of the 10-minute query development claim
3. Implement ablation studies removing either the probabilistic or neural IR component to quantify the contribution of each stage to overall performance