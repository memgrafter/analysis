---
ver: rpa2
title: Cayley Graph Propagation
arxiv_id: '2410.03424'
source_url: https://arxiv.org/abs/2410.03424
tags:
- graph
- cayley
- graphs
- nodes
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses over-squashing in graph neural networks by
  proposing a novel approach that leverages complete Cayley graph structures to improve
  information flow. While prior work like Expander Graph Propagation (EGP) truncates
  Cayley graphs to match input graphs, this truncation can harm the theoretical expansion
  properties.
---

# Cayley Graph Propagation

## Quick Facts
- arXiv ID: 2410.03424
- Source URL: https://arxiv.org/abs/2410.03424
- Authors: JJ Wilson; Maya Bechler-Speicher; Petar Veličković
- Reference count: 40
- One-line primary result: CGP consistently outperforms EGP and other graph rewiring techniques on multiple datasets while maintaining efficiency

## Executive Summary
This paper addresses over-squashing in graph neural networks by proposing Cayley Graph Propagation (CGP), which leverages complete Cayley graph structures to improve information flow. Unlike Expander Graph Propagation (EGP) that truncates Cayley graphs to match input graphs, CGP preserves the full structure and uses virtual nodes to enhance connectivity. The approach demonstrates consistent performance improvements across multiple datasets including OGB and TUDataset, while maintaining computational efficiency compared to more intensive methods.

## Method Summary
CGP is a graph neural network approach that alternates between propagating over the input graph and a complete Cayley graph structure derived from the special linear group SL(2, Z_n). The method constructs the smallest Cayley graph with at least as many nodes as the input graph, then adds virtual nodes to bridge connectivity gaps. During message passing, odd layers operate on the input graph while even layers operate on the Cayley graph, with only the original nodes retained for final predictions. This preserves both local topology and global expansion properties while avoiding the bottlenecks created by truncation in previous approaches.

## Key Results
- CGP consistently outperforms EGP and other graph rewiring techniques across OGB, TUDataset, and LRGB benchmarks
- The method shows competitive performance against computationally intensive approaches while maintaining better efficiency
- CGP demonstrates robustness to overfitting and oversmoothing compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CGP uses the complete Cayley graph structure to eliminate bottlenecks that arise from truncation in EGP.
- Mechanism: The complete Cayley graph is constructed from the special linear group SL(2, Z_n), yielding a graph with optimal spectral gap, low diameter, and linear commute time. This ensures efficient information propagation without bottlenecks. By avoiding truncation, CGP retains all nodes and leverages virtual nodes to bridge poorly connected regions.
- Core assumption: The expansion properties of the complete Cayley graph are preserved when no truncation is applied, and the virtual nodes act as effective shortcuts.
- Evidence anchors:
  - [abstract]: "CGP, a method to propagate information over a complete Cayley graph structure, thereby ensuring it is bottleneck-free to better alleviate over-squashing."
  - [section]: "We show that truncation is detrimental to the coveted expansion properties. Instead, we propose CGP, a method to propagate information over a complete Cayley graph structure..."
  - [corpus]: Weak—no direct mentions of Cayley graphs in corpus, but related works on expander graphs and temporal graph rewiring provide indirect support.
- Break condition: If the Cayley graph construction fails to maintain expansion properties (e.g., due to poor choice of generating set), or if virtual nodes introduce noise instead of connectivity.

### Mechanism 2
- Claim: CGP maintains message-passing expressivity by alternating between the input graph and the Cayley graph, preserving both local and global information flow.
- Mechanism: Odd layers process the input graph to retain local topology, while even layers process the complete Cayley graph to enable global, bottleneck-free communication. This interleaving mirrors the JK network principle, improving structure-aware representations.
- Core assumption: The alternating scheme effectively combines local inductive bias with global expansion benefits without causing oversmoothing or over-squashing.
- Evidence anchors:
  - [abstract]: "CGP modifies EGP by avoiding the truncation step... utilising the additional nodes as virtual nodes."
  - [section]: "CGP now proceeds in the same manner as EGP: alternating GNN layers, such that every odd layer operates over the input graph... and every even layer operates over the generated Cayley graph..."
  - [corpus]: Weak—related work mentions expander graph propagation and message passing, but no direct evidence for alternating layer benefits in Cayley graphs.
- Break condition: If alternating layers cause instability in training or if the Cayley graph layers dominate and wash out local structure.

### Mechanism 3
- Claim: Virtual nodes in CGP are initialised in a way that supports learning without introducing harmful inductive bias.
- Mechanism: Virtual nodes are added to the feature matrix and initialised (e.g., to zeros or random values). These nodes are sparsely connected, minimizing computational overhead while acting as bridges. Their embeddings evolve during training to capture useful global context.
- Core assumption: Sparse connectivity of virtual nodes keeps computational complexity low, and their learned embeddings contribute positively to the task without overfitting.
- Evidence anchors:
  - [abstract]: "CGP modifies EGP by avoiding the truncation step... utilising the additional nodes as virtual nodes."
  - [section]: "To construct this, we featurise the first |V| nodes using the data from X, and treat any additional nodes as virtual nodes, initialised in some pre-defined way."
  - [corpus]: Weak—related works mention virtual nodes in expander graphs and GNNs, but no direct evidence for initialisation strategies in Cayley graphs.
- Break condition: If virtual nodes are initialised poorly (e.g., too noisy) or if their embeddings diverge and harm model performance.

## Foundational Learning

- Concept: Expander graphs and their spectral properties
  - Why needed here: CGP relies on the expander properties of Cayley graphs (high spectral gap, low diameter) to ensure bottleneck-free message propagation.
  - Quick check question: What is the Cheeger constant, and how does it relate to the spectral gap of a graph?
- Concept: Message passing neural networks and over-squashing
  - Why needed here: CGP is designed to mitigate over-squashing in MPNNs by improving long-range information flow via graph structure.
  - Quick check question: How does over-squashing manifest in a GNN, and what role does graph topology play?
- Concept: Graph rewiring and computational templates
  - Why needed here: CGP uses an independent graph (Cayley) as a computational template, unlike direct graph rewiring methods.
  - Quick check question: What is the difference between using an auxiliary graph for message passing versus rewiring the input graph?

## Architecture Onboarding

- Component map: Input graph -> Cayley graph -> Virtual nodes -> Alternating GNN layers -> Output embeddings
- Critical path:
  1. Precompute Cayley graph (smallest n such that |Cayley| >= |input|)
  2. Extend feature matrix with virtual nodes
  3. Extend adjacency matrix (self-loops on virtual nodes only)
  4. Alternate message passing layers
  5. Extract final embeddings for original nodes
- Design tradeoffs:
  - Complete Cayley vs. truncated: better expansion but more virtual nodes
  - Virtual node init: zeros (stable) vs. random (potentially richer) vs. learnable (flexible)
  - Layer alternation: balances local/global but may require tuning
- Failure signatures:
  - Overfitting to virtual nodes (check embedding norms)
  - Training instability (check alternating layer outputs)
  - Memory blowup (monitor virtual node count and connectivity)
- First 3 experiments:
  1. Replace Cayley graph with empty graph (baseline for global layer effect)
  2. Use truncated Cayley graph (compare with CGP)
  3. Vary virtual node initialization (zeros vs. random vs. learnable)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the initialization strategy of virtual nodes in CGP affect the final model performance across different graph datasets and tasks?
- Basis in paper: [explicit] The paper compares multiple initialization strategies (ZEROS, ONES, RANDOM, θ) on TUDataset and finds that ZEROS performs best, but all approaches yield similar performance within variance.
- Why unresolved: The experiments only tested a limited set of initialization strategies on one dataset collection. The impact across diverse datasets, tasks, and initialization methods remains unclear.
- What evidence would resolve it: Systematic experiments testing various initialization strategies (e.g., learned embeddings, task-specific initializations) across multiple diverse datasets and tasks would clarify optimal initialization approaches.

### Open Question 2
- Question: What is the optimal way to align Cayley graph edges with the input graph structure to preserve task-relevant inductive bias while maintaining the expansion properties?
- Basis in paper: [inferred] The paper notes that Cayley graphs are sparse compared to some datasets (e.g., IMDB-BINARY, COLLAB) and suggests this could be improved by better alignment with input graph structure, but doesn't propose a solution.
- Why unresolved: The paper identifies this as a limitation but doesn't explore methods for optimal alignment. Balancing edge alignment with maintaining expansion properties is theoretically challenging.
- What evidence would resolve it: Empirical studies comparing different alignment strategies (edge-preserving mappings, adaptive edge addition/removal) across diverse datasets would reveal optimal alignment approaches.

### Open Question 3
- Question: How do Cayley graphs perform on tasks that require dynamic graph structures, such as temporal graphs or evolving networks?
- Basis in paper: [inferred] The paper mentions temporal graph rewiring as a potential future application but doesn't evaluate CGP on temporal datasets or discuss how Cayley graphs would adapt to changing graph structures.
- Why unresolved: The current CGP formulation is static and doesn't address how to handle time-varying graph structures or maintain expansion properties over time.
- What evidence would resolve it: Experiments applying CGP to temporal graph benchmarks and developing mechanisms for dynamic Cayley graph construction/update would demonstrate effectiveness on evolving networks.

### Open Question 4
- Question: What is the relationship between Cayley graph regularity and oversmoothing in deep GNN architectures, and how can this be leveraged to design more robust GNN layers?
- Basis in paper: [explicit] The paper demonstrates that Cayley graphs exhibit robustness to graph overfitting and maintain higher Dirichlet energy compared to other methods, suggesting better resistance to oversmoothing.
- Why unresolved: While the paper shows empirical evidence of Cayley graphs' resistance to oversmoothing, the underlying mechanisms and how to systematically incorporate this property into GNN architecture design remain unexplored.
- What evidence would resolve it: Theoretical analysis of the connection between graph regularity and oversmoothing, combined with architectural modifications that explicitly leverage Cayley graph properties, would clarify how to build more robust deep GNN layers.

## Limitations

- The exact construction of the Cayley graph (particularly the generating set for SL(2, Z_n)) is not fully specified, affecting reproducibility
- Virtual node initialization strategy is not clearly defined, potentially impacting performance
- The claim of scalability for large graphs may not hold for graphs with high node-to-edge ratios due to increased virtual nodes

## Confidence

- **High Confidence**: The core hypothesis that truncation in EGP harms expansion properties is well-supported by spectral graph theory and experimental results
- **Medium Confidence**: The claim that virtual nodes effectively bridge connectivity gaps is plausible but lacks direct evidence for initialization strategies
- **Low Confidence**: The assertion that CGP is scalable for large graphs is based on sparse virtual node connectivity, but this may not hold in practice for graphs with high node-to-edge ratios

## Next Checks

1. **Cayley Graph Construction**: Validate the expansion properties of the complete Cayley graph for different generating sets and sizes to ensure the theoretical foundation holds in practice
2. **Virtual Node Initialization**: Experiment with different initialization strategies (zeros, random, learnable) for virtual nodes and analyze their impact on model performance and stability
3. **Scalability Testing**: Test CGP on large-scale graphs with high node-to-edge ratios to evaluate its computational efficiency and memory usage, particularly in handling the increased number of virtual nodes