---
ver: rpa2
title: 'GraphScale: A Framework to Enable Machine Learning over Billion-node Graphs'
arxiv_id: '2407.15452'
source_url: https://arxiv.org/abs/2407.15452
tags:
- training
- graphscale
- graph
- data
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GraphScale is a framework designed to scale machine learning over
  billion-node graphs by decoupling storage and computation in distributed training.
  The key insight is to use separate workers for data storage (actors) and computation
  (trainers), allowing asynchronous overlap of feature fetching and training.
---

# GraphScale: A Framework to Enable Machine Learning over Billion-node Graphs

## Quick Facts
- arXiv ID: 2407.15452
- Source URL: https://arxiv.org/abs/2407.15452
- Authors: Vipul Gupta; Xin Chen; Ruoyun Huang; Fanlong Meng; Jianjun Chen; Yujun Yan
- Reference count: 40
- Key outcome: Achieves at least 40% reduction in end-to-end training times for billion-node graphs using separate storage and computation workers

## Executive Summary
GraphScale is a distributed framework designed to scale machine learning over billion-node graphs by decoupling storage and computation in training. The framework separates workers who store data (actors) from those who perform training (trainers), allowing asynchronous overlap of feature fetching and computation. For Graph Neural Networks, this reduces communication overhead by consolidating feature requests. For unsupervised node embedding training, GraphScale employs hybrid parallelism to address memory and communication bottlenecks by leveraging the sparsity of gradients.

## Method Summary
GraphScale implements a distributed training framework using Ray's actor model, separating storage (actors) and computation (trainers) workers. For GNNs, trainers keep full graph topology and consolidate duplicate vertex IDs before requesting features from actors. For node embeddings, hybrid parallelism partitions the graph across trainers (data parallelism) while the embedding matrix is partitioned across storage actors (model parallelism), communicating only sparse gradient updates for sampled nodes.

## Key Results
- Achieves at least 40% reduction in end-to-end training times compared to popular distributed frameworks
- Successfully deployed at TikTok for efficient learning on large graphs
- Demonstrated scalability to graphs with over 1 billion nodes without loss in performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating storage and computation workers reduces communication overhead by consolidating feature requests
- Mechanism: Trainers keep full graph topology and merge duplicate vertex IDs before requesting features from actors, requiring only one request per mini-batch
- Core assumption: Graph topology is much smaller than feature matrix and can be replicated across trainers
- Evidence anchors:
  - [abstract] "The key insight in our design is the separation of workers who store data and those who perform the training"
  - [section] "This separation allows us to decouple computing and storage in graph training, thus effectively building a pipeline where data fetching and data computation can overlap asynchronously"
  - [corpus] Weak evidence; no corpus papers explicitly discuss this separation mechanism
- Break condition: When graph topology becomes too large to replicate efficiently across trainers

### Mechanism 2
- Claim: Hybrid parallelism for node embedding training reduces both memory and communication bottlenecks
- Mechanism: Graph is partitioned across trainers (data parallelism) while embedding matrix is partitioned across storage actors (model parallelism), allowing sparse gradient updates
- Core assumption: Gradients during node embedding training are sparse because only nodes in mini-batch and their neighbors are updated
- Evidence anchors:
  - [abstract] "GraphScale employs hybrid parallelism to address memory and communication bottlenecks, leveraging sparsity of gradients to communicate only updates for sampled nodes"
  - [section] "During training a mini-batch with ðµ source nodes, let ð‘› be the maximum number of nodes obtained during graph sampling... We generally haveð‘ â‰«ð‘›, that is, the total number of nodes in the graph (e.g., ð‘ = 1 ð‘ð‘–ð‘™ð‘™ð‘–ð‘œð‘› ) is much greater than the number of nodes sampled in a mini-batch (e.g.,ð‘› = 5000)"
  - [corpus] Weak evidence; no corpus papers explicitly discuss hybrid parallelism for large-scale node embeddings
- Break condition: When gradients become dense (e.g., full-batch training or very small graphs)

### Mechanism 3
- Claim: Ray actor model enables serverless, elastic resource allocation with failure tolerance
- Mechanism: Ray automatically manages resource provisioning and abstracts away resource allocation complexity, allowing GraphScale to scale dynamically
- Core assumption: Ray's actor model can efficiently handle the storage-computation separation pattern
- Evidence anchors:
  - [abstract] "GraphScale, leveraging the capabilities of Rayâ€“a framework designed for distributed serverless computationâ€“offers significant advantages such as elasticity in resource allocation and failure tolerance"
  - [section] "With Ray, resources can be provisioned in real-time, and the complexities of managing them are abstracted away by Ray's user-friendly API"
  - [corpus] Weak evidence; no corpus papers explicitly discuss Ray's role in this specific architecture
- Break condition: When Ray's scheduling overhead becomes significant compared to training time

## Foundational Learning

- Concept: Distributed systems and parallel computing
  - Why needed here: Understanding how to partition data and computation across multiple machines is fundamental to GraphScale's design
  - Quick check question: What are the tradeoffs between data parallelism and model parallelism in distributed training?

- Concept: Graph neural networks and node embedding algorithms
  - Why needed here: GraphScale is a framework for both GNN training and node embedding, so understanding these algorithms is essential
  - Quick check question: How does mini-batch sampling work in GraphSAGE vs DeepWalk?

- Concept: Ray actor model and serverless computing
  - Why needed here: GraphScale is implemented using Ray, so understanding its actor model is crucial for working with the framework
  - Quick check question: How does Ray's actor model differ from traditional parameter servers?

## Architecture Onboarding

- Component map: Trainers -> Actors -> Ray scheduler
- Critical path:
  1. Trainers sample subgraphs or mini-batches
  2. Trainers request features/embeddings from actors
  3. Actors return data to trainers
  4. Trainers perform computation (GNN forward/backward or embedding updates)
  5. Trainers send sparse gradient updates to actors
  6. Actors update stored parameters

- Design tradeoffs:
  - Replication vs partitioning of graph topology
  - Number of trainers vs communication overhead
  - Embedding dimension vs memory requirements
  - Synchronous vs asynchronous parameter updates

- Failure signatures:
  - High latency: Too many actors, network bottlenecks, or Ray scheduler issues
  - Memory overflow: Embedding size too large for available memory
  - Slow convergence: Incorrect learning rate, poor sampling strategy, or synchronization issues

- First 3 experiments:
  1. Run GraphScale on a small graph (e.g., Reddit) with default configuration to verify basic functionality
  2. Vary the number of trainers and measure throughput to understand scalability
  3. Compare GraphScale with PyTorch DDP on a medium-sized graph to validate performance claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GraphScale's hybrid parallelism approach compare to other distributed training methods in terms of scalability and communication efficiency for node embedding training on graphs with billions of nodes?
- Basis in paper: [explicit] The paper mentions that GraphScale uses hybrid parallelism for training node embeddings at scale, dividing the graph into disjoint subgraphs across multiple trainers and the model (embedding matrix) across storage workers. It also states that this allows GraphScale to scale both computation and storage independently with the size of the graph and offers significant communication savings over data parallelism alone.
- Why unresolved: The paper does not provide a detailed comparison of GraphScale's hybrid parallelism with other distributed training methods in terms of scalability and communication efficiency for large graphs.
- What evidence would resolve it: Empirical results comparing GraphScale's hybrid parallelism with other distributed training methods on graphs with billions of nodes, focusing on scalability and communication efficiency metrics.

### Open Question 2
- Question: What are the potential limitations of GraphScale's approach of separating storage and computation workers in terms of resource utilization and system complexity?
- Basis in paper: [inferred] The paper highlights that GraphScale's key insight is the separation of workers who store data and those who perform the training, allowing for decoupling computing and storage in graph training. However, it does not discuss potential limitations of this approach.
- Why unresolved: The paper does not address potential limitations of separating storage and computation workers, such as resource utilization and system complexity.
- What evidence would resolve it: A thorough analysis of the resource utilization and system complexity of GraphScale's approach compared to traditional distributed training methods, including potential bottlenecks and trade-offs.

### Open Question 3
- Question: How does GraphScale handle the trade-off between the granularity of feature requests and the overhead of consolidating requests in the context of GNN training?
- Basis in paper: [explicit] The paper mentions that GraphScale reduces the number of feature requests by consolidating multiple requests into a single batch at the end of each mini-batch training session. It also states that this method of consolidating requests and removing duplicate vertices greatly reduces the network bandwidth needed.
- Why unresolved: The paper does not discuss the potential trade-off between the granularity of feature requests and the overhead of consolidating requests in the context of GNN training.
- What evidence would resolve it: Empirical results analyzing the impact of different levels of request consolidation on the overall training time and resource utilization in GraphScale, considering various graph sizes and GNN architectures.

## Limitations

- Proprietary Dataset Dependency: Evaluation relies on a proprietary TikTok dataset, limiting independent verification of the 40% performance improvement
- Ray Framework Specificity: Performance benefits may be tied to Ray's actor model rather than the architectural pattern itself
- Memory Management Assumptions: Framework assumes sufficient memory to replicate graph topology across trainers without clear thresholds for infeasibility

## Confidence

**High Confidence**:
- The architectural separation of storage and computation workers is technically sound and addresses known communication bottlenecks
- The hybrid parallelism approach for node embeddings logically follows from sparsity of gradients in sampled-based training
- Ray's actor model is a reasonable choice for implementing this architecture given its serverless and elastic properties

**Medium Confidence**:
- The 40% end-to-end training time reduction claim, as it's based on comparisons with specific frameworks
- The scalability claims to billion-node graphs, primarily validated through synthetic data and a single proprietary dataset

**Low Confidence**:
- The specific performance characteristics under different failure scenarios or network conditions
- The framework's behavior with non-sparse or fully-connected graphs where sparsity assumptions break down

## Next Checks

1. Implement GraphScale on multiple public large-scale graph datasets (e.g., OGB datasets, Amazon products graph) to verify the claimed performance improvements across diverse graph structures and sizes.

2. Conduct stress testing under various failure conditions (actor failures, network partitions, resource constraints) to evaluate the framework's resilience and identify potential bottlenecks in the Ray-based implementation.

3. Benchmark GraphScale against alternative distributed computing frameworks (e.g., Horovod, Dask) using the same architectural pattern to isolate whether performance gains are due to the separation-of-concerns design versus Ray-specific optimizations.