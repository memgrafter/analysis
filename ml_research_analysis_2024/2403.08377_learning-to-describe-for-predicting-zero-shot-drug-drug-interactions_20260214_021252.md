---
ver: rpa2
title: Learning to Describe for Predicting Zero-shot Drug-Drug Interactions
arxiv_id: '2403.08377'
source_url: https://arxiv.org/abs/2403.08377
tags:
- drug
- drugs
- prompt
- prediction
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses zero-shot drug-drug interaction (DDI) prediction
  for new drugs, leveraging textual descriptions from DrugBank and PubChem. The core
  method, TextDDI, combines a language model-based DDI predictor with a reinforcement
  learning-based information selector to generate concise, relevant prompts from lengthy
  drug descriptions.
---

# Learning to Describe for Predicting Zero-shot Drug-Drug Interactions

## Quick Facts
- arXiv ID: 2403.08377
- Source URL: https://arxiv.org/abs/2403.08377
- Authors: Fangqi Zhu; Yongqi Zhang; Lei Chen; Bing Qin; Ruifeng Xu
- Reference count: 20
- Primary result: 52.5 F1-score on DrugBank, 67.3 accuracy on TWOSIDES for zero-shot DDI prediction

## Executive Summary
This paper tackles the challenge of predicting drug-drug interactions (DDIs) for new drugs using only textual descriptions, without prior interaction data. The proposed TextDDI framework combines a language model-based DDI predictor with a reinforcement learning-based information selector to generate concise, relevant prompts from lengthy drug descriptions. The approach significantly outperforms existing symbolic methods in both zero-shot and few-shot DDI prediction scenarios, achieving strong results on the DrugBank and TWOSIDES datasets. Case studies demonstrate that the selected text is semantically relevant to the target prediction, highlighting the model's interpretability.

## Method Summary
TextDDI addresses zero-shot DDI prediction by leveraging textual descriptions from DrugBank and PubChem. The method uses a fine-tuned language model as a DDI predictor, trained to classify interaction types from specially designed prompts that combine drug names and selected descriptions. An RL-based information selector iteratively chooses relevant sentences to construct these prompts, optimizing for both informativeness and brevity. The predictor and selector are trained alternately, with the selector's reward derived from the predictor's confidence on correct versus incorrect interaction types. This alternating training allows mutual enhancement of both modules, enabling effective zero-shot DDI prediction for new drugs.

## Key Results
- Achieves 52.5 F1-score on DrugBank and 67.3 accuracy on TWOSIDES in zero-shot DDI prediction
- Outperforms existing symbolic methods for zero-shot and few-shot DDI prediction
- Selected prompts are concise and semantically relevant to target interactions, as shown in case studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RL-based information selector generates prompts more relevant to DDI prediction than random descriptions.
- Mechanism: The selector encodes each candidate sentence and the current prompt, scores sentences with an MLP, and samples according to this distribution to build the prompt iteratively. The reward signal is the difference in DDI predictor confidence between correct and incorrect interaction types.
- Core assumption: Relevance can be learned from the reward signal without explicit human annotation.
- Evidence anchors: Abstract and section statements about prompt selection and reward design.
- Break condition: If the reward signal is too noisy or sparse, the selector may fail to learn meaningful relevance.

### Mechanism 2
- Claim: Fine-tuning a pre-trained LM with task-specific prompts enables effective domain adaptation for DDI prediction.
- Mechanism: Prompts combine drug names, selected descriptions, and a task instruction; the LM is fine-tuned to predict interaction types from these prompts.
- Core assumption: The LM can learn domain-specific knowledge from textual drug descriptions when fine-tuned with appropriate prompts.
- Evidence anchors: Abstract and section statements about prompt design and LM fine-tuning.
- Break condition: If the prompt design is inadequate or the model is too large, fine-tuning may not capture relevant domain knowledge.

### Mechanism 3
- Claim: Alternating training of the DDI predictor and information selector improves both modules by allowing each to specialize.
- Mechanism: The predictor is first trained with random prompts, then the selector generates better prompts, and the cycle repeats until validation performance stabilizes.
- Core assumption: Joint training in alternating phases allows mutual enhancement.
- Evidence anchors: Abstract and section statements about alternating training pipeline.
- Break condition: If either module plateaus early, further alternating may not yield improvements.

## Foundational Learning

- Concept: Reinforcement learning and Markov Decision Processes (MDPs)
  - Why needed here: The information selector is framed as an MDP to handle sequential sentence selection under a length constraint, using rewards from the DDI predictor to guide learning.
  - Quick check question: In the selector's MDP formulation, what defines the state space, and how is the action space constrained at each step?

- Concept: Prompt engineering and in-context learning in language models
  - Why needed here: Effective prompts are crucial for the DDI predictor to understand the task and leverage drug descriptions; prompt design balances informativeness and length.
  - Quick check question: What key elements must be included in the prompt to ensure the language model can predict DDIs accurately?

- Concept: Fine-tuning pre-trained language models for domain-specific tasks
  - Why needed here: General LLMs lack the specialized knowledge required for DDI prediction; fine-tuning adapts the model to the pharmaceutical domain using drug descriptions.
  - Quick check question: Why does fine-tuning with random prompts precede the alternating training phase, and what is the purpose of this initial step?

## Architecture Onboarding

- Component map:
  - Drug Descriptions -> Information Selector -> Prompt Construction -> DDI Predictor -> Interaction Prediction

- Critical path:
  1. Load drug descriptions from DrugBank/PubChem.
  2. Fine-tune DDI predictor with random prompts.
  3. Initialize information selector with predictor's LM weights.
  4. Iteratively: selector generates prompt → predictor evaluates → update selector → retrain predictor.
  5. Output final predictor and selector for inference.

- Design tradeoffs:
  - Prompt length vs. computational cost: Shorter prompts reduce quadratic LM complexity but may lose relevant information; selector balances this.
  - General vs. domain-specific LM: Using RoBERTa-Base limits parameter count and training cost but may underperform larger models.
  - Random vs. learned prompts: Random prompts enable initial fine-tuning; learned prompts improve relevance but require RL training.

- Failure signatures:
  - DDI predictor accuracy plateaus or degrades: May indicate overfitting, poor prompt quality, or insufficient domain knowledge.
  - Information selector produces irrelevant or repetitive sentences: Could signal weak reward signal or poor encoding of sentence relevance.
  - Long training times without improvement: May result from inefficient prompt generation or suboptimal hyperparameters.

- First 3 experiments:
  1. **Baseline DDI Predictor**: Fine-tune RoBERTa-Base with random prompts on DrugBank; evaluate F1-score on validation set.
  2. **Information Selector Training**: Initialize selector, run RL training for a few epochs; inspect generated prompts for relevance and diversity.
  3. **Alternating Training Cycle**: Perform one full cycle of selector → predictor updates; compare DDI predictor performance on validation set before and after.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TextDDI scale with increasingly larger language models beyond GPT-2 XL?
- Basis in paper: The paper mentions that larger language models generally yield superior results and that GPT-XL exhibits a 14% improvement compared to GPT2, but computational constraints limited their experiments to GPT-2 XL.
- Why unresolved: The paper did not have access to or did not experiment with language models larger than GPT-2 XL due to computational constraints.
- What evidence would resolve it: Experiments comparing TextDDI's performance using progressively larger language models (e.g., GPT-3, GPT-4) on the same datasets.

### Open Question 2
- Question: How would replacing the greedy search in the information selector with a beam search strategy affect TextDDI's performance?
- Basis in paper: The paper mentions that the greedy search used in their information selector could be replaced with a beam search strategy, which may lead to better performance.
- Why unresolved: The paper did not implement or test the beam search strategy as an alternative to the greedy search in the information selector.
- What evidence would resolve it: Experiments comparing TextDDI's performance using both greedy search and beam search strategies in the information selector.

### Open Question 3
- Question: How would TextDDI's performance change if the information selector was trained to generate prompts with higher density of task-relevant information?
- Basis in paper: The paper observed that as the prompt length increased beyond 256 tokens, the performance improvement became negligible, possibly due to a decrease in the density of task-relevant information within the longer prompt.
- Why unresolved: The paper did not experiment with training the information selector to specifically optimize for higher density of task-relevant information in the generated prompts.
- What evidence would resolve it: Experiments training the information selector to optimize for both prompt length and density of task-relevant information, and comparing the resulting TextDDI performance to the original approach.

## Limitations
- Performance depends on the quality and coverage of textual drug descriptions; may struggle with incomplete or noisy descriptions.
- Potential overfitting to the DrugBank and TWOSIDES datasets, limiting generalizability to new drug classes or interaction types.
- Assumes relevant information for DDI prediction can be extracted from unstructured text alone; may miss important domain-specific features not present in descriptions.

## Confidence
- **High Confidence**: The overall framework (TextDDI) and its modular design are well-specified and supported by experimental results.
- **Medium Confidence**: The mechanism by which the information selector generates relevant prompts via RL is plausible but relies on the assumption that the reward signal is sufficiently informative.
- **Low Confidence**: The generalizability of the approach to entirely new drug classes or interaction types not present in the training data is uncertain.

## Next Checks
1. **Ablation Study**: Remove the information selector and use random prompts for DDI prediction; compare F1-score to the full TextDDI model to quantify the selector's contribution.
2. **Prompt Quality Analysis**: Sample and manually inspect the prompts generated by the information selector for a set of drug pairs; assess semantic relevance and conciseness relative to the target interaction.
3. **Cross-Dataset Generalization**: Train TextDDI on DrugBank and evaluate on a separate, held-out dataset (e.g., TWOSIDES or another DDI corpus) to measure zero-shot generalization to new drugs and interactions.