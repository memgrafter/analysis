---
ver: rpa2
title: 'Rethinking Comprehensive Benchmark for Chart Understanding: A Perspective
  from Scientific Literature'
arxiv_id: '2412.12150'
source_url: https://arxiv.org/abs/2412.12150
tags:
- questions
- data
- chart
- question
- charts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SCI-CQA, a new benchmark for chart understanding
  that addresses limitations in existing datasets by using real scientific literature
  charts, including flowcharts, and employing a comprehensive evaluation framework
  inspired by human exams. The dataset consists of 37,607 high-quality charts with
  context, paired with 5,629 expert-curated questions covering multiple-choice, true/false,
  and open-ended formats.
---

# Rethinking Comprehensive Benchmark for Chart Understanding: A Perspective from Scientific Literature

## Quick Facts
- arXiv ID: 2412.12150
- Source URL: https://arxiv.org/abs/2412.12150
- Reference count: 19
- Introduces SCI-CQA, a comprehensive benchmark for chart understanding using real scientific literature charts with 37,607 high-quality charts and 5,629 expert-curated questions

## Executive Summary
This paper addresses critical limitations in existing chart understanding benchmarks by introducing SCI-CQA, a new dataset that uses real scientific literature charts rather than synthetic ones. The benchmark incorporates complex visual elements including flowcharts, multi-plot figures, and image-chart combinations that better reflect real-world scenarios. By employing a human-inspired exam format with multiple question types and contextual reasoning, SCI-CQA exposes significant performance gaps in current multimodal models, particularly on flowcharts and context-dependent questions.

## Method Summary
The SCI-CQA benchmark was created through a comprehensive pipeline involving data collection from scientific literature, filtering and classification of chart types, and automated annotation with iterative fine-tuning of open-source models. The dataset includes 37,607 charts paired with 5,629 expert-curated questions covering multiple-choice, true/false, and open-ended formats. Evaluation was conducted across eight multimodal models including both open-source (ChartLlama, InternVL2) and proprietary (GPT-4o, GPT-4o-mini) models, using a composite scoring system that scales open-ended question performance to a 100-point scale.

## Key Results
- Proprietary models like GPT-4o achieved composite scores around 70, while open-source models like InternVL2 reached about 60
- Models struggled significantly more with flowcharts (41.7 for InternVL2) compared to data charts (55.4 for InternVL2)
- Adding contextual information improved performance on complex reasoning questions, with InternVL2 showing 5.8% improvement on certain question types
- Open-source models showed a particularly large performance gap on flowcharts compared to data charts

## Why This Works (Mechanism)

### Mechanism 1
Using real scientific literature charts with complex visual elements exposes model limitations that synthetic datasets miss. Scientific literature charts contain multi-plot figures, flowcharts, structural diagrams, and image-chart combinations that require integration of visual and contextual information. These complex visual relationships cannot be adequately represented by template-based synthetic charts. Core assumption: Model performance on synthetic charts correlates with real-world chart understanding capabilities.

### Mechanism 2
Context-based questions prevent models from bypassing visual input through hallucination. By incorporating chart context and captions into questions, models cannot answer solely from image analysis. Questions that require contextual information force models to acknowledge limitations rather than fabricate answers. Core assumption: Models can distinguish between answerable and unanswerable questions when context is provided.

### Mechanism 3
Automated annotation pipeline reduces costs while maintaining quality through iterative fine-tuning. Using high-quality annotated data to fine-tune an open-source model, then using that model for pseudo-labeling with quality checks from free models and manual verification creates a cost-effective annotation process. Core assumption: Open-source models can achieve comparable annotation quality to proprietary models after sufficient fine-tuning.

## Foundational Learning

- **Multimodal model architecture and vision-language alignment**: Understanding how visual encoders and language models interact is crucial for analyzing model performance on chart understanding tasks. Quick check: What are the key differences between how open-source and proprietary multimodal models handle chart understanding?

- **Scientific literature comprehension and context integration**: Chart understanding in scientific papers requires integrating visual information with surrounding textual context, which differs from general chart interpretation. Quick check: How does the inclusion of context change the types of questions that can be asked about charts?

- **Benchmark design and evaluation methodology**: Creating effective evaluation frameworks requires understanding different question types, scoring methods, and how to measure model capabilities comprehensively. Quick check: What are the advantages and disadvantages of using multiple-choice versus open-ended questions for chart understanding evaluation?

## Architecture Onboarding

- **Component map**: Data processing pipeline -> Question generation and curation system -> Automated annotation pipeline -> Model evaluation
- **Critical path**: Data collection → Filtering and classification → Question generation → Manual curation → Model evaluation → Automated annotation refinement
- **Design tradeoffs**: High-quality manual curation vs. automated annotation speed; comprehensive evaluation vs. computational cost; diverse chart types vs. focused analysis
- **Failure signatures**: Models scoring high on synthetic benchmarks but low on real scientific charts; consistent hallucinations on context-dependent questions; poor performance on flowcharts despite strong data chart results
- **First 3 experiments**: 
  1. Test models on flowcharts vs. data charts to identify specific weaknesses
  2. Evaluate model performance with and without contextual information to measure context dependence
  3. Compare open-source vs. proprietary model performance across different question types

## Open Questions the Paper Calls Out

### Open Question 1
How does the inclusion of flowcharts as a distinct chart category impact the performance gap between open-source and proprietary models compared to traditional data charts? While the paper establishes that flowcharts present unique challenges, it does not fully explore whether targeted training on flowchart-specific data could close the performance gap between open-source and proprietary models. Experiments comparing model performance on flowcharts before and after targeted flowchart training would resolve this question.

### Open Question 2
What is the optimal balance between visual complexity and text context in training data for improving chart understanding performance? The paper shows that adding contextual information significantly improves performance on complex reasoning questions, but does not systematically investigate how different levels of visual complexity and text context affect model performance. Controlled experiments varying levels of visual complexity and text context in training data would resolve this question.

### Open Question 3
Can the automated annotation pipeline be improved to generate higher quality open-ended questions and answers? The paper acknowledges that the automated annotation pipeline produces lower quality open-ended questions compared to GPT-4o. While the paper demonstrates the pipeline's effectiveness in reducing costs, it does not explore specific modifications to improve the quality of generated open-ended content. Comparative studies of different base models and prompt engineering techniques would resolve this question.

## Limitations
- Evaluation framework relies on GPT-4o for open-ended question scoring, potentially introducing inconsistencies and bias
- Dataset focuses primarily on scientific literature charts, which may not represent the diversity of real-world chart usage across different domains
- Automated annotation pipeline, while efficient, may propagate errors from pseudo-labeled data despite quality checks

## Confidence

- **High confidence**: SCI-CQA provides more realistic evaluation than synthetic benchmarks (well-supported by dataset design and inclusion of complex scientific charts)
- **Medium confidence**: Proprietary models significantly outperform open-source models (needs further validation with more models and configurations)
- **Medium confidence**: Effectiveness of automated annotation pipeline in maintaining quality while reducing costs (requires more detailed analysis of pseudo-labeling process)

## Next Checks

1. **Cross-domain validation**: Test model performance on SCI-CQA versus charts from non-scientific domains (business, education, etc.) to verify domain-specific limitations
2. **Annotation quality audit**: Conduct manual verification of a sample of pseudo-labeled data to quantify error rates and identify systematic issues in the automated annotation pipeline
3. **Benchmark comparison**: Run identical models on SCI-CQA and existing synthetic benchmarks to measure the correlation gap and quantify the real-world relevance of current evaluation standards