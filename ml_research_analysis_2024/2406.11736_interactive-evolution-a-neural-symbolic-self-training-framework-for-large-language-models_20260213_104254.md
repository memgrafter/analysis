---
ver: rpa2
title: 'Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large
  Language Models'
arxiv_id: '2406.11736'
source_url: https://arxiv.org/abs/2406.11736
tags:
- self-training
- envisions
- arxiv
- reasoning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training large language models
  (LLMs) for neural-symbolic scenarios, where symbolic data is scarce and LLMs have
  limited proficiency in processing symbolic language. To overcome these challenges,
  the authors propose ENVISIONS, a novel environment-guided self-training framework.
---

# Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models

## Quick Facts
- arXiv ID: 2406.11736
- Source URL: https://arxiv.org/abs/2406.11736
- Reference count: 40
- Primary result: ENVISIONS achieves 30.00% and 24.95% average performance improvements on LLaMA2-Chat 7B and 13B variants respectively for neural-symbolic tasks.

## Executive Summary
This paper addresses the challenge of training large language models (LLMs) for neural-symbolic scenarios, where symbolic data is scarce and LLMs have limited proficiency in processing symbolic language. To overcome these challenges, the authors propose ENVISIONS, a novel environment-guided self-training framework. ENVISIONS enables LLMs to learn symbolic language processing abilities by iteratively interacting with an embodied environment, generating candidate actions, and executing them to obtain feedback. Through extensive evaluations on three distinct domains (web agents, math reasoning, and logical reasoning), ENVISONS consistently outperforms previous state-of-the-art self-training methods, demonstrating significant improvements in average performance.

## Method Summary
ENVISONS is a self-training framework that enables LLMs to learn symbolic language processing abilities without human-annotated data. The framework iteratively interacts with an embodied environment, where the LLM generates candidate symbolic actions, executes them, and collects binary feedback. The LLM then refines its policy using contrastive learning on positive and negative trajectory pairs derived from self-rewarding scores. The method consists of three main steps: online exploration, self-refinement, and self-rewarding. The candidate trajectory pool preserves high-quality positive solutions and mitigates forgetting during training.

## Key Results
- ENVISONS consistently outperforms previous state-of-the-art self-training methods across three domains (web agents, math reasoning, and logical reasoning)
- LLaMA2-Chat 7B variant achieves 30.00% average performance improvement
- LLaMA2-Chat 13B variant achieves 24.95% average performance improvement
- The RL-free contrastive loss L2 leads to 3.10%-4.57% improvements by effectively utilizing negative trajectories

## Why This Works (Mechanism)

### Mechanism 1
ENVISONS achieves neural-symbolic self-training by iterative exploration, self-refinement, and self-rewarding without human-annotated symbolic data. The LLM generates candidate symbolic actions, executes them in the environment, collects binary feedback, and refines via contrastive training on positive/negative pairs derived from self-rewarding scores. The framework assumes environment feedback is reliable and the LLM's sequence probability can serve as a soft reward score.

### Mechanism 2
The RL-free contrastive loss L2 improves stability by learning from negative trajectories without KL constraints. Positive-negative trajectory pairs are constructed from the candidate pool, and the loss maximizes log pθ(a+|x; a−), pushing the model to distinguish correct from incorrect symbolic outputs. The mechanism assumes contrastive learning on symbolic actions is effective even without explicit reward gradients.

### Mechanism 3
The candidate trajectory pool preserves high-quality positive solutions and mitigates forgetting. After each iteration, trajectories are filtered by binary and self-rewards, and only top-ranked ones are kept for the next round, enabling sustained knowledge accumulation. The mechanism assumes ranking by self-reward correlates with long-term usefulness and the pool size can be controlled without losing diversity.

## Foundational Learning

- **Concept: Neural-symbolic integration**
  - Why needed here: ENVISONS bridges natural language understanding and symbolic reasoning by converting NL inputs into executable symbolic actions.
  - Quick check question: What is the intermediate representation between input and output in a neural-symbolic system?

- **Concept: Reinforcement learning vs contrastive learning**
  - Why needed here: ENVISONS replaces RL with an RL-free contrastive loss to avoid KL constraints and improve training efficiency.
  - Quick check question: How does a contrastive loss differ from a policy gradient loss in terms of feedback signal?

- **Concept: Self-training and bootstrapping**
  - Why needed here: ENVISONS uses its own generated trajectories as training data, eliminating the need for external symbolic annotations.
  - Quick check question: What is the role of the candidate trajectory pool in the self-training loop?

## Architecture Onboarding

- **Component map**: LLM policy (πθ) -> Candidate generator -> Environment executor -> Feedback collector -> Trajectory filter -> Candidate pool -> Contrastive trainer -> Updated LLM
- **Critical path**: Generate -> Execute -> Filter -> Train (in each iteration)
- **Design tradeoffs**: K (candidate size) vs exploration breadth vs computational cost; Pool size vs memory usage vs diversity retention; Self-reward score vs binary feedback weight
- **Failure signatures**: Degraded performance if pool overfits to early trajectories; Instability if contrastive loss overwhelms self-reward guidance; Low diversity if K is too small or filtering too strict
- **First 3 experiments**:
  1. Run ENVISONS with K=1 on MiniWob++ and observe drop in exploratory ability.
  2. Remove the candidate pool and retrain to confirm forgetting effect.
  3. Replace L2 loss with a simple cross-entropy on positive-only samples and compare average scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do ENVISONS and previous methods (e.g., Distill-then-Finetune, Reinforced Self-Training) synergize to further enhance the performance of LLMs in neural-symbolic scenarios?
- Basis in paper: The authors state that "ENVISONS and previous methods are not mutually exclusive, but we leave it as a future work to explore their synergy."
- Why unresolved: The paper focuses on the effectiveness of ENVISONS and does not investigate the potential benefits of combining it with other methods.
- What evidence would resolve it: Conducting experiments that combine ENVISONS with Distill-then-Finetune and Reinforced Self-Training methods, and comparing their performance with ENVISONS alone.

### Open Question 2
- Question: How does the performance of ENVISONS scale with the size of the base LLM?
- Basis in paper: The authors evaluate ENVISONS on LLaMA2-Chat 7B and 13B variants, but do not explore its performance on larger or smaller models.
- Why unresolved: The paper only tests ENVISONS on two specific model sizes, and it is unclear how it would perform on other sizes.
- What evidence would resolve it: Evaluating ENVISONS on a range of LLM sizes, from small to large, and analyzing the relationship between model size and performance.

### Open Question 3
- Question: How does the choice of environment (e.g., Chrome browser, Python compiler, Pyke engine) affect the performance of ENVISONS?
- Basis in paper: The authors evaluate ENVISONS on three distinct environments, but do not investigate the impact of environment choice on performance.
- Why unresolved: The paper does not explore the sensitivity of ENVISONS to different environments, and it is unclear whether certain environments are more conducive to its success.
- What evidence would resolve it: Evaluating ENVISONS on a variety of environments and analyzing the relationship between environment characteristics and performance.

## Limitations
- Heavy reliance on environment feedback quality without systematic analysis of noise sensitivity
- Limited empirical validation for long-term stability across extended training runs
- No systematic analysis of how trajectory quality impacts contrastive learning effectiveness

## Confidence

- **High Confidence**: The basic architectural design of ENVISONS (iterative exploration, environment interaction, contrastive learning) is well-specified and theoretically sound.
- **Medium Confidence**: The reported performance improvements on benchmark tasks, though based on internal ablation studies rather than external validation.
- **Low Confidence**: Claims about the framework's ability to generalize across diverse symbolic domains and maintain stability over long training horizons.

## Next Checks

1. **Environment Feedback Robustness Test**: Systematically inject varying levels of noise into the binary feedback signal and measure degradation in performance to establish the framework's sensitivity to feedback quality.

2. **Trajectory Pool Diversity Analysis**: Track and visualize the diversity of solutions in the candidate pool over multiple training iterations to verify that the framework maintains exploration capability and doesn't converge prematurely to local optima.

3. **Cross-Domain Transfer Validation**: Apply the pre-trained ENVISONS model from one symbolic domain (e.g., math reasoning) to a novel but related domain (e.g., logical puzzles) to assess generalization capabilities beyond the training benchmarks.