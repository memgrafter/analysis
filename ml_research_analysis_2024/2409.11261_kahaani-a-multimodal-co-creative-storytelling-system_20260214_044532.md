---
ver: rpa2
title: 'Kahaani: A Multimodal Co-Creative Storytelling System'
arxiv_id: '2409.11261'
source_url: https://arxiv.org/abs/2409.11261
tags:
- story
- system
- overall
- children
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Kahaani, a multimodal, co-creative storytelling
  system for children that combines Generative AI components (LLM, TTS, TTM, TTV)
  with classical storytelling frameworks (Freytag's Pyramid and Propp's Narrative
  Functions) to create immersive educational narratives. The system enables children
  to actively participate in story creation through a five-phase process, with AI
  agents generating text, narration, music, and animation based on user inputs.
---

# Kahaani: A Multimodal Co-Creative Storytelling System

## Quick Facts
- arXiv ID: 2409.11261
- Source URL: https://arxiv.org/abs/2409.11261
- Authors: Samee Arif; Muhammad Saad Haroon; Aamina Jamal Khan; Taimoor Arif; Agha Ali Raza; Awais Athar
- Reference count: 40
- One-line primary result: Kahaani combines multiple generative AI models to create multimodal, co-created stories for children, showing high engagement and strong technical performance in individual component evaluations.

## Executive Summary
Kahaani is a multimodal, co-creative storytelling system designed to engage children in active story creation while teaching important life lessons and improving language skills. The system integrates Large Language Models, Text-to-Speech, Text-to-Music, and Text-to-Video generation technologies within classical storytelling frameworks (Freytag's Pyramid and Propp's Narrative Functions). Through a five-phase interactive process, children contribute to story creation while AI agents generate text, narration, music, and animation based on their inputs. The system aims to provide an immersive, educational experience that goes beyond passive content consumption.

The system was evaluated through both component-level technical assessments and a small-scale user study with parent-child pairs. Technical evaluations identified Gemma-2-9b as the optimal LLM for story generation, while user feedback indicated high satisfaction with story comprehension, age-appropriateness, and engagement. The multimodal approach combines visual, auditory, and textual elements to create an accessible and enriching storytelling experience that supports diverse learning needs.

## Method Summary
Kahaani employs a multi-agent architecture where user inputs collected through a five-phase interactive form (based on Freytag's Pyramid) are processed by specialized AI components. The Writer LLM (Gemma-2-9b) generates stories from user inputs, the Reviewer LLM (GPT-4o) ensures child-appropriateness, the Narrator (XTTSv2) converts text to speech, the Director LLM (GPT-4o) creates scene prompts, the Music Director LLM (GPT-4o) generates music prompts, the Musician (MusicGen-Large) produces background music, and the Animator (CogVideoX-5b with Animated style) generates video content. Each component was evaluated individually using standardized datasets and metrics, followed by a user study with three parent-child pairs to assess engagement and educational value.

## Key Results
- Gemma-2-9b LLM achieved strong performance across creativity, structural consistency, and instruction adherence for story generation
- User study with three parent-child pairs showed high engagement, with children finding stories easy to understand (average 4.0/5) and parents rating content appropriateness as 5.0/5
- Comparative evaluations across three media modalities (text, visual, audio) established benchmark frameworks for future multimodal storytelling systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system improves children's engagement and learning by combining multimodal AI outputs with classical storytelling frameworks.
- Mechanism: Multimodal stimuli (text, narration, music, animation) activate multiple sensory channels, reducing cognitive load and sustaining attention. Classical frameworks (Freytag's Pyramid, Propp's Functions) provide coherent narrative scaffolding, ensuring stories are both engaging and pedagogically sound.
- Core assumption: Multimodal presentation increases retention and comprehension compared to text-only delivery, especially for younger learners.
- Evidence anchors:
  - [abstract] "The system combines Large Language Model (LLM), Text-to-Speech (TTS), Text-to-Music (TTM), and Text-to-Video (TTV) generation to produce a rich, immersive, and accessible storytelling experience."
  - [section 1] "The conversion of written text into spoken words provides auditory stimuli that can improve reading comprehension, especially for students with learning disabilities."
  - [corpus] Weak evidence for direct multimodal learning gains; most cited papers focus on co-creation, not modality-specific effects.
- Break condition: If the multimodal outputs are of poor quality (e.g., unclear TTS, incoherent animation), the system may overwhelm or confuse children, negating engagement benefits.

### Mechanism 2
- Claim: The co-creative process increases children's sense of agency and motivation to learn.
- Mechanism: Children actively participate in story creation by selecting narrative functions and answering guided questions, which aligns with constructivist learning theories. This involvement makes the content personally relevant and increases intrinsic motivation.
- Core assumption: Active participation in creative tasks leads to deeper learning and higher engagement than passive consumption.
- Evidence anchors:
  - [abstract] "Here we define co-creative as a collaborative creative process in which both the child and Kahaani contribute to the generation of the story."
  - [section 1] "From a pedagogical perspective, these frameworks allow children to actively participate in the storytelling process rather than passively consuming content."
  - [corpus] Moderate evidence; co-creation papers support engagement but not specific learning outcomes.
- Break condition: If the input interface is too complex or the AI's suggestions are too restrictive, children may feel frustrated or disengaged.

### Mechanism 3
- Claim: Content moderation ensures child safety and maintains trust in the system.
- Mechanism: A dedicated LLM content reviewer filters out inappropriate language and themes before stories are narrated or animated, preventing exposure to harmful content.
- Core assumption: Even with built-in LLM safeguards, an additional human-in-the-loop moderation layer is necessary to ensure child-appropriateness.
- Evidence anchors:
  - [section 4.2] "The goal of this evaluation is to ensure that children are not exposed to inappropriate content within stories."
  - [section 4.2] "GPT-4o achieves the lowest FPR, with 9% FPR for the Project Gutenberg dataset and 0% FPR for the LLM-generated story dataset."
  - [corpus] Limited evidence; no peer-reviewed studies cited on content moderation effectiveness in children's AI systems.
- Break condition: If the moderation model has high false negatives, inappropriate content could reach children; if high false positives, it could overly restrict creative expression.

## Foundational Learning

- Concept: Freytag's Pyramid (exposition, rising action, climax, falling action, resolution)
  - Why needed here: Provides a clear, five-phase structure that guides both user input collection and AI story generation, ensuring coherent narrative flow.
  - Quick check question: Can you name the five phases of Freytag's Pyramid and explain their role in a story?
- Concept: Propp's Narrative Functions (e.g., Interdiction, Villainy, Absentation)
  - Why needed here: Supplies a set of archetypal story elements that children can select, ensuring their inputs map to recognizable narrative patterns and enabling consistent AI generation.
  - Quick check question: What is one example of a Propp function, and how might it appear in a children's story?
- Concept: Multimodal learning principles
  - Why needed here: Guides the design of the system's output modalities to maximize comprehension and engagement for diverse learners.
  - Quick check question: Why might combining audio and visual elements improve learning outcomes compared to text alone?

## Architecture Onboarding

- Component map:
  Frontend -> Writer (LLM) -> Reviewer (LLM) -> Narrator (TTS) + Movie Director (LLM) + Music Director (LLM) -> Musician (TTM) + Animator (TTV) -> Final multimodal output
- Critical path: User input → Writer → Reviewer → Narrator + Movie Director + Music Director → Musician + Animator → Final multimodal output
- Design tradeoffs:
  - LLM size vs. speed: Gemma-2-9b chosen for balance of creativity and responsiveness
  - Modality fidelity vs. generation time: TTV requires ~3 minutes per 6s video, limiting interactivity
  - Content safety vs. creativity: Strict moderation may reduce story variety
- Failure signatures:
  - Long generation delays: Likely TTV bottleneck
  - Inappropriate content: Reviewer or Writer failing moderation
  - Disjointed narrative: Writer not following Freytag/Propp structure
- First 3 experiments:
  1. Replace TTV with static images to test if interactivity improves with faster output
  2. Swap Gemma-2-9b for Gemma-2-27b to measure impact of model size on creativity scores
  3. Remove Reviewer step to measure false positive rate on a held-out inappropriate dataset

## Open Questions the Paper Calls Out

- Question: Does the choice of Freytag's Pyramid and Propp's Narrative Functions limit the creativity and diversity of the generated stories, or can the system adapt to other storytelling frameworks?
- Basis in paper: Explicit - The paper states that the system grounds the co-creation process in Freytag's Pyramid and Propp's Narrative Functions, but also mentions that these frameworks can enhance storytelling skills.
- Why unresolved: The paper does not explore alternative storytelling frameworks or discuss the potential limitations of using only Freytag's Pyramid and Propp's Narrative Functions. It also does not provide evidence of the system's adaptability to other frameworks.
- What evidence would resolve it: Experiments comparing the system's performance using different storytelling frameworks, or a discussion of how the system could be adapted to incorporate alternative frameworks.

- Question: How does the system handle the generation of stories with complex or nuanced themes that may be challenging for young children to understand?
- Basis in paper: Inferred - The paper mentions that the system aims to teach important life lessons through story morals and help children understand story structure, but does not discuss how it handles complex or nuanced themes.
- Why unresolved: The paper does not provide information on how the system simplifies or adapts complex themes for young children, or if it has any mechanisms to ensure age-appropriateness in terms of theme complexity.
- What evidence would resolve it: Examples of stories generated by the system that tackle complex themes, or a discussion of the system's approach to simplifying or adapting complex themes for young children.

- Question: What is the impact of the system on children's long-term language development and creativity, beyond the immediate engagement and satisfaction observed in the user study?
- Basis in paper: Inferred - The paper mentions that the system's main goals include helping children improve their English skills and creativity, and the user study shows high engagement and satisfaction. However, it does not discuss the long-term impact on language development and creativity.
- Why unresolved: The user study only involved three parent-child pairs and focused on immediate engagement and satisfaction, without measuring long-term language development or creativity outcomes.
- What evidence would resolve it: Longitudinal studies tracking children's language development and creativity over time with the system, or a discussion of potential mechanisms by which the system could impact long-term language and creativity outcomes.

## Limitations

- Small user study sample size (three parent-child pairs) limits generalizability of engagement and educational effectiveness claims
- Lack of empirical evidence for direct learning outcomes and long-term impact on language development and creativity
- Technical constraints with TTV generation speed (approximately 3 minutes per 6-second video) and occasional visual distortions in animated outputs

## Confidence

- High confidence: Technical performance of individual AI components (LLM creativity scores, TTS naturalness, content moderation accuracy)
- Medium confidence: Engagement and satisfaction scores from the user study, though based on a small sample
- Low confidence: Claims about improved language skills and creativity development, as these were not directly measured

## Next Checks

1. Expand user study sample size: Conduct evaluation with 20-30 parent-child pairs across different age groups to establish more robust measures of engagement, comprehension, and perceived educational value, with pre/post assessments of language skills.

2. A/B test multimodal vs. unimodal delivery: Compare learning outcomes and engagement between children using the full multimodal system versus text-only or audio-only versions to empirically validate the claimed benefits of multimodal presentation.

3. Long-term retention assessment: Implement a follow-up study at 1-week and 1-month intervals to measure story recall and language skill retention, providing evidence for the system's effectiveness as an educational tool beyond immediate engagement metrics.