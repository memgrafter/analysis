---
ver: rpa2
title: Heterophilous Distribution Propagation for Graph Neural Networks
arxiv_id: '2405.20640'
source_url: https://arxiv.org/abs/2405.20640
tags:
- heterophilous
- graph
- neighborhood
- representations
- homophilous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Heterophilous Distribution Propagation (HDP) is proposed to address
  the limitations of existing graph neural networks (GNNs) in modeling heterophilous
  graphs, where connected nodes exhibit dissimilar behaviors. HDP adaptively partitions
  the neighborhood into homophilous and heterophilous parts based on pseudo assignments
  and models the heterophilous neighborhood distribution using an orthogonality-oriented
  constraint.
---

# Heterophilous Distribution Propagation for Graph Neural Networks

## Quick Facts
- arXiv ID: 2405.20640
- Source URL: https://arxiv.org/abs/2405.20640
- Reference count: 40
- Key outcome: HDP achieves 87.84% accuracy on Cornell, 88.38% on Texas, 88.82% on Wisconsin, 71.56% on Chameleon, 37.26% on Actor, and 62.07% on Squirrel

## Executive Summary
Heterophilous Distribution Propagation (HDP) addresses the limitations of existing graph neural networks in modeling heterophilous graphs, where connected nodes exhibit dissimilar behaviors. The method adaptively partitions neighborhoods into homophilous and heterophilous parts based on pseudo assignments and models heterophilous neighbor distributions using an orthogonality-oriented constraint. HDP also introduces a semantic-aware message passing mechanism to propagate both homophilous and heterophilous patterns. Experimental results demonstrate that HDP outperforms representative baselines on heterophilous datasets, achieving significant accuracy improvements across 9 benchmark datasets.

## Method Summary
HDP operates by first constructing ego and structural embeddings for each node, then partitioning the neighborhood into homophilous (Ahm) and heterophilous (Aht) parts based on semantic assignments rather than similarity thresholds. For heterophilous neighbors, it models the distribution using a mean operator with an orthogonality-oriented constraint enforced through Trusted Prototype Contrastive (TPC) loss. The method then applies Semantic-aware Message Passing (SMP) to propagate both homophilous and heterophilous information through homophilous edges, approximating class-level patterns. Finally, it concatenates the processed representations and applies a classifier to predict node labels.

## Key Results
- HDP achieves 87.84% accuracy on Cornell dataset
- HDP achieves 88.38% accuracy on Texas dataset
- HDP achieves 88.82% accuracy on Wisconsin dataset

## Why This Works (Mechanism)

### Mechanism 1
HDP partitions neighborhoods into homophilous and heterophilous parts based on semantic assignments rather than similarity thresholds. It uses soft assignments (Z) to compute homophily probabilities (P_uv) between nodes, then applies a threshold (ϵ) to split Ahm and Aht. The threshold is dynamically adjusted using an estimated heterophily ratio (bh). This semantic grounding provides a better basis for partitioning than raw representation similarity, though poor soft assignments early in training can lead to suboptimal partitions.

### Mechanism 2
Heterophilous neighbor distributions are modeled via a mean operator with orthogonality-oriented constraint. The method aggregates heterophilous neighbor embeddings with a mean operator (Dht⁻¹AhtHego), then enforces orthogonality of class prototypes via Trusted Prototype Contrastive (TPC) loss. Under orthogonal prototypes, the mean operator becomes injective, mapping each heterophilous distribution to a unique embedding. However, if the TPC loss is too weak or β too small, prototypes may collapse and heterophilous patterns become indistinguishable.

### Mechanism 3
Semantic-aware Message Passing (SMP) propagates both homophilous and heterophilous information through homophilous edges to approximate class-level patterns. SMP aggregates homophilous neighbors over multiple hops with learnable weights (α_l), applied to both ego nodes (Hhm) and heterophilous distributions (Hht). This overcomes the noise and sparsity of single-node heterophilous distributions. However, if homophilous edges are too few in very heterophilous graphs, SMP cannot propagate sufficient class-level information.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: HDP builds on GNN foundations but modifies the message passing to handle heterophily
  - Quick check question: In standard GNNs, how is information aggregated from neighbors? (Answer: via sum/mean/max pooling of neighbor embeddings.)

- Concept: Homophily vs. heterophily
  - Why needed here: The paper's core motivation is that many real graphs violate the homophily assumption
  - Quick check question: What is the homophily ratio formula used in the paper? (Answer: h = |{(u,v)∈E ∧ yu=yv}| / |E|.)

- Concept: Soft assignments and pseudo labels
  - Why needed here: HDP uses soft assignments (Z) as a basis for partitioning, updated during training
  - Quick check question: How is the soft assignment Z defined? (Answer: Each row sums to 1; zij is the probability that node i belongs to class j.)

## Architecture Onboarding

- Component map: Input -> Ego/Structural Embeddings -> Neighborhood Partition -> Heterophilous Distribution -> SMP (homophilous) -> SMP (heterophilous) -> Concatenation -> Classifier
- Critical path: Input → Ego/Structural Embeddings → Neighborhood Partition → Heterophilous Distribution → SMP (homophilous) → SMP (heterophilous) → Concatenation → Classifier
- Design tradeoffs:
  - Partition accuracy vs. computational cost (computing Ahm, Aht each epoch)
  - Heterophilous modeling expressiveness vs. reliance on prototype orthogonality
  - SMP hop count vs. oversmoothing risk
- Failure signatures:
  - Poor partition → confused Hnb, noisy Hht
  - Weak TPC loss → collapsed prototypes, indistinguishable heterophilous patterns
  - Too few homophilous edges → SMP cannot propagate class-level information
- First 3 experiments:
  1. Verify partition correctness on a toy graph with known homophily (visualize Ahm, Aht)
  2. Check TPC loss drives prototype orthogonality (compute cosine similarity between prototypes)
  3. Ablation: Run with and without SMP to confirm it improves heterophilous representation discriminability

## Open Questions the Paper Calls Out
None

## Limitations
- Partition stability depends on reliable estimation of heterophily ratio (bh), which may be poor in noisy or sparse graphs
- The effectiveness of Semantic-aware Message Passing (SMP) may be limited in graphs with very low homophily ratios due to insufficient homophilous edges
- Computational overhead is significant due to dynamic computation of heterophilous/homophilous adjacency matrices and multi-hop SMP

## Confidence

**High Confidence**: The overall experimental superiority on heterophilous datasets (accuracy improvements of 4.31%-21.74% over GGCN) is well-supported by the results. The mechanism of separating neighborhoods based on semantic assignments is clearly articulated and implemented.

**Medium Confidence**: The theoretical guarantee of injectivity under orthogonal prototypes (Theorem 1) is mathematically sound, but its practical realization depends heavily on the TPC loss strength and training dynamics, which are not extensively analyzed.

**Low Confidence**: The robustness of the dynamic thresholding mechanism (ϵ adjustment based on bh) to estimation errors and its impact on final performance in diverse graph scenarios is not thoroughly validated.

## Next Checks

1. **Partition Robustness Analysis**: Systematically vary the heterophily ratio (bh) in synthetic graphs with known ground truth homophily and measure how partition accuracy (Ahm, Aht) and final classification performance change. This will validate the dynamic thresholding mechanism's stability.

2. **TPC Loss Ablation with Prototype Monitoring**: Run ablations with TPC loss strength (β) set to 0, 0.1, 1.0, 10.0. For each setting, compute and visualize the pairwise cosine similarities between learned class prototypes. This will directly show whether orthogonality is maintained and correlates with performance.

3. **SMP Effectiveness in Low-Homophily Graphs**: Evaluate HDP on graphs with artificially reduced homophily ratios (e.g., 0.1, 0.2, 0.3) by edge rewiring or label shuffling. Compare performance with and without SMP to quantify its contribution when homophilous edges are scarce.