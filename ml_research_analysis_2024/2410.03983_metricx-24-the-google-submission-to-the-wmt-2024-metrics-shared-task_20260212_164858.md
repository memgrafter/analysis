---
ver: rpa2
title: 'MetricX-24: The Google Submission to the WMT 2024 Metrics Shared Task'
arxiv_id: '2410.03983'
source_url: https://arxiv.org/abs/2410.03983
tags:
- data
- translation
- synthetic
- training
- ratings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MetricX-24 is a learned MT evaluation metric that predicts a floating
  point quality score for a translation. To improve robustness to failure modes such
  as undertranslation, duplication, and fluent but unrelated translation, the metric
  is trained on a combination of DA and MQM ratings, augmented with synthetic data.
---

# MetricX-24: The Google Submission to the WMT 2024 Metrics Shared Task

## Quick Facts
- arXiv ID: 2410.03983
- Source URL: https://arxiv.org/abs/2410.03983
- Authors: Juraj Juraska; Daniel Deutsch; Mara Finkelstein; Markus Freitag
- Reference count: 21
- Primary result: Achieved 61.11 segment-level pairwise accuracy, state-of-the-art performance at segment level in WMT24

## Executive Summary
MetricX-24 is a learned MT evaluation metric that predicts floating point quality scores for translations. The metric addresses common failure modes through a two-stage fine-tuning approach using DA ratings followed by a mixture of MQM and DA ratings, augmented with synthetic training data. A hybrid reference-based/-free variant is trained in three input modes (source+hypothesis, hypothesis+reference, and all three), allowing it to learn when to rely less on poor-quality references. Experiments show significant improvements over previous versions on WMT23 MQM ratings and a new synthetic challenge set.

## Method Summary
MetricX-24 employs a two-stage fine-tuning pipeline on an mT5-XXL (13B parameters) encoder-decoder model. The first stage fine-tunes on WMT DA ratings (2015-2022), followed by the second stage which mixes MQM ratings (2020-2022) with DA ratings and synthetic data covering failure modes like undertranslation, duplication, and unrelated translations. The model uses a hybrid input mode trained on source+hypothesis, hypothesis+reference, and all three input combinations. Synthetic data is mixed at 1:100 ratio in stage 1 and 1:5000 in stage 2, with the optimal DA:MQM ratio determined to be 1:10 for the second stage.

## Key Results
- Achieved 61.11 segment-level pairwise accuracy on primary submission
- Significant improvements over previous version on WMT23 MQM ratings
- Demonstrated state-of-the-art performance at segment level in WMT24 shared task

## Why This Works (Mechanism)

### Mechanism 1
Synthetic training data can make the metric more robust to failure modes that rarely occur in real WMT data. By constructing synthetic examples for specific failure modes (empty translation, gibberish, unrelated translation, undertranslation, duplication, missing punctuation, reference-matching), the model learns to recognize and score these problematic translations correctly. The synthetic data is mixed into the training set at different ratios for the two fine-tuning stages (1:100 in stage 1, 1:5000 in stage 2).

### Mechanism 2
Mixing DA and MQM ratings in the second stage prevents catastrophic forgetting of language pairs not present in MQM data. After the first stage of fine-tuning on DA ratings (covering ~50 language pairs), the model is further fine-tuned on a mixture of MQM ratings (high quality, few language pairs) and DA ratings (lower quality, many language pairs). The DA:MQM ratio of 1:10 provides continued exposure to additional languages while maintaining high performance on MQM language pairs.

### Mechanism 3
Hybrid input mode allows the model to learn when to rely on source, reference, or both, improving performance especially with poor-quality references. The model is trained on examples in three formats: (1) source + hypothesis, (2) hypothesis + reference, and (3) source + hypothesis + reference. This allows the model to learn different weighting strategies depending on input availability and quality, and to potentially ignore poor-quality references.

## Foundational Learning

- Concept: Understanding of MT evaluation metrics and their evolution from lexical (BLEU, ChrF) to learned neural metrics
  - Why needed here: The paper builds on this evolution and positions MetricX-24 as part of the learned metrics landscape
  - Quick check question: What are the key limitations of lexical metrics like BLEU that motivated the development of learned metrics?

- Concept: Z-normalization and its role in making ratings comparable across annotators
  - Why needed here: DA ratings require z-normalization before training, while MQM ratings do not, due to differences in annotation consistency
  - Quick check question: Why do models benefit from z-normalizing DA ratings but not MQM ratings?

- Concept: Meta-evaluation of MT metrics using correlation metrics (Pearson's r, Kendall's τ) and pairwise accuracy
  - Why needed here: The paper uses these metrics to evaluate MetricX-24's performance against human judgments
  - Quick check question: What is the difference between segment-level and system-level pairwise accuracy in MT metric evaluation?

## Architecture Onboarding

- Component map: mT5-XXL encoder-decoder model -> Two-stage fine-tuning pipeline (DA then MQM) -> Synthetic data augmentation module -> Hybrid input handling (source+hypothesis, hypothesis+reference, all three) -> Evaluation module with multiple correlation metrics

- Critical path: Data preprocessing → First-stage fine-tuning (DA) → Synthetic data generation → Second-stage fine-tuning (MQM+DA+synthetic) → Evaluation and checkpoint selection

- Design tradeoffs: Model size vs. training cost (13B parameters require 256 TPUs), synthetic data ratio vs. overfitting to synthetic patterns, DA:MQM ratio vs. balancing language coverage and rating quality, hybrid vs. specialized models for reference-based vs. QE tasks

- Failure signatures: Poor performance on synthetic test set indicates failure to learn from synthetic examples, large drops in DA evaluation performance indicate catastrophic forgetting of non-MQM languages, performance degradation with poor-quality references indicates failure of hybrid learning

- First 3 experiments:
  1. Train baseline model with DA data only, evaluate on WMT23 MQM and synthetic test sets
  2. Add synthetic data in second stage, compare performance on synthetic test set categories
  3. Mix DA and MQM data in second stage, measure recovery of DA evaluation performance while maintaining MQM performance

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal ratio of DA to MQM data for fine-tuning in the second stage to maximize performance across all language pairs? The paper mentions that a DA:MQM ratio of 1:4 was initially determined to work well, but post-submission experimentation found 1:10 to be optimal. A detailed ablation study showing the performance impact of different DA:MQM ratios on various language pairs and evaluation metrics would provide a clearer understanding of the optimal ratio.

### Open Question 2
How does the quality of references impact the performance of hybrid models, and can the model effectively learn to ignore poor-quality references? While the paper suggests potential benefits of the hybrid model in handling poor-quality references, it does not provide a thorough analysis of how reference quality impacts performance or the model's ability to adapt to varying reference quality. An analysis of model performance on datasets with varying reference quality would clarify the impact of reference quality and the hybrid model's adaptability.

### Open Question 3
What are the limitations of synthetic data in addressing specific failure modes, and how can these limitations be mitigated? The paper identifies specific failure modes where synthetic data was less effective but does not explore the reasons behind these limitations or potential solutions. An investigation into the reasons for the limitations of synthetic data in certain failure modes, along with experiments to test alternative approaches or enhancements to the synthetic data generation process, would provide insights into mitigating these limitations.

## Limitations
- Evaluation relies heavily on synthetic test sets that may not capture full diversity of real translation errors
- Optimal mixing ratios for synthetic data were determined empirically without systematic exploration of hyperparameter space
- No ablation study showing how model performs when only one input format is available in hybrid mode

## Confidence
- High Confidence: Two-stage fine-tuning approach and DA:MQM ratio of 1:10 are well-supported by experimental results
- Medium Confidence: Synthetic data augmentation shows promise but real-world generalizability is uncertain
- Low Confidence: Mechanism by which hybrid input mode enables appropriate weighting strategies is not empirically verified

## Next Checks
1. Create a new test set with naturally occurring translation errors representing the seven failure modes from real-world MT systems, rather than synthetic modifications of WMT data. Compare MetricX-24's performance on this real-world set against its synthetic test set performance.

2. Train and evaluate separate models using only source+hypothesis, only hypothesis+reference, and only source+hypothesis+reference input formats. Compare their individual performance against the hybrid model to determine whether the hybrid approach provides specific advantages for handling poor-quality references or simply learns more robust representations.

3. Test MetricX-24 on MT evaluation data from domains outside the WMT shared tasks (e.g., conversational AI, technical documentation, literary translation) to validate whether synthetic data training and hybrid input learning generalize beyond the WMT evaluation framework to truly robust MT evaluation.