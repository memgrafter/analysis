---
ver: rpa2
title: Contrastive Learning Method for Sequential Recommendation based on Multi-Intention
  Disentanglement
arxiv_id: '2404.18214'
source_url: https://arxiv.org/abs/2404.18214
tags:
- learning
- user
- contrastive
- sequence
- intention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a contrastive learning sequential recommendation\
  \ method based on multi-intention disentanglement (MIDCL). The method addresses\
  \ the challenge of understanding and disentangling user\u2019s multi-intention in\
  \ behavior prediction and sequential recommendation."
---

# Contrastive Learning Method for Sequential Recommendation based on Multi-Intention Disentanglement

## Quick Facts
- arXiv ID: 2404.18214
- Source URL: https://arxiv.org/abs/2404.18214
- Reference count: 40
- Key outcome: Proposes MIDCL, a VAE-based multi-intention disentanglement method with two contrastive learning paradigms that outperforms existing sequential recommendation baselines

## Executive Summary
This paper introduces MIDCL (Multi-Intention Disentanglement Contrastive Learning), a sequential recommendation method that addresses the challenge of understanding and disentangling user multi-intentions. The method uses Variational Auto-Encoder to learn latent intention distributions and proposes two contrastive learning paradigms - one based on triplet loss for intention alignment and another based on InfoNCE loss for sequence augmentation. Experimental results on four real-world datasets demonstrate significant performance improvements over existing methods while providing more interpretable intention-based predictions.

## Method Summary
MIDCL combines VAE-based multi-intention disentanglement with two complementary contrastive learning paradigms. First, it encodes user interaction sequences into latent intention distributions using a Transformer encoder and VAE, learning k distinct intention vectors per user. Second, it applies two contrastive learning approaches: intention-based triplet loss that aligns sequence embeddings with their most relevant intentions, and sequence-based InfoNCE loss that maximizes mutual information between augmented views of the same sequence using stochastic (crop, mask, reorder) and informative (insert, substitute) augmentations. The model is trained with multi-task optimization combining cross-entropy prediction loss, VAE KL divergence, and contrastive learning losses.

## Key Results
- MIDCL achieves significant performance improvements over baseline methods including BPR-MF, FPMC, Caser, GRU4Rec, SASRec, and S3-RecMIP
- The method demonstrates superior results on four real-world datasets (Yelp, Gowalla, Toys, Beauty) across multiple evaluation metrics (HR@5/10, MRR, NDCG@5/10)
- Ablation studies confirm the effectiveness of both intention disentanglement and contrastive learning modules
- Sensitivity analysis shows MIDCL's performance is stable across different hyperparameter settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Variational Auto-Encoder (VAE) disentangles user's multi-intentions by learning latent intention distributions in a shared feature space.
- **Mechanism:** VAE encodes user interaction sequences into latent distributions parameterized by mean (μ) and log-variance (log σ²), then samples via reparameterization to generate disentangled intention representations.
- **Core assumption:** User interaction sequences can be modeled as mixtures of independent latent intentions, and these intentions are sufficiently captured by a continuous latent space.
- **Evidence anchors:**
  - [abstract] "we choose Variational Auto-Encoder (VAE) to realize the disentanglement of users' multi-intentions."
  - [section 3.2] "The VAE turns the sampling from N (µ, σ2) into sampling from N (0, 1), and then obtains the sampling from N (µ, σ2) by a parameter transformation"
  - [corpus] Weak: no direct neighbor papers cite VAE-based intention disentanglement.

### Mechanism 2
- **Claim:** Contrastive learning with triplet loss aligns sequence embeddings with their most relevant intention while pushing away irrelevant intentions.
- **Mechanism:** Uses the anchor as the user sequence embedding, positive samples as the most similar intention, and negative samples as the least similar intention; optimizes via triplet loss to minimize distance to positives and maximize distance to negatives.
- **Core assumption:** There exists a measurable similarity between sequence embeddings and intention vectors that can be exploited for contrastive learning.
- **Evidence anchors:**
  - [abstract] "We propose two types of contrastive learning paradigms for finding the most relevant user's interactive intention, and maximizing the mutual information of positive sample pairs."
  - [section 4.4.1] "The anchor point is the sequence feature eu s of u, the positive sample is the intention I u pos with the highest similarity to eu s , and the negative sample is the intention I u neg with the lowest similarity to eu s"
  - [corpus] Moderate: neighbor papers mention contrastive learning for sequential recommendation but not specifically intention-based triplet loss.

### Mechanism 3
- **Claim:** Sequence-based contrastive learning with stochastic and informative augmentations maximizes mutual information between augmented views of the same sequence.
- **Mechanism:** Applies augmentation operators (crop, mask, reorder, insert, substitute) to generate positive and negative sequence pairs, then optimizes via InfoNCE loss to align embeddings of augmented views from the same user.
- **Core assumption:** Augmented views of the same sequence share the same underlying user intention, and augmentations preserve semantic consistency.
- **Evidence anchors:**
  - [abstract] "We propose two types of contrastive learning paradigms for finding the most relevant user's interactive intention, and maximizing the mutual information of positive sample pairs, respectively."
  - [section 4.4.2] "each batch of sequences {H u } N u=1 from N users is input into the sequence contrastive learning module, and each time two kinds of A are selected to generate the augmented sequences"
  - [corpus] Strong: neighbor papers explicitly use contrastive learning with augmentations for sequential recommendation (e.g., CALRec, Dual Contrastive Transformer).

## Foundational Learning

- **Concept: Variational Auto-Encoder (VAE)**
  - Why needed here: VAE provides a principled way to learn a continuous latent representation of user intentions from discrete interaction sequences.
  - Quick check question: What is the purpose of the reparameterization trick in VAE training?

- **Concept: Contrastive Learning (InfoNCE)**
  - Why needed here: Contrastive learning maximizes mutual information between related samples, improving representation quality for recommendation.
  - Quick check question: In InfoNCE loss, what role does the temperature parameter τ play?

- **Concept: Self-Attention Mechanism**
  - Why needed here: Self-attention captures complex dependencies in user interaction sequences, enabling better sequence representation.
  - Quick check question: How does self-attention differ from recurrent networks in handling sequence data?

## Architecture Onboarding

- **Component map:** Input Embedding Layer → Transformer Encoder (shared parameters) → VAE Disentanglement → Contrastive Learning (Intent+Sequence) → Multi-Task Training (Prediction + Losses)
- **Critical path:** Sequence → Transformer Encoder → VAE Latent Space → Intention Contrastive Learning → Final Embedding → Prediction
- **Design tradeoffs:**
  - Using only VAE encoder (no decoder) saves computation but loses reconstruction supervision.
  - Two separate contrastive paradigms increase model complexity but target different aspects (intention vs sequence).
  - Learnable positional encoding adds flexibility but may overfit to dataset-specific patterns.
- **Failure signatures:**
  - If KLD loss dominates, the model may collapse to prior and lose intention specificity.
  - If InfoNCE loss is too high, augmentations may not preserve intention or similarity measure is broken.
  - If triplet loss fails to separate intentions, the model cannot disambiguate user intents.
- **First 3 experiments:**
  1. Verify that VAE disentanglement produces k distinct intention vectors per user by visualizing with t-SNE.
  2. Test contrastive learning ablation: run with only intention CL, only sequence CL, and both to measure individual contributions.
  3. Validate augmentation effectiveness by measuring similarity drop between augmented and original sequences; ensure positive pairs remain closer than negative pairs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MIDCL method perform compared to other baseline methods?
- Basis in paper: Explicit. The paper presents a comparison of MIDCL with several baseline methods, including BPR-MF, FPMC, Caser, GRU4Rec, GRU4Rec+, SASRec, BERT4Rec, S3-RecMIP, CL4SRec, and CoSeRec, on four real-world datasets.
- Why unresolved: The comparison results are presented in Table 2, but the specific performance metrics and improvements are not detailed.
- What evidence would resolve it: Detailed performance metrics and improvements of MIDCL compared to baseline methods on the four real-world datasets.

### Open Question 2
- Question: How do the various hyperparameters affect the performance of MIDCL?
- Basis in paper: Explicit. The paper discusses the sensitivity analysis of MIDCL with respect to the weighting factors α&β and the number of intentions k in the four datasets mentioned.
- Why unresolved: The specific effects of these hyperparameters on the performance of MIDCL are not detailed.
- What evidence would resolve it: Detailed analysis of how the weighting factors α&β and the number of intentions k affect the performance of MIDCL on the four real-world datasets.

### Open Question 3
- Question: Are the modules of intention disentanglement and contrastive learning in MIDCL effective?
- Basis in paper: Explicit. The paper conducts an ablation study to verify the validity of each module of MIDCL, including the intention disentanglement module of VAE, the contrastive learning module, and the original encoder module only for sequential recommendation.
- Why unresolved: The specific effectiveness of these modules is not detailed.
- What evidence would resolve it: Detailed results of the ablation study showing the effectiveness of the intention disentanglement and contrastive learning modules in MIDCL.

## Limitations

- The independence assumption underlying VAE-based intention disentanglement is not validated - real user behavior often exhibits intention dependencies
- The paper does not evaluate whether the learned intention representations are truly interpretable or merely mathematically distinct
- The effectiveness of the two-stage contrastive learning approach lacks ablation studies to quantify individual contributions

## Confidence

**High confidence**: The VAE framework for intention modeling is well-established and the experimental setup is rigorous with appropriate baselines and metrics.
**Medium confidence**: The contrastive learning components show theoretical soundness but limited empirical validation of individual contributions.
**Low confidence**: The interpretability claims regarding disentangled intentions are not substantiated with qualitative analysis or user studies.

## Next Checks

1. **Ablation study validation**: Run experiments isolating VAE-only, intention contrastive learning-only, and sequence contrastive learning-only to quantify individual contribution margins.
2. **Intention independence test**: Measure correlation between learned intention vectors across users to validate the independence assumption underlying the VAE approach.
3. **Augmentation sensitivity analysis**: Systematically vary augmentation parameters and operators to determine robustness thresholds where contrastive learning benefits degrade.