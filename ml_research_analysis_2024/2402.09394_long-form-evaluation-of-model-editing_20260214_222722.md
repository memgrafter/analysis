---
ver: rpa2
title: Long-form evaluation of model editing
arxiv_id: '2402.09394'
source_url: https://arxiv.org/abs/2402.09394
tags: []
core_contribution: This paper introduces LEME, the first comprehensive evaluation
  protocol for assessing the impact of model editing on long-form natural language
  generation. The authors create a novel dataset of coupled entity prompts and develop
  both human surveys and automatic metrics (classifier-based) to measure edit consistency,
  factual consistency, internal consistency, topicality, and naturalness in paragraph-length
  generations.
---

# Long-form evaluation of model editing

## Quick Facts
- arXiv ID: 2402.09394
- Source URL: https://arxiv.org/abs/2402.09394
- Reference count: 40
- This paper introduces LEME, the first comprehensive evaluation protocol for assessing the impact of model editing on long-form natural language generation.

## Executive Summary
This paper introduces LEME, the first comprehensive evaluation protocol for assessing the impact of model editing on long-form natural language generation. The authors create a novel dataset of coupled entity prompts and develop both human surveys and automatic metrics (classifier-based) to measure edit consistency, factual consistency, internal consistency, topicality, and naturalness in paragraph-length generations. When benchmarking multiple editing techniques (FT, MEND, ROME, MEMIT, IKE) on this protocol, they find that short-form metrics poorly correlate with long-form quality measures, and that ROME/MEMIT suffer significantly from "factual drift" - making the most edit-consistent changes while contradicting the most ground-truth facts. Their qualitative analysis reveals common failure modes including entity/topic drift, lexical cohesion issues, and internal contradictions.

## Method Summary
The study develops LEME (Long-form Evaluation of Model Editing) by creating a Coupled Entity Prompts dataset with subject and related entity prompts derived from Counterfact and zSRE datasets. They apply five model editing techniques (FT, MEND, ROME, MEMIT, IKE) to various LLMs, generating subject and related passages for each edit. Human participants rate a small sample of outputs, which are then used to train DeBERTaV3 models to automatically rate larger datasets. The protocol evaluates edit consistency, factual consistency, internal consistency, topicality, and naturalness, comparing these long-form metrics against traditional short-form evaluations.

## Key Results
- Short-form metrics poorly correlate with long-form quality measures (r < 0.1), indicating novel dimensions captured by LEME
- ROME and MEMIT exhibit significant factual drift, achieving high edit consistency while contradicting the most ground-truth facts
- All editing methods degrade topicality and naturalness compared to unedited models, with factual drift being the primary failure mode
- Counterfactual updates perform worse than novel fact injections across most editing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey rating model correlates with human ratings because it's trained on synthetic survey data generated using GPT-4 to rate model outputs.
- Mechanism: Human participants rate a small sample of model outputs, then GPT-4 generates synthetic ratings for a much larger dataset. A DeBERTaV3 model is trained on this synthetic dataset to predict ratings for new samples.
- Core assumption: GPT-4's ratings are sufficiently similar to human ratings to serve as training data for a model that generalizes to human preferences.
- Evidence anchors:
  - [abstract]: "Our protocol consists of a machine-rated survey and a classifier which correlates well with human ratings."
  - [section 3.2]: "We performed survey ratings using the same survey instructions human participants saw using GPT-4 resulting in a total of 7,164 ratings (796 per question). Treating this as a training set, we trained DeBERTaV3 large (He et al., 2022) for each question and evaluated the model using the human survey ratings as the test set."
  - [corpus]: Weak - the corpus doesn't contain direct evidence about this specific mechanism.
- Break condition: If GPT-4's ratings systematically differ from human ratings in ways not captured by the training data, the DeBERTaV3 model will fail to generalize to human preferences.

### Mechanism 2
- Claim: Factual drift occurs because some model editing methods like ROME and MEMIT make broad changes that affect more than just the target fact.
- Mechanism: When editing a model to change a specific fact, these methods update parameters in ways that affect related knowledge representations, causing unintended changes to other facts.
- Core assumption: The parameter updates made by ROME and MEMIT are not sufficiently localized to the target fact.
- Evidence anchors:
  - [abstract]: "ROME and MEMIT suffer significantly from 'factual drift' - making the most edit-consistent changes while contradicting the most ground-truth facts."
  - [section 5.3]: "We see a pattern where models that do better at edit consistency for the subject passages perform worse on reflecting the edit in the related passages."
  - [corpus]: Weak - the corpus doesn't contain direct evidence about the mechanism of factual drift.
- Break condition: If editing methods are developed that can make more localized parameter updates, factual drift could be reduced or eliminated.

### Mechanism 3
- Claim: The long-form evaluation protocol captures unique dimensions not measured by short-form metrics because it evaluates consistency across longer text generations.
- Mechanism: Short-form metrics evaluate immediate responses to prompts, while long-form evaluation assesses how edits affect coherence, factual consistency, and topic maintenance over extended text.
- Core assumption: Long-form text generation reveals failure modes in model editing that are not apparent in short-form evaluation.
- Evidence anchors:
  - [abstract]: "Importantly, we find that our protocol has very little relationship with previous short-form metrics...indicating that our method introduces a novel set of dimensions for understanding model editing methods."
  - [section 5.4]: "We only found very weak relationships between the short-form evaluations...and long-form evaluations."
  - [corpus]: Weak - the corpus doesn't contain direct evidence about why long-form evaluation captures unique dimensions.
- Break condition: If short-form metrics are developed that can predict long-form generation quality, the need for separate long-form evaluation might be reduced.

## Foundational Learning

- Concept: Model editing techniques (FT, MEND, ROME, MEMIT, IKE)
  - Why needed here: The paper evaluates and compares these different model editing approaches to understand their impact on long-form generation.
  - Quick check question: What is the key difference between ROME/MEMIT and methods like FT/MEND in terms of how they modify model parameters?

- Concept: Factual consistency vs. edit consistency
  - Why needed here: The paper distinguishes between successfully incorporating an edit (edit consistency) and maintaining other true facts (factual consistency).
  - Quick check question: Why might a model editing method achieve high edit consistency but low factual consistency?

- Concept: Counterfactual updates vs. novel fact injections
  - Why needed here: The paper analyzes how performance differs when editing known facts versus adding completely new information.
- Quick check question: Based on the results, which type of edit (counterfactual or novel fact) appears to be more challenging for model editing methods?

## Architecture Onboarding

- Component map: Dataset construction -> Human evaluation -> Synthetic ratings -> DeBERTaV3 training -> Model editing evaluation -> Results analysis
- Critical path: Dataset → Human ratings → Synthetic ratings → DeBERTaV3 training → Model editing evaluation → Results analysis
- Design tradeoffs:
  - Small human sample size vs. large synthetic dataset for training
  - Single related entity vs. multiple related entities for evaluating scope
  - Focus on factual editing vs. fictional or less harmful editing scenarios
- Failure signatures:
  - Low correlation between synthetic and human ratings
  - High factual drift indicating overly broad parameter updates
  - Poor internal consistency suggesting contradictory generations
- First 3 experiments:
  1. Evaluate a new model editing method using the existing automatic rating system
  2. Compare performance on counterfactual updates vs. novel fact injections for a specific method
  3. Analyze failure modes by manually inspecting lowest-rated samples for a given method

## Open Questions the Paper Calls Out
- What are the precise mechanisms and architectural modifications that would allow model editing techniques to achieve both high edit consistency and factual consistency without suffering from factual drift?
- How does batch editing (applying multiple edits sequentially) or chained editing (editing the same fact multiple times) affect long-form generation quality compared to single-edit scenarios?
- Can the evaluation protocol be extended to assess the impact of model editing on generation tasks beyond paragraph-length outputs, such as multi-paragraph documents or structured text generation?

## Limitations
- The study relies on synthetic training data from GPT-4 to approximate human ratings, with only moderate correlation (r = 0.30-0.40) to actual human evaluations
- The evaluation focuses primarily on factual editing scenarios using Wikidata-derived datasets, potentially missing other important domains or types of edits
- The qualitative analysis is based on a limited set of 200 passages, which may not capture the full diversity of failure modes across different editing techniques and models

## Confidence
- **High Confidence**: The finding that ROME and MEMIT exhibit significant factual drift is well-supported by multiple evaluation metrics and consistent patterns across different datasets and models.
- **Medium Confidence**: The specific ranking of editing methods and their performance differences across various dimensions has moderate confidence due to the reliance on synthetic training data and the limited scope of human evaluation.
- **Low Confidence**: The generalizability of the failure mode analysis to other editing scenarios, domains, or model architectures remains uncertain due to the specific focus on factual editing and limited qualitative sample size.

## Next Checks
1. Conduct a new human evaluation study using a different pool of annotators to validate the DeBERTaV3 model's ratings, particularly focusing on the factual consistency and internal consistency dimensions where the synthetic-human correlation is weakest.
2. Apply the LEME protocol to editing scenarios outside of factual knowledge, such as creative writing or technical documentation, to assess whether the observed patterns of factual drift and edit consistency generalize to other domains.
3. Systematically vary the amount of human-annotated data used in training the rating models to determine the minimum viable human annotation effort required for reliable automatic evaluation, and assess how this impacts the overall findings.