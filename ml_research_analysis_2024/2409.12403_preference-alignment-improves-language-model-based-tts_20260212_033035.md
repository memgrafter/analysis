---
ver: rpa2
title: Preference Alignment Improves Language Model-Based TTS
arxiv_id: '2409.12403'
source_url: https://arxiv.org/abs/2409.12403
tags:
- preference
- arxiv
- speech
- optimization
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies preference alignment methods, specifically Direct
  Preference Optimization (DPO), to language model-based text-to-speech (TTS) systems.
  Starting with a 1.15B parameter baseline model trained on 55k hours of English speech
  data, the authors demonstrate that DPO significantly improves intelligibility (WER),
  speaker similarity (SPK SIM), and proxy subjective evaluation scores (Proxy MOS).
---

# Preference Alignment Improves Language Model-Based TTS

## Quick Facts
- **arXiv ID**: 2409.12403
- **Source URL**: https://arxiv.org/abs/2409.12403
- **Reference count**: 40
- **Primary result**: DPO improves LM-based TTS intelligibility, speaker similarity, and subjective quality with as little as 1 hour of preference data

## Executive Summary
This paper applies Direct Preference Optimization (DPO) to language model-based text-to-speech (TTS) systems, demonstrating significant improvements in intelligibility, speaker similarity, and subjective quality metrics. Starting with a 1.15B parameter baseline model trained on 55k hours of English speech data, the authors show that DPO can enhance performance across multiple dimensions using generated preference pairs. The method proves effective even with minimal preference data (as little as 1 hour) and generalizes well to out-of-domain scenarios. The aligned model outperforms both the baseline and human speech in speaker similarity and subjective quality metrics.

## Method Summary
The method applies Direct Preference Optimization to a 1.15B parameter Multi-Scale Transformer TTS model trained on 55k hours of English speech data. Preference pairs are generated by ranking model outputs using proxy metrics (WER, SPK SIM, Proxy MOS), selecting top and bottom 20% as win-lose pairs. DPO training uses β=0.01, learning rate 3e-7, and large batch sizes (~80k frames) for 350 updates per epoch. The approach eliminates the need for explicit reward modeling by directly optimizing the language model toward human-preferred speech.

## Key Results
- DPO significantly improves WER, SPK SIM, and Proxy MOS scores compared to baseline
- Generated win-lose pairs outperform ground truth vs generated pairs for optimization
- Method works effectively with as little as 1 hour of preference data
- Aligned model generalizes well to out-of-domain VCTK dataset
- Iterative optimization is fragile and fails to improve after first round

## Why This Works (Mechanism)

### Mechanism 1
DPO directly optimizes the TTS model toward human-preferred speech without requiring explicit reward modeling. The method reformulates preference alignment as direct optimization using preference pairs, eliminating intermediate reward model training. This works under the assumption that preferences follow Bradley-Terry model. Break condition occurs when preference pairs don't follow Bradley-Terry model or data is too sparse.

### Mechanism 2
Generated win-lose pairs yield optimal performance compared to using ground truth as reference. When both samples are generated, the optimization explores model capabilities more effectively than trivial ground truth comparisons. This assumes generated samples have sufficient diversity for meaningful distinctions. Break condition occurs when samples become too similar or model collapses.

### Mechanism 3
Preference alignment works effectively with minimal data due to efficient gradient updates. Large batch sizes and careful scheduling enable improvements even with limited preference data. This assumes sparse data contains sufficient signal for model improvement. Break condition occurs when data becomes too sparse for meaningful gradients.

## Foundational Learning

- **Reinforcement Learning from Human Feedback (RLHF)**: Provides theoretical foundation for preference alignment methods. Quick check: What are key differences between RLHF and DPO in terms of implementation complexity?

- **Bradley-Terry model for pairwise comparisons**: Forms mathematical basis for how preferences are modeled in DPO. Quick check: How does Bradley-Terry model handle noisy or inconsistent preference data?

- **Discrete audio tokenization**: TTS models operate on discrete audio codes rather than continuous waveforms. Quick check: What are trade-offs between different audio tokenization schemes (SoundStream vs Encodec)?

## Architecture Onboarding

- **Component map**: Multi-Scale Transformer -> SoundStream codec -> Preference pair selection -> DPO optimizer
- **Critical path**: Generate 10 samples per input → Score using proxy metrics → Select top/bottom 20% → Apply DPO update → Repeat for 350 updates
- **Design tradeoffs**: Generated pairs provide meaningful optimization but may be noisier; larger batches reduce update frequency but provide stable gradients; length normalization helps regularization but may reduce speaker similarity gains
- **Failure signatures**: Model collapse to repetitive outputs; win rates approaching 100% (trivial distinctions); degradation in WER despite subjective metric improvements
- **First 3 experiments**: Compare A2 vs B2 preference pairs; test different β values with/without length normalization; evaluate different metrics individually and combined

## Open Questions the Paper Calls Out

### Open Question 1
How does preference alignment performance scale with model size in LM-based TTS systems? The study focuses on 1.15B parameters without exploring scaling effects.

### Open Question 2
What is the relationship between preference data curation methods and model robustness to out-of-domain data? The paper shows generalization exists but doesn't compare different curation strategies.

### Open Question 3
What are fundamental limitations of iterative preference alignment and can they be overcome? The paper identifies fragility but doesn't analyze underlying causes or propose solutions.

### Open Question 4
How do preference alignment methods interact with different audio tokenization schemes? The study uses SoundStream but doesn't investigate other codecs or codebook configurations.

### Open Question 5
What is optimal balance between automatic metric-based and human preference data? The study uses automatic metrics but acknowledges human data would be ideal without exploring hybrid approaches.

## Limitations

- Lack of ablation studies for individual DPO components - doesn't isolate impact of temperature scaling, batch size effects, or preference pair selection methodology
- Heavy computational requirements - relies on large batch sizes and extensive update schedules that may not be feasible for researchers with limited GPU memory
- Proxy-based evaluation - claims of outperforming human speech are based on proxy MOS scores rather than actual human evaluations

## Confidence

**High Confidence**: DPO improves TTS performance across WER, SPK SIM, and Proxy MOS metrics with clear numerical improvements on both in-domain and out-of-domain datasets.

**Medium Confidence**: DPO works effectively with minimal data (1 hour), though absolute performance levels and preference pair quality at this scale are not fully characterized.

**Low Confidence**: Generated win-lose pairs yield optimal performance compared to other pair selection strategies - based on limited comparisons without exploring full space of methods.

## Next Checks

1. **Ablation Study of DPO Components**: Systematically test individual contributions of temperature scaling, length normalization, and different β values through controlled isolation experiments.

2. **Human Evaluation Validation**: Conduct actual human preference studies to validate proxy MOS scores and verify that model truly outperforms human speech in subjective quality through diverse listener assessments.

3. **Resource Efficiency Analysis**: Replicate experiments with reduced batch sizes and fewer updates to determine minimum computational requirements for effective DPO training and assess practical applicability.