---
ver: rpa2
title: Dynamic Product Image Generation and Recommendation at Scale for Personalized
  E-commerce
arxiv_id: '2408.12392'
source_url: https://arxiv.org/abs/2408.12392
tags:
- product
- image
- generation
- images
- background
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a production system that generates personalized,
  visually appealing product images at scale using latent diffusion models and contextual
  bandits, addressing the challenge of improving user engagement with product recommendations
  in e-commerce. The core method combines Stable Diffusion for background generation
  with ControlNet constraints to minimize artifacts, while a LinUCB contextual bandit
  algorithm personalizes image prompts based on user, item, and placement context.
---

# Dynamic Product Image Generation and Recommendation at Scale for Personalized E-commerce

## Quick Facts
- arXiv ID: 2408.12392
- Source URL: https://arxiv.org/abs/2408.12392
- Authors: Ádám Tibor Czapp; Mátyás Jani; Bálint Domián; Balázs Hidasi
- Reference count: 8
- One-line primary result: Generated backgrounds increased click-through rates by ~15% compared to original product images, with additional ~5% improvement from personalization

## Executive Summary
This work presents a production system that generates personalized, visually appealing product images at scale using latent diffusion models and contextual bandits to improve user engagement with product recommendations in e-commerce. The system combines Stable Diffusion with ControlNet constraints to minimize artifacts while placing products in appropriate environments, and uses a LinUCB contextual bandit algorithm to personalize image prompts based on user, item, and placement context. Online A/B testing demonstrated significant performance gains, with the system successfully scaling to product catalogs of several thousand items, primarily in the apparel category.

## Method Summary
The system generates personalized product images by first detecting and masking the product using object detection services, then using Stable Diffusion with ControlNet to generate backgrounds around the product edges. A LinUCB contextual bandit algorithm selects prompts based on user, item, and placement features to optimize estimated click-through rates. Generated images are cached and served asynchronously, with original images provided immediately while background generation completes in the background. The pipeline was tested in production with apparel products, showing significant improvements in user engagement metrics.

## Key Results
- Generated backgrounds increased click-through rates by approximately 15% compared to original product images
- Additional ~5% improvement from pipeline refinements and ~5% from personalization via LinUCB contextual bandits
- System successfully scaled to product catalogs of several thousand items, primarily in the apparel category
- Performance gains demonstrated across multiple independent experiments with statistical significance (p < 0.05)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using ControlNet with edge constraints reduces artifacts compared to naive inpainting
- Mechanism: ControlNet constrains the diffusion process to respect product edges, preventing the model from extending or modifying the product region
- Core assumption: Edge detection reliably captures product boundaries that can guide background generation
- Evidence anchors:
  - [abstract] "Our pipeline uses the edges of the product as the constraint in ControlNet"
  - [section] "A better solution is to utilize ControlNet [8], which allows for the injection of additional constraints into the image generation process. Our pipeline uses the edges of the product as the constraint in ControlNet"
- Break condition: Poor edge detection quality, highly complex product shapes that cannot be well represented by edges, or products with transparent/translucent elements

### Mechanism 2
- Claim: Personalizing backgrounds using LinUCB contextual bandits increases CTR by matching user preferences
- Mechanism: LinUCB algorithm selects prompts based on user, item, and placement features to optimize estimated CTR
- Core assumption: User preferences for background styles vary systematically with context and can be learned from interaction data
- Evidence anchors:
  - [abstract] "A LinUCB contextual bandit algorithm personalizes image prompts based on user, item, and placement context"
  - [section] "Different users find different variations attractive and preferences might be influenced by where the recommendation is shown"
- Break condition: Insufficient interaction data for certain contexts, highly personalized preferences that don't follow systematic patterns, or changes in user preferences over time

### Mechanism 3
- Claim: Generated backgrounds increase engagement by making product images more eye-catching than original images
- Mechanism: Placing products in appropriate environments creates visually appealing compositions that attract user attention
- Core assumption: Users respond more positively to products shown in context-appropriate settings compared to isolated product images
- Evidence anchors:
  - [abstract] "generated backgrounds increased click-through rates by approximately 15% compared to original product images"
  - [section] "Creatives of ad campaigns are often designed with great care, but this approach does not scale for product level ad campaigns"
- Break condition: Backgrounds distract from products rather than enhance them, or users prefer minimalist/clean product presentations

## Foundational Learning

- Concept: Diffusion models and stable diffusion architecture
  - Why needed here: Understanding how the background generation works and what parameters can be tuned
  - Quick check question: What is the difference between text-to-image generation and image-to-image generation in diffusion models?

- Concept: Contextual bandit algorithms (LinUCB specifically)
  - Why needed here: Personalizing prompt selection based on user context requires understanding of bandit frameworks
  - Quick check question: How does LinUCB balance exploration and exploitation when selecting prompts?

- Concept: Computer vision for object detection and masking
  - Why needed here: Pipeline requires reliable product detection and background removal
  - Quick check question: What are common failure modes for object detection in e-commerce product images?

## Architecture Onboarding

- Component map:
  Object detection service -> ControlNet pipeline (edge detection + background generation) -> Image caching layer -> LinUCB bandit service (for prompt selection) -> Asynchronous processing queue (for generation jobs) -> Callback system (to update cache after generation)

- Critical path:
  1. Product recommendation request
  2. LinUCB prompt selection based on context
  3. Check cache for existing generated image
  4. If not cached, queue generation job
  5. Serve original image immediately
  6. Background generation completes asynchronously
  7. Callback updates cache
  8. Future requests use generated image

- Design tradeoffs:
  - Immediate response vs. perfect image quality (serving original image while generating)
  - Prompt variety vs. prompt quality (more prompts = more personalization but lower quality per prompt)
  - Generation speed vs. image quality (faster generation = less computational cost but potentially lower quality)

- Failure signatures:
  - Edge detection failures → artifacts around product boundaries
  - Bandits selecting inappropriate prompts → poor CTR
  - Cache misses causing repeated generation → increased costs
  - Background generation timeouts → original images served long-term

- First 3 experiments:
  1. A/B test comparing generated vs original images for CTR
  2. Bandit learning rate test (exploration vs exploitation tradeoff)
  3. Pipeline performance test (latency, cost, cache hit rate)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the background generation pipeline handle cases where the product has transparent or semi-transparent elements?
- Basis in paper: [inferred] The paper mentions that the original product is cut back onto the generated image in step 5, but doesn't address transparency handling.
- Why unresolved: The paper doesn't discuss how the system deals with products that have transparent or semi-transparent areas, which could affect the final image quality.
- What evidence would resolve it: Testing the system with products containing transparent elements and analyzing the resulting images for artifacts or quality issues.

### Open Question 2
- Question: What is the impact of the background generation system on conversion rates beyond the observed CTR improvements?
- Basis in paper: [explicit] The paper mentions that other metrics like conversions and CPA also improved as an indirect effect of driving more users to the merchant's site, but doesn't provide specific data.
- Why unresolved: The paper only reports CTR improvements and mentions that other metrics improved, but doesn't provide detailed analysis of conversion rate impacts.
- What evidence would resolve it: Detailed A/B test results showing conversion rate changes and cost per acquisition (CPA) for original vs. generated product images.

### Open Question 3
- Question: How does the system perform with product categories outside of apparel?
- Basis in paper: [inferred] The paper mentions that most products tested were in the apparel category, but doesn't provide results for other categories.
- Why unresolved: The evaluation focused primarily on apparel products, leaving the performance of the system for other product categories unexplored.
- What evidence would resolve it: Testing the background generation system with various product categories (e.g., electronics, furniture, books) and comparing the CTR improvements across categories.

## Limitations
- The system's performance with product categories outside of apparel remains unexplored, limiting generalizability claims
- Computational costs and latency implications of the background generation pipeline are not quantified
- The ~5% personalization improvement claim lacks detailed analysis of prompt diversity and optimal feature selection

## Confidence

**High Confidence**: The core claim that generated backgrounds improve CTR compared to original product images is well-supported by the multi-experiment design with statistical significance. The mechanism of using ControlNet with edge constraints to reduce artifacts is technically sound and directly implementable.

**Medium Confidence**: The ~5% improvement from personalization via LinUCB contextual bandits is plausible but less rigorously validated. The paper provides results but limited analysis of why certain prompts work better for specific contexts or how robust the personalization is across different product categories.

**Low Confidence**: The scalability claims for catalogs of "several thousand items" lack quantitative backing. The paper doesn't address computational costs, cache hit rates, or how the system performs with much larger catalogs (e.g., hundreds of thousands of products) or different product types beyond apparel.

## Next Checks

1. **Cross-product Category Validation**: Test the system on non-apparel categories (electronics, home goods, furniture) to verify the 15% baseline improvement generalizes beyond clothing items. This would reveal whether the approach is truly category-agnostic or benefits from apparel-specific prompt design.

2. **Bandit Algorithm Robustness**: Conduct ablation studies varying the number of context features, prompt diversity, and LinUCB hyperparameters to determine the sensitivity of personalization improvements to these design choices. This would clarify whether the ~5% improvement is robust or highly parameter-dependent.

3. **Long-term Performance Analysis**: Implement monitoring to track CTR changes over time, measuring whether user preferences for generated backgrounds remain stable or if there's adaptation/learning effects that could degrade performance. This would address concerns about temporal dynamics in user engagement with personalized images.