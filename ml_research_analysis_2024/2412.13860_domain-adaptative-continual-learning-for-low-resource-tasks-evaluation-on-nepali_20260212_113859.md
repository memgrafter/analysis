---
ver: rpa2
title: 'Domain-adaptative Continual Learning for Low-resource Tasks: Evaluation on
  Nepali'
arxiv_id: '2412.13860'
source_url: https://arxiv.org/abs/2412.13860
tags:
- nepali
- data
- language
- base
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates domain-adaptive continual learning (DAPT)\
  \ for low-resource languages using Nepali as a case study. The authors pretrain\
  \ Llama 3 8B using synthetic parallel Nepali\u2013English data via QLoRA, followed\
  \ by instruction finetuning on translated datasets."
---

# Domain-adaptative Continual Learning for Low-resource Tasks: Evaluation on Nepali

## Quick Facts
- **arXiv ID:** 2412.13860
- **Source URL:** https://arxiv.org/abs/2412.13860
- **Reference count:** 13
- **Primary result:** Llama 3 8B adapted with synthetic Nepali-English data shows improved Nepali generation but suffers catastrophic forgetting of English benchmarks.

## Executive Summary
This paper investigates domain-adaptive continual learning (DAPT) for low-resource languages using Nepali as a case study. The authors pretrain Llama 3 8B using synthetic parallel Nepaliâ€“English data via QLoRA, followed by instruction finetuning on translated datasets. Evaluation shows the adapted model generates grammatically correct Nepali text and performs better than the base model in Nepali language generation. However, catastrophic forgetting is observed, with reduced performance on English benchmarks. Analysis of attention heatmaps reveals improved dependency resolution for Nepali. Few-shot prompting mitigates forgetting, with the adapted model showing greater performance gains compared to the base model.

## Method Summary
The authors generate synthetic parallel Nepali-English data using translation models (NLLB, IndicTrans2), then perform 4-bit QLoRA continual pretraining of Llama 3 8B on this data. They employ two training strategies: translation task and bilingual next token prediction. The model is then fine-tuned on translated instruction sets using a smaller QLoRA adapter. Evaluation includes GPT-4o scoring for Nepali text quality and standard English benchmarks (MMLU, ARC, Winogrande, TruthfulQA), with attention heatmap analysis for dependency resolution.

## Key Results
- Adapted model generates grammatically correct Nepali text with improved dependency resolution compared to base model
- Catastrophic forgetting observed: performance drops on English benchmarks (MMLU, ARC, Winogrande, TruthfulQA)
- Few-shot prompting mitigates forgetting, with adapted model showing greater performance gains than base model
- Attention heatmap analysis reveals improved dependency resolution for Nepali in adapted model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic parallel Nepali-English data enables domain adaptation of Llama 3 8B for low-resource languages.
- Mechanism: By using translation models to generate parallel Nepali-English text, the model learns to generate Nepali text while leveraging existing English knowledge, reducing the need for large Nepali corpora.
- Core assumption: Synthetic data is sufficiently high quality to align the model to Nepali without introducing significant noise or biases.
- Evidence anchors: [abstract], [section]
- Break condition: If the quality of synthetic data is too low, the model may learn incorrect patterns or fail to adapt effectively.

### Mechanism 2
- Claim: QLoRA with rank 128 for pretraining and rank 16 for finetuning balances parameter efficiency with performance gains.
- Mechanism: Low-rank adaptation reduces the number of trainable parameters, making training feasible on limited resources while retaining most of the model's knowledge.
- Core assumption: The low-rank matrices can capture the necessary domain-specific information without significant loss of performance.
- Evidence anchors: [abstract], [section]
- Break condition: If the rank is too low, the model may not learn sufficient domain-specific information, leading to poor performance.

### Mechanism 3
- Claim: Bilingual next token prediction task improves Nepali language understanding compared to standard token prediction.
- Mechanism: By training the model to predict the next token in alternating Nepali and English sentences, it leverages existing English knowledge to improve Nepali understanding.
- Core assumption: The model can effectively transfer knowledge between languages in a bilingual setting.
- Evidence anchors: [abstract], [section]
- Break condition: If the model cannot effectively transfer knowledge between languages, the bilingual task may not provide significant benefits over standard token prediction.

## Foundational Learning

- Concept: Low-rank adaptation (LoRA/QLoRA)
  - Why needed here: Enables efficient fine-tuning of large language models on limited resources.
  - Quick check question: What is the primary advantage of using LoRA over full fine-tuning in terms of computational efficiency?

- Concept: Synthetic data generation for low-resource languages
  - Why needed here: Provides sufficient training data for languages with limited available corpora.
  - Quick check question: What are the potential risks of using synthetic data for training language models?

- Concept: Attention mechanism and self-attention heatmaps
  - Why needed here: Allows analysis of how the model processes and relates different parts of the input, particularly for understanding Nepali syntax.
  - Quick check question: How can attention heatmaps be used to interpret a model's understanding of dependency relations in a language?

## Architecture Onboarding

- Component map: Llama 3 8B base model -> QLoRA adapter (4-bit quantization) -> Synthetic data generation pipeline (NLLB, IndicTrans2) -> Training loop (translation task, bilingual next token prediction) -> Evaluation framework (LM Evaluation Harness, BertViz)
- Critical path: 1. Generate synthetic parallel Nepali-English data 2. Perform QLoRA pretraining with translation task 3. Perform QLoRA pretraining with bilingual next token prediction 4. Perform QLoRA finetuning on instruction sets 5. Evaluate model performance and analyze attention heatmaps
- Design tradeoffs:
  - Synthetic vs. organic data: Synthetic data enables training but may introduce biases or noise.
  - QLoRA rank selection: Higher rank may improve performance but increase computational requirements.
  - Bilingual vs. monolingual training: Bilingual training leverages existing knowledge but may complicate the learning process.
- Failure signatures:
  - Poor performance on Nepali benchmarks: Indicates ineffective adaptation or low-quality synthetic data.
  - Catastrophic forgetting of English: Suggests insufficient retention of base model knowledge during adaptation.
  - Uninterpretable attention heatmaps: May indicate issues with the model's understanding of Nepali syntax.
- First 3 experiments:
  1. Generate synthetic parallel data and evaluate its quality using BLEU and chrF++ scores.
  2. Perform QLoRA pretraining with translation task and evaluate on Nepali text generation.
  3. Perform QLoRA pretraining with bilingual next token prediction and compare performance to translation task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does using parallel data (Nepali-English) for domain-adaptive continual pretraining provide better results than using Nepali-only data?
- Basis in paper: [explicit] The paper uses parallel data instead of Nepali-only text for continual pretraining, stating the aim is to align the model to Nepali using its existing knowledge of English.
- Why unresolved: The paper does not compare the effectiveness of parallel data against Nepali-only data for DAPT. It only reports results using the parallel data approach.
- What evidence would resolve it: A controlled experiment comparing model performance using parallel data versus Nepali-only data for DAPT, measuring factors like generation quality, catastrophic forgetting, and linguistic knowledge acquisition.

### Open Question 2
- Question: How does the choice of translation model (e.g., NLLB vs. IndicTrans2) impact the quality of synthetic parallel data and subsequent model performance?
- Basis in paper: [explicit] The paper discusses the choice between NLLB and IndicTrans2 for translation, ultimately selecting NLLB. It mentions comparing BLEU scores but does not explore how this choice affects downstream model performance.
- Why unresolved: The paper does not investigate the impact of different translation models on the quality of synthetic data or the resulting model's performance.
- What evidence would resolve it: Experiments comparing model performance when trained on synthetic data generated using different translation models, assessing factors like generation quality, linguistic knowledge, and catastrophic forgetting.

### Open Question 3
- Question: Does increasing the number of shots during evaluation consistently mitigate catastrophic forgetting in DAPT models for low-resource languages?
- Basis in paper: [explicit] The paper observes that increasing the number of shots during evaluation yields better percent increases in the adapted model compared to the base model, suggesting latent retention.
- Why unresolved: The paper does not explore whether this trend holds consistently across different benchmarks, languages, or model architectures. It only presents initial findings for the specific case of Nepali and Llama 3 8B.
- What evidence would resolve it: Extensive experiments evaluating DAPT models for various low-resource languages across multiple benchmarks with varying shot counts, analyzing the consistency of performance gains and the relationship to catastrophic forgetting.

## Limitations

- The evaluation framework relies heavily on GPT-4o for Nepali text quality assessment, introducing potential bias and limiting reproducibility.
- The study does not report on the actual quality of synthetic parallel data beyond translation scores, leaving open questions about potential noise or biases.
- Catastrophic forgetting of English benchmarks is identified but the mitigation strategy (few-shot prompting) is presented without systematic ablation studies to quantify its effectiveness.

## Confidence

- **High Confidence:** The model demonstrates improved Nepali text generation capabilities compared to the base Llama 3 8B model, supported by multiple evaluation metrics and the attention heatmap analysis showing improved dependency resolution.
- **Medium Confidence:** The QLoRA adaptation approach with synthetic data is effective for domain adaptation to low-resource languages, based on observed performance gains in Nepali generation despite the catastrophic forgetting issue.
- **Low Confidence:** The claim that few-shot prompting effectively mitigates catastrophic forgetting is supported by limited evidence, with no quantitative comparison of performance before and after applying few-shot prompting across different benchmark types.

## Next Checks

1. **Synthetic Data Quality Validation:** Conduct human evaluation of a sample of synthetic parallel Nepali-English pairs to assess translation quality, identify potential biases, and measure the correlation between synthetic data quality and model performance on Nepali generation tasks.

2. **Catastrophic Forgetting Quantification:** Systematically measure the performance drop on English benchmarks across different domains (MMLU, ARC, Winogrande) before and after adaptation, then quantify the performance recovery achieved through few-shot prompting on each task type.

3. **Attention Mechanism Analysis:** Compare attention heatmaps between the adapted model and the base model on identical Nepali sentences to identify specific patterns of improved dependency resolution, and correlate these patterns with grammatical correctness scores from the GPT-4o evaluation.