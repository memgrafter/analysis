---
ver: rpa2
title: Context and System Fusion in Post-ASR Emotion Recognition with Large Language
  Models
arxiv_id: '2410.03312'
source_url: https://arxiv.org/abs/2410.03312
tags:
- context
- emotion
- punc
- speech
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates large language model prompting strategies
  for post-ASR speech emotion recognition without relying on audio signals or ground
  truth transcripts. The authors explore ASR transcript ranking using multiple metrics,
  conversation context modeling with variable window sizes, and fusion of multiple
  ASR outputs to improve emotion classification accuracy.
---

# Context and System Fusion in Post-ASR Emotion Recognition with Large Language Models

## Quick Facts
- arXiv ID: 2410.03312
- Source URL: https://arxiv.org/abs/2410.03312
- Authors: Pavel Stepachev; Pinzhen Chen; Barry Haddow
- Reference count: 38
- Primary result: 75.1% accuracy on IEMOCAP using LLM-based post-ASR emotion recognition, surpassing baseline by 20 percentage points

## Executive Summary
This paper investigates large language model (LLM) prompting strategies for post-ASR speech emotion recognition without relying on audio signals or ground truth transcripts. The authors explore ASR transcript ranking using multiple metrics, conversation context modeling with variable window sizes, and fusion of multiple ASR outputs to improve emotion classification accuracy. Their best configuration achieves 75.1% accuracy on the IEMOCAP dataset, significantly outperforming the provided baseline. The study finds that conversation context has diminishing returns beyond certain window sizes and that the metric used to select transcripts is crucial for performance.

## Method Summary
The authors propose a training-free approach to post-ASR speech emotion recognition using LLM prompting. They rank ASR transcripts using string-based metrics (chrF, chrF++, MER, WER, WIL, WIP) and evaluate conversation context with variable window sizes (CW=0,2,4,8,16,32,64). The method includes system fusion strategies and compares GPT-4o against GPT-3.5-turbo for emotion classification. All components are implemented without fine-tuning, relying solely on prompt engineering and transcript ranking to improve classification accuracy on the 4-class emotion task.

## Key Results
- Achieved 75.1% accuracy on IEMOCAP dataset, surpassing baseline by 20 percentage points
- GPT-4o consistently outperforms GPT-3.5-turbo across all context sizes tested
- Conversation context shows diminishing returns beyond window sizes of 16-32 utterances
- Transcript ranking metric choice significantly impacts final performance, with chrF++ showing strong results

## Why This Works (Mechanism)
The approach leverages LLMs' ability to understand context and emotional nuance from text alone, bypassing the need for audio signals or ground truth transcripts. By ranking multiple ASR outputs and incorporating conversation context, the method captures emotional patterns that single ASR systems might miss. The LLM's reasoning capabilities enable it to interpret emotional content from imperfect transcripts while the system fusion approach combines strengths of different ASR outputs.

## Foundational Learning

1. **ASR Transcript Ranking Metrics** - Needed for selecting the most reliable transcripts from multiple ASR systems; quick check: verify metric implementations produce expected scores on sample transcripts.

2. **Conversation Context Windowing** - Required to understand how much historical context improves emotion recognition; quick check: test different window sizes on sample conversations to observe performance changes.

3. **LLM Prompt Engineering** - Essential for effective emotion classification from text; quick check: validate prompt templates produce consistent outputs across different input variations.

## Architecture Onboarding

**Component Map**: ASR Systems → Ranking Metrics → Context Window → LLM Prompt → Emotion Classification

**Critical Path**: The core workflow involves ranking ASR transcripts using multiple string-based metrics, selecting transcripts based on context window size, and querying the LLM with carefully constructed prompts that include conversation context.

**Design Tradeoffs**: Training-free approach avoids overfitting but may underperform compared to fine-tuned models. Multiple ASR systems increase robustness but add complexity. Larger context windows provide more information but may introduce noise.

**Failure Signatures**: Poor transcript ranking leads to low accuracy; incorrect context window selection degrades performance; LLM API issues or quota limits prevent execution.

**First Experiments**:
1. Test basic LLM prompting with single ASR transcript to establish baseline performance
2. Implement and validate all string-based ranking metrics on sample transcripts
3. Experiment with different context window sizes (CW=0,2,4,8) to observe impact on accuracy

## Open Questions the Paper Calls Out

1. How does the performance of the proposed LLM-based approach compare to traditional machine learning models that use audio signals and ground truth transcripts for speech emotion recognition? The paper doesn't provide direct comparisons to traditional methods that use audio and ground truth transcripts.

2. How does the performance of the proposed approach vary across different languages and cultural contexts, beyond the English-speaking IEMOCAP dataset used in this study? The study only evaluates on one dataset, limiting understanding of cross-lingual performance.

3. How does the choice of LLM model (e.g., GPT-4o vs. GPT-3.5-turbo) impact the performance of the proposed approach, and are there other LLM models that could potentially yield even better results? While GPT-4o vs GPT-3.5-turbo is compared, other models are not explored.

## Limitations

- Relies entirely on OpenAI's GPT-4o API, creating potential reproducibility issues and ongoing costs
- Single dataset focus (IEMOCAP) limits generalizability to other domains or languages
- Training-free approach may not achieve optimal performance compared to fine-tuned models
- Conversation context benefits diminish after certain window sizes, suggesting potential limitations

## Confidence

- High confidence in core methodology and experimental results (sufficient implementation details provided)
- Medium confidence in generalizability to other datasets or languages (single dataset focus)
- Medium confidence in cost-effectiveness (API costs not explicitly discussed)
- Low confidence in scalability to real-time applications (latency and token costs not addressed)

## Next Checks

1. Verify the implementation of all string-based metrics (chrF, chrF++, MER, WIL, WIP) using both JiWER and sacreBLEU packages to ensure accurate transcript ranking

2. Test the LLM prompting approach on a different emotion dataset (e.g., MSP-IMPROV or DailyDialog) to assess cross-dataset generalization

3. Measure the actual token usage and cost per prediction when using GPT-4o API to evaluate practical deployment feasibility