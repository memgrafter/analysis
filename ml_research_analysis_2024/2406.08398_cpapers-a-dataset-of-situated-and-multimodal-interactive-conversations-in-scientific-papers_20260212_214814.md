---
ver: rpa2
title: 'cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in
  Scientific Papers'
arxiv_id: '2406.08398'
source_url: https://arxiv.org/abs/2406.08398
tags:
- table
- arxiv
- computational
- dataset
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces cPAPERS, a dataset of 5,030 question-answer
  pairs situated in equations, figures, and tables from scientific papers. Questions
  and answers are sourced from reviews and rebuttals on OpenReview, linked with corresponding
  arXiv papers.
---

# cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers

## Quick Facts
- arXiv ID: 2406.08398
- Source URL: https://arxiv.org/abs/2406.08398
- Reference count: 40
- Dataset contains 5,030 QA pairs from peer reviews grounded in equations, figures, and tables from scientific papers

## Executive Summary
This paper introduces cPAPERS, a dataset designed to advance conversational AI for scientific documents by providing 5,030 question-answer pairs sourced from peer reviews on OpenReview and grounded in equations, figures, and tables from arXiv papers. The dataset addresses the challenge of situated, multimodal conversational AI by linking authentic technical questions to specific document components through LaTeX parsing. Zero-shot and fine-tuned LLMs demonstrate that using neighboring contextual information (rather than all multimodal content) significantly improves performance, while fine-tuning on specific modalities yields only modest gains. This work establishes a foundation for developing conversational assistants capable of understanding and responding to scientific inquiries with proper multimodal grounding.

## Method Summary
The cPAPERS dataset was created by collecting question-answer pairs from peer reviews on OpenReview and matching them to corresponding equations, figures, and tables in LaTeX source files from arXiv papers. The data collection pipeline involved using regex to identify references to multimodal elements in reviews, extracting these elements from LaTeX sources, and having crowdworkers validate the quality of the dataset. For baseline evaluation, zero-shot prompting with LLaMA-2-70B and fine-tuning LLaMA-2-7B with QLoRA were performed, with contexts provided either as neighboring elements or all available content. The models were evaluated using ROUGE-1/2/L, METEOR, and BERTScore metrics.

## Key Results
- Using neighboring contextual information (equations/tables) significantly outperforms using all multimodal content, with up to 2x improvement in ROUGE, METEOR, and BERTScores
- Fine-tuning on specific modalities (equations, tables, figures) yields only modest gains in performance
- Zero-shot prompting with LLMs shows competitive results, particularly when provided with relevant neighboring context
- Questions in reviews often reference document elements as supplementary information rather than primary content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The cPAPERS dataset enables effective training of multimodal conversational agents by providing high-quality, domain-specific question-answer pairs grounded in scientific document components.
- Mechanism: The dataset collection process leverages peer reviews from OpenReview, ensuring that questions are authentic, technical, and grounded in specific document elements (equations, figures, tables). By linking these QA pairs to LaTeX sources from arXiv, the dataset provides both the conversational context and the multimodal grounding necessary for training models that can understand and respond to scientific inquiries.
- Core assumption: The peer review process generates questions that are both technically meaningful and representative of real scientific discourse.
- Evidence anchors:
  - [abstract]: "Question-answer pairs are sourced from reviews and rebuttals from OpenReview... associated with contextual information from LaTeX source files."
  - [section]: "Leveraging reviews of academic papers grounded in equations, figures, and tables... this dataset aims to further advance the development of conversational assistants capable of situated and multimodal interactive conversation."
  - [corpus]: Weak evidence; corpus shows related work on conversational agents and multimodal datasets, but none specifically use peer review-derived QA pairs for scientific documents.
- Break condition: If peer review questions are too domain-specific or contain insufficient grounding references, models trained on this data may fail to generalize to broader scientific contexts.

### Mechanism 2
- Claim: Zero-shot performance improves significantly when using neighboring multimodal context rather than all available content, due to reduced context noise and better retrieval alignment.
- Mechanism: The dataset structure allows models to be provided with a small, relevant subset of multimodal elements (e.g., equation i-1, i, i+1) rather than the entire paper's equations or tables. This targeted context helps the model focus on relevant information while avoiding distraction from unrelated content, leading to improved ROUGE, METEOR, and BERTScore metrics.
- Core assumption: Language models can effectively retrieve and utilize weakly-grounded context when provided with a small, relevant subset of multimodal elements.
- Evidence anchors:
  - [abstract]: "Improved performance when using neighboring contextual information (equations/tables) rather than all multimodal content."
  - [section]: "Utilizing the neighboring equations/tables provides a significant improvement over using all equations/tables to answer questions in the dataset, evidenced by up to 2x improvement in ROUGE, METEOR, and BERTScores."
  - [corpus]: Weak evidence; related work on multimodal QA exists, but none specifically demonstrate the effectiveness of neighboring context for scientific document understanding.
- Break condition: If the neighboring context does not contain the necessary information to answer the question, models may fail despite the improved retrieval setup.

### Mechanism 3
- Claim: Fine-tuning on specific modalities yields modest gains because questions in reviews often reference document elements as supplementary information rather than primary content.
- Mechanism: Analysis of the dataset shows that while questions may reference equations, tables, or figures, answers more frequently reference these elements. This suggests that reviewers use multimodal elements to support their questions rather than making them the central focus. As a result, fine-tuning on specific modalities provides limited benefit since the core task involves understanding the textual question and answer rather than the multimodal grounding.
- Core assumption: The nature of review questions prioritizes textual understanding over multimodal comprehension.
- Evidence anchors:
  - [abstract]: "Fine-tuning on specific modalities yielded modest gains, highlighting the dataset's potential for advancing multimodal conversational assistants in scientific domains."
  - [section]: "The results from finetuning cPAPERS-TBLS and cPAPERS-FIGS... indicate that using just the question results in the best performance across most metrics."
  - [corpus]: Weak evidence; related work on multimodal QA exists, but none specifically analyze the relationship between question focus and multimodal grounding in review-derived datasets.
- Break condition: If future review datasets show questions that more directly target multimodal elements, fine-tuning on specific modalities may yield greater improvements.

## Foundational Learning

- Concept: LaTeX document structure and parsing
  - Why needed here: Understanding how equations, figures, and tables are represented in LaTeX is crucial for extracting the correct multimodal context from arXiv sources and matching them to questions from OpenReview.
  - Quick check question: Given a LaTeX source file, can you identify the environments used for equations (\begin{equation}), tables (\begin{tabular}), and figures (\begin{figure})?

- Concept: Regular expressions for pattern matching
  - Why needed here: Regex is used extensively in the dataset collection process to identify references to equations, tables, and figures in review text, as well as to extract these elements from LaTeX sources.
  - Quick check question: Write a regex pattern that matches references to equations in the format "Equation 3" or "Eq. (4)" regardless of case.

- Concept: Evaluation metrics for text generation
  - Why needed here: Understanding ROUGE, METEOR, and BERTScore is essential for interpreting the baseline results and comparing model performance on the cPAPERS dataset.
  - Quick check question: What is the key difference between ROUGE-1 and ROUGE-2, and why might ROUGE-2 be more challenging for conversational QA tasks?

## Architecture Onboarding

- Component map: OpenReview API -> LaTeX source retrieval -> Regex filtering -> LLM extraction -> Crowdworker validation
- Critical path: Data collection -> Context extraction -> Model training -> Evaluation
  The most time-consuming step is typically data collection and validation, particularly ensuring accurate matching between OpenReview references and LaTeX sources.
- Design tradeoffs:
  - Using all multimodal content vs. neighboring context: Completeness vs. noise reduction
  - LaTeX vs. compiled PDF parsing: Source availability vs. visual fidelity
  - Peer review vs. synthetic question generation: Authenticity vs. control
- Failure signatures:
  - Low evaluation scores across all metrics: Potential issues with data quality or model architecture
  - High scores on ROUGE-1 but low on ROUGE-2: Model may be generating relevant but not coherent responses
  - Disproportionate improvement with neighboring context: Context length limitations in the model
- First 3 experiments:
  1. Zero-shot evaluation with all multimodal content vs. neighboring context to verify the performance difference
  2. Ablation study removing context entirely to measure its contribution to performance
  3. Fine-tuning on each modality separately to confirm the modest gains observed in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on cPAPERS vary across different scientific domains or fields of study?
- Basis in paper: [inferred] The paper mentions that the dataset spans 2350 unique scientific papers, suggesting potential domain variations.
- Why unresolved: The paper does not analyze performance differences across scientific domains or fields of study.
- What evidence would resolve it: Performance metrics (ROUGE, METEOR, BERTScore) broken down by scientific field or domain for each modality (equations, tables, figures).

### Open Question 2
- Question: What is the impact of using different types of context (neighboring equations/tables vs. all equations/tables) on LLM performance for each specific scientific domain?
- Basis in paper: [explicit] The paper discusses the use of neighboring vs. all multimodal content and its impact on performance.
- Why unresolved: The paper does not explore how this impact varies across different scientific domains.
- What evidence would resolve it: Comparative analysis of LLM performance using different context types across various scientific domains for each modality.

### Open Question 3
- Question: How does the quality and relevance of referring text affect the performance of LLMs on cPAPERS?
- Basis in paper: [explicit] The paper mentions providing "context" and "references" for each question-answer pair.
- Why unresolved: The paper does not investigate the relationship between the quality/relevance of referring text and LLM performance.
- What evidence would resolve it: Correlation analysis between the quality/relevance of referring text (e.g., based on semantic similarity or manual annotation) and LLM performance metrics for each modality.

## Limitations

- The dataset's reliance on peer review-derived questions may not fully represent the diversity of scientific inquiry beyond the review context
- Modest gains from fine-tuning on specific modalities suggest the current formulation of the task may not fully exploit the multimodal nature of the data
- The use of zero-shot and few-shot learning approaches, while practical, may not capture the full potential of the dataset for more complex conversational scenarios

## Confidence

- High confidence: The claim that cPAPERS enables training of multimodal conversational agents for scientific documents is supported by the dataset's size (5,030 QA pairs) and the structured collection process from authentic peer reviews
- Medium confidence: The claim that neighboring context outperforms all multimodal content is demonstrated through automatic metrics (ROUGE, METEOR, BERTScore) but not validated through human evaluation or qualitative analysis of model outputs
- Medium confidence: The claim that fine-tuning on specific modalities yields modest gains is supported by the results showing marginal improvements, but lacks detailed analysis of why questions reference document elements as supplementary rather than primary information

## Next Checks

1. Conduct a comprehensive human evaluation comparing model responses generated with all multimodal content versus neighboring context to validate the automatic metric improvements and assess qualitative differences in response quality

2. Perform detailed error analysis on the fine-tuned models to understand why gains are modest, examining whether questions that reference document elements as supplementary information are the primary cause of limited improvement

3. Evaluate model performance on questions that more directly target multimodal elements (e.g., "What does Equation 3 show?" vs. "How does Equation 3 support your argument?") to determine if the dataset's current formulation limits the potential benefits of fine-tuning on specific modalities