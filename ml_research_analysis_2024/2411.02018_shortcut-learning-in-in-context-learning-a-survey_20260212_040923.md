---
ver: rpa2
title: 'Shortcut Learning in In-Context Learning: A Survey'
arxiv_id: '2411.02018'
source_url: https://arxiv.org/abs/2411.02018
tags:
- llms
- shortcuts
- https
- shortcut
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively reviews shortcut learning in Large
  Language Models'' In-Context Learning (ICL), identifying two main types: instinctive
  shortcuts (inherent model biases like vanilla-label bias, context-label bias, domain-label
  bias, and reasoning-label bias) and acquired shortcuts (patterns captured from demonstration
  examples, including lexicon, concept, overlap, position, text style, and group dynamics).
  The paper discusses causes of shortcut learning from LLMs training (pre-training
  data distribution, instruction tuning), skewed demonstrations, and model size.'
---

# Shortcut Learning in In-Context Learning: A Survey

## Quick Facts
- arXiv ID: 2411.02018
- Source URL: https://arxiv.org/abs/2411.02018
- Reference count: 40
- Authors: Rui Song; Yingji Li; Lida Shi; Fausto Giunchiglia; Hao Xu

## Executive Summary
This survey systematically examines shortcut learning in Large Language Models' In-Context Learning (ICL), identifying two main types: instinctive shortcuts (inherent model biases) and acquired shortcuts (patterns from demonstration examples). The paper provides a comprehensive taxonomy of shortcut types, their causes, evaluation benchmarks, and mitigation strategies. It highlights the limitations of current ICL evaluation methods and calls for more robust benchmarks and better understanding of shortcut mechanisms.

## Method Summary
The paper conducts a comprehensive literature review of shortcut learning in ICL, organizing existing research into a systematic framework. It categorizes shortcuts into instinctive (vanilla-label bias, context-label bias, domain-label bias, reasoning-label bias) and acquired types (lexicon, concept, overlap, position, text style, and group dynamics). The survey synthesizes findings from 40+ references to create a taxonomy of causes (LLMs training, skewed demonstrations, model size), evaluation methods, and mitigation strategies (data-centric, model-centric, and prompt-centric approaches).

## Key Results
- Identifies two main categories of shortcuts: instinctive (inherent model biases) and acquired (learned from demonstrations)
- Systematically categorizes shortcut types including vanilla-label bias, context-label bias, domain-label bias, and reasoning-label bias
- Proposes three categories of mitigation strategies: data-centric, model-centric, and prompt-centric approaches
- Highlights evaluation limitations focusing primarily on English and limited task types

## Why This Works (Mechanism)
The paper's framework works by systematically categorizing shortcut learning behaviors observed in ICL into two distinct types based on their origins. Instinctive shortcuts arise from inherent model biases developed during pre-training, while acquired shortcuts are learned patterns extracted from demonstration examples. This dual categorization enables targeted mitigation strategies by identifying whether the shortcut stems from the model's internal biases or from specific patterns in the provided demonstrations.

## Foundational Learning

**In-Context Learning (ICL)**: The ability of LLMs to learn from demonstration examples without parameter updates. Why needed: Forms the foundation for understanding how models can learn shortcuts from few-shot examples. Quick check: Verify that ICL enables zero-shot/few-shot learning capabilities.

**Shortcut Learning**: Models exploiting simple correlations or patterns to achieve high accuracy without learning the intended task. Why needed: Central concept for understanding failures in model generalization. Quick check: Confirm that shortcuts provide easy but incorrect solutions.

**Demonstration Skewing**: When provided examples contain patterns that mislead the model. Why needed: Explains how acquired shortcuts develop. Quick check: Ensure demonstration quality affects learning outcomes.

## Architecture Onboarding

Component map: LLMs (pre-training) -> Instruction Tuning -> ICL Demonstrations -> Model Output

Critical path: Model architecture and pre-training data → Demonstration selection → Input formatting → Model prediction → Performance evaluation

Design tradeoffs: Between model size and shortcut susceptibility, between demonstration quality and task performance, between prompt engineering complexity and robustness

Failure signatures: Over-reliance on surface features, poor generalization to novel examples, performance drops when key shortcut features are removed

First experiments:
1. Test vanilla-label bias by providing identical labels with different contexts
2. Evaluate position bias by varying demonstration order
3. Measure domain-label bias using different domain contexts with same labels

## Open Questions the Paper Calls Out
- Need for more robust evaluation benchmarks that generalize beyond English
- Understanding unknown shortcut scenarios that haven't been identified yet
- Decoupling the relationship between instinctive and acquired shortcut types
- Addressing multiple shortcut coexistence in complex tasks
- Improving interpretability of shortcut mechanisms in larger models

## Limitations
- Classification framework may not capture all emergent shortcut behaviors in larger models
- Most evaluation benchmarks focus on English and limited task types, constraining generalizability
- Model size effects on shortcut learning are primarily correlational rather than causal

## Confidence
High: Identification of shortcut types and mitigation strategies are supported by multiple empirical studies
Medium: Framework for distinguishing instinctive vs acquired shortcuts may have ambiguous boundaries
Low: Discussion of model size effects lacks causal evidence and mechanistic understanding

## Next Checks
1. Conduct cross-linguistic evaluations to test whether identified shortcut types generalize beyond English benchmarks
2. Design controlled experiments that systematically vary demonstration quality and quantity to isolate effects of different shortcut types
3. Develop ablation studies that test the effectiveness of each mitigation strategy independently, particularly examining whether prompt-centric approaches address symptoms rather than root causes