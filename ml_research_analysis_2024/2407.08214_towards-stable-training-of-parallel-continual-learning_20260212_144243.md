---
ver: rpa2
title: Towards stable training of parallel continual learning
arxiv_id: '2407.08214'
source_url: https://arxiv.org/abs/2407.08214
tags:
- learning
- gradient
- training
- tasks
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Stable Parallel Continual Learning (SPCL) to
  address training instability in Parallel Continual Learning (PCL) where multiple
  tasks are learned simultaneously. PCL suffers from unstable forward and backward
  propagation due to entangled features and conflicting gradients across tasks.
---

# Towards stable training of parallel continual learning

## Quick Facts
- arXiv ID: 2407.08214
- Source URL: https://arxiv.org/abs/2407.08214
- Reference count: 30
- Primary result: SPCL outperforms state-of-the-art methods on EMNIST, CIFAR-100, and ImageNet datasets with better training stability

## Executive Summary
Parallel Continual Learning (PCL) suffers from unstable training due to entangled features and conflicting gradients across tasks. SPCL addresses this by employing doubly-block Toeplitz (DBT) matrix-based orthogonality constraints on CNN filters for stable forward propagation and orthogonal decomposition for gradient management to stabilize backpropagation. The method uses soft orthogonal regularization under the Frobenius norm, adding minimal computational overhead while enhancing training stability. Experiments demonstrate SPCL's effectiveness in improving average accuracy and backward transfer metrics across multiple datasets.

## Method Summary
SPCL is a framework for stabilizing parallel continual learning by addressing two key issues: unstable forward propagation due to feature entanglement and conflicting gradients during backpropagation. The method applies DBT-based orthogonality constraints to CNN filters to ensure stable forward propagation with consistent feature representations. For backpropagation, SPCL employs orthogonal decomposition to adjust the gradient matrix, finding an approximately orthogonal approximation that minimizes the distance to original gradients while reducing condition number. Both strategies use soft orthogonal regularization under the Frobenius norm, adding minimal computational overhead while enhancing training stability.

## Key Results
- Outperforms state-of-the-art methods on EMNIST, CIFAR-100, and ImageNet datasets
- Achieves better average accuracy (Āe) and backward transfer (F̄e) metrics
- Demonstrates improved training stability with reduced condition numbers and gradient conflicts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DBT matrix-based orthogonality constraints stabilize forward propagation by ensuring feature diversity and reducing redundancy.
- **Mechanism:** DBT regularization structures convolution as a DBT matrix-vector multiplication, enforcing orthogonality among convolutional kernels. This preserves norm consistency and prevents feature entanglement across tasks.
- **Core assumption:** Orthogonal convolutional kernels lead to stable forward propagation with consistent feature representations across tasks.
- **Evidence anchors:**
  - [abstract] "we apply Doubly-block Toeplit (DBT) Matrix based orthogonality constraints to network parameters to ensure stable and consistent propagation"
  - [section] "we utilize a DBT matrix representation of the kernel" and "These conditions ensure that the filters within a layer and across layers remain orthogonal, reducing redundancy and improving feature diversity."
- **Break condition:** If the DBT matrix representation fails to maintain strict orthogonality due to numerical instability or improper kernel initialization, feature redundancy could increase, destabilizing forward propagation.

### Mechanism 2
- **Claim:** Orthogonal decomposition for gradient management stabilizes backpropagation and mitigates gradient conflicts across tasks.
- **Mechanism:** The method adjusts the gradient matrix G by finding a matrix ˆG that is approximately orthogonal while minimizing the distance to the original gradients. This reduces condition number and aligns gradient directions across tasks.
- **Core assumption:** Reducing the condition number of the gradient matrix and enforcing approximate orthogonality among task gradients will stabilize optimization and reduce conflicts.
- **Evidence anchors:**
  - [abstract] "we employ orthogonal decomposition for gradient management stabilizes backpropagation and mitigates gradient conflicts across tasks"
  - [section] "we focus on the following optimization problem, where λ is the hyperparameter constraining the condition number of ˆG: min ˆG,κ( ˆG) ∥G − ˆG∥2 F + λκ( ˆG)" and "By employing a large penalty term σ to ∥ ˆG⊤ ˆG − diag(d)∥2 F, we ensure ˆG⊤ ˆG is close to a diagonal matrix, achieving approximate orthogonality among gradients"
- **Break condition:** If the gradient adjustment process fails to maintain meaningful information from the original gradients (i.e., ˆG deviates too much from G), the optimization could converge to poor local minima or lose task-specific learning signals.

### Mechanism 3
- **Claim:** Soft orthogonal regularization under the Frobenius norm adds minimal computational overhead while enhancing training stability in PCL environments.
- **Mechanism:** The method incorporates orthogonality constraints as regularization terms in the loss function using the Frobenius norm, which is computationally efficient compared to hard orthogonality constraints.
- **Core assumption:** Adding orthogonality regularization with minimal computational cost will improve stability without significantly increasing training time or memory requirements.
- **Evidence anchors:**
  - [abstract] "Both strategies use soft orthogonal regularization under the Frobenius norm, adding minimal computational overhead while enhancing training stability"
  - [section] "we incorporate a soft orthogonal convolution regularization loss into the overall loss function" and "This formulation not only addresses the traditional constraints but also considers the unique interactions of DBT-based orthonormality"
- **Break condition:** If the regularization strength is poorly tuned (too strong or too weak), it could either overly constrain the model or fail to provide meaningful stability improvements, respectively.

## Foundational Learning

- **Concept:** Orthogonal matrices preserve norms and angles between vectors.
  - Why needed here: Understanding this property is crucial because SPCL relies on maintaining orthogonality to prevent gradient explosion/vanishing and ensure stable feature propagation.
  - Quick check question: If W is an orthogonal matrix and x is any vector, what is the relationship between ||Wx|| and ||x||?

- **Concept:** Condition number as a measure of matrix stability.
  - Why needed here: The condition number of the gradient matrix directly relates to training stability, and SPCL aims to minimize it to prevent optimization instability.
  - Quick check question: If a matrix has condition number κ = 1000, how does this compare to a matrix with κ = 10 in terms of sensitivity to small changes in input?

- **Concept:** SVD (Singular Value Decomposition) and its role in matrix optimization.
  - Why needed here: SPCL uses SVD to decompose the gradient matrix and find an orthogonal approximation, which is central to the gradient management strategy.
  - Quick check question: Given a matrix G = UΣV⊤, what is the orthogonal matrix that minimizes ∥G − ˆG∥F?

## Architecture Onboarding

- **Component map:** Input layer → CNN backbone with DBT-based orthogonal regularization → Task-specific classifiers → Loss computation
- **Critical path:** Forward pass through orthogonalized CNN → Gradient computation → Orthogonal gradient adjustment → Parameter update → Backward pass
- **Design tradeoffs:**
  - Soft vs. hard orthogonality: Soft constraints (SPCL) provide flexibility and computational efficiency but may not guarantee strict orthogonality, while hard constraints ensure orthogonality but increase computational complexity.
  - Rehearsal buffer size: Larger buffers reduce catastrophic forgetting but increase memory requirements and computational overhead during training.
- **Failure signatures:**
  - Training instability (oscillating loss, exploding/vanishing gradients) indicates DBT orthogonality constraints may be too strict or improperly tuned.
  - Degraded performance on individual tasks suggests gradient adjustment may be over-regularizing and losing task-specific information.
  - Slow convergence indicates regularization strength may be too high, overly constraining the optimization landscape.
- **First 3 experiments:**
  1. Implement DBT-based orthogonal regularization on a simple CNN and verify that convolutional kernels maintain approximate orthogonality throughout training.
  2. Apply orthogonal gradient adjustment to a multi-task learning setup and measure the condition number reduction in the gradient matrix.
  3. Combine both strategies in a PCL framework on a small dataset (e.g., Split CIFAR-10) and compare training stability metrics (condition number, gradient conflicts) against baseline methods.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations
- Lacks comprehensive ablation studies to isolate the contribution of DBT-based orthogonality constraints versus gradient decomposition
- Computational overhead claims are not rigorously validated across different hardware configurations and batch sizes
- Does not address potential scalability issues when applying SPCL to very deep networks or extremely large-scale datasets

## Confidence

- **High:** The general approach of using orthogonality constraints for training stability is well-established in the literature.
- **Medium:** The specific DBT matrix formulation and its implementation details may have subtle variations that could affect performance.
- **Low:** The gradient management strategy's impact on final model performance requires further validation across diverse task distributions.

## Next Checks
1. Perform ablation studies to quantify the individual contributions of DBT-based orthogonality constraints versus gradient decomposition to overall performance gains.
2. Measure actual computational overhead across different hardware configurations (GPU/CPU) and batch sizes to validate the "minimal overhead" claim.
3. Test SPCL on more diverse task distributions (varying difficulty, domain shifts) to assess robustness beyond the current controlled experimental setup.