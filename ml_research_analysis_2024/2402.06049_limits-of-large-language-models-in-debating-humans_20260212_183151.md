---
ver: rpa2
title: Limits of Large Language Models in Debating Humans
arxiv_id: '2402.06049'
source_url: https://arxiv.org/abs/2402.06049
tags:
- agents
- agent
- conversation
- game
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study rigorously tests whether Large Language Models (LLMs)
  can act as human-like confederates or surrogates in sociological experiments involving
  conversation. Using GPT-4 and Llama 2, the researchers designed AI agents with unique
  personas to participate in debate games alongside humans.
---

# Limits of Large Language Models in Debating Humans

## Quick Facts
- **arXiv ID**: 2402.06049
- **Source URL**: https://arxiv.org/abs/2402.06049
- **Reference count**: 40
- **Key outcome**: LLMs can enhance engagement and focus but are measurably distinct from humans in debate settings, being 6× less likely to persuade humans to change opinions.

## Executive Summary
This study rigorously tests whether Large Language Models (LLMs) can act as human-like confederates in sociological experiments involving conversation. Using GPT-4 and Llama 2, researchers designed AI agents with unique personas to participate in debate games alongside humans. Agents were less effective at persuading humans to change opinions and were perceived as less confident, but they stayed more on-topic and increased overall productivity. Behavioral metrics and machine learning classifiers showed clear differences between agent and human interactions, indicating that while agents can enhance engagement and focus, their conversational behavior remains measurably distinct from humans.

## Method Summary
The study used a custom web interface for debate games where participants engaged in one-on-one conversations to reach consensus on diet choices (Vegan, Vegetarian, Omnivorous, Pescatarian). Agents were assigned unique personas with traits like stubbornness and grammar sophistication. The research included 37 games (10 human-human, 17 human-agent, 10 agent-agent) with participants recruited via Discord. Data was collected on opinion changes, confidence levels, productivity metrics, and conversation patterns, analyzed using Bayesian multilevel regression models and machine learning classifiers.

## Key Results
- Agents were 6 times less likely to persuade humans to change opinions compared to human participants
- Agents produced more on-topic keywords and earned more reward points than humans
- Machine learning classifiers achieved 80% accuracy in distinguishing human-agent conversations from human-human conversations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agents' behavior remains distinct from humans due to systematic differences in communication style and persuasion effectiveness
- Core assumption: The observed behavioral differences are stable across different game types and participant compositions
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 2
- Claim: Agents can enhance human productivity through focused communication and goal-oriented behavior
- Core assumption: The increased productivity from agents translates to meaningful engagement improvements in human participants
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 3
- Claim: Simple machine learning classifiers can differentiate between agent and human interactions based on behavioral metrics
- Core assumption: The behavioral differences between agents and humans are sufficiently pronounced to be captured by relatively simple classification models
- Evidence anchors: [abstract], [section], [corpus]

## Foundational Learning

- **Bayesian multilevel regression models**
  - Why needed here: To account for hierarchical data structure where participants and games have varying effects on outcomes
  - Quick check question: What does the conditional R2 value represent in a Bayesian multilevel model?

- **Effect size and power analysis**
  - Why needed here: To determine the minimum number of games required to detect significant behavioral differences between agents and humans
  - Quick check question: How does the effect size impact the required sample size for statistical significance?

- **Machine learning classification metrics**
  - Why needed here: To evaluate the effectiveness of classifiers in distinguishing between agent and human interactions
  - Quick check question: What is the difference between precision and recall in a confusion matrix?

## Architecture Onboarding

- **Component map**: Web interface -> Discord integration -> GPT-4/Llama 2 APIs -> Statistical analysis tools
- **Critical path**: Participant joins game → receives prompt and selects opinion → engages in one-on-one conversations → re-evaluates opinions → completes exit survey → data analysis
- **Design tradeoffs**: Using multiple LLMs to prevent quality decay vs. consistency in agent behavior; implementing message budgets to prevent infinite conversations vs. natural conversation flow
- **Failure signatures**: Agents failing to terminate conversations properly, participants disconnecting before active participation, inconsistent opinion change patterns across games
- **First 3 experiments**:
  1. Test agent response quality with different persona configurations to optimize persuasiveness
  2. Evaluate the impact of message budget limits on conversation naturalness and length
  3. Compare classification accuracy using different feature sets to identify the most distinguishing behavioral metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the agents' persuasiveness and perceived confidence change if they were explicitly labeled as AI from the beginning of the game?
- Basis in paper: The authors deliberately did not label agents as AI to avoid biasing human participants' perceptions, and they discuss this as a limitation
- Why unresolved: The study was designed to test agents' ability to blend in as confederates, so explicit labeling was intentionally avoided
- What evidence would resolve it: A follow-up study where agents are clearly identified as AI from the start, with behavioral and perceptual metrics compared to the current study

### Open Question 2
- Question: Would agents perform better in consensus games on topics with less perceived societal polarization compared to highly polarized topics like politics?
- Basis in paper: The authors chose a diet topic specifically because it does not trigger deep societal polarization, implying they recognize polarization could affect outcomes
- Why unresolved: The study only tested one topic type. The effect of topic polarization on agent performance is not explored
- What evidence would resolve it: Repeating the experiment with topics of varying levels of societal polarization (e.g., climate change, politics) and comparing agent performance

### Open Question 3
- Question: What specific linguistic or behavioral patterns cause humans to detect agents as non-human, and can these be mitigated through agent design?
- Basis in paper: The authors conducted a qualitative analysis of agent detection incidents and categorized causes (e.g., AI system provoked, AI language provoked, human provoked)
- Why unresolved: While patterns were identified, the study did not test whether modifying agent behavior based on these patterns would reduce detection rates
- What evidence would resolve it: Iterative agent design improvements targeting identified detection patterns, followed by testing in new games to measure changes in detection rates

## Limitations

- **Low generalizability across debate topics**: The study focuses on a single debate topic (diet choice), limiting confidence in whether observed behavioral differences between agents and humans generalize to other domains
- **Unknown sample composition effects**: With only 37 games total and 10% of human participants correctly identifying agent interlocutors, the study cannot fully account for how participant awareness of agent presence affects behavior and interaction patterns

## Confidence

- **High Confidence Claims**:
  - Agents are measurably less effective at persuading humans to change opinions (6× less likely)
  - Behavioral metrics show clear, statistically significant differences between agent and human interactions
  - Simple ML classifiers can differentiate agent from human conversations with 80%+ accuracy

- **Medium Confidence Claims**:
  - Agents enhance overall productivity through increased on-topic focus
  - Agent presence affects human behavioral patterns in AH games
  - The behavioral distinction between agents and humans is stable across different game types

## Next Checks

1. **Cross-topic validation**: Replicate the study using 3-4 different debate topics (e.g., climate change, education policy) to test whether the observed behavioral differences persist across domains

2. **Mixed-agent environments**: Conduct experiments with games containing both GPT-4 and Llama 2 agents to determine if behavioral differences are consistent across different LLM architectures

3. **Longitudinal behavior tracking**: Implement a follow-up study tracking the same participants across multiple games to assess whether human-agent interaction patterns converge over time or remain consistently distinct