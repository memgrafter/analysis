---
ver: rpa2
title: Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling
arxiv_id: '2403.16293'
source_url: https://arxiv.org/abs/2403.16293
tags:
- tree
- scheduling
- decision
- learning
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the black-box nature of deep reinforcement learning
  (DRL) for HPC cluster scheduling, which hinders deployment due to lack of interpretability.
  The authors propose IRL, a framework that converts a trained DRL policy (DQN) into
  an interpretable decision tree using imitation learning.
---

# Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling

## Quick Facts
- arXiv ID: 2403.16293
- Source URL: https://arxiv.org/abs/2403.16293
- Authors: Boyang Li; Zhiling Lan; Michael E. Papka
- Reference count: 38
- One-line primary result: IRL achieves up to 70% reduction in average job wait time and slowdown compared to FCFS while converting DRL policies to interpretable decision trees

## Executive Summary
This paper addresses the black-box nature of deep reinforcement learning (DRL) for HPC cluster scheduling by proposing IRL, a framework that converts trained DRL policies into interpretable decision trees using imitation learning. The framework integrates DAgger to iteratively improve decision tree performance and introduces critical state concepts to reduce tree size without sacrificing effectiveness. Experiments on real HPC workloads demonstrate that IRL maintains scheduling performance comparable to the original DRL model while providing the interpretability needed for practical deployment.

## Method Summary
The IRL framework converts a trained DQN agent into an interpretable decision tree through imitation learning. The process begins by training a DQN on HPC workload traces, then generating a dataset of (state, Q-value) pairs through trace replay. A decision tree is trained to mimic the DQN's outputs, with DAgger iterations used to improve performance by aggregating new samples from the tree's own policy. The concept of critical state is applied to prune the dataset, reducing tree size while maintaining effectiveness. The final interpretable decision tree can then be used for scheduling with comparable performance to the original DRL model.

## Key Results
- IRL achieves comparable scheduling performance to original DRL models on real HPC workloads
- Reduces average job wait time and slowdown by up to 70% compared to FCFS
- Decreases decision tree size by up to 48% compared to baseline DAgger conversion
- Enables analysis of reward settings in DRL scheduling

## Why This Works (Mechanism)

### Mechanism 1
The framework converts a trained DRL policy into an interpretable decision tree using imitation learning. The DQN agent generates input-output samples from workload traces, forming a training dataset for a decision tree that learns to mimic the DQN's Q-value outputs. The core assumption is that the decision tree can sufficiently approximate the complex DQN policy to maintain scheduling performance. This mechanism works because decision trees provide interpretable rules while retaining the scheduling decisions learned by the DRL model.

### Mechanism 2
DAgger algorithm improves decision tree scheduling performance through iterative refinement. After each training iteration, the newly generated decision tree replays the workload trace, creating new trajectories of (state, Q-value) pairs that are aggregated back into the training dataset. This process repeats, allowing the tree to learn from its own improved decisions. The mechanism relies on the assumption that aggregated datasets will help the decision tree learn better policies over iterations by capturing improved decision-making patterns.

### Mechanism 3
Critical state concept reduces decision tree size without sacrificing effectiveness. Critical states are defined as system states with nontrivial impact on scheduling performance, such as when the number of jobs in the waiting queue exceeds a threshold. Only samples with critical states are used to generate the decision tree, pruning away less impactful states. The mechanism assumes that non-critical states have minimal impact on scheduling performance, so pruning them won't significantly affect the tree's effectiveness while substantially reducing its size.

## Foundational Learning

- **Reinforcement Learning (RL)**: An area of machine learning focused on dynamic decision making where an agent takes actions in an environment to maximize cumulative rewards. Why needed here: IRL is built on top of RL; understanding RL is fundamental to grasping how DRL scheduling works and why interpretability is important. Quick check: What is the difference between model-based and model-free reinforcement learning?

- **Deep Neural Networks (DNNs)**: Neural networks with multiple layers that can approximate complex functions. Why needed here: DRL scheduling uses DNNs to approximate Q-values, and the black-box nature of DNNs is the problem IRL aims to solve. Quick check: What are the advantages and disadvantages of using DNNs in reinforcement learning?

- **Decision Trees**: A supervised learning method that uses a tree-like model of decisions and their possible consequences. Why needed here: IRL converts the DRL policy into a decision tree to provide interpretability. Quick check: What are the advantages and disadvantages of decision trees compared to DNNs?

## Architecture Onboarding

- **Component map**: DQN agent -> Workload trace replay -> Dataset generation -> Decision tree training -> DAgger iterations -> Critical state filtering -> Interpreted decision tree policy

- **Critical path**:
  1. Train DQN agent on workload trace
  2. Generate initial dataset by replaying trace with DQN
  3. Train initial decision tree on dataset
  4. Iterate DAgger process: replay trace with decision tree, aggregate new samples, retrain tree
  5. Filter dataset to critical states
  6. Generate final interpretable decision tree

- **Design tradeoffs**:
  - Tree depth vs. interpretability: deeper trees may capture more complex policies but are harder to interpret
  - Critical state threshold: lower thresholds may include more states but increase tree size; higher thresholds may prune too much
  - DAgger iterations: more iterations may improve performance but increase training time

- **Failure signatures**:
  - Significant performance degradation compared to original DQN policy
  - Extremely large decision tree that defeats the purpose of interpretability
  - Decision tree that doesn't generalize well to new workload traces

- **First 3 experiments**:
  1. Verify that the initial decision tree trained on DQN-generated samples achieves reasonable performance on the training trace
  2. Run DAgger iterations and measure improvement in decision tree performance after each iteration
  3. Compare decision tree size and performance with and without critical state filtering

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of critical state threshold affect the balance between decision tree size and scheduling performance across different workload characteristics? The paper only demonstrates results for one threshold value (3) on two specific workloads, without exploring how different threshold values might perform on diverse workload types or providing a general methodology for threshold selection. Systematic experiments testing various threshold values across diverse workloads would reveal the relationship between threshold choice and performance tradeoffs.

### Open Question 2
How does IRL's interpretability translate to practical deployment scenarios in production HPC environments? While the paper emphasizes IRL's interpretability advantage, it focuses on trace-based simulations rather than real-world deployment and only briefly mentions interpretability's utility for "reward setting." Deployment studies in actual production HPC environments would demonstrate practical utility.

### Open Question 3
Can IRL's approach be extended to handle more complex DRL scheduling methods beyond DQN? The paper states IRL can be applied to other DRL methods but doesn't provide empirical evidence or discuss potential challenges. Experiments applying IRL to convert various DRL scheduling methods would establish generalizability.

## Limitations
- Novel contributions lack direct corpus validation, limiting confidence in general applicability
- Key hyperparameters and threshold values for critical state filtering are not fully specified
- Experiments are based on specific workload traces without testing on diverse or synthetic workloads

## Confidence
- High confidence: The core premise that DRL policies are black boxes unsuitable for HPC deployment
- Medium confidence: The IRL framework's effectiveness in converting DQN policies to decision trees
- Low confidence: The specific performance improvements (70% reduction) due to lack of detailed methodology disclosure

## Next Checks
1. Apply IRL to DQN policies trained on synthetic workload distributions with known properties to validate decision tree conversion accuracy across different scheduling scenarios
2. Have HPC practitioners evaluate the generated decision trees for actual interpretability and practical utility in understanding scheduling decisions
3. Test IRL-converted decision trees on workload traces from different HPC systems beyond SP2 and DataStar to assess robustness and transferability