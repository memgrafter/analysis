---
ver: rpa2
title: On the Evaluation of Speech Foundation Models for Spoken Language Understanding
arxiv_id: '2406.10083'
source_url: https://arxiv.org/abs/2406.10083
tags:
- sfms
- tasks
- speech
- prediction
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SLUE-PERB, a benchmark for evaluating speech
  foundation models (SFMs) on complex spoken language understanding (SLU) tasks. It
  comprehensively compares self-supervised, supervised ASR, and supervised SLU models
  across classification and sequence generation tasks using three evaluation protocols:
  frozen SFMs with lightweight/complex prediction heads, and fine-tuned SFMs.'
---

# On the Evaluation of Speech Foundation Models for Spoken Language Understanding

## Quick Facts
- **arXiv ID**: 2406.10083
- **Source URL**: https://arxiv.org/abs/2406.10083
- **Reference count**: 20
- **Primary result**: Introduces SLUE-PERB benchmark comparing self-supervised, supervised ASR, and supervised SLU models across classification and sequence generation tasks

## Executive Summary
This paper presents SLUE-PERB, a comprehensive benchmark for evaluating speech foundation models (SFMs) on complex spoken language understanding tasks. The study systematically compares self-supervised, supervised ASR, and supervised SLU models across multiple SLU tasks using three evaluation protocols: frozen SFMs with lightweight/complex prediction heads, and fine-tuned SFMs. The research reveals nuanced performance patterns, showing that supervised ASR models excel at classification tasks while self-supervised models perform comparably or better on sequence generation tasks. The work provides valuable insights into SFM capabilities and limitations for SLU, highlighting the importance of task-specific approaches and the potential of self-supervised models for complex downstream applications.

## Method Summary
The evaluation employs three protocols: (1) frozen SFMs with lightweight prediction heads (linear classifier or shallow conformer encoder with CTC loss), (2) frozen SFMs with complex prediction heads (12-layer conformer encoder + 6-layer transformer decoder), and (3) fine-tuned SFMs with lightweight prediction heads. Models tested include self-supervised SFMs (HuBERT, Wav2Vec2, WavLM), supervised ASR SFMs (Whisper, OWSM), and supervised SLU SFMs, evaluated on SLUE benchmark datasets covering sentiment analysis, named entity recognition, dialogue act classification, question answering, and summarization. Performance is measured using F1 scores, WER, frame F1, ROUGE-L, and BERTScore depending on the task type.

## Key Results
- Supervised ASR SFMs (OWSM) perform best on classification tasks, while self-supervised SFMs (WavLM) perform equally well or better on sequence generation tasks
- Complex prediction heads yield the best overall performance but increase inference time significantly
- Fine-tuning SFMs can hurt performance on certain tasks (NER/NEL) due to excessive trainable parameters
- No universally optimal SFM or evaluation protocol exists across all SLU tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised ASR SFMs excel in classification tasks, while self-supervised SFMs perform equally well or better on sequence generation tasks
- Mechanism: Supervised models benefit from labeled speech-text corpora encoding task-specific linguistic knowledge, while self-supervised models learn robust acoustic representations less biased toward supervised patterns
- Core assumption: Pre-training data type (labeled vs unlabeled) shapes feature usefulness for downstream task types
- Evidence anchors: [abstract] supervised SFMs don't always outperform self-supervised ones; [section] supervised SFMs best for classification, SSL SFMs best for sequence generation; [corpus] pattern holds across multiple tasks but lacks explicit decoder ablation

### Mechanism 2
- Claim: Increasing prediction head complexity reduces performance gaps between SFMs
- Mechanism: Complex heads (encoder-decoder) compensate for weaker base representations by learning richer task-specific transformations
- Core assumption: Performance depends on both SFM feature quality and prediction head capacity
- Evidence anchors: [abstract] complex head gives best performance for most tasks; [section] performance improvement more pronounced for SSL SFMs; [corpus] weak evidence - related works cited but no direct ablation

### Mechanism 3
- Claim: Fine-tuning SFMs can hurt performance on certain tasks due to excessive trainable parameters
- Mechanism: Fine-tuning large SFMs introduces many degrees of freedom, leading to overfitting or instability on smaller datasets
- Core assumption: More trainable parameters don't always improve performance
- Evidence anchors: [abstract] complex head best performance but increases inference time; [section] supervised ASR SFMs perform worse on NER/NEL; [corpus] weak evidence - observation-based without explicit parameter count comparison

## Foundational Learning

- **Concept**: Speech representation extraction from hidden layers
  - Why needed here: The paper relies on weighted sums of hidden layer activations to create task-agnostic speech features
  - Quick check question: What is the difference between using only the top encoder layer vs. a weighted sum of multiple layers in SFM feature extraction?

- **Concept**: Encoder-decoder vs. encoder-only architectures
  - Why needed here: Supervised ASR SFMs use encoder-decoder models, but only the encoder is used, potentially missing decoder information
  - Quick check question: How would results change if decoder representations from supervised SFMs were incorporated?

- **Concept**: CTC vs. cross-entropy loss for sequence tasks
  - Why needed here: The paper uses CTC loss for sequence generation and alignment tasks (NER, NEL, QA)
  - Quick check question: Why is CTC preferred over cross-entropy for tasks where token boundaries are not given?

## Architecture Onboarding

- **Component map**: Pre-trained SFM -> Feature extraction layer (weighted sum) -> Prediction head (lightweight/complex/fine-tuned) -> Task-specific output layer

- **Critical path**: 1) Load pre-trained SFM and extract features, 2) Apply weighted sum of hidden layers, 3) Pass features to chosen prediction head, 4) Compute loss (CE/CTC/seq2seq), 5) Update prediction head parameters (or SFM if fine-tuning)

- **Design tradeoffs**: Frozen SFM + lightweight head: fast inference, limited adaptability; Frozen SFM + complex head: better performance, slower inference; Fine-tuned SFM: highest adaptability, highest compute cost, risk of overfitting

- **Failure signatures**: Degraded performance on long-form inputs (QA with Whisper): likely due to 30-second pre-training limit; Poor NER/NEL with fine-tuned ASR models: too many trainable parameters; Inconsistent results across SSL vs. supervised models: prediction head capacity or task alignment mismatch

- **First 3 experiments**: 1) Run SA with HuBERT large frozen + lightweight head; verify F1 ~40, 2) Swap to WavLM large; observe performance gain to ~43 F1, 3) Replace lightweight head with complex encoder-decoder; confirm performance improves to ~52 F1

## Open Questions the Paper Calls Out

- **Open Question 1**: How do supervised SFMs with both encoder and decoder compare to encoder-only supervised SFMs and SSL SFMs on complex SLU tasks? The paper speculates decoder representations may retain semantic information not currently used.
- **Open Question 2**: How does performance change when SFMs are pre-trained on longer speech utterances vs. current 30-second segments? Whisper's poor QA performance may be due to this limitation.
- **Open Question 3**: How do parameter-efficient fine-tuning methods compare to full fine-tuning of SFMs in terms of performance and computational efficiency? The paper mentions this as future work due to computational costs of full fine-tuning.

## Limitations

- Evaluation confined to SLUE benchmark datasets, which may not represent full diversity of real-world SLU applications
- Focus primarily on English language tasks, limiting generalizability to multilingual scenarios
- Comparison constrained by specific pre-training objectives and datasets used, not capturing all potential SFM architecture variations
- Analysis based on single complex head design (conformer encoder + transformer decoder), potentially overlooking other architectural variations

## Confidence

- **High Confidence**: Supervised ASR SFMs excel in classification tasks and self-supervised SFMs perform comparably or better on sequence generation tasks - well-supported by consistent results across multiple benchmarks
- **Medium Confidence**: Increasing prediction head complexity reduces performance gaps between SFMs - supported by data but relies on single complex head architecture
- **Low Confidence**: Fine-tuning SFMs can hurt performance due to excessive trainable parameters - based on observed patterns rather than controlled ablation studies

## Next Checks

1. **Replicate the prediction head complexity experiment**: Train the same set of SFMs with varying prediction head complexities (lightweight, medium, complex) on a held-out classification task from SLUE to verify that increased head complexity consistently reduces performance gaps between self-supervised and supervised models.

2. **Decoder representation incorporation test**: Modify the evaluation protocol to include decoder representations from supervised ASR SFMs (Whisper, OWSM) and compare performance on sequence generation tasks against the current encoder-only approach to assess if decoder features improve results.

3. **Parameter-efficient fine-tuning comparison**: Replace full fine-tuning with parameter-efficient methods (e.g., LoRA or adapter layers) for the supervised ASR SFMs on NER/NEL tasks to determine if the performance degradation observed with full fine-tuning can be mitigated while maintaining model adaptability.