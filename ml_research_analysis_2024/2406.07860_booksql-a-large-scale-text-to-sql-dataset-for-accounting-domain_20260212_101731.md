---
ver: rpa2
title: 'BookSQL: A Large Scale Text-to-SQL Dataset for Accounting Domain'
arxiv_id: '2406.07860'
source_url: https://arxiv.org/abs/2406.07860
tags:
- table
- date
- master
- dataset
- service
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BookSQL, a large-scale Text-to-SQL dataset
  for the accounting domain containing 100k query-SQL pairs and 1 million records.
  The dataset addresses the lack of domain-specific Text-to-SQL resources for accounting
  and finance, reflecting real-world accounting databases used globally.
---

# BookSQL: A Large Scale Text-to-SQL Dataset for Accounting Domain

## Quick Facts
- arXiv ID: 2406.07860
- Source URL: https://arxiv.org/abs/2406.07860
- Authors: Rahul Kumar; Amar Raja Dibbu; Shrutendra Harsola; Vignesh Subrahmaniam; Ashutosh Modi
- Reference count: 40
- Best model achieves only 47.5% exact match accuracy on BookSQL, highlighting domain complexity

## Executive Summary
This paper introduces BookSQL, a large-scale Text-to-SQL dataset specifically designed for the accounting domain. The dataset contains 100,000 query-SQL pairs and 1 million records across 27 businesses, addressing the critical gap in domain-specific resources for finance and accounting. The authors evaluate state-of-the-art Text-to-SQL models including GPT-4 on this challenging dataset, finding significant performance gaps compared to general-purpose datasets like Spider. The results demonstrate the unique complexity of accounting domain queries and establish BookSQL as a benchmark for developing specialized models in this domain.

## Method Summary
The authors created BookSQL by collecting and anonymizing accounting databases from 27 businesses, generating 100,000 natural language queries paired with SQL statements through collaboration with financial experts. The dataset was constructed to reflect real-world accounting databases and includes various SQL operations such as ORDER BY, GROUP BY, and nested queries. State-of-the-art Text-to-SQL models (SEDE, UniSAr, RESDSQL) were fine-tuned on the training split, while GPT-4 was evaluated using dynamic few-shot prompting. Models were assessed using Exact Match Accuracy, Execution Accuracy, Partial Component Match F1, BLEU-4, and ROUGE-L metrics.

## Key Results
- BookSQL achieves only 47.5% exact match accuracy with the best model (RESDSQL), significantly lower than performance on general datasets
- Existing state-of-the-art models trained on Spider perform poorly on BookSQL, with SEDE achieving only 22.1% accuracy
- GPT-4 with dynamic few-shot prompting achieves 41.8% accuracy, outperforming fine-tuned models on complex queries
- The dataset contains 17,529 ORDER BY, 11,508 GROUP BY, and 4,456 nested queries, demonstrating its complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BookSQL addresses the gap in domain-specific Text-to-SQL datasets for accounting and finance
- Mechanism: By creating a dataset that reflects real-world accounting databases used globally, BookSQL enables the development of models tailored to the accounting domain's unique requirements
- Core assumption: Accounting databases have specific characteristics and constraints that differ from general-purpose databases
- Evidence anchors: [abstract], [section 1], [corpus]

### Mechanism 2
- Claim: BookSQL's large scale and complexity enable the development of more robust Text-to-SQL models
- Mechanism: The dataset's size (100k query-SQL pairs) and complexity (involving various SQL operations and nested queries) provide a rich training ground for models to learn the intricacies of accounting domain queries
- Core assumption: Larger and more complex datasets lead to better model performance
- Evidence anchors: [abstract], [section 3.4], [corpus]

### Mechanism 3
- Claim: Collaboration with financial experts ensures the dataset's relevance and quality
- Mechanism: Financial experts help create queries that reflect real-world use cases and adhere to accounting principles, ensuring the dataset's practicality and accuracy
- Core assumption: Expert involvement in dataset creation leads to higher quality and more relevant data
- Evidence anchors: [section 3.3], [section 3.2], [corpus]

## Foundational Learning

- Concept: SQL and database fundamentals
  - Why needed here: Understanding SQL syntax, database schema, and query execution is essential for working with Text-to-SQL datasets and models
  - Quick check question: What is the difference between a SELECT statement and an INSERT statement in SQL?

- Concept: Accounting principles and database structure
  - Why needed here: Familiarity with accounting concepts (e.g., double-entry accounting, chart of accounts) and the typical structure of accounting databases is crucial for understanding the BookSQL dataset
  - Quick check question: What is the purpose of the "Chart of Accounts" table in an accounting database?

- Concept: Natural language processing and text generation
  - Why needed here: Knowledge of NLP techniques and text generation models is necessary for developing and improving Text-to-SQL models
  - Quick check question: What is the difference between a seq2seq model and a transformer model in NLP?

## Architecture Onboarding

- Component map: BookSQL dataset consists of financial-accounts database (1 million records) -> 100k natural language queries-SQL pairs -> Schema reflecting real-life accounting databases (7 tables) -> Coverage of 27 different businesses -> Various SQL operations (ORDER BY, GROUP BY, nested queries)

- Critical path: Understand dataset structure and content -> Experiment with Text-to-SQL models -> Analyze model performance -> Identify areas for improvement -> Develop specialized models for accounting domain

- Design tradeoffs: Choice of dataset size, complexity, and expert involvement involves tradeoffs between practicality, model performance, and development effort. Larger and more complex datasets may lead to better model performance but require more resources and expertise.

- Failure signatures: Poor model performance on BookSQL may indicate lack of understanding of accounting domain concepts, insufficient training data, or limitations in model architecture. Common failure modes include confusion between different columns, incorrect handling of credit and debit columns, and inability to generate nested SQL queries.

- First 3 experiments:
  1. Evaluate performance of existing Text-to-SQL models (SEDE, UniSAr, RESDSQL) on BookSQL dataset to establish baseline
  2. Analyze errors made by these models to identify common failure modes and areas for improvement
  3. Experiment with model modifications or new architectures to address identified weaknesses and improve performance on BookSQL

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the upper bound performance on BookSQL for general-purpose Text-to-SQL models without domain adaptation?
- Basis in paper: [explicit] The paper notes that "existing state-of-the-art (SOTA) Text-to-SQL models trained on Spider have very poor performance on domain-specific BookSQL dataset" but doesn't establish performance limits for non-adapted models
- Why unresolved: The paper focuses on demonstrating the need for domain-specific models but doesn't explore how far general models could progress with additional training on BookSQL alone
- What evidence would resolve it: Training a diverse set of general-purpose models (SEDE, UniSAr, RESDSQL) exclusively on BookSQL data and measuring their performance would establish baseline capabilities

### Open Question 2
- Question: How does BookSQL performance compare when using prompt engineering techniques beyond few-shot examples?
- Basis in paper: [inferred] The paper mentions "Dynamic few-shot prompt + GPT4" but doesn't explore other prompt engineering strategies like chain-of-thought or tree-of-thought approaches
- Why unresolved: The analysis focuses on comparing model architectures rather than exploring the full space of prompt engineering techniques that could enhance performance
- What evidence would resolve it: Systematic evaluation of various prompt engineering strategies (chain-of-thought, tree-of-thought, least-to-most prompting) on the same GPT-4 model would reveal their relative effectiveness

### Open Question 3
- Question: What is the relationship between accounting domain complexity and performance degradation across different model architectures?
- Basis in paper: [explicit] The paper notes that "SEDE fails to generate correct SQL, possibly due to a lack of question and schema linking" while "RESDSQL is the best-performing model" but doesn't analyze how different architectural choices affect performance on various accounting query types
- Why unresolved: The error analysis identifies failure modes but doesn't systematically map them to specific architectural features or design choices
- What evidence would resolve it: Detailed analysis correlating specific failure types (nested queries, date filters, domain-specific filters) with model architectural components would reveal which design choices matter most for accounting domain adaptation

## Limitations

- The paper focuses primarily on exact match accuracy without deeper error analysis to understand which specific accounting domain challenges most impact performance
- Only three existing Text-to-SQL models were tested, limiting the assessment of whether performance gaps stem from dataset difficulty or model limitations
- The direct comparability of BookSQL metrics to general datasets like Spider is unclear, making it difficult to isolate the impact of domain specificity

## Confidence

- High confidence in dataset creation methodology and novelty of BookSQL contribution
- Medium confidence in assertion that accounting domain queries are inherently more complex
- Medium confidence in conclusion that specialized models are needed

## Next Checks

1. Conduct detailed error analysis categorizing failures by SQL operation type (JOINs, aggregations, nested queries) to determine whether accounting-specific concepts or general SQL complexity drives performance gaps

2. Test additional model architectures beyond the three fine-tuned models, including few-shot learning approaches and specialized encoders for financial terminology, to better assess whether performance ceiling is due to dataset difficulty or model limitations

3. Compare BookSQL performance against domain-specific subsets of general Text-to-SQL datasets (accounting-related queries from Spider) to isolate impact of domain specificity versus dataset scale and complexity