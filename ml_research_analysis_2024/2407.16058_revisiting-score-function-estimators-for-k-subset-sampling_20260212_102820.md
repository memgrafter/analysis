---
ver: rpa2
title: Revisiting Score Function Estimators for $k$-Subset Sampling
arxiv_id: '2407.16058'
source_url: https://arxiv.org/abs/2407.16058
tags:
- sampling
- score
- function
- learning
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits score function estimators for k-subset sampling,
  addressing the challenge of differentiable optimization in discrete subset selection.
  The authors propose an efficient method to compute the score function using discrete
  Fourier transforms and reduce variance with control variates.
---

# Revisiting Score Function Estimators for $k$-Subset Sampling

## Quick Facts
- arXiv ID: 2407.16058
- Source URL: https://arxiv.org/abs/2407.16058
- Authors: Klas Wijk; Ricardo Vinuesa; Hossein Azizpour
- Reference count: 23
- Key outcome: SFESS with control variates achieves competitive results with current methods for feature selection on MNIST, Fashion-MNIST, and KMNIST datasets.

## Executive Summary
This paper addresses the challenge of differentiable optimization for k-subset sampling by introducing the Score Function Estimator for k-Subset Sampling (SFESS). The method leverages discrete Fourier transforms to efficiently compute the Poisson binomial distribution's probability mass function, enabling exact sampling and unbiased gradient estimates. The authors demonstrate that their approach works even for non-differentiable downstream models, a capability lacking in existing methods. Experiments on three benchmark datasets show that SFESS with control variates achieves competitive performance while maintaining weaker assumptions than current approaches.

## Method Summary
The SFESS method computes gradients for k-subset sampling by applying the log derivative trick to the k-subset distribution. The key innovation is using a discrete Fourier transform to efficiently evaluate the Poisson binomial distribution's density function, which is essential for computing the score function. The method constructs control variates using multiple samples to reduce variance without introducing bias. Unlike pathwise gradient estimators that require differentiable relaxations, SFESS only needs to evaluate the downstream model, making it applicable to non-differentiable functions. The approach is evaluated on feature selection tasks for both reconstruction and classification using MNIST, Fashion-MNIST, and KMNIST datasets.

## Key Results
- SFESS-V achieves PSNR scores of 17.775, 17.805, and 12.696 on MNIST, Fashion-MNIST, and KMNIST respectively
- SFESS-V achieves accuracy scores of 0.921, 0.809, and 0.634 on the same datasets for classification
- The method demonstrates faster runtime compared to some baselines
- SFESS performs competitively with current methods despite weaker assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SFESS provides exact samples and unbiased gradient estimates by leveraging the log derivative trick
- Mechanism: The method computes the gradient of the log probability of a k-subset distribution using automatic differentiation and a discrete Fourier transform to efficiently evaluate the Poisson binomial distribution's density function
- Core assumption: The log derivative rule holds for the k-subset distribution and that the DFT provides an exact and efficient way to compute the Poisson binomial probability mass function
- Evidence anchors: The abstract states "The resulting estimator provides both exact samples and unbiased gradient estimates while also applying to non-differentiable downstream models, unlike existing methods." The score function estimator is explicitly written using the log derivative rule in the methodology section.

### Mechanism 2
- Claim: Control variates significantly reduce the variance of the score function estimator
- Mechanism: Multiple samples are used to construct control variates that subtract the mean of fϕ(z,x) across samples, reducing variance without introducing bias
- Core assumption: The control variate approach can effectively reduce variance for k-subset sampling without changing the expected value of the estimator
- Evidence anchors: The abstract mentions "reduce the estimator's variance with control variates" and the methodology section states they "choose to employ control variates using multiple samples... in this work due to its simplicity, unbiasedness, and lack of additional assumptions."

### Mechanism 3
- Claim: SFESS is applicable to non-differentiable downstream models
- Mechanism: Since SFESS only requires the ability to evaluate fϕ(z,x) and not differentiate through it, it can be used with any function, differentiable or not
- Core assumption: The downstream model's gradient is not required for the optimization to work, only the function value
- Evidence anchors: The abstract states "while also applying to non-differentiable downstream models, unlike existing methods" and the methodology section notes "Furthermore, it does not assume differentiable downstream models, broadening the possible applications of k-subset sampling to cases when the downstream model's gradient is unavailable or computationally expensive."

## Foundational Learning

- Concept: Discrete Fourier Transform (DFT)
  - Why needed here: The DFT is used to efficiently compute the Poisson binomial distribution's probability mass function, which is essential for calculating the score function
  - Quick check question: What is the time complexity of computing the Poisson binomial PMF using the DFT approach versus the naive approach?

- Concept: Control Variates
  - Why needed here: Control variates are used to reduce the high variance of the score function estimator, making it practical for optimization
  - Quick check question: How do control variates reduce variance without introducing bias in Monte Carlo estimation?

- Concept: Score Function Estimators (REINFORCE)
  - Why needed here: The score function estimator (also known as REINFORCE) provides an unbiased gradient estimate for discrete random variables by using the log derivative trick
  - Quick check question: What is the key advantage of the score function estimator over the pathwise gradient estimator for discrete variables?

## Architecture Onboarding

- Component map:
  - Subset distribution parameters (θ) -> Bernoulli probabilities for each element
  -> Poisson binomial PMF computation -> DFT-based implementation
  -> Score function calculation -> Automatic differentiation of log probabilities
  -> Control variate construction -> Multiple samples to estimate mean
  -> Downstream model (fϕ) -> Can be any function, differentiable or not
  -> Optimizer -> Adam with separate learning rates for selector and downstream model

- Critical path:
  1. Initialize θ parameters and downstream model
  2. Sample k-subsets using the Bernoulli distribution
  3. Compute Poisson binomial PMF using DFT
  4. Calculate score function using automatic differentiation
  5. Apply control variates to reduce variance
  6. Update parameters using the gradient estimate

- Design tradeoffs:
  - Exact vs. relaxed samples: SFESS provides exact samples but may have higher variance without control variates
  - Unbiased vs. biased gradients: SFESS provides unbiased gradients but may converge slower than biased methods
  - General vs. specialized: SFESS works with any downstream model but may be less efficient than specialized methods

- Failure signatures:
  - High variance in gradient estimates despite control variates: Check Poisson binomial PMF computation and control variate construction
  - Poor performance on downstream task: Verify that the subset distribution is learning useful representations
  - Slow convergence: Consider adjusting learning rates or increasing the number of samples for control variates

- First 3 experiments:
  1. Verify Poisson binomial PMF computation: Compare DFT-based implementation against brute-force calculation for small n
  2. Test score function estimator: Implement a simple Bernoulli example and verify that the gradient estimates are unbiased
  3. Evaluate control variates: Measure variance reduction on a simple example with known gradients

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, several implicit questions emerge from the methodology and experimental design:

- How does SFESS scale to problems with thousands of variables beyond the tested feature selection tasks?
- What is the theoretical variance bound for the control variate-enhanced estimator compared to other variance reduction techniques?
- How does the performance gap between SFESS and differentiable methods change as the downstream model becomes increasingly non-differentiable?

## Limitations

- The paper only evaluates on feature selection tasks, leaving the claimed broader applicability to non-differentiable models unverified
- Empirical improvements over baselines are modest (0.1-0.5 dB PSNR), suggesting limited practical impact
- Computational efficiency claims lack specific timing comparisons for the Poisson binomial PMF computation

## Confidence

- **High Confidence**: The core mathematical framework (DFT-based Poisson binomial PMF computation, score function estimator formulation) appears sound based on established results in discrete probability and automatic differentiation
- **Medium Confidence**: The empirical results showing competitive performance with baselines, though the improvements are modest and may not justify the added complexity for all applications
- **Low Confidence**: Claims about applicability to non-differentiable models and the practical significance of variance reduction benefits lack supporting evidence

## Next Checks

1. **Benchmark Poisson Binomial PMF Computation**: Implement both DFT-based and naive approaches for Poisson binomial PMF calculation and benchmark their performance across varying subset sizes (k) and universe sizes (n) to verify claimed efficiency gains

2. **Theoretical Variance Analysis**: Derive and compare the theoretical variance of the basic score function estimator versus the control variate-enhanced version for k-subset sampling to quantify expected improvements and identify scenarios where control variates provide meaningful benefits

3. **Cross-Domain Evaluation**: Apply SFESS to a non-differentiable downstream task (e.g., subset selection for a decision tree or reinforcement learning problem) to validate the claimed broader applicability beyond differentiable models