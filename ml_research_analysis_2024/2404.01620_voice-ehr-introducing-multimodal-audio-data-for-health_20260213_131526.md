---
ver: rpa2
title: 'Voice EHR: Introducing Multimodal Audio Data for Health'
arxiv_id: '2404.01620'
source_url: https://arxiv.org/abs/2404.01620
tags:
- data
- voice
- health
- which
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a multimodal audio data collection system
  for health AI, addressing limitations of existing voice datasets that are small,
  expensive to collect, and biased toward high-income settings. The system, called
  HEAR, uses a mobile/web app to capture patient-reported health information alongside
  voice, speech, and breathing data through guided questions, creating a "Voice EHR"
  that includes both acoustic features and semantic context.
---

# Voice EHR: Introducing Multimodal Audio Data for Health

## Quick Facts
- arXiv ID: 2404.01620
- Source URL: https://arxiv.org/abs/2404.01620
- Authors: James Anibal; Hannah Huth; Ming Li; Lindsey Hazen; Veronica Daoud; Dominique Ebedes; Yen Minh Lam; Hang Nguyen; Phuc Hong; Michael Kleinman; Shelley Ost; Christopher Jackson; Laura Sprabery; Cheran Elangovan; Balaji Krishnaiah; Lee Akst; Ioan Lina; Iqbal Elyazar; Lenny Ekwati; Stefan Jansen; Richard Nduwayezu; Charisse Garcia; Jeffrey Plum; Jacqueline Brenner; Miranda Song; Emily Ricotta; David Clifton; C. Louise Thwaites; Yael Bensoussan; Bradford Wood
- Reference count: 0
- Primary result: Voice EHR audio transcripts were more informative than manual input data in 83% of cases (mean rating 4.10/5)

## Executive Summary
This study introduces Voice EHR, a multimodal audio data collection system for health AI that addresses limitations of existing voice datasets. The system uses a mobile/web app to capture patient-reported health information alongside voice, speech, and breathing data through guided questions, creating a comprehensive "Voice EHR" that includes both acoustic features and semantic context. Experiments with GPT-4o demonstrated that audio transcripts provided more clinical information than traditional manual input methods, enabling scalable, low-cost data collection in diverse settings including hospitals.

## Method Summary
The study collected semi-structured multimodal audio data using the HEAR mobile/web application, which captured voice, speech, and breathing recordings alongside patient-reported health information through 17 guided question pages. Audio recordings were transcribed using Whisper automated speech recognition, then analyzed by GPT-4o to compare the informativeness of audio transcripts against manually input demographic and clinical variables. The evaluation used a 5-point rubric to assess health assessment utility, with results showing audio data consistently provided richer clinical context than checkbox-based forms.

## Key Results
- Voice EHR audio transcripts were more informative than manual input data in 83% of cases
- Mean rating for audio transcript informativeness was 4.10/5 compared to manual input
- The HEAR app successfully collected detailed health histories and illness progression data that would not be available in traditional datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Voice EHR data contains richer clinical information than manual input alone
- Mechanism: Semi-structured audio recordings capture patient-reported context, temporal progression, and self-identified voice changes that are difficult to capture through checkboxes and short answers
- Core assumption: Patients can provide more detailed and nuanced health information through spoken narrative than through structured forms
- Evidence anchors:
  - [abstract] "Experiments with GPT-4o showed that Voice EHR audio transcripts were more informative than manually input data in 83% of cases"
  - [section 4.2] "Results of this experiment showed that, despite averaging less than 90 seconds in length, audio data on health history and current complaint was consistently more informative"
  - [corpus] Weak - corpus papers focus on multimodal EHR analysis but don't directly address voice-only data richness
- Break condition: If patients provide incomplete or incoherent audio recordings that cannot be accurately transcribed

### Mechanism 2
- Claim: Voice EHR enables scalable, low-cost data collection in diverse settings
- Mechanism: Using only mobile/web applications eliminates need for expensive recording equipment, enabling deployment in resource-constrained settings and hospitals
- Core assumption: Clinical audio quality from consumer devices is sufficient for both acoustic feature extraction and speech-to-text transcription
- Evidence anchors:
  - [abstract] "uses a mobile/web app to capture patient-reported health information alongside voice, speech, and breathing data"
  - [section 3.1] "The HEAR app is low-cost, low-bandwidth, fast/easy to use, and does not rely on any specific expensive technologies"
  - [corpus] Weak - corpus papers discuss EHR multimodal analysis but not specifically low-cost audio collection
- Break condition: If consumer device audio quality is insufficient for reliable biomarker detection or transcription

### Mechanism 3
- Claim: Voice EHR data can compensate for missing longitudinal EHR data in low-resource settings
- Mechanism: Audio recordings of health history and illness progression provide temporal context that would normally require access to comprehensive EHR systems
- Core assumption: Patient self-reporting of health history is sufficiently accurate to serve as a proxy for formal medical records
- Evidence anchors:
  - [abstract] "creating a 'Voice EHR' that includes both acoustic features and semantic context"
  - [section 3.2] "The application asks patients to use basic terminology to describe, in chronological order, the progression of their illness"
  - [section 4.3] Table 3 shows detailed health histories captured through audio that would not be available in traditional datasets
- Break condition: If patient recall of health history is inaccurate or incomplete enough to mislead AI models

## Foundational Learning

- Concept: Multimodal data fusion
  - Why needed here: Voice EHR combines acoustic features, speech patterns, and semantic meaning from patient narratives
  - Quick check question: How does combining voice, speech, and text data improve clinical AI performance compared to unimodal approaches?

- Concept: Audio biomarker detection
  - Why needed here: The system relies on detecting health signals in voice, speech, and breathing sounds
  - Quick check question: What acoustic features are most predictive of respiratory conditions in voice data?

- Concept: Semi-structured data collection
  - Why needed here: The HEAR app uses guided questions rather than open-ended forms to ensure comprehensive data capture
  - Quick check question: How does semi-structured audio collection compare to fully structured forms in terms of data completeness and patient burden?

## Architecture Onboarding

- Component map: Mobile/web app → Audio recording → Whisper transcription → GPT-4o analysis → AWS cloud storage → NIH data governance
- Critical path: Patient completes guided audio prompts → System records and transcribes → Data stored with metadata → Analysis by AI models
- Design tradeoffs: Consumer device audio quality vs. professional recording equipment; patient burden vs. data completeness; privacy vs. data utility
- Failure signatures: Incomplete recordings, poor audio quality, transcription errors, missing metadata, data storage issues
- First 3 experiments:
  1. Compare GPT-4o analysis of Voice EHR transcripts vs. manual input data across 50 patients
  2. Test Whisper transcription accuracy on hospital vs. home recordings
  3. Validate acoustic feature extraction from consumer device recordings against clinical gold standards

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do voice changes due to chronic conditions like MS, thyroid disorders, or dysautonomia affect the accuracy of AI models trained to detect acute respiratory infections?
- Basis in paper: [explicit] The paper explicitly notes that controls with chronic conditions (MS, thyroid disorders, dysautonomia) report voice changes that could be confused with biomarkers of acute illness, and this type of information is not captured in existing datasets.
- Why unresolved: The current dataset includes only 130 patients, with a mix of chronic and acute conditions, but lacks sufficient data to quantify the confounding effect of chronic voice changes on disease detection models.
- What evidence would resolve it: A larger, longitudinal dataset tracking both chronic and acute voice changes, with model performance metrics disaggregated by patient health history.

### Open Question 2
- Question: Can semi-structured voice EHR data collected in low-bandwidth settings be as informative as data collected in high-resource clinical environments?
- Basis in paper: [explicit] The paper highlights that the HEAR app is designed for low-connectivity areas and presents preliminary evidence that voice EHR is more informative than manual inputs, but does not compare performance across connectivity settings.
- Why unresolved: The study collected data primarily from hospital and home settings, with no direct comparison between low-bandwidth and high-bandwidth environments.
- What evidence would resolve it: A controlled study comparing model performance and data quality using voice EHR collected in low-bandwidth vs. high-bandwidth settings with identical prompts and populations.

### Open Question 3
- Question: How does the inclusion of patient-reported context (e.g., lifestyle, healthcare system challenges) improve the robustness of AI models for disease detection?
- Basis in paper: [explicit] The paper introduces prompts to capture patient-centered data about lifestyle and healthcare system challenges, suggesting this may reduce bias and improve model performance, but does not empirically test this claim.
- Why unresolved: While the paper presents case studies showing additional context is collected, it does not demonstrate whether this information actually improves model accuracy or reduces bias in practice.
- What evidence would resolve it: Experimental comparison of AI model performance with and without patient-reported context data, measuring changes in sensitivity, specificity, and bias across demographic groups.

## Limitations
- Small sample size of 50 patients (30 clinical, 20 controls) limits generalizability
- All participants recruited through online platforms, potentially introducing selection bias
- Evaluation relied on GPT-4o ratings rather than clinical validation by healthcare professionals
- Did not assess whether audio data actually improves downstream AI model performance for clinical tasks

## Confidence

**High confidence**: The technical feasibility of collecting multimodal audio data via mobile/web applications and the comparative advantage of audio transcripts over manual input (based on GPT-4o evaluation)

**Medium confidence**: The claim that Voice EHR enables scalable, low-cost data collection in diverse settings (limited to online recruitment evidence)

**Low confidence**: Clinical utility claims regarding improved health equity and AI model performance for diagnosis/triage (not directly validated in study)

## Next Checks

1. **Clinical Expert Validation**: Have 3-5 healthcare providers independently rate the informativeness and clinical utility of Voice EHR transcripts versus manual input data using the same 5-point rubric to confirm GPT-4o findings.

2. **Downstream Model Performance**: Train and evaluate a clinical AI model (e.g., respiratory condition classifier) using Voice EHR data versus traditional EHR data to verify claimed improvements in diagnostic accuracy.

3. **Real-World Deployment Study**: Deploy the HEAR app in 2-3 hospital settings with diverse patient populations over 3 months to assess scalability, data quality, and patient acceptance in actual clinical workflows.