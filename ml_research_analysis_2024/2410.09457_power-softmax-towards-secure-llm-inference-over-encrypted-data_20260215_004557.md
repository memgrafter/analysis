---
ver: rpa2
title: 'Power-Softmax: Towards Secure LLM Inference over Encrypted Data'
arxiv_id: '2410.09457'
source_url: https://arxiv.org/abs/2410.09457
tags:
- attention
- polynomial
- variant
- softmax
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of implementing privacy-preserving
  Large Language Model (LLM) inference over encrypted data using Homomorphic Encryption
  (HE), which requires all computations to be in polynomial form. The core method
  introduces a novel HE-friendly self-attention variant called PowerSoftmax, which
  replaces the standard Softmax function with a polynomial-based normalization.
---

# Power-Softmax: Towards Secure LLM Inference over Encrypted Data

## Quick Facts
- arXiv ID: 2410.09457
- Source URL: https://arxiv.org/abs/2410.09457
- Authors: Itamar Zimerman, Allon Adir, Ehud Aharoni, Matan Avitan, Moran Baruch, Nir Drucker, Jenny Lerner, Ramy Masalha, Reut Meiri, Omri Soceanu
- Reference count: 28
- Primary result: First polynomial LLMs with 32 layers and over 1.4 billion parameters demonstrating reasoning and in-context learning capabilities over encrypted data

## Executive Summary
This paper introduces PowerSoftmax, a novel approach enabling privacy-preserving Large Language Model inference over encrypted data using Homomorphic Encryption (HE). The key innovation replaces the standard Softmax function with a polynomial-based normalization variant that maintains essential attention properties while being HE-friendly. The authors demonstrate the first polynomial LLMs with 32 layers and over 1.4 billion parameters, achieving competitive performance on standard benchmarks while enabling secure inference over encrypted data.

## Method Summary
The authors develop PowerSoftmax, a power-based variant of self-attention that replaces Softmax's exponential scaling with polynomial scaling (x^p / Σx^p). They introduce ε-bounded division to ensure Lipschitz continuity and length-agnostic scaling to eliminate sequence-length-dependent approximation difficulty. The method involves modifying transformer architecture by replacing Softmax with PowerSoftmax, adding range-loss regularization to minimize input ranges, and approximating non-polynomial operations with polynomials using the Goldschmidt algorithm. Models are trained from scratch using standard datasets and then optimized for HE constraints.

## Key Results
- First polynomial LLMs with 32 layers and over 1.4 billion parameters
- Zero-shot and 5-shot performance metrics on Lambada, PIQA, WinoGrande close to non-polynomial baselines
- Matrix multiplication accounts for 67% of computation time in encrypted inference, with polynomial approximations contributing 24%

## Why This Works (Mechanism)

### Mechanism 1
PowerSoftmax preserves attention matrix properties while enabling polynomial representation by replacing Softmax's exponential scaling with power-based scaling (x^p / Σx^p), maintaining bounded [0,1] outputs and monotonic ordering while avoiding non-polynomial exponentials. The mechanism works when p values are chosen to provide sufficient amplification without causing training instability. Break condition occurs when p is too small (insufficient amplification) or too large (overflow in training).

### Mechanism 2
1/ε²-Lipschitz division makes polynomial approximation tractable by adding ε to denominator (x + ε), creating a Lipschitz continuous function that avoids singularities near zero and makes Goldschmidt algorithm convergence more reliable. The mechanism assumes making division function smoother near zero enables practical polynomial approximation without significant accuracy loss. Break condition occurs when ε is too large (poor approximation accuracy) or too small (remaining instability).

### Mechanism 3
Length-agnostic range eliminates sequence-length-dependent approximation difficulty by dividing attention scores by L^(1/p) before applying PowerSoftmax, making the denominator scale with mean rather than sum and preventing unbounded growth with sequence length. The mechanism assumes attention score means converge to stable values while sums grow with L, making mean-based normalization scale-invariant. Break condition occurs when attention score distributions have high variance or when L is small.

## Foundational Learning

- **Homomorphic Encryption (HE) and polynomial constraints**: Why needed - The entire approach relies on CKKS HE scheme's limitation to polynomial computations only. Quick check - Why can't standard transformers be directly evaluated over encrypted data?

- **Attention mechanism properties and normalization**: Why needed - Understanding what properties (boundedness, normalization, scaling) are essential to preserve when replacing Softmax. Quick check - What three key properties of Softmax does PowerSoftmax aim to preserve?

- **Polynomial approximation techniques (Goldschmidt algorithm)**: Why needed - Used to approximate the remaining non-polynomial operations (division, inverse square root) after architectural modifications. Quick check - Why is the Goldschmidt algorithm preferred for division approximation in this context?

## Architecture Onboarding

- **Component map**: Input → Query/Key/Value projection → PowerSoftmax attention (with ε-bounded division and length-agnostic scaling) → Output projection → Residual + LayerNorm → Feed-forward → Repeat for 32 layers → Polynomial approximation of non-polynomial ops

- **Critical path**: PowerSoftmax computation (QK^T scaling, power operation, length-agnostic normalization, ε-bounded division) dominates latency (67% matrix mult + 24% polynomial approx)

- **Design tradeoffs**: Higher p values improve Softmax approximation but increase training instability; larger ε values improve approximation tractability but reduce accuracy; length-agnostic approach adds precomputation but eliminates sequence-length scaling issues

- **Failure signatures**: Training instability (overflow/underflow in power operations), poor approximation accuracy (large ε or insufficient polynomial degree), latency bottlenecks (matrix multiplication dominance)

- **First 3 experiments**:
  1. Verify PowerSoftmax training stability on small transformer (2-4 layers) with different p values on Wikitext-103
  2. Benchmark polynomial approximation error for division with varying ε values using Goldschmidt algorithm
  3. Profile latency breakdown on encrypted data for PowerSoftmax vs standard Softmax attention implementation

## Open Questions the Paper Calls Out

### Open Question 1
How do PowerSoftmax-based transformers perform on autoregressive generative tasks compared to standard transformers, both in plaintext and encrypted environments? The paper states "a full evaluation of the auto-regressive generative abilities of our models in both sequential decoding over plain and encrypted environments has not yet been conducted." This remains unresolved because the paper focuses on classification and reasoning tasks, leaving generative performance unexplored.

### Open Question 2
What is the impact of different values of p in PowerSoftmax on the performance and inductive biases of transformers across various tasks and datasets? The authors mention that "PowerSoftmax introduces an important hyperparameter p that differentiates it from the traditional Softmax function" and show that varying p affects attention localization, but do not thoroughly investigate how different p values influence task performance and model behavior across diverse applications.

### Open Question 3
How does the computational efficiency of PowerSoftmax-based transformers scale with increasing sequence length and model size in encrypted inference scenarios? The authors provide latency profiling for a 32-layer model and note that "the latency bottleneck is dictated by the polynomials' degree," but do not explore scaling beyond this point or address how performance is affected as models grow larger or when processing longer sequences in encrypted environments.

## Limitations

- Training stability depends critically on parameter choices (p values, ε magnitudes) that may not generalize across different model sizes and tasks
- The approach's efficiency benefits for larger models beyond 1.4B parameters remain unproven
- Latency measurements are specific to encrypted data and may not translate directly to plaintext implementation scenarios

## Confidence

**High Confidence**: PowerSoftmax models can be trained to completion and achieve competitive performance on standard benchmarks; the approach successfully enables LLM inference over encrypted data, the first to demonstrate this with 32-layer models; latency profiling on encrypted data shows predictable bottlenecks with matrix multiplication dominance.

**Medium Confidence**: PowerSoftmax preserves the essential properties of standard attention mechanisms; the ε-bounded division and length-agnostic scaling effectively address approximation tractability; polynomial approximations of non-polynomial operations are sufficiently accurate for practical use.

**Low Confidence**: The specific parameter choices (p values, ε magnitudes) are optimal or even stable across different model sizes and tasks; the approach scales efficiently to larger models beyond the 1.4B parameter range demonstrated; the latency benefits translate to practical deployment scenarios beyond the controlled experimental setup.

## Next Checks

1. **Training stability validation**: Conduct controlled experiments varying p values (e.g., p=2, 4, 8, 16) and ε values on a 2-4 layer transformer trained on Wikitext-103 to map the stability boundaries and identify failure modes beyond the reported successful configurations.

2. **Approximation accuracy vs sequence length**: Systematically measure the division approximation error as a function of sequence length (L=64, 128, 256, 512) with different ε values to validate the length-agnostic scaling claims and identify when approximation quality degrades.

3. **Cross-model generalization**: Train PowerSoftmax variants of different model scales (e.g., 125M, 350M, 760M parameters) and evaluate whether the architectural modifications maintain training stability and performance across scales, testing the claim of general applicability beyond the specific 1.4B parameter model.