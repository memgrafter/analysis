---
ver: rpa2
title: Efficiently Computing Susceptibility to Context in Language Models
arxiv_id: '2410.14361'
source_url: https://arxiv.org/abs/2410.14361
tags:
- susceptibility
- fisher
- sample-dependent
- language
- open
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Fisher susceptibility as a computationally
  efficient alternative to Monte Carlo estimation for measuring how easily input contexts
  influence a language model's responses to queries. The method leverages Fisher information
  to approximate susceptibility without requiring expensive sampling of contexts and
  answers.
---

# Efficiently Computing Susceptibility to Context in Language Models

## Quick Facts
- arXiv ID: 2410.14361
- Source URL: https://arxiv.org/abs/2410.14361
- Reference count: 40
- Primary result: Fisher susceptibility correlates strongly with Monte Carlo estimates (Pearson correlations up to 0.76) while being approximately 70× faster to compute

## Executive Summary
This paper introduces Fisher susceptibility as an efficient alternative to Monte Carlo estimation for measuring how easily input contexts influence language model responses. The method leverages Fisher information to approximate susceptibility without requiring expensive sampling of contexts and answers. Empirical results demonstrate that Fisher susceptibility correlates strongly with Monte Carlo estimates while being approximately 70× faster to compute. Using this efficient metric, the authors find that larger models are not less susceptible than smaller ones, instruction-tuning increases susceptibility, and queries about both well-known and unfamiliar entities show similar susceptibility levels.

## Method Summary
The method computes Fisher information of the answer distribution with respect to context embeddings, using a second-order Taylor expansion of KL divergence to approximate susceptibility. The embedding function θ maps context-query pairs to a continuous parameter space, enabling Fisher information computation via gradients. To make computation tractable, the answer distribution is truncated to top-K tokens, reducing computational complexity. The trace of the Fisher information matrix serves as the susceptibility estimate, which is then validated against Monte Carlo sampling using the YAGO knowledge graph dataset with 48,800 queries across 122 domains.

## Key Results
- Fisher susceptibility shows Pearson correlations up to 0.76 with Monte Carlo estimates across diverse query domains
- The method achieves approximately 70× speedup compared to Monte Carlo estimation
- Larger language models are not less susceptible to context than smaller models
- Instruction-tuned models show higher susceptibility than base models
- Queries about both well-known and unfamiliar entities show similar susceptibility levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fisher susceptibility approximates Monte Carlo susceptibility by leveraging Fisher information as a second-order Taylor expansion of KL divergence
- Mechanism: The KL divergence between perturbed and unperturbed distributions is approximated via a second-order Taylor expansion, where the Fisher information matrix captures the local curvature
- Core assumption: The perturbation δ(c, q) is small enough that higher-order terms beyond quadratic can be ignored
- Evidence anchors: Abstract mentions Fisher information; section shows Taylor expansion formula; corpus neighbors discuss Monte Carlo and Fisher information

### Mechanism 2
- Claim: The embedding function θ maps contexts to a continuous parameter space, enabling Fisher information computation
- Mechanism: By treating the language model as a parameterized family indexed by real vectors θ(c⊕q), Fisher information can be computed via gradients of log-likelihood
- Core assumption: The embedding function θ is injective and differentiable, preserving information about context and query
- Evidence anchors: Section defines θ as injective embedding; abstract mentions Fisher information computation; corpus lacks direct validation

### Mechanism 3
- Claim: Truncating the answer distribution to top-K tokens makes Fisher information computation tractable
- Mechanism: Computing the full Fisher information matrix over the infinite answer space is infeasible; truncating to top-K tokens reduces computational complexity
- Core assumption: The top-K tokens capture most of the probability mass, making the truncated Fisher information a good approximation
- Evidence anchors: Section shows truncation approximation formula; abstract mentions comparable performance; corpus neighbors discuss truncation approaches

## Foundational Learning

- Concept: Mutual information and KL divergence
  - Why needed here: Susceptibility is defined as conditional mutual information between contexts and answers; understanding KL divergence is crucial for the Taylor expansion argument
  - Quick check question: What does it mean for two distributions to have zero KL divergence?

- Concept: Fisher information matrix
  - Why needed here: Fisher susceptibility is directly computed from the Fisher information matrix of the answer distribution with respect to context embeddings
  - Quick check question: How does the Fisher information matrix relate to the curvature of the log-likelihood?

- Concept: Taylor series approximation
  - Why needed here: The core approximation relies on a second-order Taylor expansion of KL divergence
  - Quick check question: Under what conditions does a second-order Taylor approximation become inaccurate?

## Architecture Onboarding

- Component map: Embedding function θ → language model pM → Fisher information J → truncated top-K approximation → susceptibility score
- Critical path: θ(c⊕q) → forward pass → log-likelihood gradients → Fisher information computation → truncation → trace → susceptibility
- Design tradeoffs: Exact Monte Carlo vs. Fisher approximation (accuracy vs. speed), full vs. truncated answer space (completeness vs. tractability)
- Failure signatures: Low Pearson/Spearman correlation with Monte Carlo estimates, sensitivity to truncation parameter K, numerical instability in gradient computation
- First 3 experiments:
  1. Verify Pearson correlation between Fisher and Monte Carlo susceptibility on a small dataset
  2. Test sensitivity to truncation parameter K by varying K and measuring correlation stability
  3. Benchmark runtime speedup across different model sizes and embedding dimensions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the approximation error in Fisher susceptibility change when considering different numbers of top-K tokens in Eq. (11)?
- Basis in paper: [explicit] The paper mentions using top-K highest-probability tokens to approximate the Fisher information matrix, but does not investigate how this choice affects accuracy
- Why unresolved: The paper only validates Fisher susceptibility against Monte Carlo estimates without systematically studying how the choice of K affects approximation quality
- What evidence would resolve it: A study comparing Pearson/Spearman correlations between Monte Carlo and Fisher susceptibility across different values of K, along with runtime measurements to establish the accuracy-efficiency tradeoff

### Open Question 2
- Question: Does Fisher susceptibility capture the same entity familiarity effects as Monte Carlo susceptibility, where real entities show lower susceptibility than fake entities?
- Basis in paper: [inferred] The paper notes that Fisher susceptibility shows similar levels for real and fake entities, contrasting with Monte Carlo susceptibility findings
- Why unresolved: The paper raises this discrepancy as a potential limitation but does not investigate whether this represents a fundamental difference in what the metrics measure
- What evidence would resolve it: Additional experiments examining whether the discrepancy persists with different embedding functions or different model architectures

### Open Question 3
- Question: Can Fisher susceptibility be extended to measure susceptibility to other types of context changes beyond the string concatenation used in this work?
- Basis in paper: [explicit] The paper defines the embedding function as concatenating the query and context embeddings, but notes this is just one possible choice
- Why unresolved: The paper validates the method for one specific reparameterization approach but does not explore alternatives
- What evidence would resolve it: Experiments applying Fisher susceptibility with alternative context representations and comparing results with the baseline approach

## Limitations

- The Taylor expansion approximation assumes small perturbations and Gaussian-like behavior, which may not hold for all query types
- Truncation to top-K tokens introduces approximation error whose bounds are not explicitly characterized
- The injectivity and smoothness assumptions about the embedding function θ lack empirical validation

## Confidence

- **High confidence**: The runtime efficiency claims (70× speedup) and empirical correlations with Monte Carlo estimates are well-supported by experimental results
- **Medium confidence**: The theoretical foundation of the Fisher susceptibility approximation via Taylor expansion is sound, but practical limits are not fully explored
- **Medium confidence**: The finding that larger models are not less susceptible than smaller ones is supported by data, but causal interpretation requires further investigation

## Next Checks

1. **Perturbation size sensitivity**: Systematically vary the perturbation magnitude in Monte Carlo estimation to identify the threshold where Fisher approximation error increases significantly
2. **Top-K truncation analysis**: Measure the cumulative probability mass captured by different K values across various query types to establish when truncation becomes problematic
3. **Distributional validation**: Test whether the perturbed answer distributions exhibit approximately Gaussian behavior, as assumed by the Taylor expansion, across different query categories