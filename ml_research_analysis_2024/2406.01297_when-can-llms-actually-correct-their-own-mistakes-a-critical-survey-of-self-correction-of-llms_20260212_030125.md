---
ver: rpa2
title: When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction
  of LLMs
arxiv_id: '2406.01297'
source_url: https://arxiv.org/abs/2406.01297
tags:
- self-correction
- feedback
- responses
- language
- initial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a critical survey of self-correction methods
  for large language models (LLMs), analyzing when and how LLMs can successfully correct
  their own mistakes. The authors identify that many prior studies lack clear research
  questions and use unrealistic or unfair experimental settings, often over-evaluating
  self-correction performance.
---

# When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs

## Quick Facts
- **arXiv ID**: 2406.01297
- **Source URL**: https://arxiv.org/abs/2406.01297
- **Reference count**: 40
- **Primary result**: Self-correction of LLMs works well with external feedback or fine-tuning but not with prompted LLMs alone in general tasks

## Executive Summary
This critical survey examines when and how large language models can successfully correct their own mistakes through self-correction methods. The authors identify that many prior studies lack clear research questions and use unrealistic or unfair experimental settings, often over-evaluating self-correction performance. Through systematic analysis of existing literature, they categorize research questions into three types: self-correction using only inherent LLM capabilities, self-correction with external information, and comparisons with other methods. The paper provides a detailed checklist for designing appropriate self-correction experiments and identifies that certain tasks, particularly those with decomposable responses, are exceptionally suited for self-correction.

## Method Summary
The authors conduct a comprehensive survey and critical analysis of self-correction methods for large language models, examining existing literature to identify patterns in research questions, experimental settings, and evaluation methodologies. They categorize self-correction approaches based on the feedback sources used (intrinsic, external tools/knowledge, or fine-tuning) and analyze the conditions under which self-correction succeeds or fails. The survey includes a systematic review of experimental designs and provides guidelines for fair evaluation of self-correction methods. The analysis focuses on identifying methodological flaws in prior work and establishing when self-correction is most effective compared to alternative approaches.

## Key Results
- No prior work demonstrates reliable self-correction using only prompted LLMs in general tasks
- Self-correction works well when reliable external feedback is available or when large-scale fine-tuning is applied
- Certain tasks with decomposable responses are exceptionally suited for self-correction
- Many existing studies use unrealistic or unfair experimental settings that over-evaluate self-correction performance

## Why This Works (Mechanism)
Self-correction in LLMs works when the model can identify discrepancies between its initial response and reliable feedback, then iteratively refine its output. The mechanism relies on the model's ability to process feedback, recognize errors, and generate improved responses through a feedback loop. Success depends on the quality and reliability of the feedback source, whether intrinsic (from the model itself), external (from tools or knowledge bases), or learned (through fine-tuning). The process is most effective when tasks have clear error signals and when feedback is sufficiently reliable to guide meaningful corrections.

## Foundational Learning

**Feedback Quality Assessment**: Understanding how to evaluate the reliability and usefulness of different feedback sources is critical for designing effective self-correction systems. Quick check: Can you distinguish between reliable and unreliable feedback for a given task?

**Task Decomposability**: Recognizing which tasks can be broken down into smaller, verifiable components determines when self-correction will be most effective. Quick check: Can you identify decomposable vs. monolithic task structures in sample problems?

**Evaluation Methodology**: Knowing how to design fair comparisons between self-correction and baseline methods, including matching computational costs and using appropriate metrics. Quick check: Can you construct a fair experimental design comparing self-correction with generate-and-rank approaches?

## Architecture Onboarding

**Component Map**: LLM Initial Response -> Feedback Generation -> Error Detection -> Response Refinement -> Final Output

**Critical Path**: The most critical path for successful self-correction is the feedback generation and error detection loop, as the quality of feedback directly determines whether meaningful corrections can be made.

**Design Tradeoffs**: Using external tools provides more reliable feedback but increases computational cost and complexity, while relying on intrinsic feedback is cheaper but less reliable. Fine-tuning improves performance but requires significant resources and may not generalize well.

**Failure Signatures**: Self-correction fails when feedback is unreliable or when the initial response is too far from correct to be meaningfully improved through iteration. Common failures include hallucination of corrections and getting stuck in local minima.

**First Experiments**: 
1. Test self-correction on arithmetic reasoning tasks with varying levels of decomposability
2. Compare self-correction with self-consistency on question answering tasks using identical computational budgets
3. Evaluate the impact of feedback quality by using different external tools for the same self-correction task

## Open Questions the Paper Calls Out
None

## Limitations
- Conclusions are largely based on analysis of existing literature rather than new experimental validation
- Limited quantitative analysis of which specific task characteristics most influence self-correction success
- Acknowledges that certain tasks are exceptionally suited for self-correction but doesn't fully quantify these relationships

## Confidence
- **High confidence**: Identification of common methodological flaws in self-correction research
- **Medium confidence**: Categorization of research questions and identification of when self-correction is most effective
- **Medium confidence**: Claim that no prior work demonstrates reliable self-correction using only prompted LLMs in general tasks

## Next Checks
1. Conduct controlled experiments testing self-correction across a spectrum of task decomposability to quantify the relationship between task structure and self-correction success.

2. Implement the recommended checklist from the paper to re-evaluate 3-5 existing self-correction studies, measuring how their reported performance changes when methodological issues are addressed.

3. Design experiments comparing self-correction with alternative approaches (self-consistency, generate-and-rank) on identical computational budgets across multiple task types to validate the claimed advantages of each approach.