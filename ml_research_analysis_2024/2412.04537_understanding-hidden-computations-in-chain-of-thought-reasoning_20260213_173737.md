---
ver: rpa2
title: Understanding Hidden Computations in Chain-of-Thought Reasoning
arxiv_id: '2412.04537'
source_url: https://arxiv.org/abs/2412.04537
tags:
- reasoning
- hidden
- tokens
- filler
- characters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transformer models process reasoning
  steps when explicit chain-of-thought (CoT) sequences are replaced with filler tokens.
  The authors analyze a small transformer trained on a synthetic 3SUM task with instance-adaptive
  CoT, using the logit lens method to study layer-wise representations and token rankings
  during decoding.
---

# Understanding Hidden Computations in Chain-of-Thought Reasoning

## Quick Facts
- arXiv ID: 2412.04537
- Source URL: https://arxiv.org/abs/2412.04537
- Reference count: 8
- Key outcome: Models perform internal reasoning computations beneath the surface output, even when these computations are not explicitly reflected in the model's final predictions.

## Executive Summary
This paper investigates how transformer models process reasoning steps when explicit chain-of-thought (CoT) sequences are replaced with filler tokens. The authors analyze a small transformer trained on a synthetic 3SUM task with instance-adaptive CoT, using the logit lens method to study layer-wise representations and token rankings during decoding. Results show that although filler tokens dominate top predictions in later layers, the original reasoning steps remain accessible at lower ranks. A modified decoding algorithm that selects the second-ranked non-filler token recovers the hidden characters without sacrificing task performance. This demonstrates that models perform internal reasoning computations beneath the surface output, offering insights into model interpretability and potential pathways for decoding hidden model behaviors.

## Method Summary
The authors analyze a small transformer trained on a synthetic 3SUM task with instance-adaptive CoT, using the logit lens method to study layer-wise representations and token rankings during decoding. The logit lens method allows examining the model's internal representations at each layer by projecting the activations back to the token space. The authors then propose a modified decoding algorithm that selects the second-ranked non-filler token to recover the hidden reasoning steps.

## Key Results
- Models trained with instance-adaptive CoT retain internal representations of the original reasoning steps, even when filler tokens dominate top predictions in later layers.
- The original reasoning steps remain accessible at lower ranks in the token distribution, allowing for recovery through modified decoding algorithms.
- A decoding algorithm that selects the second-ranked non-filler token can recover the hidden characters without sacrificing task performance.

## Why This Works (Mechanism)
The mechanism underlying this work relies on the transformer's ability to maintain internal representations of the original reasoning steps, even when these steps are not explicitly reflected in the model's final predictions. The logit lens method allows the authors to examine these internal representations at each layer by projecting the activations back to the token space. By analyzing the token rankings at each layer, the authors observe that while filler tokens dominate the top predictions in later layers, the original reasoning steps remain accessible at lower ranks. This suggests that the model performs internal computations related to the reasoning steps, but these computations are not necessarily surfaced in the final output.

## Foundational Learning
- **Logit lens method**: A technique for analyzing layer-wise representations in transformers by projecting activations back to the token space. *Why needed*: Allows examination of internal model representations without relying solely on final predictions. *Quick check*: Verify that the projected tokens align with expected reasoning steps at earlier layers.
- **Instance-adaptive CoT**: A training approach where the model generates chain-of-thought sequences tailored to each specific input instance. *Why needed*: Enables the model to learn task-specific reasoning patterns while maintaining flexibility. *Quick check*: Ensure that the generated CoT sequences improve task performance compared to fixed templates.
- **Token ranking analysis**: Examining the distribution of token probabilities at each layer to identify the model's internal reasoning steps. *Why needed*: Reveals the model's hidden computations that may not be reflected in top predictions. *Quick check*: Confirm that original reasoning steps appear at lower ranks in later layers when filler tokens dominate top predictions.

## Architecture Onboarding
- **Component map**: Input tokens -> Embedding layer -> Transformer layers (with logit lens projection) -> Token probability distribution -> Modified decoding algorithm
- **Critical path**: The critical path involves the transformer layers, where the model performs internal reasoning computations and maintains representations of the original reasoning steps. The logit lens method allows examining these representations at each layer.
- **Design tradeoffs**: The use of instance-adaptive CoT allows the model to learn task-specific reasoning patterns but may limit generalizability to other tasks. The modified decoding algorithm can recover hidden reasoning steps but may not scale to more complex tasks or larger models.
- **Failure signatures**: If the original reasoning steps are not accessible at lower ranks in the token distribution, or if the modified decoding algorithm fails to recover the hidden characters without sacrificing task performance, it may indicate that the model is not performing internal reasoning computations or that the reasoning steps are more deeply embedded.
- **3 first experiments**:
  1. Apply the logit lens method to a larger language model trained on more complex reasoning tasks to assess the generalizability of the findings.
  2. Investigate the impact of different training objectives, such as reinforcement learning from human feedback, on the model's tendency to surface or hide reasoning steps in its outputs.
  3. Examine the token rankings at each layer for a diverse set of input instances to determine if the model consistently maintains internal representations of the original reasoning steps.

## Open Questions the Paper Calls Out
None

## Limitations
- The synthetic nature of the 3SUM task limits generalizability to real-world reasoning problems.
- The logit lens method provides a static snapshot of activations rather than a dynamic view of reasoning processes over time.
- The modified decoding algorithm that recovers hidden characters by selecting second-ranked non-filler tokens may not scale to tasks where the reasoning steps are more deeply embedded or where multiple valid reasoning paths exist.

## Confidence
- High: The observation that original reasoning steps remain accessible at lower ranks during decoding, even when filler tokens dominate top predictions, is well-supported by the experimental evidence and analyses provided.
- Medium: The claim that models perform internal reasoning computations beneath the surface output is plausible given the evidence, but may not fully capture the complexity of how models actually process and represent reasoning steps.
- Low: The suggestion that these findings offer insights into model interpretability and potential pathways for decoding hidden model behaviors requires further validation on more diverse tasks and model architectures.

## Next Checks
1. Replicate the experiments on a larger language model trained on more complex reasoning tasks, such as mathematical word problems or logical inference, to assess the generalizability of the findings.
2. Apply the modified decoding algorithm to a dataset of human-written chain-of-thought explanations to determine if it can recover the underlying reasoning steps from the model's token distributions.
3. Investigate the impact of different training objectives, such as reinforcement learning from human feedback, on the model's tendency to surface or hide reasoning steps in its outputs.