---
ver: rpa2
title: Retrieval Augmentation via User Interest Clustering
arxiv_id: '2408.03886'
source_url: https://arxiv.org/abs/2408.03886
tags:
- user
- items
- interest
- users
- interests
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UIC (User Interest Clustering), a method
  for improving recommender systems by addressing the challenges of modeling light
  users with sparse interaction data and heavy users with diverse niche interests.
  The approach constructs an intermediate "interest" layer between users and items
  by clustering engagement graphs and incorporates user-interest attention, enabling
  more personalized similarity metrics.
---

# Retrieval Augmentation via User Interest Clustering

## Quick Facts
- arXiv ID: 2408.03886
- Source URL: https://arxiv.org/abs/2408.03886
- Authors: Hanjia Lyu; Hanqing Zeng; Yinglong Xia; Ren Chen; Jiebo Luo
- Reference count: 7
- Primary result: Improves recommendation accuracy by 1.24-4% relative gain and speeds up inference by 48-46% compared to item-level attention models

## Executive Summary
This paper introduces UIC (User Interest Clustering), a method for improving recommender systems by addressing the challenges of modeling light users with sparse interaction data and heavy users with diverse niche interests. The approach constructs an intermediate "interest" layer between users and items by clustering engagement graphs and incorporates user-interest attention, enabling more personalized similarity metrics. This intermediate layer balances scalability and expressiveness while facilitating computationally efficient inference through interest-level attention instead of item-level attention.

## Method Summary
UIC constructs user interest clusters by transforming the user-item bipartite graph into an item-item co-engagement graph, then applying Louvain community detection to create K interest clusters. User interests are assigned using Personalized PageRank on the bipartite graph, producing weighted interest profiles. The two-tower model integrates these profiles into user embeddings via concatenation or attention weighting. At inference, KNN search is performed within a sampled subset of interest clusters (N), reducing computational complexity from O(|I|) to O(N·|I|/K). This approach has been successfully deployed in Meta products for short-form video recommendations.

## Key Results
- Achieves 1.24-4% relative gains in precision, recall, and NDCG compared to item-level attention models
- Reduces inference time by 48-46% while maintaining or improving recommendation quality
- Successfully deployed in Meta products for short-form video recommendations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The intermediate "interest" layer mitigates data sparsity for light users by leveraging co-engagement graphs to infer shared interests with heavy users.
- Mechanism: Clustering the user-item bipartite graph into an item-item co-engagement graph, then applying Louvain community detection, creates clusters of items that represent user interests. Light users are linked to interests inferred from their interactions and those of similar users, improving preference modeling.
- Core assumption: Co-engagement patterns reliably reflect underlying user interests, and these patterns can be meaningfully clustered into interpretable interest groups.
- Evidence anchors: [abstract] "This method enhances the understanding of light users' preferences by linking them with heavy users." [section] "We propose a novel approach that efficiently constructs user interest and facilitates low computational cost inference by clustering engagement graphs and incorporating user-interest attention."
- Break condition: If co-engagement does not align with actual user interests (e.g., due to noise or irrelevant interactions), the interest clusters will misrepresent user preferences and degrade performance.

### Mechanism 2
- Claim: Interest-level attention reduces computational complexity by limiting similarity calculations to a sampled subset of interest clusters instead of all items.
- Mechanism: After clustering items into K interest clusters, the system samples N clusters per user and applies KNN within those clusters. This reduces inference complexity from O(|I|) to O(N·|I|/K), enabling scalable serving.
- Core assumption: Interest clusters are coherent and representative enough that sampling a small number of them still covers most relevant items for a user.
- Evidence anchors: [abstract] "Departing from traditional item-level attention, our model pivots towards interest-level attention... significantly optimizing the computational efficiency during the inference phase." [section] "Our approach significantly streamlines the serving cost, confining the KNN operations to a limited subset of clusters."
- Break condition: If N is too small or clusters are poorly formed, the system may miss relevant items, hurting recommendation quality.

### Mechanism 3
- Claim: Integrating interest profiles as input features preserves visibility of tail interests, counteracting popularity bias.
- Mechanism: Interest clusters are constructed from co-engagement graphs with potential debiasing (e.g., constrained Louvain), ensuring that niche interests are represented. These profiles are concatenated with user embeddings so the model always considers all interests during training.
- Evidence anchors: [abstract] "By integrating user-interest attention, our approach allows a more personalized similarity metric, adept at capturing the complex dynamics of user-item interactions." [section] "The clustering phase inherently offers a mechanism to counteract popularity bias... promoting a more balanced representation of both dominant and niche interests within the clusters."
- Break condition: If the clustering algorithm itself is biased toward popular items or if interest profiles are not well integrated, popularity bias may persist.

## Foundational Learning

- Concept: User-item bipartite graph transformation into item-item co-engagement graph.
  - Why needed here: The co-engagement graph captures shared user behavior patterns, which are the basis for inferring item clusters that represent interests.
  - Quick check question: Given users who interact with items A, B, and C, which item pairs would be connected in the co-engagement graph?

- Concept: Louvain community detection algorithm.
  - Why needed here: Louvain efficiently partitions the item-item co-engagement graph into clusters (communities) that likely reflect shared user interests.
  - Quick check question: What is the objective function optimized by Louvain, and why is it suitable for interest clustering?

- Concept: Personalized PageRank (PPR) for user-interest assignment.
  - Why needed here: PPR identifies the most relevant items for a user in the bipartite graph; these items' cluster labels become the user's interest profile.
  - Quick check question: How does PPR differ from standard PageRank, and why is that difference important for assigning interests to users?

## Architecture Onboarding

- Component map:
  - User-item interactions -> co-engagement graph construction
  - Item-item graph -> Louvain community detection -> K interest clusters
  - Bipartite graph + clusters -> PPR -> ηu (user interest weights)
  - User tower huser(·) and item tower hitem(·)
  - W1ηu ⊕ xu -> huser -> eu
  - Optional αu,c weights for interest-level attention
  - KNN search within sampled interest clusters

- Critical path:
  1. Preprocess interactions -> build co-engagement graph
  2. Cluster items -> assign interest clusters
  3. Compute user interest profiles (PPR)
  4. Train two-tower model with interest fusion
  5. At inference, sample user interests -> KNN within clusters

- Design tradeoffs:
  - Cluster granularity (K): larger K -> finer interests but higher computation; smaller K -> coarser but faster
  - Number of sampled clusters (N): larger N -> better coverage but slower; smaller N -> faster but risk of missing items
  - Interest representation: concatenation vs. attention weighting - concatenation is simpler; attention is more flexible but adds complexity

- Failure signatures:
  - Low recommendation precision/recall: clusters may be too coarse or not aligned with true interests
  - High inference latency: N too large or clustering not effective
  - Training instability: interest profiles too noisy or not well integrated

- First 3 experiments:
  1. Vary K (cluster count) and measure recommendation accuracy vs. inference time to find optimal tradeoff
  2. Compare interest-level attention vs. direct concatenation for integrating ηu into user embeddings
  3. Evaluate impact of sampling N clusters at inference vs. using all clusters on both quality and latency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the stability of interest clusters impact recommendation performance over time?
- Basis in paper: [explicit] The paper mentions that the Louvain community detection algorithm is used to construct user interest clusters and states that the cluster assignment is relatively stable, with ARI values ranging from 0.75 to 0.85 for different time intervals.
- Why unresolved: The paper does not provide detailed analysis on how changes in cluster stability over longer periods might affect recommendation accuracy or user satisfaction.
- What evidence would resolve it: Longitudinal studies tracking recommendation performance and user engagement metrics as cluster stability varies over extended periods.

### Open Question 2
- Question: What are the specific debiasing techniques integrated into the clustering algorithm to address popularity bias?
- Basis in paper: [explicit] The paper mentions that debiased PPR and Louvain clustering with constraints on cluster sizes can be used to diminish the influence of overly popular items, but does not provide specific details on the implementation of these techniques.
- Why unresolved: The paper only briefly mentions the potential for debiasing techniques without elaborating on their specific implementation or effectiveness.
- What evidence would resolve it: Detailed descriptions of the debiasing techniques used, along with quantitative comparisons of their effectiveness in reducing popularity bias.

### Open Question 3
- Question: How does the UIC method perform compared to other state-of-the-art recommendation algorithms on larger, more diverse datasets?
- Basis in paper: [inferred] The paper evaluates UIC on two public datasets (MovieLens-1M and Recipe) and mentions successful deployment in Meta products, but does not provide comparisons with other state-of-the-art algorithms on larger, more diverse datasets.
- Why unresolved: The paper's evaluation is limited to two specific datasets, and there is no information on how UIC performs relative to other advanced recommendation methods on a broader range of data.
- What evidence would resolve it: Comparative studies of UIC against other leading recommendation algorithms on larger, more diverse datasets, including both accuracy and computational efficiency metrics.

## Limitations
- Implementation details for Louvain clustering, PPR computation, and attention fusion are underspecified
- Assumes co-engagement patterns reliably reflect user interests, which may not hold in all domains
- Choice of number of clusters (K) and sampled clusters (N) involves nontrivial tradeoffs requiring domain-specific tuning

## Confidence
- **High confidence**: The computational efficiency gains from interest-level attention and cluster sampling are well-supported by the theoretical complexity reduction and empirical timing results
- **Medium confidence**: The recommendation quality improvements (1.24-4% relative gains) are demonstrated on two datasets, but generalizability to other domains remains to be seen
- **Medium confidence**: The claims about mitigating popularity bias through clustering are plausible but would benefit from explicit bias measurements

## Next Checks
1. Conduct ablation studies varying the number of clusters (K) and sampled clusters (N) to quantify the precision-latency tradeoff curve and identify optimal values for different dataset characteristics
2. Measure popularity bias explicitly by comparing the distribution of recommended items against the overall item popularity distribution for both UIC and baseline methods
3. Test the method on additional datasets with different characteristics (e.g., higher sparsity, different domain) to assess robustness and generalizability beyond MovieLens and Recipe datasets