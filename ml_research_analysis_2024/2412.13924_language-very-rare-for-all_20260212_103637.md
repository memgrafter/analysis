---
ver: rpa2
title: Language verY Rare for All
arxiv_id: '2412.13924'
source_url: https://arxiv.org/abs/2412.13924
tags:
- translation
- language
- data
- arxiv
- gasque
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study presents LYRA, a novel approach combining LLM fine-tuning,\
  \ retrieval-augmented generation (RAG), and transfer learning to improve rare language\
  \ translation. The method focuses on two-way translation between French and Mon\xE9\
  gasque, a low-resource language with limited parallel data."
---

# Language verY Rare for All

## Quick Facts
- arXiv ID: 2412.13924
- Source URL: https://arxiv.org/abs/2412.13924
- Reference count: 19
- Key outcome: LYRA approach combining LLM fine-tuning, RAG, and transfer learning achieves competitive rare language translation performance on French-Monégasque

## Executive Summary
This study presents LYRA, a novel approach for rare language translation that combines LLM fine-tuning, retrieval-augmented generation (RAG), and transfer learning. The method is demonstrated on French-Monégasque translation, a low-resource language pair with limited parallel data. LYRA employs three key strategies: leveraging related high-resource languages through transfer learning, standardizing training data to improve quality, and using RAG to enhance test-time performance. Experiments on a newly created 10K+ sentence dataset demonstrate that LYRA frequently surpasses and consistently matches state-of-the-art encoder-decoder models while maintaining single-GPU accessibility.

## Method Summary
LYRA combines three strategies for rare language translation: transfer learning from related high-resource languages, data standardization, and RAG. The approach first performs preliminary fine-tuning on a high-resource language pair (French-Italian) to leverage grammatical similarity, then fine-tunes on the low-resource French-Monégasque data. Training data is standardized to fix capitalization, punctuation, and quotation marks. At inference, RAG retrieves the 10 most similar training examples based on French embeddings and includes them in the prompt. The methodology is evaluated on NLLB-200 and three LLMs (Llama-3.1-8B, Gemma-2-9B, Mistral-Nemo-12B) using BLEU, METEOR, and chrF++ scores.

## Key Results
- LYRA achieves BLEU scores of 32.83-35.25 for French-to-Monégasque translation
- LYRA achieves BLEU scores of 51.95-57.27 for Monégasque-to-French translation
- Data standardization and RAG strategies each provide significant improvements, with transfer learning further boosting performance
- Single-GPU training makes the approach accessible while matching state-of-the-art encoder-decoder models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning from French-Italian to French-Monégasque improves performance by exploiting grammatical similarity.
- Mechanism: Pre-training on high-resource French-Italian data transfers general translation patterns and structural understanding to the low-resource task, reducing the learning burden when fine-tuning on scarce French-Monégasque data.
- Core assumption: French and Italian share sufficient linguistic structure that generalizes to Monégasque, which is closely related to Italian.
- Evidence anchors: [abstract], [section 4], [corpus] (weak)
- Break condition: If Monégasque is not sufficiently similar to Italian, or if French-Italian data doesn't cover needed structural features.

### Mechanism 2
- Claim: Data standardization (fixing capitalization, punctuation, quotation marks) significantly improves translation performance in low-resource settings.
- Mechanism: Inconsistent formatting causes irregular tokenization and confuses the model. Standardizing ensures consistent token boundaries and representation, improving reliable mappings between source and target languages.
- Core assumption: Small formatting inconsistencies measurably impact model learning when data is scarce.
- Evidence anchors: [section 4], [section 5], [corpus] (weak)
- Break condition: If dataset is large enough, formatting inconsistencies become negligible compared to other noise sources.

### Mechanism 3
- Claim: RAG with nearest-neighbor examples from training set improves test-time performance for decoder-only LLMs.
- Mechanism: At inference, retrieves 10 most similar training examples based on French embeddings and includes them in the prompt, providing contextual examples that guide generation.
- Core assumption: Training data contains relevant examples for each test instance, and embedding model can meaningfully match French sentences to guide Monégasque generation.
- Evidence anchors: [abstract], [section 4], [section 5], [corpus] (weak)
- Break condition: If training data lacks diversity/relevance, or embedding model poorly represents language pairs.

## Foundational Learning

- Concept: Transfer learning in neural machine translation
  - Why needed here: Relies on transferring knowledge from French-Italian (high-resource) to French-Monégasque (low-resource) to compensate for scarce parallel data.
  - Quick check question: Why might transfer learning be more effective when source and target languages are grammatically similar?

- Concept: Data standardization and its impact on model performance
  - Why needed here: Authors show standardizing punctuation, capitalization, and quotation marks leads to significant BLEU score improvements, especially in low-resource settings.
  - Quick check question: How could inconsistent capitalization or punctuation in training data affect tokenization and downstream translation quality?

- Concept: Retrieval-Augmented Generation (RAG) and nearest-neighbor retrieval
  - Why needed here: RAG retrieves similar training examples at test time and includes them in the prompt, improving generation quality for decoder-only models.
  - Quick check question: What is the role of embeddings in retrieval-based methods, and why are French sentences used to retrieve Monégasque examples?

## Architecture Onboarding

- Component map:
  OCR -> parallel sentence creation -> manual annotation -> two versions (raw/curated)
  NLLB encoder-decoder + LLM decoder-only (Llama-3.1-8B, Gemma-2-9B, Mistral-Nemo-12B)
  Transfer learning: preliminary French-Italian fine-tuning -> French-Monégasque fine-tuning
  RAG system: French embeddings (BAAI/bge-multilingual-gemma2) -> nearest-neighbor retrieval (10 samples) -> prompt augmentation
  Evaluation: BLEU, METEOR, chrF++ scores on test set

- Critical path:
  1. Acquire and standardize parallel dataset
  2. Perform preliminary French-Italian fine-tuning (for LLMs)
  3. Fine-tune on French-Monégasque data
  4. Apply RAG at inference
  5. Evaluate translation quality

- Design tradeoffs:
  - Single-GPU training limits model size but increases accessibility
  - Using French embeddings for Monégasque retrieval may reduce relevance compared to Monégasque-specific embeddings
  - Data standardization improves performance but requires manual effort

- Failure signatures:
  - Overfitting on small dataset (rapid BLEU plateau and decline)
  - RAG degrading performance (e.g., LYRA-M in mo→fr direction)
  - Transfer learning backfiring (performance loss in mo→fr after Italian fine-tuning)

- First 3 experiments:
  1. Train NLLB and LLMs on raw vs. standardized French-Monégasque data; compare BLEU scores to measure data standardization impact.
  2. Apply RAG to LLMs (without transfer learning) and evaluate BLEU/METEOR gains for both translation directions.
  3. Perform French-Italian fine-tuning followed by French-Monégasque fine-tuning with RAG; compare results to baseline LLMs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LYRA's performance scale when applied to other low-resource language pairs beyond French-Monégasque?
- Basis in paper: [explicit] Focuses on French-Monégasque translation and notes that not all low-resource languages are related to high-resource ones, limiting transfer learning applicability.
- Why unresolved: Study only evaluates LYRA on single language pair, making it unclear whether methodology generalizes to other low-resource languages with different linguistic relationships.
- What evidence would resolve it: Testing LYRA on multiple low-resource language pairs with varying degrees of similarity to high-resource languages and comparing performance across these pairs.

### Open Question 2
- Question: What is the optimal number of retrieval instances (k) in RAG for maximizing translation performance across different model sizes?
- Basis in paper: [explicit] Uses fixed k=10 for retrieval but notes that RAG benefits vary by model, with LYRA-M showing performance degradation.
- Why unresolved: Study uses single k value without exploring impact of different k values on translation quality or investigating model-specific optimal k values.
- What evidence would resolve it: Systematic experiments varying k from 1 to 20 for each model type and measuring corresponding BLEU/METEOR score changes.

### Open Question 3
- Question: How does the quality of embeddings affect RAG performance when the target language is unknown to embedding models?
- Basis in paper: [explicit] Uses French sentences to generate embeddings for Monégasque translation tasks, acknowledging that Monégasque is unknown to embedding models.
- Why unresolved: Study uses single embedding model (BAAI/bge-multilingual-gemma2) without comparing alternative approaches or measuring impact of embedding quality on RAG effectiveness.
- What evidence would resolve it: Comparative experiments using different embedding models, including language-specific models when available, and measuring RAG performance degradation as a function of embedding quality.

## Limitations

- Narrow linguistic scope: Study focuses on single rare language pair (French-Monégasque) with ~10K training sentences, limiting generalizability to other language pairs.
- Embedding limitations: RAG relies on French embeddings to retrieve examples for Monégasque translation, potentially introducing semantic mismatches.
- Computational uncertainty: While single-GPU training is accessible, RAG retrieval process at inference could become bottleneck for production deployment.

## Confidence

**High Confidence**: Core claim that LYRA achieves competitive BLEU scores compared to encoder-decoder models is well-supported by experimental results across multiple LLM configurations and translation directions.

**Medium Confidence**: Mechanism explanations for why transfer learning and RAG work are reasonable but not definitively proven - study shows correlation but doesn't isolate causal factors or test alternative explanations.

**Low Confidence**: Claims about practical accessibility and widespread applicability of LYRA are speculative - study demonstrates success on one rare language pair with specific model sizes and training conditions but doesn't validate performance across different rare languages, model scales, or real-world deployment scenarios.

## Next Checks

1. **Cross-linguistic validation**: Apply LYRA to different rare language pair (e.g., Basque-Spanish or Welsh-English) to test whether transfer learning mechanism generalizes beyond French-Italian to French-Monégasque scenario. Measure whether similar BLEU improvements occur and whether choice of source language matters.

2. **Ablation study on RAG embeddings**: Replace French-based embedding retrieval with either Monégasque-specific embeddings (if available) or language-agnostic sentence embeddings to quantify impact of using French embeddings for Monégasque example retrieval. Compare retrieval relevance scores and translation performance.

3. **Data size sensitivity analysis**: Systematically vary training dataset size (e.g., 2K, 5K, 10K, 20K sentences) to determine minimum effective dataset size for each LYRA component. Identify at what point data standardization effects plateau and whether transfer learning remains beneficial as more monolingual data becomes available.