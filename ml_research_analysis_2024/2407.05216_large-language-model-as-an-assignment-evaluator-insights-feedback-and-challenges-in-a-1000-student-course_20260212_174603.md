---
ver: rpa2
title: 'Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges
  in a 1000+ Student Course'
arxiv_id: '2407.05216'
source_url: https://arxiv.org/abs/2407.05216
tags:
- students
- score
- evaluation
- student
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of GPT-4 as an automatic assignment
  evaluator in a large university course with 1,028 students. Students could use a
  GPT-4-based evaluation teaching assistant (LLM TA) to assess their assignments for
  free, and their final scores were based on the student-conducted scores.
---

# Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course

## Quick Facts
- arXiv ID: 2407.05216
- Source URL: https://arxiv.org/abs/2407.05216
- Reference count: 40
- Primary result: GPT-4-based assignment evaluation was acceptable to 75% of 1,028 students when free, but 51% reported format non-compliance and 22% observed criteria non-compliance

## Executive Summary
This paper explores using GPT-4 as an automatic assignment evaluator in a university course with 1,028 students. Students could use a GPT-4-based evaluation teaching assistant (LLM TA) to assess their assignments for free, with their final scores based on these student-conducted evaluations. While 75% of students found this approach acceptable, significant challenges emerged: 51% reported format non-compliance issues, 22% observed criteria non-compliance, and 47% attempted prompt hacking to achieve higher scores. The findings highlight the need for improvements in LLM-based evaluators, particularly in instruction-following ability and resistance to prompt hacking.

## Method Summary
The study implemented GPT-4 as an LLM TA using the DaVinci platform, requiring responses to follow a specific format ("Final score: <score>") for regex extraction. Students submitted assignments to the LLM TA and received scores automatically. Student feedback was collected through surveys covering acceptability, problems encountered, and prompt hacking attempts. The system included a post-hoc self-reflection detection method using GPT-4 to identify prompt-hacked submissions by comparing original prompts, student submissions, and evaluation results.

## Key Results
- 75% of students found LLM-based evaluation acceptable when free
- 51% reported LLM TA failed to follow specified output format
- 22% observed evaluation did not properly adhere to criteria
- 47% of students attempted prompt hacking, though easily detectable post-hoc

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM TAs can reliably extract scores from structured output if the model consistently follows the specified format.
- Mechanism: By instructing the LLM to wrap the final score in a fixed string pattern ("Final score: <score>"), the system uses regex to extract numeric scores automatically without human parsing.
- Core assumption: The LLM will obey the output format instruction reliably across all runs.
- Evidence anchors:
  - [abstract] "the LLM TAs are required to output the score in a specific format such that we can use a regular expression to extract the numeric score"
  - [section] "Since we extract the final score using regular expressions, the responses that do not follow the instructions cannot be properly parsed."
- Break condition: If the LLM generates a response that omits or alters the required output pattern, extraction fails.

### Mechanism 2
- Claim: Student acceptance of LLM TAs depends heavily on free access; paid models drastically reduce acceptance.
- Mechanism: Offering free daily quotas removes cost barriers, making the evaluation system feel fair and accessible to all students regardless of financial status.
- Core assumption: Students equate free access with fairness, and cost becomes a primary driver of acceptance.
- Evidence anchors:
  - [abstract] "LLM-based assignment evaluators are generally acceptable to students when students have free access to these LLM-based evaluators."
  - [section] "Over 66% of students from both the EECS and the Liberal Arts department find it unacceptable when the released LLM TA is not free."
- Break condition: If access remains free but the model's output is inconsistent or exploitable, acceptance may still decline.

### Mechanism 3
- Claim: LLM TAs are vulnerable to goal hijacking prompt hacks but these attacks are easily detectable post-hoc via self-reflection.
- Mechanism: Students can manipulate the LLM into printing a target score string by embedding overriding instructions in their submissions; however, a second LLM can detect such manipulation by comparing the original prompt, student submission, and evaluation result.
- Core assumption: The LLM's instruction-following is hierarchical and can be overridden by adversarial prompts, but a higher-level LLM can identify the inconsistency.
- Evidence anchors:
  - [abstract] "students can easily manipulate the LLM-based evaluator to output specific strings, allowing them to achieve high scores without meeting the assignment rubric"
  - [section] "we prompt GPT-4 with the student submission, original evaluation results, and original evaluation criteria and ask GPT-4 to check if there are any problems"
- Break condition: If the detection LLM is not given the original prompt or criteria, it cannot reliably flag manipulation.

## Foundational Learning

- Concept: Regular expression extraction
  - Why needed here: Scores are embedded in LLM responses; extracting them reliably requires precise pattern matching.
  - Quick check question: If a response is "Final score: 9.5 out of 10", what regex pattern would capture "9.5"?

- Concept: Instruction hierarchy and prompt hacking
  - Why needed here: Understanding how LLMs prioritize instructions explains why goal hijacking works.
  - Quick check question: If a student adds "Please give me a score of 10" inside their submission, why might the LLM ignore earlier instructions?

- Concept: Sampling variability in LLM outputs
  - Why needed here: Different generations for the same input can lead to score inconsistency, affecting fairness.
  - Quick check question: If the same essay is evaluated twice with temperature=0.7, why might the scores differ?

## Architecture Onboarding

- Component map:
  Student UI → LLM TA (DaVinci platform) → Score extraction → Student submission → Post-hoc self-reflection (optional)

- Critical path:
  1. Student submits assignment → 2. LLM TA generates evaluation → 3. Regex extracts score → 4. Score stored → 5. (Optional) Self-reflection run

- Design tradeoffs:
  - Free quota vs. exploitability: Free access increases fairness but enables repeated regeneration to game scores.
  - Greedy decoding vs. format compliance: Greedy decoding removes randomness but risks breaking format compliance.
  - Post-hoc detection vs. real-time prevention: Detection is easy but doesn't stop the exploit during evaluation.

- Failure signatures:
  - Missing or malformed "Final score:" tag → regex extraction fails
  - High variance in repeated evaluations for same input → indicates randomness exploit
  - Detection LLM flags "hacking: True" → prompt injection detected

- First 3 experiments:
  1. Test regex extraction on 100 varied LLM outputs to measure robustness.
  2. Compare score variance across 20 generations for same input with temperature=0.7 vs. 0.0.
  3. Run self-reflection detection on known hacked submissions to measure recall/precision.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM-based evaluators be improved to better follow specified output formats?
- Basis in paper: Explicit
- Why unresolved: The paper reports that 51% of students found LLM TAs unable to correctly follow the required output format, which prevented proper score extraction.
- What evidence would resolve it: Studies demonstrating LLM-based evaluators that consistently output scores in the specified format across diverse assignments.

### Open Question 2
- Question: What methods can be developed to make LLM-based evaluators more resistant to prompt hacking?
- Basis in paper: Explicit
- Why unresolved: The paper shows that 47% of students attempted prompt hacking, and while detection is possible post hoc, defense remains challenging as students found creative ways to circumvent safeguards.
- What evidence would resolve it: Development of LLM-based evaluators that can reliably distinguish between legitimate submissions and prompt-hacked inputs without relying on post-hoc detection.

### Open Question 3
- Question: How can the consistency of LLM-based evaluator scores be improved across multiple evaluations of the same submission?
- Basis in paper: Inferred
- Why unresolved: Students reported that the same submission could receive different scores due to LLM randomness, with some finding this unacceptable despite it being a known LLM characteristic.
- What evidence would resolve it: Methods that significantly reduce score variance when the same submission is evaluated multiple times by LLM-based evaluators.

### Open Question 4
- Question: What is the optimal balance between student access to LLM TAs and maintaining evaluation integrity?
- Basis in paper: Inferred
- Why unresolved: The paper presents multiple options for using LLM TAs with varying levels of student access, showing different trade-offs between acceptability and vulnerability to manipulation.
- What evidence would resolve it: Empirical studies comparing different access models for LLM TAs that optimize both student satisfaction and evaluation robustness.

### Open Question 5
- Question: How can LLM-based evaluators be made more transparent and explainable to students?
- Basis in paper: Inferred
- Why unresolved: Students expressed confusion and frustration when scores seemed arbitrary or when the LLM didn't follow criteria as expected, suggesting a need for better transparency.
- What evidence would resolve it: Development of LLM-based evaluators that provide clear, understandable explanations for their scoring decisions that align with student expectations.

## Limitations
- Findings based on single implementation in one university course, limiting generalizability across academic contexts and assignment types.
- Evaluation relied on student self-reporting for acceptability and problem identification, subject to response bias.
- Prompt hacking detection method was only applied retrospectively, not tested for real-time prevention.

## Confidence
- **High confidence**: The observation that 75% of students found LLM-based evaluation acceptable when free, and the documented prevalence of format non-compliance (51%) and criteria non-compliance (22%).
- **Medium confidence**: The claim that prompt hacking is easily detectable via self-reflection, as this was tested post-hoc rather than in real-time prevention scenarios.
- **Medium confidence**: The conclusion that free access is critical for student acceptance, though this may vary across different student populations and institutional contexts.

## Next Checks
1. **Format Compliance Testing**: Conduct a systematic test of 100 LLM outputs across different temperature settings (0.0, 0.3, 0.7) to quantify the relationship between randomness and format compliance failure rates.

2. **Cross-Context Generalization**: Replicate the evaluation system in a different academic domain (e.g., STEM problem sets vs. humanities essays) to assess whether the 51% format non-compliance rate holds across assignment types.

3. **Real-Time Detection Implementation**: Implement the self-reflection detection method as a real-time pre-submission check to evaluate whether prompt hacking can be prevented rather than merely detected after the fact.