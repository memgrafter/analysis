---
ver: rpa2
title: 'SwiftRL: Towards Efficient Reinforcement Learning on Real Processing-In-Memory
  Systems'
arxiv_id: '2405.03967'
source_url: https://arxiv.org/abs/2405.03967
tags:
- learning
- cores
- memory
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SwiftRL accelerates RL workloads on real-world PIM systems to overcome
  memory-bound performance bottlenecks during RL training. The work adapts Tabular
  Q-learning and SARSA algorithms on UPMEM PIM systems using performance optimizations
  like Q-value function approximation and PIM-specific routines.
---

# SwiftRL: Towards Efficient Reinforcement Learning on Real Processing-In-Memory Systems

## Quick Facts
- arXiv ID: 2405.03967
- Source URL: https://arxiv.org/abs/2405.03967
- Reference count: 40
- Key outcome: PIM implementations outperform Intel Xeon Silver 4110 CPU by at least 1.62× and NVIDIA RTX 3090 GPU by at least 4.84×

## Executive Summary
SwiftRL addresses the memory-bound nature of reinforcement learning training by implementing Q-learning and SARSA algorithms on real Processing-In-Memory (PIM) systems. The work demonstrates that RL workloads, which require frequent access to large experience datasets, can achieve significant performance gains when computation is moved closer to memory. Using the UPMEM PIM architecture, SwiftRL implements performance optimizations including Q-value function approximation and fixed-point arithmetic to overcome hardware limitations. Experiments show near-linear scaling of 15× performance when increasing PIM cores from 125 to 2000, with PIM implementations outperforming traditional CPU and GPU systems.

## Method Summary
SwiftRL implements Tabular Q-learning and SARSA algorithms on UPMEM PIM systems by adapting standard RL algorithms to work with fixed-point arithmetic and PIM-specific programming models. The method involves partitioning experience datasets across PIM cores, executing parallel Q-value updates directly in memory, and aggregating results on the host CPU. Key optimizations include approximating the Q-value update function to avoid expensive floating-point emulation and implementing custom PIM-specific routines for random sampling. The implementation uses 32-bit fixed-point representations with a scaling factor of 10,000 to handle the integer-only nature of UPMEM DPUs.

## Key Results
- Near-linear scaling of 15× performance when increasing PIM cores from 125 to 2000
- PIM implementations outperform Intel Xeon Silver 4110 CPU by at least 1.62×
- PIM implementations outperform NVIDIA RTX 3090 GPU by at least 4.84×

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RL training workloads are memory-bound, not compute-bound.
- Mechanism: RL training requires repeated access to large experience datasets stored in memory. The frequent data movement between memory and processor units creates a bottleneck that limits performance, regardless of compute capability.
- Core assumption: The roofline model analysis showing RL workloads in the memory-bound region is accurate and representative of typical RL training scenarios.
- Evidence anchors:
  - [abstract] "RL training is significantly memory-bound due to sampling large experience datasets that may not fit entirely into the hardware caches and frequent data transfers needed between memory and the computation units"
  - [section] "We employ a roofline model [44] to visualize the extent to which our workloads are constrained by memory bandwidth and computational limits. Figure 2 shows the roofline model... both the Q-learner and SARSA-learner CPU versions are in the memory-bound region"
  - [corpus] Weak evidence - no direct comparison of memory vs compute bottlenecks in corpus papers
- Break condition: If RL algorithms could be modified to reduce memory access frequency or if datasets could be compressed to fit in cache, the memory-bound assumption breaks down.

### Mechanism 2
- Claim: PIM architecture accelerates RL training by eliminating data movement between memory and processors.
- Mechanism: Processing elements are placed inside or close to memory banks where the experience datasets reside. This allows RL computations (Q-value updates) to happen directly where data is stored, avoiding costly data transfers.
- Core assumption: The PIM architecture's processing elements can efficiently perform the required RL computations without excessive overhead.
- Evidence anchors:
  - [abstract] "PIM systems' potential as accelerators for memory-intensive RL workloads"
  - [section] "The Processing-In-Memory (PIM) [11, 12, 15–17] computing paradigm, which places the processing elements inside or close to the memory chips, is well positioned to address the performance bottlenecks of memory-intensive workloads"
  - [corpus] Moderate evidence - related works show PIM acceleration for other memory-intensive workloads like GNNs and DLRMs
- Break condition: If PIM processing elements are significantly slower than traditional processors for the required operations, or if data transfer overhead between PIM cores and host CPU becomes prohibitive.

### Mechanism 3
- Claim: Fixed-point arithmetic optimization overcomes PIM hardware limitations.
- Mechanism: Since UPMEM PIM cores only support native integer arithmetic, the implementation converts floating-point operations to fixed-point representations using scaling factors, avoiding expensive runtime emulation of floating-point operations.
- Core assumption: The fixed-point approximation maintains sufficient precision for RL algorithm convergence while significantly improving performance.
- Evidence anchors:
  - [abstract] "We implement performance optimization strategies during RL adaptation to the PIM system via approximating the Q-value update function (which avoids high performance costs due to emulation used by runtime libraries)"
  - [section] "To tackle this, we adopt 32-bit fixed-point representations (Section 4.3)" and "We scale up the reward r for each experience Ti, learning rate α = 0.1, and the discount factor γ = 0.95 by using a constant scale factor=10,000"
  - [corpus] Moderate evidence - related PIM works also use fixed-point arithmetic for ML workloads
- Break condition: If the fixed-point approximation introduces significant error that prevents RL algorithms from converging to optimal policies, or if scaling factors cannot be chosen appropriately for all environments.

## Foundational Learning

- Concept: Roofline model analysis
  - Why needed here: To understand why RL workloads are memory-bound and suitable for PIM acceleration
  - Quick check question: What metric determines if a workload is compute-bound vs memory-bound in a roofline model?

- Concept: Single Program Multiple Data (SPMD) programming model
  - Why needed here: UPMEM PIM uses SPMD, requiring understanding of how to partition data across PIM cores
  - Quick check question: How does SPMD differ from SIMD in terms of data distribution and execution?

- Concept: Q-learning and SARSA algorithm fundamentals
  - Why needed here: These are the specific RL algorithms being implemented and optimized
  - Quick check question: What is the key difference between Q-learning and SARSA update rules?

## Architecture Onboarding

- Component map: Host CPU with standard main memory → memory channel → PIM-enabled memory modules → PIM chips → 8 DRAM banks per chip → programmable PIM core (DPU) with 24KB instruction memory and 64KB scratchpad memory
- Critical path: Data transfer from host CPU to PIM cores → parallel Q-value updates on PIM cores → partial results transfer back to host CPU → aggregation of final Q-table
- Design tradeoffs: Fixed-point vs floating-point precision (performance vs accuracy), core-level vs thread-level parallelism (simplicity vs potential performance), PIM-CPU vs inter-PIM communication overhead (depends on dataset size and synchronization requirements)
- Failure signatures: Poor performance scaling when increasing PIM cores suggests communication bottlenecks; accuracy degradation suggests fixed-point approximation issues; slow kernel execution suggests inefficient utilization of PIM processing elements
- First 3 experiments:
  1. Measure execution time breakdown (PIM kernel, CPU-PIM transfer, PIM-CPU transfer, inter-PIM communication) with varying numbers of PIM cores to identify bottlenecks
  2. Compare accuracy of fixed-point vs floating-point implementations across different environments to determine acceptable precision levels
  3. Test different data layout strategies (sequential, random, stride-based sampling) to evaluate impact on memory access patterns and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SwiftRL scale when applied to more complex reinforcement learning environments with larger state-action spaces?
- Basis in paper: [explicit] The paper mentions evaluating on OpenAI Gym environments like frozen lake and taxi, but notes that more complex environments (e.g., Atari, StarCraft) typically require exploring a broader range of the state-action space.
- Why unresolved: The paper only demonstrates performance on relatively simple environments with limited state-action spaces. It does not evaluate SwiftRL on more complex environments that would better represent real-world applications.
- What evidence would resolve it: Experimental results showing SwiftRL's performance on complex environments like Atari or StarCraft, including scaling analysis with respect to state-action space size.

### Open Question 2
- Question: What is the impact of different synchronization periods (τ) on SwiftRL's training quality and performance?
- Basis in paper: [explicit] The paper mentions evaluating with different τ values (10, 25, 50) for the frozen lake environment and notes that training quality is "relatively same or slightly better than CPU implementation."
- Why unresolved: The paper does not provide a comprehensive analysis of how different synchronization periods affect either training quality or performance across different environments and workloads.
- What evidence would resolve it: Systematic experiments varying τ across multiple environments, showing trade-offs between training quality, convergence speed, and communication overhead.

### Open Question 3
- Question: How does SwiftRL's performance compare to emerging PIM architectures beyond UPMEM, such as HBM-PIM or AiM?
- Basis in paper: [explicit] The paper notes that SwiftRL's optimization strategies "can be deployed on other real PIM hardware, resembling the architecture illustrated in Figure 3" which includes HBM-PIM and AiM.
- Why unresolved: The paper only evaluates on UPMEM architecture and does not explore how SwiftRL would perform on other PIM architectures with different characteristics (e.g., different memory bandwidth, processing element capabilities).
- What evidence would resolve it: Performance comparisons of SwiftRL across multiple PIM architectures (UPMEM, HBM-PIM, AiM) using the same workloads and metrics.

## Limitations

- Limited evaluation to two simple OpenAI Gym environments (Frozen Lake and Taxi) with small state-action spaces
- Fixed-point arithmetic approximation may introduce accuracy degradation that affects RL convergence quality
- Scalability analysis limited to 125-2000 PIM cores, with performance at higher core counts unverified

## Confidence

*High Confidence Claims:*
- RL training is memory-bound based on roofline model analysis
- PIM systems can accelerate memory-intensive workloads
- Q-learning and SARSA implementations are correct and functional on PIM

*Medium Confidence Claims:*
- Fixed-point arithmetic provides sufficient precision for RL convergence
- Near-linear scaling holds across the tested core range
- Performance improvements over CPU/GPU are representative of typical RL workloads

*Low Confidence Claims:*
- Generalization to more complex RL environments
- Performance with larger datasets and higher core counts
- Applicability to non-tabular RL algorithms (e.g., deep RL)

## Next Checks

1. **Precision Validation**: Implement both fixed-point and floating-point versions of the RL algorithms and compare policy convergence quality across multiple environments with varying reward scales to quantify the accuracy tradeoff.

2. **Scalability Boundary**: Test the PIM implementation with 4000+ cores and larger dataset sizes (10M+ transitions) to identify communication bottlenecks and determine the true scalability limits.

3. **Algorithm Diversity**: Extend the implementation to Deep Q-Networks (DQN) using function approximation on PIM, comparing performance and accuracy against the tabular approach to evaluate PIM's effectiveness for modern RL algorithms.