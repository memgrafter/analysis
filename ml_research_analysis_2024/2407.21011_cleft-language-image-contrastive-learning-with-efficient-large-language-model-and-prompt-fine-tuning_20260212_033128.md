---
ver: rpa2
title: 'CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model
  and Prompt Fine-Tuning'
arxiv_id: '2407.21011'
source_url: https://arxiv.org/abs/2407.21011
tags:
- learning
- data
- language
- training
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of applying Contrastive Language-Image
  Pre-training (CLIP) to medical imaging domains where large labeled datasets are
  scarce. The authors propose CLEFT, a framework that leverages a pre-trained large
  language model (LLM) with parameter-efficient fine-tuning (PEFT) to reduce the total
  trainable model size by 39% while improving performance.
---

# CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning

## Quick Facts
- arXiv ID: 2407.21011
- Source URL: https://arxiv.org/abs/2407.21011
- Reference count: 33
- Primary result: Achieves state-of-the-art performance on chest X-ray and mammography datasets while reducing trainable parameters by 39% using LLM with PEFT

## Executive Summary
CLEFT introduces a novel framework for medical image-language contrastive learning that leverages pre-trained large language models with parameter-efficient fine-tuning and context-based prompt learning. The method addresses the challenge of applying CLIP to medical imaging domains where labeled datasets are scarce by using a GPT-2-based LLM instead of traditional BERT encoders, achieving significant parameter reduction while improving performance. CLEFT demonstrates superior results across zero-shot, linear probing, and full fine-tuning settings on two chest X-ray datasets and one mammography dataset.

## Method Summary
CLEFT employs a two-stage training process. First, it pre-trains a vision transformer (ViT-Base) and a pre-trained GPT-2 LLM with parameter-efficient fine-tuning modules (LoRA, IA3, or Prefix-tuning) using contrastive loss to align image and text embeddings. Second, it freezes the pre-trained encoders and trains context-based prompts using zero-shot classification loss to adapt to different class labels. The framework uses BioMedLM-3B as the LLM, reducing the total trainable model size by 39% compared to vanilla CLIP while maintaining state-of-the-art performance on medical imaging tasks.

## Key Results
- Achieves state-of-the-art performance on CheXpert-5x200, RSNA, and EMBED datasets
- Reduces total trainable parameters by 39% with only 4% of trainable language model parameters compared to vanilla CLIP
- Outperforms existing CLIP-based methods in zero-shot, linear probing, and full fine-tuning settings
- Demonstrates robust performance across chest X-ray and mammography imaging tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a pre-trained medical LLM with PEFT provides stronger supervision for contrastive learning than a small BERT model
- Mechanism: The LLM embeds prompts into a higher-quality feature space, improving alignment between visual and textual representations
- Core assumption: The LLM's pre-trained embedding space generalizes well to medical domain prompts after minimal PEFT adjustment
- Evidence anchors:
  - "Our method demonstrates state-of-the-art performance on multiple chest X-ray and mammography datasets compared with various baselines"
  - "We use a GPT-2-based causal language model as our text encoder rather than a BERT-based language model"
  - "the causal LLM has shown a better capability as it scales up to over a billion parameters"
- Break condition: If medical domain shift between LLM pre-training and medical prompts is too large, PEFT cannot bridge the gap

### Mechanism 2
- Claim: Prompt context learning with frozen encoders improves generalization by adapting to diverse class labels
- Mechanism: Trainable prompt tokens are optimized with cross-entropy loss to better match unseen class names while encoders remain fixed
- Core assumption: The pre-trained encoders provide sufficient feature extraction capability for prompt learning to succeed
- Evidence anchors:
  - "we present an efficient strategy for learning context-based prompts that mitigates the gap between informative clinical diagnostic data and simple class labels"
  - "we optimize the trainable context tokens with a zero-shot classification cross-entropy loss"
- Break condition: If prompt context length is too short to capture class distinctions, or if encoders degrade during pre-training

### Mechanism 3
- Claim: Parameter-efficient fine-tuning preserves LLM knowledge while reducing training cost
- Mechanism: Small adaptation layers (LoRA/IA3/Prefix) adjust LLM outputs without updating full parameters
- Core assumption: Minimal parameter updates can adapt LLM embeddings for medical contrastive learning
- Evidence anchors:
  - "Our model reduces the total number of trainable parameters by 39% with only 4% of the trainable language model parameters compared to the vanilla CLIP"
  - "we introduce the parameter-efficient fine-tuning (PEFT) module to the frozen LLM"
- Break condition: If PEFT adapters are insufficient to adapt embeddings, catastrophic forgetting occurs

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: Forms the core objective that aligns image and text embeddings in shared space
  - Quick check question: How does the InfoNCE loss balance positive pair attraction and negative pair repulsion?

- Concept: Vision transformers for image encoding
  - Why needed here: ViT provides patch-based token embeddings that interface with transformer-based text encoders
  - Quick check question: Why does averaging ViT token embeddings work for contrastive learning?

- Concept: Multi-modal representation alignment
  - Why needed here: Critical for medical applications where text reports and images must be semantically connected
  - Quick check question: What challenges arise when aligning high-dimensional medical image features with text embeddings?

## Architecture Onboarding

- Component map: Image → ViT → avg pooling → projection → contrastive loss; Text → GPT-2 → PEFT → projection → contrastive loss; Final stage: Prompt tokens → GPT-2 → classification loss

- Critical path: Image → ViT → avg pooling → projection → contrastive loss; Text → GPT-2 → PEFT → projection → contrastive loss; Final stage: Prompt tokens → GPT-2 → classification loss

- Design tradeoffs:
  - LLM size vs. computational cost (39% fewer trainable params)
  - PEFT method selection (LoRA, IA3, Prefix-tuning)
  - Prompt context length (30 tokens chosen empirically)

- Failure signatures:
  - Poor zero-shot performance → mismatched embedding spaces
  - Overfitting on training set → insufficient prompt diversity
  - Memory errors → PEFT configuration too large

- First 3 experiments:
  1. Verify contrastive loss decreases during pre-training with frozen LLM
  2. Test zero-shot classification on held-out validation set
  3. Compare PEFT methods (LoRA vs IA3 vs Prefix) on accuracy vs parameter count

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal prompt lengths for context-based prompts across different medical imaging tasks?
- Basis in paper: The paper evaluates different prompt lengths (L = 30) but notes that "Increasing the prompt length does not always improve the performance according to this experiment"
- Why unresolved: The paper only experiments with a limited range of prompt lengths (L = 30) and doesn't systematically explore the optimal prompt length for different tasks or datasets
- What evidence would resolve it: Systematic experiments varying prompt lengths (e.g., L = 10, 20, 30, 40, 50) across multiple medical imaging tasks and datasets, analyzing the relationship between prompt length and performance

### Open Question 2
- Question: How do different PEFT methods (LoRA, IA3, Prefix-tuning) compare in terms of performance and computational efficiency for medical imaging tasks?
- Basis in paper: The paper experiments with LoRA, IA3, and Prefix-tuning but only reports final results without detailed comparative analysis of their trade-offs
- Why unresolved: The paper mentions using these methods but doesn't provide a detailed comparative analysis of their strengths and weaknesses for medical imaging tasks
- What evidence would resolve it: Detailed ablation studies comparing the three PEFT methods across multiple metrics (accuracy, AUC, training time, memory usage) for different medical imaging tasks

### Open Question 3
- Question: Can the proposed CLEFT framework be extended to other medical imaging modalities beyond chest X-rays and mammography?
- Basis in paper: The paper suggests "The proposed framework can be easily merged with other related CLIP methods like MedCLIP [27] to further improve the performance" and mentions potential for other medical domains
- Why unresolved: The paper only evaluates the method on chest X-rays and mammography datasets, leaving the question of generalizability to other medical imaging modalities unanswered
- What evidence would resolve it: Experiments applying CLEFT to other medical imaging modalities (e.g., CT scans, MRI, pathology slides) and comparing performance with existing methods on these tasks

## Limitations

- Limited baseline comparison with only CLIP-Med in results tables despite mentioning multiple baselines
- Unusual performance pattern where full fine-tuning sometimes underperforms linear probing, suggesting potential implementation issues
- Prompt learning evaluation constrained to single context length (30 tokens) without systematic exploration of optimal lengths

## Confidence

- High Confidence: Technical feasibility of PEFT with LLMs for medical CLIP training and 39% parameter reduction claims
- Medium Confidence: Core claim that LLM-based text encoders outperform BERT-based encoders for medical imaging
- Low Confidence: Generalizability of prompt learning approach across diverse medical imaging tasks and performance robustness claims

## Next Checks

1. **Cross-Modality Transfer**: Evaluate CLEFT's zero-shot performance on a completely different medical imaging modality (e.g., CT scans or pathology slides) using the same pre-trained model to assess true generalization capability beyond chest X-ray and mammography.

2. **Ablation on PEFT Methods**: Systematically compare the three PEFT methods (LoRA, IA3, Prefix-tuning) under identical training conditions and report their individual contributions to performance gains versus parameter reduction to identify the most effective approach for medical imaging.

3. **Extended Baseline Comparison**: Include comprehensive comparisons with additional medical imaging baselines such as DINO, MoCo, and other recent CLIP variants, particularly focusing on methods that use medical domain-specific pre-training to contextualize the claimed performance improvements.