---
ver: rpa2
title: 'Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared
  Prefixes in LLMs'
arxiv_id: '2403.08845'
source_url: https://arxiv.org/abs/2403.08845
tags:
- attention
- context
- latency
- bifurcated
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Bifurcated attention addresses high memory I/O costs in massively\
  \ parallel decoding by splitting the attention mechanism into two separate GEMM\
  \ operations\u2014one for the KV cache from the initial context encoding and another\
  \ for subsequent incremental decoding steps. This approach maintains computational\
  \ equivalence while significantly reducing memory I/O."
---

# Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs

## Quick Facts
- arXiv ID: 2403.08845
- Source URL: https://arxiv.org/abs/2403.08845
- Reference count: 40
- Key outcome: Over 2.1× speedup for 16-sequence sampling and more than 6.2× speedup for 32-sequence sampling at context lengths exceeding 8k tokens on a 7B model

## Executive Summary
Bifurcated attention is a technique that accelerates massively parallel decoding in large language models by splitting the attention mechanism into two separate GEMM operations—one for the KV cache from the initial context encoding and another for subsequent incremental decoding steps. This approach addresses the high memory I/O costs that dominate latency in high batch sizes and extended context lengths. The method maintains computational equivalence while significantly reducing redundant memory reads, enabling real-time massively parallel answer generation without substantial latency increases. When combined with multi-query attention and post-processing techniques like re-ranking, it enables substantial improvements in throughput and performance metrics like pass@k in code generation tasks.

## Method Summary
The method addresses memory I/O bottlenecks in transformer inference by recognizing that during incremental decoding with shared prefixes, the KV cache for the context prefix is identical across all batch sequences. Bifurcated attention splits the attention computation into two parts: one computing attention between the query and the context KV cache (loaded once), and another computing attention between the query and the incremental KV cache (loaded per batch element). This reduces memory reads from O(bm) to O(mc + bmd) where mc is context length and md is incremental length. The approach is compatible with multi-query attention, which further compresses the KV cache by using fewer attention groups, creating multiplicative memory savings.

## Key Results
- Over 2.1× speedup for 16-sequence sampling and more than 6.2× speedup for 32-sequence sampling at context lengths exceeding 8k tokens on a 7B model
- Enables batch size increases from 5 to 128 in CodeGen 16B multi-head models, improving pass@k from 59.0% to 84.6%
- Maintains exact computational equivalence while reducing memory I/O
- Bifurcated attention + multi-head rivals multi-query performance in latency, while multi-query offers greater memory savings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bifurcated attention splits the KV cache into context-shared and incremental parts to reduce redundant memory I/O.
- Mechanism: The KV cache for the context (Kc) is identical across all batch indices. By loading Kc once and reusing it for each query in the batch, memory reads are reduced from O(bm) to O(mc + bmd) where mc is context length and md is incremental length.
- Core assumption: The KV cache for the shared context prefix is identical across all batch indices in single-context batch sampling.
- Evidence anchors:
  - [abstract] "Our approach addresses the challenge of redundant memory IO costs, a critical factor contributing to latency in high batch sizes and extended context lengths."
  - [section 4.1] "Since tensor Kc is the same across all indices in the b axis, we can also represent Kc with a more compact shape 1hmck or simply hmck."
  - [corpus] Weak evidence: only one neighbor paper (Hydragen) discusses shared prefixes, but does not detail the KV split mechanism.
- Break condition: If batch sequences have different contexts or if the context KV cache is not identical, the memory I/O savings disappear.

### Mechanism 2
- Claim: Multi-query attention compresses KV keys/values to a single head, reducing memory I/O by a factor of h/g.
- Mechanism: With g attention groups, each group shares a single head for keys/values, reducing the KV cache size from bhmk to bgmk, lowering memory reads proportionally.
- Core assumption: Reducing the number of attention groups g proportionally reduces KV cache size and memory I/O.
- Evidence anchors:
  - [section 3.3] "The memory IO complexity for the multi-query attention becomes bgmk compared to bhmk in the multi-head setting, a reduction by a factor of h/g times."
  - [section 5.2] "For multi-query (g = 1), the gain can be substantial as well in the case of high mc or b."
  - [corpus] Weak evidence: no direct mention of multi-query attention in neighbor papers.
- Break condition: If g approaches h (multi-head), memory I/O benefits vanish; if g=1, representation power drops requiring larger models.

### Mechanism 3
- Claim: Combining bifurcated attention with multi-query enables extreme batch sizes and context lengths without significant latency increase.
- Mechanism: Bifurcated attention reduces redundant context KV reads, while multi-query further compresses KV representation, yielding multiplicative savings in memory I/O.
- Core assumption: Both techniques are compatible and their effects on memory I/O are additive.
- Evidence anchors:
  - [section 5.2.2] "With multi-query attention, bifurcated attention permits us to use batch sizes as high as 256 or 512 with lower latency than in the multi-head scenario."
  - [section 5.2] "Bifurcated attention + multi-head rivals multi-query... multi-head is even has lower latency."
  - [corpus] Weak evidence: neighbor papers discuss shared prefixes but not the specific combination of multi-query + bifurcated attention.
- Break condition: If the workload does not exhibit large shared prefixes or if the batch size is small, gains diminish.

## Foundational Learning

- Concept: KV cache and its role in transformer inference
  - Why needed here: Understanding why KV cache memory I/O dominates latency in incremental decoding is key to grasping bifurcated attention's motivation.
  - Quick check question: In incremental decoding, why does the KV cache size scale with both batch size b and context length m? What is the memory I/O implication?

- Concept: Attention groups and multi-query attention
  - Why needed here: The paper leverages the generalized multi-query framework; knowing how g affects KV cache size and model expressiveness is essential.
  - Quick check question: How does changing g from 1 (multi-query) to h (multi-head) affect the KV cache memory I/O and the number of model parameters?

- Concept: Einstein summation notation for tensor operations
  - Why needed here: The paper uses einsum notation to express attention operations; understanding it helps decode the proposed bifurcated attention equations.
  - Quick check question: Translate the einsum "bgpnk, bgmk → bgpnm" into a high-level description of the operation being performed.

## Architecture Onboarding

- Component map:
  - Input query tensor q (shape: b × g × p × n × k)
  - Key cache Kc (context) (shape: g × m_c × k)
  - Key cache Kd (incremental) (shape: b × g × m_d × k)
  - Value cache Vc, Vd analogous to Kc, Kd
  - Bifurcated attention splits attention into ⟨q, Kc⟩ and ⟨q, Kd⟩, concatenates results
  - Weight-value attention ⟨w, V⟩ splits similarly and sums

- Critical path:
  - Incremental decoding step → Load q → Load Kc (once) → Load Kd (b times) → Compute ⟨q, Kc⟩ and ⟨q, Kd⟩ → Concatenate → Compute ⟨w, V⟩ → Concatenate/sum → Output logits

- Design tradeoffs:
  - Multi-query vs multi-head: lower memory I/O but reduced representation power; compensated by model size scaling.
  - Bifurcated attention: exact computation, lower memory I/O, but potential kernel fusion and parallelization overhead for small workloads.
  - Model quantization: orthogonal to bifurcated attention; may further reduce memory I/O but not essential.

- Failure signatures:
  - No latency improvement: workload lacks large shared prefixes, batch size is small, or context length is short.
  - Increased latency: overhead from splitting attention into two GEMM ops dominates when memory I/O savings are minimal.
  - Out-of-memory errors: extreme batch sizes/context lengths exceed GPU memory even with bifurcated attention savings.

- First 3 experiments:
  1. Implement multi-group attention with g=1,4,h and measure KV cache size and inference latency on a small model.
  2. Implement bifurcated attention for single-context batch sampling and compare per-step latency with and without bifurcation at batch sizes 1,8,32,64.
  3. Combine multi-query (g=1) with bifurcated attention and test at extreme batch sizes (128,256,512) and context lengths (8k,16k,32k).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between model size compensation and attention group reduction (g) for maximizing inference efficiency without significant capability loss?
- Basis in paper: [explicit] The paper demonstrates that lower g values require model size compensation (approximately 10% for g=1) to match multi-head performance, but also shows that lower g values can significantly reduce memory I/O during incremental decoding.
- Why unresolved: The paper establishes the relationship between g values and model size compensation but doesn't provide a systematic framework for determining the optimal trade-off point across different model sizes and use cases.
- What evidence would resolve it: A comprehensive study mapping inference latency, capability retention, and memory usage across various g values and model sizes under different batch sizes and context lengths would provide the optimal trade-off framework.

### Open Question 2
- Question: How does bifurcated attention perform with speculative decoding techniques that generate multiple tokens per step?
- Basis in paper: [explicit] The paper mentions compatibility with speculative decoding in Appendix G but doesn't provide experimental results on how bifurcated attention affects performance in this regime.
- Why unresolved: The paper only provides theoretical analysis suggesting orthogonal benefits but lacks empirical validation of combined performance improvements.
- What evidence would resolve it: Experiments comparing inference latency and throughput of models using both bifurcated attention and speculative decoding versus each technique individually across various context lengths and batch sizes would demonstrate the combined benefits.

### Open Question 3
- Question: What are the limitations of bifurcated attention when applied to batch inference scenarios where inputs have different contexts?
- Basis in paper: [explicit] The paper explicitly states in the conclusion that bifurcated attention is not applicable for batch inference scenarios where different inputs are processed in a batch.
- Why unresolved: The paper doesn't explore alternative approaches or hybrid solutions that could extend bifurcated attention's benefits to these scenarios.
- What evidence would resolve it: Research into hybrid attention mechanisms that can dynamically switch between bifurcated and standard attention based on context similarity within a batch would determine if any benefits can be extended to general batch inference.

## Limitations
- The approach only works for single-context batch sampling scenarios, not for general batch inference with different contexts
- Memory I/O savings may not outweigh the overhead of splitting attention into two GEMM operations for small batch sizes or short context lengths
- Compatibility with other optimization techniques like quantization and speculative decoding is mentioned but not thoroughly explored

## Confidence
- **High Confidence**: The core mechanism of bifurcated attention and its memory I/O reduction for single-context batch sampling (Mechanism 1). The experimental results showing latency improvements at large batch sizes and context lengths are well-supported.
- **Medium Confidence**: The benefits of combining bifurcated attention with multi-query attention (Mechanism 3). While the theoretical savings are clear, the interaction between the two techniques and their effect on model expressiveness is not fully explored.
- **Low Confidence**: The generalization of results to models with different architectures, attention mechanisms (like sliding window attention), or workloads that do not exhibit large shared prefixes.

## Next Checks
1. Implement and benchmark bifurcated attention on workloads with varying degrees of prefix sharing (e.g., multi-context batch sampling) to quantify when the optimization breaks down.
2. Characterize the overhead of the bifurcated attention split across a range of batch sizes and context lengths to identify the breakeven point where it stops providing benefits.
3. Test the compatibility and combined performance of bifurcated attention with other inference optimizations like quantization, speculative decoding, or sliding window attention to assess practical deployment scenarios.