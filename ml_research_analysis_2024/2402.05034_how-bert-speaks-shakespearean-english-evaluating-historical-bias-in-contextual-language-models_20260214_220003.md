---
ver: rpa2
title: How BERT Speaks Shakespearean English? Evaluating Historical Bias in Contextual
  Language Models
arxiv_id: '2402.05034'
source_url: https://arxiv.org/abs/2402.05034
tags:
- language
- bert
- sentences
- english
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to evaluate the historical bias in
  contextual language models (CLMs) by measuring their adequacy with respect to Early
  Modern English (EME) and Modern English (ME) language varieties. The core idea is
  to use a fill-in-the-blank task with masked sentences, where models predict the
  masked word, and then rate the predictions on a 5-point bipolar scale between the
  two language varieties.
---

# How BERT Speaks Shakespearean English? Evaluating Historical Bias in Contextual Language Models

## Quick Facts
- arXiv ID: 2402.05034
- Source URL: https://arxiv.org/abs/2402.05034
- Reference count: 8
- Primary result: Proposed method measures historical bias in CLMs by evaluating adequacy with Early Modern English and Modern English language varieties

## Executive Summary
This paper introduces a novel method to evaluate historical bias in contextual language models (CLMs) by measuring their adequacy with respect to Early Modern English (EME) and Modern English (ME) language varieties. The authors propose a fill-in-the-blank task with masked sentences, where models predict masked words and these predictions are rated on a 5-point bipolar scale between the two language varieties. The method derives weighted scores from response probabilities and their respective scores on the scale to measure model adequacy to EME and ME. Testing three BERT-based models (BERT Base, MacBERTh, and English HLM) on a test set of 60 masked sentences, the results show that MacBERTh is most aligned with EME, BERT Base is most aligned with ME, and English HLM tends towards a more neutral language.

## Method Summary
The methodology involves creating a test set of 60 masked sentences (20 EME-specific, 20 ME-specific, 20 generic) with assigned temporal valence scores ρ(s). For each model, a fill-in-the-blank task is performed on these sentences to obtain predicted tokens and their probabilities. Temporal valence scores σ are assigned to each predicted token based on its proximity to language varieties in context. The bias score β(m, s) is calculated as the dot product of valence scores and prediction probabilities, while the domain adequacy score δ(m, s) measures how well the model's predictions match the expected temporal valence of a sentence.

## Key Results
- MacBERTh shows highest domain adequacy for EME sentences, while BERT Base shows highest adequacy for ME sentences
- English HLM demonstrates a tendency towards more neutral language than the other two models in marked sentences
- The proposed method successfully captures diachronic language bias through weighted scoring based on model prediction probabilities and temporal valence scale

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method captures diachronic language bias by assigning weighted scores based on model prediction probabilities and a manually defined temporal valence scale
- Mechanism: For each masked sentence, the model's top predictions are assigned a temporal valence score (T = {-1, -0.5, 0, 0.5, 1}), then the bias score β is calculated as the dot product of these scores with the prediction probabilities
- Core assumption: Temporal valence scores assigned to words and sentences accurately reflect their historical linguistic positioning
- Evidence anchors:
  - [abstract] "derive a weighted score from the response probabilities and their respective scores on the scale"
  - [section] "For this set, we exploit the score functionσ in order to define a token-in-sentence temporal valence score vector xm for m given the sentence s, that is xm = (σ(w1, s), σ(w2, s), . . . , σ(wn, s))T. This allows us to define the bias of a model regarding the sentence as a weighted score: β(m, s) = xT m pm"
- Break condition: If the manual temporal valence assignments are systematically biased or inconsistent, the weighted score will not accurately reflect true diachronic bias

### Mechanism 2
- Claim: The domain adequacy score δ measures how well a model's predictions match the expected temporal valence of a sentence, with 1 indicating perfect alignment
- Mechanism: δ is calculated as 1 - (1/2)|ρ(s) - β(m,s)|, where ρ(s) is the sentence's expected temporal valence and β(m,s) is the model's predicted bias
- Core assumption: The absolute difference between expected and predicted temporal valence is a valid measure of domain adequacy
- Evidence anchors:
  - [section] "we proceed to define the domain adequacy of a model with respect to a sentence s as δ(m, s) = 1 − 1/2 | ρ(s) − β(m, s) | based on the difference between the sentence temporal valence score ρ(s) and the model bias β(m, s)"
  - [corpus] Weak evidence - the corpus neighbors are unrelated to diachronic language evaluation, providing no validation for this mechanism
- Break condition: If the relationship between temporal valence difference and model adequacy is non-linear or context-dependent, the linear transformation will misrepresent actual performance

### Mechanism 3
- Claim: Different CLMs exhibit predictable biases toward specific language varieties based on their training data, with MacBERTh favoring EME, BERT Base favoring ME, and English HLM showing more neutrality
- Mechanism: The models' domain adequacy scores across different test sets (EME, ME, neutral) reveal their inherent temporal biases, with MacBERTh achieving highest adequacy for EME sentences, BERT Base for ME sentences, and English HLM showing balanced performance
- Core assumption: Training data composition directly determines model bias toward specific language varieties
- Evidence anchors:
  - [section] "MacBERTh is most aligned with EME, whereas BERT Base is always most aligned as ME. Historical BERT shows a tendency towards a more neutral language than the other two models in marked sentences"
  - [section] "Figure 2 shows that MacBERTh has best domain adequacy for EME, and BERT Base has best domain adequacy for ME"
- Break condition: If models have undergone fine-tuning or domain adaptation that overrides initial training biases, the predicted relationships may not hold

## Foundational Learning

- Concept: Temporal valence scoring systems
  - Why needed here: The entire evaluation method depends on assigning numerical values to linguistic elements based on their historical positioning
  - Quick check question: If "thou" gets -1 and "you" gets 0 in EME context, what score should "ye" receive and why?

- Concept: Weighted score aggregation from probability distributions
  - Why needed here: Model predictions come as probability distributions, which must be combined with valence scores to create meaningful bias metrics
  - Quick check question: Given predictions p = (0.7, 0.2, 0.1) and valence scores σ = (-1, 0, 1), what is the resulting bias score?

- Concept: Domain-specific evaluation metrics
  - Why needed here: Standard language model metrics don't capture diachronic adequacy, requiring custom metrics like domain adequacy δ
  - Quick check question: If a sentence has ρ = -1 and a model predicts β = -0.8, what is the domain adequacy score?

## Architecture Onboarding

- Component map:
  - Test set generator (creates masked sentences with known temporal valence) -> Model interface (loads and queries different BERT variants) -> Prediction collector (gathers top-k predictions and probabilities) -> Valence scorer (assigns temporal valence scores to predictions) -> Metric calculator (computes β and δ scores) -> Result visualizer (plots distributions and comparisons)

- Critical path:
  1. Load test sentences with known ρ values
  2. For each sentence and model, get top-k predictions with probabilities
  3. Apply valence scoring to predictions to create σ vectors
  4. Calculate bias score β = σ · p
  5. Compute domain adequacy δ = 1 - (1/2)|ρ - β|
  6. Aggregate and visualize results

- Design tradeoffs:
  - Manual valence assignment vs. automated scoring: Manual allows nuanced philological judgment but introduces subjectivity
  - Fixed vs. adaptive valence scale: Fixed provides consistency but may not capture all linguistic phenomena
  - Top-k prediction vs. full distribution: Top-k is computationally efficient but may miss important lower-probability predictions

- Failure signatures:
  - Unexpected model behavior: If MacBERTh doesn't show highest EME adequacy, check if test sentences are properly EME-marked
  - Score compression: If all δ scores cluster near 1, verify that valence assignments have sufficient range and that probability distributions vary meaningfully
  - Negative adequacy: If δ scores become negative, check that β doesn't deviate too far from ρ (should be bounded by design)

- First 3 experiments:
  1. Verify valence scoring consistency by manually checking 10 random predictions against gold-standard valence assignments
  2. Test metric sensitivity by creating synthetic sentences with known valence shifts and confirming β changes appropriately
  3. Cross-model comparison validation by running the same test set through all three models and confirming the expected bias patterns (MacBERTh → EME, BERT Base → ME, English HLM → neutral)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed methodology be adapted to evaluate models for languages other than English, considering the specific linguistic and cultural nuances of different languages?
- Basis in paper: [explicit] The paper discusses the adaptability of the methodology to diverse fields of interest, suggesting its potential application beyond English language analysis
- Why unresolved: The paper primarily focuses on English language varieties (Early Modern English and Modern English) and does not provide explicit examples or guidelines for adapting the methodology to other languages
- What evidence would resolve it: A detailed case study or guidelines demonstrating the application of the methodology to evaluate models for languages other than English, highlighting the challenges and adaptations required

### Open Question 2
- Question: What are the implications of the observed biases in CLMs for tasks that require historical accuracy, such as digital humanities research or historical text analysis?
- Basis in paper: [inferred] The paper discusses the potential benefits of CLMs for philological, pragmatic, critical, and literary work, emphasizing the importance of their ability to adapt to historical linguistic contexts
- Why unresolved: The paper does not delve into the specific implications of the observed biases in CLMs for tasks that demand historical accuracy, nor does it propose strategies to mitigate these biases
- What evidence would resolve it: A comprehensive analysis of the impact of CLM biases on historical text analysis tasks, along with proposed strategies to mitigate these biases and improve the accuracy of CLMs in historical contexts

### Open Question 3
- Question: How can the proposed methodology be extended to evaluate the bias of CLMs in terms of socio-cultural factors, such as attitudes towards social groups or historical events, rather than just linguistic variations?
- Basis in paper: [explicit] The paper suggests that future work could create a test set for model interrogation that is culture-oriented, delving into socio-culturally significant elements
- Why unresolved: The paper does not provide a concrete framework or examples of how the methodology can be extended to evaluate socio-cultural biases in CLMs
- What evidence would resolve it: A detailed framework or case study demonstrating the application of the methodology to evaluate CLMs for socio-cultural biases, including the creation of test sets and the interpretation of results in the context of socio-cultural factors

## Limitations
- The evaluation depends on a small, manually curated test set that may not capture the full range of diachronic variation in Early Modern and Modern English
- Temporal valence scoring requires philological expertise and introduces subjective judgments that may vary between researchers
- The method focuses on word-level predictions and may miss broader discourse-level historical patterns that affect language model performance

## Confidence
- **High confidence**: The mathematical framework for calculating bias scores β and domain adequacy δ is clearly defined and internally consistent
- **Medium confidence**: The claim that MacBERTh is most aligned with EME, BERT Base with ME, and English HLM with neutral language is supported by the presented results, but the small test set (60 sentences) limits generalizability
- **Low confidence**: The assumption that temporal valence scores accurately capture the full complexity of diachronic language variation is not fully validated

## Next Checks
1. Apply the method to a larger, independently curated test set of 200+ masked sentences spanning different EME and ME registers to verify the observed model biases persist
2. Apply the temporal valence scoring framework to evaluate CLMs trained on other language pairs with known diachronic shifts (e.g., Latin to Romance languages) to test generalizability
3. Systematically vary the temporal valence assignments by ±0.5 points for 50 randomly selected predictions to measure how sensitive the bias scores and domain adequacy metrics are to scoring variations