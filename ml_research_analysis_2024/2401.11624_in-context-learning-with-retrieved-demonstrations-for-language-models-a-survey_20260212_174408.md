---
ver: rpa2
title: 'In-context Learning with Retrieved Demonstrations for Language Models: A Survey'
arxiv_id: '2401.11624'
source_url: https://arxiv.org/abs/2401.11624
tags:
- demonstrations
- arxiv
- language
- learning
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews recent advances in retrieval-based in-context
  learning (RetICL) for large language models (LLMs), focusing on methods that retrieve
  demonstrations tailored to each input query rather than using a fixed set. The paper
  analyzes 22 key studies, categorizing them by retrieval models (e.g., BM25, SBERT,
  fine-tuned retrievers), training objectives (e.g., contrastive learning, KL divergence,
  list-wise ranking), and retrieval strategies (e.g., one-shot, clustering, iterative).
---

# In-context Learning with Retrieved Demonstrations for Language Models: A Survey

## Quick Facts
- arXiv ID: 2401.11624
- Source URL: https://arxiv.org/abs/2401.11624
- Reference count: 31
- Authors: Man Luo; Xin Xu; Yue Liu; Panupong Pasupat; Mehran Kazemi
- Primary result: Retrieval-based ICL (RetICL) improves LLM performance by selecting task-relevant demonstrations, with fine-tuned retrievers generally outperforming off-the-shelf models.

## Executive Summary
This survey reviews recent advances in retrieval-based in-context learning (RetICL) for large language models, focusing on methods that retrieve demonstrations tailored to each input query rather than using a fixed set. The paper analyzes 22 key studies, categorizing them by retrieval models, training objectives, and retrieval strategies. RetICL has been shown to improve LLM performance across tasks like natural language understanding, reasoning, knowledge-based QA, and text generation by selecting more relevant and diverse demonstrations. Fine-tuned retrievers generally outperform off-the-shelf models, though no single retriever consistently excels across all tasks.

## Method Summary
The survey categorizes RetICL methods by their retrieval models (e.g., BM25, SBERT, fine-tuned retrievers), training objectives (e.g., contrastive learning, KL divergence, list-wise ranking), and retrieval strategies (e.g., one-shot, clustering, iterative). Fine-tuned retrievers are trained on task-specific data using objectives like contrastive learning or KL distillation to align with LLM preferences. Iterative retrieval strategies select demonstrations based on both the query and previously retrieved examples to maximize complementarity. The performance of RetICL is evaluated across various NLP tasks using metrics such as accuracy and F1 score.

## Key Results
- Retrieval-based ICL improves performance by dynamically selecting demonstrations tailored to each query.
- Fine-tuned retrievers generally outperform off-the-shelf models in selecting effective demonstrations.
- Iterative retrieval strategies can yield more complementary and diverse demonstration sets than one-shot or clustering approaches.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-based ICL (RetICL) improves performance by dynamically selecting demonstrations tailored to each query rather than using static, fixed sets.
- Mechanism: A specialized retriever curates demonstrations that are most relevant to the current input, increasing the likelihood that the LLM learns the correct pattern from context.
- Core assumption: The relevance and usefulness of demonstrations directly influence the LLM's ability to perform ICL; static demonstrations may not cover the diversity or complexity of incoming queries.
- Evidence anchors:
  - [abstract]: "instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query"
  - [section 3.1]: "Two primary retrieval objectives for selecting demonstrations: similarity and diversity"
- Break condition: If the retrieval corpus lacks high-quality, annotated examples or the retriever fails to measure relevance accurately, the performance gains may vanish.

### Mechanism 2
- Claim: Fine-tuned retrievers outperform off-the-shelf models in selecting effective demonstrations for ICL.
- Mechanism: Training a retriever on task-specific data using objectives like contrastive learning or KL divergence aligns its notion of "good" demonstrations with the LLM's preferences, improving downstream accuracy.
- Core assumption: Signals from the LLM itself (e.g., conditional probabilities of correct answers given a demonstration) can be used to label and train a retriever that captures what the LLM finds useful.
- Evidence anchors:
  - [abstract]: "Fine-tuned retrievers generally outperform off-the-shelf models"
  - [section 5.1]: "a scoring LLM... is used to score each candidate demonstration... retriever can be trained that predicts these scores directly"
- Break condition: If the scoring LLM is not representative of the inference LLM, or if the training data is insufficient, the fine-tuned retriever may not generalize well.

### Mechanism 3
- Claim: Iterative retrieval strategies, where each new demonstration is chosen based on previously retrieved ones, can yield more complementary and diverse demonstration sets than one-shot or clustering approaches.
- Mechanism: The retriever updates its query representation after each selection, allowing it to fill in gaps left by earlier choices and avoid redundancy.
- Core assumption: Demonstrations that complement each other are more effective than a set of independently selected top-k similar examples.
- Evidence anchors:
  - [section 3.2]: "in iterative retrieval, a retriever selects demonstrations based on both the query and previously retrieved demonstrations"
  - [section 6]: "The iterative retrieval strategy shows the most significant improvement on mathematical reasoning tasks"
- Break condition: If the iterative process gets stuck in a local pattern or the retriever's update rule is poorly designed, the benefit over simpler strategies may diminish.

## Foundational Learning

- Concept: In-context learning (ICL) - the ability of LLMs to perform tasks by conditioning on a few input-output examples without updating parameters.
  - Why needed here: RetICL builds directly on ICL by improving the selection of examples used in the prompt; understanding ICL's sensitivity to demonstration choice is key.
  - Quick check question: Why might the same demonstration set work poorly for some queries but well for others in ICL?

- Concept: Retrieval-augmented generation (RAG) - retrieving relevant external text to augment the input context for a generative model.
  - Why needed here: RetICL is a specialized form of RAG where the retrieval target is demonstration examples, not raw knowledge passages; the same principles of relevance scoring and corpus design apply.
  - Quick check question: How does the choice of retrieval corpus (e.g., annotated vs. raw text) affect the quality of demonstrations in RetICL?

- Concept: Contrastive learning - a training paradigm that pulls representations of similar items together and pushes dissimilar items apart.
  - Why needed here: Many retriever training methods (e.g., InfoNCE loss) use contrastive objectives to learn which demonstrations are most useful for a given query.
  - Quick check question: What is the difference between using in-batch negatives versus hard negatives in contrastive retriever training?

## Architecture Onboarding

- Component map: Input query -> Retriever (BM25/SBERT/fine-tuned) -> Selected demonstrations -> LLM prompt -> Output
- Critical path: Input query → Retriever → Selected demonstrations → LLM prompt → Output
- Design tradeoffs:
  - Corpus size vs. annotation cost (larger annotated sets improve quality but are expensive)
  - Retriever complexity vs. inference speed (fine-tuned models may be more accurate but slower)
  - Retrieval strategy diversity vs. computational cost (iterative retrieval is more expensive but can yield better coverage)
- Failure signatures:
  - Performance matches or drops below random demonstration baseline → retriever relevance signals are weak or noisy
  - Slow inference times → retrieval step or corpus size is a bottleneck
  - Inconsistent results across tasks → retriever overfits to one domain or task type
- First 3 experiments:
  1. Compare BM25 vs. SBERT retrievers on a small in-domain task with fixed demonstration counts.
  2. Train a simple contrastive retriever on task-specific data and evaluate vs. off-the-shelf retrievers.
  3. Test iterative vs. one-shot retrieval on a reasoning task to measure coverage and accuracy differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between similarity and diversity when retrieving demonstrations for in-context learning?
- Basis in paper: [explicit] The paper discusses two primary retrieval objectives - similarity and diversity, but notes that no single retriever consistently outperforms others across different tasks.
- Why unresolved: While the paper explores both similarity and diversity as important factors, it doesn't provide a clear methodology for determining the optimal balance between them for different tasks.
- What evidence would resolve it: Systematic experiments comparing different similarity-diversity trade-offs across various tasks, ideally with a method to automatically determine the optimal balance for each task.

### Open Question 2
- Question: How can we develop a universal retriever that consistently outperforms task-specific retrievers across all types of in-context learning tasks?
- Basis in paper: [explicit] The paper mentions that Li et al. (2023b) proposed training a universal retriever that shows stronger performance than task-specific retrievers on most tasks, but doesn't achieve consistent superiority.
- Why unresolved: The challenge lies in creating a single retriever model that can adapt to the nuances of various tasks while maintaining high performance across all of them.
- What evidence would resolve it: Development and evaluation of a universal retriever model that demonstrates consistent outperformance of task-specific retrievers across a wide range of in-context learning tasks.

### Open Question 3
- Question: How does the quality of pseudo-demonstrations generated from free-form corpora compare to human-annotated demonstrations in terms of improving in-context learning performance?
- Basis in paper: [inferred] The paper suggests using free-form corpora to generate pseudo-demonstrations when human-annotated data is unavailable, but doesn't provide a direct comparison of their effectiveness.
- Why unresolved: While generating pseudo-demonstrations is a promising approach for low-resource scenarios, the quality and impact of these generated examples on model performance remains uncertain.
- What evidence would resolve it: Comprehensive studies comparing the performance of models trained with pseudo-demonstrations from free-form corpora against those using human-annotated demonstrations across various tasks and domains.

## Limitations
- Analysis is based on a relatively small set of 22 studies, which may not capture the full diversity of approaches or task types.
- The paper does not provide systematic empirical comparisons across all methods, so claims about relative performance are based on findings from individual studies rather than direct benchmarking.
- The survey does not address potential biases in the retrieval corpus or the impact of demonstration quality on downstream performance.

## Confidence
- **High confidence**: Retrieval-based ICL improves performance over static demonstration sets by selecting task-relevant examples; fine-tuned retrievers generally outperform off-the-shelf models.
- **Medium confidence**: Iterative retrieval strategies can yield more complementary demonstrations than one-shot or clustering approaches; no single retriever consistently excels across all tasks.
- **Low confidence**: Specific claims about the superiority of particular retriever architectures or training objectives, due to lack of direct, systematic comparisons.

## Next Checks
1. Conduct a controlled empirical study comparing the performance of fine-tuned vs. off-the-shelf retrievers across a diverse set of NLP tasks, using the same demonstration corpus and evaluation metrics.
2. Analyze the impact of retrieval corpus quality (e.g., annotated vs. raw text) on RetICL performance, controlling for retriever architecture and retrieval strategy.
3. Investigate the robustness of RetICL to noisy or biased demonstrations by systematically injecting errors or biases into the retrieval corpus and measuring the effect on downstream task performance.