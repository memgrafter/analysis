---
ver: rpa2
title: Following Length Constraints in Instructions
arxiv_id: '2406.17744'
source_url: https://arxiv.org/abs/2406.17744
tags:
- length
- lift-dpo
- instructions
- instruction
- following
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Existing instruction-following models fail to adhere to explicit
  length constraints, with GPT-4 violating them nearly half the time. To address this,
  the authors propose Length-Instruction Fine-Tuning (LIFT), which augments training
  data with length instructions and trains via Direct Preference Optimization (DPO).
---

# Following Length Constraints in Instructions

## Quick Facts
- arXiv ID: 2406.17744
- Source URL: https://arxiv.org/abs/2406.17744
- Reference count: 24
- Existing instruction-following models fail to adhere to explicit length constraints, with GPT-4 violating them nearly half the time.

## Executive Summary
Current instruction-following models struggle to follow explicit length constraints, with GPT-4 violating them nearly 50% of the time. The authors propose Length-Instruction Fine-Tuning (LIFT), which augments training data with length instructions and trains via Direct Preference Optimization (DPO). This approach significantly reduces length constraint violations (from ~65% to <10% on Llama-2-70B-Base) while maintaining or improving response quality and win rates on both length-instructed and standard benchmarks.

## Method Summary
The authors propose Length-Instruction Fine-Tuning (LIFT), which augments a conventional instruction-following dataset by inserting length instructions into original prompts. They construct length-instructed preference pairs where winners and losers are determined based on both quality and length constraint adherence. The model is then trained using Direct Preference Optimization (DPO) on the combined original and augmented datasets. The method was evaluated on Llama-2-70B-Base and Llama3-8B-Instruct models using AlpacaEval 2, MT-Bench, and OpenAssistant datasets.

## Key Results
- LIFT-DPO models reduce length constraint violations from ~65% to <10% on Llama-2-70B-Base
- Models maintain or improve response quality on both length-instructed and standard benchmarks
- LIFT-DPO exhibits comparable performance to standard DPO when length instructions are not applied

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct Preference Optimization (DPO) trained on length-instructed preference pairs can learn to balance response quality and adherence to length constraints.
- Mechanism: By augmenting the original preference dataset with new pairs where the longer response becomes the loser due to violating the length constraint, the model learns to prioritize the length instruction over the original quality preference when they conflict.
- Core assumption: The preference pairs constructed in LIFT preserve the quality ordering while enforcing length constraints, allowing DPO to learn a reward function that trades off length and quality appropriately.
- Evidence anchors: [abstract] "Our approach, Length-Instruction Fine-Tuning (LIFT), involves taking a conventional instruction following dataset and constructing augmented training data by inserting length instructions in the original prompts."

### Mechanism 2
- Claim: Explicitly stating length constraints in the prompt helps the model better understand the user's intent and generate more appropriate responses.
- Mechanism: By including a clear instruction like "Answer the following instruction using <MAX_LEN> words or less", the model receives unambiguous guidance on the desired output length, reducing ambiguity in the task specification.
- Core assumption: Models can effectively parse and utilize explicit length instructions when they are clearly formatted and consistently applied during training.
- Evidence anchors: [section 1] "To resolve this we propose that evaluation should include further disambiguating instructions that prescribe the length of the desired response."

### Mechanism 3
- Claim: LIFT-DPO models maintain or improve response quality on standard instruction-following tasks while significantly reducing length constraint violations.
- Mechanism: By training on both the original dataset D and the augmented dataset D' with length instructions, the model learns to handle both types of prompts, generalizing its instruction-following ability to include length constraints without sacrificing general performance.
- Core assumption: The model can learn to apply length instructions when present while defaulting to quality-based preferences when no length constraint is given.
- Evidence anchors: [abstract] "Models trained with LIFT significantly reduce length constraint violations (from ~65% to <10% on Llama-2-70B-Base) while maintaining or improving response quality and win rates on both length-instructed and standard benchmarks."

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is used to train the model on the augmented preference pairs created by LIFT, allowing it to learn a reward function that balances response quality and length constraints.
  - Quick check question: How does DPO differ from standard reinforcement learning approaches in terms of computational efficiency and stability?

- Concept: Length-instructed preference pairs
  - Why needed here: These augmented preference pairs are the core of the LIFT method, providing the model with explicit examples of how to trade off quality for adherence to length constraints.
  - Quick check question: What is the key difference in how winners and losers are determined in the augmented preference pairs compared to the original pairs?

- Concept: Length bias in instruction following
  - Why needed here: Understanding the problem of length bias motivates the need for length instructions and the LIFT method, as it highlights the shortcomings of current evaluation and training approaches.
  - Quick check question: How does length bias manifest in both human and model preferences, and why is it problematic for instruction following?

## Architecture Onboarding

- Component map: Base model (e.g., Llama-2-70B-Base) -> LIFT method (augment training data with length instructions) -> DPO training process (optimize on augmented preference pairs) -> Length-instructed model
- Critical path: Construct augmented dataset D' from original dataset D using LIFT method, followed by training the model using DPO on combined dataset D âˆª D'
- Design tradeoffs: Between strictness of length constraints in augmented dataset and model's ability to maintain high response quality; between amount of augmented data and computational cost of training
- Failure signatures: Continued frequent violation of length constraints suggests issues with augmented dataset construction or DPO training; significant quality degradation on standard tasks suggests length instructions are overly influencing model behavior
- First 3 experiments:
  1. Evaluate base model's violation rate on length-instructed benchmarks (AlpacaEval-LI and MT-Bench-LI) to establish baseline
  2. Train model using DPO on augmented dataset D' and evaluate performance on length-instructed benchmarks
  3. Compare LIFT-DPO model with base model and standard DPO model on both length-instructed and standard benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do humans actually prefer different response lengths across various instruction types, and how does this compare to model-generated length preferences?
- Basis in paper: [explicit] The paper discusses length bias in model alignment and mentions that longer responses are not necessarily better even if preferred by annotators, but does not explore actual human length preferences across different instruction types.
- Why unresolved: The paper focuses on model behavior and training methods to follow length instructions, but doesn't investigate the underlying human preferences that might drive length bias.
- What evidence would resolve it: A comprehensive study comparing human preferences for response lengths across different instruction categories (e.g., creative writing vs. factual questions) using controlled experiments with varying instruction lengths and detailed analysis of preference patterns.

### Open Question 2
- Question: What is the relationship between computational allowance (token count) and response quality, and how does this affect length following behavior?
- Basis in paper: [inferred] The paper mentions that longer responses are more likely to contain inaccuracies and that increased computation allowance comes with more tokens, suggesting a potential trade-off between length and quality.
- Why unresolved: While the paper touches on this relationship, it doesn't provide a detailed analysis of how computational resources allocated to token generation affect response quality and length following behavior.
- What evidence would resolve it: Systematic experiments varying computational resources (e.g., different temperature settings, token limits) and measuring their impact on response quality, length adherence, and the trade-off between the two across different model sizes and instruction types.

### Open Question 3
- Question: How effective is the LIFT method when length instructions are phrased using different wording or more complex phrasing instead of a fixed template?
- Basis in paper: [explicit] The paper acknowledges that it only uses a fixed template for length instructions and mentions the possibility of allowing length instructions to be phrased using different wording.
- Why unresolved: The current LIFT method is limited to a specific template format, and its effectiveness with more natural or varied phrasing of length instructions remains untested.
- What evidence would resolve it: Training and evaluating models using LIFT with length instructions in various natural language formats (e.g., "Keep the response brief," "Write a concise answer," "Please limit your response to 100 words") and comparing performance to the fixed template approach across multiple benchmarks.

## Limitations

- The paper's results rely heavily on the quality of the augmented dataset and the effectiveness of the DPO training process
- Limited analysis of trade-offs between length adherence and response quality across different instruction types
- Results may not generalize to other instruction-following tasks and model architectures beyond those tested

## Confidence

- **High Confidence**: The claim that LIFT significantly reduces length constraint violations is supported by the empirical results, with violation rates dropping from ~65% to <10% on Llama-2-70B-Base
- **Medium Confidence**: The claim that LIFT maintains or improves response quality on standard instruction-following tasks is supported by the results, but the analysis is limited to a few benchmarks and model sizes
- **Low Confidence**: The claim that LIFT generalizes to other instruction-following tasks and model architectures is not extensively validated in the paper

## Next Checks

1. **Generalization to Other Tasks**: Evaluate the LIFT-trained models on a wider range of instruction-following tasks, including those with different types of constraints (e.g., format, style, or content requirements) to assess the method's generalizability.

2. **Trade-off Analysis**: Conduct a more detailed analysis of the trade-offs between length adherence and response quality, including human evaluations of the generated responses to understand the impact of strict length constraints on the overall quality and usefulness of the outputs.

3. **Scaling Study**: Investigate the effectiveness of LIFT across different model sizes and architectures, including smaller models and those with different pretraining objectives, to understand the method's scalability and applicability to a broader range of language models.