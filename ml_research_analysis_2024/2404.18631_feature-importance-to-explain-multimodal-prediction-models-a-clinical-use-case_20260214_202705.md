---
ver: rpa2
title: Feature importance to explain multimodal prediction models. A clinical use
  case
arxiv_id: '2404.18631'
source_url: https://arxiv.org/abs/2404.18631
tags:
- data
- prediction
- multimodal
- static
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors develop a multimodal deep learning model to predict
  30-day mortality after hip fracture surgery in elderly patients. The model combines
  static patient data, hip and chest X-ray images, vital signs, and medication data
  using early fusion of unimodal ResNet and LSTM feature extractors.
---

# Feature importance to explain multimodal prediction models. A clinical use case

## Quick Facts
- arXiv ID: 2404.18631
- Source URL: https://arxiv.org/abs/2404.18631
- Reference count: 40
- Multimodal deep learning model predicts 30-day mortality after hip fracture surgery with AUC of 0.78

## Executive Summary
This paper presents a multimodal deep learning approach for predicting 30-day mortality in elderly hip fracture patients. The model integrates static patient data, hip and chest X-ray images, vital signs, and medication information through early fusion of ResNet and LSTM feature extractors. Shapley value propagation is employed to provide interpretable explanations at both modality and feature levels, demonstrating that static patient data contributes most significantly to predictions.

## Method Summary
The authors develop a multimodal architecture combining unimodal ResNet and LSTM feature extractors for image and time-series data respectively. These are fused early in the network to predict 30-day mortality after hip fracture surgery. Shapley values are computed to quantify the contribution of each modality and individual features to predictions. The method uses SHAP (SHapley Additive exPlanations) to generate both global and local explanations, with Shapley value propagation enabling feature-level interpretation from the unimodal components.

## Key Results
- Multimodal model achieves AUC of 0.78 for 30-day mortality prediction
- Outperforms unimodal baselines across all tested modalities
- Static patient data contributes 70.3% to predictions globally, while per-operative data adds minimal value
- Shapley value propagation enables interpretable local explanations from complex multimodal models

## Why This Works (Mechanism)
The model works by combining complementary information sources through early fusion of unimodal feature extractors. ResNet networks process medical images to capture spatial patterns, while LSTMs handle temporal vital sign sequences. The early fusion approach allows the model to learn joint representations that capture interactions between modalities before the final prediction layer. Shapley values provide theoretically grounded attribution by considering all possible feature coalitions, ensuring fair distribution of prediction credit across modalities and features.

## Foundational Learning
- Shapley values (why needed: fair attribution in cooperative game theory; quick check: verify that all marginal contributions sum to prediction difference)
- SHAP framework (why needed: practical implementation of Shapley values for ML; quick check: confirm local accuracy property holds)
- Early fusion architecture (why needed: learn joint representations across modalities; quick check: compare with late fusion performance)
- ResNet architecture (why needed: effective image feature extraction; quick check: verify skip connections improve gradient flow)
- LSTM networks (why needed: temporal pattern recognition in vital signs; quick check: confirm sequence modeling outperforms static features)
- Feature importance interpretation (why needed: build trust in clinical ML models; quick check: validate explanations with clinical experts)

## Architecture Onboarding

Component map: Images -> ResNet -> Features; Time-series -> LSTM -> Features; Static data -> Direct; All Features -> Early Fusion -> Prediction; Prediction -> Shapley Values

Critical path: Data ingestion → Unimodal feature extraction (ResNet + LSTM) → Early fusion → Prediction → Shapley value computation → Interpretation

Design tradeoffs: Early fusion enables joint learning but increases computational complexity; unimodal baselines provide performance baselines but miss cross-modal interactions; Shapley values ensure fair attribution but are computationally expensive

Failure signatures: Poor image quality degrades ResNet performance; missing vital sign data affects LSTM reliability; modality imbalance skews Shapley value interpretation; feature correlation can lead to attribution ambiguity

First experiments: 1) Compare early vs late fusion performance; 2) Ablate each modality to assess individual contribution; 3) Validate Shapley-based explanations with clinical expert review

## Open Questions the Paper Calls Out
None

## Limitations
- Moderate AUC of 0.78 suggests room for predictive improvement
- Computational expense of Shapley value calculations affects scalability
- Attribution precision may be affected by approximation errors in Shapley value estimation

## Confidence
- Global modality contributions: Medium (based on aggregated Shapley values)
- Local explanation validity: Medium (requires further validation on diverse populations)
- Clinical utility claims: Medium (need external validation)

## Next Checks
1. Test model on independent external dataset to assess real-world performance and robustness
2. Conduct ablation studies to isolate impact of each modality on both predictive performance and Shapley-based explanations
3. Compare multimodal model's explanations with clinical expert annotations to validate interpretability