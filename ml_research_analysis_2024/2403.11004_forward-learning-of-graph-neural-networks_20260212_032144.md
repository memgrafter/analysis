---
ver: rpa2
title: Forward Learning of Graph Neural Networks
arxiv_id: '2403.11004'
source_url: https://arxiv.org/abs/2403.11004
tags:
- learning
- layers
- forward
- graph
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ForwardGNN, a biologically plausible forward
  learning framework for graph neural networks that eliminates the need for backpropagation.
  Unlike traditional backpropagation, which requires storing intermediate activations
  and depends on non-local error signals, ForwardGNN uses a single forward pass with
  contrastive learning to train each layer locally while incorporating both bottom-up
  and top-down signals.
---

# Forward Learning of Graph Neural Networks

## Quick Facts
- arXiv ID: 2403.11004
- Source URL: https://arxiv.org/abs/2403.11004
- Authors: Namyong Park, Xing Wang, Antoine Simoulin, Shuai Yang, Grey Yang, Ryan Rossi, Puja Trivedi, Nesreen Ahmed
- Reference count: 40
- Primary result: ForwardGNN achieves BP-level performance on node classification and link prediction while reducing memory usage up to 18× with deeper models

## Executive Summary
This paper introduces ForwardGNN, a biologically plausible forward learning framework for graph neural networks that eliminates the need for backpropagation. Unlike traditional backpropagation, which requires storing intermediate activations and depends on non-local error signals, ForwardGNN uses a single forward pass with contrastive learning to train each layer locally while incorporating both bottom-up and top-down signals. The method extends the forward-forward algorithm to graph data by using virtual nodes and label-based connections, enabling effective learning without explicitly generating negative samples.

## Method Summary
ForwardGNN implements a single-forward learning approach for GNNs by augmenting the graph structure with virtual nodes corresponding to class labels and using contrastive learning between real nodes and virtual nodes. The framework operates without storing intermediate activations or computing gradients through backpropagation, instead generating its own training targets through the augmented graph structure. The method incorporates top-down signals from higher layers to improve lower-layer representations while maintaining the efficiency benefits of single-pass training.

## Key Results
- Matches or exceeds backpropagation performance on five real-world datasets across node classification and link prediction tasks
- Reduces memory usage up to 18× compared to backpropagation, with memory requirements remaining constant regardless of network depth
- Demonstrates improved training efficiency and stability compared to existing forward learning methods
- Compatible with various GNN architectures including GCN, GraphSAGE, and GAT

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The single-forward learning approach eliminates the need to store intermediate activations by generating learning signals through contrastive learning within a single forward pass.
- **Mechanism:** By augmenting the graph structure with virtual nodes corresponding to class labels and using contrastive learning between real nodes and virtual nodes, the model can generate its own training targets without requiring positive/negative sample pairs from perturbed inputs.
- **Core assumption:** The augmented graph structure with virtual nodes provides sufficient signal to train each layer effectively without explicit negative samples.
- **Evidence anchors:** [abstract] "makes it possible to operate without generating negative inputs (hence no longer forward-forward)" and [section] "This way, GNNs can produce the embeddings of real nodes and class representatives simultaneously in just one forward pass"
- **Break condition:** If the contrastive learning objective fails to provide discriminative signals between classes, or if the virtual node connections don't adequately represent class structure.

### Mechanism 2
- **Claim:** The proposed method achieves similar or better performance than backpropagation while being more scalable in memory usage.
- **Mechanism:** By avoiding the backward pass and gradient storage, memory requirements remain constant regardless of network depth, while the single forward pass with contrastive learning maintains or improves accuracy.
- **Core assumption:** The local layer-wise training with contrastive objectives can capture the same information as backpropagation without requiring error propagation through layers.
- **Evidence anchors:** [abstract] "ForwardGNN matches or exceeds backpropagation performance while significantly reducing memory usage—up to 18× more scalable with deeper models" and [section] "As we use GNNs with a larger number of layers, BP requires more memory for training... while the memory required by SF remains nearly the same"
- **Break condition:** If the layer-wise training cannot capture long-range dependencies that backpropagation typically handles through error propagation.

### Mechanism 3
- **Claim:** Incorporating top-down signals improves classification accuracy by allowing each layer to learn from representations learned by upper layers.
- **Mechanism:** The top-to-input and top-to-loss signal paths provide information from higher layers to inform the training of lower layers without requiring backpropagation, creating a hierarchical learning process.
- **Core assumption:** The top-down signals contain useful information that can improve lower-layer representations without creating circular dependencies or instability.
- **Evidence anchors:** [abstract] "incorporating both bottom-up and top-down signals" and [section] "This way, each layer can learn from the representations learned by upper layers"
- **Break condition:** If the top-down signals introduce noise or instability that degrades performance, or if the signal paths create unintended dependencies.

## Foundational Learning

- **Concept:** Graph neural networks and message passing
  - **Why needed here:** Understanding how GNNs aggregate information from neighbors is crucial for grasping how the forward learning approach works without backpropagation
  - **Quick check question:** How does the message passing equation (1) enable information propagation in GNNs, and why is this important for the forward learning approach?

- **Concept:** Contrastive learning
  - **Why needed here:** The single-forward approach relies on contrastive learning between real nodes and virtual nodes to generate training signals
  - **Quick check question:** What is the role of the temperature parameter τ in the contrastive learning objective, and how does it affect the learning process?

- **Concept:** Virtual nodes and graph augmentation
  - **Why needed here:** The approach uses virtual nodes to represent class labels and create learning signals, which is fundamental to understanding how the method works
  - **Quick check question:** How do virtual nodes with label-based connections help the model learn class representations without explicit negative samples?

## Architecture Onboarding

- **Component map:** Input features → Graph augmentation (virtual nodes) → GNN forward pass → Contrastive learning objective computation → Parameter updates
- **Critical path:** The forward pass through augmented graph → Contrastive learning signal generation → Parameter updates must be efficient and stable for effective training
- **Design tradeoffs:** Single forward pass vs. accuracy (FF methods require multiple passes but may capture more information), virtual node augmentation complexity vs. learning signal quality, top-down signal incorporation complexity vs. potential performance gains
- **Failure signatures:** Poor convergence during training, unstable performance across runs, memory usage increasing with depth (should remain constant), significant accuracy degradation compared to BP
- **First 3 experiments:**
  1. Implement and test the basic single-forward approach on a small citation network (e.g., Cora) to verify the contrastive learning mechanism works
  2. Compare memory usage between single-forward and backpropagation as depth increases on the same dataset
  3. Test the impact of top-down signal incorporation by comparing with and without the signal paths on a mid-sized dataset (e.g., PubMed)

## Open Questions the Paper Calls Out

### Open Question 1  
- **Question:** How do different goodness functions affect the performance and biological plausibility of ForwardGNN compared to the L2 norm used in the current implementation?  
- **Basis in paper:** [explicit] The paper mentions that FF defines goodness as the sum of squares of ReLU activations and uses L2 norm, but notes that the choice of goodness function could impact both performance and biological plausibility.  
- **Why unresolved:** The paper only uses one specific goodness function (L2 norm of ReLU activations) and doesn't systematically explore alternatives like negative entropy or other biologically plausible measures.  
- **What evidence would resolve it:** Experiments comparing ForwardGNN performance using different goodness functions (e.g., negative entropy, L1 norm, etc.) across multiple datasets and tasks would show which functions balance performance with biological plausibility.

### Open Question 2  
- **Question:** Can ForwardGNN's single-forward approach be effectively extended to other graph learning tasks beyond node classification and link prediction, such as graph classification or graph clustering?  
- **Basis in paper:** [explicit] The paper discusses potential extensions to graph classification and clustering in Appendix D, noting challenges like lack of labels for clustering and the need for graph-level class representatives.  
- **Why unresolved:** The paper only demonstrates ForwardGNN on node classification and link prediction, leaving open whether the single-forward framework generalizes to tasks requiring different output levels or unsupervised objectives.  
- **What evidence would resolve it:** Applying ForwardGNN to graph classification and clustering tasks with appropriate modifications (e.g., graph pooling, self-supervised objectives) and comparing results to baseline methods would demonstrate generalizability.

### Open Question 3  
- **Question:** What is the impact of incorporating top-down signals through different signal paths (top-to-input vs. top-to-loss) on ForwardGNN's performance, and how do these compare to standard backpropagation?  
- **Basis in paper:** [explicit] The paper introduces two top-down signal paths in Section 3.3 and shows in Table 1 that incorporating top-down signals improves classification accuracy, but doesn't provide a comprehensive comparison between the two paths or against backpropagation.  
- **Why unresolved:** While the paper shows that top-down signals help, it doesn't systematically analyze which signal path is more effective or how they compare to the full backpropagation of errors.  
- **What evidence would resolve it:** Detailed experiments comparing top-to-input vs. top-to-loss signal paths across multiple datasets, and benchmarking both against backpropagation in terms of accuracy, memory usage, and training stability, would clarify their relative effectiveness.

## Limitations

- The performance claims rely on specific implementation details of virtual node augmentation that are not fully specified
- The memory efficiency improvements assume proper implementation of the single forward pass approach
- The effectiveness of top-down signal incorporation lacks comprehensive empirical validation

## Confidence

- **Memory efficiency claims (High):** The theoretical basis for constant memory usage with increasing depth is sound, as the single forward pass eliminates gradient storage requirements
- **Performance matching BP (Medium):** While the paper claims ForwardGNN matches or exceeds BP performance, this depends heavily on proper hyperparameter tuning and implementation details that are not fully specified
- **Top-down signal benefits (Low):** The paper mentions incorporating top-down signals but provides limited empirical evidence for their effectiveness, and the mechanism for how these signals improve learning without causing instability is not fully explained

## Next Checks

1. **Memory profiling experiment:** Implement both ForwardGNN and BP on the same hardware and systematically measure memory usage as GNN depth increases from 2 to 16 layers on PubMed dataset
2. **Hyperparameter sensitivity analysis:** Conduct ablation studies varying the temperature parameter τ, learning rate, and virtual node augmentation strength to determine their impact on both convergence and final accuracy
3. **Top-down signal isolation test:** Create controlled experiments comparing ForwardGNN with and without top-down signal incorporation on CoraML dataset to quantify their contribution to performance improvements