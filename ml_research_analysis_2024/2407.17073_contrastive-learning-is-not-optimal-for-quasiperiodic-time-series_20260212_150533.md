---
ver: rpa2
title: Contrastive Learning Is Not Optimal for Quasiperiodic Time Series
arxiv_id: '2407.17073'
source_url: https://arxiv.org/abs/2407.17073
tags:
- time
- different
- learning
- representations
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the limitation of contrastive learning methods
  in quasiperiodic time series analysis, where they fail to capture dynamic patterns
  within recordings. The authors propose DEAPS, a non-contrastive method that avoids
  negative pairs and introduces a "Gradual Loss" function to focus on dynamic patterns.
---

# Contrastive Learning Is Not Optimal for Quasiperiodic Time Series

## Quick Facts
- arXiv ID: 2407.17073
- Source URL: https://arxiv.org/abs/2407.17073
- Authors: Adrian Atienza; Jakob Bardram; Sadasivan Puthusserypady
- Reference count: 21
- Primary result: DEAPS achieves up to 10% improvement over state-of-the-art methods in quasiperiodic time series tasks

## Executive Summary
This work addresses the fundamental limitation of contrastive learning methods in quasiperiodic time series analysis, where they fail to capture dynamic patterns within recordings. The authors propose DEAPS (Distilled Embedding for Almost-Periodic Time Series), a non-contrastive method that avoids negative pairs and introduces a "Gradual Loss" function to focus on dynamic patterns. By disentangling static and dynamic features, DEAPS achieves significant performance improvements in tasks like atrial fibrillation detection and gender classification, using fewer labeled records. The approach is validated across multiple datasets and tasks, demonstrating superior performance in generalization.

## Method Summary
DEAPS is a self-supervised learning method designed to overcome the limitations of contrastive learning in quasiperiodic time series. The method uses a student-teacher framework with two projectors: one for static patterns (Gs) and one for dynamic patterns (Gd). Instead of contrastive loss, DEAPS employs a "Gradual Loss" function that encourages smooth interpolation between representations of different time points within the same record. The model also uses selective optimization to focus on the most important features for capturing dynamic patterns. The total loss combines similarity loss for static projections, gradual loss for dynamic patterns, and covariance loss to prevent redundancy. The encoder is a Vision Transformer with 6 transformer blocks, optimized using Adam with a learning rate of 3e-4.

## Key Results
- DEAPS achieves up to 10% improvement over state-of-the-art methods in atrial fibrillation detection and gender classification
- The method successfully disentangles static and dynamic features in quasiperiodic time series
- DEAPS requires fewer labeled records to achieve competitive performance compared to supervised methods
- The approach demonstrates superior generalization across multiple datasets including SHHS, MIT-ARR, MIT-AFIB, and Physionet Challenge 2017

## Why This Works (Mechanism)

### Mechanism 1
Contrastive learning forces representations of different records to be distant, causing the model to neglect dynamic patterns within a record. By minimizing distance between positive pairs and maximizing distance between negative pairs, the model focuses on static, subject-specific features and ignores temporal variations. This works because quasiperiodic signals have more pronounced differences between subjects than between classes within a subject. Evidence shows contrastive methods drive models to capture unique record-based representations while neglecting changes across the entire record.

### Mechanism 2
DEAPS introduces a gradual loss function (Lgra) to encourage the model to capture dynamic patterns. Lgra enforces that the representation of an intermediate time point (Xt) is an interpolation of representations of two other time points (Xt-i and Xt+j) from the same record, assuming smooth and linear evolution of dynamic patterns. This loss function guides the model to effectively capture dynamic patterns evolving throughout the record.

### Mechanism 3
DEAPS uses selective optimization to focus on the most important features for capturing dynamic patterns. The method calculates absolute difference between dynamic projections of Xt-i and Xt+j, and only considers N features with largest differences when computing Lgra. This ensures only features encoding changing patterns within a particular time window are considered when computing the loss function.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: Understanding limitations of contrastive learning in quasiperiodic time series analysis is crucial for appreciating need for DEAPS
  - Quick check question: What is the main objective of contrastive learning, and how does it potentially neglect dynamic patterns in quasiperiodic time series?

- Concept: Self-supervised learning (SSL)
  - Why needed here: DEAPS is a self-supervised learning method, so understanding general principles of SSL is important for grasping its approach
  - Quick check question: What is the main goal of self-supervised learning, and how does it differ from supervised learning?

- Concept: Principal Component Analysis (PCA)
  - Why needed here: PCA is used in evaluation of DEAPS to analyze representations and disentangle static and dynamic features
  - Quick check question: What is purpose of PCA, and how can it be used to analyze representations learned by DEAPS?

## Architecture Onboarding

- Component map: Input time series -> Encoder -> Static projector (Gs) + Dynamic projector (Gd) -> Predictor -> Student network; Teacher network (EMA of student)

- Critical path: 1) Input time series encoded into representations; 2) Static and dynamic projectors process representations; 3) Static loss (Lsim) computed between static projections of different records; 4) Gradual loss (Lgra) computed using dynamic projections within same record; 5) Covariance loss (Lc) applied to prevent redundancy; 6) Total loss backpropagated to update student network; 7) Teacher network parameters updated via EMA

- Design tradeoffs: Using two projectors allows disentangling static and dynamic features but adds complexity; gradual loss function requires window size parameter that needs tuning; selective optimization focuses on important features but may miss relevant information

- Failure signatures: Model fails to capture dynamic patterns if gradual loss is ineffective or window size is inappropriate; model overfits to static patterns if static projector is too dominant; model fails to generalize if selective optimization is too aggressive

- First 3 experiments: 1) Evaluate impact of window size on gradual loss by testing different sizes and measuring performance; 2) Assess effect of number of features in selective optimization by varying feature count; 3) Compare performance of DEAPS with different static/dynamic projector configurations

## Open Questions the Paper Calls Out
- What is optimal window size for capturing dynamic patterns in different quasiperiodic time series modalities?
- How does performance of DEAPS change with different levels of data augmentation?
- Can DEAPS be effectively extended to handle multivariate time series data?
- What is impact of different encoder architectures on performance of DEAPS?

## Limitations
- Assumption that quasiperiodic signals exhibit more pronounced between-subject differences than within-subject class differences may not hold for all datasets
- Effectiveness of selective optimization depends on assumption that only subset of features encode dynamic patterns
- Gradual loss function assumes smooth and linear evolution of dynamic patterns, which may not hold for all quasiperiodic signals

## Confidence
- **High confidence**: Empirical results showing DEAPS outperforms contrastive methods across multiple datasets and tasks
- **Medium confidence**: Theoretical explanation of why contrastive learning fails for quasiperiodic signals
- **Medium confidence**: Proposed solution of disentangling static and dynamic features through gradual loss function

## Next Checks
1. Test DEAPS on non-physiological quasiperiodic time series (e.g., synthetic signals with varying periodicities) to validate generalizability beyond biomedical data
2. Conduct ablation studies to quantify individual contributions of gradual loss, selective optimization, and static/dynamic projector disentanglement
3. Evaluate DEAPS's performance when assumption about between-subject vs within-subject differences is violated by creating controlled synthetic datasets