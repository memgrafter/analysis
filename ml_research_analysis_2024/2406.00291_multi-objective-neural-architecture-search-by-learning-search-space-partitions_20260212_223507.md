---
ver: rpa2
title: Multi-Objective Neural Architecture Search by Learning Search Space Partitions
arxiv_id: '2406.00291'
source_url: https://arxiv.org/abs/2406.00291
tags:
- search
- space
- lamoo
- architecture
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LaMOO introduces a novel approach to multi-objective neural architecture
  search by learning to partition the search space into promising and non-promising
  regions. It leverages Monte Carlo Tree Search to focus sampling on the most promising
  areas, guided by dominance numbers and hypervolume metrics.
---

# Multi-Objective Neural Architecture Search by Learning Search Space Partitions

## Quick Facts
- arXiv ID: 2406.00291
- Source URL: https://arxiv.org/abs/2406.00291
- Authors: Yiyang Zhao; Linnan Wang; Tian Guo
- Reference count: 28
- Key outcome: LaMOO improves sample efficiency by 200-500% compared to state-of-the-art multi-objective optimizers on benchmark NAS tasks.

## Executive Summary
LaMOO introduces a novel approach to multi-objective neural architecture search by learning to partition the search space into promising and non-promising regions. It leverages Monte Carlo Tree Search to focus sampling on the most promising areas, guided by dominance numbers and hypervolume metrics. LaMOO can be integrated with various NAS methods including one-shot, few-shot, and predictor-based approaches. On benchmark datasets like NasBench201, NasBench301, and HW-NAS-Bench, LaMOO demonstrates significant improvements in sample efficiency and architecture quality.

## Method Summary
LaMOO is a meta-algorithm that accelerates multi-objective neural architecture search by learning to partition the search space. It uses SVM classifiers to divide the space into promising and non-promising regions based on dominance numbers of observed architectures. Monte Carlo Tree Search then focuses sampling on the most promising regions, using Upper Confidence Bound values to balance exploration and exploitation. The method can be combined with various NAS evaluation approaches including one-shot, few-shot, and predictor-based methods, treating the partitioning as a meta-algorithm that adapts to different performance estimation strategies.

## Key Results
- Achieves 97.36% accuracy with only 1.62M parameters on CIFAR-10
- Reaches 80.4% top-1 accuracy with 522M FLOPs on ImageNet
- Outperforms MobileNetV2 in object detection while using fewer FLOPs
- Improves sample efficiency by 200-500% compared to state-of-the-art multi-objective optimizers on benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning space partitions speeds up multi-objective NAS by concentrating search on promising regions identified through dominance number ranking.
- Mechanism: LaMOO uses observed architectures to build a search tree that recursively partitions the space into good and bad regions based on dominance number distributions, then uses MCTS to focus sampling on the most promising leaves.
- Core assumption: Good architectures cluster in small, identifiable regions of the search space, making partitioning effective.
- Evidence anchors:
  - [abstract] "LaMOO speedups the search process by learning a model from observed samples to partition the search space and then focusing on promising regions likely to contain a subset of the Pareto frontier."
  - [section 2.1] "This observation implies that the identification of promising regions and the subsequent concentration of optimization algorithms within these regions can significantly enhance search efficiency."
  - [corpus] Weak - neighbors focus on similar partitioning or hardware-aware approaches but not dominance-based clustering.
- Break condition: If architectures are evenly distributed across objectives or if dominance numbers don't correlate with architecture quality.

### Mechanism 2
- Claim: Leaf selection in MCTS reduces computational cost by limiting hypervolume calculations to leaf nodes only.
- Mechanism: Instead of computing hypervolume at every node along a path, leaf selection only calculates UCB values at leaves, avoiding expensive hypervolume computation in non-leaf nodes.
- Core assumption: Hypervolume computation cost grows exponentially with number of objectives, making leaf-only calculation more efficient.
- Evidence anchors:
  - [section 4.2.1] "Leaf selection significantly reduces the hypervolume computation by only calculating the UCB1 value for all the leaf nodes...This integration allows LaMOO to explore potentially overlooked areas that might contain superior samples."
  - [section 4.2.1] "the computation cost of hypervolume is O(N^(M/2) + N log N), where N is the number of searched samples in total... That is, the hypervolume computation cost is growing exponentially with M when M > 3."
  - [corpus] Missing - no neighbor papers discuss MCTS or hypervolume computation tradeoffs.
- Break condition: If the number of objectives is small (M â‰¤ 3) or if leaf nodes contain too many samples to make computation efficient.

### Mechanism 3
- Claim: LaMOO can integrate with any NAS evaluation method (one-shot, few-shot, predictor-based) while maintaining sample efficiency gains.
- Mechanism: LaMOO treats the search space partition as a meta-algorithm that can be combined with any sampling strategy, adapting to different performance evaluation approaches without requiring retraining.
- Core assumption: Different NAS evaluation methods provide comparable quality rankings even if absolute performance estimates differ.
- Evidence anchors:
  - [abstract] "LaMOO can be integrated with various NAS methods including one-shot, few-shot, and predictor-based approaches."
  - [section 5] "We demonstrate the efficacy of LaMOO when applied to the problem of multi-objective neural architecture search. We detail how LaMOO can be combined with three major NAS evaluation approaches to enhance search efficiency."
  - [corpus] Weak - neighbors focus on specific NAS methods but don't discuss meta-algorithm integration.
- Break condition: If performance estimation methods produce rankings that are inconsistent with true architecture quality.

## Foundational Learning

- Concept: Dominance number and Pareto frontier in multi-objective optimization
  - Why needed here: LaMOO uses dominance numbers to rank architectures and identify promising regions in the search space.
  - Quick check question: If architecture A has lower accuracy but lower #FLOPs than architecture B, and there exists architecture C that dominates both, what is the dominance number of A and B?

- Concept: Hypervolume as a quality indicator for multi-objective optimization
  - Why needed here: LaMOO uses hypervolume to quantify the "goodness" of partitioned search regions and guide MCTS selection.
  - Quick check question: Given two sets of architectures, how would you determine which set represents a better approximation of the Pareto frontier using hypervolume?

- Concept: Monte Carlo Tree Search (MCTS) and Upper Confidence Bound (UCB) selection
  - Why needed here: LaMOO uses MCTS with UCB values to select promising regions for sampling in the partitioned search space.
  - Quick check question: In the context of LaMOO, what trade-off does the exploration term in UCB control during region selection?

## Architecture Onboarding

- Component map: Initial random sampling -> SVM-based space partitioning -> MCTS tree construction -> Leaf selection with UCB -> Architecture sampling using chosen NAS method -> Repeat
- Critical path: 1) Initialize with random samples, 2) Partition space recursively using SVM classifiers, 3) Build MCTS tree, 4) Select promising leaf node using UCB, 5) Sample new architectures using chosen NAS method, 6) Repeat until budget exhausted
- Design tradeoffs: Space partitioning vs. exploration balance - aggressive partitioning may miss good regions, while conservative partitioning reduces efficiency gains
- Failure signatures: 1) Poor performance if dominance numbers don't correlate with architecture quality, 2) High computational cost if hypervolume calculations become expensive, 3) Ineffective sampling if integration with chosen NAS method doesn't work well
- First 3 experiments:
  1. Implement LaMOO with random sampling on NasBench201 to verify space partitioning effectiveness before integrating with qEHVI
  2. Test different SVM kernels (linear vs. RBF) on a small search space to determine which works best for your specific problem
  3. Compare leaf selection vs. path selection on a medium-sized problem to quantify computational savings and performance impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LaMOO scale with the number of objectives beyond three (many-objective optimization)?
- Basis in paper: [explicit] The paper mentions that hypervolume computation becomes exponentially expensive as the number of objectives increases beyond three, potentially slowing down the search speed.
- Why unresolved: The paper only evaluates LaMOO on tasks with up to four objectives (HW-NAS-Bench), and does not explore the performance limitations or adaptations needed for higher-dimensional objective spaces.
- What evidence would resolve it: Empirical results showing LaMOO's performance and computational efficiency on benchmark problems with five or more objectives, compared to other many-objective optimization methods.

### Open Question 2
- Question: How sensitive is LaMOO's performance to the choice of classification algorithm for space partitioning beyond SVM?
- Basis in paper: [explicit] The paper uses SVM for space partitioning but mentions that other classification models, such as deep neural networks, could potentially enhance the quality of space partitioning.
- Why unresolved: The paper only evaluates SVM and does not explore alternative classification methods or their impact on search efficiency and final architecture quality.
- What evidence would resolve it: Comparative experiments using different classification algorithms (e.g., neural networks, decision trees) for space partitioning within LaMOO, measuring their impact on convergence speed and final architecture performance.

### Open Question 3
- Question: Can LaMOO's space partitioning approach be effectively combined with search space optimization techniques like MCUNet?
- Basis in paper: [explicit] The paper discusses MCUNet as a complementary approach that could work with LaMOO by refining the search space based on historical architecture samples.
- Why unresolved: The paper does not implement or evaluate this combination, leaving open the question of whether integrating MCUNet's device-specific search space design with LaMOO's adaptive partitioning would yield better results.
- What evidence would resolve it: Experimental results comparing LaMOO alone versus LaMOO combined with MCUNet's search space optimization on tasks requiring specific hardware constraints (latency, memory, etc.).

## Limitations

- Effectiveness depends on the assumption that promising architectures cluster in identifiable regions of the search space
- Computational overhead of SVM-based partitioning and hypervolume calculations could offset efficiency gains, especially for problems with many objectives (M > 3)
- Integration with different NAS evaluation methods may produce inconsistent performance improvements depending on the quality of the performance estimation method

## Confidence

- High confidence in the computational efficiency of leaf-only hypervolume calculation and the integration flexibility with various NAS methods
- Medium confidence in the general effectiveness of space partitioning for accelerating multi-objective NAS, pending validation across diverse problem types
- Low confidence in the scalability of the approach to very large search spaces or problems with many objectives due to potential computational bottlenecks

## Next Checks

1. Test LaMOO's performance on search spaces where architecture quality is not clustered to validate the core partitioning assumption
2. Measure actual computational overhead of SVM training and hypervolume calculations versus claimed efficiency gains across different numbers of objectives
3. Compare LaMOO's integration with different NAS evaluation methods (one-shot vs. predictor-based) on the same benchmark to quantify consistency of performance improvements