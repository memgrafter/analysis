---
ver: rpa2
title: Improving Model Evaluation using SMART Filtering of Benchmark Datasets
arxiv_id: '2410.20245'
source_url: https://arxiv.org/abs/2410.20245
tags:
- examples
- filtering
- dataset
- smart
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SMART filtering improves model evaluation by removing less informative
  examples from benchmark datasets. The method filters out easy examples, data-contaminated
  examples, and similar examples based on embeddings.
---

# Improving Model Evaluation using SMART Filtering of Benchmark Datasets

## Quick Facts
- arXiv ID: 2410.20245
- Source URL: https://arxiv.org/abs/2410.20245
- Reference count: 30
- Primary result: SMART filtering reduces dataset sizes by 34-69% while maintaining model rankings and improving correlation with human preferences

## Executive Summary
SMART filtering is a methodology for improving NLP model evaluation by removing less informative examples from benchmark datasets. The approach filters out easy examples (where all top models agree and answer correctly with high confidence), data-contaminated examples (likely present in training data), and similar examples (redundant based on embedding distance). Tested on ARC, MMLU, and CommonsenseQA datasets, SMART filtering reduces dataset sizes by 34-69% while maintaining model rankings and showing higher correlation with human preference rankings from ChatBot Arena. The method increases computational efficiency and suggests these datasets are not yet saturated, leaving room for further model improvements.

## Method Summary
SMART filtering applies three criteria to improve model evaluation: removing easy examples identified by unanimous correct predictions among top models with confidence >0.8, removing data-contaminated examples identified by correct predictions without question context, and removing similar examples based on cosine distance in embedding space. The methodology uses SentenceBERT embeddings for similarity detection, kernel density estimation to determine similarity thresholds, and maintains evaluation integrity while reducing computational requirements. The approach is generic and can be applied to various dataset types beyond the tested multiple-choice question answering benchmarks.

## Key Results
- Dataset size reduction: 34-69% across ARC, MMLU, and CommonsenseQA
- Model ranking preservation: High Kendall's Tau correlations between original and filtered datasets
- Human preference correlation: Higher Pearson correlation with ChatBot Arena Elo scores after filtering
- Computational efficiency: Reduced evaluation time while maintaining informative assessment of model capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing easy examples improves model differentiation and increases headroom for future models.
- Mechanism: Easy examples create artificial ceilings where all models score near-perfectly, masking differences in capability. By filtering these out, remaining examples better differentiate models based on their actual reasoning abilities.
- Core assumption: Examples that all top models answer correctly with high confidence are genuinely easy and not reflective of true model capability differences.
- Evidence anchors:
  - [abstract]: "Our approach applies three filtering criteria, removing (i) easy examples, (ii) data-contaminated examples, and (iii) examples that are similar to each other based on distance in an embedding space."
  - [section]: "We define easy examples based on the agreement between top-performing open-source models from Open LLM leaderboard... Specifically, we consider an example to be easy if there is unanimous agreement among all top-performing models, with each model answering the example correctly with high confidence (greater than 0.8)."
- Break condition: If model architectures converge such that they all struggle equally with previously "hard" examples, or if the definition of "easy" becomes too narrow and removes genuinely useful examples.

### Mechanism 2
- Claim: Filtering data-contaminated examples prevents inflated performance measurements and maintains evaluation integrity.
- Mechanism: Examples present in training data give models unfair advantages through memorization. By removing examples that models can answer correctly without context (question text), we eliminate leaked examples that would otherwise overstate model capabilities.
- Core assumption: Models that can answer questions correctly without context are likely to have seen these exact examples during training, indicating contamination.
- Evidence anchors:
  - [abstract]: "removing data-contaminated examples that are highly likely to have been leaked into the training datasets"
  - [section]: "We modify the prompt to the model by removing the question text and presenting only the answer choices... In this step, we again take agreement between top-performing models. We employ a stringent criterion: an example is deemed contaminated only if all top n open-source models answer it correctly without any context and with high confidence (greater than 0.8)."
- Break condition: If models develop genuine reasoning capabilities that allow them to answer questions correctly without context (not due to contamination), or if the contamination detection method produces too many false positives.

### Mechanism 3
- Claim: Removing similar examples reduces redundancy and prevents bias toward specific knowledge patterns.
- Mechanism: Highly similar examples provide overlapping information about model performance. By clustering and randomly removing half of similar examples, we maintain dataset diversity while reducing computational load and preventing models from being unfairly advantaged on specific types of examples.
- Core assumption: Examples with embeddings closer than a threshold δ in the embedding space are sufficiently similar to be redundant for evaluation purposes.
- Evidence anchors:
  - [abstract]: "removing examples that are similar to each other based on distance in an embedding space"
  - [section]: "We compute the cosine distance between each pair of example embeddings... We employ kernel density estimation to locate the first local maximum of this distribution, capturing the point at which the similarity begins to decrease significantly."
- Break condition: If the embedding threshold δ is set too low and removes genuinely distinct examples, or if the random selection process inadvertently removes the most informative examples from each cluster.

## Foundational Learning

- Concept: Cosine distance and similarity in embedding space
  - Why needed here: Used to identify similar examples by measuring the distance between example embeddings
  - Quick check question: If two examples have a cosine distance of 0.1, are they considered similar or dissimilar?

- Concept: Pearson correlation coefficient
  - Why needed here: Used to measure the linear relationship between model accuracies and ChatBot Arena Elo scores
  - Quick check question: What Pearson correlation value would indicate a perfect linear relationship between two variables?

- Concept: Kendall's Tau correlation
  - Why needed here: Used to measure the consistency of model rankings between original and filtered datasets
  - Quick check question: If Kendall's Tau is 0.95, what does this indicate about the relationship between two ranking lists?

## Architecture Onboarding

- Component map:
  Pre-filtering -> Easy examples filter -> Data contamination filter -> Similar examples filter -> Evaluation pipeline

- Critical path:
  1. Load dataset and apply pre-filtering
  2. Identify easy examples using model agreement
  3. Identify data-contaminated examples using context-free prompting
  4. Compute embeddings and cluster similar examples
  5. Apply all filters and create SMART-filtered dataset
  6. Evaluate models on both original and filtered datasets
  7. Compare rankings and correlations

- Design tradeoffs:
  - Strict vs. lenient filtering thresholds: Stricter thresholds remove more examples but risk losing valuable data
  - Model selection for agreement: Using more models increases reliability but increases computational cost
  - Embedding choice: SentenceBERT is faster but LLM2Vec might capture semantics better

- Failure signatures:
  - Too many examples removed: Indicates overly strict thresholds or poor model selection
  - Model rankings change significantly: Suggests filtering is removing informative examples
  - High computational cost: Could indicate inefficient implementation of similarity detection

- First 3 experiments:
  1. Apply SMART filtering to a small subset of MMLU and verify that model rankings remain stable
  2. Test the sensitivity of the easy examples filter by varying the confidence threshold from 0.7 to 0.9
  3. Compare the effectiveness of SentenceBERT vs. LLM2Vec embeddings for identifying similar examples on a sample dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SMART filtering perform on non-MCQA datasets, and what modifications would be needed for different task types?
- Basis in paper: [explicit] The authors state "we think our inclusion of different numbers of MCQA answer options between 4 and 5 gives some indication that the methodology will be adaptable to smaller numbers of answer options (2 or 3) too" and acknowledge their methodology is generic and can be applied to various dataset types.
- Why unresolved: The paper only tests SMART filtering on three multiple-choice question answering datasets (ARC, MMLU, and CommonsenseQA). While the authors suggest it could work for other classification tasks recast into MCQA format, they haven't actually tested this on tasks like NLI, coreference resolution, or sentiment analysis.
- What evidence would resolve it: Empirical results showing SMART filtering performance on at least two different non-MCQA NLP tasks, demonstrating both the effectiveness of the methodology and any necessary adaptations for different task formats.

### Open Question 2
- Question: How would SMART filtering methodology change if we could access model training data to identify data contamination?
- Basis in paper: [explicit] The authors note "We recognize that this strict criterion may underestimate the number of contaminated examples. As the research in detecting data contamination improves, we plan to incorporate more sophisticated techniques that can detect contamination more effectively while balancing precision and recall."
- Why unresolved: The current SMART filtering approach for detecting data contamination uses a conservative method that only identifies examples all models can answer correctly without question context. The authors acknowledge this may underestimate contamination and express interest in more sophisticated techniques.
- What evidence would resolve it: Comparative analysis showing the difference in dataset size reduction and model rankings when using SMART filtering with the current approach versus an approach with access to training data, demonstrating whether the additional data access leads to meaningfully different results.

### Open Question 3
- Question: How would SMART filtering results differ if using LLM-based embeddings versus SentenceBERT for identifying similar examples?
- Basis in paper: [explicit] The authors conducted experiments comparing SentenceBERT with LLM2Vec and LLM-based embeddings, finding "a high degree of similarity between the two embedding methods, with an average overlap of 88.7% between SentenceBERT and different LLM models."
- Why unresolved: While the authors found high overlap between embedding methods, they chose SentenceBERT due to "widespread usage in various applications" and "lower computational requirements." They haven't explored whether the slight differences in overlap might lead to meaningfully different filtered datasets or model rankings.
- What evidence would resolve it: Direct comparison of SMART filtering results (dataset size reduction, model rankings, correlation with ChatBot Arena) using both SentenceBERT and LLM-based embeddings on the same datasets, showing whether the choice of embedding method has practical implications for the methodology's outcomes.

## Limitations

- The approach may underestimate data contamination by only identifying examples all models can answer correctly without context
- Random selection of similar examples for removal may systematically eliminate more informative examples from certain knowledge domains
- The choice of SentenceBERT embeddings, while computationally efficient, may not capture semantic nuances as effectively as LLM-based embeddings

## Confidence

*High Confidence:* The core observation that SMART filtering reduces dataset sizes by 34-69% while maintaining model rankings is well-supported by the experimental results.

*Medium Confidence:* The assumption that easy examples create artificial ceilings for model evaluation is logically sound, but the specific threshold may be overly conservative.

*Low Confidence:* The long-term implications of removing similar examples through random selection are uncertain and may introduce subtle biases in evaluation.

## Next Checks

1. **Sensitivity Analysis of Filtering Thresholds:** Systematically vary the confidence thresholds for easy example detection (0.7, 0.8, 0.9) and data contamination detection to quantify the impact on dataset reduction and ranking stability.

2. **Embedding Method Comparison:** Implement the similarity detection step using LLM2Vec embeddings in addition to SentenceBERT, then compare the resulting filtered datasets in terms of size reduction, model ranking preservation, and correlation with human preferences.

3. **Cluster-Based Example Selection:** Modify the similar examples removal step to use k-means clustering and retain the most central example from each cluster rather than random selection, then compare the resulting dataset quality metrics with the current approach.