---
ver: rpa2
title: 'FinTextQA: A Dataset for Long-form Financial Question Answering'
arxiv_id: '2405.09980'
source_url: https://arxiv.org/abs/2405.09980
tags:
- bge-reranker-base
- llmrerank
- all-mpnet-base-v2
- question
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FinTextQA, the first long-form question answering
  (LFQA) dataset for finance, containing 1,262 high-quality QA pairs with an average
  document length of 19,779 words. The dataset includes complex financial questions
  spanning six categories, sourced from finance textbooks and government agency websites.
---

# FinTextQA: A Dataset for Long-form Financial Question Answering

## Quick Facts
- arXiv ID: 2405.09980
- Source URL: https://arxiv.org/abs/2405.09980
- Authors: Jian Chen, Peilin Zhou, Yining Hua, Yingxin Loh, Kehui Chen, Ziyuan Li, Bing Zhu, Junwei Liang
- Reference count: 17
- Key outcome: Introduces FinTextQA, the first LFQA dataset for finance with 1,262 QA pairs and an average document length of 19,779 words, achieving strong performance with a RAG-based system using Ada2, AMR, Bge-Reranker-Base, and Baichuan2-7B.

## Executive Summary
This paper introduces FinTextQA, the first long-form question answering dataset specifically designed for the financial domain. The dataset contains 1,262 high-quality QA pairs with complex questions spanning six categories, sourced from finance textbooks and government agency websites. The authors develop a RAG-based LFQA system and evaluate it using multiple metrics including human ranking, automatic metrics, and GPT-4 scoring. Results show that the optimal system configuration achieves strong performance, with Baichuan2-7B closely matching GPT-3.5-turbo in accuracy. The system demonstrates robustness to noise when input contexts exceed approximately 34,000 words.

## Method Summary
The authors constructed the FinTextQA dataset by collecting and processing QA pairs from finance textbooks and government websites, ensuring quality through human annotation and relevance scoring. They implemented a modular RAG system with embedder, retriever, reranker, and generator modules, testing different model configurations including embedders (Ada2, Ember-v1, Bge-Small-en-v1.5), retrievers (AMR, SWR, vector-based), rerankers (Bge-Reranker-Base, LLMRerank, All-Mpnet-Base-v2), and generators (GPT-3.5-turbo, Baichuan2-7B, Qwen-7B, LLaMA2-7B, Solar-10.7B, Gemini-Pro). Generators were fine-tuned on the FinTextQA training set, and system performance was evaluated using automatic metrics (ROUGE, BLEU) and GPT-4 scoring for question-evidence and evidence-answer relevance, along with human ranking of top system outputs.

## Key Results
- Optimal system configuration uses Ada2, AMR, Bge-Reranker-Base, and Baichuan2-7B
- Baichuan2-7B closely matches GPT-3.5-turbo in accuracy score
- Models are less susceptible to noise when input contexts exceed approximately 34,000 words
- Different model configurations significantly impact performance

## Why This Works (Mechanism)

### Mechanism 1
RAG-based LFQA systems significantly improve performance over fine-tuning-only approaches by providing relevant context from external documents. The RAG framework combines retrieval (finding relevant document chunks) with generation (producing answers based on retrieved context), allowing the model to leverage up-to-date and comprehensive information beyond its training data, leading to more accurate and detailed responses. Core assumption: The retrieved context is relevant to the question and enhances the model's ability to generate a correct answer. Evidence: [abstract] "we opt for the Retrieval-augmented generation (RAG) framework... By processing documents in multiple steps, RAG systems can pre-process and provide the most relevant information to LLMs, enhancing their performance and explanation capabilities". Break condition: If the retriever consistently fails to find relevant documents, or if the reranker does not effectively filter noise, the system's performance will degrade significantly.

### Mechanism 2
The choice of embedding model significantly impacts the quality of retrieved documents, which in turn affects the final answer quality. The embedder converts text into vector representations, and a high-quality embedder like Ada2 (which achieved the highest GPT-4 score of 4.586) can capture semantic similarities more effectively, leading to better retrieval of relevant documents. Core assumption: Semantic similarity in the embedding space correlates with relevance for the specific task of financial question answering. Evidence: [abstract] "Among all compared generators, Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score" and [section] "Table 4 shows the GPT-4 score of different embedders... It also shows at the end the best-performing evidence-generation module combinations. We observe that the highest-performing embedding model is Ada2, achieving a score of 4.586". Break condition: If the embedder fails to capture domain-specific terminology or the nuances of financial language, retrieval quality will suffer regardless of the retriever or reranker used.

### Mechanism 3
Systems are less susceptible to noise when input contexts exceed approximately 34,000 words, indicating a threshold effect in information processing. As more documents are added, the system initially experiences performance degradation due to noise, but after a certain threshold, the additional information begins to outweigh the noise, stabilizing or slightly improving performance. Core assumption: There exists an optimal range of context where relevant information dominates over noise, and this range is specific to the dataset and task complexity. Evidence: [abstract] "models are less susceptible to noise after the length of contexts reaching a specific threshold" and [section] "When the number of context words reaches about 34k words, adding more input documents exerts a less marginal effect on system performance". Break condition: If the threshold is significantly different for different types of questions or if the documents vary greatly in relevance, this mechanism may not hold consistently.

## Foundational Learning

- Concept: Semantic embeddings and vector space models
  - Why needed here: The embedder module relies on converting text into vector representations to enable semantic search and retrieval.
  - Quick check question: How does cosine similarity between document and query vectors indicate relevance in information retrieval?

- Concept: Information retrieval and ranking algorithms
  - Why needed here: The retriever and reranker modules are responsible for finding and prioritizing relevant document chunks from a large corpus.
  - Quick check question: What is the difference between keyword-based retrieval and semantic retrieval, and why is the latter preferred for complex financial questions?

- Concept: Evaluation metrics for text generation
  - Why needed here: The system's performance is evaluated using metrics like ROUGE, BLEU, and human rankings to assess answer quality.
  - Quick check question: Why might matching-based metrics like ROUGE overestimate performance in long sequences, and what alternative evaluation methods could address this limitation?

## Architecture Onboarding

- Component map: Question → Embedder → Retriever → Reranker → Generator → Answer
- Critical path: Question → Embedder → Retriever → Reranker → Generator → Answer
- Design tradeoffs:
  - Embedding quality vs. computational cost: More sophisticated embeddings may improve retrieval but increase latency.
  - Retriever recall vs. precision: Aggressive retrieval may find more relevant documents but also more noise.
  - Reranker complexity vs. effectiveness: More advanced rerankers may better filter noise but add computational overhead.
  - Generator size vs. performance: Larger models may generate better answers but are more expensive to run.
- Failure signatures:
  - Poor retrieval quality: Low GPT-4 scores for evidence generation, irrelevant documents in top results
  - Inadequate reranking: High noise in final context, irrelevant information included in answers
  - Generator limitations: High ratio of unanswered questions, low ROUGE/BLEU scores, poor human rankings
- First 3 experiments:
  1. Test different embedder models (Ada2, Ember-v1, Bge-Small-en-v1.5) with the same retriever and reranker to isolate their impact on retrieval quality.
  2. Compare the AMR and SWR retrievers with the same embedder and reranker to evaluate their effectiveness in finding relevant documents.
  3. Evaluate different generator models (GPT-3.5-turbo, Baichuan2-7B, LLaMA2-7B) with the same retrieval pipeline to assess their ability to produce high-quality answers.

## Open Questions the Paper Calls Out

### Open Question 1
What is the precise impact of document length on model performance in the presence of noise, and does this relationship vary across different model architectures? Basis in paper: [explicit] The paper states that "models are less susceptible to noise after the length of contexts reaching a specific threshold" and provides a threshold of approximately 34,000 words. Why unresolved: The paper only provides a single threshold value without exploring whether this relationship is linear, non-linear, or varies across different model types (generators, retrievers, etc.). What evidence would resolve it: Systematic experiments varying document lengths beyond 34,000 words and across multiple model architectures, measuring performance degradation as noise increases.

### Open Question 2
How does the performance of fine-tuned generators compare to their base forms when evaluated using task-specific metrics beyond ROUGE and BLEU? Basis in paper: [explicit] The paper notes that fine-tuned models "do not have significant improvement" in automatic metrics but "have less unanswered questions." Why unresolved: The paper only compares fine-tuned vs. base forms using matching-based metrics, but doesn't explore other task-specific evaluation methods that might better capture the benefits of fine-tuning. What evidence would resolve it: Evaluation using task-specific metrics like question-answer relevance, factual consistency, or domain-specific financial accuracy measures.

### Open Question 3
What is the optimal balance between document relevance and document quantity in the RAG system, and how does this balance affect overall performance? Basis in paper: [inferred] The paper tests configurations with 1-3 documents but doesn't systematically explore the trade-off between retrieving highly relevant single documents versus multiple moderately relevant documents. Why unresolved: The paper only tests discrete document quantities (1, 2, or 3) without exploring the continuous trade-off or the diminishing returns of adding more documents. What evidence would resolve it: Experiments varying the number of retrieved documents from 1 to 10+ while measuring both retrieval quality and final answer quality, identifying the point of diminishing returns.

## Limitations
- Dataset size (1,262 QA pairs) may limit robustness of conclusions about optimal model configurations
- Findings may not generalize beyond financial domain due to domain-specific construction
- Reliance on GPT-4 for scoring may introduce bias favoring certain answer patterns

## Confidence
- High confidence: The effectiveness of the RAG framework in improving LFQA performance by providing relevant context from external documents
- Medium confidence: The specific optimal configuration (Ada2, AMR, Bge-Reranker-Base, Baichuan2-7B) achieving the best results, as this may vary with different datasets or evaluation metrics
- Medium confidence: The threshold effect at approximately 34,000 words where models become less susceptible to noise, as this may depend on document characteristics and question types

## Next Checks
1. Test the system on a non-financial long-form QA dataset to assess domain generalization and determine if the optimal configuration remains effective
2. Conduct ablation studies by removing the reranker component to quantify its specific contribution to performance and validate the claim that it effectively filters noise
3. Evaluate the system with human-only annotations (without GPT-4 scoring) to verify the accuracy of the GPT-4-based evaluation methodology and check for potential scoring biases