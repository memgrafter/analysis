---
ver: rpa2
title: 'F-Eval: Assessing Fundamental Abilities with Refined Evaluation Methods'
arxiv_id: '2401.14869'
source_url: https://arxiv.org/abs/2401.14869
tags:
- evaluation
- llms
- each
- prompt
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'F-Eval introduces a bilingual evaluation benchmark to assess the
  fundamental abilities of large language models, addressing the gap in existing benchmarks
  that focus on instruction-following capabilities while overlooking fundamental abilities
  emerging during pre-training. The benchmark evaluates models across three dimensions:
  expression, commonsense, and logic, using 15 sub-datasets with formats including
  multi-choice, open-ended, reference-based, and reference-free subjective tasks.'
---

# F-Eval: Assessing Fundamental Abilities with Refined Evaluation Methods

## Quick Facts
- arXiv ID: 2401.14869
- Source URL: https://arxiv.org/abs/2401.14869
- Reference count: 31
- Key outcome: F-Eval introduces a bilingual evaluation benchmark to assess fundamental abilities of LLMs, achieving correlation coefficients up to 0.797 with human judgments and outperforming traditional baselines like BLEU and BERTScore.

## Executive Summary
F-Eval addresses the gap in existing benchmarks that focus on instruction-following capabilities while overlooking fundamental abilities emerging during pre-training. The benchmark evaluates models across three dimensions—expression, commonsense, and logic—using 15 sub-datasets with diverse formats. For reference-free subjective tasks, F-Eval devises new evaluation methods as alternatives to API model scoring, leveraging assistant tools like dictionaries and judge models. Experiments on 13 advanced LLMs show that F-Eval achieves higher correlation coefficients with human judgments and larger score distinctions compared to traditional baselines, while highlighting substantial room for improvement in fundamental abilities, particularly for open-source models.

## Method Summary
F-Eval is a bilingual evaluation benchmark designed to assess fundamental abilities of large language models across three dimensions: expression, commonsense, and logic. The benchmark uses 15 sub-datasets with formats including multi-choice, open-ended, reference-based, and reference-free subjective tasks. For reference-free subjective tasks, the authors devise new evaluation methods leveraging assistant tools like dictionaries and judge models as alternatives to API model scoring. The evaluation pipeline includes data collection, evaluation method selection, model inference, score calculation, self-adaptive normalization, result aggregation, and meta-evaluation. The self-adaptive normalization method dynamically scales scores using dataset-specific parameters calculated from the second-highest and second-lowest scores, compressing the range to 10-90 while maintaining relative differences.

## Key Results
- F-Eval achieves correlation coefficients with human judgments up to 0.797, outperforming traditional baselines like BLEU and BERTScore.
- The benchmark shows larger score distinctions between models compared to baselines, with GPT4.0 and GPT3.5 outperforming other models while open-source models lag significantly.
- Self-adaptive normalization outperforms rank-based normalization, with correlations reaching 0.735 for judge models compared to 0.688 for rank-based approaches.

## Why This Works (Mechanism)

### Mechanism 1
The self-adaptive normalization method preserves score distinctions between models while enabling fair comparison across heterogeneous evaluation methods. The normalization dynamically scales each sub-dataset's scores using dataset-specific parameters (α, β) calculated from the second-highest and second-lowest scores, compressing the range to 10-90. This maintains relative differences within each sub-dataset while avoiding outlier distortion. The core assumption is that using second-highest/second-lowest scores effectively removes boundary effects without losing too much information.

### Mechanism 2
Assistant-tool evaluation methods provide more reliable and consistent scoring for reference-free subjective tasks compared to API model scoring. Dictionary-based evaluation for Word Diversity assesses vocabulary rarity, while judge models calculate probability differences for Informative tasks, providing objective metrics less susceptible to model-specific biases than LLM-as-a-judge approaches. The core assumption is that dictionary-based rarity scores and probability difference calculations are more stable indicators of text quality than subjective LLM judgments.

### Mechanism 3
The benchmark's focus on fundamental abilities (expression, commonsense, logic) rather than instruction-following provides more comprehensive assessment of pre-trained model capabilities. By evaluating base models without instruction-following enhancements, the benchmark captures intrinsic model abilities that emerge during pre-training, including diverse vocabulary usage, commonsense reasoning, and logical coherence. The core assumption is that fundamental abilities assessed in base models correlate with downstream task performance and are not merely artifacts of fine-tuning.

## Foundational Learning

- Concept: Zero-shot versus few-shot evaluation settings
  - Why needed here: The benchmark uses different settings across sub-datasets (e.g., ICL uses 0-shot and 4-shot, Commonsense Triple uses 5-shot) to assess different aspects of model capability.
  - Quick check question: Why does ICL use both 0-shot and 4-shot settings while most other tasks use zero-shot evaluation?

- Concept: Probability-based evaluation for multi-choice tasks
  - Why needed here: For tasks like CommonsenseQA and Story, the benchmark uses perplexity-based selection of the lowest PPL option to determine correct answers.
  - Quick check question: How does the probability evaluation method handle API models differently from open-source models?

- Concept: Reference-based versus reference-free subjective evaluation
  - Why needed here: The benchmark distinguishes between tasks with gold references (using GPT4.0 scoring) and those without (using assistant-tool methods).
  - Quick check question: What evaluation method is used for Emotion Consistency, and why does it differ from the method used for Commonsense Triple?

## Architecture Onboarding

- Component map: Data collection (15 sub-datasets across 3 dimensions) → Evaluation method assignment (rule-based, probability, assistant-tool, API) → Model inference → Score calculation → Normalization (self-adaptive) → Result aggregation → Meta-evaluation (correlation with human judgments)
- Critical path: Data collection → Evaluation method assignment → Model inference → Score calculation → Normalization → Result aggregation → Meta-evaluation
- Design tradeoffs: Using GPT4.0 as evaluator provides high correlation but increases cost; assistant-tool methods reduce cost but may have lower correlation; self-adaptive normalization preserves distinctions but adds complexity.
- Failure signatures: Low correlation with human judgments suggests evaluation method issues; compressed score distributions indicate normalization problems; inconsistent results across model sizes suggest dataset quality issues.
- First 3 experiments:
  1. Run all 13 models on a small subset (3 sub-datasets, 50 instances each) to verify pipeline functionality and identify any data collection issues.
  2. Test different normalization parameters (γ values) on the small subset to understand their impact on score distributions and model rankings.
  3. Compare assistant-tool evaluation methods against GPT4.0 scoring on reference-free tasks to quantify the correlation difference and identify any systematic biases.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations section, key areas for future research include expanding the benchmark to cover additional fundamental abilities beyond the current three dimensions, exploring how different normalization methods affect evaluation results across model sizes, and investigating the impact of pre-training data distributions on fundamental abilities.

## Limitations
- The benchmark's reliance on GPT4.0 for reference-based subjective evaluation and human annotations for correlation validation introduces potential bias and limits generalizability across different model families.
- The self-adaptive normalization method lacks comprehensive ablation studies to validate the choice of second-highest/second-lowest scores for parameter calculation.
- The evaluation methods for reference-free subjective tasks (dictionary-based and judge model approaches) have not been validated against human judgments for all sub-datasets.

## Confidence
- High Confidence: The benchmark's structure (15 sub-datasets across three dimensions), evaluation pipeline, and basic methodology are clearly specified and reproducible.
- Medium Confidence: The correlation results with human judgments and score distinctions are well-documented, though the exact human evaluation procedures remain unclear.
- Low Confidence: The assistant-tool evaluation methods for reference-free tasks and the self-adaptive normalization approach lack sufficient validation against human judgments across all sub-datasets.

## Next Checks
1. Conduct human evaluation studies comparing dictionary-based and judge model approaches against human judgments for all reference-free subjective tasks to establish reliability metrics.
2. Perform ablation studies testing different parameter calculation methods (min/max vs second-highest/second-lowest) and compression ranges to understand their impact on score distributions and model rankings.
3. Evaluate the benchmark's correlation with human judgments across different model families (GPT vs open-source) separately to identify any systematic biases in the evaluation methods.