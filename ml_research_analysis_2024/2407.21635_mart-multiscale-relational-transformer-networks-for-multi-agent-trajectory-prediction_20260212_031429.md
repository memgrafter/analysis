---
ver: rpa2
title: 'MART: MultiscAle Relational Transformer Networks for Multi-agent Trajectory
  Prediction'
arxiv_id: '2407.21635'
source_url: https://arxiv.org/abs/2407.21635
tags:
- group
- trajectory
- prediction
- transformer
- mart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MART, a hypergraph transformer network for
  multi-agent trajectory prediction. MART captures individual and group behaviors
  through a novel MultiscAle Relational Transformer Encoder (MARTE) and an Adaptive
  Group Estimator (AGE).
---

# MART: MultiscAle Relational Transformer Networks for Multi-agent Trajectory Prediction

## Quick Facts
- arXiv ID: 2407.21635
- Source URL: https://arxiv.org/abs/2407.21635
- Reference count: 40
- 3.9% and 11.8% improvements in ADE and FDE on the NBA dataset compared to previous methods

## Executive Summary
MART introduces a hypergraph transformer network for multi-agent trajectory prediction that captures both individual and group behaviors. The method combines a novel MultiscAle Relational Transformer Encoder (MARTE) with an Adaptive Group Estimator (AGE) to model complex interactions in real-world environments. MARTE extends relational transformers with a Hyper Relational Transformer that incorporates hyperedge features to focus attention on group-wise relations, while AGE infers overlapping group relations using adaptive thresholding. The approach achieves state-of-the-art performance on NBA, SDD, and ETH-UCY datasets while demonstrating significant computational efficiency gains.

## Method Summary
MART consists of four main components: feature initialization, AGE, MARTE, and a decoder. The method transforms past trajectories into agent embeddings through node and edge feature initialization, then uses AGE to infer group relations via adaptive thresholding inspired by Straight-Through Estimation. MARTE combines Pair-wise Relational Transformer (PRT) and Hyper Relational Transformer (HRT) to capture both individual and group behaviors, with HRT integrating hyperedge features into attention mechanisms. The model is trained using the Adam optimizer with learning rate decay, and predictions are made through multi-head decoding with selection based on minimum ADE or FDE.

## Key Results
- Achieves 3.9% and 11.8% improvements in ADE and FDE on NBA dataset compared to previous methods
- Demonstrates 7.3x fewer parameters and 347.3x fewer MACs than previous best method
- Shows state-of-the-art performance on NBA, SDD, and ETH-UCY datasets

## Why This Works (Mechanism)

### Mechanism 1
MART captures both individual and group behaviors by integrating hyperedge features into transformer attention via Hyper Relational Attention (HRA). HRA aggregates hyperedge features belonging to each agent and injects them as relative positional information into the query, key, and value vectors, allowing attention to focus on group-wise relations in addition to pairwise ones. The core assumption is that hyperedge features can encode meaningful group information that, when added to the attention computation, improves relational reasoning over groups.

### Mechanism 2
Adaptive Group Estimator (AGE) infers overlapping group relations in real-world environments using adaptive thresholding inspired by Straight-Through Estimation. AGE computes an affinity matrix from node embeddings, applies a learnable threshold Θ, and uses STE to backpropagate through the unit step function, yielding a group incidence matrix G where agents can belong to multiple groups. The core assumption is that real-world group relations are overlapping and can be inferred by thresholding pairwise similarities between agent embeddings.

### Mechanism 3
MARTE, combining Pair-wise Relational Transformer (PRT) and Hyper Relational Transformer (HRT), outperforms pure pairwise or group-only models by jointly modeling individual and group interactions. PRT handles pairwise relations using relational attention while HRT handles group relations using HRA, and concatenating their outputs gives the decoder richer interaction context. The core assumption is that joint modeling of pair-wise and group-wise relations is more effective than modeling only one type of relation.

## Foundational Learning

- **Relational Transformer (RT)**: Base architecture for MARTE to model pairwise relations with edge features. Quick check: How does RT incorporate edge vectors into the attention mechanism?
- **Hypergraph structures**: Allow modeling of group-wise relations beyond pairwise edges. Quick check: What is the difference between a graph edge and a hyperedge in the context of trajectory prediction?
- **Straight-Through Estimation (STE)**: Enables gradient flow through non-differentiable thresholding in AGE. Quick check: Why is STE used in AGE instead of a smooth approximation?

## Architecture Onboarding

- **Component map**: Feature Initialization → AGE → MARTE (PRT + HRT) → Decoder → Loss
- **Critical path**: Input trajectories → Node/edge/hyperedge embeddings → AGE (group incidence) → MARTE encoding → Multi-head decoder → Loss computation
- **Design tradeoffs**: Using overlapping groups increases representational power but adds complexity in AGE; concatenating PRT and HRT outputs increases parameter count but provides richer interaction modeling
- **Failure signatures**: Poor ADE/FDE indicates issues in feature initialization or AGE; overfitting suggests too many parameters or insufficient regularization; slow convergence may indicate learning rate or threshold adaptation problems
- **First 3 experiments**:
  1. Remove AGE and test with a fixed group incidence matrix; compare ADE/FDE
  2. Replace HRT with an additional PRT layer; measure group-wise interaction performance
  3. Vary the learnable threshold Θ in AGE and observe group incidence stability and prediction accuracy

## Open Questions the Paper Calls Out

- **Open Question 1**: How would MART perform on datasets with explicit group relationships defined (like sports datasets with team assignments)? The paper focuses on inferring group relationships in real-world scenarios where they are not explicitly defined, but doesn't test performance when groups are known. Experiments comparing MART's inferred groups versus performance using ground-truth group labels on the same datasets would resolve this.

- **Open Question 2**: How sensitive is MART's performance to the number of prediction heads (k) used during inference? The paper mentions using k predicted trajectories and selecting the best one, but doesn't systematically analyze how performance scales with different k values. Ablation studies showing performance metrics for different k values (e.g., 5, 10, 20, 50) and analysis of computational cost vs accuracy trade-offs would resolve this.

- **Open Question 3**: Can MART's AGE module be extended to handle group relationships that change over time (dynamic groups)? While AGE handles overlapping groups, the paper doesn't address temporal dynamics of group membership. Experiments showing MART's performance on datasets with temporal group dynamics, or modifications to AGE that incorporate temporal information about group membership, would resolve this.

## Limitations

- The effectiveness of hyperedge features depends heavily on whether they truly capture group dynamics or merely add noise, lacking empirical ablation studies comparing alternative group encoding methods
- AGE's adaptive thresholding approach using STE is novel but lacks sensitivity analysis on threshold initialization and convergence behavior across diverse datasets
- While the PRT+HRT concatenation design is well-motivated, the paper doesn't fully explore whether this is optimal versus other fusion strategies or provide detailed complexity analysis

## Confidence

- **High confidence**: Dataset results (ADE/FDE improvements are clearly demonstrated), architectural overview (components and flow are well-specified)
- **Medium confidence**: Mechanisms 1-3 (theoretical soundness is good, but empirical validation has gaps), computational efficiency claims (requires more detailed analysis)
- **Low confidence**: Optimal hyperparameters for AGE threshold initialization, alternative fusion strategies for PRT/HRT outputs

## Next Checks

1. **Ablation study on hyperedge feature quality**: Remove hyperedge features from MART and replace with alternative group encoding (e.g., mean pooling of node features). Compare whether the 11.8% FDE improvement on NBA dataset persists.

2. **Threshold sensitivity analysis**: Systematically vary the initial threshold value Θ in AGE across multiple orders of magnitude. Measure group incidence matrix stability and track corresponding changes in prediction accuracy to validate adaptive thresholding effectiveness.

3. **Alternative MARTE fusion strategies**: Replace the concatenation of PRT and HRT outputs with weighted summation or attention-based fusion. Compare performance to determine if concatenation is indeed optimal or merely one viable option.