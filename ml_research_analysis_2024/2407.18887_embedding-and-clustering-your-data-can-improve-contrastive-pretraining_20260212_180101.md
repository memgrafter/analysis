---
ver: rpa2
title: Embedding And Clustering Your Data Can Improve Contrastive Pretraining
arxiv_id: '2407.18887'
source_url: https://arxiv.org/abs/2407.18887
tags:
- clustering
- cluster
- embedding
- training
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes clustering training examples by semantic similarity
  using pretrained embeddings to create more granular data stratification for contrastive
  pretraining. By splitting the data into clusters of semantically similar query-item
  pairs and training on one cluster at a time, the model learns from harder negative
  examples within the same topic, avoiding uninformative negatives from different
  topics.
---

# Embedding And Clustering Your Data Can Improve Contrastive Pretraining

## Quick Facts
- arXiv ID: 2407.18887
- Source URL: https://arxiv.org/abs/2407.18887
- Reference count: 9
- Primary result: Clustering semantically similar query-item pairs during contrastive pretraining improves NDCG@10 by 2% (passage clustering) and 1.89% (query clustering) on MSMARCO

## Executive Summary
This paper proposes clustering training examples by semantic similarity using pretrained embeddings to create more granular data stratification for contrastive pretraining. By splitting the data into clusters of semantically similar query-item pairs and training on one cluster at a time, the model learns from harder negative examples within the same topic, avoiding uninformative negatives from different topics. Experiments on the MSMARCO dataset show a 2% improvement in NDCG@10 when clustering by passage embeddings and a 1.89% improvement when clustering by query embeddings compared to un-clustered training.

## Method Summary
The method involves three main steps: (1) embedding queries or passages using a pretrained model (Arctic Embed M), (2) applying k-means clustering (k=10) to create semantic clusters, and (3) stratifying training data by clusters instead of mixing all data during contrastive pretraining. The pretraining uses BERT with InfoNCE loss, batch size 4096, 3 epochs, and temperature 0.02. The approach connects conceptually to Topic Aware Sampling and ANCE hard-negative mining methodologies.

## Key Results
- 2% improvement in NDCG@10 on MSMARCO dev set when clustering by passage embeddings
- 1.89% improvement in NDCG@10 when clustering by query embeddings
- Similar improvements observed on MTEB Retrieval benchmark
- Clustering creates minibatches with harder negative examples within the same semantic topic

## Why This Works (Mechanism)

### Mechanism 1
- Clustering creates minibatches with harder negative examples within the same semantic topic, improving training efficiency.
- When query-item pairs are grouped by semantic similarity, negatives from the same cluster are more likely to be topically related to the positive item, avoiding "easy" negatives from unrelated topics.
- Core assumption: Embedding space becomes geometrically aligned with semantic topics so that same-cluster items are harder negatives.
- Evidence: Abstract states "by splitting the data into clusters of semantically similar query-item pairs and training on one cluster at a time, the model learns from harder negative examples within the same topic."

### Mechanism 2
- Stratifying by cluster prevents uninformative negative examples from different topics, reducing gradient noise.
- Random sampling from the full dataset can include trivially dissimilar negatives from different topics. By sampling only within a cluster, the model focuses on more informative comparisons.
- Core assumption: The dataset contains natural topical groupings where negatives from different topics are uninformative.
- Evidence: Abstract mentions "avoiding uninformative negatives from different topics" and connects to the cluster hypothesis.

### Mechanism 3
- Clustering approximates hard-negative mining without the computational cost of embedding negatives per query.
- Instead of dynamically mining negatives per query, clustering pre-groups semantically similar items so that random negatives within a cluster are already harder than random negatives from the full dataset.
- Core assumption: Clustering by embeddings produces groups where intra-cluster negatives are meaningfully harder than random negatives.
- Evidence: The paper connects clustering to "nearest-neighbor-based hard negative mining aspect of the ANCE methodology."

## Foundational Learning

- Concept: InfoNCE contrastive loss
  - Why needed here: The paper uses InfoNCE loss for contrastive pretraining; understanding its structure explains why hard negatives matter.
  - Quick check question: In InfoNCE, what happens to the contribution of negatives that are much less similar to the query than the positive?

- Concept: k-means clustering on embeddings
  - Why needed here: The method clusters data by embedding similarity; knowing how k-means works clarifies how clusters form.
  - Quick check question: If you cluster by query embeddings vs. passage embeddings, how might the resulting clusters differ in coherence?

- Concept: Triangle inequality in embedding space
  - Why needed here: The paper argues that same-topic items cluster tightly, making some negatives harder via geometric constraints.
  - Quick check question: If two items are very similar in embedding space, what does the triangle inequality say about the similarity of a third item to each of them?

## Architecture Onboarding

- Component map: Pretrained embedding model (Arctic Embed M) → Embeddings of queries/passages → k-means clustering (FAISS spherical) → Cluster assignments → Data splitting → Cluster-specific datasets → Contrastive pretraining pipeline (BERT, InfoNCE) → Trained model

- Critical path:
  1. Embed queries/passages
  2. Cluster embeddings (k-means)
  3. Split data into cluster datasets
  4. Train BERT model using cluster-stratified minibatches

- Design tradeoffs:
  - k value: Too small → clusters too broad, too large → too few examples per cluster
  - Clustering by queries vs. passages: Different cluster coherence; passage clustering may group by topic, query clustering by intent
  - Cluster size vs. batch size: Must balance to ensure enough negatives per minibatch

- Failure signatures:
  - Training loss plateaus early → clusters too broad or lack semantic coherence
  - No NDCG improvement → clustering not capturing useful structure or clusters too large
  - High variance in training → some clusters too small or poorly formed

- First 3 experiments:
  1. Compare NDCG@10 with k=5 vs. k=20 clustering to find optimal granularity
  2. Test clustering by concatenated query+passage embeddings vs. single field
  3. Evaluate impact of cluster size thresholds (e.g., drop clusters < batch size)

## Open Questions the Paper Calls Out

- How does the performance of clustering-based contrastive pretraining scale with dataset size and number of clusters?
  - The paper suggests potential for greater benefits with more clusters and larger datasets but only experiments with k=10 on MSMARCO.

- What is the optimal curriculum for when to apply clustering-based stratification during pretraining?
  - The paper raises whether clustering should be applied continuously, introduced at a specific point, or adjusted over time.

- How does combining query and passage embeddings for clustering compare to using either alone?
  - The paper mentions combining embeddings might yield better results but only tests clustering by query embeddings and passage embeddings separately.

## Limitations
- Claims about cluster-based stratification rely heavily on the assumption that embedding space geometry aligns with semantic topics.
- Comparison to hard-negative mining is conceptual rather than empirical - doesn't measure whether intra-cluster negatives are actually harder.
- Computational overhead of clustering is not fully characterized, and optimal k value appears dataset-dependent.

## Confidence
- **High confidence**: NDCG@10 improvements on MSMARCO (2% for passage clustering, 1.89% for query clustering) are directly measured and reproducible.
- **Medium confidence**: Theoretical connection to Topic Aware Sampling and ANCE hard-negative mining is conceptually sound but not empirically validated.
- **Medium confidence**: Mechanism that clustering creates harder negatives is plausible but actual hardness distribution within clusters is not measured.
- **Low confidence**: Claim that approach generalizes well to other datasets or embedding models is not supported by experiments beyond MSMARCO.

## Next Checks
1. Measure and compare the average cosine similarity between positives and negatives in clustered vs. un-clustered training to quantify whether clustering actually produces harder negatives.

2. Systematically vary k (e.g., k=5, 10, 20, 50) and measure the impact on NDCG@10 and training dynamics to identify optimal granularity for different batch sizes.

3. Apply the same clustering + pretraining pipeline to another passage retrieval dataset (e.g., Natural Questions or TriviaQA) to test whether benefits generalize beyond MSMARCO's topical structure.