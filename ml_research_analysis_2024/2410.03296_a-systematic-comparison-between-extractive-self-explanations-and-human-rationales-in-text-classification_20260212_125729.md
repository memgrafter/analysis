---
ver: rpa2
title: A Systematic Comparison between Extractive Self-Explanations and Human Rationales
  in Text Classification
arxiv_id: '2410.03296'
source_url: https://arxiv.org/abs/2410.03296
tags:
- human
- rationales
- tokens
- explanations
- rafola
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically compares extractive self-explanations
  and human rationales in text classification across three tasks: sentiment classification,
  forced labor detection, and claim verification. The study evaluates four open-weight
  language models (Gemma3, Llama3, Qwen3, Mistral) on multilingual sentiment data,
  a multi-class forced labor detection task, and a claim verification dataset with
  collected human rationale annotations.'
---

# A Systematic Comparison between Extractive Self-Explanations and Human Rationales in Text Classification

## Quick Facts
- arXiv ID: 2410.03296
- Source URL: https://arxiv.org/abs/2410.03296
- Reference count: 35
- Primary result: Self-explanations yield faithful token-level rationales with steep probability drops when key tokens are masked, whereas post-hoc attribution methods emphasize structural tokens

## Executive Summary
This paper systematically compares extractive self-explanations from language models with human rationales across three text classification tasks: sentiment classification, forced labor detection, and claim verification. The study evaluates four open-weight models (Gemma3, Llama3, Qwen3, Mistral) on multilingual sentiment data, a multi-class forced labor detection task, and a claim verification dataset with newly collected human rationale annotations. The research finds that self-explanations produce faithful rationales that significantly impact model predictions when masked, while post-hoc attribution methods tend to emphasize structural tokens rather than task-relevant content. The study also reveals distinct explanation strategies between humans and models, with humans focusing on narrative content and models providing factual detail.

## Method Summary
The study conducts zero-shot experiments using four instruction-tuned language models on three text classification tasks: SST2 sentiment classification (English, Danish, Italian), RaFoLa forced labor detection, and Climate-Fever claim verification. Plausibility is measured via Cohen's Kappa agreement between model-generated and human rationales, while faithfulness is assessed through token masking interventions comparing model self-explanations, human rationales, and post-hoc attribution methods (LRP and Gradient×Input). The research evaluates both the effectiveness of self-explanations versus post-hoc methods and the differences in explanation strategies between humans and models.

## Key Results
- Self-explanations yield faithful token-level rationales with steep probability drops when key tokens are masked
- Post-hoc attribution methods emphasize structural/formatting tokens rather than task-relevant content
- Human rationales focus on narrative/context while model rationales provide factual detail, leading to different faithfulness patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-explanations yield faithful token-level rationales that sharply reduce model probability when key tokens are masked.
- **Mechanism:** When models generate rationales grounded in the input text, the selected tokens carry direct causal weight for the prediction. Masking these tokens causes a steep drop in the probability difference between the correct and alternative class, indicating that the rationale is faithful to the model's decision process.
- **Core assumption:** The rationale tokens selected by the model are those most influential to the prediction, and masking them will disrupt the prediction more than masking random tokens.
- **Evidence anchors:**
  - [abstract] "self-explanations yield faithful subsets of token-level rationales, whereas post-hoc attribution methods tend to emphasize structural and formatting tokens"
  - [section 4.1] "Model probability difference after masking tokens extracted from human rationales, model self-explanation rationales and post-hoc attributions (LRP, GxI) for Mistral and Llama3"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.488, average citations=0.0.
- **Break condition:** If the rationale tokens are not actually influential to the prediction, masking them will not cause a significant drop in probability difference.

### Mechanism 2
- **Claim:** Post-hoc attribution methods emphasize structural/formatting tokens rather than task-relevant content, reflecting different explanation strategies.
- **Mechanism:** Attribution methods like LRP and Gradient×Input assign relevance scores across the entire input, including system and task prompts. This leads to highlighting structural tokens (e.g., "be", "<s>", "<bos>") that are required for the model's internal processing, rather than evidence-focused tokens that humans would select.
- **Core assumption:** The attribution methods' scoring mechanism inherently favors tokens critical for sequence processing, not just task-specific evidence.
- **Evidence anchors:**
  - [abstract] "post-hoc attribution methods tend to emphasize structural and formatting tokens, reflecting fundamentally different explanation strategies"
  - [section 4.1] "The most attributed tokens are often not task-specific content...but structural tokens from the system prompt"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.488, average citations=0.0.
- **Break condition:** If the attribution method is modified to down-weight structural tokens or if the task prompt is removed, the emphasis on structural tokens may decrease.

### Mechanism 3
- **Claim:** Human rationales focus on narrative/content while model rationales provide factual detail, leading to different faithfulness patterns.
- **Mechanism:** Humans select tokens that convey lived experiences and broader significance through story-like language, while models provide denser, technical phrasing to document mechanisms and procedures. This difference in focus leads to variations in how faithfully the rationales reflect the model's decision process.
- **Core assumption:** The difference in rationale selection strategies between humans and models is due to their distinct approaches to understanding and explaining the task.
- **Evidence anchors:**
  - [section 4.2] "human rationales contain more tokens that convey narrative content, emphasizing lived experiences and the broader significance of exploitation through story-like language"
  - [section 4.2] "Model-generated rationales provide more factual and analytical detail, using denser, technical phrasing to document mechanisms and procedures"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.488, average citations=0.0.
- **Break condition:** If the task is modified to emphasize factual detail over narrative content, the difference in rationale selection strategies may decrease.

## Foundational Learning

- **Concept: Token-level rationale extraction**
  - Why needed here: The study evaluates the plausibility and faithfulness of token-level rationales, requiring a clear understanding of how to extract and analyze these rationales.
  - Quick check question: What is the difference between extractive and abstractive explanations, and why is extractive explanation used in this study?

- **Concept: Faithfulness evaluation via token masking**
  - Why needed here: The study assesses faithfulness by measuring the probability difference after masking tokens, requiring an understanding of how this intervention-based analysis works.
  - Quick check question: How does masking influential tokens help evaluate the faithfulness of a rationale to the model's decision process?

- **Concept: Plausibility evaluation via agreement with human rationales**
  - Why needed here: The study measures plausibility by computing agreement scores between human and model rationales, requiring an understanding of how to quantify this agreement.
  - Quick check question: What metric is used to measure agreement between human and model rationales, and why is it chosen over other metrics like F1 score?

## Architecture Onboarding

- **Component map:** Data processing -> Model experimentation -> Rationale extraction -> Evaluation -> Analysis
- **Critical path:**
  1. Load datasets and preprocess text
  2. Run zero-shot experiments with models
  3. Extract human, model, and post-hoc rationales
  4. Compute plausibility and faithfulness scores
  5. Analyze and compare rationale strategies
- **Design tradeoffs:**
  - Zero-shot vs. fine-tuned models: Zero-shot experiments allow for a fair comparison across models but may limit performance on complex tasks.
  - Extractive vs. abstractive explanations: Extractive explanations are easier to evaluate against human annotations but may miss nuanced explanations.
  - Token masking vs. other faithfulness metrics: Token masking provides a direct intervention-based analysis but may not capture all aspects of faithfulness.
- **Failure signatures:**
  - Low plausibility scores: Indicates poor agreement between human and model rationales, possibly due to differences in explanation strategies or task complexity.
  - Inconsistent faithfulness patterns: May indicate issues with token masking intervention or differences in how models process and explain the input.
  - High variance in scores: Could be due to dataset characteristics, model performance, or annotation quality.
- **First 3 experiments:**
  1. Run zero-shot experiments on SST dataset with all four models and extract rationales.
  2. Compute plausibility scores (Cohen's Kappa) between human and model rationales for SST.
  3. Evaluate faithfulness by masking tokens from human, model, and post-hoc rationales and measuring probability difference.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but identifies several limitations that point to future research directions, particularly regarding the generalizability of findings across different tasks and models, and the need for more comprehensive evaluation frameworks.

## Limitations
- The faithfulness evaluation through token masking assumes probability drops directly indicate rationale quality, which may not capture all aspects of explanation utility
- The comparison between human and model rationales conflates explanation strategy differences with potential quality differences
- Post-hoc attribution methods' emphasis on structural tokens may be influenced by implementation details not fully specified

## Confidence
- **High confidence:** Self-explanations produce faithful token-level rationales with significant probability drops when key tokens are masked
- **Medium confidence:** Characterization of different explanation strategies between humans (narrative/context-focused) and models (factual/detail-focused)
- **Medium confidence:** Plausibility results showing varying agreement levels across tasks

## Next Checks
1. Implement the token masking intervention protocol with the exact greedy selection and fraction flipping parameters specified in the paper to verify the faithfulness patterns across all three datasets and four models.
2. Replicate the plausibility analysis using Cohen's Kappa agreement between human and model rationales, ensuring proper handling of the multi-class RaFoLa conversion to binary classification as described in the methodology.
3. Examine the post-hoc attribution implementation details for LRP and Gradient×Input across different model architectures to verify that structural token emphasis is not an artifact of implementation variations.