---
ver: rpa2
title: 'PUZZLES: A Benchmark for Neural Algorithmic Reasoning'
arxiv_id: '2407.00401'
source_url: https://arxiv.org/abs/2407.00401
tags:
- puzzles
- puzzle
- grid
- learning
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PUZZLES is a benchmark of 40 logic puzzles designed to evaluate\
  \ reinforcement learning agents\u2019 algorithmic reasoning and generalization capabilities.\
  \ The benchmark allows adjustable puzzle size and difficulty, supports both visual\
  \ and discrete internal state observations, and includes action masking to improve\
  \ training efficiency."
---

# PUZZLES: A Benchmark for Neural Algorithmic Reasoning

## Quick Facts
- arXiv ID: 2407.00401
- Source URL: https://arxiv.org/abs/2407.00401
- Reference count: 40
- Primary result: RL agents can solve logic puzzles using algorithmic reasoning, with DreamerV3 achieving 62.7% success rate on 40 puzzles

## Executive Summary
PUZZLES is a benchmark of 40 logic puzzles from Simon Tatham's Portable Puzzle Collection designed to evaluate reinforcement learning agents' algorithmic reasoning and generalization capabilities. The benchmark supports adjustable puzzle size and difficulty, offers both visual and discrete internal state observations, and includes action masking to improve training efficiency. Experiments show that DreamerV3 achieves the best performance, solving 62.7% of puzzles within optimal step bounds, while action masking and discrete internal state observations significantly improve results.

## Method Summary
The benchmark uses Gymnasium environments with a C backend for puzzle logic, supporting both pixel and discrete internal state observations. Agents are trained using sparse rewards (only on completion) with maximum episode length of 10,000 steps and early termination after visiting the same state more than 10 times. Six RL algorithms (DreamerV3, PPO, TRPO, A2C, RecurrentPPO, DQN, QRDQN) are evaluated with 2M training steps and 5 random seeds each. Action masking identifies state-changing actions to improve exploration efficiency.

## Key Results
- DreamerV3 achieves highest success rate of 62.7% on puzzles within optimal step bounds
- Action masking and discrete internal state observations significantly improve performance
- Early episode termination based on state repetition helps in sparse-reward settings
- Generalization to larger puzzle sizes remains challenging but possible

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Action masking improves training efficiency by reducing exploration of futile actions.
- **Mechanism**: The environment provides a mask indicating which actions change the game state. By filtering out actions that do not alter the state, agents avoid wasting episodes on ineffective moves, increasing reward density and learning signal quality.
- **Core assumption**: The action mask correctly identifies all state-changing actions without omitting valid ones.
- **Evidence anchors**:
  - [abstract]: "Experiments with multiple RL algorithms show that DreamerV3 achieves the best performance, solving 62.7% of puzzles within optimal step bounds... Action masking and discrete internal state observations significantly improve performance..."
  - [section 3.3]: "As we can observe in Figure 4, action masking has a strongly positive effect on training performance. This benefit is observed both in the discrete internal game state observations and on the pixel observations."
  - [corpus]: No direct evidence; inferred from paper claims.
- **Break condition**: If the mask incorrectly excludes valid actions, the agent may be unable to solve certain puzzles.

### Mechanism 2
- **Claim**: Discrete internal state observations provide richer information than pixel observations for puzzle solving.
- **Mechanism**: Internal state includes game-specific data (e.g., grid positions, candidate values) that directly represents the puzzle logic. This reduces the agent's need to learn visual-to-logical mappings, enabling faster policy learning.
- **Core assumption**: The internal state representation is both complete and interpretable by the agent.
- **Evidence anchors**:
  - [abstract]: "Action masking and discrete internal state observations significantly improve performance..."
  - [section 3.3]: "Pixel observations allow for the exact same input representation to be used for all puzzles... On the other hand, it forces the agent to learn to solve the puzzles only based on the visual representation of the puzzles..."
  - [corpus]: No direct evidence; inferred from paper claims.
- **Break condition**: If the internal state is overly complex or not normalized, it may slow learning.

### Mechanism 3
- **Claim**: Early episode termination based on state repetition prevents wasted training steps on unsolvable trajectories.
- **Mechanism**: In puzzles without natural termination, agents may cycle through states indefinitely. By ending episodes after repeated states, the environment redirects learning toward productive exploration.
- **Core assumption**: State repetition reliably indicates unproductive exploration.
- **Evidence anchors**:
  - [abstract]: "early episode termination helps in sparse-reward settings."
  - [section 3.4]: "For computational reasons, we truncated all episodes during training and testing at 10,000 steps... We evaluate whether the cutoff episode length or early termination have an effect on training performance... increasing the maximum episode length during training from 10,000 to 100,000 does not improve performance. Only when episodes get terminated after visiting the exact same state more than 10 times, the agent is able to solve more puzzle instances on average..."
  - [corpus]: No direct evidence; inferred from paper claims.
- **Break condition**: If the repetition threshold is too low, valid long-horizon strategies may be prematurely terminated.

## Foundational Learning

- **Concept**: Discrete action spaces
  - **Why needed here**: All puzzles use keyboard-like input with a fixed set of actions, enabling precise state changes and simplifying the action selection problem.
  - **Quick check question**: Does your agent handle the full cardinality of each puzzle's action space without conflating unrelated actions?

- **Concept**: Sparse reward learning
  - **Why needed here**: Rewards are only given upon puzzle completion, requiring the agent to learn from delayed feedback and explore effectively without intermediate guidance.
  - **Quick check question**: How does your agent balance exploration and exploitation when positive rewards are rare?

- **Concept**: State space representation
  - **Why needed here**: Two observation modes (internal state vs. pixel) require different feature extractors; choosing the right one affects learning speed and generalization.
  - **Quick check question**: Have you validated that your observation preprocessing preserves all relevant game information?

## Architecture Onboarding

- **Component map**:
  PuzzleEnv wrapper -> C backend game logic -> Action mask generator -> Observation wrappers -> RL agent

- **Critical path**:
  1. Reset -> get initial observation, info with internal state
  2. Agent selects action
  3. Step -> apply action, return next observation, reward, terminated/truncated flags, and updated info
  4. Repeat until terminated/truncated

- **Design tradeoffs**:
  - Internal state vs. pixel observations: richer info vs. uniformity across puzzles
  - Action masking: improved learning efficiency vs. risk of excluding valid actions
  - Episode truncation vs. early termination: resource limits vs. preventing futile exploration

- **Failure signatures**:
  - Agent cycles through same states without progress -> consider lowering repetition threshold or adding intermediate rewards
  - No improvement after many episodes -> check action mask correctness and observation preprocessing
  - Overfitting to small puzzle sizes -> test generalization to larger sizes using variable-length observation wrappers

- **First 3 experiments**:
  1. Run a random policy on the smallest puzzle size to confirm solvability and measure baseline success rate
  2. Train a PPO agent with action masking on internal state observations and record episode lengths
  3. Compare performance with and without action masking on pixel observations to quantify masking benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can RL agents be designed to consistently generalize to larger puzzle sizes without significant performance degradation?
- Basis in paper: [inferred] The paper shows that while generalization is possible, it remains challenging, with performance varying substantially across different random seeds.
- Why unresolved: The paper suggests that identifying the appropriate inductive biases is necessary for consistent generalization, but does not provide a concrete solution or framework for achieving this.
- What evidence would resolve it: Experiments demonstrating RL agents that can reliably generalize to larger puzzle sizes with minimal performance loss, possibly through novel architectural designs or training techniques.

### Open Question 2
- Question: What is the impact of different intermediate reward structures on the training efficiency and final performance of RL agents in solving logic puzzles?
- Basis in paper: [explicit] The paper mentions that PUZZLES allows for custom reward structures and suggests that intermediate rewards could help improve training progress, but does not explore this in detail.
- Why unresolved: The paper only uses sparse rewards in its baseline experiments and acknowledges the potential benefit of intermediate rewards without testing them.
- What evidence would resolve it: Comparative experiments showing the effects of various intermediate reward structures on training speed and final performance across multiple puzzle types.

### Open Question 3
- Question: How can action masking be further improved to ensure adherence to game rules while still providing the benefits of enhanced exploration?
- Basis in paper: [explicit] The paper discusses the positive impact of action masking on performance but notes that it does not ensure adherence to game rules.
- Why unresolved: While the paper demonstrates the benefits of action masking, it does not address how to extend it to enforce rule compliance, which could further improve agent performance.
- What evidence would resolve it: Experiments with enhanced action masking techniques that incorporate game rule knowledge, showing improved performance and rule compliance compared to the current implementation.

## Limitations

- Generalization to larger puzzle sizes remains limited, with success rates dropping significantly beyond training distributions
- Sparse reward structure creates challenging learning conditions that may not reflect real-world algorithmic problem-solving scenarios
- The benchmark doesn't fully address the algorithmic reasoning challenge as agents may learn puzzle-specific strategies rather than general algorithmic thinking

## Confidence

- **High confidence**: Action masking benefits are well-supported by direct experimental comparison showing consistent performance gains
- **Medium confidence**: Discrete state superiority shows improvement but pixel observations allow for puzzle-agnostic feature learning
- **Low confidence**: Scalability claims are limited as generalization results don't fully demonstrate algorithmic reasoning capability

## Next Checks

1. Test whether intermediate rewards for valid intermediate states improve generalization to larger puzzle sizes while maintaining solution optimality
2. Evaluate whether curriculum learning (gradually increasing puzzle difficulty during training) improves final performance on the hardest puzzles
3. Compare the PUZZLES benchmark performance against human baseline times to establish whether the 62.7% success rate represents meaningful algorithmic reasoning capability