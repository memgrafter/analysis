---
ver: rpa2
title: 'Cocktail: A Comprehensive Information Retrieval Benchmark with LLM-Generated
  Documents Integration'
arxiv_id: '2405.16546'
source_url: https://arxiv.org/abs/2405.16546
tags:
- retrieval
- dataset
- datasets
- llm-generated
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cocktail introduces the first comprehensive benchmark for Information
  Retrieval in the Large Language Model era, featuring 16 datasets with mixed human-written
  and LLM-generated content. The benchmark addresses the challenge of evaluating retrieval
  models when both human and AI-generated content coexist in IR systems.
---

# Cocktail: A Comprehensive Information Retrieval Benchmark with LLM-Generated Documents Integration

## Quick Facts
- **arXiv ID**: 2405.16546
- **Source URL**: https://arxiv.org/abs/2405.16546
- **Reference count**: 40
- **Primary result**: Introduces Cocktail, the first comprehensive IR benchmark with mixed human-written and LLM-generated content, revealing neural retrievers exhibit source bias toward LLM-generated documents.

## Executive Summary
Cocktail introduces the first comprehensive benchmark for Information Retrieval in the Large Language Model era, featuring 16 datasets with mixed human-written and LLM-generated content. The benchmark addresses the challenge of evaluating retrieval models when both human and AI-generated content coexist in IR systems. Through over 1,000 experiments on state-of-the-art retrieval models, Cocktail reveals a clear trade-off between ranking performance and source bias, showing that neural retrievers consistently exhibit bias towards LLM-generated content, with stronger models showing more severe bias.

## Method Summary
The core method involves rewriting human-written documents using Llama2 to create semantically equivalent LLM-generated content, then mixing these sources while preserving original relevance labels. The benchmark construction process generates LLM-based documents by leveraging Llama2 to rewrite existing human-written documents, ensuring that the rewritten content maintains the same semantic information while potentially introducing distinct linguistic patterns. Over 1,000 experiments were conducted on state-of-the-art retrieval models to evaluate both ranking performance and source bias.

## Key Results
- Neural retrievers consistently exhibit bias towards LLM-generated content across all 16 benchmark datasets
- Stronger retrieval models show more severe source bias, revealing a performance-bias trade-off
- Neural re-rankers, despite superior ranking performance, are not exempt from pervasive source bias

## Why This Works (Mechanism)
The benchmark works by creating a controlled environment where human-written and LLM-generated documents with identical semantic content are mixed, allowing researchers to isolate and measure the source bias in retrieval models. By preserving original relevance labels while changing document generation methods, the framework enables systematic evaluation of how retrieval models respond to different document sources.

## Foundational Learning

**Document Rewriting with LLMs**: Understanding how LLMs transform human-written content into semantically equivalent but stylistically different versions - needed to comprehend how source bias emerges from linguistic differences rather than semantic content.

**Source Bias Measurement**: Learning how to quantify preference for one document source over another in retrieval models - needed to evaluate the trade-off between ranking performance and fairness.

**Mixed-Source Evaluation**: Understanding how to construct and evaluate benchmarks with multiple document sources - needed to assess real-world IR system performance where human and AI content coexist.

## Architecture Onboarding

**Component Map**: Human Documents -> Llama2 Rewriting -> LLM-Generated Documents -> Mixed Dataset Creation -> Retrieval Model Evaluation -> Source Bias Measurement

**Critical Path**: The most critical evaluation path involves: (1) document rewriting with Llama2, (2) mixing human and LLM-generated documents while preserving relevance labels, (3) running retrieval models on mixed datasets, and (4) measuring source bias against ranking performance.

**Design Tradeoffs**: The benchmark trades comprehensive coverage for practical feasibility by using only Llama2 for document generation due to inference costs, potentially limiting generalizability to other LLM architectures.

**Failure Signatures**: Excessive source bias manifests as retrieval models consistently preferring LLM-generated documents regardless of actual relevance, while poor ranking performance indicates the model fails to distinguish between relevant and irrelevant documents across both sources.

**First Experiments**:
1. Run baseline retrieval models (BM25, DPR) on mixed datasets to establish performance baselines
2. Measure source bias magnitude across different mixing ratios of human and LLM-generated documents
3. Compare bias patterns between neural retrievers and neural re-rankers on the same datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does source bias persist when using LLMs other than Llama2 for content generation in IR benchmarks?
- Basis in paper: The paper acknowledges that "the construction of LLM-generated corpus in our benchmarked 16 datasets was significantly influenced by the inference costs of LLM, leading us to rely exclusively on Llama2 for generating LLM-based content."
- Why unresolved: The study only used Llama2 for generating LLM-generated content, limiting generalizability to other LLMs like GPT-4 or Gemini.
- What evidence would resolve it: Experiments using the same benchmark construction method but with different LLMs (GPT-4, Claude, Gemini) to compare source bias patterns across different model architectures.

### Open Question 2
- Question: What is the relationship between source bias and specific linguistic features (e.g., sentence complexity, vocabulary diversity) that distinguish human vs. LLM-generated content?
- Basis in paper: The paper notes "minimal differences in average word length" but observes "significant differences in terms despite their ostensibly similar semantic information" and mentions that "larger models may be more sensitive to the nuanced distinctions between human-written and LLM-generated texts."
- Why unresolved: While the paper demonstrates source bias exists, it doesn't identify which linguistic features drive the bias or how these features interact with model architecture.
- What evidence would resolve it: Controlled experiments manipulating specific linguistic features (syntactic complexity, lexical diversity, coherence patterns) in human and LLM-generated texts while measuring changes in source bias across different model sizes.

### Open Question 3
- Question: Can retrieval models be effectively trained to eliminate source bias without sacrificing ranking performance?
- Basis in paper: The paper reveals "a clear trade-off between ranking performance and source bias in neural retrieval models" and suggests "attempts to boost model performance could unintentionally magnify source bias issues."
- Why unresolved: The paper identifies the trade-off but doesn't explore training methods or architectural modifications that might simultaneously optimize for both performance and bias reduction.
- What evidence would resolve it: Comparative experiments testing bias-mitigation training techniques (adversarial training, multi-task learning with bias detection, contrastive learning with balanced sources) against standard training while measuring both ranking performance and source bias.

## Limitations
- Benchmark construction relies exclusively on Llama2 for document generation, limiting generalizability to other LLM architectures
- The 16 datasets may not capture all real-world variations in document types and query distributions
- Causal mechanisms behind source bias remain incompletely explained

## Confidence

- **High confidence**: The benchmark construction methodology and experimental setup are clearly described and reproducible
- **Medium confidence**: The observed trade-off between ranking performance and source bias, as this depends on the specific experimental conditions and may vary with different model architectures
- **Medium confidence**: The claim that stronger models show more severe bias, as this relationship could be influenced by factors not fully controlled in the experiments

## Next Checks

1. Replicate the experiments using different LLM models (e.g., GPT-4, Claude) for document generation to assess whether the observed biases are model-specific or general phenomena
2. Conduct ablation studies varying the mixing ratio of human-written and LLM-generated documents to determine at what point source bias becomes most pronounced
3. Test whether fine-tuning retrieval models on mixed-sourced Cocktail data reduces source bias while maintaining ranking performance on traditional benchmarks