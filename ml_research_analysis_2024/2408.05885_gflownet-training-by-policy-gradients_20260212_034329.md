---
ver: rpa2
title: GFlowNet Training by Policy Gradients
arxiv_id: '2408.05885'
source_url: https://arxiv.org/abs/2408.05885
tags:
- training
- policy
- methods
- gflownet
- gradients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops a new training framework for Generative Flow
  Networks (GFlowNets) by connecting GFlowNet training to traditional reinforcement
  learning (RL) with policy-dependent rewards. The key insight is reformulating the
  flow-balance condition of GFlowNets as an RL problem with expected accumulated reward,
  enabling derivation of policy-based training strategies rather than existing value-based
  methods.
---

# GFlowNet Training by Policy Gradients

## Quick Facts
- arXiv ID: 2408.05885
- Source URL: https://arxiv.org/abs/2408.05885
- Reference count: 40
- Key outcome: This work develops a new training framework for Generative Flow Networks (GFlowNets) by connecting GFlowNet training to traditional reinforcement learning (RL) with policy-dependent rewards. The key insight is reformulating the flow-balance condition of GFlowNets as an RL problem with expected accumulated reward, enabling derivation of policy-based training strategies rather than existing value-based methods. The authors propose a coupled training strategy that jointly optimizes forward policy training and backward policy design, providing theoretical guarantees on convergence. Experimental results on hyper-grid modeling, biological/molecular sequence design, and Bayesian network structure learning demonstrate that policy-based methods achieve faster convergence and better performance than value-based alternatives, particularly in complex combinatorial spaces. The approach offers improved gradient estimation and provides an advanced RL perspective for robust GFlowNet training.

## Executive Summary
This paper presents a novel approach to training Generative Flow Networks (GFlowNets) by reformulating the flow-balance condition as a policy-dependent reinforcement learning problem. Instead of traditional value-based methods, the authors derive policy-based optimization strategies that enable more efficient gradient estimation and convergence. The key innovation is a coupled training framework that jointly optimizes forward and backward policies, providing theoretical convergence guarantees while achieving superior performance across diverse domains including combinatorial optimization, molecular design, and structure learning.

## Method Summary
The authors reformulate GFlowNet training as an RL problem with policy-dependent rewards, where the flow-balance condition corresponds to maximizing expected accumulated reward. They propose a coupled training strategy that jointly optimizes a forward policy (for trajectory generation) and a backward policy (for return estimation). The forward policy learns to generate trajectories that satisfy the flow constraints, while the backward policy estimates the accumulated reward for each state-action pair. This dual-policy approach enables more stable and efficient training compared to existing value-based methods. The authors provide theoretical analysis showing convergence guarantees under reasonable assumptions and demonstrate improved gradient estimation quality through the policy-based formulation.

## Key Results
- Policy-based GFlowNet training achieves faster convergence than value-based alternatives across all tested domains
- The coupled training approach with forward and backward policies provides more stable optimization and better performance on complex combinatorial spaces
- Experimental results on hyper-grid modeling, molecular sequence design, and Bayesian network structure learning show consistent improvements over existing methods
- The policy-based formulation enables more accurate gradient estimation, particularly in high-dimensional and sparse-reward scenarios

## Why This Works (Mechanism)
The core mechanism works by transforming the GFlowNet flow-balance condition into an RL objective with policy-dependent rewards. This reformulation allows the use of policy gradient methods, which provide better gradient estimates than value-based approaches, especially in sparse-reward or high-dimensional settings. The coupled training of forward and backward policies creates a self-reinforcing system where the forward policy generates high-quality trajectories and the backward policy provides accurate return estimates. This dual structure addresses the credit assignment problem inherent in GFlowNet training while maintaining the theoretical guarantees of the original flow-based formulation.

## Foundational Learning
- **Generative Flow Networks (GFlowNets)**: A framework for learning stochastic policies that generate diverse samples with probability proportional to a reward function. Why needed: Forms the foundational model being trained. Quick check: Understand the flow-balance condition and its role in ensuring proper sampling distribution.

- **Flow-balance Condition**: The constraint that flow into each state equals flow out, ensuring proper probability distribution over generated samples. Why needed: The target condition that policy-based methods aim to satisfy through RL optimization. Quick check: Verify how the flow-balance translates to expected accumulated reward in the RL formulation.

- **Policy Gradient Methods**: RL techniques that directly optimize the policy parameters using gradient ascent on expected return. Why needed: Provides the mathematical foundation for deriving the new training algorithm. Quick check: Understand the difference between policy-based and value-based RL approaches.

- **Coupled Policy Training**: Joint optimization of forward and backward policies to create a self-reinforcing training system. Why needed: The key architectural innovation enabling stable and efficient GFlowNet training. Quick check: Trace how the forward and backward policies interact during training.

- **Expected Accumulated Reward**: The RL objective that replaces the flow-balance condition, representing the total reward expected from following a policy. Why needed: Provides the optimization target for policy-based training. Quick check: Verify the mathematical equivalence between maximizing this objective and satisfying flow-balance.

- **Credit Assignment in GFlowNets**: The challenge of properly attributing rewards to different actions in the generation process. Why needed: Explains why policy-based methods can outperform value-based alternatives. Quick check: Understand how the backward policy helps with credit assignment.

## Architecture Onboarding

**Component Map**: Forward Policy -> Trajectory Generator -> Reward Collector -> Backward Policy -> Gradient Estimator -> Parameter Updater -> Forward Policy

**Critical Path**: Forward policy generates trajectories → rewards are collected → backward policy estimates returns → gradients are computed → forward policy parameters are updated → repeat

**Design Tradeoffs**: The coupled training approach requires maintaining and updating two separate policies, increasing computational overhead but providing more stable and accurate training. Policy-based methods offer better gradient estimation but may require more samples compared to value-based alternatives.

**Failure Signatures**: Poor convergence occurs when the backward policy fails to accurately estimate returns, leading to incorrect gradient signals. Mode collapse happens when the forward policy becomes too deterministic, reducing diversity in generated samples.

**First Experiments**: 1) Train on a simple 2D grid world to verify basic functionality and convergence behavior, 2) Compare policy-based vs value-based training on a small molecular design task to measure performance differences, 3) Test robustness to reward noise by adding uncertainty to the reward function in the Bayesian network structure learning task.

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes access to accurate reward functions, which may be challenging in real-world applications with sparse or noisy rewards
- Computational overhead of maintaining and training both forward and backward policies could be significant in high-dimensional spaces
- Experimental validation focuses on relatively controlled environments - performance in truly open-ended, noisy real-world scenarios remains untested

## Confidence

**Theoretical Foundation**: High - The mathematical connection between flow-balance and policy-dependent RL is rigorous and well-established

**Experimental Validation**: Medium - Results are comprehensive across three domains but may not generalize to more complex, real-world scenarios

**Scalability Analysis**: Low - The work does not extensively test performance on very large-scale problems or assess computational efficiency at scale

## Next Checks

1. Test scalability and performance on larger combinatorial spaces (e.g., >100-dimensional problems) to assess computational efficiency and gradient estimation quality

2. Evaluate robustness to noisy or sparse reward signals by introducing reward uncertainty in the experimental domains

3. Compare against state-of-the-art RL baselines (e.g., PPO, SAC) that also use policy gradients to isolate the benefits specific to the GFlowNet formulation