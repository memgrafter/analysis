---
ver: rpa2
title: 'CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations'
arxiv_id: '2405.10212'
source_url: https://arxiv.org/abs/2405.10212
tags:
- psychology
- questions
- psychological
- knowledge
- cpsyexam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CPsyExam, a Chinese psychology benchmark designed
  to comprehensively evaluate large language models' understanding of psychological
  knowledge and case analysis skills. The benchmark includes over 4,000 questions
  sourced from 39 psychology-related subjects across four Chinese examination systems.
---

# CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations

## Quick Facts
- **arXiv ID:** 2405.10212
- **Source URL:** https://arxiv.org/abs/2405.10212
- **Reference count:** 40
- **Primary result:** Introduces CPsyExam, a Chinese psychology benchmark evaluating LLMs' psychological knowledge and case analysis skills across 4,000 questions from 39 subjects and four examination systems.

## Executive Summary
CPsyExam is a Chinese psychology benchmark designed to comprehensively evaluate large language models' understanding of psychological knowledge and case analysis skills. The benchmark includes over 4,000 questions sourced from 39 psychology-related subjects across four Chinese examination systems. It is divided into two parts: Knowledge (KG) for assessing theoretical understanding, and Case Analysis (CA) for evaluating practical application skills. Experiments with multiple models, including open-source, psychology-oriented, and proprietary LLMs, reveal that while models like GPT-4 perform well in knowledge tasks, they struggle with case analysis. The benchmark serves as a valuable tool for advancing LLMs' psychological capabilities and provides insights into their strengths and limitations in this domain.

## Method Summary
The CPsyExam benchmark was constructed by collecting over 22,000 questions from 39 psychology-related subjects across four Chinese examination systems (Graduate Entrance Examination, Psychological Counselor Examination, Teacher Qualification Examination, and Adult Self-study Examination). The dataset was then filtered to 4,000 questions, organized into two main sections: Knowledge (KG) for theoretical understanding and Case Analysis (CA) for practical application. Questions were formatted as single-choice (SCQ), multiple-choice (MAQ), and open-ended (QA). The benchmark was evaluated using zero-shot and few-shot settings on various models, including open-source, psychology-oriented, and proprietary LLMs. A supervised fine-tuning pipeline was also applied to ChatGLM2-6B using the training split of CPsyExam.

## Key Results
- GPT-4 achieved high accuracy on knowledge tasks but struggled with case analysis
- Psychology-oriented models showed marginal gains or no improvement over foundation models in understanding psychological knowledge
- Model size does not necessarily correlate with improved performance on CPsyExam tasks
- Few-shot learning advantages became more noticeable as model size increased

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark effectively evaluates LLMs by testing both psychological knowledge and case analysis separately.
- Mechanism: The dataset is structured into two distinct tasks - Knowledge (KG) for theoretical understanding and Case Analysis (CA) for practical application skills. This separation allows targeted assessment of different cognitive capabilities required in psychology.
- Core assumption: Psychological competence requires both theoretical knowledge and practical application abilities, which can be effectively measured through distinct evaluation tasks.
- Evidence anchors:
  - [abstract] "CPsyExam is designed to prioritize psychological knowledge and case analysis separately, recognizing the significance of applying psychological knowledge to real-world scenarios"
  - [section] "Our benchmark is structured to mirror real-world exams, which emphasize both psychological knowledge and the application of that knowledge"
  - [corpus] Weak evidence - no corpus support for this mechanism
- Break condition: If LLMs demonstrate high performance in one task but poor performance in the other, suggesting the separation is artificial rather than reflective of genuine psychological competence.

### Mechanism 2
- Claim: The benchmark's comprehensive coverage of Chinese examination systems provides robust evaluation across the entire field of psychology.
- Mechanism: By collecting questions from 39 psychology-related subjects across four examination systems (GEE, PCE, TQE, SSE), the benchmark captures the full spectrum of psychological knowledge and applications in the Chinese context.
- Core assumption: Psychology education and assessment in China is distributed across multiple examination systems, each testing different aspects of psychological competence.
- Evidence anchors:
  - [abstract] "We collect over 22k questions from 39 psychology-related subjects across four Chinese examination systems"
  - [section] "These examination systems collectively contribute to a well-rounded understanding and application of psychology across academic, counseling, educational, and professional domains in China"
  - [corpus] Weak evidence - no corpus support for this mechanism
- Break condition: If the questions from different examination systems show significant overlap or redundancy, suggesting the comprehensive coverage is superficial rather than substantive.

### Mechanism 3
- Claim: The use of multiple question formats (SCQ, MAQ, QA) provides a more nuanced assessment of LLM capabilities.
- Mechanism: Different question formats test different aspects of LLM performance - SCQ for clear assessment outcomes, MAQ for preventing elimination strategies, and QA for language organization abilities.
- Core assumption: LLMs may excel in certain question formats while struggling in others, and a comprehensive assessment requires testing across all formats.
- Evidence anchors:
  - [abstract] "The dataset includes various question formats, such as single-choice, multiple-choice, and open-ended questions, to ensure a robust assessment"
  - [section] "Multiple-choice questions provide clear and visual assessment outcomes, while question-answering questions evaluate the LLM's language organization abilities"
  - [corpus] Weak evidence - no corpus support for this mechanism
- Break condition: If LLM performance across question formats shows no meaningful differentiation, suggesting the format variation does not add assessment value.

## Foundational Learning

- Concept: Psychological case analysis methodology
  - Why needed here: Understanding how case analysis is structured and evaluated in psychology is crucial for interpreting the benchmark's CA component
  - Quick check question: What are the three categories of case analysis in the benchmark and what does each assess?

- Concept: Chinese psychology examination systems
  - Why needed here: The benchmark draws from multiple Chinese examination systems, each with distinct purposes and content
  - Quick check question: What are the four Chinese examination systems included and what type of psychology does each primarily test?

- Concept: LLM evaluation methodologies
  - Why needed here: Understanding how LLMs are typically evaluated helps in interpreting the benchmark results and their implications
  - Quick check question: What is the difference between zero-shot and few-shot evaluation settings?

## Architecture Onboarding

- Component map: Data Collection Pipeline (Crawling + OCR) -> Data Preprocessing (Deduplication, Formatting, Shuffling) -> Taxonomy Creation (KG vs CA, SCQ vs MAQ vs QA) -> Dataset Splitting (Train/Dev/Test/Reserved) -> Evaluation Framework (Prompt Engineering, Scoring) -> SFT Training Pipeline

- Critical path: Data Collection → Preprocessing → Taxonomy Creation → Dataset Splitting → Evaluation Framework → SFT Training → Benchmarking

- Design tradeoffs: Comprehensive coverage vs. dataset size (22k questions vs. 4k benchmark questions), balanced representation vs. subject depth, multiple formats vs. evaluation complexity

- Failure signatures:
  - Poor LLM performance across all question types suggests either fundamental LLM limitations or benchmark difficulty issues
  - Disproportionate performance differences between KG and CA suggest LLM strengths/weaknesses in theoretical vs. applied knowledge
  - Significant performance gaps between examination systems suggest coverage or representation issues

- First 3 experiments:
  1. Test baseline LLM performance on KG vs CA to establish relative strengths
  2. Compare LLM performance across different question formats to identify format-specific capabilities
  3. Evaluate impact of different prompt configurations (expert vs student vs ordinary person) on LLM performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific performance differences between psychology-oriented models and base models on case analysis tasks, and why do psychology-oriented models sometimes underperform?
- Basis in paper: [explicit] The paper states that "compared to the foundation models, these fine-tuned models show marginal gains or no improvement in understanding psychological knowledge. In some cases, their ability to analyze cases may even be compromised."
- Why unresolved: The paper identifies this issue but does not provide a detailed analysis of why psychology-oriented models underperform on case analysis tasks.
- What evidence would resolve it: Comparative analysis of case analysis performance between psychology-oriented and base models, along with qualitative analysis of model responses to identify specific weaknesses.

### Open Question 2
- Question: How does the size of language models impact their performance on psychological knowledge and case analysis tasks in CPsyExam?
- Basis in paper: [explicit] The paper mentions that "increased model size does not necessarily ensure improved performance on the CPsyExam" and that "as model size increases, the advantages of few-shot learning become significantly more noticeable."
- Why unresolved: The paper does not provide a detailed analysis of the relationship between model size and performance on different types of tasks within CPsyExam.
- What evidence would resolve it: Detailed analysis of model performance across different size categories for both knowledge and case analysis tasks, controlling for other variables.

### Open Question 3
- Question: What is the impact of different prompting strategies (expert, student, ordinary person) on model performance, and how does this affect the validity of CPsyExam as a benchmark?
- Basis in paper: [explicit] The paper shows that different prompting strategies lead to varying performance levels, with the expert prompt performing best and the ordinary person prompt performing worst.
- Why unresolved: The paper does not explore the implications of these differences for the validity of CPsyExam as a benchmark or the potential for prompt engineering to improve model performance.
- What evidence would resolve it: Analysis of model performance across different prompting strategies for various task types, along with validation of CPsyExam's ability to differentiate between levels of psychological knowledge.

## Limitations

- Potential data leakage from publicly available psychological resources used in pre-training LLMs may inflate model performance scores
- Benchmark's focus on Chinese psychology examination systems may limit generalizability to other cultural contexts
- Relatively small benchmark size (4,000 questions) compared to source material (22,000 questions) raises questions about question selection criteria

## Confidence

**High Confidence:** The claim that CPsyExam effectively evaluates LLMs' psychological knowledge through its separation of theoretical knowledge (KG) and case analysis (CA) tasks.

**Medium Confidence:** The assertion that CPsyExam provides comprehensive coverage of Chinese psychology through its inclusion of questions from 39 subjects across four examination systems.

**Low Confidence:** The claim that the benchmark's use of multiple question formats (SCQ, MAQ, QA) provides nuanced assessment of LLM capabilities.

## Next Checks

1. **Data Leakage Assessment:** Conduct a systematic analysis comparing the CPsyExam questions against publicly available pre-training corpora used by the evaluated LLMs to quantify the potential impact of data leakage on benchmark results.

2. **Cross-Cultural Validation:** Test the benchmark's effectiveness on LLMs trained on non-Chinese psychological datasets to assess whether the performance patterns generalize beyond the Chinese psychological context.

3. **Format Performance Analysis:** Conduct controlled experiments varying only the question format while keeping content constant to determine whether observed performance differences across SCQ, MAQ, and QA formats reflect genuine capability differences or format-specific artifacts.