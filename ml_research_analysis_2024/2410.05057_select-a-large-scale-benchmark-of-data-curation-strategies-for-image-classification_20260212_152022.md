---
ver: rpa2
title: 'SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification'
arxiv_id: '2410.05057'
source_url: https://arxiv.org/abs/2410.05057
tags:
- data
- dataset
- curation
- images
- imagenet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SELECT, the first large-scale benchmark for
  comparing data curation strategies in image classification. The authors create ImageNet++,
  a dataset extending ImageNet-1K with 5 new training shifts of similar size, each
  assembled using a distinct curation strategy including synthetic generation, embedding-based
  search, and crowdsourced labeling.
---

# SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification

## Quick Facts
- arXiv ID: 2410.05057
- Source URL: https://arxiv.org/abs/2410.05057
- Reference count: 40
- Primary result: The first large-scale benchmark comparing data curation strategies, finding that embedding-based search methods show strong performance but expert-curated ImageNet-1K remains the gold standard.

## Executive Summary
This paper introduces SELECT, a comprehensive benchmark for evaluating data curation strategies in image classification. The authors create ImageNet++, extending ImageNet-1K with 5 new training shifts assembled using distinct curation methods including synthetic generation, embedding-based search, and crowdsourced labeling. Through extensive evaluation across 130+ models on multiple utility metrics including accuracy, robustness to distribution shifts, fine-tuning on diverse tasks, and self-supervised learning performance, the study reveals that while embedding-based search methods outperform synthetic approaches, expert human curation still provides unique value that automated methods struggle to replicate.

## Method Summary
The authors construct ImageNet++ by augmenting ImageNet-1K with 5 curated shifts, each using different data collection strategies: OpenImages (OI1000), LAION with image-to-image generation (LA1000 img2img), LAION with text-to-image generation (LA1000 txt2img), Stable Diffusion with image-to-image generation (SD1000 img2img), and Stable Diffusion with text-to-image generation (SD1000 txt2img). They train identical ResNet-50 models on each shift for 600 epochs using weighted loss functions to handle class imbalance. Models are evaluated across multiple utility metrics including base accuracy, out-of-distribution robustness (synthetic and natural distribution shifts), VTAB fine-tuning performance, and self-supervised learning guidance quality.

## Key Results
- Embedding-based search methods (LA1000 img2img) consistently outperform synthetic generation methods across most utility metrics
- The original ImageNet-1K curation strategy remains the gold standard overall, despite advances in automated curation techniques
- Class imbalance significantly degrades model performance, with long-tailed classes responsible for most performance decreases
- Image-to-image curation strategies outperform text-to-image approaches for synthetic data generation

## Why This Works (Mechanism)

### Mechanism 1
Embedding-based search methods outperform synthetic image generation for dataset curation despite not matching expert-curated data. Embedding search retrieves real images that are semantically close to ImageNet classes, preserving natural image distributions while synthetic methods introduce noise and artifacts that degrade model performance. Real images contain features and distributions that synthetic generation cannot replicate accurately. This is supported by findings that embedding-based search methods show strong performance on certain metrics and are the best reduced-cost curation methods, though corpus evidence is limited.

### Mechanism 2
Class imbalance significantly degrades model performance, particularly for underrepresented classes. When certain classes have very few samples, models cannot learn robust representations for those classes, leading to accuracy drops that outweigh benefits of larger overall dataset size. Models require sufficient samples per class to learn meaningful representations. Evidence shows that the presence of classes with very few samples drives performance declines, and long-tailed classes are responsible for most performance decrease. However, direct corpus evidence for this specific mechanism is lacking.

### Mechanism 3
Image-to-image curation strategies outperform text-to-image strategies for synthetic data generation. Conditioning image generation on existing images preserves more realistic features and distributions compared to text conditioning, which relies on semantic descriptions that may not capture all visual nuances. Text descriptions cannot fully capture the visual complexity needed for realistic image generation. This is supported by findings that img2img strategies outperform txt2img strategies according to most metrics, though direct corpus evidence is lacking.

## Foundational Learning

- Concept: Data curation as utility maximization
  - Why needed here: Understanding that dataset curation involves trade-offs between cost and utility is fundamental to interpreting the benchmark results
  - Quick check question: Why would a curator choose a reduced-cost method even if it produces lower quality data?

- Concept: Distribution shifts and robustness
  - Why needed here: The paper evaluates models across multiple distribution shifts, requiring understanding of how models generalize beyond their training distribution
  - Quick check question: What distinguishes synthetic from natural distribution shifts in the evaluation?

- Concept: Self-supervised learning evaluation
  - Why needed here: The benchmark includes evaluation of how well curated datasets guide self-supervised models, which is distinct from supervised evaluation
  - Quick check question: How does kNN classification work for evaluating self-supervised models?

## Architecture Onboarding

- Component map: ImageNet-1K (baseline) -> 5 curated shifts (OI1000, LA1000 img2img, LA1000 txt2img, SD1000 img2img, SD1000 txt2img) -> ResNet-50 training -> Multi-metric evaluation
- Critical path: Dataset curation → Model training → Multi-metric evaluation → Analysis of trade-offs
- Design tradeoffs: Cost vs quality, real vs synthetic data, image-to-image vs text-to-image generation methods
- Failure signatures: Poor performance on long-tailed classes, failure to generalize to distribution shifts, low self-supervised learning guidance quality
- First 3 experiments:
  1. Train ResNet-50 on ImageNet-1K and one curated shift (e.g., LA1000 img2img) and compare base accuracy
  2. Evaluate both models on ImageNet-C to measure synthetic distribution shift robustness
  3. Use both models as initialization for VTAB fine-tuning and compare adaptation performance

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal cost-benefit tradeoff point for data curation strategies in image classification tasks? The paper models data curation as a utility function with marginal cost producing expected utility gains, and mentions discretization of cost into low and high bins. This remains unresolved because the paper treats cost coarsely and fixes data quantity, preventing meaningful comparisons of marginal utility per unit of data. Empirical studies varying both data quantity and curation strategy cost to identify the point of diminishing returns would resolve this.

### Open Question 2
How do different image quality metrics correlate with downstream task performance for synthetic versus natural images? The paper finds that Inception Score and CMMD score do not reliably predict utility as measured by IN1000-Val accuracy, particularly favoring synthetic images over real ones. This remains unresolved because current image quality metrics appear to have low rank-order agreement with data curation utility, especially for diffusion-based methods. Comparative studies of various quality metrics against multiple downstream task performance measures for both synthetic and natural image datasets would resolve this.

### Open Question 3
What specific characteristics of fine-grained classes make them particularly susceptible to label noise when using embedding-based search methods? The paper finds that label noise is more likely in fine-grained classes where the embedding-generating classifier is less accurate. While correlation is observed, the underlying mechanisms causing this relationship are not fully explored. Detailed analysis of embedding space geometry and classifier accuracy patterns across different levels of class granularity would resolve this.

## Limitations

- The reliance on ResNet-50 as the evaluation architecture may not capture architecture-specific effects of different curation strategies
- The study focuses on image classification tasks, limiting generalizability to other vision tasks or multimodal applications
- Synthetic evaluation of robustness through ImageNet-C may not fully represent real-world distribution shifts
- The benchmarking does not account for temporal dynamics in data quality or evolving curation methods

## Confidence

- High confidence in the overall benchmarking methodology and dataset construction
- Medium confidence in the comparative rankings of curation strategies, as results may vary with different architectures or evaluation protocols
- Medium confidence in the conclusions about synthetic vs real data performance, given the current state of synthetic generation technology
- Low confidence in long-term generalizability as data curation techniques continue to evolve

## Next Checks

1. Replicate key findings using different backbone architectures (e.g., Vision Transformers) to verify architecture independence
2. Test curation strategies on out-of-domain tasks beyond ImageNet classification to assess broader applicability
3. Conduct ablation studies on the impact of class imbalance correction methods to validate the observed performance degradation