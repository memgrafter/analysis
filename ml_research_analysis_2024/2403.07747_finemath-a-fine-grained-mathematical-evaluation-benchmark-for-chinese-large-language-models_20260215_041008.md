---
ver: rpa2
title: 'FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large
  Language Models'
arxiv_id: '2403.07747'
source_url: https://arxiv.org/abs/2403.07747
tags:
- mathematical
- reasoning
- llms
- gpt-4
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FineMath, a benchmark for assessing Chinese
  large language models on mathematical reasoning. The authors created 17 types of
  elementary school math word problems and categorized them into three difficulty
  levels based on the number of reasoning steps required to solve them.
---

# FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models

## Quick Facts
- arXiv ID: 2403.07747
- Source URL: https://arxiv.org/abs/2403.07747
- Reference count: 0
- One-line primary result: Chinese LLMs still struggle with elementary math word problems despite extensive training, with performance varying significantly based on prompt format and potential dataset contamination

## Executive Summary
This paper introduces FineMath, a benchmark for evaluating Chinese large language models on elementary school mathematics word problems. The benchmark covers 17 problem categories organized into three difficulty levels based on the number of reasoning steps required. Extensive experiments with 10 LLMs reveal that prompt format significantly affects performance, with differences up to 15% observed across different prompt variations. The study also identifies dataset contamination as a critical factor that can artificially inflate model performance, emphasizing the need for careful benchmark design and evaluation practices.

## Method Summary
FineMath consists of 1,584 Chinese elementary school math word problems manually annotated with difficulty levels based on reasoning steps required. The benchmark evaluates models using zero-shot prompting with multiple prompt formats, comparing both generation and multiple-choice evaluation methods. Contamination analysis is performed by checking n-gram overlap with known training datasets like Ape210K. The evaluation examines three aspects of mathematical reasoning: understanding abstract concepts, reasoning accuracy, and overall accuracy, while also analyzing response length and format sensitivity.

## Key Results
- Chinese LLMs show significant performance variation (up to 15%) based on prompt format changes, even with minor wording differences
- Models with RLHF fine-tuning, >6 billion parameters, and >1 trillion training tokens consistently outperform smaller models
- Dataset contamination from training data can artificially inflate performance by up to 12.5% on affected problem categories

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The difficulty categorization based on reasoning steps effectively stratifies problem complexity for model evaluation.
- **Mechanism:** By manually annotating each problem with the number of reasoning steps required, the benchmark creates a fine-grained difficulty scale that reveals model performance differences across complexity levels.
- **Core assumption:** The number of reasoning steps correlates with human-perceived problem difficulty and captures meaningful differences in mathematical reasoning requirements.
- **Evidence anchors:**
  - [abstract]: "All the 17 categories of math word problems are manually annotated with their difficulty levels according to the number of reasoning steps required to solve these problems."
  - [section]: "MWPs that can be solved in a single step are level-1 MWPs; MWPs that require two steps to solve are level-2 MWPs; level-3 MWPs are those that require three or more steps to solve."
- **Break Condition:** If models show inconsistent performance patterns across step levels within the same category, or if human annotators disagree on step counts for the same problems.

### Mechanism 2
- **Claim:** Multiple prompt variations significantly influence model performance, affecting reliability of benchmark results.
- **Mechanism:** Different prompt formulations (e.g., "Answer:" vs detailed instructions) trigger different response patterns in LLMs, leading to performance variance up to 15% as observed in the experiments.
- **Core assumption:** LLMs are sensitive to prompt formatting and instruction wording, treating them as implicit task definitions.
- **Evidence anchors:**
  - [section]: "we have further analyzed factors in the evaluation process... these overlooked factors greatly affect the evaluation results"
  - [section]: "even a single word like 'Answer:' can significantly affect the model's accuracy"
- **Break Condition:** If performance differences across prompt variations are random rather than systematic, or if other models show no sensitivity to prompt changes.

### Mechanism 3
- **Claim:** Contamination from training data significantly inflates model performance on mathematical benchmarks.
- **Mechanism:** When benchmark problems overlap with training data, models can achieve artificially high accuracy through memorization rather than genuine reasoning ability.
- **Core assumption:** Mathematical problems, especially those with unique numerical values and contexts, are unlikely to appear independently in both training and test sets.
- **Evidence anchors:**
  - [section]: "To determine potential contamination from Ape210K in FineMath, we adopt the identical methodology leveraged in GPT-3"
  - [section]: "MathGLM-10B performs significantly better on the contaminated dataset compared to the clean dataset"
- **Break Condition:** If contamination analysis methods miss semantic overlaps that don't involve exact n-gram matches, or if models perform similarly on both contaminated and clean subsets.

## Foundational Learning

- **Concept:** Mathematical reasoning step decomposition
  - Why needed here: Understanding how to break down complex problems into atomic reasoning steps is essential for creating and interpreting the benchmark's difficulty levels.
  - Quick check question: Given a problem requiring finding the area of a circle with diameter 10, then calculating the cost at $2 per square unit, how many reasoning steps does this require?

- **Concept:** Prompt engineering principles for LLMs
  - Why needed here: Different prompt formulations can significantly affect model outputs, making it crucial to understand how to construct effective evaluation prompts.
  - Quick check question: If you have a math problem and want to minimize the chance of the model generating reasoning instead of just an answer, which prompt format should you use: detailed instructions, minimal instructions, or an answer template?

- **Concept:** Contamination analysis methodology
  - Why needed here: Understanding how to detect and quantify training data contamination is essential for ensuring benchmark validity and interpreting results accurately.
  - Quick check question: What n-gram length threshold is typically used when checking for potential overlap between test sets and training data?

## Architecture Onboarding

- **Component map:**
  Data Collection Pipeline -> Preprocessing Module -> Annotation System -> Evaluation Engine -> Contamination Checker

- **Critical path:**
  1. Problem collection and preprocessing
  2. Manual annotation for categorization and difficulty
  3. Multiple-choice option generation
  4. LLM evaluation with various prompts
  5. Results analysis and contamination assessment

- **Design tradeoffs:**
  - Manual vs. automatic annotation: Manual ensures quality but limits scalability
  - Multiple-choice vs. free response: Multiple-choice is easier to evaluate but may influence model behavior
  - Granular vs. coarse difficulty levels: More levels provide better resolution but increase complexity

- **Failure signatures:**
  - Inconsistent performance across similar problems suggests annotation quality issues
  - High sensitivity to minor prompt changes indicates instability in model reasoning
  - Large performance gaps between contaminated and clean subsets reveal data leakage

- **First 3 experiments:**
  1. Run GPT-4 on the same problem set with three different prompt formats to measure sensitivity
  2. Compare model performance on level-1 vs level-3 problems within the same category to validate difficulty stratification
  3. Test a subset of problems against known training corpora to verify contamination analysis methodology

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt formats and task structures (e.g., direct answer vs. multiple-choice options) systematically affect the mathematical reasoning accuracy of Chinese large language models, and can we develop prompt engineering guidelines to minimize this bias?
- Basis in paper: [explicit] The paper explicitly demonstrates that prompt format significantly influences model accuracy, with differences of up to 15% observed when changing from "Prompt 0" to "Prompt 1" or "Prompt 4" on GPT-4, and that option prediction methods can introduce substantial accuracy differences compared to direct generation.
- Why unresolved: While the paper shows prompt sensitivity exists, it doesn't provide systematic guidelines for prompt engineering or explore the full space of prompt variations to understand which elements cause the most bias.
- What evidence would resolve it: A comprehensive study testing various prompt structures (instruction wording, template formats, question presentation styles) across multiple models, followed by meta-analysis to identify which prompt elements contribute most to accuracy variance, would provide actionable guidelines.

### Open Question 2
- Question: What is the optimal method for detecting and quantifying dataset contamination from training data in mathematical reasoning benchmarks, particularly for Chinese language models, and how can we establish contamination thresholds that balance dataset size with evaluation integrity?
- Basis in paper: [explicit] The paper conducts contamination analysis using n-gram overlap between FineMath and Ape210K, finding significant overlap rates (up to 0.92 for Analytic Geometry) and demonstrating that MathGLM-10B performs significantly better on contaminated data versus clean data, suggesting overfitting to overlapped examples.
- Why unresolved: The paper uses a basic n-gram overlap method but doesn't establish what constitutes an acceptable contamination threshold or explore more sophisticated contamination detection methods that might better capture semantic overlap.
- What evidence would resolve it: Developing and validating multiple contamination detection methods (semantic similarity measures, model-specific overlap metrics) and establishing evidence-based contamination thresholds through controlled experiments where known contaminated models are evaluated would provide clearer guidelines.

### Open Question 3
- Question: What is the relationship between model parameter count, training data scale, and mathematical reasoning performance in Chinese large language models, and are there diminishing returns or optimal thresholds beyond which additional scale provides minimal improvement?
- Basis in paper: [explicit] The paper shows that models with RLHF fine-tuning, >6 billion parameters, and >1 trillion training tokens (GPT-4, GPT-3.5-Turbo, MathGLM-10B) perform significantly better than smaller models, while Baichuan-7B without RLHF performs poorly despite having 7 billion parameters, suggesting these three factors are crucial.
- Why unresolved: While the paper identifies these three factors as important, it doesn't explore whether there are optimal thresholds or diminishing returns, nor does it systematically vary these factors to understand their relative importance.
- What evidence would resolve it: Conducting controlled scaling experiments where models of different sizes are trained on varying amounts of data, with and without RLHF fine-tuning, would reveal the marginal benefit of each factor and identify optimal scaling strategies for mathematical reasoning performance.

## Limitations

- Dataset contamination analysis using n-gram matching may miss semantically similar problems that use different wording
- Manual annotation process for difficulty levels introduces potential subjectivity and inter-annotator variability
- Zero-shot evaluation without extensive prompt engineering may not reflect true model capabilities when properly instruction-tuned

## Confidence

**High Confidence:** The contamination analysis methodology and its findings regarding performance inflation from training data overlap. The systematic observation that different prompt formats significantly affect model outputs is well-supported by experimental evidence.

**Medium Confidence:** The difficulty stratification based on reasoning steps effectively captures problem complexity differences. While the methodology is sound, the manual annotation process introduces some subjectivity that could affect reliability.

**Low Confidence:** Direct cross-model comparisons using accuracy scores across different prompt formats. The format-dependent nature of results suggests that absolute accuracy comparisons may be misleading without proper normalization.

## Next Checks

1. **Semantic contamination analysis:** Conduct a more sophisticated contamination check using semantic similarity measures (e.g., embedding-based comparison) in addition to n-gram analysis to identify problems that are semantically similar but use different wording.

2. **Annotator agreement study:** Have multiple annotators independently label a subset of problems with difficulty levels and reasoning steps to quantify inter-annotator agreement and identify problematic cases where consensus is low.

3. **Format-normalized performance comparison:** Re-analyze model performance by normalizing accuracy scores based on the observed format effects (generation vs. option prediction) to enable more meaningful cross-model comparisons that account for the systematic bias introduced by evaluation format.