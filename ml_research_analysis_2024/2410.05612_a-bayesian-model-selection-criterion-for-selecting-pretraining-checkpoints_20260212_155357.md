---
ver: rpa2
title: A Bayesian Model Selection Criterion for Selecting Pretraining Checkpoints
arxiv_id: '2410.05612'
source_url: https://arxiv.org/abs/2410.05612
tags:
- pretraining
- downstream
- free
- energy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a Bayesian model selection criterion called
  downstream free energy for selecting pretraining checkpoints that are most adaptable
  to downstream tasks. The method quantifies a checkpoint's adaptability by measuring
  the concentration of nearby favorable parameters for the downstream task, without
  requiring access to downstream data or prior knowledge of the task.
---

# A Bayesian Model Selection Criterion for Selecting Pretraining Checkpoints

## Quick Facts
- arXiv ID: 2410.05612
- Source URL: https://arxiv.org/abs/2410.05612
- Reference count: 33
- Primary result: Bayesian free energy criterion for selecting pretraining checkpoints that predicts downstream adaptability without requiring downstream data

## Executive Summary
This paper introduces a Bayesian model selection criterion called downstream free energy for selecting pretraining checkpoints that are most adaptable to downstream tasks. The method quantifies a checkpoint's adaptability by measuring the concentration of nearby favorable parameters for the downstream task, without requiring access to downstream data or prior knowledge of the task. The authors prove that minimizing pretraining free energy serves as a reliable proxy for minimizing downstream free energy under certain distributional shift conditions. Empirically, they demonstrate that checkpoints with lower pretraining free energy consistently show better downstream adaptation performance across various datasets and architectures.

## Method Summary
The method computes a Bayesian free energy criterion that estimates the adaptability of pretraining checkpoints to downstream tasks. Pretraining free energy is calculated using Watanabe-Akaike Bayesian Information Criterion (WBIC) computed via Stochastic Gradient Langevin Dynamics (SGLD) sampling around each checkpoint. This provides a practical proxy for downstream free energy, which measures the concentration of parameters that perform well on the target task. The approach requires no downstream data during pretraining, making it suitable for selecting checkpoints from large-scale pretraining runs. During fine-tuning, models are adapted using limited fine-tuning with differential learning rates for the backbone and classification head.

## Key Results
- Lower pretraining free energy correlates strongly with better downstream adaptation performance across multiple architectures and datasets
- Pretraining WBIC serves as an effective estimator of pretraining free energy, computed without downstream data
- The criterion outperforms geometric complexity and neural collapse metrics in predicting downstream performance
- Strong correlation between free energy values and transfer accuracy is maintained across few-shot and full-dataset fine-tuning scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining checkpoints with lower downstream free energy are more adaptable to downstream tasks.
- Mechanism: Downstream free energy measures the concentration of favorable parameters near a checkpoint by computing the negative log of a local marginal likelihood. Lower free energy indicates higher density of well-performing weights in the vicinity, meaning the checkpoint is already near good solutions for downstream tasks.
- Core assumption: The downstream distribution is not too different from the pretraining distribution, allowing pretraining free energy to serve as a reliable proxy.
- Evidence anchors:
  - [abstract] "We introduce a Bayesian model selection criterion, called the downstream free energy, which quantifies a checkpoint's adaptability by measuring the concentration of nearby favorable parameters for the downstream task."
  - [section 4.1] "Lower downstream free energy indicates a higher concentration of parameters in parameter space for which the model is more adaptable and capable of generalizing well on downstream tasks."
  - [corpus] Weak - no direct neighbor evidence found
- Break condition: When pretraining and downstream distributions are disjoint or highly divergent, making the proxy relationship invalid.

### Mechanism 2
- Claim: Minimizing pretraining free energy serves as a reliable proxy for minimizing downstream free energy under certain distributional shift conditions.
- Mechanism: The asymptotic expansion shows that pretraining free energy bounds the downstream free energy when the distributional shift is bounded. The pretraining free energy can be computed without downstream data, making it practical for model selection during pretraining.
- Core assumption: The maximum ratio of probability densities between downstream and pretraining distributions is finite (M < ∞).
- Evidence anchors:
  - [section 5] "We prove that minimizing pretraining free energy serves as a reliable proxy for minimizing downstream free energy under certain distributional shift conditions."
  - [section 4.2] "Unlike ¯Z1(Bγ(w∗)) and ¯F1(Bγ(w∗)), here the quantities Z0(Bγ(w∗); β) and F0(Bγ(w∗); β) are stochastic."
  - [corpus] Weak - no direct neighbor evidence found
- Break condition: When the maximum density ratio M is infinite, typically when pretraining and downstream distributions have disjoint supports.

### Mechanism 3
- Claim: Lower pretraining WBIC correlates with better downstream performance because it estimates pretraining free energy.
- Mechanism: Pretraining WBIC is an asymptotically unbiased estimator of the pretraining free energy, computed via SGLD sampling. Checkpoints with lower WBIC have lower free energy, indicating better adaptability.
- Core assumption: The SGLD sampling accurately approximates the posterior distribution around checkpoints.
- Evidence anchors:
  - [section 5.2] "WBIC(w∗; β∗) can be reliably computed via SGLD sampling methods"
  - [section 6] "We observe a strong correlation between lower pretraining free energy (as measured by the pretraining WBIC) and better downstream performance"
  - [corpus] Weak - no direct neighbor evidence found
- Break condition: When SGLD sampling fails to converge or when the posterior distribution is too complex to sample accurately.

## Foundational Learning

- Concept: Bayesian model selection
  - Why needed here: The free energy criterion is fundamentally a Bayesian approach to model selection, choosing checkpoints based on their marginal likelihood.
  - Quick check question: What is the relationship between free energy and marginal likelihood?

- Concept: Asymptotic expansions of free energy
  - Why needed here: Understanding the leading order terms in free energy expansions reveals why pretraining free energy can proxy downstream free energy.
  - Quick check question: What are the two main components in the asymptotic expansion of free energy?

- Concept: Distributional shift and bounded density ratios
  - Why needed here: The theoretical guarantee that pretraining free energy proxies downstream free energy requires the maximum density ratio M to be finite.
  - Quick check question: When would the maximum density ratio M be infinite?

## Architecture Onboarding

- Component map: Pretraining phase -> Checkpoint selection via WBIC -> Fine-tuning phase -> Evaluation
- Critical path: Pretraining → Checkpoint selection via WBIC → Fine-tuning → Performance evaluation
- Design tradeoffs:
  - WBIC computation vs. training efficiency: SGLD sampling adds computational overhead but enables better checkpoint selection
  - Hyperparameter exploration: More hyperparameter values increase checkpoint diversity but also computational cost
  - Model architecture: Different architectures (ResNet vs VGG) may have different free energy landscapes
- Failure signatures:
  - Pretraining train loss collapse: When pretraining loss becomes similar across checkpoints, WBIC becomes crucial for differentiation
  - Poor downstream correlation: Indicates distributional shift is too large or WBIC estimation is inaccurate
  - SGLD sampling failure: Checkpointing with poor mixing or high variance in WBIC estimates
- First 3 experiments:
  1. Vary learning rate while keeping other hyperparameters fixed to observe WBIC vs accuracy correlation
  2. Vary batch size to test the effect on free energy and downstream performance
  3. Vary momentum to see how optimizer dynamics affect free energy minimization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the downstream free energy criterion perform when comparing models trained on completely disjoint data distributions?
- Basis in paper: [inferred] from Section 5 which notes that Proposition 5.1 would be uninformative if pretraining and downstream data have disjoint label supports
- Why unresolved: The paper only tests cases where pretraining dataset is larger and more diverse than downstream dataset, not truly disjoint distributions
- What evidence would resolve it: Empirical experiments comparing downstream free energy predictions across models trained on non-overlapping data distributions versus overlapping ones

### Open Question 2
- Question: Can the pretraining free energy be computed tractably for extremely large models (tens or hundreds of billions of parameters)?
- Basis in paper: [explicit] from Section 7 which states "the practical computation of the pretraining WBIC...remains challenging for large models which may possess tens or hundreds of billions of parameters"
- Why unresolved: Current SGLD methods are computationally expensive and don't scale to massive parameter counts
- What evidence would resolve it: Development and demonstration of efficient algorithms for approximating pretraining free energy in billion-parameter models, or identification of computational shortcuts

### Open Question 3
- Question: Does downstream free energy provide reliable predictions for fine-tuning methods beyond limited fine-tuning and full fine-tuning?
- Basis in paper: [inferred] from Section 3 which focuses on limited fine-tuning experiments, while Proposition 5.1 provides theoretical guarantees for general Bayesian prediction
- Why unresolved: All empirical validation focuses on limited fine-tuning and full fine-tuning, not other adaptation methods like prompt tuning or adapter-based approaches
- What evidence would resolve it: Experimental validation showing downstream free energy correlates with adaptation performance across diverse fine-tuning paradigms including prompt tuning, adapter methods, and other parameter-efficient techniques

## Limitations

- Theoretical guarantees rely on bounded distributional shift assumptions that may not hold in practice
- Empirical validation is limited to specific architectures (ResNet-18, VGG-16) and datasets (CIFAR-FS, mini-Imagenet)
- Computational overhead of SGLD sampling for WBIC computation could be prohibitive for large-scale pretraining runs

## Confidence

High confidence in the theoretical framework and mathematical derivations, as the asymptotic expansions and free energy relationships are well-established in Bayesian statistics. Medium confidence in the empirical validation, given the controlled experimental setup but limited architectural and dataset diversity. Low confidence in the practical utility claims, as the computational cost of WBIC evaluation during pretraining has not been thoroughly characterized.

## Next Checks

1. **Distributional Shift Stress Test**: Systematically vary the downstream task distribution (e.g., different ImageNet variants with increasing domain shift) to empirically validate when the pretraining free energy proxy breaks down and M becomes infinite.

2. **Architectural Generalization**: Evaluate the WBIC criterion on transformer-based architectures (ViT, BERT) and compare its effectiveness against geometric complexity metrics across diverse task families including NLP, vision, and multimodal settings.

3. **Computational Overhead Analysis**: Benchmark the wall-clock time and resource requirements of computing pretraining WBIC via SGLD sampling against the total pretraining time, and evaluate whether the marginal improvement in downstream performance justifies the additional cost.