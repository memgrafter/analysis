---
ver: rpa2
title: 'M$^{3}$D: A Multimodal, Multilingual and Multitask Dataset for Grounded Document-level
  Information Extraction'
arxiv_id: '2412.04026'
source_url: https://arxiv.org/abs/2412.04026
tags:
- entity
- multimodal
- visual
- missing
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces M\xB3D, a multimodal, multilingual, and\
  \ multitask dataset designed to address shortcomings in existing multimodal information\
  \ extraction (IE) datasets. The dataset contains paired document-level text and\
  \ video, supports English and Chinese, and includes four IE tasks: entity recognition,\
  \ entity chain extraction, relation extraction, and visual grounding."
---

# M$^{3}$D: A Multimodal, Multilingual and Multitask Dataset for Grounded Document-level Information Extraction

## Quick Facts
- **arXiv ID**: 2412.04026
- **Source URL**: https://arxiv.org/abs/2412.04026
- **Reference count**: 40
- **Primary result**: Proposed model achieves average F1 scores of 53.80% (English) and 53.77% (Chinese) on multimodal IE tasks

## Executive Summary
This paper introduces M³D, a novel multimodal, multilingual, and multitask dataset designed to address critical gaps in existing multimodal information extraction (IE) resources. The dataset contains paired document-level text and video content in both English and Chinese, supporting four IE tasks: entity recognition, entity chain extraction, relation extraction, and visual grounding. To establish a benchmark for this dataset, the authors propose a hierarchical multimodal IE model featuring a Denoised Feature Fusion Module (DFFM) for effective multimodal integration and a Missing Modality Construction Module (MMCM) to handle incomplete modal information.

## Method Summary
The method involves creating a multimodal dataset by collecting biography videos from YouTube (English) and bilibili (Chinese), splitting them into clips, generating subtitles, and manually annotating entities, entity chains, relations, and visual targets. The proposed hierarchical multimodal IE model uses Longformer for text encoding, ViT for image encoding, and YOLO for visual grounding. The model features two key modules: DFFM, which uses a variational auto-encoder (VAE) to denoise and fuse features from different layers of pre-trained models, and MMCM, which constructs missing modality features using convolutional layers when either text or visual information is absent.

## Key Results
- The model achieves average F1 scores of 53.80% on the English dataset and 53.77% on the Chinese dataset
- Ablation studies demonstrate the effectiveness of both the Denoised Feature Fusion Module (DFFM) and Missing Modality Construction Module (MMCM)
- The model shows improved performance in handling missing modalities compared to baseline approaches
- Visual grounding performance varies with prompt length, with optimal results achieved at specific prompt sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Denoised Feature Fusion Module (DFFM) improves multimodal information integration by denoising features before fusion.
- Mechanism: DFFM uses a variational auto-encoder (VAE) to map features to latent space where noise is reduced, then fuses low, middle, and high-level features from pre-trained models across modalities.
- Core assumption: Noise present in both text and image features reduces fusion effectiveness, and denoising in latent space removes this noise.
- Evidence anchors:
  - [abstract]: "This model effectively leverages and integrates multimodal information through a Denoised Feature Fusion Module (DFFM)."
  - [section V-D]: "Considering the inadequate utilization and fusion of multimodal information in existing models, we developed a denoised feature fusion module (DFFM). Specifically, this module first employs a variational auto-encoder (VAE) to denoise features from different layers of a pre-trained model..."
  - [corpus]: Weak - no direct corpus evidence for denoising effectiveness.
- Break condition: If the VAE fails to properly denoise features, or if the noise is not the primary factor limiting fusion performance.

### Mechanism 2
- Claim: The Missing Modality Construction Module (MMCM) mitigates performance degradation when modalities are missing by generating plausible missing modality features.
- Mechanism: MMCM uses convolutional layers to construct missing modality features from available modalities by concatenating with randomly initialized prompts and processing through convolutions.
- Core assumption: Missing modalities can be effectively reconstructed from available modalities using learned transformations, and this reconstruction is better than simple zero-filling.
- Evidence anchors:
  - [abstract]: "Furthermore, in non-ideal scenarios, modal information is often incomplete. Thus, we designed a Missing Modality Construction Module (MMCM) to alleviate the issues caused by missing modalities."
  - [section V-E]: "In non-ideal situations, modal information is often incomplete. When a modal information is missing, our DFFM will lose its function, resulting in a decrease in the overall performance of the model. Therefore, we designed a missing modality construction module (MMCM)."
  - [corpus]: Weak - no direct corpus evidence for missing modality reconstruction effectiveness.
- Break condition: If the convolutional reconstruction fails to capture essential information from available modalities, or if the missing modality prompt initialization is inadequate.

### Mechanism 3
- Claim: Hierarchical features from different layers of pre-trained models capture complementary information that improves multimodal IE tasks.
- Mechanism: The model extracts low, middle, and high-level features from Longformer and ViT layers, with each level capturing different aspects (lexical/syntactic/semantic for text, color/texture/structure for images), then fuses them.
- Core assumption: Different layers of pre-trained models capture distinct types of information, and combining these hierarchical features provides more comprehensive representations than using any single layer.
- Evidence anchors:
  - [section V-C]: "Some existing studies have shown that different layers of pre-trained models can capture different features. Rogers et al. [47] found that the lower layers, middle layers, and higher layers of BERT [48] capture lexical features, syntactic features, and semantic features of text, respectively."
  - [section V-C]: "Additionally, Dosovitskiy et al. [49] discovered that the lower layers of ViT capture features such as color, texture, and edges, while the middle layers capture local structures and regional dependencies in images, and the higher layers capture overall object structures, categories, and global dependencies."
  - [corpus]: Weak - no direct corpus evidence for hierarchical feature effectiveness.
- Break condition: If the hierarchical feature combination doesn't provide meaningful improvement over using a single representative layer.

## Foundational Learning

- Concept: Multimodal information fusion
  - Why needed here: The paper combines text and video information for information extraction tasks, requiring effective integration of different modalities.
  - Quick check question: What are the main challenges in fusing text and visual information for information extraction?

- Concept: Variational auto-encoders (VAEs)
  - Why needed here: VAEs are used in the DFFM to denoise features before fusion, requiring understanding of how VAEs work.
  - Quick check question: How does a VAE differ from a standard auto-encoder in its approach to learning representations?

- Concept: Visual grounding
  - Why needed here: The dataset includes visual grounding tasks that require locating entities in video frames based on text descriptions.
  - Quick check question: What metrics are typically used to evaluate visual grounding performance?

## Architecture Onboarding

- Component map: Text/video encoding (Longformer/ViT) → DFFM (VAE-based fusion) → MMCM (if needed) → Task-specific output layers
- Critical path: Text/video encoding → DFFM → MMCM (if needed) → Task-specific output layers
- Design tradeoffs: Using VAEs adds computational complexity but potentially improves fusion quality; MMCM adds robustness but requires additional parameters for missing modality generation.
- Failure signatures: Poor performance on missing modality cases suggests MMCM inadequacy; subpar fusion performance suggests DFFM issues; individual task failures may indicate encoding problems.
- First 3 experiments:
  1. Test model performance with complete modalities only (no MMCM needed) to establish baseline
  2. Evaluate model with different missing modality ratios to assess MMCM effectiveness
  3. Compare DFFM performance against simple concatenation or attention-based fusion methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the M³D dataset change when evaluated on languages other than English and Chinese?
- Basis in paper: [explicit] The paper states that M³D supports two widely-used languages, namely English and Chinese, but does not explore performance on other languages.
- Why unresolved: The dataset was specifically constructed using English and Chinese videos and subtitles, and no experiments were conducted with other languages.
- What evidence would resolve it: Performance metrics (F1 scores) for entity recognition, relation extraction, and visual grounding tasks on additional languages would provide insights into the dataset's multilingual applicability.

### Open Question 2
- Question: How does the Denoised Feature Fusion Module (DFFM) perform when applied to other multimodal tasks beyond information extraction, such as multimodal sentiment analysis or question answering?
- Basis in paper: [inferred] The DFFM is designed to effectively leverage and integrate multimodal information through denoising and fusion of features from different layers of pre-trained models, which could be applicable to other tasks requiring multimodal integration.
- Why unresolved: The paper focuses on evaluating the DFFM within the context of the M³D dataset and its specific tasks, without exploring its effectiveness in other domains.
- What evidence would resolve it: Comparative studies showing the performance of DFFM against other fusion methods on benchmark datasets for multimodal sentiment analysis or question answering would demonstrate its broader applicability.

### Open Question 3
- Question: What is the impact of varying the prompt length in the Missing Modality Construction Module (MMCM) on the model's performance for tasks other than those evaluated in the paper?
- Basis in paper: [explicit] The paper discusses the effect of prompt length on model performance, particularly in the visual grounding task, but does not explore its impact on other tasks.
- Why unresolved: The experiments conducted focus on a specific set of tasks and do not provide insights into how prompt length variations might affect other potential applications of the MMCM.
- What evidence would resolve it: Performance metrics for tasks such as multimodal sentiment analysis or visual question answering, with varying prompt lengths in the MMCM, would clarify the module's sensitivity to prompt length across different applications.

## Limitations

- The effectiveness of DFFM and MMCM mechanisms lacks robust empirical validation, with weak corpus evidence supporting their claimed benefits
- The model achieves modest F1 scores (53.80% English, 53.77% Chinese) without adequate comparison against simpler baseline approaches
- The paper does not adequately explore how the denoising and missing modality reconstruction mechanisms perform on tasks beyond the specific IE tasks evaluated

## Confidence

*High Confidence:* The dataset creation methodology is clearly described and the motivation for addressing multimodal, multilingual, and multitask requirements in information extraction is well-founded. The four IE tasks (NER, CR, RE, VG) are standard and appropriate for the domain.

*Medium Confidence:* The hierarchical feature extraction approach using different layers of pre-trained models is theoretically sound and supported by existing literature, though the specific implementation details for fusion are not fully specified.

*Low Confidence:* The denoising and missing modality reconstruction mechanisms lack robust empirical validation. The paper asserts their effectiveness but provides limited ablation studies or comparative analysis against simpler alternatives.

## Next Checks

1. **Ablation study on DFFM and MMCM:** Conduct comprehensive ablation experiments comparing the full model against variants with only DFFM, only MMCM, neither module, and simpler fusion approaches (e.g., concatenation, attention) to quantify the actual contribution of each complex component.

2. **Robustness testing across missing modality ratios:** Systematically evaluate model performance with varying percentages of missing text (0-100%) and visual information (0-100%) to determine the operational limits of the MMCM and identify failure thresholds.

3. **Cross-dataset generalization evaluation:** Test the trained model on an independent multimodal information extraction dataset (if available) or on different domains within the M³D dataset to assess whether the denoising and missing modality mechanisms generalize beyond the biography video domain where the model was trained.