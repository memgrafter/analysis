---
ver: rpa2
title: Towards Safer Large Language Models through Machine Unlearning
arxiv_id: '2402.10058'
source_url: https://arxiv.org/abs/2402.10058
tags:
- harmful
- unlearning
- knowledge
- prompts
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Selective Knowledge negation Unlearning (SKU),
  a two-stage framework for removing harmful knowledge from large language models
  while preserving utility on normal prompts. The method first identifies harmful
  knowledge through guided distortion, random disassociation, and preservation divergence
  modules, then negates this knowledge from the pretrained model.
---

# Towards Safer Large Language Models through Machine Unlearning

## Quick Facts
- arXiv ID: 2402.10058
- Source URL: https://arxiv.org/abs/2402.10058
- Authors: Zheyuan Liu; Guangyao Dou; Zhaoxuan Tan; Yijun Tian; Meng Jiang
- Reference count: 40
- Primary result: SKU achieves up to 57% reduction in harmful responses while maintaining perplexity near baseline levels

## Executive Summary
This paper introduces Selective Knowledge negation Unlearning (SKU), a two-stage framework for removing harmful knowledge from large language models while preserving utility on normal prompts. The method first identifies harmful knowledge through guided distortion, random disassociation, and preservation divergence modules, then negates this knowledge from the pretrained model. Experiments across OPT-2.7B, LLAMA2-7B, and LLAMA2-13B show SKU achieves up to 57% reduction in harmful responses (from 54% to 3% on OPT-2.7B) while maintaining perplexity scores near baseline levels (25.46 vs 25.21 without random disassociation).

## Method Summary
SKU operates through a two-stage framework: first identifying harmful knowledge via three modules (guided distortion, random disassociation, preservation divergence), then negating this knowledge from the pretrained model. The guided distortion module creates controlled harmful outputs, random disassociation introduces noise to break harmful associations, and preservation divergence ensures normal knowledge remains intact. The negation stage removes identified harmful knowledge while preserving model utility. The framework was evaluated across three model sizes (OPT-2.7B, LLAMA2-7B, LLAMA2-13B) and compared against baselines, demonstrating significant reduction in harmful responses while maintaining performance on standard benchmarks.

## Key Results
- SKU achieves up to 57% reduction in harmful responses (from 54% to 3% on OPT-2.7B)
- Maintains perplexity scores near baseline levels (25.46 vs 25.21 without random disassociation)
- Outperforms baselines by 10-19x in harmful rate reduction while maintaining comparable utility metrics

## Why This Works (Mechanism)
SKU works by systematically identifying and removing harmful knowledge patterns while preserving beneficial knowledge. The framework uses guided distortion to create controlled examples of harmful outputs, random disassociation to break unwanted associations, and preservation divergence to protect normal knowledge. The two-stage approach ensures that only harmful knowledge is targeted for removal, preventing catastrophic forgetting of useful information. This selective unlearning process allows the model to maintain its general capabilities while reducing harmful responses to specific prompts.

## Foundational Learning
- **Knowledge Identification**: Critical for distinguishing harmful from benign knowledge patterns. Quick check: Verify that the identification modules can consistently detect known harmful patterns across different model architectures.
- **Selective Unlearning**: Enables targeted removal of harmful knowledge without affecting general model capabilities. Quick check: Ensure that utility metrics remain stable after unlearning harmful content.
- **Knowledge Preservation**: Protects beneficial knowledge during the unlearning process. Quick check: Validate that preservation mechanisms effectively maintain performance on standard benchmarks.
- **Two-Stage Framework**: Separates identification from negation for more controlled and effective unlearning. Quick check: Confirm that the two-stage approach provides better results than single-stage methods.
- **Random Disassociation**: Introduces controlled noise to break harmful associations. Quick check: Measure the impact of random disassociation on both harmful reduction and utility preservation.
- **Guided Distortion**: Creates controlled harmful outputs for training purposes. Quick check: Verify that guided distortion produces representative harmful patterns.

## Architecture Onboarding

**Component Map**: Harmful Prompt Input -> Guided Distortion -> Random Disassociation -> Preservation Divergence -> Knowledge Identification -> Negation Module -> Safer Model Output

**Critical Path**: The critical path involves harmful prompt input flowing through all three identification modules (guided distortion, random disassociation, preservation divergence) before reaching the knowledge identification stage, which then feeds into the negation module for final unlearning.

**Design Tradeoffs**: The framework balances harmful knowledge removal with utility preservation. Removing the random disassociation module increases perplexity (25.46 vs 25.21), indicating a tradeoff between unlearning effectiveness and model performance. The three-module approach provides comprehensive identification but adds complexity compared to single-method approaches.

**Failure Signatures**: Potential failures include: 1) Over-generalization leading to removal of beneficial knowledge, 2) Incomplete harmful knowledge identification, 3) Catastrophic forgetting of general capabilities, 4) Imbalance between harmful reduction and utility preservation.

**First 3 Experiments**:
1. Test harmful rate reduction on baseline models without unlearning to establish initial metrics
2. Evaluate perplexity and standard benchmark performance after applying SKU to measure utility preservation
3. Conduct ablation studies removing each module to quantify their individual contributions

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on toxicity and bias metrics, lacking extensive testing of task-specific capabilities and downstream applications
- Limited to three model architectures (OPT-2.7B, LLAMA2-7B, LLAMA2-13B), raising questions about generalizability to other model families and larger parameter scales
- Does not test scalability to industrial-scale models or real-world deployment scenarios, limiting practical applicability

## Confidence
- Harmful response reduction claims: Medium confidence - results show significant improvement but evaluation methodology could influence outcomes
- Utility preservation: Medium confidence - perplexity scores are maintained but task-specific capabilities need further verification
- Framework generalizability: Low-Medium confidence - effectiveness across three model sizes is promising but broader architecture testing is needed

## Next Checks
1. Conduct extensive evaluations on task-specific benchmarks (MMLU, SuperGLUE) to verify that critical capabilities are preserved after unlearning
2. Test the framework's effectiveness across diverse model architectures (GPT, Mistral, Claude) and parameter scales (30B, 70B) to assess generalizability
3. Implement long-term monitoring to detect potential knowledge recovery or regression patterns over extended inference periods