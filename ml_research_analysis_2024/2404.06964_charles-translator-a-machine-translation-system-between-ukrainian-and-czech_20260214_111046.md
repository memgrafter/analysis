---
ver: rpa2
title: 'Charles Translator: A Machine Translation System between Ukrainian and Czech'
arxiv_id: '2404.06964'
source_url: https://arxiv.org/abs/2404.06964
tags:
- translation
- czech
- ukrainian
- machine
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Charles Translator provides high-quality Czech-Ukrainian machine
  translation developed rapidly during the 2022 refugee crisis. It uses iterated block
  back-translation with Transformer models, trained on parallel and monolingual data.
---

# Charles Translator: A Machine Translation System between Ukrainian and Czech

## Quick Facts
- arXiv ID: 2404.06964
- Source URL: https://arxiv.org/abs/2404.06964
- Reference count: 0
- Primary result: Direct Czech-Ukrainian MT system with BLEU 35.8-37.0, chrF 59.0-60.7 on WMT test sets

## Executive Summary
Charles Translator is a high-quality machine translation system developed in spring 2022 to address the urgent need for Czech-Ukrainian translation during the refugee crisis. The system uses iterated block back-translation with Transformer models, trained on parallel and monolingual data from multiple sources. It translates directly between languages rather than through English, leveraging their typological similarities. The service is deployed via API, web app, and Android app with speech input and Cyrillic-Latin transliteration, supporting translation across news, voice, personal, official, and gaming domains.

## Method Summary
The system uses Transformer architecture with iterated block back-translation, trained on parallel data from OPUS repositories and monolingual data for backtranslation. Direct translation between Czech and Ukrainian is employed rather than using English as a pivot, taking advantage of their typological similarity. The model was developed rapidly in spring 2022 with community support, including volunteer translators and data providers.

## Key Results
- BLEU scores of 35.8-37.0 and chrF scores of 59.0-60.7 on WMT test sets
- Strong performance across five domains: news, voice, personal, official, and gaming
- Direct translation approach outperforms pivot-based systems that use English as intermediary

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct translation between typologically similar languages preserves grammatical information that would be lost in pivot-based translation.
- Mechanism: By translating directly between Czech and Ukrainian without using English as an intermediary, the system retains language-specific grammatical features like gender, formality, and case markings.
- Core assumption: The two Slavic languages share enough structural similarity that direct translation is more effective than pivot-based approaches.
- Evidence anchors:
  - [abstract] "The system translates directly, compared to other available systems that use English as a pivot, and thus take advantage of the typological similarity of the two languages."
  - [section] "With English as the pivot, some information is inevitably lost. This is most visible in the grammatical categories of gender and politeness..."
- Break condition: The typological similarity assumption fails if the languages diverge significantly in key grammatical features or if the direct translation model is not sufficiently trained on parallel data.

### Mechanism 2
- Claim: Iterated block back-translation efficiently utilizes monolingual data to improve translation quality.
- Mechanism: The system generates synthetic parallel data by translating monolingual text from the target language back to the source language, then trains on this augmented dataset iteratively.
- Core assumption: Monolingual data contains valuable linguistic patterns that can improve translation quality when properly leveraged.
- Evidence anchors:
  - [abstract] "It uses the block back-translation method, which allows for efficient use of monolingual training data."
  - [section] "We also used data available at the OPUS repository... In addition to the (authentic) parallel data, we also used monolingual data for backtranslation: 50M originally Czech sentences..."
- Break condition: The quality of back-translated synthetic data degrades significantly, or the iterative process leads to model collapse.

### Mechanism 3
- Claim: Rapid development with community data collection meets urgent translation needs during crisis situations.
- Mechanism: Leveraging volunteer translators, existing corpora, and quick deployment infrastructure enables rapid system development when commercial solutions are unavailable.
- Core assumption: Community resources and existing infrastructure can be mobilized quickly to address urgent translation needs.
- Evidence anchors:
  - [abstract] "The system was developed in the spring of 2022 with the help of many language data providers in order to quickly meet the demand for such a service, which was not available at the time in the required quality."
  - [section] "Cooperation with Czech-Ukrainian translators, translation agencies, and the authors of the InterCorp parallel corpus... was important for obtaining good quality parallel data."
- Break condition: Community data collection proves insufficient in quality or quantity, or the existing infrastructure cannot support rapid deployment.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The core translation system uses Transformer models, which rely on self-attention to capture dependencies between words in different languages.
  - Quick check question: How does the multi-head attention mechanism in Transformers help capture long-range dependencies in machine translation?

- Concept: Back-translation techniques
  - Why needed here: The system uses iterated block back-translation to leverage monolingual data, requiring understanding of how to generate and use synthetic parallel data.
  - Quick check question: What is the difference between standard back-translation and iterated back-translation in machine translation?

- Concept: BLEU and chrF evaluation metrics
  - Why needed here: The system's performance is evaluated using BLEU and chrF scores, which require understanding how these metrics measure translation quality.
  - Quick check question: Why might chrF be more appropriate than BLEU for evaluating translation between morphologically rich languages like Czech and Ukrainian?

## Architecture Onboarding

- Component map: Translation service (Transformer models with block back-translation) → REST API → Web interface (React/Node.js) → Android app (Kotlin/Jetpack Compose)
- Critical path: Data collection → Model training → API deployment → Interface development → User deployment
- Design tradeoffs: Direct translation vs. pivot-based approaches; model complexity vs. deployment speed; open-source vs. proprietary solutions
- Failure signatures: Low BLEU/chrF scores indicating poor translation quality; API response time degradation; interface crashes or unresponsive behavior
- First 3 experiments:
  1. Train a basic Transformer model on a small subset of parallel data and evaluate BLEU score
  2. Implement block back-translation with monolingual data and compare performance
  3. Deploy the trained model via REST API and test with sample translation requests

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the translation quality of Charles Translator compare to commercial MT systems like Google Translate or DeepL for Czech-Ukrainian translation?
- Basis in paper: [inferred] The paper evaluates Charles Translator against other systems in WMT competitions but does not compare it to major commercial MT providers.
- Why unresolved: Commercial systems' performance on this language pair is not publicly available or tested in academic evaluations.
- What evidence would resolve it: Direct comparison of Charles Translator with commercial systems on the same test sets using standard metrics like BLEU, chrF, and human evaluation.

### Open Question 2
- Question: What is the long-term impact of Charles Translator on Ukrainian refugee integration and language learning in the Czech Republic?
- Basis in paper: [inferred] The paper mentions plans to adapt the model for educational purposes but does not provide data on actual usage patterns or integration outcomes.
- Why unresolved: The paper focuses on technical development and initial usage statistics, not longitudinal studies of social impact.
- What evidence would resolve it: Longitudinal studies tracking refugee integration metrics, language acquisition rates, and educational outcomes correlated with Charles Translator usage.

### Open Question 3
- Question: How does the performance of Charles Translator vary across different dialects or regional variations of Ukrainian and Czech?
- Basis in paper: [inferred] The paper evaluates on general test sets but does not analyze performance on dialect-specific data.
- Why unresolved: Dialectal variation is not addressed in the evaluation methodology or results.
- What evidence would resolve it: Evaluation on dialect-specific test sets for both languages, with performance metrics broken down by dialect.

### Open Question 4
- Question: What are the specific limitations of block back-translation compared to other data augmentation techniques for this language pair?
- Basis in paper: [explicit] The paper uses block back-translation but does not compare it to alternative methods.
- Why unresolved: Only one data augmentation method is employed and evaluated.
- What evidence would resolve it: Comparative experiments using different data augmentation techniques (e.g., forward translation, knowledge distillation) with the same training data and evaluation setup.

### Open Question 5
- Question: How does the inclusion of speech recognition errors in the "Voice" domain test set affect the overall system performance and what error correction mechanisms could improve results?
- Basis in paper: [explicit] The paper includes an ASR error test set but does not analyze error types or propose correction strategies.
- Why unresolved: ASR errors are included in evaluation but not analyzed or addressed.
- What evidence would resolve it: Detailed error analysis of ASR-induced errors and experiments with ASR error correction modules integrated into the MT pipeline.

## Limitations
- Evaluation based on WMT test sets may not fully represent real-world usage diversity
- Limited comparison with commercial MT systems and other approaches
- No detailed error analysis or human evaluation results provided

## Confidence

**High confidence**: The technical approach of using direct translation between typologically similar languages is well-supported by the evidence and aligns with established linguistic principles. The implementation of Transformer models with iterated block back-translation follows standard practices in the field. The deployment infrastructure (API, web app, Android app) is clearly described and plausible.

**Medium confidence**: The claimed performance metrics (BLEU 35.8-37.0, chrF 59.0-60.7) are based on WMT test sets, but without access to the exact test sets or detailed evaluation procedures, independent verification is difficult. The effectiveness of the community-driven data collection approach is plausible given the crisis context, but the paper does not provide detailed quality control procedures for volunteer-contributed translations.

**Low confidence**: The paper does not provide sufficient detail about specific model hyperparameters, data filtering thresholds, or training procedures to enable precise reproduction. The comparison with other systems (particularly those using English as pivot) is limited, making it difficult to assess the relative advantage of the direct translation approach.

## Next Checks
1. Reproduce the baseline model: Train a basic Transformer model on a small subset of the described parallel data and evaluate BLEU/chrF scores on a standard WMT Ukrainian-Czech test set to verify the reported performance range.
2. Test domain generalization: Evaluate the trained system on test sets from each of the five reported domains (news, voice, personal, official, gaming) to verify that performance is consistent across domains as claimed.
3. Compare pivot vs direct translation: Train and evaluate both a pivot-based system (through English) and the direct translation system on the same data to quantify the actual performance difference between approaches.