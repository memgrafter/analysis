---
ver: rpa2
title: 'Thematic Analysis with Large Language Models: does it work with languages
  other than English? A targeted test in Italian'
arxiv_id: '2404.08488'
source_url: https://arxiv.org/abs/2404.08488
tags:
- data
- research
- themes
- codes
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study tested whether a large language model (LLM) could perform
  thematic analysis on semi-structured interviews in Italian, not just English. Using
  GPT-3.5 and GPT-4, the model produced initial codes and themes from Italian interview
  data.
---

# Thematic Analysis with Large Language Models: does it work with languages other than English? A targeted test in Italian

## Quick Facts
- arXiv ID: 2404.08488
- Source URL: https://arxiv.org/abs/2404.08488
- Authors: Stefano De Paoli
- Reference count: 0
- Primary result: LLMs can perform thematic analysis on Italian interview data effectively, with semantic similarity scores above 0.6 and high human evaluation ratings

## Executive Summary
This study tested whether large language models (LLMs) can perform thematic analysis on semi-structured interviews in Italian, not just English. Using GPT-3.5 and GPT-4, the model produced initial codes and themes from Italian interview data that were comparable to human-coded categories. Themes were validated using semantic similarity metrics (scores above 0.6) and human evaluation (ratings of 8-10 out of 10). A brief test showed prompts in Italian and English yielded similar codes, suggesting LLMs can process thematic analysis tasks effectively in multiple languages without requiring translation to English.

## Method Summary
The study used 19 semi-structured interview transcriptions in Italian from the Gualandi et al. (2022) dataset. GPT-3.5-Turbo generated initial codes from the interviews, while GPT-4-Turbo identified themes from these codes. Prompts were provided in Italian, and a brief test used equivalent English prompts to compare results. LLM-generated themes were compared with human-coded categories using semantic similarity (SBERT) and human evaluation. The zero-shot prompting strategy relied entirely on the LLM's general language understanding capabilities without task-specific fine-tuning.

## Key Results
- LLM-generated themes from Italian interviews showed semantic similarity scores above 0.6 when compared to human-coded categories
- Human evaluators rated LLM themes highly, scoring 8-10 out of 10 for capturing category meaning
- Prompts in Italian and English produced similar initial codes, indicating multilingual capability
- The approach required no translation of source material and used zero-shot prompting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained LLMs can perform thematic analysis on qualitative interview data in languages other than English without requiring translation.
- Mechanism: The LLM's multilingual training allows it to understand prompts and process content in multiple languages simultaneously, producing comparable thematic structures regardless of prompt language.
- Core assumption: The LLM was pre-trained on sufficient multilingual data, including the target language, enabling semantic understanding and generation in that language.
- Evidence anchors:
  - [abstract]: "A brief test showed prompts in Italian and English yielded similar codes"
  - [section]: "A test for the themes was done with an equivalent prompt in English, showing that the original language and the English prompt can generally produce similar themes"
  - [corpus]: Weak evidence - corpus shows limited related papers on multilingual transfer in thematic analysis

### Mechanism 2
- Claim: Semantic similarity metrics (like SBERT) can effectively compare LLM-generated themes with human-coded categories across languages.
- Mechanism: Semantic similarity models capture latent meaning regardless of exact phrasing or language, allowing cross-language validation of thematic coherence.
- Core assumption: Semantic similarity models are trained on multilingual data and can map concepts across languages to measure meaningful similarity.
- Evidence anchors:
  - [abstract]: "Themes were compared with human-coded categories using semantic similarity; scores above 0.6 indicated strong resemblance"
  - [section]: "Using semantic similarity between the categories from the original analysis and the TA produced here, we could offer an assessment of how well the LLM performs"
  - [corpus]: Weak evidence - corpus contains few papers directly addressing semantic similarity for cross-lingual thematic analysis validation

### Mechanism 3
- Claim: Prompting strategy (zero-shot) is sufficient for thematic analysis when combined with LLMs' general language understanding capabilities.
- Mechanism: The LLM's pre-training on diverse textual material provides enough general knowledge to extract themes without requiring few-shot examples or extensive task-specific fine-tuning.
- Core assumption: The LLM's pretraining corpus includes sufficient examples of thematic discourse patterns and qualitative analysis structures.
- Evidence anchors:
  - [section]: "For TA so far, the author has used a zero-shot prompting strategy, which therefore relies entirely on the LLM capacity to produce a downstream output from a set of instructions"
  - [abstract]: "Using GPT-3.5 and GPT-4, the model produced initial codes and themes from Italian interview data"
  - [corpus]: Weak evidence - corpus shows limited exploration of zero-shot prompting effectiveness for thematic analysis specifically

## Foundational Learning

- Concept: Semantic similarity in NLP
  - Why needed here: To validate LLM-generated themes against human-coded categories across languages
  - Quick check question: What does a cosine similarity score of 0.7 indicate about two text passages?

- Concept: Prompt engineering strategies (zero-shot vs few-shot)
  - Why needed here: To understand why zero-shot prompting was chosen for thematic analysis
  - Quick check question: What's the key difference between zero-shot and few-shot prompting approaches?

- Concept: Thematic analysis methodology (phases and coding)
  - Why needed here: To understand the workflow being automated and evaluated
  - Quick check question: What are the core phases in Braun and Clarke's thematic analysis framework?

## Architecture Onboarding

- Component map: LLM API access → Prompt generation (Italian/English) → Initial coding output → Theme generation → Semantic similarity comparison → Human evaluation
- Critical path: Prompt → LLM processing → Theme output → Validation (semantic similarity + human review)
- Design tradeoffs: Zero-shot prompting simplicity vs potential accuracy gains from few-shot examples; multilingual model capability vs domain-specific fine-tuning requirements
- Failure signatures: Low semantic similarity scores (<0.6); inconsistent themes across language prompts; human evaluators scoring themes poorly
- First 3 experiments:
  1. Test semantic similarity consistency across multiple LLM models on same dataset
  2. Compare zero-shot vs few-shot prompting for thematic analysis quality
  3. Evaluate theme stability when varying temperature parameter in theme refinement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of LLM-generated thematic analysis compare between pre-trained models and fine-tuned models for a specific language?
- Basis in paper: [explicit] The paper notes that fine-tuned open-source models like Llamantino exist for Italian but are harder to use, while commercial pre-trained models are easier and show promising results.
- Why unresolved: The paper only tested pre-trained commercial models and did not compare them with fine-tuned models for Italian.
- What evidence would resolve it: A direct comparison of thematic analysis quality between pre-trained models (like GPT-3.5/4) and fine-tuned models (like Llamantino) on the same Italian dataset.

### Open Question 2
- Question: Does the language of the prompt affect the quality or nature of the initial codes generated by the LLM when analyzing non-English data?
- Basis in paper: [explicit] The paper tested prompts in Italian and English and found similar initial codes, but did not systematically investigate the impact of prompt language on code quality.
- Why unresolved: The paper only briefly tested one interview with English prompts and did not explore broader variations in code quality or content.
- What evidence would resolve it: A systematic study testing multiple interviews with prompts in both Italian and English, analyzing differences in code quality, depth, and relevance.

### Open Question 3
- Question: Can LLM-generated themes from non-English data be reliably compared to human-generated themes in languages other than Italian?
- Basis in paper: [inferred] The paper demonstrated reliable comparison between LLM themes and human-generated categories in Italian using semantic similarity and human evaluation, but did not test other languages.
- Why unresolved: The study focused only on Italian, leaving open whether similar reliability holds for other languages with different linguistic structures.
- What evidence would resolve it: Replicating the study with qualitative data in other languages (e.g., German, Croatian) and comparing LLM themes to human-generated themes using the same evaluation methods.

## Limitations
- Reliance on semantic similarity metrics without direct comparison to bilingual human coders
- Brief test of prompt language effects with limited sample size (one interview)
- Sample size of 19 interviews may not represent full diversity of thematic analysis tasks

## Confidence
- **High confidence**: The LLM can generate initial codes and themes from Italian interview data using zero-shot prompting
- **Medium confidence**: Semantic similarity metrics can effectively compare themes across languages
- **Medium confidence**: Prompting strategy works similarly across Italian and English for this task

## Next Checks
1. Conduct a direct comparison where bilingual human coders evaluate LLM-generated themes in Italian against original human-coded categories
2. Test theme stability across different temperature settings during the theme refinement stage to assess output consistency
3. Evaluate whether few-shot prompting with examples in Italian improves thematic analysis quality compared to zero-shot approaches