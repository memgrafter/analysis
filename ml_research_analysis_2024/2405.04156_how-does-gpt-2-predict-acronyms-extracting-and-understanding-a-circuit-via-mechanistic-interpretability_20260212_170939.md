---
ver: rpa2
title: How does GPT-2 Predict Acronyms? Extracting and Understanding a Circuit via
  Mechanistic Interpretability
arxiv_id: '2405.04156'
source_url: https://arxiv.org/abs/2405.04156
tags:
- letter
- token
- attention
- heads
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work mechanistically investigates how GPT-2 Small predicts
  three-letter acronyms by identifying a circuit composed of 8 attention heads (~5%
  of total heads). The authors discovered that 4 "letter mover" heads copy capital
  letters from words to the corresponding acronym letters, while 4 other heads propagate
  positional information via the causal mask mechanism.
---

# How does GPT-2 Predict Acronyms? Extracting and Understanding a Circuit via Mechanistic Interpretability

## Quick Facts
- arXiv ID: 2405.04156
- Source URL: https://arxiv.org/abs/2405.04156
- Reference count: 32
- This work mechanistically investigates how GPT-2 Small predicts three-letter acronyms by identifying a circuit composed of 8 attention heads (~5% of total heads).

## Executive Summary
This paper mechanistically investigates how GPT-2 Small predicts three-letter acronyms by identifying a specialized circuit of 8 attention heads that copy capital letters from words to corresponding acronym positions. The authors discovered that 4 "letter mover" heads perform the actual letter copying while 4 other heads propagate positional information through attention patterns rather than positional embeddings. Through systematic ablation and activation patching experiments, they demonstrated that this small circuit is sufficient for the task, with performance preserved or slightly improved when isolating these components. This represents the first mechanistic interpretation of multi-token prediction behavior in transformer models.

## Method Summary
The authors use activation patching experiments on GPT-2 Small to identify which components are necessary for acronym prediction. They construct a dataset of 800 acronyms by combining 381 filtered nouns with 1154 three-letter acronyms, where each word tokenizes as two tokens (capital letter + space + remainder) and each acronym letter as a single token. The method involves systematically corrupting different components (current word, previous words, previous predicted letters) and measuring performance changes through logit difference metrics. Circuit discovery proceeds by progressively adding components and mean-ablating non-circuit elements to isolate the 8-head circuit responsible for acronym prediction.

## Key Results
- A circuit of 8 attention heads (~5% of GPT-2's heads) is sufficient for predicting three-letter acronyms
- Four "letter mover" heads (8.11, 10.10, 9.9, 11.4) copy capital letters from words to acronym positions via OV circuit weights
- Letter mover heads use positional information derived from attention patterns to BOS token rather than positional embeddings
- Ablation experiments confirm the circuit's sufficiency, with performance preserved or slightly improved when isolating these 8 heads

## Why This Works (Mechanism)

### Mechanism 1
GPT-2 predicts three-letter acronyms through a specialized circuit of 8 attention heads that copy capital letters from spaced words to unspaced acronym tokens. Four "letter mover" heads (8.11, 10.10, 9.9, 11.4) attend from each predicted acronym letter position to the corresponding capital letter token in the word sequence, copying its content via the OV circuit weights. The OV circuit analysis shows these heads translate capital letters preceded by spaces to capital letters without spaces, writing the result into the residual stream.

### Mechanism 2
The circuit uses positional information derived from attention patterns to the BOS token rather than positional embeddings to identify which capital letter to copy. Due to causal masking, attention to the BOS token decreases with token position. Letter mover heads use this pattern to infer token positions, and swapping BOS attention values causes performance degradation and changes attention patterns, particularly for the first letter prediction.

### Mechanism 3
The circuit concentrates acronym prediction functionality in a small subset of attention heads, making the behavior modular and interpretable. Activation patching experiments show that only 8 attention heads are necessary for acronym prediction, with performance preserved or improved when ablating all other components. This concentration enables mechanistic interpretation of the behavior.

## Foundational Learning

- **Attention head functionality and residual stream operations**
  - Why needed here: Understanding how individual attention heads move information and how the residual stream accumulates these operations is crucial for interpreting the circuit components
  - Quick check question: Can you explain how an attention head's QK matrix determines which tokens to attend to, and how the OV matrix determines what to write into the residual stream?

- **Activation patching methodology for circuit discovery**
  - Why needed here: The entire circuit discovery process relies on systematically replacing activations to identify which components are necessary for the behavior
  - Quick check question: What does it mean if patching a component's activations causes performance degradation, and how does this help locate circuit components?

- **Positional information encoding in transformers**
  - Why needed here: Understanding both positional embeddings and attention-based positional cues is essential for interpreting how the circuit identifies which capital letters to copy
  - Quick check question: How does causal masking create position-dependent attention patterns to the BOS token, and why might this serve as positional information?

## Architecture Onboarding

- **Component map**: GPT-2 Small architecture with 12 layers, each containing 12 attention heads and an MLP. The acronym prediction circuit uses 8 specific attention heads (8.11, 10.10, 9.9, 11.4 as letter movers; 5.8, 4.11, 2.2, 1.0 as supporting heads) while other components can be ablated without performance loss.

- **Critical path**: For predicting the first acronym letter, letter mover head 8.11 attends from A1 to C1, using BOS attention patterns for positional information. For subsequent letters, previous predicted letters provide context, allowing heads like 9.9 and 10.10 to copy from C2→A2 and C3→A3 using both direct attention and information propagated through previous token heads.

- **Design tradeoffs**: The circuit uses attention-based positional information rather than positional embeddings, which may be more robust to position shifts but requires careful attention pattern analysis. The concentration in few heads enables interpretability but may limit generalization to more complex acronym structures.

- **Failure signatures**: If the circuit fails, expect: (1) complete inability to predict any acronym letters when ablating all 8 heads, (2) specific letter prediction failures when ablating individual letter mover heads, (3) position-dependent failures when swapping BOS attention values, particularly for the first letter.

- **First 3 experiments**:
  1. Replicate the activation patching experiments by corrupting the current word and patching residual streams before each layer to confirm the flow of information from Ci to Ai positions.
  2. Perform OV circuit analysis on letter mover heads by passing capital letter embeddings through their OV matrices to verify the copying behavior shown in the paper.
  3. Test the positional information hypothesis by swapping BOS attention values between C1 and C3 and measuring the impact on logit differences and attention patterns.

## Open Questions the Paper Calls Out

- **Open Question 1**: Do letter mover heads also copy positional information (not just the capital letter itself) when predicting acronyms?
- **Open Question 2**: Can the discovered circuit for acronym prediction generalize to predict acronyms of different lengths (e.g., 2-letter or 4-letter acronyms)?
- **Open Question 3**: How does the circuit for acronym prediction interact with other circuits in GPT-2 for different tasks (e.g., predicting common nouns or verbs)?

## Limitations

- The dataset construction method relies on filtering 381 nouns from a larger list and combining them with 1154 possible acronyms, without fully specifying how noun filtering criteria were determined or whether the resulting dataset captures sufficient linguistic diversity.
- The identified circuit successfully predicts three-letter acronyms in the specific dataset format, but it's unclear whether this circuit would generalize to longer acronyms, different capitalization patterns, or more complex linguistic structures.
- While the paper presents evidence that letter mover heads use BOS attention patterns for positional information, the alternative hypothesis that positional embeddings are actually being used is not conclusively ruled out.

## Confidence

- **High Confidence (Mechanistic Claims)**: The existence of a specialized circuit composed of 8 attention heads that predict three-letter acronyms; the sufficiency of these 8 heads for the task, as demonstrated by ablation experiments; the basic letter copying behavior where letter mover heads attend from predicted acronym positions to corresponding capital letters in words.
- **Medium Confidence (Interpretive Claims)**: The specific roles of individual heads within the circuit (letter movers vs. positional information propagators); the interpretation that attention patterns to BOS token provide positional information rather than positional embeddings; the OV circuit analysis showing direct translation of spaced to unspaced capital letters.
- **Low Confidence (Generalization Claims)**: Whether the discovered mechanisms would generalize to other acronym structures or transformer architectures; the claim that this represents a foundational understanding of multi-token prediction behavior; the broader implications for understanding complex transformer behaviors through circuit analysis.

## Next Checks

1. Construct additional acronym prediction datasets with varying complexity: longer acronyms (4-5 letters), different capitalization patterns (mixed case, all caps), and more diverse vocabulary. Test whether the identified circuit generalizes or whether new mechanisms emerge for different patterns.
2. Design a controlled experiment that isolates positional embeddings from attention-based positional cues. Create prompts where attention to BOS is constant across positions while varying positional embeddings, and vice versa. This would definitively determine whether the circuit uses attention patterns or positional embeddings for identifying letter positions.
3. Systematically perturb the identified circuit by: (a) swapping attention patterns between letter mover heads, (b) modifying OV circuit weights, and (c) introducing noise into the residual stream. Measure which perturbations preserve functionality to determine which aspects of the circuit are critical versus redundant, and whether the circuit has redundant pathways or is fragile to perturbations.