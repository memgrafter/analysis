---
ver: rpa2
title: Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology
  Report Generation
arxiv_id: '2407.15268'
source_url: https://arxiv.org/abs/2407.15268
tags:
- report
- multimodal
- radiology
- retriever
- factual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FactMM-RAG, a fact-aware multimodal retrieval-augmented
  pipeline for accurate radiology report generation. It addresses the challenge of
  factual inaccuracy in generated reports by leveraging RadGraph to mine factual report
  pairs and train a universal multimodal retriever.
---

# Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation

## Quick Facts
- **arXiv ID**: 2407.15268
- **Source URL**: https://arxiv.org/abs/2407.15268
- **Reference count**: 15
- **Primary result**: FactMM-RAG achieves up to 6.5% and 2% improvement in F1CheXbert and F1RadGraph metrics on MIMIC-CXR and CheXpert datasets.

## Executive Summary
FactMM-RAG is a fact-aware multimodal retrieval-augmented pipeline designed to improve the accuracy of radiology report generation. It addresses factual inaccuracy in generated reports by leveraging RadGraph to mine factual report pairs and training a universal multimodal retriever. Given a radiology image, the retriever identifies high-quality reference reports to augment multimodal foundation models, enhancing factual completeness and correctness. Experiments demonstrate that the proposed retriever outperforms state-of-the-art models and that the factually-informed training strategy effectively propagates fact-aware capabilities from the retriever to the foundation model without explicit diagnostic label guidance.

## Method Summary
The method involves mining factual report pairs using RadGraph to extract entities and relations from radiology reports, then computing factual similarity between reports. Only report pairs exceeding a similarity threshold are used as positive examples during training the multimodal retriever with contrastive loss. The trained retriever finds high-quality, factually consistent reports that are passed into the foundation model during training, providing strong supervision that encourages the foundation model to align its generation with the retrieved facts rather than hallucinate. The foundation model is then fine-tuned on retrieval-augmented generation using the mined factual pairs as retrieved documents for each query image.

## Key Results
- FactMM-RAG achieves up to 6.5% improvement in F1CheXbert and 2% in F1RadGraph metrics compared to state-of-the-art models.
- The factually-informed training strategy effectively propagates fact-aware capabilities from the retriever to the foundation model without explicit diagnostic label guidance.
- Retrieval quality improvements correlate with downstream generation accuracy gains.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Factually-informed report pair mining improves retrieval quality by grounding the retriever in clinically consistent image-report pairs.
- **Mechanism**: RadGraph extracts entities and relations from reports, factual similarity is computed, and only pairs exceeding a threshold are used as positive examples, injecting factual consistency into embeddings.
- **Core assumption**: Clinically consistent report pairs share similar RadGraph structures and the similarity threshold effectively filters noisy pairs.
- **Evidence anchors**: [abstract] "We first leverage RadGraph to mine factual report pairs..." [section 3.1] "We first leverage RadGraph... to mine factually-oriented report pairs..."
- **Break condition**: If RadGraph fails to extract consistent entities or relations, or if the threshold is too strict/loose, training signal degrades.

### Mechanism 2
- **Claim**: Factually-informed reference reports during RAG finetuning propagate fact-awareness from retriever to foundation model.
- **Mechanism**: Retrieved high-quality, factually consistent reports provide strong supervision, encouraging the foundation model to align generation with retrieved facts.
- **Core assumption**: Foundation model can internalize factual signal from retrieved reports during autoregressive training without explicit labels.
- **Evidence anchors**: [abstract] "employing our factually-informed training strategy imposes an effective supervision signal... and successfully propagates fact-aware capabilities..."
- **Break condition**: If retrieved reports are not factually consistent or foundation model overfits to language patterns, hallucinations persist.

### Mechanism 3
- **Claim**: Fact-aware capability can be controlled and evaluated via similarity thresholds and intermediate retrieval metrics.
- **Mechanism**: Varying F1CheXbert and F1RadGraph thresholds balances recall/precision of mined pairs; MRR tracks retriever ranking and correlates with generation quality.
- **Core assumption**: Factual similarity thresholds and MRR scores are valid proxies for generation accuracy.
- **Evidence anchors**: [section 5.3] "We examine the performance of FactMM-RAG under different thresholds..." [section 5.4] "we use the retrieval metric Mean Reciprocal Rank (MRR)..."
- **Break condition**: If MRR does not correlate with factual correctness or thresholds exclude too many useful pairs, control mechanism fails.

## Foundational Learning

- **Concept**: RadGraph entity and relation extraction
  - **Why needed**: Provides structured representation of reports for factual similarity computation and report pair mining.
  - **Quick check**: What are the two main components RadGraph extracts from free-text reports?
  - **Answer**: Radiological entities (e.g., organs, abnormalities) and clinical relations between them (e.g., modify, located at).

- **Concept**: Multimodal dense retrieval with contrastive learning
  - **Why needed**: Enables retriever to learn joint image-text embeddings that prioritize factually consistent pairs over general semantic similarity.
  - **Quick check**: What loss function is used to train the multimodal retriever?
  - **Answer**: In-batch contrastive loss with hard negatives, minimizing negative log-likelihood over positive report pairs.

- **Concept**: Retrieval-augmented generation (RAG) pipeline
  - **Why needed**: Combines retrieved factual reports with foundation model generation to improve factual accuracy.
  - **Quick check**: What is the main advantage of using retrieved reports during generation instead of generating from scratch?
  - **Answer**: It grounds the generation in factually consistent external knowledge, reducing hallucination.

## Architecture Onboarding

- **Component map**: RadGraph extractor → factual report pair miner → multimodal retriever trainer → RAG finetuner → evaluation pipeline
- **Critical path**: 1. Mine factual report pairs using RadGraph similarity. 2. Train retriever on these pairs with contrastive loss. 3. Retrieve top factual report for query image. 4. Fine-tune foundation model with retrieved report + image. 5. Evaluate generation accuracy using clinical metrics.
- **Design tradeoffs**:
  - Strict vs loose similarity thresholds: Strict improves precision but may reduce recall of useful pairs.
  - In-batch vs hard negatives: Hard negatives improve retrieval but increase training cost.
  - Single vs multiple retrieved reports: Multiple can provide richer context but may introduce conflicting facts.
- **Failure signatures**:
  - Low MRR during training → retriever not learning factual alignment.
  - High BLEU/ROUGE but low F1CheXbert/F1RadGraph → fluent but factually incorrect generations.
  - Training collapse → incorrect loss scaling or too aggressive negative sampling.
- **First 3 experiments**:
  1. Train retriever with varying similarity thresholds (0.2, 0.4, 0.6) and measure MRR/F1RadGraph.
  2. Compare RAG with/without retrieved reports to measure factual gain.
  3. Ablation: swap factual report pairs with random pairs during retriever training and observe downstream impact.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of FactMM-RAG compare to retrieval-based methods that use explicit diagnostic labels during training?
  - **Basis**: The paper states that FactMM-RAG's training strategy "imposes an effective supervision signal, without relying on explicit diagnostic label guidance," suggesting a comparison with methods that do use such labels would be informative.
  - **Why unresolved**: The paper does not directly compare FactMM-RAG to methods that explicitly use diagnostic labels during training.
  - **What evidence would resolve it**: An experiment comparing FactMM-RAG to a similar retrieval-augmented method that uses explicit diagnostic labels during training, evaluating both methods on the same metrics on MIMIC-CXR and CheXpert datasets.

- **Open Question 2**: How does the fact-aware capability of the retriever generalize to other medical imaging domains beyond chest X-rays, such as brain scans or histology images?
  - **Basis**: The paper acknowledges this as a limitation, stating that the work "only emphasizes chest radiology domains" and that exploring the pipeline in "broader medical domains" would be valuable.
  - **Why unresolved**: The paper only evaluates FactMM-RAG on chest X-ray datasets. It is unclear whether the fact-aware training strategy and the RadGraph-based report pair mining would be effective for other medical imaging modalities.
  - **What evidence would resolve it**: Applying FactMM-RAG to other medical imaging datasets (e.g., brain MRI, histology slides) and evaluating its performance on relevant metrics for those domains.

## Limitations

- **Limited ablation studies**: The paper does not provide direct ablations comparing factual report pair mining vs random pairs during retriever training, or RAG with/without retrieved reports during foundation model finetuning.
- **Threshold sensitivity**: The performance impact of different RadGraph similarity thresholds is examined visually, but quantitative sensitivity analysis and correlation with downstream metrics is absent.
- **Evaluation scope**: While the paper reports strong performance on MIMIC-CXR and CheXpert, the generalizability to other medical imaging modalities or institutions is not tested.

## Confidence

- **High confidence**: The factual similarity computation using RadGraph entities and relations is well-grounded in the methodology section, with clear mathematical formulation and implementation details.
- **Medium confidence**: The retrieval quality improvements (MRR scores) and downstream generation accuracy gains (F1CheXbert/F1RadGraph) are demonstrated with quantitative results, though some mechanisms rely on indirect evidence.
- **Low confidence**: The claim that fact-aware capabilities propagate from retriever to foundation model without explicit diagnostic label supervision lacks direct ablation evidence and relies on correlation between training stages.

## Next Checks

1. **Ablation study**: Compare FactMM-RAG performance when training the retriever with factual report pairs versus randomly sampled pairs, and when fine-tuning the foundation model with/without retrieved reports, to isolate the contribution of each component.

2. **Threshold sensitivity analysis**: Systematically vary the RadGraph similarity threshold δ across a wider range (0.1 to 0.9) and measure the correlation between MRR scores and F1CheXbert/F1RadGraph metrics to validate the control mechanism.

3. **Generalization test**: Evaluate FactMM-RAG on a different medical imaging dataset (e.g., NIH ChestX-ray or OpenI) to assess performance outside the training distribution and identify potential domain-specific limitations.