---
ver: rpa2
title: 'Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process'
arxiv_id: '2405.11870'
source_url: https://arxiv.org/abs/2405.11870
tags:
- preference
- arxiv
- optimization
- data
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Intuitive Fine-Tuning (IFT), a novel method
  for aligning large language models with human preferences. The core idea is to unify
  Supervised Fine-Tuning (SFT) and Preference Optimization (PO) into a single process
  by interpreting them within a Markov Decision Process framework.
---

# Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process

## Quick Facts
- arXiv ID: 2405.11870
- Source URL: https://arxiv.org/abs/2405.11870
- Reference count: 15
- This paper proposes Intuitive Fine-Tuning (IFT), a novel method for aligning large language models with human preferences by unifying SFT and PO into a single process.

## Executive Summary
This paper introduces Intuitive Fine-Tuning (IFT), a method that unifies Supervised Fine-Tuning (SFT) and Preference Optimization (PO) into a single alignment process for large language models. By interpreting both methods within a Markov Decision Process framework, IFT addresses the limitation that SFT uses ground-truth tokens as priors while PO estimates preferences from the model's own generation. The core innovation is a temporal residual connection that blends ground-truth and model-predicted embeddings, enabling better preference estimation. Experiments show IFT performs comparably or superiorly to SFT and various PO methods across NLP tasks, with advantages in data and computational efficiency.

## Method Summary
IFT unifies SFT and PO by introducing a temporal residual connection that blends ground-truth tokens with model-predicted embeddings to create priors closer to the model's actual generation distribution. The method uses a single policy and the same volume of non-preference-labeled data as SFT, with a loss function that implicitly satisfies the Bellman equation through cumulative summation across tokens. This approach captures the model's intuitive sense of its entire answer while maintaining the efficiency of SFT and the effectiveness of RLHF-style optimization.

## Key Results
- IFT achieves comparable or superior performance to SFT and various PO methods on NLP benchmarks including ARC-Challenge, MMLU, TruthfulQA, WinoGrande, and GSM8K
- IFT demonstrates advantages in data and computational efficiency, requiring only non-preference-labeled data like SFT
- IFT shows strong performance on generation, reasoning, and fact-following tasks, though relatively weaker on multi-choice tasks
- In the Frozen Lake environment, IFT successfully learns the optimal policy while SFT fails

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SFT's use of ground-truth intermediate states as priors deviates from the model's own token distribution, leading to biased preference estimation.
- Mechanism: The model is trained to predict the next token given a prior sampled from the human reference distribution rather than its own distribution, causing distributional mismatch in transition probabilities.
- Core assumption: The model's own generation distribution differs from the human reference distribution.
- Evidence anchors:
  - [abstract] "PO estimates the model's preference by its entire generation, while SFT only scores model's subsequent predicted tokens based on prior tokens from ground truth answer. These priors deviates from model's distribution..."
  - [section 3] "SFT provides an unbiased estimation of human preference, but a biased estimation for model... Consequently, the Transition Optimization objective of SFT: Tθ(s∗n, s∗n−1) → T∗(s∗n, s∗n−1) secretly sets Tθ(s∗n−1, ρ0) = 1 during aligning Tθ(s∗n, ρ0) with T∗(s∗n, ρ0)."
- Break condition: If the model's own distribution is already aligned with the human reference, the distributional mismatch disappears.

### Mechanism 2
- Claim: IFT's temporal residual connection better approximates the model's entire answer generation preference by blending ground-truth tokens with the model's own predictions.
- Mechanism: By mixing the ground-truth embedding with the model's predicted embedding for the previous token (λ·E(sθ) + (1-λ)·E(s*)), IFT creates a prior that is closer to the model's actual generation distribution, improving preference estimation.
- Core assumption: A convex combination of ground-truth and model-predicted embeddings provides a better prior state for transition optimization.
- Evidence anchors:
  - [section 4.1] "To obtain a prior state estimation ˆsθi closer to model distribution, we introduce a model-based distribution disturbance function δθ for the biased prior state: ˆsθi = δθ(s∗i) = (1 − λ)s∗i + λπθ(s∗i−1)"
  - [section 4.1] "Through this approach, model can predict not only the next token from intermediate state of target answer, but also develop an intuitive sense to the entire answer generation solely based on the initial instruction, deriving more unbiased prior and accurate Preference Estimation for model"
- Break condition: If λ=0 (no model input) or λ=1 (no ground truth), the residual connection becomes pure SFT or pure model prediction respectively.

### Mechanism 3
- Claim: IFT's dynamic relation propagation implicitly satisfies the Bellman equation, enabling effective transition optimization without requiring negative samples.
- Mechanism: The differentiable cumulative summation across tokens in IFT's loss function ensures that gradients flow backward through the entire sequence, accounting for the impact of earlier predictions on later outcomes, similar to TD learning.
- Core assumption: The cumulative loss formulation can approximate Bellman updates in the context of language model alignment.
- Evidence anchors:
  - [section 4.2] "This reformulation implicitly satisfies the Bellman Equation for each state, which guarantees the optimization enjoys both of the effectiveness as RLHF and efficiency as SFT"
  - [section 4.2] "Vθ( ˆsθn) = exp(−L|ˆTθ(s∗n, ρ0)|)" - This is the derived value function from the cumulative loss
- Break condition: If sequence lengths are very short, the Bellman approximation may become less meaningful.

## Foundational Learning

- Concept: Markov Decision Process (MDP) framework
  - Why needed here: The paper frames both SFT and PO as transition optimization problems within an MDP, allowing direct comparison of their estimation mechanisms
  - Quick check question: In the MDP formulation, what does the transition matrix T represent for language models?

- Concept: Preference estimation vs. transition optimization
  - Why needed here: Understanding that alignment involves both estimating which generations the model prefers and then optimizing the transition probabilities to match human preferences is crucial to grasping IFT's contribution
  - Quick check question: How does SFT's preference estimation differ from PO's according to the paper?

- Concept: Bellman equation and temporal difference learning
  - Why needed here: IFT's dynamic relation propagation implicitly satisfies Bellman updates, connecting it to reinforcement learning theory while maintaining SFT's efficiency
  - Quick check question: What aspect of IFT's loss function allows it to implicitly satisfy the Bellman equation?

## Architecture Onboarding

- Component map: Ground truth input → Model inference (one-step ahead) → Embedding fusion (temporal residual) → Token-level loss computation → Cumulative sum (Bellman-style) → Final loss aggregation
- Critical path: Ground truth input → Model inference (one-step ahead) → Embedding fusion (temporal residual) → Token-level loss computation → Cumulative sum (Bellman-style) → Final loss aggregation
- Design tradeoffs: IFT trades some preference estimation accuracy (compared to online DPO) for data efficiency by avoiding negative sampling, while maintaining better estimation than SFT through the residual connection
- Failure signatures: Poor performance on multi-choice tasks while excelling at generation tasks suggests the model may be overfitting to token-by-token generation patterns rather than learning complete answer mappings
- First 3 experiments:
  1. Run IFT with λ=0 (reduces to SFT) and compare performance to verify the residual connection adds value
  2. Run IFT with λ=1 (reduces to pure model prediction) to confirm the need for ground truth supervision
  3. Run IFT without cumulative sum (remove Bellman-style propagation) to validate the importance of temporal credit assignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does IFT scale to extremely large language models (e.g., >100B parameters) in terms of data and computational efficiency compared to SFT and PO methods?
- Basis in paper: [explicit] The paper states that IFT and SFT are the only methods that conduct alignment without preference data, offering significant benefits in terms of memory consumption and training duration. It also mentions the potential for scaling in alignment or even in pre-training.
- Why unresolved: The paper's validation of IFT is limited to the fine-tuning setting with constrained data volume, leaving the scalability of IFT unexplored for extremely large language models.
- What evidence would resolve it: Experiments comparing the data and computational efficiency of IFT, SFT, and PO methods when scaling up to extremely large language models (>100B parameters) on various tasks.

### Open Question 2
- Question: How does the performance of IFT compare to SFT and PO methods when the available data is noisy or of lower quality?
- Basis in paper: [explicit] The paper mentions that IFT with noisy lambda achieves a higher average score on the Open-LLM Leaderboard compared to IFT with a fixed lambda, suggesting that IFT may be more robust to noise in the data.
- Why unresolved: The paper does not provide a comprehensive comparison of IFT's performance with SFT and PO methods under various levels of noise or data quality degradation.
- What evidence would resolve it: Experiments comparing the performance of IFT, SFT, and PO methods on tasks with varying levels of noise or data quality degradation.

### Open Question 3
- Question: How does the dynamic relation propagation mechanism in IFT affect the model's ability to generalize to unseen tasks or domains?
- Basis in paper: [explicit] The paper introduces dynamic relation propagation in IFT to implicitly satisfy the Bellman Equation for each state, which is claimed to guarantee effectiveness similar to RLHF and efficiency similar to SFT.
- Why unresolved: The paper does not provide evidence on how the dynamic relation propagation mechanism in IFT affects the model's generalization ability to unseen tasks or domains.
- What evidence would resolve it: Experiments evaluating the generalization performance of IFT, SFT, and PO methods on a diverse set of tasks or domains that were not seen during training.

## Limitations

- The empirical evaluation lacks extensive ablation studies to isolate the contribution of individual components like the temporal residual connection and dynamic relation propagation
- The LLM-based evaluations (Alpaca-Eval, TL;DR) are subject to known biases in automated alignment assessment
- The Frozen Lake environment experiment, while useful for demonstrating theoretical properties, represents a simplified domain that may not capture the complexity of natural language generation tasks

## Confidence

- **High Confidence**: The mathematical framework connecting SFT and PO through MDP theory is sound and well-articulated. The core mechanism of using temporal residual connections to blend ground-truth and model-predicted embeddings is technically coherent and represents a reasonable approach to improving preference estimation.
- **Medium Confidence**: The empirical claims about performance improvements across benchmarks are reasonably supported by the presented results, though the lack of extensive ablation studies and the limited scope of task types (primarily generation-focused) reduce confidence in generalizability. The data efficiency claims are supported but could benefit from more rigorous comparison.
- **Low Confidence**: The claim that IFT satisfies the Bellman equation implicitly through its dynamic relation propagation is theoretically plausible but requires deeper verification. The mechanism described connects to TD learning principles, but the direct relationship to reinforcement learning theory in the context of language model alignment needs more rigorous validation.

## Next Checks

1. **Ablation Study**: Run IFT with λ=0 (pure SFT) and λ=1 (pure model prediction) conditions to quantify the exact contribution of the temporal residual connection. This would isolate whether the improvement comes from the residual mechanism or other factors in the training procedure.

2. **Sequence Length Sensitivity**: Test IFT's performance on tasks with varying sequence lengths to verify the Bellman equation approximation holds across different contexts. Short sequences might break the temporal credit assignment mechanism that the paper claims provides theoretical benefits.

3. **Generalization Across Task Types**: Evaluate IFT on tasks requiring multi-choice reasoning and fact-following (where the paper notes it performed relatively poorly) alongside generation tasks to understand the conditions under which the temporal residual connection helps versus hinders performance.