---
ver: rpa2
title: 'Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning'
arxiv_id: '2402.12177'
source_url: https://arxiv.org/abs/2402.12177
tags:
- arxiv
- embedding
- performance
- black-box
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of fine-tuning black-box embedding
  models for retrieval augmented generation (RAG) systems, which are limited by their
  inability to access the internal parameters of pre-trained models. The proposed
  Model Augmented Fine-Tuning (Mafin) approach overcomes this limitation by augmenting
  the black-box model with a trainable embedding model, enabling domain-specific fine-tuning.
---

# Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning

## Quick Facts
- arXiv ID: 2402.12177
- Source URL: https://arxiv.org/abs/2402.12177
- Authors: Mingtian Zhang; Shawn Lan; Peter Hayes; David Barber
- Reference count: 13
- The proposed Mafin approach significantly outperforms black-box models and alternative fine-tuning methods on FiQA-2018 and NFCorpus datasets, achieving 3-6% performance gains across recall and NDCG metrics.

## Executive Summary
This paper addresses the challenge of fine-tuning black-box embedding models for retrieval augmented generation (RAG) systems when model parameters are inaccessible. The authors propose Model Augmented Fine-Tuning (Mafin), which augments the black-box model with a trainable embedding model to enable domain-specific fine-tuning. By concatenating the black-box and trainable embeddings with normalization, Mafin achieves significant performance improvements while only requiring training of a small augmented model. The method is effective in both supervised and unsupervised settings, offering a cost-efficient solution for enhancing RAG systems with domain-specific knowledge.

## Method Summary
Mafin addresses the black-box embedding fine-tuning problem by concatenating the output of a pre-trained black-box embedding model with a trainable embedding model, then normalizing the combined representation. The trainable model (eθ) is a small neural network that learns domain-specific adjustments while preserving the strong semantic priors from the black-box model. The method uses either supervised ranking loss or unsupervised LLM-generated queries for training, and employs a probabilistic ranking framework with top-1 approximation for efficient optimization. Experiments validate effectiveness on FiQA-2018 and NFCorpus datasets using OpenAI text-embedding-ada-002 as the black-box model.

## Key Results
- Mafin achieves 3-6% performance gains across various recall and NDCG metrics compared to the original black-box model
- The method outperforms alternative fine-tuning approaches including L2R and DREM on both FiQA-2018 and NFCorpus datasets
- Mafin demonstrates effectiveness in both supervised and unsupervised settings, with unsupervised fine-tuning using LLM-generated queries showing competitive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concatenation of black-box and trainable embeddings increases representational capacity while preserving pre-trained semantic priors
- Mechanism: Normalizing concatenated embeddings and summing cosine similarities leverages both general semantic knowledge and domain-specific adjustments
- Core assumption: Black-box embeddings are normalized and trainable model learns complementary representations
- Evidence anchors: [abstract] "significantly enhances the performance of the black-box embeddings"; [section 2] "increases the representational capacity...while also benefiting from domain-specific adaptability"
- Break condition: If black-box embeddings aren't normalized or trainable model learns conflicting representations

### Mechanism 2
- Claim: Weighted sum formulation provides flexibility by learning relative importance of embedding components
- Mechanism: Weighting parameters derived from trainable embedding norms create adaptive combination based on input characteristics
- Core assumption: Norm of trainable embedding serves as meaningful signal for component importance
- Evidence anchors: [section 2] "norm of the vector as the additional degree of freedom"; [abstract] no direct mention
- Break condition: If norm-based weighting becomes unstable or model ignores one component

### Mechanism 3
- Claim: Probabilistic ranking framework with top-1 approximation provides efficient learning from sparse labels
- Mechanism: Focusing on top-1 ranking probability enables efficient optimization without expensive normalization
- Core assumption: Top-1 approximation is sufficient for learning effective embeddings with sparse relevance labels
- Evidence anchors: [section 3.1.1] "use the sampled sub-list to replace the full-list"; [abstract] "broad applicability and efficiency"
- Break condition: If top-1 approximation fails to capture necessary ranking structure

## Foundational Learning

- Concept: Cosine similarity and embedding normalization
  - Why needed here: Method relies on normalized embeddings where cosine similarity equals dot product for concatenation and weighting mechanisms
  - Quick check question: If two embeddings have L2 norms of 0.5 and 0.8, what is their cosine similarity given their dot product is 0.3?

- Concept: Learning-to-rank and contrastive learning equivalence
  - Why needed here: Understanding probabilistic ranking framework's relationship to contrastive learning explains method effectiveness
  - Quick check question: What is the relationship between minimizing KL divergence in top-1 ranking and maximizing InfoNCE loss?

- Concept: Black-box model constraints and API-based embeddings
  - Why needed here: Method designed for inaccessible parameters, so understanding API-based embedding limitations is crucial
  - Quick check question: What information is typically available from an embedding API, and what is typically not accessible?

## Architecture Onboarding

- Component map: Black-box embedding model -> Trainable embedding model -> Concatenation layer -> Training loop -> Inference pipeline
- Critical path: Query → Black-box embedding → Trainable embedding → Concatenation → Combined embedding → Retrieval
- Design tradeoffs:
  - Small trainable model size vs. adaptation capacity
  - Supervised vs. unsupervised training (data availability vs. cost)
  - Standard concatenation vs. weighted formulation (simplicity vs. flexibility)
  - Top-1 approximation vs. full ranking (efficiency vs. accuracy)
- Failure signatures:
  - No performance improvement over black-box model alone
  - Performance degradation compared to black-box model
  - Training instability or convergence to trivial solutions
  - Large discrepancy between validation and test performance
- First 3 experiments:
  1. Verify that concatenating black-box and trainable embeddings produces normalized outputs as expected
  2. Test method on small labeled dataset to confirm supervised training works before scaling up
  3. Compare performance between standard concatenation and λ-maafin to determine if additional complexity is justified

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Mafin performance scale with different sizes of the trainable embedding model eθ relative to black-box model?
- Basis in paper: [inferred] Paper mentions training only small augmented model but doesn't explore impact of varying eθ size
- Why unresolved: No experiments or analysis on how trainable model size affects performance gains
- What evidence would resolve it: Experiments comparing Mafin performance with different sizes of eθ

### Open Question 2
- Question: Can Mafin be effectively applied to non-text modalities such as images or audio for retrieval tasks?
- Basis in paper: [inferred] Focuses on text retrieval without addressing generalization to other data types
- Why unresolved: Methodology described in text context without addressing other modalities
- What evidence would resolve it: Experiments applying Mafin to image or audio retrieval tasks

### Open Question 3
- Question: What are the long-term effects of using synthetic data generated by LLMs for fine-tuning in unsupervised settings?
- Basis in paper: [explicit] Discusses using LLMs for synthetic queries but doesn't address potential long-term impacts
- Why unresolved: No exploration of sustainability or performance degradation over time with synthetic data
- What evidence would resolve it: Longitudinal studies comparing performance of models fine-tuned with synthetic data over extended periods

## Limitations
- Limited evaluation on only two datasets (FiQA-2018 and NFCorpus) may not generalize to all retrieval scenarios
- Lack of detailed architectural specifications for trainable embedding model eθ makes exact reproduction challenging
- No computational cost or memory requirement analysis provided for fine-tuning process

## Confidence
- **High Confidence**: Core mechanism of concatenating black-box and trainable embeddings with normalization is well-justified and empirical results show consistent improvements
- **Medium Confidence**: Weighted sum formulation shows promise but lacks extensive ablation studies to demonstrate necessity over standard concatenation
- **Medium Confidence**: Unsupervised training approach using LLM-generated queries is innovative but quality of synthetic queries and their impact is not thoroughly evaluated

## Next Checks
1. **Ablation Study**: Conduct systematic ablation experiments comparing standard concatenation vs. λ-maafin weighting, and evaluate impact of trainable model size on performance
2. **Generalization Testing**: Evaluate Mafin on additional benchmark datasets (MS MARCO, Natural Questions) and different embedding models (Sentence-BERT, OpenAI embeddings)
3. **Efficiency Analysis**: Measure and report training time, memory usage, and inference latency for different model configurations to assess computational overhead and practical deployment considerations