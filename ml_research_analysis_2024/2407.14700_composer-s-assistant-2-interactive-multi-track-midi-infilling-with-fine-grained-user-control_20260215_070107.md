---
ver: rpa2
title: 'Composer''s Assistant 2: Interactive Multi-Track MIDI Infilling with Fine-Grained
  User Control'
arxiv_id: '2407.14700'
source_url: https://arxiv.org/abs/2407.14700
tags:
- music
- pitch
- note
- controls
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Composer's Assistant 2, an interactive multi-track
  MIDI infilling system with fine-grained user controls integrated into the REAPER
  digital audio workstation. The system extends the original Composer's Assistant
  by adding a comprehensive set of controls including rhythmic conditioning, horizontal
  and vertical density controls, pitch step/leap propensity controls, pitch range
  controls, and a different-note-onset-chromagram token.
---

# Composer's Assistant 2: Interactive Multi-Track MIDI Infilling with Fine-Grained User Control

## Quick Facts
- arXiv ID: 2407.14700
- Source URL: https://arxiv.org/abs/2407.14700
- Reference count: 0
- Key outcome: F1 scores improve from 32-53% to 76-77% with new control tokens

## Executive Summary
Composer's Assistant 2 is an interactive multi-track MIDI infilling system integrated into the REAPER digital audio workstation that provides fine-grained user control over generated music. Building on the original Composer's Assistant, it introduces a comprehensive set of control tokens including rhythmic conditioning, horizontal and vertical density controls, pitch step/leap propensity controls, pitch range controls, and a different-note-onset-chromagram token. The system uses a T5-like transformer model trained on copyright-free and permissively-licensed MIDI data, achieving dramatic improvements in objective metrics over the baseline system. A listening study with 28 participants found no significant difference between real music and music composed in a co-creative fashion with the system.

## Method Summary
The system trains a T5-like transformer model on masked track-measure infilling tasks with control tokens appended to prompts. Control tokens are added to the MIDI-like token-based language to enable conditioning on user specifications for rhythmic patterns, note densities, pitch characteristics, and chromagram properties. The model, with 512-dimensional embeddings and 16 encoder/decoder layers, is integrated into REAPER DAW where users place empty MIDI items to define infilling regions. During inference, control tokens are generated based on user specifications and the model generates masked tokens conditioned on both the prompt and controls, which are then decoded to MIDI and written back to REAPER.

## Key Results
- F1 scores improve from 32-53% (baseline) to 76-77% with new control tokens
- Model demonstrates good understanding of control tokens with success rates over 50% for all tokens
- 26 out of 34 control tokens achieve over 80% success rate when allowing one-bin tolerance
- Listening study (28 participants) found no significant difference between real music and co-created music

## Why This Works (Mechanism)

### Mechanism 1
- Claim: T5-like transformer architecture enables precise conditioning on user controls for multi-track MIDI infilling
- Mechanism: The model uses a language modeling approach where control tokens are appended or inserted into prompts, allowing the transformer to learn associations between control specifications and musical outputs during training
- Core assumption: Control tokens can be effectively learned as part of the language modeling task without requiring explicit architectural modifications
- Evidence anchors:
  - [abstract] "We train a T5-like transformer model to implement these controls and to serve as the backbone of our system"
  - [section] "We use the MIDI-like token-based language from [6] to tokenize music... We add additional tokens to the language to serve as control tokens"
  - [corpus] Weak evidence - no direct citations about T5 effectiveness for this specific control conditioning task
- Break condition: If control tokens are too ambiguous or overlapping, the model may not learn distinct associations, leading to poor control fidelity

### Mechanism 2
- Claim: Separate horizontal and vertical density controls provide more granular user steering than aggregate density metrics
- Mechanism: By decomposing note density into horizontal (rhythmic) and vertical (polyphonic) components, users can independently control rhythmic speed and chord thickness
- Core assumption: These two dimensions capture the most perceptually relevant aspects of musical density for user control
- Evidence anchors:
  - [section] "In this work, we take a different approach to note density, first by factoring note density into horizontal (rhythmic) and vertical densities"
  - [section] "We define the horizontal note onset density... We define the vertical note onset density..."
  - [corpus] Weak evidence - no direct citations comparing this decomposition approach to alternatives
- Break condition: If users cannot perceive or articulate the difference between horizontal and vertical density, the added complexity may not provide practical benefit

### Mechanism 3
- Claim: 2D rhythmic conditioning enables precise rhythmic control while maintaining pitch coherence
- Mechanism: Masked pitch tokens are paired with rhythmic tokens describing both onset position and pitch class count, allowing the model to generate pitches that fit specific rhythmic patterns
- Core assumption: The model can learn to associate specific pitch patterns with rhythmic templates during training
- Evidence anchors:
  - [section] "For 2D rhythmic conditioning, we include masked pitch tokens describing both the number of note onsets and the number of pitch classes at each onset"
  - [section] "With this option, the user can supply rhythms in their model prompts, and the model chooses only the pitches"
  - [corpus] Weak evidence - no direct citations about 2D rhythmic conditioning effectiveness
- Break condition: If the model cannot generalize pitch patterns to novel rhythmic templates, 2D conditioning may fail for user-specified rhythms outside the training distribution

## Foundational Learning

- Concept: Transformer language modeling for music generation
  - Why needed here: The T5-like transformer architecture treats music as a sequence of tokens, enabling flexible conditioning on user controls through token manipulation
  - Quick check question: How does the model distinguish between prompt tokens and control tokens during inference?

- Concept: Token-based music representation
  - Why needed here: The MIDI-like token language allows arbitrary masking and insertion of control tokens while maintaining musical coherence
  - Quick check question: What happens if control tokens are inserted at positions that create musically invalid sequences?

- Concept: Statistical measurement of musical features
  - Why needed here: User controls are defined by quantifiable musical measurements (density, pitch range, etc.) that must be computed from MIDI data
  - Quick check question: How are pitch steps and leaps defined when chords are involved in the input?

## Architecture Onboarding

- Component map:
  MIDI tokenizer (from CA system) -> Control token registry and quantization logic -> T5-like transformer model (512-dim, 16 encoder/decoder layers) -> REAPER DAW integration scripts -> Evaluation pipeline (F1, pitch class histogram entropy, groove similarity)

- Critical path:
  1. User places empty MIDI items in REAPER to define infilling regions
  2. Control tokens are generated based on user specifications
  3. Model generates masked tokens conditioned on prompt and controls
  4. Generated tokens are decoded to MIDI and written back to REAPER

- Design tradeoffs:
  - Model size (large vs small) vs inference speed and control fidelity
  - Granularity of control quantization (more bins = more precise but harder to learn)
  - REAPER integration vs web-based alternatives for accessibility

- Failure signatures:
  - Control tokens ignored: Outputs show no correlation with specified controls
  - Rhythmic conditioning failure: Generated pitches don't match specified rhythms
  - REAPER integration issues: MIDI items not created or placed incorrectly

- First 3 experiments:
  1. Test control token understanding: Generate empty prompts with single control tokens and verify output characteristics
  2. Validate REAPER integration: Place empty MIDI items and verify generated MIDI appears in correct locations
  3. Benchmark control fidelity: Compare F1 scores with and without specific control tokens on test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the addition of user controls in Composer's Assistant 2 significantly improve the perceived musical quality compared to the original Composer's Assistant system?
- Basis in paper: [explicit] The paper states that a listening study did not find a significant difference between real music and music composed with Composer's Assistant 2, but does not directly compare it to the original Composer's Assistant system.
- Why unresolved: The subjective evaluation in the paper focuses on comparing real music to music composed with Composer's Assistant 2, but does not include a comparison with the original system.
- What evidence would resolve it: Conducting a listening study that directly compares music composed with Composer's Assistant 2 to music composed with the original Composer's Assistant system would provide evidence to answer this question.

### Open Question 2
- Question: How do the newly introduced controls in Composer's Assistant 2 impact the model's ability to generate music that matches user intent compared to the original system?
- Basis in paper: [explicit] The paper introduces a wide range of new controls and claims that they give users fine-grained control over the system's outputs, but does not provide a direct comparison with the original system.
- Why unresolved: While the paper demonstrates the effectiveness of the new controls through objective metrics and model understanding tests, it does not compare the user experience or ability to match user intent between the two systems.
- What evidence would resolve it: Conducting a user study where composers use both the original Composer's Assistant system and Composer's Assistant 2 to create music based on specific user intents would provide evidence to answer this question.

### Open Question 3
- Question: To what extent do the user controls in Composer's Assistant 2 enable composers to steer the model towards generating music in specific styles or genres?
- Basis in paper: [inferred] The paper mentions that the models can be finetuned on a relatively small number of MIDI files to write in the style of those files, but does not explore the impact of user controls on style generation.
- Why unresolved: While the paper demonstrates the effectiveness of the controls in terms of objective metrics and model understanding, it does not investigate how well these controls enable composers to generate music in specific styles or genres.
- What evidence would resolve it: Conducting a study where composers use the user controls in Composer's Assistant 2 to generate music in various styles or genres, and then evaluating the generated music for stylistic accuracy, would provide evidence to answer this question.

## Limitations

- Evaluation Design Concerns: The listening study design may be confounded by priming effects, as participants were exposed to system-generated music before evaluating whether real music was computer-generated, potentially inflating the inability to distinguish between real and co-created music.
- Control Token Effectiveness Variation: While objective metrics show dramatic improvements, success rates for control tokens vary significantly, with only 26 out of 34 tokens achieving over 80% success rate when allowing one-bin tolerance, and pitch range controls performing particularly poorly.
- Implementation Dependency: The system's effectiveness is tightly coupled to REAPER DAW integration, limiting accessibility and reproducibility due to limited technical details about the REAPER integration scripts.

## Confidence

- High Confidence: The dramatic improvement in objective metrics (F1 scores increasing from 32-53% to 76-77%) is well-supported by quantitative evaluation across the test set. The T5-like transformer architecture for token-based music generation with control conditioning is technically sound.
- Medium Confidence: The listening study results showing no significant difference between real and co-created music are methodologically sound but limited by sample size and potential confounding factors. The decomposition of density into horizontal and vertical components is conceptually reasonable but lacks comparative validation.
- Low Confidence: The effectiveness of 2D rhythmic conditioning for maintaining pitch coherence while following user-specified rhythms is weakly supported, with no direct evidence about performance on novel rhythmic templates outside the training distribution.

## Next Checks

1. **Control Token Validation Study**: Conduct a controlled experiment where users specify individual control tokens (e.g., specific density levels or pitch ranges) and verify that generated outputs match the intended specifications. This would directly test whether the model truly understands each control token type rather than just showing aggregate metric improvements.

2. **Cross-Platform Integration Test**: Implement the system outside REAPER (e.g., as a web-based interface) to verify that the core model and control mechanisms work independently of the DAW-specific integration. This would isolate the model's capabilities from the implementation details.

3. **Generalization Evaluation**: Test the system's performance on musical styles and rhythmic patterns not present in the training data, particularly focusing on the 2D rhythmic conditioning capability. This would validate whether the model can generalize pitch patterns to novel rhythmic templates as claimed.