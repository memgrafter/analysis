---
ver: rpa2
title: Evaluating Consistency and Reasoning Capabilities of Large Language Models
arxiv_id: '2404.16478'
source_url: https://arxiv.org/abs/2404.16478
tags:
- llms
- arxiv
- reasoning
- consistency
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates consistency and reasoning capabilities of
  public and proprietary large language models using the Boolq dataset. The authors
  assess consistency by repeating queries and measuring variation in responses, and
  evaluate reasoning by comparing generated explanations to ground truth using BERT,
  BLEU, and F1 scores.
---

# Evaluating Consistency and Reasoning Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2404.16478
- Source URL: https://arxiv.org/abs/2404.16478
- Reference count: 26
- Key outcome: Proprietary models outperform public models in consistency and reasoning, but none achieve 90% in either metric

## Executive Summary
This study evaluates consistency and reasoning capabilities of large language models using the Boolq dataset, comparing public models (Llama-2-7b, Mistral-7b, Mixtral-8x7b) against proprietary models (GPT-4-Turbo, GPT-4, GPT-3.5-Turbo). The authors assess consistency by repeating queries and measuring variation in responses, and evaluate reasoning by comparing generated explanations to ground truth using BERT, BLEU, and F1 scores. The study finds that proprietary models generally outperform public models in both metrics, with inconsistencies primarily occurring when models provide incorrect answers accompanied by wrong explanations. A direct relationship exists between consistency and reasoning abilities, and all models tend to provide answers rather than skipping queries, even when uncertain.

## Method Summary
The study uses zero-shot prompting with a specific format to evaluate models on the Boolq dataset containing 9427 true/false questions with answers and explanations. For consistency evaluation, each query is repeated three times per model and responses are compared to calculate consistency percentages. Reasoning capabilities are assessed by comparing generated explanations to ground truth using BERT, BLEU, and F1 scores. The evaluation covers both public models (Llama-2-7b, Mistral-7b, Mixtral-8x7b) and proprietary models (GPT-4-Turbo, GPT-4, GPT-3.5-Turbo).

## Key Results
- Proprietary models (GPT-4-Turbo, GPT-4, GPT-3.5-Turbo) outperform public models in both consistency and reasoning metrics
- No model achieves 90% consistency or reasoning capability
- Inconsistencies primarily occur with incorrect answers accompanied by wrong explanations
- A direct correlation exists between consistency and reasoning capabilities
- All models tend to provide answers rather than skipping queries, even when uncertain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Proprietary models outperform public models in consistency due to richer internal reasoning contexts
- Mechanism: Proprietary models are trained with more diverse and higher-quality datasets, allowing them to maintain contextual awareness during answer generation, reducing inconsistencies
- Core assumption: The quality and diversity of training data directly correlates with a model's ability to maintain consistency in reasoning
- Evidence anchors:
  - [abstract] "proprietary models generally outperform public models in terms of both consistency and reasoning capabilities"
  - [section] "The inconsistency observed in both public and proprietary models occurred when they provided incorrect answers initially...This might mean that these models have less context about the questions when they provide wrong answers"
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.421" (weak corpus support, FMR score only moderately high)

### Mechanism 2
- Claim: Consistency and reasoning capabilities are directly correlated in LLMs
- Mechanism: When a model lacks reasoning capability for a specific query, it fails to maintain contextual awareness, leading to inconsistent answers across multiple attempts
- Core assumption: Reasoning capability is a prerequisite for maintaining contextual consistency in responses
- Evidence anchors:
  - [abstract] "This study underscores the direct correlation between consistency and reasoning abilities in LLMs"
  - [section] "There is a direct relationship that exists between the consistency and reasoning capabilities of these models"
  - [corpus] "Evaluating the quality and variability of text generated by Large Language Models (LLMs) poses a significant, yet unresolved research challenge" (indirect support from related work)

### Mechanism 3
- Claim: Models tend to answer rather than skip queries even when uncertain, leading to hallucinated explanations
- Mechanism: LLMs are optimized for completion and prediction, creating a bias toward generating answers even when lacking sufficient information, resulting in fabricated explanations
- Core assumption: The training objective of next-token prediction inherently creates a completion bias that overrides uncertainty acknowledgment
- Evidence anchors:
  - [abstract] "All models tend to provide answers rather than skipping queries, even when uncertain"
  - [section] "Moreover, these models don't 'skip' the queries/questions and tend to give wrong answers even after explicitly mentioning in the prompt"
  - [corpus] "Large Language Models (LLMs) are widely used in both industry and academia for various tasks, yet evaluating the consistency of generated text responses continues to be a challenge" (indirect support)

## Foundational Learning

- Concept: Zero-shot prompting and instruction-following
  - Why needed here: The paper uses zero-shot prompts to evaluate models without fine-tuning, making understanding prompt engineering critical
  - Quick check question: What are the key components of an effective zero-shot prompt for consistency evaluation?

- Concept: Semantic similarity metrics (BERT Score, BLEU, F1)
  - Why needed here: These metrics are used to evaluate reasoning capabilities by comparing generated explanations to ground truth
  - Quick check question: How does BERT Score differ from BLEU in measuring explanation quality?

- Concept: Consistency measurement methodology
  - Why needed here: Understanding how consistency is quantified (same query repeated, response comparison) is essential for interpreting results
  - Quick check question: What formula is used to calculate consistency percentage in this study?

## Architecture Onboarding

- Component map:
  - Boolq dataset -> Prompt generator -> LLM API interfaces -> Consistency checker -> Reasoning evaluator -> Result aggregator

- Critical path:
  1. Load Boolq dataset
  2. Generate prompts for each question
  3. Send prompts to all target LLMs
  4. Repeat each query three times per model
  5. Compare answers for consistency
  6. Generate and compare explanations using metrics
  7. Aggregate results by model type

- Design tradeoffs:
  - Zero-shot vs. few-shot prompting: Zero-shot provides cleaner evaluation but may disadvantage models that benefit from examples
  - Metric selection: BERT captures semantics but may miss surface-level differences that BLEU catches
  - Query repetition: Three repetitions balance statistical significance with resource usage

- Failure signatures:
  - High inconsistency percentage (>20%): Likely reasoning or contextual awareness issues
  - Low BLEU but high BERT: Explanations are semantically similar but use different phrasing
  - All models skipping queries: Dataset may contain questions outside model knowledge

- First 3 experiments:
  1. Test consistency on a single model with 10 easy Boolq questions, three repetitions each
  2. Compare BERT and BLEU scores on a subset of explanations to understand metric differences
  3. Run the full pipeline on one proprietary and one public model to validate the complete evaluation workflow

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do language models exhibit consistency differences when evaluating factual versus opinion-based questions in the BoolQ dataset?
- Basis in paper: [explicit] The paper evaluates consistency using true/false questions from BoolQ, noting that inconsistencies occur primarily with incorrect answers.
- Why unresolved: The study does not differentiate between factual and opinion-based questions or analyze whether consistency patterns vary by question type.
- What evidence would resolve it: A breakdown of consistency metrics by question type (factual vs. opinion-based) would reveal whether certain question categories lead to more inconsistent responses.

### Open Question 2
- Question: How do different prompting strategies affect the reasoning capabilities and consistency of language models beyond the zero-shot prompt used in this study?
- Basis in paper: [explicit] The paper uses a zero-shot prompt and notes that models tend to provide answers rather than skipping queries, suggesting prompting limitations.
- Why unresolved: The study employs only one prompting approach, leaving unexplored how alternative prompt designs might influence model performance.
- What evidence would resolve it: Comparative experiments using multiple prompting strategies (few-shot, chain-of-thought, etc.) would demonstrate whether different approaches improve reasoning and consistency.

### Open Question 3
- Question: What specific factors contribute to the high BERT scores but low BLEU scores observed across all models in this study?
- Basis in paper: [explicit] The authors note that BERT scores remain high while BLEU scores are very low, indicating semantic similarity but low n-gram overlap.
- Why unresolved: The paper observes this discrepancy but does not investigate the underlying causes of this pattern across models.
- What evidence would resolve it: Detailed linguistic analysis of generated explanations versus ground truth, including examination of paraphrasing patterns and semantic equivalence, would clarify why this score pattern emerges.

## Limitations

- Boolq dataset represents a narrow slice of question-answering tasks, limiting generalizability
- Zero-shot evaluation approach may not fully capture models' capabilities under different prompting strategies
- Correlation between consistency and reasoning could be influenced by shared measurement artifacts rather than genuine model properties

## Confidence

- Consistency evaluation methodology: Medium - well-defined but may have implementation variations
- Reasoning capability measurement: Medium - established metrics but potential metric-specific biases
- Correlation between consistency and reasoning: Medium - observed relationship but causal mechanisms unclear
- Proprietary vs. public model performance: Medium - clear trends but dataset limitations

## Next Checks

1. Replicate the consistency calculation using different threshold values to assess sensitivity of the consistency percentage metric
2. Test the same models with few-shot prompting to determine if zero-shot bias affects the proprietary/public performance gap
3. Evaluate models on a diverse set of reasoning tasks beyond Boolq to validate whether the consistency-reasoning correlation generalizes