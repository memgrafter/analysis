---
ver: rpa2
title: 'Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only
  LLMs for Sequence Labeling'
arxiv_id: '2401.14556'
source_url: https://arxiv.org/abs/2401.14556
tags:
- tasks
- arxiv
- llms
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the use of decoder-only large language models
  (LLMs) for sequence labeling tasks. The authors propose a layer-wise causal mask
  removal approach during fine-tuning to enable bidirectional context.
---

# Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only LLMs for Sequence Labeling

## Quick Facts
- arXiv ID: 2401.14556
- Source URL: https://arxiv.org/abs/2401.14556
- Authors: David Dukić; Jan Šnajder
- Reference count: 19
- Primary result: Layer-wise causal mask removal improves sequence labeling performance, with optimal configurations being task-dependent and benefits scaling with model size

## Executive Summary
This paper investigates why decoder-only LLMs struggle with sequence labeling tasks and proposes a solution through selective causal mask removal during fine-tuning. The authors demonstrate that removing the causal mask from specific decoder layers enables bidirectional context for sequence labeling without sacrificing autoregressive generation capabilities. Their approach outperforms standard causal masking and achieves results competitive with encoder baselines, though the optimal unmasking configuration is task-dependent. The benefits are primarily observed in larger models (>7B parameters), with smaller models showing no improvement or degradation.

## Method Summary
The authors propose a layer-wise causal mask removal approach during fine-tuning of decoder-only LLMs for sequence labeling tasks. They group the 32 decoder blocks into 4 layer groups of 8 blocks each and explore 16 different unmasking configurations. During fine-tuning, the causal mask is selectively removed from specific layer groups, allowing bidirectional attention in those layers while maintaining autoregressive capabilities in others. The models are fine-tuned using QLoRA for parameter-efficient adaptation, and performance is evaluated using micro F1 score on IOB2 tag predictions with strict matching.

## Key Results
- Selective causal mask removal improves sequence labeling performance compared to standard causal masking
- Optimal unmasking configurations are task-dependent, with some tasks benefiting from deeper layer unmasking
- On 7B parameter models, the approach outperforms causal masking and matches encoder baseline performance
- Smaller 68M parameter models show no improvement or degradation with causal mask removal, suggesting benefits scale with model size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing the causal mask from specific decoder layers enables bidirectional context for sequence labeling tasks.
- Mechanism: The causal mask restricts attention to past tokens only, preventing the model from attending to future tokens. By selectively removing this mask in certain layers, the model gains access to bidirectional context without losing autoregressive generation capabilities in other layers.
- Core assumption: Some layers can be made bidirectional while others remain causal without degrading overall model performance.
- Evidence anchors:
  - [abstract] "We hypothesize that LLMs' poor SL performance stems from causal masking, which prevents the model from attending to tokens on the right of the current token."
  - [section 3] "Building on insight from Li et al. (2023), we hypothesize that removing the CM from all decoder layers may not be beneficial for all SL tasks."
  - [corpus] "Sequence Repetition Enhances Token Embeddings and Improves Sequence Labeling with Decoder-only Language Models" - suggests sequence labeling benefits from bidirectional context.
- Break condition: If bidirectional layers are too close to input or output, the model may lose either generation capability or context understanding.

### Mechanism 2
- Claim: The optimal layer-wise causal mask removal configuration is task-dependent.
- Mechanism: Different sequence labeling tasks have varying dependencies on right-side context. Tasks with high right-side dependency ratios benefit more from deeper layer unmasking.
- Core assumption: The position of unmasked layers correlates with the task's right-side dependency requirements.
- Evidence anchors:
  - [abstract] "The optimal unmasking configuration is task-dependent."
  - [section 5.1] "We observe that unmasking layers closer to the model's output yields higher gains than unmasking layers closer to the model's input, except for ATE+ATP and TC tasks."
  - [corpus] Weak evidence - the corpus provides related papers but doesn't directly address task-dependent unmasking configurations.
- Break condition: If task dependency ratios are uniform or low, the specific configuration becomes less important.

### Mechanism 3
- Claim: The benefits of causal mask removal scale with model size.
- Mechanism: Larger models have more capacity to handle bidirectional attention in some layers while maintaining generation quality in others. Small models lack this capacity, resulting in no performance gains.
- Core assumption: Model capacity determines the ability to benefit from partial bidirectionality.
- Evidence anchors:
  - [abstract] "On smaller-scale models, causal mask removal does not yield gains, suggesting the benefits are due to model scale."
  - [section 5.4] "Removing the CM on the 68M parameters scale produces no gains. On average, Decoder Unmask performs worse than Decoder Mask."
  - [corpus] "When are 1.58 bits enough? A Bottom-up Exploration of BitNet Quantization" - suggests model efficiency and scale are critical for performance.
- Break condition: If model architecture changes significantly (e.g., different attention mechanisms), the scaling relationship may not hold.

## Foundational Learning

- Concept: Causal attention mask
  - Why needed here: Understanding how the causal mask works is fundamental to grasping why its removal helps sequence labeling.
  - Quick check question: What happens to the attention scores when the causal mask contains -∞ values?
  - Answer: Softmax converts -∞ to 0 attention weights, effectively blocking attention to future tokens.

- Concept: Layer-wise vs. block-wise operations
  - Why needed here: The paper groups layers into blocks for unmasking, which is crucial for understanding the experimental design.
  - Quick check question: Why did the authors choose to group layers into blocks rather than unmask individual layers?
  - Answer: To reduce the search space from 2^n to 2^m configurations, making the problem computationally tractable.

- Concept: Sequence labeling vs. sequence classification
  - Why needed here: The paper contrasts these two task types to explain why decoders struggle with sequence labeling.
  - Quick check question: How does the causal mask affect token-level classification differently than sequence-level classification?
  - Answer: Sequence labeling requires bidirectional context for each token, while sequence classification can use the final token's representation, which still has access to the full context.

## Architecture Onboarding

- Component map: Input sequence → Token embedding → 32 decoder blocks (with selective causal mask removal) → Sequence labeling head → Token predictions

- Critical path: Input sequence → Token embedding → 32 decoder blocks (with selective causal mask removal) → Sequence labeling head → Token predictions

- Design tradeoffs: Full causal mask removal enables bidirectional context but breaks generation capability. Selective removal preserves some generation ability while enabling context for labeling. The tradeoff is finding the optimal configuration per task.

- Failure signatures: Poor performance on sequence labeling tasks, especially when the task has high right-side dependency. Inconsistent results between validation and test sets may indicate overfitting to specific unmasking configurations.

- First 3 experiments:
  1. Test causal mask removal in all layers (configuration 1111) on a simple NER dataset to establish baseline improvement.
  2. Test causal mask in all layers (configuration 0000) as a control condition.
  3. Test selective unmasking in the top layer group only (configuration 1000) to see if depth matters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does removing the causal mask from specific decoder blocks improve sequence labeling performance while potentially harming other tasks like aspect term extraction and polarity?
- Basis in paper: [explicit] The authors observe that removing the causal mask from all layers (configuration 1111) can sometimes yield lower scores compared to selective removal, and specifically note that ATE+ATP task shows better performance when the causal mask is preserved in all layer groups.
- Why unresolved: The paper provides empirical observations but does not offer a theoretical explanation for why certain tasks benefit from bidirectional context while others are harmed by it. The relationship between task characteristics and optimal masking configuration remains unclear.
- What evidence would resolve it: A comprehensive analysis comparing task characteristics (like the right-side dependency ratio) with optimal masking configurations across a broader range of tasks, combined with attention pattern analysis to understand how context flow affects different task types.

### Open Question 2
- Question: Does the benefit of causal mask removal scale with model size, or is there a threshold below which it becomes detrimental?
- Basis in paper: [explicit] The authors conduct experiments comparing small-scale (68M parameters) models with larger models (7B parameters) and find that causal mask removal produces no gains and can even be detrimental at the small scale.
- Why unresolved: The paper only tests two scales (small and 7B parameters) without exploring intermediate sizes or identifying the exact threshold where benefits begin to appear. The relationship between model scale and the effectiveness of causal mask removal is not fully characterized.
- What evidence would resolve it: Systematic experiments testing models of varying sizes (e.g., 100M, 500M, 1B, 3B parameters) to identify the exact scale at which causal mask removal begins to show benefits, and analysis of how attention patterns and contextual representations change across scales.

### Open Question 3
- Question: Can the optimal causal mask removal configuration be predicted without extensive fine-tuning of all possible configurations?
- Basis in paper: [inferred] The authors note that correlations between validation and test configurations are high, suggesting the optimal configuration can be determined using the validation set, but they don't explore methods to predict the optimal configuration without exhaustive search.
- Why unresolved: While the paper shows that validation performance correlates with test performance for different configurations, it doesn't investigate whether there are more efficient methods to predict the best configuration, such as using representation similarity analysis or other model-based approaches.
- What evidence would resolve it: Development and validation of predictive methods (like Fisher information-based approaches or representation similarity metrics) that can accurately predict the optimal causal mask configuration without requiring exhaustive fine-tuning of all possible configurations.

## Limitations
- Optimal unmasking configuration is task-dependent, requiring exploration of multiple configurations per task
- Benefits are primarily observed in larger models (>7B parameters), with smaller models showing no improvement
- Computational cost of exploring multiple configurations significantly increases fine-tuning overhead
- Focus on English datasets limits generalizability to multilingual or low-resource scenarios

## Confidence
**High Confidence:** The core finding that selective causal mask removal improves sequence labeling performance compared to standard causal masking in decoder-only models is well-supported by experimental results across multiple datasets and tasks. The observation that benefits scale with model size is also strongly supported by direct comparisons between 7B and 68M parameter models.

**Medium Confidence:** The claim that optimal unmasking configurations are task-dependent is supported by the data but requires more systematic analysis to establish definitive patterns. The assertion that decoder models can match encoder baselines through this technique is supported but based on a limited set of encoder models (RoBERTa variants).

**Low Confidence:** The specific mechanisms explaining why certain layer groups benefit more than others remain speculative. The paper suggests depth matters but doesn't provide a clear theoretical framework for predicting optimal configurations. The generalizability to other decoder-only architectures beyond Llama2 and Mistral remains untested.

## Next Checks
1. **Cross-architecture validation:** Test the layer-wise causal mask removal approach on additional decoder-only architectures (e.g., GPT-3 variants, OPT models) to verify that the benefits are not specific to Llama2 and Mistral architectures.

2. **Multilingual extension:** Evaluate the approach on non-English sequence labeling datasets to assess cross-lingual generalization and determine if the task-dependent configuration patterns hold across languages.

3. **Generation capability assessment:** Systematically evaluate whether the selective causal mask removal affects the model's generation capabilities by testing perplexity on held-out text and comparing generation quality between masked and unmasked variants.