---
ver: rpa2
title: Evaluating and Enhancing LLMs for Multi-turn Text-to-SQL with Multiple Question
  Types
arxiv_id: '2412.17867'
source_url: https://arxiv.org/abs/2412.17867
tags:
- question
- questions
- text-to-sql
- types
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MMSQL, a comprehensive benchmark for evaluating
  LLMs on multi-turn text-to-SQL tasks across diverse question types including answerable,
  ambiguous, unanswerable, and improper queries. The study assesses eight popular
  LLMs and finds that while closed-source models like GPT-4 Turbo achieve the highest
  overall performance (TDEX: 67.0), all models struggle significantly with ambiguous
  and unanswerable questions.'
---

# Evaluating and Enhancing LLMs for Multi-turn Text-to-SQL with Multiple Question Types

## Quick Facts
- **arXiv ID**: 2412.17867
- **Source URL**: https://arxiv.org/abs/2412.17867
- **Reference count**: 40
- **Primary result**: Proposed multi-agent framework improves LLM performance on multi-turn text-to-SQL tasks, with Llama3-70B achieving 70.7 TDEX (vs 62.8 baseline) and GPT-4 Turbo improving to 70.0 TDEX (vs 67.0 baseline)

## Executive Summary
This paper addresses the challenge of multi-turn text-to-SQL tasks by introducing MMSQL, a comprehensive benchmark that evaluates LLMs across four question types: answerable, ambiguous, unanswerable, and improper. The study finds that while closed-source models like GPT-4 Turbo achieve the highest overall performance (67.0 TDEX), all models struggle significantly with ambiguous and unanswerable questions. To address these limitations, the authors propose a multi-agent framework that decomposes complex questions, generates multiple interpretations for ambiguous queries, and refines SQL outputs. Experiments demonstrate this framework improves performance across all tested models, with Llama3-70B achieving a 7.9 point increase in TDEX and GPT-4 Turbo improving by 3.0 points.

## Method Summary
The paper evaluates eight popular LLMs (including GPT-4 Turbo, Llama3-70B, and various open-source models) on multi-turn text-to-SQL tasks using the newly proposed MMSQL benchmark. The benchmark includes 6,493 training dialogues and 149 test dialogues across four question types. Models are evaluated using zero-shot inference with greedy decoding. The authors then introduce a multi-agent framework comprising specialized agents for question detection, schema selection, question decomposition, and SQL refinement. This framework employs a Question Detector to identify question types and determine answering strategies, a Schema Selector to focus on relevant database schema components, a Question Decomposer to break down complex questions, and a SQL Refiner to verify and improve generated queries.

## Key Results
- GPT-4 Turbo achieves the highest overall performance (TDEX: 67.0) but struggles with ambiguous questions (TDEX: 58.0) and unanswerable questions (TDEX: 31.8)
- Llama3-70B shows competitive performance (TDEX: 62.8), narrowing the gap with closed-source alternatives
- The multi-agent framework improves performance across all tested models, with Llama3-70B achieving a 7.9 point increase in TDEX (from 62.8 to 70.7)
- All models demonstrate significant room for improvement in handling ambiguous and unanswerable questions
- Closed-source models generally outperform open-source models, though the gap is narrower than in previous studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-agent framework improves performance by decomposing complex questions into simpler sub-questions and generating multiple SQL interpretations for ambiguous queries.
- Mechanism: The framework uses a Question Detector to identify question types and decide on answering strategies. For ambiguous questions, it generates multiple possible interpretations and corresponding SQL queries. The Question Decomposer breaks down complex questions into manageable sub-questions, each with its own sub-SQL. The SQL Refiner then verifies and refines these queries to ensure accuracy.
- Core assumption: Decomposing complex questions and generating multiple interpretations for ambiguous queries leads to more accurate and reliable SQL generation.
- Evidence anchors:
  - [abstract]: "we introduce an LLM-based multi-agent framework that employs specialized agents to identify question types and determine appropriate answering strategies."
  - [section]: "For ambiguous questions, where the query cannot be precisely mapped to the database schema, the system explains the ambiguity and attempts to rewrite it into answerable forms."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.494, average citations=0.2. Top related titles: QDA-SQL: Questions Enhanced Dialogue Augmentation for Multi-Turn Text-to-SQL, SynTQA: Synergistic Table-based Question Answering via Mixture of Text-to-SQL and E2E TQA, Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL.
- Break condition: If the decomposition process introduces too much noise or if the multiple interpretations for ambiguous queries confuse the system rather than clarify, the framework's performance may degrade.

### Mechanism 2
- Claim: The MMSQL benchmark provides a comprehensive evaluation of LLMs' ability to handle diverse question types in multi-turn text-to-SQL tasks.
- Mechanism: MMSQL includes four question types: answerable, ambiguous, unanswerable, and improper. It evaluates models on their ability to correctly classify these question types and generate appropriate responses, including SQL queries for answerable and ambiguous questions, and natural language responses for unanswerable and improper questions.
- Core assumption: A comprehensive benchmark that includes diverse question types and evaluates both classification and response generation is necessary to assess the true capabilities of LLMs in real-world text-to-SQL scenarios.
- Evidence anchors:
  - [abstract]: "we propose MMSQL, a comprehensive test suite designed to evaluate the question classification and SQL generation capabilities of LLMs by simulating real-world scenarios with diverse question types and multi-turn Q&A interactions."
  - [section]: "MMSQL aims to assess the effectiveness of language models in managing multi-type and multi-turn text-to-SQL tasks."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.494, average citations=0.2. Top related titles: QDA-SQL: Questions Enhanced Dialogue Augmentation for Multi-Turn Text-to-SQL, SynTQA: Synergistic Table-based Question Answering via Mixture of Text-to-SQL and E2E TQA, Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL.
- Break condition: If the benchmark does not accurately represent real-world scenarios or if it fails to include a diverse enough set of question types, it may not effectively evaluate the capabilities of LLMs.

### Mechanism 3
- Claim: The Schema Selector component improves performance by focusing the model on the most relevant parts of the database schema.
- Mechanism: The Schema Selector identifies the essential subset of the database schema necessary for answering a given question. By selectively focusing on the most relevant tables and columns, it reduces noise and improves the accuracy of SQL generation.
- Core assumption: Focusing on the most relevant parts of the database schema improves the accuracy of SQL generation by reducing noise and confusion from irrelevant data.
- Evidence anchors:
  - [section]: "The Schema Selector is designed to identify the essential subset of a database schema necessary for answering a given question."
  - [section]: "By selectively focusing on the most relevant tables and columns, the Selector curtails the interference from extraneous data, enhancing operational accuracy."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.494, average citations=0.2. Top related titles: QDA-SQL: Questions Enhanced Dialogue Augmentation for Multi-Turn Text-to-SQL, SynTQA: Synergistic Table-based Question Answering via Mixture of Text-to-SQL and E2E TQA, Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL.
- Break condition: If the Schema Selector incorrectly identifies irrelevant parts of the schema as relevant or vice versa, it may lead to decreased performance by either missing important information or including too much noise.

## Foundational Learning

- Concept: Question classification and intent recognition
  - Why needed here: To accurately determine the type of question (answerable, ambiguous, unanswerable, improper) and apply the appropriate answering strategy.
  - Quick check question: Can you explain the difference between an answerable and an ambiguous question in the context of text-to-SQL?

- Concept: SQL generation and refinement
  - Why needed here: To generate accurate SQL queries based on natural language questions and refine them to ensure correctness.
  - Quick check question: What are the key components of a SQL query and how do they relate to the elements of a natural language question?

- Concept: Multi-turn dialogue and context understanding
  - Why needed here: To maintain context across multiple turns of dialogue and understand how earlier questions and answers relate to the current question.
  - Quick check question: How would you handle a situation where a user asks a follow-up question that references information from a previous turn?

## Architecture Onboarding

- Component map: Question Detector → Schema Selector → Question Decomposer → SQL Refiner → LLM Models
- Critical path: Question Detector → Schema Selector → Question Decomposer → SQL Refiner → LLM Models
- Design tradeoffs:
  - Tradeoff between comprehensiveness and efficiency: Including more components (e.g., additional agents) could improve performance but may also increase computational cost and complexity.
  - Tradeoff between accuracy and speed: More thorough SQL refinement may lead to more accurate queries but could also slow down the response time.
- Failure signatures:
  - Incorrect question type classification leading to inappropriate answering strategies.
  - Schema Selector selecting irrelevant parts of the schema, leading to incorrect SQL generation.
  - Question Decomposer failing to break down complex questions effectively, resulting in incomplete or incorrect sub-questions.
  - SQL Refiner failing to catch errors in the generated SQL queries.
- First 3 experiments:
  1. Evaluate the performance of the framework on a subset of the MMSQL benchmark with only answerable questions to assess the core SQL generation capabilities.
  2. Test the framework's ability to handle ambiguous questions by comparing its performance with and without the Question Decomposer component.
  3. Measure the impact of the Schema Selector by comparing the framework's performance on datasets with varying schema sizes and complexities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would performance change if the MMSQL dataset included more diverse question types beyond the four currently covered (answerable, ambiguous, unanswerable, improper)?
- Basis in paper: [explicit] The paper mentions MMSQL includes four question types and provides distribution details, but acknowledges this may not cover all real-world scenarios
- Why unresolved: The paper focuses on these four types as a foundation but doesn't explore whether additional categories (like clarification requests, comparative questions, or follow-up refinements) would improve model robustness
- What evidence would resolve it: Performance comparison studies showing how models trained/tested on expanded question type taxonomies perform relative to the current four-type framework

### Open Question 2
- Question: What is the optimal balance between schema selectivity and information availability in the Schema Selector component?
- Basis in paper: [inferred] The paper describes the Schema Selector as choosing relevant tables/columns but doesn't provide systematic analysis of how much schema pruning is optimal before performance degrades
- Why unresolved: While the paper mentions the selector activates when schema size exceeds thresholds, it doesn't empirically determine the ideal pruning ratio or investigate whether aggressive vs. conservative selection strategies perform better across different database complexities
- What evidence would resolve it: Ablation studies varying schema selection ratios and measuring corresponding performance impacts across multiple database sizes and domains

### Open Question 3
- Question: How would the multi-agent framework perform if open-source LLMs were used for all agent components instead of relying on closed-source models?
- Basis in paper: [explicit] The paper acknowledges that the current framework relies on closed-source LLMs and suggests exploring open-source alternatives, but doesn't implement this
- Why unresolved: The paper uses GPT-4 Turbo for the baseline and doesn't evaluate how the framework performs when all agents use open-source models, which is important for deployment considerations
- What evidence would resolve it: Head-to-head comparisons of the framework using different combinations of closed-source vs. open-source models for each agent component, measuring both performance and cost-effectiveness

## Limitations

- The evaluation relies heavily on zero-shot inference without fine-tuning, which may not fully represent the models' potential capabilities
- The TDEX metric combines type detection and execution accuracy, making it difficult to isolate the impact of improvements in each component
- The benchmark focuses on specific question types and includes only 149 test dialogues, potentially limiting generalizability to real-world scenarios

## Confidence

- **High confidence**: The overall ranking of models (GPT-4 Turbo > Llama3-70B > other open-source models) and the consistent improvement from the multi-agent framework across all models.
- **Medium confidence**: The specific performance gains (e.g., 7.9 point increase for Llama3-70B) due to potential sensitivity to implementation details and prompt engineering.
- **Low confidence**: The generalizability of results to real-world scenarios given the benchmark's focus on specific question types and the limited number of test dialogues (149).

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically vary the prompt templates and system messages for each component of the multi-agent framework to assess the robustness of the reported performance improvements.

2. **Cross-Benchmark Validation**: Evaluate the models on an independent multi-turn text-to-SQL benchmark to verify if the performance trends and model rankings hold across different datasets.

3. **Error Analysis**: Conduct a detailed error analysis on the types of questions (answerable, ambiguous, unanswerable, improper) that each model struggles with most, to identify specific areas for improvement in both the models and the multi-agent framework.