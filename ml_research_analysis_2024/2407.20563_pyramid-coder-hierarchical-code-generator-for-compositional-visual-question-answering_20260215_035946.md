---
ver: rpa2
title: 'Pyramid Coder: Hierarchical Code Generator for Compositional Visual Question
  Answering'
arxiv_id: '2407.20563'
source_url: https://arxiv.org/abs/2407.20563
tags:
- code
- answer
- query
- pyramidcoder
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PyramidCoder, a novel prompting framework
  for compositional visual question answering (VQA). PyramidCoder employs a hierarchical
  approach consisting of three levels: query rephrasing, code generation, and answer
  aggregation.'
---

# Pyramid Coder: Hierarchical Code Generator for Compositional Visual Question Answering

## Quick Facts
- arXiv ID: 2407.20563
- Source URL: https://arxiv.org/abs/2407.20563
- Authors: Ruoyue Shen; Nakamasa Inoue; Koichi Shinoda
- Reference count: 0
- Key outcome: Achieves 0.5-2.9% accuracy gains over state-of-the-art programmatic VQA models on GQA, VQAv2, and NLVR2 datasets

## Executive Summary
This paper introduces PyramidCoder, a novel prompting framework for compositional visual question answering that employs a three-level hierarchical approach: query rephrasing, code generation, and answer aggregation. Using a single frozen large language model with predefined prompts at each level, PyramidCoder generates multiple code candidates to maximize solution diversity for a given input query. The framework demonstrates significant improvements over state-of-the-art programmatic VQA models without requiring additional training or fine-tuning.

## Method Summary
PyramidCoder decomposes the VQA task into three sequential LLM-based stages using a single frozen LLM. First, the query rephraser generates multiple semantically equivalent variations of the input query. Second, the code generator produces multiple code candidates for each rephrased query, leveraging in-context examples and API descriptions. Third, the answer aggregator selects the final answer based on semantic compatibility between queries and candidate answers rather than simple majority voting. The framework interfaces with pre-defined image processing APIs for object detection, image cropping, and captioning, executing all generated code candidates to obtain answers before final selection.

## Key Results
- Achieves at least 0.5% accuracy gains over state-of-the-art programmatic VQA models on GQA dataset
- Improves VQAv2 performance by 1.4% compared to baseline methods
- Demonstrates 2.9% accuracy improvement on NLVR2 dataset
- Shows consistent performance gains across all three compositional VQA benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical decomposition through three sequential LLM prompts improves compositional VQA performance by enabling diverse solution exploration
- Mechanism: The PyramidCoder framework decomposes the VQA task into three distinct LLM-based stages—query rephrasing, code generation, and answer aggregation. This hierarchical structure allows the model to first explore diverse linguistic formulations of the query, then generate multiple code solutions for each formulation, and finally select the best answer through semantic matching rather than simple voting
- Core assumption: A frozen LLM with well-designed prompts can effectively handle all three stages of the compositional VQA pipeline without additional fine-tuning
- Evidence anchors:
  - [abstract]: "PyramidCoder consists of three hierarchical levels, each serving a distinct purpose: query rephrasing, code generation, and answer aggregation"
  - [section]: "The PyramidCoder framework consists of three modules: a query rephraser R, a code generator G, and an answer aggregator A"
  - [corpus]: Weak corpus evidence—no directly comparable hierarchical prompting frameworks found

### Mechanism 2
- Claim: Query rephrasing generates diverse linguistic formulations that lead to more robust code generation and reduce single-path failure modes
- Mechanism: The query rephraser module dynamically reformulates input queries into multiple semantically equivalent expressions. This diversity allows the subsequent code generator to approach the problem from multiple linguistic angles, increasing the likelihood of finding a correct code solution
- Core assumption: Different linguistic formulations of the same query can activate different reasoning paths in the LLM, leading to more diverse and potentially correct code solutions
- Evidence anchors:
  - [abstract]: "The first level rephrases a given query into multiple variations"
  - [section]: "The query rephraser module in PyramidCoder is designed to enhance the interpretability and robustness of the model's comprehension of input queries"
  - [corpus]: Weak corpus evidence—no direct studies on query rephrasing for VQA found

### Mechanism 3
- Claim: Answer aggregation through semantic compatibility matching outperforms simple majority voting by selecting the answer that best aligns with the query's intent
- Mechanism: Rather than using majority voting, the answer aggregator employs LLM capabilities to select the final answer based on semantic compatibility between queries and candidate answers. This two-step process first selects the final answer, then identifies the best-matching code that produced it
- Core assumption: LLMs can effectively evaluate semantic compatibility between natural language queries and candidate answers to make better selection decisions than statistical voting methods
- Evidence anchors:
  - [abstract]: "The third level aggregates them and generates the final code and answer for the input query"
  - [section]: "In response to these limitations, the answer aggregator utilizes the capabilities of LLMs to select the final output, considering not only the frequency but also the semantic compatibility between the query and potential answers"
  - [corpus]: Weak corpus evidence—no direct studies on semantic-based answer aggregation for VQA found

## Foundational Learning

- Concept: Large Language Model prompting techniques (Chain-of-Thought, few-shot learning)
  - Why needed here: PyramidCoder relies heavily on carefully crafted prompts for each of its three modules. Understanding how to design effective prompts, including the use of demonstrations and structured reasoning steps, is essential for implementing and potentially improving this framework
  - Quick check question: What is the difference between input-output prompting and chain-of-thought prompting, and why does the paper suggest IO prompting leads to "simplistic and uniform code generation"?

- Concept: Programmatic VQA and code generation for visual reasoning
  - Why needed here: The framework generates executable Python code that uses image processing APIs to answer questions. Understanding how programmatic VQA works, including the relationship between natural language queries, generated code, and visual processing modules, is crucial for grasping PyramidCoder's approach
  - Quick check question: How does the CodeVQA baseline model differ from PyramidCoder in terms of code generation strategy, and what limitation does this create?

- Concept: Hierarchical problem decomposition and parallel solution exploration
  - Why needed here: The pyramid structure represents a hierarchical decomposition where each level adds a layer of abstraction and exploration. Understanding how hierarchical decomposition can improve problem-solving and how parallel exploration of multiple solutions can enhance robustness is key to understanding why this architecture works
  - Quick check question: Why does generating multiple code candidates for each rephrased query (rather than just one) potentially improve accuracy, and what computational tradeoff does this create?

## Architecture Onboarding

- Component map: Query → Rephraser (produces N variations) → Code Generator (produces M codes per variation) → Python execution (generates answers) → Answer Aggregator (selects final answer and code)
- Critical path: The most computationally intensive path is generating and executing all M×N code candidates, with the LLM prompt evaluation at each stage potentially creating bottlenecks for very long queries
- Design tradeoffs: The framework trades computational cost (generating and executing many code candidates) for accuracy gains through solution diversity. Using a single frozen LLM provides flexibility but may limit performance compared to specialized fine-tuned models. The three-stage hierarchy adds complexity but enables more robust reasoning
- Failure signatures: Poor performance may manifest as: (1) All code candidates failing execution, indicating issues with prompt design or API understanding; (2) Aggregator consistently selecting default answers, suggesting semantic matching is ineffective; (3) Minor accuracy improvements despite the complex architecture, indicating the hierarchical decomposition isn't providing sufficient benefit
- First 3 experiments:
  1. Baseline comparison: Implement the default IO prompting approach from CodeVQA as a baseline, using the same frozen LLM and APIs, to establish the performance delta
  2. Ablation study: Test the system with one module disabled at a time (no rephraser, no generator diversity, no aggregator) to measure each component's contribution to overall performance
  3. Cross-dataset evaluation: Test the trained system on all three datasets (GQA, VQAv2, NLVR2) to assess generalization and identify dataset-specific strengths or weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PyramidCoder perform on VQA datasets with longer, more complex queries compared to existing methods?
- Basis in paper: [explicit] The paper demonstrates effectiveness on GQA, VQAv2, and NLVR2 datasets but does not explore performance on datasets with longer, more complex queries
- Why unresolved: The paper focuses on datasets with relatively shorter queries and does not investigate how the framework handles more intricate query structures
- What evidence would resolve it: Conducting experiments on datasets with longer and more complex queries, such as TextVQA or OCR-VQA, and comparing the performance of PyramidCoder with other methods would provide insights into its scalability and effectiveness in handling complex queries

### Open Question 2
- Question: Can PyramidCoder be adapted to handle multimodal inputs beyond images and text, such as videos or audio?
- Basis in paper: [inferred] The paper's framework is designed for visual question answering using images and text, suggesting potential adaptability to other multimodal inputs
- Why unresolved: The paper does not explore the framework's applicability to other modalities, leaving the question of its versatility open
- What evidence would resolve it: Testing PyramidCoder on multimodal datasets involving videos or audio, such as AVSD or ActivityNet, and evaluating its performance would demonstrate its adaptability to different input types

### Open Question 3
- Question: How does the diversity of code solutions generated by PyramidCoder affect its performance in real-world applications?
- Basis in paper: [explicit] The paper highlights the framework's ability to generate diverse code solutions but does not assess the impact of this diversity on real-world applications
- Why unresolved: The paper focuses on synthetic datasets and does not investigate how the diversity of solutions translates to practical use cases
- What evidence would resolve it: Applying PyramidCoder to real-world VQA applications, such as customer service chatbots or educational tools, and analyzing the effectiveness of diverse code solutions in these contexts would provide insights into its practical impact

## Limitations

- The evaluation is limited by incomplete disclosure of critical implementation details including exact prompt templates and specific few-shot examples
- The reported accuracy improvements are measured against a single baseline (CodeVQA), making it difficult to assess whether gains stem from the hierarchical architecture or better prompt engineering
- The computational cost of generating and executing M×N code candidates is substantial but not quantified in terms of inference time or memory requirements

## Confidence

- **High confidence**: The hierarchical decomposition approach is technically sound and the conceptual framework for query rephrasing → code generation → answer aggregation is well-founded in existing LLM prompting literature
- **Medium confidence**: The reported accuracy improvements of 0.5-2.9% over CodeVQA baseline are likely real but may be partially attributable to prompt engineering rather than the hierarchical architecture itself
- **Low confidence**: The claim that semantic compatibility matching in the answer aggregator consistently outperforms majority voting across diverse query types is not fully validated

## Next Checks

1. **Implementation fidelity validation**: Recreate the baseline CodeVQA model using the exact same frozen LLM, APIs, and few-shot examples specified in the paper, then compare against PyramidCoder's reported improvements to isolate the contribution of the hierarchical architecture versus prompt engineering differences

2. **Computational cost benchmarking**: Measure and report the inference time and memory usage for generating and executing the full set of M×N code candidates, then analyze the accuracy-improvement-to-computational-cost ratio to assess practical viability for deployment scenarios

3. **Component ablation study**: Systematically disable each of the three modules (rephraser, generator diversity, aggregator) in controlled experiments to quantify the marginal contribution of each component to overall performance, particularly testing whether the semantic-based answer aggregation provides statistically significant gains over simple majority voting across all datasets