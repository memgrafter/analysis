---
ver: rpa2
title: Provably Efficient Infinite-Horizon Average-Reward Reinforcement Learning with
  Linear Function Approximation
arxiv_id: '2409.10772'
source_url: https://arxiv.org/abs/2409.10772
tags:
- linear
- regret
- learning
- lemma
- proceedings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a computationally efficient algorithm, LSVI-DC,
  for learning infinite-horizon average-reward linear and linear mixture Markov decision
  processes (MDPs) under the Bellman optimality condition. The algorithm achieves
  the best-known regret upper bound of $\tilde{O}(d^{3/2}\mathrm{sp}(v^)\sqrt{T})$
  for linear MDPs and $\tilde{O}(d\cdot\mathrm{sp}(v^)\sqrt{T})$ for linear mixture
  MDPs, where $d$ is the feature dimension, $\mathrm{sp}(v^)$ is the span of the optimal
  bias function, and $T$ is the time horizon.
---

# Provably Efficient Infinite-Horizon Average-Reward Reinforcement Learning with Linear Function Approximation

## Quick Facts
- arXiv ID: 2409.10772
- Source URL: https://arxiv.org/abs/2409.10772
- Authors: Woojin Chae; Dabeen Lee
- Reference count: 40
- One-line primary result: LSVI-DC achieves regret bounds of $\tilde{O}(d^{3/2}\mathrm{sp}(v^*)\sqrt{T})$ for linear MDPs and $\tilde{O}(d\cdot\mathrm{sp}(v^*)\sqrt{T})$ for linear mixture MDPs

## Executive Summary
This paper presents a computationally efficient algorithm, LSVI-DC, for learning infinite-horizon average-reward linear and linear mixture Markov decision processes (MDPs) under the Bellman optimality condition. The algorithm achieves the best-known regret upper bounds by combining optimistic value iteration with discounting, clipping operations, and max-pooling steps. The key insight is to approximate the average-reward MDP using a discounted MDP with discount factor $\gamma = 1 - 1/\sqrt{T}$, while controlling the span of value functions through clipping to ensure bounded regret.

## Method Summary
The LSVI-DC algorithm operates in two phases: planning and execution. During planning, it runs optimistic value iteration for a discounted MDP using least-squares estimation of transition coefficients. The execution phase uses a max-pooled action-value function based on the current state to select actions. For linear MDPs, it estimates transition coefficients $w$ using least squares, while for linear mixture MDPs, it estimates parameters $\theta$ using value-targeted regression. The algorithm applies clipping to constrain the span of value functions and max-pooling to maintain monotonicity, both critical for achieving tight regret bounds.

## Key Results
- Achieves $\tilde{O}(d^{3/2}\mathrm{sp}(v^*)\sqrt{T})$ regret for linear MDPs
- Achieves $\tilde{O}(d\cdot\mathrm{sp}(v^*)\sqrt{T})$ regret for linear mixture MDPs
- Introduces clipping operation to control span of value functions
- Employs max-pooling to maintain monotonicity of value functions across time steps

## Why This Works (Mechanism)

### Mechanism 1: Clipping Operation
- Claim: The clipping operation constrains the span of the value function to be bounded above by H, critical for handling weakly communicating MDPs and ensuring bounded regret.
- Mechanism: After each value iteration, the algorithm clips the value function $V_k^{(n+1)}$ to be no greater than $\min\{V_k^{(n+1)}(s), \min_{s'} V_k^{(n+1)}(s') + H\}$. This enforces that the span of the value function does not exceed H.
- Core assumption: The clipping threshold H is chosen as an upper bound on $2\cdot\mathrm{sp}(v^*)$, where $v^*$ is the optimal bias function from the Bellman optimality condition.
- Evidence anchors: [abstract] "novel techniques to control the covering number of the value function class and the span of optimistic estimators of the value function"; [section 4] "The clipping operation given in line 8... it is clear that the span of $V_k^{(n+1)}$ becomes bounded above as $\mathrm{sp}(V_k^{(n+1)}) \leq H$"; [corpus] Weak/no direct evidence from neighbors, but Hong et al. (2024) is cited as inspiration for the clipping idea.

### Mechanism 2: Max-Pooling Step
- Claim: The max-pooling step in the execution phase ensures that value functions are monotonically non-decreasing across time steps, which is necessary for the regret decomposition to telescope correctly.
- Mechanism: In each time step t during execution, the algorithm chooses an action based on the maximum over a set of value functions: $Q_t(s_t, a) = \max_{n\in[T-t+1:N_k]} \tilde{Q}_k^{(n)}(s_t, a)$. This selection depends on the current state, ensuring that the chosen value function is at least as large as previous ones.
- Core assumption: The value functions generated during the planning phase are such that later rounds (larger n) have larger or equal values, and the max-pooling operation selects an appropriate index that maintains this monotonicity.
- Evidence anchors: [section 4] "Given state $s_t$ in time step t, we choose an action based on the action-value function $Q_t(s_t, a)$, which is given as the maximum of $\tilde{Q}_k^{(n)}(s_t, a)$ over $n \in [T-t+1:N_k]$"; [section 5.2] "$V_k^{(\tau_t-1)}(s_{t+1}) \leq Q_{t+1}(s_{t+1}, a_{t+1})$ for any $t \in [t_k : t_{k+1}-1)$"; [section 5.3] "This implies that $V_k^{(\tau_t-1)}(s_{t+1}) \leq Q_{t+1}(s_{t+1}, a_{t+1})$"

### Mechanism 3: Discounting Approximation
- Claim: The algorithm approximates the infinite-horizon average-reward MDP by a discounted-reward MDP with discount factor $\gamma = 1 - 1/\sqrt{T}$, bridging the gap between average and discounted settings.
- Mechanism: By running optimistic value iteration on a discounted MDP with the chosen $\gamma$, the algorithm leverages known bounds on the difference between average and discounted rewards. The regret is decomposed into terms involving the discounted value function and the difference between average and discounted rewards.
- Core assumption: As $\gamma$ approaches 1, the discounted cumulative reward converges to the average reward for stationary policies, and the span of the optimal discounted value function is bounded by $2\cdot\mathrm{sp}(v^*)$.
- Evidence anchors: [section 3] "Our approach is to approximate an average-reward MDP by a discounted-reward MDP. In fact, as the discount factor gets close to 1, the discounted cumulative reward converges to the average reward for a stationary policy"; [section 5.2] "Lemma 5.8... $\max_{s\in S} |J^* - (1-\gamma)V^*(s)| \leq (1-\gamma)\mathrm{sp}(v^*)$"; [corpus] No direct evidence from neighbors, but this is a standard technique in the literature.

## Foundational Learning

- Concept: Bellman Optimality Condition
  - Why needed here: The algorithm is designed for MDPs satisfying the Bellman optimality condition, which guarantees the existence of optimal average reward $J^*$ and bias function $v^*$ such that for all $(s,a)$, $J^* + q^*(s,a) = r(s,a) + E_{s'\sim P(\cdot|s,a)}[v^*(s')]$ and $v^*(s) = \max_a q^*(s,a)$.
  - Quick check question: What is the Bellman optimality condition for infinite-horizon average-reward MDPs, and why is it important for this algorithm?

- Concept: Linear and Linear Mixture MDPs
  - Why needed here: The algorithm handles both linear MDPs (where transition probabilities and rewards are linear in feature mappings over state-action pairs) and linear mixture MDPs (where rewards are linear in state-action features and transitions are linear in state-action-state triplet features). Understanding these structures is crucial for the parameter estimation steps.
  - Quick check question: How do linear MDPs differ from linear mixture MDPs in terms of their feature mappings and parameterization?

- Concept: Covering Numbers and Self-Normalization
  - Why needed here: The regret analysis relies on controlling the covering number of the value function class and applying self-normalization inequalities to bound estimation errors. These are essential for the concentration results on the estimated parameters.
  - Quick check question: Why is controlling the covering number of the value function class important for the concentration of the estimated parameters in this algorithm?

## Architecture Onboarding

- Component map: Planning Phase -> Clipping Operation -> Max-Pooling during Execution -> Regret Accumulation
- Critical path: Planning phase (value iteration with parameter estimation) → Clipping operation → Max-pooling during execution → Regret accumulation and analysis
- Design tradeoffs: The clipping operation simplifies implementation compared to constrained optimization but requires choosing H appropriately. The max-pooling step avoids the issue of exploding covering numbers in linear MDPs but is more complex than simply taking the minimum of value functions.
- Failure signatures: If the clipping threshold H is too small, value estimates may be overly conservative. If the max-pooling index selection fails to maintain monotonicity, the regret bound may not telescope correctly. If parameter estimation confidence bounds are too loose, the optimistic value iteration may not be truly optimistic.
- First 3 experiments:
  1. Implement the algorithm for a simple tabular MDP and verify that the clipping operation correctly bounds the span of the value function.
  2. Test the max-pooling step on a small linear MDP instance to ensure that it maintains the monotonicity of value functions across time steps.
  3. Run the full algorithm on a synthetic linear MDP with known parameters and compare the empirical regret to the theoretical bound.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the regret bounds for linear and linear mixture MDPs be further tightened to close the gap with the best-known lower bounds?
- Basis in paper: [explicit] The paper acknowledges a gap between the upper and lower bounds for both linear and linear mixture MDPs.
- Why unresolved: The current analysis relies on standard techniques like covering numbers and self-normalization inequalities, which may not be tight enough.
- What evidence would resolve it: A refined analysis using variance-aware parameter estimation schemes or weighted ridge regression techniques could potentially lead to tighter bounds.

### Open Question 2
- Question: Is the max-pooling step in LSVI-DC essential for achieving tight regret bounds in linear MDPs, or can it be replaced with an alternative approach?
- Basis in paper: [inferred] The paper mentions that the max-pooling step is crucial to avoid issues with the covering number exploding in linear MDPs, but it's not strictly necessary for linear mixture MDPs.
- Why unresolved: The paper does not provide a detailed comparison of the max-pooling step with alternative approaches.
- What evidence would resolve it: An empirical or theoretical study comparing the performance of LSVI-DC with and without the max-pooling step in linear MDPs could shed light on its necessity.

### Open Question 3
- Question: Can the clipping operation in LSVI-DC be further optimized to achieve better performance or computational efficiency?
- Basis in paper: [explicit] The paper highlights the clipping operation as a novel component that controls the span of value functions, but it does not explore alternative clipping strategies.
- Why unresolved: The paper focuses on proving the effectiveness of the clipping operation rather than exploring its potential for optimization.
- What evidence would resolve it: An analysis of different clipping strategies and their impact on regret bounds and computational efficiency could provide insights into potential improvements.

## Limitations

- Requires prior knowledge of upper bound on optimal bias function span for setting clipping threshold H
- Computational complexity scales with O(√T) planning iterations, potentially expensive for very large T
- Analysis assumes access to exact feature mappings and bounded parameter spaces

## Confidence

- **High**: The regret bounds for linear mixture MDPs (O(d·sp(v*)√T)) are well-supported by the analysis and leverage established techniques from the literature.
- **Medium**: The regret bound for linear MDPs (O(d^(3/2)·sp(v*)√T)) relies on more novel technical contributions, particularly the max-pooling operation and its analysis.
- **Medium**: The computational efficiency claim depends on the practical implementation of the max-pooling step, which is not fully detailed in the paper.

## Next Checks

1. **Empirical validation of clipping threshold**: Implement the algorithm with varying choices of H relative to the true optimal bias function span, and measure how sensitive regret performance is to this hyperparameter choice.

2. **Verification of max-pooling monotonicity**: Design a synthetic linear MDP where the theoretical guarantee of monotonicity can be explicitly tested, and verify that the max-pooling operation maintains the required properties.

3. **Comparison with baseline algorithms**: Implement and compare against alternative approaches for infinite-horizon average-reward RL (such as those in Dann et al., 2017 or Jin et al., 2020) on standard benchmark MDPs to assess practical performance differences.