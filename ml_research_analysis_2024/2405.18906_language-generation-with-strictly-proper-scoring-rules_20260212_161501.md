---
ver: rpa2
title: Language Generation with Strictly Proper Scoring Rules
arxiv_id: '2405.18906'
source_url: https://arxiv.org/abs/2405.18906
tags:
- score
- scoring
- rules
- proper
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using alternative strictly proper scoring rules
  (Brier score, Spherical score) for training language generation models, instead
  of the commonly used log-likelihood loss. The authors propose a token-level scoring
  strategy to adapt non-local scoring rules and introduce score smoothing for regularization.
---

# Language Generation with Strictly Proper Scoring Rules

## Quick Facts
- arXiv ID: 2405.18906
- Source URL: https://arxiv.org/abs/2405.18906
- Reference count: 40
- Primary result: Fine-tuning LLaMA-7B with Brier or Spherical scores improved WMT22 translation tasks by over 3 BLEU points

## Executive Summary
This paper explores using alternative strictly proper scoring rules (Brier score, Spherical score) for training language generation models, instead of the commonly used log-likelihood loss. The authors propose a token-level scoring strategy to adapt non-local scoring rules and introduce score smoothing for regularization. Experiments show that fine-tuning models pre-trained with log-likelihood using Brier or Spherical scores can improve generation capabilities, especially for large language models like LLaMA-7B and LLaMA-13B. On WMT22 translation tasks, fine-tuning LLaMA-7B with Brier or Spherical scores achieved average improvements of over 3 BLEU points. The authors also find that fine-tuning with different scoring rules leads to better performance than fine-tuning with the same score, suggesting complementarity between scoring rules.

## Method Summary
The authors adapt strictly proper scoring rules for language modeling by decomposing sequence-level scores into token-level components, enabling the use of non-local scoring rules like Brier and Spherical scores. They implement a token-level scoring strategy where each token's probability is evaluated independently given its context, and introduce score smoothing to enable label smoothing for arbitrary scoring rules. The method involves fine-tuning pre-trained language models (LLaMA-7B and LLaMA-13B) using these adapted scoring rules instead of the standard cross-entropy loss, with evaluation on machine translation and abstractive summarization tasks.

## Key Results
- Fine-tuning LLaMA-7B with Brier or Spherical scores improved WMT22 translation tasks by over 3 BLEU points
- Score smoothing provides regularization benefits when fine-tuning with alternative scoring rules
- Fine-tuning with different scoring rules leads to better performance than fine-tuning with the same score, suggesting complementarity between scoring rules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The token-level scoring rule adaptation allows non-local strictly proper scoring rules (like Brier and Spherical) to be used for language modeling by distributing the scoring over individual token predictions.
- Mechanism: The scoring rule is decomposed into a sum of token-level scores, where each token's probability is evaluated independently given its context. This makes the computation tractable for large vocabularies and long sequences, while preserving the strictly proper property at the sequence level.
- Core assumption: If each token-level conditional probability is well-calibrated, the product of these probabilities will yield a well-calibrated sequence probability.
- Evidence anchors:
  - [abstract]: "We propose a straightforward strategy for adapting scoring rules to language generation, allowing for language modeling with any non-local scoring rules."
  - [section 3.2]: "We distribute the scoring rule at the token level to promote well-calibrated prediction of conditional probabilities at each time step."
  - [corpus]: Weak evidence - related papers discuss proper scoring rules for classification and regression, but none directly address token-level decomposition for language modeling.
- Break condition: If the autoregressive decomposition introduces bias in the conditional distributions, the global sequence probability may become miscalibrated despite well-calibrated token-level predictions.

### Mechanism 2
- Claim: Fine-tuning pre-trained models with different strictly proper scoring rules can improve generation capabilities due to complementarity between scoring rules.
- Mechanism: Different strictly proper scoring rules follow unique optimization trajectories toward the same global optimum. Fine-tuning with a different rule helps the model escape from potentially suboptimal regions in the loss landscape, leading to improved generation performance.
- Core assumption: The optimization trajectories of different strictly proper scoring rules are sufficiently distinct to provide complementary information about the model parameters.
- Evidence anchors:
  - [abstract]: "Experimental results indicate that simply substituting the loss function... can yield substantial improvements in model's generation capabilities."
  - [section 4.3]: "We conjecture that such improvements stem from the complementarity between scoring rules. As different scoring rules follow unique optimization trajectories towards the same global optimum..."
  - [corpus]: No direct evidence in corpus - related papers discuss scoring rules for evaluation but not for complementary fine-tuning strategies.
- Break condition: If the scoring rules have very similar optimization landscapes, fine-tuning with a different rule may not provide significant benefit.

### Mechanism 3
- Claim: Score smoothing enables label smoothing for arbitrary strictly proper scoring rules by modifying the expected score to be maximized at a smoothed distribution.
- Mechanism: The smoothed score is defined as a convex combination of the original score and the average score over all labels. This ensures the expected score is maximized when the model produces the desired smoothed distribution, preserving the strictly proper property.
- Core assumption: The smoothed score remains strictly proper if the original score is strictly proper, and the smoothing term is sufficiently weighted to encourage the desired distribution.
- Evidence anchors:
  - [abstract]: "We further introduce score smoothing to enable honest label smoothing for arbitrary scoring rules."
  - [section 3.3]: "We define a smoothed score as proper if the expected score satisfies Sϵ(p, q) ≤ Sϵ(qϵ, q), and it is strictly proper when the equality only holds at p = qϵ."
  - [corpus]: Weak evidence - related papers discuss proper scoring rules for uncertainty quantification but not for label smoothing in language modeling.
- Break condition: If the smoothing term is too small relative to the original score, the model may ignore the smoothing effect and fail to produce the desired smooth distribution.

## Foundational Learning

- Concept: Strictly proper scoring rules
  - Why needed here: They provide a principled way to train and evaluate probabilistic models by ensuring the model reports true probabilities.
  - Quick check question: What property must a scoring rule have to be considered strictly proper, and why is this important for training probabilistic models?

- Concept: Token-level vs sequence-level scoring
  - Why needed here: Token-level scoring makes non-local scoring rules computationally tractable for language modeling while preserving their theoretical properties.
  - Quick check question: How does decomposing a sequence-level scoring rule into token-level components affect its computation and theoretical properties?

- Concept: Autoregressive decomposition of sequence probabilities
  - Why needed here: It allows the model to predict sequences by decomposing the joint probability into conditional probabilities, enabling the use of token-level scoring rules.
  - Quick check question: What is the mathematical form of autoregressive decomposition, and how does it enable token-level scoring in language modeling?

## Architecture Onboarding

- Component map: Transformer model -> Loss function module (logarithmic, Brier, or Spherical score) -> Token-level scoring implementation -> Score smoothing module (optional) -> Fine-tuning pipeline

- Critical path:
  1. Load pre-trained model
  2. Replace loss function with alternative scoring rule
  3. Implement token-level scoring
  4. Apply score smoothing if needed
  5. Fine-tune on target dataset
  6. Evaluate generation capabilities

- Design tradeoffs:
  - Computational efficiency vs. scoring rule complexity
  - Model performance vs. hyperparameter tuning requirements
  - Flexibility vs. implementation complexity

- Failure signatures:
  - Degraded performance on pre-training tasks after fine-tuning
  - Unstable training with certain scoring rules
  - Poor generation quality despite improved scoring metrics

- First 3 experiments:
  1. Fine-tune a pre-trained model with Brier score and compare to log-likelihood baseline
  2. Fine-tune a pre-trained model with Spherical score and compare to log-likelihood baseline
  3. Fine-tune a pre-trained model with score smoothing and compare to unsmoothed baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do strictly proper scoring rules exhibit varying performance when training language generation models from scratch, despite sharing the same global optimum?
- Basis in paper: [explicit] The authors observe that different scoring rules (logarithmic, Brier, spherical) show noticeable differences in convergence speed and final performance when training from scratch, even though they all have the same optimum of p = q.
- Why unresolved: The paper suggests two factors - different learning dynamics and hyperparameter settings - but doesn't provide a definitive explanation for the performance differences.
- What evidence would resolve it: Comparative experiments analyzing the gradient characteristics and optimization trajectories of different scoring rules during training, along with systematic hyperparameter tuning for each score.

### Open Question 2
- Question: Can other strictly proper scoring rules outperform the logarithmic score as evaluation metrics for assessing language model calibration?
- Basis in paper: [inferred] The authors suggest that using multiple scores collectively for evaluation could provide a more accurate assessment than relying solely on perplexity, and they investigate using scoring rules as beam search objectives.
- Why unresolved: The paper only explores a few scoring rules as beam search objectives and doesn't comprehensively evaluate their effectiveness as standalone evaluation metrics.
- What evidence would resolve it: Extensive experiments comparing the correlation between various scoring rules and human judgments of text quality and model calibration across different language generation tasks.

### Open Question 3
- Question: What factors contribute to the improved performance of fine-tuning large language models with alternative scoring rules, and can this be generalized to other model scales?
- Basis in paper: [explicit] The authors observe significant improvements (over 3 BLEU points) when fine-tuning LLaMA-7B and LLaMA-13B with Brier or Spherical scores, but note that this improvement doesn't occur when training from scratch.
- Why unresolved: The paper conjectures about complementarity between scoring rules but doesn't provide a detailed analysis of why this fine-tuning approach works particularly well for large language models.
- What evidence would resolve it: In-depth analysis of model representations and decision boundaries before and after fine-tuning with different scoring rules, along with experiments on a wider range of model scales to identify patterns in performance improvements.

## Limitations

- The paper uses relatively small-scale models (LLaMA-7B and LLaMA-13B) rather than larger foundation models, limiting generalizability
- The ablation studies for score smoothing and token-level adaptation are not comprehensive enough to establish the relative importance of each contribution
- The claim about complementarity between scoring rules lacks rigorous mathematical justification or empirical evidence beyond observed performance gains

## Confidence

**High Confidence**: The core theoretical framework for token-level scoring adaptation is sound, with clear mathematical formulation and proper grounding in strictly proper scoring rule theory.

**Medium Confidence**: The experimental results showing BLEU improvements of 3+ points are promising but require replication with larger models and more diverse datasets.

**Low Confidence**: The practical benefits of score smoothing for arbitrary scoring rules in language modeling, while theoretically justified, lack sufficient empirical validation.

## Next Checks

1. **Scaling Experiment**: Fine-tune LLaMA-33B or LLaMA-65B models on the same translation tasks to determine if the reported improvements scale with model size. Measure both BLEU improvements and computational overhead compared to log-likelihood training.

2. **Complementarity Analysis**: Design an ablation study that systematically varies the order and combination of fine-tuning with different scoring rules. Measure whether the complementarity effect is consistent across multiple scoring rule pairs and whether it exceeds the sum of individual improvements.

3. **Smoothing Parameter Sweep**: Conduct a comprehensive hyperparameter study on the score smoothing coefficient across different scoring rules and datasets. Identify the optimal smoothing values and determine whether the benefits justify the additional computational cost and complexity.