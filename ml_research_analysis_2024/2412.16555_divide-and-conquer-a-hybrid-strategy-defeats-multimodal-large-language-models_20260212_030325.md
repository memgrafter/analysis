---
ver: rpa2
title: 'Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models'
arxiv_id: '2412.16555'
source_url: https://arxiv.org/abs/2412.16555
tags:
- jailbreak
- jmllm
- attack
- methods
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes JMLLM, a hybrid strategy-based multimodal\
  \ jailbreak framework that integrates alternating translation, word encryption,\
  \ feature collapse, and harmful injection techniques to bypass safety mechanisms\
  \ across text, visual, and speech modalities of large language models. Experiments\
  \ on the newly constructed TriJail dataset and the AdvBench benchmark show that\
  \ JMLLM achieves state-of-the-art attack success rates while significantly reducing\
  \ query counts and time overhead\u2014outperforming existing methods like ReNeLLM\
  \ and AutoDAN by 5.36\xD7 in execution efficiency."
---

# Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models

## Quick Facts
- **arXiv ID**: 2412.16555
- **Source URL**: https://arxiv.org/abs/2412.16555
- **Reference count**: 40
- **Primary result**: JMLLM framework achieves state-of-the-art jailbreak attack success rates while reducing query counts and time overhead by 5.36× compared to existing methods

## Executive Summary
This paper introduces JMLLM, a hybrid strategy-based multimodal jailbreak framework that successfully bypasses safety mechanisms across text, visual, and speech modalities in large language models. The framework integrates four techniques—alternating translation across low-resource languages, word encryption through character shuffling and Caesar cipher, feature collapse in visual inputs, and harmful injection—to defeat safety filters while maintaining high efficiency. Experiments on the newly constructed TriJail dataset and AdvBench benchmark demonstrate superior performance, achieving higher attack success rates with significantly reduced computational overhead compared to existing methods like ReNeLLM and AutoDAN.

## Method Summary
JMLLM employs a divide-and-conquer approach that transforms harmful prompts across multiple modalities before submission to target LLMs. For text, the framework uses alternating translation (mixing Czech, Norwegian, Danish, and Romanian) and word encryption (character shuffling plus Caesar cipher). Visual inputs undergo feature collapse through grayscale conversion, edge detection, and Gaussian blur, combined with harmful injection via noise addition and text overlay. Speech prompts are generated using TTS-1 from processed text. The framework supports both single-query and multi-query attack modes (up to 6 rounds) and evaluates success using four metrics: GPT-ASR (GPT-4 evaluator), KW-ASR (keyword dictionary), TOX-ASR (toxicity API scores), and HM-ASR (human annotation).

## Key Results
- JMLLM achieves state-of-the-art attack success rates across all evaluation metrics on both TriJail and AdvBench datasets
- The framework reduces query counts and time overhead by 5.36× compared to ReNeLLM and AutoDAN
- Ablation studies confirm each component's critical role, with the hybrid approach outperforming individual strategies
- A defense strategy using harmful separators is proposed and shown to mitigate attacks effectively

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Alternating translation across low-resource languages bypasses LLM safety filters by exploiting multilingual processing limitations
- **Mechanism**: Each word in the original English prompt is translated into different low-resource languages (Czech, Norwegian, Danish, Romanian) in sequence, creating a multilingual prompt that LLMs struggle to fully align against safety policies
- **Core assumption**: LLMs perform worse on low-resource languages compared to high-resource languages, making them less likely to recognize harmful intent in mixed-language prompts
- **Evidence anchors**: [abstract] Language selection based on vocabulary coverage and basic LLM understanding; [section 4.1] LLMs typically perform worse in low-resource languages; [corpus] Weak support - related papers discuss divide-and-conquer prompting but not specifically for jailbreaking via multilingual techniques
- **Break condition**: If LLMs improve multilingual alignment or implement language-specific safety filters that recognize harmful content regardless of language

### Mechanism 2
- **Claim**: Word encryption through character shuffling and Caesar cipher exploits LLM's task processing limitations by creating a decryption task that bypasses immediate content filtering
- **Mechanism**: Original harmful prompt is transformed by shuffling characters within each word and applying Caesar cipher encryption, then the LLM is prompted to first decrypt and then reconstruct the harmful content
- **Core assumption**: LLMs process complex multi-tasking less effectively when the first task doesn't involve prohibited scenarios, allowing the second task to proceed
- **Evidence anchors**: [abstract] LLMs generally don't reject requests when first task doesn't involve prohibited scenarios; [section 4.2] Two-task format implementation; [corpus] Weak support - no direct evidence for this specific word encryption mechanism
- **Break condition**: If LLMs implement multi-task safety analysis that detects harmful intent even when presented as secondary tasks

### Mechanism 3
- **Claim**: Feature collapse in visual inputs exploits transformer architecture limitations to reduce image feature diversity, making harmful content harder to detect
- **Mechanism**: Images are converted to grayscale, edge detection applied to reduce noise, Gaussian blur used to smooth features, and edge image multiplied with blurred image to create processed image preserving only harmful features
- **Core assumption**: Self-attention mechanism in transformers causes rapid decline in image feature diversity, which can be intentionally exploited to bypass visual safety mechanisms
- **Evidence anchors**: [abstract] Self-attention mechanism considered key factor in rapid decline of image feature diversity; [section 4.3] Feature collapse may cause biases in LLM-generated results; [corpus] Weak support - corpus contains papers on transformer feature collapse but not specifically for jailbreaking through feature collapse exploitation
- **Break condition**: If visual safety mechanisms implement feature preservation techniques or use alternative architectures less susceptible to feature collapse

## Foundational Learning

- **Concept**: Transformer architecture and self-attention mechanisms
  - **Why needed here**: Understanding how feature collapse occurs in transformer-based models is crucial for implementing the visual jailbreak strategy and understanding why it works
  - **Quick check question**: How does the self-attention mechanism in transformers contribute to feature diversity reduction in visual inputs?

- **Concept**: Caesar cipher and character-level encryption
  - **Why needed here**: The word encryption mechanism relies on basic cryptographic techniques to transform harmful content into seemingly benign text that requires decryption
  - **Quick check question**: What are the mathematical operations involved in a Caesar cipher and how does character shuffling complement this encryption?

- **Concept**: Low-resource language processing in LLMs
  - **Why needed here**: The alternating translation strategy exploits known weaknesses in how LLMs handle low-resource languages compared to high-resource languages
  - **Quick check question**: Why do LLMs typically perform worse on low-resource languages and how does this create vulnerabilities for jailbreaking?

## Architecture Onboarding

- **Component map**: Input Processing Layer -> Strategy Selection Module -> Text/Visual/Speech Transformation Engine -> LLM Interface Layer -> Evaluation Framework -> Defense Mitigation Module
- **Critical path**: Adversarial data → Strategy selection → Transformation (text/visual/speech) → LLM input → Response generation → Evaluation → Iteration (for multi-round attacks)
- **Design tradeoffs**: Single-query vs multi-query (speed vs success rates); Evaluation comprehensiveness vs speed (four metrics vs faster assessment); Defense vs attack balance (reduced success vs legitimate functionality)
- **Failure signatures**: Low ASR across all evaluation metrics indicates ineffective transformation strategies; High time overhead with minimal ASR improvement suggests inefficient multi-round iterations; Inconsistent results across evaluation metrics may indicate evaluation methodology issues
- **First 3 experiments**: 1) Test alternating translation with a single low-resource language to establish baseline effectiveness before expanding to multiple languages; 2) Implement basic word encryption with only character shuffling (no Caesar cipher) to isolate impact of each transformation component; 3) Apply feature collapse to a simple edge-detection image to verify visual processing pipeline works before adding harmful injection components

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How effective would JMLLM be against MLLMs with additional modalities beyond text, image, and speech, such as video or haptic inputs?
- **Basis in paper**: [inferred] Paper mentions future work involving MLLMs with expanded modalities and acknowledges the challenge of jailbreaking in more complex multimodal environments
- **Why unresolved**: Current JMLLM framework is designed specifically for text, visual, and speech modalities, and its performance on other modalities has not been tested or evaluated
- **What evidence would resolve it**: Conducting experiments with JMLLM against MLLMs incorporating video, haptic, or other modalities, and measuring attack success rates and time overhead compared to current results

### Open Question 2
- **Question**: What is the long-term effectiveness of JMLLM as MLLMs are continuously updated and patched against known vulnerabilities?
- **Basis in paper**: [explicit] Paper acknowledges that as MLLMs are updated, jailbreak methods may become ineffective, and researchers must revalidate and adjust strategies after each model update
- **Why unresolved**: Paper does not provide data on how JMLLM performs over time as MLLMs evolve, nor does it offer strategies for adapting JMLLM to future model versions
- **What evidence would resolve it**: Longitudinal studies tracking JMLLM's performance across multiple versions of the same MLLMs, or developing adaptive components within JMLLM that can automatically adjust to new model defenses

### Open Question 3
- **Question**: How does the performance of JMLLM compare to other state-of-the-art jailbreak methods when evaluated using a unified and deterministic benchmark?
- **Basis in paper**: [inferred] Paper mentions lack of unified and deterministic benchmark for evaluating jailbreak methods and suggests this limits full demonstration of JMLLM's superiority
- **Why unresolved**: Current evaluation relies on multiple existing metrics, but standardized benchmark allowing direct comparison across all jailbreak methods is not yet available
- **What evidence would resolve it**: Developing and implementing standardized benchmark for jailbreak evaluation, then running JMLLM and competing methods through this benchmark to obtain comparable results

## Limitations

- The paper's claims about exploiting transformer feature collapse for visual jailbreaking rely heavily on theoretical foundations weakly supported by corpus evidence
- The alternating translation strategy's effectiveness assumes consistent performance degradation across low-resource languages, but this may not generalize to all MLLM implementations
- The word encryption mechanism's success depends on LLMs processing multi-task scenarios in predictable ways that may not hold as models evolve

## Confidence

- **High Confidence**: Overall framework architecture and evaluation methodology are well-specified and reproducible; attack success rates measured across multiple robust metrics (GPT-ASR, KW-ASR, TOX-ASR, HM-ASR) provide strong validation
- **Medium Confidence**: Effectiveness of individual transformation strategies (alternating translation, word encryption, feature collapse) is supported by experimental results but relies on assumptions about LLM processing that may vary across implementations
- **Low Confidence**: Theoretical foundations for why feature collapse specifically enables jailbreaking attacks are weakly supported by existing literature; long-term effectiveness against evolving safety mechanisms is uncertain

## Next Checks

1. **Feature Collapse Validation**: Test the visual jailbreaking strategy on multiple transformer architectures with known feature collapse characteristics to verify whether the observed effectiveness generalizes beyond the specific MLLMs tested in the paper

2. **Language Generalization Test**: Evaluate the alternating translation strategy using different combinations of low-resource languages and high-resource languages to determine whether effectiveness depends specifically on the chosen language set or generalizes to any multilingual mixing approach

3. **Multi-Task Detection Test**: Implement a controlled experiment where harmful content is presented as secondary tasks in various formats to determine whether the word encryption mechanism's success depends on specific task presentation or represents a broader vulnerability in multi-task processing