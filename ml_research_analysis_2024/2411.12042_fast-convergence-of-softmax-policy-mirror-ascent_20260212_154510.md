---
ver: rpa2
title: Fast Convergence of Softmax Policy Mirror Ascent
arxiv_id: '2411.12042'
source_url: https://arxiv.org/abs/2411.12042
tags:
- policy
- spma
- convergence
- mirror
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Softmax Policy Mirror Ascent (SPMA), a policy
  optimization algorithm that uses mirror ascent in the dual space of logits rather
  than probabilities. Unlike Natural Policy Gradient (NPG) which updates probabilities
  directly, SPMA updates the logits of a softmax policy, eliminating the need for
  explicit normalization across actions.
---

# Fast Convergence of Softmax Policy Mirror Ascent

## Quick Facts
- arXiv ID: 2411.12042
- Source URL: https://arxiv.org/abs/2411.12042
- Authors: Reza Asad; Reza Babanezhad; Issam Laradji; Nicolas Le Roux; Sharan Vaswani
- Reference count: 40
- One-line primary result: SPMA achieves linear convergence to optimal policy matching NPG while outperforming SPG, without requiring compatible function approximation.

## Executive Summary
This paper introduces Softmax Policy Mirror Ascent (SPMA), a policy optimization algorithm that updates policy logits rather than probabilities, enabling linear convergence to optimal policies in tabular MDPs while matching Natural Policy Gradient's (NPG) convergence rate. Unlike NPG, SPMA operates in the dual space of logits and doesn't require explicit normalization across actions. The method extends to function approximation settings by interpreting policy parameterization as a constraint on logits, solving convex softmax classification problems rather than non-convex surrogates like MDPO.

## Method Summary
SPMA performs mirror ascent in the dual space of logits using the log-sum-exp mirror map, avoiding explicit normalization across actions required by probability-based methods. For tabular MDPs, it achieves linear convergence to optimal policies with constant step-size. In function approximation settings, SPMA interprets policy parameterization as a constraint on logits and uses projected mirror ascent, solving convex softmax classification problems at each iteration. The method handles both log-linear and neural policy parameterizations, converging linearly to a neighborhood of the optimal value function whose size depends on sampling error, optimization error, and model bias.

## Key Results
- SPMA achieves linear convergence to optimal policy in tabular MDPs with constant step-size, matching NPG's rate while outperforming constant step-size SPG
- For function approximation, SPMA converges linearly to a neighborhood of optimal value function without requiring compatible function approximation
- Empirically competitive with or better than PPO, TRPO, and MDPO on Atari and MuJoCo benchmarks without additional algorithmic modifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SPMA achieves linear convergence to the optimal policy by updating logits directly instead of probabilities.
- Mechanism: Mirror ascent in the dual space of logits avoids normalization across actions, reducing computational overhead and stabilizing updates.
- Core assumption: The logits are constrained to lie in the span of the features (linear FA) or are realizable by the model.
- Evidence anchors:
  - [abstract] "SPMA corresponds to mirror ascent in the dual space of logits and does not require a normalization across actions."
  - [section 3.2] "Unlike NPG, the SPMA update in Eq. (3) is linear in both η and Aπt(s, a) and does not require an explicit normalization across actions to ensure valid probability distributions."
  - [corpus] Weak evidence; no direct comparison of computational overhead in cited papers.
- Break condition: If the logits are not realizable by the model (large model bias), the linear convergence guarantees fail.

### Mechanism 2
- Claim: SPMA with linear FA requires solving convex softmax classification problems, unlike MDPO which results in non-convex surrogates.
- Mechanism: By interpreting the policy parameterization as a constraint on the logits, the Bregman projection becomes a convex optimization problem over the feasible set.
- Core assumption: The surrogate function ℓt(θ) is convex in θ for the log-linear parameterization.
- Evidence anchors:
  - [abstract] "Unlike MDPO, a practical generalization of NPG, SPMA with linear FA only requires solving convex softmax classification problems."
  - [section 4.1] "For this special case, the problem in Eq. (4) is equivalent to a weighted (according to dπt(s)) multi-class classification for each state."
  - [corpus] Weak evidence; no explicit proofs of convexity in cited papers.
- Break condition: If the policy parameterization is non-linear (e.g., neural networks), the convexity assumption may not hold.

### Mechanism 3
- Claim: SPMA achieves faster convergence than constant step-size softmax policy gradient (SPG) by using the log-sum-exp mirror map.
- Mechanism: The log-sum-exp mirror map leads to an exponential improvement over SPG's Euclidean mirror map, resulting in linear convergence instead of sublinear convergence.
- Core assumption: The step-size η is chosen appropriately (η < min{1 - γ, 1/(Ct(1-γ))}).
- Evidence anchors:
  - [abstract] "SPMA with a constant step-size matches the linear convergence of NPG and achieves a faster convergence than constant step-size (accelerated) softmax policy gradient."
  - [section 3.3] "In contrast, the above theorem demonstrates that by choosing the appropriate mirror map, constant step-size SPMA can achieve a faster O(log(1/ϵ)) rate of convergence."
  - [corpus] Weak evidence; no direct comparison of convergence rates in cited papers.
- Break condition: If the step-size is not chosen appropriately, the linear convergence guarantees may not hold.

## Foundational Learning

- Concept: Mirror ascent and Bregman divergences
  - Why needed here: SPMA is a mirror ascent algorithm that uses the log-sum-exp mirror map, which induces a Bregman divergence.
  - Quick check question: What is the relationship between the log-sum-exp mirror map and the dual space of logits?

- Concept: Policy gradient methods and natural policy gradient
  - Why needed here: SPMA is a policy gradient method that is related to natural policy gradient, but updates logits instead of probabilities.
  - Quick check question: How does SPMA differ from natural policy gradient in terms of the space it operates on?

- Concept: Function approximation and compatible function approximation
  - Why needed here: SPMA extends to function approximation settings by interpreting the policy parameterization as a constraint on the logits.
  - Quick check question: What is the difference between SPMA and natural policy gradient in terms of the requirements for function approximation?

## Architecture Onboarding

- Component map: Policy parameters -> Logits -> Mirror ascent update -> Bregman projection -> Updated logits -> Probability distribution
- Critical path: 1. Initialize policy parameters 2. Collect trajectories and estimate policy gradients 3. Update logits using mirror ascent 4. Project logits onto feasible set using function approximation 5. Repeat steps 2-4 until convergence
- Design tradeoffs:
  - Computational complexity: SPMA requires solving convex optimization problems, which may be more expensive than natural policy gradient
  - Convergence guarantees: SPMA has stronger convergence guarantees than natural policy gradient, especially in the function approximation setting
  - Implementation complexity: SPMA requires implementing mirror ascent and Bregman projection, which may be more complex than natural policy gradient
- Failure signatures:
  - If the logits are not realizable by the model, the linear convergence guarantees may fail
  - If the step-size is not chosen appropriately, the linear convergence guarantees may not hold
  - If the function approximation is poor, the convergence rate may degrade
- First 3 experiments:
  1. Implement SPMA on a simple bandit problem and compare its performance to natural policy gradient and softmax policy gradient
  2. Implement SPMA with linear function approximation on a small MDP and compare its performance to natural policy gradient with compatible function approximation
  3. Implement SPMA with neural network function approximation on a continuous control task and compare its performance to PPO and TRPO

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SPMA achieve global super-linear convergence rates for MDPs beyond the tabular setting?
- Basis in paper: [explicit] The paper proves super-linear convergence for multi-armed bandits (Theorem 4) and linear convergence for tabular MDPs (Theorem 2), but only linear convergence to a neighborhood for function approximation settings (Theorem 3).
- Why unresolved: The theoretical analysis for function approximation settings is limited to linear convergence to a neighborhood due to practical constraints like sampling error and model bias, rather than achieving super-linear rates.
- What evidence would resolve it: Empirical or theoretical results showing SPMA achieving faster-than-linear convergence rates (e.g., super-linear) for MDPs with function approximation, particularly in log-linear or neural policy parameterizations.

### Open Question 2
- Question: How does the performance of SPMA scale with the size of the state-action space in non-tabular settings?
- Basis in paper: [inferred] The paper demonstrates competitive performance on Atari and MuJoCo benchmarks, but these are specific environments. The theoretical analysis shows convergence to a neighborhood whose size depends on sampling error and model bias, which could grow with state-action space size.
- Why unresolved: While SPMA shows strong empirical performance, the paper doesn't provide a systematic analysis of how performance scales with increasing state-action space complexity in practical applications.
- What evidence would resolve it: Empirical results across a range of environments with varying state-action space sizes, or theoretical bounds on how sampling error and model bias scale with the complexity of the policy parameterization.

### Open Question 3
- Question: Can an adaptive step-size strategy improve SPMA's performance compared to the constant step-size used in the experiments?
- Basis in paper: [explicit] The paper mentions plans to develop techniques for adaptively tuning the step-size in future work, and uses a grid search for the step-size η in experiments.
- Why unresolved: The experiments use a fixed step-size chosen via grid search, which is computationally expensive and may not be optimal across all environments or stages of training.
- What evidence would resolve it: Empirical results comparing SPMA with adaptive step-size strategies (e.g., line search, learning rate schedules) against the constant step-size baseline, showing consistent improvements in performance or stability across multiple environments.

## Limitations

- Theoretical guarantees rely on strong assumptions including exact policy gradients, bounded advantage gaps, and realizability of optimal logits by the model
- Experimental evaluation lacks ablation studies to isolate the contributions of mirror ascent update versus other algorithmic components
- Empirical results don't provide systematic analysis of how performance scales with state-action space complexity in non-tabular settings

## Confidence

- Theoretical claims: Medium
  - Linear convergence proof for tabular MDPs is well-established in mirror descent literature
  - Extension to function approximation relies on stronger assumptions that may not hold in practice
- Empirical results: Medium
  - Demonstrates competitive performance on benchmark tasks
  - Lacks ablation studies and comparisons to state-of-the-art methods on challenging continuous control tasks

## Next Checks

1. Implement SPMA on a simple bandit problem and compare its performance to natural policy gradient and softmax policy gradient with different step-sizes
2. Analyze the impact of model bias on the convergence rate of SPMA with function approximation by varying the expressiveness of the policy parameterization
3. Perform ablation studies to isolate the contributions of the mirror ascent update versus other algorithmic components (e.g., entropy regularization) on the empirical performance of SPMA