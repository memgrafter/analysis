---
ver: rpa2
title: 'MoreHopQA: More Than Multi-hop Reasoning'
arxiv_id: '2406.13397'
source_url: https://arxiv.org/abs/2406.13397
tags:
- question
- answer
- reasoning
- dataset
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoreHopQA, a new multi-hop QA dataset that
  shifts from extractive to generative answers by extending existing 2-hop questions
  with additional reasoning types like arithmetic, commonsense, and symbolic reasoning.
  The dataset is created through a semi-automated process using templates and undergoes
  human verification, resulting in 1,118 high-quality samples.
---

# MoreHopQA: More Than Multi-hop Reasoning

## Quick Facts
- **arXiv ID**: 2406.13397
- **Source URL**: https://arxiv.org/abs/2406.13397
- **Reference count**: 22
- **Primary result**: Models struggle with extended multi-hop questions, achieving only 38.7% perfect reasoning (GPT-4) on questions combining 2-hop reasoning with arithmetic, commonsense, and symbolic reasoning

## Executive Summary
This paper introduces MoreHopQA, a novel multi-hop question answering dataset that moves beyond traditional extractive answers to generative reasoning. The dataset is created by extending existing 2-hop questions with additional reasoning types including arithmetic, commonsense, and symbolic reasoning through a semi-automated process using templates. After human verification, the dataset contains 1,118 high-quality samples. Evaluation across five large language models (Mistral 7B, Gemma 7B, Llama 3 8B/70B, and GPT-4) reveals that while models perform reasonably on the base 2-hop questions, they struggle significantly with the extended reasoning tasks, with only 38.7% of GPT-4 and 33.4% of Llama3-70B achieving perfect reasoning across all sub-questions. This demonstrates that MoreHopQA presents a more challenging test of true multi-hop reasoning capabilities compared to existing datasets.

## Method Summary
MoreHopQA is constructed through a semi-automated process that extends existing 2-hop questions from DROP and HotpotQA datasets with additional reasoning types. The approach uses templated modifications to incorporate arithmetic, commonsense, and symbolic reasoning into the question-answer pairs. Each extended question requires solving multiple sub-questions in sequence, where the answer to one sub-question becomes necessary for solving the next. The dataset creation involves initial template-based generation followed by human verification from two annotators, with a third resolving disagreements. The final dataset contains 1,118 samples that test models' ability to combine multiple reasoning types rather than just retrieving information from text.

## Key Results
- Models perform well on base 2-hop questions but struggle significantly with extended reasoning tasks
- Only 38.7% of GPT-4 and 33.4% of Llama3-70B achieve perfect reasoning across all sub-questions
- MoreHopQA demonstrates that templated extensions create more challenging multi-hop reasoning questions than existing datasets
- The dataset effectively tests true multi-hop reasoning capabilities beyond simple information retrieval

## Why This Works (Mechanism)
The dataset's effectiveness stems from its structured approach to combining multiple reasoning types in a sequential manner. By requiring models to solve arithmetic, commonsense, and symbolic reasoning sub-questions in addition to the base 2-hop retrieval, MoreHopQA forces models to demonstrate genuine multi-step reasoning rather than pattern matching or simple chain-of-thought prompting. The templated approach ensures consistent difficulty progression while maintaining the natural language flow of questions.

## Foundational Learning
**Multi-hop reasoning** - Why needed: Core capability being tested; quick check: Can models combine information from multiple sources in sequence?
**Arithmetic reasoning** - Why needed: Tests numerical computation integration with language understanding; quick check: Can models perform calculations embedded in natural language?
**Commonsense reasoning** - Why needed: Evaluates real-world knowledge application; quick check: Can models apply implicit knowledge to solve problems?
**Symbolic reasoning** - Why needed: Tests logical and relational reasoning; quick check: Can models handle abstract relationships and comparisons?
**Template-based dataset generation** - Why needed: Enables systematic creation of complex reasoning tasks; quick check: Are generated questions consistent and challenging?

## Architecture Onboarding

**Component map**: Question templates -> Template application -> Human verification -> Dataset compilation -> Model evaluation

**Critical path**: Template generation → Human verification → Answer validation → Model testing → Performance analysis

**Design tradeoffs**: 
- Template-based generation enables scalability but may create artificial patterns
- Human verification ensures quality but limits dataset size
- Binary correctness scoring provides clear metrics but may overstate difficulty
- Focus on generative answers increases complexity but enables richer reasoning evaluation

**Failure signatures**: 
- Models failing on specific reasoning types (arithmetic vs commonsense vs symbolic)
- Performance drops on questions requiring information from multiple sources
- Difficulty spikes when sub-question answers depend on previous results
- Zero-shot settings revealing fundamental reasoning gaps

**3 first experiments**:
1. Evaluate inter-annotator agreement rates during human verification process
2. Test partial credit scoring to identify specific reasoning bottlenecks
3. Compare zero-shot vs few-shot performance to assess instruction tuning impact

## Open Questions the Paper Calls Out
None

## Limitations
- Human verification process lacks detailed metrics on inter-annotator agreement rates
- Templated modifications may create artificial reasoning patterns that don't reflect natural scenarios
- Binary correctness metric may overstate dataset difficulty without revealing specific failure modes
- Evaluation limited to zero-shot settings, leaving open questions about few-shot performance

## Confidence
- Confidence: Medium for claims about dataset quality and difficulty
- Confidence: Low for claims about general multi-hop reasoning capability gaps
- Confidence: High for the specific observation that templated extensions create challenging questions

## Next Checks
1. Measure inter-annotator agreement rates during the human verification process and report Fleiss' kappa scores for different reasoning types
2. Conduct partial credit evaluation to identify which specific reasoning components (arithmetic, commonsense, symbolic) contribute most to model failures
3. Test model performance with few-shot examples and instruction tuning to establish whether the difficulty is intrinsic to the reasoning tasks or specific to zero-shot settings