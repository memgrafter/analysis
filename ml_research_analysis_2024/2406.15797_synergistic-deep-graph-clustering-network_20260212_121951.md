---
ver: rpa2
title: Synergistic Deep Graph Clustering Network
arxiv_id: '2406.15797'
source_url: https://arxiv.org/abs/2406.15797
tags:
- graph
- uni00000013
- clustering
- uni00000011
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SynC, a graph clustering framework that enhances
  both representation learning and structure augmentation synergistically. The key
  innovation is a Transform Input Graph Auto-Encoder (TIGAE) that incorporates explicit
  structural information through linear transformation, addressing representation
  collapse in traditional GAE.
---

# Synergistic Deep Graph Clustering Network

## Quick Facts
- arXiv ID: 2406.15797
- Source URL: https://arxiv.org/abs/2406.15797
- Authors: Benyu Wu; Shifei Ding; Xiao Xu; Lili Guo; Ling Ding; Xindong Wu
- Reference count: 40
- Primary result: State-of-the-art graph clustering performance with improved ACC, NMI, and ARI metrics

## Executive Summary
This paper introduces SynC, a graph clustering framework that enhances both representation learning and structure augmentation synergistically. The key innovation is a Transform Input Graph Auto-Encoder (TIGAE) that incorporates explicit structural information through linear transformation, addressing representation collapse in traditional GAE. SynC further refines the graph using a structure fine-tuning strategy and shares weights between representation learning and structure augmentation to reduce model complexity.

## Method Summary
SynC operates through a synergistic approach combining representation learning and structure augmentation. The Transform Input Graph Auto-Encoder (TIGAE) serves as the core component, using linear transformation to incorporate explicit structural information and prevent representation collapse. The framework employs a structure fine-tuning strategy to iteratively refine the graph structure while maintaining computational efficiency through weight-sharing between the representation learning and structure augmentation components. This dual approach allows for simultaneous improvement of both learned representations and graph topology.

## Key Results
- Achieves state-of-the-art clustering performance on benchmark datasets
- Demonstrates significant improvements in accuracy (ACC), normalized mutual information (NMI), and adjusted rand index (ARI)
- Shows fast convergence and effective handling of datasets with low homophily ratios and imbalanced classes

## Why This Works (Mechanism)
The synergistic approach works by simultaneously optimizing representation learning and graph structure. The linear transformation in TIGAE explicitly incorporates structural information, preventing the common issue of representation collapse in traditional auto-encoders. By sharing weights between the representation and structure components, the model reduces complexity while maintaining strong performance. The structure fine-tuning strategy iteratively improves the graph topology, which in turn enhances the quality of learned representations, creating a positive feedback loop that leads to better clustering results.

## Foundational Learning

**Graph Auto-Encoder (GAE)**: Why needed - To learn node representations from graph structure; Quick check - Can encode adjacency matrix to latent space
**Linear Transformation**: Why needed - To incorporate explicit structural information; Quick check