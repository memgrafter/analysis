---
ver: rpa2
title: 'AfriHG: News headline generation for African Languages'
arxiv_id: '2412.20223'
source_url: https://arxiv.org/abs/2412.20223
tags:
- languages
- news
- summarization
- headline
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces AfriHG\u2014a news headline generation dataset\
  \ for 16 African languages by combining XLSum and MasakhaNEWS datasets. The authors\
  \ evaluated two seq2seq models (mT5-base and AfriTeVa V2) and Aya-101 LLM on this\
  \ task."
---

# AfriHG: News headline generation for African Languages

## Quick Facts
- arXiv ID: 2412.20223
- Source URL: https://arxiv.org/abs/2412.20223
- Authors: Toyib Ogunremi; Serah Akojenu; Anthony Soronnadi; Olubayo Adekanmbi; David Ifeoluwa Adelani
- Reference count: 10
- Primary result: Africa-centric model AfriTeVa V2 outperforms mT5-base with +2.8/+1.5/+2.6 Rouge score gains, achieving competitive performance to 13B parameter Aya-101 despite being only 313M parameters

## Executive Summary
This paper introduces AfriHG, a news headline generation dataset for 16 African languages created by combining XLSum and MasakhaNEWS datasets. The authors evaluate mT5-base and AfriTeVa V2 seq2seq models along with Aya-101 LLM on this task. Their results demonstrate that AfriTeVa V2, pretrained specifically on African languages, consistently outperforms the massively multilingual mT5-base model. Notably, despite Aya-101 having 13B parameters versus AfriTeVa V2's 313M parameters, their performance is competitive, with AfriTeVa V2 achieving the best results for 9 out of 16 languages.

## Method Summary
The authors created the AfriHG dataset by combining article-headline pairs from XLSum and MasakhaNEWS sources covering 16 African languages. They fine-tuned two seq2seq models (mT5-base and AfriTeVa V2) and evaluated the Aya-101 LLM using Rouge-1, Rouge-2, and Rouge-L metrics. Models were trained with batch size 4, 3 epochs, and learning rate 5e-5 on Nvidia A10 GPU. The study revealed significant performance drops for non-Latin script languages when using default tokenizers, highlighting the importance of script-specific tokenization strategies.

## Key Results
- AfriTeVa V2 achieves average gains of +2.8/+1.5/+2.6 in Rouge scores (R1/R2/RL) over mT5-base
- Despite 43x parameter difference, AfriTeVa V2 (313M) is competitive to Aya-101 (13B), achieving best results for 9/16 languages
- Non-Latin script languages (Arabic, Amharic, Tigrinya) score poorly (<4.0 Rouge) compared to their strong summarization performance (>20.0 Rouge)
- Script-specific tokenizers improve performance by up to 20 points in R1/RL scores for non-Latin scripts

## Why This Works (Mechanism)

### Mechanism 1
Africa-centric pretraining on WURA dataset gives AfriTeVa V2 better language modeling capabilities for African languages compared to mT5-base. AfriTeVa V2 was pretrained on 29.3GB of African language data across 20 languages, while mT5-base covers only 17 African languages in general mC4. This focused pretraining leads to better downstream performance on AfriHG languages.

### Mechanism 2
Script-specific tokenizers are crucial for non-Latin script languages. Default HuggingFace tokenizers struggle with Arabic, Amharic, and Tigrinya scripts, causing poor performance. Using language-specific tokenizers like AraT5-base, bert-amharic-tokenizer, or TigXLNet significantly improves results, with performance gaps reaching -20 points in R1/RL scores.

### Mechanism 3
Fine-tuning smaller models can achieve competitive performance to prompting larger LLMs when sufficient training data exists. The 313M parameter AfriTeVa V2 achieves competitive results to 13B parameter Aya-101 through effective fine-tuning on the AfriHG dataset, demonstrating that parameter count alone doesn't determine performance when adequate training data is available.

## Foundational Learning

- **Seq2seq architecture and T5 framework**: The paper uses T5-based models for headline generation, which is a seq2seq task. Understanding T5's encoder-decoder architecture is crucial for modifying this work.
  - Quick check: How does T5's encoder-decoder architecture differ from autoregressive models like GPT for text generation?

- **ROUGE metrics for evaluation**: Models are evaluated using ROUGE-1, ROUGE-2, and ROUGE-L scores. Understanding these metrics is essential for interpreting results.
  - Quick check: What's the difference between ROUGE-1, ROUGE-2, and ROUGE-L, and why might ROUGE-2 be particularly important for headline generation?

- **Multilingual tokenization challenges**: The paper highlights issues with non-Latin scripts and the importance of script-specific tokenizers. Understanding tokenization across scripts is crucial for extending this work.
  - Quick check: How do subword tokenization algorithms like WordPiece handle characters from non-Latin scripts differently than Latin scripts?

## Architecture Onboarding

- **Component map**: XL-Sum + MasakhaNEWS → AfriHG dataset → mT5-base/AfriTeVa V2/Aya-101 → ROUGE evaluation
- **Critical path**: Dataset creation → Model fine-tuning → Evaluation → Script-specific tokenizer analysis
- **Design tradeoffs**: Model size vs. performance (AfriTeVa V2 313M vs. Aya-101 13B), dataset coverage vs. language diversity, default vs. script-specific tokenizers
- **Failure signatures**: Non-Latin scripts scoring <4.0 ROUGE indicates tokenization issues; performance gaps >20 points suggest script-specific challenges; competitive performance between small fine-tuned and large prompted models suggests sufficient training data
- **First 3 experiments**:
  1. Reproduce AfriTeVa V2 fine-tuning results with default tokenizer
  2. Implement script-specific tokenizers for Arabic, Amharic, and Tigrinya and measure performance improvement
  3. Compare fine-tuning AfriTeVa V2 vs. prompting Aya-101 on AfriHG languages

## Open Questions the Paper Calls Out

### Open Question 1
How do script-specific tokenizers impact the performance of headline generation models for non-Latin scripts?
- Basis: The paper shows up to 20-point performance gaps between default and script-specific tokenizers
- Why unresolved: Implementation details and comparative analysis of different tokenization strategies are lacking
- What evidence would resolve it: Systematic evaluation of multiple tokenization strategies specifically tailored for non-Latin scripts across all 16 languages

### Open Question 2
Can the performance gap between fine-tuning smaller models and prompting larger models be further reduced or eliminated?
- Basis: AfriTeVa V2 (313M) achieves competitive results to Aya-101 (13B)
- Why unresolved: Only one small fine-tuned model vs. one large prompted model was compared
- What evidence would resolve it: Head-to-head comparisons of multiple fine-tuned small models against multiple prompted large models with ablation studies

### Open Question 3
What are the optimal model architectures and training strategies for African languages with limited data?
- Basis: The paper uses mT5-base and AfriTeVa V2 uniformly without exploring language-specific optimizations
- Why unresolved: One-size-fits-all approach doesn't investigate specialized architectures for low-resource languages
- What evidence would resolve it: Comparative studies of various transformer architectures, data augmentation, and transfer learning approaches optimized for low-resource African languages

## Limitations

- Data quality inconsistencies between XLSum and MasakhaNEWS datasets may affect reliability
- Heavy dependency on script-specific tokenizers limits generalizability to other languages
- Limited language diversity excludes major African languages like Hausa, Igbo, Yoruba, and Somali

## Confidence

**High Confidence Claims**: AfriTeVa V2 architecture and WURA pretraining is well-documented; general seq2seq methodology is sound; performance gap between AfriTeVa V2 and mT5-base on Latin scripts is consistent

**Medium Confidence Claims**: Competitive performance between 313M AfriTeVa V2 and 13B Aya-101 is demonstrated but tokenizer-dependent; specific performance numbers may vary with tokenization choices; script-specific tokenizer importance is supported but implementation details are insufficient

**Low Confidence Claims**: Absolute performance scores for non-Latin scripts are unreliable due to tokenization issues; claims about AfriTeVa V2 being "best for 9/16 languages" may be tokenizer-dependent; generalization to other African languages is highly uncertain

## Next Checks

1. **Tokenizer Implementation Verification**: Reproduce headline generation experiments for Arabic, Amharic, and Tigrinya using both default and script-specific tokenizers (AraT5-base, bert-amharic-tokenizer, TigXLNet) to verify the claimed -20 point performance gap.

2. **Cross-Dataset Performance Analysis**: Evaluate AfriTeVa V2 and mT5-base models on standard summarization datasets (XL-Sum, MasakhaNEWS) for overlapping languages to determine if performance drops are task-specific or dataset-related.

3. **Script-Agnostic Tokenization Experiment**: Implement and test alternative tokenization strategies for non-Latin scripts (Unicode-aware tokenizers, language-agnostic subword algorithms) against script-specific tokenizers to assess generalizability beyond tested languages.