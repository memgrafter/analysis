---
ver: rpa2
title: 'GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language
  Data Generation'
arxiv_id: '2403.19754'
source_url: https://arxiv.org/abs/2403.19754
tags:
- data
- samples
- gold
- generated
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GOLD, a task-agnostic data generation and knowledge
  distillation framework that improves the generalizability of distilled small language
  models by employing an iterative out-of-distribution-guided feedback mechanism.
  The method generates training data with an LLM and uses an energy-based out-of-distribution
  evaluation approach to identify failure modes of the SLM, providing feedback to
  guide the LLM toward generating more diverse and challenging samples.
---

# GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation

## Quick Facts
- arXiv ID: 2403.19754
- Source URL: https://arxiv.org/abs/2403.19754
- Reference count: 40
- Primary result: GOLD achieves state-of-the-art performance, outperforming prior arts and LLM by an average of 5% and 14% respectively

## Executive Summary
This paper proposes GOLD, a task-agnostic data generation and knowledge distillation framework that improves the generalizability of distilled small language models by employing an iterative out-of-distribution-guided feedback mechanism. The method generates training data with an LLM and uses an energy-based out-of-distribution evaluation approach to identify failure modes of the SLM, providing feedback to guide the LLM toward generating more diverse and challenging samples. Experiments on 10 classification and sequence-to-sequence tasks show that GOLD achieves state-of-the-art performance.

## Method Summary
GOLD is a task-agnostic knowledge distillation framework that improves SLM generalizability through iterative data generation guided by out-of-distribution (OOD) feedback. The method uses an LLM to generate synthetic training data, then trains an SLM on this data. The SLM's performance is evaluated on an OOD validation set using energy-based scoring, and the resulting OOD samples are fed back to the LLM to guide generation of more diverse and challenging data in the next iteration. This process repeats until performance plateaus or a maximum iteration count is reached.

## Key Results
- GOLD outperforms prior arts by an average of 5% on 10 NLP tasks
- GOLD achieves 14% better performance than the LLM on average
- Significant improvements on both classification (6 tasks) and sequence-to-sequence (4 tasks) tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative OOD-guided feedback improves SLM generalizability by forcing the LLM to generate diverse, low-probability samples that address SLM failure modes.
- Mechanism: The feedback loop identifies OOD samples (via energy-based scoring) where the SLM performs poorly, then uses these samples as prompts to guide the LLM toward generating more challenging, diverse data in the next iteration.
- Core assumption: LLMs tend to generate high-likelihood samples repeatedly, which causes distilled models to forget the tails of the data distribution and reduces their ability to handle diverse inputs.
- Evidence anchors:
  - [abstract]: "We argue that generating data with LLMs is prone to sampling mainly from the center of original content distribution. This limitation hinders the distilled model from learning the true underlying data distribution and to forget the tails of the distributions (samples with lower probability)."
  - [section 3.2]: "We argue that vanilla data generation with LLMs is prone to only generate high likelihood samples which negatively affects the SLM performance with increasing the number of samples."
  - [corpus]: Weak - related papers focus on general knowledge distillation but not specifically on OOD-guided feedback mechanisms.
- Break condition: If the OOD selection mechanism becomes too aggressive and selects samples that are too different from the target task domain, it could degrade performance by introducing irrelevant or noisy data.

### Mechanism 2
- Claim: Energy-based OOD evaluation is robust to noisy labels generated by LLMs because it operates without requiring ground truth labels.
- Mechanism: The free energy score is computed from the SLM's output logits, selecting samples with low negative energy scores as OOD feedback. This unsupervised approach avoids the problem of selecting data with incorrect labels.
- Core assumption: The energy score distribution can effectively separate OOD samples from in-distribution samples based on the SLM's confidence in its predictions.
- Evidence anchors:
  - [section 3.3]: "The energy score does not require the labels of the generated sample and therefore is not prone to pick the data with noisy labels as the OOD samples."
  - [section 4.4]: "σ=0.1 and λ=1 in most of the experiments... We also experimented on different values of σ, while fixing λ to 1 over the ANLI dataset."
  - [corpus]: Weak - while energy-based methods exist for OOD detection, the specific application to noisy LLM-generated data in knowledge distillation is not well-represented in related papers.
- Break condition: If the SLM's logits are poorly calibrated or the model architecture doesn't support meaningful energy scoring, the OOD detection could fail to identify useful failure modes.

### Mechanism 3
- Claim: Task-agnostic framework works because it treats all tasks as sequence-to-sequence problems, allowing the same methodology to apply across diverse NLP tasks.
- Mechanism: By converting classification tasks to sequence-to-sequence format, the framework uses consistent prompting and generation strategies regardless of the specific task type.
- Core assumption: The T5 model architecture can effectively handle both classification and generation tasks when reformulated as sequence-to-sequence, and LLMs can understand the task formulation regardless of original task type.
- Evidence anchors:
  - [section 4.1]: "As T5 is a sequence-to-sequence model, we define all of the tasks including the classification ones as sequence-to-sequence tasks. Doing so, our framework is flexible to be applied to various NLP tasks."
  - [section 4.2]: "Our proposed method outperforms prior works by obtaining a ROUGE-L of 72.8%... we demonstrated the potential of preparing SLMs for NL4OPT, a novel task for which the LLM has not been specifically trained."
  - [corpus]: Weak - related papers focus on task-specific distillation methods rather than unified frameworks.
- Break condition: If certain tasks fundamentally require different architectures or prompting strategies that cannot be effectively reformulated as sequence-to-sequence, the approach may not generalize well.

## Foundational Learning

- Concept: Energy-based models and free energy scoring
  - Why needed here: To identify OOD samples without relying on potentially noisy labels, using the SLM's output logits to measure sample difficulty
  - Quick check question: How does the free energy score formula E(x) = -log(∑c e^(MS(c)(x))) identify OOD samples?

- Concept: Knowledge distillation principles and teacher-student model training
  - Why needed here: The framework transfers knowledge from LLM (teacher) to SLM (student) through synthetic data generation rather than direct parameter transfer
  - Quick check question: What's the difference between data-free knowledge distillation and traditional knowledge distillation with real labeled data?

- Concept: In-context learning and prompting strategies
  - Why needed here: The iterative process uses generated samples as in-context examples to guide the LLM toward better data generation in subsequent iterations
  - Quick check question: How does adding OOD feedback samples to the prompt influence the LLM's next data generation?

## Architecture Onboarding

- Component map:
  - LLM generator (LLaMA2-7B) -> SLM (T5-base) -> Energy scoring module -> Feedback loop -> LLM generator

- Critical path: Data generation → SLM training → OOD evaluation → Feedback incorporation → Next iteration

- Design tradeoffs:
  - Model size vs. performance: Larger LLMs provide better few-shot performance but increase computational cost
  - Sample diversity vs. accuracy: Higher temperature in LLM generation increases diversity but may reduce accuracy
  - OOD threshold selection: Too narrow misses failure modes, too broad includes irrelevant samples

- Failure signatures:
  - Performance plateaus despite more iterations: Feedback mechanism may not be effective
  - Degradation on validation set: OOD samples may be too different from target distribution
  - High variance across runs: Prompt sensitivity or random generation issues

- First 3 experiments:
  1. Baseline comparison: Run GOLD vs. vanilla data generation (no feedback) on a simple classification task to verify OOD feedback improves performance
  2. OOD threshold sensitivity: Test different α and β values on a validation task to find optimal selection range
  3. Model size ablation: Compare results using different SLM sizes (T5-small, T5-base, T5-large) on the same task to verify scaling relationships

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the energy-based OOD evaluation approach perform when dealing with highly noisy or ambiguous generated data?
- Basis in paper: [explicit] The paper mentions that the energy-based OOD evaluation approach is used to handle noisy data generated by LLMs, but does not provide extensive details on its performance in such scenarios.
- Why unresolved: The paper does not provide specific experiments or analysis on the performance of the energy-based OOD evaluation approach when dealing with highly noisy or ambiguous generated data.
- What evidence would resolve it: Conducting experiments with varying levels of noise in the generated data and analyzing the performance of the energy-based OOD evaluation approach in these scenarios would provide insights into its effectiveness.

### Open Question 2
- Question: How does the performance of GOLD scale with larger LLM and SLM sizes?
- Basis in paper: [explicit] The paper mentions an ablation study on LLM and SLM sizes, but does not explore the performance of GOLD with significantly larger models.
- Why unresolved: The paper only provides results for a specific range of LLM and SLM sizes, leaving open the question of how the performance of GOLD scales with even larger models.
- What evidence would resolve it: Conducting experiments with larger LLM and SLM sizes and comparing the performance of GOLD in these scenarios would provide insights into its scalability.

### Open Question 3
- Question: How does the performance of GOLD compare to other knowledge distillation methods on tasks outside the NLP domain?
- Basis in paper: [explicit] The paper focuses on NLP tasks and does not explore the performance of GOLD on tasks from other domains.
- Why unresolved: The paper does not provide any results or analysis on the performance of GOLD on tasks outside the NLP domain.
- What evidence would resolve it: Conducting experiments on tasks from other domains (e.g., computer vision, speech recognition) and comparing the performance of GOLD to other knowledge distillation methods would provide insights into its generalizability.

## Limitations
- Critical implementation details like specific prompts are relegated to the appendix, making faithful reproduction challenging
- Assumes all tasks can be effectively reformulated as sequence-to-sequence, which may not hold for specialized tasks
- Limited ablation studies on energy-based OOD evaluation parameters leave questions about robustness across different domains

## Confidence
**High Confidence Claims:**
- Task-agnostic framework design is valid for sequence-to-sequence formulation
- Symmetric cross-entropy loss handles noisy labels appropriately
- GOLD outperforms prior arts on average across 10 tasks

**Medium Confidence Claims:**
- OOD-guided feedback mechanism specifically improves generalizability
- Energy-based OOD evaluation is more robust than traditional methods for noisy labels
- Performance improvements translate to true generalization rather than overfitting

**Low Confidence Claims:**
- The 14% average improvement over LLM represents a meaningful comparison
- The approach generalizes to tasks outside the tested domains
- The feedback mechanism will maintain effectiveness with different LLM sizes or architectures

## Next Checks
1. **Prompt sensitivity analysis**: Systematically vary the prompt structure and content across multiple tasks to quantify the impact of prompt engineering on GOLD's performance
2. **OOD threshold robustness test**: Conduct an ablation study varying α and β parameters across different task domains to determine the sensitivity of the OOD selection mechanism
3. **Teacher model size scaling experiment**: Compare GOLD's performance using different LLM sizes (7B vs 13B vs 70B) on identical tasks to quantify the relationship between teacher model capacity and student model performance