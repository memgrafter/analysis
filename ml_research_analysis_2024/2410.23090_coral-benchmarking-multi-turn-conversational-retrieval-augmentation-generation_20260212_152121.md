---
ver: rpa2
title: 'CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation'
arxiv_id: '2410.23090'
source_url: https://arxiv.org/abs/2410.23090
tags:
- conversational
- question
- conversation
- response
- history
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CORAL is a large-scale benchmark for multi-turn conversational
  retrieval-augmented generation (RAG), addressing the gap between single-turn academic
  research and multi-turn real-world applications. The dataset is automatically constructed
  from Wikipedia by extracting title trees and converting them into information-seeking
  conversations using LLMs to generate natural language questions.
---

# CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation

## Quick Facts
- arXiv ID: 2410.23090
- Source URL: https://arxiv.org/abs/2410.23090
- Reference count: 40
- Primary result: Large-scale benchmark for multi-turn conversational RAG using Wikipedia-derived conversations

## Executive Summary
CORAL introduces a large-scale benchmark for multi-turn conversational retrieval-augmented generation (RAG) that bridges the gap between single-turn academic research and multi-turn real-world applications. The dataset is automatically constructed from Wikipedia by extracting title trees and converting them into information-seeking conversations using LLMs to generate natural language questions. CORAL supports three key tasks: passage retrieval, response generation, and citation labeling. Experiments with 8,000 conversations show that fine-tuned open-source LLMs outperform commercial models in retrieval, and compressing conversation history improves both response quality and citation accuracy. The dataset and unified framework provide standardized evaluation for conversational RAG systems.

## Method Summary
The CORAL benchmark is constructed by extracting Wikipedia title trees and using LLMs to convert these hierarchical structures into natural language conversations. The dataset includes 8,000 diverse information-seeking conversations with contextualized questions, free-form responses with citations, and golden retrieval passages. The evaluation framework supports three tasks: passage retrieval (using metrics like MRR, MAP, NDCG, Recall), response generation (evaluated with BLEU-1, ROUGE-L, and model-based metrics), and citation labeling (measuring Citation Recall and Precision). The system employs conversation compression strategies including Last Response, Rewrite, and LLM Summarization to manage conversation history length while maintaining performance.

## Key Results
- Fine-tuned open-source LLMs outperform commercial models in passage retrieval tasks
- Conversation compression strategies improve both response quality and citation accuracy
- Wikipedia-based conversation construction provides coherent multi-turn dialogues with logical topic progression
- The unified framework enables standardized evaluation across retrieval, generation, and citation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using Wikipedia's title trees as conversation seeds ensures coherent multi-turn dialogues.
- Mechanism: The hierarchical structure of Wikipedia pages naturally provides a logical flow from broad topics to specific subtopics, mirroring real conversational depth progression.
- Core assumption: Wikipedia's editorial quality and structural consistency translate directly into meaningful conversational paths.
- Evidence anchors:
  - [abstract]: "we introduce CORAL, a large-scale benchmark designed to assess RAG systems in realistic multi-turn conversational settings... derived from Wikipedia"
  - [section]: "treat the (sub)titles of Wikipedia pages as the source of questions... The content itself is originally well-cited"
- Break condition: Wikipedia page structure becomes too fragmented or inconsistent, breaking the logical progression assumption.

### Mechanism 2
- Claim: Context-aware question rewriting via LLMs improves retrieval quality by making queries more conversational.
- Mechanism: LLM-based rewriting incorporates co-reference, ellipsis, and omission patterns typical in real conversations, making queries more natural and aligned with how users actually ask questions.
- Core assumption: LLMs trained on conversational data can effectively transform keyword titles into natural language queries that preserve semantic intent.
- Evidence anchors:
  - [section]: "utilize powerful LLMs (e.g., GPT-4), to refine the original Wikipedia titles into well-formed conversational queries by incorporating contextual dependencies"
  - [corpus]: "Average neighbor FMR=0.508" indicates moderate relatedness between CORAL and related work, suggesting novel approach
- Break condition: LLM rewriting introduces noise or alters query intent, degrading retrieval performance.

### Mechanism 3
- Claim: Conversation compression strategies reduce noise and improve both generation quality and citation accuracy.
- Mechanism: Compressing conversation history (via last response, rewriting, or summarization) filters out irrelevant context while preserving critical information needed for current turn generation.
- Core assumption: Not all conversation history is equally relevant to the current query; selective compression can maintain performance while reducing input length.
- Evidence anchors:
  - [section]: "compressing conversation history and retrieved passages can negatively impact the system's efficiency and effectiveness"
  - [section]: "shortening the input length to filter noise can not only maintain response quality but also improve citation labeling accuracy"
- Break condition: Compression removes too much context, causing generation to lose track of conversation flow.

## Foundational Learning

- Concept: Information retrieval fundamentals (relevance ranking, evaluation metrics like MRR, MAP, NDCG)
  - Why needed here: The paper evaluates retrieval performance using these metrics, requiring understanding of how they measure ranking quality
  - Quick check question: If a system retrieves 5 relevant documents at ranks 1, 3, 5, 10, and 20, what is its MRR?

- Concept: Natural language generation evaluation (BLEU, ROUGE, model-based metrics)
  - Why needed here: The paper uses multiple metrics to evaluate generated responses, including rule-based and model-based approaches
  - Quick check question: Why might BLEU-1 be preferred over BLEU-4 for evaluating long-form responses?

- Concept: Knowledge graph and citation linking concepts
  - Why needed here: The citation labeling task requires understanding how to map generated statements to source passages
  - Quick check question: What distinguishes citation precision from citation recall in this context?

## Architecture Onboarding

- Component map: Data source → Wikipedia extraction → Title tree construction → Sampling strategy → LLM rewriting → Final dataset
  - Retrieval stage → Dense retrieval / Query rewriting → Compression → Generation stage → Response generation → Citation labeling

- Critical path: Wikipedia extraction → Title tree construction → Sampling strategy → LLM rewriting → Evaluation tasks (retrieval → generation → citation)

- Design tradeoffs:
  - Wikipedia-based construction provides high-quality content but may introduce domain bias
  - LLM rewriting improves query quality but adds computational cost and potential for intent drift
  - Compression improves efficiency but risks losing critical context

- Failure signatures:
  - Low retrieval scores indicate query rewriting isn't capturing intent correctly
  - Poor citation accuracy suggests generation isn't properly grounding in retrieved passages
  - Performance degradation with longer conversations indicates compression strategy issues

- First 3 experiments:
  1. Compare retrieval performance using original Wikipedia titles vs. LLM-rewritten queries
  2. Test different conversation compression strategies (last response, rewrite, summarization) on generation quality
  3. Evaluate scaling effects of generator model size on both generation quality and citation accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of conversational RAG systems degrade as conversation history length increases beyond the tested 10 turns?
- Basis in paper: [explicit] The paper tests history lengths from 2-10 turns and observes performance trade-offs between richer information and irrelevant noise.
- Why unresolved: The experiments only evaluate up to 10 turns, while real-world conversations often exceed this length.
- What evidence would resolve it: Extended experiments with conversation histories of 15-20+ turns, measuring performance degradation rates and identifying the optimal history length for different tasks.

### Open Question 2
- Question: How does the automatic Wikipedia-based conversation construction method compare to human-annotated conversational datasets in terms of real-world applicability?
- Basis in paper: [inferred] The paper uses an automatic LLM-based approach to construct conversations from Wikipedia, noting that no existing dataset satisfies all critical features.
- Why unresolved: The paper doesn't validate whether automatically constructed conversations capture the nuances and complexity of human-generated dialogues.
- What evidence would resolve it: Direct comparison studies between CORAL and human-annotated conversational datasets on downstream RAG task performance.

### Open Question 3
- Question: What are the long-term effects of using LLMs for both conversation construction and conversation compression in RAG systems?
- Basis in paper: [explicit] The paper uses LLMs (GPT-4) for question contextualization and LLM Summarization strategy for conversation compression.
- Why unresolved: The paper acknowledges potential contamination issues due to overlap between training data and Wikipedia, but doesn't investigate how this affects system performance over time.
- What evidence would resolve it: Studies measuring performance changes when using LLMs with varying degrees of training data overlap with Wikipedia content, and when using alternative compression methods.

## Limitations

- Dataset construction relies on Wikipedia title trees and LLM-generated conversations that may not fully capture real-world conversational complexity
- Evaluation framework depends on human annotations for citation labeling, introducing potential subjective bias and scalability limitations
- The synthetic nature of the dataset construction process limits confidence in claims about addressing real-world multi-turn application gaps

## Confidence

- **High Confidence**: The retrieval performance improvements from conversation compression are well-supported by quantitative results showing consistent gains across multiple metrics (MRR, Recall@20/100) when using compression strategies.
- **Medium Confidence**: The claim that fine-tuned open-source LLMs outperform commercial models in retrieval is supported by experiments, though the specific conditions and model configurations require more detail for full reproducibility.
- **Low Confidence**: The assertion that CORAL comprehensively addresses the gap between single-turn research and multi-turn real-world applications is limited by the synthetic nature of the dataset construction process.

## Next Checks

1. Conduct human evaluation studies comparing CORAL conversations with naturally occurring multi-turn dialogues to assess ecological validity and identify potential domain biases.
2. Test the conversation compression strategies on real-world conversational datasets (not synthetically generated) to verify whether the performance gains generalize beyond Wikipedia-based conversations.
3. Perform ablation studies isolating the impact of LLM rewriting versus title tree structure on retrieval performance to determine which mechanism drives the observed improvements.