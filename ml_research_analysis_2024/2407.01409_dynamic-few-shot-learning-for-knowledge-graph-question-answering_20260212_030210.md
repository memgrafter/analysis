---
ver: rpa2
title: Dynamic Few-Shot Learning for Knowledge Graph Question Answering
arxiv_id: '2407.01409'
source_url: https://arxiv.org/abs/2407.01409
tags:
- dfsl
- wikidata
- question
- where
- sparql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dynamic Few-Shot Learning (DFSL), a novel
  approach for Knowledge Graph Question Answering (KGQA) that leverages semantic search
  to retrieve relevant training examples dynamically and inject them into prompts
  for large language models (LLMs). DFSL addresses the challenge of generating SPARQL
  queries from natural language questions over knowledge graphs by combining in-context
  learning with semantic similarity retrieval.
---

# Dynamic Few-Shot Learning for Knowledge Graph Question Answering

## Quick Facts
- arXiv ID: 2407.01409
- Source URL: https://arxiv.org/abs/2407.01409
- Reference count: 25
- Key outcome: DFSL achieves up to 21 absolute F1 points improvement over few-shot learning baselines on KGQA benchmarks

## Executive Summary
This paper introduces Dynamic Few-Shot Learning (DFSL), a novel approach for Knowledge Graph Question Answering that leverages semantic search to retrieve relevant training examples dynamically and inject them into prompts for large language models. DFSL addresses the challenge of generating SPARQL queries from natural language questions over knowledge graphs by combining in-context learning with semantic similarity retrieval. The method outperforms both standard in-context learning and state-of-the-art fine-tuned models on multiple benchmarks including QALD-9, QALD-9 Plus, QALD-10, and LC-QuAD 2.0.

## Method Summary
DFSL uses semantic search to retrieve similar questions from training data and enrich prompts for LLMs to generate SPARQL queries. The method encodes questions, entities, and relations into vectors using sentence transformers, retrieves top-k similar examples based on cosine similarity, and constructs enriched prompts for query generation. DFSL-MQ extends this with multi-query generation using beam search to mitigate triple-flip errors, employing answer selection heuristics (First Set or Largest Set) to choose the final query.

## Key Results
- DFSL outperforms both standard few-shot learning and fine-tuned models on QALD-9, QALD-10, and LC-QuAD 2.0 benchmarks
- DFSL-MQ with First Set heuristic consistently outperforms Largest Set for answer selection
- Including entities and relations in embeddings improves query generation quality
- Mixtral, Llama-3, and CodeLlama backbones all perform well, with Mixtral showing strong results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic retrieval of semantically similar examples improves prompt quality over static few-shot prompts
- Mechanism: By embedding the query, entities, and relations into a vector space and retrieving k-nearest examples, the prompt is enriched with contextually relevant demonstrations that guide the LLM toward correct SPARQL syntax and entity/relation usage
- Core assumption: The similarity metric (cosine) and embedding model capture semantic equivalence between questions and demonstrations
- Evidence anchors: [abstract], [section 3.1], [corpus]

### Mechanism 2
- Claim: Multi-query generation mitigates triple-flip errors by presenting alternative subject-object orderings
- Mechanism: Beam search produces multiple candidate queries; each candidate may flip subject-object roles; answer selection picks the query yielding non-empty, most relevant answer set
- Core assumption: Triple-flip errors are symmetric and can be corrected by including both orientations
- Evidence anchors: [abstract], [section 3.3], [section 4.5], [corpus]
- Break condition: LLM never produces the correct orientation in beam hypotheses; answer selection heuristic selects incorrect query

### Mechanism 3
- Claim: Encoding both entities and relations in the retrieval embedding improves example relevance and query accuracy
- Mechanism: Embedding function concatenates question, entities, and relations into a single vector, capturing richer semantic context for retrieval
- Core assumption: Including entities and relations in the embedding improves similarity scoring for KGQA tasks
- Evidence anchors: [section 4.6], [abstract], [corpus]
- Break condition: Entities/relations are noisy or irrelevant, degrading retrieval quality

## Foundational Learning

- Concept: Semantic similarity and vector embeddings
  - Why needed here: DFSL relies on retrieving similar examples based on semantic proximity; understanding how embeddings capture meaning is critical
  - Quick check question: Given two sentences with same intent but different wording, will their embeddings be close under cosine similarity?

- Concept: Beam search and hypothesis ranking
  - Why needed here: DFSL-MQ uses beam search to generate multiple SPARQL candidates; engineer must understand beam pruning and score propagation
  - Quick check question: If beam size is 3, how many distinct SPARQL queries are produced per prompt?

- Concept: SPARQL query structure and triple ordering
  - Why needed here: Triple-flip errors arise from subject-object confusion; engineer must recognize correct subject-object pairs in queries
  - Quick check question: In the triple `?uri wdt:P19 wd:Q1339`, which is subject and which is object?

## Architecture Onboarding

- Component map:
  Question + gold entities + relations → embedding encoder → cosine similarity → top-k retrieval → prompt template → LLM input → SPARQL output → beam search (DFSL-MQ) → SPARQL engine → answer sets → answer selection (FS/LS) → final answer

- Critical path:
  Retrieve examples → build prompt → generate query → execute → select answer

- Design tradeoffs:
  - k vs. prompt size: larger k may improve retrieval but risk exceeding LLM context window
  - Beam size b: larger b increases coverage but raises computation cost
  - Embedding granularity: including entities/relations improves relevance but may overfit to specific entities

- Failure signatures:
  - Empty answer sets: possible triple-flip error or missing entities/relations
  - Wrong answers: incorrect retrieval or prompt structure
  - Runtime errors: malformed SPARQL or LLM hallucination

- First 3 experiments:
  1. Verify embedding similarity: encode two semantically equivalent questions, confirm high cosine score
  2. Test retrieval quality: given a held-out question, retrieve top-3 examples and inspect their relevance
  3. Validate DFSL baseline: run DFSL with k=3, b=3, FS on a small subset, compare F1 vs. zero-shot

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of retrieved examples (k) for DFSL across different dataset sizes and knowledge graph types?
- Basis in paper: Explicit - The paper states "We set k = 5 for all the forthcoming experiments, which is a good trade-off across all the datasets" but acknowledges that "increasing k seems to be beneficial" on larger datasets like LC-QuAD 2.0
- Why unresolved: The paper only tested k values of {1, 3, 5, 7} and found k = 5 to be a reasonable compromise, but did not conduct an exhaustive search for the optimal k value
- What evidence would resolve it: A comprehensive study testing a wider range of k values (e.g., 1-20) on various datasets and knowledge graphs, potentially using techniques like cross-validation or Bayesian optimization to find the optimal k for each scenario

### Open Question 2
- Question: How does DFSL perform on languages other than English, and what are the potential limitations or biases introduced by training on English-centric datasets?
- Basis in paper: Inferred - The paper states "Our experiments are all on English-based datasets, where notoriously LLMs are better performing" and acknowledges this as a limitation
- Why unresolved: The paper only evaluated DFSL on English datasets and did not explore its performance on multilingual or non-English datasets
- What evidence would resolve it: Experiments evaluating DFSL on multilingual datasets or datasets in languages other than English, comparing its performance to English datasets and analyzing any potential biases or limitations that arise

### Open Question 3
- Question: How does the choice of sentence encoder and similarity metric impact the performance of DFSL, and are there more effective alternatives to the all-mpnet-base-v2 encoder and cosine similarity used in the paper?
- Basis in paper: Inferred - The paper states "To encode examples, we limited the investigation to what kind of text to encode (just the question, or the question and its entities and relations), without exploring different embedding models, similarity criteria or other input concatenation strategies"
- Why unresolved: The paper used a specific sentence encoder (all-mpnet-base-v2) and similarity metric (cosine similarity) without exploring alternatives
- What evidence would resolve it: Experiments comparing the performance of DFSL using different sentence encoders (e.g., BERT, RoBERTa, or domain-specific encoders) and similarity metrics (e.g., Euclidean distance, dot product, or learned similarity functions) to determine which combination yields the best retrieval and overall DFSL performance

## Limitations

- Embedding Quality Dependency: The method's success heavily depends on the quality of the semantic embeddings used for retrieval
- Triple-Flip Error Mitigation: DFSL-MQ only "alleviates" rather than solves triple-flip errors completely
- Context Window Constraints: Effectiveness may degrade as knowledge graphs grow larger or questions become more complex

## Confidence

- High Confidence: The empirical results showing DFSL outperforming both standard few-shot learning and fine-tuned models on multiple benchmarks
- Medium Confidence: The mechanism explanations for why dynamic retrieval improves performance
- Low Confidence: The claim that including entities and relations in embeddings consistently improves retrieval quality

## Next Checks

1. **Embedding Robustness Test**: Run DFSL with different embedding models (including KG-specific embeddings) on the same benchmarks to quantify how sensitive performance is to embedding quality and whether domain-specific embeddings provide additional gains

2. **Triple-Flip Error Analysis**: Conduct a systematic error analysis on a subset of questions where DFSL-MQ fails, categorizing whether failures stem from incorrect beam hypotheses, poor answer selection, or fundamental limitations in the LLM's ability to generate correct SPARQL

3. **Retrieval Quality Evaluation**: Implement human evaluation of retrieved examples for a sample of test questions, measuring whether top-k retrievals are semantically relevant SPARQL demonstrations and whether this relevance correlates with downstream performance improvements