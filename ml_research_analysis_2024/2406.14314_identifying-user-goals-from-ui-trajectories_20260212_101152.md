---
ver: rpa2
title: Identifying User Goals from UI Trajectories
arxiv_id: '2406.14314'
source_url: https://arxiv.org/abs/2406.14314
tags:
- task
- user
- intent
- arxiv
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new task of identifying user goals from
  UI trajectories, which involves inferring detailed user intentions from observed
  interactions within UI environments. This task is framed as the inverse of UI automation,
  leveraging existing datasets designed for that purpose.
---

# Identifying User Goals from UI Trajectories

## Quick Facts
- arXiv ID: 2406.14314
- Source URL: https://arxiv.org/abs/2406.14314
- Authors: Omri Berkovitch; Sapir Caduri; Sapir Caduri; Anatoly Efros; Avi Caciularu; Ido Dagan
- Reference count: 40
- One-line primary result: Human performance exceeds state-of-the-art LMMs on UI trajectory goal identification task.

## Executive Summary
This paper introduces a novel task of identifying user goals from UI trajectories, where the objective is to infer detailed user intentions from observed interactions within UI environments. The task is framed as the inverse of UI automation, leveraging existing datasets designed for that purpose. A novel evaluation methodology is proposed to assess whether predicted and gold task descriptions are paraphrases within a specific UI context. Experiments compare human performance with state-of-the-art models (GPT-4 and Gemini-1.5 Pro) on web and Android datasets, showing that both models underperform compared to humans.

## Method Summary
The task is formulated as generating natural language task descriptions from sequences of UI interactions (screenshots + actions). Existing UI automation datasets (Mind2Web for web, Android in the Zoo for Android) are leveraged by reversing their input/output roles. State-of-the-art LMMs are used with Chain-of-Thought prompting to generate task descriptions from trajectories. Evaluation uses a satisfaction relation between tasks - two descriptions are considered a match if each satisfies the other within the UI environment context. Both manual evaluation and automatic evaluation using GPT-4o are employed.

## Key Results
- Human performance exceeds both GPT-4 and Gemini-1.5 Pro on web and Android datasets
- Model-generated task descriptions are either too specific or too general compared to gold references
- Automatic evaluation using LMMs shows moderate agreement (Kappa 0.48) with human evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The task can be framed as the inverse of UI automation by swapping input and output roles
- Mechanism: Existing UI automation datasets contain natural language task descriptions paired with UI trajectories. By reversing the roles - using the trajectory as input and the task description as output - the task of identifying user goals becomes feasible without collecting new data
- Core assumption: UI automation datasets are sufficiently diverse and representative of real user goals
- Evidence anchors:
  - [abstract] "Furthermore, we demonstrate how this task can leverage datasets designed for the inverse problem of UI automation, utilizing Android and web datasets for our experiments."
  - [section 3] "We therefore adopt their input and output definitions, swapping their roles, which enables the use of UI automation datasets for our task as well."
- Break condition: If UI automation datasets are too narrow in scope, or if task descriptions are not truly representative of natural user intents, the inversion approach fails

### Mechanism 2
- Claim: A satisfaction relation between tasks allows for meaningful evaluation of predicted vs. gold task descriptions
- Mechanism: Two task descriptions are considered a match if each satisfies the other within the UI environment context. This handles the inherent ambiguity where multiple goals can lead to the same trajectory
- Core assumption: The notion of task satisfaction is well-defined and computationally tractable
- Evidence anchors:
  - [section 4.1] "We say that A satisfies B in that environment if every reasonable trajectory that fulfills A would also fulfill B."
  - [section 4.2] "We consider a predicted task description to successfully match the gold description if the two mutually satisfy each other."
- Break condition: If the satisfaction relation is too strict or too lenient, evaluation becomes meaningless or overly permissive

### Mechanism 3
- Claim: Large multimodal models can serve as automatic evaluators for task satisfaction
- Mechanism: GPT-4o is prompted to assess whether two task descriptions are mutually satisfied in the context of a trajectory, providing a scalable alternative to human evaluation
- Core assumption: LMMs can reliably interpret both visual and textual information to judge task satisfaction
- Evidence anchors:
  - [section 4.3] "We leverage the latest GPT-4o model as the automatic evaluator, to determine whether two task descriptions are mutually satisfied in the context of the trajectory."
  - [section 4.3] "Measuring agreement with our human evaluation yielded a Kappa value of 0.48 (moderate agreement), suggesting a potential utility of model-based automatic evaluation for development cycles while highlighting the need for manual evaluation."
- Break condition: If LMMs lack sufficient understanding of UI context or task semantics, their judgments will be unreliable

## Foundational Learning

- Concept: UI trajectory structure and semantics
  - Why needed here: Models must interpret sequences of UI interactions (screenshots + actions) to infer user goals
  - Quick check question: Can you describe the difference between a click action and a type action in a UI trajectory?

- Concept: Task satisfaction and fulfillment
  - Why needed here: Evaluation hinges on understanding when one task logically implies another
  - Quick check question: Does "Buy a train ticket to Edinburgh" satisfy "Purchase the earliest train ticket to Edinburgh"? Why or why not?

- Concept: Multimodal model prompting for task generation
  - Why needed here: Models must be guided through a step-by-step analysis of the trajectory to produce accurate task descriptions
  - Quick check question: What is the purpose of using a Chain-of-Thought approach when prompting LMMs for this task?

## Architecture Onboarding

- Component map: Data ingestion -> Trajectory preprocessing -> Model prediction -> Evaluation (manual/automatic)
- Critical path: Raw trajectory -> Model input (formatted screenshots + actions) -> Predicted task description -> Evaluation
- Design tradeoffs: Manual vs. automatic evaluation (accuracy vs. scalability); detailed vs. general task descriptions (precision vs. ambiguity handling)
- Failure signatures: Model produces navigation steps instead of goals; model misidentifies task intent; evaluation disagreement between human and automatic methods
- First 3 experiments:
  1. Test model prediction on a small subset of Mind2Web with manual evaluation only
  2. Implement and validate the automatic evaluator on the same subset
  3. Compare manual vs. automatic evaluation results and analyze disagreement patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to multimodal models (GPT-4 or Gemini) would improve performance on UI trajectory intent recognition?
- Basis in paper: Explicit - The paper shows current models underperform humans and discusses limitations like missing details, incorporating irrelevant information, and hallucinations
- Why unresolved: The paper only uses off-the-shelf models without fine-tuning or architectural modifications. It identifies problems but doesn't explore solutions
- What evidence would resolve it: Controlled experiments comparing baseline models against models with fine-tuned visual components, specialized UI understanding modules, or domain adaptation techniques

### Open Question 2
- Question: How does task ambiguity in UI trajectories vary across different domains (e.g., web shopping vs. general vs. settings configuration)?
- Basis in paper: Explicit - The paper mentions different performance across domains in Android dataset and discusses ambiguous trajectories causing human disagreements
- Why unresolved: The paper provides preliminary observations but doesn't systematically measure or characterize ambiguity across domains
- What evidence would resolve it: Quantitative analysis of agreement rates across different domain categories, identifying which types of tasks are inherently more ambiguous and why

### Open Question 3
- Question: Would incorporating user behavior patterns (like common navigation paths or typical interaction sequences) improve intent recognition accuracy?
- Basis in paper: Inferred - The paper discusses limitations of current models and mentions that real-world scenarios might involve interleaved tasks and evolving goals
- Why unresolved: The current evaluation framework assumes a single, clear trajectory but doesn't leverage historical user behavior or common patterns
- What evidence would resolve it: Experiments comparing models trained on individual trajectories versus models incorporating user behavior patterns or interaction history

## Limitations

- The study is limited to Android and web environments, potentially limiting generalizability to other GUI environments like iOS or desktop applications
- Automatic evaluation using LMMs shows only moderate agreement with human evaluation, raising questions about its reliability
- The evaluation protocol may be too strict, rejecting valid task descriptions that are paraphrases but not exact matches

## Confidence

- **High**: The novelty of the task and the demonstration of human performance as an upper bound
- **Medium**: The inversion approach using UI automation datasets and the satisfaction relation for evaluation
- **Low**: The reliability of automatic evaluation using LMMs as a scalable alternative to human judgment

## Next Checks

1. Evaluate the satisfaction relation's sensitivity by testing how the evaluation protocol handles borderline cases where predicted and gold tasks are semantically related but not paraphrases

2. Benchmark automatic evaluator reliability by conducting a larger-scale human evaluation to establish a more robust baseline for the automatic evaluator's performance

3. Assess dataset representativeness by analyzing the coverage of real-world user goals in the Mind2Web and AitZ datasets and comparing with actual UI usage statistics