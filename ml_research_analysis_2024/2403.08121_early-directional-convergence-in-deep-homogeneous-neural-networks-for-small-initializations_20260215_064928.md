---
ver: rpa2
title: Early Directional Convergence in Deep Homogeneous Neural Networks for Small
  Initializations
arxiv_id: '2403.08121'
source_url: https://arxiv.org/abs/2403.08121
tags:
- neural
- networks
- have
- small
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes that deep homogeneous neural networks (order
  of homogeneity 2, locally Lipschitz gradients) exhibit early directional convergence
  when trained with small initializations. For sufficiently small initializations,
  the network weights remain small in norm and approximately converge in direction
  to KKT points of the Neural Correlation Function (NCF) during early training stages.
---

# Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations

## Quick Facts
- arXiv ID: 2403.08121
- Source URL: https://arxiv.org/abs/2403.08121
- Reference count: 40
- One-line primary result: Deep homogeneous neural networks (order > 2) with small initializations exhibit early directional convergence to KKT points of the Neural Correlation Function

## Executive Summary
This paper establishes that deep homogeneous neural networks with order of homogeneity greater than 2 and locally Lipschitz gradients exhibit early directional convergence when trained with small initializations. The authors prove that for sufficiently small initializations, the network weights remain small in norm and approximately converge in direction to KKT points of the Neural Correlation Function (NCF) during early training stages. The theoretical framework extends previous work limited to two-homogeneous networks to deeper architectures, providing new insights into the behavior of homogeneous neural networks during early optimization.

## Method Summary
The authors analyze gradient flow dynamics in deep homogeneous neural networks by comparing them with the gradient flow of the Neural Correlation Function (NCF). They use time rescaling by δ^(L-2), where δ controls initialization scale, to compensate for the L-homogeneity of the network. The analysis establishes that for small enough δ, the gradient flow dynamics of the network and NCF remain close, leading to directional convergence of the weights. The proof technique relies on the unbounded version of the Kurdyka-Lojasiewicz inequality and requires the network to be definable under an o-minimal structure.

## Key Results
- Deep homogeneous neural networks (L > 2) with small initializations exhibit early directional convergence to KKT points of the NCF
- Two scenarios emerge: weights either converge in direction to a non-negative KKT point of the constrained NCF or approximately converge to zero
- The results extend directional convergence theory from two-homogeneous to L-homogeneous networks (L > 2)
- ReLU networks are excluded due to non-differentiability, though the theory applies to deep linear networks and networks with differentiable homogeneous activation functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: For sufficiently small initialization, neural network weights remain small in norm and approximately converge in direction to KKT points of the Neural Correlation Function (NCF) during early training stages.
- Mechanism: The paper compares gradient flow dynamics of the deep homogeneous neural network with that of the NCF by rescaling time by δ^(L-2), where δ controls initialization scale. This rescaling compensates for the L-homogeneity of the network, making the gradient dynamics effectively independent of initialization scale for small δ.
- Core assumption: The neural network is L-homogeneous with L > 2 and has locally Lipschitz gradients. The initialization δ is sufficiently small.
- Evidence anchors:
  - [abstract]: "for sufficiently small initializations, during the early stages of training, the weights of the neural network remain small in norm and approximately converge in direction to the Karush-Kuhn-Tucker (KKT) points of the recently introduced neural correlation function."
  - [section 5.1.1]: "We next explain our proof technique for Theorem 5.1 assuming square loss is used for training... Now, we compare the dynamics for s(t) in (8) with the dynamics of gradient flow of the NCF defined with respect to y and neural network H."
  - [corpus]: Weak evidence - the corpus contains related works on directional convergence but not specifically on deep homogeneous networks with L > 2.
- Break condition: If the initialization is not sufficiently small, or if the network is not L-homogeneous with L > 2, or if the network does not have locally Lipschitz gradients.

### Mechanism 2
- Claim: The weights of small magnitude wz approximately converge in direction to a KKT point of the constrained NCF defined with respect to the residual error at the saddle point, or approximately converge to zero.
- Mechanism: The paper studies gradient flow dynamics near certain saddle points where only a certain number of weights are non-zero while others are zero. It shows that for small initialization near such saddle points, the weights of small magnitude remain small and either converge in direction to a KKT point of the constrained NCF or approximately converge to zero.
- Core assumption: The saddle point satisfies Assumption 2, which requires that only a certain number of weights are non-zero while others are zero, and there exists a γ > 0 such that ⟨wn - wn, ∇wnL(wn, 0)⟩ ≥ 0 if ∥wn - wn∥2 ≤ γ.
- Evidence anchors:
  - [section 5.2]: "In this section, we assume that the neural network has a separable structure... Now, let Hn(X; wn) and Hz(X; wz) be defined similarly to H(X; w), then for square loss, the training loss is L(wn, wz) = 1/2 ∥Hn(X; wn) + Hz(X; wz) - y∥2."
  - [section 5.2]: "Theorem 5.3 establishes a similar directional convergence for deep homogeneous neural networks near certain saddle points, provided the weights of neural network have a separable structure."
  - [corpus]: Weak evidence - the corpus contains related works on saddle-to-saddle dynamics but not specifically on deep homogeneous networks with separable structures.
- Break condition: If the saddle point does not satisfy Assumption 2, or if the neural network does not have a separable structure.

### Mechanism 3
- Claim: For deep homogeneous neural networks with L > 2, the gradient flow dynamics of the NCF can become unbounded at some finite time, unlike for two-homogeneous networks.
- Mechanism: The paper uses the unbounded version of the Kurdyka-Lojasiewicz inequality to establish directional convergence, which is necessary because the gradient flow dynamics of the NCF for deep homogeneous networks can become unbounded at some finite time.
- Core assumption: The neural network is definable under some o-minimal structure that includes polynomials and exponential.
- Evidence anchors:
  - [section 5.1.1]: "To prove directional convergence in the above lemma, we require the neural networks to be definable with respect to an o-minimal structure. Such a requirement allows us to use the unbounded version of Kurdyka-Lojasiewicz inequality proved in [31] for establishing directional convergence."
  - [section 5.1.1]: "However, in that instance, the solution u(t) remains finite for all finite time, whereas in our case, u(t) may become unbounded at some finite time."
  - [corpus]: Weak evidence - the corpus contains related works on directional convergence but not specifically on the use of the unbounded version of the Kurdyka-Lojasiewicz inequality for deep homogeneous networks.
- Break condition: If the neural network is not definable under some o-minimal structure that includes polynomials and exponential.

## Foundational Learning

- Concept: L-homogeneity of neural networks
  - Why needed here: The paper studies deep homogeneous neural networks with L > 2, and the L-homogeneity property is crucial for the time rescaling by δ^(L-2) and for establishing the directional convergence.
  - Quick check question: What is the order of homogeneity L for a neural network where H(x; cw) = c^L H(x; w) for all c ≥ 0?

- Concept: Karush-Kuhn-Tucker (KKT) points
  - Why needed here: The paper establishes that the weights of the neural network approximately converge in direction to KKT points of the constrained NCF, and understanding KKT points is essential for interpreting the results.
  - Quick check question: What are the necessary conditions for a point to be a KKT point of a constrained optimization problem?

- Concept: Gradient flow dynamics
  - Why needed here: The paper studies the gradient flow dynamics of deep homogeneous neural networks, and understanding gradient flow is essential for interpreting the results.
  - Quick check question: What is the differential equation that describes the gradient flow dynamics for minimizing a function L(w)?

## Architecture Onboarding

- Component map: Neural Network (L-homogeneous, L > 2) -> Gradient Flow Optimization -> Small Initialization (δ) -> Time Rescaling (δ^(L-2)) -> Directional Convergence to KKT Points

- Critical path:
  1. Initialize the neural network with small weights
  2. Compute the gradient flow dynamics of the neural network and the NCF
  3. Compare the gradient flow dynamics using time rescaling by δ^(L-2)
  4. Establish the directional convergence of the weights to KKT points of the constrained NCF

- Design tradeoffs:
  - The paper assumes that the neural network has locally Lipschitz gradients, which excludes deep ReLU networks. However, it includes deep linear networks and deep neural networks with differentiable and homogeneous activation functions such as higher powers of ReLU and polynomials.
  - The paper assumes that the neural network has a separable structure when studying gradient flow dynamics near certain saddle points, which excludes fully connected deep neural networks.

- Failure signatures:
  - If the initialization is not sufficiently small, the weights may not remain small in norm and may not converge in direction to KKT points of the constrained NCF.
  - If the neural network is not L-homogeneous with L > 2, the time rescaling by δ^(L-2) may not be effective, and the directional convergence may not be established.
  - If the neural network does not have locally Lipschitz gradients, the results may not be applicable.

- First 3 experiments:
  1. Implement a deep homogeneous neural network with L > 2 and locally Lipschitz gradients, and train it with small initialization using gradient flow. Observe the directional convergence of the weights to KKT points of the constrained NCF.
  2. Implement a deep homogeneous neural network with L > 2 and a separable structure, and train it with small initialization near a saddle point using gradient flow. Observe the directional convergence of the weights of small magnitude to KKT points of the constrained NCF or their convergence to zero.
  3. Implement a deep homogeneous neural network with L > 2 and compare the gradient flow dynamics with and without time rescaling by δ^(L-2). Observe the effect of time rescaling on the directional convergence of the weights.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the emergence of low-rank structures in the weights of deep homogeneous neural networks during early training with small initializations?
- Basis in paper: [explicit] The paper mentions that similar low-rank behavior emerges in experiments with 3-homogeneous ReLU networks and that this phenomenon is not explained by Theorem 5.1.
- Why unresolved: The proof of Theorem 5.1 only establishes directional convergence to KKT points of the NCF without explaining structural properties of the weights themselves. The low-rank structure appears to be an additional phenomenon that requires separate investigation.
- What evidence would resolve it: A mathematical analysis showing how the gradient flow dynamics with small initializations naturally leads to weight matrices with low-rank structures, possibly by connecting the directional convergence to specific weight configurations that minimize certain regularizers.

### Open Question 2
- Question: Can Theorem 5.1 be extended to ReLU neural networks despite the lack of locally Lipschitz gradients?
- Basis in paper: [explicit] The paper acknowledges that ReLU networks are excluded due to non-differentiability and identifies the key challenge in extending the result: showing that solutions of the Clarke subdifferential dynamics remain close for sufficiently long time when δ is small.
- Why unresolved: The proof relies critically on locally Lipschitz gradients to establish that the difference between the network gradient flow and NCF gradient flow remains small. For ReLU networks, the Clarke differential is not Lipschitz, making this comparison difficult.
- What evidence would resolve it: A rigorous proof demonstrating that for ReLU networks, the solutions of the gradient flow dynamics (using Clarke subdifferentials) and NCF gradient flow remain sufficiently close in ℓ2 norm for a duration inversely proportional to δ^(L-2).

### Open Question 3
- Question: Can Theorem 5.3 be extended to fully connected deep neural networks without the separable structure assumption?
- Basis in paper: [explicit] The paper states that the separable structure assumption is crucial for their analysis and that Theorem 5.3 is limited to neural networks with such structure, explicitly excluding fully connected deep networks.
- Why unresolved: The current proof relies on partitioning weights into non-zero and zero sets with additive network outputs, which is not possible for fully connected architectures. The authors suggest that structural assumptions on saddle points would be necessary but have not been identified.
- What evidence would resolve it: A characterization of the saddle points encountered during training of fully connected deep networks, followed by a proof showing that gradient flow initialized near such saddle points exhibits directional convergence of the small-norm weights to KKT points of the residual NCF.

## Limitations
- The theoretical guarantees exclude popular ReLU networks due to non-differentiability requirements
- Results rely on specific structural assumptions (separable weight configurations) that don't hold for standard fully connected architectures
- Time-rescaling by δ^(L-2) requires careful numerical implementation and may introduce instability
- The asymptotic nature of the results may not capture practical finite-time behaviors during training

## Confidence
- High confidence: The mathematical framework for L-homogeneous networks and directional convergence to KKT points is well-established for the theoretical class of networks studied
- Medium confidence: The extension from two-homogeneous to L-homogeneous networks (>2) is theoretically sound but may have practical limitations due to numerical considerations
- Low confidence: The applicability to practical deep learning architectures is limited due to structural constraints (Lipschitz gradients, separable configurations)

## Next Checks
1. Implement gradient flow with varying initialization scales δ to verify the threshold behavior where directional convergence emerges only for sufficiently small initializations
2. Test the time-rescaling mechanism δ^(L-2) on synthetic L-homogeneous networks to confirm it stabilizes the gradient flow dynamics as predicted
3. Construct examples of saddle points satisfying Assumption 2 to validate the directional convergence behavior for weights of small magnitude near these critical points