---
ver: rpa2
title: Elucidating Optimal Reward-Diversity Tradeoffs in Text-to-Image Diffusion Models
arxiv_id: '2409.06493'
source_url: https://arxiv.org/abs/2409.06493
tags:
- reward
- base
- images
- diffusion
- regularization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies reward hacking in text-to-image diffusion models,
  where finetuning on human preference data causes mode collapse and loss of diversity.
  It proves that reward hacking is inevitable without regularization and analyzes
  existing approaches like KL divergence and LoRA scaling, identifying their limitations
  in indiscriminate regularization and reference mismatch.
---

# Elucidating Optimal Reward-Diversity Tradeoffs in Text-to-Image Diffusion Models

## Quick Facts
- arXiv ID: 2409.06493
- Source URL: https://arxiv.org/abs/2409.06493
- Reference count: 40
- This paper proposes Annealed Importance Guidance (AIG) to prevent reward hacking and preserve diversity in text-to-image diffusion models during reward-based finetuning.

## Executive Summary
This paper addresses the challenge of reward hacking in text-to-image diffusion models, where finetuning on human preference data causes mode collapse and loss of diversity. The authors prove that reward hacking is inevitable without regularization and analyze existing approaches like KL divergence and LoRA scaling, identifying their limitations. They propose Annealed Importance Guidance (AIG), an inference-time regularization that gradually transitions sampling dynamics from the base model to the reward-finetuned model, preserving diversity while optimizing rewards. Experiments with Stable Diffusion v1.4 and XL on PickScore and HPSv2 rewards show AIG achieves Pareto-optimal reward-diversity tradeoffs compared to baselines.

## Method Summary
The paper proposes finetuning diffusion models with Direct Reward Alignment with Transformers (DRaFT) using LoRA parameters, then applying different regularization methods during inference. Annealed Importance Guidance (AIG) interpolates between base and finetuned score functions using a monotonic schedule γ(t), allowing early timesteps to recover diversity from the base model while later timesteps optimize rewards. The method is evaluated against baselines including KL regularization and LoRA scaling on reward diversity tradeoffs using metrics like reward scores, FID, Recall, and a novel Spectral Covariance Distance metric.

## Key Results
- Reward hacking is mathematically inevitable in diffusion model finetuning without regularization, causing mode collapse to a Dirac delta distribution
- AIG achieves better reward-diversity tradeoffs than KL regularization and LoRA scaling by selectively applying regularization across timesteps
- User study confirms AIG improves both diversity and quality of generated images across different model architectures and reward functions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reward hacking is inevitable in diffusion model finetuning without regularization.
- **Mechanism:** The expected reward maximization objective over non-parametric distributions collapses to a Dirac delta distribution, leading to mode collapse.
- **Core assumption:** The reward function is sufficiently smooth and bounded above.
- **Evidence anchors:**
  - [abstract]: "Although effective, these methods are vulnerable to reward hacking, where the model overfits to the reward function, leading to a loss of diversity in the generated images."
  - [section 3.1]: "Lemma 1. Inevitability of Reward Hacking... the optimal probability distribution p(x0|c) collapses to p(x0|c) = δ(x0 − x∗rc0), x∗rc0 = arg maxx0 r(x0, c), where δ is the Dirac-delta function."
- **Break condition:** If the reward function has multiple global maxima with equal values, or if the distribution is restricted to a low-dimensional parameter space that cannot represent the delta distribution.

### Mechanism 2
- **Claim:** Annealed Importance Guidance (AIG) achieves better reward-diversity tradeoffs by selectively applying regularization across timesteps.
- **Mechanism:** AIG interpolates between base and finetuned score functions using a monotonic schedule γ(t), allowing early timesteps to recover diversity from the base model while later timesteps optimize rewards.
- **Core assumption:** Early sampling steps contribute more to mode recovery while later steps add fine-grained details.
- **Evidence anchors:**
  - [abstract]: "We also introduce Annealed Importance Guidance (AIG), an inference-time regularization inspired by Annealed Importance Sampling, which retains the diversity of the base model while achieving Pareto-Optimal reward-diversity tradeoffs."
  - [section 3.3]: "Early mixing from pbase ensures that modes from the data distribution are recovered... later mixing from pθ then pushes the noisy data to the nearest high-reward mode."
- **Break condition:** If the monotonic function γ(t) is poorly chosen, leading to insufficient regularization or over-regularization at critical timesteps.

### Mechanism 3
- **Claim:** Spectral Covariance Distance (SCD) provides a more appropriate diversity metric than FID by accounting for reference mismatch.
- **Mechanism:** SCD measures differences in the spread of feature distributions by comparing eigenvalues after aligning principal eigenvectors, ignoring translational and rotational differences.
- **Core assumption:** Reference mismatch between base and finetuned models should not be penalized as loss of diversity.
- **Evidence anchors:**
  - [section 4.1]: "One limitation of FID and Generalized Recall is that these metrics were created to ensure exact overlap between the data and generator distribution. Consequently, they penalize any 'reference mismatch' w.r.t. the base distribution as well."
  - [section 4.1]: "We propose the Spectral distance as the differences in corresponding eigenvalues...SCD(D1, D2) = ∥Σ′1 − Σ2∥2 2 = PN i=1(λ(i) 1 − λ(i) 2 )2"
- **Break condition:** If the reference model and finetuned model have fundamentally different feature distributions that should be considered diverse, not just misaligned.

## Foundational Learning

- **Concept:** Diffusion models and score matching
  - Why needed here: The paper builds on diffusion model fundamentals to understand reward hacking and design AIG.
  - Quick check question: How does the reverse-time SDE formulation relate to score matching in diffusion models?

- **Concept:** Expected reward maximization and its limitations
  - Why needed here: Understanding why reward hacking is inevitable requires knowledge of the mathematical formulation of expected reward maximization.
  - Quick check question: What is the optimal distribution under expected reward maximization in the non-parametric case, and why does it lead to mode collapse?

- **Concept:** Regularization techniques in generative models
  - Why needed here: The paper analyzes KL divergence and LoRA scaling as regularizations and proposes AIG as an alternative.
  - Quick check question: How do KL divergence and LoRA scaling regularize diffusion models differently, and what are their limitations?

## Architecture Onboarding

- **Component map:** Diffusion model (UNet) with LoRA parameters for finetuning -> Reward model (CLIP-based or other preference models) -> Annealed Importance Guidance inference-time regularization -> Evaluation metrics (reward scores, diversity metrics, CLIP alignment)

- **Critical path:**
  1. Initialize diffusion model with LoRA parameters
  2. Train on reward model using DRaFT with various regularization methods
  3. Apply AIG during inference with different γ(t) schedules
  4. Evaluate on PartiPrompt and HPSv2 datasets
  5. Compare reward-diversity tradeoffs across methods

- **Design tradeoffs:**
  - KL regularization requires expensive hyperparameter tuning but is simple to implement
  - LoRA scaling is inference-time but suffers from reference mismatch and indiscriminate regularization
  - AIG is inference-time and addresses both issues but requires choosing an appropriate γ(t) schedule

- **Failure signatures:**
  - Model collapse during training (especially SDv1.4 with high KL values)
  - Poor reward-diversity tradeoff (low diversity despite high rewards)
  - Reference mismatch (images differ stylistically from base model without gaining diversity)

- **First 3 experiments:**
  1. Compare DRaFT with no regularization vs. DRaFT with KL regularization (λ = 0.1, 1, 10) on SDXL with Pickscore reward
  2. Compare DRaFT with LoRA scaling (α' = 0.1, 0.5, 0.9) vs. DRaFT with AIG (γ(t) = 1 - (1-t/T)²) on SDv1.4 with HPSv2 reward
  3. Evaluate all three methods on PartiPrompt dataset using reward score, FID, Recall, and Spectral Distance metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal scheduling function γ(t) for Annealed Importance Guidance across different model architectures and reward functions?
- Basis in paper: [inferred] The paper only experiments with power-law scheduling functions and mentions that more sophisticated scheduling (e.g., Heaviside step function or sigmoid) could be explored.
- Why unresolved: The paper demonstrates effectiveness of AIG but does not systematically explore the design space of γ(t) functions beyond simple power-law forms.
- What evidence would resolve it: A comprehensive ablation study comparing various scheduling functions (power-law, sigmoid, step, custom curves) across multiple model architectures and reward functions, showing which scheduling yields optimal reward-diversity tradeoffs.

### Open Question 2
- Question: How does Annealed Importance Guidance perform when finetuning diffusion models for text-to-image alignment tasks (e.g., spatial relationships, counting, attribute binding) versus stylistic changes?
- Basis in paper: [explicit] The paper notes that AIG assumes "each mode of the original data distribution has a 'high-reward region' close to it" which holds for stylistic changes but may not for alignment tasks.
- Why unresolved: The current evaluation focuses on stylistic reward models (PickScore, HPSv2) and doesn't test AIG's effectiveness on alignment-specific tasks where reference mismatch is more pronounced.
- What evidence would resolve it: Experiments applying AIG to diffusion models finetuned on alignment-specific reward functions (e.g., for spatial reasoning or attribute binding) with quantitative metrics showing improvements in alignment while maintaining diversity.

### Open Question 3
- Question: What is the theoretical relationship between the base model's score function and the finetuned model's score function that makes AIG effective?
- Basis in paper: [inferred] The paper shows AIG works empirically but doesn't provide theoretical justification for why gradually annealing between base and finetuned score functions preserves diversity while optimizing rewards.
- Why unresolved: The paper demonstrates practical effectiveness through experiments but lacks a theoretical framework explaining the mechanism by which AIG prevents mode collapse while allowing reward optimization.
- What evidence would resolve it: A mathematical analysis proving that AIG maintains certain properties of the base distribution (e.g., entropy bounds, mode preservation guarantees) while converging to high-reward regions, possibly through connections to optimal transport theory or information geometry.

## Limitations

- The theoretical proof of reward hacking inevitability relies on non-parametric distribution assumptions that may not hold in practice.
- The proposed Spectral Covariance Distance metric, while addressing reference mismatch issues, requires further validation across diverse datasets and reward functions.
- The effectiveness of AIG depends on proper scheduling function selection, which is not systematically explored.

## Confidence

**High Confidence**: The inevitability of reward hacking under expected reward maximization (Lemma 1) - this follows directly from mathematical optimization principles and is well-supported by the proof. The mechanism of AIG working through gradual interpolation between base and finetuned models is also highly confident, as it builds on established Annealed Importance Sampling theory.

**Medium Confidence**: The superiority of AIG over existing regularization methods (KL divergence, LoRA scaling) is moderately supported by experiments but may depend on specific hyperparameter choices and reward functions. The claim that early sampling steps primarily contribute to mode recovery while later steps add details is plausible but not rigorously proven.

**Low Confidence**: The effectiveness of SCD as a universal diversity metric across all scenarios is the least certain claim, as it requires more extensive validation and comparison with other diversity metrics in different contexts.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary the regularization strength λ in KL regularization and scaling factor α' in LoRA scaling across multiple orders of magnitude to determine the full Pareto frontier and validate that AIG consistently outperforms baselines.

2. **Cross-Dataset Generalization**: Test AIG on additional text-to-image datasets beyond PartiPrompts and HPSv2 (such as MS-COCO or LAION) with different reward models to verify that the diversity preservation generalizes beyond the specific experimental conditions.

3. **User Study Validation**: Conduct a comprehensive user study comparing base, finetuned, and AIG-generated images across multiple reward functions and model architectures to empirically validate that AIG achieves both higher diversity and quality as claimed.