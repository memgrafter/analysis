---
ver: rpa2
title: oRetrieval Augmented Generation for 10 Large Language Models and its Generalizability
  in Assessing Medical Fitness
arxiv_id: '2410.08431'
source_url: https://arxiv.org/abs/2410.08431
tags:
- international
- local
- page
- gpt4
- gemini-1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated Retrieval Augmented Generation (RAG) models
  in preoperative assessment, finding that GPT4-based RAG models outperformed human
  clinicians in accuracy (96.4% vs. 86.6%) while generating responses 60x faster (20
  seconds vs.
---

# oRetrieval Augmented Generation for 10 Large Language Models and its Generalizability in Assessing Medical Fitness

## Quick Facts
- arXiv ID: 2410.08431
- Source URL: https://arxiv.org/abs/2410.08431
- Authors: Yu He Ke; Liyuan Jin; Kabilan Elangovan; Hairil Rizal Abdullah; Nan Liu; Alex Tiong Heng Sia; Chai Rick Soh; Joshua Yi Min Tung; Jasmine Chiat Ling Ong; Chang-Fu Kuo; Shao-Chun Wu; Vesela P. Kovacheva; Daniel Shu Wei Ting
- Reference count: 0
- Primary result: GPT4-based RAG models achieved 96.4% accuracy vs 86.6% for human clinicians in preoperative assessment

## Executive Summary
This study evaluates Retrieval Augmented Generation (RAG) models across 10 large language models for preoperative surgical assessment. Using 35 local and 23 international guidelines as knowledge bases, the RAG-enhanced GPT4 model achieved 96.4% accuracy in predicting fitness for surgery compared to 86.6% for human clinicians (p=0.016). The models generated responses in 20 seconds versus 10 minutes for humans, demonstrating significant efficiency gains. Across 14 clinical scenarios and 3,682 responses, the RAG models showed high consistency, low hallucination rates, and strong generalizability between different guideline sets.

## Method Summary
The study developed a LlamaIndex-based RAG framework using 35 local and 23 international preoperative guidelines as knowledge bases. Clinical scenarios were processed through the RAG pipeline with Auto-MergingRetrieval (similarity_top_k=30) to extract relevant guideline chunks. Ten LLM models (GPT3.5, GPT4, GPT4-o, Llama2 series, Llama3 series, Gemini-1.5-Pro, Claude-3-Opus) generated responses using standardized prompts. Responses were evaluated against human-generated answers using the S.C.O.R.E. framework (Safety, Consensus, Objectivity, Reproducibility, Explainability) with Fisher's exact test for statistical significance.

## Key Results
- GPT4 RAG model achieved 96.4% accuracy vs 86.6% for human clinicians (p=0.016)
- Response generation time reduced from 10 minutes (human) to 20 seconds (model)
- RAG models demonstrated generalizability across local and international guidelines with low hallucination rates
- GPT4 showed highest performance in predicting fitness for surgery and identifying optimization needs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG enables LLMs to access up-to-date, domain-specific guidelines without retraining the base model
- Mechanism: Retrieval component queries a curated knowledge corpus, merges relevant chunks using Auto-MergingRetrieval, and injects them into the LLM prompt, grounding generation in current evidence
- Core assumption: The knowledge corpus is comprehensive, up-to-date, and correctly formatted for retrieval
- Evidence anchors:
  - [abstract] "We developed LLM-RAG models using 35 local and 23 international preoperative guidelines and tested them against human-generated responses"
  - [section] "In the current study, we explore an advanced LLM-RAG framework using Python3.11 with Llamaindex, for its optimized and streamlined pipeline for RAG"
  - [corpus] Weak: no direct corpus access to verify guideline coverage or update frequency
- Break condition: Knowledge corpus is incomplete, outdated, or poorly indexed, leading to retrieval of irrelevant or missing information

### Mechanism 2
- Claim: Using GPT4 with RAG achieves higher accuracy than human clinicians in assessing surgical fitness
- Mechanism: GPT4 leverages its advanced reasoning to interpret retrieved guidelines, synthesize complex clinical scenarios, and produce consistent, guideline-aligned decisions
- Core assumption: GPT4's reasoning capacity and the quality of retrieved guidelines combine to outperform human variability
- Evidence anchors:
  - [abstract] "The GPT4 LLM-RAG model achieved the highest accuracy (96.4% vs. 86.6%, p=0.016)"
  - [section] "The GPT4_international model emerged as the accurate model with the highest accuracy in predicting medical fitness for surgery (96.4%) compared to human-generated answers"
  - [corpus] Moderate: related papers show similar RAG improvements but none specifically compare GPT4 to humans in preoperative medicine
- Break condition: Clinical scenarios are too complex or nuanced for the LLM to resolve, or retrieval fails to provide necessary context

### Mechanism 3
- Claim: RAG reduces hallucination rates compared to native LLMs
- Mechanism: By constraining generation to retrieved guideline content, the model is less likely to fabricate information outside the knowledge base
- Core assumption: Retrieval consistently provides relevant, correct information and the prompt engineering enforces adherence
- Evidence anchors:
  - [abstract] "The RAG models demonstrated generalizable performance... Additionally, the GPT4 LLM-RAG model exhibited an absence of hallucinations"
  - [section] "The evaluation revealed low hallucination rates across several LLM systems, including GPT3.5, GPT4, GPT4o, LLAMA3, Gemini, and Claude"
  - [corpus] Weak: no corpus data on hallucination frequency in non-RAG vs RAG models
- Break condition: Retrieval returns insufficient or irrelevant context, forcing the model to hallucinate to complete the response

## Foundational Learning

- Concept: Retrieval Augmented Generation (RAG)
  - Why needed here: Enables LLMs to use domain-specific, up-to-date guidelines without costly fine-tuning
  - Quick check question: What are the two main components of a RAG system and their roles?

- Concept: Prompt Engineering for Clinical LLMs
  - Why needed here: Ensures responses are specific, contextualized, and aligned with clinical standards
  - Quick check question: Which prompt principles are emphasized for eliciting comprehensive clinical responses?

- Concept: Evaluation Frameworks for AI in Medicine
  - Why needed here: Provides a structured way to assess safety, consistency, and clinical validity of LLM outputs
  - Quick check question: What are the five dimensions of the S.C.O.R.E. framework used in this study?

## Architecture Onboarding

- Component map:
  - Knowledge Corpus: 35 local + 23 international preoperative guidelines (PDF → text → indexed chunks)
  - Retrieval Engine: Llamaindex with Auto-MergingRetrieval (similarity_top_k=30)
  - LLM Backends: GPT3.5, GPT4, GPT4-o, Llama2 series, Llama3 series, Gemini-1.5-Pro, Claude-3-Opus
  - Prompt Generator: Role-play, specificity, contextualization, open-endedness
  - Evaluator: S.C.O.R.E. framework + Fisher's exact test vs human clinicians

- Critical path:
  1. Load and preprocess guidelines into knowledge corpus
  2. Receive clinical scenario query
  3. Retrieve and merge relevant guideline chunks
  4. Generate response via LLM with embedded context
  5. Evaluate against established standards and human benchmarks

- Design tradeoffs:
  - Retrieval vs Fine-tuning: RAG avoids retraining costs and enables dynamic updates, but depends on retrieval quality
  - Context length limits: Larger models (GPT4) handle more context, smaller models (Llama2-7B) require truncation
  - Accuracy vs speed: Retrieval + generation takes ~20s vs 10min for humans

- Failure signatures:
  - Low retrieval relevance → incomplete or incorrect responses
  - Hallucinations in non-RAG models → factual errors
  - High inter-rater disagreement → inconsistency with guidelines
  - System latency >30s → performance bottleneck

- First 3 experiments:
  1. Compare hallucination rates between RAG-enhanced and native LLM versions on identical queries
  2. Test retrieval accuracy with synthetic queries against known guideline content
  3. Measure inter-rater agreement of LLM outputs vs human clinicians on a subset of clinical scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RAG-enhanced LLM models compare to fine-tuned models for preoperative assessment tasks, and what are the relative computational costs and environmental impacts?
- Basis in paper: [explicit] The paper explicitly states that fine-tuning was not explored due to dataset limitations, and notes that RAG offers advantages in terms of computational efficiency and lower carbon footprint compared to fine-tuning
- Why unresolved: The study did not directly compare RAG-enhanced models to fine-tuned models, leaving an open question about their relative performance and efficiency
- What evidence would resolve it: A direct comparative study evaluating both RAG-enhanced and fine-tuned LLM models on the same preoperative assessment tasks, including metrics on accuracy, computational costs, and environmental impact

### Open Question 2
- Question: What are the ethical implications and potential biases in using RAG-enhanced LLM models for preoperative decision-making, particularly in complex scenarios involving patient autonomy and critical care decisions?
- Basis in paper: [explicit] The paper discusses the potential for bias in LLM decisions and emphasizes the need for cautious implementation, especially in ethically ambiguous scenarios like cancer treatment decisions
- Why unresolved: While the paper acknowledges the ethical considerations, it does not provide a comprehensive framework for addressing these issues in clinical practice
- What evidence would resolve it: Development and validation of ethical guidelines and bias mitigation strategies specifically tailored for LLM use in preoperative settings, along with empirical studies on their impact on decision-making

### Open Question 3
- Question: How can the generalizability of RAG-enhanced LLM models be improved to handle diverse patient populations and varying healthcare practices across different regions?
- Basis in paper: [explicit] The study demonstrates the model's ability to generalize across local and international guidelines but acknowledges the need for further research to ensure adaptability to diverse healthcare contexts
- Why unresolved: The study's scope was limited to specific guidelines and scenarios, and did not explore the model's performance in a wider range of clinical environments
- What evidence would resolve it: A broader study evaluating the model's performance across multiple healthcare systems, patient demographics, and clinical practices, with a focus on identifying and addressing gaps in generalizability

## Limitations

- The exact composition and update frequency of the knowledge corpus remains unclear, potentially affecting retrieval quality and model reliability
- The study was limited to 14 clinical scenarios and 10 anesthesiologists as reference standard, which may not capture full surgical complexity
- The study did not explore edge cases where guidelines conflict or lack clarity, which could reveal model limitations under ambiguous clinical scenarios

## Confidence

- High confidence: RAG models significantly outperform human clinicians in accuracy (96.4% vs 86.6%) and demonstrate low hallucination rates when retrieval is properly implemented
- Medium confidence: The generalizability of RAG performance across local and international guidelines is supported, but the underlying knowledge corpus transparency remains limited
- Medium confidence: The scalability and adaptability claims are theoretically sound given RAG's architecture, but real-world deployment challenges were not addressed

## Next Checks

1. Conduct a systematic comparison of hallucination rates between RAG-enhanced and native LLM versions using identical clinical queries to quantify the retrieval constraint effect
2. Test retrieval accuracy and relevance with synthetic queries against a controlled subset of known guideline content to validate the knowledge corpus coverage and indexing quality
3. Perform inter-rater reliability analysis between LLM outputs and a larger, more diverse group of human clinicians (including surgeons, not just anesthesiologists) on complex edge-case scenarios where guidelines conflict or lack specificity