---
ver: rpa2
title: 'State Space Model for New-Generation Network Alternative to Transformers:
  A Survey'
arxiv_id: '2404.09516'
source_url: https://arxiv.org/abs/2404.09516
tags:
- arxiv
- mamba
- state
- preprint
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive review of State Space Models
  (SSMs) as an alternative to Transformer architectures. The review covers the origin
  and variations of SSMs, including their applications in natural language processing,
  computer vision, graph data, multi-modal/multi-media, point cloud/event streams,
  time series data, and other domains.
---

# State Space Model for New-Generation Network Alternative to Transformers: A Survey

## Quick Facts
- arXiv ID: 2404.09516
- Source URL: https://arxiv.org/abs/2404.09516
- Reference count: 40
- Authors: Xiao Wang, Shiao Wang, Yuhe Ding, Yuehang Li, Wentao Wu, Yao Rong, Weizhe Kong, Ju Huang, Shihao Li, Haoxiang Yang, Ziwen Wang, Bo Jiang, Chenglong Li, Yaowei Wang, Yonghong Tian, Jin Tang
- Primary result: SSMs achieve performance comparable to some Transformer networks but still lag behind state-of-the-art models across various vision and language tasks

## Executive Summary
This survey paper provides a comprehensive review of State Space Models (SSMs) as an emerging alternative to Transformer architectures. The paper systematically examines the origin and evolution of SSMs, their theoretical foundations, and their applications across multiple domains including natural language processing, computer vision, graph data, multi-modal data, point clouds, event streams, and time series. Through experimental comparisons, the authors demonstrate that while current SSMs can match the performance of some Transformer networks, they have not yet surpassed state-of-the-art models. The survey identifies key challenges and proposes future research directions including development of larger SSM models, exploration of their advantages in high-resolution or long-term data processing, and creation of novel scan operators for improved feature learning.

## Method Summary
The paper evaluates State Space Models (SSMs) on various computer vision tasks by replacing CNN or Transformer backbones with SSM-based architectures (Vim, VMamba) in existing frameworks. The evaluation covers classification (ImageNet-1K), visual object tracking (EventVOT), segmentation (MRI Cardiac), image-to-text generation (IU-Xray), and person/vehicle re-identification (MSMT17, Market1501, DukeMTMC, Occluded-Duke, VeRi-776, VehicleID). The experiments use standard metrics including top-1 accuracy, mAP, CMC, SR, PR, NPR, Dice, IoU, BLEU-4, and ROUGE-L. The reproduction plan involves downloading specified datasets, implementing SSM architectures, and training with specified metrics while noting unknown training hyperparameters and data augmentation strategies as potential blockers.

## Key Results
- SSMs achieve performance comparable to some Transformer networks but still lag behind state-of-the-art models
- Current Mamba-based vision models are primarily small or base versions, with few large-scale pre-trained SSMs
- SSMs show potential advantages in processing high-resolution or long-term data due to linear computational complexity
- Limited domain generalization ability observed in current SSM-based networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SSMs reduce computational complexity from quadratic to linear for sequence modeling
- Mechanism: The structured state space layer replaces the attention matrix multiplication with a recurrence or convolution operation, where the kernel K is precomputed as a function of the input-independent state matrices
- Core assumption: The state matrices A, B, C remain input-independent during the linear scan, allowing the kernel to be computed once and reused
- Evidence anchors: [abstract] "To further reduce the complexity of attention models, numerous efforts have been made to design more efficient methods. Among them, the State Space Model (SSM)... has drawn more and more attention in recent years." [section] "However, similar to the RNN model, we face the dilemma that the computation cannot be parallelized. By simply expanding the above formula, we have... It is easy to find that the multiplier of the last and penultimate term is always C¯A0¯B and C¯A1¯B. Therefore, we can treat these multipliers as the convolutional kernel K = C¯B · (¯A0, ¯A1, ¯A2, ..., ¯AL)." [corpus] Weak: No direct citations or numerical evidence provided for the O(N) claim; the paper only states the theoretical motivation
- Break condition: If the state matrices become input-dependent (as in Mamba), the kernel can no longer be precomputed, breaking the linear complexity advantage

### Mechanism 2
- Claim: Mamba introduces a selective scan operator to address SSM's weakness in context modeling
- Mechanism: The selective scan allows the parameters B and C to become functions of the input, gating the information flow based on input content, while A remains fixed. This mimics attention-like behavior while maintaining linear complexity
- Core assumption: Input-dependent parameters can be computed on the fly without destroying the linear scan efficiency
- Evidence anchors: [abstract] "Mamba introduces a selection mechanism in the structured State Space Model to filter out irrelevant information and retain useful information." [section] "To address this issue, Gu et al. propose the Mamba [12] architecture which improves the SSM from the following two aspects: 1). Selective Scan Operator allows the model to filter relevant information out. In practical implementation, the ∆, B, and C become the functions of the input, meanwhile, the matrix A keeps unchanged." [corpus] Weak: No quantitative results or ablation studies showing the exact contribution of the selective scan to performance
- Break condition: If the input-dependent parameters become too complex, the overhead may negate the linear complexity benefit

### Mechanism 3
- Claim: SSMs can be applied to multiple data modalities by reinterpreting the input as a sequence or signal
- Mechanism: By flattening spatial dimensions into a sequence (e.g., image patches, point cloud coordinates) and using a consistent scan order, SSMs can process 2D/3D data with the same recurrence/convolution framework
- Core assumption: Spatial locality is preserved in the chosen scan order, so the model can learn meaningful dependencies
- Evidence anchors: [abstract] "SSMs can be adopted for various data processing and feature learning, including image/video data, text data, structured graph data, event streams/point cloud data, multi-modal/multi-media data, audio and speech, time series data, tabular data, etc." [section] "VMamba [60] uses linear complexity to capture the full range of sensory fields, introduces traversal of spatial information across scan blocks, and converts non-causal visual images into ordered patch sequences." [corpus] Weak: No explicit discussion of scan order design or its impact on performance across modalities
- Break condition: If the scan order destroys spatial coherence (e.g., random ordering), the model cannot learn useful features

## Foundational Learning

- Concept: State Space Models as linear dynamical systems
  - Why needed here: Understanding SSMs requires knowing how continuous-time systems are discretized and represented in discrete-time for computation
  - Quick check question: What is the role of the zero-order hold (ZOH) discretization in converting the continuous SSM equations to a recurrent form?

- Concept: Convolution as a collapsed recurrence
  - Why needed here: The paper shows that unrolling the recurrence yields a convolution, which is the key to parallelizing training
  - Quick check question: How does the kernel K = C¯B · (¯A0, ¯A1, ...) relate to the original state matrices A, B, C?

- Concept: Selective vs. fixed parameterization
  - Why needed here: Mamba's innovation is making B and C input-dependent; understanding this distinction is crucial for grasping its advantage
  - Quick check question: Why does making B and C functions of the input preserve linear complexity while improving context modeling?

## Architecture Onboarding

- Component map: Input embedding -> flatten to sequence -> discretize state matrices (A, B(x), C(x)) -> compute kernel K or run recurrence -> apply convolution/recurrence -> final linear layer
- Critical path:
  1. Input embedding → flatten to sequence
  2. Discretize state matrices (A, B(x), C(x))
  3. Compute kernel K or run recurrence
  4. Apply convolution/recurrence
- Design tradeoffs: Fixed A vs. input-dependent B, C for efficiency vs. expressiveness; scan order selection for spatial coherence
- Failure signatures: Poor performance due to improper initialization or incorrect implementation of SSM architectures; overfitting or underfitting due to mismatched training hyperparameters
- First experiments: 1) Implement basic SSM recurrence with fixed A, B, C and verify linear convolution equivalence; 2) Add input-dependent B, C and measure performance gain; 3) Test different scan orders on image data and observe impact on accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do large-scale pre-trained State Space Models (SSMs) perform on visual tasks compared to Transformer-based models?
- Basis in paper: [explicit] The paper states that "current Mamba-based vision models are all tiny, small, or base versions, and seldom pre-train a large or huge version of the Mamba network."
- Why unresolved: The lack of large-scale pre-trained SSMs limits the exploration of their potential performance on complex visual tasks
- What evidence would resolve it: Training and evaluating large-scale SSMs (e.g., with 1B+ parameters) on benchmarks like ImageNet and comparing their performance to large-scale Transformer models

### Open Question 2
- Question: Can novel scan operators for SSMs improve feature learning for specific data types like point clouds and event streams?
- Basis in paper: [explicit] The paper suggests "it is natural to design novel scan schemes to enhance the feature learning of SSMs" and mentions developing "new track-changing scan methods to better encode the point cloud and event streams."
- Why unresolved: Current scan operators may not fully capture the unique characteristics of point cloud and event stream data, limiting SSM performance on these tasks
- What evidence would resolve it: Designing and evaluating new scan operators tailored for point cloud and event stream data, and comparing their performance to existing methods

### Open Question 3
- Question: What is the impact of SSMs on the generalization performance of deep learning models across different domains?
- Basis in paper: [explicit] The paper states that "current SSM based networks illustrate limited domain generalization ability" and suggests that SSMs "may have greater advantages and potential in the field of domain generalization."
- Why unresolved: The generalization ability of SSMs across diverse domains is not well understood, and existing methods may not fully exploit their potential
- What evidence would resolve it: Conducting extensive experiments on domain generalization tasks using SSMs and comparing their performance to other architectures like CNNs and Transformers

## Limitations

- Current SSM-based vision models are limited to small or base versions, with few large-scale pre-trained models available
- Limited domain generalization ability observed in current SSM-based networks
- No quantitative ablation studies provided to isolate the contributions of specific architectural components

## Confidence

- High: Theoretical foundations of SSMs and their complexity advantages are well-established
- Medium: Experimental comparisons show SSMs achieve comparable performance to some Transformers but lack state-of-the-art results
- Low: Claims about scan order impact and generalization benefits lack empirical validation in the paper

## Next Checks

- Verify the mathematical equivalence between SSM recurrence and convolution by implementing both forms and comparing outputs
- Test the impact of input-dependent vs. fixed parameters (B, C) on model performance through controlled ablation studies
- Evaluate different scan orders (raster, serpentine, diagonal) on image classification tasks to quantify their effect on accuracy