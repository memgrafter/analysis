---
ver: rpa2
title: Faster Algorithms for User-Level Private Stochastic Convex Optimization
arxiv_id: '2410.18391'
source_url: https://arxiv.org/abs/2410.18391
tags:
- algorithm
- user-level
- privacy
- risk
- excess
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of user-level differentially private
  stochastic convex optimization (SCO), where each user contributes multiple data
  samples. Existing algorithms for this setting have impractical runtime or restrictive
  assumptions.
---

# Faster Algorithms for User-Level Private Stochastic Convex Optimization

## Quick Facts
- **arXiv ID**: 2410.18391
- **Source URL**: https://arxiv.org/abs/2410.18391
- **Authors**: Andrew Lowy; Daogao Liu; Hilal Asi
- **Reference count**: 40
- **Key outcome**: Novel algorithms for user-level private stochastic convex optimization achieving state-of-the-art excess risk and runtime guarantees without restrictive assumptions

## Executive Summary
This paper addresses the challenge of user-level differentially private stochastic convex optimization (SCO), where each user contributes multiple data samples. Existing algorithms for this setting suffer from impractical runtime or restrictive assumptions on smoothness parameters and the number of users. The authors propose three novel algorithms that achieve optimal excess risk bounds with significantly improved runtime complexity. For smooth loss functions, their accelerated algorithm achieves optimal excess risk using approximately (mn)^{9/8} gradient computations, improving upon prior work by polynomial factors. They also develop a linear-time algorithm with state-of-the-art excess risk under mild smoothness assumptions and extend results to non-smooth losses using randomized smoothing.

## Method Summary
The paper proposes three main algorithms for user-level private SCO. Algorithm 1 uses outlier removal on SGD iterates to achieve linear-time complexity with state-of-the-art excess risk under mild assumptions. Algorithm 3 applies accelerated minibatch SGD with privacy amplification by subsampling and outlier removal to achieve optimal excess risk in approximately (mn)^{9/8} gradient computations for smooth losses. For non-smooth losses, they use randomized smoothing to approximate non-smooth functions with smooth convolutions, then apply Algorithm 3 to these approximations, achieving optimal excess risk in n^{11/8}m^{5/4} gradient computations. The key technical innovations include applying outlier removal to SGD iterates rather than gradients, combining privacy amplification with AboveThreshold for optimal error, and careful analysis of gradient estimator variance scaling with 1/m.

## Key Results
- Algorithm 3 achieves optimal excess risk for smooth losses using approximately (mn)^{9/8} gradient computations
- Linear-time Algorithm 1 achieves state-of-the-art excess risk under mild smoothness assumptions
- For non-smooth losses, optimal excess risk is achieved in n^{11/8}m^{5/4} gradient computations using randomized smoothing
- Outlier removal on SGD iterates (rather than gradients) enables linear-time performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: User-level DP SCO algorithms can achieve optimal excess risk without restrictive assumptions on smoothness or number of users by using outlier removal techniques.
- **Mechanism**: The algorithm partitions users into groups, runs SGD on each group, and privately detects and removes outlier iterates. This allows noise to scale with the concentration of iterates rather than the worst-case sensitivity, enabling optimal risk bounds.
- **Core assumption**: The stability of user-level DP ensures that SGD iterates from different users remain close when data is i.i.d., making outlier detection effective.
- **Evidence anchors**:
  - [abstract]: "Existing algorithms for user-level DP SCO are impractical in many large-scale machine learning scenarios because: (i) they make restrictive assumptions on the smoothness parameter of the loss function and require the number of users to grow polynomially with the dimension of the parameter space; or (ii) they are prohibitively slow"
  - [section]: "Our linear-time Algorithm 1 takes a different approach to outlier removal, compared to prior works. Instead of removing outliergradients, we aim to detect and remove outlier SGDiterates."
  - [corpus]: Weak - corpus papers focus on different DP optimization problems without direct evidence for this specific outlier-removal mechanism.
- **Break condition**: If the stability bound in Lemma 2.3 fails (i.e., iterates from different users diverge), outlier detection becomes ineffective and privacy guarantees break.

### Mechanism 2
- **Claim**: Accelerated minibatch SGD with privacy amplification by subsampling and outlier removal can achieve optimal excess risk with improved runtime.
- **Mechanism**: The algorithm draws random minibatches of users, applies outlier removal to gradients within each minibatch, and combines privacy amplification by subsampling with AboveThreshold for optimal error bounds.
- **Core assumption**: The variance of the minibatch gradient estimator scales with 1/m rather than 1/Ki, where Ki is the minibatch size, due to the stability of user-level DP.
- **Evidence anchors**:
  - [abstract]: "Our second algorithm applies to arbitrary smooth losses and achieves optimal excess risk in ≈ (mn)^{9/8} gradient computations"
  - [section]: "To make this procedure private while also achieving optimal excess risk, we combineAboveThreshold [DR14] with privacy amplification by subsampling[BBG18]."
  - [corpus]: Weak - corpus papers focus on different DP optimization settings without direct evidence for this specific combination of techniques.
- **Break condition**: If the variance bound in Lemma 3.5 doesn't hold (i.e., variance doesn't scale with 1/m), the accelerated algorithm cannot achieve optimal excess risk.

### Mechanism 3
- **Claim**: Randomized smoothing can extend the accelerated algorithm to non-smooth loss functions while maintaining optimal excess risk and improved runtime.
- **Mechanism**: The algorithm approximates non-smooth losses with smooth convolution functions, then applies the accelerated user-level DP algorithm to these smooth approximations.
- **Core assumption**: The smooth approximation maintains the essential properties of the original loss function while enabling the use of accelerated algorithms designed for smooth functions.
- **Evidence anchors**:
  - [abstract]: "Third, for non-smooth loss functions, we obtain optimal excess risk in n^{11/8}m^{5/4} gradient computations"
  - [section]: "To accomplish this with minimal computational cost, we applyrandomized (convolution) smoothing [YNS12, DBW12] to approximate non-smoothf by a β-smooth ˜f."
  - [corpus]: Weak - corpus papers focus on different optimization problems without direct evidence for this specific application of randomized smoothing to user-level DP.
- **Break condition**: If the smooth approximation introduces excessive error (i.e., the approximation error dominates the optimization error), the algorithm cannot maintain optimal excess risk.

## Foundational Learning

- **Concept**: Differential Privacy and its variants (item-level vs user-level)
  - Why needed here: The entire paper addresses the challenge of protecting user privacy in machine learning, requiring understanding of how differential privacy works and why user-level DP is stronger than item-level DP.
  - Quick check question: What is the key difference between item-level and user-level differential privacy, and why is user-level DP more appropriate for scenarios where each user contributes multiple data samples?

- **Concept**: Stochastic Convex Optimization and excess risk bounds
  - Why needed here: The paper's goal is to minimize the excess risk (difference between the expected population loss and the optimal loss) while maintaining privacy, requiring understanding of SCO problem formulation and risk analysis.
  - Quick check question: How is excess risk defined in stochastic convex optimization, and why is achieving optimal excess risk bounds important for practical machine learning applications?

- **Concept**: Gradient-based optimization algorithms and their convergence properties
  - Why needed here: The algorithms proposed rely on SGD and accelerated SGD, requiring understanding of how these algorithms converge and how their convergence rates depend on smoothness and strong convexity parameters.
  - Quick check question: What is the convergence rate of standard SGD for smooth convex functions, and how does this change when we use accelerated methods like AC-SA?

## Architecture Onboarding

- **Component map**: Algorithm 1 (linear-time with outlier iterate removal) -> Algorithm 3 (accelerated with privacy amplification) -> Smoothed version for non-smooth losses
- **Critical path**: 
  1. Partition users into groups
  2. Run SGD on each group
  3. Privately detect and remove outliers
  4. Combine results with noise for privacy
  5. Iterate with localization for improved accuracy
- **Design tradeoffs**:
  - Linear-time vs optimal risk: Algorithm 1 achieves state-of-the-art excess risk in linear time but with suboptimal risk bounds compared to Algorithm 3
  - Smooth vs non-smooth: The smoothed algorithm extends results to non-smooth functions but requires additional approximation error
  - Group size vs privacy: Smaller groups provide better privacy but may reduce statistical efficiency
- **Failure signatures**:
  - Privacy violation: If the stability bounds fail, the ℓ2-sensitivity of iterates exceeds the assumed bounds
  - Suboptimal performance: If the variance doesn't scale as 1/m, the accelerated algorithm cannot achieve optimal excess risk
  - Computational inefficiency: If the number of users doesn't grow sufficiently, the privacy amplification by subsampling becomes ineffective
- **First 3 experiments**:
  1. Implement Algorithm 1 on synthetic data with known smoothness parameters to verify the linear-time complexity and excess risk bounds
  2. Compare Algorithm 3 with baseline user-level DP algorithms on benchmark datasets to measure runtime improvements and risk performance
  3. Test the smoothed algorithm on non-smooth loss functions (e.g., hinge loss) to verify the extension to non-smooth settings maintains optimal excess risk

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can we achieve optimal excess risk in linear time for user-level private stochastic convex optimization with smooth losses?
- **Basis in paper**: [explicit] The authors state "the question of whether there exists a linear-time algorithm that can attain the user-level DP lower bound for smooth losses remains open" and express optimism about a positive answer.
- **Why unresolved**: While the paper provides a linear-time algorithm with state-of-the-art excess risk under mild assumptions, it does not achieve the optimal excess risk bound. The authors note that obtaining optimal risk in linear time will require "a fundamentally different user-level DP mean estimation procedure that does not suffer from the instability issue."
- **What evidence would resolve it**: A new algorithm that achieves the optimal excess risk bound (1/√nm + √(d log(1/δ))/(εn√m)) in O(mn) gradient evaluations, along with a proof of its correctness and privacy guarantees.

### Open Question 2
- **Question**: What rates are achievable under pure ε-user-level differential privacy for stochastic convex optimization?
- **Basis in paper**: [explicit] The authors state "the study of user-level DP SCO has been largely limited to approximate (ε, δ)-DP. What rates are achievable under the stronger notion of pure ε-user-level DP?"
- **Why unresolved**: All existing algorithms for user-level private SCO use approximate (ε, δ)-DP, and it's unclear what the fundamental limits are under the stronger pure ε-DP notion.
- **What evidence would resolve it**: Lower bounds showing the optimal excess risk achievable under pure ε-DP, and upper bounds from algorithms that achieve these bounds.

### Open Question 3
- **Question**: Can we develop fast and optimal algorithms for user-level private SCO tailored to federated learning environments?
- **Basis in paper**: [explicit] The authors mention "it would be useful to develop fast and optimal algorithms that are tailored to federated learning environments, where only a small number of users may be available to communicate with the server in each iteration."
- **Why unresolved**: Existing algorithms for user-level private SCO assume synchronous communication with all users, which is not practical in federated learning settings with partial participation and communication constraints.
- **What evidence would resolve it**: New algorithms that achieve near-optimal excess risk bounds while only requiring communication with a small subset of users in each iteration, along with empirical evaluations showing their effectiveness in federated learning scenarios.

## Limitations
- The analysis relies heavily on i.i.d. assumptions for both users and their samples, which may not hold in real-world scenarios
- Privacy guarantees depend critically on stability bounds that may be violated with non-i.i.d. data or adversarial users
- Empirical validation is limited to synthetic experiments with no real-world dataset evaluation provided

## Confidence
- **High confidence**: The core algorithmic frameworks (Algorithms 1 and 3) are well-defined and the privacy analysis follows standard techniques
- **Medium confidence**: The excess risk bounds for non-smooth functions via randomized smoothing, as this extends previous results with additional approximation error
- **Low confidence**: The practical performance claims, as no experiments on real datasets are provided to validate the theoretical improvements

## Next Checks
1. **Stability bound verification**: Implement numerical experiments to verify the stability bounds (Lemma 2.3) hold empirically across different datasets and parameter settings.
2. **Real-world dataset evaluation**: Test the algorithms on benchmark machine learning datasets (e.g., Adult, MNIST) to measure practical runtime improvements and excess risk compared to existing user-level DP methods.
3. **Sensitivity analysis**: Experimentally assess how violations of i.i.d. assumptions affect privacy guarantees and excess risk bounds across different data distributions.