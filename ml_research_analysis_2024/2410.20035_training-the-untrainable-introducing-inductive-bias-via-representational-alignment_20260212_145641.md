---
ver: rpa2
title: 'Training the Untrainable: Introducing Inductive Bias via Representational
  Alignment'
arxiv_id: '2410.20035'
source_url: https://arxiv.org/abs/2410.20035
tags:
- network
- guide
- networks
- guidance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces guidance, a method that makes traditionally
  untrainable neural networks trainable by aligning their internal representations
  with those of a frozen guide network. The core idea is to add a layerwise representational
  alignment term to the target network's loss, minimizing a neural distance function
  like CKA between guide and target activations.
---

# Training the Untrainable: Introducing Inductive Bias via Representational Alignment

## Quick Facts
- arXiv ID: 2410.20035
- Source URL: https://arxiv.org/abs/2410.20035
- Authors: Vighnesh Subramaniam; David Mayo; Colin Conwell; Tomaso Poggio; Boris Katz; Brian Cheung; Andrei Barbu
- Reference count: 40
- One-line primary result: Guidance makes traditionally untrainable networks trainable by aligning their internal representations with a frozen guide network, with untrained guides often outperforming trained ones.

## Executive Summary
This paper introduces guidance, a method that enables training of traditionally untrainable neural networks by aligning their internal representations with those of a frozen guide network. The approach adds a layerwise representational alignment term to the target network's loss, minimizing a neural distance function like CKA between guide and target activations. This transfers architectural priors from the guide to the target, even when the guide is untrained. The method demonstrates effectiveness across multiple domains, preventing overfitting in deep fully-connected networks, closing performance gaps between RNNs and Transformers, and improving plain CNNs toward ResNet accuracy.

## Method Summary
Guidance works by having a target network minimize its task loss plus a layerwise representational dissimilarity (1-CKA) against a frozen guide network. Both networks receive the same input per batch, and for each mapped layer pair, the CKA dissimilarity is computed and added to the target's loss. This alignment steers the target's internal geometry toward that of the guide, transferring architectural priors. The method is flexible enough to work with untrained guides, which often match or outperform trained guides, suggesting that pure architectural bias alone can provide effective regularization.

## Key Results
- Deep fully-connected networks with guidance achieve top-5 ImageNet accuracy up to 5× higher than without guidance
- 4-layer RNN with untrained Transformer guide matches or exceeds performance of 40-layer RNN on copy-paste task
- Deep ConvNet without skips guided by trained ResNet-50 achieves accuracy close to ResNet baseline
- Untrained guides often match or outperform trained guides, revealing pure architectural bias transfer

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Aligning internal representations via CKA dissimilarity minimization transfers architectural priors from guide to target
- **Mechanism**: For each layer pair in the guide-target mapping, guidance adds a layerwise representational dissimilarity term to the target's loss. Minimizing CKA (or its complement) between guide and target activations steers the target's internal geometry toward that of the guide
- **Core assumption**: Representational similarity is a differentiable proxy for architectural bias transfer
- **Evidence anchors**:
  - [abstract] "We introduce guidance, where a guide network steers a target network using a neural distance function"
  - [section] "The target minimizes its task loss plus a layerwise representational similarity against the frozen guide"
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.465..." (weak corpus coverage for this specific mechanism)

### Mechanism 2
- **Claim**: An untrained guide network can outperform a trained one when transferring architectural bias alone
- **Mechanism**: Randomly initialized guides lack task-specific knowledge but preserve architectural structure. Aligning to them imposes the guide's structural prior without overfitting to a particular solution space
- **Core assumption**: Architectural priors are separable from learned knowledge and are encoded in the network's topology and initialization geometry
- **Evidence anchors**:
  - [abstract] "Notably, an untrained guide often matches or outperforms a trained one..."
  - [section] "If the guide is untrained, this transfers over only part of the architectural prior of the guide"
  - [corpus] No direct corpus evidence; stated as novel empirical finding

### Mechanism 3
- **Claim**: Guidance prevents overfitting in deep fully-connected networks by preserving intrinsic dimensionality during early training
- **Mechanism**: Without guidance, deep FCNs collapse to low intrinsic dimensionality early, leading to memorization. Guidance with an untrained guide keeps ID high initially, then allows natural collapse only after useful representation formation
- **Core assumption**: Intrinsic dimensionality correlates with generalization capacity; early ID collapse is a precursor to overfitting
- **Evidence anchors**:
  - [abstract] "We further identify that guidance-driven initialization alone can mitigate FCN overfitting"
  - [section] "The Deep FCN without guidance achieves a low ID too early in training..."
  - [corpus] No direct corpus evidence; inferred from intrinsic dimensionality literature

## Foundational Learning

- **Concept**: Representational similarity metrics (CKA, RSA, ridge regression)
  - Why needed here: These metrics quantify how closely the target's internal activations match the guide's, enabling differentiable alignment
  - Quick check question: What does CKA=1 signify between two activation sets?

- **Concept**: Neural distance functions as architectural priors
  - Why needed here: They provide a mathematical handle to transfer structural properties between networks without requiring shared architectures
  - Quick check question: How does CKA differ from Euclidean distance in capturing representational similarity?

- **Concept**: Layerwise mapping strategies
  - Why needed here: Determines which guide layers supervise which target layers, critical for meaningful architectural transfer
  - Quick check question: What happens if you map guide layers non-uniformly (e.g., skip some target layers)?

## Architecture Onboarding

- **Component map**: Guide network -> Layerwise mapping -> Target network -> Similarity metric -> Loss function -> Optimizer

- **Critical path**:
  1. Initialize guide (trained or random) and target
  2. Define layer mapping I
  3. For each batch: collect activations from both networks
  4. Compute representational dissimilarity for each mapped layer pair
  5. Add dissimilarity to target's task loss
  6. Backpropagate only through target parameters

- **Design tradeoffs**:
  - More guide layers → better transfer but higher memory usage
  - Trained vs random guide → knowledge + architecture vs architecture alone
  - Similarity metric choice → degrees of freedom vs invariance properties

- **Failure signatures**:
  - Overfitting persists → ID collapse still occurring, try untrained guide
  - Training diverges → similarity term too strong, reduce weight
  - No performance gain → mapping incorrect, verify layer correspondence

- **First 3 experiments**:
  1. Deep FCN on ImageNet with untrained ResNet-18 guide
  2. 4-layer RNN on copy-paste task with untrained Transformer guide
  3. Deep ConvNet without skips on ImageNet with trained ResNet-50 guide

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we determine whether an untrained guide network truly transfers only architectural bias versus inadvertently capturing some form of learned knowledge from initialization?
- Basis in paper: [explicit] The paper observes that untrained guides often match or outperform trained ones, but acknowledges this could stem from either pure architectural bias or initialization-induced patterns
- Why unresolved: Distinguishing architectural bias from initialization effects requires isolating the guide's initialization from its structure, which is not directly measurable with current methods
- What evidence would resolve it: Controlled experiments ablating initialization patterns (e.g., random orthogonal weights) while preserving architecture, and comparing their guidance effects

### Open Question 2
- Question: What is the precise mechanism by which guidance prevents overfitting in deep fully-connected networks, and can this be generalized to other overfitting-prone architectures?
- Basis in paper: [explicit] The paper shows that guidance reduces overfitting in FCNs but does not fully explain the underlying regularization mechanism
- Why unresolved: The relationship between representational alignment and overfitting suppression is not theoretically characterized, and its generalizability to other architectures remains unclear
- What evidence would resolve it: Theoretical analysis linking CKA-based alignment to effective model capacity, and empirical validation across diverse overfitting-prone architectures

### Open Question 3
- Question: Can guidance be extended to non-differentiable neural distance functions, and what trade-offs arise in terms of alignment quality and computational efficiency?
- Basis in paper: [inferred] The paper uses differentiable CKA for alignment but does not explore non-differentiable metrics that might capture different aspects of representational similarity
- Why unresolved: Non-differentiable metrics could offer complementary inductive biases but would require approximation or relaxation techniques to integrate into gradient-based training
- What evidence would resolve it: Empirical comparison of differentiable vs. non-differentiable alignment metrics on guidance performance, and analysis of approximation errors introduced by relaxations

## Limitations
- Effectiveness appears highly dependent on choice of similarity metric and layer mapping strategy
- Computational overhead of computing layerwise similarity metrics could be prohibitive for large-scale applications
- Claim that untrained guides outperform trained ones lacks sufficient ablation studies to rule out confounding factors

## Confidence
- **High confidence**: The core mechanism of representational alignment works as described for preventing overfitting in deep FCNs and closing architectural gaps (RNNs vs Transformers)
- **Medium confidence**: The claim that untrained guides match or exceed trained guides in performance - this is the most surprising finding and would benefit from more rigorous ablation studies
- **Low confidence**: The generalizability of guidance to other domains beyond the presented experiments, particularly in reinforcement learning or structured prediction tasks

## Next Checks
1. **Ablation on similarity metrics**: Systematically compare guidance with CKA against alternatives (CCA, SVCCA, centered kernel alignment) on the same tasks to isolate the contribution of the similarity measure itself
2. **Scaling analysis**: Evaluate guidance on larger architectures (e.g., ViT, GPT-2 small) to test whether representational alignment scales beyond the current experimental scope
3. **Transfer learning variant**: Test whether a guide pre-trained on one task can effectively regularize a target learning a different but related task, probing the limits of architectural vs task-specific bias transfer