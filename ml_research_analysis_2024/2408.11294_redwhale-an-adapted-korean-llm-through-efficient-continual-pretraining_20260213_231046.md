---
ver: rpa2
title: 'RedWhale: An Adapted Korean LLM Through Efficient Continual Pretraining'
arxiv_id: '2408.11294'
source_url: https://arxiv.org/abs/2408.11294
tags:
- korean
- training
- language
- tokens
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents RedWhale, an efficient continual pretraining
  approach to adapt English LLMs for Korean language processing. The work addresses
  the computational and memory challenges of adapting LLMs to low-resource languages
  like Korean by proposing a comprehensive pipeline including extensive Korean corpus
  preprocessing, specialized tokenizer adaptation, effective model initialization,
  and multistage training.
---

# RedWhale: An Adapted Korean LLM Through Efficient Continual Pretraining

## Quick Facts
- arXiv ID: 2408.11294
- Source URL: https://arxiv.org/abs/2408.11294
- Authors: Anh-Dung Vo; Minseong Jung; Wonbeen Lee; Daewoo Choi
- Reference count: 11
- One-line primary result: RedWhale demonstrates superior performance on Korean NLP benchmarks through efficient continual pretraining, outperforming existing models on KoBEST.

## Executive Summary
This paper presents RedWhale, an efficient continual pretraining approach to adapt English LLMs for Korean language processing. The work addresses the computational and memory challenges of adapting LLMs to low-resource languages like Korean by proposing a comprehensive pipeline including extensive Korean corpus preprocessing, specialized tokenizer adaptation, effective model initialization, and multistage training. Key innovations include optimizing vocabulary size to balance input and embedding complexity, initializing new token embeddings via decomposition methods, and a staged training strategy to mitigate memory constraints. RedWhale demonstrates superior performance on Korean NLP benchmarks such as KoBEST, outperforming existing models. The approach is scalable, resource-efficient, and effective for adapting LLMs to other low-resource languages or domains.

## Method Summary
RedWhale employs a four-stage training strategy to adapt English-centric LLMs for Korean language processing. The approach begins with extensive corpus preprocessing and tokenizer optimization, selecting an optimal vocabulary size of 20,000 tokens to balance input and embedding complexity. New Korean token embeddings are initialized using a decomposition method that effectively integrates them with the base model's embeddings. The training proceeds in stages: first training the embedding and LM Head layers, then alternating between odd and even transformer layers, and finally applying LoRA consolidation across the entire model. This staged approach addresses memory constraints while preventing catastrophic forgetting. The model is trained on a single Nvidia H100 GPU for approximately 498 hours.

## Key Results
- RedWhale outperforms existing models on Korean NLP benchmarks including KoBEST tasks (BoolQ, COPA, HellaSwag, SentiNeg)
- The 20,000 token vocabulary size represents an optimal balance between input complexity and embedding complexity for Korean language processing
- Staged training strategy effectively addresses GPU memory constraints while maintaining model performance
- The approach demonstrates superior computational efficiency compared to full model training approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Efficient continual pretraining prevents catastrophic forgetting while adapting English LLMs to Korean.
- Mechanism: By training new Korean token embeddings and fine-tuning existing layers in stages, the model retains English knowledge while acquiring Korean-specific linguistic features.
- Core assumption: The pretrained English model has transferable knowledge that can be leveraged for Korean language processing.
- Evidence anchors:
  - [abstract]: "By leveraging cross-lingual transfer learning, which exploits shared linguistic similarities across languages, RedWhale builds on English models to enhance Korean language processing."
  - [section]: "This approach serves as a vital intermediary stage between the initial pretraining phase and subsequent domain-specific adaptation or alignment."
  - [corpus]: Weak - no direct evidence on catastrophic forgetting prevention.

### Mechanism 2
- Claim: Optimizing tokenizer vocabulary size balances input and embedding complexity, improving efficiency.
- Mechanism: Larger vocabulary reduces token count (lower input complexity) but increases embedding parameters (higher embedding complexity). The optimal size (20,000 tokens) minimizes both.
- Core assumption: There exists a sweet spot in vocabulary size that optimizes both input and embedding complexity.
- Evidence anchors:
  - [section]: "The goal was to find a balance between these measures, reducing the Ratio of Input Complexity without significantly increasing the Ratio of Embedding Complexity."
  - [section]: "The experiments revealed that a vocabulary size of 20,000 was optimal for the Korean corpus used."
  - [corpus]: Weak - no direct evidence on efficiency gains from optimal vocabulary size.

### Mechanism 3
- Claim: Staged training strategy reduces memory constraints and prevents out-of-memory errors.
- Mechanism: Training new components (embedding, LM head) first, then transformer blocks in odd/even layers, and finally applying LoRA, distributes memory usage across stages.
- Core assumption: Sequential training of model components is more memory-efficient than training all components simultaneously.
- Evidence anchors:
  - [section]: "This approach offers two main benefits: it can be performed with limited GPU memory, and it allows each part of the model to learn distinct portions of the data."
  - [section]: "The staged-training strategy also addresses memory constraints by training different parts of the model sequentially."
  - [corpus]: Weak - no direct evidence on memory usage reduction.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: Enables adaptation of English LLMs to Korean by leveraging shared linguistic patterns between languages.
  - Quick check question: What linguistic similarities between English and Korean enable cross-lingual transfer?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Understanding how to prevent the model from losing English language capabilities while learning Korean.
  - Quick check question: How does staged training prevent catastrophic forgetting in continual pretraining?

- Concept: Tokenizer vocabulary optimization
  - Why needed here: Balancing input complexity (token count) and embedding complexity (parameter count) for efficient processing.
  - Quick check question: Why does increasing vocabulary size reduce input complexity but increase embedding complexity?

## Architecture Onboarding

- Component map:
  Tokenizer -> Embedding layer (pretrained English + new Korean embeddings) -> Transformer blocks -> LM Head (adapted for Korean) -> LoRA adapters

- Critical path:
  1. Corpus preprocessing → Tokenizer training → Model initialization → Staged training → Evaluation
  2. Tokenizer training depends on corpus quality
  3. Model initialization depends on tokenizer vocabulary
  4. Staged training depends on memory constraints and component dependencies

- Design tradeoffs:
  - Vocabulary size: Larger size reduces tokens but increases parameters
  - Learning rate: Higher rates for new components, lower for pretrained components
  - Memory vs. training time: Staged training saves memory but extends total training duration

- Failure signatures:
  - Out-of-memory errors: Indicate insufficient memory for current training stage
  - Poor convergence: Suggest suboptimal initialization or learning rate
  - Performance degradation: May indicate catastrophic forgetting or inadequate adaptation

- First 3 experiments:
  1. Tokenizer vocabulary optimization: Test different vocabulary sizes (8k, 12k, 16k, 20k, 24k, 28k, 32k, 36k, 40k) and measure token count and embedding parameters
  2. Initialization method comparison: Compare random initialization, averaging, and decomposition methods on small subset of data
  3. Staged training validation: Train embedding+LM head only vs. full model to verify memory efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does filtering genuinely enhance model performance by reducing the number of training samples while maintaining accuracy?
- Basis in paper: [explicit] The paper mentions that corpus filtering aims to improve computational efficiency and model quality, but notes that a formal quantitative evaluation remains to be completed.
- Why unresolved: The authors have not conducted a rigorous comparison between filtered and unfiltered datasets to validate the effectiveness of filtering on model performance.
- What evidence would resolve it: A controlled experiment comparing model performance using filtered vs. unfiltered datasets, with quantitative metrics showing accuracy and computational efficiency gains.

### Open Question 2
- Question: What is the most effective and appropriate filtering method for corpus preprocessing in continual pretraining?
- Basis in paper: [explicit] The paper suggests multiple filtering approaches (random, length-based, perplexity-based, diversity-based) but does not evaluate which method yields the best results.
- Why unresolved: The authors acknowledge the need to compare various filtering methods but have not implemented or tested them systematically.
- What evidence would resolve it: Empirical results from experiments testing different filtering methods, showing which approach maximizes model performance and efficiency.

### Open Question 3
- Question: Is the staged training strategy with alternating odd/even layers the optimal approach for adapting LLMs to new languages?
- Basis in paper: [inferred] The authors propose this strategy to manage GPU memory constraints and facilitate efficient learning, but admit that alternative layer-training orders have not been tested.
- Why unresolved: The effectiveness of this specific training order compared to other possible strategies (e.g., sequential layer training) remains unverified due to lack of comparative experiments.
- What evidence would resolve it: Experimental results comparing the proposed staged training strategy with alternative approaches, demonstrating whether the current method yields superior performance or efficiency.

## Limitations
- The optimal vocabulary size of 20,000 tokens may not generalize to other low-resource languages or different corpus characteristics
- Staged training strategy significantly extends total training time and may be unnecessary with sufficient computational resources
- Paper lacks comprehensive ablation studies to quantify individual contributions of each component to final performance

## Confidence
- High Confidence: Cross-lingual transfer learning for adapting English LLMs to Korean is well-established; RedWhale outperforms existing models on KoBEST benchmarks
- Medium Confidence: Vocabulary size optimization (20,000 tokens) is theoretically sound and supported by experimental results on specific Korean corpus
- Low Confidence: Staged training strategy's effectiveness in preventing catastrophic forgetting lacks rigorous empirical validation; scalability to structurally different languages is speculative

## Next Checks
1. Conduct vocabulary size sensitivity analysis across multiple Korean datasets (5k, 10k, 15k, 20k, 25k, 30k, 35k, 40k) to verify 20,000 tokens remains optimal across different corpus characteristics

2. Design controlled experiments comparing RedWhale's staged training approach against continuous full-model training, using English benchmarks before and after Korean adaptation to quantitatively measure catastrophic forgetting

3. Apply RedWhale pipeline to another low-resource language with different linguistic characteristics (e.g., Finnish or Arabic) to test scalability claims and compare performance gains against direct training from scratch