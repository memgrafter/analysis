---
ver: rpa2
title: 'FDM-Bench: A Comprehensive Benchmark for Evaluating Large Language Models
  in Additive Manufacturing Tasks'
arxiv_id: '2412.09819'
source_url: https://arxiv.org/abs/2412.09819
tags:
- each
- these
- across
- user
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FDM-Bench, the first comprehensive benchmark
  dataset for evaluating large language models (LLMs) on fused deposition modeling
  (FDM) tasks. The dataset includes user queries across experience levels and G-code
  samples with various anomalies.
---

# FDM-Bench: A Comprehensive Benchmark for Evaluating Large Language Models in Additive Manufacturing Tasks

## Quick Facts
- **arXiv ID**: 2412.09819
- **Source URL**: https://arxiv.org/abs/2412.09819
- **Reference count**: 40
- **Key outcome**: FDM-Bench is the first comprehensive benchmark for evaluating LLMs on FDM tasks, showing closed-source models outperform open-source models in G-code anomaly detection while Llama-3.1-405B achieves comparable performance to closed-source models in user query responses.

## Executive Summary
This paper introduces FDM-Bench, the first comprehensive benchmark dataset for evaluating large language models (LLMs) on fused deposition modeling (FDM) tasks. The dataset includes user queries across experience levels and G-code samples with various anomalies. Four state-of-the-art LLMs—GPT-4o, Claude 3.5 Sonnet, Llama-3.1-70B, and Llama-3.1-405B—are evaluated on FDM-Bench. Results show that closed-source models (GPT-4o and Claude 3.5 Sonnet) outperform open-source models in G-code anomaly detection, while Llama-3.1-405B achieves comparable performance to closed-source models in responding to user queries.

## Method Summary
The paper evaluates four LLMs on FDM-Bench, which includes G-code samples with anomalies and user queries across three expertise levels. Models are assessed using zero temperature settings for deterministic outputs, with expert panels evaluating free-form responses and automated scoring for multiple-choice questions. The evaluation focuses on two main tasks: G-code anomaly detection (accuracy and probability scoring) and user query responses (accuracy, precision, relevance).

## Key Results
- Closed-source models (GPT-4o and Claude 3.5 Sonnet) outperform open-source models in G-code anomaly detection
- Llama-3.1-405B achieves comparable performance to closed-source models in user query responses
- Llama-3.1-70B shows the lowest performance across both tasks, likely due to smaller model size
- GPT-4o achieves the highest accuracy in G-code anomaly detection at 62% correct identification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM performance in G-code anomaly detection correlates with model scale and training data domain alignment.
- Mechanism: Larger parameter counts and training data containing technical text/code enable better parsing of G-code syntax and understanding of parameter relationships that cause defects.
- Core assumption: Model parameter size and domain-specific training data quality directly impact performance on specialized technical tasks.
- Evidence anchors: [abstract] "Llama-3.1-70B generally performs the lowest across both tasks, likely due to its smaller model size"; [section] "Llama-70B does not show a clear distinction between correct and incorrect labels, indicating a limited ability to identify anomalies accurately."
- Break condition: If fine-tuning on domain-specific G-code datasets shows negligible performance gains for smaller models, the scaling hypothesis would be challenged.

### Mechanism 2
- Claim: Closed-source models have access to specialized training data or architectures that improve technical task performance.
- Mechanism: Proprietary training approaches, including curated technical datasets or specialized fine-tuning, enable better handling of domain-specific tasks like G-code analysis.
- Core assumption: Training methodology and data curation significantly impact model performance on specialized technical tasks.
- Evidence anchors: [abstract] "closed-source models (GPT-4o and Claude 3.5 Sonnet) outperform open-source models in G-code anomaly detection"; [section] "GPT-4o achieves the highest accuracy, correctly identifying the ground truth anomaly label in 62% of cases."
- Break condition: If open-source models achieve comparable performance through fine-tuning on public technical datasets, the proprietary advantage would be questioned.

### Mechanism 3
- Claim: Prompt engineering and temperature settings significantly impact model performance consistency on technical tasks.
- Mechanism: Zero temperature settings and task-specific prompts reduce variability and improve reliability in technical reasoning tasks.
- Core assumption: Consistent prompt structure and deterministic settings are critical for reproducible technical task performance.
- Evidence anchors: [section] "To evaluate the LLMs' performance, we craft prompts tailored to the specific requirements of each task, emphasizing reasoning and factual accuracy over creative output. Accordingly, we set the temperature parameter to zero across all models"; [section] "This approach ensures a fair comparison, as all models operate under identical conditions."
- Break condition: If varying temperature settings shows minimal impact on technical task performance, the deterministic approach importance would be questioned.

## Foundational Learning

- Concept: G-code syntax and structure in FDM
  - Why needed here: Models must parse and understand G-code commands to detect anomalies
  - Quick check question: What are the key G-code commands that control extrusion and temperature in FDM printing?

- Concept: FDM printing parameters and their relationships
  - Why needed here: Understanding how parameters like flow rate, temperature, and speed interact is crucial for anomaly detection
  - Quick check question: How does increasing the extrusion multiplier affect print quality and what defects might result?

- Concept: Prompt engineering for technical tasks
  - Why needed here: Effective prompts are essential for obtaining reliable technical responses from LLMs
  - Quick check question: What are the key elements of an effective technical prompt for LLMs?

## Architecture Onboarding

- Component map: G-code files, user queries, prompt templates -> LLM models (GPT-4o, Claude 3.5 Sonnet, Llama-3.1-70B, Llama-3.1-405B) -> Expert panels, automated scoring systems, MCQs -> Performance metrics, anomaly predictions, user responses

- Critical path: 1. G-code generation and anomaly injection; 2. Prompt formulation with zero temperature setting; 3. LLM inference and response collection; 4. Expert evaluation of free-form responses; 5. Automated scoring of MCQs; 6. Performance analysis and comparison

- Design tradeoffs: Model selection: Closed-source vs open-source models; Evaluation method: Expert evaluation vs automated scoring; Dataset size: Balance between comprehensive coverage and practical evaluation time; Prompt complexity: Detailed prompts vs simpler instructions

- Failure signatures: Inconsistent anomaly detection across similar G-code samples; Preference for certain labels regardless of input; Difficulty handling complex user queries at higher expertise levels; Large performance variation across different experience levels

- First 3 experiments: 1. Compare LLM performance on clean vs intentionally corrupted G-code samples; 2. Test model responses to progressively complex user queries; 3. Evaluate model confidence scores vs actual accuracy in anomaly detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can FDM-Bench be extended to evaluate LLMs across additional AM technologies beyond FDM, such as selective laser sintering (SLS), stereolithography (SLA), and metal additive manufacturing?
- Basis in paper: [explicit] The authors suggest expanding FDM-Bench to "broaden its applicability across a wider range of AM tasks" and explicitly mention evaluating "additional AM technologies such as selective laser sintering, stereolithography, and metal AM."
- Why unresolved: The current benchmark is FDM-specific, and no methodology or dataset for other AM processes has been developed yet.
- What evidence would resolve it: A modified or expanded version of FDM-Bench that includes representative datasets, tasks, and evaluation metrics for other AM technologies, with performance comparisons across models.

### Open Question 2
- Question: How can large vision-language models (LVLMs) be integrated into FDM-Bench to improve defect detection through image-based analysis, particularly for defects caused by external factors rather than process parameters?
- Basis in paper: [explicit] The authors propose "the inclusion of image-based tasks that utilize large vision-language models (LVLMs) to improve visual defect detection and enable the prediction of anomalies arising from external causes."
- Why unresolved: Current FDM-Bench focuses on G-code and textual analysis, with no integration of image-based tasks or LVLMs.
- What evidence would resolve it: A version of FDM-Bench that incorporates image datasets, LVLMs, and evaluation metrics for visual defect detection, with comparative results against the existing text-based tasks.

### Open Question 3
- Question: How do advanced prompting techniques, such as few-shot prompting, chain-of-thought, and tree-of-thought, affect the performance of LLMs in FDM-specific tasks, and which techniques are most effective for reasoning and calculations in AM contexts?
- Basis in paper: [explicit] The authors mention that "given the sensitivity of these models to prompt structure, incorporating advanced prompting techniques... may offer deeper insights into their reasoning processes."
- Why unresolved: The study uses a fixed prompt structure without exploring variations or advanced techniques, leaving the impact of prompting strategies unexplored.
- What evidence would resolve it: Systematic experiments comparing LLM performance on FDM-Bench tasks using different prompting techniques, with quantitative results showing which methods yield the best accuracy, consistency, or reasoning capabilities.

## Limitations
- Evaluation based on only four specific LLM models, limiting generalizability
- G-code anomaly detection task uses a relatively small sample size (24 G-code files)
- Expert panel evaluation introduces potential subjectivity in free-form response assessment

## Confidence
- **High confidence**: Model performance rankings (GPT-4o and Claude 3.5 Sonnet outperforming Llama-3.1-70B in G-code anomaly detection)
- **Medium confidence**: Claims about parameter scaling effects on performance
- **Low confidence**: Generalizability of results to other FDM scenarios and model architectures

## Next Checks
1. Replication with expanded dataset: Test the same four models on a larger, more diverse set of G-code samples with additional anomaly types to validate the consistency of performance rankings.

2. Fine-tuning validation: Fine-tune open-source models on domain-specific G-code datasets and compare performance against the baseline results to assess the impact of specialized training.

3. Cross-task generalization: Evaluate the same models on related additive manufacturing tasks (e.g., metal printing, SLA) to determine if performance patterns hold across different manufacturing processes.