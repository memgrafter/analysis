---
ver: rpa2
title: Calibrating LLMs with Preference Optimization on Thought Trees for Generating
  Rationale in Science Question Scoring
arxiv_id: '2406.19949'
source_url: https://arxiv.org/abs/2406.19949
tags: []
core_contribution: This paper introduces a novel framework for generating more faithful
  rationales in automated science question scoring by mimicking human assessment processes
  through thought trees. The method involves generating synthetic rationale data and
  preference data from thought tree paths, followed by supervised fine-tuning and
  preference optimization to calibrate large language models.
---

# Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring

## Quick Facts
- arXiv ID: 2406.19949
- Source URL: https://arxiv.org/abs/2406.19949
- Reference count: 40
- Primary result: 38% improvement in Quadratic Weighted Kappa (QWK) score compared to prior work

## Executive Summary
This paper introduces a novel framework for generating more faithful rationales in automated science question scoring by mimicking human assessment processes through thought trees. The method involves generating synthetic rationale data and preference data from thought tree paths, followed by supervised fine-tuning and preference optimization to calibrate large language models. Extensive experiments show significant improvements in scoring accuracy and rationale quality compared to existing approaches.

## Method Summary
The framework employs a two-stage approach: first generating synthetic rationale data through thought tree paths that simulate human assessment processes, then applying preference optimization to align model outputs with these generated rationales. The thought tree methodology captures the reasoning process behind science question scoring, enabling the model to produce more interpretable and accurate justifications. Supervised fine-tuning is used to initially align the model with the generated data, followed by preference optimization to further refine the output quality.

## Key Results
- Achieved 38% improvement in Quadratic Weighted Kappa (QWK) score compared to prior work
- Generated rationales demonstrated higher quality as recognized by both human evaluators and language models
- Successfully bridged the performance gap with traditional text classification methods while enhancing explainability

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to simulate human assessment processes through thought trees, creating synthetic data that captures the nuanced reasoning behind science question scoring. By generating multiple reasoning paths and applying preference optimization, the model learns to produce rationales that align more closely with human judgment patterns.

## Foundational Learning
- Thought trees: Hierarchical reasoning structures that capture assessment logic (why needed: to model human scoring processes; quick check: verify tree completeness)
- Preference optimization: Method for aligning model outputs with desired behaviors (why needed: to refine rationale quality; quick check: compare pre/post optimization outputs)
- Synthetic data generation: Creating training data from thought tree paths (why needed: to overcome limited labeled data; quick check: validate data diversity)
- Quadratic Weighted Kappa: Evaluation metric for scoring agreement (why needed: to measure model performance; quick check: calculate baseline scores)
- Supervised fine-tuning: Initial model alignment phase (why needed: to establish basic rationale generation; quick check: assess alignment quality)
- Human evaluation: Qualitative assessment of rationale quality (why needed: to validate model outputs; quick check: establish evaluation criteria)

## Architecture Onboarding

Component Map: Input -> Thought Tree Generator -> Supervised Fine-tuning -> Preference Optimization -> Calibrated Model

Critical Path: The pipeline follows a sequential process where thought tree generation feeds into supervised fine-tuning, which then feeds into preference optimization, culminating in the calibrated model.

Design Tradeoffs: The approach trades computational efficiency for higher-quality rationales, requiring multiple generation paths and optimization steps. This increases processing time but improves output quality and explainability.

Failure Signatures: Common failure modes include incomplete thought tree generation, preference optimization misalignment, and synthetic data bias. These typically manifest as inconsistent scoring or poorly structured rationales.

First Experiments:
1. Baseline comparison: Test model performance against traditional text classification methods
2. Thought tree validation: Verify synthetic data quality and diversity
3. Preference optimization assessment: Measure impact of optimization on rationale quality

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data generation may not fully capture real-world response complexity
- Performance generalizability across different assessment domains is unclear
- Computational overhead and scalability challenges due to multiple generation paths

## Confidence
- QWK improvement claim: Medium-High
- Rationale quality improvement claim: Medium
- Generalizability claim: Low

## Next Checks
1. Conduct cross-domain validation tests to assess performance consistency across different types of assessment tasks beyond science questions
2. Perform ablation studies to quantify the individual contributions of supervised fine-tuning versus preference optimization to the overall performance improvement
3. Implement a temporal stability analysis to evaluate whether the generated rationales maintain their quality and consistency across multiple runs and over time