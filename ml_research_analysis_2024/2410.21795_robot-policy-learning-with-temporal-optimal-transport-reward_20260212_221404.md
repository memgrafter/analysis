---
ver: rpa2
title: Robot Policy Learning with Temporal Optimal Transport Reward
arxiv_id: '2410.21795'
source_url: https://arxiv.org/abs/2410.21795
tags:
- reward
- learning
- expert
- information
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TemporalOT, a method for improving imitation
  learning from expert demonstrations by incorporating temporal order information
  into optimal transport (OT) rewards. The authors address the limitation of standard
  OT rewards that ignore temporal order, which can lead to noisy reward signals.
---

# Robot Policy Learning with Temporal Optimal Transport Reward

## Quick Facts
- arXiv ID: 2410.21795
- Source URL: https://arxiv.org/abs/2410.21795
- Authors: Yuwei Fu; Haichao Zhang; Di Wu; Wei Xu; Benoit Boulet
- Reference count: 40
- Key outcome: TemporalOT achieves 61.1% success rate on Meta-world tasks vs 35.9% for previous best method

## Executive Summary
This paper addresses a fundamental limitation in imitation learning from expert demonstrations by introducing TemporalOT, which incorporates temporal order information into optimal transport (OT) rewards. Standard OT-based imitation learning ignores temporal order, leading to noisy reward signals that can degrade policy learning. TemporalOT solves this by using context embeddings for more accurate cost matrices and a temporal mask mechanism that focuses OT calculations on relevant time steps. The method demonstrates significant performance improvements across nine Meta-world benchmark tasks, achieving an average success rate of 61.1% compared to 35.9% for the previous best method.

## Method Summary
TemporalOT enhances imitation learning by addressing the temporal order limitation in standard OT rewards. The method uses two key innovations: context embeddings that incorporate local temporal information when computing transport costs between agent and expert observations, and a temporal mask that limits OT reward calculations to nearby time steps. This approach is combined with a pretrained visual encoder (ResNet50) and an RL agent (DrQ-v2) trained using the OT-based proxy rewards. The method is evaluated on Meta-world tasks with both state-based and pixel-based observations, using two expert demonstrations per task and measuring success rate over 100 evaluations every 20,000 training steps.

## Key Results
- TemporalOT achieves 61.1% average success rate on nine Meta-world tasks, outperforming the previous best method (ADS) at 35.9%
- The temporal mask mechanism provides significant improvement over vanilla OT rewards, particularly in tasks requiring precise temporal coordination
- Context embeddings improve transport cost accuracy compared to single-frame comparisons
- The method demonstrates robustness with both state-based and pixel-based observations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal order information improves reward accuracy by focusing on relevant time steps
- Mechanism: The temporal mask limits OT reward calculation to nearby time steps, reducing noise from irrelevant observations
- Core assumption: Expert and agent move at similar speeds, making temporal proximity meaningful
- Evidence anchors:
  - [abstract]: "incorporates temporal order information to the OT-based proxy reward via using context embeddings and a mask mechanism"
  - [section]: "we introduce a concise solution by adding a temporal mask to the cost matrix"
  - [corpus]: Weak - related papers focus on cross-embodiment or constrained demonstrators, not temporal masking
- Break condition: If expert and agent move at significantly different speeds, the temporal mask assumption fails

### Mechanism 2
- Claim: Context embeddings provide more accurate transport costs by incorporating local temporal information
- Mechanism: Group-wise cosine similarity considers nearby frames rather than single frame pairs, capturing local movement patterns
- Core assumption: Local temporal context provides meaningful information about state similarity
- Evidence anchors:
  - [section]: "we adopt a group-wise cosine similarity that we define the transport cost between agent observation oi and expert observation oE j"
  - [abstract]: "incorporates temporal order information...via using context embeddings"
  - [corpus]: Weak - no direct evidence in related papers about context embedding approaches
- Break condition: If local temporal context doesn't correlate with task-relevant similarity, context embeddings add noise

### Mechanism 3
- Claim: Non-stationary OT rewards provide more informative signals than static rewards
- Mechanism: OT rewards adapt based on entire trajectory, providing relative rankings of state goodness
- Core assumption: Relative ranking of states is more useful than absolute reward values
- Evidence anchors:
  - [section]: "OT reward is used to distinguish the goodness of different states"
  - [abstract]: "the OT reward is invariant to temporal order information, which could bring extra noise"
  - [corpus]: Weak - related papers focus on alignment and transferability, not reward non-stationarity
- Break condition: If trajectory-based reward calculation becomes too unstable during training

## Foundational Learning

- Concept: Optimal Transport (Wasserstein distance)
  - Why needed here: Forms the basis for measuring trajectory similarity and generating proxy rewards
  - Quick check question: What mathematical problem does optimal transport solve in the context of imitation learning?

- Concept: Markov Decision Process (MDP)
  - Why needed here: Framework for modeling the robot control problem and defining the learning objective
  - Quick check question: How does the reward specification problem in RL relate to the inverse reinforcement learning approach used here?

- Concept: Policy Gradient Methods
  - Why needed here: Required to understand how the proxy reward from OT is used to update the policy
  - Quick check question: Why is it important that the OT reward is non-stationary when using policy gradient methods?

## Architecture Onboarding

- Component map: Visual encoder → Context cost matrix → Temporal mask → OT solver → Reward → RL agent
- Critical path: Expert demonstrations → Visual embeddings → Cost matrix computation → Temporal masking → OT optimization → Policy update
- Design tradeoffs: Temporal mask window size vs. reward accuracy vs. computational cost
- Failure signatures: Policy not learning (bad visual encoder), Slow convergence (mask too narrow), Suboptimal behavior (mask too wide)
- First 3 experiments:
  1. Validate that temporal mask improves performance on simple tasks with clear temporal structure
  2. Test context embedding length sensitivity on tasks with varying temporal dependencies
  3. Compare fixed vs. dynamic temporal masks on tasks with known speed discrepancies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TemporalOT perform with more than two expert demonstrations, and is there an optimal number beyond which performance plateaus or degrades?
- Basis in paper: [explicit] The paper mentions that using more expert demonstrations can mitigate overfitting and improve performance, with results shown in Figure 5.
- Why unresolved: The experiments only tested up to 4 demonstrations (1x, 2x, 3x, 4x speed variations), leaving open questions about scalability to larger demonstration sets.
- What evidence would resolve it: Systematic experiments testing TemporalOT with varying numbers of demonstrations (e.g., 2, 5, 10, 20) to identify performance trends and optimal demonstration count.

### Open Question 2
- Question: Can the temporal mask mechanism be extended to handle demonstrations with significantly different movement speeds than the agent?
- Basis in paper: [inferred] The paper notes that TemporalOT assumes similar movement speeds between expert and agent, and performance degrades with speed discrepancies (Table 3).
- Why unresolved: The current temporal mask design relies on distance between time step indices, which breaks down when speed differences are large.
- What evidence would resolve it: Development and evaluation of a dynamic temporal mask that can adapt to speed differences, potentially using alignment techniques or learned transformations.

### Open Question 3
- Question: How robust is TemporalOT to suboptimal or noisy expert demonstrations?
- Basis in paper: [explicit] The paper mentions that performance relies on high-quality expert demonstrations and inherits flaws from suboptimal demonstrations.
- Why unresolved: All experiments used clean expert demonstrations, and no systematic evaluation of performance under demonstration noise or suboptimality was conducted.
- What evidence would resolve it: Experiments introducing various levels of noise or suboptimality into expert demonstrations to test TemporalOT's robustness and identify failure modes.

### Open Question 4
- Question: What is the computational complexity of TemporalOT compared to standard OT-based methods, and how does it scale with trajectory length and number of demonstrations?
- Basis in paper: [explicit] The paper mentions that computation cost is related to the number of expert demonstrations and that a larger number increases computation cost.
- Why unresolved: No detailed analysis of computational complexity or runtime scaling with problem size was provided.
- What evidence would resolve it: Theoretical analysis of computational complexity and empirical runtime measurements across varying trajectory lengths and demonstration counts.

## Limitations
- The temporal mask assumes similar movement speeds between expert and agent, with performance degrading under speed discrepancies
- Computational overhead of the OT solver is not discussed, though noted to be expensive for long trajectories
- Experiments are limited to Meta-world tasks, leaving generalization to more complex scenarios untested

## Confidence

**High Confidence Claims:**
- TemporalOT outperforms all baseline methods on the nine Meta-world tasks tested
- The temporal mask mechanism provides meaningful improvement over vanilla OT rewards
- Context embeddings improve transport cost accuracy compared to single-frame comparisons

**Medium Confidence Claims:**
- TemporalOT will generalize well to other robotic tasks beyond Meta-world
- The computational cost of OT is acceptable for real-world deployment
- The improvements scale proportionally with more expert demonstrations

**Low Confidence Claims:**
- Exact parameter sensitivity (mask size, context length) across diverse task domains
- Performance comparison with human-level demonstrations
- Robustness to significant speed discrepancies between expert and agent

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically test the sensitivity of TemporalOT to different temporal mask sizes (w=1,3,5,7) and context lengths (k=1,2,4,8) across a broader range of task types to establish robust parameter guidelines.

2. **Cross-Domain Generalization**: Evaluate TemporalOT on non-Meta-world robotic tasks with varying temporal structures, including tasks with significant speed variations between expert and agent to test temporal mask robustness.

3. **Computational Efficiency Benchmarking**: Measure the wall-clock time and memory requirements of the OT solver across different trajectory lengths and batch sizes, comparing against alternative alignment methods to quantify the practical deployment overhead.