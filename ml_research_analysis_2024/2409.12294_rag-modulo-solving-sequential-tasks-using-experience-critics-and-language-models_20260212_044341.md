---
ver: rpa2
title: 'RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language
  Models'
arxiv_id: '2409.12294'
source_url: https://arxiv.org/abs/2409.12294
tags:
- language
- rag-modulo
- tasks
- memory
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAG-Modulo introduces a framework for enhancing LLM-based agents
  with memory of past interactions, enabling learning from experience in sequential
  decision-making tasks. The core method integrates a retrieval-augmented generation
  system that stores successful interactions with critic feedback and retrieves them
  as in-context examples for future tasks.
---

# RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models

## Quick Facts
- arXiv ID: 2409.12294
- Source URL: https://arxiv.org/abs/2409.12294
- Authors: Abhinav Jain; Chris Jermaine; Vaibhav Unhelkar
- Reference count: 40
- One-line primary result: Memory-augmented LLM agents with critic feedback achieve 0.57 success rate on BabyAI-BossLevel, outperforming static planners and interactive planners without experience memory.

## Executive Summary
RAG-Modulo introduces a framework for enhancing LLM-based agents with memory of past interactions, enabling learning from experience in sequential decision-making tasks. The core method integrates a retrieval-augmented generation system that stores successful interactions with critic feedback and retrieves them as in-context examples for future tasks. Experiments in BabyAI and AlfWorld domains show significant improvements: RAG-Modulo achieves higher success rates (up to 0.57 vs 0.24 for LLM-Planner in BabyAI-BossLevel), reduces in-executability by approximately 7-16 actions, and shortens episode lengths by 1.4-2.0 steps compared to baseline approaches. The framework demonstrates that learning from past successes and failures through memory retrieval outperforms both static planners and interactive planners without experience memory.

## Method Summary
RAG-Modulo enhances LLM-based agents with a memory component that stores successful task interactions along with critic feedback. When faced with a new task, the agent retrieves K similar past interactions using cosine similarity between embedding vectors and incorporates them as in-context examples in the LLM prompt. Multiple critics (syntax, semantics, low-level policy) evaluate each LLM-generated action, providing structured feedback that guides future decisions. The system continuously updates its memory with successful interactions, enabling learning from experience without model fine-tuning. The framework is evaluated on BabyAI and AlfWorld environments, comparing against baselines including ProgPrompt and LLM-Planner using success rate, in-executability, and episode length metrics.

## Key Results
- RAG-Modulo achieves 0.57 success rate on BabyAI-BossLevel vs 0.24 for LLM-Planner baseline
- Reduces in-executability by approximately 7-16 actions compared to baselines
- Shortens episode lengths by 1.4-2.0 steps compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memory retrieval improves LLM decision-making by providing relevant past experiences as in-context examples
- Mechanism: RAG-Modulo stores successful task interactions with critic feedback and retrieves them using cosine similarity between embeddings. These retrieved interactions serve as in-context examples in the LLM prompt, providing context-aware guidance for current decision-making steps.
- Core assumption: Past successful interactions contain transferable patterns that can guide current decisions, and the LLM can effectively learn from these in-context examples.
- Evidence anchors:
  - [abstract] "The memory component allows the agent to automatically retrieve and incorporate relevant past experiences as in-context examples"
  - [section] "The memory component allows the agent to automatically retrieve and incorporate relevant past experiences as in-context examples, providing context-aware feedback for more informed decision-making"
  - [corpus] Weak evidence - corpus contains related work on memory management but lacks direct evidence for this specific mechanism
- Break condition: If retrieved examples are not sufficiently similar to current context, or if LLM cannot effectively utilize in-context examples for learning

### Mechanism 2
- Claim: Critic feedback transforms raw interactions into actionable learning signals
- Mechanism: Multiple critics (syntax, semantics, low-level policy) evaluate each LLM-generated action and provide detailed feedback. This feedback is stored alongside successful interactions and retrieved with them, creating a complete learning signal that includes both what worked and why previous attempts failed.
- Core assumption: LLM can effectively parse and utilize structured feedback about action feasibility to improve future decisions
- Evidence anchors:
  - [abstract] "incorporates critics to evaluate the agents' decisions"
  - [section] "critics provide feedback on actions selected by the LLM... each critic either returns SUCCESS or FAILURE along with the corresponding REASON"
  - [corpus] Weak evidence - corpus shows related work on critics but lacks direct evidence for this specific feedback mechanism
- Break condition: If feedback is too complex for LLM to parse, or if critics provide inconsistent/inaccurate evaluations

### Mechanism 3
- Claim: Online memory updates enable continuous learning without expensive fine-tuning
- Mechanism: After successfully completing tasks, RAG-Modulo stores the complete interaction history (including corrections) in memory. This creates an expanding knowledge base that the agent accesses for future tasks, enabling learning from experience without requiring model fine-tuning or backpropagation.
- Core assumption: Online memory storage is sufficient for meaningful learning improvements, and the retrieval mechanism can effectively find relevant past experiences
- Evidence anchors:
  - [abstract] "Further by updating its memory, the agent improves its performance over time, thereby exhibiting learning"
  - [section] "Once the goal is achieved, the interaction memory is updated for future retrieval (lines 14 − 16), enabling learning from experience"
  - [corpus] Weak evidence - corpus contains related work on experience replay but lacks direct evidence for this specific online learning mechanism
- Break condition: If memory becomes too large for efficient retrieval, or if new experiences are too dissimilar from past experiences to provide useful guidance

## Foundational Learning

- Concept: In-context learning and few-shot prompting
  - Why needed here: RAG-Modulo relies on providing past interactions as in-context examples to guide current LLM decisions
  - Quick check question: What is the maximum number of in-context examples typically effective for GPT-4 before performance degrades?

- Concept: Vector embeddings and similarity search
  - Why needed here: The retrieval mechanism uses cosine similarity between embedding vectors to find relevant past interactions
  - Quick check question: What embedding model is used in the paper for encoding interactions into vectors?

- Concept: Reinforcement learning vs. prompt-based learning
  - Why needed here: Understanding why this approach avoids traditional RL while achieving learning through memory updates
  - Quick check question: How does RAG-Modulo's learning approach differ fundamentally from standard reinforcement learning?

## Architecture Onboarding

- Component map: LLM → Critics → Memory Database → Retrieval Engine → Prompt Constructor
- Critical path: Current observation → Retrieve K similar interactions → Construct prompt with examples → LLM generates action → Critics evaluate → Store if successful
- Design tradeoffs: Memory size vs. retrieval efficiency, number of critics vs. evaluation time, embedding quality vs. retrieval accuracy
- Failure signatures: High in-executability despite memory, memory retrieval returning irrelevant examples, critics providing inconsistent feedback
- First 3 experiments:
  1. Test baseline LLM performance without memory on BabyAI-Synth level
  2. Test retrieval mechanism with synthetic memory entries to verify cosine similarity works
  3. Test complete system with memory seeded from expert demonstrations on simple tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between the number of retrieved interactions (K) and the quality of those interactions for maximizing task success rates?
- Basis in paper: [explicit] The paper explores the effect of varying K on performance, finding success rates peak at K = 5 for BossLevel and K = 10 for SynthLevel.
- Why unresolved: The paper does not investigate how the quality of retrieved interactions impacts performance or how to optimize retrieval for quality over quantity.
- What evidence would resolve it: Experiments comparing performance when varying both the number and quality of retrieved interactions, potentially using different retrieval models or quality metrics.

### Open Question 2
- Question: How can RAG-Modulo be adapted for physical robots in real-world environments with continuous action spaces and unstructured observations?
- Basis in paper: [inferred] The paper focuses on discrete action spaces in simulated environments (AlfWorld and BabyAI), with no mention of adapting to continuous control or real-world sensory data.
- Why unresolved: The paper's experimental domains are simulated and discrete, leaving the question of real-world applicability open.
- What evidence would resolve it: Demonstrations of RAG-Modulo successfully solving tasks in real robotic systems with continuous control and unstructured sensor data.

### Open Question 3
- Question: How does the performance of RAG-Modulo scale with increasing task complexity and memory size?
- Basis in paper: [explicit] The paper shows performance improvements in existing BabyAI and AlfWorld tasks but does not explore scalability to more complex tasks or larger memory databases.
- Why unresolved: The experiments are limited to specific benchmark tasks with controlled memory sizes, not exploring the limits of scalability.
- What evidence would resolve it: Systematic experiments testing RAG-Modulo on progressively more complex tasks while scaling memory size, measuring performance degradation or improvements.

## Limitations
- Evaluation limited to specific environments (BabyAI and AlfWorld) with relatively small state spaces
- Reliance on expert demonstrations for memory seeding creates dependency that may not be practical
- Does not thoroughly investigate computational overhead of retrieval mechanism or memory storage requirements

## Confidence

- **High Confidence**: The core empirical results showing improved success rates and reduced in-executability are well-supported by the experimental data. The comparison methodology against baselines is sound.
- **Medium Confidence**: The mechanism claims about why memory retrieval improves performance are reasonable but rely on implicit assumptions about LLM in-context learning capabilities that are not fully validated.
- **Low Confidence**: The claims about the framework's ability to generalize to unseen environments are based on limited evidence, as the AlfWorld unseen validation set only contains 132 tasks.

## Next Checks

1. **Scalability Test**: Evaluate RAG-Modulo on a more complex environment with larger state spaces (e.g., ALFRED or similar) to assess whether the memory-based approach scales effectively beyond the current domains.

2. **Memory Size Impact**: Systematically vary the size of the interaction memory (both K retrieved examples and total stored interactions) to identify the optimal balance between retrieval quality and computational efficiency, and to understand the saturation point of the learning effect.

3. **Generalization Analysis**: Conduct a more thorough analysis of performance on truly novel tasks by testing in environments with significantly different characteristics from the training distribution, rather than just the unseen validation sets within the same domain.