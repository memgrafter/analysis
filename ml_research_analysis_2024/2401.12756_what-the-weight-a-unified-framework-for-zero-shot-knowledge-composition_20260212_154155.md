---
ver: rpa2
title: What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition
arxiv_id: '2401.12756'
source_url: https://arxiv.org/abs/2401.12756
tags:
- adapter
- adapters
- domain
- strategies
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework for zero-shot knowledge
  composition using adapter layers, encompassing existing and novel strategies for
  selecting, weighting, and combining parameter modules. The authors conduct a comprehensive
  benchmarking study of various composition strategies, testing two module combination
  methods (averaging and ensembling) and five selection and weighting strategies (uniform,
  model entropy, domain prior, semantic sentence similarity, and TF-IDF) across three
  model types and 21 training/10 evaluation domains.
---

# What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition

## Quick Facts
- arXiv ID: 2401.12756
- Source URL: https://arxiv.org/abs/2401.12756
- Authors: Carolin Holtermann; Markus Frohmann; Navid Rekabsaz; Anne Lauscher
- Reference count: 40
- Key outcome: A unified framework for zero-shot knowledge composition using adapter layers, demonstrating that ensembling typically outperforms parameter averaging and simpler corpus-based weighting strategies often surpass more complex model-based approaches

## Executive Summary
This paper introduces a unified framework for zero-shot knowledge composition using adapter layers, encompassing existing and novel strategies for selecting, weighting, and combining parameter modules. The authors conduct a comprehensive benchmarking study of various composition strategies, testing two module combination methods (averaging and ensembling) and five selection and weighting strategies (uniform, model entropy, domain prior, semantic sentence similarity, and TF-IDF) across three model types and 21 training/10 evaluation domains. The results demonstrate that ensembling typically outperforms parameter averaging, simpler corpus-based weighting and selection strategies (TF-IDF and Sentence Similarity) often surpass more complex model-based approaches, and the performance of adapter composition can be partially predicted using meta-regression.

## Method Summary
The unified framework for zero-shot knowledge composition uses adapter layers to enable efficient domain adaptation without access to labeled data from target domains. The method involves training separate adapter modules on 21 source domains, then composing these adapters for evaluation on 10 target domains using various selection and weighting strategies. The framework tests two combination approaches (parameter averaging and output ensembling) and five scoring strategies (uniform, model entropy, domain prior, semantic sentence similarity, and TF-IDF) across three model types (gpt2-base, gpt2-large, deberta-base). The composition performance is evaluated using perplexity, and a meta-regression model is developed to predict optimal composition settings based on metadata from previous experiments.

## Key Results
- Ensemble combination of adapter parameters yields better perplexity than parameter averaging across most models and domain selections
- Simpler corpus-based scoring strategies (TF-IDF, Sentence Similarity) often outperform more complex model-based strategies (entropy, domain prior) for adapter selection and weighting
- Zero-shot adapter composition performance can be partially predicted using meta-regression based on adapter weights, number of adapters, combination strategy, scoring strategy, and evaluation domain

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Ensemble combination of adapter parameters yields better perplexity than parameter averaging across most models and domain selections.
- **Mechanism:** Output vector ensembling preserves the diversity of individual adapter predictions by combining their full probability distributions, whereas parameter averaging collapses these into a single averaged model that may lose nuanced domain distinctions.
- **Core assumption:** The diversity of domain-specific adapters contributes positively to generalization when combined at the output level rather than the parameter level.
- **Evidence anchors:**
  - [abstract] "Our results highlight the efficacy of ensembling but also hint at the power of simple though often-ignored weighting methods."
  - [section 3.2] "From k = 2 on, ensembling leads to better domain adaptation across most model types and scoring strategies, indicated by lower model perplexities."
  - [corpus] Weak: Related work shows similar ensemble strategies but not directly compared to parameter averaging in zero-shot domain adaptation context.
- **Break condition:** If adapter parameters are highly redundant or if the number of adapters becomes too large, ensembling may no longer outperform averaging due to computational cost or noise amplification.

### Mechanism 2
- **Claim:** Simpler corpus-based scoring strategies (TF-IDF, Sentence Similarity) often outperform more complex model-based strategies (entropy, domain prior) for adapter selection and weighting.
- **Mechanism:** Corpus-based methods directly measure lexical and semantic overlap between training and evaluation domains, capturing domain similarity without requiring computationally expensive model inference or uncertainty estimation.
- **Core assumption:** Domain similarity at the corpus level is a strong predictor of adapter usefulness for zero-shot adaptation.
- **Evidence anchors:**
  - [section 3.2] "Surprising[ly], we observe that simpler (and previously ignored) approaches to determine the weighting, e.g., SENT SIM and TFâ€“IDF, often lead to better results compared to more sophisticated approaches."
  - [section 3.2] "However, for smaller numbers of adapters, the picture can vary" indicating context dependency.
  - [corpus] Weak: Related papers mention model-based scoring but no direct comparison to TF-IDF or sentence similarity in this zero-shot composition setting.
- **Break condition:** When domain similarity is not primarily lexical or semantic (e.g., stylistic or structural differences), corpus-based methods may fail and model-based approaches could perform better.

### Mechanism 3
- **Claim:** Zero-shot adapter composition performance can be partially predicted using meta-regression based on adapter weights, number of adapters, combination strategy, scoring strategy, and evaluation domain.
- **Mechanism:** The relationship between these features and final perplexity is learnable because adapter composition effectiveness depends systematically on domain similarity, adapter diversity, and combination method.
- **Core assumption:** The metadata from previous composition experiments contains enough signal to predict new compositions without retraining.
- **Evidence anchors:**
  - [abstract] "Further in-depth analyses allow us to understand the role of weighting vs. top-k selection, and show that, to a certain extent, the performance of adapter composition can even be predicted."
  - [section 4] "Both models surpass the baseline... The highest scores are achieved with Ridge regression on the gpt2-base results (0.9641 Spearman)."
  - [corpus] Weak: No direct evidence in corpus neighbors; related work focuses on adapter composition but not on meta-prediction.
- **Break condition:** If new domains or models differ substantially from training data distribution, or if composition strategies change significantly, meta-regression predictions may degrade.

## Foundational Learning

- **Concept:** Adapter layers and parameter-efficient fine-tuning
  - Why needed here: The paper's framework is built on inserting lightweight adapter modules into pre-trained models for domain adaptation; understanding how adapters work is essential to grasp composition strategies.
  - Quick check question: What is the key advantage of using adapter layers instead of full model fine-tuning in terms of parameter efficiency?

- **Concept:** Zero-shot domain adaptation
  - Why needed here: The framework aims to compose adapters without access to labeled data from the target domain; understanding zero-shot learning is critical for interpreting results.
  - Quick check question: How does zero-shot domain adaptation differ from few-shot or supervised domain adaptation in terms of available data?

- **Concept:** Perplexity as a language modeling evaluation metric
  - Why needed here: All experiments measure adapter composition effectiveness using perplexity on held-out data; knowing how perplexity reflects model quality is important.
  - Quick check question: What does a lower perplexity value indicate about a language model's predictions on a given dataset?

## Architecture Onboarding

- **Component map:**
  Pre-trained models (gpt2-base, gpt2-large, deberta-base) -> Adapter modules trained on 21 source domains -> Scoring strategies (uniform, sentence similarity, TF-IDF, domain prior, entropy) -> Combination strategies (parameter averaging, output ensembling) -> Evaluation datasets (10 target domains) -> Meta-regression pipeline for prediction

- **Critical path:**
  1. Train adapters on source domains
  2. For each target domain, compute scores using chosen strategy
  3. Select top-k adapters based on scores
  4. Apply weights and combine adapters using chosen method
  5. Evaluate perplexity on target domain
  6. (Optional) Use meta-regression to predict optimal settings

- **Design tradeoffs:**
  - Ensembling vs averaging: Ensembling better performance but higher computational cost; averaging cheaper but potentially worse.
  - Corpus-based vs model-based scoring: Corpus-based cheaper and often better, but may miss non-lexical domain differences.
  - Number of adapters (k): More adapters can add diversity but also noise; optimal k varies by model and domain.

- **Failure signatures:**
  - Perplexity increases with more adapters (overfitting or noise)
  - Model-based scoring strategies underperform corpus-based ones unexpectedly
  - Meta-regression predictions deviate significantly from actual results

- **First 3 experiments:**
  1. Run adapter composition with k=1 using all scoring strategies and both combination methods to establish baseline performance.
  2. Compare corpus-based (TF-IDF, Sentence Similarity) vs model-based (Entropy, Domain Prior) scoring with k=5 to observe strategy effectiveness.
  3. Perform meta-regression using results from first two experiments to predict performance of a held-out composition setting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the results change when applying the unified framework to a wider variety of NLP tasks beyond domain adaptation, such as sentiment analysis or named entity recognition?
- Basis in paper: [inferred] The authors mention that their framework is generic and applicable to various composition scenarios, but they only tested it on domain adaptation with adapter layers.
- Why unresolved: The effectiveness and efficiency of the framework for other NLP tasks and modularization techniques are unknown.
- What evidence would resolve it: Conducting experiments applying the framework to other NLP tasks and modularization techniques, and comparing the results to task-specific baselines.

### Open Question 2
- Question: Can the optimal number of adapters to select be determined more effectively than through manual tuning of the k parameter, such as by using an automatic selection method?
- Basis in paper: [explicit] The authors mention that selecting a good number of adapters is crucial for optimal performance and propose an early stopping approach as a preliminary solution.
- Why unresolved: The early stopping method has limitations and the optimal automatic selection approach is unclear.
- What evidence would resolve it: Developing and evaluating more sophisticated automatic adapter selection methods and comparing their performance to manual k tuning.

### Open Question 3
- Question: How do the performance and efficiency of the unified framework scale with the size of the language model and the number of available adapters?
- Basis in paper: [inferred] The authors tested the framework on three different model sizes and 21 training domains, but the scalability to larger models and more adapters is unknown.
- Why unresolved: The computational resources and memory requirements for larger models and more adapters are not addressed.
- What evidence would resolve it: Conducting experiments with larger language models and a greater number of adapters, measuring the performance and efficiency metrics, and analyzing the scaling behavior.

## Limitations

- Computational cost of ensembling (especially with large k values) is not fully characterized beyond basic CO2 measurements, making it difficult to assess practical deployment tradeoffs
- Meta-regression model shows promising predictive power but is only validated on the same experimental setup it was trained on, raising questions about generalization to truly novel domains or models
- Study focuses exclusively on language modeling tasks with perplexity as the evaluation metric, limiting conclusions about applicability to other NLP tasks or modalities

## Confidence

- **High Confidence:** Ensemble combination outperforms parameter averaging for adapter composition
- **Medium Confidence:** Simpler corpus-based scoring strategies outperform more complex model-based approaches
- **Low Confidence:** Meta-regression can predict adapter composition performance with reasonable accuracy

## Next Checks

1. **Compute Cost Validation:** Implement timing measurements for ensembling vs. averaging across different k values to quantify the computational tradeoff between performance gains and efficiency costs, particularly for deployment scenarios.

2. **Cross-Domain Meta-Regression:** Test the meta-regression model on a held-out set of domains not seen during training to verify whether performance predictions generalize beyond the experimental dataset used in the paper.

3. **Task Generalization Study:** Evaluate the same adapter composition framework on non-language modeling tasks (e.g., text classification or question answering) to assess whether the observed patterns in scoring strategy effectiveness and combination method preferences hold across different NLP task types.