---
ver: rpa2
title: Towards modeling evolving longitudinal health trajectories with a transformer-based
  deep learning model
arxiv_id: '2412.08873'
source_url: https://arxiv.org/abs/2412.08873
tags:
- health
- trajectories
- data
- time
- forecast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a transformer-based model for predicting the
  onset of common diseases using longitudinal health data. The model uses causal attention
  masking and a modified training objective to generate continuous predictions at
  each time point in a patient's health trajectory, rather than just a single prediction
  at the end.
---

# Towards modeling evolving longitudinal health trajectories with a transformer-based deep learning model

## Quick Facts
- arXiv ID: 2412.08873
- Source URL: https://arxiv.org/abs/2412.08873
- Reference count: 40
- This paper presents a transformer-based model for predicting the onset of common diseases using longitudinal health data with continuous predictions at each time point.

## Executive Summary
This paper introduces Evolve, a transformer-based model that generates continuous disease predictions throughout a patient's health trajectory rather than just at the end. The model uses causal attention masking to ensure predictions at each time step are based only on past events, enabling analysis of how health trajectories evolve over time. Experiments on a large Finnish health dataset show the model performs comparably to bidirectional transformers and XGBoost while offering unique trajectory modeling capabilities.

## Method Summary
The Evolve model uses a transformer encoder with causal attention masking to predict disease onset at each time point in a patient's historical health data. It employs Position-Weighted Mean Pooling to create age embeddings that emphasize recent events, and uses cosine similarity between embeddings to detect trajectory changes. The model is trained on a nationwide Finnish health dataset with clinical codes, procedures, and drug purchases, and evaluated using AUROC, AUPRC, and Recall@4 metrics.

## Key Results
- Evolve model performs comparably to bidirectional transformers and XGBoost for disease prediction tasks
- Causal attention masking enables trajectory analysis by preventing future information leakage during prediction
- Neighborhood analysis in embedding space can detect significant health trajectory changes and identify similar health profiles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The causal attention mask in the Evolve model enables trajectory modeling by conditioning each prediction only on past events.
- Mechanism: By applying a causal attention mask, the model predicts at each time step t using only inputs from positions 1 through t, preventing information leakage from future events. This mirrors autoregressive prediction and preserves temporal order.
- Core assumption: The order of medical events in the historical interval contains predictive information about future diagnoses, and masking prevents "cheating" by peeking ahead.
- Evidence anchors:
  - [abstract]: "This is achieved by modifying the training objective and by applying a causal attention mask."
  - [section]: "Attention masking makes the prediction ŷn(t) conditioned only on xn(1 : t)."
  - [corpus]: Weak corpus evidence for this specific masking technique in EHR settings.
- Break condition: If the temporal order of events is not predictive (e.g., unordered code lists), the causal mask provides no advantage over bidirectional approaches.

### Mechanism 2
- Claim: Position-weighted mean pooling (PWMPooling) creates stable age embeddings by emphasizing recent health events.
- Mechanism: At each age a, embeddings from all events occurring at that age are combined with weights 1, 2, 3, ..., m, where m is the number of events at that age. This biases the embedding toward the most recent event at that age.
- Core assumption: Recent events at a given age are more informative for characterizing that age's health state than older events at the same age.
- Evidence anchors:
  - [section]: "Weights are 1, 2, 3, ... m, where m is the number of inputs at age a. Most recent embedding gets the largest weight."
  - [corpus]: Weak corpus evidence; this specific weighting scheme is not common in EHR literature.
- Break condition: If event timing within an age is not predictive (e.g., all events at age 45 are equally informative), the weighting adds unnecessary complexity.

### Mechanism 3
- Claim: Comparing cosine similarities between target and reference individuals in the embedding space reveals shifts in health trajectory patterns.
- Mechanism: The model calculates age-wise similarities between a target individual's embeddings and those of k nearest neighbors. Changes in these similarities indicate shifts in health trajectory patterns relative to the reference population.
- Core assumption: Individuals with similar health outcomes will have similar embedding representations at corresponding ages, and changes in neighborhood composition reflect meaningful changes in health status.
- Evidence anchors:
  - [section]: "We iteratively calculate the fraction of changes among reference individuals that are most similar to the target individual, from one age (a − 1) to the next (a)."
  - [section]: "The rate of change at age a with neighborhood size k is thus: rn(a, k) = 1 − |Nn(a − 1, k) ∩ Nn(a, k)| / k"
  - [corpus]: Weak corpus evidence; this specific neighborhood change approach is not widely documented in EHR literature.
- Break condition: If the embedding space does not meaningfully capture health trajectories (e.g., embeddings are random), neighborhood similarity changes will not correlate with actual health changes.

## Foundational Learning

- Concept: Transformer architecture with self-attention
  - Why needed here: Transformers can capture long-range dependencies in sequential health data better than RNNs, which is critical for modeling complex health trajectories
  - Quick check question: What is the key difference between transformer self-attention and RNN recurrence in handling sequential data?

- Concept: Causal attention masking
  - Why needed here: Prevents the model from using future information when making predictions at each time step, enabling genuine trajectory modeling
  - Quick check question: How does causal masking differ from bidirectional attention in terms of information flow during prediction?

- Concept: Multi-label classification with sigmoid activation
  - Why needed here: Each individual can have multiple diagnoses in the forecast interval, requiring independent probability predictions for each class
  - Quick check question: Why is sigmoid activation used instead of softmax for this multi-label prediction task?

## Architecture Onboarding

- Component map:
  Input embeddings (codes, ages, positions, years-to-forecast) -> Causal attention layers -> Output embeddings -> Sigmoid predictions -> Binary cross-entropy loss

- Critical path:
  Historical interval codes → Input embeddings → Causal attention layers → Output embeddings → Sigmoid predictions → Binary cross-entropy loss

- Design tradeoffs:
  - Causal masking enables trajectory analysis but may reduce predictive performance compared to bidirectional models
  - PWMPooling emphasizes recent events but may underweight important earlier events at the same age
  - Position information is crucial for temporal modeling but adds complexity to the embedding space

- Failure signatures:
  - If the model predicts the same probabilities at all time steps, the causal mask or training objective may be incorrectly implemented
  - If neighborhood similarity changes are random, the embedding space may not capture meaningful health trajectory information
  - If prediction performance is significantly worse than bidirectional models, the causal masking may be too restrictive

- First 3 experiments:
  1. Compare prediction performance of the Evolve model with and without causal masking to verify the impact of trajectory modeling capability
  2. Test different weighting schemes in PWMPooling (uniform vs position-weighted) to determine optimal embedding aggregation
  3. Analyze neighborhood change patterns in synthetic data with known trajectory shifts to validate the neighborhood analysis approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Evolve model's performance compare to state-of-the-art transformer-based models when pre-trained on large unlabeled health datasets before fine-tuning on specific prediction tasks?
- Basis in paper: [inferred] The authors acknowledge that pre-training transformer models has been widely successful, especially in NLP, but did not explore it due to their data being fully labeled.
- Why unresolved: The paper only reports results from training the model directly on the labeled dataset without any pre-training phase.
- What evidence would resolve it: Comparative experiments showing AUROC/AUPRC scores of the Evolve model with and without pre-training on unlabeled EHR data.

### Open Question 2
- Question: What is the optimal neighborhood size (k) for calculating trajectory similarity changes, and how sensitive are the results to this parameter?
- Basis in paper: [explicit] The authors use k=1000 for their mother-child death analysis but acknowledge this is a parameter choice without systematic exploration.
- Why unresolved: The paper only uses one fixed value for k without exploring sensitivity or optimization.
- What evidence would resolve it: Systematic experiments varying k values and showing stability/robustness of trajectory change detection across different neighborhood sizes.

### Open Question 3
- Question: How do the model's continuous predictions at each time point compare to clinical assessments of disease onset timing in real patient records?
- Basis in paper: [inferred] The model generates sigmoid probabilities at each time point but the paper doesn't validate these against ground truth clinical documentation of when diseases actually began.
- Why unresolved: The paper evaluates only final predictions in the forecast interval, not the intermediate predictions' alignment with clinical reality.
- What evidence would resolve it: Validation studies comparing model-predicted onset times against clinical notes and physician assessments of when conditions first appeared.

## Limitations
- The model performs comparably to bidirectional approaches rather than demonstrating clear superiority, suggesting trajectory modeling benefits may be offset by information constraints
- The specific Position-Weighted Mean Pooling technique lacks validation against simpler alternatives, and the neighborhood analysis methodology is not benchmarked against established change detection methods
- Claims about clinical utility and practical deployment readiness are not supported by real-world validation studies or user feedback

## Confidence
- **High Confidence**: The transformer architecture implementation, causal attention masking mechanism, and multi-label classification setup are technically correct and well-documented
- **Medium Confidence**: The trajectory analysis methods and health event detection approaches are novel but lack extensive validation on synthetic or controlled datasets
- **Low Confidence**: Claims about clinical utility and practical deployment readiness are not supported by real-world validation studies or user feedback

## Next Checks
1. Conduct ablation studies comparing the Evolve model with and without causal masking on synthetic data where ground truth trajectory changes are known, to quantify the actual benefit of trajectory modeling
2. Validate the neighborhood analysis approach by applying it to controlled datasets with artificially induced health trajectory shifts, measuring detection accuracy and false positive rates
3. Compare PWMPooling against simpler aggregation methods (uniform weighting, mean pooling) on the same task to determine if the added complexity provides measurable benefits