---
ver: rpa2
title: 'ACING: Actor-Critic for Instruction Learning in Black-Box LLMs'
arxiv_id: '2411.12736'
source_url: https://arxiv.org/abs/2411.12736
tags:
- word
- tasks
- input
- acing
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ACING, a method for optimizing instructions
  for black-box large language models (LLMs) using actor-critic reinforcement learning.
  The key idea is to treat instruction optimization as a stateless, continuous-action
  problem, enabling exploration of infinite instruction spaces using only black-box
  feedback.
---

# ACING: Actor-Critic for Instruction Learning in Black-Box LLMs

## Quick Facts
- arXiv ID: 2411.12736
- Source URL: https://arxiv.org/abs/2411.12736
- Reference count: 40
- This paper proposes ACING, a method for optimizing instructions for black-box large language models (LLMs) using actor-critic reinforcement learning.

## Executive Summary
ACING addresses the challenge of optimizing instructions for black-box LLMs by formulating it as a stateless, continuous-action reinforcement learning problem. The method uses actor-critic algorithms to explore infinite instruction spaces through a white-box LLM transformation, enabling gradient-free optimization without predefined prompt candidates. Across 33 diverse tasks spanning instruction-induction, summarization, and chain-of-thought reasoning, ACING consistently outperforms baseline methods and human-crafted expert instructions, achieving median score improvements of 10 percentage points.

## Method Summary
ACING frames instruction optimization as an agent navigating an infinite action space within a d'-dimensional unit hypercube, where actions represent soft prompts transformed into discrete instructions via a white-box LLM. The actor-critic framework learns to optimize these continuous prompts through black-box feedback, with entropy maximization promoting exploration. The method employs a fixed budget of 165 black-box LLM calls and uses projection matrices to reduce the high-dimensional soft prompt space to an intrinsic dimension, making the optimization tractable.

## Key Results
- ACING achieves a median score improvement of 10 percentage points over baseline methods
- Surpasses human-crafted expert instructions by up to 39 percentage points on 33 tasks
- Demonstrates robustness with only a few black-box LLM calls needed for optimal instruction discovery on many tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Actor-critic framework enables continuous exploration in infinite instruction space
- Mechanism: The actor network samples stochastic actions from a learned Gaussian distribution, while the critic evaluates these actions using estimated Q-values. This allows the agent to navigate a continuous action space [0,1]^d' without requiring predefined prompt candidates.
- Core assumption: The black-box LLM's response to instruction variations can be modeled as a stateless, stochastic bandit problem where actions generate rewards independently of history.
- Evidence anchors:
  - [abstract]: "formulates instruction optimization as a stateless, continuous-action problem, enabling exploration of infinite instruction spaces using only black-box feedback"
  - [section 3.1]: "frame the prompt-learning problem as an agent navigating an infinite action space within a d′-dimensional unit hypercube"
- Break condition: If the black-box LLM exhibits state-dependent behavior or the reward distributions are not independent across actions, the stateless bandit assumption fails.

### Mechanism 2
- Claim: Entropy maximization balances exploration-exploitation trade-off in best-arm identification
- Mechanism: The actor-critic objective includes an entropy term that encourages stochastic policies, preventing premature convergence to suboptimal prompts. The temperature parameter α dynamically adjusts exploration level based on target entropy.
- Core assumption: Pure exploration is optimal for best-arm identification problems, but some exploitation helps guide search toward promising regions of instruction space.
- Evidence anchors:
  - [section 3.3]: "we consider the maximum entropy objective, as discussed in previous RL works... This objective promotes stochastic policies by augmenting the reward with an entropy term"
  - [section 3.3]: "entropy maximization is crucial for encouraging the actor to explore different actions"
- Break condition: If the reward landscape is highly multimodal with sharp peaks, entropy maximization may prevent convergence to the optimal instruction.

### Mechanism 3
- Claim: White-box LLM transformation enables gradient-free optimization in discrete instruction space
- Mechanism: The white-box LLM converts continuous soft prompts (high-dimensional vectors) into discrete instructions through its embedding and generation layers, bridging the continuous optimization space with the discrete instruction space required by black-box LLMs.
- Core assumption: The white-box LLM can reliably transform arbitrary continuous vectors into meaningful discrete instructions that the black-box LLM can process effectively.
- Evidence anchors:
  - [section 2.2]: "we employ a white-box LLM, represented by h, to convert the discrete optimization problem into a continuous one by introducing a soft prompt vector z"
  - [section 4]: "The white-box LLM produces a discrete prompt, τ, which is evaluated using the validation dataset V based on the responses from the black-box LLM f"
- Break condition: If the white-box LLM fails to generate coherent instructions from certain regions of the continuous space, the transformation breaks down.

## Foundational Learning

- Concept: Reinforcement Learning basics (actor-critic methods)
  - Why needed here: ACING is fundamentally an actor-critic algorithm that learns to optimize instructions through trial-and-error interaction with the black-box LLM environment.
  - Quick check question: What distinguishes actor-critic methods from pure policy gradient or value-based approaches?

- Concept: Multi-armed bandit problems and exploration-exploitation tradeoff
  - Why needed here: The instruction optimization problem is framed as a stateless continuum bandit, requiring understanding of how to balance exploration of new instructions versus exploitation of known good ones.
  - Quick check question: How does the entropy regularization in ACING modify the standard exploration-exploitation tradeoff?

- Concept: Neural network function approximation
  - Why needed here: Both actor and critic networks are parameterized neural networks that approximate the policy and value functions, respectively.
  - Quick check question: Why are two separate networks (actor and critic) needed instead of a single network approach?

## Architecture Onboarding

- Component map: Actor network → Projection matrix P → White-box LLM → Black-box LLM → Score → Critic network update → Actor network update
- Critical path: Actor → Projection → White-box LLM → Black-box LLM → Score → Critic update → Actor update (repeat)
- Design tradeoffs:
  - Fixed vs. learned projection matrix: Fixed matrix simplifies implementation but may not be optimal for all tasks
  - Budget allocation: Fixed budget T=165 ensures fairness but may not be optimal for all task complexities
  - Single vs. multiple exemplars: More exemplars provide better task context but increase white-box LLM input complexity
- Failure signatures:
  - Actor network collapses to deterministic policy → insufficient exploration
  - Critic network provides noisy or inconsistent Q-value estimates → poor policy updates
  - White-box LLM generates nonsensical instructions → broken transformation pipeline
  - Black-box LLM returns identical scores for diverse instructions → task too easy or metric insensitive
- First 3 experiments:
  1. Verify actor-critic learning on a simple synthetic bandit task with known optimal action
  2. Test white-box LLM transformation with fixed random soft prompts to ensure basic functionality
  3. Run ACING on a single instruction-induction task with reduced budget to observe learning dynamics

## Foundational Learning

- Concept: Continuous control vs. discrete action spaces in RL
  - Why needed here: ACING operates in a continuous action space, unlike most traditional RL methods that work with discrete actions, requiring different policy representation and optimization techniques.
  - Quick check question: How does the Gaussian policy parameterization enable exploration in continuous action spaces?

- Concept: Stochastic vs. deterministic policies in bandit settings
  - Why needed here: The stateless bandit formulation allows for pure exploration strategies, but ACING uses stochastic policies with entropy regularization to balance exploration and exploitation.
  - Quick check question: What would happen if ACING used a purely deterministic policy in the continuous action space?

- Concept: Transfer learning and model-based vs. model-free approaches
  - Why needed here: ACING uses a white-box LLM as a transformation model to enable model-free optimization of black-box LLMs, leveraging transfer learning capabilities.
  - Quick check question: How does using a white-box LLM as an intermediary affect the optimization landscape compared to direct black-box optimization?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal intrinsic dimension d' for ACING across different task categories?
- Basis in paper: [inferred] from ablation study showing d' = 5, 10, 20, 40, 100 performance comparison
- Why unresolved: The paper shows varying performance across dimensions but doesn't establish optimal dimension per task category
- What evidence would resolve it: Task-specific optimal dimension findings through systematic cross-validation across all categories

### Open Question 2
- Question: How does the choice of white-box LLM affect ACING's performance on complex cognitive tasks?
- Basis in paper: [explicit] from comparison between Vicuna-13B and WizardLM-13B results
- Why unresolved: While differences are shown, the paper doesn't analyze which white-box models perform best on specific cognitive task types
- What evidence would resolve it: Performance analysis of different white-box models on specific cognitive task subcategories

### Open Question 3
- Question: What is the relationship between the number of exemplars and performance on more complex instruction-induction tasks?
- Basis in paper: [explicit] from comparison between |E| = 1 and |E| = 5 performance
- Why unresolved: The paper shows general trends but doesn't establish specific exemplar requirements for different complexity levels
- What evidence would resolve it: Complexity-based exemplar number requirements through systematic testing across task difficulty levels

### Open Question 4
- Question: How does ACING's exploration strategy scale with different budget sizes beyond the fixed 165 calls?
- Basis in paper: [explicit] from budget splitting experiments and reward plots showing performance over calls
- Why unresolved: The paper only tests up to 165 calls and doesn't explore scaling behavior at larger budgets
- What evidence would resolve it: Performance scaling analysis across multiple budget sizes and task categories

### Open Question 5
- Question: What is the impact of entropy coefficient α tuning on ACING's exploration-exploitation balance across different task types?
- Basis in paper: [explicit] from entropy-based exploration discussion and dynamic α adjustment
- Why unresolved: The paper uses fixed entropy settings without exploring how different α values affect different task types
- What evidence would resolve it: Task-specific optimal α values and their impact on performance across different task categories

## Limitations

- The stateless bandit assumption may not hold for tasks requiring contextual information or exhibiting temporal dependencies in optimal instructions
- The method's performance depends on the quality and capabilities of the white-box LLM used for transformation
- Limited exploration of how different budget sizes affect performance scaling across task categories

## Confidence

- High confidence: The actor-critic framework for continuous action spaces is well-established and the experimental results show consistent improvements over baselines across multiple tasks
- Medium confidence: The stateless bandit assumption appears reasonable based on experimental results, but its general applicability across diverse task types requires further validation
- Low confidence: The transformation mechanism from continuous soft prompts to discrete instructions via white-box LLM is empirically validated but lacks theoretical guarantees about its effectiveness across all possible continuous action vectors

## Next Checks

1. Test ACING on a task with known temporal dependencies in optimal instructions to validate whether the stateless bandit assumption holds or breaks down in such scenarios.

2. Systematically vary the white-box LLM (e.g., different model sizes or architectures) to determine how robust the continuous-to-discrete transformation is to the choice of intermediary model.

3. Compare ACING's performance with and without the actor network on a subset of tasks to isolate whether the policy learning component or the continuous action space exploration is primarily driving performance improvements.