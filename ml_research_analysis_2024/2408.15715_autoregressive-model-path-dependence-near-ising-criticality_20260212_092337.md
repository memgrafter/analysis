---
ver: rpa2
title: Autoregressive model path dependence near Ising criticality
arxiv_id: '2408.15715'
source_url: https://arxiv.org/abs/2408.15715
tags:
- autoregressive
- https
- path
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores how the choice of 1D autoregressive sequence
  affects the performance of language models (RNNs and transformers) trained on higher-dimensional
  physical data. Using 2D Ising model spin configurations near criticality, the authors
  systematically compare different lattice-to-sequence mappings including zigzag,
  snake, Hilbert, and Morton curves.
---

# Autoregressive model path dependence near Ising criticality

## Quick Facts
- arXiv ID: 2408.15715
- Source URL: https://arxiv.org/abs/2408.15715
- Reference count: 40
- Primary result: Choice of 1D autoregressive sequence significantly impacts language model performance on higher-dimensional physical data

## Executive Summary
This work systematically investigates how different 1D autoregressive sequences affect language model performance on 2D Ising model spin configurations near criticality. The authors compare zigzag, snake, Hilbert, and Morton curve mappings, finding that paths with longer 1D segments (zigzag, snake) converge faster than locality-preserving curves (Hilbert, Morton), despite the latter's advantages in preserving 2D locality. Transformers learn correlations significantly faster per epoch than RNNs, though both architectures show similar path-dependent behavior. These findings highlight the importance of sequence ordering in autoregressive models and suggest that optimizing the autoregressive path could improve training efficiency for large language models applied to physical systems.

## Method Summary
The study uses Monte Carlo simulations to generate 2D Ising model spin configurations near criticality (L=4,8,16, β between 0.286-0.667, with 10^5 samples per size). Four autoregressive paths (zigzag, snake, Hilbert, Morton) map the 2D lattice to 1D sequences. Both RNNs (hidden size 16) and transformers (embedding 32, heads 4, FFNN 512, blocks 2) are trained on these sequences using ADAM optimizer (lr=1e-3). Performance is evaluated using negative log-likelihood loss and two-point spin-spin correlation function G(Δx,Δy) to measure correlation learning during training.

## Key Results
- Paths with long 1D segments (zigzag, snake) yield faster convergence than locality-preserving curves (Hilbert, Morton)
- Transformers learn correlations significantly faster per epoch than RNNs due to explicit long-range context encoding
- Initial anisotropy in learned correlations for zigzag and snake paths slows early training but doesn't prevent faster overall convergence
- RNNs and transformers show similar path-dependent behavior despite different architectural approaches to context handling

## Why This Works (Mechanism)

### Mechanism 1
The choice of path affects how correlations in the 2D system are represented in the 1D sequence, impacting how easily the model can learn these correlations. Paths with long 1D segments (zigzag, snake) allow faster convergence than locality-preserving curves (Hilbert, Morton), despite the latter's advantages in preserving 2D locality.

Core assumption: The mapping from 2D lattice to 1D sequence significantly impacts the model's ability to learn correlations.

### Mechanism 2
Transformers use a causally masked self-attention mechanism that directly encodes long-range correlations consistent with the autoregressive path. This explicit encoding allows transformers to access and retrieve past context more efficiently than RNNs, which compress and truncate context through multiple nonlinear functions.

Core assumption: The self-attention mechanism in transformers provides more efficient access to long-range context compared to RNNs' hidden state approach.

### Mechanism 3
The zigzag and snake paths introduce anisotropy in the correlations captured by the models during early-phase training. This anisotropy causes a plateau in training loss, but once resolved, the models quickly reach optimal loss. Despite this initial slowdown, models with zigzag and snake paths converge faster than locality-preserving paths for the Ising model.

Core assumption: The anisotropy introduced by certain paths is a temporary effect that doesn't prevent faster overall convergence.

## Foundational Learning

- Concept: Autoregressive models
  - Why needed here: Understanding how autoregressive models decompose joint probability distributions into conditional probabilities is crucial for grasping why the choice of sequence path matters.
  - Quick check question: How does an autoregressive model decompose a joint probability distribution into conditional probabilities?

- Concept: Ising model and criticality
  - Why needed here: The Ising model near criticality exhibits long-range correlations that decay as a power law, making it a challenging test case for autoregressive models.
  - Quick check question: What characterizes the behavior of the Ising model near its critical point?

- Concept: Sequence-to-lattice mapping
  - Why needed here: Different methods of mapping a 2D lattice to a 1D sequence (paths) have varying effects on model performance, which is central to the paper's findings.
  - Quick check question: How does the choice of path (e.g., zigzag, snake, Hilbert) affect the representation of 2D correlations in a 1D sequence?

## Architecture Onboarding

- Component map: Data generation -> Path mapping -> Model training -> Performance evaluation
- Critical path: 1) Generate Ising model data near criticality 2) Choose autoregressive path (zigzag, snake, Hilbert, or Morton) 3) Train model (RNN or transformer) on the data 4) Evaluate training performance and correlation learning
- Design tradeoffs:
  - Path choice: Long 1D segments (zigzag, snake) vs. locality preservation (Hilbert, Morton)
  - Model architecture: RNNs (linear scaling with sequence length) vs. transformers (quadratic scaling)
  - Training efficiency: Faster per-epoch learning (transformers) vs. potentially better final performance (RNNs with optimal paths)
- Failure signatures:
  - Poor convergence with locality-preserving paths (Hilbert, Morton)
  - Initial plateau in training loss due to anisotropic correlation learning
  - Suboptimal performance when using paths not suited to the system's geometry
- First 3 experiments:
  1. Train a transformer on Ising model data using the zigzag path and observe the convergence behavior
  2. Repeat experiment 1 with the Hilbert path and compare performance
  3. Train an RNN on the same data using both paths and compare convergence rates and final performance

## Open Questions the Paper Calls Out

### Open Question 1
Does the superior performance of zigzag/snake paths over locality-preserving paths hold for other physical systems beyond the 2D Ising model, such as higher-dimensional systems or different Hamiltonians? The study is limited to 2D Ising model spin configurations, and the authors explicitly call for extending the analysis to other physical systems.

### Open Question 2
What is the optimal autoregressive path for large language models on physical systems, and can this path be learned rather than hand-designed? Current work uses fixed, hand-designed paths without optimization, and the potential for learning the optimal path is unexplored.

### Open Question 3
How does the choice of autoregressive path affect the computational efficiency and memory requirements of training large language models on physical data? The study focuses on convergence rates and correlation learning, but does not analyze the computational costs associated with different paths.

## Limitations
- Results are limited to 2D Ising model near criticality, limiting generalizability to other physical systems
- Only four specific path orderings were tested, without exploring intermediate options between pure 1D segments and space-filling curves
- The study doesn't systematically vary architectural parameters to determine sensitivity of path dependence to model scale

## Confidence

- High confidence: Transformers learn correlations faster per epoch than RNNs - supported by multiple convergence curves and consistent across all path types
- Medium confidence: Zigzag and snake paths yield faster convergence despite initial anisotropy - results show consistent patterns, but the mechanism for overcoming initial anisotropy plateaus could be further validated
- Low confidence: Generalizability to other physical systems - while the Ising model is a canonical system, extending these findings to other physical models would require additional validation

## Next Checks

1. **Cross-system validation**: Test the same path-dependent training behavior on the 3D Ising model and other spin systems (e.g., XY model) to assess generalizability of the convergence patterns observed

2. **Intermediate path analysis**: Systematically explore path orderings between pure 1D segments and space-filling curves (e.g., mixed Hilbert-zigzag patterns) to identify optimal tradeoffs between locality preservation and training efficiency

3. **Architectural sensitivity analysis**: Vary transformer architecture parameters (attention heads, feed-forward network size, number of layers) and RNN hidden state sizes to determine if the observed path dependence holds across different model scales and configurations