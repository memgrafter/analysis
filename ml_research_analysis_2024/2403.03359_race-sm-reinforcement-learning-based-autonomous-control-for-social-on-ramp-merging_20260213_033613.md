---
ver: rpa2
title: 'RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp
  Merging'
arxiv_id: '2403.03359'
source_url: https://arxiv.org/abs/2403.03359
tags:
- vehicle
- merging
- function
- learning
- vehicles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of autonomous parallel-style on-ramp
  merging in human-controlled traffic. It proposes a novel deep reinforcement learning
  (DRL) approach that explicitly considers the utility to both the ego vehicle and
  surrounding vehicles to produce socially acceptable behavior.
---

# RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp Merging

## Quick Facts
- arXiv ID: 2403.03359
- Source URL: https://arxiv.org/abs/2403.03359
- Reference count: 40
- The paper proposes a novel deep reinforcement learning approach using Social Value Orientation (SVO) to enable socially courteous autonomous on-ramp merging with reduced collisions.

## Executive Summary
This paper addresses the challenge of autonomous on-ramp merging in human-controlled traffic by proposing a deep reinforcement learning approach that explicitly considers both ego vehicle and surrounding vehicle utilities. The key innovation is a reward function based on Social Value Orientation (SVO) that weights the agent's level of social cooperation. The model is trained in a simulated two-lane highway environment and demonstrates that SVO-based weighting can produce socially acceptable behavior while matching or surpassing existing methods in collision rate reduction.

## Method Summary
The method employs a deep reinforcement learning agent trained using Proximal Policy Optimization (PPO) in the SUMO simulator. The state space captures surrounding vehicle positions, velocities, and gaps, while the action space consists of acceleration and lane change decisions. The novel aspect is the SVO-based reward function that combines ego vehicle utility (maximizing velocity while maintaining safe gaps) with surrounding vehicle utility (preferring larger gaps and center merging) weighted by the SVO parameter φ. The model is trained across 20 parallel environments with randomized traffic patterns to ensure robust generalization.

## Key Results
- The SVO-based reward function enables socially courteous merging behavior while maintaining competitive collision rates
- At SVO φ = π/4, the model achieves the best balance between social acceptability and efficiency with minimal conflicts and no collisions
- The approach outperforms baseline DQN methods in social metrics (near misses, gap centrality) while matching performance in collision avoidance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SVO-based reward weighting enables socially courteous merging by explicitly considering surrounding vehicle utility.
- Mechanism: The reward function combines ego vehicle utility and surrounding vehicle utility weighted by SVO (cos(φ) for ego, sin(φ) for SV), directly embedding social preferences into learning.
- Core assumption: Drivers can be modeled as having utility functions that balance their own speed progress with surrounding vehicle safety and comfort.
- Evidence anchors:
  - [abstract] "explicitly considers the utility to both the ego vehicle and surrounding vehicles which may be cooperative or uncooperative to produce behaviour that is socially acceptable"
  - [section] "The reward function is defined with the concept of SVO from social psychology in mind... Equation (9) defines the utility to the ego vehicle in merging and assumes that the ego vehicle wishes to have a high velocity but a velocity no higher than its leading vehicle... Equation (10) defines the utility to the SVs in merging and assumes that the SVs prefer the ego vehicle to merge into larger gaps over smaller ones, into the centre of a gap as opposed to at the edge and at a speed no less than that of the immediately trailing vehicle."
- Break condition: If surrounding vehicles have utility functions that deviate significantly from the assumed form (e.g., aggressive drivers prioritize speed over safety), the learned policy may misalign with real-world expectations.

### Mechanism 2
- Claim: PPO training in parallel simulation environments with varied traffic densities ensures robust generalization.
- Mechanism: The model is trained using 20 parallel SUMO environments with random seeds controlling vehicle spawning, allowing exposure to stochastic traffic patterns and preventing overfitting to a single flow.
- Core assumption: Simulated stochasticity with randomized vehicle inflows and speeds captures sufficient real-world variability for policy generalization.
- Evidence anchors:
  - [section] "A new random seed was used every time the simulator was reset due to a collision or timeout. The random seed controls the timing of vehicle spawning and thus this prevented the model from being overfitted to any one vehicle flow pattern."
  - [section] "During training the probability of a vehicle entering the left or right lane each second is 0.1 and 0.3 respectively. This results in approximate inflows of 360 vehicles/hr in the left lane and 1080 vehicles/hr in the right lane."
- Break condition: If the simulation's vehicle interaction model (IDM) does not capture real human driver variability, the policy may fail in deployment.

### Mechanism 3
- Claim: SVO parameter tuning (φ = π/4) balances ego and SV utility, minimizing conflicts while achieving efficient merging.
- Mechanism: At φ = π/4, the reward equally weights ego and SV utility, leading to a policy that merges at near-network speed while maintaining safe head/tail gaps and avoiding hard braking.
- Core assumption: A linear combination of ego and SV utility weighted by SVO angle accurately represents real human driver tradeoffs between self-interest and social cooperation.
- Evidence anchors:
  - [section] "At an SVO of π/4, the model seeks to maximize utility to both ego and SVs... It presents the fewest conflicts and no collisions while also attaining acceptable values in all other categories."
  - [section] "Adjusting the SVO varies the degree to which the agent prioritises the utility to other road users."
- Break condition: If real human drivers do not exhibit such balanced tradeoffs (e.g., consistently more egoistic or altruistic), the π/4 policy may be suboptimal.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: On-ramp merging is modeled as an MDP where states are observable road configurations, actions are acceleration/lane changes, and rewards encode merging success and social acceptability.
  - Quick check question: In this paper, what are the state variables used to represent the merging environment?

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: PPO is used to train the agent because it handles continuous state spaces, provides stable learning via clipped objective functions, and is effective in complex decision-making tasks like merging.
  - Quick check question: What are the two neural networks used in the PPO algorithm, and what do they approximate?

- Concept: Social Value Orientation (SVO)
  - Why needed here: SVO provides a quantitative framework to embed social preferences into the reward function, allowing the agent to learn varying degrees of cooperation or egoism.
  - Quick check question: In the SVO ring model, what geometric property determines the balance between self and other utility?

## Architecture Onboarding

- Component map: SUMO simulation environment -> OpenAI Gym wrapper -> PPO training loop (actor/critic networks) -> Parallel environments with random seeds -> Evaluation scripts for different SVO values and traffic densities
- Critical path: Environment reset -> State observation -> Action selection -> Environment step -> Reward computation (with SVO weighting) -> Buffer update -> PPO optimization -> Policy update
- Design tradeoffs: Discrete action space simplifies learning but limits fine-grained control; parallel environments speed training but require significant compute; SVO-based reward is interpretable but may not capture all real driver behaviors
- Failure signatures: High collision rate -> reward function or state space insufficient; poor generalization across densities -> overfitting to training conditions; unstable training -> learning rate or clipping range misconfigured
- First 3 experiments:
  1. Train with φ = 0 (ego-only) and φ = π/2 (SV-only) to confirm SVO effect
  2. Evaluate collision rate and conflict rate across easy/medium/hard traffic densities for φ = π/4
  3. Compare merging speed and gap centrality metrics against baseline DQN-trained model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed SVO-based reward function perform in more complex and varied traffic scenarios, such as multi-lane highways, roundabouts, or intersections?
- Basis in paper: [inferred] The paper mentions that the model is optimized for a specific road network and traffic conditions, and future work should consider more varied scenarios.
- Why unresolved: The current study focuses on a specific road network and traffic conditions, limiting the generalizability of the results.
- What evidence would resolve it: Testing the SVO-based reward function in a wider range of traffic scenarios and road networks, and comparing its performance to existing methods.

### Open Question 2
- Question: How does the accuracy of the model and the road network simulation affect the real-world applicability of the proposed approach?
- Basis in paper: [explicit] The paper acknowledges that the accuracy of the model and road network is limited by the SUMO simulator and its constraints, and higher fidelity simulators are available but have technical overheads.
- Why unresolved: The current study uses a simplified simulator, which may not fully capture the complexities of real-world driving scenarios.
- What evidence would resolve it: Evaluating the proposed approach using higher fidelity simulators or real-world data, and comparing its performance to existing methods.

### Open Question 3
- Question: How does the proposed approach handle sensor limitations and uncertainties in real-world driving scenarios?
- Basis in paper: [inferred] The paper mentions that the model assumes all vehicles within the network can be detected, and future work should consider sensing hardware restraints.
- Why unresolved: The current study assumes perfect sensing capabilities, which may not be realistic in real-world driving scenarios.
- What evidence would resolve it: Incorporating sensor limitations and uncertainties into the simulation environment and evaluating the proposed approach's performance under these conditions.

## Limitations

- Simulation-only evaluation without real-world testing limits generalizability to actual human driver behavior
- Assumes surrounding vehicles follow predictable utility functions that may not capture all human driver types
- Perfect sensing assumption ignores real-world sensor limitations and uncertainties

## Confidence

- Technical implementation of SVO-based reward and PPO training: High
- Generalizability to real-world scenarios: Medium
- Assumptions about surrounding vehicle utility functions: Medium
- Effectiveness of parallel environment training: Medium

## Next Checks

1. Implement real-world testing in controlled environments with human drivers to validate simulation results and assess policy robustness to real human behavior variability.

2. Conduct ablation studies varying the SVO parameter across a wider range (0 to π/2) to confirm the specific claim that φ = π/4 provides optimal social acceptability without sacrificing efficiency.

3. Test the policy's performance with different surrounding vehicle models in SUMO (e.g., Krauss, Wiedemann) to assess sensitivity to the assumed IDM parameters and verify that results are not artifacts of a specific simulation model.