---
ver: rpa2
title: 'MANTIS: Interleaved Multi-Image Instruction Tuning'
arxiv_id: '2405.01483'
source_url: https://arxiv.org/abs/2405.01483
tags:
- multi-image
- image
- images
- semanticscholar
- corpusid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Mantis, a family of large multimodal models
  designed to handle interleaved text-image inputs. Unlike existing models that rely
  on extensive pre-training, Mantis achieves state-of-the-art performance on multi-image
  tasks through instruction tuning on a carefully curated dataset, Mantis-Instruct.
---

# MANTIS: Interleaved Multi-Image Instruction Tuning

## Quick Facts
- **arXiv ID**: 2405.01483
- **Source URL**: https://arxiv.org/abs/2405.01483
- **Reference count**: 28
- **Key outcome**: Mantis models achieve state-of-the-art performance on multi-image tasks through instruction tuning on a curated dataset, outperforming baselines like Idefics2-8B by an average of 13 absolute points while maintaining strong single-image performance.

## Executive Summary
Mantis is a family of large multimodal models designed to handle interleaved text-image inputs and perform complex multi-image reasoning tasks. Unlike existing approaches that rely on extensive pre-training, Mantis achieves state-of-the-art performance through instruction tuning on a carefully curated dataset called Mantis-Instruct, containing 721K multi-image instruction data. The models demonstrate strong capabilities in co-reference, comparison, reasoning, and temporal understanding across multiple images while maintaining competitive performance on single-image tasks. This approach proves that multi-image reasoning abilities can be effectively acquired through low-cost instruction tuning rather than massive pre-training.

## Method Summary
Mantis employs instruction tuning on a curated dataset (Mantis-Instruct) containing 721K multi-image instruction data, covering skills like co-reference, comparison, reasoning, and temporal understanding. The models are built upon pre-trained language models (LLaMA-3, Fuyu) and vision transformer encoders (CLIP, SigLIP), using a text-image interleaving format to process interleaved inputs. Training combines Mantis-Instruct with 268K single-image vision-language data to balance multi-image and single-image abilities. Models are fine-tuned for 1 epoch with a batch size of 128, learning rate of 1e-5, and maximum context length of 8192, using academic-level computational resources.

## Key Results
- Mantis models outperform Idefics2-8B by an average of 13 absolute points on multi-image benchmarks despite being trained on 200x less data
- Mantis maintains strong single-image performance comparable to models like CogVLM and Emu2
- Models demonstrate stable performance across varying numbers of input images on MVBench, handling up to 128 images
- Instruction tuning approach proves effective for acquiring multi-image reasoning skills without extensive pre-training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-image reasoning abilities can be effectively acquired through instruction tuning rather than massive pre-training.
- Mechanism: The instruction tuning process on Mantis-Instruct, a curated dataset of 721K multi-image instruction data, equips the model with essential skills such as co-reference, comparison, reasoning, and temporal understanding. This targeted training approach allows the model to develop multi-image capabilities efficiently without requiring extensive pre-training on large-scale web corpora.
- Core assumption: High-quality, task-specific instruction data can substitute for massive pre-training in developing complex reasoning skills.
- Evidence anchors:
  - [abstract] "Mantis achieves state-of-the-art performance on multi-image tasks through instruction tuning on a carefully curated dataset, Mantis-Instruct."
  - [section 2.2] "Mantis-Instruct containing 721K multi-image instruction data to train a family of Mantis models. The instruction tuning empowers Mantis with different multi-image skills like co-reference, comparison, reasoning, and temporal understanding."
- Break condition: If the instruction data lacks diversity or is of poor quality, the model may not acquire the necessary skills, leading to suboptimal performance on multi-image tasks.

### Mechanism 2
- Claim: Mantis models maintain strong performance on single-image tasks while excelling in multi-image tasks.
- Mechanism: By including single-image reasoning datasets in the training process, Mantis models preserve their single-image abilities. This balanced approach ensures that the model does not lose proficiency in single-image tasks while gaining multi-image skills.
- Core assumption: Incorporating single-image data in the training process prevents catastrophic forgetting of single-image skills.
- Evidence anchors:
  - [abstract] "Mantis also maintains strong performance on single-image tasks, comparable to models like CogVLM and Emu2."
  - [section 3.5] "Mantis-SigLIP gets significant improvements on MMBench-English, MMMU, and ScienceQA benchmarks though we did not specifically optimize the single-image abilities."
- Break condition: If the balance between multi-image and single-image data is not properly maintained, the model may experience a decline in single-image performance.

### Mechanism 3
- Claim: The text-image interleaving format and architecture design are crucial for acquiring multi-image understanding and reasoning ability.
- Mechanism: The interleaving format "(image {i}: <BOI><image><EOI>) " clearly marks boundaries between images and denotes the serial number of images. This format, combined with the architecture that supports multi-image inputs, enables the model to effectively process and reason across multiple images.
- Core assumption: A well-designed text-image interleaving format and architecture are essential for multi-image reasoning.
- Evidence anchors:
  - [section 2.1] "We contend that a good text-image interleaving format should: (1) mark boundaries between images clearly, and (2) denote the serial number of images."
  - [section 2.1] "We designed our interleaving format as follows: "(image {i}: <BOI><image><EOI>) ", where <BOI> is the begin of image token and<EOI> is the end of image token.<image> is the placeholder for image patches."
- Break condition: If the interleaving format is ambiguous or the architecture does not support multi-image inputs, the model may struggle to understand and reason across multiple images.

## Foundational Learning

- Concept: Multimodal instruction tuning
  - Why needed here: To develop multi-image reasoning abilities without extensive pre-training.
  - Quick check question: What is the purpose of instruction tuning in the context of Mantis models?

- Concept: Text-image interleaving format
  - Why needed here: To clearly mark boundaries between images and denote the serial number of images, enabling effective multi-image reasoning.
  - Quick check question: Why is the text-image interleaving format crucial for Mantis models?

- Concept: Multi-image reasoning skills (co-reference, comparison, reasoning, temporal understanding)
  - Why needed here: To enable the model to effectively process and reason across multiple images in various tasks.
  - Quick check question: What are the four essential multi-image reasoning skills that Mantis models are designed to acquire?

## Architecture Onboarding

- Component map: LLaMA-3/Fuyu -> CLIP/SigLIP -> Multimodal Projector -> Text-Image Interleaving Format
- Critical path: Text-Image Interleaving Format -> Multimodal Projector -> Model Processing
- Design tradeoffs: Choice of language model and vision encoder affects performance and efficiency; interleaving format must balance clarity and processing efficiency
- Failure signatures: Poor multi-image performance may indicate issues with interleaving format, multimodal projector, or training data balance
- First 3 experiments:
  1. Evaluate performance on multi-image benchmarks (NLVR2, Q-Bench, Mantis-Eval, BLINK, MVBench)
  2. Assess single-image performance on TextVQA, VQA-v2, MMBench, MMMU
  3. Conduct ablation studies varying multi-image skill subsets in training data

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important areas remain unexplored:
- Optimal ratio of multi-image to single-image data in instruction tuning
- Scaling behavior with increasing number of input images
- Integration of temporal information from video data
- Impact of different vision encoder choices on task-specific performance

## Limitations

- The exact composition and quality control procedures of the Mantis-Instruct dataset are not fully detailed
- Limited analysis of model scaling effects beyond the 7B parameter version
- Specific tokenization and implementation details for the interleaving format remain underspecified
- Direct pre-training versus instruction tuning comparisons are not provided

## Confidence

- Claims about instruction tuning's effectiveness versus pre-training: Medium
- Claims about maintaining single-image performance while gaining multi-image abilities: Medium
- Claims regarding the necessity of the interleaving format: Medium
- Overall performance claims on benchmarks: High (supported by quantitative results)

## Next Checks

1. Conduct a controlled experiment comparing instruction tuning versus continued pre-training on the same multi-image data to directly assess the claimed efficiency advantage
2. Perform detailed ablation studies varying the interleaving format parameters and architecture choices to quantify their individual contributions to performance
3. Test model generalization on out-of-distribution multi-image scenarios not represented in the Mantis-Instruct training data to evaluate true multi-image reasoning capabilities beyond pattern matching