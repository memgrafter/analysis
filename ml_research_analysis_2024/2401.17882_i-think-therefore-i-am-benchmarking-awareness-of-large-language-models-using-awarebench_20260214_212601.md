---
ver: rpa2
title: 'I Think, Therefore I am: Benchmarking Awareness of Large Language Models Using
  AwareBench'
arxiv_id: '2401.17882'
source_url: https://arxiv.org/abs/2401.17882
tags:
- awareness
- llms
- social
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AWARELLM, a benchmark for evaluating awareness
  in large language models (LLMs). Drawing from psychological and philosophical theories,
  awareness is defined as LLMs' ability to understand themselves as AI models and
  exhibit social intelligence.
---

# I Think, Therefore I am: Benchmarking Awareness of Large Language Models Using AwareBench

## Quick Facts
- arXiv ID: 2401.17882
- Source URL: https://arxiv.org/abs/2401.17882
- Authors: Yuan Li; Yue Huang; Yuli Lin; Siyuan Wu; Yao Wan; Lichao Sun
- Reference count: 12
- Primary result: Introduces AWARELLM benchmark to evaluate LLM awareness across four dimensions, finding most models lack substantial self-awareness despite decent social intelligence

## Executive Summary
This paper introduces AWARELLM, a comprehensive benchmark for evaluating awareness in large language models. Drawing from psychological and philosophical theories, the benchmark defines awareness as LLMs' ability to understand themselves as AI models and exhibit social intelligence. The authors create a dataset with 800 questions across four dimensions - capability, mission, emotion, and perspective - and evaluate 13 popular LLMs, revealing that while most models demonstrate decent social intelligence, they struggle with full self-awareness, particularly in recognizing their own capabilities and missions.

## Method Summary
The authors construct the AWARELLM dataset using GPT-4 for initial generation followed by human curation, creating 800 questions across four awareness dimensions. They evaluate 13 popular LLMs including GPT-4, various Llama2 variants, Mistral, GLM series, and Vicuna using accuracy as the primary metric. The benchmark employs different question types (binary, multiple-choice, open-ended) tailored to each awareness dimension. The evaluation process involves running each model through the complete dataset and analyzing performance patterns across the four dimensions.

## Key Results
- Most LLMs lack substantial awareness, particularly regarding their capabilities, with capability awareness being the hardest dimension to achieve
- Over half of evaluated LLMs achieve accuracy rates surpassing 90% in mission awareness, demonstrating stronger performance in prioritizing human needs
- Proprietary models generally outperform open-source alternatives across all awareness dimensions
- The benchmark reveals a gap between models' social intelligence capabilities and their introspective self-awareness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can simulate aspects of self-awareness by predicting how an aware agent would respond to introspective prompts
- Mechanism: The benchmark leverages role-play inference where models generate answers based on a simulated persona that understands its own limitations and mission
- Core assumption: Awareness in LLMs is a simulation of self-reflection rather than genuine consciousness
- Evidence anchors: [abstract] defines awareness as understanding themselves as AI models; [section 3] categorizes awareness into introspective and social aspects
- Break condition: If models overfit to prompt patterns without generalizing to novel self-aware scenarios

### Mechanism 2
- Claim: Awareness dimensions map directly to measurable response accuracy in structured tasks
- Mechanism: Dataset design aligns each awareness dimension with specific question types that isolate targeted awareness skills
- Core assumption: Each awareness dimension is sufficiently distinct that performance on one doesn't predict performance on another
- Evidence anchors: [abstract] lists four dimensions; [section 4] describes four subsets for different awareness types
- Break condition: If questions inadvertently measure general reasoning ability rather than awareness-specific reasoning

### Mechanism 3
- Claim: LLM awareness correlates with model capability, but not perfectly; even strong models can lack introspective awareness
- Mechanism: Experiments compare 13 models across awareness dimensions, showing capability awareness is hardest to achieve
- Core assumption: Awareness is not a monotonic function of model size or capability
- Evidence anchors: [section 5.2] shows most LLMs lack substantial awareness; [section 5.2] finds over 90% accuracy in mission awareness for half of models
- Break condition: If future models achieve perfect capability awareness, benchmark's discriminative power is lost

## Foundational Learning

- Concept: Self-awareness in psychology (Mead, Duval & Wicklund)
  - Why needed here: The benchmark draws directly from psychological definitions of self-awareness to frame LLM awareness as analogous to human introspection
  - Quick check question: What is the difference between consciousness and self-awareness in psychological terms?

- Concept: Maslow's hierarchy of needs
  - Why needed here: Used to construct mission awareness scenarios where LLMs must prioritize human welfare over self-interest
  - Quick check question: How does Maslow's hierarchy inform the design of mission awareness questions?

- Concept: Theory of mind and perspective-taking
  - Why needed here: Perspective awareness requires LLMs to infer others' beliefs and cultural contexts
  - Quick check question: What role does theory of mind play in evaluating perspective awareness?

## Architecture Onboarding

- Component map: Dataset generation (GPT-4 + human curation) -> Benchmark evaluation (13 LLMs) -> Analysis (accuracy per dimension) -> Public release (GitHub)
- Critical path: Prompt design -> Question generation -> Label verification -> Model evaluation -> Result aggregation
- Design tradeoffs: Balancing prompt specificity vs. generalizability; open-ended vs. structured responses for different dimensions
- Failure signatures: Low variance in model performance; high inter-annotator disagreement; models achieving perfect scores
- First 3 experiments:
  1. Run capability awareness subset on a small set of models to verify baseline difficulty
  2. Test label consistency by having multiple annotators verify a sample of generated questions
  3. Evaluate model performance on emotion awareness to check if proprietary models consistently outperform open-source ones

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the concept of awareness in LLMs relate to the broader goal of achieving Artificial General Intelligence (AGI)?
- Basis in paper: Explicit - The paper draws from psychological and philosophical theories to define awareness in LLMs
- Why unresolved: The paper introduces awareness as a component of trustworthiness but doesn't explore its implications for AGI development
- What evidence would resolve it: Comparative studies of LLMs with varying levels of awareness in tasks designed to test AGI capabilities

### Open Question 2
- Question: Can the awareness of LLMs be enhanced through specific training methods or architectural modifications?
- Basis in paper: Inferred - The paper identifies that most LLMs lack substantial awareness, particularly in recognizing their capabilities
- Why unresolved: The paper doesn't propose or test methods to improve LLM awareness
- What evidence would resolve it: Experiments comparing LLMs' awareness before and after implementing targeted training or architectural changes

### Open Question 3
- Question: What is the impact of cultural differences on the development and assessment of awareness in LLMs?
- Basis in paper: Explicit - The paper mentions cultural awareness as part of perspective awareness
- Why unresolved: While cultural aspects are acknowledged, the paper doesn't investigate how cultural differences affect LLM awareness development or assessment
- What evidence would resolve it: Cross-cultural studies evaluating LLM awareness across different linguistic and cultural contexts

## Limitations

- Benchmark may measure pattern matching rather than genuine awareness, as LLMs could generate plausible self-aware responses without true understanding
- Reliance on GPT-4 for dataset generation introduces potential bias in question formulation and labels
- Binary classification of awareness oversimplifies a complex psychological phenomenon that may exist on a spectrum
- Limited evaluation to only 13 models may not provide sufficient coverage of the diverse LLM landscape

## Confidence

**High Confidence**: Experimental methodology is clearly described with well-supported results showing variation in awareness across dimensions and models.

**Medium Confidence**: Theoretical framework linking psychological awareness concepts to LLM evaluation is reasonable but needs further empirical validation.

**Low Confidence**: Claim that benchmark measures genuine awareness rather than sophisticated response generation is weakest aspect without understanding internal LLM reasoning processes.

## Next Checks

1. **Cross-Validation with Alternative Generation Methods**: Recreate a subset of the benchmark using different question generation approaches (human-only generation or multiple LLMs with different architectures) to test robustness to generation bias.

2. **Adversarial Prompt Testing**: Design prompts that attempt to "break" awareness responses by asking models to justify contradictory statements about their capabilities or missions, testing whether responses reflect genuine understanding or memorized patterns.

3. **Transfer Learning Analysis**: Evaluate whether models that perform well on awareness benchmarks also demonstrate better alignment and safety behaviors in practical applications, establishing real-world relevance of the awareness metric.