---
ver: rpa2
title: 'Performance of large language models in numerical vs. semantic medical knowledge:
  Benchmarking on evidence-based Q&As'
arxiv_id: '2406.03855'
source_url: https://arxiv.org/abs/2406.03855
tags:
- medical
- answers
- llms
- question
- median
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks two large language models (LLMs) on a dataset
  of evidence-based medical questions and answers (EBMQA), which contains 105,222
  multiple-choice questions derived from a knowledge graph. The EBMQA dataset is labeled
  with medical and non-medical topics and classified into numerical or semantic questions.
---

# Performance of large language models in numerical vs. semantic medical knowledge: Benchmarking on evidence-based Q&As

## Quick Facts
- **arXiv ID**: 2406.03855
- **Source URL**: https://arxiv.org/abs/2406.03855
- **Reference count**: 40
- **Primary result**: Both LLMs excel more in semantic than numerical Q&As, with Claude3 surpassing GPT4 in numerical QAs, though both remain inferior to humans.

## Executive Summary
This study benchmarks two large language models (GPT-4 and Claude-3 Opus) on a dataset of 105,222 evidence-based medical questions and answers (EBMQA) derived from a knowledge graph. The research systematically evaluates model performance on semantic versus numerical question types across different medical domains. Results demonstrate that while both models perform better on semantic questions than numerical ones, Claude-3 outperforms GPT-4 specifically on numerical questions. Despite their capabilities, both models remain inferior to human performance, highlighting the need for caution when deploying LLMs for medical advice.

## Method Summary
The study constructs the EBMQA dataset by extracting multiple-choice questions from a medical knowledge graph, with questions labeled by topic (medical/non-medical) and classified as either numerical or semantic. Two state-of-the-art LLMs (GPT-4 and Claude-3 Opus) are evaluated on this dataset using standard accuracy metrics. Performance is analyzed across different medical specialties and question types, with results compared to human performance baselines. The methodology focuses on systematic benchmarking rather than clinical application testing.

## Key Results
- Both LLMs perform significantly better on semantic questions than numerical questions
- Claude-3 Opus surpasses GPT-4 specifically in numerical question answering
- Both models show performance gaps in different medical specialties compared to humans
- LLMs remain inferior to human performance across all evaluated metrics

## Why This Works (Mechanism)
The study demonstrates that LLMs leverage their pattern recognition capabilities more effectively for semantic reasoning than numerical calculations. Semantic questions likely align better with the statistical language patterns LLMs learn during training, while numerical questions require precise computational abilities that current LLMs handle less reliably. The performance differences between models on numerical versus semantic tasks suggest architectural variations in how they process different question types.

## Foundational Learning

1. **Medical Knowledge Graph Structure**
   - Why needed: Provides the source of standardized medical questions and answers
   - Quick check: Verify the graph contains comprehensive coverage of relevant medical domains

2. **Numerical vs. Semantic Question Classification**
   - Why needed: Enables targeted analysis of LLM performance across different cognitive tasks
   - Quick check: Ensure classification accuracy exceeds 95% through human validation

3. **Multiple-Choice Question Format**
   - Why needed: Standardizes evaluation and enables direct performance comparison
   - Quick check: Confirm question difficulty distribution matches real-world clinical scenarios

4. **Medical Specialty Domain Knowledge**
   - Why needed: Allows analysis of model performance across different medical fields
   - Quick check: Map question distribution to ensure representative specialty coverage

5. **Evidence-Based Medicine Principles**
   - Why needed: Ensures questions reflect current medical knowledge and guidelines
   - Quick check: Validate question sources against established medical literature

6. **Large Language Model Evaluation Metrics**
   - Why needed: Provides standardized methods for comparing model performance
   - Quick check: Confirm metric consistency across different model versions and datasets

## Architecture Onboarding

**Component Map**: Knowledge Graph -> Question Extraction -> Classification Pipeline -> LLM Evaluation -> Performance Analysis

**Critical Path**: Knowledge graph extraction → question classification → LLM inference → accuracy calculation → comparative analysis

**Design Tradeoffs**: The study prioritizes breadth (large question set) over clinical realism (multiple-choice format). This enables systematic comparison but may not reflect actual clinical decision-making complexity.

**Failure Signatures**: Performance degradation on numerical questions suggests computational limitations. Specialty-specific gaps indicate incomplete domain coverage in training data.

**First Experiments**:
1. Replicate analysis on a subset of questions with human-annotated labels to verify classification accuracy
2. Test model performance on mixed semantic-numerical questions requiring multi-step reasoning
3. Compare results using different evaluation metrics (e.g., F1-score vs. accuracy)

## Open Questions the Paper Calls Out
None

## Limitations
- Multiple-choice format may not capture real-world clinical complexity
- Dataset derived from knowledge graph may not represent full medical knowledge spectrum
- Human performance comparisons are indirect rather than directly measured

## Confidence

| Claim | Confidence |
|-------|------------|
| LLMs perform better on semantic vs numerical questions | Medium |
| Claude-3 outperforms GPT-4 on numerical questions | Medium |
| Both models remain inferior to humans | Medium |
| Performance gaps vary across medical specialties | Medium |

## Next Checks

1. **Direct human benchmarking**: Conduct parallel testing of the same EBMQA questions with medical professionals to establish baseline human performance metrics for direct comparison.

2. **Cross-dataset validation**: Test the same models on alternative medical question-answering datasets (e.g., MedQA, PubMedQA) to verify whether the semantic/numerical performance patterns hold across different question sources and formats.

3. **Clinical scenario testing**: Develop and evaluate performance on clinical vignettes requiring multi-step reasoning and integration of multiple data points, moving beyond multiple-choice formats to assess real-world applicability.