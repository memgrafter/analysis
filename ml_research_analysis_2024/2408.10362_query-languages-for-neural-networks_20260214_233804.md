---
ver: rpa2
title: Query languages for neural networks
arxiv_id: '2408.10362'
source_url: https://arxiv.org/abs/2408.10362
tags:
- cell
- then
- hyperplane
- function
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces query languages for neural networks that
  go beyond simple function evaluation to support declarative querying for interpretability
  and verification. The authors present two approaches: FO(R), which treats the network
  as a black box and can quantify over real numbers, and FO(SUM), which treats it
  as a white box weighted structure and can quantify over neurons.'
---

# Query languages for neural networks

## Quick Facts
- arXiv ID: 2408.10362
- Source URL: https://arxiv.org/abs/2408.10362
- Reference count: 40
- Primary result: FO(SUM) can express all FO(Rlin) queries over fixed-depth neural networks through cell decomposition simulation

## Executive Summary
This paper introduces query languages for neural networks that go beyond simple function evaluation to support declarative querying for interpretability and verification. The authors present two approaches: FO(R), which treats the network as a black box and can quantify over real numbers, and FO(SUM), which treats it as a white box weighted structure and can quantify over neurons. The main technical result shows that for neural networks of fixed depth, every FO(Rlin) query can be expressed in FO(SUM). This is proven by constructing FO(SUM) translations that simulate real arithmetic using cell decompositions of the input space.

## Method Summary
The paper theoretically analyzes query languages for neural networks, introducing FO(R) as a black-box approach that can quantify over real numbers and FO(SUM) as a white-box approach that can quantify over neurons. The main method involves constructing FO(SUM) translations that transform neural networks into piecewise linear function structures, then build hyperplane arrangements and cylindrical cell decompositions to represent and manipulate real arithmetic operations. The approach relies on the fact that ReLU-activated feedforward neural networks compute piecewise linear functions that can be decomposed into finite cells where arithmetic operations become combinatorial problems.

## Key Results
- FO(SUM) can express all FO(Rlin) queries over fixed-depth neural networks
- FO(SUM) can compute integrals over neural network functions by summing areas under piecewise linear segments
- FO(SUM) can distinguish between different neural network architectures while FO(R,F) cannot, enabling model-agnostic queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FO(SUM) can express all FO(Rlin) queries over fixed-depth neural networks because it can simulate real arithmetic through cell decomposition.
- Mechanism: The proof constructs FO(SUM) translations that transform neural networks into piecewise linear function structures, then build hyperplane arrangements and cylindrical cell decompositions to represent and manipulate real arithmetic operations.
- Core assumption: Fixed-depth networks have finite piecewise linear representations that can be decomposed into cells where arithmetic operations become finite combinatorial problems.
- Evidence anchors:
  - [abstract]: "every FO(Rlin) query can be expressed in FO(SUM)"
  - [section 7]: "The challenge in proving this result is to simulate, using quantification and summation over neurons, the unrestricted access to real numbers that is available in FO(Rlin)"
- Break condition: If the depth is not fixed or the activation functions are not piecewise linear, the cell decomposition approach fails and FO(SUM) cannot simulate real arithmetic.

### Mechanism 2
- Claim: FO(SUM) can compute integrals over neural network functions because it can represent and sum over piecewise linear segments.
- Mechanism: The proof shows how to identify breakpoints in the piecewise linear function, calculate areas under each segment, and sum them while accounting for duplicate breakpoints through counting.
- Core assumption: Breakpoints can be uniquely identified through network weights and biases, and the piecewise linear structure allows exact area calculation.
- Evidence anchors:
  - [section 5, example 5.3]: "There exists anFO(SUM) term t over Υ net(m, 1) with m additional pairs of weight constant symbols mini and maxi for i ∈ {1,...,m}, such that for any networkN in F(m,ℓ), and values ai and bi for the mini and maxi, we havetN,a1,b1,...,am,bm =∫b1a1···∫bma mFNdx1...dxm"
  - [appendix A]: Detailed construction showing how to calculate areas between breakpoints and sum them correctly
- Break condition: If breakpoints are not uniquely identifiable or the function has infinite complexity within the integration bounds.

### Mechanism 3
- Claim: FO(SUM) can distinguish between different neural network architectures while FO(R,F) cannot, enabling model-agnostic queries beyond what black-box approaches support.
- Mechanism: FO(SUM) can query the network structure directly (weights, biases, topology) while FO(R,F) only has access to input-output behavior, allowing it to express queries about network properties that don't depend on the specific function computed.
- Core assumption: The network structure contains information about the function that can be queried independently of the function values.
- Evidence anchors:
  - [abstract]: "In general, indeed the two expressive powers are incomparable"
  - [section 6]: "Black-box queries are commonly calledmodel agnostic" and shows examples of model-agnostic queries FO(SUM) can express that FO(R,F) cannot
- Break condition: If the query depends only on function values rather than network structure, both approaches have equivalent expressive power.

## Foundational Learning

- Concept: Piecewise linear functions and their representation
  - Why needed here: The entire approach relies on neural networks computing piecewise linear functions that can be decomposed into cells for arithmetic operations
  - Quick check question: Can you explain how a ReLU activation creates breakpoints in a neural network function?

- Concept: Cylindrical cell decomposition
  - Why needed here: This is the key technique for representing and manipulating real arithmetic in FO(SUM) by dividing space into finite cells
  - Quick check question: How does cylindrical decomposition differ from regular space partitioning?

- Concept: First-order interpretations and translations
  - Why needed here: The proof uses FO(SUM) translations to convert between different representations of neural networks and functions
  - Quick check question: What is the difference between a standard first-order interpretation and an FO(SUM) translation?

## Architecture Onboarding

- Component map:
  - Υ net(m,n) -> Υ pwl_m -> Υ arr_d -> Υ cell_d -> Query evaluation

- Critical path: Network → Piecewise linear function → Hyperplane arrangement → Cell decomposition → Query evaluation

- Design tradeoffs:
  - Fixed depth requirement: Makes the approach practical but limits expressiveness
  - No recursion: Prevents handling unbounded-depth networks but keeps the logic decidable
  - Computational complexity: FO(SUM) queries can be very complex to evaluate

- Failure signatures:
  - Query returns empty set unexpectedly: Likely issue with cell decomposition or breakpoint identification
  - Query takes too long: FO(SUM) with arithmetic operations can be computationally expensive
  - Incorrect results for integration: Probably error in breakpoint counting or area calculation

- First 3 experiments:
  1. Implement FO(SUM) query to evaluate a simple one-layer neural network on a fixed input
  2. Create FO(SUM) query to find breakpoints in a two-layer network and verify they match analytical calculations
  3. Implement FO(SUM) integral calculation for a simple piecewise linear function and compare with numerical integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FO(SUM) express queries on neural networks of unbounded depth?
- Basis in paper: [explicit] The paper states that FO(SUM) lacks recursion and cannot express function evaluation on networks of unbounded depth without knowing the depth beforehand.
- Why unresolved: While the paper proves limitations for bounded-depth networks, it doesn't explore potential extensions or alternative approaches that might enable querying deeper networks.
- What evidence would resolve it: A formal proof showing whether FO(SUM) with recursion or an alternative formalism can express queries on arbitrary-depth neural networks, or a demonstration that this remains impossible even with extensions.

### Open Question 2
- Question: How does the expressiveness of FO(SUM) compare to FO(R) for neural networks with activation functions other than ReLU?
- Basis in paper: [inferred] The paper focuses specifically on ReLU-FNNs and proves that FO(SUM) can express all FO(Rlin) queries for these networks. However, it mentions other activation functions as an open direction without analysis.
- Why unresolved: The proof techniques and cell decomposition methods used for ReLU might not directly apply to other activation functions, but the paper doesn't explore this comparison.
- What evidence would resolve it: A formal comparison showing whether FO(SUM) remains more expressive than FO(R) for other common activation functions like sigmoid or tanh, either through simulation results or impossibility proofs.

### Open Question 3
- Question: Can the cell decomposition technique used in the main proof be pre-produced as an index structure for practical query processing?
- Basis in paper: [explicit] The conclusion mentions that cell decompositions might be preproduced by query processors as a novel kind of index data structure, but doesn't explore this possibility.
- Why unresolved: While the theoretical construction is proven, the paper doesn't investigate practical implementation considerations, performance characteristics, or comparison with existing indexing methods.
- What evidence would resolve it: Implementation and benchmarking of cell decomposition-based indexing for neural network queries, comparing query performance against traditional methods and measuring storage overhead.

## Limitations
- The approach is limited to neural networks of fixed depth, excluding modern deep networks
- The computational complexity of FO(SUM) queries with arithmetic operations can be very high
- The technique relies on piecewise linear activation functions, limiting applicability to networks with other activation types

## Confidence
- Expressive power comparison (FO(SUM) vs FO(R) for fixed-depth networks): **High**
- Computational complexity claims: **Medium**
- Practical applicability to real-world neural networks: **Low**
- Black-box vs white-box approach utility: **Medium**

## Next Checks
1. Implement the FO(SUM) translation for a two-layer neural network and verify that it correctly computes outputs for all inputs in the cell decomposition
2. Test the integral computation on a known piecewise linear function with analytical solution to validate the breakpoint identification and area calculation
3. Compare query performance between FO(R) and FO(SUM) approaches on a fixed-depth network for queries that can be expressed in both languages to quantify the computational overhead