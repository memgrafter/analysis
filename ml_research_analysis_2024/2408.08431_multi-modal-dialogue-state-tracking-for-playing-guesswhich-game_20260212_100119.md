---
ver: rpa2
title: Multi-Modal Dialogue State Tracking for Playing GuessWhich Game
arxiv_id: '2408.08431'
source_url: https://arxiv.org/abs/2408.08431
tags:
- dialogue
- state
- qbot
- visual
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new approach for GuessWhich game called
  Dialogue State Tracking (DST) that uses mental imagery to reason about images. The
  model builds a mental model of the undisclosed image through textual semantics and
  tracks dialogue states composed of word and image representations.
---

# Multi-Modal Dialogue State Tracking for Playing GuessWhich Game

## Quick Facts
- arXiv ID: 2408.08431
- Source URL: https://arxiv.org/abs/2408.08431
- Reference count: 21
- Primary result: Achieves state-of-the-art performance on VisDial datasets with visually related question generation and state tracking

## Executive Summary
This paper introduces a novel Dialogue State Tracking (DST) approach for the GuessWhich game that leverages mental imagery to reason about images through textual semantics and visual representations. The proposed model tracks dialogue states combining word and image representations, engaging in visually related reasoning at each round to generate questions and update states. Experimental results demonstrate superior performance across all metrics on VisDial datasets compared to previous models, with the ability to generate more visually relevant questions while avoiding repetition through combined visual reasoning and state tracking.

## Method Summary
The proposed DST model builds a mental model of undisclosed images through textual semantics and tracks dialogue states composed of word and image representations. At each round, the model engages in visually related reasoning using the dialogue state to generate questions and update the states. The approach combines visual reasoning on dialogue states with state tracking to enhance performance in the GuessWhich game scenario.

## Key Results
- Achieves state-of-the-art performance across all metrics on VisDial datasets
- Generates more visually related questions compared to previous models
- Successfully avoids question repetition through combined visual reasoning and state tracking

## Why This Works (Mechanism)
Assumption: The mental imagery component allows the model to build internal representations of visual concepts through textual descriptions, enabling more effective question generation and state tracking. Unknown: Specific architectural details of how mental imagery interacts with visual representations.

## Foundational Learning
- Mental imagery in dialogue systems: Enables reasoning about visual information through textual representations; quick check: validate through visual reasoning accuracy
- Multi-modal state tracking: Combines text and image representations for dialogue understanding; quick check: assess tracking accuracy across modalities
- Visual reasoning in dialogue: Generates contextually relevant questions based on visual information; quick check: measure question relevance scores

## Architecture Onboarding
Component map: Visual input -> Mental imagery module -> Dialogue state tracker -> Question generator -> State updater

Critical path: Visual input → Mental imagery → Dialogue state → Question generation → State update

Design tradeoffs: Mental imagery reasoning vs. direct visual processing; state complexity vs. computational efficiency

Failure signatures: Incorrect visual reasoning leading to irrelevant questions; state tracking errors causing question repetition

First experiments:
1. Baseline comparison with traditional dialogue state tracking methods
2. Ablation study removing mental imagery component
3. Performance analysis on out-of-domain visual dialogue tasks

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly identify open questions or future research directions in the available context.

## Limitations
- Evaluation methodology details are not fully specified, making performance claims difficult to verify
- Model's ability to handle complex visual reasoning beyond simple object recognition remains unclear
- Specific adaptation for GuessWhich game dynamics is not fully detailed

## Confidence
- High confidence in model's ability to generate visually related questions and avoid repetition
- Medium confidence in claimed state-of-the-art performance across all metrics
- Low confidence in model's generalizability to complex visual reasoning tasks

## Next Checks
1. Conduct ablation studies to isolate contributions of mental imagery reasoning versus traditional dialogue state tracking components
2. Test model on out-of-domain visual dialogue tasks to assess generalization beyond GuessWhich game
3. Perform human evaluation studies to validate quality and relevance of generated questions compared to human dialogue