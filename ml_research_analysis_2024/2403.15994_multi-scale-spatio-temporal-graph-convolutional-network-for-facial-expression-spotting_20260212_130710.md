---
ver: rpa2
title: Multi-Scale Spatio-Temporal Graph Convolutional Network for Facial Expression
  Spotting
arxiv_id: '2403.15994'
source_url: https://arxiv.org/abs/2403.15994
tags:
- facial
- spotting
- learning
- expression
- optical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-scale spatio-temporal graph convolutional
  network for facial expression spotting. The authors address the challenge of detecting
  subtle micro-expressions in long videos, which is affected by irrelevant facial
  movements and difficulty in perceiving subtle motions.
---

# Multi-Scale Spatio-Temporal Graph Convolutional Network for Facial Expression Spotting

## Quick Facts
- arXiv ID: 2403.15994
- Source URL: https://arxiv.org/abs/2403.15994
- Authors: Yicheng Deng; Hideaki Hayashi; Hajime Nagahara
- Reference count: 40
- Key outcome: Introduces a multi-scale spatio-temporal graph convolutional network for facial expression spotting, achieving state-of-the-art performance on SAMM-LV and CAS(ME)2 datasets with F1-scores of 0.4454 and 0.4154 respectively, particularly for micro-expression spotting.

## Executive Summary
This paper addresses the challenging problem of facial expression spotting in long videos, focusing on detecting subtle micro-expressions amid irrelevant facial movements. The authors propose a novel multi-scale spatio-temporal graph convolutional network that leverages a receptive field adaptive sliding window strategy to compute short- and long-term optical flows, effectively magnifying motion information while mitigating head movement interference. By incorporating facial local graph pooling and supervised contrastive learning, the model learns discriminative features at multiple spatial scales and enhances boundary discrimination between expression types, resulting in superior performance on benchmark datasets.

## Method Summary
The method employs a multi-scale spatio-temporal graph convolutional network that first extracts motion features using a receptive field adaptive sliding window strategy to compute short- and long-term optical flows. These flows are used to construct a facial graph representation, which is processed through a stack of spatial-temporal graph convolutional layers. A facial local graph pooling strategy extracts multi-scale facial graph-structured features, while supervised contrastive learning enhances discriminative capability for difficult-to-classify frames. The model is trained using focal loss and evaluated on SAMM-LV and CAS(ME)2 datasets following the MEGC2021 protocol.

## Key Results
- Achieves state-of-the-art F1-scores of 0.4454 for micro-expression spotting on SAMM-LV dataset
- Achieves state-of-the-art F1-scores of 0.4154 for micro-expression spotting on CAS(ME)2 dataset
- Demonstrates significant improvements over existing methods, particularly in detecting subtle micro-expressions
- Shows effectiveness of receptive field adaptive sliding window in magnifying motion information while reducing head movement interference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Receptive field adaptive sliding window magnifies subtle motion while avoiding head movement interference
- Mechanism: By aligning window length to network's temporal receptive field, the method computes both short- and long-term optical flows within each clip. This captures motion variations across multiple time scales while the nose-tip-based bounding box adjustment removes global head movement.
- Core assumption: Subtle micro-expression motions are more reliably extracted when computed over a window that fully utilizes the network's temporal receptive field, rather than just adjacent frames or overly large windows.
- Evidence anchors:
  - [abstract] "receptive field adaptive sliding window strategy to compute short- and long-term optical flows, which magnifies motion information while alleviating the problem of severe head movement"
  - [section] "design a receptive field adaptive sliding window strategy, where the temporal window size corresponds to the receptive field of the network, to compute and combine short- and long-term optical flows as input for frame-level apex or boundary (onset or offset) probability estimation, amplifying the motion features while avoiding significant head movement problems"
- Break condition: If the nose tip region is not stationary during the expression, the head movement correction fails, reintroducing motion artifacts.

### Mechanism 2
- Claim: Multi-scale facial graph pooling captures spatial relationships at different granularities
- Mechanism: The proposed Facial Local Graph Pooling (FLGP) downsamples the spatial scale of the facial graph through max pooling while preserving the temporal dimension. This creates a hierarchy of graph structures, from detailed ROI relationships to global facial dynamics, improving representational capacity.
- Core assumption: Facial expressions involve both local muscle movements and global facial configurations, so learning features at multiple spatial scales is necessary for accurate spotting.
- Evidence anchors:
  - [abstract] "This network learns both local and global features from multiple scales of facial graph structures using our proposed facial local graph pooling (FLGP)"
  - [section] "we introduce FLGP, specifically designed for extracting multi-scale facial graph features. In practice, we design three scales of facial structures"
- Break condition: If the pooling strategy discards too much spatial detail, local expression cues may be lost, harming spotting accuracy.

### Mechanism 3
- Claim: Supervised contrastive learning improves discrimination between macro- and micro-expressions near boundaries
- Mechanism: By introducing a supervised contrastive loss, the model pushes feature representations of the same expression type closer and those of different types farther apart. This is especially effective for distinguishing frames where macro- and micro-expressions are close in duration or intensity.
- Core assumption: The main challenge in expression spotting is not just classification but recognizing boundaries between expression types; contrastive learning enhances this boundary discrimination.
- Evidence anchors:
  - [abstract] "we introduce supervised contrastive learning to enhance the discriminative capability of our model for difficult-to-classify frames"
  - [section] "we introduce supervised contrastive loss to minimize the distance between feature representations of the same class while simultaneously pushing apart feature representations of different classes"
- Break condition: If the batch size is too small or class imbalance is severe, contrastive pairs may be insufficient to learn meaningful boundaries.

## Foundational Learning

- Concept: Graph Convolutional Networks (GCNs)
  - Why needed here: Facial expressions are inherently graph-structured (facial parts connected in spatial patterns), and GCNs can model both spatial relationships and temporal dynamics across frames.
  - Quick check question: How does a GCN aggregate information from neighboring nodes compared to a standard CNN?

- Concept: Temporal receptive field in CNNs/GCNs
  - Why needed here: The receptive field determines how much temporal context the network uses to make a prediction; matching the sliding window size to this receptive field ensures full utilization of learned temporal patterns.
  - Quick check question: If the temporal window is smaller than the network's receptive field, what information might be lost?

- Concept: Contrastive learning in supervised settings
  - Why needed here: Contrastive loss helps the model learn more discriminative features by explicitly enforcing separation between different expression classes, crucial for spotting subtle micro-expressions.
  - Quick check question: What is the effect of temperature parameter τ in the contrastive loss formula?

## Architecture Onboarding

- Component map: Raw video -> Face detection -> Optical flow extraction -> Graph construction -> ST-GCN -> FLGP -> TCN -> probabilities -> contrastive loss -> output
- Critical path: Raw video → face detection → optical flow extraction → graph construction → ST-GCN → FLGP → TCN → probabilities → contrastive loss → output
- Design tradeoffs:
  - Sliding window size vs. computational load vs. temporal context captured
  - Number of graph scales vs. model complexity vs. feature richness
  - Contrastive loss weight λ vs. classification stability vs. boundary discrimination
- Failure signatures:
  - Low recall for micro-expressions → insufficient temporal context or poor motion magnification
  - High false positives near expression boundaries → contrastive learning underweighted or insufficient
  - Noisy optical flows → poor face alignment or ROI selection
- First 3 experiments:
  1. Test receptive field adaptive sliding window with different window sizes (5, 11, 17, 25) on a small subset to find optimal temporal context.
  2. Compare single-scale ST-GCN vs. multi-scale with FLGP to validate feature learning improvement.
  3. Vary λ in supervised contrastive loss to find the sweet spot between classification accuracy and boundary discrimination.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed receptive field adaptive sliding window strategy perform on datasets with different frame rates or severe head movements?
- Basis in paper: [explicit] The authors mention that the SAMM-LV dataset has a frame rate of 200fps, which is subsampled to 30fps for experiments. They also state that the strategy alleviates the problem of severe head movement.
- Why unresolved: The paper does not provide experiments or analysis on datasets with different frame rates or more severe head movements to validate the generalizability of the proposed strategy.
- What evidence would resolve it: Experiments on datasets with various frame rates and head movement levels, along with quantitative comparisons to existing methods.

### Open Question 2
- Question: What is the impact of the number of regions of interest (ROIs) on the performance of the proposed method?
- Basis in paper: [explicit] The authors extract R=10 ROIs that are most representative of facial expressions. However, they do not provide an ablation study on the effect of varying the number of ROIs.
- Why unresolved: The paper does not explore how the number of ROIs affects the performance, which could provide insights into the trade-off between computational complexity and accuracy.
- What evidence would resolve it: Experiments with different numbers of ROIs, along with analysis of the computational cost and performance trade-offs.

### Open Question 3
- Question: How does the proposed method handle micro-expressions with very low intensity or those involving eye blinking?
- Basis in paper: [inferred] The authors mention that some ground-truth micro-expressions exhibit very low intensity, and noises in optical flow features can lead to misclassification of normal frames as micro-expression frames. They also consider eye blinking as irrelevant motion.
- Why unresolved: The paper does not provide a detailed analysis of the method's performance on micro-expressions with very low intensity or those involving eye blinking, which are common challenges in micro-expression spotting.
- What evidence would resolve it: Experiments on datasets with micro-expressions of varying intensities and those involving eye blinking, along with a discussion of the method's limitations and potential improvements.

## Limitations
- The proposed method relies heavily on accurate facial alignment and ROI extraction, which may fail under extreme head poses or occlusions.
- The performance gains from multi-scale graph pooling and supervised contrastive learning are not independently validated, making it difficult to assess the contribution of each component.
- The evaluation protocol uses a fixed IoU threshold of 0.5, which may not reflect real-world spotting requirements where partial overlaps are acceptable.

## Confidence
- **High confidence**: The receptive field adaptive sliding window strategy is clearly described and its motivation is well-grounded in the need to capture temporal context matching the network's receptive field.
- **Medium confidence**: The multi-scale facial graph pooling approach is conceptually sound, but the specific pooling mechanism and its impact on downstream performance are not fully detailed.
- **Low confidence**: The exact formulation and hyperparameters of the supervised contrastive loss (e.g., temperature τ, class weighting) are not explicitly stated, making it difficult to reproduce the reported improvements.

## Next Checks
1. **Component ablation study**: Train and evaluate the model with only ST-GCN (no FLGP), ST-GCN + FLGP (no contrastive loss), and the full pipeline to quantify the contribution of each innovation.
2. **Robustness to alignment errors**: Intentionally degrade facial alignment quality (e.g., by adding synthetic head pose variations) and measure the impact on micro-expression spotting F1-score.
3. **IoU threshold sensitivity**: Evaluate spotting performance across a range of IoU thresholds (0.3 to 0.7) to assess whether the claimed improvements hold under different evaluation criteria.