---
ver: rpa2
title: Evaluating LLMs with Multiple Problems at once
arxiv_id: '2406.10786'
source_url: https://arxiv.org/abs/2406.10786
tags:
- llms
- text
- task
- problems
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces multi-problem evaluation (MPE), a paradigm
  that assesses large language models (LLMs) by placing multiple problems together
  in a single prompt and evaluating their ability to answer all problems in one output.
  The authors create ZeMPE, a benchmark of 53,100 zero-shot multi-problem prompts
  built from 6 classification and 12 reasoning datasets.
---

# Evaluating LLMs with Multiple Problems at once

## Quick Facts
- arXiv ID: 2406.10786
- Source URL: https://arxiv.org/abs/2406.10786
- Reference count: 17
- One-line primary result: LLMs can handle multiple classification problems from a single source with minimal performance loss, but struggle with index selection reformulation and mixed reasoning sources

## Executive Summary
This study introduces multi-problem evaluation (MPE), a paradigm that assesses large language models (LLMs) by placing multiple problems together in a single prompt and evaluating their ability to answer all problems in one output. The authors create ZeMPE, a benchmark of 53,100 zero-shot multi-problem prompts built from 6 classification and 12 reasoning datasets. They evaluate 13 LLMs on tasks like batch classification and multi-source reasoning. Results show LLMs can handle multiple problems from the same source with minimal performance loss and benefit from reduced inference costs, but performance drops sharply when problems are reformulated as index-selection tasks or mixed across different reasoning sources. Zero-shot chain-of-thought prompting transfers well under MPP. Instruction tuning is identified as a key enabler of multi-problem handling. The study demonstrates the feasibility, benefits, and limitations of MPE for probing LLM capabilities.

## Method Summary
The study constructs a benchmark of 53,100 zero-shot multi-problem prompts using 6 classification and 12 reasoning datasets. LLMs are evaluated on various multi-problem tasks including batch classification (multiple problems with shared context), select-one/select-all (index-based selection), and multi-source reasoning (combining problems from different datasets). The evaluation uses greedy decoding for classification tasks and zero-shot chain-of-thought prompting for reasoning tasks. Performance is measured using average per-problem accuracy (PPA) to unify evaluation across all task types. The study compares instruction-tuned and base model variants across multiple model families including Vicuna, Mistral, Mixtral, Llama-3, GPT-3.5, and GPT-4.

## Key Results
- LLMs achieve at least 90% accuracy of single-problem classification when handling multiple problems from the same source
- Multi-problem prompting reduces inference costs by eliminating redundant task instructions
- Zero-shot chain-of-thought prompting transfers well to multi-problem reasoning tasks
- Performance degrades significantly when classification problems are reformulated as index selection tasks
- Mixing reasoning problems from different sources causes substantial performance drops

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can handle multiple classification problems from a single source with minimal performance loss.
- Mechanism: The model uses the same shared context to solve each problem independently within a single prompt.
- Core assumption: The model can maintain task understanding across multiple problem instances without interference.
- Evidence anchors:
  - [abstract] "LLMs are capable of handling multiple problems from a single data source as well as handling them separately"
  - [section 4.3] "all 7 LLMs achieve accuracy of at least 90% that of SingleClf across the benchmarks most of the time"
  - [corpus] "REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once" (0.5154 FMR) suggests this area is underexplored
- Break condition: Performance degrades when problems are reformulated as index selection tasks or when reasoning problems are mixed across different sources.

### Mechanism 2
- Claim: Zero-shot multi-problem prompting can be highly cost-efficient.
- Mechanism: Sharing task instructions across multiple problems reduces redundant token usage compared to single-problem prompting.
- Core assumption: The token savings from shared context outweigh any performance degradation from combining problems.
- Evidence anchors:
  - [abstract] "LLMs can handle multiple problems from a single data source as well as handling them separately, but there are conditions this multiple problem handling capability falls short"
  - [section 4.3] "we observe that while large language models (LLMs) demonstrate strong zero-shot classification capabilities and prompting with multiple problems can be cost-efficient"
  - [section 4.3] "Multi-problem prompting reduces this redundancy, and this saving is larger the more problems are combined in a single prompt"
- Break condition: When the batch size becomes too large relative to context window, causing token overflow or performance collapse.

### Mechanism 3
- Claim: Zero-shot chain-of-thought prompting transfers well under multi-problem prompting.
- Mechanism: The model applies step-by-step reasoning to each individual problem within the multi-problem context.
- Core assumption: The model can independently generate reasoning steps for each problem without cross-contamination.
- Evidence anchors:
  - [abstract] "zero-shot chain-of-thought prompting transfers well under MPP"
  - [section 4.4] "GPT-3.5 generally performs better with CoT than without it under zero-shot MPP for both MultiReasonSS and MultiReasonMS"
  - [section 4.4] "This again implies the strong capabilities of LLMs to utilize information across positions under MPP"
- Break condition: When reasoning problems from different sources are mixed, causing interference in the reasoning process.

## Foundational Learning

- Concept: Multi-problem evaluation (MPE) vs single-problem evaluation (SPE)
  - Why needed here: Understanding the distinction is crucial for interpreting results and designing experiments
  - Quick check question: What is the key difference between MPE and SPE in terms of how problems are presented to the model?

- Concept: Zero-shot vs few-shot prompting
  - Why needed here: The study focuses on zero-shot MPE, which has different implications than few-shot approaches
  - Quick check question: How does zero-shot MPE differ from the few-shot approaches mentioned in related work?

- Concept: Task size and its effects
  - Why needed here: The number of problems in a prompt (task size) directly impacts performance and cost efficiency
  - Quick check question: How does increasing task size typically affect LLM performance in MPE?

## Architecture Onboarding

- Component map:
  Prompt construction module (ZeMPE generator) -> LLM inference service (multiple model families) -> Evaluation pipeline (accuracy calculation, comparison) -> Error analysis tools (positional bias, error distribution)

- Critical path:
  1. Generate multi-problem prompts from benchmark datasets
  2. Submit prompts to LLMs with greedy decoding
  3. Parse and evaluate model outputs
  4. Perform statistical analysis and error investigation

- Design tradeoffs:
  - Larger task sizes improve cost efficiency but may reduce accuracy
  - Mixed-source prompts provide broader evaluation but introduce performance degradation
  - Zero-shot approach avoids data contamination but may miss benefits of few-shot exemplars

- Failure signatures:
  - Sharp accuracy drops when problems are reformulated as index selection
  - Performance below expected levels when reasoning problems are mixed across sources
  - Token overflow errors when task size exceeds context window limits

- First 3 experiments:
  1. Run BatchClf vs SingleClf comparison on a single benchmark to verify cost efficiency claims
  2. Test SelectOne vs BatchClf performance gap on a small task size (n=3)
  3. Compare MultiReasonSS vs MultiReasonMS performance on a single benchmark combination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions do LLMs fail to maintain performance when solving multi-problem prompts, and how do these conditions vary across model families and reasoning types?
- Basis in paper: The authors identify two general conditions where LLMs perform significantly worse: reformulating classification as index selection and mixing reasoning problems from different sources. They explore these conditions but note limitations in understanding the full scope.
- Why unresolved: The study primarily tests a limited set of conditions (e.g., small task sizes, specific benchmark combinations) and does not systematically explore the boundaries of when performance degradation occurs across all model types and reasoning paradigms.
- What evidence would resolve it: A comprehensive ablation study testing multi-problem prompts across all reasoning types, varying task sizes, and systematically comparing all model families (including smaller models and encoder-decoder architectures) would clarify the generalizability of performance drops.

### Open Question 2
- Question: How does the internal reasoning process of LLMs differ when handling single-source versus mixed-source multi-problem prompts, and what architectural or training factors contribute to these differences?
- Basis in paper: The authors observe that LLMs perform worse on mixed-source reasoning problems and suggest this indicates a lack of human-like understanding, but do not analyze the internal mechanisms behind this behavior.
- Why unresolved: The study uses only output-level metrics (accuracy) and does not examine intermediate reasoning steps, attention patterns, or activation differences that could explain why mixed-source prompts are more challenging.
- What evidence would resolve it: Analyzing intermediate outputs (e.g., chain-of-thought reasoning steps, attention weights) for both single-source and mixed-source prompts across model families would reveal whether the degradation stems from reasoning fragmentation, context interference, or other architectural limitations.

### Open Question 3
- Question: To what extent does instruction tuning versus model scale contribute to multi-problem handling capabilities, and are there synergistic effects between these factors?
- Basis in paper: The authors find that instruction-tuned models outperform base models and suggest instruction tuning is important, but note that even base models like Llama-3 8B can perform reasonably well. They do not test the interaction between scale and instruction tuning.
- Why unresolved: The study compares a limited set of models (7 instruction-tuned decoders vs. 6 base models including FLAN-T5) without systematically varying both scale and instruction tuning status to isolate their individual and combined contributions.
- What evidence would resolve it: A controlled experiment varying both model scale (small, medium, large) and instruction tuning status (base vs. instruction-tuned) across multiple model families would quantify the relative importance of each factor and reveal any synergistic effects.

## Limitations

- Limited scope to classification and reasoning tasks, excluding generation, summarization, and other LLM capabilities
- Zero-shot constraint may miss performance improvements achievable through few-shot prompting or fine-tuning
- Benchmark selection bias from 18 specific datasets may not generalize to other domains or task complexities

## Confidence

**High confidence**: The core finding that LLMs can handle multiple problems from the same source with minimal performance loss is well-supported by experimental results across 7 LLMs and multiple benchmark combinations. The cost efficiency benefits are mathematically straightforward and empirically validated.

**Medium confidence**: The transferability of zero-shot chain-of-thought prompting under multi-problem prompting is supported by results on GPT-3.5 but would benefit from broader model family testing. The identification of instruction tuning as a key enabler is based on observed performance differences but lacks detailed analysis of the underlying mechanisms.

**Low confidence**: The specific performance thresholds (90% PPA for BatchClf vs SingleClf) may be sensitive to benchmark selection and task difficulty variations. The error analysis findings about positional bias are based on aggregate statistics that may mask individual model behaviors.

## Next Checks

1. **Cross-task generalization**: Test multi-problem prompting on generation and summarization tasks to determine if the observed benefits extend beyond classification and reasoning domains.

2. **Few-shot extension**: Replicate key experiments using few-shot MPE to quantify the performance gap and identify which task types benefit most from exemplar inclusion.

3. **Scaling analysis**: Systematically vary task size (n) across a wider range to identify the optimal batch size that maximizes cost efficiency without significant accuracy degradation, and test the impact of context window limitations.