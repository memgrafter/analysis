---
ver: rpa2
title: 'ToDo: Token Downsampling for Efficient Generation of High-Resolution Images'
arxiv_id: '2402.13573'
source_url: https://arxiv.org/abs/2402.13573
tags:
- attention
- tokens
- image
- token
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient high-resolution
  image generation with diffusion models, which suffer from quadratic computational
  complexity in their attention mechanisms. The authors propose a novel, training-free
  method called Token Downsampling (ToDo) that accelerates inference by reducing the
  number of key and value tokens through spatial downsampling, while preserving full
  query tokens.
---

# ToDo: Token Downsampling for Efficient Generation of High-Resolution Images

## Quick Facts
- **arXiv ID**: 2402.13573
- **Source URL**: https://arxiv.org/abs/2402.13573
- **Reference count**: 5
- **Primary result**: Training-free Token Downsampling (ToDo) achieves up to 4.5x speedup in high-resolution image generation while maintaining comparable quality to baseline

## Executive Summary
This paper addresses the computational bottleneck in high-resolution image generation with diffusion models, specifically the quadratic complexity of attention mechanisms. The authors propose a novel training-free method called Token Downsampling (ToDo) that accelerates inference by spatially downsampling key and value tokens while preserving all query tokens. Tested on Stable Diffusion with resolutions up to 2048x2048, ToDo demonstrates significant speed improvements (up to 4.5x) while maintaining image quality comparable to baseline models. The method outperforms previous approaches in balancing throughput and fidelity.

## Method Summary
The authors introduce Token Downsampling (ToDo), a training-free approach that reduces computational complexity in diffusion models by downsampling key and value tokens in the attention mechanism. Unlike previous methods that modify both query and key/value tokens or require architectural changes, ToDo preserves all query tokens to maintain image fidelity while spatially downsampling the KV tokens. This selective reduction allows for significant speedup without extensive retraining. The method is evaluated across multiple resolutions (512x512, 1024x1024, and 2048x2048) on Stable Diffusion, demonstrating consistent performance improvements across different compression levels.

## Key Results
- Achieves up to 4.5x speedup compared to baseline models
- Maintains comparable image quality with baseline as measured by MSE and HPF metrics
- Outperforms previous methods (DNS, DNS-ES, AccDo) in balancing throughput and image fidelity
- Demonstrates consistent performance across multiple resolutions (512x512, 1024x1024, 2048x2048)

## Why This Works (Mechanism)
ToDo works by exploiting the redundancy in high-resolution attention maps. By spatially downsampling key and value tokens while preserving all query tokens, the method reduces the computational load of the attention mechanism without significantly impacting the quality of generated images. The preservation of query tokens ensures that all spatial locations maintain their ability to attend to the downsampled context, while the reduction in KV tokens decreases the quadratic complexity from O(N²) to approximately O((N/k)²) where k is the downsampling factor. This selective downsampling approach maintains the integrity of the generation process while achieving substantial computational savings.

## Foundational Learning

**Attention Mechanism**: The core operation in transformer-based diffusion models that computes relationships between tokens. Why needed: Understanding attention complexity is crucial as ToDo directly modifies this component. Quick check: Verify that attention complexity scales quadratically with token count.

**Spatial Downsampling**: Reducing the resolution of spatial data by combining or discarding information from neighboring positions. Why needed: ToDo's primary mechanism relies on this concept applied to KV tokens. Quick check: Confirm that downsampling preserves essential spatial relationships.

**Diffusion Models**: Generative models that iteratively denoise random noise into structured images through a learned reverse process. Why needed: ToDo is specifically designed for this class of models. Quick check: Understand the role of attention in the denoising process.

**Stable Diffusion Architecture**: A specific diffusion model architecture combining UNet with transformer-based cross-attention. Why needed: Primary evaluation platform for ToDo. Quick check: Identify where attention bottlenecks occur in the architecture.

**Inference vs Training**: Different computational requirements and optimizations for model deployment versus learning. Why needed: ToDo is specifically an inference optimization. Quick check: Distinguish between training-free and training-based acceleration methods.

## Architecture Onboarding

**Component Map**: Stable Diffusion UNet -> Cross-Attention Layers -> ToDo KV Downsampling -> Attention Computation -> Image Generation

**Critical Path**: Input latent -> UNet processing -> Cross-attention with downsampled KV tokens -> Residual connections -> Output latent -> Final image

**Design Tradeoffs**: ToDo trades computational efficiency for potential quality degradation at high compression ratios. The training-free approach sacrifices potential performance gains from learned optimizations but offers immediate deployment benefits. Preserving query tokens maintains generation quality while downsampling KV tokens provides computational savings.

**Failure Signatures**: Excessive downsampling leading to quality degradation, artifacts at token boundaries, loss of fine details in generated images, and potential instability in early denoising steps.

**3 First Experiments**:
1. Baseline comparison at 512x512 resolution without any downsampling
2. Progressive downsampling tests (2x, 4x, 8x) at 1024x1024 resolution
3. Quality-speed tradeoff analysis at maximum speedup (4.5x) across all tested resolutions

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Limited testing across different diffusion model architectures beyond Stable Diffusion
- Evaluation focused primarily on unconditional generation with limited exploration of text-to-image tasks
- Quality metrics (MSE, HPF) may not fully capture perceptual differences at high compression ratios
- Scalability to resolutions beyond 2048x2048 remains unexplored

## Confidence

**High confidence in**:
- The core mechanism of spatial downsampling for KV tokens
- Basic implementation feasibility of ToDo
- Reported speed improvements and comparative performance

**Medium confidence in**:
- Generalizability across different diffusion model architectures
- Long-term stability of image quality at maximum speedup
- Performance scaling with resolutions beyond tested ranges

**Low confidence in**:
- Effectiveness for extremely high-resolution generation (4096x4096+)
- Performance with non-Transformer attention mechanisms
- Behavior under resource-constrained deployment scenarios

## Next Checks
1. Evaluate ToDo across multiple diffusion model architectures (Imagen, DALL-E variants) to assess generalizability beyond Stable Diffusion.

2. Conduct perceptual user studies comparing images generated with ToDo at maximum speedup against baseline models to validate quality preservation claims beyond quantitative metrics.

3. Test scalability by evaluating performance and quality preservation at resolutions significantly higher than 2048x2048 (e.g., 4096x4096 or higher) to understand practical limits of the approach.