---
ver: rpa2
title: 'SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific
  Literature'
arxiv_id: '2406.07835'
source_url: https://arxiv.org/abs/2406.07835
tags:
- sciriff
- task
- scientific
- tasks
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces SciRIFF, a comprehensive dataset with 137K
  instruction-following demonstrations for 54 tasks across five scientific domains.
  SciRIFF covers five essential scientific literature understanding capabilities:
  information extraction, summarization, question answering, claim verification, and
  classification.'
---

# SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific Literature

## Quick Facts
- arXiv ID: 2406.07835
- Source URL: https://arxiv.org/abs/2406.07835
- Reference count: 36
- Models finetuned on SciRIFF achieve 70.6% average improvement on scientific tasks while maintaining general instruction-following within 2% of baseline

## Executive Summary
This paper introduces SciRIFF, a comprehensive dataset of 137K instruction-following demonstrations across 54 tasks in five scientific domains. SciRIFF covers information extraction, summarization, question answering, claim verification, and classification tasks, making it unique as an entirely expert-written resource with long input contexts and structured outputs. The authors demonstrate its utility by finetuning large language models (SciTulu) using a mix of general-domain and SciRIFF instructions, achieving significant improvements on scientific literature understanding tasks while maintaining general capabilities.

## Method Summary
The authors create SciRIFF by curating existing human-annotated scientific datasets and converting them into instruction-following demonstrations using expert-written templates. They finetune pre-trained models (Llama 2 and Tulu V2) using supervised learning on a mix of general-domain instructions (from Tulu V2 Mix) and SciRIFF demonstrations, with 1000 examples per SciRIFF task. The finetuned models are evaluated on 9 held-out tasks using multiple metrics including exact match F1, BLEU, and LLM judge similarity.

## Key Results
- Models finetuned on SciRIFF achieve 70.6% average improvement over baselines trained only on general-domain instructions
- SciTulu improves performance by 28.1% at 7B scale and 6.5% at 70B scale on scientific tasks
- Maintains general instruction-following capabilities within 2% of baseline while gaining scientific literature understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finetuning on SciRIFF provides domain-specific knowledge that improves scientific instruction-following while maintaining general capabilities through mixed-data training.
- Mechanism: By starting from a general instruction-following model (Tulu V2) and performing additional finetuning on a mix of general-domain and SciRIFF instructions, the model gains scientific literature understanding capabilities without catastrophic forgetting of general abilities.
- Core assumption: The mixed training data prevents domain drift and maintains general instruction-following performance within 2% of baseline.
- Evidence anchors: [abstract], [section 2.1]

### Mechanism 2
- Claim: The structured output format in SciRIFF enables better evaluation and aggregation of scientific literature understanding results.
- Mechanism: By using JSON as the common output format for all structured tasks, SciRIFF facilitates consistent evaluation and matches industry trends for requesting JSON model outputs.
- Core assumption: Structured outputs improve both evaluation consistency and downstream utility for scientific workflows.
- Evidence anchors: [abstract], [section 2.1]

### Mechanism 3
- Claim: The sample-efficient training strategy using downsampled SciRIFF data achieves comparable performance to full training while reducing computational costs.
- Mechanism: By starting from Tulu V2 and performing additional finetuning on a downsampled mix of SciRIFF and general instructions, the model achieves similar performance to training from scratch on all available data.
- Core assumption: The downsampling strategy preserves the most informative examples while reducing training costs.
- Evidence anchors: [abstract], [section 3.2]

## Foundational Learning

- Concept: Supervised finetuning on instruction-following datasets
  - Why needed here: The paper relies on finetuning existing models on instruction datasets to improve their scientific literature understanding capabilities
  - Quick check question: What is the primary difference between pretraining and supervised finetuning in the context of language models?

- Concept: Catastrophic forgetting in sequential learning
  - Why needed here: The paper addresses this by mixing general and domain-specific instructions during finetuning to maintain general capabilities
  - Quick check question: How does mixing general-domain instructions with SciRIFF during finetuning prevent catastrophic forgetting?

- Concept: Structured output formats for evaluation consistency
  - Why needed here: The paper uses JSON output format for all structured tasks to enable consistent evaluation and downstream utility
  - Quick check question: Why is using a consistent output format (JSON) beneficial for evaluating instruction-following models?

## Architecture Onboarding

- Component map: Instruction templates (Jinja format) -> Input context processing (long context handling) -> Output format validation (JSON structure) -> Evaluation metrics (task-specific metrics) -> Mixed-data training pipeline (general + SciRIFF)
- Critical path: Data preparation -> Model finetuning -> Evaluation -> Deployment
- Design tradeoffs: Longer context windows enable more comprehensive scientific understanding but increase computational costs
- Failure signatures: Poor performance on held-out tasks, loss of general instruction-following capabilities, or inconsistent output formatting
- First 3 experiments:
  1. Verify JSON output format validation on sample tasks
  2. Test long context handling with truncated examples
  3. Evaluate catastrophic forgetting by comparing performance on general tasks before/after finetuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of models trained on synthetic scientific data compare to those trained on expert-written demonstrations like SciRIFF?
- Basis in paper: [inferred] The paper discusses that existing instruction-following resources for scientific literature are limited, and they chose to create SciRIFF from existing human-annotated datasets rather than generating synthetic data.
- Why unresolved: The paper explicitly states that given the lack of existing evaluation benchmarks for scientific literature understanding, it may be difficult to assess the utility of synthetic data generation approaches.
- What evidence would resolve it: Direct comparison studies training models on equivalent amounts of high-quality synthetic scientific data versus SciRIFF, evaluated on the same held-out tasks in SciRIFF-Eval.

### Open Question 2
- Question: What is the optimal balance between general instruction-following data and domain-specific scientific data for maximizing performance across both domains?
- Basis in paper: [explicit] The paper performs ablations examining the effects of different training data compositions, finding that models trained only on SciRIFF perform well on science evaluations but struggle at general instruction-following.
- Why unresolved: While the paper identifies that a mixed approach works well, it doesn't determine the optimal ratio of general to scientific data, or whether this ratio should vary based on model size or target application.
- What evidence would resolve it: Systematic experiments varying the ratio of general to scientific data across different model scales, measuring performance trade-offs on both scientific and general tasks to identify optimal mixing strategies.

### Open Question 3
- Question: How can evaluation of structured outputs be improved to better capture semantic equivalence when surface forms differ from references?
- Basis in paper: [explicit] The paper notes that GPT-4 performance on SciRIFF-Eval is fairly low partly due to "evaluation challenges," specifically mentioning that current evaluations are unreliable because "the predicted surface form does not match the reference, but the underlying meaning is the same."
- Why unresolved: The paper acknowledges this is a significant challenge but doesn't provide solutions or quantify the extent of this problem.
- What evidence would resolve it: Comparative studies using different evaluation approaches (exact match, semantic similarity, LLM-based evaluation) on the same predictions to measure correlation with human judgments.

## Limitations

- Evaluation scope limited to only 9 held-out tasks from the 54-task dataset
- Resource-intensive creation process requiring expert annotators for instruction templates
- Reliance on LLM-based evaluation introduces potential biases and variability

## Confidence

- **High Confidence**: Core claims about dataset size (137K demonstrations) and task coverage (54 tasks across 5 domains) are well-supported
- **Medium Confidence**: Performance improvements (70.6% average on held-out tasks) are credible but limited by evaluation scope
- **Low Confidence**: Claim about maintaining general capabilities within 2% of baseline based on specific mix ratio that may not generalize

## Next Checks

1. Test the finetuned models on a broader set of held-out tasks from SciRIFF to verify that the 70.6% average improvement holds across all 54 tasks, not just the 9 evaluated

2. Conduct ablation studies by varying the ratio of general-domain to SciRIFF instructions in the finetuning mix to determine the minimum general instruction proportion needed to maintain capabilities within 2% of baseline

3. Compare the computational costs of finetuning on full SciRIFF data versus the downsampled approach to quantify the actual savings and verify that performance remains comparable