---
ver: rpa2
title: Magnetic Hysteresis Modeling with Neural Operators
arxiv_id: '2407.03261'
source_url: https://arxiv.org/abs/2407.03261
tags:
- neural
- hysteresis
- magnetic
- fields
- operator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Neural operators\u2014DeepONet, Fourier neural operator (FNO),\
  \ and a newly proposed rate-independent FNO\u2014are employed to model magnetic\
  \ hysteresis by learning mappings between magnetic fields, overcoming limitations\
  \ of traditional recurrent architectures that fail to generalize to novel input\
  \ fields. The operators predict first-order reversal curves and minor loops not\
  \ seen during training, outperforming RNN, LSTM, and GRU on multiple metrics."
---

# Magnetic Hysteresis Modeling with Neural Operators

## Quick Facts
- arXiv ID: 2407.03261
- Source URL: https://arxiv.org/abs/2407.03261
- Reference count: 36
- Neural operators (DeepONet, FNO, RIFNO) model magnetic hysteresis by learning mappings between magnetic fields, outperforming RNN/LSTM/GRU on generalization to novel input fields

## Executive Summary
This work addresses the challenge of modeling magnetic hysteresis in materials, where traditional recurrent neural networks fail to generalize to new input magnetic field patterns. The authors propose using neural operators—specifically DeepONet, Fourier neural operator (FNO), and a newly introduced rate-independent FNO (RIFNO)—to learn mappings between magnetic fields directly. These operators demonstrate superior generalization, accurately predicting first-order reversal curves and minor loops not seen during training. The approach enables real-time, generalizable predictions for magnetic material-based device modeling.

## Method Summary
The authors compare neural operators against traditional recurrent architectures for modeling magnetic hysteresis. They employ DeepONet, FNO, and a newly proposed rate-independent FNO to learn mappings between input and output magnetic fields, capturing the complex nonlinear behavior of hysteresis loops. The models are trained on datasets containing major loops, first-order reversal curves (FORC), and minor loops. The rate-independent FNO is specifically designed to maintain consistent performance across varying sampling rates, reflecting the inherent rate-independent nature of magnetic hysteresis. Model performance is evaluated using relative error metrics on unseen test data.

## Key Results
- FNO achieves lowest FORC prediction error: 1.34e-3 relative error
- RIFNO excels at minor loop prediction: 1.26e-2 relative error
- RIFNO maintains consistent performance across varying sampling rates
- All neural operators outperform RNN, LSTM, and GRU architectures

## Why This Works (Mechanism)
Neural operators overcome the fundamental limitation of recurrent architectures by learning mappings between entire input and output fields rather than sequential time steps. This field-to-field learning approach captures the global structure of magnetic hysteresis without being constrained by the temporal ordering of the input sequence. The Fourier neural operator leverages spectral methods to efficiently represent these field mappings in frequency space, while the rate-independent variant explicitly accounts for the material's intrinsic rate-independent behavior.

## Foundational Learning
- **Magnetic hysteresis**: The nonlinear magnetic response of materials to applied fields, creating history-dependent loops
  - Why needed: Core phenomenon being modeled; understanding loop shapes and reversal curves
  - Quick check: Can identify major loops, FORC, and minor loops in BH curves

- **Recurrent neural networks**: Sequential models processing time-ordered data point by point
  - Why needed: Current standard approach being outperformed
  - Quick check: Can implement LSTM/GRU for time series prediction

- **Neural operators**: Function-to-function mappings learning relationships between entire fields
  - Why needed: Enable generalization to unseen input patterns
  - Quick check: Can distinguish between point-wise and field-wise learning approaches

## Architecture Onboarding

**Component map**: Input field → Neural Operator → Output field mapping

**Critical path**: Field encoding → Spectral/spatial transformation → Field decoding → Prediction

**Design tradeoffs**: Neural operators trade sequential processing efficiency for superior generalization to novel field patterns; RIFNO adds complexity to capture rate-independence

**Failure signatures**: RNN/LSTM/GRU fail on novel input patterns; standard FNO may show rate-dependent errors; DeepONet may have higher computational overhead

**3 first experiments**:
1. Train baseline RNN/LSTM/GRU on major loops, test on FORC sequences
2. Implement FNO on same dataset, compare generalization to unseen field patterns
3. Test RIFNO across multiple sampling rates to verify rate-independence

## Open Questions the Paper Calls Out
None

## Limitations
- Limited comparison to RNN/LSTM/GRU only on fixed-rate datasets
- Rate-independence demonstrated only within narrow frequency range
- "Real-time" prediction claim lacks quantitative validation
- Single-material focus without cross-material generalization testing

## Confidence
- FNO accuracy improvement (1.34e-3 FORC error): High
- Generalization to novel input fields: Medium
- Rate-independent property demonstration: Low

## Next Checks
1. Benchmark inference speed and memory usage for real-time device modeling applications
2. Test generalization across multiple magnetic materials and geometries
3. Validate rate-independence across a broader range of excitation frequencies (orders of magnitude difference)