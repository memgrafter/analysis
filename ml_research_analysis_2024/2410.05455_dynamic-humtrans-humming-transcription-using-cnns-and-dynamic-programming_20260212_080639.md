---
ver: rpa2
title: 'Dynamic HumTrans: Humming Transcription Using CNNs and Dynamic Programming'
arxiv_id: '2410.05455'
source_url: https://arxiv.org/abs/2410.05455
tags:
- transcription
- note
- onsets
- dataset
- offsets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a CNN-based architecture combined with dynamic
  programming post-processing for humming transcription, using the HumTrans dataset.
  It addresses issues with ground truth onset/offset annotations by developing heuristics
  to create more accurate labels.
---

# Dynamic HumTrans: Humming Transcription Using CNNs and Dynamic Programming

## Quick Facts
- arXiv ID: 2410.05455
- Source URL: https://arxiv.org/abs/2410.05455
- Authors: Shubham Gupta; Isaac Neri Gomez-Sarmiento; Faez Amjed Mezdari; Mirco Ravanelli; Cem Subakan
- Reference count: 40
- Primary result: CNN-based architecture with dynamic programming post-processing achieves state-of-the-art humming transcription F1-scores of 0.673 (octave-invariant) and 0.651 (octave-aware)

## Executive Summary
This paper addresses the challenging task of humming transcription by proposing a CNN-based architecture combined with dynamic programming post-processing. The authors tackle the problem of inaccurate ground truth annotations in the HumTrans dataset by developing heuristics to create more accurate labels. Their model uses CQT representation with harmonic stacking and is trained to predict notes directly, with dynamic programming employed during inference to enforce realistic constraints and clean up note predictions. The approach significantly outperforms existing methods, achieving state-of-the-art performance in both octave-invariant and octave-aware note transcription tasks.

## Method Summary
The method employs a CNN-based architecture that takes CQT representation with harmonic stacking as input and predicts note onsets and offsets directly. During inference, dynamic programming post-processing is applied to enforce musically plausible constraints on the predictions, such as minimum note durations and realistic pitch transitions. The authors also address annotation quality issues in the HumTrans dataset by developing heuristic algorithms to correct onset and offset labels, creating a more reliable evaluation benchmark. The model is trained on the cleaned dataset and evaluated using standard transcription metrics including precision, recall, and F1-scores.

## Key Results
- Achieves F1-scores of 0.673 (octave-invariant) and 0.651 (octave-aware) for note + onset predictions
- Outperforms existing humming transcription methods on the HumTrans dataset
- Dynamic programming post-processing improves transcription accuracy by enforcing realistic musical constraints
- Provides corrected annotations for the HumTrans dataset, enabling more reliable future research

## Why This Works (Mechanism)
The approach works by combining strong feature representation (CQT with harmonic stacking) with temporal consistency enforcement through dynamic programming. The CNN learns to identify note events from the spectral representation, while the dynamic programming step ensures that the predicted sequence of notes follows musically plausible patterns. This two-stage approach addresses both the local feature detection problem and the global temporal coherence problem in humming transcription. The harmonic stacking helps the model capture the rich harmonic content in humming, while the post-processing step corrects isolated prediction errors that would otherwise break musical continuity.

## Foundational Learning
- **Constant-Q Transform (CQT)**: A time-frequency representation that provides logarithmically spaced frequency bins, better suited for musical signals than linear-frequency transforms like STFT. Why needed: Musical notes are logarithmically spaced in frequency, making CQT more appropriate for pitch detection tasks.
- **Dynamic Programming for Sequence Optimization**: An algorithmic technique for finding optimal solutions to problems with overlapping subproblems by breaking them down into simpler subproblems. Why needed: To enforce temporal constraints and correct isolated prediction errors while maintaining global coherence in the transcribed sequence.
- **Harmonic Stacking**: A preprocessing technique that adds harmonic information to the spectral representation to enhance pitch detection. Why needed: Humming contains rich harmonic content that can help distinguish between notes, especially in ambiguous frequency regions.
- **Note Onset/Offset Detection**: The task of identifying when musical notes begin and end in a continuous audio signal. Why needed: Accurate transcription requires precise temporal localization of notes, not just pitch identification.
- **Octave-Invariant vs Octave-Aware Evaluation**: Two evaluation paradigms where octave-invariant considers pitch classes regardless of octave, while octave-aware requires exact pitch matching. Why needed: Different applications may require different levels of pitch precision, and octave errors are common in pitch detection systems.

## Architecture Onboarding

Component Map:
Raw Audio -> CQT + Harmonic Stacking -> CNN Feature Extraction -> Note Prediction -> Dynamic Programming Post-processing -> Final Transcription

Critical Path:
CQT representation with harmonic stacking serves as the critical feature extraction step. The quality of this representation directly impacts the CNN's ability to learn note patterns, making it essential for the overall system performance.

Design Tradeoffs:
The choice between octave-invariant and octave-aware evaluation represents a key tradeoff between robustness and precision. Octave-invariant evaluation is more forgiving and may be preferable for applications where exact octave is less critical, while octave-aware evaluation provides stricter assessment of pitch accuracy. The harmonic stacking adds computational overhead but significantly improves pitch detection accuracy in the presence of complex harmonic content.

Failure Signatures:
- Poor performance on low-quality humming recordings due to noise sensitivity in CQT representation
- Octave errors when harmonic stacking fails to properly emphasize the fundamental frequency
- Temporal misalignment between predicted and ground truth onsets/offsets when dynamic programming constraints are too strict

First Experiments:
1. Test CQT representation quality by comparing note detection accuracy with and without harmonic stacking on a subset of the dataset
2. Evaluate dynamic programming post-processing impact by comparing transcription F1-scores with and without post-processing on a validation set
3. Assess octave error rates by analyzing pitch prediction accuracy across different vocal ranges and humming styles

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the dynamic programming post-processing method perform when applied to polyphonic humming transcription instead of monophonic?
- Basis in paper: The paper mentions that a natural extension of this work is to transcribe polyphonic humming samples and that they would like to explore using this postprocessing in other transcription problems.
- Why unresolved: The current method was only evaluated on monophonic humming data. Polyphonic transcription introduces additional complexity that may affect the effectiveness of the dynamic programming approach.
- What evidence would resolve it: Experimental results showing performance metrics (precision, recall, F1-score) when applying the same dynamic programming post-processing to polyphonic humming transcription tasks.

### Open Question 2
- Question: What is the optimal threshold value for the waveform envelope algorithm across different humming samples and singers?
- Basis in paper: The paper describes a heuristic algorithm for onset/offset detection that uses threshold values, but notes that they found "a very tight hugging envelope when we take a min of these two values" without specifying how to choose the threshold.
- Why unresolved: The method uses a fixed threshold calculation (mw_min + thresholds * (mw_max - mw_min)) without explaining how to select the optimal threshold parameter for different audio characteristics.
- What evidence would resolve it: A systematic evaluation showing how different threshold values affect transcription accuracy across various singers and recording conditions.

### Open Question 3
- Question: Can the dynamic programming post-processing be integrated into the training process as a differentiable loss function?
- Basis in paper: The paper states "It is also possible to use this postprocessing as a part of the loss function during training thus enabling better transcriptions from the get go."
- Why unresolved: The paper only mentions this as a possibility but does not explore or implement it, leaving the question of feasibility and effectiveness unanswered.
- What evidence would resolve it: Experimental results comparing models trained with and without the dynamic programming constraints incorporated into the loss function, showing whether this improves transcription accuracy.

## Limitations
- The heuristic approach for correcting ground truth annotations introduces potential subjectivity and may not generalize to all annotation errors
- The method's reliance on CQT representation with harmonic stacking may limit performance on humming recordings with unusual timbral characteristics
- Evaluation focuses primarily on F1-scores without extensive analysis of rhythm accuracy or performance across diverse humming styles

## Confidence
- Architecture Design and Implementation: High
- Dataset Curation and Annotation Corrections: Medium
- Performance Claims and State-of-the-Art Status: High
- Generalizability to Diverse Humming Styles: Low

## Next Checks
1. Conduct ablation studies to quantify the impact of each component (CQT representation, harmonic stacking, dynamic programming post-processing) on overall performance.

2. Test the model's robustness across different humming styles, vocal ranges, and recording qualities to assess generalizability beyond the HumTrans dataset.

3. Implement cross-dataset validation by evaluating the model on publicly available singing voice transcription datasets to verify real-world applicability.