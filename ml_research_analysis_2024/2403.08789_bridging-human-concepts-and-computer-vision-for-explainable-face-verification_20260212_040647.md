---
ver: rpa2
title: Bridging Human Concepts and Computer Vision for Explainable Face Verification
arxiv_id: '2403.08789'
source_url: https://arxiv.org/abs/2403.08789
tags:
- face
- semantic
- images
- similarity
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of making face verification
  systems more interpretable by bridging human and computer vision approaches. The
  authors propose a method that segments faces into human-semantic regions using Mediapipe,
  then analyzes how these regions contribute to face verification decisions through
  perturbation techniques.
---

# Bridging Human Concepts and Computer Vision for Explainable Face Verification

## Quick Facts
- **arXiv ID:** 2403.08789
- **Source URL:** https://arxiv.org/abs/2403.08789
- **Reference count:** 40
- **Key outcome:** Method that segments faces into human-semantic regions and analyzes their contribution to face verification decisions through perturbation techniques, producing interpretable similarity maps

## Executive Summary
This paper addresses the challenge of making face verification systems more interpretable by bridging human and computer vision approaches. The authors propose a method that segments faces into human-semantic regions using Mediapipe, then analyzes how these regions contribute to face verification decisions through perturbation techniques. They adapt model-agnostic algorithms to generate similarity maps showing which facial regions are considered similar or dissimilar during face comparison tasks. The method combines both single and collaborative feature importance assessments to better align with human face perception processes.

## Method Summary
The approach segments faces into 13 human-semantic regions using Mediapipe landmarks, then applies KernelSHAP to extract feature importance scores for each region. Three perturbation algorithms (single removal, greedy removal, and average similarity map) systematically remove or modify these regions to assess their impact on verification similarity scores. The method is model-agnostic and works with any pre-trained face verification model, generating interpretable similarity maps that visualize which facial areas contribute most to verification decisions.

## Key Results
- The approach successfully identifies meaningful facial regions and their contributions to verification decisions
- Visualizations produced are more easily interpretable by humans compared to traditional saliency maps
- Incorporating human-semantic concepts improves interpretability while maintaining faithfulness to the model's actual reasoning process
- The method works across different face verification architectures (CasiaNet, VGGFace2) and demonstrates consistent behavior

## Why This Works (Mechanism)

### Mechanism 1
Semantic perturbation using Mediapipe-defined facial regions produces similarity maps that align with human face perception. The method segments faces into human-semantic regions, then perturbs these regions to assess their contribution to verification decisions. By analyzing how removing or modifying these semantic regions affects similarity scores, the system creates interpretable maps showing which areas humans would naturally consider similar or dissimilar. Core assumption: Human-semantic face regions correspond to regions that machine learning models actually use for face verification.

### Mechanism 2
Model-agnostic perturbation techniques can reveal feature importance for face verification without requiring access to model internals. By systematically removing semantic regions and measuring the impact on similarity scores, the approach identifies which facial areas contribute most to verification decisions. The single removal and greedy removal algorithms assess both individual and collaborative contributions of facial features. Core assumption: Feature importance can be accurately measured through perturbation without needing gradient information or model access.

### Mechanism 3
Combining human semantic concepts with model importance scores creates more interpretable explanations than either approach alone. The method first uses KernelSHAP to extract model importance scores for each semantic region, then ranks these regions by their global importance across multiple images. This creates a prioritized list of concepts that are both human-understandable and model-relevant. Core assumption: The combination of human semantics and model importance provides better interpretability than pure model-based or pure human-based approaches.

## Foundational Learning

- **Concept:** Face verification using cosine similarity between feature vectors
  - Why needed here: The entire explanation framework builds on understanding how face verification systems make decisions through feature comparison
  - Quick check question: What does a cosine similarity score of 1 indicate about two face images?

- **Concept:** Saliency maps and their limitations in face verification
  - Why needed here: The paper positions its approach as an improvement over traditional saliency maps, so understanding their limitations is crucial
  - Quick check question: Why might traditional saliency maps be inadequate for explaining face verification decisions?

- **Concept:** Shapley values and feature importance in machine learning
  - Why needed here: The KernelSHAP algorithm uses Shapley values to determine the importance of different facial regions, which is central to the concept extraction process
  - Quick check question: How do Shapley values differ from simple feature importance scores in terms of capturing feature contributions?

## Architecture Onboarding

- **Component map:** Mediapipe face segmentation → KernelSHAP feature importance extraction → Semantic perturbation algorithms (single/greedy removal) → Similarity map generation → Visualization
- **Critical path:** Face segmentation → Concept extraction → Perturbation → Map generation
- **Design tradeoffs:** Using human semantic regions provides interpretability but may not align with model features; perturbation is model-agnostic but may miss complex interactions
- **Failure signatures:** Similarity maps that don't correlate with human intuition, inconsistent importance rankings across different images, poor performance on non-frontal faces
- **First 3 experiments:**
  1. Test single removal algorithm on a simple face verification model with known feature importance
  2. Compare similarity maps generated with different masking strategies (black, white, noise)
  3. Validate concept extraction by comparing human-annotated important regions with model-extracted rankings

## Open Questions the Paper Calls Out

### Open Question 1
How does the perturbation algorithm perform when applied to diverse face orientations and profiles, and what are the implications for its robustness? The paper mentions that Mediapipe's face detection is sensitive to variations in facial orientation, but does not provide experimental data on diverse face orientations. What evidence would resolve it: Conducting experiments with a dataset containing faces with varying orientations and profiles, and analyzing the algorithm's performance and output consistency.

### Open Question 2
What are the potential biases introduced by incorporating human-based semantics in the explanation process, and how can they be mitigated? The paper states that incorporating human-based semantics can introduce human bias to the explanations but does not explore the nature or extent of these biases. What evidence would resolve it: A study comparing the biases in explanations generated with and without human-based semantics, along with proposed strategies for bias mitigation.

### Open Question 3
How does the choice of masking technique (e.g., black, white, random noise) affect the interpretability and accuracy of the similarity maps? The paper investigates the impact of different masking types but does not delve into the reasons behind the sensitivity differences. What evidence would resolve it: A detailed analysis of how each masking technique influences the algorithm's perception of facial features and the resulting similarity maps, including user studies to assess interpretability.

## Limitations

- The reliance on Mediapipe-defined semantic regions may not align with how deep learning models actually process facial features
- Perturbation-based approach cannot capture complex feature interactions or hierarchical feature representations that modern face verification systems likely employ
- Validation methods may not comprehensively test the approach's effectiveness across diverse real-world scenarios

## Confidence

- **High confidence:** The technical implementation of semantic perturbation and similarity map generation is well-defined and reproducible
- **Medium confidence:** The claim that combining human semantics with model importance improves interpretability is plausible but lacks strong empirical validation
- **Low confidence:** The assertion that the approach successfully bridges human and computer vision perspectives is largely based on qualitative visualizations rather than rigorous quantitative comparisons

## Next Checks

1. **Cross-model validation:** Test the semantic perturbation approach on multiple face verification architectures (not just CasiaNet and VGGFace2) to verify consistency of similarity maps across different model types
2. **Human study replication:** Conduct controlled user studies where participants attempt to predict verification outcomes using both traditional saliency maps and the proposed similarity maps to measure actual interpretability gains
3. **Feature alignment analysis:** Perform ablation studies to quantify the correlation between Mediapipe semantic regions and the actual feature activations learned by the verification models, establishing whether the human semantic framework meaningfully captures model behavior