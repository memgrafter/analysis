---
ver: rpa2
title: 'MoE-CT: A Novel Approach For Large Language Models Training With Resistance
  To Catastrophic Forgetting'
arxiv_id: '2407.00875'
source_url: https://arxiv.org/abs/2407.00875
tags:
- language
- multilingual
- training
- data
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in multilingual language
  models by proposing a Mixture-of-Experts Continual Training (MoE-CT) framework that
  preserves original language capabilities while enhancing low-resource language performance.
  The approach freezes base model parameters and adds an MoE module for multilingual
  training, using a fusion mechanism to combine knowledge from both components.
---

# MoE-CT: A Novel Approach For Large Language Models Training With Resistance To Catastrophic Forgetting

## Quick Facts
- arXiv ID: 2407.00875
- Source URL: https://arxiv.org/abs/2407.00875
- Reference count: 19
- Key outcome: MoE-CT improves multilingual performance while maintaining original language capabilities with only 1/5 the data of standard CT

## Executive Summary
This paper addresses catastrophic forgetting in multilingual language models by proposing a Mixture-of-Experts Continual Training (MoE-CT) framework. The approach freezes base model parameters while adding an MoE module for multilingual training, using a fusion mechanism to combine knowledge from both components. Experiments on Qwen-1.8B and Qwen-7B models demonstrate significant improvements in multilingual tasks while maintaining or enhancing performance on original languages, with MoE-CT outperforming conventional CT and LoRA methods.

## Method Summary
MoE-CT extends a pre-trained LLM by adding an MoE layer with trainable experts and embeddings while freezing all original model parameters. The method employs a fusion mechanism that combines outputs from both the frozen base model and the MoE module through learned weights. The model is trained exclusively on the MoE experts and embeddings using multilingual datasets, requiring only 1/5 the amount of original language data compared to standard continual training while achieving better multilingual capability enhancement and resistance to forgetting.

## Key Results
- MoE-CT significantly improves multilingual understanding on XCOPA, XNLI, and PAWS-X benchmarks
- The method maintains or enhances performance on original languages while improving low-resource language proficiency
- MoE-CT outperforms conventional CT and LoRA methods in both multilingual capability enhancement and resistance to forgetting
- Only 4 billion tokens of Chinese and English data are required to achieve resistance to forgetting in these languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Freezing the original LLM parameters prevents catastrophic forgetting of high-resource language capabilities during multilingual training.
- Mechanism: By keeping the base model's parameters frozen, the knowledge acquired during pretraining on high-resource languages remains intact and is not overwritten by the multilingual training process.
- Core assumption: The frozen parameters contain sufficient representation of the original language knowledge to maintain performance.
- Evidence anchors:
  - [abstract] "Our design freezes the original LLM parameters, thus safeguarding its performance in high-resource languages"
  - [section 3.3] "we exclusively train the expert networks and the embedding layer within the expanded MoE model architecture, while all other structural parameters and shared-FFN parameters are kept fixed"
- Break condition: If the frozen parameters become suboptimal for the multilingual tasks, or if the MoE module cannot adequately compensate for the frozen knowledge, performance degradation may occur.

### Mechanism 2
- Claim: The MoE module dynamically combines new multilingual knowledge with frozen original knowledge through a fusion mechanism.
- Mechanism: The MoE layer learns to process multilingual inputs while the frozen shared feed-forward network (shared-ffn) preserves original language knowledge. A fusion module combines their outputs using learned weights.
- Core assumption: The fusion mechanism can effectively balance contributions from both knowledge sources.
- Evidence anchors:
  - [abstract] "while an appended MoE module, trained on diverse language datasets, augments low-resource language proficiency"
  - [section 3.3] "we employed a frozen shared feed-forward network (shared-ffn) to preserve the original knowledge, and implemented a gating mechanism to dynamically merge the original knowledge with the newly acquired knowledge from the expert networks"
- Break condition: If the fusion weights become imbalanced, either the original knowledge or the new multilingual knowledge may dominate excessively, leading to performance degradation.

### Mechanism 3
- Claim: Selective training of only the MoE parameters and embedding layer allows efficient multilingual knowledge acquisition without requiring extensive original language data.
- Mechanism: By training only the MoE experts and embeddings while keeping the rest frozen, the model can learn multilingual capabilities with minimal original language data, reducing training costs.
- Core assumption: The MoE experts can effectively learn multilingual patterns without extensive retraining of the base model.
- Evidence anchors:
  - [abstract] "Our approach requires only 1/5 the amount of original language data compared to standard CT while achieving better multilingual capability enhancement"
  - [section 3.3] "we exclusively train the expert networks and the embedding layer within the expanded MoE model architecture"
- Break condition: If the MoE experts cannot adequately learn multilingual patterns with the limited training data, or if the frozen base model lacks sufficient generalization, performance may suffer.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding how neural networks lose previously learned knowledge when trained on new tasks is fundamental to grasping the problem MoE-CT addresses.
  - Quick check question: What happens to a neural network's performance on task A when it is fine-tuned on task B without any mitigation strategy?

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: The MoE module is the core component that enables multilingual capability expansion without affecting the base model.
  - Quick check question: How does a gating mechanism in MoE determine which expert to activate for a given input?

- Concept: Parameter-efficient fine-tuning
  - Why needed here: MoE-CT is a form of parameter-efficient fine-tuning that preserves base model parameters while adding new capabilities.
  - Quick check question: What are the trade-offs between freezing base parameters versus updating them during fine-tuning?

## Architecture Onboarding

- Component map:
  Base LLM (frozen) -> MoE layer (trainable) -> Shared FFN (frozen) -> Fusion module (trainable) -> Embedding layer (trainable) -> Final prediction

- Critical path:
  1. Input passes through frozen base LLM layers
  2. MoE layer processes input through activated experts
  3. Shared FFN provides original knowledge context
  4. Fusion module combines outputs
  5. Final prediction generated

- Design tradeoffs:
  - Freezing base model preserves original capabilities but limits adaptation
  - Training only MoE experts reduces parameter count but may limit fine-grained control
  - Fusion mechanism complexity vs. performance gain
  - Number of experts vs. computational cost and potential overfitting

- Failure signatures:
  - Performance degradation on original languages: Indicates insufficient preservation of frozen knowledge
  - Poor multilingual performance: Suggests MoE experts are not learning effectively
  - Imbalance in fusion: May show as inconsistent performance across different language types
  - Overfitting to training data: Could manifest as poor generalization to unseen languages

- First 3 experiments:
  1. Baseline test: Evaluate frozen base model on original and multilingual tasks to establish performance levels
  2. MoE training: Train only MoE experts on multilingual data, evaluate performance gains and original language preservation
  3. Fusion optimization: Experiment with different fusion weight configurations to find optimal balance between original and multilingual performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio between original language data and multilingual data for MoE-CT to achieve the best balance between resistance to catastrophic forgetting and multilingual capability enhancement?
- Basis in paper: [explicit] The paper discusses the data ratio problem, noting that conventional CT requires at least five times more original language data than multilingual data to prevent catastrophic forgetting, while MoE-CT only needs one-fifth the proportion of original language data.
- Why unresolved: The paper only provides a comparison between the conventional CT ratio (5:1) and the MoE-CT ratio (1:5), but does not explore the full spectrum of possible ratios or determine the optimal point for maximizing both resistance to forgetting and multilingual capability.
- What evidence would resolve it: Systematic experiments testing various ratios between original and multilingual data (e.g., 10:1, 5:1, 3:1, 2:1, 1:1, 1:2, 1:3, 1:5, 1:10) on MoE-CT would identify the point where multilingual capability enhancement is maximized while maintaining resistance to catastrophic forgetting.

### Open Question 2
- Question: How does the performance of MoE-CT scale with model size, and what are the theoretical limits of its effectiveness on extremely large language models?
- Basis in paper: [explicit] The paper tests MoE-CT on Qwen-1.8B and Qwen-7B models, demonstrating effectiveness at these scales, but does not explore larger models or provide theoretical analysis of scaling limits.
- Why unresolved: The experiments are limited to relatively small models (1.8B and 7B parameters), and the paper does not address how MoE-CT would perform on much larger models (e.g., 100B+ parameters) or discuss theoretical constraints on scalability.
- What evidence would resolve it: Experiments applying MoE-CT to models of varying scales (e.g., 10B, 30B, 70B, 175B parameters) and theoretical analysis of computational complexity, parameter efficiency, and convergence behavior would clarify scalability limits.

### Open Question 3
- Question: What is the optimal number of experts per layer for MoE-CT across different model sizes and multilingual tasks?
- Basis in paper: [explicit] The paper tests 2, 4, and 8 experts on Qwen-7B models and finds that increasing experts does not significantly improve multilingual abilities, hypothesizing insufficient training data as the limiting factor.
- Why unresolved: The paper only tests a narrow range of expert counts (2-8) on one model size and concludes that more experts don't help, but doesn't systematically explore the relationship between expert count, model size, task complexity, and data volume.
- What evidence would resolve it: Comprehensive experiments varying expert counts (1, 2, 4, 8, 16, 32) across multiple model sizes (1.8B, 7B, 30B, 70B) with different amounts of training data and diverse multilingual tasks would identify optimal expert configurations for different scenarios.

## Limitations

- Evaluation relies on standard multilingual benchmarks that may not fully capture nuanced language understanding and knowledge retention aspects
- Training details for baseline models are not fully specified, making it difficult to assess whether the 1/5 data reduction claim is directly comparable
- No long-term assessment of whether the model maintains its resistance to forgetting over extended use or additional fine-tuning

## Confidence

- **High confidence**: The fundamental premise that freezing base model parameters prevents catastrophic forgetting of high-resource languages is well-established in the continual learning literature
- **Medium confidence**: The specific implementation of MoE-CT with fusion mechanisms and its claimed superiority over LoRA and conventional CT methods requires more detailed validation
- **Medium confidence**: The 1/5 data reduction claim needs careful examination to ensure fair comparison with baseline methods

## Next Checks

1. **Ablation study on fusion mechanism**: Systematically evaluate the impact of different fusion strategies (weighted sum, concatenation, gating) and routing configurations (top-k selection, router temperature) on both multilingual performance and resistance to forgetting

2. **Long-term stability assessment**: Evaluate model performance on original and multilingual tasks after extended periods of use or additional fine-tuning to confirm sustained resistance to catastrophic forgetting

3. **Comprehensive parameter efficiency analysis**: Compare the actual number of trainable parameters in MoE-CT versus LoRA and standard CT approaches to verify the claimed parameter efficiency benefits