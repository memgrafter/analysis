---
ver: rpa2
title: 'Guardians of Discourse: Evaluating LLMs on Multilingual Offensive Language
  Detection'
arxiv_id: '2410.15623'
source_url: https://arxiv.org/abs/2410.15623
tags:
- offensive
- language
- detection
- llms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates large language models (LLMs) for multilingual
  offensive language detection across English, Spanish, and German. It compares GPT-3.5,
  Flan-T5, and Mistral under monolingual, multilingual, and data-augmented settings.
---

# Guardians of Discourse: Evaluating LLMs on Multilingual Offensive Language Detection

## Quick Facts
- arXiv ID: 2410.15623
- Source URL: https://arxiv.org/abs/2410.15623
- Reference count: 35
- Key outcome: LLMs achieve comparable or better performance than previous state-of-the-art methods in multilingual offensive language detection, with multilingual fine-tuning and native-language prompts improving accuracy.

## Executive Summary
This study evaluates large language models (LLMs) for multilingual offensive language detection across English, Spanish, and German. The researchers compare GPT-3.5, Flan-T5, and Mistral under monolingual, multilingual, and data-augmented settings. Results show that multilingual fine-tuning significantly improves detection accuracy, particularly for smaller models like Flan-T5. Native-language prompts enhance model comprehension for non-English contexts, while translation-based data augmentation does not improve performance. The study also identifies model and dataset biases affecting predictions on sensitive topics like gender.

## Method Summary
The researchers fine-tuned three LLMs (GPT-3.5 via OpenAI API, Flan-T5, and Mistral via LLAFA-Factory with LoRA) using monolingual and multilingual data from three datasets: OLID + SOLID (English), OffendES (Spanish), and GermEval 2018 (German). Each English and Spanish dataset was balanced to 5,000 samples with a 1:1 offensive-to-non-offensive ratio, while German used the full training set. Models were evaluated using macro precision, recall, F1-score, and accuracy, with additional experiments using German prompts and translation data augmentation.

## Key Results
- Multilingual fine-tuning improves offensive language detection accuracy across all tested models
- Native-language prompts enhance model comprehension for non-English detection tasks
- Translation-based data augmentation does not improve performance and may introduce noise
- Model and dataset biases affect predictions on sensitive topics like gender and race

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual fine-tuning improves offensive language detection accuracy, especially for models with fewer parameters.
- Mechanism: Training on multiple languages allows the model to learn shared linguistic features and context patterns that generalize across languages, reducing the need for separate monolingual models.
- Core assumption: The model can effectively transfer knowledge from high-resource languages (e.g., English) to low-resource languages (e.g., German) when trained jointly.
- Evidence anchors:
  - [abstract]: "Multilingual fine-tuning improves detection accuracy, especially for Flan-T5."
  - [section]: "Multilingual fine-tuning LLMs enhances models' offensive language detection ability. Particularly, Flan-T5 benefits the most among all experimented models."
  - [corpus]: Weak evidence. Related papers discuss cross-lingual transfer but do not directly confirm fine-tuning gains for Flan-T5 specifically.
- Break condition: If the low-resource language has significantly different linguistic structure or cultural context, transfer may not occur effectively.

### Mechanism 2
- Claim: Using native-language prompts improves model comprehension for non-English detection tasks.
- Mechanism: Prompts in the target language align with the linguistic structure of the dataset, reducing translation ambiguity and improving contextual understanding.
- Core assumption: The model has sufficient multilingual pretraining to understand prompts in multiple languages.
- Evidence anchors:
  - [abstract]: "Using native-language prompts enhances model performance in non-English contexts."
  - [section]: "Utilising prompts in native languages enhance model comprehension in non-English contexts."
  - [corpus]: Weak evidence. Related papers discuss prompt design but not native-language prompt effects specifically.
- Break condition: If the model lacks sufficient exposure to the target language during pretraining, native prompts may not help.

### Mechanism 3
- Claim: Translation-based data augmentation does not improve performance for offensive language detection.
- Mechanism: Off-the-shelf translation tools may not preserve offensive content semantics, and adding translation data introduces noise rather than useful signal.
- Core assumption: The quality of translation data is insufficient to capture nuanced offensive language cues.
- Evidence anchors:
  - [abstract]: "Using native-language prompts enhances model performance in non-English contexts, while adding translation data does not."
  - [section]: "We investigated the usefulness of translation data... the results after adding translation data were lower than those originally achieved by the models."
  - [corpus]: Weak evidence. Related papers discuss data augmentation but not specifically translation data for offensive detection.
- Break condition: If high-quality, domain-specific translation data is available, augmentation might become beneficial.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding how knowledge from one language can be applied to another is crucial for multilingual model design.
  - Quick check question: Why might a model trained on English offensive language detection struggle with German data without multilingual fine-tuning?

- Concept: Data augmentation strategies
  - Why needed here: Knowing when and how to augment data is important for improving model performance without introducing noise.
  - Quick check question: What are the risks of using translation data as augmentation for tasks requiring nuanced language understanding?

- Concept: Model bias and fairness
  - Why needed here: Recognizing how biases in training data affect predictions, especially on sensitive topics like gender and race.
  - Quick check question: How might label-level correlations in training data lead to biased predictions in offensive language detection?

## Architecture Onboarding

- Component map: Input → Prompt Processing → Multilingual Fine-tuning → Detection → Output Classification
- Critical path: Data preprocessing → Model fine-tuning → Prompt application → Prediction evaluation
- Design tradeoffs: Larger models (GPT-3.5, Mistral) require more resources but perform better; smaller models (Flan-T5) are more efficient but benefit more from multilingual training.
- Failure signatures: Poor performance on low-resource languages; biased predictions on sensitive topics; content moderation blocks valid outputs.
- First 3 experiments:
  1. Fine-tune Flan-T5 on English data only, evaluate performance on English test set.
  2. Fine-tune Flan-T5 on multilingual data (English, Spanish, German), compare performance across all languages.
  3. Test native-language prompts vs English prompts on German dataset to measure comprehension improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM-based multilingual offensive language detection systems perform when evaluated on additional languages beyond English, Spanish, and German?
- Basis in paper: [explicit] The study focuses on three specific languages (English, Spanish, German) and concludes with a future work suggestion to experiment with more languages.
- Why unresolved: The current evaluation only covers three languages, limiting generalizability to other language families and resource levels.
- What evidence would resolve it: Performance comparisons of the same LLM models across a broader range of languages with varying resource availability.

### Open Question 2
- Question: What is the optimal strategy for incorporating translation data to improve multilingual offensive language detection performance?
- Basis in paper: [explicit] The study found that incorporating translation data (both translated English corpus and WMT20) did not improve performance, contradicting prior research.
- Why unresolved: The study only tested two specific approaches to translation data incorporation, leaving open the question of whether other strategies might be effective.
- What evidence would resolve it: Systematic evaluation of different translation data incorporation strategies, including timing, quantity, and quality of translated data.

### Open Question 3
- Question: How can inherent biases in LLMs and datasets related to sensitive topics be mitigated in offensive language detection?
- Basis in paper: [explicit] The study identifies model and dataset biases affecting predictions on sensitive topics like race, sexual orientation, and gender, but does not propose mitigation strategies.
- Why unresolved: The paper identifies the problem but does not explore solutions for bias mitigation in the context of offensive language detection.
- What evidence would resolve it: Comparative evaluation of bias mitigation techniques (e.g., data augmentation, model fine-tuning, adversarial debiasing) on offensive language detection performance and fairness metrics.

## Limitations
- Evaluation limited to only three languages, constraining generalizability across diverse language families
- Bias analysis focuses primarily on gender and racial stereotypes without examining intersectional biases
- Does not address safety concerns or real-world deployment challenges for content moderation systems

## Confidence
- High Confidence: Multilingual fine-tuning improves offensive language detection accuracy is well-supported by experimental results
- Medium Confidence: Native-language prompts enhance model performance has moderate support but limited testing
- Low Confidence: Translation-based data augmentation does not improve performance is based on limited experimentation

## Next Checks
1. Evaluate the multilingual fine-tuning approach on a broader set of language pairs spanning different language families to assess generalizability beyond Indo-European languages.
2. Conduct a systematic analysis of intersectional biases by creating test cases that combine multiple sensitive attributes to understand how models handle complex demographic combinations.
3. Design a controlled deployment experiment where models process live social media content under realistic operational constraints to identify practical limitations not apparent in controlled evaluation settings.