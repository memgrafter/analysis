---
ver: rpa2
title: Creating a Taxonomy for Retrieval Augmented Generation Applications
arxiv_id: '2408.02854'
source_url: https://arxiv.org/abs/2408.02854
tags:
- retrieval
- application
- taxonomy
- research
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops a taxonomy to systematically capture the key
  characteristics of retrieval augmented generation (RAG) applications. It defines
  five meta-dimensions and sixteen dimensions covering the general purpose, structural
  components, data modalities, evaluation approaches, and limitations of RAG systems.
---

# Creating a Taxonomy for Retrieval Augmented Generation Applications

## Quick Facts
- arXiv ID: 2408.02854
- Source URL: https://arxiv.org/abs/2408.02854
- Reference count: 28
- Key outcome: Taxonomy with five meta-dimensions and sixteen dimensions systematically captures RAG application characteristics

## Executive Summary
This work develops a comprehensive taxonomy to systematically capture the key characteristics of retrieval augmented generation (RAG) applications. The taxonomy defines five meta-dimensions covering general purpose, structural components, data modalities, evaluation approaches, and limitations, along with sixteen specific dimensions. Constructed through an iterative process informed by 28 papers and augmented by automated domain clustering using ChatGPT, the taxonomy provides a structured framework for understanding and comparing RAG applications across domains, modalities, and architectural choices. The authors validate the taxonomy's coverage and propose it as a foundation for future research and practical implementation of RAG systems.

## Method Summary
The authors developed a taxonomy through a four-iteration methodology based on Nickerson et al. (2013), combining conceptual-to-empirical and empirical-to-conceptual approaches. They conducted a systematic literature review using search terms like "RAG & application" on Google Scholar and ACL Anthology to identify 28 relevant papers. The iterative process involved initial conceptualization, paper analysis, refinement, and validation phases. To augment manual review, they applied automated domain clustering using ChatGPT on extracted paper titles, comparing the results to ensure comprehensive coverage of application domains. The final taxonomy comprises five meta-dimensions (General, Structure, Data, Evaluation, Limitations) and sixteen dimensions that capture various aspects of RAG applications.

## Key Results
- Five meta-dimensions (General, Structure, Data, Evaluation, Limitations) provide hierarchical framework for RAG taxonomy
- Sixteen dimensions systematically capture application purpose, architectural paradigms, data modalities, evaluation metrics, and limitations
- Automated domain clustering using ChatGPT identified application domains that complemented manual paper analysis
- Taxonomy validated through iterative refinement and shows potential for application in design science research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG applications can be comprehensively understood by structuring them along five meta-dimensions and sixteen dimensions.
- Mechanism: The taxonomy provides a hierarchical framework where each meta-dimension captures a distinct aspect of RAG systems, allowing systematic analysis and comparison across different implementations.
- Core assumption: The complexity of RAG applications can be reduced to a finite set of characteristics that are both exhaustive and mutually exclusive.
- Evidence anchors:
  - [abstract] "It defines five meta-dimensions and sixteen dimensions covering the general purpose, structural components, data modalities, evaluation approaches, and limitations of RAG systems."
  - [section] "We have developed a total of five meta-dimensions and sixteen dimensions to comprehensively capture the concept of RAG applications."
- Break condition: If new RAG applications emerge that cannot be mapped to any existing dimension, the taxonomy would require extension or reorganization.

### Mechanism 2
- Claim: Iterative paper analysis combined with automated domain clustering provides both rigor and coverage in taxonomy development.
- Mechanism: The four-iteration process ensures saturation by repeatedly reviewing papers and refining dimensions, while ChatGPT clustering identifies application domains that might be missed by manual review alone.
- Core assumption: Both manual expert review and automated clustering contribute complementary strengths to domain identification.
- Evidence anchors:
  - [section] "We performed four iterations to build our initial RAG application taxonomy... no new dimensions or characteristics were added, merged, or split in the iteration."
  - [section] "To facilitate the time-consuming process of paper analysis, we decided to apply it for identifying the application domains and further compare the outcome to see, whether such an automated technique applies to the taxonomy construction."
- Break condition: If automated clustering consistently produces irrelevant or missing domains compared to manual review, the hybrid approach would need reconsideration.

### Mechanism 3
- Claim: The taxonomy's extensibility allows it to accommodate emerging RAG paradigms and modalities without losing coherence.
- Mechanism: By organizing dimensions hierarchically and maintaining clear separation between structural and functional aspects, the taxonomy can absorb new characteristics (like multimodal extensions) while preserving the overall framework.
- Core assumption: New RAG developments will fit within the existing meta-dimension structure or require minimal additions.
- Evidence anchors:
  - [abstract] "The taxonomy is constructed through an iterative process informed by 28 papers and augmented by automated domain clustering using ChatGPT."
  - [section] "We see potential in incorporating RAGs into design science research endeavors to address a variety of domain-specific applications."
- Break condition: If future RAG applications require fundamentally different conceptual frameworks that don't map to the five meta-dimensions, the taxonomy would need complete restructuring.

## Foundational Learning

- Concept: Bloom's Taxonomy
  - Why needed here: The paper references Bloom's Taxonomy when discussing cognitive benchmarking for LLM evaluation, indicating its relevance to understanding how RAG systems process and apply knowledge.
  - Quick check question: How does Bloom's Taxonomy relate to the evaluation of knowledge-intensive tasks in RAG systems?

- Concept: Vector retrieval methods
  - Why needed here: The taxonomy distinguishes between sparse-vector retrieval (TF-IDF, BM25) and dense-vector retrieval (BERT-encoders), which are fundamental to understanding how RAG systems retrieve relevant information.
  - Quick check question: What are the key differences between sparse-vector and dense-vector retrieval approaches in RAG systems?

- Concept: Design science research methodology
  - Why needed here: The paper mentions using design science research for domain-specific applications, suggesting that practitioners need to understand how to apply RAG taxonomy findings to real-world implementations.
  - Quick check question: How can design science research methodology be applied to develop RAG solutions for specific business domains?

## Architecture Onboarding

- Component map: RAG systems consist of pre-retrieval (chunking, vectorizing, indexing), retrieval (sparse/dense/task-specific), post-retrieval (re-ranking, subgraph construction), generation (LLM response), and post-generation (output rewrite) components. These map to the five meta-dimensions: General (purpose/domain), Structure (paradigm/role), Data (modalities/granularity), Evaluation (metrics/datasets), and Limitations (failure points/future directions).

- Critical path: The retrieval process is the critical path that determines RAG system performance. Failures in retrieval (noise, missing content, consolidation issues) cascade to generation and final output quality.

- Design tradeoffs: Single vs. multiple vs. adaptive retrieval affects latency and relevance; naive vs. modular paradigms trade simplicity for flexibility; publicly available vs. proprietary datasets impact reproducibility and customization.

- Failure signatures: Retrieval failures manifest as irrelevant content or missing information; post-retrieval failures show as incorrect consolidation; generation failures appear as incorrect format, specificity, or incompleteness; integration failures show as increased latency and complexity.

- First 3 experiments:
  1. Implement a basic RAG pipeline using sparse-vector retrieval (BM25) on a public dataset like SQuAD to understand the retrieval-generation loop.
  2. Compare dense-vector retrieval using BERT-encoders against sparse retrieval on the same dataset to measure performance differences.
  3. Add post-retrieval re-ranking to the dense-vector retrieval system and measure improvements in generation quality using BLEU/METEOR metrics.

## Open Questions the Paper Calls Out
None

## Limitations
- Taxonomy relies on 28 papers, which may not capture full diversity of rapidly evolving RAG field
- Automated domain clustering using ChatGPT introduces potential bias based on prompt formulation and model's training cutoff
- Manual validation process depends on authors' domain expertise and may miss nuanced applications
- Focus primarily on text-based RAG systems potentially limits applicability to emerging multimodal implementations

## Confidence
- **High Confidence**: Hierarchical meta-dimension structure provides logical framework for organizing RAG characteristics
- **Medium Confidence**: Sixteen specific dimensions are well-justified based on literature review, though some may require refinement
- **Medium Confidence**: Automated domain clustering results align reasonably well with manual review, though exact prompt formulation and ChatGPT version could affect reproducibility

## Next Checks
1. **Expansion Test**: Apply the taxonomy to recent RAG papers published after the initial literature review to assess its coverage of emerging applications and identify any missing dimensions.
2. **Cross-Domain Application**: Validate the taxonomy's utility by having domain experts from different fields (e.g., healthcare, legal, education) map their RAG use cases to the defined dimensions and report any gaps or misalignments.
3. **Reproducibility Test**: Replicate the automated domain clustering using different prompt formulations and ChatGPT versions to determine the robustness of the automated component and establish best practices for future taxonomy extensions.