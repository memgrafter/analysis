---
ver: rpa2
title: 'HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding'
arxiv_id: '2403.00425'
source_url: https://arxiv.org/abs/2403.00425
tags:
- halc
- visual
- decoding
- arxiv
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HALC, a novel decoding algorithm designed
  to mitigate object hallucinations in large vision-language models (LVLMs). HALC
  operates by adaptively adjusting visual context windows during text generation,
  employing a focal-contrast grounding mechanism to identify fine-grained visual information
  for each object-related token and a matching-based beam search to maintain generation
  quality.
---

# HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding

## Quick Facts
- **arXiv ID:** 2403.00425
- **Source URL:** https://arxiv.org/abs/2403.00425
- **Reference count:** 40
- **Key outcome:** HALC reduces object hallucinations by up to 50% while preserving text generation quality across multiple LVLM backbones

## Executive Summary
HALC is a novel decoding algorithm that mitigates object hallucinations in large vision-language models by adaptively adjusting visual context windows during text generation. The approach combines focal-contrast grounding to identify fine-grained visual information for each object-related token with matching-based beam search to maintain generation quality. Extensive experiments on four benchmarks demonstrate significant improvements over existing methods, reducing hallucinations by up to 50% while preserving text generation quality.

## Method Summary
HALC operates as a plug-and-play decoding algorithm that integrates with existing LVLM backbones without requiring additional training. It identifies object-related tokens through POS tagging, retrieves relevant visual contexts using zero-shot detectors, samples expanding field-of-view windows, computes Jensen-Shannon divergences to find contrasting FOV pairs, and uses beam search with visual matching scores to select the best sequences. The method works by redistributing probabilities to favor faithful tokens over hallucinated ones through contrastive decoding.

## Key Results
- Reduces object hallucinations by up to 50% across multiple LVLM backbones
- Outperforms existing methods on CHAIR and POPE metrics for hallucination detection
- Maintains text generation quality (measured by BLEU scores) while reducing hallucinations
- Shows consistent performance across four different benchmarks (MSCOCO, MME, LLaVA-Bench)

## Why This Works (Mechanism)

### Mechanism 1
Object hallucination in LVLMs is partly due to disproportionate reliance on textual context during autoregressive decoding. Adaptive focal-contrast grounding redirects the decoding process toward fine-grained visual context specific to each token, reducing the influence of textual priors that cause hallucination. Core assumption: Certain visual contexts within an image correspond more closely to the true object being referenced, and identifying these contexts can correct hallucinated tokens.

### Mechanism 2
Contrastive sampling between visual contexts amplifies the likelihood of faithful tokens over hallucinated ones. By sampling a sequence of expanding FOVs and selecting the most contrasting pairs, the model redistributes probabilities such that the faithful token has a higher chance of being selected. Core assumption: The probability distribution of faithful tokens peaks sharply with certain visual contexts, while hallucinated tokens do not show this pattern.

### Mechanism 3
A global matching score based on visual-textual similarity ensures sequence-level quality and faithfulness. After local corrections, beam search selects the top-k sequences based on similarity between the generated text and the original image, rather than purely textual likelihood. Core assumption: A sequence that is visually consistent with the input image is more likely to be faithful and less hallucinated.

## Foundational Learning

- **Concept: Jensen-Shannon Divergence (JSD)**
  - Why needed here: Used to measure the discrepancy between probability distributions from different visual contexts, enabling selection of the most contrasting FOV pairs
  - Quick check question: How does JSD differ from KL divergence, and why is it preferred for comparing FOV distributions?

- **Concept: Beam Search Algorithm**
  - Why needed here: Core to the global level of HALC, maintaining multiple candidate sequences and selecting the best based on visual matching scores
  - Quick check question: What is the trade-off between beam size and computational cost, and how does it affect the diversity of candidates?

- **Concept: Part-of-Speech (POS) Tagging**
  - Why needed here: Identifies object-related tokens (nouns, adjectives, etc.) that are candidates for focal-contrast grounding
  - Quick check question: Which POS tags are considered object-related, and why are they prioritized for hallucination correction?

## Architecture Onboarding

- **Component map:** LVLM backbone → POS tagger → Object-related token filter → Visual context detector → FOV sampler → JSD calculator → Contrastive decoder → Beam search → Visual matching scorer → Final output
- **Critical path:** Visual context detector → FOV sampler → JSD calculator → Contrastive decoder. Delays in these components directly impact runtime.
- **Design tradeoffs:** Higher FOV sampling (n) and beam size (k) improve hallucination reduction but increase computation. JSD buffer size (m) balances local correction quality with runtime.
- **Failure signatures:**
  - POS tagger misses object-related tokens → fewer candidates for correction
  - Visual context detector returns inaccurate FOVs → focal-contrast grounding ineffective
  - JSD calculator selects non-contrasting FOV pairs → contrastive decoding fails
- **First 3 experiments:**
  1. Run HALC on a simple image with one clear object and verify that the object-related token is correctly grounded
  2. Test with an image containing co-occurring objects to check if focal-contrast grounding avoids confusion
  3. Evaluate runtime overhead with different values of n, m, and k to find a balance between performance and speed

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of HALC scale with increasing image complexity (number of objects, scene clutter, resolution)? The paper doesn't provide a systematic analysis of HALC's performance as a function of image complexity metrics.

### Open Question 2
What is the optimal FOV sampling distribution (normal vs. exponential) for different types of LVLM architectures or downstream tasks? The paper only provides theoretical justification for both distributions and uses exponential expansion in experiments.

### Open Question 3
How does the choice of visual grounding detector (Gd) impact HALC's performance, and is there an optimal detector for specific types of hallucinations (existence, attribute, relationship)? The paper only briefly mentions using different detectors in ablation studies.

## Limitations
- Scalability concerns with visual context detection mechanism when applied to domains with significantly different visual characteristics
- Limited ablation studies on hyperparameter sensitivity, particularly regarding the trade-offs between FOV sampling rate, JSD buffer size, and beam search width
- Dependency on specific pre-trained models (Grounding DINO, BLIP) raises questions about performance when components are replaced

## Confidence
- **High confidence:** Core mechanism of focal-contrast grounding for individual token correction, supported by empirical evidence of probability distribution peaks for faithful tokens
- **Medium confidence:** Sequence-level beam search effectiveness, as the global matching approach is less rigorously validated across diverse failure modes
- **Medium confidence:** Generalizability claim of 50% hallucination reduction, given that all experiments use the same visual matching model (BLIP) which may have inherent biases

## Next Checks
1. **Cross-detector validation:** Replace Grounding DINO with alternative zero-shot detectors (e.g., GLIP, OWL-ViT) and measure degradation in HALC's performance to assess dependency on the specific detector architecture.

2. **Runtime overhead benchmarking:** Implement HALC with varying (n, m, k) configurations and measure actual inference time increases compared to standard autoregressive decoding, establishing the practical computational cost for different deployment scenarios.

3. **Domain generalization test:** Apply HALC to medical imaging or satellite imagery domains where the visual characteristics differ substantially from natural images, measuring whether the focal-contrast mechanism maintains effectiveness when visual priors differ significantly from training data.