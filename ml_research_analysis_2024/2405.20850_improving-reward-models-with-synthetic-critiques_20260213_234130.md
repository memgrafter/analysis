---
ver: rpa2
title: Improving Reward Models with Synthetic Critiques
arxiv_id: '2405.20850'
source_url: https://arxiv.org/abs/2405.20850
tags:
- critiques
- training
- command
- reward
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using synthetic natural language critiques
  generated by large language models (LLMs) to improve reward models (RMs) for aligning
  language models via reinforcement learning from human feedback. Synthetic critiques
  provide additional feedback evaluating aspects such as instruction following, correctness,
  and style, offering richer signals and more robust features for RMs to assess and
  score on.
---

# Improving Reward Models with Synthetic Critiques

## Quick Facts
- **arXiv ID**: 2405.20850
- **Source URL**: https://arxiv.org/abs/2405.20850
- **Reference count**: 22
- **Primary result**: Synthetic critiques from LLMs improve reward model performance and data efficiency by providing richer feedback signals

## Executive Summary
This paper proposes using synthetic natural language critiques generated by large language models (LLMs) to improve reward models (RMs) for aligning language models via reinforcement learning from human feedback. Synthetic critiques provide additional feedback evaluating aspects such as instruction following, correctness, and style, offering richer signals and more robust features for RMs to assess and score on. Experiments demonstrate that high-quality critiques significantly improve RM performance and data efficiency, especially in low-resource regimes. For instance, a single high-quality critique-enhanced preference pair is roughly equivalent to 40 standard preference pairs. The method is accessible and cost-effective as synthetic critiques can be efficiently generated using open-source models.

## Method Summary
The method generates synthetic natural language critiques for each completion in human preference data using LLMs, then trains reward models on this critique-augmented data using binary ranking loss. The approach conditions RMs on both the completion and its corresponding critique during training, providing explicit reasoning about instruction-following, correctness, and style. Experiments evaluate this approach across different base models (LLaMA2-7B, Command-35B, Command R) and critique generators (GPT-4, Command R+, Mixtral-8x7B) on benchmarks like REWARD BENCH and PandaLM, comparing performance with and without critique augmentation.

## Key Results
- A single high-quality critique-enhanced preference pair is roughly equivalent to 40 standard preference pairs
- High-quality critiques significantly improve RM performance and data efficiency, especially in low-resource regimes
- Training with critiques leads to greater robustness against adversarial examples on Chat Hard
- 5k Command R+ Critiques achieve test accuracy comparable to approximately 90k No-Critiques

## Why This Works (Mechanism)

### Mechanism 1
Synthetic critiques provide richer signals for RMs to learn from, addressing overfitting on superficial features. By generating point-wise critiques for each completion in preference data, the RM receives explicit reasoning about instruction-following, correctness, and style, rather than just binary labels. Core assumption: LLMs can generate high-quality critiques that accurately capture the aspects humans would evaluate. Break condition: If critique quality is poor (low Metacritique F1), the additional signals may mislead the RM rather than help it generalize.

### Mechanism 2
Critiques improve data efficiency by making each preference pair more informative. A single critique-enhanced preference pair provides equivalent learning to multiple vanilla pairs because it contains explicit reasoning about multiple dimensions of quality. Core assumption: The Chain-of-Thought style feedback in critiques provides additional context that RMs can leverage. Break condition: When abundant preference data is available (200k+ examples), the marginal benefit of critiques diminishes as the RM can learn superficial patterns from volume alone.

### Mechanism 3
Critiques enhance interpretability and robustness of RM training. The natural language critiques serve as an interpretable Chain-of-Thought that makes the RM's decision process more transparent and helps it handle adversarial examples. Core assumption: Conditioning on critiques during both training and inference provides a consistent reasoning framework. Break condition: If critiques are inconsistent or hallucinated, they may reduce robustness by introducing spurious correlations.

## Foundational Learning

- **Concept**: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The paper builds on RLHF framework where RMs are trained to predict human preferences
  - Quick check question: What are the two main stages of RLHF, and what role does the RM play in each?

- **Concept**: Preference-based reward modeling
  - Why needed here: Understanding how RMs are trained on pairwise preference data is essential to grasp the critique augmentation approach
  - Quick check question: How does the binary ranking loss function work in training RMs on preference data?

- **Concept**: Knowledge distillation
  - Why needed here: The critique generation process can be viewed as a form of knowledge distillation from the critique generator to the RM
  - Quick check question: What distinguishes the critique approach from standard knowledge distillation in this context?

## Architecture Onboarding

- **Component map**: Critique Generator (LLM) → Critique Enriched Dataset → RM Training Pipeline → Evaluation on REWARD BENCH
- **Critical path**: Generate critiques → Augment training data with critiques → Train RM with ranking loss → Evaluate on benchmarks
- **Design tradeoffs**:
  - Critique quality vs. computation cost: Higher quality critiques (GPT-4) improve performance but are more expensive to generate
  - Pretrained model capacity: Stronger base models (Command R-35B) need fewer critiques, while weaker ones benefit more
  - Training data size: Critiques are most valuable in low-data regimes (5k-50k examples)

- **Failure signatures**:
  - Low test accuracy with high-quality critiques suggests issues with RM architecture or training hyperparameters
  - Performance worse than No-Critiques baseline indicates poor critique quality or inappropriate model initialization
  - Sensitivity to learning rate suggests the base model isn't well-suited for critique conditioning

- **First 3 experiments**:
  1. Train RM with Command R+ critiques on 5k examples vs. No-Critiques baseline to verify data efficiency gains
  2. Compare different critique generators (LLaMA2-7B-Chat vs GPT4-Turbo) on same RM to assess quality impact
  3. Test RM on Chat Hard subset to measure robustness against adversarial examples

## Open Questions the Paper Calls Out
- How do synthetic critiques impact LLM reasoning abilities when preference-tuned with critique-enhanced RMs?
- How does the choice of prompting method affect the quality of synthetic critiques and subsequent RM performance?
- What is the effect of incorporating synthetic critiques on the robustness of RMs against adversarial attacks beyond what is measured in Chat Hard?

## Limitations
- The 40x data efficiency claim rests on a single comparison without ablation on intermediate data sizes
- Critique quality assessment relies on Metacritique F1 scores which may not fully capture downstream RM performance
- The robustness claims on adversarial examples need more systematic evaluation beyond the Chat Hard subset

## Confidence
- **High Confidence**: Synthetic critiques improve RM performance when generated by strong models (GPT-4, Command R+) and when base models are appropriately initialized
- **Medium Confidence**: Data efficiency gains are model-dependent, with weaker base models (LLaMA2-7B) showing more dramatic improvements than stronger ones (Command-35B)
- **Low Confidence**: Claims about interpretability improvements lack quantitative validation beyond qualitative observations

## Next Checks
1. Conduct ablation studies varying critique quality systematically (using different LLM generators) to isolate the impact of critique generation quality on RM performance
2. Test the robustness claims on a broader set of adversarial examples and compare against other RM training methods
3. Evaluate the method's performance on out-of-distribution prompts to assess whether critique conditioning introduces bias or improves generalization