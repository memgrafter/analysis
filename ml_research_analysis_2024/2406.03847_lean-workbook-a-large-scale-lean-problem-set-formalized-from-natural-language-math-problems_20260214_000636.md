---
ver: rpa2
title: 'Lean Workbook: A large-scale Lean problem set formalized from natural language
  math problems'
arxiv_id: '2406.03847'
source_url: https://arxiv.org/abs/2406.03847
tags:
- problems
- language
- lean
- data
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Lean Workbook, a pipeline for automatically
  translating natural language math problems into formal Lean 4 statements with high
  accuracy. The method uses active learning with iterative synthetic data generation,
  filtering, and human diagnostics to improve translation quality.
---

# Lean Workbook: A large-scale Lean problem set formalized from natural language math problems

## Quick Facts
- arXiv ID: 2406.03847
- Source URL: https://arxiv.org/abs/2406.03847
- Reference count: 40
- Key outcome: 93.5% accuracy rate on sampled data for translating natural language math problems into Lean 4 statements

## Executive Summary
This paper presents Lean Workbook, an active learning pipeline that automatically translates natural language mathematical problems into formal Lean 4 statements with high accuracy. The approach iteratively generates synthetic data, filters through compilation and semantic verification, and incorporates human-corrected samples to improve the translation model. The resulting dataset contains 57K formalized problems plus 21 new IMO problems, achieving 93.5% accuracy and improving downstream theorem proving performance by over 6 percentage points on MiniF2F benchmarks.

## Method Summary
The method involves fine-tuning a pre-trained math model (InternLM-Math-Plus-20B) on collected formal-informal pairs from MiniF2F and ProofNet, then applying it to AOPS forum problems. The translation pipeline filters outputs through Lean compilation, back-translation with NLI verification, and human diagnostic correction. Each iteration adds approximately 30 human-labeled samples to the training data, addressing recurring error patterns and improving translation accuracy.

## Key Results
- 93.5% accuracy rate on sampled data for natural-to-Lean translation
- 57K formalized math problems in the main dataset
- 6+ percentage point improvement in MiniF2F accuracy when using the extended dataset
- Successful formalization of 21 new IMO problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Active learning with iterative synthetic data generation improves translation accuracy
- Mechanism: The pipeline iteratively generates synthetic Lean 4 statements from natural language problems, filters them through compilation and NLI checks, and adds human-corrected samples to the training set
- Core assumption: Human-corrected samples effectively teach the model to avoid recurring error patterns
- Evidence anchors:
  - [abstract]: "iterative generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements"
  - [section 4.2]: "Each iteration will add an average of about 30 human-labeled samples into the training data, addressing the current model's weakness"
  - [corpus]: Weak - no direct citations found for this specific active learning mechanism
- Break condition: Human correction becomes too time-consuming relative to accuracy gains, or error patterns become too diverse to capture with manual corrections

### Mechanism 2
- Claim: Back-translation with NLI verification ensures semantic equivalence
- Mechanism: After generating formal statements, they are translated back to natural language and compared with the original using NLI to verify semantic preservation
- Core assumption: NLI models can reliably detect semantic differences between original and back-translated problems
- Evidence anchors:
  - [section 4.2]: "After the formal statement is translated back into natural questions, we can turn to using a general domain LLM to leverage its Natural Language Inference ability"
  - [section 4.2]: "If we do not get a positive response, the sample is marked as needing human revision and correction"
  - [corpus]: Weak - no direct citations found for NLI use in formal statement verification
- Break condition: NLI model fails to capture subtle semantic differences or becomes unreliable on mathematical language

### Mechanism 3
- Claim: Large-scale synthetic data improves downstream theorem proving performance
- Mechanism: The generated dataset (57K formal-informal pairs) serves as additional training data for theorem proving models, improving their accuracy on MiniF2F benchmarks
- Core assumption: More formalized problems in the training data directly improves theorem proving capabilities
- Evidence anchors:
  - [abstract]: "Our results indicate that the synthetic data pipeline can provide useful training data and improve the performance of LLMs"
  - [section 5.4]: "The increase in MiniF2F accuracy also demonstrates a significant improvement in performance when using our extended dataset"
  - [section 5.4]: "MiniF2F accuracy by over 6 percentage points when combined with the new dataset"
  - [corpus]: Found 5 related papers but none directly testing this specific dataset combination effect

## Foundational Learning

- Concept: Lean 4 theorem proving basics
  - Why needed here: Understanding Lean syntax and compilation process is essential for implementing the correctness checks
  - Quick check question: What does Lean return when a proof is successfully completed?

- Concept: Natural Language Inference (NLI)
  - Why needed here: NLI is used to verify semantic equivalence between original problems and back-translated versions
  - Quick check question: How does an NLI model determine if two statements express the same meaning?

- Concept: Active learning principles
  - Why needed here: The pipeline uses active learning to iteratively improve the translation model by focusing on problematic cases
  - Quick check question: What distinguishes active learning from passive learning in machine learning?

## Architecture Onboarding

- Component map:
  Data collection -> Fine-tuning translation model -> Translation generation -> Compilation filtering -> Back-translation -> NLI verification -> Human diagnostic correction -> Dataset update

- Critical path:
  1. Collect natural language problems
  2. Translate to Lean 4 using current model
  3. Filter through compilation and NLI
  4. Human correct failed samples
  5. Add corrections to training data
  6. Fine-tune model for next iteration

- Design tradeoffs:
  - Translation model initialization: Using pre-trained math model vs. training from scratch
  - Filtering stringency: Balancing quality vs. quantity of generated data
  - Human involvement: Manual correction ensures quality but limits scalability

- Failure signatures:
  - High compilation failure rate → translation model generating invalid Lean syntax
  - High NLI failure rate → semantic misalignment between natural and formal statements
  - Stagnant accuracy improvement → error patterns not being effectively captured

- First 3 experiments:
  1. Test translation model on a small validation set to establish baseline accuracy
  2. Run compilation-only filtering on 100 samples to measure syntactic correctness rate
  3. Implement full pipeline (compilation + NLI) on 50 samples to establish end-to-end accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper bound on translation accuracy for Lean formalization of natural language math problems, and how close does the current 93.5% accuracy approach this limit?
- Basis in paper: [explicit] The paper reports 93.5% accuracy on sampled data and discusses remaining error patterns
- Why unresolved: The paper identifies specific error patterns but does not characterize the theoretical limits of autoformalization accuracy or whether 93.5% represents a ceiling or intermediate milestone
- What evidence would resolve it: Systematic analysis of error types across different mathematical domains and comparison with human formalization accuracy rates

### Open Question 2
- Question: How does the active learning approach scale when applied to mathematical domains beyond contest-level problems (e.g., undergraduate mathematics or research-level theorems)?
- Basis in paper: [explicit] The paper states their model is "focused on contest-level problems during active learning which may not be appropriate to formalize other level math problems"
- Why unresolved: The paper only evaluates on high-school contest level problems and acknowledges limitations for other mathematical domains
- What evidence would resolve it: Experiments applying the pipeline to formalizations from undergraduate or graduate-level mathematics with quantitative accuracy comparisons

### Open Question 3
- Question: What is the relationship between translation accuracy and downstream theorem proving performance, and can we establish a theoretical framework for this connection?
- Basis in paper: [explicit] The paper shows a 6+ percentage point improvement in MiniF2F accuracy when using the Lean Workbook dataset
- Why unresolved: The paper demonstrates correlation but does not explore whether there are diminishing returns, optimal dataset sizes, or theoretical limits to how translation quality affects proving performance
- What evidence would resolve it: Controlled experiments varying translation accuracy while holding other factors constant, and mathematical analysis of the information flow from formal statements to proof search

## Limitations
- The approach relies heavily on human expertise for error correction, limiting scalability to new mathematical domains
- NLI verification may miss subtle mathematical nuances that affect semantic equivalence
- The dataset construction focuses on well-defined problems, potentially excluding more complex or ambiguous mathematical statements

## Confidence
- High confidence: The claim that iterative synthetic data generation improves translation accuracy is strongly supported by the 6+ percentage point improvement in MiniF2F performance and the systematic 30-sample human correction per iteration
- Medium confidence: The semantic equivalence verification through NLI models has moderate confidence due to limited direct citations for this specific application and potential challenges in mathematical language understanding
- Low confidence: The assumption that error patterns can be effectively captured through manual corrections has low confidence because mathematical error diversity may exceed what human reviewers can systematically address across large datasets

## Next Checks
1. Test the semantic equivalence detection on a curated set of mathematically subtle pairs to measure false positive/negative rates in mathematical contexts
2. Analyze the diversity and distribution of error types across 100 randomly sampled failed translations to determine if manual correction can feasibly address the full error spectrum
3. Apply the trained model to formalize problems from entirely different mathematical domains (e.g., topology or abstract algebra) not represented in the training data to test domain transfer capabilities