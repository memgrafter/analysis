---
ver: rpa2
title: Transformation-Invariant Learning and Theoretical Guarantees for OOD Generalization
arxiv_id: '2410.23461'
source_url: https://arxiv.org/abs/2410.23461
tags:
- learning
- transformations
- where
- distribution
- collection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the problem of learning under distribution
  shifts, where the test distribution can differ from the training distribution due
  to transformations. The key contributions are: A learning rule that minimizes empirical
  worst-case risk across transformations, with sample complexity bounded by the VC
  dimension of the composition of the hypothesis class with the transformation class.'
---

# Transformation-Invariant Learning and Theoretical Guarantees for OOD Generalization

## Quick Facts
- arXiv ID: 2410.23461
- Source URL: https://arxiv.org/abs/2410.23461
- Authors: Omar Montasser; Han Shao; Emmanuel Abbe
- Reference count: 40
- Key outcome: Provides theoretical guarantees and sample complexity bounds for learning rules that minimize worst-case risk across transformations, with algorithmic reductions to ERM

## Executive Summary
This paper studies out-of-distribution generalization under distribution shifts caused by transformations, providing theoretical foundations for learning predictors that perform well across all possible transformed distributions. The authors develop learning rules that minimize worst-case risk across transformations, establishing sample complexity bounds in terms of the VC dimension of the composition of hypothesis and transformation classes. They also present algorithmic reductions that solve the worst-case risk minimization problem using only an ERM oracle, and propose alternative approaches for handling unknown invariant transformations.

## Method Summary
The paper addresses OOD generalization by modeling distribution shifts as transformations applied to the input space. The core approach minimizes worst-case empirical risk across all transformations in a class T, leveraging VC theory on the composition class H∘T. For finite transformation collections, the authors provide an algorithmic reduction that solves this problem using only an ERM oracle by framing it as a zero-sum game between predictor and transformation players. When transformations are unknown, they propose a coverage maximization approach that seeks predictors achieving low error on as many transformations as possible. The framework also extends to minimizing worst-case regret when noise heterogeneity exists across distributions.

## Key Results
- Sample complexity for worst-case risk minimization is bounded by O(vc(H∘T)+log(1/δ)/ε²), offering uniform generalization guarantees
- Algorithmic reduction to ERM oracle enables worst-case risk minimization for finite T without explicit knowledge of H
- Coverage maximization approach handles unknown invariant transformations by maximizing the number of distributions with low error
- Extension to worst-case regret minimization provides favorable guarantees under heterogeneous noise across distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing worst-case empirical risk across transformations yields a predictor with uniform generalization guarantees
- Mechanism: The learner constructs a composite hypothesis class H ∘ T containing all functions h∘T where h∈H and T∈T. By applying VC theory to this composition class, uniform convergence bounds guarantee that minimizing the worst-case empirical risk yields a predictor with low worst-case true risk across all transformations
- Core assumption: The transformation class T is either known or can be effectively approximated by a finite set
- Evidence anchors:
  - [abstract]: "we obtain upper bounds on the sample complexity in terms of the VC dimension of the class composing predictors with transformations"
  - [section]: "By setting m(ε, δ) = O(vc(H∘T)+log(1/δ)/ε²) and invoking Proposition A.1, we have the guarantee that with probability at least 1−δ over S∼D^m^(ε,δ), (∀h∈H)(∀T∈T): |err(h,T(S))−err(h,T(D))|≤ε"
  - [corpus]: Weak evidence - no directly related work on transformation-based OOD bounds found in corpus
- Break condition: When vc(H∘T) is much larger than vc(H), making sample complexity impractical

### Mechanism 2
- Claim: Algorithmic reduction to ERM oracle enables worst-case risk minimization without explicit knowledge of H
- Mechanism: The reduction frames worst-case risk minimization as a zero-sum game between predictor and transformation players. The T-player uses multiplicative weights to update transformation weights based on predictor performance, while the H-player uses the ERM oracle to find the best predictor under current transformation weights
- Core assumption: Access to an ERM oracle for the hypothesis class H and finite transformation collection T
- Evidence anchors:
  - [section]: "we present a generic algorithmic reduction (Algorithm 1) solving Objective 1 using only an ERM oracle for H, when the collection T is finite"
  - [section]: "This is established by solving a zero-sum game where the H-player runs ERM and the T-player runs Multiplicative Weights"
  - [corpus]: Weak evidence - no directly related work on ERM-based reductions for transformation-invariant learning found in corpus
- Break condition: When T is infinite or when the ERM oracle is computationally expensive

### Mechanism 3
- Claim: Unknown invariant transformations can be handled by maximizing the number of transformations with low error
- Mechanism: Instead of requiring low error on all transformations, the learning rule selects predictors that achieve low error on as many transformations as possible. This is implemented by maximizing the count of transformations where empirical error is below a threshold
- Core assumption: There exists a predictor in H that achieves low error on a significant fraction of transformations
- Evidence anchors:
  - [section]: "the learning rule searches for a predictor ˆh∈H that achieves low empirical error on as many transformations T∈T as possible, say err(ˆh,T(S))≤ε"
  - [section]: "We then present a different generic learning rule (Equation (4)) that learns a predictor ˆh achieving low error (say ε) on as many target distributions in {T(D)}T∈T as possible"
  - [corpus]: Weak evidence - no directly related work on maximizing transformation coverage found in corpus
- Break condition: When no predictor achieves low error on any transformation, or when transformations are highly heterogeneous

## Foundational Learning

- Concept: VC dimension and uniform convergence
  - Why needed here: The theoretical guarantees rely on bounding the VC dimension of the composite class H∘T to establish sample complexity and generalization bounds
  - Quick check question: If H has VC dimension 2 and T has VC dimension 3, what is the maximum possible VC dimension of H∘T?

- Concept: Zero-sum games and regret minimization
  - Why needed here: The algorithmic reductions use game-theoretic formulations where the learner and adversary optimize over predictors and transformations respectively, requiring understanding of regret minimization techniques
  - Quick check question: In the multiplicative weights update rule, what happens to the weight of a transformation that consistently causes high error for the current predictor?

- Concept: Empirical risk minimization (ERM) and agnostic PAC learning
  - Why needed here: The algorithmic reductions require access to an ERM oracle and rely on agnostic PAC learning guarantees to ensure the oracle produces good predictors under weighted training distributions
  - Quick check question: What guarantee does an (ε,δ)-agnostic-PAC-learner provide about the excess risk of its output predictor?

## Architecture Onboarding

- Component map: H -> T -> H∘T (hypothesis class, transformation class, composite class)
- Critical path: For direct optimization - compute worst-case empirical risk over T and find minimizing h∈H; for reduction approach - iteratively update transformation weights using multiplicative weights and solve weighted ERM problems
- Design tradeoffs: Direct optimization requires knowledge of H and T but is conceptually simple; reduction approach requires only ERM oracle but needs finite T and multiple oracle calls; maximizing transformation coverage is more flexible but may sacrifice worst-case guarantees
- Failure signatures: Poor performance when vc(H∘T)≫vc(H) (sample complexity issues), when T is infinite (algorithmic limitations), or when no predictor achieves low error on any transformation (coverage maximization limitations)
- First 3 experiments:
  1. Implement direct worst-case risk minimization on synthetic data with known transformations (e.g., rotations, translations) and verify uniform convergence bounds
  2. Implement algorithmic reduction using multiplicative weights on the same synthetic data and compare sample complexity requirements
  3. Test coverage maximization approach on data with heterogeneous transformations to evaluate trade-off between worst-case and average-case performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sample complexity bounds for transformation-invariant learning be improved using improper learning algorithms rather than proper ones?
- Basis in paper: [explicit] The paper shows that the sample complexity of any proper learning rule is at least Ω(vc(H ◦ T )), but leaves open the possibility of improving the sample complexity with improper learning rules
- Why unresolved: The paper only provides upper bounds for proper learning rules and does not explore improper learning approaches
- What evidence would resolve it: A proof showing that improper learning algorithms can achieve sample complexity bounds lower than Ω(vc(H ◦ T )) for transformation-invariant learning

### Open Question 2
- Question: Can the algorithmic reduction for minimizing worst-case risk be extended to handle infinite collections of transformations T when only an ERM oracle is available?
- Basis in paper: [inferred] The paper requires T to be finite for its algorithmic reduction, and informally argues that this requirement may be necessary in general when only an ERM oracle is allowed
- Why unresolved: The paper does not explore additional structural conditions that would enable handling infinite T
- What evidence would resolve it: A proof showing that the algorithmic reduction can be extended to infinite T under certain structural conditions, or a proof that finite T is indeed necessary for the reduction to work

### Open Question 3
- Question: How can the learning rules for transformation-invariant learning be implemented in practice using neural network architectures for both the hypothesis class H and the collection of transformations T?
- Basis in paper: [explicit] The paper mentions that both predictors H and transformations T can be parameterized by neural network architectures, which is an interesting direction to explore further
- Why unresolved: The paper focuses on theoretical guarantees and does not provide practical implementation details or experiments
- What evidence would resolve it: A practical implementation of the learning rules using neural networks, along with experimental results demonstrating their effectiveness on real-world datasets

## Limitations
- High sample complexity when VC dimension of H∘T is much larger than VC dimension of H alone
- Algorithmic reduction requires finite transformation collection, limiting applicability to infinite transformation classes
- Coverage maximization approach may sacrifice worst-case guarantees for average-case performance

## Confidence

**Major Uncertainties:**
- The practical scalability of the approach when VC(H∘T) is large, potentially making sample complexity prohibitive
- The effectiveness of the algorithmic reduction when transformation class T is infinite or very large
- The trade-off between worst-case guarantees and empirical performance when using the coverage maximization approach

**Confidence Assessment:**
- High confidence in theoretical guarantees for finite transformation classes with bounded VC dimension
- Medium confidence in practical implementation due to lack of specific neural network architecture details
- Medium confidence in algorithmic reduction effectiveness, pending empirical validation

## Next Checks
1. Implement controlled experiments comparing direct worst-case optimization vs. algorithmic reduction on synthetic data with known transformation classes, measuring sample complexity and generalization performance
2. Test the coverage maximization approach on heterogeneous transformation scenarios to quantify the trade-off between worst-case and average-case performance
3. Analyze computational complexity of the algorithmic reduction as |T| grows, identifying practical limits for transformation class size