---
ver: rpa2
title: Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost
  Benchmarks
arxiv_id: '2403.01888'
source_url: https://arxiv.org/abs/2403.01888
tags: []
core_contribution: This paper introduces a Python package for efficient parallel hyperparameter
  optimization (HPO) benchmarking on zero-cost benchmarks. The key challenge addressed
  is that naive parallel simulations of multi-fidelity optimization (MFO) waste time
  waiting for simulated runtimes, making HPO research slow.
---

# Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost Benchmarks

## Quick Facts
- arXiv ID: 2403.01888
- Source URL: https://arxiv.org/abs/2403.01888
- Authors: Shuhei Watanabe; Neeratyoy Mallik; Edward Bergman; Frank Hutter
- Reference count: 35
- Primary result: Introduces a Python package for efficient parallel HPO benchmarking with 1000x speedup

## Executive Summary
This paper presents a novel approach to benchmarking asynchronous multi-fidelity optimization (MFO) on zero-cost benchmarks. The key innovation addresses the inefficiency of naive parallel simulations where processes waste time waiting for simulated runtimes. By leveraging file system synchronization to eliminate waiting while preserving evaluation return order, the authors achieve a 1000x speedup in HPO experiments. The solution is packaged as a Python library supporting major HPO frameworks including SMAC3, Optuna, and HpBandSter.

## Method Summary
The proposed method uses file system synchronization to enable efficient parallel execution of multi-fidelity optimization algorithms on zero-cost benchmarks. Traditional parallel simulation approaches waste computational resources waiting for simulated runtimes to complete. The solution creates a synchronization mechanism where each evaluation process writes to a shared file system, allowing other processes to proceed without blocking. This approach maintains the exact order of evaluation returns while eliminating idle waiting time. The implementation is provided as a Python package that integrates with existing HPO libraries and zero-cost benchmark suites.

## Key Results
- Achieves 1000x speedup compared to traditional parallel simulation approaches
- Maintains exact preservation of evaluation return order
- Compatible with major HPO libraries including SMAC3, Optuna, and HpBandSter
- Enables researchers to conduct HPO experiments significantly faster
- Available via pip install mfhpo-simulator with open-source implementation

## Why This Works (Mechanism)
The mechanism works by replacing time-based waiting with file system-based synchronization. When an evaluation process completes its simulated computation, it writes a marker to a shared file system location. Other processes can check these markers to determine when to proceed, eliminating the need to wait for artificial time delays. This approach maintains the causal relationships between evaluations while removing the computational waste of idle waiting.

## Foundational Learning

1. **Zero-cost benchmarks**
   - Why needed: Provide computationally inexpensive proxy functions for expensive black-box optimization problems
   - Quick check: Verify that benchmark functions return results instantly while preserving problem characteristics

2. **Asynchronous multi-fidelity optimization**
   - Why needed: Allows evaluations to proceed at different resource levels without blocking
   - Quick check: Confirm that lower fidelity evaluations can complete while higher fidelity ones are still running

3. **File system synchronization**
   - Why needed: Provides a platform-independent mechanism for process coordination
   - Quick check: Test that file write/read operations work consistently across different operating systems

## Architecture Onboarding

**Component Map:** Simulator -> File System Sync -> HPO Library -> Zero-Cost Benchmark

**Critical Path:** HPO algorithm request → File system marker check → Evaluation execution → Result storage → Return to algorithm

**Design Tradeoffs:** 
- File system I/O overhead vs. elimination of waiting time
- Platform compatibility vs. potential performance variations
- Simplicity of implementation vs. fine-grained control

**Failure Signatures:** 
- Missing file system markers indicate evaluation failures
- Out-of-order results suggest synchronization issues
- Excessive I/O wait times may indicate hardware limitations

**First Experiments:**
1. Run single-fidelity optimization to verify basic functionality
2. Test with multiple concurrent evaluations to check synchronization
3. Compare results with and without the simulator to validate correctness

## Open Questions the Paper Calls Out

None identified in the provided content.

## Limitations

- Effectiveness depends on zero-cost nature of benchmark functions, which may not represent real-world scenarios
- File system synchronization assumes host system can handle increased I/O operations efficiently
- Focus on asynchronous MFO may limit applicability to synchronous or single-fidelity optimization

## Confidence

**High Confidence:** The 1000x speedup claim is well-supported by the deterministic nature of the file system synchronization mechanism.

**Medium Confidence:** The exact preservation of evaluation return order is plausible but requires verification across different systems.

**Low Confidence:** The claim of enabling "much faster" HPO research is subjective and context-dependent.

## Next Checks

1. Verify preservation of evaluation return order across different operating systems and file systems using controlled experiments.

2. Benchmark file system I/O overhead on various hardware configurations to establish practical speedup limits.

3. Test simulator compatibility with additional HPO libraries beyond those mentioned in the paper.