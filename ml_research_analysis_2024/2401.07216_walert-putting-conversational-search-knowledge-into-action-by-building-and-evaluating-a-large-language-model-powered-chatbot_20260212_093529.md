---
ver: rpa2
title: 'Walert: Putting Conversational Search Knowledge into Action by Building and
  Evaluating a Large Language Model-Powered Chatbot'
arxiv_id: '2401.07216'
source_url: https://arxiv.org/abs/2401.07216
tags:
- questions
- rmit
- passages
- answer
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Walert addresses the challenge of building and evaluating LLM-powered
  conversational agents for frequently asked questions about university programs,
  focusing on privacy, hallucination control, and evaluation practices. The authors
  developed two approaches: an Intent-Based system using Amazon Alexa with semantic
  variations generated by an open-source LLM, and a Retrieval-Augmented Generation
  system combining BM25 and dense retrieval with Falcon-7B for answer generation.'
---

# Walert: Putting Conversational Search Knowledge into Action by Building and Evaluating a Large Language Model-Powered Chatbot

## Quick Facts
- arXiv ID: 2401.07216
- Source URL: https://arxiv.org/abs/2401.07216
- Authors: Sachin Pathiyan Cherumanal; Lin Tian; Futoon M. Abushaqra; Angel Felipe Magnossao de Paula; Kaixin Ji; Danula Hettiachchi; Johanne R. Trippas; Halil Ali; Falk Scholer; Damiano Spina
- Reference count: 23
- One-line primary result: Intent-Based approach achieved NDCG@1 of 0.643 for known answers, while RAG approaches with DPR retrieval achieved comparable performance (NDCG@1 of 0.691) and better performance on inferred answers.

## Executive Summary
This paper presents Walert, a chatbot designed to answer frequently asked questions about university programs using LLM technology. The authors developed two approaches: an Intent-Based system using Amazon Alexa with semantic variations generated by an open-source LLM, and a Retrieval-Augmented Generation system combining BM25 and dense retrieval with Falcon-7B for answer generation. The system was evaluated using a testbed of 106 questions across three categories (known answers, inferred answers, and out-of-KB questions) and demonstrated effective knowledge transfer from research to practical deployment at university open days.

## Method Summary
The authors created a testbed with 106 questions and 120 passages from RMIT University's FAQ, categorizing questions into three types: known answers, inferred answers, and out-of-knowledge base questions. They implemented two approaches: an Intent-Based system using Amazon Alexa with semantic variations generated by Falcon-7B for training data augmentation, and a Retrieval-Augmented Generation system using either BM25 or DPR for passage retrieval combined with Falcon-7B for answer generation. The system was evaluated using NDCG for retrieval effectiveness and BERTScore and ROUGE-1 for natural language generation quality.

## Key Results
- Intent-Based approach achieved NDCG@1 of 0.643 for known-answer questions and correctly identified 80% of out-of-KB questions
- RAG approaches with DPR retrieval achieved comparable retrieval performance (NDCG@1 of 0.691) and better performance on inferred answers
- BM25 with k=3 obtained the highest NDCG score for inferred questions in the RAG approach
- The demo successfully showcased at university open days, generating institutional interest

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intent-Based approach achieves high precision for known-answer questions by mapping FAQ entries to predefined intents
- Mechanism: IB uses semantic variations generated by Falcon-7B to expand training utterances, allowing Amazon Alexa NLU to recognize diverse phrasings of the same question intent
- Core assumption: The set of FAQ questions is limited and predictable enough that intent-based matching can cover most user queries effectively
- Evidence anchors:
  - [abstract] "Our demo aims to showcase how conversational information-seeking researchers can effectively communicate the benefits of using best practices to stakeholders interested in developing and deploying LLM-based chatbots."
  - [section 2.2] "Since manually creating variations of training utterances is time-consuming, we experimented with using open-source LLMs to automatically create semantically equivalent variations of utterances (i.e., training data augmentation)."
  - [corpus] Weak - related work discusses intent-based systems but doesn't specifically validate the semantic variation generation approach described here
- Break condition: If user questions fall outside the FAQ domain or require complex reasoning beyond simple fact retrieval, IB precision drops significantly and recall becomes insufficient

### Mechanism 2
- Claim: Retrieval-Augmented Generation (RAG) improves answer quality for inferred-answer questions by combining multiple relevant passages through summarization
- Mechanism: RAG uses either BM25 or DPR for passage retrieval, then generates answers using Falcon-7B with top-k passages as context, allowing synthesis of information spread across multiple documents
- Core assumption: Relevant passages exist in the KB for inferred questions, and combining them through LLM summarization produces coherent, accurate answers
- Evidence anchors:
  - [section 2.3] "For the retrieval model, we experimented with two approaches: (i) Okapi BM25... and (ii) dense retrieval using Dense Passage Retrieval (DPR)... To generate a summary from the top-ùêæ retrieved passages, we used the same LLM that was used to generate semantically equivalent variations of the questions..."
  - [section 3] "For the Inferred questions, RAG approaches outperform IB (which can only return, at most, a passage partially relevant to the question). RAG approaches benefit from more context, and BM25 with ùëò = 3 obtains the highest NDCG score."
  - [corpus] Weak - related work mentions RAG for conversational systems but doesn't specifically validate the multi-document summarization approach for inferred answers
- Break condition: If relevant passages are missing or too sparse, RAG may hallucinate or produce irrelevant answers despite the retrieval step

### Mechanism 3
- Claim: The hybrid testbed with known, inferred, and out-of-KB questions enables comprehensive evaluation of both IB and RAG approaches across different difficulty levels
- Mechanism: The testbed structure allows evaluation of retrieval effectiveness (NDCG) and generation quality (BERTScore, ROUGE-1) while testing each system's ability to handle questions with direct answers, questions requiring inference, and unanswerable questions
- Core assumption: Creating a diverse question set with clear answer types provides meaningful comparison between approaches and reveals their respective strengths and limitations
- Evidence anchors:
  - [section 2.1] "We created a testbed that consists of relevance judgments at the passage level and gold answers for three types of questions: Questions with Known Answers, Questions with Inferred Answers, and Out-of-Knowledge Base Questions."
  - [section 3] "When it comes to retrieving a response, the IB approach only retrieves one passage (i.e., the response associated with the recognized intent), whereas RAG approaches retrieve multiple passages for a given question... Table 2 shows that, for the Known questions, IB and RAG using DPR have comparable performance in terms of NDCG@1..."
  - [corpus] Weak - related work discusses evaluation metrics but doesn't specifically validate the three-way question categorization approach
- Break condition: If question types overlap or gold answers are ambiguous, evaluation results may not accurately reflect system performance differences

## Foundational Learning

- Concept: Natural Language Understanding (NLU) for intent recognition
  - Why needed here: IB approach relies on correctly mapping user utterances to predefined intents using Amazon Alexa's NLU capabilities
  - Quick check question: How does semantic variation generation improve intent recognition accuracy for limited training data?

- Concept: Dense Passage Retrieval (DPR) vs. sparse retrieval (BM25)
  - Why needed here: RAG approach compares two retrieval methods to understand their impact on answer quality and hallucination control
  - Quick check question: What are the trade-offs between dense and sparse retrieval methods for conversational QA systems?

- Concept: Automatic evaluation metrics for text generation
  - Why needed here: The paper uses BERTScore and ROUGE-1 to evaluate generated answers, requiring understanding of their strengths and limitations
  - Quick check question: Why might BERTScore be more suitable than ROUGE-1 for evaluating LLM-generated answers?

## Architecture Onboarding

- Component map:
  - Intent-Based system: Amazon Alexa Skill ‚Üí NLU ‚Üí Intent mapping ‚Üí FAQ response
  - RAG system: User question ‚Üí Retrieval (BM25/DPR) ‚Üí Top-k passages ‚Üí Falcon-7B summarization ‚Üí Answer
  - Testbed generation: FAQ ‚Üí Question variations ‚Üí Relevance judgments ‚Üí Gold answers

- Critical path:
  - IB: User utterance ‚Üí Intent recognition ‚Üí Response retrieval (single passage)
  - RAG: User question ‚Üí Passage retrieval ‚Üí Answer generation (multi-passage context)
  - Evaluation: System output ‚Üí Metric calculation (NDCG/BERTScore/ROUGE-1)

- Design tradeoffs:
  - IB: High precision, low recall, no hallucination risk, limited coverage
  - RAG: Better coverage, potential hallucination risk, requires passage retrieval quality
  - Privacy: Local Falcon-7B deployment vs. third-party LLM services

- Failure signatures:
  - IB: Missed intents for out-of-domain questions, false positives for similar intents
  - RAG: Retrieval failure leading to hallucination, over-reliance on irrelevant passages
  - Evaluation: Metrics not capturing semantic correctness, low inter-annotator agreement

- First 3 experiments:
  1. Test IB system with FAQ questions and their semantic variations to measure intent recognition accuracy
  2. Evaluate RAG retrieval effectiveness using known-answer questions with different k values
  3. Compare BERTScore vs. ROUGE-1 for generated answers to understand metric reliability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Walert chatbot change when scaled to larger, more diverse knowledge bases containing brochures and internal web pages?
- Basis in paper: [inferred] The authors mention as a limitation that the current demo relies upon a limited knowledge base and plan to reproduce their methodology with a more extensive set of documents in future work.
- Why unresolved: The current evaluation was conducted on a small, manually curated FAQ dataset. Scaling to larger, more diverse documents would likely introduce new challenges in retrieval effectiveness and hallucination control.
- What evidence would resolve it: Conducting a comprehensive evaluation of Walert on a larger knowledge base with thousands of documents, comparing performance metrics (NDCG, BERTScore, ROUGE) against the current results, and analyzing changes in hallucination rates and retrieval accuracy.

### Open Question 2
- Question: What alternative evaluation measures beyond BERTScore and ROUGE-1 could better capture the quality and validity of generated responses in LLM-based conversational systems?
- Basis in paper: [explicit] The authors state that "Further research on evaluation measures (beyond BERTScore and ROUGE) is needed to evaluate the validity of generated responses" and plan to explore other evaluation measures.
- Why unresolved: BERTScore and ROUGE-1 are primarily designed for summarization tasks and may not fully capture the nuances of conversational responses, including factual accuracy, relevance, and user satisfaction.
- What evidence would resolve it: Developing and validating new evaluation metrics specifically designed for conversational systems, conducting user studies to correlate these metrics with human judgments, and comparing their effectiveness against existing measures.

### Open Question 3
- Question: How does the performance of the Walert chatbot change in real-world deployment scenarios with actual users, compared to controlled evaluation settings?
- Basis in paper: [inferred] The authors mention plans to deploy RAG approaches to perform online experimentation, suggesting that real-world deployment has not yet been fully evaluated.
- Why unresolved: Laboratory evaluations using predefined test sets may not capture the full range of user behaviors, question variations, and real-world complexities that occur during actual deployment.
- What evidence would resolve it: Conducting a longitudinal study of Walert's performance in production, collecting user feedback, analyzing conversation logs for common failure patterns, and comparing these results with controlled evaluation metrics.

## Limitations

- The evaluation corpus is relatively small (106 questions, 120 passages), which may not capture the full complexity of real-world university FAQ scenarios
- The testbed construction relied heavily on a single annotator for gold answers, introducing potential bias
- The human-in-the-loop approach for hallucination control, while effective, doesn't scale well for production deployment

## Confidence

- High Confidence: The quantitative evaluation results showing IB's superior performance on known-answer questions and RAG's advantage for inferred questions are well-supported by the data
- Medium Confidence: The claim that the three-question categorization effectively captures system differences is reasonable but could benefit from additional validation on larger, more diverse datasets
- Medium Confidence: The assertion that Falcon-7B deployment on SageMaker provides adequate privacy and hallucination control is supported by the results but lacks comparison with alternative approaches

## Next Checks

1. Replicate the evaluation on a larger, more diverse FAQ corpus (e.g., multiple universities or different domains) to test generalizability of the findings
2. Conduct inter-annotator agreement studies on the gold answer creation process to quantify potential bias in the testbed construction
3. Compare the human-in-the-loop hallucination control approach against automated detection methods to assess scalability and practical deployment feasibility