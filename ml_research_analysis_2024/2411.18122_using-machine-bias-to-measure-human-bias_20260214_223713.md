---
ver: rpa2
title: Using Machine Bias To Measure Human Bias
arxiv_id: '2411.18122'
source_url: https://arxiv.org/abs/2411.18122
tags: []
core_contribution: This paper addresses the challenge of measuring bias in human decisions
  when gold standard labels are scarce. The proposed machine learning-based framework,
  MDBA, trains models to predict each human decision-maker's decisions and then adjusts
  these models to equalize the recall-precision ratio across groups.
---

# Using Machine Bias To Measure Human Bias

## Quick Facts
- arXiv ID: 2411.18122
- Source URL: https://arxiv.org/abs/2411.18122
- Reference count: 40
- Key outcome: MDBA consistently outperforms alternatives with 33-88.5% improvement in mean absolute error for measuring human decision bias

## Executive Summary
This paper addresses the challenge of measuring bias in human decisions when gold standard labels are scarce. The proposed machine learning-based framework, MDBA, trains models to predict each human decision-maker's decisions and then adjusts these models to equalize the recall-precision ratio across groups. By comparing the adjusted models' predictions with gold standard labels, MDBA estimates human decision bias. Theoretical guarantees are provided, and empirical evaluations across various datasets demonstrate MDBA's superior performance compared to existing benchmarks.

## Method Summary
The MDBA framework trains machine learning models to predict human decisions from features, then applies fairness post-processing to adjust these models by equalizing recall-precision ratios across groups. The adjusted models are then used to make predictions on a small set of gold standard labels, and the comparison between these predictions and the gold standard outcomes reveals the bias in human decisions. The method combines the large volume of historical human decisions with a small set of gold standard labels to provide reliable bias estimates.

## Key Results
- MDBA consistently outperforms alternatives like selection rates and confident learning, with improvements ranging from 33% to 88.5% in mean absolute error
- The method shows superior performance across various datasets and experimental settings
- MDBA effectively leverages both human historical decisions and gold standard labels to estimate bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MDBA estimates human decision bias by training predictive models on human decisions and adjusting for algorithmic bias using fairness post-processing
- Mechanism: Human decisions Y' are modeled as outputs of underlying decision processes f(X). ML models f_k predict Y' from features X, capturing these decision processes. A recall-precision ratio (RPR) constraint via post-processing equalizes bias introduced during training, isolating the bias in human decisions
- Core assumption: The functional form of the relationship between features and human decisions is correctly specified in the trained models
- Evidence anchors:
  - [abstract] "The proposed machine learning-based framework, MDBA, trains models to predict each human decision-maker's decisions and then adjusts these models to equalize the recall-precision ratio across groups."
  - [section 4.1] "Specifically, we consider the group-specific recall (also known as true positive rate), and precision (also known as positive predictive value), of models' predictions ˆY with respect to the human labelers' decisions Y′ given a protected group, namely T PR ˆY|Y′, A and PPV ˆY|Y′, A."
  - [corpus] Weak evidence - related work discusses algorithmic fairness but does not specifically address this recall-precision ratio adjustment mechanism
- Break condition: If the functional form assumption is violated, the method may still work but with reduced accuracy

### Mechanism 2
- Claim: Comparing adjusted model predictions on gold standard labels with gold standard outcomes estimates bias in human decisions
- Mechanism: After adjusting models to equalize RPR across groups, the method uses these adjusted models to predict outcomes on the gold standard dataset GS. The comparison between these predictions and gold standard labels reveals the bias in original human decisions
- Core assumption: Gold standard labels Y are accurate representations of correct decisions
- Evidence anchors:
  - [abstract] "By comparing the adjusted models' predictions with gold standard labels, MDBA estimates human decision bias."
  - [section 4.1] "In a third step, the set of adjusted models, denoted as { f k}K k=1, is used to make predictions on the disjoint and scarce gold standard dataset GS."
  - [corpus] Weak evidence - related work discusses using gold standards but not in this specific context of bias assessment via adjusted models
- Break condition: If gold standard labels are themselves biased or inaccurate, the method will produce incorrect bias estimates

### Mechanism 3
- Claim: The method outperforms alternatives by effectively leveraging both large volumes of human historical decisions and small sets of gold standard labels
- Mechanism: Unlike selection rates which only consider decision rates, or GS-based approaches which only use gold standard labels, MDBA combines both data sources. Historical human decisions allow training accurate predictive models, while gold standard labels enable bias assessment
- Core assumption: Human decisions contain enough signal about the underlying decision process to train useful predictive models
- Evidence anchors:
  - [abstract] "Specifically, MDBA consistently outperforms alternatives like selection rates and confident learning, with improvements ranging from 33% to 88.5% in mean absolute error."
  - [section 6] "MDBA does use these labels, but their sensitivity to the magnitude of this set differs. For CL, these results suggest that the signal contained in this small set is overpowered by the data coming from human decisions in its estimate of the distribution of a latent gold standard, yielding stable but relatively poor performance across all settings."
  - [corpus] Moderate evidence - related work discusses learning from noisy labels but not in this specific hybrid approach context
- Break condition: If human decisions are too noisy or the decision process is too complex to model accurately, the method's performance will degrade

## Foundational Learning

- Concept: Machine learning model training and prediction
  - Why needed here: The method relies on training ML models to predict human decisions from features, then using these models to estimate bias
  - Quick check question: Can you explain how a trained ML model can be used to make predictions on new data?

- Concept: Fairness and bias in machine learning
  - Why needed here: The method uses fairness post-processing to adjust for algorithmic bias, which is essential for isolating human decision bias
  - Quick check question: What is the difference between bias in training data and bias introduced during model training?

- Concept: Gold standard labels and their role in evaluation
  - Why needed here: The method requires a small set of gold standard labels to assess the bias in human decisions, and understanding their role is crucial
  - Quick check question: Why might gold standard labels be scarce or difficult to obtain in real-world settings?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> Bias adjustment -> Gold standard evaluation -> Bias estimation
- Critical path: Data preprocessing → Model training → Bias adjustment → Gold standard evaluation → Bias estimation
- Design tradeoffs:
  - Model complexity vs. interpretability: More complex models may capture decision processes better but be harder to interpret
  - Amount of gold standard data vs. bias estimation accuracy: More gold standard data generally leads to more accurate bias estimates
  - Fairness constraint choice: Different fairness metrics may lead to different bias adjustment results
- Failure signatures:
  - Poor model performance on human decisions: Indicates issues with model specification or data quality
  - Large variance in bias estimates: Suggests instability in the method or insufficient gold standard data
  - Bias estimates not aligning with expectations: May indicate issues with the gold standard labels or the fairness adjustment
- First 3 experiments:
  1. Train ML models to predict human decisions on a small synthetic dataset and evaluate their performance
  2. Apply fairness post-processing to adjust the models for algorithmic bias and observe the changes
  3. Use the adjusted models to predict outcomes on a small gold standard dataset and calculate bias metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the MDBA framework when the gold standard labels are not completely accurate or are themselves biased?
- Basis in paper: [inferred] The paper discusses the scarcity of gold standard labels and the framework's reliance on a small, disjoint set of such labels for bias assessment
- Why unresolved: The paper does not address scenarios where the gold standard labels might contain inaccuracies or biases, which could affect the framework's performance
- What evidence would resolve it: Testing the framework's performance with gold standard labels that have known inaccuracies or biases, and comparing the results to those obtained with perfect gold standard labels

### Open Question 2
- Question: Can the MDBA framework be extended to assess biases that are not binary (e.g., multi-class or continuous outcomes)?
- Basis in paper: [explicit] The paper focuses on biases defined as disparities in rates of error for a certain type of error of interest, such as gaps in true positive rates (TPRs), which are typically binary
- Why unresolved: The paper does not explore how the framework might handle non-binary outcomes, which could be relevant in many real-world applications
- What evidence would resolve it: Adapting the framework to handle multi-class or continuous outcomes and evaluating its performance in these scenarios

### Open Question 3
- Question: How does the MDBA framework perform in real-world settings where the assumptions about data distribution and human decision-making processes may not hold?
- Basis in paper: [inferred] The paper's empirical evaluation relies on simulated data and controlled settings, which may not fully capture the complexities of real-world data and human behavior
- Why unresolved: The paper does not provide evidence of the framework's effectiveness in uncontrolled, real-world environments where assumptions about data and decision-making may be violated
- What evidence would resolve it: Applying the framework to real-world datasets with known biases and comparing its performance to other methods in those settings

## Limitations

- The method's performance heavily depends on the quality and representativeness of both the human decision data and the gold standard labels
- The assumption that human decisions can be accurately modeled by ML algorithms may not hold in all domains
- The approach requires careful handling of sensitive attributes and potential feedback loops in real-world applications

## Confidence

- **High confidence** in the theoretical framework and mathematical derivations
- **Medium confidence** in empirical results due to potential variability in real-world datasets
- **Low confidence** in scalability to highly complex decision processes with many interacting factors

## Next Checks

1. Test the method on real-world human decision datasets from different domains to assess generalizability
2. Conduct sensitivity analysis on the amount and quality of gold standard data needed for reliable bias estimation
3. Evaluate the method's performance when human decision processes involve complex, non-linear interactions between features