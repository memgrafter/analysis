---
ver: rpa2
title: Uncovering Autoregressive LLM Knowledge of Thematic Fit in Event Representation
arxiv_id: '2410.15173'
source_url: https://arxiv.org/abs/2410.15173
tags:
- thematic
- reasoning
- prompting
- role
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether autoregressive large language
  models (LLMs) possess consistent, expressible knowledge about thematic fit - the
  compatibility between a predicate, argument, and semantic role. The authors evaluate
  both closed (GPT-4-turbo) and open (CodeLlama2) state-of-the-art LLMs on psycholinguistic
  datasets along three axes: reasoning form (simple vs.'
---

# Uncovering Autoregressive LLM Knowledge of Thematic Fit in Event Representation

## Quick Facts
- arXiv ID: 2410.15173
- Source URL: https://arxiv.org/abs/2410.15173
- Reference count: 24
- Key outcome: GPT models achieve state-of-the-art thematic fit estimation results across multiple datasets

## Executive Summary
This paper investigates whether autoregressive large language models (LLMs) possess consistent, expressible knowledge about thematic fit - the compatibility between a predicate, argument, and semantic role. The authors evaluate both closed (GPT-4-turbo) and open (CodeLlama2) state-of-the-art LLMs on psycholinguistic datasets along three experimental axes: reasoning form (simple vs. step-by-step prompting), input form (lemma tuples vs. generated sentences), and output form (numeric vs. categorical scores).

Key findings show that GPT models acquired substantial linguistic knowledge for thematic fit estimation, achieving new state-of-the-art results on all tested datasets. Step-by-step prompting dramatically improved performance on location roles but had mixed effects elsewhere. Generated sentences helped in some settings but hurt performance in others, particularly when incoherent sentences were generated. Predefined categorical output generally improved GPT results across the board, while having the opposite effect on Llama.

## Method Summary
The study evaluates off-the-shelf pre-trained autoregressive LLMs (GPT-4-turbo and CodeLlama2) on four psycholinguistic thematic fit datasets: McRae et al. (1998), Padó et al. (2006), and Ferretti et al. (2001) Fer-Loc and Fer-Ins. The evaluation tests eight experimental settings combining Simple vs. Step-by-Step Prompting, Head Lemma Tuples vs. Generated Sentences input, and Numeric vs. Categorical output. The models generate thematic fit scores that are compared to human ratings using Spearman's Rank Correlation coefficient, with p-values < 10^-7 considered significant.

## Key Results
- GPT models achieved new state-of-the-art thematic fit estimation results across all tested datasets
- Step-by-step prompting dramatically improved performance on location roles but had mixed effects elsewhere
- Generated sentences helped only in few settings and lowered results in many others
- Predefined categorical output raised GPT's results across the board with few exceptions, but lowered Llama's

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-thought prompting significantly improves LLM thematic fit estimation for roles with self-explanatory semantic meaning.
- Mechanism: Step-by-step prompting decomposes thematic fit estimation into sub-steps (listing argument properties, listing required role properties, estimating fit), providing a "scratchpad" that helps LLMs leverage their internal linguistic knowledge.
- Core assumption: LLMs possess relevant linguistic knowledge but need structured reasoning to access it effectively.
- Evidence anchors:
  - [abstract] "Step-by-step prompting dramatically improved performance on location roles but had mixed effects elsewhere"
  - [section] "Step-by-Step Prompting fulfilled its intended goal of providing both LLMs with a 'scratchpad' to help them calculate inferences given their internal knowledge"
  - [corpus] Found 25 related papers; average neighbor FMR=0.412 indicates moderate relevance to reasoning techniques
- Break condition: When reasoning chains include "bad" reasoning early on, it negatively affects the rest of the chain and thematic fit estimation quality.

### Mechanism 2
- Claim: Generated sentences as context help thematic fit estimation only when they are semantically coherent and properly filtered.
- Mechanism: Providing full sentences as context helps LLMs understand the semantic role better than lemma tuples alone, but incoherent sentences can hurt reasoning and performance.
- Core assumption: LLMs were trained on full sentences rather than lemma tuples, so sentence context should help understanding.
- Evidence anchors:
  - [abstract] "Generated sentences helped only in few settings, and lowered results in many others"
  - [section] "we find that the human raters were indeed provided full sentences for the rating... but this fact seems to have been lost over the years"
  - [corpus] Found 25 related papers; average citations=0.0 suggests limited direct research on sentence generation for thematic fit
- Break condition: When LLMs generate semantically incoherent sentences or sentences where the argument doesn't appear in the requested semantic role.

### Mechanism 3
- Claim: Predefined categorical output improves thematic fit estimation results for GPT models but has the opposite effect on Llama.
- Mechanism: LLMs designed for textual tasks perform better with categorical outputs that align with their training, while numeric outputs require post-processing that may introduce inconsistencies.
- Core assumption: LLMs are primarily designed for text-based tasks, so categorical outputs are more natural than numeric conversions.
- Evidence anchors:
  - [abstract] "Predefined categorical (compared to numeric) output raised GPT's results across the board with few exceptions, but lowered Llama's"
  - [section] "LLMs were designed to handle textual data, so outputting numeric scores may lead to inconsistent results"
  - [corpus] Found 25 related papers; no direct evidence on output format effects on thematic fit
- Break condition: When the post-training methods differ significantly between models (like RLHF), affecting how they handle different output formats.

## Foundational Learning

- Concept: Thematic fit estimation
  - Why needed here: The paper's core task is measuring compatibility between predicates, arguments, and semantic roles, which requires understanding this specific linguistic concept.
  - Quick check question: What is the difference between thematic fit estimation and semantic role labeling?

- Concept: Chain-of-thought prompting
  - Why needed here: The paper tests whether structured reasoning improves thematic fit estimation, making understanding this prompting technique essential.
  - Quick check question: How does chain-of-thought prompting differ from simple prompting in breaking down complex tasks?

- Concept: Spearman's Rank Correlation
  - Why needed here: The paper uses this statistical measure to evaluate thematic fit estimation results against human judgments.
  - Quick check question: What does Spearman's correlation measure, and why is it appropriate for comparing model outputs to human ratings?

## Architecture Onboarding

- Component map: Prompt → LLM inference → (optional) sentence generation and filtering → output formatting → correlation calculation → comparison with baselines
- Critical path: Prompt → LLM inference → (optional) sentence generation and filtering → output formatting → correlation calculation → comparison with baselines
- Design tradeoffs: Using generated sentences adds context but requires filtering for coherence; categorical outputs simplify LLM responses but require conversion; step-by-step prompting helps some roles but hurts others
- Failure signatures: Low correlation scores, semantically incoherent generated sentences, inability to distinguish between different semantic roles, early bad reasoning affecting entire chains
- First 3 experiments:
  1. Simple prompting with head lemma tuples and numeric output (baseline experiment)
  2. Simple prompting with generated sentences and numeric output (testing input form impact)
  3. Step-by-step prompting with head lemma tuples and numeric output (testing reasoning form impact)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do large language models perform on thematic fit tasks across different languages, and what factors contribute to any observed differences?
- Basis in paper: [inferred]
- Why unresolved: The study was limited to English data, as noted in the limitations section. Thematic fit is considered a universal concept, but the paper acknowledges that findings may not generalize across languages without further validation.
- What evidence would resolve it: Conducting experiments on thematic fit tasks using datasets from multiple languages and comparing the performance of LLMs across these datasets would provide insights into language-specific factors influencing thematic fit estimation.

### Open Question 2
- Question: What specific prompt engineering techniques could further improve the performance of LLMs on thematic fit tasks, particularly for complex roles like Instrument?
- Basis in paper: [explicit]
- Why unresolved: The paper suggests that complex reasoning is required for roles like Instrument, but it does not explore advanced prompt engineering techniques that could enhance LLM performance in these cases.
- What evidence would resolve it: Testing various prompt engineering strategies, such as more detailed step-by-step reasoning prompts or the use of external knowledge bases, could reveal techniques that improve thematic fit estimation for complex roles.

### Open Question 3
- Question: How does the inclusion of context, such as generated sentences, affect the ability of LLMs to filter out semantically incoherent inputs, and what methods could enhance this filtering?
- Basis in paper: [explicit]
- Why unresolved: The paper notes that generated sentences sometimes include incoherent content, which can negatively impact thematic fit estimation. However, it does not propose specific methods for improving the filtering of such sentences.
- What evidence would resolve it: Developing and testing improved filtering algorithms or techniques, such as using coherence scores or semantic validation models, could enhance the ability of LLMs to distinguish between coherent and incoherent sentences, thereby improving thematic fit estimation.

## Limitations
- Findings based on four psycholinguistic datasets may not generalize to broader linguistic phenomena
- Performance gap between GPT and Llama models raises questions about whether results reflect fundamental LLM capabilities or differences in training approaches
- Study's reliance on Spearman correlation as the sole metric may miss other important aspects of thematic fit estimation quality

## Confidence

**High Confidence:** The finding that GPT models achieve state-of-the-art results on thematic fit datasets is well-supported by the data. The improvement from step-by-step prompting on location roles is also clearly demonstrated.

**Medium Confidence:** The claim that generated sentences generally hurt performance is supported but may depend heavily on the specific filtering process used. The difference between numeric and categorical output effects requires careful interpretation given the post-processing involved.

**Low Confidence:** The explanation for why Llama underperforms GPT is speculative. The claim about LLMs performing better on location roles due to simpler reasoning is based on indirect evidence.

## Next Checks

1. **Generalization Test:** Evaluate the same experimental setup on additional psycholinguistic datasets to assess whether the observed patterns hold across different thematic fit scenarios and semantic roles.

2. **Output Format Validation:** Compare LLM-generated numeric scores against directly outputted categorical scores using the same models to isolate whether performance differences are due to format conversion or underlying reasoning differences.

3. **Reasoning Chain Analysis:** Systematically analyze the reasoning chains produced by both models across different semantic roles to identify specific failure patterns and validate the claim about simpler reasoning for location roles versus complex reasoning for instrument roles.