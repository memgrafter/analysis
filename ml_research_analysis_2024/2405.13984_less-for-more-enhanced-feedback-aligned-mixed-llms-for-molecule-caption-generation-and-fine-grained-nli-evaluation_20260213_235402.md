---
ver: rpa2
title: 'Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption Generation
  and Fine-Grained NLI Evaluation'
arxiv_id: '2405.13984'
source_url: https://arxiv.org/abs/2405.13984
tags:
- molecule
- arxiv
- impacts
- merging
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work enhances molecule language models by improving inference
  and evaluation capabilities with minimal training data. It explores synergies between
  alignment fine-tuning and model merging in a crossmodal setup, revealing that extensive
  training does not guarantee better models and that effective alignment methods balance
  structured learning and generalization.
---

# Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption Generation and Fine-Grained NLI Evaluation

## Quick Facts
- **arXiv ID**: 2405.13984
- **Source URL**: https://arxiv.org/abs/2405.13984
- **Reference count**: 29
- **Primary result**: Proposed approach significantly outperforms state-of-the-art models on out-of-distribution data while reducing dependence on human-labeled data

## Executive Summary
This paper presents an innovative approach to enhancing molecule language models by improving inference and evaluation capabilities with minimal training data. The authors explore synergies between alignment fine-tuning and model merging in a crossmodal setup, demonstrating that extensive training does not guarantee better models. By balancing structured learning and generalization through specific alignment methods, the proposed approach achieves superior performance on out-of-distribution data while significantly reducing dependence on human-labeled data. The work also introduces a novel atomic-level cross-NLI evaluation method that provides more accurate and granular assessment of generated captions compared to widely adopted NLI methods.

## Method Summary
The approach combines model merging techniques (SLERP and TIES) with alignment fine-tuning methods (CPO, DPO, KTO, and SFT) to create enhanced molecule language models. Starting with pretrained Meditron models, the method applies alignment fine-tuning using minimal training data from the L+M-24 dataset (only 10% used), then merges per-task pretrained models to create versatile models without requiring original training data. The atomic-level cross-NLI evaluation method decomposes reference and generated texts into atomic premises and hypotheses, calculating probability distributions of contradiction and entailment via an NLI model to obtain hallucination and coverage metrics.

## Key Results
- Outperforms state-of-the-art models on out-of-distribution data while using only 10% of training data
- CPO alignment method effectively balances structured learning and generalization through uniform reference model
- Atomic-level cross-NLI evaluation demonstrates superior performance compared to widely adopted NLI methods for caption assessment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CPO balances structured learning and generalization, enabling effective crossmodal training with minimal data
- **Mechanism**: CPO uses behavior cloning and SFT to align with preferred data while reducing bias through a uniform reference model that assigns equal likelihood to all outputs
- **Core assumption**: A uniform reference model prevents overfitting and ensures balanced learning across all potential outputs
- **Evidence anchors**: CPO is the only alignment method that managed both crossmodal agnostic and minimal training effectively; it aligns with preferred data while reducing bias through uniform reference model
- **Break condition**: If uniform reference model fails to capture true output distribution or behavior cloning/SFT components are unbalanced

### Mechanism 2
- **Claim**: Model merging combines pretrained models to reduce reliance on human-labeled data and improve generalization
- **Mechanism**: Techniques like SLERP and TIES interpolate weights or merge parameter magnitudes to create versatile models without original training data
- **Core assumption**: Strengths of individual pretrained models can be effectively combined for more robust generalization
- **Evidence anchors**: Model merging builds versatile models without needing original training data or expensive computation
- **Break condition**: If merging techniques fail to combine strengths effectively or introduce conflicts/instability

### Mechanism 3
- **Claim**: Atomic-level cross-NLI evaluation provides more accurate assessment of generated captions
- **Mechanism**: Decomposes texts into atomic premises and hypotheses, calculates contradiction/entailment probability distributions, and applies row-wise operations for hallucination and coverage metrics
- **Core assumption**: Atomic decomposition captures subtle nuances and distinctions in captions for reliable evaluation
- **Evidence anchors**: Method operates at right granularity, precisely capturing small distinctions and subtle nuances in captions
- **Break condition**: If decomposition is inaccurate or NLI model struggles with domain-specific content

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - *Why needed here*: Aligns language models with human preferences for improving generated caption quality and relevance
  - *Quick check question*: What are the three main steps involved in training a language model with RLHF?

- **Concept: Model Merging**
  - *Why needed here*: Combines pretrained models without original training data, enabling versatile models with improved performance
  - *Quick check question*: What are the two main types of model merging techniques mentioned in the paper?

- **Concept: Natural Language Inference (NLI)**
  - *Why needed here*: Assesses relationships between generated and reference captions to measure accuracy and completeness
  - *Quick check question*: What are the two main types of relationships that NLI models evaluate between premises and hypotheses?

## Architecture Onboarding

- **Component map**: Pretrained Meditron models -> Alignment fine-tuning (CPO, DPO, KTO) -> Model merging (SLERP, TIES) -> Atomic-level cross-NLI evaluation -> Evaluation metrics

- **Critical path**:
  1. Initialize Meditron with crossmodal weights
  2. Apply alignment fine-tuning using CPO
  3. Merge pretrained models using SLERP
  4. Evaluate performance using atomic-level cross-NLI
  5. Compare results with baseline models and established metrics

- **Design tradeoffs**:
  - Minimal training vs. performance: Less training data may lead to lower performance compared to larger datasets
  - Model merging techniques: Different techniques may have varying effects on performance and stability
  - NLI evaluation granularity: Atomic-level evaluation may be more accurate but computationally expensive

- **Failure signatures**:
  - Alignment fine-tuning fails to improve performance: Check crossmodal weight initialization and algorithm suitability
  - Model merging introduces instability: Investigate merging technique and model strength combination
  - NLI evaluation produces inaccurate results: Verify atomic decomposition accuracy and NLI model performance on domain content

- **First 3 experiments**:
  1. Fine-tune Meditron using CPO on 10% of L+M-24 dataset, evaluate on out-of-distribution data
  2. Merge Meditron with another pretrained model using SLERP, assess performance impact
  3. Implement atomic-level cross-NLI evaluation, compare with established NLI methods on sample captions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does atomic-level cross-NLI evaluation perform on longer, more complex captions with interdependent sentences or contextual relationships?
- **Basis in paper**: Method handles multiple content units effectively but may struggle with complex text where decomposition could lose context or coherence
- **Why unresolved**: Experiments focus on shorter captions (average 31 words) with simple, independent content units
- **What evidence would resolve it**: Experiments comparing atomic-level NLI on long captions (100+ words) with interdependent sentences versus traditional NLI methods, showing both accuracy and contextual relationship preservation

### Open Question 2
- **Question**: What is the impact of different decomposition strategies or LLMs on atomic-level cross-NLI evaluation performance?
- **Basis in paper**: Uses specific models (Meta-Llama-3-8B for decomposition, DeBERTa for NLI) without exploring sensitivity to different approaches
- **Why unresolved**: Paper doesn't explore how alternative decomposition strategies or different LLMs might affect evaluation accuracy and robustness
- **What evidence would resolve it**: Comparative studies using different LLMs for decomposition and different NLI models, measuring consistency and reliability across combinations

### Open Question 3
- **Question**: How does model merging performance change when applied to other scientific domains beyond chemistry?
- **Basis in paper**: Methods are tailored specifically for chemical domain, with uncertainty about generalization to other fields
- **Why unresolved**: Paper demonstrates success in chemistry but doesn't explore applicability to domains like physics, biology, or materials science
- **What evidence would resolve it**: Experiments applying techniques to other scientific domains, measuring performance improvements and identifying domain-specific modifications needed

## Limitations
- Data-specific concerns: Heavy reliance on L+M-24 dataset with limited testing across diverse molecular domains
- Methodological constraints: Lacks ablation studies isolating contributions of individual components
- Implementation details: Key hyperparameters and exact atomic-level NLI implementation details not fully specified

## Confidence

- **High confidence**: Core observation that extensive training doesn't guarantee better models is well-supported by experimental results
- **Medium confidence**: CPO mechanism balancing structured learning through uniform reference model is theoretically sound but lacks direct empirical validation
- **Medium confidence**: Atomic-level cross-NLI evaluation shows superior performance but lacks head-to-head comparisons on established benchmarks

## Next Checks

1. **Mechanism validation experiment**: Conduct controlled experiments isolating CPO's uniform reference model component by comparing performance against variants using different reference distributions (learned vs. uniform vs. random)

2. **Cross-domain generalization test**: Evaluate proposed models and atomic-level cross-NLI method on at least two additional molecular datasets from different domains (e.g., drug discovery vs. materials science)

3. **Ablation study on model components**: Perform systematic ablation experiments removing or replacing individual components (alignment fine-tuning method, model merging technique, evaluation metric) to quantify relative contributions to performance gains