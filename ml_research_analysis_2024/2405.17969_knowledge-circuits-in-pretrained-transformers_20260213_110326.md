---
ver: rpa2
title: Knowledge Circuits in Pretrained Transformers
arxiv_id: '2405.17969'
source_url: https://arxiv.org/abs/2405.17969
tags:
- knowledge
- language
- circuit
- attention
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how knowledge is stored and expressed\
  \ in pretrained transformers by introducing the concept of knowledge circuits\u2014\
  subgraphs in the computation graph that collaboratively encode specific knowledge.\
  \ Unlike prior work focusing on isolated components like MLPs or attention heads,\
  \ the authors analyze the cooperative behavior of these components across the model."
---

# Knowledge Circuits in Pretrained Transformers

## Quick Facts
- arXiv ID: 2405.17969
- Source URL: https://arxiv.org/abs/2405.17969
- Reference count: 40
- This paper investigates how knowledge is stored and expressed in pretrained transformers by introducing the concept of knowledge circuits—subgraphs in the computation graph that collaboratively encode specific knowledge.

## Executive Summary
This paper introduces the concept of knowledge circuits to understand how pretrained transformers store and express knowledge. Unlike previous work focusing on isolated components, the authors analyze cooperative behavior across the model's computation graph. They identify circuits for various knowledge types (factual, commonsense, linguistic, bias) and demonstrate that these circuits can maintain over 70% of original performance using less than 10% of the original graph. The study reveals that knowledge is gradually aggregated from early to middle layers, with special components like mover heads and relation heads playing key roles. The authors also use circuits to explain behaviors like hallucination and in-context learning, and show how different knowledge editing methods (ROME vs FT-M) affect circuit structure differently.

## Method Summary
The authors construct knowledge circuits by identifying critical edges in the transformer's computation graph using causal mediation analysis and ablation studies. They start with a complete graph and iteratively remove edges with minimal impact on knowledge recall performance, stopping when a threshold τ is reached. The method analyzes how information flows through attention heads, MLPs, and embeddings across layers, identifying special components like mover heads (which transfer context information) and relation heads (which capture relational information). The circuits are evaluated for completeness by testing standalone performance on isolated test splits, and the impact of knowledge editing methods (ROME and FT-M) is analyzed by comparing circuit changes between edited and original models.

## Key Results
- Knowledge circuits can maintain over 70% of original performance using less than 10% of the original computation graph
- Mover heads and relation heads play distinct specialized roles in transferring and contextualizing knowledge within circuits
- ROME editing gradually integrates knowledge through circuits while FT-M directly overwrites information, creating different circuit behaviors
- Hallucination occurs when mover heads fail to select correct information, and in-context learning emerges from new attention heads that form after fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge circuits are human-interpretable subgraphs in the transformer's computation graph that collaboratively encode specific knowledge.
- Mechanism: The model stores knowledge in distributed components (attention heads, MLPs, embeddings) that work together as circuits, with information gradually aggregated from early to middle layers.
- Core assumption: Knowledge is not stored in isolated neurons but emerges from the interaction of multiple components working together.
- Evidence anchors:
  - [abstract] "we delve into the computation graph of the language model to uncover the knowledge circuits that are instrumental in articulating specific knowledge"
  - [section 3.1] "we aim to explore the cooperation between different components in Transformers like attention heads, MLPs, and embeddings"
  - [corpus] Strong - corpus provides multiple related papers on circuit discovery and knowledge editing

### Mechanism 2
- Claim: Special components like mover heads and relation heads play distinct roles in knowledge circuits by transferring and contextualizing information.
- Mechanism: Mover heads extract relevant information from context and transport it to the final token position, while relation heads capture relational information that guides subsequent processing.
- Core assumption: Specific attention heads have specialized functions that can be identified through ablation studies and their distinct activation patterns.
- Evidence anchors:
  - [section 4] "We can find several important attention heads that demonstrate specific behavior, including the mover head [31], relation head [17, 43] and mixture head [17, 43]"
  - [section 4] "mover head L15H3 attends to the 'controller' token and returns 'controller' as the output, while in the edited model, the attention head's output moves to the 'Intel'"
  - [corpus] Moderate - corpus includes papers on attention head specialization but limited direct evidence for these specific head types

### Mechanism 3
- Claim: Knowledge editing methods (ROME vs FT-M) affect circuits differently - ROME gradually integrates edits while FT-M directly overwrites information.
- Mechanism: ROME modifies MLP weights to add new key-value pairs that are then propagated through circuits, while FT-M directly writes edited knowledge into specific components, creating dominant influence.
- Core assumption: The layer at which editing occurs determines how the edited information propagates through the circuit structure.
- Evidence anchors:
  - [section 5] "ROME tends to incorporate edited information primarily at the edited layer. Subsequent mover heads then transport this information to the residual stream"
  - [section 5] "during fine-tuning, the edited token is directly integrated into the language model, exerting a dominant influence on subsequent predictions"
  - [corpus] Moderate - corpus includes related work on knowledge editing but limited comparative analysis of ROME vs FT-M circuit effects

## Foundational Learning

- Concept: Computation graph and residual connections in transformers
  - Why needed here: Understanding how attention heads and MLPs connect through residual connections is fundamental to grasping circuit theory
  - Quick check question: How does the residual connection formula Rl = Rl-1 + Σ(Al,j + Ml) enable information flow between components?

- Concept: Ablation studies and causal mediation analysis
  - Why needed here: The paper uses systematic ablation of edges/nodes to identify critical components in knowledge circuits
  - Quick check question: What distinguishes zero ablation from mean ablation, and how does each affect the measurement of component importance?

- Concept: Knowledge neuron theory and its limitations
  - Why needed here: The paper builds on but extends beyond the knowledge neuron hypothesis by examining component cooperation
  - Quick check question: What evidence suggests that knowledge may not be stored in isolated neurons but requires distributed circuit representation?

## Architecture Onboarding

- Component map:
  - Input embeddings → Residual blocks (Attention heads + MLPs) → Output unembedding
  - Special components: Mover heads (extract context information), Relation heads (capture relationships), Mixture heads (combine functions)
  - Circuit discovery tool: ACDC2 for automated circuit identification

- Critical path:
  1. Construct circuit using validation data with threshold τ for edge removal
  2. Analyze special component behaviors through ablation studies
  3. Evaluate circuit performance in isolation vs original model
  4. Apply knowledge editing methods and observe circuit changes
  5. Use circuits to explain model behaviors (hallucinations, in-context learning)

- Design tradeoffs:
  - Granularity: Coarse-grained circuits vs fine-grained neuron-level analysis
  - Automation: Automated circuit discovery vs manual inspection
  - Generality: Circuit-specific findings vs general principles applicable across models

- Failure signatures:
  - Incomplete circuits (too high τ threshold) showing poor standalone performance
  - Overly complex circuits (too low τ threshold) with unnecessary components
  - Inconsistent special component identification across different knowledge types

- First 3 experiments:
  1. Replicate circuit construction for a simple factual knowledge triple and verify standalone performance
  2. Conduct ablation studies on identified mover and relation heads to confirm their specialized roles
  3. Compare circuit changes after ROME vs FT-M editing on the same knowledge fact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the exact mechanisms by which mover heads are activated to transfer knowledge across different types of knowledge or relational contexts?
- Basis in paper: [explicit] The paper discusses mover heads and their role in transferring information to the final token position, noting that these heads are reused across different types of knowledge or relational contexts. It also mentions that understanding why neurons are sometimes "monosemantic" and sometimes "polysemantic" is an area requiring further exploration.
- Why unresolved: The paper highlights that while the behavior of mover heads is observed, the specific mechanisms by which they are activated and the conditions under which they operate are not fully understood. This is due to the complexity of neural networks and the need for more detailed studies to elucidate these mechanisms.
- What evidence would resolve it: Detailed experimental studies that isolate the conditions and inputs that trigger specific mover heads, possibly using techniques like ablation studies or fine-tuning experiments, would help clarify the activation mechanisms.

### Open Question 2
- Question: How does the discrepancy between middle layers and the output unembedding matrix affect the analysis of circuit components' behavior in early layers?
- Basis in paper: [explicit] The paper mentions that using the logit lens to detect and analyze component information may encounter discrepancies between the middle layers and the output unembedding matrix, which can hinder a comprehensive analysis of the circuit components' behavior in early layers.
- Why unresolved: The issue of discrepancies between intermediate representations and final outputs is complex and requires more robust techniques to bridge this gap. The current methods, like the logit lens, may not fully capture the nuances of these early layers.
- What evidence would resolve it: Developing and applying new techniques that can more accurately map the intermediate representations to the final outputs, such as training specific unembedding matrices or using attention lens methods, would provide clearer insights into the behavior of early layer components.

### Open Question 3
- Question: What are the implications of knowledge circuit reuse across different tasks and how does it affect the overall performance and generalization of the model?
- Basis in paper: [explicit] The paper discusses the phenomenon of circuit component reuse across tasks, noting that some attention heads are reused for related relations. It suggests that these reused heads can be considered topic heads and that further investigation into this distinction is warranted.
- Why unresolved: While the reuse of circuits across tasks is observed, the broader implications for model performance and generalization are not fully explored. Understanding how this reuse affects the model's ability to generalize to new tasks and maintain performance is crucial for practical applications.
- What evidence would resolve it: Conducting experiments that test the model's performance on new tasks using the same circuits, and analyzing the impact of circuit reuse on generalization and performance metrics, would provide insights into the implications of this phenomenon.

## Limitations

- Circuit identification method relies heavily on threshold selection (τ) which varies across knowledge types but lacks clear justification for specific values chosen
- Generalizability of identified circuits across different model architectures remains uncertain, as the study primarily focuses on GPT-2 and TinyLLaMA
- Interpretation of mover and relation heads as having specialized functions is based on observed behaviors during specific tasks, but causal evidence for these roles across diverse contexts is limited

## Confidence

- High confidence: The overall framework for identifying knowledge circuits and demonstrating that they can maintain significant performance with reduced computation (70%+ performance with <10% of graph)
- Medium confidence: The characterization of specific component roles (mover heads, relation heads) as having specialized functions
- Medium confidence: The comparative analysis of ROME vs FT-M editing methods on circuit behavior

## Next Checks

1. **Circuit Robustness Test**: Apply the same circuit discovery methodology to a different transformer architecture (e.g., BERT or RoBERTa) using identical knowledge types to verify if the identified special components (mover heads, relation heads) show consistent patterns across architectures.

2. **Cross-Knowledge Generalization**: Test whether circuits identified for factual knowledge can be successfully applied to or partially overlap with circuits for commonsense or linguistic knowledge, measuring performance degradation and identifying shared vs. specialized components.

3. **Temporal Dynamics of Editing**: Conduct longitudinal analysis of circuit changes over multiple ROME editing iterations on the same knowledge fact, measuring how edited information propagates through the circuit structure over time and whether circuits eventually stabilize or continue evolving.