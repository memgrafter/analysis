---
ver: rpa2
title: 'REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal
  Body Movement Signals in the Wild'
arxiv_id: '2403.01229'
source_url: https://arxiv.org/abs/2403.01229
tags:
- speaking
- status
- video
- audio
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REWIND, a new multimodal dataset for studying
  speaking status segmentation from body movement signals in natural, crowded social
  settings. Unlike prior work that relied on video annotations or noisy low-frequency
  audio, REWIND provides synchronized high-quality individual audio recordings alongside
  video, pose tracks, and chest-worn accelerometer data from 33 participants in a
  professional networking event.
---

# REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal Body Movement Signals in the Wild

## Quick Facts
- **arXiv ID**: 2403.01229
- **Source URL**: https://arxiv.org/abs/2403.01229
- **Reference count**: 40
- **Primary result**: Multimodal fusion of video, pose, and acceleration achieved AUC=0.648 for speaking status segmentation

## Executive Summary
This paper introduces REWIND, a new multimodal dataset for studying speaking status segmentation from body movement signals in natural, crowded social settings. Unlike prior work that relied on video annotations or noisy low-frequency audio, REWIND provides synchronized high-quality individual audio recordings alongside video, pose tracks, and chest-worn accelerometer data from 33 participants in a professional networking event. The authors present three baseline methods—video-based, acceleration-based, and pose-based—to predict speaking status at a fine 20Hz temporal resolution, surpassing previous datasets in granularity. A multimodal fusion approach combining all three modalities achieved the best performance (AUC = 0.648). The availability of high-fidelity audio enables precise speaking status labels and opens new possibilities for cross-modal studies of social signals like laughter and back-channeling, while also addressing privacy concerns by supporting audio-free detection methods.

## Method Summary
The REWIND dataset was collected during a professional networking event with 33 participants wearing chest-mounted accelerometers, individual microphones, and being recorded by multiple video cameras. Speaking status was annotated using a pipeline that denoised individual audio recordings with SepFormer, performed speaker diarization with NeMo, and manually selected speaker clusters. Three baseline methods were implemented: a 3D CNN for video-based detection, a time series CNN for acceleration data, and another time series CNN for pose tracks. All models predicted 20Hz binary speaking status using 3-second input segments. A multimodal fusion approach combined these three modalities by averaging their output masks. Models were trained and evaluated using 3-fold cross-validation at the subject level, with AUC as the primary evaluation metric.

## Key Results
- Multimodal fusion combining video, pose, and acceleration achieved AUC=0.648
- Video-based method achieved AUC=0.609, acceleration-based achieved AUC=0.613, and pose-based achieved AUC=0.532
- REWIND provides speaking status labels at 20Hz temporal resolution, significantly finer than previous datasets
- Audio-based annotations captured more back-channeling events and short utterances compared to video-based annotations

## Why This Works (Mechanism)

### Mechanism 1
High-quality individual audio recordings provide more accurate and fine-grained speaking status labels compared to video-based annotations, especially for short utterances like back-channels. The audio-based VAD process (denoising via SepFormer + speaker diarization via NeMo) isolates the microphone wearer's voice from background noise and interlocutor speech, enabling precise segmentation at 20Hz resolution. Core assumption: The denoising and diarization pipeline reliably separates the target speaker's voice from other speakers and background noise in cocktail party conditions.

### Mechanism 2
Body movement signals (video, pose, acceleration) can predict speaking status without audio access, enabling privacy-preserving detection. Speech-related gestures (hand/head movements) are captured by video, pose tracks, and accelerometer data, providing modality-agnostic cues for speaking status. Core assumption: Speech-related body movements are consistent and detectable across different people and contexts.

### Mechanism 3
Multimodal fusion of video, pose, and acceleration outperforms individual modalities for speaking status segmentation. Each modality captures different aspects of speech-related movement (pose for gestures, video for subtle movements, acceleration for 3D characteristics), and their combination provides complementary information. Core assumption: The modalities are complementary and their fusion improves prediction accuracy.

## Foundational Learning

- **Concept: Speech processing and VAD (Voice Activity Detection)**
  - Why needed here: To understand how audio-based labels are generated and their importance for ground truth
  - Quick check question: What are the main challenges in VAD for cocktail party environments?

- **Concept: Multimodal machine learning and fusion techniques**
  - Why needed here: To understand how different body movement modalities are combined for improved prediction
  - Quick check question: What are the advantages and challenges of late fusion (averaging output masks) versus early fusion?

- **Concept: Action recognition from pose and skeleton data**
  - Why needed here: To understand the limitations of pose-based methods and potential improvements
  - Quick check question: How does noise in pose tracking affect action recognition performance?

## Architecture Onboarding

- **Component map**: Data collection (microphones, accelerometers, video) -> Annotation pipeline (denoising → diarization → manual selection) -> Model training (3 baseline methods + fusion) -> Evaluation (3-fold CV, AUC metric)
- **Critical path**: Data collection → Annotation → Model training → Evaluation → Analysis
- **Design tradeoffs**: High-resolution audio enables precise labels but raises privacy concerns; multimodal fusion improves accuracy but increases complexity
- **Failure signatures**: Poor VAD accuracy, noisy pose tracks, model overfitting to individual subjects
- **First 3 experiments**:
  1. Train and evaluate video-based model alone
  2. Train and evaluate acceleration-based model alone
  3. Train and evaluate multimodal fusion model

## Open Questions the Paper Calls Out

### Open Question 1
How does the temporal resolution of audio-based speaking status annotations compare to video-based annotations in terms of capturing back-channeling events? The paper suggests that audio-based annotations capture more back-channeling events but does not provide a direct comparison of temporal resolution or number of events captured by each method.

### Open Question 2
How does the performance of pose-based speaking status detection methods compare to video and acceleration-based methods when using high-quality pose tracks? The paper notes poor pose method performance but attributes this to noisy tracks, suggesting pre-training could improve results without exploring this approach.

### Open Question 3
How does the mixed-consent data collection design of REWIND affect the analysis and prediction of social signals from group-level information? The paper mentions this design affords opportunities for investigating partially complete data but does not explore the implications for group-level analysis.

## Limitations

- Dataset size of 33 participants may limit generalizability to broader populations
- Performance depends on reliability of denoising and diarization pipeline in noisy environments
- 20Hz temporal resolution may still miss very short back-channel utterances
- Findings may not generalize to social contexts beyond professional networking events

## Confidence

- **High confidence**: Dataset collection methodology and technical specifications are well-documented and reproducible
- **Medium confidence**: Baseline model performances and multimodal fusion results are reported but lack detailed hyperparameter settings
- **Low confidence**: Generalization to other social contexts and robustness to individual variation remain untested

## Next Checks

1. Test model performance on held-out participants not seen during training to assess generalization
2. Evaluate speaking status detection accuracy across different cultural contexts or social settings
3. Conduct ablation studies to quantify the contribution of each modality to the multimodal fusion performance