---
ver: rpa2
title: 'LAPDoc: Layout-Aware Prompting for Documents'
arxiv_id: '2402.09841'
source_url: https://arxiv.org/abs/2402.09841
tags:
- document
- layout
- prompt
- which
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to improve large language models (LLMs)
  for document understanding tasks by incorporating layout information into prompts.
  The authors propose "verbalization" strategies that encode document text and spatial
  structure into a purely textual format, which is then used as input to text-based
  LLMs without fine-tuning.
---

# LAPDoc: Layout-Aware Prompting for Documents

## Quick Facts
- arXiv ID: 2402.09841
- Source URL: https://arxiv.org/abs/2402.09841
- Reference count: 38
- Primary result: Layout-aware prompting improves LLM performance by up to 15% compared to plain text

## Executive Summary
This paper investigates how to improve large language models (LLMs) for document understanding tasks by incorporating layout information into prompts. The authors propose "verbalization" strategies that encode document text and spatial structure into a purely textual format, which is then used as input to text-based LLMs without fine-tuning. They evaluate multiple verbalization approaches on various document benchmarks, comparing performance with and without layout information. Results show that layout-aware prompting improves LLM performance by up to 15% compared to plain text, with SpatialFormat being the most effective strategy. The approach achieves competitive results with state-of-the-art multi-modal models while offering simplicity and efficiency benefits.

## Method Summary
The method converts document text and geometries extracted via OCR into purely textual representations using different verbalization strategies (PlainText, BoundingBox, BoundingBoxMarkup, CenterPoint, SpatialFormat, SpatialFormatY, PlainHTML). These verbalized documents are inserted into task-specific prompt templates and fed into pre-trained LLMs (ChatGPT 3.5, Solar 70B8Bit) to perform document understanding tasks. The approach evaluates performance across various benchmarks including DUE Benchmark, WebSRC, SROIE, and proprietary KIE datasets, measuring improvements from layout enrichment while considering robustness to OCR inaccuracies through synthetic noise models.

## Key Results
- Layout enrichment improves text-only LLM performance by up to 15% compared to plain text
- SpatialFormat verbalization strategy proves most effective on average across benchmarks
- Text-only LLMs with layout-enriched prompts achieve competitive results with multi-modal models
- Layout-aware prompting shows robustness to OCR errors when using appropriate verbalization strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layout information encoded as textual spatial descriptions can be processed by text-only LLMs to achieve performance competitive with multimodal models
- Mechanism: The SpatialFormat verbalizer converts document geometry into a grid-like textual representation that preserves relative positioning of text elements, allowing the LLM to reason about spatial relationships through text-based pattern recognition
- Core assumption: LLMs can infer document structure from textual representations of spatial relationships without direct visual input
- Evidence anchors:
  - [abstract] "layout enrichment can improve the performance of purely text-based LLMs for document understanding by up to 15% compared to just using plain document text"
  - [section] "SpatialFormat proves to be the best verbalization strategy on average" and "the SpatialFormat verbalization with respect to OCR layout misinterpretations"
- Break condition: When document layout complexity exceeds the LLM's ability to parse spatial relationships from textual descriptions, or when documents contain significant visual elements that cannot be adequately represented textually

### Mechanism 2
- Claim: Prompt templates that clearly specify the expected output format and task structure improve LLM performance on document understanding tasks
- Mechanism: By providing explicit format instructions and examples in the prompt, the LLM can better understand the task requirements and structure its response accordingly
- Core assumption: LLMs can follow detailed format specifications and use them to guide their reasoning and output generation
- Evidence anchors:
  - [section] "We identified two patterns A, B to work best" and the detailed description of prompt templates with DOCUMENT, TASK, FORMAT, and OUTPUT components
  - [section] "Due to the probabilistic nature of LLMs as text generators, their outputs are not guaranteed to conform with the requested format" leading to specific answer extraction procedures
- Break condition: When task complexity exceeds the LLM's ability to follow format specifications, or when the format instructions conflict with the LLM's learned patterns

### Mechanism 3
- Claim: Open-source LLMs can achieve performance comparable to commercial models when given appropriately structured layout-enriched prompts
- Mechanism: The open-source Solar model demonstrates similar capabilities to ChatGPT when processing layout-enriched prompts, suggesting that model size and architecture are sufficient for the task
- Core assumption: The fundamental capabilities needed for document understanding through layout-enriched prompts are present in both commercial and open-source models of similar scale
- Evidence anchors:
  - [section] "The results of both LLMs are on par on SROIE, with Solar performing slightly better" and "Solar is apparently able to make better usage of the layout information"
  - [section] Comparison of ChatGPT 3.5 and Solar 70B8Bit showing competitive performance
- Break condition: When specific capabilities required for document understanding are present in commercial models but absent in open-source alternatives, or when fine-tuning differences create significant performance gaps

## Foundational Learning

- Concept: Document layout understanding
  - Why needed here: The core contribution is adding layout information to text-only prompts, requiring understanding of how spatial relationships in documents affect comprehension
  - Quick check question: What are the key spatial relationships in documents that affect reading order and interpretation?

- Concept: Prompt engineering and template design
  - Why needed here: The paper emphasizes the importance of prompt structure and format specifications for guiding LLM responses
  - Quick check question: How do different prompt template structures affect LLM performance on structured output tasks?

- Concept: OCR and text extraction with geometry
  - Why needed here: The approach relies on accurate extraction of both text and spatial coordinates from documents
  - Quick check question: What are the common sources of error in OCR geometry extraction and how do they affect downstream processing?

## Architecture Onboarding

- Component map:
  - Document processing pipeline: OCR → Layout enrichment → Prompt generation → LLM inference → Answer extraction
  - Verbalization strategies: PlainText, BoundingBox, BoundingBoxMarkup, CenterPoint, SpatialFormat, SpatialFormatY, PlainHTML
  - Noise models: NONE, TRANSLATE, SHUFFLE, NEAREST_NEIGHBOR
  - LLMs: ChatGPT 3.5, Solar 70B8Bit
  - Datasets: DUE benchmark, WebSRC, SROIE, SROIE Challenge, ITForms, ITInvoices

- Critical path:
  1. Document OCR with geometry extraction
  2. Layout enrichment through verbalization
  3. Prompt template assembly with format specifications
  4. LLM inference with temperature=0 for deterministic output
  5. Answer extraction from JSON-formatted responses

- Design tradeoffs:
  - Token efficiency vs layout detail: SpatialFormatY uses fewer tokens than SpatialFormat but may lose some spatial precision
  - Complexity vs performance: More sophisticated verbalizers may improve accuracy but increase computational overhead
  - Format specification vs flexibility: Detailed output formats improve consistency but may limit LLM's natural problem-solving approaches

- Failure signatures:
  - Incorrect spatial reasoning: LLMs misinterpreting document structure despite layout information
  - Format non-compliance: Generated output not matching specified JSON structure
  - Token limit issues: Prompts exceeding context window when using detailed verbalization
  - OCR dependency: Performance degradation when OCR geometry is inaccurate

- First 3 experiments:
  1. Compare PlainText vs SpatialFormat performance on a simple document understanding task to validate layout enrichment benefits
  2. Test different prompt template structures (A vs B) with the same verbalization to isolate prompt engineering effects
  3. Apply noise models to OCR output and measure degradation in performance across verbalization strategies to assess robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of layout-aware prompting scale with increasingly longer documents, particularly for multi-page documents where context length becomes a limiting factor?
- Basis in paper: [inferred] The paper mentions that "A limitation when working with long documents is the context length of LLMs" and that "more work is needed when scaling our solution to multi-page reasoning problems, especially when the number of pages becomes larger."
- Why unresolved: The paper explicitly acknowledges this as an open challenge but does not investigate solutions or performance degradation at scale.
- What evidence would resolve it: Experiments comparing performance across documents of varying lengths and page counts, analysis of token efficiency as document size increases, and exploration of techniques like document chunking or hierarchical processing.

### Open Question 2
- Question: How would incorporating visual input (document images) into the current layout-aware prompting approach compare to text-only methods in terms of performance, cost, and efficiency?
- Basis in paper: [explicit] The conclusion states "An interesting study focus are recent instruction-tuned LLMs with additional visual input such as GPT-4" and notes that "exploring the benefits and trade-offs of including visual input is worthwhile."
- Why unresolved: The paper deliberately focused on text-only representations due to cost considerations but acknowledges this as a future research direction without investigating it.
- What evidence would resolve it: Head-to-head comparisons between text-only layout-aware prompting and multimodal approaches using the same document understanding tasks, analysis of cost per task, and evaluation of when visual input provides meaningful advantages over layout encoding.

### Open Question 3
- Question: What specific prompt template modifications could improve the LLM's ability to recognize when no valid answer exists for a given key in KIE tasks, reducing the tendency to hallucinate responses?
- Basis in paper: [explicit] The paper notes that "we observed that in most cases the LLMs produce outputs and rarely provide an empty response, which lowers their overall score in the evaluation" and suggests "this problem can be reduced by clearer instructions in the prompt."
- Why unresolved: While the paper identifies the problem and suggests prompt improvements, it does not explore or test specific prompt modifications to address this issue.
- What evidence would resolve it: Systematic evaluation of different prompt formulations (explicit rejection instructions, confidence thresholds, multiple rejection examples) on KIE datasets, measurement of empty response rates, and comparison of precision-recall trade-offs for different approaches.

## Limitations

- The evaluation relies on synthetic noise models rather than real-world OCR errors to simulate inaccuracies
- Proprietary datasets (ITForms, ITInvoices) limit reproducibility and external validation
- Performance comparisons between commercial and open-source models may be influenced by fine-tuning differences not fully disclosed
- The approach may struggle with documents containing significant visual elements that cannot be adequately represented textually

## Confidence

- High confidence: Layout-enriched prompts improve text-only LLM performance by 15% compared to plain text (supported by multiple experimental results)
- Medium confidence: SpatialFormat strategy is optimal (based on average performance across datasets but may vary with specific document types)
- Low confidence: Open-source models can achieve performance parity with commercial models for layout-enriched document understanding (limited comparison scope and potential fine-tuning differences)

## Next Checks

1. **Real OCR Error Validation**: Test the verbalization strategies with actual OCR outputs from multiple OCR engines on the same document corpus to validate that the synthetic noise models accurately represent real-world error patterns.

2. **Cross-Model Generalization**: Evaluate the same layout-enriched prompting approach across a broader range of open-source and commercial LLMs (including smaller models) to determine if the performance benefits are model-agnostic or specific to the tested models.

3. **Visual Element Handling**: Create a benchmark specifically designed to test how well layout-enriched text-only approaches handle documents with significant visual elements (charts, diagrams, logos) that cannot be adequately represented through textual spatial descriptions, comparing performance against true multi-modal approaches.