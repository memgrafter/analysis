---
ver: rpa2
title: Constrained Reasoning Chains for Enhancing Theory-of-Mind in Large Language
  Models
arxiv_id: '2409.13490'
source_url: https://arxiv.org/abs/2409.13490
tags:
- ccotom
- belief
- agent
- constraints
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the limited Theory-of-Mind (ToM) ability in
  Large Language Models (LLMs). The authors propose a zero-shot prompting method called
  Constrained Chain-of-ToM (CCoToM) that uses domain knowledge and causal relations
  between ToM dimensions to improve reasoning.
---

# Constrained Reasoning Chains for Enhancing Theory-of-Mind in Large Language Models

## Quick Facts
- arXiv ID: 2409.13490
- Source URL: https://arxiv.org/abs/2409.13490
- Authors: Zizheng Lin; Chunkit Chan; Yangqiu Song; Xin Liu
- Reference count: 40
- Primary result: Zero-shot method (CCoToM) improves ToM reasoning by 15-41 absolute points across LLMs on BigToM/FANTOM datasets

## Executive Summary
This paper addresses the limited Theory-of-Mind (ToM) capabilities in Large Language Models by proposing a zero-shot prompting method called Constrained Chain-of-ToM (CCoToM). The method leverages domain knowledge and causal relations between ToM dimensions to construct explicit reasoning chains. By decomposing complex ToM tasks into sub-tasks and imposing adaptive constraints, CCoToM guides LLMs to reason step-by-step through belief, desire, and action inference. Experiments demonstrate significant improvements over state-of-the-art methods on two prominent ToM datasets, with consistent performance gains across various LLMs and both narrative and conversational contexts.

## Method Summary
CCoToM is a zero-shot prompting method that enhances Theory-of-Mind reasoning in LLMs by constructing explicit reasoning chains. The method first identifies the agent in question, then sequentially infers related ToM dimensions (percept, belief, desire) based on causal relations from the BDI model. Finally, it infers the queried dimension using these related dimensions. Throughout the process, CCoToM adaptively imposes definitional constraints (describing concept semantics) and dependency constraints (specifying causal relations) to introduce inductive biases and improve consistency between inferred dimensions. The approach uses multi-step prompting rather than one-shot inference, reducing cognitive load on the LLM and allowing it to build on previously inferred dimensions.

## Key Results
- CCoToM achieves up to 41.5 and 18.4 absolute point improvements in accuracy on BigToM and FANTOM respectively using GPT-3.5-Turbo
- The method consistently outperforms previous state-of-the-art methods across multiple LLMs (GPT-4, GPT-3.5-Turbo, Llama-2 Chat 70B, Mistral Instruct 7B)
- Removing constraints always leads to accuracy drops across all task types, demonstrating their importance
- Multi-step prompting outperforms one-step variants, with the largest performance gap observed in Forward Action tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CCoToM improves ToM reasoning by explicitly constructing reasoning chains that mirror the causal structure of mental states (belief, desire, percept, action).
- Mechanism: The method decomposes complex ToM tasks into sub-tasks (e.g., infer percept → infer belief → infer action), enforcing dependency constraints that reflect the BDI model's causal relations. This guides the LLM to reason step-by-step rather than attempting the full inference in one pass.
- Core assumption: LLMs can accurately perform sub-task reasoning when given explicit instructions and causal structure, and chaining these sub-tasks yields better overall reasoning than holistic inference.
- Evidence anchors:
  - [abstract] "CCoToM guides LLMs to construct explicit reasoning chains by first prompting LLMs to infer related ToM dimensions (e.g., belief). Afterward, CCoToM prompts the LLM to infer the queried ToM dimension based on the generated related ToM dimensions and corresponding causal relations."
  - [section 3.2] "CCoToM first prompts the LLM to infer the agent’s percept based on the context and imposed constraints... Next, CCoToM prompts the LLM to infer the agent’s belief based on the inferred percept, the context, and the imposed constraints."
  - [corpus] Weak: No direct citation of this specific decomposition approach, but related works on reasoning chains (e.g., CoT) exist.
- Break condition: If the LLM fails to accurately perform sub-task reasoning, the chained approach will propagate errors and degrade performance.

### Mechanism 2
- Claim: Adaptive constraints introduce inductive biases that improve consistency between inferred ToM dimensions.
- Mechanism: CCoToM imposes definitional constraints (e.g., "Belief of {agent} is what {agent} believes about the state of the environment") and dependency constraints (e.g., "Action of {agent} is determined by the belief of {agent} and the desire of {agent}"). These constraints guide the LLM to maintain semantic coherence and causal validity across reasoning steps.
- Core assumption: Explicit semantic definitions and causal dependencies in prompts can correct LLM tendencies toward inconsistent or arbitrary inferences in multi-step reasoning.
- Evidence anchors:
  - [abstract] "Additionally, CCoToM adaptively imposes constraints on prompts to introduce inductive biases and improve consistency between ToM dimensions."
  - [section 3.3] "Definitional constraints describes the semantics of concepts that are crucial in constructing reasoning chains... Dependency constraints specify the dependency between ToM dimensions based on the causal relation model."
  - [section 4.4] "Removing constraints always leads to a drop in accuracy for all three types of tasks, showing the significance of constraints in constructing explicit, ToM-specific reasoning chains."
- Break condition: If constraints are too rigid or misaligned with the LLM's reasoning style, they may force incorrect inferences or degrade performance.

### Mechanism 3
- Claim: Multi-step prompting outperforms one-step prompting by reducing cognitive load on the LLM.
- Mechanism: Instead of asking the LLM to infer all related ToM dimensions and the target dimension in a single prompt, CCoToM breaks the task into sequential prompts, each focusing on one sub-task. This reduces the complexity of each prompt and allows the LLM to build on previously inferred dimensions.
- Core assumption: LLMs handle simpler, focused prompts better than complex, multi-objective prompts, especially in tasks requiring multiple reasoning steps.
- Evidence anchors:
  - [section 4.5] "CCoToM consistently outperforms its variant across all tasks. The reason can be that in CCoToM the LLM only needs to tackle one sub-task at each prompting step, whereas in the variant the LLM needs to tackle all sub-tasks in one prompting step, thus making it more difficult for the LLM to accurately construct the reasoning chains."
  - [section 4.5] "The largest performance drop is from the FA task. This can be due to that the decomposition of the FA task generates the most sub-tasks."
- Break condition: If the LLM's context window or attention span is sufficient to handle complex prompts without degradation, multi-step prompting may offer no advantage.

## Foundational Learning

- Concept: Causal relations in Theory of Mind (BDI model)
  - Why needed here: CCoToM is built on the Belief-Desire-Intention (BDI) model, which defines how mental states causally relate (percept → belief, belief+desire → action). Understanding this model is essential to grasp why CCoToM's reasoning chains are structured as they are.
  - Quick check question: In the BDI model, what mental state determines an agent's action, and what determines that mental state?

- Concept: Zero-shot prompting vs. few-shot prompting
  - Why needed here: CCoToM is a zero-shot method, meaning it doesn't use example demonstrations. Knowing the difference helps understand why the method relies on constraints and explicit instructions rather than exemplars.
  - Quick check question: What is the key difference between zero-shot and few-shot prompting in terms of how the LLM is guided?

- Concept: Inductive biases in machine learning
  - Why needed here: Constraints in CCoToM act as inductive biases, steering the LLM's reasoning toward more consistent and valid inferences. Understanding inductive biases helps explain how constraints improve performance.
  - Quick check question: How do inductive biases influence a model's learning or reasoning process?

## Architecture Onboarding

- Component map: Context → Agent identification → Percept inference → Belief inference → Desire inference → Action inference → Output
- Critical path: Context → Agent identification → Related dimension inference(s) → Target dimension inference → Output
- Design tradeoffs:
  - Constraint specificity vs. flexibility: More specific constraints improve consistency but may reduce adaptability to diverse contexts.
  - Prompt complexity vs. LLM capacity: Complex prompts may exceed LLM context or attention limits, necessitating multi-step approaches.
  - Task decomposition granularity: Finer decomposition improves accuracy but increases the number of prompts and potential error propagation.
- Failure signatures:
  - Inconsistent inferences across reasoning steps (e.g., belief contradicts percept)
  - Incorrect agent identification leading to wrong dimension inference
  - Failure to handle non-narrative contexts like conversations due to over-reliance on narrative-specific constraints
  - Degradation when constraints are removed or when using one-step prompting
- First 3 experiments:
  1. Validate agent identification: Test the prompt for identifying the agent in various question formats.
  2. Test constraint effectiveness: Compare performance with and without constraints on a simple ToM task (e.g., Forward Belief).
  3. Compare multi-step vs. one-step prompting: Implement the one-step variant and measure performance drop on a complex task (e.g., Forward Action).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CCoToM's performance scale with increasing model size and capability of the underlying LLM?
- Basis in paper: [explicit] The paper tests CCoToM across multiple LLMs (GPT-4, GPT-3.5-Turbo, Llama-2 Chat 70B, Mistral Instruct 7B) and shows consistent improvements, with maximum gains of 15.5, 41.5, 37.5, and 34.5 absolute points respectively. However, the relationship between model size/capability and performance gains is not fully characterized.
- Why unresolved: The paper does not provide a systematic analysis of how CCoToM's performance improvements vary as a function of underlying model size or capability. It only reports results for a fixed set of models.
- What evidence would resolve it: Experiments testing CCoToM with a wider range of model sizes/capabilities, ideally including a systematic scaling analysis to determine if there are diminishing returns or other patterns.

### Open Question 2
- Question: Can CCoToM be effectively extended to handle multimodal inputs (e.g., text + images) for ToM reasoning?
- Basis in paper: [inferred] The paper focuses exclusively on text-based ToM reasoning tasks. The authors mention in the conclusion that they plan to extend CCoToM to handle more input modalities besides text, suggesting this is an open direction.
- Why unresolved: The paper does not explore or provide any results for multimodal ToM reasoning. It's unclear how the reasoning chains and constraints would need to be adapted for non-text inputs.
- What evidence would resolve it: Experiments applying CCoToM to multimodal ToM datasets (e.g., combining text narratives with relevant images), along with analysis of how the method needs to be adapted for different modalities.

### Open Question 3
- Question: How does CCoToM's performance compare to fine-tuned models on ToM tasks, especially given the authors' argument that fine-tuning is not suitable due to small dataset sizes?
- Basis in paper: [inferred] The paper explicitly states that supervised fine-tuning is not suitable for ToM tasks due to small dataset sizes and overfitting concerns. However, it doesn't actually compare CCoToM's performance to any fine-tuned models.
- Why unresolved: While the paper argues against fine-tuning, it doesn't provide empirical evidence by comparing CCoToM to the performance of fine-tuned models on the same tasks.
- What evidence would resolve it: Experiments fine-tuning various LLMs on the ToM datasets and comparing their performance to CCoToM, even if the fine-tuned models are expected to underperform due to overfitting.

## Limitations
- Performance relies heavily on the accuracy of BDI model's causal relations, which may not capture all complexities of human mental state reasoning
- Constraint effectiveness may diminish or reverse if constraints are too rigid or misaligned with LLM's inherent reasoning patterns
- Limited evidence for generalizability to non-narrative contexts beyond the specific conversational data tested

## Confidence
- **High confidence** in the core mechanism of decomposing ToM tasks into sequential sub-tasks with explicit reasoning chains
- **Medium confidence** in the adaptive constraint mechanism and its sensitivity to different formulations
- **Low confidence** in generalizability to non-narrative contexts and diverse real-world scenarios

## Next Checks
1. **Constraint Sensitivity Analysis**: Systematically vary the specificity and formulation of constraints to identify the optimal balance between consistency and flexibility. Measure performance degradation when constraints are removed or altered.

2. **Cross-Domain Generalization**: Test CCoToM on ToM datasets from domains not represented in BigToM or FANTOM (e.g., social media conversations, multi-party dialogues) to assess robustness to diverse context types.

3. **Error Analysis on Constraint Failures**: Analyze cases where CCoToM's performance drops with constraints, identifying patterns in contexts where the BDI model's causal assumptions break down or constraints introduce inconsistencies.