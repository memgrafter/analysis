---
ver: rpa2
title: A variational Bayes approach to debiased inference for low-dimensional parameters
  in high-dimensional linear regression
arxiv_id: '2406.12659'
source_url: https://arxiv.org/abs/2406.12659
tags:
- posterior
- i-svb
- prior
- variational
- oracle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a scalable variational Bayes method for statistical
  inference on a low-dimensional subset of parameters in high-dimensional sparse linear
  regression. The method decorrelates the target parameter from the nuisance parameters
  using a reparametrization, allowing for accurate uncertainty quantification even
  with a mean-field approximation.
---

# A variational Bayes approach to debiased inference for low-dimensional parameters in high-dimensional linear regression

## Quick Facts
- arXiv ID: 2406.12659
- Source URL: https://arxiv.org/abs/2406.12659
- Reference count: 40
- Method for scalable Bayesian inference on low-dimensional parameters in high-dimensional sparse linear regression with theoretical guarantees

## Executive Summary
This paper introduces a variational Bayes method for statistical inference on low-dimensional parameters in high-dimensional sparse linear regression. The approach decorrelates the target parameter from nuisance parameters through a reparametrization, allowing for accurate uncertainty quantification while maintaining computational scalability. The method combines mean-field variational inference for high-dimensional nuisance parameters with exact inference for the transformed low-dimensional target. Theoretical guarantees are provided in the form of a Bernstein-von Mises theorem showing asymptotic normality and optimal uncertainty quantification.

## Method Summary
The method decorrelates the target parameter from nuisance parameters using a reparametrization β₁* = β₁ + Σ₁ₖX₁ₖᵀX₋ₖβ₋ₖ, which orthogonalizes the likelihood. This allows independent variational approximation where the high-dimensional nuisance parameters β₋₁ are approximated with mean-field variational inference while the transformed low-dimensional target β₁* has an exact posterior. The approach preserves computational advantages of mean-field VB while ensuring accurate inference for the target parameter. The method is scalable as it only requires preprocessing steps and CAVI optimization for the nuisance parameters.

## Key Results
- Theoretical guarantees show asymptotic normality and optimal uncertainty quantification via Bernstein-von Mises theorem
- Empirical results demonstrate competitive performance with frequentist debiased methods in both estimation accuracy and uncertainty quantification
- The method is computationally scalable, outperforming MCMC alternatives while maintaining similar performance to other scalable methods
- The reparametrization effectively captures correlation structure, leading to improved uncertainty quantification compared to standard mean-field variational inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reparametrization decorrelates target from nuisance parameters under posterior
- Mechanism: The transformation β₁* = β₁ + Σ₁ₖX₁ₖᵀX₋ₖβ₋ₖ orthogonalizes the likelihood of β₁* and β₋₁, enabling independent variational approximation
- Core assumption: The reparametrization sufficiently decorrelates target and nuisance under posterior
- Evidence anchors:
  - [abstract]: "Our approach relies on assigning a mean-field approximation to the nuisance coordinates and carefully modelling the conditional distribution of the target given the nuisance"
  - [section 2.1]: "By using the orthogonal decomposition... the likelihood decouples β₁* from β₋₁"
  - [corpus]: Weak - no direct mentions of reparametrization mechanism
- Break condition: When covariate correlations are too strong, the reparametrization fails to adequately decorrelate

### Mechanism 2
- Claim: Combining exact posterior for target with mean-field for nuisance preserves uncertainty quantification
- Mechanism: Independent posteriors for β₁* and β₋₁ allow using exact posterior for low-dimensional β₁* while approximating high-dimensional β₋₁ with mean-field
- Core assumption: Mean-field approximation error for β₋₁ doesn't significantly impact variance estimation for β₁
- Evidence anchors:
  - [abstract]: "This requires only a preprocessing step and preserves the computational advantages of mean-field variational Bayes, while ensuring accurate and reliable inference for the target parameter"
  - [section 2.2]: "The idea behind our method... is to combine a fast MF approximation for β₋₁ with the exact posterior distribution for β₁*"
  - [corpus]: Weak - no direct discussion of combining exact and approximate posteriors
- Break condition: When nuisance parameter uncertainty contributes significantly to target variance

### Mechanism 3
- Claim: Bernstein-von Mises theorem provides asymptotic frequentist validity for credible intervals
- Mechanism: Under certain conditions, the marginal VB posterior for β₁:ₖ converges to Gaussian centered at efficient estimator with optimal variance
- Core assumption: Design matrix satisfies compatibility conditions and isn't too correlated
- Evidence anchors:
  - [abstract]: "We further establish accompanying theoretical guarantees for estimation and uncertainty quantification in the form of a Bernstein--von Mises theorem"
  - [section 3.1]: "Equation (16) states that the marginal VB posterior for β₁ is asymptotically Gaussian"
  - [section 3.1]: "An especially important implication of such a result is that standard (e.g. quantile-based) credible intervals for β₁ computed from the VB posterior are asymptotic confidence intervals"
- Break condition: When design matrix correlations violate condition (19) or compatibility conditions fail

## Foundational Learning

- Concept: Mean-field variational inference
  - Why needed here: The method builds directly on mean-field VB as the computational foundation, modifying it for better uncertainty quantification
  - Quick check question: What is the key limitation of standard mean-field VB in correlated settings?

- Concept: Debiased inference in high-dimensional regression
  - Why needed here: The paper addresses the same problem as debiased methods but uses a Bayesian variational approach instead
  - Quick check question: How does the reparametrization relate to the orthogonalization used in debiased LASSO methods?

- Concept: Spike-and-slab priors and model selection
- Why needed here: The method uses spike-and-slab priors for the nuisance parameters, requiring understanding of their properties and computational challenges
  - Quick check question: Why are spike-and-slab priors computationally challenging in high dimensions?

## Architecture Onboarding

- Component map: Preprocessing → CAVI for nuisance → Exact posterior for target → Transformation back → Sampling
- Critical path: Preprocessing → CAVI for β₋₁ → Exact posterior for β₁* → Transformation back → Sampling
- Design tradeoffs:
  - Exact vs approximate: Using exact posterior for low-dimensional target vs full mean-field
  - Computational cost: CAVI for high-dimensional nuisance vs MCMC alternatives
  - Correlation handling: Reparametrization effectiveness vs alternative approaches
- Failure signatures:
  - Undercoverage: Indicates correlation structure not properly captured
  - High bias: Suggests reparametrization insufficient or prior misspecification
  - Slow convergence: May indicate poor initialization or ill-conditioned design
- First 3 experiments:
  1. Verify preprocessing correctly computes orthogonal projections and transformed matrices
  2. Test CAVI implementation on simulated low-dimensional problem with known solution
  3. Compare posterior samples from exact vs mean-field approximation for β₁*:ₖ on simple case

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the I-SVB method perform in strongly correlated settings where the theoretical assumptions break down, and what are the underlying mechanisms that enable its continued success?
- Basis in paper: [explicit] The paper discusses that the method performs well empirically in strongly correlated settings, which is outside the scope of the theoretical guarantees.
- Why unresolved: The theoretical analysis requires weak correlation assumptions that don't hold in these settings, and the exact mechanisms behind the method's success are not fully understood.
- What evidence would resolve it: Additional theoretical analysis showing why the method works in strongly correlated settings, or extensive empirical studies comparing I-SVB to other methods across various correlation structures.

### Open Question 2
- Question: How sensitive is the I-SVB method to the choice of prior for the low-dimensional parameter β₁:ₖ, and how does this compare to sensitivity in standard mean-field VB?
- Basis in paper: [explicit] The paper mentions using Laplace and improper priors for g, but notes that Gaussian priors require careful hyperparameter tuning.
- Why unresolved: The paper only explores a limited set of prior choices and doesn't provide a comprehensive comparison of how different priors affect performance.
- What evidence would resolve it: Systematic experiments comparing I-SVB with various priors (Laplace, Gaussian, improper, heavy-tailed) across multiple scenarios, analyzing sensitivity to hyperparameters.

### Open Question 3
- Question: What are the computational trade-offs of using the I-SVB method compared to other scalable Bayesian methods for high-dimensional inference?
- Basis in paper: [explicit] The paper compares computation time to ZZ and JM methods, showing I-SVB is faster than JM but similar to ZZ.
- Why unresolved: The paper doesn't explore trade-offs in terms of implementation complexity, memory usage, or scalability to extremely high dimensions.
- What evidence would resolve it: Detailed benchmarking of I-SVB against other scalable Bayesian methods (e.g., expectation propagation, integrated nested Laplace approximation) across various problem sizes and dimensions.

## Limitations
- The method relies on reparametrization that may fail to adequately decorrelate parameters when covariate correlations are strong
- Theoretical guarantees require compatibility conditions on design matrix that may not hold in practice
- Computational scalability, while better than MCMC, still requires solving high-dimensional optimization for nuisance parameters
- Method's robustness when design matrix conditions are violated (strong correlations, near-collinearity) is not fully established

## Confidence

**High Confidence**: The basic algorithmic framework and computational approach (mean-field VB for nuisance + exact posterior for target) is well-established and should work as described. The asymptotic normality results and coverage guarantees in the Bernstein-von Mises theorem are theoretically sound under stated conditions.

**Medium Confidence**: The empirical performance claims relative to frequentist debiased methods, particularly in moderate sample sizes. The practical effectiveness of the reparametrization in decorrelating correlated parameters may vary significantly with data structure.

**Low Confidence**: The robustness of the method when design matrix conditions are violated (strong correlations, near-collinearity), and how severe the breakdown in uncertainty quantification becomes in these cases.

## Next Checks

1. **Design correlation stress test**: Systematically vary the correlation structure in simulated data to identify when the reparametrization fails to adequately decorrelate parameters, measuring breakdown points for coverage accuracy.

2. **Finite-sample performance**: Compare coverage and length of credible intervals against frequentist debiased methods across a range of sample sizes (n = 100 to 1000) and dimensions (p = 500 to 5000) to verify computational advantages hold in practice.

3. **Prior sensitivity analysis**: Vary the spike-and-slab prior parameters (slab width λ, inclusion probability q) to determine how sensitive the uncertainty quantification is to prior specification, particularly for the nuisance parameters.