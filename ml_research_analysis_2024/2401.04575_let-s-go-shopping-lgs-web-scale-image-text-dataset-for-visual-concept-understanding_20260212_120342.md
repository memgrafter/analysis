---
ver: rpa2
title: Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept
  Understanding
arxiv_id: '2401.04575'
source_url: https://arxiv.org/abs/2401.04575
tags:
- imagenet
- e-commerce
- datasets
- dataset
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Let\u2019s Go Shopping (LGS), a web-scale\
  \ image-text dataset containing 15 million image-caption pairs collected from e-commerce\
  \ websites. The authors address the need for high-quality, large-scale datasets\
  \ by leveraging e-commerce data that is clean, informative, and fluent."
---

# Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept Understanding

## Quick Facts
- arXiv ID: 2401.04575
- Source URL: https://arxiv.org/abs/2401.04575
- Reference count: 40
- Introduces LGS, a 15M image-text dataset for e-commerce visual understanding

## Executive Summary
This paper introduces Let's Go Shopping (LGS), a web-scale image-text dataset containing 15 million image-caption pairs collected from e-commerce websites. The authors address the need for high-quality, large-scale datasets by leveraging e-commerce data that is clean, informative, and fluent. The LGS dataset includes images with clear foreground objects and descriptive captions. Experiments demonstrate that classifiers trained on existing datasets like ImageNet do not generalize well to e-commerce data, while models trained on LGS perform better. LGS also enhances vision-language tasks such as image captioning and text-to-image generation, making it a valuable resource for both research and practical applications.

## Method Summary
The authors collected 15 million image-text pairs from e-commerce websites, focusing on products with clear foreground objects and descriptive captions. They created three variants: LGS-117 (balanced with 117 classes), LGS-710 (unbalanced with 710 classes), and LGS-Overlap (176 classes overlapping with ImageNet). The dataset was used to train models for image classification, image reconstruction (MAE), image captioning (OFA), and text-to-image generation (Stable Diffusion). The training procedures included linear probing, fine-tuning, and self-supervised learning approaches.

## Key Results
- Models trained on LGS outperform ImageNet models on e-commerce classification tasks
- MAE models trained on LGS generalize better to COCO reconstruction than ImageNet-only models
- LGS enhances vision-language tasks like image captioning and text-to-image generation
- LGS-Overlap subset enables direct comparison with ImageNet-1k classification performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: E-commerce image-text pairs provide cleaner visual-textual alignment than general web data.
- Mechanism: E-commerce sites enforce structured product titles and descriptions that directly describe foreground objects, avoiding noisy or irrelevant alt-texts common in social media or general web sources.
- Core assumption: E-commerce data sources consistently enforce informative, descriptive metadata for product images.
- Evidence anchors:
  - [abstract] "e-commerce data meet three criteria: cleanliness, informativeness, and fluency."
  - [section] "For this reason, we turn to commercial shopping websites whose data meet three criteria: cleanliness, informativeness, and fluency."
- Break condition: If e-commerce sites begin allowing user-generated, low-quality alt-texts or inconsistent product descriptions.

### Mechanism 2
- Claim: LGS's image distribution is sufficiently different from ImageNet to benefit domain-specific pre-training.
- Mechanism: E-commerce images focus on single foreground objects with simple backgrounds, while ImageNet contains more complex scenes, leading to distributional shift that improves performance on e-commerce-related tasks when pre-trained on LGS.
- Core assumption: The foreground-background structure difference between LGS and ImageNet is meaningful for downstream tasks.
- Evidence anchors:
  - [section] "the e-commerce domain forms a natural distribution shift even for the classes that also exist in ImageNet."
  - [section] "the classifiers trained on existing benchmark datasets do not readily generalize to e-commerce data"
- Break condition: If downstream tasks do not benefit from the simpler foreground-focused image distribution.

### Mechanism 3
- Claim: Self-supervised models trained on LGS generalize better to general domain datasets than ImageNet-only models.
- Mechanism: MAE models trained on LGS can reconstruct COCO images with higher quality than ImageNet-only models, indicating shared visual feature extractors between domains.
- Core assumption: Visual features learned from e-commerce data are transferable to general domain images.
- Evidence anchors:
  - [section] "MAE trained on LGS is able to generate COCO images with higher qualities compared with the ImageNet model."
  - [section] "the feature extractors can generalize between LGS and general-domain datasets"
- Break condition: If LGS features fail to transfer to other general domain datasets beyond COCO.

## Foundational Learning

- Concept: Distributional shift in image classification
  - Why needed here: Understanding why models trained on ImageNet fail on e-commerce data requires grasping how different image distributions affect model generalization.
  - Quick check question: What is the key difference between ImageNet and LGS image distributions that causes classification performance degradation?

- Concept: Self-supervised learning for feature extraction
  - Why needed here: MAE models demonstrate that label-free training on LGS improves generalization, showing the value of self-supervised methods for cross-domain feature learning.
  - Quick check question: How does self-supervised training on LGS improve model performance on general domain datasets like COCO?

- Concept: Vision-language pre-training with image-text pairs
  - Why needed here: LGS's bimodal nature enables better image captioning and text-to-image generation by providing rich, descriptive captions paired with focused product images.
  - Quick check question: Why does training image captioning models on LGS produce richer attribute-rich captions compared to models trained on COCO?

## Architecture Onboarding

- Component map: Data collection pipeline → taxonomy generation → classification variants (LGS-117, LGS-710, LGS-Overlap) → MAE pre-training → downstream task evaluation
- Critical path: Data collection → taxonomy generation → model pre-training → downstream task evaluation
- Design tradeoffs: LGS prioritizes e-commerce-specific data over general domain coverage, trading breadth for domain relevance
- Failure signatures: Poor downstream performance on e-commerce tasks, MAE reconstruction quality degradation on COCO images
- First 3 experiments:
  1. Compare ResNet-50 performance on LGS-Overlap vs ImageNet validation set
  2. Train MAE on LGS vs ImageNet and evaluate reconstruction quality on COCO
  3. Fine-tune Stable Diffusion on LGS and evaluate FID on held-out LGS and DeepFashion validation sets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific limitations of the LGS dataset's domain-specificity, and how do these limitations affect its applicability to broader visual recognition tasks?
- Basis in paper: [inferred] The paper mentions that LGS has a distinctive distribution compared to general-domain datasets like ImageNet, but does not explicitly discuss the limitations of this domain-specificity.
- Why unresolved: The paper does not provide a detailed analysis of the limitations of LGS's domain-specificity, such as the potential lack of diversity in object types or scenarios.
- What evidence would resolve it: A comprehensive study comparing the performance of models trained on LGS versus models trained on more diverse datasets like ImageNet on a wide range of downstream tasks would provide evidence to resolve this question.

### Open Question 2
- Question: How does the quality of the captions in LGS compare to the quality of captions in other image-text datasets, and what impact does this have on the performance of vision-language models?
- Basis in paper: [explicit] The paper states that LGS captions are "precise and elaborative," but does not provide a quantitative comparison of caption quality with other datasets.
- Why unresolved: The paper does not provide a direct comparison of caption quality between LGS and other datasets, which is crucial for understanding the impact of caption quality on vision-language model performance.
- What evidence would resolve it: A detailed analysis of the caption quality in LGS compared to other datasets, using metrics such as caption informativeness, relevance, and grammatical correctness, would provide evidence to resolve this question.

### Open Question 3
- Question: What are the potential biases in the LGS dataset, and how do these biases affect the performance of models trained on LGS?
- Basis in paper: [inferred] The paper does not explicitly discuss potential biases in the LGS dataset, but the mention of a "distinctive distribution" suggests that there may be biases related to the e-commerce domain.
- Why unresolved: The paper does not provide an analysis of potential biases in the LGS dataset, such as biases related to product types, demographics, or cultural contexts.
- What evidence would resolve it: A thorough analysis of the LGS dataset for potential biases, including demographic, cultural, and product-related biases, would provide evidence to resolve this question.

## Limitations
- Lack of direct comparison with other e-commerce datasets like DeepFashion or Amazon product data
- No analysis of long-term dataset maintenance and potential data drift issues
- Limited discussion of potential biases in e-commerce data (Western-centric categories, brand over-representation)

## Confidence

**High Confidence Claims:**
- LGS contains 15 million image-text pairs from e-commerce sources (supported by dataset statistics and validation methodology)
- Models trained on LGS outperform ImageNet models on e-commerce classification tasks (demonstrated through controlled experiments)
- MAE models trained on LGS generalize better to COCO reconstruction than ImageNet-only models (quantitatively validated)

**Medium Confidence Claims:**
- E-commerce data is inherently cleaner and more informative than general web data (plausible but not directly compared to alternatives)
- LGS significantly improves text-to-image generation quality (supported by FID scores but lacks perceptual studies)
- LGS's taxonomy generation approach effectively captures product concepts (methodologically sound but not externally validated)

**Low Confidence Claims:**
- LGS is the "largest and highest-quality" e-commerce dataset (unverified claim lacking comparative analysis)
- LGS's performance gains will generalize to all e-commerce applications (extrapolated from limited downstream tasks)
- The distributional shift between LGS and ImageNet is the primary cause of performance differences (correlation not proven as causation)

## Next Checks
1. **Benchmark Against Specialized Datasets**: Compare LGS-trained models against models trained on established e-commerce datasets (DeepFashion, Amazon product data) across the same downstream tasks to quantify relative performance gains.

2. **Cross-Platform Generalization**: Test LGS-trained models on product images from different e-commerce platforms not included in the original dataset collection to assess domain transfer capabilities and potential platform-specific biases.

3. **Temporal Robustness Analysis**: Train models on LGS data from different time periods and evaluate performance on current e-commerce data to quantify dataset drift and establish maintenance requirements for continued effectiveness.