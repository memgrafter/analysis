---
ver: rpa2
title: 'Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering'
arxiv_id: '2409.16025'
source_url: https://arxiv.org/abs/2409.16025
tags:
- question
- answer
- questions
- reviews
- product
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new task of multilingual cross-market product
  question answering (MCPQA) that aims to answer product questions in a resource-scarce
  marketplace by leveraging information from a resource-rich auxiliary marketplace.
  The authors propose a large-scale dataset covering 17 marketplaces and 11 languages,
  with over 7 million questions and 52 million reviews.
---

# Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering

## Quick Facts
- **arXiv ID:** 2409.16025
- **Source URL:** https://arxiv.org/abs/2409.16025
- **Reference count:** 35
- **Primary result:** New multilingual cross-market QA dataset with 7M questions, 52M reviews across 17 marketplaces and 11 languages

## Executive Summary
This paper introduces the first large-scale multilingual benchmark for cross-market product question answering (MCPQA), addressing the challenge of answering product questions in resource-scarce marketplaces by leveraging information from resource-rich auxiliary marketplaces. The authors create a comprehensive dataset spanning 17 Amazon marketplaces across 11 languages, containing over 7 million questions and 52 million reviews. They propose two subtasks—review-based answer generation and product-related question ranking—and provide LLM-annotated subsets for both. Experiments demonstrate that incorporating cross-market information significantly improves performance, with cross-market models outperforming single-market counterparts across multiple model architectures.

## Method Summary
The authors construct a large-scale dataset covering 17 Amazon marketplaces and 11 languages, with over 7 million questions and 52 million reviews. They create LLM-annotated subsets (McMarketr and McMarketq) using GPT-4 for review-based answer generation and product-related question ranking subtasks. The dataset is automatically translated from non-English to English for the Electronics category. Experiments are conducted using models ranging from traditional lexical models (BM25) to LLMs (LLaMA-2, Flan-T5) in both single-market and cross-market scenarios. Performance is evaluated using BLEU-4 and ROUGE-L for answer generation, and MRR and Precision@3 for question ranking.

## Key Results
- Cross-market models outperform single-market counterparts in both answer generation and question ranking tasks
- GPT-4-generated answers are rated as "better" than human ground truth 61.8% of the time in human evaluation
- Non-English marketplaces benefit from automatic translation, though mBERT shows advantages over translated BERT in some non-Latin languages
- Cross-market retrieval performance improves with more retrieved resources, benefiting cross-market models more than single-market ones

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-market product question answering improves performance by leveraging product-related resources from a resource-rich auxiliary marketplace.
- **Mechanism:** The model retrieves and uses reviews and questions from the auxiliary marketplace to answer questions in the main marketplace, increasing the pool of relevant information.
- **Core assumption:** There is sufficient overlap in product types and related questions between the main and auxiliary marketplaces.
- **Evidence anchors:**
  - [abstract]: "incorporating cross-market information significantly enhances performance in both tasks"
  - [section]: "we notice that the portion of answerable questions gets raised in every marketplace with cross-market reviews"
- **Break condition:** If the product overlap between marketplaces is low, or if the auxiliary marketplace lacks relevant reviews or questions for the products in the main marketplace.

### Mechanism 2
- **Claim:** LLM-generated data can be used to label subsets of the dataset for training and evaluation.
- **Mechanism:** GPT-4 is used to automatically generate answers and rank questions, which are then used as ground truth for training and evaluation.
- **Core assumption:** GPT-4's generated answers and rankings are of sufficient quality to serve as ground truth.
- **Evidence anchors:**
  - [abstract]: "GPT-4 auto-labeling... human assessment to analyze their characteristics"
  - [section]: "61.8% LLM-generated answers are assumed 'better' than the human ground truth"
- **Break condition:** If the quality of GPT-4's generated data is low, or if there is a significant discrepancy between GPT-4's generated data and human-annotated data.

### Mechanism 3
- **Claim:** Multilingual translation of the dataset improves performance on non-English marketplaces.
- **Mechanism:** The dataset is automatically translated from non-English to English, allowing models trained on the translated data to perform better on non-English questions.
- **Core assumption:** The translation quality is high enough that the meaning of the original text is preserved.
- **Evidence anchors:**
  - [abstract]: "We then perform automatic translation on the Electronics category of our dataset"
  - [section]: "in the AG task, concerning some non-Latin languages (i.e., cn, jp), the performance of single-market mBERT without translation results in higher score compared with BERT on two datasets"
- **Break condition:** If the translation quality is poor, or if the models do not perform well on the translated data.

## Foundational Learning

- **Concept:** Product-related question answering (PQA)
  - **Why needed here:** PQA is the core task that the paper is addressing, and understanding its principles is essential for understanding the proposed approach.
  - **Quick check question:** What are the main challenges in PQA, and how does the proposed approach address them?

- **Concept:** Cross-lingual transfer learning
  - **Why needed here:** The paper proposes a multilingual approach to PQA, which relies on the ability to transfer knowledge between languages.
  - **Quick check question:** How does cross-lingual transfer learning differ from monolingual learning, and what are its potential benefits and challenges?

- **Concept:** Large language models (LLMs)
  - **Why needed here:** The paper uses GPT-4, a large language model, to generate data for training and evaluation.
  - **Quick check question:** What are the key characteristics of LLMs, and how can they be used for tasks like data generation and evaluation?

## Architecture Onboarding

- **Component map:** Data collection and preprocessing -> Automatic translation -> LLM annotation -> Model training and evaluation -> Human assessment
- **Critical path:** Data collection and preprocessing → Automatic translation → LLM annotation → Model training and evaluation → Human assessment
- **Design tradeoffs:** Tradeoff between data quality and quantity in LLM annotation; tradeoff between translation quality and speed; tradeoff between model complexity and performance
- **Failure signatures:** Low performance on cross-market tasks; poor quality of LLM-generated data; high discrepancy between LLM-generated data and human-annotated data
- **First 3 experiments:**
  1. Evaluate the performance of the model on single-market vs. cross-market tasks
  2. Compare the quality of LLM-generated data with human-annotated data
  3. Assess the impact of translation quality on model performance

## Open Questions the Paper Calls Out

The paper explicitly identifies several open questions:
1. How does the performance of multilingual models (mBERT) compare to translated models (BERT) in non-Latin languages like Chinese and Japanese for product-related question answering?
2. What is the impact of the number of retrieved product-related resources (K) on the performance of cross-market models compared to single-market models for both answer generation and question ranking tasks?
3. How does the quality of LLM-generated answers compare to human-annotated answers when evaluated by domain experts or in real-world applications?
4. How does the temporal availability of questions in different marketplaces affect the performance of cross-market QA models over time?
5. How does the performance of cross-market QA models vary across different product categories beyond electronics?

## Limitations

- The study relies heavily on LLM-generated annotations, which may introduce evaluation biases despite human assessment showing 61.8% of GPT-4 answers rated as "better" than human ground truth
- Cross-market retrieval effectiveness depends on product overlap between marketplaces, which may vary significantly across different categories
- Translation quality for non-Latin languages (particularly Chinese and Japanese) shows inconsistent impacts on model performance, suggesting potential quality variations

## Confidence

- **High confidence:** Dataset construction methodology and core finding that cross-market information improves performance across both subtasks
- **Medium confidence:** LLM annotation quality and its impact on evaluation, due to single-annotator approach and lack of detailed annotation criteria
- **Low confidence:** Translation quality assessment for non-English languages, particularly given inconsistent performance patterns and lack of detailed analysis

## Next Checks

1. **Cross-market retrieval validation:** Conduct ablation studies removing cross-market information to quantify its exact contribution to performance improvements, and test on marketplaces with varying product overlap levels
2. **LLM annotation robustness:** Implement multi-annotator human evaluation of a subset of LLM-generated answers and rankings, comparing agreement rates and identifying systematic biases in the annotation process
3. **Translation quality assessment:** Perform detailed evaluation of translation quality using both automated metrics and human evaluation, particularly focusing on Chinese and Japanese content, and test model performance with and without translation across different language pairs