---
ver: rpa2
title: Intelligence Analysis of Language Models
arxiv_id: '2407.18968'
source_url: https://arxiv.org/abs/2407.18968
tags:
- tasks
- reasoning
- dataset
- llms
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates Large Language Models (LLMs) on the Abstraction
  and Reasoning Corpus (ARC), a benchmark for abstract reasoning. It converts ARC
  tasks into text prompts and tests models using zero-shot and Chain-of-Thought (CoT)
  approaches.
---

# Intelligence Analysis of Language Models

## Quick Facts
- arXiv ID: 2407.18968
- Source URL: https://arxiv.org/abs/2407.18968
- Authors: Liane Galanti; Ethan Baron
- Reference count: 2
- Key outcome: LLMs achieve only 2 correct solutions out of 50 ARC tasks, showing significant struggle with abstract reasoning

## Executive Summary
This paper evaluates Large Language Models (LLMs) on the Abstraction and Reasoning Corpus (ARC), a benchmark for abstract reasoning. The study converts ARC tasks into text prompts and tests models using zero-shot and Chain-of-Thought (CoT) approaches. Results show that LLMs struggle significantly with non-linguistic reasoning tasks, achieving only 2 correct solutions out of 50 tasks. Even with CoT prompting, models often fail to produce coherent reasoning despite correct answers. This is the first study to focus specifically on open-source LLMs in this context, revealing a substantial gap toward Artificial General Intelligence (AGI).

## Method Summary
The study evaluates LLaMA, Phind, and Mixtral models on 50 ARC tasks solvable by the ARGA solver. Tasks are converted from 2D matrices to text-based encoding using numeric representations of colors. Both zero-shot and Chain-of-Thought prompting methods are employed, with each task run three times. The evaluation measures the number of correctly solved tasks out of 50, focusing on open-source models without any fine-tuning.

## Key Results
- LLMs achieved only 2 correct solutions out of 50 ARC tasks tested
- Chain-of-Thought prompting did not consistently improve reasoning quality, with models often producing incoherent reasoning even when answers were correct
- The study represents the first focused evaluation of open-source LLMs on ARC, highlighting their significant limitations with abstract reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs struggle with ARC tasks because their internal reasoning is misaligned with required abstraction steps, not due to insufficient linguistic capacity
- Mechanism: Models generate output based on pattern matching in training data rather than true step-by-step deduction, leading to correct answers without valid reasoning traces
- Core assumption: LLMs lack grounded representation of visual/geometric concepts ARC tasks depend on (objectness, basic geometry, counting)
- Evidence anchors:
  - [abstract] "models often fail to produce coherent reasoning despite correct answers"
  - [section] "even when they eventually provide the correct answer e.g., see tables 3 and 4"
  - [corpus] Weak: corpus shows related work but no direct mechanism studies; only outcome comparisons
- Break condition: If models are fine-tuned on synthetic ARC-like data or augmented with object-centric modules, reasoning quality could improve

### Mechanism 2
- Claim: Textual encoding of 2D matrices into space-separated digits loses spatial structure, impairing geometric reasoning
- Mechanism: Encoding collapses multi-dimensional relationships into linear string, so adjacency and symmetry cues are flattened; model must infer them implicitly from token order, which is error-prone
- Core assumption: Spatial reasoning in ARC depends on preserving relative positions; tokenization destroys this
- Evidence anchors:
  - [section] "Given an ARC task, i.e., 2D matrix, we converted it into text by encoding each element color numerically"
  - [section] "Alternatively, colors could be encoded with letters 'a' to 'j' or by their names"
  - [corpus] Weak: related work mentions vision-based approaches but no encoding impact study
- Break condition: If graph or grid-based encodings are used, geometric reasoning could be preserved

### Mechanism 3
- Claim: CoT prompting does not guarantee improved reasoning because step-by-step rationale is not verified against visual logic of the task
- Mechanism: Model is given fixed example reasoning path but applies it blindly; if path is mismatched, model still outputs answer without detecting inconsistency
- Core assumption: CoT assumes example reasoning is valid for the task; in ARC, reasoning patterns vary per task
- Evidence anchors:
  - [abstract] "models using CoT don't consistently reason correctly towards the right solution"
  - [section] "even when models correctly solved tasks using CoT, their reasoning processes were not necessarily sound"
  - [corpus] Weak: corpus lists related prompting studies but no CoT effectiveness validation on ARC
- Break condition: If CoT examples are dynamically adapted to task type, reasoning consistency could improve

## Foundational Learning

- Concept: Spatial reasoning with grids
  - Why needed here: ARC tasks are fundamentally about manipulating 2D matrices; understanding how positions, neighbors, and symmetries work is essential to solving them
  - Quick check question: Given a 3x3 grid with a single "1" in the center, what is the result after shifting all non-zero cells one step right?

- Concept: Abstraction from examples
  - Why needed here: ARC requires inferring the transformation rule from a few demonstrations; without this, zero-shot attempts fail
  - Quick check question: If input [[1,0],[0,1]] maps to output [[0,1],[1,0]], what is the underlying rule?

- Concept: Chain-of-thought prompting structure
  - Why needed here: CoT is meant to guide multi-step reasoning; understanding its scaffolding is key to diagnosing why it fails here
  - Quick check question: In a CoT prompt, should the reasoning steps be fixed per task or adapted dynamically?

## Architecture Onboarding

- Component map: Text encoder → LLM core → Text decoder; plus prompt template manager; evaluation harness for ARC subset
- Critical path: Encode ARC task → Generate solution → Verify correctness; failure at any stage aborts task
- Design tradeoffs: Simple numeric encoding is fast but lossy; richer encodings preserve structure but increase token length and model load
- Failure signatures: Correct output without valid reasoning; consistent failure on symmetry/rotation tasks; CoT sometimes worsens performance
- First 3 experiments:
  1. Replace numeric encoding with letter-based encoding and compare success rate
  2. Add a dynamic CoT template per task type and test reasoning quality
  3. Implement a minimal object-centric pre-processor to preserve adjacency before feeding to LLM

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning LLMs on a broader range of abstract reasoning tasks improve their performance on the ARC dataset?
- Basis in paper: [inferred] The paper suggests exploring more advanced models, potentially fine-tuned for abstract reasoning, as future work
- Why unresolved: The study focused only on open-source models without any fine-tuning, and did not explore the impact of specialized training
- What evidence would resolve it: Results comparing the performance of fine-tuned LLMs versus non-fine-tuned models on the ARC dataset

### Open Question 2
- Question: How do alternative prompting techniques, such as Tree-of-Thoughts (ToT), compare to Chain-of-Thought (CoT) in solving ARC tasks?
- Basis in paper: [explicit] The paper mentions that future work could explore a variety of prompting techniques, including ToT, which is an extension of CoT
- Why unresolved: The study only evaluated Zero-shot and CoT methods, leaving the effectiveness of other techniques untested
- What evidence would resolve it: Empirical results showing the performance of different prompting techniques, including ToT, on the ARC dataset

### Open Question 3
- Question: To what extent does including multiple example tasks in prompts, tailored to each of ARC's core knowledge priors, enhance LLM performance?
- Basis in paper: [inferred] The paper suggests including multiple example tasks in the prompt that correspond to each of ARC's core knowledge priors with tailored reasoning processes as a potential improvement
- Why unresolved: The study used only one example task in the CoT method, limiting the exploration of diverse examples' impact
- What evidence would resolve it: Comparative results showing the effectiveness of prompts with multiple example tasks versus single example tasks on ARC performance

## Limitations

- The study focuses exclusively on open-source LLMs without benchmarking against proprietary models that may have different capabilities
- The evaluation is constrained to 50 tasks from ARC, which may not represent the full complexity spectrum of the benchmark
- The textual encoding approach fundamentally alters the spatial nature of ARC tasks, potentially underestimating model capabilities if visual processing were available

## Confidence

- **High Confidence**: The finding that LLMs achieve only 2/50 correct solutions on ARC tasks is well-supported by the experimental methodology and results presented
- **Medium Confidence**: The assertion that CoT prompting does not consistently improve reasoning quality is supported by examples in the paper, though the sample size of analyzed reasoning traces is not specified
- **Low Confidence**: The claim that this is the "first study to focus on open-source LLMs" for ARC evaluation requires verification against the broader literature

## Next Checks

1. **Encoding Sensitivity Test**: Systematically evaluate the impact of different text encoding schemes (numeric, alphabetic, descriptive) on ARC task performance to determine if encoding choice significantly affects results

2. **Cross-Model Benchmark**: Test the same 50 ARC tasks across both open-source and proprietary LLMs (when available) to establish whether the observed limitations are model-specific or represent fundamental constraints of current LLM architectures

3. **Visual Preprocessing Experiment**: Implement a simple visual preprocessing pipeline that extracts spatial features (symmetry, adjacency, counting) from ARC tasks before textual encoding, then evaluate whether this augmentation improves LLM performance on geometrically complex tasks