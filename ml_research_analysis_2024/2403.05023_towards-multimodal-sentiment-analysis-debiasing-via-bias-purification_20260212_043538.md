---
ver: rpa2
title: Towards Multimodal Sentiment Analysis Debiasing via Bias Purification
arxiv_id: '2403.05023'
source_url: https://arxiv.org/abs/2403.05023
tags:
- multimodal
- bias
- counterfactual
- sentiment
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multimodal sentiment analysis
  (MSA), where models often suffer from dataset biases, specifically multimodal utterance-level
  label bias and word-level context bias. These biases can mislead models to focus
  on statistical shortcuts and spurious correlations, leading to poor performance.
---

# Towards Multimodal Sentiment Analysis Debiasing via Bias Purification

## Quick Facts
- arXiv ID: 2403.05023
- Source URL: https://arxiv.org/abs/2403.05023
- Reference count: 40
- State-of-the-art results on MOSI (47.9% accuracy, 86.5% F1) and MOSEI (55.2% accuracy, 87.3% F1) datasets

## Executive Summary
This paper addresses multimodal sentiment analysis (MSA) dataset biases through a novel counterfactual inference framework called MCIS. The framework identifies two primary biases - multimodal utterance-level label bias and word-level context bias - and purifies them through counterfactual scenarios. By imagining alternative representations where label distributions are averaged or context words are isolated, MCIS can make unbiased decisions from biased observations. Extensive experiments on MOSI and MOSEI benchmarks demonstrate consistent improvements across multiple state-of-the-art models.

## Method Summary
The MCIS framework operates by generating counterfactual multimodal inputs to isolate and remove dataset biases. Given a trained MSA model, MCIS creates two counterfactual scenarios: one with averaged embeddings to eliminate label bias, and another with masked content words and zeroed audio/visual features to isolate context bias. The framework then applies weighted subtraction of these counterfactual predictions from the factual prediction using tuned parameters λ. A grid search strategy optimizes these parameters on validation data using weighted F1-score, allowing the framework to adapt to different bias distributions across datasets.

## Key Results
- Achieved new state-of-the-art results on MOSI dataset with 47.9% accuracy and 86.5% F1 score
- Set best performance on MOSEI dataset with 55.2% accuracy and 87.3% F1 score
- Consistently improved performance across multiple MSA models including MulT, MISA, MMIM, CubeMLP, and DMD
- Demonstrated effectiveness of bias elimination strategy across different evaluation metrics (Acc-7, Acc-2, F1)

## Why This Works (Mechanism)

### Mechanism 1
The framework corrects predictions by removing spurious correlations from label bias (statistical shortcuts from imbalanced training labels) and context bias (spurious associations between context words and sentiment categories). It intervenes on multimodal inputs using two counterfactual scenarios: one with averaged embeddings to remove label bias, and another with masked main content words and zeroed audio/visual to isolate context bias. The factual prediction is adjusted by subtracting these two purified biases.

### Mechanism 2
The bias elimination strategy subtracts counterfactual predictions from the factual prediction to obtain unbiased results. After obtaining two counterfactual outputs, MCIS applies a weighted subtraction: ℵ(m) = F(m) - (λ̂ F(̂m) + λ̃ F(̃m)), where λ̂ and λ̃ are tuned per dataset. This assumes the counterfactual predictions isolate the pure bias effects and can be linearly combined to correct the factual prediction.

### Mechanism 3
The grid search strategy tunes the balance between label and context bias elimination per dataset. The parameters λ̂ and λ̃ are optimized on a validation set using a weighted F1-score to determine the right proportion of each bias to remove. This assumes the relative impact of label and context bias varies by dataset and can be learned from validation performance.

## Foundational Learning

- **Causal graphs and backdoor adjustment**: Understanding how dataset biases confound the prediction and defining interventions that remove them. Quick check: In a causal graph, what does the do-operator do to a variable's incoming edges?
- **Counterfactual inference**: Imagining outcomes under different treatment conditions to isolate bias effects. Quick check: What is the difference between an intervention and a counterfactual in causal inference?
- **Multimodal representation fusion**: Understanding how language, audio, and visual modalities are combined before prediction, since interventions target this representation. Quick check: What are two common fusion strategies for multimodal sentiment analysis?

## Architecture Onboarding

- **Component map**: Input multimodal utterance → Vanilla MSA model F(·) → Counterfactual generators → Bias subtraction unit → Debiased sentiment prediction
- **Critical path**: 1) Encode multimodal input with vanilla model, 2) Generate counterfactual embeddings, 3) Run vanilla model on counterfactuals, 4) Subtract weighted counterfactual predictions
- **Design tradeoffs**: Averaging embeddings vs. random noise for label bias (averaging better represents bias but may underfit), masking vs. replacing content words (masking is cleaner but may lose semantics), fixed vs. learned weights for bias subtraction (fixed is simpler, learned adapts to dataset)
- **Failure signatures**: If debiased predictions get worse, likely bias weights are off or counterfactuals don't isolate biases; if performance plateaus, may have reached bias limit or need better counterfactuals; if training is unstable, could be from averaging or masking artifacts
- **First 3 experiments**: 1) Run vanilla model on MOSI test set, record accuracy/F1, 2) Apply MCIS with fixed weights (λ̂=1, λ̃=1), compare performance, 3) Tune weights on validation set, apply to test set, measure improvement

## Open Questions the Paper Calls Out

### Open Question 1
How does the MCIS framework perform on datasets with extreme class imbalance, such as a 95% : 5% distribution? The paper mentions that a binary sentiment analysis dataset could have such label distribution but does not provide experimental results on such datasets.

### Open Question 2
Can the MCIS framework be extended to handle modality missingness in realistic applications? The paper mentions this as future work including modality reconstruction techniques but does not explore this extension.

### Open Question 3
How does the performance of MCIS vary across different types of context words (e.g., stop words, adjectives)? The paper discusses context bias but does not provide detailed analysis of performance across different types of context words.

## Limitations

- The framework assumes linear bias composability through weighted subtraction, which may not capture complex interactions between modalities and biases
- Effectiveness depends on validation set being representative of test set's bias distribution for lambda parameter tuning
- Averaging embeddings to remove label bias assumes uniform bias distribution, which may not hold in practice

## Confidence

- **High**: Mathematical formulation of counterfactual inference and causal graph representation
- **Medium**: Implementation details of counterfactual embedding generation
- **Medium**: Experimental results showing performance improvements
- **Low**: Theoretical guarantee that biases are completely removed rather than partially mitigated

## Next Checks

1. **Ablation study on counterfactual generation**: Compare performance when using random noise instead of averaged embeddings for label bias removal to verify that averaging specifically targets the bias distribution.

2. **Bias visualization analysis**: Compute and visualize the distribution shift in predictions before and after MCIS application on both training and test sets to empirically demonstrate bias reduction.

3. **Cross-dataset lambda transferability**: Test whether lambda parameters optimized on one dataset transfer to another to validate whether the framework learns dataset-specific bias patterns versus universal debiasing strategies.