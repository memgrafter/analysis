---
ver: rpa2
title: 'Fuse Before Transfer: Knowledge Fusion for Heterogeneous Distillation'
arxiv_id: '2410.12342'
source_url: https://arxiv.org/abs/2410.12342
tags:
- knowledge
- heterogeneous
- features
- different
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of knowledge distillation (KD)
  across heterogeneous model architectures (CAKD), where significant feature gaps
  arise from differences in inductive biases and module functions between models like
  CNNs, ViTs, and MLPs. To mitigate these gaps, the authors propose a method called
  Fuse Before Transfer (FBT) that first fuses heterogeneous knowledge by combining
  convolution, attention, and MLP modules derived from both teacher and student architectures,
  and then transfers the fused knowledge.
---

# Fuse Before Transfer: Knowledge Fusion for Heterogeneous Distillation

## Quick Facts
- arXiv ID: 2410.12342
- Source URL: https://arxiv.org/abs/2410.12342
- Reference count: 40
- Up to 11.47% improvement on CIFAR-100 and 3.67% on ImageNet-1K in heterogeneous KD settings

## Executive Summary
This paper addresses the challenge of knowledge distillation across heterogeneous model architectures, where significant feature gaps arise from differences in inductive biases and module functions between models like CNNs, ViTs, and MLPs. The authors propose Fuse Before Transfer (FBT), a method that first fuses heterogeneous knowledge by combining convolution, attention, and MLP modules from both teacher and student architectures, then transfers the fused knowledge. The fused model uses a local-to-global feature projector and connects modules in a weight-sharing manner to align different inductive biases and module functions. The method replaces pixel-wise MSE loss with a spatial-agnostic InfoNCE loss to better handle diverse spatial feature distributions. FBT is evaluated on CIFAR-100 and ImageNet-1K across numerous homogeneous and heterogeneous model pairs, achieving state-of-the-art performance with significant improvements in CAKD settings.

## Method Summary
Fuse Before Transfer (FBT) addresses knowledge distillation across heterogeneous architectures by first creating a fused model that combines CNN modules from early stages and MSA/MLP modules from later stages of both teacher and student models. The fused model includes a local-to-global (L2G) feature projector that connects CNN features to MSA/MLP modules, with weight-sharing between the fused model and original teacher/student modules to ensure functional alignment. The method uses spatial-agnostic InfoNCE loss instead of pixel-wise MSE loss to align features after spatial smoothing through average pooling. The training procedure involves three knowledge transfer stages: teacher to fused model (using OFA loss), fused model to student (using InfoNCE loss), and direct teacher to student (using OFA loss), with training conducted for 300 epochs on CIFAR-100 and 100/300 epochs on ImageNet-1K for CNNs/MSA-MLP models respectively.

## Key Results
- Achieves up to 11.47% improvement on CIFAR-100 and 3.67% on ImageNet-1K compared to existing CAKD methods
- Demonstrates consistent performance gains across 15 different teacher-student pairs including CNN-CNN, CNN-ViT, CNN-MLP, and ViT-MLP combinations
- Shows FBT maintains competitive performance in standard homogeneous KD settings while significantly outperforming existing methods in heterogeneous settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fusing CNN and MSA/MLP modules before transfer reduces the inductive bias gap between heterogeneous architectures.
- Mechanism: By constructing a fused model that combines CNN modules in early stages and MSA/MLP modules in later stages, the fused model captures both local and global receptive field information, aligning the heterogeneous feature distributions.
- Core assumption: CNN and MSA/MLP inductive biases are complementary and can be combined to produce better intermediate representations than either alone.
- Evidence anchors:
  - [abstract] "the assistant model combines the advantages of cross-architecture inductive biases and module functions by merging convolution and attention modules"
  - [section] "CNNs and MSAs are complementary, which inspires them to design a hybrid model following the rules of 'alternately replacing CNN blocks with MSA blocks from the end of a baseline CNN model'"
- Break condition: If CNN and MSA/MLP modules are incompatible in deeper layers, the fused model may introduce training instability or degrade performance.

### Mechanism 2
- Claim: Spatial-agnostic InfoNCE loss better aligns heterogeneous features than pixel-wise MSE loss.
- Mechanism: InfoNCE loss aligns features based on their structural information without requiring spatial correspondence, making it robust to different spatial distributions between CNN and MSA/MLP features.
- Core assumption: Heterogeneous models produce features with different spatial layouts, making pixel-wise alignment ineffective.
- Evidence anchors:
  - [abstract] "we leverage a spatial-agnostic InfoNCE loss to align features after spatial smoothing"
  - [section] "widely used MSE loss aligns the pixel-wise features, which is inadequate for spatially diverse heterogeneous features"
- Break condition: If features from different architectures happen to have similar spatial layouts, the added complexity of InfoNCE may not provide benefits over simpler losses.

### Mechanism 3
- Claim: Weight-sharing between fused model and student/teacher modules improves functional similarity alignment.
- Mechanism: By using weight-sharing modules derived from both student and teacher, the fused model implicitly aligns heterogeneous module functions while introducing minimal additional parameters.
- Core assumption: Module function differences between architectures can be bridged by sharing weights across corresponding modules.
- Evidence anchors:
  - [section] "a fused model comprising student and teacher modules not only optimizes the functional similarity between heterogeneous T.-S. pairs, but also introduces minimal additional learnable parameters"
  - [section] "weight-sharing modules also ensure the bridge role of our fusion model"
- Break condition: If module functions are fundamentally incompatible, weight-sharing may prevent proper learning of architecture-specific behaviors.

## Foundational Learning

- Concept: Knowledge distillation fundamentals
  - Why needed here: Understanding the difference between logits-based and feature-based KD is crucial for grasping why FBT focuses on feature fusion
  - Quick check question: What are the key differences between logits-based and feature-based knowledge distillation approaches?

- Concept: Inductive biases in neural networks
  - Why needed here: Different architectures (CNNs, ViTs, MLPs) have distinct inductive biases that cause feature distribution differences
  - Quick check question: How do the inductive biases of CNNs differ from those of vision transformers and MLPs?

- Concept: Contrastive learning principles
  - Why needed here: InfoNCE loss is based on contrastive learning, which requires understanding positive/negative sample concepts
  - Quick check question: What is the key principle behind InfoNCE loss and how does it differ from traditional cross-entropy?

## Architecture Onboarding

- Component map:
  Teacher model -> Fused model (CNN stages + L2G projector + MSA/MLP stages) -> Student model
  Direct: Teacher model -> Student model

- Critical path: Teacher → Fused model (via L(Kt,Kf)) → Student (via L(Kf,Ks)) + Direct Teacher → Student (via L(Kt,Ks))

- Design tradeoffs:
  - Fusion depth: Deeper fusion captures more complementary information but increases complexity
  - Module selection: More modules in fusion improve bridging but add parameters
  - Loss balance: Weighting between L(Kt,Ks), L(Kt,Kf), and L(Kf,Ks) affects convergence

- Failure signatures:
  - Student performance worse than training from scratch
  - Fused model diverging during training
  - Loss values oscillating or not converging
  - Large gap between teacher and fused model performance

- First 3 experiments:
  1. Implement basic FBT with minimal fusion (one CNN stage + one MSA stage) on CIFAR-100 to verify framework works
  2. Compare MSE vs InfoNCE loss on a simple heterogeneous pair to validate spatial-agnostic benefit
  3. Test different fusion depths (1, 2, 3 CNN stages) to find optimal balance for specific T-S pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal architecture of the fused model vary across different teacher-student pairs, and what principles govern these architectural choices?
- Basis in paper: [explicit] The authors note that different T-S pairs may benefit from different fusion strategies (e.g., Tab. 5 shows varying results for different module combinations), and they currently use a simple heuristic (3 CNN stages + 1 MSA stage) but acknowledge that the optimal fusion is pair-dependent.
- Why unresolved: The paper does not provide a systematic method for determining the optimal fusion architecture for arbitrary T-S pairs, instead relying on empirical testing.
- What evidence would resolve it: A comprehensive study mapping T-S architectural characteristics to optimal fusion configurations, potentially through automated architecture search or analytical characterization of feature space alignments.

### Open Question 2
- Question: What is the impact of spatial smoothing on the preservation of important spatial information in heterogeneous feature distillation?
- Basis in paper: [explicit] The authors replace pixel-wise MSE loss with spatial-agnostic InfoNCE loss after applying average pooling to smooth spatial features, acknowledging this as a potential limitation.
- Why unresolved: The paper does not investigate whether the spatial smoothing step might discard important spatial information that could be valuable for certain tasks or model pairs.
- What evidence would resolve it: Comparative studies showing the trade-off between spatial-agnostic alignment and spatial-specific information preservation across different datasets and model pairs.

### Open Question 3
- Question: How well does FBT generalize to other domains beyond image classification, such as object detection, segmentation, or natural language processing?
- Basis in paper: [inferred] The authors explicitly mention in the limitations section that validating their method across a broader range of domains remains for future investigation.
- Why unresolved: The current evaluation is limited to image classification tasks on CIFAR-100 and ImageNet-1K, with no exploration of other vision or NLP tasks.
- What evidence would resolve it: Successful application and performance evaluation of FBT on object detection benchmarks (COCO), semantic segmentation tasks, and NLP tasks with transformer-based architectures.

## Limitations
- The fused model architecture is designed for specific T-S pairs and may not generalize to arbitrary heterogeneous combinations without architectural modifications
- Spatial smoothing via average pooling may discard important spatial information that could be valuable for certain tasks
- The method's effectiveness beyond image classification tasks (e.g., object detection, segmentation, NLP) remains unexplored

## Confidence

**High confidence**: The empirical improvements (up to 11.47% on CIFAR-100, 3.67% on ImageNet-1K) are well-documented across multiple heterogeneous model pairs, and the core FBT framework is clearly specified.

**Medium confidence**: The theoretical justification for why spatial-agnostic InfoNCE outperforms pixel-wise MSE is sound, but the relative importance of each component (fusion, weight-sharing, loss choice) isn't isolated through ablation studies.

**Low confidence**: The claim that weight-sharing modules "ensure the bridge role" lacks rigorous analysis of when this mechanism might fail or introduce instability, particularly for highly divergent architectures.

## Next Checks
1. Conduct controlled ablation studies to isolate the contribution of each FBT component (fusion depth, weight-sharing, InfoNCE loss) on a representative heterogeneous pair.
2. Test FBT on additional heterogeneous combinations beyond the 15 T-S pairs evaluated, particularly including more diverse architecture types or different scale ratios.
3. Evaluate FBT's robustness to training instability by monitoring the fused model's convergence behavior and student performance when teacher-student architecture gaps are maximal.