---
ver: rpa2
title: NUTS, NARS, and Speech
arxiv_id: '2405.17874'
source_url: https://arxiv.org/abs/2405.17874
tags:
- speech
- nars
- which
- performance
- intelligence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the application of Non-Axiomatic Reasoning
  System (NARS) for speech recognition under the definition of intelligence as "the
  capacity of an information-processing system to adapt to its environment while operating
  with insufficient knowledge and resources." The proposed NUTS method combines random
  dimensionality reduction with NARS-based reasoning for few-shot learning in perception
  tasks. Using only 2 training examples per word class from the Speech Commands dataset,
  NUTS achieved 64% accuracy in discrete word identification, which is comparable
  to Whisper Tiny's 58% performance but with significantly fewer resources.
---

# NUTS, NARS, and Speech

## Quick Facts
- arXiv ID: 2405.17874
- Source URL: https://arxiv.org/abs/2405.17874
- Authors: D. van der Sluis
- Reference count: 40
- Key outcome: NUTS achieved 64% accuracy on Speech Commands dataset with only 2 training examples per word class using random dimensionality reduction and NARS-based reasoning

## Executive Summary
This paper investigates the application of Non-Axiomatic Reasoning System (NARS) for speech recognition under the definition of intelligence as "the capacity of an information-processing system to adapt to its environment while operating with insufficient knowledge and resources." The proposed NUTS method combines random dimensionality reduction with NARS-based reasoning for few-shot learning in perception tasks. Using only 2 training examples per word class from the Speech Commands dataset, NUTS achieved 64% accuracy in discrete word identification, which is comparable to Whisper Tiny's 58% performance but with significantly fewer resources. The method required just 2 training samples per class and achieved inference times of 0.02 seconds, compared to Whisper's 0.8 seconds.

## Method Summary
NUTS processes speech by first converting 16-bit, 16kHz audio into 80-bin MEL spectrograms (8000 values), then applying random dimensionality reduction to create a lower-dimensional representation. The Naliifier preprocessor synthesizes similarity relationships between instances by comparing properties and generating new Narsese statements. NARS then performs similarity-based classification using subjective truth values rather than objective probabilities. The system was tested on the Speech Commands dataset v2 with 35 word classes, using only 2 training examples per class and 1 test example.

## Key Results
- NUTS achieved 64% accuracy with 2 training examples per class and 4 reduced dimensions
- Performance peaked at 4 dimensions and improved with more training examples, reaching 90% accuracy with 20 examples per class
- NUTS required 2 training samples per class and achieved inference times of 0.02 seconds, compared to Whisper's 0.8 seconds and 58% accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Random dimensionality reduction combined with NARS enables few-shot learning by creating a compressed representation space where similarity-based reasoning can operate effectively
- Random projection matrix transforms high-dimensional speech features into a lower-dimensional space where essential discriminative information is preserved while noise is reduced
- Core assumption: Random projections preserve pairwise distances between data points with high probability (Johnson-Lindenstrauss lemma)
- Evidence anchors: Performance peaked at 4 reduced dimensions (64% accuracy); random reduction worked even with small training samples
- Break condition: If random projection destroys too much discriminative information, NARS cannot establish meaningful similarity relationships

### Mechanism 2
- NARS can perform perception tasks by treating perceptual data as subjective truth values rather than objective facts
- NARS assigns subjective truth values to perceptual properties, allowing it to handle uncertainty and conflicting information naturally
- Core assumption: Perceptual data can be meaningfully represented as truth values between 0 and 1
- Evidence anchors: NARS can cope with conflicting information, requires less data for inference, and explicitly implements logic for concept removal
- Break condition: If perceptual data requires absolute truth values rather than subjective ones, NARS's approach becomes less effective

### Mechanism 3
- The Naliifier preprocessor bridges the gap between raw perceptual data and NARS reasoning by synthesizing similarity relationships
- Naliifier compares properties across instances to find similarities, then generates new Narsese statements encoding these relationships
- Core assumption: Similarity relationships between perceptual instances can be algorithmically determined from their properties
- Evidence anchors: NARS successfully determined that an unlabelled instance was similar to a labelled instance using the Naliifier
- Break condition: If similarity computation becomes too computationally expensive (O(n²) complexity), the system becomes impractical

## Foundational Learning

- Concept: Non-Axiomatic Reasoning System (NARS)
  - Why needed here: NARS provides the reasoning framework that can handle insufficient knowledge and resources, central to the intelligence definition being tested
  - Quick check question: What is the key difference between NARS and traditional logical reasoning systems?

- Concept: Dimensionality reduction and random projections
  - Why needed here: High-dimensional speech data (8000 values) must be compressed to make reasoning computationally feasible while preserving discriminative information
  - Quick check question: What mathematical property ensures that random projections preserve distances between points?

- Concept: Few-shot learning
  - Why needed here: The system is designed to work with minimal training data (2 examples per class), testing the intelligence definition's assumption of insufficient resources
  - Quick check question: Why does few-shot learning require different approaches than traditional supervised learning?

## Architecture Onboarding

- Component map: MEL encoding → Random projection → Naliifier comparison → NARS reasoning → Classification
- Critical path: 16-bit, 16kHz audio → MEL spectrogram (80 bins × 100 frames) → Random projection matrix (8000 × D) → Naliifier preprocessor → NARS reasoning → Category label prediction
- Design tradeoffs: Low dimensionality (D=4) enables fast reasoning but may lose information; Naliifier O(n²) complexity vs. NARS's ability to handle uncertainty; subjective truth values vs. objective probabilities
- Failure signatures: Random performance (~2% for 35 classes) indicates dimensionality reduction is ineffective; high computation time in Naliifier suggests scaling issues; poor performance on specific words indicates systematic biases
- First 3 experiments: 1) Test random projection preservation of distances using synthetic data with known similarity structure; 2) Measure Naliifier performance scaling with increasing number of instances and properties; 3) Compare NARS reasoning performance with and without the Naliifier preprocessor

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does NARS' ability to handle conflicting information provide a significant advantage over other reasoning systems for speech recognition tasks?
- Basis in paper: The paper states that NARS "can cope with holding conflicting information in its knowledge base" and this was one of the motivations for using it in speech recognition
- Why unresolved: The paper did not compare NARS to other reasoning systems (e.g., predicate logic systems) for speech recognition tasks. The advantage of NARS' conflict handling was not empirically tested
- What evidence would resolve it: A direct comparison of NARS with other reasoning systems (e.g., CYC) on speech recognition tasks, measuring performance with conflicting or ambiguous input data

### Open Question 2
- Question: What is the optimal dimensionality reduction technique for NUTS to maximize speech recognition accuracy?
- Basis in paper: The paper tested random projection without sparsity but notes that "randomly reducing dimensions worked, even for small numbers of training examples" and performance peaked at 4 reduced dimensions
- Why unresolved: Only one dimensionality reduction technique was tested. The paper does not explore whether more sophisticated techniques (e.g., PCA, autoencoders) could yield better results
- What evidence would resolve it: Systematic testing of various dimensionality reduction techniques on the same speech recognition task, comparing accuracy and computational efficiency

### Open Question 3
- Question: How does NUTS' performance scale with vocabulary size and word complexity?
- Basis in paper: The paper tested NUTS on a 35-word vocabulary from the Speech Commands dataset, but does not explore performance with larger vocabularies or more complex words
- Why unresolved: The paper focused on a specific, limited vocabulary task. Scaling to larger vocabularies or continuous speech recognition was not investigated
- What evidence would resolve it: Testing NUTS on progressively larger vocabulary sizes and more complex speech datasets, measuring accuracy and computational requirements

## Limitations

- The exact Naliifier algorithm implementation and Narsese encoding format are only partially described, making faithful reproduction difficult
- The O(n²) complexity of the Naliifier raises concerns about scalability beyond the 35-class dataset used
- The claim that performance peaks at 4 dimensions needs validation across different datasets and tasks, as this may be dataset-specific

## Confidence

- High confidence: The core claim that NUTS achieves 64% accuracy with 2 training examples per class is well-supported by the experimental results presented
- Medium confidence: The comparison to Whisper Tiny (58% accuracy) is valid, though the resource usage claims need independent verification as exact hardware specifications are not provided
- Low confidence: The mechanism explanations, particularly around how NARS handles perceptual data and the specific role of the Naliifier, lack sufficient detail for full mechanistic understanding

## Next Checks

1. Reproduce the core experiment with the exact Speech Commands dataset using the specified 2 training examples per class and 4 reduced dimensions, measuring both accuracy and inference time on identical hardware

2. Test the dimensionality sensitivity by systematically varying the number of reduced dimensions (2, 4, 8, 16) and measuring performance degradation/gains across multiple random seeds

3. Implement a baseline comparison using traditional few-shot learning methods (like Prototypical Networks or Matching Networks) on the same dataset with identical training constraints to establish whether the NARS-based approach provides unique advantages beyond standard few-shot techniques