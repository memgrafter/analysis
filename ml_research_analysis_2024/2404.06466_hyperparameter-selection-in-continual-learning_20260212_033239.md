---
ver: rpa2
title: Hyperparameter Selection in Continual Learning
arxiv_id: '2404.06466'
source_url: https://arxiv.org/abs/2404.06466
tags:
- task
- data
- learning
- frameworks
- end-of-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates hyperparameter optimization (HPO) frameworks
  for continual learning (CL), where standard HPO cannot be applied due to the sequential
  nature of the data stream. The paper benchmarks several realistic HPO frameworks
  including end-of-training HPO, first-task HPO, current-task HPO, and seen-tasks
  HPO variants across popular CL methods (ER, iCaRL, ER-ACE, ESMER, DER++) on standard
  CL benchmarks (CIFAR-10, CIFAR-100, CORe50, Tiny ImageNet) and heterogeneous task
  settings.
---

# Hyperparameter Selection in Continual Learning

## Quick Facts
- arXiv ID: 2404.06466
- Source URL: https://arxiv.org/abs/2404.06466
- Reference count: 40
- Standard HPO methods fail in CL due to sequential data streams; all HPO frameworks perform similarly on current benchmarks

## Executive Summary
This paper investigates hyperparameter optimization (HPO) frameworks for continual learning (CL), where standard HPO cannot be applied due to the sequential nature of data streams. The authors benchmark several realistic HPO approaches including end-of-training HPO, first-task HPO, current-task HPO, and seen-tasks HPO across popular CL methods (ER, iCaRL, ER-ACE, ESMER, DER++) on standard CL benchmarks (CIFAR-10, CIFAR-100, CORe50, Tiny ImageNet) and heterogeneous task settings.

The key finding is that all HPO frameworks perform similarly in terms of predictive performance, with no consistent advantage of one method over others. Notably, first-task HPO—which only uses data from the first task to fit hyperparameters—performs comparably to more computationally expensive approaches. This suggests that for current CL benchmarks, dynamic adaptation of hyperparameters is not necessary, and practitioners can select HPO frameworks based on computational efficiency rather than performance.

## Method Summary
The paper evaluates multiple HPO frameworks adapted for continual learning: end-of-training HPO (optimizes after all tasks), first-task HPO (optimizes using only first task data), current-task HPO (optimizes using current task data), and seen-tasks HPO (optimizes using all seen task data). These frameworks are tested across five CL methods (Experience Replay, iCaRL, ER-ACE, ESMER, DER++) on four standard benchmarks with both homogeneous and heterogeneous task settings. The experiments systematically compare predictive performance across frameworks while accounting for computational costs.

## Key Results
- All HPO frameworks achieve similar predictive performance across all CL methods and benchmarks tested
- First-task HPO performs comparably to more computationally expensive approaches despite using only first task data
- No consistent advantage exists between HPO frameworks, suggesting dynamic hyperparameter adaptation is unnecessary for current CL benchmarks

## Why This Works (Mechanism)
The lack of performance differences between HPO frameworks suggests that current CL benchmarks may not be sufficiently challenging or diverse to require sophisticated hyperparameter adaptation strategies. The uniformity of task distributions and data streams in standard benchmarks may mask the need for dynamic HPO approaches that would be necessary in more realistic, heterogeneous environments.

## Foundational Learning
- **Continual Learning**: Learning from sequential data streams without forgetting previous tasks; needed because real-world data arrives continuously and standard training causes catastrophic forgetting
- **Hyperparameter Optimization**: Systematic search for optimal model hyperparameters; needed because manual tuning is inefficient and suboptimal
- **Experience Replay**: Storing and replaying samples from previous tasks; quick check: verify buffer size and sampling strategy affect performance
- **Task-Agnostic Learning**: Methods that don't require task boundaries; quick check: assess performance when task identities are unknown
- **Benchmark Homogeneity**: Standard CL benchmarks have uniform task distributions; quick check: test on more diverse, non-stationary data streams
- **Catastrophic Forgetting**: Degradation of performance on previous tasks when learning new ones; quick check: monitor accuracy decay on earlier tasks

## Architecture Onboarding

Component Map: Data Stream -> CL Method -> HPO Framework -> Performance Evaluation

Critical Path: Sequential task data → CL method training → HPO framework application → performance measurement

Design Tradeoffs: Computational cost vs. performance gain; simpler HPO frameworks (first-task) vs. more complex ones (end-of-training); offline optimization vs. online adaptation

Failure Signatures: Performance degradation when task distributions change; overfitting to early tasks in first-task HPO; computational bottlenecks in end-of-training HPO

First Experiments:
1. Compare first-task HPO vs. end-of-training HPO on a benchmark with highly non-stationary task distributions
2. Test HPO framework performance when task difficulty varies significantly across the sequence
3. Evaluate online HPO approaches that update hyperparameters without storing previous task data

## Open Questions the Paper Calls Out
The paper explicitly states that future research should move beyond standard CL benchmarks to develop more robust HPO frameworks. The current benchmarks may not reflect real-world scenarios where task distributions and data streams are more complex and heterogeneous.

## Limitations
- Findings are confined to current CL benchmarks which may not reflect real-world complexity
- Study focuses on specific CL methods and benchmarks, potentially missing diverse continual learning scenarios
- The lack of performance differences might reflect benchmark limitations rather than true equivalence of HPO frameworks

## Confidence
- Key finding (all HPO frameworks perform similarly): **Medium** - consistent across methods but benchmark limitations are concerning
- Practical recommendation (choose based on computational efficiency): **Medium** - logically follows from results but depends on benchmark validity
- First-task HPO performance: **Medium** - surprising result that warrants additional scrutiny

## Next Checks
1. Test HPO frameworks on more diverse and challenging CL benchmarks with non-stationary task distributions and heterogeneous data streams
2. Evaluate HPO frameworks in online/streaming settings where hyperparameters must be updated without access to stored data from previous tasks
3. Investigate whether first-task HPO's strong performance persists when task difficulty varies significantly or when early tasks are not representative of later ones