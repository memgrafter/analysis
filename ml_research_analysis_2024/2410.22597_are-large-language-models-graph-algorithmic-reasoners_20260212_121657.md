---
ver: rpa2
title: Are Large-Language Models Graph Algorithmic Reasoners?
arxiv_id: '2410.22597'
source_url: https://arxiv.org/abs/2410.22597
tags:
- output
- reasoning
- steps
- intermediate
- input-output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MAGMA, a benchmark for evaluating Large Language
  Models (LLMs) on classical graph algorithms by incorporating intermediate solution
  steps. Unlike prior graph reasoning tasks that only assess final outputs, MAGMA
  tests models' step-by-step execution of algorithms including BFS, DFS, Dijkstra,
  Floyd-Warshall, and Prim's MST.
---

# Are Large-Language Models Graph Algorithmic Reasoners?

## Quick Facts
- arXiv ID: 2410.22597
- Source URL: https://arxiv.org/abs/2410.22597
- Authors: Alexander K Taylor; Anthony Cuturrufo; Vishal Yathish; Mingyu Derek Ma; Wei Wang
- Reference count: 40
- Primary result: MAGMA benchmark shows fine-tuned LLMs with intermediate steps outperform larger zero-shot models on graph algorithms

## Executive Summary
This work introduces MAGMA, a benchmark for evaluating Large Language Models on classical graph algorithms by incorporating intermediate solution steps. Unlike prior graph reasoning tasks that only assess final outputs, MAGMA tests models' step-by-step execution of algorithms including BFS, DFS, Dijkstra, Floyd-Warshall, and Prim's MST. The benchmark adapts the CLRS Algorithmic Reasoning framework to generate structured trajectories of problem statements, intermediate steps, and final outputs. Experiments across five algorithms show that fine-tuning LLMs with intermediate steps significantly improves performance on both intermediate and final steps compared to input-output only models.

## Method Summary
The MAGMA benchmark generates graph datasets using Erdős-Rényi graphs (sizes 5-50, edge probability 0.5) and adapts CLRS benchmark trajectories to natural language format. Three prompting strategies are evaluated: Input-Output (IO), Intermediate Steps (IS), and Intermediate Steps with Hints (ISH). Models including Llama-3-8B, Mistral-7B, and GPT-4o are fine-tuned using parameter-efficient methods with 80:10:10 data splits. Performance is measured using exact-match accuracy for final steps, intermediate steps, and full trajectories on held-out test sets.

## Key Results
- Fine-tuned smaller models (Llama-3-8B, Mistral-7B) significantly outperform larger zero-shot models (GPT-4o) on graph algorithm reasoning
- Incorporating intermediate steps into training improves performance on both intermediate and final steps compared to input-output only approaches
- Including hints in training (ISH) can hinder performance compared to Intermediate Steps (IS) alone
- Models show sensitivity to extraneous information and struggle with longer trajectories like Floyd-Warshall

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intermediate steps improve performance by providing structured reasoning guidance
- Mechanism: When models receive intermediate steps during training, they learn to follow the algorithmic flow rather than jumping to final answers. This structured approach helps models maintain logical consistency across multiple reasoning steps.
- Core assumption: Models can learn to replicate step-by-step reasoning patterns when provided with intermediate supervision
- Evidence anchors: Incorporating intermediate solution steps into training and in-context reasoning benefits the performance of both fine-tuned and zero-shot LLMs; Incorporating intermediate steps into in-context learning and fine-tuning significantly improves algorithmic reasoning performance

### Mechanism 2
- Claim: Fine-tuned smaller models outperform larger zero-shot models due to task-specific optimization
- Mechanism: Fine-tuning on algorithm-specific intermediate steps allows smaller models to develop specialized reasoning capabilities that generalize better than larger models' broad but shallow understanding
- Core assumption: Task-specific fine-tuning can overcome the inherent advantages of larger foundation models
- Evidence anchors: Smaller fine-tuned models outperform larger zero-shot models; Fine-tuning even relatively small language models for complex graph reasoning tasks provides vast performance enhancements over the state-of-the-art zero-shot model (GPT-4o)

### Mechanism 3
- Claim: Models are sensitive to extraneous information in intermediate steps
- Mechanism: When intermediate steps include additional context beyond what's needed for the algorithm, models become confused and performance degrades. Clean, focused intermediate steps work better than verbose ones.
- Core assumption: Model attention mechanisms are disrupted by irrelevant information in context windows
- Evidence anchors: Models fine-tuned with intermediate steps demonstrate sensitivity to extraneous information; The inclusion of additional context for intermediate steps (translated from CLRS hints) hinders performance

## Foundational Learning

- Concept: Graph representation and traversal fundamentals
  - Why needed here: The benchmark tests classical graph algorithms (BFS, DFS, Dijkstra, etc.) which require understanding graph structures and traversal patterns
  - Quick check question: Can you explain the difference between BFS and DFS traversal strategies and when each would be preferred?

- Concept: Algorithmic reasoning and step decomposition
  - Why needed here: The core contribution is evaluating models on intermediate reasoning steps, requiring understanding of how algorithms break down into sequential operations
  - Quick check question: How would you decompose Dijkstra's algorithm into intermediate reasoning steps?

- Concept: Transformer architecture and fine-tuning mechanics
  - Why needed here: The experiments involve fine-tuning various transformer models, requiring understanding of how parameter-efficient fine-tuning works
  - Quick check question: What are the key differences between full fine-tuning and parameter-efficient fine-tuning approaches?

## Architecture Onboarding

- Component map: Data generation → Model selection → Fine-tuning pipeline → Evaluation framework
- Critical path: CLRS benchmark adaptation → Graph sampling → Natural language translation → Model training → Performance evaluation
- Design tradeoffs: Smaller models + fine-tuning vs larger zero-shot models; intermediate steps with vs without hints
- Failure signatures: Hallucinations in predictions, missing prefixes, invalid answer formats, declining accuracy with longer trajectories
- First 3 experiments:
  1. Replicate BFS results using Llama-3-8B with intermediate steps vs input-output only
  2. Test sensitivity to hints by comparing models trained with vs without CLRS hints
  3. Evaluate error type distributions across algorithms to identify common failure patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of intermediate steps impact model performance on graph algorithms with significantly longer trajectories versus shorter ones?
- Basis in paper: The paper observes that models perform better on algorithms with shorter trajectories (BFS, DFS) compared to longer ones (Floyd-Warshall, Prim's MST), but doesn't systematically analyze the relationship between trajectory length and performance improvement from intermediate steps.
- Why unresolved: The paper provides some analysis of trajectory length effects but doesn't conduct controlled experiments varying trajectory lengths across algorithms to isolate this factor.
- What evidence would resolve it: Systematic experiments varying graph sizes to create controlled differences in trajectory lengths while keeping other factors constant, measuring performance improvements from intermediate steps across these variations.

### Open Question 2
- Question: What specific types of hints from the CLRS benchmark are most beneficial versus detrimental to model performance?
- Basis in paper: The paper observes that including translated hints (Intermediate Steps with Hints) hinders performance compared to Intermediate Steps alone, but doesn't analyze which specific hint types cause this.
- Why unresolved: The paper treats all hints as a monolithic group without decomposing which specific hint elements (e.g., queue states, distance matrices, predecessor nodes) contribute to performance degradation.
- What evidence would resolve it: Ablation studies testing individual hint components separately to identify which specific types of algorithmic information improve versus harm performance.

### Open Question 3
- Question: How do different error types (missing prefixes, false negatives, hallucinations, invalid answer items) correlate with model architecture choices versus prompting strategies?
- Basis in paper: The paper provides error analysis showing error type distributions across models but doesn't systematically correlate these with architectural differences or prompting strategies.
- Why unresolved: The error analysis is descriptive rather than explanatory, showing patterns without establishing causal relationships between model choices and specific error types.
- What evidence would resolve it: Controlled experiments varying model architectures and prompting strategies while measuring changes in specific error type distributions to establish correlations.

## Limitations
- The benchmark relies on synthetic graph data using Erdős-Rényi models, which may not capture real-world graph complexity
- Natural language translation from CLRS matrices to chat format represents a potential source of noise affecting model performance
- The study focuses exclusively on classical graph algorithms, limiting generalizability to other algorithmic domains
- Exact-match accuracy metrics may be overly strict for assessing algorithmic reasoning capabilities

## Confidence

- **High confidence**: The finding that fine-tuned smaller models outperform larger zero-shot models is well-supported by experimental results and aligns with established trends in transfer learning
- **Medium confidence**: The claim that intermediate steps improve performance is supported by ablation studies, though the magnitude of improvement varies significantly across algorithms
- **Medium confidence**: The observation that hints can hinder performance is based on controlled experiments, but the underlying mechanisms require further investigation

## Next Checks

1. **Robustness testing**: Evaluate model performance on non-synthetic graph datasets with real-world characteristics to assess generalization beyond Erdős-Rényi graphs
2. **Alternative metric validation**: Implement semantic similarity metrics for intermediate step evaluation to complement exact-match accuracy and capture alternative valid reasoning paths
3. **Scaling analysis**: Conduct experiments across a broader range of model sizes and fine-tuning durations to determine optimal resource allocation for algorithmic reasoning tasks