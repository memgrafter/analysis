---
ver: rpa2
title: Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models
arxiv_id: '2410.24079'
source_url: https://arxiv.org/abs/2410.24079
tags:
- marginalization
- effects
- random
- marginalize
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Marginalizing random effects in linear mixed-effects models (LMMs)
  can improve Hamiltonian Monte Carlo (HMC) efficiency, but naive marginalization
  introduces cubic-time complexity. This paper develops fast linear algebra techniques
  to reduce marginalization complexity to linear time for LMMs with normal or log-normal
  likelihoods.
---

# Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models

## Quick Facts
- **arXiv ID:** 2410.24079
- **Source URL:** https://arxiv.org/abs/2410.24079
- **Authors:** Jinlin Lai; Justin Domke; Daniel Sheldon
- **Reference count:** 40
- **Key outcome:** Marginalizing random effects in LMMs improves HMC efficiency through fast linear algebra techniques that reduce complexity from cubic to linear time

## Executive Summary
This paper develops fast linear algebra techniques to enable efficient marginalization of random effects in linear mixed-effects models (LMMs) for Hamiltonian Monte Carlo (HMC) inference. The key insight is leveraging the tree structure of design matrices to accelerate matrix operations during marginalization, reducing complexity from cubic to linear time for models with normal or log-normal likelihoods. Experiments on nine cognitive science datasets and an ETH instructor evaluation model demonstrate that marginalization consistently improves effective sample size per iteration and sampling speed, with up to 20× speedup in some cases.

## Method Summary
The method implements vectorized marginalization for LMMs in NumPyro using specialized classes to express models. It automatically detects LMM structure and applies marginalization using matrix inversion and determinant lemmas, exploiting the tree-structured sparsity pattern in design matrices. The approach reduces computational complexity from O(N³) to O(Nd²) where d is the maximum number of non-zero elements per row. The system integrates marginalized likelihood functions into HMC sampling and provides posterior samples of marginalized random effects after inference.

## Key Results
- Marginalization improves ESS per iteration and sampling speed across all nine tested cognitive science datasets
- Up to 20× speedup observed in the grouse ticks model when marginalizing u2 random effects
- Automatic marginalization consistently outperforms non-marginalized versions without harming sampling quality
- The method successfully resolves funnel pathologies in hierarchical models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Marginalizing random effects reduces the number of latent variables HMC must sample, lowering computational complexity from O(H^5/4) to O((H-r)^5/4) where r is the number of marginalized variables.
- Mechanism: The joint distribution of observations and random effects forms a multivariate normal. By integrating out the random effects analytically using properties of multivariate normal distributions, the posterior sampling space is reduced while preserving the information needed for inference.
- Core assumption: The model structure must be a linear mixed-effects model (LMM) with normal or log-normal likelihood, allowing the joint distribution to be multivariate normal.
- Evidence anchors:
  - [abstract]: "Marginalizing random effects in linear mixed-effects models (LMMs) can improve Hamiltonian Monte Carlo (HMC) efficiency"
  - [section]: "Since the mean of y is linear in each ui and all of these variables are normally distributed, the joint distribution of (y, u1, ..., uL) is also multivariate normal"
  - [corpus]: No direct evidence found in corpus papers about marginalization in LMMs specifically
- Break condition: The method fails when the model structure doesn't form a multivariate normal joint distribution, or when the random effects have non-normal distributions.

### Mechanism 2
- Claim: Fast linear algebra techniques reduce marginalization complexity from cubic to linear time by exploiting the tree structure of design matrices.
- Mechanism: Using matrix inversion lemma and matrix determinant lemma, combined with the tree-structured sparsity pattern in the design matrix A, the computationally expensive matrix operations (inversion and determinant calculation of N×N matrices) are reduced to operations on smaller block-diagonal matrices.
- Core assumption: The design matrix A has tree structure where each row has at most d nonzero elements, and covariance matrices Σu and Σy have special structures (block-diagonal and diagonal respectively).
- Evidence anchors:
  - [abstract]: "develops fast linear algebra techniques to reduce marginalization complexity to linear time for LMMs"
  - [section]: "We show that marginalization for a single random effect can be achieved with linear time complexity"
  - [corpus]: No direct evidence found in corpus papers about fast linear algebra techniques for marginalization
- Break condition: The method fails when the design matrix doesn't have tree structure, or when covariance matrices don't have the assumed special structures.

### Mechanism 3
- Claim: Marginalization resolves funnel pathologies in hierarchical models by removing the correlation between variance parameters and random effect parameters.
- Mechanism: In hierarchical models, variance parameters (like σ) are often highly correlated with the random effects they govern, creating funnel-shaped posterior geometries that are difficult for HMC to sample efficiently. Marginalizing the random effects breaks this correlation.
- Core assumption: The funnel pathology is caused by the correlation between variance parameters and random effect parameters in the hierarchical model.
- Evidence anchors:
  - [abstract]: "marginalizing some variables can greatly improve inference" and "A notable one is the 'funnel' shape created by correlation between variance parameters and parameters for fixed or random effects"
  - [section]: "Marginalization [35] and other program transformations [26] have been shown to be useful in addressing such pathologies"
  - [corpus]: No direct evidence found in corpus papers about funnel pathologies in marginalization
- Break condition: The method fails when the funnel pathology is caused by other factors not addressed by marginalization, or when the model structure doesn't create funnel geometries.

## Foundational Learning

- Concept: Multivariate normal distribution properties
  - Why needed here: The core mechanism relies on analytically integrating out random effects using properties of multivariate normal distributions
  - Quick check question: Given a multivariate normal distribution p(x,y) where x ∈ R^m and y ∈ R^n, what are the analytical expressions for p(x) and p(y|x)?

- Concept: Matrix inversion lemma (Woodbury formula)
  - Why needed here: This lemma is used to efficiently compute the inverse of (AΣuAT + Σy) without explicitly forming the N×N matrix
  - Quick check question: For matrices A (N×M), U (M×M), C (N×N), and V (M×M), what is the Woodbury formula for (A U AT + C)^-1?

- Concept: Tree-structured sparsity patterns in design matrices
  - Why needed here: The tree structure of the design matrix A is crucial for reducing computational complexity from O(N^3) to O(Nd^2)
  - Quick check question: If each row of matrix A has at most d non-zero elements, what is the computational complexity of computing A^T A?

## Architecture Onboarding

- Component map: User model specification -> Marginalization transformation layer -> Fast linear algebra engine -> HMC sampler integration -> Recovery module

- Critical path:
  1. User specifies LMM model
  2. System detects LMM structure and identifies candidates for marginalization
  3. System transforms model by analytically marginalizing selected random effects
  4. System optimizes marginalization using fast linear algebra techniques
  5. HMC sampler runs on reduced-dimensional space
  6. System recovers marginalized random effects from posterior samples

- Design tradeoffs:
  - Full marginalization vs partial marginalization: Marginalizing all random effects gives best performance but requires stronger assumptions (scaled identity covariance matrices)
  - Vectorized vs scalar marginalization: Vectorized approach is much faster but requires LMM structure
  - Automatic vs manual marginalization: Automatic detection is convenient but may miss optimization opportunities

- Failure signatures:
  - Slow performance: Model doesn't match LMM assumptions or covariance matrices don't have expected structure
  - Sampling issues: Funnel pathologies persist because marginalization didn't break the problematic correlations
  - Memory errors: Very large models exceed available memory when forming intermediate matrices

- First 3 experiments:
  1. Simple linear regression with random intercepts: Create a basic LMM with one group of random effects and verify marginalization improves ESS and sampling speed
  2. Cross-effects model: Implement the ETH instructor evaluation model and test different marginalization strategies (u1, u2, u3, or all)
  3. Model with funnel pathology: Implement the grouse ticks model and verify marginalization resolves the funnel issue while reparameterization does not

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does marginalization always improve sampling efficiency compared to non-centered parameterization in LMMs?
- Basis in paper: [explicit] The paper states that "reparameterization does not improve the running time of HMC, while marginalizing u2 speeds up sampling by about 20%" in the grouse ticks model, suggesting marginalization can outperform reparameterization.
- Why unresolved: The paper only compares these two techniques on a single model. It's unclear if this result generalizes across different LMM structures and datasets.
- What evidence would resolve it: Systematic comparison of marginalization vs reparameterization across multiple LMMs with varying random effect structures and likelihood types would provide definitive evidence.

### Open Question 2
- Question: How does marginalization perform in LMMs with more than two levels of hierarchical grouping?
- Basis in paper: [inferred] The experiments focus on models with one or two classes of random effects. The paper mentions "cross-effects models" but doesn't explore deeper hierarchies.
- Why unresolved: The paper doesn't test models with three or more levels of grouping (e.g., students within classrooms within schools). The computational complexity and effectiveness of marginalization in deeper hierarchies remains unknown.
- What evidence would resolve it: Experimental results on LMMs with three or more hierarchical levels would demonstrate whether the benefits of marginalization scale with hierarchy depth.

### Open Question 3
- Question: Can the marginalization techniques be extended to non-normal likelihoods beyond probit regression?
- Basis in paper: [explicit] The discussion states "Another potential future direction is to marginalize classification models with probit regressions" and notes that marginalization would turn probit models into multivariate probit models.
- Why unresolved: The paper only implements marginalization for normal and log-normal likelihoods. The extension to other non-normal likelihoods is mentioned as future work without implementation.
- What evidence would resolve it: Successful implementation and evaluation of marginalization techniques on a variety of non-normal likelihoods (e.g., Poisson, negative binomial, categorical) would demonstrate the method's generality.

## Limitations
- The method assumes tree-structured design matrices and specific covariance matrix structures, limiting applicability to arbitrary hierarchical models
- Performance degrades when design matrices don't have perfect tree structure or when covariance matrices don't have the assumed special forms
- The complexity analysis assumes specific sparsity patterns that may not hold in practice for all LMM applications

## Confidence
- Mechanism 1 (dimensionality reduction through marginalization): **High** - well-established statistical principle with clear mathematical foundation
- Mechanism 2 (fast linear algebra techniques): **Medium** - theoretically sound but implementation details critical for practical performance
- Mechanism 3 (funnel pathology resolution): **Medium** - demonstrated in specific cases but general applicability to all funnel geometries needs verification

## Next Checks
1. **Scalability testing**: Implement the algorithm on synthetic LMMs with varying numbers of random effects and groups to empirically verify the claimed linear complexity scaling
2. **Non-tree structure cases**: Test the method on models where design matrices don't have perfect tree structure to identify performance degradation points
3. **Funnel pathology breadth**: Systematically test the method on a suite of hierarchical models known to exhibit funnel pathologies to verify it consistently resolves these issues