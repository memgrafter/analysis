---
ver: rpa2
title: Common 7B Language Models Already Possess Strong Math Capabilities
arxiv_id: '2403.04706'
source_url: https://arxiv.org/abs/2403.04706
tags:
- math
- data
- question
- number
- gsm8k
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that common 7B language models, such as
  LLaMA-2 7B, already possess strong mathematical capabilities, contrary to the belief
  that advanced mathematical reasoning is exclusive to larger models. By scaling up
  supervised fine-tuning data using synthetic math questions, the model's stability
  in generating correct answers is significantly improved.
---

# Common 7B Language Models Already Possess Strong Math Capabilities

## Quick Facts
- arXiv ID: 2403.04706
- Source URL: https://arxiv.org/abs/2403.04706
- Reference count: 40
- Common 7B language models already possess strong mathematical capabilities when scaled with synthetic data

## Executive Summary
This paper challenges the conventional wisdom that advanced mathematical reasoning is exclusive to larger language models by demonstrating that common 7B models like LLaMA-2 7B already possess strong mathematical capabilities. The key insight is that the primary limitation is not capability but stability - the base model can solve math problems but struggles to consistently generate correct answers. By scaling supervised fine-tuning data with synthetic math questions, the authors achieve state-of-the-art accuracy on GSM8K (82.6%) and MATH (40.6%) benchmarks, surpassing previous models by 14.2% and 20.8% respectively.

## Method Summary
The authors fine-tune LLaMA-2 7B using supervised fine-tuning (SFT) with synthetic math questions generated by GPT-4 Turbo. The synthetic data is scaled up to approximately one million samples, with a three-step generation process: question generation, verification using GPT-4 Turbo, and answer generation. The model is evaluated on GSM8K and MATH benchmarks using both single-attempt accuracy (Pass@1) and multi-attempt accuracy (PassRatio@1), where multiple attempts are made and the best result is taken.

## Key Results
- Achieved 82.6% accuracy on GSM8K, surpassing previous state-of-the-art by 14.2%
- Achieved 40.6% accuracy on MATH, surpassing previous state-of-the-art by 20.8%
- Demonstrated that synthetic data is nearly as effective as real data and shows no clear saturation when scaled to ~1M samples
- Showed that single-step reasoning accuracy improvement drives overall performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling SFT data improves the stability of correct answer generation.
- Mechanism: Increasing the amount of supervised fine-tuning data reduces the variance in model outputs, making it more likely to consistently produce correct answers across multiple attempts.
- Core assumption: The base model already possesses strong mathematical capabilities, and additional training data primarily enhances reliability rather than capability.
- Evidence anchors:
  - [abstract]: "The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities."
  - [section]: "We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers."
  - [corpus]: Weak - no direct evidence in related papers.
- Break condition: If the base model lacks fundamental mathematical reasoning abilities, scaling data will not improve stability.

### Mechanism 2
- Claim: Synthetic data is nearly as effective as real data for scaling mathematical capabilities.
- Mechanism: High-quality synthetic questions generated by a strong model (GPT-4 Turbo) can mimic the distribution of real mathematical problems, providing a scalable alternative when real data is scarce.
- Core assumption: The synthetic question generation process preserves the key characteristics of real mathematical problems.
- Evidence anchors:
  - [abstract]: "To overcome this limitation, we employ synthetic data, which proves to be nearly as effective as real data and shows no clear saturation when scaled up to approximately one million samples."
  - [section]: "The synthetic math questions are nearly as effective as the real ones."
  - [corpus]: Weak - no direct evidence in related papers.
- Break condition: If synthetic data generation fails to capture the complexity or variety of real problems, effectiveness will degrade.

### Mechanism 3
- Claim: Single-step reasoning accuracy improvement drives overall performance gains.
- Mechanism: As SFT data increases, the model becomes more reliable at each reasoning step, which compounds to improve final answer accuracy, especially for complex problems requiring multiple steps.
- Core assumption: Mathematical problem-solving accuracy can be approximated by the power function of single-step reasoning accuracy.
- Evidence anchors:
  - [section]: "The accuracy of solving math problems follows a power law with respect to the number of chain-of-thought (CoT) steps with different SFT data quantities."
  - [section]: "We hypothesize that the increase in final answer accuracy can be interpreted by the improvement in single-step reasoning accuracy."
  - [corpus]: Weak - no direct evidence in related papers.
- Break condition: If multi-step reasoning introduces compounding errors that single-step improvements cannot mitigate.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: Mathematical problem-solving often requires multiple reasoning steps, and CoT allows the model to break down complex problems into manageable parts.
  - Quick check question: Can you explain how a model would solve "If I have 3 apples and buy 2 more, how many do I have?" using CoT reasoning?

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT adapts the base model to follow instructions and improve performance on specific tasks like mathematical reasoning.
  - Quick check question: What is the difference between pre-training and fine-tuning in language model development?

- Concept: Data scaling laws
  - Why needed here: Understanding how model performance improves with increasing amounts of training data is crucial for determining when additional data will be beneficial.
  - Quick check question: If a model's accuracy improves from 50% to 70% when data increases from 1K to 10K samples, what might you expect when increasing to 100K samples?

## Architecture Onboarding

- Component map: LLaMA-2 7B base model → Supervised Fine-Tuning → Synthetic Data Generation → Evaluation
- Critical path: Data generation → SFT → Inference → Evaluation
- Design tradeoffs: Synthetic vs real data (scalability vs authenticity), data quantity vs quality, base model strength vs fine-tuning effort
- Failure signatures: Plateauing accuracy despite more data, synthetic data quality issues, overfitting to specific problem types
- First 3 experiments:
  1. Compare Pass@1 and PassRatio@1 on GSM8K with 7.5K vs 15K SFT data
  2. Generate 100 synthetic questions and evaluate their quality against real questions
  3. Test single-step vs multi-step reasoning accuracy on a subset of MATH problems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact point of saturation for synthetic data scaling on GSM8K and MATH benchmarks?
- Basis in paper: [explicit] The paper mentions that accuracy has not yet reached its peak and explores effects of additional scaling as future research.
- Why unresolved: The study did not push the synthetic data to its theoretical limit due to computational constraints and did not define a clear stopping criterion.
- What evidence would resolve it: Systematic experiments scaling synthetic data beyond current limits until performance plateaus, with clear statistical tests to confirm saturation.

### Open Question 2
- Question: How does the choice of base model affect the efficacy of synthetic data scaling for mathematical reasoning?
- Basis in paper: [explicit] The paper tests on LLaMA-2 series and Mistral-7B but does not systematically compare base model architectures or pre-training strategies.
- Why unresolved: Limited exploration of different base models prevents understanding whether synthetic scaling benefits are model-agnostic or architecture-dependent.
- What evidence would resolve it: Comparative studies across diverse base models (e.g., different transformer variants, sizes, and pre-training regimes) using identical synthetic scaling procedures.

### Open Question 3
- Question: What is the impact of synthetic data quality versus quantity on model performance?
- Basis in paper: [inferred] The paper uses a straightforward synthetic generation method and does not explore alternative data quality improvement techniques.
- Why unresolved: No comparison between different synthetic data generation methods or quality control mechanisms to determine if higher quality data could reduce required scaling.
- What evidence would resolve it: Controlled experiments varying synthetic data quality (e.g., using different GPT-4 Turbo prompts, verification methods, or human-annotated data) while keeping quantity constant.

### Open Question 4
- Question: How do different error types (reasoning vs. calculation) respond to synthetic data scaling over time?
- Basis in paper: [explicit] The paper analyzes error types but does not track their evolution as data scaling progresses.
- Why unresolved: The study provides a snapshot of error distributions at specific data scales but does not examine how the balance between reasoning and calculation errors shifts with continued scaling.
- What evidence would resolve it: Longitudinal analysis tracking error type distributions across multiple scaling stages to identify patterns in how different error types are mitigated.

### Open Question 5
- Question: What is the relationship between chain-of-thought step accuracy and final answer accuracy at different data scales?
- Basis in paper: [explicit] The paper estimates single-step reasoning accuracy but does not explore how this relationship changes with data scaling.
- Why unresolved: The study provides estimates at specific data points but does not examine whether the relationship between step accuracy and final accuracy remains consistent across scales.
- What evidence would resolve it: Systematic analysis of step-by-step reasoning accuracy across multiple data scales to determine if the power law relationship holds or if it changes with scale.

## Limitations

- Data Quality Dependency: The paper relies heavily on synthetic data generated by GPT-4 Turbo without comprehensive evaluation of synthetic data quality or its coverage of diverse mathematical problem types.
- Single-Model Evaluation: All experiments use LLaMA-2 7B as the base model, limiting generalizability to other 7B models or different architectural families.
- Limited Statistical Reporting: The paper does not report confidence intervals, variance across multiple runs, or statistical significance tests for the reported accuracy improvements.

## Confidence

- High Confidence: The claim that scaling SFT data improves answer stability is well-supported by the experimental results showing consistent improvements in Pass@1 and PassRatio@1 metrics across increasing data quantities (7.5K to 120K samples).
- Medium Confidence: The assertion that 7B models already possess strong mathematical capabilities is supported by the experimental results but depends on the quality of the base LLaMA-2 7B model and the synthetic data generation process.
- Low Confidence: The claim that single-step reasoning accuracy improvements fully explain multi-step problem performance gains lacks rigorous mathematical proof and relies on observed correlations rather than causal evidence.

## Next Checks

1. **Synthetic Data Quality Audit**: Conduct a systematic comparison of synthetic vs real mathematical problems using human evaluators to assess problem complexity, solution path diversity, and alignment with benchmark distributions.

2. **Cross-Model Generalization Test**: Repeat the scaling experiments with at least two other 7B models (e.g., Mistral 7B, Qwen 7B) to verify that the observed scaling laws and capability emergence are not specific to LLaMA-2 architecture.

3. **Statistical Significance Analysis**: Run multiple independent fine-tuning experiments with different random seeds for each data quantity level and compute confidence intervals and p-values for the accuracy improvements to establish statistical significance of the reported gains.