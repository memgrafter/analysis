---
ver: rpa2
title: 'Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis'
arxiv_id: '2402.14797'
source_url: https://arxiv.org/abs/2402.14797
tags:
- video
- data
- videos
- diffusion
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Snap Video, a video-first text-to-video synthesis
  model that addresses the challenges of applying image-based models to video generation.
  The authors argue that naively repurposing image models for video generation reduces
  motion fidelity, visual quality, and scalability due to the high redundancy in video
  content.
---

# Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis

## Quick Facts
- arXiv ID: 2402.14797
- Source URL: https://arxiv.org/abs/2402.14797
- Authors: Willi Menapace; Aliaksandr Siarohin; Ivan Skorokhodov; Ekaterina Deyneka; Tsai-Shien Chen; Anil Kag; Yuwei Fang; Aleksei Stoliar; Elisa Ricci; Jian Ren; Sergey Tulyakov
- Reference count: 40
- Primary result: Introduces Snap Video, a video-first text-to-video synthesis model achieving state-of-the-art results with a transformer architecture that trains 3.31x faster than U-Nets

## Executive Summary
This paper introduces Snap Video, a video-first text-to-video synthesis model that addresses the challenges of applying image-based models to video generation. The authors argue that naively repurposing image models for video generation reduces motion fidelity, visual quality, and scalability due to the high redundancy in video content. To tackle these issues, Snap Video employs two key innovations: extending the EDM framework to account for spatially and temporally redundant pixels, enabling joint video-image training, and proposing a transformer-based architecture that scales 3.31x faster than U-Nets during training and ~4.5x faster at inference. These improvements allow Snap Video to train a text-to-video model with billions of parameters for the first time, achieving state-of-the-art results on benchmarks.

## Method Summary
Snap Video uses a two-stage cascaded approach: first-stage generates 36×64px videos, second-stage upsamples to 288×512px. The model employs a FIT (Fast Image-Transformer) architecture with 768 learnable latent tokens, processing videos through patchification (1×4×4 patches) → group formation (16×5×4 groups). Training uses the EDM diffusion framework with modified input scaling (σin = s√T) and treats images as infinite-framerate videos for unified training. The model is trained on an internal dataset of 1.265M images and 238k hours of videos, with LAMB optimizer (learning rate 5e-3, cosine learning schedule, batch size 2048 videos and 2048 images).

## Key Results
- User studies show Snap Video is preferred in 81% of cases for prompt-video alignment compared to Gen-2
- Generated the most dynamic videos with the most amount of motion (96% preference)
- Achieved best motion quality (79% preference) in head-to-head comparisons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint spatiotemporal modeling on a compressed latent space enables superior motion fidelity compared to separable U-Net architectures
- Mechanism: FIT-based architecture learns a compressed representation (768 latent tokens) that jointly encodes spatial and temporal dimensions, avoiding the frame-by-frame redundancy inherent in U-Net approaches
- Core assumption: Spatial and temporal redundancy in video content can be effectively exploited through learned compression, and this compression preserves sufficient information for high-quality motion modeling
- Evidence anchors: Abstract claims "proposes a new transformer-based architecture that trains 3.31 times faster than U-Nets...This allows us to efficiently train a text-to-video model with billions of parameters for the first time"

### Mechanism 2
- Claim: Scaling EDM diffusion framework for high-resolution video generation by adjusting input scaling factor σin maintains optimal signal-to-noise ratio
- Mechanism: When processing videos with T frames and upsampling factor s, the input signal is scaled by σin = s√T to maintain the same SNR as the original EDM framework designed for 64×64 images
- Core assumption: The original EDM framework's performance is optimal at its designed resolution, and maintaining equivalent SNR across resolutions preserves this optimality
- Evidence anchors: Section provides mathematical derivation showing "we redefine the forward process as p(xσ|x) ~ N(x/σin, σ2I)" and "we set σin = s√T"

### Mechanism 3
- Claim: Treating images as infinite-framerate videos enables unified training across modalities without compromising temporal modeling
- Mechanism: Images are treated as T-frame videos with infinite frame-rate, allowing the same diffusion process to be applied to both modalities
- Core assumption: Images can be meaningfully represented as high-frame-rate videos without introducing artifacts, and this representation improves the model's temporal understanding
- Evidence anchors: Section states "we match the image and video modalities by treating images as T frames videos with infinite frame-rate" and "treat images as infinite-frame-rate videos consistently improves FID"

## Foundational Learning

- Concept: Diffusion probabilistic models and the EDM framework
  - Why needed here: The entire generation pipeline is built on diffusion principles, requiring understanding of forward/reverse processes, denoising objectives, and sampling
  - Quick check question: What is the key difference between the original EDM framework and the modified version for high-resolution video generation?

- Concept: Transformer architectures and attention mechanisms
  - Why needed here: The FIT architecture relies on cross-attention between latent and patch tokens, self-attention within latent tokens, and understanding of how compression affects attention patterns
  - Quick check question: How does the patchification process work in FIT, and why are patches typically 4×4 pixels?

- Concept: Video compression and redundancy
  - Why needed here: The core innovation relies on exploiting spatial and temporal redundancy through learned compression, requiring understanding of how video compression algorithms work
  - Quick check question: Why does averaging similar video frames increase SNR, and how does this relate to the need for input scaling?

## Architecture Onboarding

- Component map: Input → Patchification (1×4×4 patches) → Group formation (16×5×4 groups) → Cross-attention read → Self-attention → Cross-attention write → Decompression → Output
- Critical path: Input → Patchification → Cross-attention read → Self-attention → Cross-attention write → Output
- Design tradeoffs:
  - Local attention vs feed-forward: Replaced local attention with feed-forward for computational efficiency at high resolution
  - Number of latent tokens: Increased from typical 256 to 768 to capture more temporal information
  - Patch size: Kept 1×4×4 (temporal×spatial) to maintain temporal modeling capability
- Failure signatures:
  - Visible patch artifacts in output videos
  - Temporal inconsistencies or flickering
  - Poor motion quality despite good static quality
- First 3 experiments:
  1. Validate patchification and group formation produce expected token counts (147,456 patches for 512×288×16 video)
  2. Test cross-attention operations with synthetic conditioning to verify information flow
  3. Compare output quality with and without compression to verify learned representation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed joint spatiotemporal modeling approach compare to other video generation methods in terms of computational efficiency and scalability?
- Basis in paper: The authors claim that their method is 3.31x faster in training and ~4.5x faster at inference compared to U-Nets
- Why unresolved: While the authors provide quantitative comparisons of their method to U-Nets, they do not compare it to other state-of-the-art video generation methods in terms of computational efficiency and scalability
- What evidence would resolve it: A comprehensive comparison of Snap Video's computational efficiency and scalability to other video generation methods, including both transformer-based and U-Net-based approaches

### Open Question 2
- Question: How does the proposed diffusion framework handle high-frequency details and fine-grained textures in generated videos?
- Basis in paper: The authors mention that their two-stage cascaded model consists of a first-stage model focusing on motion modeling and scene structure, and a second-stage upsampling model focusing on high-frequency details
- Why unresolved: While the authors describe the general approach of their cascaded model, they do not provide specific details on how their diffusion framework handles high-frequency details and fine-grained textures in generated videos
- What evidence would resolve it: A detailed analysis of the generated videos, focusing on the quality of high-frequency details and fine-grained textures, and a comparison to other video generation methods in terms of this aspect

### Open Question 3
- Question: How does the proposed method handle videos with complex motion patterns, such as non-linear camera movements or intricate object interactions?
- Basis in paper: The authors claim that their method can generate videos with "substantially higher quality, temporal consistency, and motion complexity" compared to recent methods
- Why unresolved: While the authors claim improved motion modeling capabilities, they do not provide specific examples or quantitative measures of their method's performance on videos with complex motion patterns
- What evidence would resolve it: A comprehensive evaluation of Snap Video's performance on videos with various complex motion patterns, including non-linear camera movements and intricate object interactions, and a comparison to other video generation methods in terms of this aspect

## Limitations
- Heavy reliance on an internal dataset of 238k hours of video limits reproducibility and external validation
- Computational efficiency claims based on relative comparisons within their implementation rather than absolute benchmarks
- User study methodology lacks detailed statistical validation with confidence intervals or significance testing

## Confidence
- **High confidence**: Architectural innovations (FIT transformer design, input scaling modifications) are clearly specified and mathematically justified
- **Medium confidence**: Empirical performance claims supported by user studies and benchmark metrics, but internal dataset and lack of statistical validation reduce generalizability confidence
- **Low confidence**: Scalability claims to billions of parameters demonstrated through training curves but lack detailed analysis of practical limitations

## Next Checks
1. Replicate the EDM diffusion framework modifications on a smaller scale using public datasets to verify the input scaling factor calculations and modality matching approach
2. Implement the FIT architecture with configurable parameters (latent tokens, patch sizes) to empirically determine the optimal configuration for different video resolutions and durations
3. Conduct statistical validation of user study results by calculating confidence intervals and performing significance tests on preference percentages across different quality metrics