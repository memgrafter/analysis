---
ver: rpa2
title: 'BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture
  of Experts'
arxiv_id: '2408.08274'
source_url: https://arxiv.org/abs/2408.08274
tags:
- attention
- experts
- dense
- parameters
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BAM introduces a novel method for upcycling dense models into Mixture
  of Experts (MoE) architectures by leveraging both feed-forward network (FFN) and
  attention parameters from pre-trained models. Unlike previous approaches that only
  utilize FFN parameters, BAM employs a soft-routing variant of Mixture of Attention
  (MoA) to incorporate attention experts, maximizing the use of specialized knowledge
  from dense models.
---

# BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts

## Quick Facts
- **arXiv ID:** 2408.08274
- **Source URL:** https://arxiv.org/abs/2408.08274
- **Reference count:** 40
- **Primary result:** BAM upcycles dense models into MoE architectures by leveraging both FFN and attention parameters, outperforming baseline models in perplexity and downstream tasks under equivalent computational constraints.

## Executive Summary
BAM introduces a novel approach for upcycling dense models into Mixture of Experts (MoE) architectures by leveraging both feed-forward network (FFN) and attention parameters from pre-trained models. Unlike previous approaches that only utilize FFN parameters, BAM employs a soft-routing variant of Mixture of Attention (MoA) to incorporate attention experts, maximizing the use of specialized knowledge from dense models. Two variants are proposed: one using separate attention experts for optimal performance and another sharing key-value parameters for improved inference efficiency. BAM also adopts a parallel attention transformer architecture to compute attention and FFN experts concurrently, enhancing throughput. Experiments on models ranging from 590 million to 2 billion parameters demonstrate that BAM consistently outperforms baseline models in perplexity and downstream task performance under equivalent computational and data constraints.

## Method Summary
BAM follows a three-phase approach: branching (creating N copies of a seed model), continued pre-training (specializing each copy on domain-specific data), and mixture model training (initializing MoE with specialized experts using BAM variants). The framework upcycles both FFN and attention parameters from specialized dense models, using soft routing for attention experts and parallel attention transformer architecture for concurrent computation. Two variants are explored: one with separate attention experts for optimal performance and another sharing key-value parameters for improved inference efficiency.

## Key Results
- BAM consistently outperforms baseline upcycling methods (BTX) in perplexity across different model scales
- Including attention experts in upcycling provides complementary benefits to FFN experts
- The parallel attention transformer architecture enables efficient concurrent computation of attention and FFN experts
- KV-sharing variant offers a viable trade-off between performance and inference efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using soft routing for attention experts improves performance compared to top-k routing.
- Mechanism: Soft routing assigns each token to all attention experts, allowing more gradual specialization and avoiding the instability of discrete expert selection.
- Core assumption: The parallel attention transformer architecture can handle the increased computation from soft routing without degrading throughput.
- Evidence anchors:
  - [abstract] "BAM uses a variant of Mixture of Attention [MoA; 18] with soft routing where each token is assigned to every attention expert."
  - [section] "To adapt MoA to our BAM framework, we propose two modifications to the original architecture. First, instead of the traditional top-k routing, BAM employs soft routing for the attention experts..."
  - [corpus] Weak corpus support - neighbors do not mention soft routing specifically, suggesting this is a novel contribution.
- Break condition: If the parallel attention architecture cannot compute attention and FFN experts concurrently efficiently, the computational cost of soft routing would outweigh benefits.

### Mechanism 2
- Claim: Including key-value projections in attention experts (rather than sharing them) improves model performance.
- Mechanism: By having separate K-V parameters for each attention expert, BAM preserves more specialized knowledge from the dense expert models that would be lost through parameter averaging or sharing.
- Core assumption: The performance gain from preserving specialized K-V knowledge outweighs the increased memory requirements and inference latency.
- Evidence anchors:
  - [abstract] "We explore two methods for upcycling attention parameters: 1) initializing separate attention experts from dense models including all attention parameters for the best model performance..."
  - [section] "We propose a variant of MoA where that also incorporates the key-value projections (W k and W v) as part of the attention experts..."
  - [corpus] Weak corpus support - neighbors focus on FFN upcycling or sharing strategies, not KV expert inclusion.
- Break condition: If memory constraints or inference latency become prohibitive, the KV-sharing variant would need to be used despite lower performance.

### Mechanism 3
- Claim: Upcycling both FFN and attention parameters from specialized dense models outperforms upcycling only FFN parameters.
- Mechanism: By initializing both FFN and attention experts from specialized dense models, BAM leverages the full complement of specialized knowledge rather than just the FFN portion, avoiding the performance degradation from averaging attention parameters across models.
- Core assumption: The specialized attention parameters contain domain-specific knowledge that contributes to overall model performance.
- Evidence anchors:
  - [abstract] "Unlike previous approaches that only utilize FFN parameters, BAM employs a soft-routing variant of Mixture of Attention (MoA) to incorporate attention experts, maximizing the use of specialized knowledge from dense models."
  - [section] "A key differentiator for BAM is it takes full advantage of the dense specialized models by leveraging both the FFN and attention layers."
  - [corpus] Weak corpus support - neighbors focus on FFN upcycling but don't explore attention parameter upcycling as BAM does.
- Break condition: If the attention parameters from specialized models don't contain complementary knowledge to the FFN parameters, the additional complexity wouldn't yield performance benefits.

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture and routing mechanisms
  - Why needed here: BAM builds on MoE foundations but extends them to include attention experts and soft routing
  - Quick check question: What is the difference between top-k routing and soft routing in MoE layers, and why might soft routing be beneficial for attention experts?

- Concept: Parallel attention transformer architecture
  - Why needed here: BAM uses this architecture to compute attention and FFN experts concurrently, mitigating computational overhead
  - Quick check question: How does the parallel attention transformer differ from the standard transformer architecture, and what are its computational implications?

- Concept: Parameter upcycling and model merging techniques
  - Why needed here: BAM's core innovation is upcycling both FFN and attention parameters from pre-trained dense models
  - Quick check question: What are the challenges of parameter averaging across different models, and how does BAM avoid these issues?

## Architecture Onboarding

- Component map: Branching phase -> Continued pre-training -> Mixture model training -> Router layers -> Parallel attention transformer
- Critical path:
  1. Create specialized dense expert models through continued pre-training
  2. Initialize MoE with FFN experts from specialized models
  3. Initialize MoE with attention experts using soft routing variant
  4. Train mixture model with auxiliary losses for load balancing and router stability
- Design tradeoffs:
  - Soft routing vs. top-k routing: Better performance but higher computation
  - KV expert inclusion vs. sharing: Better performance but higher memory usage
  - Parallel attention architecture: Better throughput but more complex implementation
- Failure signatures:
  - Training instability: May indicate issues with router initialization or load balancing
  - Performance degradation: Could suggest attention parameters aren't complementary to FFN parameters
  - High inference latency: May require switching to KV-sharing variant
- First 3 experiments:
  1. Implement BAM with KV sharing (simpler variant) and compare perplexity to BTX baseline
  2. Add KV experts to BAM and measure performance improvement over KV-sharing variant
  3. Compare soft routing vs. top-k routing for attention experts while keeping other components constant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the soft-routing mechanism in BAM's attention experts lead to more stable and effective knowledge transfer from specialized dense models compared to hard routing (top-k)?
- Basis in paper: [explicit] The paper explicitly states that soft routing is "crucial" for surpassing baseline performance and that it allows for "more stable training dynamics" since the router is no longer a discrete optimization problem.
- Why unresolved: While the paper demonstrates that soft routing outperforms hard routing in BAM, it doesn't provide a detailed analysis of the specific mechanisms by which soft routing improves stability and knowledge transfer. It also doesn't explore alternative routing strategies or compare the long-term effects of soft vs. hard routing.
- What evidence would resolve it: Experiments comparing BAM with different routing strategies (e.g., top-k, sparse gating, hierarchical routing) and analyzing the learned router distributions and their impact on downstream task performance over time.

### Open Question 2
- Question: How does the choice of data mixture for the specialized dense models affect the performance of BAM?
- Basis in paper: [inferred] The paper mentions that incorporating a portion of general-text data into the expert training mix enhances model performance, but it doesn't explore the optimal data distribution or the impact of different data mixtures on the final BAM model.
- Why unresolved: The paper only uses a simple data mixture with equal distribution of data from each domain. It doesn't investigate the impact of different data distributions, the importance of domain diversity, or the potential benefits of using more or fewer specialized models.
- What evidence would resolve it: Experiments varying the data distribution for the specialized dense models (e.g., different proportions of domain-specific and general data, different number of domains) and analyzing the impact on BAM's perplexity and downstream task performance.

### Open Question 3
- Question: What are the long-term effects of BAM's approach on model robustness and generalization?
- Basis in paper: [inferred] The paper focuses on evaluating BAM's performance on specific downstream tasks, but it doesn't explore its robustness to adversarial examples, its ability to generalize to unseen domains, or its performance on tasks requiring complex reasoning or common sense.
- Why unresolved: The paper doesn't provide a comprehensive analysis of BAM's limitations or potential failure modes. It also doesn't compare BAM's robustness and generalization capabilities to other MoE architectures or dense models.
- What evidence would resolve it: Experiments evaluating BAM's robustness to adversarial attacks, its performance on out-of-distribution data, and its ability to generalize to new tasks or domains. Comparing these results to other MoE architectures and dense models would provide insights into BAM's strengths and weaknesses.

## Limitations
- Limited analysis of router stability mechanisms and their quantitative impact on model performance
- Lack of detailed computational overhead analysis comparing soft vs. top-k routing efficiency
- Insufficient exploration of attention parameter initialization mechanisms from multiple specialized models

## Confidence

**High Confidence**
- BAM outperforms baseline upcycling methods (BTX) in perplexity and downstream tasks
- The parallel attention transformer architecture effectively enables concurrent computation of attention and FFN experts
- KV-sharing variant provides a viable trade-off between performance and inference efficiency

**Medium Confidence**
- Soft routing for attention experts provides performance benefits over top-k routing
- Including key-value projections in attention experts improves model performance
- Upcycling both FFN and attention parameters provides complementary benefits

**Low Confidence**
- The specific mechanism for attention parameter initialization from multiple specialized models
- Long-term router stability and its impact on model performance
- Comprehensive computational efficiency analysis across different routing strategies

## Next Checks

1. **Router Stability Analysis**: Implement detailed monitoring of router load distribution and gate entropy during training to quantify router stability over time and correlate it with model performance metrics.

2. **Computational Overhead Benchmark**: Conduct controlled experiments comparing soft routing vs. top-k routing for attention experts, measuring both perplexity improvement and actual inference latency on representative hardware to validate the claimed efficiency benefits.

3. **Attention Parameter Initialization Ablation**: Create controlled experiments testing different attention parameter initialization strategies (simple averaging vs. weighted combinations vs. BAM's approach) while keeping all other components constant to isolate the impact of attention upcycling methodology.