---
ver: rpa2
title: 'Usage Governance Advisor: From Intent to AI Governance'
arxiv_id: '2412.01957'
source_url: https://arxiv.org/abs/2412.01957
tags:
- risk
- risks
- governance
- system
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Usage Governance Advisor is a semi-automated system that uses a
  Knowledge Graph to evaluate AI safety risks, recommend appropriate models, and propose
  mitigation strategies based on intended use cases. The system automatically populates
  a domain-specific ontology with entities and relationships extracted from technical
  documentation using generative AI, then uses this knowledge graph to map user intent
  to relevant risks, model recommendations, and appropriate evaluations.
---

# Usage Governance Advisor: From Intent to AI Governance

## Quick Facts
- arXiv ID: 2412.01957
- Source URL: https://arxiv.org/abs/2412.01957
- Reference count: 6
- Achieves 73-82% accuracy in questionnaire auto-completion using Chain-of-Thought reasoning

## Executive Summary
Usage Governance Advisor is a semi-automated system that uses a Knowledge Graph to evaluate AI safety risks, recommend appropriate models, and propose mitigation strategies based on intended use cases. The system automatically populates a domain-specific ontology with entities and relationships extracted from technical documentation using generative AI, then uses this knowledge graph to map user intent to relevant risks, model recommendations, and appropriate evaluations. The system achieves 73-82% accuracy in questionnaire auto-completion using Chain-of-Thought reasoning, and 73-92% accuracy in knowledge graph construction with LLM-as-judge verification, enabling organizations to systematically assess and mitigate AI risks while maintaining an audit trail for regulatory compliance.

## Method Summary
The system employs a generative AI pipeline to construct a Knowledge Graph by extracting entities and relationships from technical documentation, which is then populated into a domain-specific ontology using LinkML. User intent is converted into structured questionnaire responses through Chain-of-Thought reasoning with LLM-as-judge validation. The system maps these responses to identified risks, prioritizes them based on severity, and recommends appropriate models with evaluation scores. Risk mitigation strategies include guardrails and specific actions, all documented in an audit trail for regulatory compliance. The architecture integrates Unitxt framework for flexible risk evaluation creation and supports multiple regulatory frameworks through taxonomy mappings.

## Key Results
- 73-82% accuracy in questionnaire auto-completion using Chain-of-Thought reasoning
- 73-92% accuracy in knowledge graph construction with LLM-as-judge verification
- Enables systematic AI risk assessment and mitigation while maintaining audit trails for regulatory compliance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Thought reasoning with LLM-as-judge improves questionnaire auto-completion accuracy.
- Mechanism: The system uses CoT to decompose complex questions into sub-steps, allowing the LLM to reason through the intent-to-risk mapping more accurately. The LLM-as-judge then validates the reasoning quality and corrects errors.
- Core assumption: LLMs can reliably decompose complex governance questions and validate their own reasoning when provided with appropriate few-shot examples.
- Evidence anchors: 73-82% accuracy in questionnaire auto-completion; auto-assist functionality with few-shot examples; related papers focus on LLM alignment but not specifically on CoT for governance questionnaires.

### Mechanism 2
- Claim: Knowledge Graph populated with generative AI ingestion provides scalable risk assessment.
- Mechanism: The system automatically extracts entities and relationships from technical documentation using LLM-based entity/relationship extraction, creating a domain graph that maps AI models to risks, benchmarks, and mitigations.
- Core assumption: Generative AI can accurately extract structured governance information from unstructured technical documentation at scale.
- Evidence anchors: 73-92% accuracy with LLM-as-judge verification; automatic population from technical documentation; related papers discuss KG for legal governance but not for AI risk assessment.

### Mechanism 3
- Claim: Risk prioritization using LLM-as-a-judge creates actionable risk assessments.
- Mechanism: The system maps questionnaire answers to risks using an LLM-as-judge approach that evaluates whether specific answers reduce or amplify each risk, then assigns severity levels (High/Medium/Low).
- Core assumption: LLMs can accurately interpret questionnaire responses in context and map them to appropriate risk severity levels.
- Evidence anchors: Prioritizes risks based on inferred answers; uses LLM-as-a-judge to connect questions to risks; no directly related papers on LLM-based risk prioritization for AI governance.

## Foundational Learning

- Concept: Chain-of-Thought reasoning
  - Why needed here: Enables LLMs to break down complex governance questions into manageable sub-steps, improving reasoning accuracy for questionnaire completion
  - Quick check question: How does CoT differ from direct prompting when solving multi-step problems?

- Concept: Knowledge Graph construction and ontology
  - Why needed here: Provides structured representation of AI governance concepts and relationships, enabling systematic risk assessment and model recommendations
  - Quick check question: What are the key components of a LinkML-based ontology for AI governance?

- Concept: LLM-as-a-judge methodology
  - Why needed here: Validates the quality of LLM outputs (questionnaire answers, KG extractions) through self-assessment, improving reliability
  - Quick check question: How does LLM-as-judge differ from traditional evaluation metrics?

## Architecture Onboarding

- Component map: Intent input → Auto-assist questionnaire (CoT + LLM-as-judge) → Risk prioritization (LLM-as-judge) → Model recommendation (KG query) → Risk evaluation (benchmark execution) → Mitigation recommendation (guardrail + action mapping) → Audit trail storage

- Critical path: User intent → Questionnaire completion → Risk identification → Model selection → Risk evaluation → Mitigation proposal

- Design tradeoffs: Automated KG population trades accuracy for scalability; LLM-based components provide flexibility but introduce uncertainty; multi-pass extraction improves accuracy but increases processing time

- Failure signatures: Low questionnaire accuracy (below 70%), KG extraction precision/recall drops, risk prioritization doesn't match domain expert expectations, model recommendations don't align with benchmark data

- First 3 experiments:
  1. Test questionnaire auto-completion with synthetic intents to verify CoT accuracy
  2. Validate KG extraction on known model documentation to measure precision/recall
  3. Verify risk prioritization matches expert assessments on sample use cases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal frequency for updating the Knowledge Graph to balance accuracy with computational efficiency?
- Basis in paper: The paper mentions that the catalog of benchmarks is "regenerated frequently" but doesn't specify what frequency is optimal.
- Why unresolved: The trade-off between keeping the KG current with evolving AI risks and the computational cost of frequent updates is not addressed.
- What evidence would resolve it: Empirical studies measuring the degradation of KG accuracy over time versus the computational resources required for different update frequencies.

### Open Question 2
- Question: How can the system improve its accuracy in entity/relationship extraction beyond the current 73-82% range while maintaining scalability?
- Basis in paper: The paper acknowledges that generative AI doesn't allow 100% certainty about extracted facts and current accuracy is 73-82%.
- Why unresolved: The paper identifies the accuracy limitation but doesn't propose solutions for improving it beyond current methods.
- What evidence would resolve it: Comparative studies of alternative extraction methods (e.g., hybrid approaches combining rule-based and generative AI methods) that achieve higher accuracy while maintaining the ability to scale.

### Open Question 3
- Question: What are the long-term effectiveness metrics for the proposed mitigation strategies, particularly guardrails?
- Basis in paper: The paper discusses guardrails as real-time filters but states that documentation allows for evaluating effectiveness "over the model's lifetime" without specifying what those metrics are.
- Why unresolved: The paper proposes mitigation strategies but doesn't define how to measure their sustained effectiveness in real-world deployments.
- What evidence would resolve it: Longitudinal studies tracking the performance of deployed systems with and without recommended mitigations across different use cases and timeframes.

## Limitations
- System effectiveness depends heavily on quality and coverage of technical documentation used for Knowledge Graph construction
- LLM-as-judge methodology introduces uncertainty as it relies on the same models being evaluated for reliability
- Performance in real-world deployment scenarios may differ significantly from controlled test conditions

## Confidence

- **High Confidence**: Knowledge Graph construction methodology using LLM-based entity extraction (73-92% accuracy) is well-established with verifiable LLM-as-judge validation
- **Medium Confidence**: Chain-of-Thought reasoning approach for questionnaire auto-completion (73-82% accuracy) shows promise but depends heavily on prompt engineering quality
- **Low Confidence**: Risk prioritization and model recommendation components lack sufficient validation data to assess real-world effectiveness

## Next Checks

1. **Real-world Deployment Testing**: Deploy the system with at least three different organizations across different industries to measure accuracy and effectiveness in actual governance workflows, tracking both technical performance and user satisfaction metrics over 3-6 months.

2. **Cross-Regulatory Compliance Validation**: Test the system's ability to adapt to different regulatory frameworks (EU AI Act, NIST AI RMF, ISO standards) by having domain experts evaluate whether risk assessments and mitigation recommendations align with each framework's requirements.

3. **Hallucination and Error Rate Analysis**: Conduct systematic testing with synthetic and adversarial inputs to measure hallucination rates in Knowledge Graph construction and questionnaire completion, establishing confidence thresholds and fallback mechanisms for high-risk scenarios.