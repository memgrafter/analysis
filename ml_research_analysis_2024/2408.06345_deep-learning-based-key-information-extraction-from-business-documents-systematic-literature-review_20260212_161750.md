---
ver: rpa2
title: 'Deep Learning based Key Information Extraction from Business Documents: Systematic
  Literature Review'
arxiv_id: '2408.06345'
source_url: https://arxiv.org/abs/2408.06345
tags:
- document
- information
- extraction
- documents
- approaches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic literature review analyzed 130 Deep Learning-based
  Key Information Extraction (KIE) approaches for business documents published between
  2017 and 2024. The study found that sequence-based methods dominate (49% of approaches)
  since 2021, while graph-based methods remain popular (30%) and grid-based approaches
  are less common (8%).
---

# Deep Learning based Key Information Extraction from Business Documents: Systematic Literature Review

## Quick Facts
- arXiv ID: 2408.06345
- Source URL: https://arxiv.org/abs/2408.06345
- Reference count: 40
- Analyzed 130 Deep Learning-based KIE approaches for business documents (2017-2024)

## Executive Summary
This systematic literature review provides a comprehensive analysis of Deep Learning-based Key Information Extraction (KIE) approaches for business documents published between 2017 and 2024. The study reveals that sequence-based methods have become dominant since 2021, while graph-based approaches remain popular and grid-based methods are less common. Large Language Models represent an emerging category in the field. The research identifies significant gaps in current approaches, including limited integration of domain knowledge, lack of practical evaluations in real-world settings, and the need for more diverse and higher-quality benchmark datasets.

## Method Summary
The systematic review analyzed 130 papers through keyword-based searches across Scopus, Web of Science, arXiv, and Google Scholar. The study categorizes approaches into four main architectural paradigms: sequence-based (49%), graph-based (30%), grid-based (8%), and LLM-based (8%). Analysis covers model architectures, feature integration strategies (text, layout, visual), pre-training methodologies, and performance metrics across major benchmarks including CORD, FUNSD, and SROIE. The review examines technical characteristics such as model size, parameter counts, and feature combinations to identify trends and research gaps in the KIE field.

## Key Results
- Sequence-based methods dominate the field (49% of approaches) since 2021, while graph-based methods remain popular (30%)
- Most approaches integrate text, layout, and visual features, with text and layout being almost universal
- Model size (parameter count) shows no significant correlation with performance on common benchmarks (CORD, FUNSD, SROIE)
- Significant research gaps exist in domain knowledge integration, practical real-world evaluations, and benchmark dataset diversity

## Why This Works (Mechanism)
The effectiveness of Deep Learning-based KIE approaches stems from their ability to jointly model textual content, spatial layout, and visual features from business documents. Sequence-based methods leverage transformer architectures to capture long-range dependencies and contextual relationships between entities. Graph-based approaches model entity relationships and document structure through node and edge representations. Grid-based methods process documents as pixel arrays, while LLM-based approaches utilize pre-trained language understanding for zero-shot or few-shot extraction. The integration of multi-modal features enables these systems to handle the complexity and variability of visually-rich business documents.

## Foundational Learning
**Tokenization**: Breaking text into smaller units for model processing
- Why needed: Enables neural networks to process text as discrete inputs
- Quick check: Test tokenization consistency across different document layouts

**Layout Embedding**: Representing spatial positions of text elements
- Why needed: Captures visual structure essential for understanding document semantics
- Quick check: Verify layout coordinates align with text bounding boxes

**Visual Feature Extraction**: Processing document images for visual information
- Why needed: Provides contextual cues beyond text content for entity identification
- Quick check: Compare feature representations across document types

**Pre-training Strategies**: Training on large document corpora before fine-tuning
- Why needed: Enables models to learn general document patterns and reduce data requirements
- Quick check: Measure performance improvement from pre-training on domain-specific data

## Architecture Onboarding

**Component Map**: Document Image -> Feature Extractors (Text, Layout, Visual) -> Encoder (Transformer/Graph/Grid) -> Decoder (Sequence/Graph/Grid) -> Structured Output

**Critical Path**: Feature extraction and encoding represents the critical path, as accurate representation of document content and structure directly impacts extraction quality.

**Design Tradeoffs**: Sequence-based methods offer flexibility and strong performance but may struggle with complex relationships. Graph-based approaches excel at modeling entity relationships but require sophisticated graph construction. Grid-based methods are computationally intensive but handle visual patterns well. LLM-based approaches provide strong generalization but may lack domain-specific optimization.

**Failure Signatures**: Poor performance on complex layouts indicates inadequate layout encoding. Low precision suggests insufficient entity boundary detection. Domain mismatch between training and application data leads to generalization failures.

**First Experiments**:
1. Implement basic LayoutLM-style sequence model with text and layout features on CORD dataset
2. Train graph-based approach using DocFormer embeddings on FUNSD dataset
3. Fine-tune pre-trained language model on SROIE receipts for zero-shot extraction

## Open Questions the Paper Calls Out

**Open Question 1**: How do different tokenization strategies impact the performance of KIE systems on complex visually-rich documents, and what domain-specific adaptations are most effective?
- Basis in paper: The paper notes that tokenization is underexplored in KIE and that out-of-the-box tokenizers may not be optimal for this domain.
- Why unresolved: The paper highlights a lack of extensive investigation into tokenizers' role in KIE, despite their importance in sequence-based approaches.
- What evidence would resolve it: Systematic ablation studies comparing different tokenization strategies (domain-specific vocabulary, pre-tokenization setups) across diverse KIE benchmarks.

**Open Question 2**: What are the trade-offs between model size and performance in KIE systems, and at what point do larger models provide diminishing returns?
- Basis in paper: The paper's quantitative analysis found no significant correlation between model size and performance across three major benchmarks.
- Why unresolved: The paper shows smaller models can outperform larger ones, but doesn't establish clear thresholds for when increasing model size becomes ineffective.
- What evidence would resolve it: Controlled experiments varying model sizes while keeping architecture design constant, identifying performance plateaus.

**Open Question 3**: How can domain knowledge be effectively integrated into KIE architectures beyond hand-crafted features to improve real-world applicability?
- Basis in paper: The paper identifies a severe lack of domain knowledge integration, with only a few approaches using hand-crafted features or rules.
- Why unresolved: The paper demonstrates limited practical perspectives in current KIE research, but doesn't provide solutions for systematic domain knowledge incorporation.
- What evidence would resolve it: Novel architectures that incorporate business process constraints or workflow context as architectural elements rather than post-processing steps.

## Limitations
- Systematic review methodology may have missed relevant papers using different terminology or appearing in non-indexed venues
- Analysis of 130 papers may not capture full diversity, particularly from industry or non-English publications
- Correlation analysis between model size and performance lacks statistical rigor and doesn't control for confounding variables

## Confidence
- High confidence in methodological categorization (sequence-based, graph-based, grid-based, LLM-based) as these are well-defined architectural paradigms with clear distinctions
- Medium confidence in feature integration analysis (text, layout, visual) due to potential underreporting of feature engineering details in published papers
- Low confidence in performance benchmarks and model size correlations due to inconsistent evaluation protocols across studies and lack of standardized testing conditions

## Next Checks
1. Conduct additional literature searches using alternative keywords and databases to identify potentially missed papers, particularly from industry conferences and regional venues
2. Perform statistical analysis controlling for document domain, pre-training data size, and evaluation dataset characteristics when examining model size-performance relationships
3. Design and execute real-world pilot studies to validate the identified research gaps, particularly focusing on practical deployment challenges and domain knowledge integration in production environments