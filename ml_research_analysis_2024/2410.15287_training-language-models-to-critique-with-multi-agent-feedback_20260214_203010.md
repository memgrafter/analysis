---
ver: rpa2
title: Training Language Models to Critique With Multi-agent Feedback
arxiv_id: '2410.15287'
source_url: https://arxiv.org/abs/2410.15287
tags:
- critique
- criteria
- critiques
- response
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiCritique, a novel data generation pipeline
  that improves the critique ability of large language models (LLMs) by utilizing
  multi-agent feedback in both supervised fine-tuning (SFT) and reinforcement learning
  (RL) stages. The method aggregates high-quality critiques from multiple LLMs while
  filtering out flawed critiques through meta-critique classification and revision
  quality scoring.
---

# Training Language Models to Critique With Multi-agent Feedback

## Quick Facts
- **arXiv ID:** 2410.15287
- **Source URL:** https://arxiv.org/abs/2410.15287
- **Reference count:** 40
- **Primary result:** MultiCritique framework significantly improves LLM critique ability through multi-agent feedback

## Executive Summary
This paper introduces MultiCritique, a novel data generation pipeline that enhances the critique ability of large language models (LLMs) through multi-agent feedback mechanisms. The framework aggregates high-quality critiques from multiple LLMs while filtering out flawed critiques using meta-critique classification and revision quality scoring. By incorporating task descriptions, customized evaluation criteria, and reference responses as inputs, the approach simplifies the critique process while maintaining high quality. The method demonstrates substantial improvements in critique generation capabilities across two benchmark datasets.

## Method Summary
The MultiCritique framework employs a two-stage approach combining supervised fine-tuning (SFT) and reinforcement learning (RL) to improve LLM critique abilities. The pipeline generates critiques using multiple LLMs, then applies meta-critique classification to filter out flawed critiques and uses revision quality scoring to ensure high standards. Three key information types serve as inputs to simplify the critique process: task description, customized evaluation criteria, and reference response. The aggregated high-quality critiques are then used to fine-tune both the SFT and RL stages of the model training process.

## Key Results
- Fine-tuned 7B model significantly outperforms other advanced 7B-13B open-source models
- Approach achieves performance approaching that of advanced 70B LLMs and GPT-4
- Extensive experiments on two benchmarks (Eason and Self-Critique) validate the effectiveness

## Why This Works (Mechanism)
The multi-agent feedback mechanism works by leveraging diverse perspectives from multiple LLMs to generate more comprehensive and accurate critiques. By incorporating meta-critique classification, the system can identify and filter out critiques with logical flaws or inconsistencies that a single LLM might miss. The revision quality scoring ensures that only high-quality critiques are retained for training, creating a positive feedback loop that improves the model's critique capabilities over time. The use of task-specific inputs (description, criteria, reference) provides necessary context while maintaining focus and relevance.

## Foundational Learning
- **Meta-critique classification**: Understanding how to evaluate and filter critiques is essential for maintaining quality control in the feedback loop. Quick check: Can the classifier reliably distinguish between high-quality and flawed critiques across different domains?
- **Multi-agent aggregation**: Learning how multiple LLM perspectives can be combined to produce superior outputs. Quick check: Does the aggregation method effectively balance diverse viewpoints without introducing contradictions?
- **Reinforcement learning for critique generation**: Recognizing how RL can optimize critique generation based on feedback signals. Quick check: Does the RL stage show measurable improvement over pure SFT approaches?
- **Critique quality scoring**: Understanding metrics for evaluating critique effectiveness beyond simple accuracy measures. Quick check: Are the scoring mechanisms aligned with human judgment of critique quality?

## Architecture Onboarding
- **Component map**: Multi-agent LLM calls → Meta-critique classification → Revision quality scoring → Filtered critique aggregation → SFT/RL fine-tuning
- **Critical path**: The most time-sensitive sequence is the feedback loop where critiques are generated, classified, scored, and filtered before being used for training. Bottlenecks can occur at the meta-critique classification stage.
- **Design tradeoffs**: The framework trades computational efficiency for quality by using multiple agents instead of a single critic, but this increases resource requirements. The three-input simplification reduces complexity but may limit applicability to open-ended critique tasks.
- **Failure signatures**: Poor performance may manifest as inconsistent critique quality, over-reliance on reference responses, or failure to generalize beyond benchmark tasks. System failures could occur if meta-critique classification becomes overwhelmed or revision scoring becomes biased.
- **First experiments**: 1) Run ablation study removing meta-critique classification to measure its impact. 2) Test with single agent versus multi-agent setup to quantify improvement. 3) Evaluate on out-of-domain tasks to assess generalization.

## Open Questions the Paper Calls Out
None

## Limitations
- Framework effectiveness limited to specific benchmark datasets and may not generalize to diverse critique tasks
- Multi-agent pipeline significantly increases computational overhead compared to single-agent approaches
- Quality control mechanisms themselves may introduce biases or errors that propagate through training

## Confidence
- **High Confidence**: Multi-agent feedback methodology is technically sound and shows consistent improvements on tested benchmarks
- **Medium Confidence**: Claims of approaching 70B LLM performance are supported but require independent verification
- **Low Confidence**: Real-world application effectiveness beyond curated benchmarks has not been demonstrated

## Next Checks
1. Conduct ablation studies to isolate contributions of meta-critique classification, revision scoring, and multi-agent aggregation
2. Test framework on out-of-domain critique tasks to assess generalization capabilities
3. Evaluate computational efficiency and cost-effectiveness compared to single-agent approaches