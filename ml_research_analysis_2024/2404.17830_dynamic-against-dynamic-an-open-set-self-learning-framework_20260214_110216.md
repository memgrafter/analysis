---
ver: rpa2
title: 'Dynamic Against Dynamic: An Open-set Self-learning Framework'
arxiv_id: '2404.17830'
source_url: https://arxiv.org/abs/2404.17830
tags:
- classes
- samples
- data
- ossl
- dynamic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of open-set recognition (OSR)
  in dynamic and open scenarios, where unknown classes can appear at any position
  in the feature space. The proposed method, open-set self-learning (OSSL), follows
  a dynamic against dynamic idea, i.e., dynamic methods against dynamic changing open-set
  world.
---

# Dynamic Against Dynamic: An Open-set Self-learning Framework

## Quick Facts
- **arXiv ID**: 2404.17830
- **Source URL**: https://arxiv.org/abs/2404.17830
- **Reference count**: 12
- **Key outcome**: Proposes OSSL framework achieving state-of-the-art performance on standard and cross-dataset benchmarks for open-set recognition

## Executive Summary
This paper addresses the challenge of open-set recognition (OSR) in dynamic scenarios where unknown classes can appear anywhere in feature space. The proposed Open-Set Self-Learning (OSSL) framework adopts a "dynamic against dynamic" approach, starting with a closed-set classifier and adapting during testing using available test samples. The method introduces a novel self-matching module that automatically identifies known class samples while rejecting unknown ones, which are then utilized to enhance model discriminability. OSSL achieves new performance milestones on standard benchmarks (MNIST, SVHN, CIFAR-10) and cross-dataset evaluations, outperforming existing methods by significant margins.

## Method Summary
OSSL begins with a well-trained closed-set classifier and iteratively refines it during testing. The framework partitions test samples into three sets based on logit scores: known-label (T1), uncertainty (T2), and unknown-label (T3). A self-matching module updates the classifier through adversarial matching and detection components. The method uses a weighted mechanism where known-label samples directly update the classifier, while uncertainty samples participate in adversarial matching to detect and reject unknown samples. Enhancement strategies include injecting small amounts of ground-truth data and using marginal logit loss for unknown classes to improve reliability and performance.

## Key Results
- Achieves state-of-the-art performance on standard OSR benchmarks (MNIST, SVHN, CIFAR-10)
- Demonstrates significant improvements on cross-dataset evaluations (CIFAR-10 with ImageNet/LSUN as OOD)
- Shows effectiveness of the "dynamic against dynamic" adaptation approach
- Outperforms existing methods by a significant margin across multiple evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
The dynamic against dynamic idea enables the model to adapt decision boundaries in response to changing test distributions, overcoming static limitations of existing OSR methods. The framework iteratively refines the classifier during testing by dividing samples into known, uncertainty, and unknown sets based on logit scores, then uses a self-matching module with adversarial matching and detection to enhance adaptability.

### Mechanism 2
The self-matching module effectively utilizes rejected unknown samples as instantiated representations of unknown classes to enhance model discriminability. Known-label samples directly update the classifier while uncertainty samples participate in adversarial matching to detect and reject unknown samples, which are then leveraged to refine the model's ability to reject them in the future.

### Mechanism 3
Injecting ground-truth data and using marginal logit loss for unknown classes further improve framework reliability and performance. Small amounts of labeled data from the training set stabilize self-training, while marginal logit loss encourages unknown class logits to approach uniform distribution with small values, improving rejection capabilities.

## Foundational Learning

- **Concept**: Open-set recognition (OSR) - Why needed: Core problem being addressed; essential to understand framework's purpose. Quick check: What is the key difference between open-set and closed-set recognition?
- **Concept**: Self-training in semi-supervised learning - Why needed: Framework uses self-training to adapt model during testing using unlabeled data. Quick check: How does self-training differ from supervised learning?
- **Concept**: Adversarial learning - Why needed: Self-matching module uses adversarial matching to align known-class distributions and detect unknown samples. Quick check: What is the role of the discriminator in adversarial learning?

## Architecture Onboarding

- **Component map**: Well-trained closed-set classifier -> Self-matching module (adversarial matching, detection, classifier parts) -> Enhanced classifier
- **Critical path**: Initial classifier inference → test set partitioning → self-matching module updates → convergence
- **Design tradeoffs**: Balancing leveraging test data for adaptation vs. avoiding overfitting or drift from initial classifier knowledge
- **Failure signatures**: Poor initial classifier performance, mispartitioning of test samples, ineffective adversarial matching leading to model degradation
- **First 3 experiments**:
  1. Evaluate initial classifier performance on held-out validation set to ensure reliability
  2. Test self-matching module's partitioning and updating ability on synthetic dataset with known unknown classes
  3. Assess impact of ground-truth data injection and marginal logit loss on benchmark dataset like CIFAR-10

## Open Questions the Paper Calls Out

### Open Question 1
How does OSSL performance compare when using different types of initial classifiers as starting points? The paper mentions all existing OSR approaches can serve as initial classifiers but only compares using ARPL. Experiments comparing OSSL's performance using various initial classifiers would provide insights into the impact of the initial classifier on OSSL's performance.

### Open Question 2
How does the proposed marginal logit loss for unknown classes affect the model's ability to generalize to unseen unknown classes, and are there alternative loss functions that could potentially improve performance? The paper introduces marginal logit loss but does not explore alternative loss functions or their impact on generalization to unseen unknown classes.

### Open Question 3
How does the proposed self-matching module handle the trade-off between leveraging the uncertainty set and avoiding the introduction of noise from potential unknown class samples in the uncertainty set? The paper introduces a self-matching module that utilizes the uncertainty set but does not explicitly discuss how it handles potential noise from unknown class samples.

## Limitations

- The framework critically depends on the quality of the initial closed-set classifier, with no thorough examination of performance when the initial classifier is poorly trained
- The effectiveness of the self-matching module relies heavily on accurate partitioning of test samples, but sensitivity to threshold parameters across different datasets is not deeply explored
- Enhancement strategies (ground-truth data injection and marginal logit loss) are introduced with limited ablation studies to demonstrate their individual contributions

## Confidence

- **High confidence**: Overall framework design and ability to achieve state-of-the-art results on benchmark datasets
- **Medium confidence**: Effectiveness of self-matching module's adversarial matching and detection components
- **Low confidence**: Robustness of framework when initial closed-set classifier is poorly trained or test data distribution shifts dramatically

## Next Checks

1. Conduct sensitivity analysis on threshold parameters (µ and γ) across different datasets to determine optimal settings and their impact on performance
2. Perform ablation studies to quantify individual contributions of ground-truth data injection and marginal logit loss to overall framework performance
3. Test framework's robustness by deliberately using suboptimal initial classifiers to evaluate failure conditions and identify breaking points