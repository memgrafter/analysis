---
ver: rpa2
title: What Representational Similarity Measures Imply about Decodable Information
arxiv_id: '2411.08197'
source_url: https://arxiv.org/abs/2411.08197
tags:
- distance
- decoding
- similarity
- neural
- matrices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a unifying theoretical framework that connects
  several existing methods of measuring representational similarity in terms of linear
  decoding statistics. The core method idea is to quantify similarity between neural
  representations X and Y by comparing the behavior of optimal linear decoders for
  each representation.
---

# What Representational Similarity Measures Imply about Decodable Information

## Quick Facts
- **arXiv ID**: 2411.08197
- **Source URL**: https://arxiv.org/abs/2411.08197
- **Reference count**: 40
- **Primary result**: Several representational similarity measures (CKA, CCA, GULP distance, ENSD) can be equivalently interpreted as expected alignment or distance between optimal linear decoders across a distribution of decoding tasks.

## Executive Summary
This paper provides a unifying theoretical framework connecting popular representational similarity measures to linear decoding statistics. The core insight is that measures like CKA and CCA quantify the average alignment between optimal linear readouts across a distribution of decoding tasks. The authors show that these measures, along with others like GULP distance and ENSD, can be interpreted as alignment scores between optimal linear decoders with particular weight regularizations. The work also establishes bounds relating Procrustes distance to average decoding distance, with the participation ratio playing a key role in determining when these bounds are tight.

## Method Summary
The method involves computing optimal linear decoder weights for neural representations X and Y by maximizing a decoding objective that includes weight regularization. For different choices of the regularization function G(X), this framework recovers various existing similarity measures. The authors derive that CKA corresponds to decoding with G(X) = bI, while CCA corresponds to G(X) = CX. They also establish bounds relating Procrustes distance between representations to the expected distance between their optimal linear readouts, with the participation ratio determining when these bounds are tight.

## Key Results
- Several distance measures (linear CKA, GULP distance, CCA, ENSD) can be equivalently interpreted as the expected alignment or distance between optimal linear readouts across a distribution of decoding tasks.
- The Procrustes distance between appropriately normalized representations provides upper and lower bounds on the average decoding distance, with the converse holding for representations with low participation ratio.
- The participation ratio, a measure of effective dimensionality, plays a key role in determining the relationship between Procrustes distance and average decoding distance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representational similarity measures like CKA and CCA can be interpreted as expected alignment between optimal linear decoders across a distribution of decoding tasks.
- Mechanism: The paper shows that these measures quantify the average inner product between optimal linear readout weights (w* and v*) when decoding from two neural representations X and Y. Specifically, CKA corresponds to decoding with weight regularization G(X) = bI, while CCA corresponds to G(X) = CX.
- Core assumption: The distribution of decoding targets z satisfies E[zz⊤] = I (uncorrelated with unit variance).
- Break condition: If the distribution of decoding targets does not satisfy E[zz⊤] = I, the equivalence between these measures and average decoding similarity breaks down.

### Mechanism 2
- Claim: The Procrustes distance between neural representations provides bounds on the average decoding distance, with the converse holding for representations with low participation ratio.
- Mechanism: The paper derives that the Procrustes distance between normalized representations (after applying G(·) transformations) upper bounds the average decoding distance, while the reverse inequality holds when the participation ratio of the kernel matrix difference is small.
- Core assumption: The representations are preprocessed to have zero mean and the Procrustes distance is computed on appropriately normalized representations.
- Break condition: When the participation ratio R∆ is large, the Procrustes distance can be large while the average decoding distance remains small, breaking the converse relationship.

### Mechanism 3
- Claim: Existing representational similarity measures can be unified under a framework of linear decoding with different weight regularizations.
- Mechanism: The paper shows that various measures (CKA, CCA, GULP distance, ENSD) correspond to specific choices of the weight regularization function G(X) in the linear decoding problem, providing a unified interpretation of seemingly disparate methods.
- Core assumption: The weight regularization function G(X) can be parameterized as G(X) = aCX + bI for non-negative a and b.
- Break condition: If weight regularization cannot be parameterized in the form G(X) = aCX + bI, or if the linear decoding framework itself is inadequate for the representation type, this unification breaks down.

## Foundational Learning

- Concept: Linear decoding and ridge regression
  - Why needed here: The paper's framework is built on linear decoding from neural representations, and understanding ridge regression helps grasp how different weight regularizations correspond to different similarity measures.
  - Quick check question: What is the closed-form solution for ridge regression weights w* when minimizing (1/M)∥z - Xw∥² + λ∥w∥²?

- Concept: Kernel methods and kernel matrices
  - Why needed here: The paper uses kernel matrices KX and KY to represent the neural representations in a way that's invariant to certain transformations, and these appear in the similarity/distance calculations.
  - Quick check question: How does the kernel matrix KX = (1/M)XG(X)⁻¹X⊤ change when X is transformed by an orthogonal matrix R?

- Concept: Participation ratio as a measure of effective dimensionality
  - Why needed here: The participation ratio R(CX) = (Tr[CX])²/Tr[CX²] appears in the bounds relating Procrustes distance to average decoding distance, and understanding it helps interpret when these bounds are tight.
  - Quick check question: What are the minimum and maximum possible values of the participation ratio for a positive semi-definite matrix, and when are these achieved?

## Architecture Onboarding

- Component map: X,Y → G(X),G(Y) → KX,KY → Similarity/Distance measure
- Critical path: X,Y → G(X),G(Y) → KX,KY → Similarity/Distance measure
  - Each step depends on the previous: The kernel matrices KX and KY are computed from the regularized representations, and these determine the final similarity/distance.
- Design tradeoffs:
  - Choice of G(·) function: Different choices (e.g., G(X)=bX, G(X)=CX, G(X)=CX+λI) lead to different similarity measures with different properties
  - Distribution of decoding targets: The assumption E[zz⊤]=I simplifies the framework but may not always hold
  - Sample size M: The framework assumes M is finite but large enough for stable estimates; the asymptotic behavior as M→∞ is discussed in the appendix
- Failure signatures:
  - If G(X) is ill-conditioned or singular, the inverse G(X)⁻¹ may not exist or be numerically unstable
  - If the representations X and Y have very different dimensionalities (NX≠NY), the Procrustes distance requires zero-padding, which may affect interpretation
  - If the participation ratio R∆ is very large, the Procrustes distance bounds become loose
- First 3 experiments:
  1. Verify the equivalence between CKA and average decoding similarity: Compute both measures on synthetic data with known properties and check they match
  2. Test the Procrustes distance bounds: Generate random positive semi-definite matrices with varying participation ratios, compute both Procrustes and average decoding distances, and check if they satisfy the derived bounds
  3. Explore the effect of different G(·) choices: Apply different weight regularizations to the same data and observe how the resulting similarity/distance measures change, particularly focusing on cases where CCA, CKA, and ENSD would give different results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the participation ratio influence the relationship between Procrustes distance and average decoding distance?
- Basis in paper: [explicit] The paper explicitly discusses how the participation ratio affects the bounds relating Procrustes distance and average decoding distance, particularly in Section 4.
- Why unresolved: While the paper establishes theoretical bounds and provides numerical examples, it doesn't fully explore the practical implications of varying participation ratios in real neural datasets.
- What evidence would resolve it: Empirical studies comparing Procrustes and average decoding distances across neural datasets with varying participation ratios would clarify this relationship.

### Open Question 2
- Question: How do the proposed similarity measures perform compared to existing methods in practical applications?
- Basis in paper: [inferred] The paper focuses on theoretical connections but doesn't extensively benchmark the proposed measures against existing methods on real-world neural datasets.
- Why unresolved: The paper establishes theoretical foundations but doesn't provide comprehensive empirical validation.
- What evidence would resolve it: Systematic comparisons of the proposed measures with established methods like CKA and Procrustes distance on diverse neural datasets would provide practical insights.

### Open Question 3
- Question: How does the choice of weight regularization function G(X) affect the interpretation of representational similarity?
- Basis in paper: [explicit] The paper discusses various choices of G(X) and their corresponding similarity measures (e.g., linear CKA, CCA, ENSD) but doesn't fully explore the implications of different choices.
- Why unresolved: While the paper presents different options for G(X), it doesn't provide guidance on selecting the most appropriate choice for different scenarios.
- What evidence would resolve it: Systematic analysis of how different G(X) choices affect similarity measures across various neural datasets and tasks would clarify this issue.

## Limitations
- The theoretical framework assumes linear decoding optimality and specific weight regularization forms (G(X) = aCX + bI).
- The equivalence between similarity measures and decoding statistics relies on the assumption that E[zz⊤] = I for the distribution of decoding targets.
- The Procrustes distance bounds depend on participation ratio, which may be difficult to estimate accurately in practice.

## Confidence
- High confidence: The core equivalence between CKA and average decoding similarity - supported by multiple theoretical derivations and direct correspondence to established definitions
- Medium confidence: The Procrustes distance bounds - while theoretically derived, the practical tightness of these bounds depends on participation ratio estimation
- Medium confidence: The unification of existing measures under linear decoding - well-supported for the measures explicitly discussed, but the parameterization may not capture all possible similarity measures

## Next Checks
1. Empirical verification of the Procrustes distance bounds across a range of participation ratios using both synthetic and real neural data
2. Systematic exploration of how different G(X) choices affect similarity/distance measures on benchmark datasets with known representational relationships
3. Extension of the framework to non-linear decoding scenarios and comparison with existing non-linear representational similarity measures