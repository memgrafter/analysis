---
ver: rpa2
title: 'Align Your Intents: Offline Imitation Learning via Optimal Transport'
arxiv_id: '2402.13037'
source_url: https://arxiv.org/abs/2402.13037
tags:
- learning
- expert
- ailot
- offline
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AILOT, a novel offline imitation learning
  method that aligns the intents of an agent with those of an expert using optimal
  transport in a distance-preserving latent space. The core idea is to represent states
  as intents that incorporate pairwise spatial distances within the data, and then
  define an intrinsic reward function via optimal transport distance between the expert's
  and the agent's trajectories in this latent space.
---

# Align Your Intents: Offline Imitation Learning via Optimal Transport

## Quick Facts
- **arXiv ID:** 2402.13037
- **Source URL:** https://arxiv.org/abs/2402.13037
- **Reference count:** 14
- **One-line result:** AILOT achieves state-of-the-art performance in 7 out of 9 MuJoCo locomotion tasks and significantly improves sparse-reward tasks like AntMaze-large-diverse.

## Executive Summary
AILOT introduces a novel offline imitation learning method that aligns the intents of an agent with those of an expert using optimal transport in a distance-preserving latent space. By representing states as intents that incorporate pairwise spatial distances and defining an intrinsic reward function via optimal transport distance between trajectories in this latent space, AILOT eliminates the need for explicit rewards or action labels. This approach makes it applicable to real-world scenarios where such information is often unavailable.

The method outperforms state-of-the-art offline imitation learning algorithms on D4RL benchmarks, achieving the best performance in 7 out of 9 MuJoCo locomotion tasks. It also improves the performance of other offline RL algorithms by providing dense reward relabeling in sparse-reward tasks like AntMaze and Adroit. For example, on the challenging AntMaze-large-diverse task, AILOT achieves a normalized score of 66.6, significantly outperforming the previous best of 56.6.

## Method Summary
AILOT is an offline imitation learning algorithm that uses optimal transport to align the intents of an agent with those of an expert. The method involves pretraining a latent representation of states using an Inverse Conditional Value Function (ICVF) objective to capture temporal structure. Optimal transport is then computed between the expert's and agent's trajectories in this intent space, with the resulting distance transformed into an intrinsic reward via exponential scaling. The agent is trained using IQL with these relabeled rewards, enabling learning from demonstrations without explicit rewards or action labels.

## Key Results
- AILOT achieves state-of-the-art performance in 7 out of 9 MuJoCo locomotion tasks on D4RL benchmarks
- On AntMaze-large-diverse, AILOT achieves a normalized score of 66.6, significantly outperforming the previous best of 56.6
- AILOT improves performance of other offline RL algorithms by providing dense reward relabeling in sparse-reward tasks

## Why This Works (Mechanism)

### Mechanism 1
The method pretrains a latent representation `ψ(s)` via an ICVF objective that explicitly minimizes the temporal distance between states within the same trajectory. This yields a mapping where Euclidean distance in latent space is proportional to time-steps in the original trajectory. OT then operates on these intents rather than raw states, yielding a geometrically meaningful alignment. The core assumption is that the pretraining loss produces an isometry or near-isometry between state space and intent space, preserving pairwise temporal structure.

### Mechanism 2
Optimal transport between intent pairs (not single states) captures the sequential structure of trajectories, enforcing that the agent not only reaches the same states but follows similar temporal dynamics as the expert. The cost matrix includes both `∥ψ(s_a_i) - ψ(s_e_j)∥²` and `∥ψ(s_a_{i+k}) - ψ(s_e_{j+k})∥²`, so OT matches pairs of states offset by `k` steps, effectively aligning whole trajectory segments rather than isolated points.

### Mechanism 3
The exponential scaling of the OT-derived cost into rewards ensures stable training by keeping rewards in a bounded range and emphasizing states that are close in intent space. Rewards are computed as `α * exp(-τ * total_cost)`, which transforms the raw Wasserstein distance into a smooth, bounded signal that rewards states aligned with the expert more strongly.

## Foundational Learning

- **Markov Decision Process (MDP)**
  - **Why needed here:** The whole problem setup is framed as learning a policy that maximizes return in an MDP, even though the reward is missing and must be inferred from demonstrations.
  - **Quick check question:** What tuple defines an MDP and what does each component represent?

- **Optimal Transport (OT)**
  - **Why needed here:** OT is used to compute a geometric distance between the expert's and agent's trajectory distributions, which is then transformed into an intrinsic reward.
  - **Quick check question:** How does the entropy-regularized OT objective (Sinkhorn) differ from the classic Wasserstein distance?

- **Temporal Difference Learning / Value Function Representation**
  - **Why needed here:** The ICVF pretraining learns a latent representation that encodes temporal structure; understanding this is key to grasping why intent distances correlate with behavioral similarity.
  - **Quick check question:** In the ICVF formulation, what does the triple `(ϕ, ψ, T)` represent, and how do they interact in the value function decomposition?

## Architecture Onboarding

- **Component map:** `ψ(s)` (intent encoder) → pretrain via ICVF → latent intent space → OT solver (Sinkhorn) → computes optimal coupling `P*` → Reward generator: `r(s) = α * exp(-τ * Σ P*_ij * C_ij)` → IQL agent → learns policy from relabeled transitions

- **Critical path:**
  1. Pretrain ICVF to obtain `ψ(s)`
  2. For each agent trajectory, compute OT against expert trajectories in intent space
  3. Generate intrinsic rewards and inject into dataset
  4. Train IQL with the relabeled rewards

- **Design tradeoffs:**
  - Pretraining ICVF is offline but adds upfront cost; skipping it reverts to OTR-style alignment in raw state space, which is less effective
  - Using intent pairs (`k>0`) improves temporal alignment but increases OT cost quadratically; `k=0` is faster but loses sequence structure
  - Exponential scaling smooths rewards but requires careful tuning of `α, τ`

- **Failure signatures:**
  - Poor pretraining → distances in intent space become random → OT alignment meaningless
  - Wrong `k` or no intent pair term → agent mimics static poses but not dynamics
  - Over-aggressive scaling (`τ` too high) → rewards collapse to near-zero → IQL sees no signal

- **First 3 experiments:**
  1. Run AILOT with `k=0` (no lookahead) on HalfCheetah-medium-v2; compare to `k=2` to see effect of temporal alignment
  2. Disable ICVF pretraining and use raw states in OT; measure performance drop vs. full AILOT
  3. Vary `τ` in `{0.1, 0.5, 1.0}` on AntMaze-large-diverse; plot reward histograms and learning curves to identify collapse vs. saturation

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out specific open questions, but several areas remain unexplored:

- How does AILOT's performance scale with the quality and diversity of the expert demonstrations provided?
- How does AILOT's approach of aligning intents in a distance-preserving latent space compare to other methods of learning intrinsic rewards in terms of sample efficiency and final performance?
- How does the choice of distance metric in the intent space (e.g., Euclidean vs. other metrics) affect AILOT's performance, and is there an optimal metric that generalizes across different environments?

## Limitations

- Performance may be sensitive to the quality of the pretraining phase and the choice of hyperparameters
- Computational overhead from pretraining and pairwise OT computation may limit scalability
- The method assumes that expert demonstrations are available and of sufficient quality to learn from

## Confidence

**Medium confidence** in the core claims of AILOT, with specific uncertainties around pretraining stability and hyperparameter sensitivity. The mechanism linking temporal pretraining to effective OT alignment is well-motivated but lacks direct empirical validation in the paper.

## Next Checks

1. **Pretraining ablation**: Disable ICVF pretraining and use raw states in OT on HalfCheetah-medium-v2; measure performance drop to isolate pretraining contribution.

2. **Hyperparameter sensitivity**: Sweep τ ∈ {0.1, 0.5, 1.0} on AntMaze-large-diverse; plot reward histograms and learning curves to identify collapse vs. saturation regimes.

3. **Temporal alignment test**: Compare k=0 vs k=2 on Walker2d-medium-v2; analyze whether the lookahead term actually improves temporal coherence or just adds computation.