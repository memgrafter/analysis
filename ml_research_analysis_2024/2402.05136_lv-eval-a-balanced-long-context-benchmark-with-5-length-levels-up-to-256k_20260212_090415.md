---
ver: rpa2
title: 'LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K'
arxiv_id: '2402.05136'
source_url: https://arxiv.org/abs/2402.05136
tags:
- context
- answer
- length
- confusing
- facts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LV-Eval, a benchmark designed to evaluate
  long-context understanding in large language models. Existing benchmarks are limited
  in context length (5k-21k words), lack effective metrics, and suffer from knowledge
  leakage.
---

# LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K

## Quick Facts
- **arXiv ID**: 2402.05136
- **Source URL**: https://arxiv.org/abs/2402.05136
- **Reference count**: 40
- **Primary result**: Introduces LV-Eval benchmark with 5 context levels up to 256K words, showing recent models like Qwen2.5-72B and Llama-3.1-70B achieve highest performance

## Executive Summary
LV-Eval is a new benchmark designed to evaluate long-context understanding in large language models across five length levels (16K, 32K, 64K, 128K, 256K words). Existing benchmarks are limited to 5K-21K words, lack effective metrics, and suffer from knowledge leakage issues. The benchmark uses 11 bilingual datasets with single-hop and multi-hop QA tasks, incorporating three key techniques: confusing facts insertion, keyword/phrase replacement, and keyword-recall-based metrics to prevent memorization and test genuine understanding.

The benchmark was evaluated on 15 models, revealing that recent large-scale open-source models (Qwen2.5-72B, Llama-3.1-70B) and Moonshot-v1 achieved the highest performance, especially at shorter lengths. Models exhibited varying degradation patterns, with some showing slower decline at longer lengths. The keyword-recall-based metric proved more objective than standard F1 scores by focusing on answer keywords while reducing the impact of non-informative words.

## Method Summary
The benchmark constructs five context length levels (16K, 32K, 64K, 128K, 256K words) using 11 bilingual datasets with extractive QA tasks. Each dataset includes supporting documents, distracting documents, confusing facts generated by GPT-4 and manually revised, and keyword/phrase replacements to mitigate knowledge leakage. The evaluation uses a keyword-recall-based metric that first identifies answer keywords, then calculates F1 score after filtering out blacklisted words. Models are evaluated across different context window sizes, with truncation and concatenation used for models with smaller context windows.

## Key Results
- Recent large-scale open-source models (Qwen2.5-72B, Llama-3.1-70B) and Moonshot-v1 achieved highest performance, especially at shorter lengths
- Models showed varying degradation patterns at longer lengths, with some exhibiting slower decline
- Confusing facts insertion and keyword replacement significantly impacted model performance, validating their effectiveness
- Keyword-recall-based metric proved more objective than standard F1 scores, mitigating knowledge leakage issues

## Why This Works (Mechanism)
The benchmark's effectiveness stems from three core mechanisms: (1) multi-level context design that progressively tests long-context understanding from 16K to 256K words, (2) confusing facts insertion that prevents models from relying on common-sense knowledge by introducing contradictory information, and (3) keyword-based evaluation that focuses on answer relevance rather than exact string matching, reducing the impact of knowledge leakage.

## Foundational Learning
- **Long-context evaluation**: Understanding model performance across varying document lengths (why needed: to test true long-context capabilities beyond short passages, quick check: verify performance degradation patterns across all 5 levels)
- **Knowledge leakage prevention**: Techniques to ensure models rely on provided context rather than pre-trained knowledge (why needed: to test genuine understanding vs. memorization, quick check: compare performance with and without confusing facts)
- **Keyword-based metrics**: Evaluation focusing on semantic relevance rather than exact matches (why needed: to reduce false positives from common phrases, quick check: validate keyword annotation consistency across datasets)

## Architecture Onboarding
- **Component map**: Datasets -> Context generation -> Model inference -> Keyword extraction -> Metric calculation
- **Critical path**: Context construction with confusing facts → Model inference with truncation for smaller context windows → Keyword-based answer evaluation
- **Design tradeoffs**: Balanced context levels vs. computational cost, keyword precision vs. recall, manual annotation quality vs. automation
- **Failure signatures**: High scores despite incorrect answers (knowledge leakage), inconsistent performance across similar-length contexts (data quality issues), metric scores not correlating with human judgment (keyword annotation problems)
- **First experiments**: 1) Test model performance at each context level individually, 2) Compare performance with confusing facts vs. without, 3) Evaluate different models with same context truncation strategy

## Open Questions the Paper Calls Out
None

## Limitations
- Manual annotation process for confusing facts and answer keywords introduces potential human bias and inconsistency
- Evaluation focuses on extractive QA tasks, may not capture full range of long-context capabilities needed for other tasks
- Findings based on specific 15 models, may not generalize to broader population of language models

## Confidence
- **High Confidence**: Technical implementation of benchmark construction methodology, experimental results showing performance differences across models and context lengths
- **Medium Confidence**: Claim that keyword-recall-based metric is more objective than standard F1 scores, observed model degradation patterns
- **Low Confidence**: Generalizability of findings to non-QA tasks and models not included in evaluation set

## Next Checks
1. Conduct inter-annotator agreement analysis on benchmark datasets to quantify consistency in confusing facts generation and answer keyword annotation
2. Evaluate same models on additional task types (e.g., summarization, code generation) using same context length framework
3. Compare performance across models with similar context windows but different architectural approaches to isolate effects of architecture design