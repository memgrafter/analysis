---
ver: rpa2
title: LLM-based Hierarchical Concept Decomposition for Interpretable Fine-Grained
  Image Classification
arxiv_id: '2405.18672'
source_url: https://arxiv.org/abs/2405.18672
tags:
- visual
- image
- concept
- each
- attribute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of achieving interpretable fine-grained
  image classification without sacrificing performance. The proposed method, Hi-CoDe,
  decomposes images into structured hierarchies of visual concepts using GPT-4, then
  employs an ensemble of simple linear classifiers operating on concept-specific features
  derived from CLIP for classification.
---

# LLM-based Hierarchical Concept Decomposition for Interpretable Fine-Grained Image Classification

## Quick Facts
- arXiv ID: 2405.18672
- Source URL: https://arxiv.org/abs/2405.18672
- Authors: Renyi Qu; Mark Yatskar
- Reference count: 29
- Achieves competitive performance on fine-grained image classification while providing interpretable concept hierarchies

## Executive Summary
This paper introduces Hi-CoDe, a method for interpretable fine-grained image classification that uses GPT-4 to decompose images into structured hierarchies of visual concepts, then employs an ensemble of linear classifiers operating on concept-specific features derived from CLIP. The approach addresses the challenge of achieving interpretability without sacrificing performance by creating consistent visual concept trees that replace unstructured LLM outputs. Experiments on six fine-grained image datasets demonstrate that Hi-CoDe achieves competitive accuracy compared to both interpretable vision-language models and traditional state-of-the-art image classification models, while providing clear insights into the decision-making process.

## Method Summary
Hi-CoDe uses GPT-4 to decompose object categories into hierarchical visual concept trees consisting of visual parts, attributes, and attribute values. Each image is encoded using CLIP to generate image embeddings, while concept descriptions are encoded to generate concept embeddings. The system computes similarity scores between the image and all concept embeddings, then segments these features by visual part. An ensemble of linear classifiers, one per visual part, makes predictions based on their respective concept-specific features. The final prediction is derived through collective voting among these classifiers, enabling direct inspection of which visual parts are most important for classification while maintaining competitive performance.

## Key Results
- Achieves competitive accuracy on six fine-grained image datasets compared to both interpretable vision-language models and traditional SOTA image classification models
- Provides clear insights into decision-making process through interpretable visual concept hierarchies
- Enables detailed analysis of potential failure modes through concept-level inspection
- Improves model compactness while maintaining interpretability

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical concept decomposition improves interpretability by creating structured, consistent visual concept trees that replace unstructured LLM outputs. GPT-4 decomposes images into visual parts, attributes, and attribute values organized in a hierarchy, with similarity scores propagating up the tree using "OR" logic for attributes and averaging for parts. This structured representation allows each visual part to be inspected independently.

### Mechanism 2
Ensemble of linear classifiers operating on concept-specific features enables direct inspection and debugging while maintaining competitive performance. Each visual part has its own linear classifier that makes predictions based only on features from that part, with the final prediction derived from collective voting. This allows inspection of which parts are most important for classification.

### Mechanism 3
Using CLIP similarity scores between images and pre-computed concept embeddings provides effective features for classification while maintaining interpretability. Instead of fine-tuning CLIP, the system computes similarity between the input image embedding and each pre-computed concept embedding. These similarity scores serve as features for the linear classifiers.

## Foundational Learning

- **Visual hierarchy decomposition**: Why needed - The method relies on breaking down images into structured hierarchies of visual parts and attributes to enable interpretable classification. Quick check - Can you explain how visual parts, attributes, and attribute values differ and how they relate in a hierarchical structure?

- **Ensemble learning and voting strategies**: Why needed - The classification uses an ensemble of linear classifiers with voting to combine predictions from different visual parts. Quick check - What are the advantages and disadvantages of majority voting versus probability-based voting in ensemble methods?

- **Vision-language models and feature extraction**: Why needed - The system uses CLIP to extract image and text embeddings and compute similarity scores for classification features. Quick check - How does CLIP's image-text contrastive learning enable zero-shot classification, and what are its limitations?

## Architecture Onboarding

- **Component map**: GPT-4 Concept Generator -> CLIP Encoders -> Concept Tree Builder -> Similarity Calculator -> Ensemble of Linear Classifiers -> Voting Mechanism

- **Critical path**:
  1. Decompose category into visual hierarchy using GPT-4
  2. Pre-compute concept embeddings for all (y,p,a,v) combinations
  3. For each image: extract image embedding, compute similarity to all concepts
  4. Segment similarity features by visual part
  5. Each linear classifier makes prediction on its part-specific features
  6. Combine predictions via voting

- **Design tradeoffs**:
  - GPT-4 vs manual annotation: Automation vs potential inconsistency
  - Depth of decomposition: More detailed decomposition vs computational cost and potential overfitting
  - Linear classifiers vs neural networks: Interpretability vs potential performance
  - Pre-computed vs on-the-fly concept generation: Efficiency vs flexibility

- **Failure signatures**:
  - Poor decomposition quality: Inconsistent or irrelevant visual parts across similar subclasses
  - Weak concept embeddings: Low similarity scores that don't distinguish between subclasses
  - Classifier imbalance: Some visual parts dominate predictions while others contribute little
  - Voting failure: Majority voting produces wrong predictions when probabilities are close

- **First 3 experiments**:
  1. Verify GPT-4 decomposition quality: Manually inspect concept trees for several categories to ensure visual parts and attributes are meaningful and consistent
  2. Test similarity effectiveness: Measure how well CLIP similarity scores distinguish between subclasses using a simple linear classifier
  3. Validate ensemble voting: Compare performance of individual classifiers vs ensemble voting to ensure the ensemble approach provides benefit

## Open Questions the Paper Calls Out

### Open Question 1
How does the depth of concept tree decomposition affect model performance and interpretability in fine-grained image classification? The paper provides initial insights into decomposition depth but doesn't fully explore the optimal depth or trade-offs between depth, performance, and interpretability. A comprehensive study comparing different levels of decomposition depth would help determine the optimal depth for various fine-grained image classification tasks.

### Open Question 2
How does the inclusion of labels in the text prompts for the CLIP text encoder impact the performance and interpretability of the concept classifier? While the paper shows that the ensemble of linear probes maintains stability regardless of prompt structure, it doesn't explore the underlying reasons for this stability or how different prompt structures might affect interpretability.

### Open Question 3
What is the most effective voting strategy for the ensemble of linear probes in the concept classifier? The paper identifies top-probability voting as most effective but doesn't explore why this strategy is superior or how it might be further optimized. A detailed investigation into the reasons behind the effectiveness of top-probability voting would help determine the most effective approach.

## Limitations

- GPT-4 Decomposition Consistency: No empirical validation of consistency across repeated decompositions of the same category
- Voting Strategy Uncertainty: The paper mentions multiple voting strategies but doesn't specify which was used or provide ablation studies
- Dataset Size Constraints: Pre-computing concept embeddings for all combinations could become computationally prohibitive for larger category spaces

## Confidence

**High Confidence**: The core mechanism of using GPT-4 for hierarchical concept decomposition and CLIP for feature extraction is well-founded and supported by existing work on vision-language models.

**Medium Confidence**: The claim of achieving "competitive performance" is supported by experiments on six datasets, but specific implementation details could significantly impact results.

**Low Confidence**: The assertion that the method provides "clear insights into the decision-making process" is largely qualitative without systematic user studies or quantitative interpretability metrics.

## Next Checks

1. **Decomposition Consistency Test**: Run GPT-4 on the same category 10+ times with identical prompts and measure variation in generated concept hierarchies using tree similarity metrics.

2. **Voting Strategy Ablation**: Implement and compare both majority voting and top-probability voting on a subset of the data to determine which provides better accuracy-interpretability tradeoffs.

3. **Concept Embedding Discriminability**: Measure how well CLIP similarity scores between concept embeddings can distinguish between subclasses in a binary classification setting, without the full ensemble model.