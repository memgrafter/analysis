---
ver: rpa2
title: Large Language Models can impersonate politicians and other public figures
arxiv_id: '2407.12855'
source_url: https://arxiv.org/abs/2407.12855
tags:
- responses
- actual
- speaker
- response
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A study investigated whether Large Language Models (LLMs) can convincingly
  impersonate public figures in political debates. Researchers used a representative
  sample of 948 UK citizens to evaluate actual versus AI-generated debate responses
  across authenticity, relevance, and coherence.
---

# Large Language Models can impersonate politicians and other public figures

## Quick Facts
- arXiv ID: 2407.12855
- Source URL: https://arxiv.org/abs/2407.12855
- Authors: Steffen Herbold; Alexander Trautsch; Zlata Kikteva; Annette Hautli-Janisz
- Reference count: 40
- Primary result: LLM-generated political debate responses judged as more authentic, coherent, and relevant than actual human responses

## Executive Summary
This study investigates whether Large Language Models can convincingly impersonate public figures in political debates. Using a representative sample of 948 UK citizens, researchers compared actual versus AI-generated debate responses across authenticity, relevance, and coherence. Results showed that LLM-generated responses were consistently rated higher than human responses on all three dimensions, with large effect sizes for coherence and relevance. The findings reveal that participants not only fail to detect AI impersonations but actually prefer the AI-generated content, while simultaneously supporting transparency and regulation of AI use in public debates.

## Method Summary
The researchers extracted 520 question/response pairs from BBC Question Time episodes (2020-2022) featuring 112 public figures across six categories. They used ChatGPT 4 Turbo with a structured prompt combining debate questions with speaker Wikipedia biographies to generate impersonated responses. A survey of 948 UK participants evaluated both actual and AI-generated responses on authenticity, coherence, and relevance using 5-point Likert scales. Statistical analysis employed non-parametric tests with Bonferroni correction for multiple comparisons. The study also conducted linguistic analysis comparing surface features between human and AI responses.

## Key Results
- LLM-generated responses rated as more authentic than human responses (medium effect size d=0.66)
- AI responses scored higher on coherence (large effect size d=1.25) and relevance (large effect size d=1.23)
- Linguistic analysis revealed higher lexical diversity (d=1.67) and nominalization rates (d=1.39) in AI-generated content
- Over 85% of participants supported mandatory disclosure of AI use in public debates

## Why This Works (Mechanism)

### Mechanism 1
LLMs can generate political debate responses that are perceived as more authentic than actual human responses. The LLM is conditioned with speaker biographies and debate questions, producing text with higher lexical diversity, nominalization rates, and topical coherence. These surface features increase perceived authenticity. Core assumption: Authenticity is judged primarily on textual coherence, lexical richness, and topical relevance rather than on speaker-specific linguistic idiosyncrasies.

### Mechanism 2
Participants underestimate AI capabilities in political debate impersonation. Survey participants rate AI-generated responses without knowing the source, leading to high authenticity scores. When source is revealed, opinions shift but not uniformly—some become more supportive, others demand regulation. Core assumption: Participants' prior beliefs about AI limits their ability to detect AI-generated content.

### Mechanism 3
Linguistic differences between AI and human responses do not reduce perceived authenticity. AI-generated responses avoid epistemic markers ("I think") and use more nominalizations, but these differences do not lower authenticity scores; in fact, they may enhance perceived coherence. Core assumption: Authenticity judgments are not sensitive to the absence of epistemic hedging or increased nominalization.

## Foundational Learning

- **Effect size interpretation (Cohen's d)**: Needed to quantify the magnitude of differences in authenticity, coherence, and relevance judgments. Quick check: If d=1.25 for coherence, what does this imply about the overlap between AI and human distributions?

- **Likert scale data handling**: Needed because survey responses use 5-point scales; proper statistical treatment requires non-parametric tests. Quick check: Why are Wilcoxon signed rank tests used instead of t-tests for this data?

- **Multiple testing correction (Bonferroni)**: Needed because 17 statistical tests are conducted; correction prevents false positives. Quick check: What is the corrected significance threshold if α=0.05 and 17 tests are run?

## Architecture Onboarding

- **Component map**: Data pipeline → LLM prompt → response generation → survey → statistical analysis → reporting. Key modules: corpus extraction, prompt templating, survey logic, analysis engine.

- **Critical path**: Generate responses → collect judgments → compute effect sizes → interpret linguistic features. Latency is low; accuracy depends on prompt quality and survey design.

- **Design tradeoffs**: Real-time generation vs. batch processing; trade-off between prompt detail (accuracy) and brevity (efficiency).

- **Failure signatures**: High inter-rater disagreement, linguistic features not correlating with human judgments, or LLM refusal to generate content.

- **First 3 experiments**:
  1. Run A/B test with/without speaker biography in prompt to measure impact on authenticity.
  2. Test effect of disclosing AI source before vs. after judgment on authenticity scores.
  3. Compare linguistic feature distributions for high vs. low authenticity-rated responses.

## Open Questions the Paper Calls Out

### Open Question 1
How does the linguistic structure of LLM-generated political content influence the perceived authenticity of public figures across different cultures and political systems? The paper shows that linguistic markers like lexical diversity and nominalization rates differ significantly between human and LLM-generated responses, yet authenticity ratings remain high despite these differences. This remains unresolved because the study focuses on UK citizens and BBC debate content, leaving open whether similar patterns hold in different cultural contexts or political systems where public trust and debate norms vary.

### Open Question 2
Can detection models reliably identify LLM-generated political impersonations at scale, and what are the limitations of current approaches? The paper mentions preliminary work showing 89% accuracy for automated detection, but acknowledges that more sophisticated approaches may fool detectors. This remains unresolved because the study provides limited detail on detection methodology and does not explore the arms race between generation and detection capabilities.

### Open Question 3
How do different prompt engineering strategies affect the authenticity and coherence of LLM-generated political content when impersonating specific public figures? The paper describes using a complex emulation protocol with system and user prompts, but does not explore how variations in this approach might influence outcomes. This remains unresolved because the study uses a single prompting strategy without testing alternative approaches that might yield different quality or authenticity levels.

## Limitations

- Study relies entirely on UK participants evaluating UK political figures, limiting generalizability to other cultural contexts
- Authenticity perception may be inflated by participants' unfamiliarity with specific speakers, as familiarity ratings were generally low
- Study measures perceived rather than actual authenticity - participants may be consistently fooled by surface-level linguistic features without detecting deeper inconsistencies

## Confidence

- **Comparative judgment results**: High confidence due to large sample size (n=948) and appropriate statistical methods
- **Underestimation of AI capabilities**: Medium confidence as this relies on self-reported survey data rather than objective detection tests
- **Linguistic mechanism**: Medium confidence as the study observes patterns but doesn't experimentally manipulate linguistic features to test causality

## Next Checks

1. **Cross-cultural replication**: Repeat the study with participants from different countries evaluating their own political figures to test generalizability of authenticity perception patterns.

2. **Familiarity manipulation experiment**: Conduct a controlled study where the same responses are evaluated by participants with varying levels of familiarity with the speakers to isolate the familiarity effect on authenticity judgments.

3. **Linguistic feature manipulation**: Systematically modify AI-generated responses to increase or decrease specific linguistic features (nominalizations, epistemic markers, discourse markers) and measure how these changes affect authenticity ratings to establish causal relationships.