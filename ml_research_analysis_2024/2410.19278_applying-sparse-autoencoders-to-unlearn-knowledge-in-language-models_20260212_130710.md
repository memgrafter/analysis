---
ver: rpa2
title: Applying sparse autoencoders to unlearn knowledge in language models
arxiv_id: '2410.19278'
source_url: https://arxiv.org/abs/2410.19278
tags:
- unlearning
- features
- feature
- questions
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using sparse autoencoders (SAEs) to remove
  knowledge from language models in an interpretable way. The authors apply SAEs to
  the gemma-2b-it and gemma-2-2b-it models using the biology subset of the Weapons
  of Mass Destruction Proxy dataset (WMDP-Bio).
---

# Applying sparse autoencoders to unlearn knowledge in language models

## Quick Facts
- arXiv ID: 2410.19278
- Source URL: https://arxiv.org/abs/2410.19278
- Authors: Eoin Farrell; Yeu-Tong Lau; Arthur Conmy
- Reference count: 17
- Key outcome: SAE-based unlearning works but faces scaling challenges and side-effect issues compared to existing fine-tuning methods

## Executive Summary
This paper explores using sparse autoencoders (SAEs) to remove knowledge from language models in an interpretable way. The authors apply SAEs to the gemma-2b-it and gemma-2-2b-it models using the biology subset of the Weapons of Mass Destruction Proxy dataset (WMDP-Bio). They demonstrate that individual interpretable biology-related SAE features can unlearn a subset of WMDP-Bio questions with minimal side-effects in other domains. The results suggest that negative scaling of feature activations is necessary while zero ablating features is ineffective. When intervening on multiple features simultaneously, they can unlearn multiple topics but with similar or larger unwanted side-effects compared to existing fine-tuning based techniques like Representation Misdirection for Unlearning (RMU).

## Method Summary
The method involves training or loading SAEs at intermediate layers of Gemma language models, identifying relevant features using sparsity or attribution methods, and intervening on model activations by clamping SAE feature activations to negative values when features activate. The authors test both single feature interventions and multiple feature interventions, measuring unlearning performance using WMDP-Bio accuracy, OpenWebText loss, and MMLU accuracy while comparing results to the RMU technique.

## Key Results
- Negative scaling of feature activations is necessary for effective unlearning, while zero ablating features is ineffective
- Individual interpretable biology-related SAE features can unlearn WMDP-Bio questions with minimal side effects
- Intervening on multiple SAE features simultaneously can unlearn multiple topics but with similar or larger unwanted side-effects than existing RMU technique

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negative scaling of feature activations is necessary for effective unlearning, while zero ablating features is ineffective.
- Mechanism: Clamping feature activations to negative values modifies the model's internal representations in a way that reduces knowledge recall without simply masking the information.
- Core assumption: The SAE features encode knowledge in a linear and modifiable way, where negative values disrupt the original knowledge encoding without causing catastrophic model degradation.
- Evidence anchors:
  - [abstract] "Our results suggest that negative scaling of feature activations is necessary and that zero ablating features is ineffective."
  - [section 3.2] "We find that a clamped value of 0 (i.e. 'ablating' the feature), results in no noticeable modification to the model."
- Break condition: If the model's knowledge encoding is highly non-linear or distributed across many features, simple negative scaling may not be sufficient to unlearn specific knowledge.

### Mechanism 2
- Claim: Intervening on multiple SAE features simultaneously can unlearn multiple different topics.
- Mechanism: Each SAE feature encodes specific knowledge, so activating multiple features with negative values simultaneously disrupts multiple knowledge domains.
- Core assumption: The SAE features are largely independent and encode different knowledge domains.
- Evidence anchors:
  - [abstract] "We find that intervening using multiple SAE features simultaneously can unlearn multiple different topics, but with similar or larger unwanted side-effects than the existing Representation Misdirection for Unlearning technique."
- Break condition: If SAE features are highly correlated or encode overlapping knowledge, intervening on multiple features may not achieve targeted unlearning.

### Mechanism 3
- Claim: The specific decoder vectors in SAEs contribute significantly to the unlearning process.
- Mechanism: When SAE features activate, the decoder vectors project the activation into a high-dimensional space that overwrites the original residual stream information.
- Core assumption: The decoder vectors in SAEs serve as interpretable intervention directions.
- Evidence anchors:
  - [section 4.2] "Using the random decoder vectors, the unlearning performance dropped substantially as compared to the standard approach, with higher loss added."
- Break condition: If the unlearning effect is primarily due to the encoder-based feature detection rather than the decoder vectors, then the specific decoder directions may not matter as much.

## Foundational Learning

- Concept: Sparse autoencoders and their role in feature extraction from neural network activations
  - Why needed here: Understanding how SAEs work is crucial for comprehending the intervention mechanism
  - Quick check question: What is the primary purpose of the sparsity constraint in SAE training?

- Concept: Residual stream architecture in transformer models
  - Why needed here: The intervention happens in the residual stream, so understanding this architecture is essential
  - Quick check question: How do residual connections in transformers allow for interventions at intermediate layers?

- Concept: Multiple-choice question evaluation and the concept of permutations
  - Why needed here: The unlearning evaluation relies on multiple-choice questions and their permutations
  - Quick check question: Why is it important to test unlearning across all 24 permutations of a multiple-choice question?

## Architecture Onboarding

- Component map: Language model (gemma-2b-it or gemma-2-2b-it) → SAE encoder → Feature activation clamping → SAE decoder → Modified residual stream → Language model continuation

- Critical path:
  1. Train or load SAE at target layer
  2. Identify relevant features using sparsity or attribution methods
  3. For each token, compute feature activations
  4. Clamp activating features to negative values
  5. Pass modified activations through decoder
  6. Replace original residual stream with modified version
  7. Continue model processing

- Design tradeoffs:
  - Feature selection method: sparsity-based vs. attribution-based
  - Number of features to intervene on: fewer features have less side effects but may be less effective
  - Clamping value magnitude: larger values are more effective at unlearning but cause more side effects

- Failure signatures:
  - No change in WMDP-Bio accuracy despite feature intervention
  - Large increase in OpenWebText loss
  - MMLU accuracy drop similar to or larger than RMU

## Open Questions the Paper Calls Out
None

## Limitations
- SAE-based unlearning faces significant scaling challenges and remains less effective than existing fine-tuning methods
- Decoder vector importance varies between Gemma model variants, suggesting architecture-dependent mechanisms
- Multi-feature interventions produce unwanted side-effects comparable to or larger than RMU

## Confidence

| Claim | Confidence |
|-------|------------|
| Negative scaling is necessary while zero ablation is ineffective | High |
| Decoder vector contribution findings | Medium |
| Multi-feature intervention side-effect comparisons | Medium |

## Next Checks

1. **Decoder vector ablation test**: Systematically test random vs. learned decoder vectors across multiple Gemma model sizes to determine if contradictory results stem from model architecture differences

2. **Feature correlation analysis**: Measure pairwise correlations between SAE features used in multi-feature interventions to quantify the independence assumption and better predict side-effect scaling

3. **Scaling study**: Test SAE-based unlearning on larger models (e.g., Gemma-7B, Gemma-27B) to determine if decoder vector importance and side-effect patterns persist at scale