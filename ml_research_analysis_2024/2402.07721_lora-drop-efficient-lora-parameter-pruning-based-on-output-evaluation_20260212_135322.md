---
ver: rpa2
title: 'LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation'
arxiv_id: '2402.07721'
source_url: https://arxiv.org/abs/2402.07721
tags:
- lora
- importance
- layers
- lora-drop
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LoRA-drop, a method for efficient fine-tuning
  of large language models by pruning less important LoRA parameters based on their
  output. The authors observe that some LoRA layers have minimal impact on downstream
  tasks, as evidenced by the concentration of their output distributions.
---

# LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation

## Quick Facts
- arXiv ID: 2402.07721
- Source URL: https://arxiv.org/abs/2402.07721
- Authors: Hongyun Zhou; Xiangyu Lu; Wang Xu; Conghui Zhu; Tiejun Zhao; Muyun Yang
- Reference count: 14
- One-line primary result: LoRA-drop achieves comparable performance to full fine-tuning and standard LoRA while reducing trainable parameters by approximately 50% on average.

## Executive Summary
This paper proposes LoRA-drop, an efficient fine-tuning method that prunes less important LoRA parameters based on their output evaluation. The authors observe that some LoRA layers have minimal impact on downstream tasks, as evidenced by the concentration of their output distributions. LoRA-drop evaluates the importance of LoRA parameters by computing the squared norm of their outputs on a sampled subset of training data, then retains LoRA for important layers while sharing a single LoRA across the remaining layers. Experiments on NLU and NLG tasks demonstrate that LoRA-drop achieves comparable performance to full fine-tuning and standard LoRA while reducing trainable parameters by approximately 50% on average.

## Method Summary
LoRA-drop evaluates LoRA parameter importance by computing the squared norm of their outputs on a sampled subset of training data. It then retains LoRA for important layers and shares a single LoRA across the remaining layers. The method uses stratified sampling (default 10%) to evaluate importance, retaining layers until cumulative importance reaches threshold T (0.9 default). This approach significantly reduces the number of trainable parameters while maintaining comparable performance to standard LoRA and full fine-tuning.

## Key Results
- Achieves comparable performance to full fine-tuning and standard LoRA on NLU tasks (CoLA, SST-2, MRPC, QQP, STS-B, MNLI, QNLI, RTE)
- Reduces trainable parameters by approximately 50% on average across NLU and NLG tasks
- Demonstrates effectiveness on both RoBERTa-base/large and Llama2-7b models
- Maintains performance on NLG tasks including E2E, DART, DialogSum, and GSM8K

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA parameters with larger output norms contribute more to model adaptation for specific tasks.
- Mechanism: The magnitude of the LoRA output (ΔW x x) determines how much the frozen pre-trained model is adjusted during fine-tuning. Larger outputs indicate stronger influence on the hidden state transformations.
- Core assumption: The squared norm of ΔW x x correlates with the importance of the corresponding LoRA layer for the task.
- Evidence anchors:
  - [abstract]: "Preliminary experiments indicate that a fraction of LoRA elements possesses significantly high output values, substantially influencing the layer output."
  - [section]: "Obviously, the ΔW x x is the factor that directly influences the frozen pre-trained model. The larger ΔW x x, the greater the impact of LoRA on the pre-trained model, and consequently, the more important LoRA is."
  - [corpus]: Weak evidence - no direct citations to output-norm based pruning methods.
- Break condition: If the correlation between output norm and task importance is not stable across different tasks or model scales.

### Mechanism 2
- Claim: Sharing LoRA parameters across layers with low importance maintains performance while reducing trainable parameters.
- Mechanism: Instead of removing unimportant LoRA layers entirely, LoRA-drop replaces them with a shared parameter matrix. This preserves some adaptability while drastically cutting trainable parameters.
- Core assumption: A single shared LoRA matrix can provide sufficient adaptation capability for multiple layers that individually have minimal task-specific impact.
- Evidence anchors:
  - [abstract]: "we retain LoRA for important layers and the other layers share the same LoRA."
  - [section]: "The LoRA of these selected layers will be retained during training, while a shared LoRA parameter will replace the LoRA of the other layers."
  - [corpus]: Weak evidence - no direct citations to shared LoRA parameter strategies.
- Break condition: If the shared LoRA cannot adequately represent the diverse needs of different layers in complex tasks.

### Mechanism 3
- Claim: Sampling a subset of training data provides sufficient information to estimate layer importance without full fine-tuning.
- Mechanism: LoRA-drop uses stratified sampling (default 10%) to create a smaller dataset for importance evaluation, reducing computational cost of the pruning step.
- Core assumption: The relative importance distribution across layers is stable and can be estimated from a representative subset of data.
- Evidence anchors:
  - [section]: "We find that sampling a small subset from the training data is able to obtain a LoRA importance distribution similar to that of the full dataset."
  - [section]: "As the training data increases, the importance order of each layer remains consistent."
  - [corpus]: Weak evidence - no direct citations to importance evaluation via data sampling.
- Break condition: If the importance distribution varies significantly with different data subsets or if rare but important patterns exist only in the full dataset.

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: Understanding LoRA's core mechanism of introducing low-rank matrices to approximate weight updates is essential for grasping how LoRA-drop prunes and shares parameters.
  - Quick check question: How does LoRA represent weight updates differently from full fine-tuning, and what computational advantage does this provide?

- Concept: Parameter importance evaluation
  - Why needed here: LoRA-drop's core innovation is evaluating importance based on output rather than parameter properties, requiring understanding of different importance metrics.
  - Quick check question: What are the differences between parameter-based and output-based importance evaluation methods?

- Concept: Matrix norms and their interpretation
  - Why needed here: The squared norm of the LoRA output (||ΔW x x||²) is the primary metric for importance, requiring understanding of how matrix norms relate to transformation strength.
  - Quick check question: How does the squared norm of a matrix-vector product relate to the magnitude of the transformation applied to the vector?

## Architecture Onboarding

- Component map:
  Input data → Sampling module (α proportion) → LoRA update step → Output norm calculation (||ΔW x x||²) → Importance normalization → Layer sorting → Threshold-based selection → Shared LoRA assignment → Standard LoRA fine-tuning
  Key components: Data sampler, LoRA updater, Norm calculator, Importance evaluator, Layer selector, Parameter sharer

- Critical path:
  1. Sample training subset
  2. Perform limited LoRA updates on sampled data
  3. Compute squared norms of LoRA outputs for each layer
  4. Normalize and sort layer importances
  5. Select top layers based on threshold T
  6. Share LoRA parameters across remaining layers
  7. Fine-tune with reduced parameter set

- Design tradeoffs:
  - Sampling ratio (α) vs. importance estimation accuracy
  - Threshold T vs. parameter reduction vs. performance
  - Norm calculation method vs. computational cost
  - Layer granularity vs. flexibility of sharing

- Failure signatures:
  - Performance degradation when T is too low (over-pruning)
  - Minimal parameter reduction when T is too high (under-pruning)
  - Unstable importance rankings across different data subsets
  - Sensitivity to initialization of shared LoRA parameters

- First 3 experiments:
  1. Verify output norm correlation: Fine-tune standard LoRA, compute layer-wise ||ΔW x x||² norms, measure correlation with final task performance when selectively removing layers.
  2. Test sampling stability: Apply different sampling ratios (5%, 10%, 20%, 50%) on same task, compare importance distributions and final performance.
  3. Threshold sensitivity analysis: Systematically vary threshold T from 0.5 to 1.0, measure parameter reduction and performance impact to identify optimal tradeoff point.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Limited testing on larger models beyond 1.5B parameters
- No exploration of how LoRA-drop performs on different LoRA ranks
- Uncertainty about stability of importance rankings across diverse task families

## Confidence
**High Confidence**: The core observation that some LoRA layers have minimal impact on downstream tasks, as evidenced by concentrated output distributions. The mechanism of using squared norm of LoRA outputs as an importance metric is theoretically sound and aligns with established principles of parameter importance evaluation.

**Medium Confidence**: The effectiveness of sharing a single LoRA across multiple layers while maintaining comparable performance to standard LoRA. While experimental results support this claim, the generalizability across diverse tasks and model architectures requires further validation.

**Low Confidence**: The stability of importance rankings when applied to entirely different task families or when using significantly different model scales (e.g., beyond 1.5B parameters tested). The paper's experiments focus on specific NLU and NLG tasks with limited model variety.

## Next Checks
1. **Cross-task importance stability**: Apply LoRA-drop to at least 5 additional task types (e.g., code generation, mathematical reasoning, summarization with different styles) and measure correlation between importance rankings across tasks.

2. **Ablation study on layer removal**: Systematically remove LoRA layers in order of importance (from least to most important) and measure performance degradation to validate whether the squared norm metric accurately predicts true layer importance.

3. **Threshold sensitivity analysis with task diversity**: Vary the threshold T from 0.5 to 1.0 across a diverse set of tasks and model scales, measuring the tradeoff between parameter reduction and performance to identify whether the fixed T=0.9 is universally optimal.