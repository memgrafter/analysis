---
ver: rpa2
title: Scene Graph Generation Strategy with Co-occurrence Knowledge and Learnable
  Term Frequency
arxiv_id: '2405.12648'
source_url: https://arxiv.org/abs/2405.12648
tags:
- graph
- cook
- scene
- generation
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CooK + TF-l-IDF, a novel scene graph generation
  (SGG) strategy that addresses two key limitations in existing SGG models: the lack
  of object co-occurrence knowledge and the long-tail problem. The proposed method
  integrates a CooK module that leverages co-occurrence probabilities between objects
  to refine message passing in graph neural networks, and a learnable TF-l-IDF layer
  that adjusts node features to mitigate the dominance of head classes.'
---

# Scene Graph Generation Strategy with Co-occurrence Knowledge and Learnable Term Frequency

## Quick Facts
- arXiv ID: 2405.12648
- Source URL: https://arxiv.org/abs/2405.12648
- Authors: Hyeongjin Kim; Sangwon Kim; Dasom Ahn; Jong Taek Lee; Byoung Chul Ko
- Reference count: 40
- One-line primary result: CooK + TF-l-IDF achieves up to 3.8% improvement in SGGen tasks on Visual Genome dataset

## Executive Summary
This paper introduces CooK + TF-l-IDF, a novel scene graph generation strategy that addresses two key limitations in existing SGG models: the lack of object co-occurrence knowledge and the long-tail problem. The proposed method integrates a CooK module that leverages co-occurrence probabilities between objects to refine message passing in graph neural networks, and a learnable TF-l-IDF layer that adjusts node features to mitigate the dominance of head classes. Evaluated on the Visual Genome and Open Images datasets, CooK + TF-l-IDF achieves state-of-the-art performance with up to 3.8% improvement in SGGen tasks and consistent performance gains across multiple MPNN-based models.

## Method Summary
The proposed CooK + TF-l-IDF method combines two complementary approaches for scene graph generation. First, the CooK (Co-occurrence Knowledge) module computes object co-occurrence probabilities from the training dataset and uses these probabilities to weight message passing in the MPNN, allowing the model to leverage prior knowledge about which objects frequently appear together. Second, the TF-l-IDF layer applies term frequency-inverse document frequency weighting to node features, with learnable parameters that adjust for the long-tail distribution of object classes. The method is integrated with existing MPNN-based SGG models and evaluated using standard metrics including Recall@K and mean Recall@K.

## Key Results
- CooK + TF-l-IDF achieves up to 3.8% improvement in SGGen tasks compared to baseline models
- Consistent performance gains across multiple MPNN-based models on Visual Genome dataset
- Strong long-tail mitigation with improved recall for body and tail classes
- Advanced CooK combining Visual Genome and Open Images datasets shows further improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CooK module improves message passing in MPNNs by incorporating object co-occurrence probabilities
- Mechanism: During MPNN feature update, each node's message is weighted by the probability that its neighboring node appears in the same image, reflecting prior knowledge of object co-occurrence
- Core assumption: The co-occurrence matrix accurately captures semantic relationships between objects across the training dataset
- Evidence anchors: [abstract] "leverages co-occurrence probabilities between objects to refine message passing in graph neural networks"; [section 3.2] "CooK was extracted from the training dataset... The CooK probability of the object classes oci and object class ocj occurring simultaneously in all images can be calculated"
- Break condition: If the co-occurrence matrix is sparse or noisy, the weighting could mislead the model by reinforcing spurious correlations

### Mechanism 2
- Claim: TF-l-IDF layer mitigates the long-tail problem by adjusting node features based on class frequency
- Mechanism: Node features are scaled by a term frequency factor (normalized frequency in the current image) and an inverse document frequency factor (inverse frequency across the dataset), with learnable parameters to balance oversampling effects
- Core assumption: Head class dominance in the training set leads to biased feature representations that can be corrected by frequency-aware scaling
- Evidence anchors: [abstract] "a learnable TF-l-IDF layer that adjusts node features to mitigate the dominance of head classes"; [section 3.3] "The TF-l-IDF layer can be expressed as the product of two terms... T F(X l|ncb, nb) · l-IDF (X l|B, nzi ; ϵ, γ)"
- Break condition: If learnable parameters ϵ and γ are poorly initialized or trained, the layer may overcorrect and harm head class performance

### Mechanism 3
- Claim: Combining CooK and TF-l-IDF yields cumulative improvements beyond either module alone
- Mechanism: CooK enhances the semantic richness of message passing, while TF-l-IDF rebalances the feature distribution; together they address both co-occurrence ignorance and class imbalance
- Core assumption: The two problems (lack of co-occurrence knowledge and long-tail bias) are largely independent and additive in their impact
- Evidence anchors: [abstract] "CooK + TF-l-IDF achieves state-of-the-art performance, with up to 3.8% improvement... consistent performance gains across multiple MPNN-based models"; [section 4.4] "The largest performance improvement was observed when using information for both CooK and TF-l-IDF... more accurate SGG is possible through these considerations"
- Break condition: If the model architecture already incorporates implicit co-occurrence modeling, adding CooK may have diminishing returns

## Foundational Learning

- Concept: Graph Neural Networks and message passing
  - Why needed here: SGG models rely on MPNNs to propagate contextual information between detected objects; understanding node and edge feature updates is essential to implement CooK weighting
  - Quick check question: In an MPNN, how is a node's feature updated during message passing, and where can we inject the co-occurrence weight?

- Concept: Term Frequency-Inverse Document Frequency (TF-IDF)
  - Why needed here: TF-l-IDF layer borrows the TF-IDF formula to adjust node features based on class frequency; knowing the formula and its role in balancing importance is critical
  - Quick check question: How does the IDF term in TF-IDF downweight common terms, and how does the learnable parameter ϵ modify this effect?

- Concept: Long-tail distribution and class imbalance
  - Why needed here: The dataset's long-tail distribution causes head classes to dominate predictions; understanding sampling bias and mitigation strategies helps tune the TF-l-IDF layer
  - Quick check question: Why does oversampling tail classes during training introduce bias, and how do learnable parameters in TF-l-IDF counteract this?

## Architecture Onboarding

- Component map: Object detector (Faster R-CNN) -> Object features + bounding boxes -> CooK matrix (precomputed) -> MPNN with CooK weighting -> TF-l-IDF layer -> Scene graph predictor -> Relation classification
- Critical path: Object detection -> CooK-weighted MPNN -> TF-l-IDF scaling -> Relation prediction
- Design tradeoffs:
  - CooK matrix size grows with number of object classes; large vocabularies may require dimensionality reduction
  - TF-l-IDF learnable parameters add overhead but improve adaptation to training bias
  - Batch size strongly affects TF-l-IDF performance; small batches reduce effectiveness
- Failure signatures:
  - Degraded performance if CooK matrix contains many zeros (sparse co-occurrence)
  - Over-suppression of head classes if TF-l-IDF parameters are not properly regularized
  - Unstable training if batch size is too small for reliable frequency estimation
- First 3 experiments:
  1. Verify that CooK weights are correctly applied during MPNN message passing by inspecting intermediate node features
  2. Test TF-l-IDF with fixed (non-learnable) parameters to confirm frequency scaling effect before adding learnability
  3. Measure performance impact when disabling CooK vs. TF-l-IDF separately to confirm additive benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CooK + TF-l-IDF change when applied to non-MPNN-based scene graph generation models?
- Basis in paper: [inferred] The paper acknowledges that the proposed method is currently limited to MPNN-based models and suggests exploring its application to non-MPNN models as future work
- Why unresolved: The paper does not provide any experimental results or analysis on applying CooK + TF-l-IDF to non-MPNN models
- What evidence would resolve it: Conducting experiments to evaluate the performance of CooK + TF-l-IDF on non-MPNN-based scene graph generation models and comparing the results with existing methods

### Open Question 2
- Question: How does the choice of the mapping function M ap : OI → V G affect the performance of the advanced CooK?
- Basis in paper: [explicit] The paper mentions that the mapping function was hand-crafted when combining CooK from VG and OI datasets, but does not provide details on the specific function or its impact on performance
- Why unresolved: The paper does not analyze the sensitivity of the advanced CooK's performance to different mapping functions
- What evidence would resolve it: Experimenting with different mapping functions and evaluating their impact on the performance of the advanced CooK

### Open Question 3
- Question: How does the proposed method perform on scene graph generation tasks with a larger number of object and relation classes?
- Basis in paper: [inferred] The paper evaluates the proposed method on VG and OI datasets, which have 150 and 301 object classes, respectively. However, it does not explore the scalability of the method to datasets with a significantly larger number of classes
- Why unresolved: The paper does not provide any analysis or experimental results on the performance of the proposed method when dealing with a larger number of object and relation classes
- What evidence would resolve it: Evaluating the proposed method on datasets with a larger number of object and relation classes and comparing its performance with existing methods

## Limitations
- The paper lacks ablation studies proving the individual contributions of CooK and TF-l-IDF modules
- TF-l-IDF layer's effectiveness depends on batch size, which is not thoroughly explored
- Implementation details of the MPNN architecture beyond basic structure are not fully specified
- No cross-dataset generalization studies beyond the mentioned experiments

## Confidence

| Claim | Confidence |
|-------|------------|
| CooK mechanism improving message passing | Medium - Theoretical framework is clear, but empirical validation of the co-occurrence weighting effect is limited |
| TF-l-IDF mitigating long-tail bias | Medium - The mechanism is sound, but learnable parameters' optimization is not thoroughly explored |
| Cumulative improvement claim | Low - Performance gains are demonstrated, but causal attribution between modules is not proven through ablation |

## Next Checks
1. Implement ablation studies to measure performance impact when disabling CooK vs. TF-l-IDF separately across multiple random seeds
2. Test TF-l-IDF layer with fixed (non-learnable) parameters to isolate the frequency scaling effect before adding learnability
3. Evaluate model performance with varying batch sizes to determine the minimum batch size required for TF-l-IDF effectiveness