---
ver: rpa2
title: 'Learn and Don''t Forget: Adding a New Language to ASR Foundation Models'
arxiv_id: '2407.06800'
source_url: https://arxiv.org/abs/2407.06800
tags:
- language
- languages
- performance
- fine-tuning
- irish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of adding a new, typically
  low-resource, language to an ASR foundation model while maintaining performance
  on the original language set. The study compares three adaptation approaches: soft
  language code tuning, soft prompt tuning, and LoRA, along with Elastic Weight Consolidation
  (EWC) to mitigate catastrophic forgetting.'
---

# Learn and Don't Forget: Adding a New Language to ASR Foundation Models

## Quick Facts
- arXiv ID: 2407.06800
- Source URL: https://arxiv.org/abs/2407.06800
- Authors: Mengjie Qian, Siyuan Tang, Rao Ma, Kate M. Knill, Mark J. F. Gales
- Reference count: 0
- Primary result: EWC constrained fine-tuning preserves performance on existing languages while direct fine-tuning achieves best new language performance

## Executive Summary
This paper addresses the challenge of adding a new, typically low-resource, language to an ASR foundation model while maintaining performance on the original language set. The study compares three adaptation approaches: soft language code tuning, soft prompt tuning, and LoRA, along with Elastic Weight Consolidation (EWC) to mitigate catastrophic forgetting. Results show that direct fine-tuning yields the best performance for the new language but degrades existing language capabilities. EWC can address this issue for specific languages. If only adaptation parameters are used, the language capabilities are maintained but at the cost of performance in the new language.

## Method Summary
The paper evaluates four adaptation strategies for adding a new language to an ASR foundation model: direct fine-tuning of all parameters, LoRA-based parameter-efficient tuning, soft language code tuning (SLCT), and soft prompt tuning (SPT). EWC is applied as a regularization technique to mitigate catastrophic forgetting when using fine-tuning. The Fisher information matrix is computed to identify important parameters for existing languages, which are then constrained during adaptation. Experiments are conducted on 10 languages from Common Voice and Open ASR datasets, measuring character error rate (CER) and word error rate (WER) across adaptation scenarios.

## Key Results
- Direct fine-tuning achieved 56.5% absolute reduction in CER and 66.7% reduction in WER on Irish test set
- Parameter-efficient methods (LoRA, SLCT, SPT) maintain existing language performance but sacrifice new language performance
- EWC constrained fine-tuning preserved performance on Welsh and English while adapting to Irish
- Fisher overlap between languages indicates parameter importance for maintaining existing language performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning all model parameters achieves the best performance for the new language but degrades performance on existing languages due to catastrophic forgetting.
- Mechanism: Direct parameter updates overwrite knowledge specific to existing languages when training on new language data, leading to loss of previously learned language representations.
- Core assumption: The Fisher overlap between languages indicates parameter importance for maintaining existing language performance.
- Evidence anchors:
  - [abstract]: "direct fine-tuning yields the best performance for the new language but degrades existing language capabilities"
  - [section]: "fine-tuning a model on a specific target language can lead to a performance decline in other languages"
  - [corpus]: Weak evidence - corpus contains related papers but none directly test Fisher overlap as an analytic tool for forgetting in ASR

### Mechanism 2
- Claim: Parameter-efficient fine-tuning methods (LoRA, SLCT, SPT) maintain existing language performance by keeping original model parameters frozen.
- Mechanism: Only a small subset of parameters (adapter layers, soft prompts, or language embeddings) are updated, leaving the bulk of the model unchanged and preserving existing language knowledge.
- Core assumption: The original model parameters contain sufficient knowledge for existing languages, and updating only a small subset won't disrupt this knowledge.
- Evidence anchors:
  - [abstract]: "If only adaptation parameters are used, the language capabilities are maintained but at the cost of performance in the new language"
  - [section]: "LoRA, SLCT, and SPT all have the ability to maintain performance on prior domains or tasks as they do not alter the original model parameters"
  - [corpus]: Moderate evidence - papers like "Towards Rehearsal-Free Multilingual ASR: A LoRA-based Case Study on Whisper" support LoRA's effectiveness

### Mechanism 3
- Claim: Elastic Weight Consolidation (EWC) mitigates catastrophic forgetting by constraining updates to parameters important for existing languages based on Fisher information.
- Mechanism: EWC adds a regularization term to the loss function that penalizes changes to parameters with high Fisher information for existing tasks, effectively preserving crucial weights.
- Core assumption: Fisher information accurately identifies parameters critical for maintaining existing language performance.
- Evidence anchors:
  - [abstract]: "Elastic Weight Consolidation (EWC) offers an alternative compromise with the potential to maintain performance in specific target languages"
  - [section]: "EWC employs the observed Fisher information matrix F to identify important parameters relative to the previous tasks during the pre-training phase"
  - [corpus]: Strong evidence - EWC is well-established in continual learning literature, including the original paper "Overcoming catastrophic forgetting in neural networks"

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding why fine-tuning degrades existing language performance is crucial for choosing appropriate adaptation strategies
  - Quick check question: What happens to model parameters when fine-tuning on new data, and why does this affect previously learned tasks?

- Concept: Fisher information matrix
  - Why needed here: Fisher information identifies which parameters are most important for existing language performance, enabling targeted regularization
  - Quick check question: How does the Fisher information matrix quantify parameter importance, and why is this useful for preventing forgetting?

- Concept: Low-rank adaptation
  - Why needed here: LoRA enables efficient adaptation by decomposing parameter updates into low-rank matrices, reducing computational cost while maintaining performance
  - Quick check question: Why does constraining parameter updates to low-rank matrices help prevent catastrophic forgetting while enabling efficient adaptation?

## Architecture Onboarding

- Component map: Whisper model (encoder, decoder, language embeddings) → Adaptation layer (LoRA, SLCT, or SPT) → Output head (language-specific)
- Critical path: Input audio → Encoder → Decoder with adaptation → Language-specific output → Transcription
- Design tradeoffs:
  - Fine-tuning: Best new language performance, worst forgetting
  - LoRA: Good balance of performance and efficiency, moderate forgetting resistance
  - SLCT/SPT: Best forgetting resistance, worst new language performance
  - EWC: Can mitigate forgetting when applied to fine-tuning, requires Fisher computation
- Failure signatures:
  - Fine-tuning: Sharp drop in existing language WER/CER
  - LoRA: New language performance plateaus below fine-tuning levels
  - SLCT/SPT: New language performance remains near baseline
  - EWC: New language performance degrades if λ is too high
- First 3 experiments:
  1. Test zero-shot ASR on a new language to establish baseline performance
  2. Apply LoRA with rank 8 to the new language and measure performance trade-offs
  3. Implement EWC with λ values [1e-5, 1e-3, 1e-1] and evaluate forgetting mitigation on existing languages

## Open Questions the Paper Calls Out

None

## Limitations

- Evaluation is based on only 10 languages from Common Voice and Open ASR datasets, limiting generalization to hundreds of other languages
- Limited detail on EWC implementation choices, particularly the critical regularization coefficient λ
- Focus on CER and WER metrics without examining qualitative aspects or computational efficiency during inference

## Confidence

**High Confidence**:
- Fine-tuning degrades existing language performance while improving new language performance
- Parameter-efficient methods (LoRA, SLCT, SPT) maintain existing language performance by design

**Medium Confidence**:
- EWC effectively balances new language learning with forgetting prevention
- The relative performance ranking of adaptation methods is consistent across different language pairs

**Low Confidence**:
- Fisher overlap between languages is a reliable predictor of adaptation difficulty
- The computational efficiency claims for parameter-efficient methods translate to real-world deployment scenarios

## Next Checks

1. **Cross-Lingual Generalization Test**: Evaluate the adaptation methods on language pairs spanning different similarity levels (e.g., Romance languages vs. Turkic languages vs. Sino-Tibetan languages) to determine if the observed performance patterns generalize beyond the 10 languages tested.

2. **Fisher Information Analysis**: Compute and visualize the Fisher overlap between languages for the specific Whisper model architecture. Test whether high Fisher overlap correlates with increased forgetting when fine-tuning, and whether this correlation holds across different language pairs.

3. **EWC Hyperparameter Sweep**: Systematically evaluate EWC performance across a wide range of λ values (e.g., [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]) for each language pair to identify optimal settings and understand the sensitivity of forgetting mitigation to this critical hyperparameter.