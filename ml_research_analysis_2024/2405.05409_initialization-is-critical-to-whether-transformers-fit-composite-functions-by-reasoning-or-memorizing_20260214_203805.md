---
ver: rpa2
title: Initialization is Critical to Whether Transformers Fit Composite Functions
  by Reasoning or Memorizing
arxiv_id: '2405.05409'
source_url: https://arxiv.org/abs/2405.05409
tags:
- anchor
- initialization
- inferential
- solutions
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transformer models generalize to unseen
  compositional tasks using anchor functions. The authors discover that parameter
  initialization scale critically determines whether models learn inferential solutions
  (reasoning-based) or symmetric solutions (memory-based) for unseen compositional
  tasks.
---

# Initialization is Critical to Whether Transformers Fit Composite Functions by Reasoning or Memorizing

## Quick Facts
- **arXiv ID**: 2405.05409
- **Source URL**: https://arxiv.org/abs/2405.05409
- **Reference count**: 14
- **Primary result**: Initialization scale determines whether transformers learn compositional reasoning or memorization for unseen tasks

## Executive Summary
This paper investigates how transformer models generalize to unseen compositional tasks using anchor functions. The authors discover that parameter initialization scale critically determines whether models learn inferential solutions (reasoning-based) or symmetric solutions (memory-based) for unseen compositional tasks. With small initialization scales, models tend to learn inferential solutions that capture underlying compositional primitives, while larger initialization scales lead to symmetric solutions that simply memorize mappings. The authors analyze information flow and vector representations to reveal distinct mechanisms: inferential solutions learn individual mappings for single anchors and compose them, while symmetric solutions directly combine anchor information without understanding structure. They find that inferential solutions exhibit low complexity bias, with parameters condensing in few directions and embeddings showing clear ordinal structure. This low complexity bias enables learning of individual single-anchor mappings. The authors validate their conclusions on various datasets and propose initialization rate γ as a tunable hyperparameter for controlling reasoning versus memorizing ability in deep learning frameworks.

## Method Summary
The paper employs a controlled experimental setup using anchor functions to study how transformers generalize to compositional tasks. The authors systematically vary initialization scales while keeping other hyperparameters constant, then analyze the resulting solution pathways using information flow analysis and vector representation studies. They compare small initialization scales (leading to inferential solutions) against larger scales (leading to symmetric solutions) across multiple datasets. The analysis framework examines parameter complexity, embedding structures, and generalization patterns to distinguish between reasoning-based and memorization-based learning approaches. The methodology includes quantitative metrics for solution complexity and qualitative analysis of embedding ordinal structures.

## Key Results
- Small initialization scales lead to inferential solutions that learn individual mappings for single anchors and compose them
- Large initialization scales produce symmetric solutions that memorize mappings without understanding compositional structure
- Inferential solutions exhibit low complexity bias with parameters condensing in few directions and clear ordinal structure in embeddings
- Initialization rate γ emerges as a tunable hyperparameter for controlling reasoning versus memorizing ability

## Why This Works (Mechanism)
The paper establishes that initialization scale fundamentally shapes the solution pathway transformers take when learning compositional tasks. Small initialization creates a low-complexity bias that forces the model to learn compact representations and understand the compositional structure. This manifests as learning individual single-anchor mappings first, then composing them for unseen combinations. Large initialization allows more parameter freedom, enabling the model to find shortcut solutions that memorize direct mappings without understanding the underlying structure. The mechanism operates through how initialization influences the optimization landscape - small scales constrain the search space toward simpler, more compositional solutions, while large scales permit more complex, memorization-based pathways.

## Foundational Learning
- **Compositional generalization**: The ability to combine learned primitives to handle unseen combinations - needed to understand what constitutes successful generalization in this context; quick check: can the model handle anchor function combinations it hasn't seen during training?
- **Initialization effects in deep learning**: How parameter initialization influences optimization trajectories and solution quality - needed to grasp why scale matters; quick check: does changing initialization scale systematically change learning outcomes?
- **Low complexity bias**: The tendency of optimization under certain conditions to find simpler solutions - needed to understand why small initialization leads to better compositional reasoning; quick check: do parameter norms or embedding dimensionalities show compression in inferential solutions?
- **Information flow analysis**: Methods for tracking how information propagates through network layers - needed to distinguish between reasoning and memorization pathways; quick check: can we trace distinct information patterns between inferential and symmetric solutions?
- **Vector representation analysis**: Techniques for examining embedding structures and relationships - needed to identify ordinal structures and compositional understanding; quick check: do embeddings show clear ordinal or hierarchical structure in inferential solutions?
- **Memory versus reasoning trade-off**: The fundamental distinction between pattern memorization and structural understanding in learning systems - needed as the conceptual framework for interpreting results; quick check: can the model generalize to novel combinations or only reproduce memorized mappings?

## Architecture Onboarding

**Component Map**: Data → Transformer Model → Initialization Scale → Solution Pathway (Inferential vs Symmetric) → Generalization Performance

**Critical Path**: The critical path runs from initialization scale through solution pathway determination to final generalization capability. The initialization scale sets the optimization constraints that determine whether the model learns compositional primitives (inferential) or direct mappings (symmetric).

**Design Tradeoffs**: Small initialization sacrifices some fitting capacity for better generalization and compositional understanding, while large initialization allows better immediate fitting but poor generalization. The tradeoff is between memorization efficiency and reasoning capability.

**Failure Signatures**: Symmetric solutions fail on unseen compositional tasks despite good training performance. Models exhibit high parameter complexity, lack of ordinal structure in embeddings, and inability to decompose problems into compositional primitives.

**3 First Experiments**:
1. Train with small initialization scale and test on unseen compositional tasks to verify inferential solution behavior
2. Train with large initialization scale and compare generalization performance to small initialization case
3. Visualize embedding spaces and parameter norms to confirm low complexity bias in inferential solutions

## Open Questions the Paper Calls Out
None

## Limitations
- Mechanistic explanations rely heavily on correlation rather than establishing definitive causal mechanisms
- Analysis framework based on observed patterns rather than theoretical guarantees
- Generalization claims limited to specific anchor functions and may not extend to all compositional tasks

## Confidence

**Confidence Assessment**:
- Central claim about initialization scale determining reasoning vs memorization: **Medium confidence** - empirical observations are robust but causal mechanisms not fully established
- Low complexity bias enabling learning of individual mappings: **Medium confidence** - characterization based on observations but theoretical framework could be strengthened
- Initialization rate γ as tunable hyperparameter: **High confidence** - straightforward empirical finding with direct validation potential

## Next Checks
1. Conduct ablation studies with different initialization distributions (not just scale variations) to determine whether the reasoning versus memorization dichotomy is truly scale-dependent or distribution-dependent.

2. Test whether the initialization effects generalize beyond the specific anchor functions used, across diverse compositional tasks and different transformer architectures to validate the universality of the findings.

3. Implement interventional experiments that directly manipulate the complexity bias (rather than just observing it) to establish causal links between initialization, complexity bias, and solution pathways.