---
ver: rpa2
title: 'TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch'
arxiv_id: '2412.08237'
source_url: https://arxiv.org/abs/2412.08237
tags:
- data
- speech
- arxiv
- training
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses challenges in LLM-based TTS systems including
  high data acquisition costs due to complex processing pipelines (10-30% data retention),
  deployment complexity from separate LLM and flow models, and difficulties in achieving
  streaming inference. To tackle these issues, the authors propose a simplified TTS
  framework called TouchTTS that achieves over 50% data retention rate for the first
  time.
---

# TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch

## Quick Facts
- arXiv ID: 2412.08237
- Source URL: https://arxiv.org/abs/2412.08237
- Authors: Xingchen Song, Mengtao Xing, Changwei Ma, Shengqiang Li, Di Wu, Binbin Zhang, Fuping Pan, Dinghao Zhou, Yuekai Zhang, Shun Lei, Zhendong Peng, Zhiyong Wu
- Reference count: 40
- Key outcome: First TTS framework achieving 50% data retention rate while supporting unified streaming/non-streaming inference with under 200ms latency

## Executive Summary
TouchTTS addresses fundamental challenges in LLM-based TTS systems by simplifying the data pipeline and unifying the architecture. The framework achieves breakthrough data retention rates exceeding 50% by leveraging S3Tokenizer's noise robustness to eliminate complex preprocessing steps like denoising and speaker diarization. By replacing the traditional flow model U-Net with LLM architecture (Qwen2), TouchTTS enables unified streaming and non-streaming inference while significantly reducing deployment complexity and costs.

## Method Summary
The TouchTTS framework introduces three key innovations: First, it simplifies data processing by using S3Tokenizer's noise robustness to eliminate denoising, speaker diarization, and punctuation modules, achieving over 50% data retention for the first time. Second, it replaces the flow model's U-Net structure with LLM architecture (Qwen2) to enable unified streaming and non-streaming inference, reducing deployment costs. Third, it explores unified TTS/ASR training using the same data. The framework achieves strong performance metrics including 7.96% PER on test-hard, 1.12% PER on test-zh, and 4.81% PER on test-en with the TouchLLM-0.5B model, while maintaining streaming inference latency under 200ms.

## Key Results
- Achieves 7.96% PER on test-hard dataset
- Achieves 1.12% PER on test-zh dataset
- Achieves 4.81% PER on test-en dataset
- Supports streaming inference with latency under 200ms
- First framework to achieve over 50% data retention rate

## Why This Works (Mechanism)
The framework's success stems from eliminating unnecessary complexity in both data processing and model architecture. By leveraging S3Tokenizer's inherent noise robustness, TouchTTS removes multiple preprocessing modules that traditionally discard 10-30% of data. The architectural unification through LLM-based models eliminates the need for separate flow models, enabling both streaming and non-streaming inference from a single model while reducing deployment overhead.

## Foundational Learning

1. **S3Tokenizer Noise Robustness**
   - Why needed: Enables elimination of denoising and preprocessing modules that discard data
   - Quick check: Verify tokenizer maintains performance with various noise levels

2. **Flow Model U-Net vs LLM Architecture**
   - Why needed: U-Net requires separate streaming/non-streaming models, increasing complexity
   - Quick check: Compare inference latency and resource usage between architectures

3. **Unified ASR/TTS Training**
   - Why needed: Leverages shared data and potentially improves both tasks through joint optimization
   - Quick check: Measure interference between tasks during joint training

## Architecture Onboarding

**Component Map:**
Data Input -> S3Tokenizer -> LLM (Qwen2) -> Audio Output

**Critical Path:**
Text/Phoneme Input → S3Tokenizer Processing → LLM Inference → Audio Synthesis

**Design Tradeoffs:**
- Simplified pipeline sacrifices some noise filtering capability for higher data retention
- Unified architecture reduces deployment complexity but may limit specialized optimizations
- Joint ASR/TTS training could introduce task interference

**Failure Signatures:**
- Low data retention indicates tokenizer noise robustness limitations
- High latency suggests LLM inference bottlenecks
- Poor ASR performance indicates joint training interference

**First 3 Experiments to Run:**
1. Measure data retention rate comparison between TouchTTS and traditional preprocessing pipelines
2. Benchmark streaming vs non-streaming inference latency and quality
3. Evaluate task-specific performance when training ASR and TTS jointly vs separately

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations

- Data retention improvement claims lack context about previous systems' performance
- PER metrics are presented without clear baseline comparisons or established benchmarks
- Streaming latency definition and measurement conditions are not fully specified
- Unified training approach results are incomplete, raising questions about task interference

## Confidence

- **High Confidence**: Simplified data processing pipeline using S3Tokenizer noise robustness
- **Medium Confidence**: LLM architecture replacement for unified inference and reduced deployment costs
- **Low Confidence**: 50% data retention achievement and PER metrics without sufficient benchmarking context

## Next Checks

1. Conduct ablation studies comparing data retention rates between TouchTTS and previous TTS frameworks using identical data sources and preprocessing pipelines.

2. Benchmark TouchTTS against established ASR and TTS systems on standardized datasets (Librispeech, LibriTTS) to contextualize PER metrics and streaming latency claims.

3. Perform detailed analysis of the unified ASR/TTS training approach, including task interference measurements and separate performance evaluation of each task when trained together versus independently.