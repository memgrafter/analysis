---
ver: rpa2
title: Sample Complexity Reduction via Policy Difference Estimation in Tabular Reinforcement
  Learning
arxiv_id: '2406.06856'
source_url: https://arxiv.org/abs/2406.06856
tags:
- policy
- unif
- have
- lemma
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the pure exploration problem in contextual
  bandits and tabular reinforcement learning (RL): identifying an epsilon-optimal
  policy from a set of policies with high probability. The key question is whether
  it suffices to estimate only the differences in the behaviors of policies in RL,
  as has been shown to be sufficient in contextual bandits.'
---

# Sample Complexity Reduction via Policy Difference Estimation in Tabular Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.06856
- Source URL: https://arxiv.org/abs/2406.06856
- Reference count: 40
- This paper develops Perp algorithm for pure exploration in tabular RL, showing that estimating policy differences rather than full behaviors reduces sample complexity, but cannot match contextual bandit performance due to transition dynamics.

## Executive Summary
This paper addresses the pure exploration problem in tabular reinforcement learning: identifying an epsilon-optimal policy from a finite set with high probability while minimizing sample complexity. The authors investigate whether, as in contextual bandits, it suffices to estimate only the differences in policy behaviors rather than their full value functions. They provide a negative answer for tabular RL, demonstrating a fundamental separation from contextual bandits due to the necessity of learning transition dynamics. However, they show that estimating a single reference policy's behavior plus differences from this reference is sufficient. The Perp algorithm they develop achieves what they claim is the tightest known sample complexity bound for tabular RL, scaling as O(ρΠ) plus an additional term for reference policy learning.

## Method Summary
The Perp algorithm operates in epochs, iteratively eliminating suboptimal policies. In each epoch, it first selects a reference policy that minimizes the maximum complexity of estimating differences to other policies. It then collects data to learn the reference policy's state visitation distribution, followed by additional data collection to estimate the difference in behavior between each remaining policy and the reference. The algorithm uses an online experiment design subroutine (OptCov) to efficiently collect data along policy-discriminative directions. Unreachable states are pruned to reduce computational burden. This approach avoids the redundancy of estimating each policy's full behavior separately while accounting for the unavoidable cost of learning transition dynamics in RL.

## Key Results
- Proves a fundamental separation between contextual bandits and tabular RL: ρΠ alone is insufficient for RL due to transition dynamics
- Develops Perp algorithm achieving O(ρΠ) complexity plus additional term for reference policy learning
- Shows Perp obtains tightest known sample complexity bounds for pure exploration in tabular RL
- Demonstrates that estimating policy differences from a reference policy suffices for PAC identification in RL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Estimating the difference in behavior between a reference policy and any other policy directly reduces sample complexity compared to estimating each policy's value separately.
- Mechanism: The algorithm first learns the behavior of a single reference policy, then estimates the deviation of every other policy from this reference. This avoids redundant estimation of overlapping state-action visitations.
- Core assumption: The difference in behavior between policies is smaller in magnitude than the full visitation profiles, and can be estimated with fewer samples.
- Evidence anchors:
  - [abstract]: "if we can estimate the behavior of a single reference policy, it suffices to only estimate how any other policy deviates from this reference policy."
  - [section 4.2]: The reduced-variance difference estimator calculation shows that estimating differences avoids summing individual variances.
  - [corpus]: No direct evidence; assumed based on theoretical construction.
- Break condition: If the reference policy's behavior is difficult to learn (e.g., rare state visitation), the savings vanish.

### Mechanism 2
- Claim: In tabular RL, learning the transition probabilities is necessary beyond estimating reward differences, unlike in contextual bandits.
- Mechanism: The algorithm explicitly collects data to estimate the transition matrix Ph, enabling accurate off-policy estimation of state visitation differences.
- Core assumption: Transition dynamics are unknown and must be learned to compute policy differences correctly.
- Evidence anchors:
  - [abstract]: "we answer this question positively for contextual bandits, but in the negative for tabular RL, showing a separation between contextual bandits and RL."
  - [section 4]: Proposition 1 and Lemma 1 show that ρΠ alone is insufficient for RL because transition learning adds extra complexity.
  - [corpus]: No direct evidence; inferred from theoretical analysis.
- Break condition: If transitions were known or action-independent, the complexity could match contextual bandits.

### Mechanism 3
- Claim: The algorithm achieves instance-optimal sample complexity by adaptively pruning unreachable states and focusing exploration on policy-discriminative directions.
- Mechanism: Prune step removes states with visitation probability below threshold, and OptCov algorithm collects data efficiently along directions (πh− ¯πh)ˆw¯πh + πhˆδπh.
- Core assumption: Many states are irrelevant for distinguishing policies, so pruning reduces effective problem size.
- Evidence anchors:
  - [section 5]: "Compute new reference policy ... Choose ¯πℓ← min¯π ∈ Πℓ maxπ ∈ Πℓ ∑H h=1 ˆUℓ− 1,h (π, ¯π )" and pruning logic.
  - [appendix F]: Algorithm 3 OptCov details and pruning criteria.
  - [corpus]: No direct evidence; based on algorithm description.
- Break condition: If pruning removes states that are actually critical for policy discrimination, optimality is lost.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and policy evaluation
  - Why needed here: The problem is formulated in terms of identifying an ε-optimal policy in a tabular MDP; understanding state visitation vectors, Q-values, and value functions is essential.
  - Quick check question: Can you write the Bellman equation for V π h(s) and explain how it relates to Q π h(s,a)?

- Concept: Off-policy estimation and importance weighting
  - Why needed here: The algorithm estimates policy differences using data collected from a logging policy µ; understanding how to correct for distribution shift is critical.
  - Quick check question: Given data from policy µ, how would you estimate Eπ [f(s,a)] using only µ's data?

- Concept: Instance-dependent sample complexity and gap-visitation complexity
  - Why needed here: The paper's complexity measure ρΠ and U(π,¯π) depend on the specific MDP instance and policy gaps; recognizing when algorithms are instance-optimal vs. minimax is key.
  - Quick check question: What is the difference between a minimax and an instance-dependent sample complexity bound?

## Architecture Onboarding

- Component map:
  - Reference policy selection (¯πℓ) → state visitation estimation → difference estimation (ˆδπℓ,h) → policy elimination loop
  - OptCov: online experiment design for efficient covariate collection
  - Prune: hard-to-reach state elimination
  - Learn2Explore: subroutine for uniform exploration

- Critical path:
  1. Choose reference policy ¯πℓ that minimizes maxπ U(π,¯π)
  2. Collect ¯nℓ episodes from ¯πℓ to estimate w¯πℓ,h
  3. For each step h, run OptCov to collect DEDℓ,h covering directions (πh−¯πℓ,h)ˆw¯πℓ,h + πhˆδπℓ,h
  4. Estimate transitions and rewards from DEDℓ,h
  5. Compute policy differences ˆD¯πℓ(π) and eliminate suboptimal policies
  6. Repeat until one policy remains or tolerance reached

- Design tradeoffs:
  - Reference policy choice: good ¯πℓ reduces U(π,¯π) but may be hard to learn
  - Pruning aggressiveness: more pruning = fewer samples but risk of removing critical states
  - OptCov tolerance: tighter tolerance = more accurate estimates but higher sample cost

- Failure signatures:
  - Algorithm stalls: reference policy too hard to learn or pruning too aggressive
  - Returned policy suboptimal: insufficient exploration or poor reference policy choice
  - High variance estimates: OptCov tolerance too loose or data insufficient

- First 3 experiments:
  1. Run on the simple MDP from Figure 1; verify that ρΠ is insufficient and algorithm collects extra samples for transition learning
  2. Test contextual bandit special case; confirm complexity reduces to O(ρΠ) as claimed
  3. Vary pruning threshold ǫunif; measure impact on sample complexity and correctness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between the lower bound complexity ρΠ and the upper bound achieved by the Perp algorithm, including the additional U(π, π*) term?
- Basis in paper: The paper shows that ρΠ is not sufficient for tabular RL and provides an upper bound that scales as O(ρΠ) plus an additional term U(π, π*)/max{ǫ², ∆(π)²}.
- Why unresolved: The paper does not provide a matching lower bound that includes the U(π, π*) term, leaving the exact tightness of the upper bound unclear.
- What evidence would resolve it: A lower bound proof that matches the upper bound complexity of Perp, including the U(π, π*) term, would resolve this question.

### Open Question 2
- Question: Can the low-order terms in the sample complexity of the Perp algorithm be improved or removed?
- Basis in paper: The paper mentions that the current complexity has unfortunate low-order terms, such as Cpoly/max{ǫ⁵/³, ∆min⁵/³}, and makes no claims on their tightness.
- Why unresolved: The paper does not provide any analysis or discussion on whether these low-order terms are necessary or if they can be improved.
- What evidence would resolve it: A refined analysis of the Perp algorithm that removes or improves the low-order terms would resolve this question.

### Open Question 3
- Question: Is there a qualitative information-theoretic separation between contextual bandits and tabular RL, as suggested by the paper?
- Basis in paper: The paper shows that in contextual bandits, the cost of learning the context distribution is dominated by that of learning the rewards, while in tabular RL, the cost of learning the state-visitation probabilities is unavoidable.
- Why unresolved: The paper provides evidence for a separation but does not provide a formal proof or a precise characterization of the separation.
- What evidence would resolve it: A formal proof of the separation, or a precise characterization of the complexity gap between contextual bandits and tabular RL, would resolve this question.

## Limitations

- The analysis is limited to tabular settings with finite state and action spaces, severely restricting practical applicability.
- The algorithm assumes access to the full policy set Π, which may be unrealistic when the policy space is exponentially large or continuous.
- The paper lacks empirical validation of theoretical claims, leaving open questions about practical performance.
- The algorithm's performance depends critically on choosing a good reference policy, introducing sensitivity to initialization and parameter tuning.

## Confidence

- **High Confidence**: The theoretical separation between contextual bandits and tabular RL is well-established through Proposition 1 and Lemma 1. The core mechanism of estimating policy differences rather than full behaviors is mathematically sound.
- **Medium Confidence**: The claim that Perp achieves tightest known bounds is supported by theoretical analysis but lacks empirical verification. The instance-optimal guarantees depend on assumptions about policy gaps that may not hold in practice.
- **Low Confidence**: The practical performance and robustness of the algorithm to poor reference policy choices or aggressive pruning remains unverified without experimental results.

## Next Checks

1. **Implement and test Perp on a simple tabular MDP**: Construct a small MDP with known optimal policy and policy set where ρΠ < 1, verify that Perp correctly identifies the optimal policy and collects samples beyond what ρΠ would suggest, confirming the necessity of transition learning.

2. **Validate contextual bandit special case**: Implement Perp on a contextual bandit instance (action-independent transitions) and confirm that the sample complexity reduces to O(ρΠ) as claimed, providing evidence that the algorithm correctly handles this special case.

3. **Test pruning sensitivity**: Systematically vary the pruning threshold ǫunif across multiple MDP instances and measure its impact on sample complexity and correctness. This would reveal whether the algorithm is robust to pruning choices or whether certain parameter regimes lead to failure.