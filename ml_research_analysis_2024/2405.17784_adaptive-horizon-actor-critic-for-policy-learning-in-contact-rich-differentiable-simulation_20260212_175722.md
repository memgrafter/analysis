---
ver: rpa2
title: Adaptive Horizon Actor-Critic for Policy Learning in Contact-Rich Differentiable
  Simulation
arxiv_id: '2405.17784'
source_url: https://arxiv.org/abs/2405.17784
tags:
- learning
- horizon
- shac
- policy
- ahac
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AHAC addresses learning instability and gradient error in first-order
  model-based RL due to stiff contact dynamics in differentiable simulation. It introduces
  an adaptive horizon objective that truncates trajectories before stiff contact and
  uses a dual-critic architecture with iterative training.
---

# Adaptive Horizon Actor-Critic for Policy Learning in Contact-Rich Differentiable Simulation

## Quick Facts
- arXiv ID: 2405.17784
- Source URL: https://arxiv.org/abs/2405.17784
- Reference count: 40
- Primary result: AHAC outperforms MFRL baselines by 40% in reward across five locomotion tasks and scales efficiently to high-dimensional control (152 actions).

## Executive Summary
AHAC introduces an adaptive horizon objective to address learning instability and gradient error in first-order model-based RL caused by stiff contact dynamics in differentiable simulation. By truncating trajectories before stiff contact and using a dual-critic architecture with iterative training, AHAC achieves superior asymptotic performance over model-free RL baselines, attaining up to 64% higher reward in complex locomotion tasks. The method efficiently scales to high-dimensional control environments while improving wall-clock-time efficiency.

## Method Summary
AHAC is an actor-critic algorithm that learns policies in differentiable physics simulation by adjusting trajectory rollout horizons to circumvent stiff contact dynamics. It employs a double-critic architecture trained iteratively until convergence and uses a first-order model-based gradient (FOBG) approach with adaptive truncation. The method formulates horizon adaptation as a constrained optimization problem using Lagrangian duality, where the horizon is shortened when contact gradients exceed a threshold to maintain stable learning.

## Key Results
- AHAC outperforms MFRL baselines by 40% in reward across five locomotion tasks
- Achieves up to 64% higher reward compared to single-environment baselines
- Scales efficiently to high-dimensional control environments with 152 actions
- Reduces run-to-run variance compared to SHAC's single target-critic approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stiff contact dynamics cause exploding gradient norms in differentiable simulation, leading to gradient sample error and learning instability.
- Mechanism: When contact forces become stiff, the Jacobian of the dynamics grows large (∥∇f(s,a)∥ ≫ 0). In first-order model-based RL, gradients are backpropagated through these dynamics; large Jacobian entries amplify noise and error, making policy gradients inaccurate and causing optimization instability.
- Core assumption: Differentiable simulators approximate stiff contact with a smooth Heaviside-like function whose gradient magnitude grows inversely with the approximation threshold.
- Evidence anchors:
  - [abstract] "First-Order Model-Based Reinforcement Learning (FO-MBRL) methods employing differentiable simulation provide gradients with reduced variance but are susceptible to sampling error in scenarios involving stiff dynamics, such as physical contact."
  - [section] "Empirical findings reveal that AHAC outperforms MFRL baselines, attaining 40% more reward across a set of locomotion tasks and efficiently scaling to high-dimensional control environments with improved wall-clock-time efficiency."

### Mechanism 2
- Claim: Truncating trajectories before stiff contact reduces gradient error without sacrificing policy optimality.
- Mechanism: By stopping model-based rollouts at the point where ∥∇f(st,at)∥ exceeds a threshold C, the algorithm avoids backpropagating through high-gradient regions, keeping gradients stable and informative. The adaptive horizon learns when to truncate based on contact stiffness.
- Core assumption: The optimal gait period for locomotion tasks is roughly equal to the horizon length at which stiff contact first occurs; thus, early truncation still captures the essential reward-relevant state transitions.
- Evidence anchors:
  - [abstract] "AHAC adjusts its trajectory rollout horizon to circumvent stiff dynamics."
  - [section] "Experimentally, our method shows superior asymptotic performance over MFRL baselines in complex locomotion tasks, achieving up to 64% higher reward."

### Mechanism 3
- Claim: Using a dual-critic architecture with iterative training improves value estimation stability in non-convex landscapes.
- Mechanism: Two critics are trained in parallel; the actor uses the min of the two value estimates, reducing overestimation bias. Iterative training (until small changes in critic loss) ensures better fit to the return distribution, which is especially helpful in complex environments like Humanoid.
- Core assumption: The value landscape is highly non-convex, and a single critic may converge to a poor local minimum; a second critic provides a better target.
- Evidence anchors:
  - [section] "we introduce a double critic that is trained until convergence, defined as a small change in the last 5 critic training iterations."
  - [section] "the double critic model notably reduces run-to-run variance, surpassing the performance stability of SHAC’s single target-critic approach."

## Foundational Learning

- Concept: Policy gradient theorem and variance reduction
  - Why needed here: The paper contrasts zeroth-order (high variance) and first-order (low variance but error-prone) policy gradient estimates; understanding how baselines reduce variance is key to interpreting the proposed adaptive horizon objective.
  - Quick check question: What is the effect of subtracting a baseline from the return in policy gradient estimation?

- Concept: Differentiable physics simulation and contact modeling
  - Why needed here: AHAC relies on differentiating through a simulator's dynamics; stiff contact is modeled via a smooth approximation to a discontinuous function (e.g., Heaviside), whose gradient behavior drives the adaptive truncation logic.
  - Quick check question: How does a soft Heaviside function approximate Coulomb friction, and what happens to its gradient as the smoothness parameter ν approaches zero?

- Concept: Constrained optimization via Lagrangian formulation
  - Why needed here: The adaptive horizon objective is formulated as a constrained optimization (keep contact gradients below threshold), solved by introducing Lagrange multipliers that adapt the horizon length.
  - Quick check question: How does the Lagrangian dual problem transform a constrained objective into an unconstrained one?

## Architecture Onboarding

- Component map: Policy network → Two critic networks → Rollout loop with adaptive truncation → Horizon adaptation via dual variable → Actor update with FOBG → Iterative critic training until convergence
- Critical path: Simulate → detect contact stiffness → truncate if needed → compute gradient through dynamics → update actor → update critics until convergence → repeat
- Design tradeoffs:
  - Fixed horizon (SHAC) vs adaptive horizon: simplicity vs stability in contact-rich tasks
  - Single critic vs double critic: lower compute vs better value estimation in non-convex landscapes
  - Iterative critic training vs fixed iterations: better fit vs wall-clock time
- Failure signatures:
  - If contact threshold C is too low, horizons become too short and reward signal is lost
  - If C is too high, gradient error returns and training becomes unstable
  - If horizon learning rate αϕ is too large, horizon oscillates wildly
  - If critics are not trained to convergence, value overestimation can mislead the actor
- First 3 experiments:
  1. Run Hopper task with fixed H=32 (SHAC baseline) to confirm gradient instability near contact
  2. Run Hopper with AHAC-1 (single environment, adaptive truncation) to confirm contact truncation improves reward
  3. Run Ant task with AHAC full (parallel, double critic) to validate scalability and compare against PPO/SAC baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AHAC's performance scale with increasing contact stiffness in the environment?
- Basis in paper: [inferred] The paper mentions that FOBGs suffer from sample error in scenarios involving stiff dynamics, and AHAC addresses this by truncating trajectories before stiff contact. However, the paper does not explore how AHAC's performance changes as contact stiffness increases.
- Why unresolved: The paper does not provide experiments or analysis on how AHAC's performance scales with varying contact stiffness.
- What evidence would resolve it: Experiments comparing AHAC's performance across environments with different contact stiffness levels, or a theoretical analysis of how contact stiffness affects AHAC's gradient estimates and learning stability.

### Open Question 2
- Question: Can the adaptive horizon mechanism in AHAC be generalized to other types of discontinuous dynamics beyond contact?
- Basis in paper: [explicit] The paper introduces the adaptive horizon mechanism to avoid stiff contact dynamics, but does not discuss its applicability to other types of discontinuities.
- Why unresolved: The paper focuses on contact-rich environments and does not explore the generalization of the adaptive horizon to other discontinuous dynamics.
- What evidence would resolve it: Experiments applying AHAC to environments with different types of discontinuities (e.g., collision, switching dynamics) or a theoretical analysis of how the adaptive horizon mechanism can be adapted to handle various types of discontinuities.

### Open Question 3
- Question: How does the double critic architecture in AHAC compare to other critic architectures in terms of variance reduction and learning stability?
- Basis in paper: [explicit] The paper introduces a double critic architecture in AHAC and shows that it reduces run-to-run variance compared to SHAC's single target-critic approach.
- Why unresolved: The paper does not compare the double critic architecture to other critic architectures (e.g., ensemble critics, distributional critics) in terms of variance reduction and learning stability.
- What evidence would resolve it: Experiments comparing the double critic architecture in AHAC to other critic architectures on the same tasks, or a theoretical analysis of how different critic architectures affect variance and learning stability.

## Limitations
- The adaptive horizon mechanism's effectiveness depends critically on the assumption that optimal locomotion can be achieved within truncated trajectories before stiff contact.
- The choice of contact gradient threshold C=500 is empirically determined but lacks theoretical justification for optimality or robustness to different simulator parameters.
- The double critic architecture's benefits in non-convex value landscapes are demonstrated but the convergence criteria and training until "small changes" is not precisely quantified.

## Confidence
- High confidence: The empirical demonstration that AHAC outperforms MFRL baselines by 40% in reward and scales to 152-action tasks is well-supported by the results section.
- Medium confidence: The mechanism explanation for why stiff contact causes gradient error relies on asymptotic behavior (∥∇f∥ → ∞) that may not manifest in practical simulation parameters.
- Low confidence: The claim that AHAC "efficiently scales to high-dimensional control" is based on a single Humanoid experiment; broader validation across diverse high-dimensional tasks would strengthen this.

## Next Checks
1. Conduct sensitivity analysis varying the contact threshold C across multiple orders of magnitude to test robustness of the adaptive horizon mechanism.
2. Implement a variant of AHAC with fixed horizon equal to the average adaptive horizon length to isolate the contribution of truncation versus horizon adaptation.
3. Test AHAC on a contact-rich manipulation task (e.g., pushing objects) where sustained contact is required, to evaluate the mechanism's limitations when truncation removes critical reward-relevant interactions.