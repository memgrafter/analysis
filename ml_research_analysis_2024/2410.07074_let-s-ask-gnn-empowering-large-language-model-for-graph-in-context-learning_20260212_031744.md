---
ver: rpa2
title: 'Let''s Ask GNN: Empowering Large Language Model for Graph In-Context Learning'
arxiv_id: '2410.07074'
source_url: https://arxiv.org/abs/2410.07074
tags:
- graph
- examples
- askgnn
- llms
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AskGNN bridges the gap between large language models (LLMs) and
  graph-structured data by integrating a Graph Neural Network (GNN)-based structure-enhanced
  retriever with In-Context Learning (ICL). The approach uses the retriever to select
  labeled nodes across graphs, optimizing its choices through a learning-to-retrieve
  algorithm that maximizes LLM performance.
---

# Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning

## Quick Facts
- arXiv ID: 2410.07074
- Source URL: https://arxiv.org/abs/2410.07074
- Authors: Zhengyu Hu; Yichuan Li; Zhengyu Chen; Jingang Wang; Han Liu; Kyumin Lee; Kaize Ding
- Reference count: 27
- Primary result: 73.86% accuracy on arxiv2023, 82.88% on ogbn-products with 1% labeled data

## Executive Summary
Let's Ask GNN (AskGNN) bridges the gap between large language models and graph-structured data by integrating a Graph Neural Network-based structure-enhanced retriever with In-Context Learning. The approach uses the retriever to select labeled nodes across graphs, optimizing its choices through a learning-to-retrieve algorithm that maximizes LLM performance. Experiments on three tasks and seven LLMs demonstrate AskGNN's superior effectiveness in node classification and other graph tasks, achieving accuracy improvements over strong baselines while requiring no extensive fine-tuning.

## Method Summary
AskGNN employs a Graph Neural Network (GNN)-powered structure-enhanced retriever to select labeled nodes across graphs for In-Context Learning (ICL) with LLMs. The framework uses GNNs to encode structural information into node representations, then selects top-K examples based on cosine similarity between query node and labeled nodes. A learning-to-retrieve algorithm optimizes the retriever by using LLM feedback (quantified through inverse perplexity) as a training signal. The final loss function combines feedback-based and structural losses, weighted by hyperparameter β, to balance graph learning with example selection quality.

## Key Results
- Achieved 73.86% accuracy on arxiv2023 dataset compared to lower baseline performance
- Obtained 82.88% accuracy on ogbn-products with only 1% labeled data
- Demonstrated consistent improvements across seven different LLMs including Qwen1.5-72B and Llama3-70B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GNN-based structure-enhanced retriever effectively selects ICL examples that maximize LLM performance by leveraging both graph structure and semantic similarity.
- Mechanism: The retriever uses GNN to encode structural information into node representations, then selects top-K examples based on cosine similarity between query node and labeled nodes. This process captures both structural patterns and semantic relevance.
- Core assumption: Graph structure provides discriminative signals that complement semantic similarity for example selection.
- Evidence anchors:
  - [abstract] "AskGNN employs a Graph Neural Network (GNN)-powered structure-enhanced retriever to select labeled nodes across graphs, incorporating complex graph structures and their supervision signals."
  - [section 4.1] "The Structure-Enhanced Retriever (SE-Retriever) plays a key role in utilizing GNNs to enhance the ICL process for LLMs."
  - [corpus] Weak evidence - corpus mentions LOGIN and Distilling Large Language Models for Text-Attributed Graph Learning but doesn't directly support this specific mechanism.
- Break condition: If graph structure doesn't provide additional discriminative power beyond semantic similarity, or if GNN encoding fails to capture relevant structural patterns.

### Mechanism 2
- Claim: Learning-to-retrieve algorithm optimizes retriever selection by using LLM feedback as a training signal.
- Mechanism: LLM feedback is quantified through utility scores based on inverse perplexity, creating a dynamic learning loop where retriever is fine-tuned to select examples that improve LLM predictions.
- Core assumption: LLM perplexity on correct predictions can serve as a reliable signal for example quality.
- Evidence anchors:
  - [abstract] "Our learning-to-retrieve algorithm optimizes the retriever to select example nodes that maximize LLM performance on graph."
  - [section 4.2] "We propose a novel Learning-to-Retrieve (L2R) approach that leverages LLM feedback as a training signal to optimize the retriever."
  - [corpus] Weak evidence - corpus mentions "Empowering GNNs for Domain Adaptation via Denoising Target Graph" but doesn't directly support this specific mechanism.
- Break condition: If LLM perplexity doesn't correlate well with example quality, or if the feedback signal becomes unstable during training.

### Mechanism 3
- Claim: Combining structural-based and feedback-based losses in retriever optimization balances graph learning with example selection quality.
- Mechanism: Final loss function combines Lfeedback (retrieval optimization) and Lclf (graph classification), weighted by hyperparameter β, ensuring retriever learns both structural patterns and example selection effectiveness.
- Core assumption: Both structural understanding and feedback-based optimization are necessary for optimal retriever performance.
- Evidence anchors:
  - [section 4.3] "The final loss function combines two components: L = β × Lfeedback + (1 − β) × Lclf"
  - [section 6.2] "We conducted a comprehensive hyperparameter analysis on the weighting factor β" showing performance varies with β.
  - [corpus] Weak evidence - corpus mentions "Parameter-Efficient Tuning Large Language Models for Graph Representation Learning" but doesn't directly support this specific mechanism.
- Break condition: If either component dominates too strongly, or if optimal β varies too widely across datasets.

## Foundational Learning

- Concept: Graph Neural Networks and message passing
  - Why needed here: GNNs are fundamental to extracting structural information from TAGs, which is the core innovation of AskGNN
  - Quick check question: How does a 2-layer GCN aggregate information from 2-hop neighbors?

- Concept: In-Context Learning and prompt engineering
  - Why needed here: AskGNN relies on LLMs' ICL capabilities to perform graph tasks using selected examples
  - Quick check question: What factors influence the effectiveness of ICL examples in LLMs?

- Concept: Loss function design and hyperparameter tuning
  - Why needed here: The learning-to-retrieve algorithm requires careful balance between structural and feedback-based losses
  - Quick check question: How would you determine the optimal value for hyperparameter β?

## Architecture Onboarding

- Component map: Query node → GNN encoder → Structure-enhanced retriever → Top-K labeled nodes → LLM with prompt template → Prediction
- Critical path: GNN encoding → Retriever selection → LLM prediction → Feedback collection → Retriever optimization
- Design tradeoffs: 
  - GNN complexity vs. computational efficiency
  - Number of ICL examples (K) vs. context window limits
  - Feedback collection cost vs. optimization quality
- Failure signatures:
  - Poor performance on datasets with weak structural signals
  - High computational cost when K is large
  - Instability in feedback-based optimization
- First 3 experiments:
  1. Baseline comparison: Run AskGNN vs. random few-shot selection on ogbn-arxiv with 1% labeled data
  2. Ablation study: Remove feedback-based loss component and compare performance
  3. Hyperparameter sensitivity: Vary β from 0 to 1 and plot performance curves on ogbn-products

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AskGNN scale with larger TAG datasets containing millions of nodes and edges?
- Basis in paper: [inferred] The paper mentions ogbn-products with ~2.4M nodes as one of their datasets, but notes limitations with current LLM context windows restricting example numbers.
- Why unresolved: The paper only evaluates on relatively moderate-sized datasets and acknowledges context window limitations for larger graphs.
- What evidence would resolve it: Systematic experiments on progressively larger TAG datasets showing performance trends, or architectural modifications enabling AskGNN to handle billion-node graphs.

### Open Question 2
- Question: Can the learning-to-retrieve algorithm be adapted to dynamically adjust example selection during inference based on query node difficulty?
- Basis in paper: [explicit] The paper describes a static selection process where K examples are chosen based on similarity, but doesn't explore adaptive selection strategies.
- Why unresolved: The current approach uses fixed K and static similarity-based selection without considering query complexity or uncertainty.
- What evidence would resolve it: Implementation showing improved accuracy when the retriever adapts K or selection criteria based on query node features or LLM confidence scores.

### Open Question 3
- Question: How would AskGNN perform on dynamic graphs where node attributes and connections change over time?
- Basis in paper: [inferred] The paper focuses on static TAGs and mentions future work on dynamic graphs as an extension, but doesn't evaluate temporal aspects.
- Why unresolved: All experiments use static snapshots of graph data without considering temporal evolution or concept drift.
- What evidence would resolve it: Experiments comparing AskGNN performance on temporally-staged graph versions, or showing degradation when applying static models to time-evolving graphs.

## Limitations
- Limited evaluation scope: Only tested on three TAG datasets, raising questions about generalization to different graph types
- Computational complexity: Learning-to-retrieve algorithm requires careful hyperparameter tuning and may be computationally expensive
- LLM dependency: Performance heavily depends on underlying LLM capabilities and context window limitations

## Confidence

**High Confidence**: The core mechanism of using GNNs for structure-aware example selection in ICL is well-founded and supported by the literature on both GNNs and prompt engineering. The experimental results on the three tested datasets demonstrate consistent improvements over baselines.

**Medium Confidence**: The learning-to-retrieve algorithm's effectiveness is supported by experimental results, but the reliance on LLM perplexity as a feedback signal may not always correlate with example quality. The optimal weighting factor β likely requires dataset-specific tuning, and the paper's sensitivity analysis provides limited guidance for new applications.

**Low Confidence**: Claims about AskGNN's effectiveness on datasets with different characteristics (e.g., heterophilic graphs, graphs with different attribute distributions) are not supported by current experiments. The method's scalability and computational efficiency for large graphs or real-time applications remain unverified.

## Next Checks

1. **Cross-dataset Robustness Test**: Evaluate AskGNN on a diverse set of graph datasets including social networks (e.g., Reddit), molecular graphs (e.g., ZINC), and knowledge graphs (e.g., NELL) to assess generalization beyond TAGs. Compare performance against the current baselines and analyze how the GNN retriever performs on graphs with different structural properties (e.g., heterophily, varying node degrees).

2. **Scalability and Efficiency Analysis**: Measure the computational cost of the learning-to-retrieve algorithm on progressively larger graphs, tracking both training time and inference latency. Compare the total computational overhead against the accuracy improvements, and assess whether the method remains practical for graphs with millions of nodes or real-time applications.

3. **LLM Architecture Sensitivity Study**: Test AskGNN with a wider range of LLM architectures including encoder-decoder models (e.g., T5), models with different context window sizes, and smaller models to determine the minimum effective LLM size. Analyze how performance varies with different prompt engineering strategies and whether the GNN structure information translates effectively across different LLM architectures.