---
ver: rpa2
title: 'iSee: Advancing Multi-Shot Explainable AI Using Case-based Recommendations'
arxiv_id: '2408.12941'
source_url: https://arxiv.org/abs/2408.12941
tags:
- case
- isee
- user
- explanation
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The iSee platform uses Case-based Reasoning (CBR) to enable non-expert
  AI system designers to create multi-shot explanation strategies for explainable
  AI (XAI) systems. The platform captures and reuses past explanation experiences
  to recommend personalized explanation strategies, adapting them based on stakeholder
  requirements.
---

# iSee: Advancing Multi-Shot Explainable AI Using Case-based Recommendations

## Quick Facts
- arXiv ID: 2408.12941
- Source URL: https://arxiv.org/abs/2408.12941
- Reference count: 40
- Platform enables non-experts to create multi-shot explanation strategies using Case-based Reasoning

## Executive Summary
The iSee platform introduces a novel approach to explainable AI by leveraging Case-based Reasoning (CBR) to enable non-expert AI system designers to create and reuse multi-shot explanation strategies. The platform captures past explanation experiences and recommends personalized strategies that adapt to stakeholder requirements. A user experience evaluation with six design users across varying expertise levels demonstrated the platform's utility and usability, with scores above average on established benchmarks. Users particularly valued the platform's ability to facilitate comparison of multiple explainers, though security concerns and interface navigation issues were identified for future improvements.

## Method Summary
The iSee platform implements a Case-based Reasoning system that captures explanation experiences as cases, which can be retrieved and reused to recommend personalized explanation strategies. The system adapts recommendations based on stakeholder requirements and supports multi-shot explanations that evolve over multiple interactions. The platform was evaluated through a user experience study with six design users who had varying levels of AI and XAI expertise. Participants interacted with the platform to create explanation strategies and provided feedback on usability, utility, and overall experience through standardized evaluation metrics.

## Key Results
- User experience evaluation scored above average on established benchmarks with six design users
- Participants found the platform motivating and novel, particularly for comparing multiple explainers
- Platform successfully enables non-experts to create multi-shot explanation strategies using CBR

## Why This Works (Mechanism)
The platform leverages Case-based Reasoning to capture and reuse past explanation experiences, creating a knowledge base that non-expert designers can access to build effective multi-shot explanation strategies. By adapting recommendations to stakeholder requirements and enabling comparison of different explainer options, the system provides a practical bridge between complex AI explainability concepts and real-world application needs. The multi-shot approach allows explanations to evolve over multiple interactions, creating more comprehensive understanding for stakeholders.

## Foundational Learning
- Case-based Reasoning (CBR) fundamentals: Why needed - to capture and reuse explanation experiences; Quick check - can the platform successfully retrieve relevant past cases?
- Multi-shot explanation strategies: Why needed - to provide evolving explanations over multiple interactions; Quick check - do explanations improve stakeholder understanding over time?
- User-centered design for XAI tools: Why needed - to make complex explainability accessible to non-experts; Quick check - can designers with varying expertise levels effectively use the platform?

## Architecture Onboarding

Component map: User Interface -> Case Repository -> Recommendation Engine -> Explainer Selection

Critical path: User inputs stakeholder requirements → Platform retrieves relevant cases → Recommendation engine suggests explanation strategies → User selects and compares explainers → Strategy is implemented

Design tradeoffs: The platform prioritizes accessibility for non-experts over advanced customization options, sacrificing some flexibility to ensure usability. The case-based approach trades real-time learning for proven experience-based recommendations.

Failure signatures: Poor case retrieval quality leads to irrelevant recommendations; interface complexity overwhelms novice users; security vulnerabilities in case storage could compromise sensitive explanation strategies.

Three first experiments:
1. Test case retrieval accuracy with varying query types and similarity measures
2. Evaluate explanation strategy effectiveness with different stakeholder personas
3. Measure platform usability improvements after interface refinements

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of six design users limits generalizability of user experience findings
- Focus on designer experiences rather than end-user validation of explanation effectiveness
- Security concerns raised about case storage and retrieval were not thoroughly addressed

## Confidence
- Platform functionality and design: High
- User experience evaluation results: Medium
- Multi-shot explanation effectiveness: Low

## Next Checks
1. Conduct a larger-scale user study with diverse stakeholders (both designers and end-users) across multiple AI domains to validate the platform's effectiveness.
2. Perform a longitudinal study tracking explanation strategy performance over time and across different user groups to assess adaptability and learning capabilities.
3. Implement comprehensive security testing and case management protocols to address participant concerns and ensure safe knowledge sharing.