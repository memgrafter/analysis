---
ver: rpa2
title: Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled
  KL Divergence
arxiv_id: '2406.10957'
source_url: https://arxiv.org/abs/2406.10957
tags:
- arxiv
- sampo
- length
- reward
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and addresses an inherent length reliance
  in Direct Preference Optimization (DPO) that causes verbosity in generated responses.
  The authors show that DPO's use of sequence-level KL divergence discrepancies between
  chosen and rejected responses leads to biased rewards, with longer responses being
  overestimated and shorter ones underestimated.
---

# Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence

## Quick Facts
- arXiv ID: 2406.10957
- Source URL: https://arxiv.org/abs/2406.10957
- Authors: Junru Lu; Jiazheng Li; Siyu An; Meng Zhao; Yulan He; Di Yin; Xing Sun
- Reference count: 26
- Primary result: SamPO reduces verbosity and improves performance by 5-12% on multiple benchmarks compared to standard DPO

## Executive Summary
This paper identifies a fundamental limitation in Direct Preference Optimization (DPO) - its tendency to produce verbose responses due to biased length reliance. The authors demonstrate that DPO's sequence-level KL divergence calculation inherently favors longer responses, leading to overestimation of their quality. They propose SamPO (Down-Sampled KL Divergence DPO), which addresses this bias by down-sampling token-level features to regularize the KL divergence. Experimental results across three LLMs show that SamPO effectively reduces verbosity while improving performance on multiple benchmarks by 5-12%.

## Method Summary
The authors propose SamPO, a modification to Direct Preference Optimization that addresses length bias through down-sampling. The core insight is that standard DPO uses sequence-level KL divergence, which creates a bias where longer responses are systematically overestimated. SamPO introduces a down-sampling mechanism that equalizes token-level features, effectively regularizing the KL divergence calculation. This approach maintains the preference optimization framework while eliminating the length-dependent bias that causes verbosity in generated responses.

## Key Results
- SamPO reduces verbosity in LLM responses compared to standard DPO
- Performance improvements of 5-12% on multiple benchmarks
- Effective across three different LLMs tested
- Maintains response quality while addressing length bias

## Why This Works (Mechanism)
Standard DPO's sequence-level KL divergence creates an inherent bias toward longer responses because the divergence calculation scales with sequence length. This causes the reward function to systematically overestimate the quality of longer responses. SamPO addresses this by down-sampling token-level features to equalize their contribution, effectively regularizing the KL divergence. This regularization removes the length-dependent bias while preserving the preference optimization framework's ability to learn from human feedback.

## Foundational Learning
- **KL Divergence**: Measures the difference between probability distributions; used in DPO to align model outputs with preferred responses. Why needed: Core mathematical foundation for preference optimization. Quick check: Verify understanding of forward vs reverse KL divergence and their properties.
- **Sequence-level vs Token-level Processing**: Sequence-level aggregates across all tokens, while token-level processes individual positions. Why needed: Central to understanding the length bias problem. Quick check: Compare computational complexity and information preservation between the two approaches.
- **Preference Optimization**: Framework for aligning models with human preferences using paired data. Why needed: SamPO builds directly on this framework. Quick check: Understand how preference data is structured and used in optimization.
- **Reward Modeling**: Uses learned scores to guide generation. Why needed: Length bias affects reward estimation. Quick check: Examine how reward functions are constructed and their impact on generation.
- **Regularization in Optimization**: Techniques to prevent overfitting and bias. Why needed: Down-sampling acts as a form of regularization. Quick check: Review common regularization methods and their effects.

## Architecture Onboarding

**Component Map**: Input Data -> Preference Pair Processing -> KL Divergence Calculation -> Reward Estimation -> Policy Update -> SamPO Down-sampling Layer -> Regularized KL Divergence

**Critical Path**: The critical path flows from preference pair processing through the modified KL divergence calculation with down-sampling, to reward estimation and policy update. The SamPO layer is inserted between the raw preference processing and the reward estimation.

**Design Tradeoffs**: The primary tradeoff is between computational efficiency and bias reduction. Down-sampling introduces additional computation but eliminates length bias. The authors chose a simple equal token-level sampling approach rather than more complex weighting schemes to maintain simplicity and interpretability.

**Failure Signatures**: Potential failure modes include under-sampling important tokens, introducing new biases through the down-sampling process, or failing to adequately address length bias if the sampling rate is inappropriate. Performance degradation on tasks where response length is actually informative could also indicate issues.

**First Experiments**:
1. Verify length bias exists in standard DPO by comparing average response lengths with and without preference optimization
2. Test SamPO with varying down-sampling rates to find optimal regularization strength
3. Compare SamPO against baseline methods on a simple task where length bias effects are easily observable

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical justification for down-sampling as a solution to length bias remains largely intuitive rather than rigorously proven
- Experimental scope is limited primarily to summarization and dialogue tasks
- Ablation studies don't explore sensitivity to different down-sampling rates or alternative regularization approaches

## Confidence

**High confidence**: The empirical observation that DPO exhibits length bias and produces verbose outputs is well-supported by both theoretical analysis and experimental results.

**Medium confidence**: The effectiveness of SamPO in reducing verbosity and improving performance is demonstrated, but the mechanism by which down-sampling specifically addresses length bias could benefit from deeper theoretical exploration.

**Medium confidence**: The claim that SamPO maintains response quality while reducing length bias is supported by experiments, though evaluation metrics may not capture all aspects of response quality.

## Next Checks

1. Conduct extensive ablation studies varying the down-sampling rate to determine optimal regularization strength and assess sensitivity to this hyperparameter.

2. Test SamPO across a broader range of NLP tasks beyond summarization and dialogue, including code generation, question answering, and creative writing, to evaluate generalizability.

3. Perform controlled experiments comparing SamPO against alternative length regularization methods (such as those proposed in related concurrent work) to establish relative effectiveness and identify potential trade-offs.