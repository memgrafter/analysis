---
ver: rpa2
title: 'BERT-LSH: Reducing Absolute Compute For Attention'
arxiv_id: '2404.08836'
source_url: https://arxiv.org/abs/2404.08836
tags:
- bert-lsh
- bert
- attention
- training
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BERT-LSH introduces a locality-sensitive hashing (LSH) approximation
  to the self-attention mechanism in BERT, reducing computational demand by ~60% (KFLOPs)
  and ~86% (dot products) while maintaining or improving downstream performance. The
  model retains distinct Q and K matrices, hashing them independently and computing
  attention only on vector pairs that collide in LSH buckets.
---

# BERT-LSH: Reducing Absolute Compute For Attention

## Quick Facts
- arXiv ID: 2404.08836
- Source URL: https://arxiv.org/abs/2404.08836
- Authors: Zezheng Li; Kingston Yip
- Reference count: 8
- Primary result: ~60% reduction in KFLOPs and ~86% reduction in dot products while maintaining or improving downstream performance

## Executive Summary
BERT-LSH introduces a locality-sensitive hashing (LSH) approximation to the self-attention mechanism in BERT, reducing computational demand by ~60% (KFLOPs) and ~86% (dot products) while maintaining or improving downstream performance. The model retains distinct Q and K matrices, hashing them independently and computing attention only on vector pairs that collide in LSH buckets. Results show BERT-LSH outperforms baseline BERT in pretraining loss and evaluation accuracy, and achieves lower test loss in GLUE SST-2 fine-tuning, suggesting better generalization despite slower execution due to less optimized implementation. The SQuAD fine-tuning results were comparable between models. The approach demonstrates LSH's potential to make transformer models more efficient and accessible, though further optimization for parallel computation is needed for real-world deployment.

## Method Summary
BERT-LSH replaces the standard QK^T attention computation with an LSH-based approximation. The model applies LSH independently to query (Q) and key (K) matrices, generating hash codes for each. Only vector pairs sharing at least one hash bucket across all hash functions are considered for dot product computation, forming the collision matrix. This reduces the number of attention calculations from O(n²) to a subset determined by collision probability. The attention score matrix becomes symmetric due to the undirected nature of LSH collisions, but positional encodings compensate for potential loss of directional information. The model was pretrained on BookCorpus and fine-tuned on GLUE SST-2 and SQuAD datasets.

## Key Results
- Achieved ~60% reduction in KFLOPs and ~86% reduction in dot products compared to baseline BERT
- BERT-LSH outperformed baseline BERT in pretraining loss and evaluation accuracy on BookCorpus
- BERT-LSH achieved lower test loss than baseline on GLUE SST-2 fine-tuning, suggesting better generalization
- SQuAD fine-tuning results were comparable between models

## Why This Works (Mechanism)

### Mechanism 1
Independent LSH hashing of Q and K matrices enables selective attention computation only on vector pairs that collide in buckets, reducing computational load while preserving semantic relevance. The model applies LSH independently to query and key matrices, generating hash codes for each. Only pairs of vectors that share at least one hash bucket across all hash functions are considered for dot product computation, forming the collision matrix C. This reduces the number of attention calculations from O(n²) to a subset determined by collision probability. Core assumption: LSH collisions between query and key vectors are a reliable proxy for semantic similarity that would yield meaningful attention scores in full self-attention.

### Mechanism 2
The symmetric attention score matrix in BERT-LSH, while different from standard BERT, does not harm performance due to BERT's inherent bidirectionality and positional encoding. BERT-LSH populates attention scores symmetrically (Ab,h[i,j] = Ab,h[j,i]) because LSH collisions are undirected. The model assumes this symmetry is acceptable since BERT processes context bidirectionally and uses positional encodings to preserve token order information. Core assumption: The bidirectional nature of BERT and positional encodings compensate for the loss of directional information in the attention matrix.

### Mechanism 3
BERT-LSH's LSH-based attention mechanism may enhance generalization by filtering out less relevant attention connections, focusing learning on more salient features. By only computing attention scores for query-key pairs that collide in LSH buckets, the model implicitly regularizes attention patterns, potentially reducing overfitting to noise in the training data and improving generalization to unseen data. Core assumption: The sparse attention pattern induced by LSH acts as a form of regularization that helps the model learn more generalizable representations.

## Foundational Learning

- Locality Sensitive Hashing (LSH)
  - Why needed here: LSH provides the mathematical foundation for approximating similarity between high-dimensional vectors in sub-quadratic time, enabling efficient attention computation.
  - Quick check question: What is the probability of collision between two vectors under LSH as a function of their angular distance?

- Self-Attention Mechanism
  - Why needed here: Understanding the full self-attention computation (QK^T/sqrt(d)) is essential to grasp what BERT-LSH is approximating and why the approximation is computationally beneficial.
  - Quick check question: What is the computational complexity of full self-attention in terms of sequence length n and hidden dimension d?

- Transformer Architecture
  - Why needed here: BERT-LSH modifies the attention mechanism within the transformer encoder, so understanding the overall architecture is crucial for proper implementation and integration.
  - Quick check question: How do query, key, and value matrices interact in the standard transformer attention mechanism?

## Architecture Onboarding

- Component map:
  Input embeddings -> Positional encodings -> LSH attention layers (instead of standard multi-head attention) -> Feed-forward network -> Output
  LSH attention: Independent LSH on Q and K -> Collision detection -> Dot product computation for colliding pairs -> Softmax -> Weighted sum with V

- Critical path:
  Token embedding and positional encoding
  LSH hash computation for Q and K matrices
  Collision matrix construction
  Selective dot product computation
  Softmax normalization
  Value-weighted aggregation

- Design tradeoffs:
  Computational efficiency vs. potential loss of attention connections
  Symmetric attention matrix vs. standard directional attention
  Number of LSH bands (r) and hash functions (n) affecting collision probability
  Implementation complexity vs. PyTorch's optimized matrix operations

- Failure signatures:
  Degraded performance on tasks requiring precise attention patterns
  Unexpected overfitting or underfitting due to LSH regularization effects
  Significant slowdown despite reduced operations (due to non-parallelized implementation)

- First 3 experiments:
  1. Verify LSH collision probability matches theoretical expectations for controlled input vectors
  2. Compare attention patterns (attention heatmap) between BERT-LSH and baseline BERT on sample sequences
  3. Measure actual computational savings (KFLOPs, execution time) across different LSH configurations (varying r and n)

## Open Questions the Paper Calls Out

### Open Question 1
Does the LSH-based attention mechanism in BERT-LSH capture different semantic relationships compared to the standard attention mechanism, and if so, how does this impact the model's performance? Basis in paper: The paper notes that BERT-LSH unexpectedly outperformed the baseline model in pretraining and fine-tuning tasks, suggesting potential differences in how semantic relationships are captured. Why unresolved: The paper does not provide a detailed analysis of the semantic relationships learned by BERT-LSH versus the standard attention mechanism. What evidence would resolve it: A qualitative analysis of the attention patterns and a comparison of the semantic relationships learned by both models on a diverse set of NLP tasks.

### Open Question 2
How does the choice of LSH parameters (number of bands, table size, hash functions) affect the trade-off between computational efficiency and model performance? Basis in paper: The paper discusses the impact of LSH parameters on computational complexity but does not explore the optimal configuration for balancing efficiency and performance. Why unresolved: The paper only provides results for a specific LSH configuration and does not systematically investigate the parameter space. What evidence would resolve it: A comprehensive study varying the LSH parameters and evaluating the resulting trade-offs in terms of computational efficiency and model performance on various NLP tasks.

### Open Question 3
Can the BERT-LSH model be further optimized for parallel computation to achieve faster execution times? Basis in paper: The paper acknowledges that the current implementation of BERT-LSH is not optimized for parallel computation, leading to slower execution times compared to the baseline BERT model. Why unresolved: The paper does not propose or evaluate any optimization strategies for parallel computation in BERT-LSH. What evidence would resolve it: A comparison of execution times between the current BERT-LSH implementation and an optimized version designed for parallel computation on modern GPUs or other hardware accelerators.

## Limitations

- The LSH-based attention mechanism's unexpected performance improvement lacks a clear theoretical explanation, with regularization effects remaining speculative
- Current implementation is not optimized for parallel computation, resulting in slower execution times despite reduced computational complexity
- Symmetric attention matrix design may not generalize well to tasks requiring strong directional attention patterns

## Confidence

**High Confidence:** The computational reduction claims (60% KFLOPs, 86% dot products) are well-supported by the mathematical formulation of replacing O(n²) attention computation with LSH-based selective computation. The theoretical framework for LSH and its application to approximate similarity search is solid.

**Medium Confidence:** The downstream performance results showing BERT-LSH matching or slightly exceeding baseline BERT in pretraining loss, SST-2 fine-tuning, and SQuAD performance are reproducible based on the described methodology, though the unexpected generalization benefits remain unexplained.

**Low Confidence:** The proposed mechanism that LSH-induced sparsity acts as regularization to enhance generalization is speculative. The symmetric attention design's universal applicability across tasks is assumed rather than empirically validated across diverse benchmarks.

## Next Checks

1. **Attention Pattern Analysis:** Generate and compare attention heatmaps between BERT-LSH and baseline BERT on identical input sequences to empirically verify whether LSH collisions capture semantically meaningful attention patterns and to what extent sparsity affects important token relationships.

2. **Ablation on Symmetric Attention:** Implement a directional variant of BERT-LSH (computing QK^T instead of symmetric attention) and evaluate performance on both bidirectional tasks (like BERT pretraining) and unidirectional tasks (like autoregressive language modeling) to isolate the impact of symmetric attention on different task types.

3. **Optimized Implementation Benchmarking:** Re-implement the LSH attention mechanism using optimized GPU operations (e.g., PyTorch's batched operations for collision detection and sparse matrix multiplication) and measure actual wall-clock time reduction compared to the baseline, separating computational complexity reduction from implementation efficiency gains.