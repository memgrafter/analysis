---
ver: rpa2
title: Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free
  Inference
arxiv_id: '2402.05330'
source_url: https://arxiv.org/abs/2402.05330
tags:
- prediction
- sets
- nuisance
- naps
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of reliable classification under
  distributional shifts that affect both class labels and latent nuisance parameters.
  The authors introduce generalized label shift (GLS), where the joint distribution
  over labels and nuisance parameters changes between training and target data.
---

# Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference

## Quick Facts
- arXiv ID: 2402.05330
- Source URL: https://arxiv.org/abs/2402.05330
- Reference count: 40
- Primary result: Introduces nuisance-aware prediction sets (NAPS) for classification under generalized label shift, achieving valid coverage and high power on challenging scientific problems

## Executive Summary
This paper addresses the challenge of reliable classification when both class labels and latent nuisance parameters undergo distributional shifts between training and target data. The authors introduce generalized label shift (GLS), where the joint distribution over labels and nuisance parameters changes while the conditional distribution of features given labels and nuisance parameters remains constant. They propose NAPS, which reformulates classification as composite hypothesis testing and estimates the classifier's ROC across the entire nuisance parameter space to construct cutoffs that are invariant under GLS.

## Method Summary
The method reformulates classification as a composite hypothesis test H0,y: θ ∈ Θ0 vs H1,y: θ ∈ Θ1 where θ = (y, ν). The key insight is that the rejection probability is invariant under GLS, allowing reliable ROC estimation using training data. NAPS constructs valid prediction sets by controlling FPR uniformly across all nuisance parameter values, using (1-γ) confidence sets for ν to increase power while maintaining validity. The approach is demonstrated on RNA-Seq data with batch effects and cosmic ray shower classification in astroparticle physics.

## Key Results
- NAPS achieves valid coverage (1-α) for all y and ν simultaneously on held-out data under GLS
- On RNA-Seq data, NAPS maintains valid coverage regardless of batch protocol while other methods under-cover for at least two protocols
- On cosmic ray shower data, NAPS achieves high precision, low false discovery rates, and outperforms standard Bayes classifier when constraining nuisance parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Estimating the rejection probability function Wλ(C; y, ν) is invariant under GLS, enabling robust ROC estimation
- Mechanism: The rejection probability depends only on the conditional distribution P(X|y, ν), which remains unchanged between train and target data under GLS. This allows using the train distribution to reliably estimate ROC curves across the entire nuisance parameter space.
- Core assumption: The likelihood p(x|y, ν) is the same under train and target distributions (no covariate shift).
- Evidence anchors:
  - [abstract]: "The key idea is to estimate the classifier's receiver operating characteristic (ROC) across the entire nuisance parameter space, which allows us to devise cutoffs that are invariant under GLS."
  - [section]: "A key insight behind our method is that the rejection probability (Equation 4) is invariant under GLS even if estimated from ptrain; in other words, it is always the same for train and target data (Lemma 1)."
  - [corpus]: Weak evidence. Corpus papers don't directly discuss GLS invariance of rejection probability.

### Mechanism 2
- Claim: NAPS constructs valid prediction sets by controlling FPR uniformly across all nuisance parameter values
- Mechanism: By computing cutoffs as the infimum over a confidence set for ν, NAPS ensures that the maximum type-I error probability is controlled at level α for any true nuisance parameter value.
- Core assumption: Valid confidence sets for ν can be constructed that achieve (1-γ) coverage uniformly across ν.
- Evidence anchors:
  - [abstract]: "Our method effectively endows a pre-trained classifier with domain adaptation capabilities and returns valid prediction sets while maintaining high power."
  - [section]: "Theorem 1 (Nuisance-aware cutoffs for FPR/TPR control). Choose a threshold α ∈ [0, 1] and γ ∈ [0, α]. Let Sy(x; γ) be a valid (1 − γ) confidence set for ν at fixed y ∈ {0, 1} according to Definition 3."
  - [corpus]: Weak evidence. Corpus papers don't discuss uniform FPR control across nuisance parameters.

### Mechanism 3
- Claim: NAPS can increase power while maintaining validity by constraining nuisance parameters to a confidence set
- Mechanism: By optimizing cutoffs over a data-dependent confidence set Sy(x; γ) rather than the entire nuisance parameter space, NAPS can derive less conservative cutoffs that increase power while still guaranteeing validity.
- Core assumption: The confidence set Sy(x; γ) contains the true ν with high probability and is smaller than the full nuisance parameter space.
- Evidence anchors:
  - [abstract]: "Furthermore, we can increase power while maintaining validity (NAPS γ > 0; green) by constructing (1 − γ) confidence sets of the nuisance parameter ν and deriving less conservative cutoffs given an observation."
  - [section]: "An alternative approach, which is still valid for any ν and can increase power, is to restrict the search over nuisance parameters to a smaller region of N."
  - [corpus]: Weak evidence. Corpus papers don't discuss power increases through nuisance parameter constraints.

## Foundational Learning

- Concept: Composite hypothesis testing with nuisance parameters
  - Why needed here: The classification problem is reformulated as testing H0,y: θ ∈ Θ0 vs H1,y: θ ∈ Θ1 where θ = (y, ν), requiring methods to handle nuisance parameters in hypothesis testing.
  - Quick check question: What is the difference between simple and composite hypothesis tests, and why does this matter for NAPS?

- Concept: Receiver operating characteristic (ROC) curves and their estimation
  - Why needed here: NAPS estimates ROC curves across the entire nuisance parameter space to select optimal cutoffs, requiring understanding of ROC curve properties and estimation techniques.
  - Quick check question: How does the ROC curve relate to FPR and TPR, and why is estimating it across ν important for GLS?

- Concept: Confidence sets and their coverage properties
  - Why needed here: NAPS uses (1-γ) confidence sets for ν to increase power, requiring understanding of confidence set construction and coverage guarantees.
  - Quick check question: What properties must a confidence set have to be valid for NAPS, and how does this differ from point estimation?

## Architecture Onboarding

- Component map:
  Pre-trained classifier -> Rejection probability estimator -> Confidence set constructor -> Cutoff optimizer -> Prediction set generator

- Critical path:
  1. Train base classifier on (Y, X) pairs
  2. Estimate rejection probability function Wλ(C; y, ν) via monotone regression
  3. For new observation xtarget:
     a. Construct confidence set Sy(xtarget; γ)
     b. Compute cutoffs Cα,y(xtarget) via infimum over confidence set
     c. Generate prediction set H(xtarget; α)

- Design tradeoffs:
  - Monotonicity constraints vs flexibility in rejection probability estimation
  - Size of confidence set γ vs power vs computational cost
  - Number of nuisance parameters vs curse of dimensionality in estimation

- Failure signatures:
  - Poor coverage: Rejection probability poorly estimated or confidence sets invalid
  - Low power: Overly conservative cutoffs or confidence sets too large
  - Computational issues: High-dimensional nuisance parameters or slow optimization

- First 3 experiments:
  1. Verify invariance of rejection probability estimation under synthetic GLS
  2. Test coverage of NAPS prediction sets on held-out data with known ν
  3. Compare power of NAPS with γ=0 vs γ>0 on a simple synthetic problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can nuisance-aware prediction sets (NAPS) be extended to multiclass classification problems beyond binary classification?
- Basis in paper: [explicit] The paper states that NAPS can be extended to multiclass problems by estimating one-vs-one ROC curves for each nuisance parameter.
- Why unresolved: The paper does not provide details on the specific methodology for extending NAPS to multiclass problems, such as how to handle the increased complexity of multiple classes and the potential for increased computational cost.
- What evidence would resolve it: A detailed methodology for extending NAPS to multiclass problems, including a description of the algorithmic changes, computational considerations, and experimental validation on multiclass datasets.

### Open Question 2
- Question: How can NAPS be adapted to handle non-stationary distributional shifts, where the relationship between labels and nuisance parameters changes over time?
- Basis in paper: [inferred] The paper focuses on static distributional shifts (generalized label shift), but many real-world applications involve non-stationary data distributions.
- Why unresolved: The paper does not address the challenge of non-stationary distributional shifts, which can significantly impact the performance of NAPS in dynamic environments.
- What evidence would resolve it: A theoretical framework and empirical evaluation of NAPS under non-stationary distributional shifts, including methods for detecting and adapting to changes in the label-nuisance parameter relationship over time.

### Open Question 3
- Question: What are the trade-offs between the power of NAPS and the computational cost of constructing (1-γ) confidence sets for nuisance parameters?
- Basis in paper: [explicit] The paper mentions that constructing (1-γ) confidence sets can be computationally costly, especially for high-dimensional nuisance parameters, and that this can impact the power of NAPS.
- Why unresolved: The paper does not provide a quantitative analysis of the trade-off between power and computational cost, nor does it explore methods for optimizing this trade-off.
- What evidence would resolve it: A comprehensive study quantifying the relationship between the power of NAPS, the dimensionality of nuisance parameters, and the computational cost of constructing confidence sets, along with strategies for balancing these factors.

## Limitations

- NAPS requires accurate estimation of rejection probabilities across the entire nuisance parameter space, which may become challenging with high-dimensional ν
- The approach assumes that the conditional distribution p(x|y, ν) remains unchanged between train and target data, which may not hold in all practical scenarios
- Constructing (1-γ) confidence sets for nuisance parameters can be computationally costly, especially for high-dimensional problems

## Confidence

- Mechanism 1 (GLS invariance of rejection probability): High - supported by theoretical proof and simulation evidence
- Mechanism 2 (Uniform FPR control): Medium - theoretically derived but depends on confidence set quality
- Mechanism 3 (Power increase via constraints): Medium - demonstrated empirically but sensitive to nuisance parameter distribution

## Next Checks

1. Test NAPS on problems where the conditional distribution p(x|y, ν) changes between train and target data to evaluate robustness beyond GLS assumptions
2. Evaluate performance degradation as the number of nuisance parameters increases to quantify the curse of dimensionality
3. Compare NAPS with alternative domain adaptation methods that directly estimate target distribution Ptarget(y|x) rather than relying on GLS assumptions