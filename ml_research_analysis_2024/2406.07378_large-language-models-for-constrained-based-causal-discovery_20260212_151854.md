---
ver: rpa2
title: Large Language Models for Constrained-Based Causal Discovery
arxiv_id: '2406.07378'
source_url: https://arxiv.org/abs/2406.07378
tags:
- causal
- llms
- graph
- variables
- discovery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using Large Language Models (LLMs) as a substitute
  for domain experts in causal graph generation, framing conditional independence
  queries as prompts to LLMs and employing the PC algorithm with the answers. The
  performance of the LLM-based conditional independence oracle on systems with known
  causal graphs shows high variability.
---

# Large Language Models for Constrained-Based Causal Discovery

## Quick Facts
- arXiv ID: 2406.07378
- Source URL: https://arxiv.org/abs/2406.07378
- Authors: Kai-Hendrik Cohrs; Gherardo Varando; Emiliano Diaz; Vasileios Sitokonstantinou; Gustau Camps-Valls
- Reference count: 18
- Primary result: LLM-based conditional independence oracle shows high variability but demonstrates potential for automated causal graph construction

## Executive Summary
This paper investigates the use of Large Language Models as substitutes for domain experts in generating causal graphs through the PC algorithm. The authors frame conditional independence queries as prompts to LLMs and evaluate their performance on systems with known causal structures. While the approach shows promise, particularly with GPT-4 demonstrating better consistency than GPT-3.5, the performance exhibits significant variability across different graph structures and query types. To address this, the authors propose a voting schema that allows control over false-positive and false-negative rates, though careful calibration is required.

## Method Summary
The authors implement a constrained-based causal discovery framework where LLMs serve as conditional independence oracles. They use the PC algorithm, which requires answering conditional independence queries to construct causal graphs. These queries are formulated as prompts to LLMs, with responses used to determine independence relationships. The method includes a statistical voting schema to improve reliability by aggregating multiple LLM responses and controlling error rates. The approach is evaluated on various benchmark problems with known causal structures to assess accuracy and consistency.

## Key Results
- LLM-based conditional independence oracles show high variability in performance across different graph structures
- GPT-4 demonstrates better consistency and accuracy compared to GPT-3.5
- The proposed voting schema allows control over false-positive and false-negative rates
- LLMs generally exhibit conservative tendencies toward answering independence queries
- The method captures meaningful causal relationships but requires further refinement for general reliability

## Why This Works (Mechanism)
The approach leverages LLMs' ability to reason about conditional dependencies through natural language understanding. By framing causal queries in conversational form, the method bypasses the need for explicit domain expertise while maintaining the logical structure required for causal discovery. The voting schema compensates for individual LLM uncertainty by aggregating multiple responses, effectively creating a statistical filter that reduces random errors and improves overall reliability.

## Foundational Learning
- **Conditional Independence**: The foundation of constraint-based causal discovery; needed to understand which variables can be separated given others
  - Quick check: Can you explain d-separation in simple terms?
- **PC Algorithm**: The primary algorithm for constraint-based causal discovery; needed to understand the iterative structure of causal graph construction
  - Quick check: What are the key steps in the PC algorithm?
- **Causal Graph Structure**: The target output format; needed to evaluate LLM performance against ground truth
  - Quick check: How do you measure similarity between two causal graphs?
- **Statistical Voting**: The aggregation method used to improve reliability; needed to understand error control mechanisms
  - Quick check: How does majority voting reduce false positives in binary classification?
- **Prompt Engineering**: The technique for formulating effective LLM queries; needed to maximize response quality
  - Quick check: What makes a good conditional independence prompt for an LLM?
- **Error Rate Control**: The calibration of false positive/negative tradeoffs; needed to balance discovery completeness vs accuracy
  - Quick check: Why might you prefer controlling false negatives over false positives in causal discovery?

## Architecture Onboarding

Component map: Data/Graph -> PC Algorithm -> LLM Prompts -> LLM Responses -> Voting Schema -> Causal Graph

Critical path: PC Algorithm generates independence queries → LLM oracle answers queries → Voting schema aggregates responses → Causal graph construction

Design tradeoffs: Using LLMs provides flexibility and accessibility but introduces variability; the voting schema improves reliability but increases computational cost and complexity.

Failure signatures: Inconsistent responses across multiple LLM queries, systematic biases toward independence, poor performance on specific graph topologies, sensitivity to prompt phrasing.

First experiments:
1. Test LLM oracle on simple linear Gaussian models with known independence structure
2. Evaluate voting schema performance across different error rate thresholds
3. Compare GPT-3.5 vs GPT-4 performance on identical causal discovery tasks

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Significant variability in LLM performance across different graph structures and query types
- Voting schema requires careful calibration and may increase computational overhead
- Potential biases in LLM training data affecting causal reasoning are not fully explored
- Scalability to larger, more complex causal graphs remains untested
- Context window limitations of LLMs may impact performance on extensive causal structures

## Confidence
- Core methodology and empirical results: Medium
- Comparative analysis between GPT models: Medium
- Generalizability to broader applications: Low
- Long-term reliability and scalability: Low

## Next Checks
1. Test the LLM oracle approach on larger-scale causal graphs (with more than 20 nodes) to evaluate scalability and identify performance bottlenecks.
2. Implement a systematic analysis of how different prompt formulations affect the accuracy of conditional independence judgments, including variations in phrasing, context provided, and reasoning steps requested.
3. Conduct ablation studies to quantify the individual contributions of the voting schema versus the base LLM performance, and determine optimal voting parameters for different types of causal structures.