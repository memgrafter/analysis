---
ver: rpa2
title: Large Language Models are Contrastive Reasoners
arxiv_id: '2403.08211'
source_url: https://arxiv.org/abs/2403.08211
tags:
- answer
- correct
- wrong
- reasoning
- give
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Contrastive prompting (CP) significantly improves LLM reasoning
  by prompting them to generate both correct and incorrect answers before selecting
  the correct one. Zero-shot CP increases accuracy on GSM8K from 35.9% to 88.8% and
  AQUA-RAT from 41.3% to 62.2% using GPT-4, outperforming standard zero-shot and zero-shot
  chain-of-thought prompting on most arithmetic and commonsense reasoning tasks.
---

# Large Language Models are Contrastive Reasoners

## Quick Facts
- arXiv ID: 2403.08211
- Source URL: https://arxiv.org/abs/2403.08211
- Authors: Liang Yao
- Reference count: 40
- Primary result: Contrastive prompting improves LLM reasoning by having models generate both correct and incorrect answers before selecting the correct one

## Executive Summary
This paper introduces contrastive prompting (CP), a novel prompting technique that significantly enhances large language models' reasoning capabilities. The method works by prompting LLMs to generate both correct and incorrect answers to a question, then select the correct one from the generated options. Zero-shot CP dramatically improves performance on arithmetic and commonsense reasoning tasks, achieving state-of-the-art results when combined with few-shot chain-of-thought prompting.

## Method Summary
Contrastive prompting is a technique that leverages LLMs' pre-trained knowledge of correct and incorrect answer patterns by having them generate multiple answer options, including deliberately incorrect ones. The model is prompted to first generate a correct answer, then create incorrect alternatives, and finally select the correct answer from among all generated options. This approach taps into the model's understanding of what makes answers right or wrong, rather than relying solely on direct answer generation. The method can be applied zero-shot or combined with other prompting techniques like chain-of-thought.

## Key Results
- Zero-shot CP increases GSM8K accuracy from 35.9% to 88.8% using GPT-4
- Zero-shot CP improves AQUA-RAT from 41.3% to 62.2% accuracy
- CP outperforms standard zero-shot and zero-shot chain-of-thought prompting on most arithmetic and commonsense reasoning tasks
- CP + few-shot CoT achieves state-of-the-art 91.9% accuracy on GSM8K

## Why This Works (Mechanism)
The paper proposes that contrastive prompting works by leveraging LLMs' pre-trained knowledge of correct and incorrect answer patterns. By forcing the model to generate both correct and incorrect answers, it activates the model's understanding of what distinguishes right from wrong responses. This self-contrastive approach allows the model to use its internal knowledge representation more effectively, as it must evaluate and compare its own generated answers rather than simply producing a single response. The selection step further reinforces this contrastive reasoning process.

## Foundational Learning

**Correct/Incorrect Answer Patterns**: Understanding how LLMs internally represent the distinction between correct and incorrect answers
- Why needed: CP relies on models having pre-trained knowledge of answer quality
- Quick check: Examine model embeddings for correct vs incorrect answer pairs

**Self-Evaluation Capabilities**: LLMs' ability to assess the quality of their own generated content
- Why needed: CP requires models to compare and select among their own answers
- Quick check: Test model's ability to rank its own answers by quality

**Contrastive Learning Principles**: The general concept of learning through comparison of contrasting examples
- Why needed: CP is fundamentally a contrastive reasoning approach
- Quick check: Apply CP to other domains where contrastive learning is established

**Prompt Engineering**: Techniques for structuring prompts to elicit desired model behaviors
- Why needed: CP is a sophisticated prompting method requiring careful prompt design
- Quick check: Test variations in prompt structure and their impact on CP performance

## Architecture Onboarding

**Component Map**: Question -> CP Prompt Generator -> LLM (generates correct + incorrect answers) -> Answer Selector -> Final Answer

**Critical Path**: The CP prompt generation and answer selection steps are critical, as they directly implement the contrastive reasoning mechanism. The quality of generated incorrect answers significantly impacts final performance.

**Design Tradeoffs**: CP trades increased inference time and token usage for improved accuracy. The method requires generating multiple answers per question, increasing computational cost but potentially yielding better results than single-answer approaches.

**Failure Signatures**: CP may fail when generated incorrect answers are too plausible (leading to wrong selection) or too obviously wrong (not providing meaningful contrast). Performance may degrade on tasks where the distinction between correct and incorrect answers is subtle or domain-specific.

**Three First Experiments**:
1. Test CP with different numbers of generated incorrect answers to find the optimal balance between performance and computational cost
2. Compare CP performance across different LLM architectures and sizes to identify generalizability
3. Apply CP to non-arithmetic reasoning tasks to test domain generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains may not generalize beyond tested arithmetic and commonsense reasoning tasks
- Computational overhead from generating multiple answers could limit practical deployment
- Reliability concerns when generated incorrect answers are too plausible or too obviously wrong
- Limited testing across different LLM architectures and sizes

## Confidence

**High Confidence**: The basic mechanism of contrastive prompting is methodologically sound and clearly demonstrated with well-documented implementation details.

**Medium Confidence**: Performance improvements on tested benchmarks are likely real but may be somewhat inflated due to task selection. The claim about leveraging pre-trained knowledge is plausible but not rigorously validated.

**Low Confidence**: Claims about CP's integration with other methods and achieving state-of-the-art performance need more substantiation through comprehensive baseline comparisons.

## Next Checks

1. Apply CP to non-arithmetic reasoning tasks (logical reasoning, causal inference, domain-specific problems) to verify cross-domain generalization

2. Systematically analyze CP failure cases by examining generated incorrect answer quality and correlating failures with specific problem characteristics

3. Measure computational overhead (inference time, token usage, cost) of CP compared to standard prompting and assess practical deployment viability