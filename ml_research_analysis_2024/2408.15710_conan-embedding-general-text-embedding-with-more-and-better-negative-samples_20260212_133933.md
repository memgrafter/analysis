---
ver: rpa2
title: 'Conan-embedding: General Text Embedding with More and Better Negative Samples'
arxiv_id: '2408.15710'
source_url: https://arxiv.org/abs/2408.15710
tags:
- negative
- loss
- text
- hard
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving embedding models
  for text representation and retrieval tasks. The authors propose a novel approach
  called Conan-embedding, which focuses on maximizing the utilization of high-quality
  negative examples during training.
---

# Conan-embedding: General Text Embedding with More and Better Negative Samples

## Quick Facts
- arXiv ID: 2408.15710
- Source URL: https://arxiv.org/abs/2408.15710
- Authors: Shiyu Li; Yang Tang; Shizhe Chen; Xi Chen
- Reference count: 5
- Primary result: Achieved top rank on Chinese MTEB (CMTEB) leaderboard with novel dynamic hard negative mining and Cross-GPU balancing techniques

## Executive Summary
This paper addresses the problem of improving embedding models for text representation and retrieval tasks by focusing on maximizing the utilization of high-quality negative examples during training. The authors propose Conan-embedding, which introduces dynamic hard negative mining that adapts to the model's evolving ability to handle negative examples, and a Cross-GPU balancing Loss that provides more negative examples while balancing batch sizes across multiple tasks. Additionally, they discover that prompt-response pairs from large language models can be used for embedding training. The proposed method significantly enhances embedding model capabilities, achieving top performance on Chinese benchmarks and demonstrating substantial improvements in retrieval and reranking tasks.

## Method Summary
Conan-embedding employs a multi-stage contrastive learning approach using BERT-large as the base model. The training process involves pre-training with InfoNCE loss and In-Batch Negative, followed by fine-tuning with dynamic hard negative mining and Cross-GPU Batch Balance Loss. The dynamic hard negative mining method iteratively replaces easy negative examples with more challenging ones based on score thresholds, while the Cross-GPU Batch Balance Loss distributes tasks across multiple GPUs to incorporate more negative examples while maintaining task balance. The model also incorporates LLM-generated prompt-response pairs as additional training data. Training uses 64 Ascend 910B GPUs for pre-training (138 hours) and 16 GPUs for fine-tuning (13 hours), with the model achieving first place on the Chinese MTEB leaderboard.

## Key Results
- Achieved top rank on Chinese MTEB (CMTEB) leaderboard
- Significant improvements in retrieval and reranking tasks compared to baseline models
- Demonstrated effectiveness in handling more challenging negative examples and enhancing recall capability
- Successfully integrated LLM prompt-response pairs into embedding training pipeline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic hard negative mining improves model performance by adapting to the model's evolving ability to handle negative examples during training.
- Mechanism: The method iteratively mines hard negatives throughout training, replacing those that become too easy for the model to handle. It monitors the scores of negative examples and replaces them when they no longer meet difficulty thresholds.
- Core assumption: The difficulty of negative examples changes as the model's weights are updated during training, making static hard negatives less effective over time.
- Evidence anchors:
  - [abstract]: "we propose dynamic hard negative mining method to expose the model to more challenging negative examples throughout the training process"
  - [section 2.2]: "Since the model's ability to handle preprocessed negative examples evolves during training, we propose dynamic hard negative mining method to expose the model to more challenging negative examples throughout the training process"
  - [corpus]: Weak - only one related paper mentions negative mining but not dynamic adaptation during training

### Mechanism 2
- Claim: Cross-GPU Batch Balance Loss provides more negative examples while balancing batch sizes across multiple tasks, improving training efficiency and effectiveness.
- Mechanism: Distributes retrieval tasks across multiple GPUs to incorporate more negative examples while maintaining task balance. Aggregates losses from different GPUs and tasks to compute a combined loss.
- Core assumption: Sequential random task training creates inconsistencies between the search space optimized in a single iteration and the global search space of the embedding model, causing oscillations.
- Evidence anchors:
  - [abstract]: "we use a Cross-GPU balancing Loss to provide more negative examples for embedding training and balance the batch size across multiple tasks"
  - [section 2.3]: "We consider introducing each task in a balanced manner during each Forward-Loss-Backward-Update cycle to obtain a stable search space"
  - [section 3.5]: "It can be observed that the loss fluctuates significantly, decreases slowly, and does not decrease simultaneously" (showing problem before CBB)

### Mechanism 3
- Claim: Prompt-response pairs from LLMs can be used as effective training data for embedding models, enhancing their performance.
- Mechanism: Utilizes instruction-tuning data from LLMs as additional training pairs, expanding the diversity and quality of training data beyond traditional sources.
- Core assumption: The semantic relationships captured in prompt-response pairs are transferable to the embedding space and can improve the model's ability to distinguish between similar texts.
- Evidence anchors:
  - [abstract]: "we also discovered that the prompt-response pairs from large language models (LLMs) can be used for embedding training"
  - [section 3.2]: "we also discovered that high-quality LLM instruction-tuning data, such as prompt-response pairs, can enhance the performance of embedding models after being filtered and screened according to rules"
  - [corpus]: Weak - no direct evidence in corpus papers about using LLM prompt-response pairs for embedding training

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: The paper builds upon contrastive learning as the foundation for embedding model training, with InfoNCE loss being the primary optimization objective
  - Quick check question: How does InfoNCE loss differ from standard cross-entropy loss in the context of embedding learning?

- Concept: Hard negative mining strategies
  - Why needed here: Understanding static vs. dynamic hard negative mining is crucial for grasping the novelty of the proposed approach
  - Quick check question: What is the key difference between preprocessing-based hard negative mining and the dynamic approach proposed in this paper?

- Concept: Cross-GPU distributed training and batch balancing
  - Why needed here: The Cross-GPU Batch Balance Loss relies on understanding how to distribute tasks across multiple GPUs while maintaining training stability
  - Quick check question: Why might sequential random task assignment across batches lead to training oscillations in multi-task learning scenarios?

## Architecture Onboarding

- Component map: BERT-large base model (326M parameters) with linear layer expanding from 1024 to 1792 dimensions → Matryoshka Representation Learning (MRL) for flexible dimension lengths → Multi-stage contrastive learning with InfoNCE loss → Dynamic hard negative mining → Cross-GPU Batch Balance Loss → Task-specific fine-tuning losses
- Critical path: Data preprocessing → Pre-training with InfoNCE loss → Dynamic hard negative mining → Cross-GPU Batch Balance Loss application → Fine-tuning with task-specific losses → Evaluation on CMTEB
- Design tradeoffs: Using more GPUs for negative examples increases computational cost but improves model performance; dynamic hard negative mining adds computational overhead but provides better training data; MRL adds flexibility but increases model complexity.
- Failure signatures: Loss curves showing oscillations (indicating Cross-GPU balancing issues), plateauing negative example scores (indicating ineffective dynamic mining), or poor performance on specific CMTEB tasks (indicating data quality or task balance problems).
- First 3 experiments:
  1. Implement static hard negative mining baseline and measure performance drop compared to dynamic approach
  2. Test Cross-GPU Batch Balance Loss with different GPU allocation ratios to find optimal balance
  3. Evaluate the impact of different proportions of LLM prompt-response pairs in the training data mixture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dynamic hard negative mining strategy perform when applied to other embedding model architectures beyond BERT large?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of dynamic hard negative mining with the Conan-embedding model based on BERT large. However, it does not explore the strategy's performance with other architectures.
- Why unresolved: The paper focuses on a specific model architecture and does not provide comparative results across different architectures.
- What evidence would resolve it: Experiments comparing the performance of dynamic hard negative mining across various embedding model architectures, such as RoBERTa, DistilBERT, or other transformer-based models, would provide insights into its generalizability.

### Open Question 2
- Question: What is the impact of the dynamic hard negative mining frequency on the model's performance and training efficiency?
- Basis in paper: [explicit] The paper mentions that dynamic hard negative mining is performed every 100 iterations, but does not explore the effects of different frequencies.
- Why unresolved: The chosen frequency of 100 iterations may not be optimal for all scenarios, and the paper does not investigate the trade-offs between different frequencies.
- What evidence would resolve it: Conducting experiments with varying frequencies of dynamic hard negative mining and analyzing the resulting performance and training efficiency would help determine the optimal frequency for different tasks and datasets.

### Open Question 3
- Question: How does the Conan-embedding model perform on cross-lingual retrieval tasks, especially for low-resource languages?
- Basis in paper: [inferred] The paper demonstrates the model's performance on Chinese benchmarks (CMTEB) but does not explore its effectiveness in cross-lingual scenarios or low-resource languages.
- Why unresolved: The model's architecture and training data are primarily focused on Chinese language tasks, and its performance in cross-lingual or low-resource language settings is not evaluated.
- What evidence would resolve it: Evaluating the Conan-embedding model on cross-lingual retrieval benchmarks and low-resource language datasets would provide insights into its effectiveness in multilingual scenarios and its potential for broader applications.

## Limitations
- Limited empirical evidence with sparse ablation studies showing individual contributions of each innovation
- Lack of detailed implementation specifications for key mechanisms (dynamic hard negative mining parameters, Cross-GPU Batch Balance Loss formula)
- Focus on Chinese language benchmarks without extensive testing on English or multilingual tasks

## Confidence

**High Confidence**:
- The overall framework of using contrastive learning with BERT-large as the base model is well-established and technically sound
- The use of InfoNCE loss for embedding training is a standard approach with proven effectiveness
- The observation that hard negative examples improve embedding quality is supported by prior literature

**Medium Confidence**:
- The dynamic hard negative mining approach shows promise but lacks detailed implementation specifications
- The Cross-GPU Batch Balance Loss addresses a real problem in multi-task training, though the exact solution could benefit from more rigorous analysis
- The use of LLM prompt-response pairs for embedding training is an interesting hypothesis but requires more empirical validation

**Low Confidence**:
- The claim that this approach achieves the "top rank" on CMTEB without providing direct comparison metrics to other methods
- The assertion that all three innovations work synergistically without clear ablation evidence
- The generalizability of results to non-Chinese language tasks

## Next Checks

1. **Ablation Study Implementation**: Conduct controlled experiments to measure the individual and combined contributions of dynamic hard negative mining, Cross-GPU Batch Balance Loss, and LLM prompt-response pairs. This would involve training versions with only one innovation active at a time and comparing performance on CMTEB tasks.

2. **Cross-Lingual Generalization Test**: Evaluate the Conan-embedding model on English and multilingual benchmarks (e.g., English MTEB, BEIR) to assess whether the performance gains observed on Chinese tasks transfer to other languages and domains.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary the key hyperparameters (1.15 multiplier for dynamic mining, 0.8 score threshold, GPU allocation ratios) to understand their impact on training stability and final performance. This would help establish more robust guidelines for applying these techniques in different settings.