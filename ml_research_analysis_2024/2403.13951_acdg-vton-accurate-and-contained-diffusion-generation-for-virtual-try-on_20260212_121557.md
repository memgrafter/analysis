---
ver: rpa2
title: 'ACDG-VTON: Accurate and Contained Diffusion Generation for Virtual Try-On'
arxiv_id: '2403.13951'
source_url: https://arxiv.org/abs/2403.13951
tags:
- garment
- image
- details
- diffusion
- garments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of maintaining garment detail
  accuracy in diffusion-based virtual try-on (VTON) systems. While diffusion models
  can generate high-quality images, they often hallucinate garment details not present
  in the input, reducing accuracy.
---

# ACDG-VTON: Accurate and Contained Diffusion Generation for Virtual Try-On

## Quick Facts
- **arXiv ID**: 2403.13951
- **Source URL**: https://arxiv.org/abs/2403.13951
- **Reference count**: 40
- **Key outcome**: Diffusion-based VTON system that maintains garment detail accuracy while outperforming state-of-the-art baselines on LPIPS (0.0579), SSIM (0.9281), FID (14.750), and KID (1.940)

## Executive Summary
This paper addresses the problem of maintaining garment detail accuracy in diffusion-based virtual try-on (VTON) systems. While diffusion models can generate high-quality images, they often hallucinate garment details not present in the input, reducing accuracy. The authors propose a novel training scheme that limits diffusion's scope by using a control image perfectly aligned with the target image during training. This approach preserves garment details during inference while maintaining the quality advantages of diffusion models. The method introduces a "simulated incomplete image" training approach where garment features are perfectly aligned with ground truth images, preventing hallucination. It also supports multi-garment layering, styling options, and shoe try-on in a single inference cycle.

## Method Summary
ACDG-VTON is a diffusion-based virtual try-on system that addresses the accuracy problem in diffusion models by using perfectly aligned control images during training. The method employs a two-stage process: first creating a control image with warped garments overlaid on the model (with skin regions filled with a constant value), then running diffusion on this incomplete image to generate the final output. The key innovation is the "simulated incomplete image" training approach where garment features are perfectly aligned with ground truth images, preventing the diffusion model from hallucinating corrections. The system uses a ControlNet-based architecture with a CLIP image encoder and supports high-resolution zoomed-in images through random cropping and upsampling during training.

## Key Results
- Achieves state-of-the-art performance on standard metrics: LPIPS (0.0579), SSIM (0.9281), FID (14.750), and KID (1.940)
- Outperforms existing diffusion-based methods (LaDI-VTON, StableVITON, GP-VTON) in both accuracy and quality according to user studies
- Successfully balances the accuracy of GAN-based methods with the quality of diffusion-based methods while adding garment controllability
- Supports multi-garment layering, styling options, and shoe try-on in a single inference cycle
- Can generate high-resolution zoomed-in images without requiring higher resolution training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models hallucinate garment details when trained with misaligned control images.
- Mechanism: During training, if the control image (warped garment) is not perfectly aligned with the target image (ground truth), the diffusion model interprets small misalignments as errors to fix, leading it to hallucinate details to "correct" these perceived problems.
- Core assumption: Small misalignments between control and target images are interpreted by diffusion models as training signals to modify garment details.
- Evidence anchors:
  - [abstract]: "We identified this problem stems from the specifics in the training formulation for diffusion."
  - [section 4.2]: "Rather than using warped garment images, we craft a control image that is perfectly aligned with the target image to limit the scope offered to the diffusion model, leading to an improvement in accuracy."
  - [corpus]: Weak evidence - related papers focus on diffusion-based VTON but don't explicitly discuss misalignment as a hallucination trigger.

### Mechanism 2
- Claim: Simulated incomplete images preserve garment details by perfectly aligning high-frequency features during training.
- Mechanism: By removing details from the ground truth image rather than using warped garments, the resulting simulated incomplete image has perfectly aligned high-frequency features (text, patterns, textures) with the ground truth, preventing the diffusion model from hallucinating corrections.
- Core assumption: Perfect alignment of high-frequency features during training prevents the diffusion model from interpreting these as errors to fix.
- Evidence anchors:
  - [section 4.2]: "The key to obtaining good accuracy is ensuring the diffusion model does not hallucinate unnecessary details... we train the diffusion model with simulated mi in which the garment features are perfectly aligned."
  - [section 4.2]: "The simulated incomplete image si is created by removing detail from the ground truth model image mg rather than using garment warps."
  - [corpus]: Moderate evidence - PFDM and Texture-Preserving Diffusion papers suggest alignment is important but don't use this specific simulated incomplete image approach.

### Mechanism 3
- Claim: Control initialization improves training-inference consistency, reducing artifacts.
- Mechanism: By using the noisy control image as initialization during both training and inference (rather than pure noise during inference), the model avoids the distribution shift that causes unexpected artifacts.
- Core assumption: Training and inference initialization distributions must match for stable diffusion generation.
- Evidence anchors:
  - [section 4.2]: "we adapt their procedure by using our control image si as initialization... We call this modified procedure Control Initialization, where we use our noisy control image as the initialization during training and inference."
  - [section 4.2]: "In Zhang et al. [39], the authors revealed that the difference in training initialization (ground truth image + noise) and inference (pure Gaussian noise) led to unexpected artifacts in inference generations."
  - [corpus]: Weak evidence - related papers don't discuss initialization consistency as a primary mechanism.

## Foundational Learning

- Concept: Variational Autoencoder (VAE) and its limitations for high-frequency details
  - Why needed here: Understanding why VAEs distort fine details like text and patterns is crucial for grasping the need for HR Zoom and the alignment approach
  - Quick check question: Why does a VAE struggle to preserve text details in images, and how does the HR Zoom method overcome this limitation?

- Concept: ControlNet architecture and cross-attention mechanisms
  - Why needed here: ACDG-VTON builds on ControlNet by using a control image and CLIP embeddings as conditions, requiring understanding of how these components interact
  - Quick check question: How does feeding the control image through a CLIP encoder and using it as a condition differ from standard ControlNet implementations?

- Concept: Diffusion model training and noise prediction
  - Why needed here: The core mechanism relies on training a diffusion model to denoise, so understanding the noise prediction objective and timestep handling is essential
- Quick check question: Explain how the noise prediction objective works in diffusion models and why it's effective for image generation.

## Architecture Onboarding

- Component map:
  - Semantic Layout Generator (H) -> Warper (W) -> Reverse Warp Network (U) -> Diffusion Denoiser (F) -> Final Output
  - CLIP Image Encoder -> Control Image Condition
  - SMPL Joints Image -> Skin Generation Guide

- Critical path: Garment → Warper → Control Image Creation → Diffusion Denoiser → Final Output
- Design tradeoffs:
  - Alignment vs. Realism: Perfect alignment prevents hallucination but may reduce natural variations
  - Training Complexity: Simulated incomplete images require training an additional U-Net
  - Resolution: HR Zoom enables high-resolution output without high-res training but adds inference complexity

- Failure signatures:
  - Misaligned text/patterns: Indicates problems with U-Net training or alignment process
  - Artifacts on skin/garment boundaries: Suggests issues with layout prediction or control initialization
  - Loss of garment identity: Points to problems with the diffusion model's conditioning or training procedure

- First 3 experiments:
  1. Compare outputs using perfect alignment vs. warped garments as control images to verify hallucination mechanism
  2. Test different skip step values (S parameter) in control initialization to find optimal balance
  3. Evaluate HR Zoom effectiveness by comparing VAE-encoded regions with and without zoom augmentation

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the limitations and scope of the work, several important questions remain unaddressed:

### Open Question 1
- Question: Can the control image training approach generalize to other image-to-image translation tasks beyond virtual try-on, and what are the limitations of this method for different types of image transformations?
- Basis in paper: [inferred] The paper demonstrates that using perfectly aligned control images during training prevents hallucination in diffusion models for virtual try-on. This approach could theoretically be applied to other image-to-image tasks where maintaining specific details from a reference image is important.
- Why unresolved: The paper only tests this approach on virtual try-on tasks. It's unclear whether the same training scheme would work for tasks like image editing, style transfer, or other transformations where the relationship between input and output is different.
- What evidence would resolve it: Testing the method on diverse image-to-image tasks (e.g., image editing, super-resolution, style transfer) and comparing results with standard diffusion training would show whether the approach generalizes or has limitations for different transformation types.

### Open Question 2
- Question: How does the simulated incomplete image training method perform when applied to garments with complex textures, patterns, or logos that have high-frequency details beyond what the VAE can encode?
- Basis in paper: [explicit] The paper notes that VAEs can distort high-frequency details and that their HR Zoom method helps overcome this by cropping and upsampling regions. However, it's unclear how well the method works for extremely complex patterns.
- Why unresolved: The paper demonstrates success with various patterns but doesn't test the limits of the approach with extremely detailed or complex designs. The performance threshold for this method remains unknown.
- What evidence would resolve it: Testing the method on garments with increasingly complex patterns (fine text, intricate embroidery, complex geometric patterns) and measuring accuracy degradation would establish the method's limits and determine when additional techniques are needed.

### Open Question 3
- Question: What is the computational overhead of the control image training approach compared to standard diffusion training, and how does this affect real-time virtual try-on applications?
- Basis in paper: [inferred] The paper introduces additional training steps (creating simulated incomplete images, training the U-Net for reverse warps) that aren't present in standard diffusion training. While inference speed is mentioned as single-cycle, the training complexity isn't analyzed.
- Why unresolved: The paper doesn't provide timing comparisons or computational cost analysis between their method and standard approaches, making it difficult to assess practical deployment implications.
- What evidence would resolve it: A detailed comparison of training time, GPU memory usage, and inference latency between the proposed method and baseline approaches would quantify the computational overhead and determine if the accuracy/quality improvements justify the additional cost.

## Limitations
- Limited to upper-body garments due to DressCode dataset constraints; effectiveness on lower-body and full-body cases untested
- Requires training an additional U-Net to create reverse warps, adding training complexity
- Computational overhead of the training procedure not quantified or compared to baseline methods
- User study methodology lacks detail on sample size and statistical significance

## Confidence
- **High Confidence**: The method achieves state-of-the-art metrics on standard VTON benchmarks (LPIPS, SSIM, FID, KID). The architectural framework (ControlNet with CLIP encoder) is well-established.
- **Medium Confidence**: The core hypothesis about hallucination being caused by training misalignment is supported by logical reasoning but lacks direct experimental validation through ablation studies.
- **Low Confidence**: Claims about generalization to lower-body garments and full-body cases are unsupported by experimental evidence in the paper.

## Next Checks
1. **Ablation Study**: Compare training with perfectly aligned control images vs. warped garments as control to directly test the hallucination hypothesis and quantify the alignment effect.
2. **Multi-Garment Stress Test**: Evaluate ACDG-VTON on datasets with multiple garments (e.g., dress + jacket) to verify the claimed multi-garment capability and identify failure modes.
3. **Resolution Scaling Analysis**: Test HR Zoom effectiveness across different resolution scales (512x384, 1024x768, 2048x1536) to determine if the benefits scale proportionally with image size.