---
ver: rpa2
title: Your Finetuned Large Language Model is Already a Powerful Out-of-distribution
  Detector
arxiv_id: '2404.08679'
source_url: https://arxiv.org/abs/2404.08679
tags:
- detection
- data
- language
- llama-13b
- llama-7b
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that the likelihood ratio between a pretrained
  large language model (LLM) and its finetuned variant serves as an effective out-of-distribution
  (OOD) detection criterion. The method leverages the pretrained LLM as an OOD proxy,
  assuming it contains general knowledge about OOD data.
---

# Your Finetuned Large Language Model is Already a Powerful Out-of-distribution Detector

## Quick Facts
- arXiv ID: 2404.08679
- Source URL: https://arxiv.org/abs/2404.08679
- Reference count: 17
- This paper demonstrates that likelihood ratios between pretrained and finetuned LLMs serve as effective OOD detection criteria, particularly for QA systems.

## Executive Summary
This paper presents a novel approach to out-of-distribution (OOD) detection using the likelihood ratio between a pretrained large language model (LLM) and its finetuned variant. The method leverages the pretrained LLM as an OOD proxy, assuming it contains general knowledge about language distributions that encompasses OOD data. The approach is particularly effective for detecting OOD questions in QA systems by generating answers and analyzing question-answer pairs. Experiments show strong performance across far OOD, near OOD, spam detection, and QA scenarios, with the likelihood ratio method achieving near-perfect AUROC scores in far OOD detection and outperforming existing unsupervised methods in near OOD and spam detection tasks.

## Method Summary
The method computes the likelihood ratio between a pretrained LLM and its finetuned variant as an OOD detection criterion. For input x, the detection score is S(x) = p_pretrained(x)/p_finetuned(x), where p_pretrained and p_finetuned are the likelihoods assigned by the respective models. The pretrained LLM serves as an OOD proxy, while the finetuned model represents the in-distribution. For QA systems, the method generates answers and computes likelihood ratios on question-answer pairs using multiple criteria (Sq, Sa, Sq,a, Sa|q). The models are finetuned using LoRA with rank=16, alpha=32, and dropout=0.05. Evaluation uses AUROC, AUPR, and FPR95 metrics across multiple datasets.

## Key Results
- Likelihood ratio method achieves near-perfect AUROC scores (>0.99) in far OOD detection scenarios
- Outperforms existing unsupervised methods in near OOD detection and spam detection tasks
- For OOD question detection in QA, Sa criterion consistently achieves AUROC values above 0.5
- 13B parameter models show better performance than 7B parameter models
- Raw likelihood methods show strong performance in spam detection when spam data deviates from natural language patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Likelihood ratio between pretrained LLM and finetuned variant serves as effective OOD detection criterion
- Mechanism: Pretrained LLM acts as OOD proxy containing general knowledge about language distribution; finetuned model learns in-distribution patterns. When OOD data appears, pretrained LLM assigns higher likelihood than finetuned model, creating detectable ratio.
- Core assumption: Pretrained LLM's training data encompasses general human language patterns sufficiently to represent OOD distribution
- Evidence anchors:
  - [abstract] "The intuition behind such a criterion is that, the pretrained LLM has the prior knowledge about OOD data due to its large amount of training data"
  - [section 3] "Given their extensive parameters and training on vast corpora... it is plausible to consider that an LLM encompasses the breadth of human language"
  - [corpus] Weak - corpus papers focus on classifier-based likelihood OOD detection, not LLM-based ratio methods

### Mechanism 2
- Claim: In QA systems, OOD questions can be detected through question-answer pair likelihood ratios
- Mechanism: Finetuned LLM generates reasonable answers for in-distribution questions but produces nonsensical outputs for OOD questions. By evaluating likelihood ratios on both question and generated answer, OOD detection becomes possible even for short, uninformative questions.
- Core assumption: Finetuned LLM's response quality correlates with input being in-distribution
- Evidence anchors:
  - [section 4] "for LLM-based question-answering (QA) systems, the same likelihood ratio excels in detecting OOD questions"
  - [section 4] "while a finetuned LLM generates pertinent answers to in-distribution questions, it tends to produce unreasonable sentences in response to OOD questions"
  - [corpus] No direct corpus evidence - this appears to be novel methodology

### Mechanism 3
- Claim: Likelihood ratio performs better than raw likelihood for OOD detection
- Mechanism: Raw likelihood tends to assign higher values to shorter sequences regardless of semantic content. Ratio normalizes this bias by comparing relative likelihoods between pretrained and finetuned models.
- Core assumption: Language models exhibit systematic bias toward shorter sequences in likelihood assignment
- Evidence anchors:
  - [section 6] "it becomes apparent that the texts from the 20 Newsgroups (20NG) dataset are significantly longer than those from the comparative OOD datasets, especially in instances where the AUROC is notably low"
  - [section 6] "This observation reveals a tendency among language models to assign higher likelihoods to shorter sentences, irrespective of their actual semantic content"
  - [corpus] No corpus evidence supporting this specific claim about length bias

## Foundational Learning

- Concept: Autoregressive language modeling and conditional probability
  - Why needed here: Method relies on computing p(x) = p(x1)p(x2|x1)p(x3|x1,x2)... for both pretrained and finetuned models
  - Quick check question: How would you compute the likelihood of sentence "the cat sat" using an autoregressive model?

- Concept: Likelihood ratio as statistical test statistic
  - Why needed here: Detection criterion S(x) = pθ(x)/pθ'(x) is fundamentally a likelihood ratio test
  - Quick check question: What is the Neyman-Pearson lemma and why does it justify using likelihood ratios for detection?

- Concept: Out-of-distribution detection evaluation metrics
  - Why needed here: Method is evaluated using AUROC, AUPR, FPR95 which require understanding of classification thresholds and true/false positive tradeoffs
  - Quick check question: How does AUROC differ from AUPR in evaluating detection performance?

## Architecture Onboarding

- Component map: Pretrained LLM -> Finetuned LLM -> Likelihood computation -> Ratio computation -> OOD decision
- Critical path:
  1. Load pretrained and finetuned LLM models
  2. For input x, compute p_pretrained(x) and p_finetuned(x)
  3. Calculate ratio S(x)
  4. Compare S(x) to threshold for OOD decision
  5. For QA: generate answer, compute Sa, Sq, Sq,a, Sa|q metrics

- Design tradeoffs:
  - Model size vs. detection accuracy: Larger models show better performance (13B > 7B)
  - Computational cost vs. real-time capability: Likelihood computation is expensive
  - Ratio stability vs. numerical precision: Very small/large likelihood values can cause numerical issues
  - Pretrained model selection: Different base models may yield different detection characteristics

- Failure signatures:
  - AUROC close to 0.5 indicates random guessing (seen in some raw likelihood cases)
  - Extremely high FPR95 suggests poor discrimination ability
  - Ratio values clustering near 1.0 indicates insufficient model differentiation
  - Numerical overflow/underflow in likelihood computation

- First 3 experiments:
  1. Far OOD detection: Compare 20NG vs SST-2 using Llama-7B and Llama-13B models, measure AUROC
  2. Near OOD detection: Use CLINC150 dataset with 150 vs 50 classes, evaluate unsupervised vs supervised methods
  3. QA OOD detection: Test MetaMath model on GSM8K (in) vs BoolQ (out), compare Sq, Sa, Sq,a, Sa|q criteria

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of likelihood ratio OOD detection scale with model size beyond 13B parameters?
- Basis in paper: [explicit] The paper mentions that "our study provides a more comprehensive analysis of likelihood ratio between base models and fine-tuned models using much larger LLMs" and references previous work with smaller models (GPT-2), but does not test beyond 13B parameters.
- Why unresolved: The paper only tests Llama-7B and Llama-13B models, leaving uncertainty about whether the strong performance continues to scale with even larger models.
- What evidence would resolve it: Testing the likelihood ratio method with models like Llama-33B, GPT-3.5, or GPT-4 to establish whether the trend of improved OOD detection with larger models continues.

### Open Question 2
- Question: Why does the likelihood-based method (LH) sometimes outperform the likelihood ratio (LR) method in spam detection, contrary to general trends?
- Basis in paper: [explicit] The paper notes that "in the spam detection experiments detailed in Table 3, particularly with data from the SMS and SpamAssassin datasets, we observe that without spam data, the likelihood (LH) method outperforms the likelihood ratio (LR)."
- Why unresolved: The paper suggests this may be due to spam messages containing "intentionally misspelled words to circumvent detection mechanisms" or being "highly structured in email and data transaction formats," but does not provide empirical verification of this hypothesis.
- What evidence would resolve it: Controlled experiments with different types of OOD data (natural language vs. structured vs. deliberately corrupted) to test whether LH outperforms LR specifically for non-natural language OOD distributions.

### Open Question 3
- Question: How sensitive is the likelihood ratio method to the quality and representativeness of the finetuning dataset?
- Basis in paper: [inferred] The method relies on a finetuned LLM that represents the "distribution of their specific datasets," but the paper does not systematically investigate how dataset quality, size, or domain specificity affects OOD detection performance.
- Why unresolved: While the paper demonstrates strong performance, it does not explore whether the finetuning dataset must be large, clean, and representative, or whether the method degrades gracefully with imperfect finetuning data.
- What evidence would resolve it: Experiments varying finetuning dataset size, quality, and domain coverage to measure the impact on OOD detection accuracy and establish practical requirements for finetuning data.

## Limitations
- Performance degrades when in-distribution and OOD datasets have significant length discrepancies
- Method may not generalize well to non-natural language OOD distributions (e.g., structured data, intentionally corrupted text)
- Numerical stability issues can arise when computing likelihood ratios for very long sequences

## Confidence
- **High Confidence**: The likelihood ratio outperforms raw likelihood for OOD detection, and the method's effectiveness on far OOD detection scenarios is well-demonstrated with near-perfect AUROC scores. The core observation that finetuned models assign lower likelihoods to OOD data than pretrained models is consistently supported across experiments.
- **Medium Confidence**: The assumption that pretrained LLMs serve as effective OOD proxies across diverse domains, and the QA-specific methodology for OOD question detection through question-answer pair analysis. While results are promising, the generality of these claims across different LLM architectures and training distributions needs further validation.
- **Low Confidence**: The explanation for why length bias affects detection performance, and the assertion that the method works for "near OOD" detection comparably to supervised methods. The paper shows mixed results for near OOD scenarios, with performance sometimes falling below supervised baselines.

## Next Checks
1. **Numerical Stability Analysis**: Systematically test the likelihood ratio computation on sequences of varying lengths (100-10000 tokens) to identify thresholds where numerical underflow/overflow occurs, and evaluate whether log-space computations provide stable alternatives.
2. **Pretrained Model Dependency**: Repeat the far OOD experiments using different pretrained LLM architectures (e.g., GPT-2, OPT, Mistral) to determine whether the detection performance is consistent across model families or specific to Llama-based models.
3. **Near OOD Generalization**: Design a controlled experiment varying the semantic similarity between in-distribution and OOD classes (using similarity metrics like cosine distance on embeddings) to quantify exactly how "near" near-OOD can be before the likelihood ratio method breaks down.