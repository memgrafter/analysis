---
ver: rpa2
title: The Hidden Pitfalls of the Cosine Similarity Loss
arxiv_id: '2406.16468'
source_url: https://arxiv.org/abs/2406.16468
tags:
- cosine
- similarity
- learning
- loss
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work shows that optimizing the cosine similarity loss in self-supervised
  learning models can cause embedding norms to grow and gradients to vanish, especially
  for large embeddings or those on opposite sides of latent space. These effects can
  slow convergence quadratically.
---

# The Hidden Pitfalls of the Cosine Similarity Loss

## Quick Facts
- arXiv ID: 2406.16468
- Source URL: https://arxiv.org/abs/2406.16468
- Reference count: 40
- Primary result: Cut-initialization improves k-NN accuracy by 5-15% across SSL methods by preventing embedding norm growth

## Executive Summary
This paper identifies a fundamental issue with cosine similarity loss in self-supervised learning: optimizing it causes embedding norms to grow, leading to vanishing gradients and slow convergence. The authors prove that gradient descent on cosine similarity inherently increases embedding magnitudes, and show this effect is more pronounced for large embeddings or those on opposite sides of latent space. They propose cut-initialization—dividing layer weights by a constant >1 at initialization—as a simple fix that improves convergence across all studied SSL methods. Extensive experiments on CIFAR-10/100, ImageNet-100, and Flowers102 demonstrate consistent accuracy improvements of 5-15% for SimCLR, SimSiam, BYOL, MoCo-v2/v3, and Dino architectures.

## Method Summary
The authors propose cut-initialization as a simple modification to weight initialization that divides layer weights by a constant c > 1. For contrastive methods (SimCLR, MoCo), they use c=3, while for non-contrastive methods (SimSiam, BYOL) they use c=9. This reduces initial embedding norms, preventing early gradient vanishing. The method is combined with ℓ2-normalization and weight decay for optimal results. The approach works by keeping initial gradients large enough to enable faster convergence, as the cosine similarity gradient magnitude is inversely proportional to embedding norms.

## Key Results
- Cut-initialization consistently improves k-NN classification accuracy by 5-15% across all tested SSL methods
- The method works best when combined with ℓ2-normalization and weight decay
- Embedding norms grow during training with standard initialization, causing gradients to vanish
- The opposite-halves effect (positive pairs with angles > π/2) is negligible in high dimensions and resolves naturally during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing cosine similarity forces embedding norms to grow during training, causing gradients to vanish
- Mechanism: When embedding norms increase, the gradient magnitude for cosine similarity becomes inversely proportional to the norm, leading to slower updates. The optimization process inherently pushes norms upward due to the geometry of the cosine similarity function
- Core assumption: Training dynamics follow standard gradient descent and loss is based on cosine similarity between positive pairs
- Evidence anchors:
  - [abstract] "we prove that optimizing the cosine similarity between points forces them to grow in magnitude"
  - [section] "Corollary 2... ||z′i|| ≥ || zi||" after gradient descent step
  - [corpus] "On the Importance of Embedding Norms in Self-Supervised Learning" suggests this is a known concern

### Mechanism 2
- Claim: Cut-initialization reduces initial embedding norms, preventing early-stage gradient vanishing and improving convergence speed
- Mechanism: By dividing layer weights by a constant >1 at initialization, initial embeddings have smaller norms. This keeps gradients from vanishing early in training, allowing faster convergence to better representations
- Core assumption: Smaller initial norms lead to larger initial gradients, accelerating early training stages
- Evidence anchors:
  - [abstract] "propose cut-initialization... helps all studied SSL methods converge faster"
  - [section] "cut-initialization improves convergence across all settings when paired with ℓ2-normalization"
  - [corpus] "Beyond Cosine Similarity: Magnitude-Aware CLIP for No-Reference Image Quality Assessment" shows magnitude-aware methods can outperform standard cosine similarity

### Mechanism 3
- Claim: The opposite-halves effect is negligible in high dimensions and does not significantly impact convergence
- Mechanism: In high-dimensional spaces, the probability of two random vectors having an angle greater than π/2 is extremely low. Any such occurrences early in training are temporary and do not meaningfully slow convergence
- Core assumption: Data distribution and training dynamics naturally avoid large angles between positive pairs as training progresses
- Evidence anchors:
  - [section] "Table 2 shows embeddings have angle > π/2 at 5% and 25% for SimCLR and SimSiam/BYOL... However, very early into training (epoch 16), every method has a rate of effectively 0"
  - [section] "Proposition 5... the likelihood that its value exceeds 1 − ε is twice the likelihood that its value exceeds 1 − ε"
  - [corpus] No direct evidence found in related papers about the opposite-halves effect

## Foundational Learning

- Concept: Gradient descent optimization dynamics
  - Why needed here: Understanding how embedding norms affect gradient magnitude is central to the paper's claims about convergence slowdown
  - Quick check question: If an embedding norm doubles, by what factor does the gradient magnitude for cosine similarity change?

- Concept: Self-supervised learning loss functions (InfoNCE, BYOL, SimSiam)
  - Why needed here: The paper claims the phenomenon affects all SSL methods using cosine similarity, so understanding these losses is essential
  - Quick check question: How does the InfoNCE loss differ from simple cosine similarity loss, and why does this matter for the paper's claims?

- Concept: Weight initialization and its impact on training dynamics
  - Why needed here: Cut-initialization is a simple modification to weight initialization, so understanding initialization schemes is crucial
  - Quick check question: What happens to initial activations if you scale all weights by a constant factor at initialization?

## Architecture Onboarding

- Component map: Encoder network -> Projection head (for SimSiam/BYOL) -> Loss function implementation
- Critical path: Forward pass → embedding generation → cosine similarity calculation → loss computation → backward pass with gradients → parameter update
- Design tradeoffs: Larger cut-constants lead to faster early convergence but may require careful tuning of weight decay. Smaller cut-constants have less impact but are safer. The optimal value depends on the specific SSL method and dataset
- Failure signatures: If cut-initialization is too aggressive (c too large), training may fail to converge or produce poor representations. If too conservative (c close to 1), the benefits will be minimal
- First 3 experiments:
  1. Compare training curves (loss and accuracy) with and without cut-initialization on CIFAR-10 using SimCLR
  2. Measure embedding norms during training with different cut-constants to verify the mechanism
  3. Test the effect of combining cut-initialization with different weight decay values to find optimal combinations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does cut-initialization interact with batch normalization in non-contrastive methods like SimSiam and BYOL?
- Basis in paper: [explicit] The paper notes that batch normalization causes initial concentration of embeddings at antipodal points in non-contrastive methods, leading to higher rates of opposite-halves effect at epoch 1. Cut-initialization is proposed to address embedding norm growth, but its specific interaction with batch normalization is not explored
- Why unresolved: The paper focuses on embedding norm growth and convergence benefits of cut-initialization but does not investigate how it modifies or mitigates the batch normalization-induced clustering at initialization
- What evidence would resolve it: Experiments comparing k-NN accuracy and embedding norm trajectories for SimSiam/BYOL with and without cut-initialization across different batch normalization settings

### Open Question 2
- Question: What is the optimal cut-constant c for different backbone architectures and dataset complexities?
- Basis in paper: [inferred] The paper uses c=3 for contrastive methods and c=9 for non-contrastive methods, but this is heuristic. It does not systematically explore the relationship between c, backbone capacity, or dataset size
- Why unresolved: The chosen cut-constants are based on empirical tuning on a few datasets and models. There is no theoretical or empirical justification for these values across different architectures or dataset complexities
- What evidence would resolve it: A comprehensive ablation study varying c across backbones (e.g., ResNet, ViT, ConvNeXt) and datasets of varying scale and complexity, measuring convergence speed and final accuracy

### Open Question 3
- Question: Does cut-initialization improve the downstream task generalization of SSL representations beyond k-NN accuracy?
- Basis in paper: [explicit] The paper evaluates k-NN accuracy as the primary metric for representation quality. It does not report linear probe, fine-tuning, or transfer learning performance
- Why unresolved: k-NN accuracy is a proxy for representation quality but does not capture generalization to downstream tasks that may require linear or nonlinear classifiers, or adaptation to new domains
- What evidence would resolve it: Experiments reporting linear probe accuracy, fine-tuning performance on held-out tasks, and transfer learning results for models trained with and without cut-initialization

## Limitations

- The optimal cut-constant appears method-dependent (3 for contrastive, 9 for non-contrastive) and may require tuning for new architectures
- Theoretical claims about norm growth rely on idealized gradient descent assumptions that may not hold in practice
- The claim that the opposite-halves effect is negligible is supported by limited empirical evidence (5-25% occurrence early in training)

## Confidence

- **High**: Cut-initialization consistently improves convergence and k-NN accuracy across diverse SSL methods and datasets; the mechanism of norm-induced gradient vanishing is mathematically proven
- **Medium**: The optimal cut-constant values (3 for contrastive, 9 for non-contrastive) are empirically determined and may require adjustment for different architectures or datasets
- **Medium**: The claim that the opposite-halves effect is negligible is supported by empirical data but lacks theoretical justification for why angles > π/2 naturally resolve during training

## Next Checks

1. **Norm Growth Verification**: Implement gradient tracking to measure the actual gradient magnitudes for cosine similarity loss as embedding norms increase during training with and without cut-initialization
2. **Cut-Constant Sensitivity**: Systematically test cut-initialization across a wider range of constants (1.5 to 15) on CIFAR-10/100 to determine if the method-dependent optimal values generalize to other architectures
3. **Low-Dimensional Analysis**: Repeat the opposite-halves effect experiment in controlled low-dimensional embedding spaces (2D, 3D) to verify whether the effect becomes significant when high-dimensionality cannot "hide" it