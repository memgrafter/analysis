---
ver: rpa2
title: Enhancing Cancer Diagnosis with Explainable & Trustworthy Deep Learning Models
arxiv_id: '2412.17527'
source_url: https://arxiv.org/abs/2412.17527
tags:
- data
- feature
- cancer
- features
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The research addresses the critical challenge of improving cancer
  diagnosis through the development of an explainable artificial intelligence (XAI)
  model. By leveraging deep learning techniques, specifically Convolutional Neural
  Networks (CNNs), the study achieved a 92% classification accuracy on a breast cancer
  dataset of 569 cases.
---

# Enhancing Cancer Diagnosis with Explainable & Trustworthy Deep Learning Models

## Quick Facts
- arXiv ID: 2412.17527
- Source URL: https://arxiv.org/abs/2412.17527
- Authors: Badaru I. Olumuyiwa; The Anh Han; Zia U. Shamszaman
- Reference count: 0
- Primary result: 92% classification accuracy on breast cancer dataset using CNN with explainability

## Executive Summary
This research develops an AI model for cancer diagnosis that achieves 92% classification accuracy on a breast cancer dataset of 569 cases. The model employs Convolutional Neural Networks (CNNs) with explainability frameworks (SHAP, LIME, and ELI5) to provide both precise outcomes and transparent decision-making insights. While demonstrating exceptional precision (100%), the model shows lower recall (83.7%), indicating perfect identification of malignant cases when predicted but missing 16.3% of actual malignancies. The study addresses critical challenges in medical AI by implementing GDPR and HIPAA compliance measures while revealing perimeter and texture measurements as the most influential diagnostic features.

## Method Summary
The study develops a deep learning model using CNNs trained on a breast cancer dataset containing 569 cases with features including radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension measurements. The model employs data preprocessing, feature selection, and hyperparameter optimization with cross-validation. Explainability is achieved through SHAP, LIME, and ELI5 frameworks to interpret model predictions. The training process includes data augmentation and dropout regularization to prevent overfitting, with performance evaluated using accuracy, precision, recall, confusion matrix, and ROC curve metrics.

## Key Results
- Achieved 92% classification accuracy on breast cancer dataset
- Demonstrated perfect precision (100%) but lower recall (83.7%)
- Identified perimeter and texture measurements as most influential features
- Successfully implemented GDPR and HIPAA compliance measures
- Showed effective integration of SHAP, LIME, and ELI5 explainability frameworks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CNN's convolutional layers effectively extract spatial features from breast cancer data, improving classification accuracy.
- Mechanism: Convolutional layers apply filters to input data, creating feature maps that capture spatial relationships and patterns. This reduces the number of parameters and enhances efficiency while maintaining strong performance on image-like data.
- Core assumption: The breast cancer dataset contains spatial or structural patterns that can be captured by convolutional filters.
- Evidence anchors:
  - [abstract] "Our study develops an AI model that provides precise outcomes and clear insights into its decision-making process, addressing the 'black box' problem of deep learning models."
  - [section] "Convolutional Neural Networks (CNNs) Deep learning models known as convolutional neural networks (CNNs) are particularly good at training and categorizing picture data, although they may also be applied to text-based cancer datasets."
  - [corpus] Weak evidence. Most related papers focus on mammography images, but this study uses tabular data with numerical features. The mechanism's applicability to non-image data is assumed but not proven.
- Break condition: If the data lacks spatial or structural patterns, convolutional filters may not provide significant advantages over simpler models like MLPs.

### Mechanism 2
- Claim: SHAP values provide theoretically grounded feature importance rankings by evaluating all possible feature combinations.
- Mechanism: SHAP uses game theory to fairly distribute feature contributions to model predictions. It examines every possible coalition of features and calculates each feature's marginal contribution to the prediction difference.
- Core assumption: The cooperative game theory framework accurately represents feature contributions in the cancer diagnosis model.
- Evidence anchors:
  - [abstract] "By employing XAI techniques, we enhance interpretability and transparency, building trust among healthcare professionals and patients."
  - [section] "SHAP (SHapley Additive exPlanations) SHAP, rooted in cooperative game theory, quantifies the contribution of each feature to the prediction for individual instances."
  - [corpus] Moderate evidence. SHAP is widely used in medical AI literature, including several related papers on breast cancer diagnosis.
- Break condition: If feature interactions are extremely complex or non-additive, SHAP's approximation may not accurately capture true feature importance.

### Mechanism 3
- Claim: Data augmentation and dropout layers prevent overfitting on the small 569-case dataset.
- Mechanism: Data augmentation artificially expands the dataset by applying transformations to existing data points. Dropout randomly deactivates neurons during training, preventing co-adaptation and improving generalization.
- Core assumption: The limited dataset size creates significant overfitting risk that can be mitigated through these regularization techniques.
- Evidence anchors:
  - [abstract] "The model demonstrated exceptional precision (100%) but lower recall (83.7%), indicating perfect identification of malignant cases when predicted, but missing 16.3% of actual malignancies."
  - [section] "The relatively limited size of our dataset, comprising 569 cases, presents a substantial risk of overfitting."
  - [corpus] Strong evidence. Multiple related papers cite overfitting as a major challenge with small medical datasets.
- Break condition: If the dataset is too small or the transformations don't preserve class-relevant information, these techniques may not effectively prevent overfitting.

## Foundational Learning

- Concept: Convolutional Neural Networks
  - Why needed here: Understanding how CNNs process image-like data and extract spatial features is crucial for implementing the core model architecture.
  - Quick check question: What is the primary advantage of using convolutional layers over fully connected layers for image classification tasks?

- Concept: Shapley Values and Game Theory
  - Why needed here: SHAP's mathematical foundation in game theory determines how feature contributions are calculated and interpreted.
  - Quick check question: How does the Shapley value formula ensure fair distribution of feature contributions in cooperative games?

- Concept: Regularization Techniques (Dropout, Data Augmentation)
  - Why needed here: These techniques prevent overfitting on small datasets, which is critical for medical applications where data is often limited.
  - Quick check question: What is the relationship between dropout rate and model generalization performance?

## Architecture Onboarding

- Component map:
  Data Pipeline: CSV loading → preprocessing → feature selection → train/test split
  Model Layer: CNN architecture with configurable hyperparameters
  Explainability Layer: SHAP, LIME, and ELI5 implementations
  Visualization Layer: Plots, heatmaps, and confusion matrices
  Evaluation Layer: Metrics calculation and ROC curve generation

- Critical path: Data preprocessing → model training → hyperparameter optimization → explainability analysis → clinical interpretation

- Design tradeoffs:
  Precision vs. Recall: The model achieved perfect precision but lower recall, requiring careful threshold adjustment for clinical use
  Model Complexity vs. Interpretability: CNN provides better accuracy but is harder to interpret than simpler models
  Computation Time vs. Explainability: SHAP provides robust explanations but is computationally expensive compared to LIME or ELI5

- Failure signatures:
  High training accuracy but low validation accuracy → overfitting
  Perfect precision but poor recall → model is conservative in positive predictions
  Inconsistent feature importance across explainability methods → potential model instability

- First 3 experiments:
  1. Baseline MLP comparison: Train a simple MLP on the same dataset to establish performance baseline
  2. Hyperparameter sensitivity: Test different activation functions, dropout rates, and filter sizes to identify optimal configuration
  3. Explainability method comparison: Run all three explainability methods (SHAP, LIME, ELI5) on the same predictions to compare consistency and insights

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the explainable AI model vary across different demographic groups, and what specific biases exist in the model's predictions?
- Basis in paper: [inferred] The paper acknowledges potential data representation bias and the need for diverse datasets, but does not provide empirical evidence of performance variation across demographic groups.
- Why unresolved: The study primarily focuses on model development and validation using a single dataset, without conducting subgroup analysis or testing on diverse population samples.
- What evidence would resolve it: Comprehensive performance metrics broken down by demographic categories (age, gender, ethnicity) across multiple independent datasets, along with statistical analysis of prediction disparities between groups.

### Open Question 2
- Question: What is the optimal balance between precision and recall for the model in clinical practice, and how should this balance be adjusted for different cancer types and patient risk profiles?
- Basis in paper: [explicit] The paper identifies a trade-off between precision (100%) and recall (83.7%) and acknowledges the critical nature of missed cancer cases, but does not propose a clinical decision framework for balancing these metrics.
- Why unresolved: While the paper highlights the importance of both precision and recall, it does not provide clinical guidelines or adaptive threshold mechanisms for optimizing this balance in different clinical scenarios.
- What evidence would resolve it: Clinical outcome data comparing different threshold settings, cost-benefit analysis of false positives versus false negatives, and expert consensus on acceptable trade-offs for various cancer types and patient populations.

### Open Question 3
- Question: How does the model's performance and reliability compare to human expert diagnosis in real-world clinical settings, and what is the optimal integration strategy for AI-assisted diagnosis?
- Basis in paper: [inferred] The paper discusses the potential of the model to revolutionize cancer diagnosis but acknowledges the limitation of lacking real-world clinical validation.
- Why unresolved: The model has only been tested on a static dataset and not in actual clinical environments with human experts, leaving its practical utility and integration strategy unexplored.
- What evidence would resolve it: Prospective clinical trials comparing model performance to expert diagnosis across multiple healthcare institutions, analysis of workflow integration impacts, and assessment of diagnostic accuracy improvements when combining AI and human expertise.

## Limitations

- Dataset size constraints (569 cases) may limit model generalizability to broader clinical populations
- High precision (100%) but lower recall (83.7%) suggests the model is conservative in positive predictions, potentially missing 16.3% of actual malignancies
- Limited external validation on independent datasets to confirm model robustness

## Confidence

- Model Architecture and Training: Medium confidence - CNN implementation details are partially specified, but exact layer configurations are unclear
- Explainability Results: High confidence - SHAP, LIME, and ELI5 are well-established methods with clear implementation procedures
- Clinical Applicability: Low confidence - No mention of prospective clinical validation or comparison with existing diagnostic standards

## Next Checks

1. **Independent Dataset Validation**: Test the trained model on an external breast cancer dataset to assess generalizability beyond the original 569 cases
2. **Precision-Recall Trade-off Analysis**: Systemmatically vary classification thresholds to identify optimal balance between sensitivity and specificity for clinical use
3. **Comparative Performance Evaluation**: Benchmark the CNN model against traditional ML algorithms (e.g., Random Forest, SVM) on the same dataset to validate the architectural choice