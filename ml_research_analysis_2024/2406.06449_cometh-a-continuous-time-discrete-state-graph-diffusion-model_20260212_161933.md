---
ver: rpa2
title: 'Cometh: A continuous-time discrete-state graph diffusion model'
arxiv_id: '2406.06449'
source_url: https://arxiv.org/abs/2406.06449
tags:
- graph
- diffusion
- generation
- process
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COMETH integrates continuous-time Markov chains into discrete-state
  graph diffusion, enabling flexible sampling schedules and improved generation quality.
  It employs marginal transition rates for nodes and edges, paired with a cosine noise
  schedule, to preserve graph sparsity.
---

# Cometh: A continuous-time discrete-state graph diffusion model

## Quick Facts
- arXiv ID: 2406.06449
- Source URL: https://arxiv.org/abs/2406.06449
- Authors: Antoine Siraudin; Fragkiskos D. Malliaros; Christopher Morris
- Reference count: 40
- Key outcome: COMETH achieves state-of-the-art graph generation performance, outperforming DIGRESS on synthetic (PLANAR, SBM), molecular (QM9), and large-scale (MOSES, GuacaMol) datasets, with 99.5% VUN on PLANAR and 12.6% higher VUN than DIGRESS on GuacaMol.

## Executive Summary
COMETH introduces a continuous-time discrete-state graph diffusion model that integrates continuous-time Markov chains (CTMCs) into graph generation. By leveraging marginal transition rates for nodes and edges, paired with a cosine noise schedule, the model preserves graph sparsity while enabling flexible sampling schedules. The use of a single random-walk-based encoding replaces the extensive structural features of prior models, simplifying the architecture and boosting expressivity. Empirical results demonstrate state-of-the-art performance across synthetic and molecular graph datasets, with significant improvements in generation quality and efficiency.

## Method Summary
COMETH extends discrete graph diffusion models by integrating continuous-time Markov chains, enabling flexible sampling schedules and improved generation quality. The forward process is modeled as a CTMC with distinct rate matrices for nodes and edges, allowing transitions at any time during training. Marginal transition rates are used to preserve graph sparsity, while a cosine noise schedule ensures the endpoint of the diffusion process approximates the product of marginal distributions. The model replaces DIGRESS's extensive structural features with a single random-walk-based encoding, simplifying the architecture and boosting expressivity. Training is performed using cross-entropy loss, and sampling is achieved through τ-leaping with an optional predictor-corrector scheme for improved quality.

## Key Results
- COMETH achieves 99.5% VUN on the PLANAR dataset, significantly outperforming DIGRESS.
- On the GuacaMol dataset, COMETH improves VUN by 12.6% compared to DIGRESS.
- The model demonstrates strong performance in conditional generation tasks and scales effectively to large molecular datasets (MOSES, GuacaMol).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: COMETH leverages continuous-time Markov chains (CTMCs) to enable flexible sampling schedules and improved generation quality.
- Mechanism: The model defines a forward process as a CTMC with distinct rate matrices for nodes and edges, allowing transitions to occur at any time t ∈ [0, 1] during training, while fixing the step length τ during sampling. This flexibility in the reverse process allows for a better trade-off between sampling efficiency and quality.
- Core assumption: The factorization of the forward process, where noise propagates independently on each node and edge, is valid and enables the use of separate rate matrices for nodes and edges.
- Evidence anchors:
  - [abstract] "COMETH integrates continuous-time Markov chains into discrete-state graph diffusion, enabling flexible sampling schedules and improved generation quality."
  - [section] "In addition, we also successfully replaced the set of structural features previously used in discrete graph diffusion models with a single random-walk-based encoding, providing a simple and principled way to boost the model's expressive power."
- Break condition: If the factorization assumption does not hold, or if the separate rate matrices for nodes and edges are not well-suited for the specific graph data, the model's performance may degrade.

### Mechanism 2
- Claim: The use of marginal transition rates for nodes and edges, paired with a cosine noise schedule, helps preserve graph sparsity.
- Mechanism: The rate matrices are constructed using the marginal distributions of node and edge labels, making the transition rate to a particular state dependent on its marginal probability. This approach favors transitions to the "no edge" label, helping to preserve sparsity in noisy graphs. The cosine noise schedule is used to design the cumulative noise schedule, ensuring that the endpoint of the diffusion process approximates the product of marginal distributions.
- Core assumption: The marginal distributions of node and edge labels in the dataset accurately reflect the desired sparsity of the graph structures.
- Evidence anchors:
  - [abstract] "It employs marginal transition rates for nodes and edges, paired with a cosine noise schedule, to preserve graph sparsity."
  - [section] "Specifically, the more common a node or edge label is in the dataset, the higher the transition rate to that label. Consequently, this approach helps preserve sparsity in noisy graphs by favoring transitions to the 'no edge' label."
- Break condition: If the marginal distributions do not accurately reflect the desired sparsity, or if the cosine noise schedule is not well-suited for the specific dataset, the model may not effectively preserve graph sparsity.

### Mechanism 3
- Claim: Replacing DIGRESS's extensive structural features with a single random-walk-based encoding boosts expressivity and simplifies the architecture.
- Mechanism: The relative random-walk probabilities (RRWP) encoding is used to capture structural information about the graph, such as whether two nodes are in the same connected component, the size of the largest connected component, and the number of small cycles. This encoding generalizes most of the features used in DIGRESS, providing a simple and elegant way to boost the model's expressive power.
- Core assumption: The RRWP encoding is sufficiently expressive to capture the relevant structural information for the graph generation task.
- Evidence anchors:
  - [abstract] "Replacing DIGRESS's extensive structural features with a single random-walk-based encoding boosts expressivity and simplifies the architecture."
  - [section] "Specifically, in our experiments, we leverage the relative random-walk probabilites (RRWP) encoding, introduced in Ma et al. (2023). [...] This encoding provides an efficient and elegant solution for boosting model expressivity and performance through a unified encoding."
- Break condition: If the RRWP encoding does not capture the relevant structural information for the specific graph generation task, or if it introduces noise or artifacts that negatively impact the model's performance, the model's expressivity and simplicity gains may be limited.

## Foundational Learning

- Concept: Continuous-time Markov chains (CTMCs)
  - Why needed here: CTMCs form the basis of the forward and reverse processes in COMETH, allowing for flexible sampling schedules and improved generation quality.
  - Quick check question: What is the key difference between CTMCs and discrete-time Markov chains, and how does this difference enable more flexible sampling schedules in COMETH?

- Concept: Random-walk-based encodings (e.g., RRWP)
  - Why needed here: RRWP encoding captures structural information about the graph, such as connected components and cycle counts, which is crucial for generating realistic graph structures.
  - Quick check question: How does the RRWP encoding generalize the structural features used in DIGRESS, and what are the advantages of using a single unified encoding over a set of specialized features?

- Concept: Marginal transition rates and noise schedules
  - Why needed here: Marginal transition rates help preserve graph sparsity by favoring transitions to common labels, while the noise schedule ensures that the endpoint of the diffusion process approximates the desired prior distribution.
  - Quick check question: How do the marginal transition rates and the cosine noise schedule work together to preserve graph sparsity and ensure the correct prior distribution in COMETH?

## Architecture Onboarding

- Component map: Input -> Noise model (marginal transition rates) -> Noise schedule (cosine) -> Denoising network (graph transformer with RRWP encoding) -> Output
- Critical path:
  1. Sample a noisy graph from the prior distribution
  2. Iteratively denoise the graph using the learned denoising network
  3. Apply τ-leaping algorithm to simulate the reverse process
  4. Use predictor-corrector scheme to improve sample quality (optional)
- Design tradeoffs:
  - Continuous-time vs. discrete-time: Continuous-time allows for more flexible sampling schedules but may be more computationally expensive.
  - Marginal transition rates vs. other noise models: Marginal rates help preserve sparsity but may not be suitable for all graph datasets.
  - RRWP encoding vs. other structural features: RRWP is a unified encoding that generalizes many features but may not capture all relevant information.
- Failure signatures:
  - Poor generation quality: May indicate issues with the noise model, noise schedule, or denoising network.
  - Lack of graph sparsity: May indicate issues with the marginal transition rates or noise schedule.
  - Limited expressivity: May indicate issues with the RRWP encoding or denoising network architecture.
- First 3 experiments:
  1. Ablation study on the noise schedule: Compare the performance of COMETH using the cosine noise schedule vs. a constant noise schedule.
  2. Ablation study on the structural encoding: Compare the performance of COMETH using RRWP encoding vs. the original set of structural features used in DIGRESS.
  3. Ablation study on the number of denoising steps: Evaluate the trade-off between sampling quality and efficiency by varying the number of τ-leaping steps during sampling.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions beyond its limitations, which include the inability to handle continuous features and the computational complexity for large graphs.

## Limitations
- COMETH is restricted to categorical attributes and does not support the generation of continuous features.
- The model's quadratic complexity from pairwise node representations limits scalability to large graphs.
- The empirical scope is limited to synthetic and molecular graphs, with no testing on real-world large graphs.

## Confidence
- Confidence in the CTMC integration and noise schedule design: High
- Confidence in the random-walk-based encoding: Medium
- Confidence in the sparsity preservation claim: Medium

## Next Checks
1. Test COMETH on a real-world large graph dataset (e.g., protein-protein interaction networks) to evaluate scalability and performance beyond molecular graphs.
2. Conduct an ablation study on the random-walk-based encoding to quantify its impact relative to DIGRESS's structural features in isolation.
3. Evaluate the model's robustness to non-uniform label distributions by testing on a dataset where the marginal distribution does not reflect the desired sparsity (e.g., preferential attachment graphs).