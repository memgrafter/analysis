---
ver: rpa2
title: Transcoders Find Interpretable LLM Feature Circuits
arxiv_id: '2406.11944'
source_url: https://arxiv.org/abs/2406.11944
tags:
- feature
- transcoder
- features
- transcoders
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Transcoders are trained approximations of MLP layers that produce
  sparse, interpretable features. They enable circuit analysis by cleanly separating
  input-invariant feature connections (computed from weights) from input-dependent
  activations (computed from the current input).
---

# Transcoders Find Interpretable LLM Feature Circuits
## Quick Facts
- arXiv ID: 2406.11944
- Source URL: https://arxiv.org/abs/2406.11944
- Reference count: 40
- Primary result: Trained approximations of MLP layers that produce sparse, interpretable features for circuit analysis

## Executive Summary
Transcoders are trained approximations of MLP layers that produce sparse, interpretable features, enabling cleaner circuit analysis by separating input-invariant feature connections from input-dependent activations. This factorization allows researchers to reason about model behavior generally rather than only on specific inputs. The method outperforms or matches SAEs in sparsity, fidelity, and interpretability on models up to 1.4B parameters, and has successfully recovered known circuits and discovered new ones on GPT2-small with far fewer features than MLP neurons.

## Method Summary
The transcoder approach trains a sparse autoencoder-like model to approximate MLP layer behavior, producing features that can be analyzed independently of specific inputs. By computing feature connections from weights (input-invariant) and activations from current input separately, transcoders enable general reasoning about model behavior. The method uses loss-based training objectives to learn feature representations that balance sparsity with reconstruction fidelity.

## Key Results
- Transcoders outperform or match SAEs in sparsity, fidelity, and interpretability on models up to 1.4B parameters
- Applied to GPT2-small, transcoders recovered known circuits and discovered new ones (e.g., greater-than circuit) with fewer features than MLP neurons
- Blind case studies showed transcoders can infer feature semantics without seeing input text

## Why This Works (Mechanism)
Transcoders work by factorizing the computation of MLP layers into input-invariant feature connections (computed from weights) and input-dependent activations (computed from current input). This separation allows researchers to analyze the general behavior of features rather than their behavior on specific examples. The sparse training objective encourages interpretable, human-readable features while maintaining reconstruction fidelity.

## Foundational Learning
- Sparse autoencoders: Neural networks trained to reconstruct inputs using sparse latent representations; needed to understand how transcoders generate interpretable features; quick check: can you explain how sparsity encourages feature interpretability?
- MLP layer factorization: Separating weight-based connections from activation-based computation; needed to understand how transcoders enable general circuit analysis; quick check: can you describe the difference between input-invariant and input-dependent computations?
- Circuit analysis in LLMs: Studying interpretable feature pathways in neural networks; needed to contextualize transcoder applications; quick check: can you name one known circuit in GPT2?

## Architecture Onboarding
Component map: Input -> MLP layer -> Transcoder approximation -> Interpretable features
Critical path: Model weights -> Feature connection computation -> Sparse feature learning -> Feature interpretation
Design tradeoffs: Sparsity vs fidelity in feature representation; interpretability vs reconstruction accuracy
Failure signatures: Features that don't activate consistently across similar inputs; poor reconstruction of MLP layer outputs
First experiments:
1. Train transcoder on small MLP layer and visualize learned features
2. Compare transcoder sparsity and fidelity against SAE on same layer
3. Apply transcoder to known circuit and verify recovery

## Open Questions the Paper Calls Out
None

## Limitations
- Scaling to frontier models (>10B parameters) remains untested
- Performance comparisons with SAEs use specific hyperparameters that may not generalize
- Reliance on human interpretability judgments introduces subjectivity
- Method assumes MLP layers are primary locus of interpretable computation

## Confidence
- Transcoders enable cleaner circuit analysis than raw MLP neurons: High
- Transcoders match or exceed SAEs in sparsity, fidelity, and interpretability: Medium
- Blind interpretability case studies are successful: Medium
- The factorization approach generalizes to larger models: Low

## Next Checks
1. Scale transcoders to models >10B parameters and evaluate computational overhead and interpretability retention
2. Test transcoder performance on non-GPT architectures (e.g., LLaMA, PaLM) to assess architectural generality
3. Conduct larger-scale blind studies with multiple independent annotators to validate feature semantics discovery without input text