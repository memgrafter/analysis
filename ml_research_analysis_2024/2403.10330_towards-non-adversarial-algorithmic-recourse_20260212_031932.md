---
ver: rpa2
title: Towards Non-Adversarial Algorithmic Recourse
arxiv_id: '2403.10330'
source_url: https://arxiv.org/abs/2403.10330
tags:
- recourse
- adversarial
- non-adversarial
- counterfactual
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of algorithmic recourse in high-stakes
  decision-making scenarios where human experts play a role alongside machine learning
  models. The authors introduce the concept of non-adversarial algorithmic recourse,
  which aims to provide actionable recommendations that change both the model's prediction
  and the true label.
---

# Towards Non-Adversarial Algorithmic Recourse

## Quick Facts
- arXiv ID: 2403.10330
- Source URL: https://arxiv.org/abs/2403.10330
- Reference count: 40
- Primary result: Choosing robust and accurate ML models is more critical for non-adversarial recourse than cost function or optimization algorithm choice

## Executive Summary
This paper addresses algorithmic recourse in high-stakes decision-making scenarios where human experts make final decisions alongside ML models. The authors introduce the concept of non-adversarial algorithmic recourse, which aims to provide actionable recommendations that change both the model's prediction and the true label. They propose using feature attributions to identify task-relevant features and develop optimal cost functions for this purpose. Through experiments on common datasets, they find that model accuracy and robustness are the primary drivers of non-adversarial recourse, more so than the choice of cost function or optimization algorithm.

## Method Summary
The authors simulate a decision-making scenario where human experts rely on ML model predictions. They create a ground truth classifier using kNN on a subset of "expert-relevant" features, then train ML models on the remaining features. Six methods are tested: SCFE, DiCE, AR, C&W, PGD, and DeepFool. The retry mechanism scales perturbations by 1.1^r to approximate real-world feedback. They evaluate non-adversarialness by checking if generated recourses also change the ground truth label.

## Key Results
- High model accuracy and robustness are the primary drivers of non-adversarial recourse
- Adversarial methods (C&W, PGD, DeepFool) can effectively compute non-adversarial recourse on tabular data
- NADV2-optimal cost function consistently outperforms unit costs and inverse weighting approaches

## Why This Works (Mechanism)

### Mechanism 1
High model accuracy and robustness are the primary drivers of non-adversarial recourse. When the ML model closely approximates the true label function (y), any change in the model's prediction also changes the true label, eliminating adversarial examples. Adversarial training further smooths decision boundaries, reducing the likelihood of small perturbations leading to misclassification. Core assumption: The human expert panel acts as a "noisy oracle" whose decisions can be approximated by a well-trained ML model.

### Mechanism 2
Adversarial example generation methods can produce non-adversarial recourse on tabular data. Methods like C&W, PGD, and DeepFool are designed to find minimal perturbations to change model predictions. On tabular data, these perturbations often align with changes that also affect the true label, especially when the model is already accurate. Core assumption: The perturbation space for tabular data is structured such that minimal changes affecting the model also affect the ground truth.

### Mechanism 3
Feature weightings based on gradient magnitudes can steer recourse toward discriminative features, reducing adversarialness. By assigning higher costs to features with low gradient magnitudes (likely non-discriminative), the optimization is forced to change discriminative features, which are more likely to align with the ground truth. Core assumption: Gradient magnitudes correlate with feature relevance to the true label function.

## Foundational Learning

- **Concept:** Counterfactual explanations vs. adversarial examples
  - Why needed here: The paper hinges on distinguishing between these two concepts and showing that practical methods often blur the line. Understanding the formal definitions and computational overlap is essential.
  - Quick check question: What is the key difference between a counterfactual explanation and an adversarial example according to the paper's definitions?

- **Concept:** Algorithmic recourse in human-AI decision systems
  - Why needed here: The paper models a realistic scenario where human experts make final decisions based on ML model scores. Understanding this setup is crucial for grasping why non-adversarial recourse matters.
  - Quick check question: Why does the paper argue that recourse should change both the model's prediction and the true label?

- **Concept:** Optimization formulations for counterfactuals and adversarial examples
  - Why needed here: Both problems are cast as similar optimization problems, and the paper investigates how different components (model, cost function, algorithm) affect the outcome. Knowing the general form helps understand the experimental design.
  - Quick check question: Write the general optimization problem that both counterfactual and adversarial methods solve.

## Architecture Onboarding

- **Component map:**
  - Data preprocessing -> Standardization, binary classification setup
  - Ground truth simulation -> k-NN classifier on expert-relevant features
  - ML models -> ANN with two hidden layers (30 units each)
  - Recourse methods -> SCFE, DiCE, AR
  - Adversarial methods -> C&W, PGD, DeepFool
  - Evaluation -> Retry mechanism to check ground truth alignment

- **Critical path:**
  1. Train expert-approximating ground truth on subset of features
  2. Train ML model on remaining data
  3. Generate recourse/adversarials using various methods and cost functions
  4. Evaluate non-adversarialness via retry mechanism
  5. Analyze impact of model accuracy, regularization, and cost functions

- **Design tradeoffs:**
  - Using a simulated ground truth limits generalizability but enables controlled experiments
  - ANN models are flexible but may overfit noisy data
  - The retry mechanism approximates real-world feedback but assumes monotonic improvement

- **Failure signatures:**
  - Low non-adversarialness across all methods suggests model-ground truth misalignment
  - High costs for adversarial methods indicate they are not optimized for cost minimization
  - Inconsistent results across datasets may indicate feature relevance differences

- **First 3 experiments:**
  1. **Model accuracy ablation:** Train logistic regression models on clean vs. noisy data, measure non-adversarialness of SCFE recourses.
  2. **Cost function comparison:** Run SCFE with NADV2-optimal, unit, and inverse squared costs on Admission dataset, plot non-adversarialness over retries.
  3. **Adversarial training impact:** Train ANN with and without lâˆž-adversarial training on German Credit, compare non-adversarialness of PGD recourses.

## Open Questions the Paper Calls Out

- **Open Question 1:** How effective are adversarial attacks at generating non-adversarial recourse on tabular data compared to dedicated recourse methods? The authors find that adversarial methods succeed at computing non-adversarial recourse but incur higher costs. It remains unclear if the higher costs are a necessary trade-off or if there are ways to optimize adversarial methods to achieve lower costs while maintaining non-adversarialness.

- **Open Question 2:** How do different cost functions impact the non-adversarialness of generated recourse? While the authors find that the NADV2-optimal cost function performs well, it remains unclear how other cost functions would impact non-adversarialness and if there are other cost functions that could perform even better.

- **Open Question 3:** How does the choice of machine learning model impact the non-adversarialness of generated recourse? While the authors find that model accuracy and robustness impact non-adversarialness, it remains unclear how other model choices such as model type or architecture would impact non-adversarialness.

## Limitations

- The use of simulated ground truth via kNN classifiers may not fully capture real human decision-making complexity
- Results are based on four tabular datasets, limiting generalizability to other data types or domains
- The retry mechanism assumes monotonic improvement, which may not hold in real-world scenarios

## Confidence

- Model accuracy/robustness claims: Medium
- Specific cost functions claims: Low
- Adversarial methods claims: Low

## Next Checks

1. **Ground Truth Validation:** Test the kNN-based ground truth approach against real human expert decisions on a small subset of cases to verify alignment between simulated and actual expert behavior.

2. **Cross-Dataset Feature Analysis:** Analyze which features are selected as expert-relevant across different datasets to determine if there are consistent patterns in feature selection that drive non-adversarialness.

3. **Model Architecture Ablation:** Repeat key experiments using different model architectures (e.g., logistic regression, decision trees) to test whether the ANN-specific results generalize to simpler models.