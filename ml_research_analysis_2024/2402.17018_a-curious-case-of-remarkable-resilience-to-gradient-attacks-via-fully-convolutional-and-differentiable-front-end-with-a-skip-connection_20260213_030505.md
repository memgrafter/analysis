---
ver: rpa2
title: A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional
  and Differentiable Front End with a Skip Connection
arxiv_id: '2402.17018'
source_url: https://arxiv.org/abs/2402.17018
tags:
- attack
- attacks
- accuracy
- adversarial
- autoattack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We investigate a fully differentiable front-end enhanced model
  with a skip connection that exhibits unusual resistance to gradient attacks, including
  those from the AutoAttack package. By training a composite model with a small learning
  rate for about one epoch, we obtain models that retain the backbone classifier's
  accuracy while being highly resistant to gradient masking.
---

# A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection

## Quick Facts
- arXiv ID: 2402.17018
- Source URL: https://arxiv.org/abs/2402.17018
- Authors: Leonid Boytsov; Ameya Joshi; Filipe Condessa
- Reference count: 15
- Primary result: A fully differentiable front-end with skip connection exhibits unusual resistance to gradient attacks, including AutoAttack, when trained minimally.

## Executive Summary
This paper investigates a fully differentiable front-end enhanced model with a skip connection that exhibits unusual resistance to gradient attacks. By training a composite model with a small learning rate for about one epoch, the authors obtain models that retain the backbone classifier's accuracy while being highly resistant to gradient masking. This masking is remarkable for fully differentiable models without gradient-shattering or diminishing components. The training recipe is stable and reproducible, and the results are demonstrated on three datasets (CIFAR10, CIFAR100, ImageNet) and multiple architectures (including vision transformers).

## Method Summary
The method involves training a composite model where a DnCNN front-end with a skip connection is minimally trained (small learning rate, one epoch) while the backbone classifier remains frozen. This creates a near-identity transform that breaks gradient fidelity for adversarial attacks, resulting in gradient masking. Randomized ensembles of such models with diverse backbones achieve near-SOTA AutoAttack accuracy while having near-zero accuracy under adaptive attacks.

## Key Results
- An ensemble achieved 90.8±2.5% accuracy under AutoAttack but only 18.2±3.6% under adaptive attacks (ε=8/255, L∞ norm) on CIFAR10.
- The gradient masking effect is reproducible across multiple architectures (ResNet, ViT) and datasets (CIFAR10, CIFAR100, ImageNet).
- Randomized ensembles of diverse models defeat gradient masking attacks by ensuring low transferability between models.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A small learning rate (≈1e-6) for one epoch with a frozen backbone creates models where the front-end behaves like a near-identity transform but breaks gradient fidelity for adversarial attacks.
- **Mechanism:** The front-end (DnCNN) is initialized to preserve input (skip connection) and is trained minimally so gradients still flow but become noisy or unreliable for attack optimization.
- **Core assumption:** The front-end remains close to identity (f(x) ≈ x) while gradients become uninformative for adversarial updates.
- **Evidence anchors:** [abstract] "By training such composite models using a small learning rate for about one epoch... unusually resistant to gradient attacks"
- **Break condition:** If the learning rate is increased or training time extended, adversarial robustness degrades and clean accuracy is lost.

### Mechanism 2
- **Claim:** Randomized ensembles defeat gradient masking attacks by ensuring no single model dominates attack transferability.
- **Mechanism:** Each model in the ensemble is trained with a different backbone (ResNet, ViT, etc.) and front-end, so adversarial examples transfer poorly between them.
- **Core assumption:** Attack transferability is low across heterogeneous backbones.
- **Evidence anchors:** [abstract] "randomized ensembles achieve near-SOTA AutoAttack accuracy... despite having near-zero accuracy under adaptive attacks"
- **Break condition:** If all ensemble models share the same architecture, standard PGD can still fool the ensemble.

### Mechanism 3
- **Claim:** BPDA (Backward Pass Differentiable Approximation) attacks can bypass gradient masking by treating the front-end as identity during backprop.
- **Mechanism:** Since f(x) ≈ x, gradients of h(f(x)) are approximated as gradients of h(x), allowing standard attack optimization to succeed even when the front-end is present.
- **Core assumption:** The front-end is close to identity, so identity substitution in BPDA is valid.
- **Evidence anchors:** [section 4.2] "we use a variant of the Backward Pass Differentiable Approximation (BPDA) approach with a straight-through gradient estimator"
- **Break condition:** If the front-end is not close to identity, BPDA fails.

## Foundational Learning

- **Concept:** Gradient masking
  - Why needed here: Explains why models appear robust under standard attacks but are vulnerable under adaptive attacks.
  - Quick check question: If a model has gradient masking, will standard PGD attacks be effective?

- **Concept:** Adversarial training objective
  - Why needed here: Shows the minimax formulation that underlies the robustness training used.
  - Quick check question: In the adversarial training objective, what does the inner maximization represent?

- **Concept:** Transferability of adversarial examples
  - Why needed here: Critical for understanding why randomized ensembles can be a practical defense.
  - Quick check question: What happens to attack transferability when the source and target models have different architectures?

## Architecture Onboarding

- **Component map:** Input → Front-end (DnCNN with skip connection) → Backbone (ResNet/ViT) → Output
- **Critical path:** 1. Input → Front-end (near-identity) → Backbone → Output; 2. For ensembles: Random model choice at each forward pass
- **Design tradeoffs:** Small learning rate + one epoch → Gradient masking but near-identity preservation; More training epochs → Loses masking effect, loses clean accuracy; Using identical backbones in ensembles → Standard PGD can fool ensemble
- **Failure signatures:** Clean accuracy drops below baseline → Learning rate too high or too many epochs; BPDA attack succeeds → Gradient masking is active; AutoAttack accuracy high but BPDA accuracy low → Masking present
- **First 3 experiments:** 1. Train single model with front-end (learning rate 1e-6, 1 epoch) → Measure PGD vs BPDA accuracy gap; 2. Build 3-model ensemble with different backbones → Measure AutoAttack vs BPDA accuracy; 3. Vary learning rate (1e-5, 1e-4) → Observe when masking effect disappears

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can randomized ensembling with diverse model architectures serve as a practical defense against adversarial attacks?
- Basis in paper: [explicit] The paper discusses that randomized ensembles of diverse models can achieve near-SOTA AutoAttack accuracy while maintaining near-zero accuracy under adaptive attacks, but transferability among models remains a concern.
- Why unresolved: The effectiveness of randomized ensembling depends on minimizing transferability between models, which requires further investigation into architectural diversity and training methods.
- What evidence would resolve it: Experiments comparing the robustness of randomized ensembles with varying degrees of architectural diversity and training strategies, measuring both AutoAttack accuracy and adaptive attack performance.

### Open Question 2
- Question: What specific properties of the front-end architecture contribute to the observed gradient masking phenomenon?
- Basis in paper: [explicit] The paper notes that fully differentiable front-end models with skip connections exhibit unusual resistance to gradient attacks, suggesting gradient masking, but the exact mechanisms are not fully understood.
- Why unresolved: The interplay between the front-end's residual learning, skip connections, and batch normalization in creating gradient masking is not yet clear.
- What evidence would resolve it: Ablation studies systematically removing or modifying components of the front-end architecture to isolate the specific factors contributing to gradient masking.

### Open Question 3
- Question: How does the strength of adversarial training affect the transferability of attacks between models with and without front-end enhancements?
- Basis in paper: [explicit] The paper observes that adversarial training can amplify the front-end's resistance to gradient attacks, but the relationship between training strength and attack transferability is not fully explored.
- Why unresolved: The paper provides initial observations but lacks a comprehensive analysis of how different levels of adversarial training impact attack transferability.
- What evidence would resolve it: Comparative studies training models with varying adversarial training strengths and measuring both self-attack and transfer attack performance.

## Limitations

- The gradient masking phenomenon relies heavily on a very narrow training regime (small learning rate for one epoch), which may not be stable across different datasets and model scales.
- The claim that BPDA attacks universally break this defense assumes the front-end remains close to identity, which may not hold under all initialization schemes.
- The practical security of the randomized ensemble approach is untested against adaptive attackers who can probe the entire ensemble distribution.

## Confidence

**Medium confidence** - The empirical observation that small learning rate training creates a robustness gap between standard attacks and BPDA is well-documented in the experiments, but the theoretical explanation (noisy/uninformative gradients) is speculative and lacks rigorous validation.

**Medium confidence** - The randomized ensemble results showing decreased attack transferability are convincing on the tested architectures, but the claim of near-SOTA AutoAttack accuracy may be specific to the exact training regime and datasets used.

**Low confidence** - The assertion that this approach provides practical defense is premature, as adaptive attacks against the ensemble (e.g., attacking multiple models and ensembling predictions) are not thoroughly explored.

## Next Checks

1. **Stability analysis across training regimes**: Systematically vary the learning rate (1e-7 to 1e-4) and training duration (0.5 to 5 epochs) to map the boundary where gradient masking appears and disappears. Verify that the robustness gap is reproducible and not an artifact of a specific hyperparameter setting.

2. **Adaptive attack on ensembles**: Design and test an adaptive attack that queries the entire ensemble (rather than a single model) to craft adversarial examples. This would test whether the ensemble truly provides robustness or merely shifts the attack surface.

3. **Alternative front-end architectures**: Replace DnCNN with other near-identity transforms (e.g., fixed random convolutions, identity mapping networks) to determine if the masking effect is specific to DnCNN or a more general phenomenon of training minimally on near-identity functions.