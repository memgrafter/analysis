---
ver: rpa2
title: Evolving Code with A Large Language Model
arxiv_id: '2401.07102'
source_url: https://arxiv.org/abs/2401.07102
tags:
- prompt
- code
- response
- language
- operators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM GP, an evolutionary algorithm that uses
  Large Language Models to evolve code. LLM GP adapts traditional genetic programming
  operators by leveraging LLMs for tasks like initialization, selection, and variation
  through prompt engineering.
---

# Evolving Code with A Large Language Model

## Quick Facts
- arXiv ID: 2401.07102
- Source URL: https://arxiv.org/abs/2401.07102
- Authors: Erik Hemberg; Stephen Moskal; Una-May O'Reilly
- Reference count: 40
- One-line primary result: LLM-based evolutionary operators can generate competitive code solutions but are significantly slower and costlier than traditional GP

## Executive Summary
This paper introduces LLM GP, an evolutionary algorithm that uses Large Language Models to evolve code by adapting traditional genetic programming operators through prompt engineering. A simplified implementation, Tutorial-LLM GP, demonstrates the approach on symbolic regression, comparing it to standard GP and random search. Results show that while LLM-based operators can generate competitive solutions, they introduce significant computational costs and challenges related to prompt design and LLM behavior.

## Method Summary
The LLM GP algorithm integrates Large Language Models into evolutionary computation by using prompt engineering to task the LLM with traditional genetic programming operator functions such as initialization, selection, mutation, crossover, and replacement. The method requires defining prompt functions that generate context-specific prompts at runtime, using few-shot examples to guide the LLM's code generation. The approach is implemented in a simplified form called Tutorial-LLM GP, which evolves symbolic expressions for regression problems by representing programs as text sequences and evaluating their fitness externally.

## Key Results
- LLM-based operators generate code solutions competitive with traditional GP on symbolic regression problems
- The approach is significantly slower and more costly than standard GP due to LLM interaction overhead
- Careful prompt design is essential for effective LLM-based evolutionary operators
- The method highlights both the potential and challenges of integrating LLMs into evolutionary algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based evolutionary operators can generate code solutions competitive with traditional GP.
- Mechanism: The LLM leverages its pre-trained pattern-matching and sequence-completion capabilities to generate code snippets that satisfy evolutionary operator tasks (initialization, mutation, crossover, selection).
- Core assumption: The LLM has been pre-trained on sufficient code examples to recognize and generate syntactically correct code patterns for the target problem domain.
- Evidence anchors:
  - [abstract]: "Results show that LLM-based operators are significantly slower and costlier but can generate competitive solutions."
  - [section]: "When prompted with a variation operators’ task, it can demonstrate startling capabilities that allow it to potentially respond, on average, with a code snippet that is a better adaptation than adaptations the equivalent GP operator generates."
  - [corpus]: No direct evidence; weak support.
- Break condition: The LLM's pre-training data lacks relevant code patterns, or the prompt engineering fails to effectively communicate the operator task.

### Mechanism 2
- Claim: Prompt engineering is crucial for effective LLM-based evolutionary operators.
- Mechanism: Carefully designed prompts guide the LLM to generate code that adheres to the desired syntax, semantics, and fitness criteria.
- Core assumption: The LLM's response is highly sensitive to the prompt's content and structure, and effective prompts can be crafted to elicit the desired code generation behavior.
- Evidence anchors:
  - [abstract]: "The work highlights both the potential and challenges of integrating LLMs into evolutionary algorithms, emphasizing the need for careful prompt design..."
  - [section]: "The prompts “task” the LLM to fulfill the purpose of the operator... The prompts must be created at run-time. This means that the LLM GP must be provided with prompt-function hyper-parameters..."
  - [corpus]: No direct evidence; weak support.
- Break condition: The LLM's response is insensitive to prompt variations, or the prompt engineering process is too complex or resource-intensive.

### Mechanism 3
- Claim: LLM-based evolutionary operators introduce new costs and challenges compared to traditional GP.
- Mechanism: Interacting with the LLM for each prompt incurs time and monetary costs, and the LLM's pre-training cost is a hidden expense. The LLM's black-box nature and sensitivity to prompts pose additional challenges.
- Core assumption: The LLM's inference time and token-based pricing model result in significant operational costs, and the LLM's behavior is unpredictable and difficult to control.
- Evidence anchors:
  - [abstract]: "Results show that LLM-based operators are significantly slower and costlier..."
  - [section]: "Because Algorithm 1 integrates an LLM, the run-time cost of interfacing with the LLM to execute a prompt and collect a response needs to be considered. It can be broken down into time and money."
  - [corpus]: No direct evidence; weak support.
- Break condition: The LLM's inference becomes faster and cheaper, or the LLM's behavior becomes more predictable and controllable.

## Foundational Learning

- Concept: Large Language Models (LLMs) and their capabilities
  - Why needed here: Understanding LLMs is crucial for grasping how they can be used to evolve code in evolutionary algorithms.
  - Quick check question: What are the key components and training process of an LLM, and how do they enable code generation?

- Concept: Genetic Programming (GP) and its operators
  - Why needed here: Comparing LLM-based operators to traditional GP operators helps highlight the differences and potential advantages of the LLM approach.
  - Quick check question: What are the main evolutionary operators in GP, and how do they manipulate code structures to evolve solutions?

- Concept: Prompt engineering and its role in guiding LLM behavior
  - Why needed here: Effective prompt engineering is essential for leveraging the LLM's capabilities in evolutionary algorithms.
  - Quick check question: What are the key considerations and techniques in prompt engineering, and how do they influence the LLM's code generation?

## Architecture Onboarding

- Component map:
  - LLM GP Algorithm -> LLM-based Operators -> Prompt Functions -> LLM Interface -> Code Evaluation

- Critical path:
  1. Initialize population using LLM-based initialization operator
  2. Evaluate and assign fitness to solutions
  3. Select parents using LLM-based selection operator
  4. Create offspring using LLM-based crossover and mutation operators
  5. Replace population using LLM-based replacement operator
  6. Repeat steps 2-5 until termination criterion is met
  7. Return the best solution found

- Design tradeoffs:
  - LLM vs. traditional GP operators: LLM-based operators offer potential advantages in code generation but introduce new costs and challenges
  - Prompt complexity vs. effectiveness: More complex prompts may lead to better code generation but increase the risk of errors and computational overhead
  - LLM model selection: Different LLM models may have varying capabilities and costs, requiring careful selection based on the problem domain and resource constraints

- Failure signatures:
  - LLM API errors or timeouts
  - Incorrectly formatted LLM responses
  - Poor code quality or fitness scores
  - Excessive computational time or costs
  - Sensitivity to prompt variations

- First 3 experiments:
  1. Implement a simple LLM-based initialization operator and compare its performance to random initialization in a basic GP problem
  2. Design and test different prompt engineering techniques for the LLM-based mutation operator, evaluating their impact on code quality and fitness
  3. Compare the runtime and cost of LLM-based selection and replacement operators to their traditional GP counterparts in a medium-scale problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM GP algorithms integrate design explorations related to cooperation, modularity, reuse, or competition?
- Basis in paper: [explicit] The paper lists this as a potential research direction in the "Research Questions for LLM GP" section.
- Why unresolved: The paper does not explore how these design principles could be incorporated into LLM GP algorithms.
- What evidence would resolve it: Implementing and testing LLM GP algorithms that incorporate these design principles, and comparing their performance to standard LLM GP algorithms.

### Open Question 2
- Question: What is the most accurate computational complexity of LLM GP?
- Basis in paper: [explicit] The paper lists this as a potential research direction in the "Research Questions for LLM GP" section.
- Why unresolved: The paper does not provide a detailed analysis of the computational complexity of LLM GP.
- What evidence would resolve it: Developing a theoretical framework for analyzing the computational complexity of LLM GP, and using this framework to derive the complexity of different LLM GP algorithms.

### Open Question 3
- Question: How can LLM GP solve prompt composition or other LLM development and use challenges?
- Basis in paper: [explicit] The paper lists this as a potential research direction in the "Research Questions for LLM GP" section.
- Why unresolved: The paper does not explore how LLM GP can be used to address these challenges.
- What evidence would resolve it: Developing and testing LLM GP algorithms that can generate high-quality prompts for LLM development and use, and comparing their performance to human-generated prompts.

## Limitations

- Limited empirical evidence: Claims about competitive solution quality are based only on symbolic regression problems
- Qualitative cost analysis: Lacks specific runtime comparisons across problem sizes and concrete cost metrics
- Insufficient exploration: Impact of different LLM models and prompt engineering strategies is not thoroughly investigated

## Confidence

- High confidence: The core methodology of integrating LLMs into evolutionary algorithms is technically sound
- Medium confidence: Claims about competitive solution quality relative to traditional GP
- Low confidence: Cost-benefit analysis and generalizability across problem domains

## Next Checks

1. Benchmark LLM GP against traditional GP on multiple problem types beyond symbolic regression, including benchmark optimization problems
2. Conduct systematic ablation studies varying LLM model sizes, prompt complexity, and few-shot examples to quantify their impact on performance
3. Measure and report concrete computational costs (tokens, time, monetary) across different problem scales and compare to traditional GP overhead