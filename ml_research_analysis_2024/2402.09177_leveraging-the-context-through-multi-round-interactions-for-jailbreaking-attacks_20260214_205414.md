---
ver: rpa2
title: Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks
arxiv_id: '2402.09177'
source_url: https://arxiv.org/abs/2402.09177
tags:
- attack
- context
- information
- attacks
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Contextual Interaction Attack, a novel jailbreaking
  method that exploits the context vector in large language models (LLMs) to elicit
  harmful responses. Unlike prior attacks that directly query harmful prompts, this
  approach uses multi-round interactions with preliminary harmless questions that
  gradually align with the attack query.
---

# Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks

## Quick Facts
- arXiv ID: 2402.09177
- Source URL: https://arxiv.org/abs/2402.09177
- Reference count: 40
- Novel jailbreaking method exploiting context vectors through multi-round interactions achieves high success rates across seven LLMs

## Executive Summary
This paper introduces the Contextual Interaction Attack, a novel jailbreaking method that exploits the context vector in large language models (LLMs) to elicit harmful responses. Unlike traditional attacks that directly query harmful prompts, this approach uses multi-round interactions with preliminary harmless questions that gradually align with the attack query. An auxiliary LLM generates these preliminary questions via in-context learning from human-crafted examples. The attack demonstrates high success rates across seven LLMs including ChatGPT, GPT-4, Llama2, Vicuna, and Mistral, outperforming both hand-crafted and automated jailbreaking methods.

## Method Summary
The Contextual Interaction Attack is a black-box jailbreaking technique that leverages multi-round interactions to exploit LLM context vectors. The attack begins with preliminary harmless questions that are semantically related to the harmful target query. An auxiliary LLM generates these preliminary questions through in-context learning using a small set of human-crafted examples. The attack gradually transitions from harmless to harmful queries across multiple interaction rounds, maintaining contextual coherence while progressively introducing harmful content. This approach is designed to circumvent traditional safety mechanisms by embedding harmful requests within seemingly innocuous conversations.

## Key Results
- Achieves high success rates in eliciting harmful responses across seven LLMs (ChatGPT, GPT-4, Llama2, Vicuna, Mistral)
- Outperforms both hand-crafted and automated jailbreaking methods in comparative evaluations
- Demonstrates strong transferability as a black-box attack across different model architectures

## Why This Works (Mechanism)
The attack exploits the context vector by maintaining semantic continuity across multiple interaction rounds. By gradually transitioning from harmless to harmful content, the attack avoids triggering immediate safety mechanisms that typically detect isolated harmful queries. The auxiliary LLM's in-context learning capability allows it to generate contextually appropriate preliminary questions that serve as stepping stones toward the harmful target. This approach leverages the model's natural tendency to maintain conversational coherence while progressively introducing content that would normally be blocked in single-round attacks.

## Foundational Learning
- **Context Vector Exploitation**: Understanding how LLMs maintain and utilize context across multiple turns is crucial for this attack's success. Quick check: Verify that context retention mechanisms differ between RLHF and constitutional AI models.
- **In-Context Learning**: The auxiliary LLM's ability to generate appropriate preliminary questions depends on learning from human-crafted examples. Quick check: Test whether automated generation of these examples maintains attack effectiveness.
- **Multi-Round Interaction Dynamics**: The gradual transition strategy relies on understanding how models process sequential prompts differently than isolated queries. Quick check: Compare success rates of multi-round vs. single-round approaches with identical final prompts.

## Architecture Onboarding
**Component Map**: Auxiliary LLM -> Preliminary Question Generator -> Multi-Round Interaction Pipeline -> Target LLM
**Critical Path**: Human-crafted examples → In-context learning → Preliminary question generation → Gradual query escalation → Harmful response elicitation
**Design Tradeoffs**: The method balances between maintaining conversational naturalness and progressively introducing harmful content, sacrificing directness for stealth.
**Failure Signatures**: Attack fails when preliminary questions trigger safety mechanisms prematurely or when context window limitations truncate the gradual transition.
**First Experiments**:
1. Test attack effectiveness against RLHF vs. constitutional AI aligned models
2. Evaluate automated vs. human-crafted example generation for preliminary questions
3. Assess impact of context window size on attack success rates

## Open Questions the Paper Calls Out
None

## Limitations
- Limited model diversity tested (seven models may not capture full range of safety mechanisms)
- Unclear scalability of human-crafted examples for in-context learning
- Effectiveness against constitutional AI alignment strategies not explicitly tested

## Confidence
- High confidence: Attack methodology and experimental results demonstrating success rates
- Medium confidence: Claims about the attack representing a "new direction" for understanding LLM vulnerabilities
- Medium confidence: Transferability claims given limited model diversity

## Next Checks
1. Test attack against models with constitutional AI alignment and compare to RLHF-trained models
2. Evaluate automation potential beyond current human-crafted examples
3. Assess robustness against models with different context window sizes and information retention mechanisms