---
ver: rpa2
title: 'GLEE: A Unified Framework and Benchmark for Language-based Economic Environments'
arxiv_id: '2410.05254'
source_url: https://arxiv.org/abs/2410.05254
tags:
- game
- games
- alice
- economic
- players
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GLEE is a unified framework for benchmarking large language models\
  \ in sequential, language-based economic games. It standardizes three fundamental\
  \ game families\u2014bargaining, negotiation, and persuasion\u2014with consistent\
  \ parameterization, allowing systematic study of agent behavior under varying information\
  \ structures, horizons, and communication forms."
---

# GLEE: A Unified Framework and Benchmark for Language-based Economic Environments

## Quick Facts
- arXiv ID: 2410.05254
- Source URL: https://arxiv.org/abs/2410.05254
- Reference count: 40
- Key outcome: GLEE standardizes benchmarking of LLMs in language-based economic games, revealing significant performance differences based on game parameters, opponent choice, and human vs. LLM distinctions

## Executive Summary
GLEE provides a unified framework for benchmarking large language models in sequential, language-based economic games. It standardizes three fundamental game families—bargaining, negotiation, and persuasion—with consistent parameterization, enabling systematic study of agent behavior under varying information structures, horizons, and communication forms. Using the framework, researchers collected data from 587K decisions across 80K LLM vs. LLM games and an additional 3.4K human vs. LLM games. The results reveal that market parameters significantly influence efficiency and fairness, and that model performance is highly interdependent on opponent choice.

## Method Summary
The framework implements three game families with consistent parameterization across horizon, information structure, communication form, and model identities. Data collection involved 587K LLM decisions from 80K games across 13 LLMs and 1,320 game configurations, plus 3.4K human vs. LLM games. Linear regression analysis predicts efficiency, fairness, and self-gain metrics from game configurations and model identities, with feature encoding for parameters and players.

## Key Results
- Humans outperform LLMs in persuasion but underperform in negotiation; in bargaining, they excel as Alice but lag as Bob
- Market parameters significantly influence efficiency and fairness outcomes across all game families
- Model performance is highly interdependent on opponent choice, with no absolute best-performing models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consistent parameterization enables systematic comparison across game types
- Mechanism: The framework uses a unified parameter space applied across bargaining, negotiation, and persuasion families, allowing isolation of variable effects
- Core assumption: Parameter effects are comparable across game families when structurally analogous
- Evidence anchors:
  - [abstract]: "standardizes three fundamental game families—bargaining, negotiation, and persuasion—with consistent parameterization"
  - [section]: "Central to the framework is a clear and comprehensive parameterization of the space of all bargaining, negotiation and persuasion games"
  - [corpus]: Weak - corpus lacks direct comparative analysis of parameter consistency across families
- Break condition: Parameter interactions differ fundamentally across game families, making cross-comparison misleading

### Mechanism 2
- Claim: Linear regression on normalized game configurations reveals model-independent effects
- Mechanism: By fitting regression models that predict metrics from full game parameterization, the analysis isolates the effect of each parameter while controlling for game structure and opponent identity
- Core assumption: Linear relationships adequately capture the dependence of outcomes on parameters
- Evidence anchors:
  - [abstract]: "fit linear regression models that predict each target metric based on the full parameterization of the game"
  - [section]: "We fit linear regression models that predict each target metric based on the full parameterization of the game and the participating players"
  - [corpus]: Weak - corpus doesn't validate the linearity assumption with alternative models
- Break condition: Non-linear interactions between parameters dominate, making linear approximations inadequate

### Mechanism 3
- Claim: Human behavioral patterns differ systematically from LLM patterns across game roles
- Mechanism: Humans exhibit anchoring bias in bargaining (Alice role) and overall performance extremes, while LLMs show more consistent behavior patterns that vary with opponent choice
- Core assumption: Observable behavioral patterns can distinguish human from LLM decision-making
- Evidence anchors:
  - [abstract]: "Humans outperform LLMs in persuasion but underperform in negotiation; in bargaining, they excel as Alice but lag as Bob"
  - [section]: "when Bob is a human and Alice is a LLM, the correlation between Alice's first offer and her final payoff was 0.63, indicating a strong anchoring effect on the human participant"
  - [corpus]: Weak - corpus doesn't provide corpus-level evidence of systematic human-LLM differences
- Break condition: LLM training data includes sufficient human behavioral patterns to make LLM and human behavior indistinguishable

## Foundational Learning

- Concept: Game theory fundamentals (Nash equilibrium, subgame perfection, information structures)
  - Why needed here: Understanding the theoretical basis for evaluating whether LLM behavior is "rational" and how different information structures affect outcomes
  - Quick check question: What distinguishes a subgame-perfect equilibrium from a Nash equilibrium in sequential games?

- Concept: Linear regression interpretation (beta coefficients, confidence intervals)
  - Why needed here: The statistical analysis relies on interpreting regression coefficients to understand parameter effects while controlling for confounders
  - Quick check question: How do you interpret a beta coefficient of 2.5 with 95% CI [1.8, 3.2] in the context of parameter effects?

- Concept: Natural language processing evaluation (prompt engineering, model behavior analysis)
  - Why needed here: Understanding how LLMs interpret instructions and generate responses in language-based economic interactions
  - Quick check question: What factors influence how an LLM responds to a negotiation prompt beyond the literal instruction?

## Architecture Onboarding

- Component map: Configuration Generator → LLM Interface → Data Collection Manager → Analysis Pipeline → Visualization Tools
- Critical path: Configuration → LLM Interface → Data Collection → Analysis Pipeline → Visualization
- Design tradeoffs: Standardized prompts vs. optimal prompts for each model; comprehensive parameter space vs. computational feasibility; controlled experiments vs. ecological validity
- Failure signatures: Inconsistent model behavior across identical configurations; regression models failing to converge; human attention checks failing at high rates
- First 3 experiments:
  1. Run a single bargaining configuration with two different LLMs to verify basic functionality
  2. Test parameter sensitivity by varying one parameter at a time across multiple models
  3. Compare LLM vs. human behavior in a simple persuasion game configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the effects of complete information on fairness and efficiency in bargaining games vary depending on whether linguistic communication is allowed?
- Basis in paper: [inferred] The paper notes that "the effect of complete information on fairness and efficiency may depend on whether message allowance is enabled," particularly in bargaining games where "complete information appears to reduce efficiency and fairness only when linguistic messages are allowed."
- Why unresolved: The paper only observes this interaction without conducting a comprehensive theoretical investigation to explain the underlying mechanisms or conditions under which this dependency occurs.
- What evidence would resolve it: Experimental results systematically varying information structure and communication forms across multiple bargaining scenarios, combined with theoretical analysis explaining the observed patterns.

### Open Question 2
- Question: What specific characteristics of human decision-making lead to the anchoring effect observed in bargaining games, where humans as Alice achieve higher payoffs than LLMs but as Bob achieve lower payoffs?
- Basis in paper: [explicit] The paper states "humans tend to anchor their negotiation behavior to the initial proposal, often in a consistent yet irrational manner," and provides correlation data showing strong anchoring when Bob is human but weak anchoring when Alice is human.
- Why unresolved: The paper identifies the anchoring effect but does not investigate the psychological mechanisms behind this differential performance or test interventions that might mitigate the bias.
- What evidence would resolve it: Controlled experiments testing various framing conditions, decision support tools, or training interventions to reduce anchoring effects, measuring their impact on human performance in different bargaining roles.

### Open Question 3
- Question: How do different LLM architectures and training approaches influence their performance in language-based economic games, and which architectural features correlate with success across different game types?
- Basis in paper: [explicit] The paper tests 13 different LLMs but notes that "the choice of the LLMs tend to have complex and interdependent effects on the economic outcome," and that "there are no absolute best-performing models in terms of any of the evaluation measures."
- Why unresolved: While the paper identifies that performance depends on opponent choice and game type, it does not analyze the underlying architectural differences or training methodologies that might explain these performance variations.
- What evidence would resolve it: Comparative analysis of model architectures, training data characteristics, and fine-tuning strategies correlated with performance metrics across different game families, identifying specific features that predict success in particular economic contexts.

## Limitations

- Parameter effects assumption: The framework assumes parameter effects are comparable across game families without direct comparative analysis validation
- Linear relationship assumption: The statistical analysis assumes linear relationships adequately capture outcome dependencies on parameters without testing non-linear alternatives
- Limited human data: Systematic differences between human and LLM behavior patterns are based on limited human data (3.4K games) and may not generalize

## Confidence

- **High Confidence**: The core framework design and parameterization consistency across game families; the basic statistical methodology using linear regression; the systematic data collection approach across 587K LLM decisions
- **Medium Confidence**: The interpretation of parameter effects from regression coefficients; the claims about human vs. LLM performance differences; the assertion that market parameters significantly influence efficiency and fairness
- **Low Confidence**: The assumption of linear parameter effects without testing non-linear alternatives; the generalizability of human behavioral patterns from limited data; the framework's applicability to more complex economic scenarios beyond the three studied game families

## Next Checks

1. **Cross-validation of parameter consistency**: Run comparative analyses across all three game families using identical parameter values to verify that the same parameters produce comparable effects, addressing the gap in corpus evidence for cross-family comparability.

2. **Non-linear relationship testing**: Implement and compare polynomial regression or other non-linear models against the reported linear regression results to quantify the potential bias from assuming linearity in parameter effects.

3. **Expanded human subject validation**: Collect additional human decision data across more diverse demographic groups and game configurations to strengthen the claims about systematic human vs. LLM behavioral differences and reduce reliance on limited human samples.