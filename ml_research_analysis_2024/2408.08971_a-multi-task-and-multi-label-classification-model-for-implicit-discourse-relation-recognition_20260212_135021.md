---
ver: rpa2
title: A Multi-Task and Multi-Label Classification Model for Implicit Discourse Relation
  Recognition
arxiv_id: '2408.08971'
source_url: https://arxiv.org/abs/2408.08971
tags:
- discourse
- pdtb
- sense
- discogem
- single-label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multi-task multi-label approach to implicit
  discourse relation recognition (IDRR), addressing the subjectivity in interpreting
  discourse relations without explicit connectives. The model jointly learns probability
  distributions over discourse senses across all three levels of the PDTB 3.0 framework
  using a DiscoGeM corpus.
---

# A Multi-Task and Multi-Label Classification Model for Implicit Discourse Relation Recognition

## Quick Facts
- arXiv ID: 2408.08971
- Source URL: https://arxiv.org/abs/2408.08971
- Authors: Nelson Filipe Costa; Leila Kosseim
- Reference count: 30
- Primary result: New state-of-the-art for DiscoGeM-based single-label IDRR with 65.89 F1-score at level-1

## Executive Summary
This paper introduces a multi-task multi-label approach to implicit discourse relation recognition (IDRR) that addresses the inherent subjectivity in interpreting discourse relations without explicit connectives. The model jointly learns probability distributions over discourse senses across all three levels of the PDTB 3.0 framework using a DiscoGeM corpus. In multi-label experiments, it achieves a Jensen-Shannon distance of 0.299 at level-1, 0.447 at level-2, and 0.529 at level-3. The approach also sets a new state-of-the-art for DiscoGeM-based single-label IDRR with F1-scores of 65.89 at level-1, 55.99 at level-2, and 50.82 at level-3.

## Method Summary
The method employs a multi-task model with a shared RoBERTa-base encoder and three parallel classification heads, one for each PDTB sense level. For multi-label classification, the model uses mean absolute error (MAE) loss to minimize the difference between predicted and target probability distributions, treating all senses equally. For single-label classification, an additional pooling layer selects the highest probability sense, and cross-entropy (CE) loss is used to maximize the probability of the correct class. The model is trained on DiscoGeM corpus and evaluated using Jensen-Shannon distance for multi-label settings and weighted F1-score for single-label settings.

## Key Results
- Multi-label IDRR achieves Jensen-Shannon distance of 0.299 at level-1, 0.447 at level-2, and 0.529 at level-3
- Single-label IDRR achieves F1-scores of 65.89 at level-1, 55.99 at level-2, and 50.82 at level-3 on DiscoGeM
- Sets new state-of-the-art for DiscoGeM-based single-label IDRR
- Transfer learning to PDTB 3.0 shows significant performance degradation due to corpus disagreement

## Why This Works (Mechanism)

### Mechanism 1
Multi-task learning with shared encoder and per-level classification heads enables hierarchical coherence in implicit discourse relation predictions. The model processes discourse argument pairs through a shared RoBERTa encoder, producing contextualized embeddings that feed into three parallel classification heads—one for each PDTB sense level. Because the same embedding is used across all levels, lower-level predictions can inform higher-level sense choices, promoting coherence between levels. Core assumption: Discourse senses at lower levels provide useful inductive bias for higher-level sense predictions, and joint training on all levels improves overall representation quality. Evidence: "Our approach features a multi-task model that jointly learns multi-label representations of implicit discourse relations across all three sense levels in the PDTB 3.0 framework." Break condition: If the correlation between levels is weak or annotations are inconsistent across levels, the shared encoder may propagate noise rather than signal.

### Mechanism 2
MAE loss for multi-label IDRR yields lower Jensen-Shannon (JS) distance between predicted and target distributions than CE loss. MAE directly minimizes the absolute difference between predicted and target probability distributions, treating all senses more equally and avoiding the softmax dominance that CE can induce. This encourages the model to assign non-zero probability mass to all plausible senses, matching the multi-label nature of DiscoGeM annotations. Core assumption: The multi-label ground truth in DiscoGeM reflects genuine uncertainty or multiple valid interpretations; minimizing average error across all senses is more appropriate than focusing only on the top prediction. Evidence: "Models trained with MAE consistently achieve lower JS distances in the multi-label setting." Break condition: If the multi-label annotations are noisy or inconsistent, MAE may overfit to spurious probability mass rather than meaningful sense distributions.

### Mechanism 3
Cross-entropy (CE) loss is more effective for single-label IDRR because it maximizes the probability of the highest-scoring sense, matching the evaluation metric (weighted F1). CE loss amplifies the penalty for misclassifying the top-ranked sense by using log-likelihood over the softmax distribution. This aligns training with the goal of correctly identifying the single most probable sense, which is exactly what weighted F1 measures. Core assumption: The evaluation of single-label IDRR is dominated by correct top-1 classification; intermediate probability distributions are irrelevant to performance. Evidence: "We evaluate model performance in this setting using the weighted F1-score based on the majority label in the predicted and target sense distributions." Break condition: If the single-label annotations are ambiguous or the F1 metric is too coarse, CE may overfit to dominant classes and ignore subtler sense distinctions.

## Foundational Learning

- Concept: Jensen-Shannon (JS) distance as a metric for comparing probability distributions.
  - Why needed here: Multi-label IDRR produces a probability distribution over possible senses; JS distance quantifies how closely the predicted distribution matches the multi-label ground truth.
  - Quick check question: Given two distributions P and Q over the same set of senses, how does JS distance behave if P is uniform but Q has all mass on one sense?

- Concept: Cross-entropy (CE) loss for classification.
  - Why needed here: CE loss maximizes the probability assigned to the correct class in single-label classification, directly aligning with the weighted F1 evaluation metric.
  - Quick check question: If the target label is "CONTINGENCY" and the model outputs a distribution with 0.6 on "CONTINGENCY" and 0.4 on "CAUSE", what is the CE loss?

- Concept: Mean absolute error (MAE) loss for regression-style prediction of probabilities.
  - Why needed here: In multi-label IDRR, the model must output soft probability scores for multiple senses; MAE treats all sense predictions equally and avoids the dominance effects of softmax.
  - Quick check question: For a two-sense prediction [0.7, 0.3] and ground truth [1.0, 0.0], what is the MAE?

## Architecture Onboarding

- Component map: Input text → RoBERTa encoder → Linear + Dropout → Level-1 head → Level-2 head → Level-3 head → Outputs

- Critical path: Input → RoBERTa → Linear+Dropout → Level-1 head → Level-2 head → Level-3 head → Outputs. Backward: Compute losses per head → Backprop through shared encoder → Update parameters.

- Design tradeoffs:
  - Shared encoder vs. separate encoders: Shared encoder reduces parameters and enforces cross-level coherence but may propagate errors.
  - Parallel heads vs. cascaded heads: Parallel heads allow independent per-level prediction but do not enforce strict hierarchical constraints; cascaded heads would enforce coherence but increase model complexity.
  - MAE vs. CE vs. MSE for multi-label: MAE provides stable gradients and balanced treatment of all senses; CE focuses on top-1 and can overfit; MSE is more sensitive to large errors.

- Failure signatures:
  - All heads output uniform distributions: Likely underfitting or incorrect loss scaling.
  - Level-1 predictions dominate Level-2/3 regardless of input: Possible overparameterization of lower-level head or lack of regularization.
  - Sudden drops in JS distance after epoch N: Possible learning rate misconfiguration or vanishing gradients in shared encoder.

- First 3 experiments:
  1. Train with only the shared encoder and a single classification head for Level-1; verify basic classification performance and loss convergence.
  2. Add Level-2 head and switch to MAE loss; monitor JS distance and check for coherence between Level-1 and Level-2 predictions.
  3. Introduce cosine annealing decay and compare JS distance vs. linear decay to identify optimal learning rate schedule.

## Open Questions the Paper Calls Out

### Open Question 1
How would targeted data augmentation for underrepresented senses in DiscoGeM impact model performance on individual senses? The authors note that the model struggles to predict underrepresented senses such as SYNCHRONOUS, CONTRAST, SIMILARITY and SUBSTITUTION, which appear infrequently in the training data. This remains unresolved as the paper identifies the imbalance in sense distribution as a potential issue but does not explore data augmentation techniques to address it. Evidence that would resolve it: Experiments comparing model performance before and after applying data augmentation techniques (e.g., paraphrasing, synonym replacement) specifically for underrepresented senses, showing improvements in F1-scores for those senses.

### Open Question 2
Would sharing predictions from higher-level to lower-level classification heads improve cross-level coherence in the model's predictions? The authors suggest that "one possible method to generate more coherent predictions across sense levels, would be to share the predictions of lower-level senses to help inform the prediction of higher-level senses within the model." This remains unresolved as the paper identifies potential coherence issues between levels but does not implement or test this architectural modification. Evidence that would resolve it: Experiments comparing cross-level coherence (percentage of coherent predictions between levels) with and without information sharing between classification heads, showing improved coherence scores when predictions are cascaded.

### Open Question 3
How would intermediate fine-tuning on PDTB 3.0 after pretraining on DiscoGeM affect transfer learning performance? The authors note that zero-shot transfer learning from DiscoGeM to PDTB 3.0 underperforms models trained directly on PDTB 3.0, and suggest that "future work should explore intermediate fine-tuning on PDTB 3.0 after pretraining on DiscoGeM." This remains unresolved as the paper only evaluates zero-shot transfer learning and does not explore intermediate fine-tuning strategies. Evidence that would resolve it: Experiments comparing model performance on PDTB 3.0 after: (1) zero-shot transfer, (2) intermediate fine-tuning with varying amounts of PDTB 3.0 data, and (3) direct training on PDTB 3.0, showing whether intermediate fine-tuning bridges the performance gap.

## Limitations
- Transfer learning performance significantly degraded due to corpus disagreement between DiscoGeM and PDTB 3.0
- Lack of detailed qualitative analysis of predicted sense distributions and their interpretability
- No exploration of data augmentation techniques for underrepresented senses

## Confidence

**High Confidence**: The mechanism of using shared encoder with parallel classification heads for hierarchical coherence is well-supported by the architecture description and standard multi-task learning principles. The choice of MAE loss for multi-label and CE loss for single-label settings aligns with established practices in classification.

**Medium Confidence**: The claim about MAE yielding better JS distance performance is supported by reported results but lacks ablation studies comparing different loss functions directly. The effectiveness of the hierarchical coherence assumption depends on the actual correlation structure between annotation levels, which is not empirically validated.

**Low Confidence**: The transfer learning results are difficult to interpret without a detailed analysis of corpus disagreement patterns. The paper establishes that transfer is challenging but does not identify specific annotation differences or propose methods to bridge the gap between corpora.

## Next Checks

1. **Ablation Study on Loss Functions**: Conduct controlled experiments comparing MAE, CE, and MSE losses for multi-label IDRR on the same DiscoGeM dataset to isolate the impact of loss choice on JS distance performance, including analysis of gradient stability and convergence patterns.

2. **Corpus Disagreement Analysis**: Perform detailed annotation comparison between DiscoGeM and PDTB 3.0 to identify specific sense distribution differences, annotation guidelines variations, and domain effects that contribute to transfer learning challenges.

3. **Qualitative Analysis of Predictions**: Analyze the multi-label output distributions on test data to assess whether the model's uncertainty predictions align with human interpretation patterns, including case studies of instances where the model assigns high probability to multiple senses versus single-sense dominance.