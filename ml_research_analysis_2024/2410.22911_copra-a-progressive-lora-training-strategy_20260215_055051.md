---
ver: rpa2
title: 'CopRA: A Progressive LoRA Training Strategy'
arxiv_id: '2410.22911'
source_url: https://arxiv.org/abs/2410.22911
tags:
- lora
- copra
- learning
- training
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CopRA, a novel progressive training strategy
  for LoRA that incorporates random layer dropping with increasing participation probability
  during training. The method treats each LoRA layer as a player in a cooperative
  game, optimizing the Shapley value of LoRA parameters.
---

# CopRA: A Progressive LoRA Training Strategy

## Quick Facts
- arXiv ID: 2410.22911
- Source URL: https://arxiv.org/abs/2410.22911
- Reference count: 30
- Key outcome: Progressive layer dropping with increasing participation probability during LoRA training enables linear mode connectivity for efficient model merging and pruning

## Executive Summary
CopRA introduces a novel progressive training strategy for LoRA that incorporates random layer dropping with increasing participation probability during training. The method treats each LoRA layer as a player in a cooperative game, optimizing the Shapley value of LoRA parameters. By progressively activating more layers throughout training while maintaining parameters near intermediate local optima, CopRA achieves linear mode connectivity that enables efficient model merging for federated and multi-task learning applications.

## Method Summary
CopRA modifies LoRA training by implementing a progressive random layer dropping mechanism where each LoRA layer is activated with increasing probability throughout training (from 0 to 1 over the first 3/4 of training iterations). This creates multiple local optima that remain close enough to enable linear interpolation between models. The method approximates Shapley value optimization by treating layers as players in a cooperative game where random subset selection estimates marginal contributions. Training uses standard LoRA with low-rank decomposition (rank r=2) on a ViT-B/16 backbone, with learning rates coordinated with the probability schedule to maintain stability.

## Key Results
- Achieves 64.07% accuracy on DTD dataset in federated learning setting vs 54.37% for standard LoRA
- Achieves 79.25% accuracy on UCF101 dataset in multi-task learning setting vs 73.01% for standard LoRA
- Excels in pruning tasks, maintaining high accuracy even with significant parameter reduction

## Why This Works (Mechanism)

### Mechanism 1
Progressive layer dropping enables linear mode connectivity by ensuring final LoRA parameters remain close to intermediate local optima that satisfy layer-wise linear mode connectivity. Early training with few active layers creates local optima that are easier to connect linearly. As training progresses with more layers and decreasing learning rates, parameters explore globally while staying near earlier local optima.

### Mechanism 2
Random layer dropping approximates optimization of Shapley values for each LoRA layer by treating layers as players in a cooperative game. Each layer's marginal contribution is estimated through sampling subsets with increasing participation probability, creating a multi-level optimization process that approximates Shapley value calculation without explicit computation.

### Mechanism 3
Progressive training enables better pruning performance by forcing each layer to be more individually expressive when active. Early-stage training with fewer active layers creates redundancy that can be exploited during pruning while maintaining accuracy.

## Foundational Learning

- **Linear mode connectivity**: Why needed - core to CopRA's claim about enabling efficient model merging; Quick check - What property must two sets of model parameters have to enable linear interpolation between them without significant loss in performance?
- **Shapley value in cooperative game theory**: Why needed - paper explicitly frames LoRA layers as players in a cooperative game; Quick check - In cooperative game theory, what does the Shapley value represent for a player in a coalition?
- **Low-rank adaptation (LoRA)**: Why needed - CopRA specifically modifies LoRA training parameters; Quick check - How does LoRA modify the weights of a pre-trained model using low-rank matrices?

## Architecture Onboarding

- **Component map**: Bernoulli sampling for layer activation with increasing probability → progressive probability schedule → standard LoRA training for active layers → Shapley value approximation through subset sampling
- **Critical path**: The probability schedule: p(t) = min{4t/(3T), 1} determines which layers are active at each training step
- **Design tradeoffs**: Exploration (early training with few active layers) vs exploitation (late training with all layers active); learning rate schedule must coordinate with probability schedule
- **Failure signatures**: Poor merging performance indicates broken linear mode connectivity; low pruning performance suggests layers weren't made sufficiently expressive; unstable training indicates probability or learning rate schedule needs adjustment
- **First 3 experiments**:
  1. Verify linear mode connectivity by training two CopRA models with different seeds and testing interpolation accuracy
  2. Test merging performance by interpolating between trained models and comparing against LoRA baseline
  3. Evaluate pruning robustness by applying structured and unstructured pruning at various sparsity levels

## Open Questions the Paper Calls Out

### Open Question 1
How does the approximation of Shapley values through multilinear extension affect the convergence and stability of the training process? The paper mentions using multilinear extension but doesn't analyze its impact on training dynamics. This remains unresolved because the paper focuses on practical benefits without theoretical analysis of approximation effects.

### Open Question 2
What is the relationship between the number of training iterations and the effectiveness of the progressive layer dropping strategy? The paper briefly investigates different numbers of iterations but doesn't provide comprehensive analysis of how iteration count affects the progressive dropping strategy.

### Open Question 3
How does CopRA perform in federated learning scenarios with heterogeneous model architectures across clients? The paper demonstrates effectiveness in federated learning but assumes homogeneous model architectures, leaving the question of architectural heterogeneity unresolved.

## Limitations
- Claims about linear mode connectivity and Shapley value optimization lack direct empirical validation
- Experimental scope limited to vision-language models using specific CLIP-LoRA configurations
- Pruning results lack ablation studies to isolate progressive training effects

## Confidence

**High Confidence**: Experimental results showing improved merging and pruning performance (64.07% vs 54.37% on DTD, 79.25% vs 73.01% on UCF101)

**Medium Confidence**: Core claims about linear mode connectivity and Shapley value optimization - compelling theoretical framework but limited direct evidence

**Low Confidence**: Generality across different model architectures, tasks, and LoRA configurations due to narrow experimental focus

## Next Checks

1. **Linear Mode Connectivity Verification**: Train two CopRA models with different random seeds, systematically test linear interpolation at ratios 0.0, 0.25, 0.5, 0.75, 1.0, and measure accuracy stability across the interpolation path.

2. **Shapley Value Approximation Analysis**: Conduct controlled experiments varying the layer dropping probability schedule while keeping all other factors constant, measuring how schedule changes affect merging performance and individual layer contributions.

3. **Cross-Architecture Generalization Test**: Apply CopRA to a different architecture family (BERT for text tasks or ResNet for vision) using the same progressive layer dropping strategy, comparing merging and pruning performance against standard LoRA.