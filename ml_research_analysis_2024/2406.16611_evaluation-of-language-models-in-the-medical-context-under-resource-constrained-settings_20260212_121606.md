---
ver: rpa2
title: Evaluation of Language Models in the Medical Context Under Resource-Constrained
  Settings
arxiv_id: '2406.16611'
source_url: https://arxiv.org/abs/2406.16611
tags:
- language
- performance
- text
- these
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates 53 small pre-trained language models (110M\u2013\
  13B parameters) on medical text classification and conditional text generation tasks\
  \ under resource-constrained settings. Three classification approaches\u2014contextual\
  \ embedding similarity, natural language inference, and multiple-choice question\
  \ answering\u2014are tested on datasets covering general medical knowledge and radiology-specific\
  \ knowledge."
---

# Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings

## Quick Facts
- arXiv ID: 2406.16611
- Source URL: https://arxiv.org/abs/2406.16611
- Authors: Andrea Posada; Daniel Rueckert; Felix Meissen; Philip Müller
- Reference count: 40
- Primary result: Model architecture and training objectives are more critical than model size for medical NLP performance

## Executive Summary
This study evaluates 53 small pre-trained language models (110M–13B parameters) on medical text classification and conditional text generation tasks under resource-constrained settings. Three classification approaches—contextual embedding similarity, natural language inference, and multiple-choice question answering—are tested on datasets covering general medical knowledge and radiology-specific knowledge. The results show that model architecture, training data, and training objectives are more critical than model size alone for generalization. BioLORD and SapBERT models excel in contextual embedding similarity, while instruction-tuned T5 models (Flan-T5, T0) and instruction-tuned LLaMA models perform best in multiple-choice QA. Prompt engineering significantly impacts performance, sometimes more than model size. For conditional text generation, LLaMA models achieve the lowest perplexities, outperforming domain-specific models like BioGPT.

## Method Summary
The study evaluates 53 pre-trained language models (110M-13B parameters) on medical text classification and generation tasks using zero-shot learning approaches. Three classification methods are implemented: contextual embedding similarity using sentence transformers, natural language inference with premise-hypothesis pairs, and multiple-choice question answering with instruction-tuned models. Models are evaluated on three datasets: Transcriptions (EHRs, 2074 samples), MS-CXR (X-ray reports, 718 samples), and MIMIC-CXR (57,711 samples) for generation tasks. Performance metrics include AUC score, F1-score, precision, recall for classification, and perplexity for generation. The evaluation is conducted on a Quadro RTX 8000 GPU with float16 precision for models under 8B parameters.

## Key Results
- Model architecture and training objectives are more critical than parameter count for task performance
- Instruction-tuned models (Flan-T5, T0, LLaMA) outperform larger base models in classification tasks
- BioLORD and SapBERT excel in contextual embedding similarity for medical classification
- LLaMA models achieve lowest perplexities in conditional text generation, outperforming domain-specific models like BioGPT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt engineering can significantly improve classification performance without retraining models
- Mechanism: Carefully constructed prompts frame classification tasks as natural language instructions, leveraging models' instruction-tuned capabilities to generalize from few or zero examples
- Core assumption: Models have internalized patterns from training data that can be activated through appropriate prompting
- Evidence anchors:
  - [abstract]: "Prompt engineering significantly impacts performance, sometimes more than model size"
  - [section D.1.3]: "Prompting importance is thus highlighted not only by the high performance achieved but also by the brittleness of the models"

### Mechanism 2
- Claim: Model architecture and training objectives are more critical than raw parameter count for task performance
- Mechanism: Specific architectural choices and pre-training tasks shape internal representations most useful for downstream tasks, outweighing benefits of simply increasing parameter count
- Core assumption: Architectural and training differences create more meaningful variance in performance than size alone
- Evidence anchors:
  - [section]: "training data and objectives are more decisive in small pre-trained language models"
  - [section D.1.1]: "BioLORD and SapBERT models excel in contextual embedding similarity... performance gains are evidenced when more training data is used"

### Mechanism 3
- Claim: Domain specialization does not guarantee superior performance; general models with appropriate training can match or exceed specialized ones
- Mechanism: General models trained on diverse, high-quality data may develop more robust and generalizable representations, while domain specialization can overfit to narrow data distributions
- Core assumption: High-quality, diverse training data is more beneficial than narrow domain-specific data
- Evidence anchors:
  - [section]: "domain specialization of models using only one of these datasets... may limit their generalization ability"
  - [section D.1.1]: "domain-specific models considered... achieve lower scores than expected... challenging the superiority of domain-specific models"

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The study relies heavily on zero-shot classification approaches to evaluate models without fine-tuning
  - Quick check question: Can you explain how zero-shot learning differs from few-shot learning and why it's valuable in resource-constrained settings?

- Concept: Transformer architectures (encoder-only, decoder-only, encoder-decoder)
  - Why needed here: Different model families are evaluated for different tasks; understanding their capabilities is crucial for interpreting results
  - Quick check question: What are the key architectural differences between BERT, GPT, and T5, and how do these differences affect their suitability for classification vs. generation tasks?

- Concept: Prompt engineering and instruction tuning
  - Why needed here: The study demonstrates that prompt design can dramatically affect model performance, sometimes more than model size
  - Quick check question: How does instruction tuning change a model's behavior compared to standard pre-training, and why does this make prompting more effective?

## Architecture Onboarding

- Component map: Data preprocessing (EHR and radiology reports) -> Model loading (53 models across three Transformer families) -> Prompt application (context-specific templates) -> Inference execution (zero-shot classification and perplexity calculation) -> Result aggregation (bootstrapping and statistical analysis)

- Critical path: Data preprocessing → Model selection → Prompt construction → Inference → Performance evaluation → Statistical validation. The most time-consuming step is typically inference on large models, especially with float16 precision for models >8B parameters.

- Design tradeoffs: Using zero-shot learning avoids fine-tuning costs but may sacrifice some performance compared to task-specific fine-tuning. Processing long sequences with sliding windows introduces approximation errors but enables evaluation of models with limited context windows.

- Failure signatures: Poor prompt alignment manifests as random or nonsensical outputs. Domain mismatch appears as consistently lower scores across multiple models. Hardware limitations cause memory errors or extremely slow inference times.

- First 3 experiments:
  1. Test a single model (e.g., BERTBASE) on a small subset of the transcriptions dataset with default prompts to verify the pipeline works
  2. Compare performance of BioLORD vs. general BERT models on the contextual embedding similarity task to validate domain specialization claims
  3. Test instruction-tuned vs. base T5 models on the multiple-choice QA task to quantify the impact of instruction tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model size specifically impact performance for different architectural families (encoder-only, decoder-only, encoder-decoder) in medical text classification tasks?
- Basis in paper: [explicit] The paper states "The results do not provide sufficient evidence that only increasing the model size, in number of parameters, leads to an improvement in performance, whether comparing different or the same models." and "Although model size may be a relevant factor in determining performance, it is hypothesized that training data and objectives are more decisive in small pre-trained language models."
- Why unresolved: The paper mentions this hypothesis but doesn't provide conclusive evidence due to limited sample size. The authors note that "Expanding the sample size and diversity could be essential to validate these observations, considering a minimum of 30 or 35 models per approach."
- What evidence would resolve it: A larger study with at least 30-35 models per architectural family, systematically varying model size while keeping training data and objectives constant, would provide conclusive evidence on the relationship between model size and performance for each architectural family.

### Open Question 2
- Question: How do different prompting strategies affect the performance of domain-specific versus general domain models in medical NLP tasks?
- Basis in paper: [explicit] The paper shows that "The effectiveness of domain specialization in improving performance is not evident in the contextual embedding similarity approach" and "Evidence supporting the effectiveness of domain specialization is still limited and unclear in the multiple-choice question answering approach."
- Why unresolved: While the paper tests various prompting strategies, it doesn't specifically analyze how these strategies differentially impact domain-specific versus general domain models.
- What evidence would resolve it: A systematic comparison of various prompting strategies (e.g., zero-shot, few-shot, chain-of-thought) on both domain-specific and general domain models, measuring performance differences across multiple medical NLP tasks, would clarify the impact of prompting on different model types.

### Open Question 3
- Question: What are the specific characteristics of the outlier samples that cause high perplexity in conditional text generation tasks?
- Basis in paper: [explicit] The paper identifies outliers in the perplexity analysis, stating "Moderate outliers, above quantile 0.75 by 1.5 times the IQR, represent between 7% and 11% of the data, with BioGPT models having the highest percentages."
- Why unresolved: The paper identifies the existence of outliers but doesn't analyze their specific characteristics or why certain models struggle with them more than others.
- What evidence would resolve it: A detailed analysis of the outlier samples, including their linguistic features, medical terminology, and report structure, combined with error analysis of model predictions, would reveal the specific challenges these samples pose and why certain models struggle with them.

## Limitations
- Limited sample size prevents definitive conclusions about the relationship between model size and performance across architectural families
- Classification results rely primarily on smaller datasets (2,074 and 718 samples) rather than the larger MIMIC-CXR dataset
- Domain specialization claims are based on a limited comparison of only three domain-specific models

## Confidence
- High Confidence: Model architecture and training objectives are more critical than parameter count for task performance
- Medium Confidence: Prompt engineering can significantly improve classification performance without retraining models
- Low Confidence: Domain specialization does not guarantee superior performance; general models with appropriate training can match or exceed specialized ones

## Next Checks
1. Conduct a prompt ablation study to systematically vary prompt templates for each classification approach and determine the sensitivity of results to specific prompt formulations
2. Replicate classification experiments using larger subsets of the MIMIC-CXR dataset to verify conclusions about model architecture importance and prompt effectiveness hold at clinical data scales
3. Evaluate the same models on additional medical subdomains (e.g., pathology reports, clinical notes) to test the generalizability of findings about domain specialization versus general model performance