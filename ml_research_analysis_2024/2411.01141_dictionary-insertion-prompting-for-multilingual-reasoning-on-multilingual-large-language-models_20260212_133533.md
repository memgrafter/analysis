---
ver: rpa2
title: Dictionary Insertion Prompting for Multilingual Reasoning on Multilingual Large
  Language Models
arxiv_id: '2411.01141'
source_url: https://arxiv.org/abs/2411.01141
tags:
- latn
- english
- languages
- translation
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dictionary Insertion Prompting (DIP), a lightweight
  method to improve multilingual reasoning in English-centric large language models.
  DIP uses a word-level bilingual dictionary to insert English counterparts into non-English
  prompts, enabling better translation and chain-of-thought reasoning.
---

# Dictionary Insertion Prompting for Multilingual Reasoning on Multilingual Large Language Models

## Quick Facts
- **arXiv ID**: 2411.01141
- **Source URL**: https://arxiv.org/abs/2411.01141
- **Reference count**: 18
- **Key outcome**: DIP improves multilingual reasoning accuracy by inserting English dictionary entries into non-English prompts, achieving up to 61.92% accuracy on GSM8K and 73.50% on SV AMP tasks across 200 languages.

## Executive Summary
This paper introduces Dictionary Insertion Prompting (DIP), a lightweight method to improve multilingual reasoning in English-centric large language models. DIP uses a word-level bilingual dictionary to insert English counterparts into non-English prompts, enabling better translation and chain-of-thought reasoning. Experiments on 200 languages from FLORES-200 using synthetic benchmarks show that DIP significantly outperforms standard prompting, English pivoting, and other baselines, with up to 61.92% accuracy on GSM8K and 73.50% on SV AMP tasks. The method is particularly effective for low-resource languages and is shown to work across multiple models, including ChatGPT, Llama-3.2, and Mixtral.

## Method Summary
Dictionary Insertion Prompting (DIP) constructs bilingual dictionaries by prompting ChatGPT to translate English sentences into target languages, extracting word-level mappings. For multilingual reasoning tasks, DIP inserts English word counterparts directly into non-English prompts using these dictionaries, creating interleaved bilingual sentences. The augmented prompts are then processed by English-centric LLMs, optionally followed by English translation and chain-of-thought reasoning. This approach leverages the model's stronger English reasoning capabilities while maintaining semantic connections to the original language.

## Key Results
- DIP achieves 61.92% accuracy on GSM8K and 73.50% on SV AMP tasks across 200 languages
- Outperforms English pivoting baseline by up to 24.65% on low-resource languages
- Demonstrates consistent improvements across multiple models (ChatGPT, Llama-3.2, Mixtral)
- Particularly effective for low-resource languages where translation quality typically suffers

## Why This Works (Mechanism)
DIP improves multilingual reasoning by creating explicit word-level bilingual alignment through interleaved dictionary insertion, which enhances translation quality by providing local word-to-word mappings that reduce ambiguity. This better translation serves as a clearer English conceptual foundation for subsequent chain-of-thought reasoning, as English-centric LLMs have stronger performance on tasks when prompts are fully in English. The method leverages the model's learned representations optimized for English, unlocking higher-quality reasoning pathways that may not be accessible in non-English languages.

## Foundational Learning
- **Bilingual dictionary construction**: Creating word-level mappings between English and target languages using LLM prompting. *Why needed*: Provides the word-level translations that DIP uses for prompt augmentation. *Quick check*: Dictionary covers most words in reasoning prompts with reasonable accuracy.
- **Chain-of-thought reasoning**: Breaking down complex reasoning tasks into intermediate steps. *Why needed*: Enables systematic problem-solving that benefits from clear English conceptual foundation. *Quick check*: Model generates coherent step-by-step reasoning for simple math problems.
- **Multilingual evaluation metrics**: Using BLEU and chrF++ for translation quality assessment. *Why needed*: Quantifies whether DIP actually improves translation quality. *Quick check*: DIP outputs show higher BLEU scores than baseline translations.

## Architecture Onboarding
**Component Map**: Original prompt → Dictionary insertion → Augmented prompt → LLM processing → English reasoning steps → Final answer

**Critical Path**: The dictionary insertion and augmented prompt generation are critical, as they directly enable the subsequent English reasoning steps that drive performance improvements.

**Design Tradeoffs**: DIP trades increased prompt length and complexity for improved reasoning accuracy. The interleaved dictionary format could potentially confuse the model if overused, but the benefits outweigh this risk for reasoning tasks.

**Failure Signatures**: Poor dictionary quality leads to incorrect word mappings that cascade into reasoning errors. Overly literal translations may not capture idiomatic expressions or context-dependent meanings.

**3 First Experiments**:
1. Verify dictionary construction produces accurate word-level mappings by comparing a sample of dictionary entries against ground truth translations
2. Test DIP on a simple math problem in one low-resource language to confirm the basic mechanism works
3. Compare translation quality (BLEU/chrF++) of DIP outputs versus baseline methods on a held-out validation set

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding DIP's broader applicability and optimization. How does DIP performance vary when using different pivot languages besides English, given that English is used due to high resource availability but other languages might be useful? What is the impact of dictionary quality and coverage on DIP performance, particularly for low-resource languages where dictionary construction may be less reliable? How does DIP scale to reasoning tasks beyond math and commonsense reasoning, such as creative writing or scientific reasoning that may require different reasoning patterns?

## Limitations
- Weak empirical grounding for proposed mechanisms - claims about translation quality and reasoning pathways are inferred from performance gains rather than directly measured
- Reliance on synthetic benchmarks translated from English sources may not reflect real-world multilingual reasoning scenarios
- Dictionary construction method using ChatGPT is opaque and may introduce variability in translation quality across different language pairs

## Confidence
**High Confidence**: DIP improves multilingual reasoning accuracy is well-supported by consistent improvements across multiple models, languages, and reasoning tasks
**Medium Confidence**: DIP works particularly well for low-resource languages is reasonably supported but requires more systematic analysis of which language characteristics correlate with effectiveness
**Low Confidence**: Mechanistic explanations for why DIP works are weakly supported, lacking direct evidence measuring intermediate outputs like translation quality and reasoning coherence

## Next Checks
1. Measure and report BLEU/chrF++ scores for translated outputs produced by DIP versus baseline methods on a held-out validation set to directly validate translation quality improvements
2. Use LLM-as-a-judge or human evaluation to assess coherence and correctness of intermediate reasoning steps in DIP outputs versus baseline outputs
3. Evaluate DIP on genuinely multilingual reasoning tasks that are not translated from English to test whether benefits extend beyond synthetic benchmark setting