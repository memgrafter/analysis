---
ver: rpa2
title: The Explanation Necessity for Healthcare AI
arxiv_id: '2406.00216'
source_url: https://arxiv.org/abs/2406.00216
tags:
- explanation
- variability
- applications
- output
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a framework for determining the appropriate
  level of explainability (XAI) needed in healthcare AI applications. The authors
  categorize explanation necessity into four classes - self-explainable, semi-explainable,
  non-explainable, and new-patterns discovery - based on three key factors: robustness
  of evaluation protocols, variability of expert observations, and representation
  dimensionality.'
---

# The Explanation Necessity for Healthcare AI

## Quick Facts
- arXiv ID: 2406.00216
- Source URL: https://arxiv.org/abs/2406.00216
- Reference count: 26
- Key outcome: Proposes a framework categorizing healthcare AI explainability needs into four classes based on robustness, variability, and dimensionality factors.

## Executive Summary
This paper addresses the critical challenge of determining appropriate levels of explainability (XAI) for healthcare AI applications. The authors propose a systematic framework that categorizes explanation necessity into four distinct classes - self-explainable, semi-explainable, non-explainable, and new-patterns discovery - based on three measurable factors: robustness of evaluation protocols, variability of expert observations, and representation dimensionality. The framework provides practical guidance for researchers and practitioners to identify when and how much explanation is required for trustworthy AI in healthcare settings, moving beyond subjective judgment to a more systematic approach.

## Method Summary
The framework uses a mathematical formulation incorporating three key factors: (1) robustness of evaluation protocols, (2) variability of expert observations, and (3) representation dimensionality. For inter-observer variability, it employs κ-values and Dice Similarity Coefficient scores to quantify agreement between experts. The method involves measuring these factors across different medical AI applications and using predefined thresholds to classify each application into one of four explanation necessity categories. The framework distinguishes between local (patient-level) and global (cohort-level) explanation requirements based on application characteristics.

## Key Results
- Introduces a systematic four-category framework for healthcare AI explainability: self-explainable, semi-explainable, non-explainable, and new-patterns discovery
- Establishes mathematical formulations using inter-observer agreement metrics (κ-values) and evaluation protocol robustness as key determinants
- Provides actionable thresholds for each category that can be applied across different medical applications
- Distinguishes between local and global explanation requirements based on application characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework successfully maps explainability needs to three measurable factors: robustness of evaluation protocols, variability of expert observations, and representation dimensionality.
- Mechanism: By quantifying these three factors, the framework creates a decision space where different combinations of values place AI applications into distinct explanation necessity categories. This structured approach replaces subjective judgment with systematic evaluation.
- Core assumption: The three factors are sufficient to capture the essential dimensions of explanation necessity in healthcare AI applications.
- Evidence anchors:
  - [abstract] "To support this system, we introduce a mathematical formulation that incorporates three key factors: (i) robustness of the evaluation protocol, (ii) variability of expert observations, and (iii) representation dimensionality of the application."
  - [section] "Based on the two parameters of variability in protocols and expert opinions, the proposed categories are defined as presented in Fig. 1."
  - [corpus] Weak - corpus neighbors discuss explainability in healthcare but don't validate the specific three-factor model.
- Break condition: If additional factors beyond these three prove critical for determining explanation necessity, the framework would need expansion.

### Mechanism 2
- Claim: The framework provides actionable thresholds for each category that can be applied across different medical applications.
- Mechanism: By establishing specific numerical thresholds (e.g., κ values for inter-observer agreement, non-acceptance rates for protocol robustness), the framework creates clear decision boundaries that practitioners can use to classify their applications.
- Core assumption: The proposed thresholds are appropriate across different medical domains and risk levels.
- Evidence anchors:
  - [section] "In medical applications, inter-observer variability (observers with same level of experience), a κ-value between 0.00 and 0.20 is classified as 'slight,' while values between 0.21 and 0.40 are deemed 'fair.'"
  - [section] "A robust evaluation protocol is defined by low variability in responses, indicating a clear, well-defined explainable protocol that can be adapted to different experience levels."
  - [corpus] Weak - corpus neighbors mention explainability thresholds but don't validate the specific values proposed in this framework.
- Break condition: If specific medical domains require different threshold values, the framework would need domain-specific calibration.

### Mechanism 3
- Claim: The framework distinguishes between local, global, and combined explanation requirements based on application characteristics.
- Mechanism: By linking the three factors to specific explanation types (local for patient-level decisions, global for cohort-level understanding, or both), the framework guides researchers to implement appropriate explanation methods rather than over- or under-explaining.
- Core assumption: The distinction between local and global explanations aligns with how medical practitioners actually use AI systems.
- Evidence anchors:
  - [abstract] "guiding the required level of explanation; whether local (patient or sample level), global (cohort or dataset level), or both."
  - [section] "Specifically, the literature lacks practical guidance on distinguishing between when the explanation necessity required is for predictions of individual patients or samples, the local level, and when it is required to decode the entire model for predictions of the whole cohort or dataset, the global level."
  - [corpus] Weak - corpus neighbors discuss explainability but don't validate the local/global distinction in healthcare contexts.
- Break condition: If medical practitioners need different explanation granularities than those proposed, the framework would need adjustment.

## Foundational Learning

- Concept: Inter-rater reliability metrics (κ-statistics, Dice Similarity Coefficient)
  - Why needed here: The framework uses these metrics to quantify variability in expert observations, which is one of the three key factors determining explanation necessity.
  - Quick check question: What κ-value range indicates "substantial" agreement between raters according to the Landis and Koch interpretation?

- Concept: Statistical shape models and rigid registration
  - Why needed here: The framework mentions using statistical shape models aligned with global explanations for new-patterns discovery applications.
  - Quick check question: What mathematical operation aligns the global explanations gz with the statistical shape model output in the framework?

- Concept: Dimensionality reduction and principal component analysis
  - Why needed here: The framework defines global explanations as a dimensionality reduction of local explanations across the entire dataset.
  - Quick check question: In the context of the framework, what would the global explanation gz correspond to in a PCA setting?

## Architecture Onboarding

- Component map: The framework consists of two main flows - inter-observer variability assessment and representation dimensionality evaluation. These feed into an XAI Need Decision component that determines the final explanation necessity category, with adjudicating experts resolving any discrepancies.

- Critical path: The key decision flow is: Measure inter-observer variability (same experience level) → Measure protocol robustness (different experience levels) → Calculate average thresholds → Determine initial explanation necessity → Evaluate representation dimensionality → Make final category decision with expert adjudication if needed.

- Design tradeoffs: The framework trades off between comprehensive coverage of medical applications and simplicity of implementation. Using only three factors makes it practical but may miss nuanced cases requiring additional considerations.

- Failure signatures: The framework will fail when: 1) The three factors don't capture all relevant dimensions of explanation necessity, 2) The proposed thresholds don't generalize across medical domains, or 3) The local/global distinction doesn't align with actual clinical needs.

- First 3 experiments:
  1. Apply the framework to a binary classification task for Alzheimer's detection from structural MRI, measuring inter-observer variability among radiologists with different experience levels.
  2. Test the framework on a multi-modal time-series prediction task for sepsis diagnosis, evaluating how the representation dimensionality factor affects the explanation necessity classification.
  3. Validate the framework by comparing its recommendations against actual explanation implementations in published medical AI studies across different risk levels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed framework be adapted to account for dynamic changes in inter-observer variability over time as medical knowledge and AI systems evolve?
- Basis in paper: [inferred] The paper discusses inter-observer variability as a key factor but does not address how this might change over time with evolving medical knowledge and AI capabilities.
- Why unresolved: The framework assumes static variability thresholds, but in practice, both expert consensus and AI performance may improve, requiring periodic recalibration of the thresholds.
- What evidence would resolve it: Longitudinal studies tracking inter-observer variability and evaluation protocol robustness over time, combined with AI performance metrics, could inform dynamic threshold adjustment mechanisms.

### Open Question 2
- Question: How can the framework be extended to incorporate patient-specific factors that may influence the necessity and level of explainability in individual cases?
- Basis in paper: [inferred] The framework focuses on cohort-level analysis but does not address individual patient characteristics that might necessitate different explanation levels.
- Why unresolved: Patient-specific factors like medical history, risk tolerance, and individual understanding of AI could significantly impact the appropriate level of explainability, but the framework does not currently account for this.
- What evidence would resolve it: Clinical studies examining the relationship between patient-specific factors and their comprehension/acceptance of different levels of AI explainability could inform personalized explanation frameworks.

### Open Question 3
- Question: How can the framework be validated across diverse medical specialties with varying levels of AI adoption and different types of decision-making processes?
- Basis in paper: [explicit] The paper states "The proposed framework applies to various computer vision fields, including natural and automotive vision, but further research is needed to define thresholds for other areas."
- Why unresolved: While the framework is proposed for healthcare AI broadly, its effectiveness and generalizability across different medical specialties with unique characteristics and AI applications remains untested.
- What evidence would resolve it: Multi-specialty validation studies applying the framework to different medical domains (e.g., radiology, pathology, cardiology) with varying AI adoption rates and decision-making contexts would demonstrate its cross-specialty applicability and identify necessary adaptations.

## Limitations

- The three-factor model may not capture all dimensions of explanation necessity, particularly for emerging AI architectures like foundation models or multi-modal systems.
- The proposed numerical thresholds for inter-observer agreement and protocol robustness lack empirical validation across diverse medical specialties and risk levels.
- The framework doesn't explicitly address temporal dynamics in healthcare AI, such as how explanation necessity might change as models are deployed and updated over time.

## Confidence

- Core categorization system: Medium
- Universal applicability across all medical domains: Low

## Next Checks

1. Conduct a systematic survey of medical AI practitioners to validate whether the framework's categories align with their actual needs for explanation in clinical practice.
2. Apply the framework to at least five diverse medical AI applications (e.g., radiology, pathology, predictive analytics, drug discovery, patient monitoring) and document where it succeeds or fails.
3. Perform sensitivity analysis on the threshold values to determine how robust the categorization is to variations in the assumed parameters.