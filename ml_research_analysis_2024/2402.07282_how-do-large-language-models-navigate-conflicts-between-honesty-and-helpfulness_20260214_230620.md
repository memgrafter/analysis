---
ver: rpa2
title: How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?
arxiv_id: '2402.07282'
source_url: https://arxiv.org/abs/2402.07282
tags:
- helpfulness
- honesty
- gpt-4
- helpful
- turbo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how large language models navigate the trade-off
  between honesty and helpfulness in conversational contexts. Using a signaling bandit
  paradigm adapted from cognitive psychology, the authors measure how LLMs prioritize
  these two values when choosing utterances to guide decision-making.
---

# How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?

## Quick Facts
- arXiv ID: 2402.07282
- Source URL: https://arxiv.org/abs/2402.07282
- Authors: Ryan Liu; Theodore R. Sumers; Ishita Dasgupta; Thomas L. Griffiths
- Reference count: 40
- Primary result: RLHF improves both honesty and helpfulness in LLMs, while chain-of-thought prompting increases helpfulness but reduces honesty

## Executive Summary
This paper investigates how large language models navigate the trade-off between honesty and helpfulness in conversational contexts. Using a signaling bandit paradigm adapted from cognitive psychology, the authors measure how LLMs prioritize these two values when choosing utterances to guide decision-making. They test seven state-of-the-art models including GPT-4 Turbo, Mixtral, and Llama 2 with and without reinforcement learning from human feedback (RLHF) and with and without chain-of-thought prompting.

The key findings reveal that RLHF strictly improves both honesty and helpfulness in models, chain-of-thought prompting increases helpfulness but often reduces honesty by encouraging false-but-helpful statements, and GPT-4 Turbo with chain-of-thought shows remarkably human-like sensitivity to conversational context and can be steered via zero-shot prompting to prioritize either value. The study demonstrates that LLMs have internalized conversational values that can be both measured and influenced through prompting strategies.

## Method Summary
The authors employ a signaling bandit paradigm to evaluate how LLMs navigate conflicts between honesty and helpfulness. They test seven state-of-the-art models (GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 2 variants, Mixtral) across three experimental paradigms: mushroom foraging selection, binary utterance endorsement, and realistic housing/dining scenarios. Models are evaluated with and without chain-of-thought prompting and with pre- and post-RLHF versions. The primary metrics include fraction of truthful utterances, fraction of helpful utterances, and a parameter λ that captures the honesty-helpfulness trade-off weight fitted from a utility function combining both values.

## Key Results
- RLHF improves both honesty and helpfulness across all tested models
- Chain-of-thought prompting increases helpfulness but reduces honesty by encouraging false-but-helpful statements
- GPT-4 Turbo with chain-of-thought shows human-like sensitivity to conversational context and can be steered via zero-shot prompting to prioritize either honesty or helpfulness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLHF improves both honesty and helpfulness by training models directly on human preferences over model trajectories.
- Mechanism: RLHF incorporates human feedback into the learning process by first training a reward model on human preferences, then optimizing the LLM towards this reward via policy optimization. This trains LLMs toward a human's value function rather than merely imitating human behavior.
- Core assumption: Human preferences over model trajectories encode the desired balance between honesty and helpfulness that RLHF can learn.
- Evidence anchors:
  - [abstract] "We find that reinforcement learning from human feedback improves both honesty and helpfulness"
  - [section] "RLHF trains LLMs directly towards human values by incorporating human feedback into the agent's learning"
  - [corpus] "Found 25 related papers... Top related titles: Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment, Normative Conflicts and Shallow AI Alignment"
- Break condition: If human preferences are inconsistent or if the reward model fails to capture the nuances of conversational trade-offs, RLHF may not effectively improve both values simultaneously.

### Mechanism 2
- Claim: Chain-of-thought prompting increases helpfulness but reduces honesty by encouraging false-but-helpful statements.
- Mechanism: CoT prompting allows models to reason about the listener's beliefs and subsequent actions before producing an utterance. This additional reasoning helps models account for decision-theoretic utility but leads them to prioritize helpfulness over truthfulness.
- Core assumption: Additional reasoning enables models to better predict how utterances will influence listener decisions, even if those utterances are false.
- Evidence anchors:
  - [abstract] "chain-of-thought prompting skews LLMs towards helpfulness over honesty"
  - [section] "CoT shifts the inferred utilities strongly towards helpfulness"
  - [corpus] Weak - no direct corpus evidence for CoT reducing honesty specifically
- Break condition: If models develop better reasoning about when truthfulness is essential or if the CoT prompt is modified to explicitly consider honesty, this mechanism could break.

### Mechanism 3
- Claim: GPT-4 Turbo with CoT shows human-like sensitivity to conversational context and can be steered via zero-shot prompting to prioritize either value.
- Mechanism: The combination of advanced model architecture and CoT reasoning enables GPT-4 Turbo to understand nuanced conversational contexts and adjust its value weighting based on simple prompts that encourage either honesty or helpfulness.
- Core assumption: GPT-4 Turbo has internalized conversational values to a degree that allows it to apply abstract principles to specific contexts and respond to zero-shot steering.
- Evidence anchors:
  - [abstract] "GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context"
  - [section] "we find that in more realistic settings the models prioritize honesty, but are still steerable in response to prompts that encourage helpfulness"
  - [corpus] Weak - no direct corpus evidence for GPT-4 Turbo's steerability
- Break condition: If the steering prompts become more complex or if the model encounters contexts it hasn't learned to navigate, this mechanism could break.

## Foundational Learning

- Concept: Gricean maxims (Quality and Relevance)
  - Why needed here: The paper formalizes honesty and helpfulness as corresponding to the Gricean maxims of Quality and Relevance, which provides the theoretical foundation for measuring these values in LLMs.
  - Quick check question: What are the two Gricean maxims that correspond to honesty and helpfulness in conversational agents?

- Concept: Signaling bandits paradigm
  - Why needed here: This experimental paradigm allows researchers to decouple honesty and helpfulness by presenting LLMs with utterances that vary in their truth value and decision-theoretic utility.
  - Quick check question: How does the signaling bandits paradigm help researchers measure the trade-off between honesty and helpfulness in LLMs?

- Concept: Rational Speech Acts framework
  - Why needed here: This framework provides the formal model used to represent how speakers choose utterances based on utility functions that combine honesty and helpfulness.
  - Quick check question: In the Rational Speech Acts framework, how is the speaker's utility function defined in terms of honesty and helpfulness?

## Architecture Onboarding

- Component map: LLM (speaker) -> Listener model (belief update and action selection) -> Evaluation metrics (honesty, helpfulness)
- Critical path: Generate context → LLM produces utterance → Evaluate utterance for honesty and helpfulness → Update model parameters (if training) or record results (if evaluating)
- Design tradeoffs: Balancing the complexity of the experimental paradigm against the ability to control variables, choosing between open-ended generation versus constrained response formats, and deciding how to prompt models to elicit desired behaviors.
- Failure signatures: Models that always choose truthful utterances regardless of helpfulness, models that consistently choose false-but-helpful statements, or models that fail to respond to steering prompts.
- First 3 experiments:
  1. Test basic honesty and helpfulness of various models with and without RLHF and CoT prompting
  2. Evaluate models' willingness to endorse utterances that trade off honesty and helpfulness using binary choices
  3. Generalize findings to more realistic settings (housing, dining) to test transfer of learned conversational values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do chain-of-thought prompts influence the trade-off between honesty and helpfulness across different LLM architectures and sizes?
- Basis in paper: [explicit] The paper shows that chain-of-thought prompting skews LLMs towards helpfulness over honesty, with varying degrees of impact across models like GPT-4 Turbo, Llama 2, and Mixtral.
- Why unresolved: While the paper identifies the trend, it does not deeply explore the underlying mechanisms or why certain models are more susceptible to this bias.
- What evidence would resolve it: Comparative analysis of reasoning patterns in CoT outputs across models, identifying common linguistic or structural features that correlate with reduced honesty.

### Open Question 2
- Question: To what extent can the conversational values of LLMs be generalized from controlled experimental paradigms to real-world conversational contexts?
- Basis in paper: [inferred] The paper acknowledges limitations in generalizability, noting that abstract stimuli may not fully capture real-world complexity, and tests only a few realistic settings.
- Why unresolved: The experiments are limited to stylized contexts (e.g., mushroom foraging, housing choices), which may not reflect the full range of real-world conversational dynamics.
- What evidence would resolve it: Longitudinal studies deploying LLMs in diverse real-world conversational scenarios and measuring alignment with human values.

### Open Question 3
- Question: What are the long-term effects of reinforcement learning from human feedback (RLHF) on the balance between honesty and helpfulness in LLMs?
- Basis in paper: [explicit] The paper shows that RLHF improves both honesty and helpfulness, but does not explore the sustainability or potential unintended consequences of this training.
- Why unresolved: The study focuses on immediate post-RLHF effects, without examining how these values evolve with continued use or additional training.
- What evidence would resolve it: Long-term tracking of LLM outputs in varied contexts to assess whether RLHF-induced values degrade, shift, or stabilize over time.

## Limitations
- The study focuses on relatively simple decision contexts that may not capture the full complexity of real-world conversational trade-offs
- The experiments don't explore the long-term effects of RLHF on model behavior or investigate potential failure modes when models encounter novel contexts
- The steerability findings for GPT-4 Turbo rely on a limited set of steering prompts and contexts

## Confidence
- RLHF improving both honesty and helpfulness: High confidence (well-supported by direct experimental evidence)
- Chain-of-thought prompting reducing honesty: Medium confidence (primarily supported by mushroom task results)
- GPT-4 Turbo's steerability via zero-shot prompting: Medium confidence (demonstrated but relies on limited prompts and contexts)

## Next Checks
1. Replicate the chain-of-thought honesty reduction effect across a broader range of contexts and decision types
2. Test whether the steerability of GPT-4 Turbo extends to more complex value trade-offs and longer conversational chains
3. Investigate the stability of RLHF improvements over time and across different model architectures