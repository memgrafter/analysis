---
ver: rpa2
title: Revisiting Semi-supervised Adversarial Robustness via Noise-aware Online Robust
  Distillation
arxiv_id: '2409.12946'
source_url: https://arxiv.org/abs/2409.12946
tags:
- adversarial
- training
- labels
- robust
- snord
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper revisits semi-supervised adversarial robustness via
  a method called SNORD (Semi-supervised Noise-aware Online Robust Distillation).
  The authors identify two key issues in existing robust self-training (RST) methods:
  low-quality pseudo labels and difficulty handling noisy training data.'
---

# Revisiting Semi-supervised Adversarial Robustness via Noise-aware Online Robust Distillation

## Quick Facts
- arXiv ID: 2409.12946
- Source URL: https://arxiv.org/abs/2409.12946
- Authors: Tsung-Han Wu; Hung-Ting Su; Shang-Tse Chen; Winston H. Hsu
- Reference count: 40
- Primary result: SNORD achieves up to 90% relative robust accuracy under ℓ∞ = 8/255 AutoAttack with only 0.1-10% labeled data

## Executive Summary
This paper introduces SNORD, a novel method for semi-supervised adversarial robustness that addresses key limitations in existing robust self-training approaches. The authors identify that poor-quality pseudo labels and difficulty handling noisy training data are the main bottlenecks in current methods. SNORD tackles these issues through advanced semi-supervised learning techniques for pseudo label generation, a noise-aware rectification strategy, and online robust distillation. The method demonstrates state-of-the-art performance across multiple datasets and labeling budgets, significantly outperforming existing baselines while maintaining compatibility with adversarial pretraining strategies.

## Method Summary
SNORD is a three-component framework for semi-supervised adversarial robustness. First, it uses advanced SSL algorithms (FixMatch/ReMixMatch) to generate high-quality pseudo labels by combining entropy minimization and consistency regularization. Second, it introduces Noise-Aware Rectification (NAR) to smooth label distributions and handle noisy pseudo labels by fusing one-hot labels with predicted distributions. Third, it employs Online Robust Distillation (ORD) which applies consistency regularization by using the model's own predictions from previous epochs as soft targets. The method trains on CIFAR-10, CIFAR-100, and TinyImageNet-200 with varying labeled/unlabeled splits, using ResNet-18 backbones and evaluating with AutoAttack under L∞ = 8/255 perturbations.

## Key Results
- SNORD achieves up to 90% relative robust accuracy under AutoAttack with only 0.1-10% labeled data
- Outperforms existing RST baselines by significant margins across all tested datasets and labeling budgets
- Shows particular effectiveness at very low labeling budgets (0.1% for CIFAR-10, 1% for CIFAR-100)
- Maintains compatibility with adversarial pretraining while providing consistent improvements

## Why This Works (Mechanism)

### Mechanism 1
Low-quality pseudo labels in existing RST methods cause performance degradation, especially with scarce labels. The RST pipeline uses a standard-trained model to generate pseudo labels, but these labels often have high error rates (>45% in initial stages) when labeled data is scarce. This leads to noisy training data in the adversarial training phase, resulting in poor robustness.

### Mechanism 2
Combining entropy minimization and consistency regularization in pseudo label generation improves label quality. SNORD uses advanced SSL algorithms like FixMatch/ReMixMatch for pseudo label generation, which incorporate both entropy minimization (reducing uncertainty in predictions) and consistency regularization (ensuring predictions remain stable under augmentations). This produces higher quality pseudo labels than standard training alone.

### Mechanism 3
Noise-aware rectification and online robust distillation address the challenge of training with noisy pseudo labels. NAR fuses one-hot labels with predicted distributions to create smoother targets, accounting for label noise in both labeled and unlabeled data. ORD applies consistency regularization by using the model's own predictions from previous epochs as soft targets, enabling learning from diverse label distributions across training.

## Foundational Learning

- Concept: Adversarial training and robustness
  - Why needed here: The paper focuses on improving semi-supervised adversarial robustness, which requires understanding how adversarial training works and how robustness is measured.
  - Quick check question: What is the difference between standard accuracy, PGD-robust accuracy, and AutoAttack accuracy?

- Concept: Semi-supervised learning (SSL) techniques
  - Why needed here: SNORD leverages advanced SSL algorithms (FixMatch, ReMixMatch) for pseudo label generation and incorporates SSL concepts like entropy minimization and consistency regularization.
  - Quick check question: How do entropy minimization and consistency regularization work together in FixMatch?

- Concept: Knowledge distillation and label smoothing
  - Why needed here: NAR uses label fusion techniques similar to knowledge distillation, and ORD is inspired by consistency regularization which often uses soft targets like in distillation.
  - Quick check question: What is the difference between hard labels, soft labels, and label smoothing in training neural networks?

## Architecture Onboarding

- Component map: Pseudo label generator (SSL-trained model using FixMatch/ReMixMatch) -> NAR module -> ORD mechanism -> Robust model (trained with adversarial examples)

- Critical path:
  1. Train pseudo label generator using SSL algorithm
  2. Generate pseudo labels for unlabeled data
  3. Apply NAR to fuse ground truth and pseudo labels
  4. Train robust model with adversarial examples using ORD for consistency
  5. Evaluate on test set with AutoAttack

- Design tradeoffs:
  - Using SSL algorithms adds computational overhead during pseudo label generation but significantly improves label quality
  - NAR and ORD add complexity but enable effective training with noisy labels
  - The framework is compatible with existing adversarial pretraining but may not always benefit from it depending on initial pseudo label quality

- Failure signatures:
  - Poor robustness performance despite high standard accuracy: Indicates overfitting to clean data or ineffective adversarial training
  - Performance degradation when combining with adversarial pretraining: Suggests the initial pseudo labels are already of high quality, and the pretraining adds little value
  - High variance in results across different random seeds: May indicate instability in the training process or sensitivity to hyperparameters

- First 3 experiments:
  1. Implement basic RST pipeline (UAT++) on CIFAR-10 with 1% labels to establish baseline performance
  2. Add SSL-trained pseudo label generator (FixMatch) to RST pipeline and compare performance
  3. Implement NAR and ORD modules separately, then together, to evaluate their individual and combined contributions to robustness

## Open Questions the Paper Calls Out

### Open Question 1
How does SNORD's performance scale with even lower labeling budgets (e.g., 0.01% or 0.001%) on complex datasets like CIFAR-100 or TinyImageNet-200?
- Basis in paper: The paper demonstrates SNORD's effectiveness at very low labeling budgets (0.1% for CIFAR-10, 1% for CIFAR-100, 10% for TinyImageNet-200) but does not explore ultra-low budgets.
- Why unresolved: The paper's experiments stop at 0.1% for CIFAR-10, leaving the performance at even lower budgets unexplored.
- What evidence would resolve it: Experiments testing SNORD at labeling budgets of 0.01% and 0.001% on CIFAR-100 and TinyImageNet-200 would show how well the method scales to extreme low-resource scenarios.

### Open Question 2
How does SNORD perform when applied to different backbone architectures beyond ResNet-18, such as ResNet-50 or EfficientNet?
- Basis in paper: The paper states that all experiments were conducted using ResNet-18, but does not explore other architectures.
- Why unresolved: The paper's results are limited to ResNet-18, so the generalizability to other architectures is unknown.
- What evidence would resolve it: Experiments applying SNORD to different backbone architectures like ResNet-50 or EfficientNet on the same datasets would show if the performance gains are architecture-dependent or more universal.

### Open Question 3
What is the impact of different SSL algorithms (e.g., ReMixMatch, UDA) on SNORD's performance compared to FixMatch?
- Basis in paper: The paper mentions using FixMatch and ReMixMatch for pseudo label generation but does not compare their performance within SNORD.
- Why unresolved: The paper only reports results using FixMatch, leaving the comparison with other SSL algorithms unexplored.
- What evidence would resolve it: Experiments comparing SNORD's performance when using different SSL algorithms (FixMatch, ReMixMatch, UDA) for pseudo label generation would show which SSL method is most effective within the SNORD framework.

## Limitations
- Lacks detailed ablation studies on individual contributions of SSL algorithms versus NAR/ORD components
- Limited analysis of computational overhead introduced by the three-stage pipeline compared to simpler baselines
- No investigation of SNORD's performance on domain-specific tasks or real-world noisy datasets beyond standard image classification benchmarks

## Confidence
- High confidence: Claims about SNORD's superior performance on CIFAR-10, CIFAR-100, and TinyImageNet-200 compared to baselines
- Medium confidence: Claims about the effectiveness of NAR and ORD in handling noisy pseudo labels
- Medium confidence: Claims about compatibility with adversarial pretraining

## Next Checks
1. Perform ablation studies to isolate the contributions of SSL algorithms, NAR, and ORD components to overall performance improvements
2. Measure and report computational overhead introduced by each component of SNORD compared to simpler baselines
3. Test SNORD on domain-specific tasks (e.g., medical imaging, satellite imagery) or real-world noisy datasets to assess generalization beyond standard benchmarks