---
ver: rpa2
title: How Private are DP-SGD Implementations?
arxiv_id: '2403.17673'
source_url: https://arxiv.org/abs/2403.17673
tags:
- privacy
- ablq
- batch
- dp-sgd
- private
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the privacy guarantees of DP-SGD under different
  batch sampling strategies, particularly comparing deterministic, Poisson, and shuffle-based
  samplers. While shuffle-based sampling is commonly used in practice, the privacy
  analysis often assumes Poisson sampling, potentially underestimating privacy loss.
---

# How Private are DP-SGD Implementations?

## Quick Facts
- arXiv ID: 2403.17673
- Source URL: https://arxiv.org/abs/2403.17673
- Reference count: 28
- Primary result: Shuffle-based DP-SGD can provide weaker privacy guarantees than Poisson sampling in certain parameter regimes, particularly for small noise values.

## Executive Summary
This paper investigates the privacy guarantees of DP-SGD under different batch sampling strategies, specifically comparing deterministic, Poisson, and shuffle-based samplers. While shuffle-based sampling is commonly used in practice, privacy analysis often assumes Poisson sampling, potentially underestimating privacy loss. The authors demonstrate that shuffle-based sampling can provide weaker privacy guarantees than Poisson sampling in certain parameter regimes, particularly when noise values are small. Through theoretical analysis and numerical demonstrations, they show significant gaps between privacy parameters computed under different sampling assumptions. The findings suggest practitioners should exercise caution when reporting privacy parameters for DP-SGD implementations, as the choice of batch sampling strategy substantially impacts actual privacy guarantees.

## Method Summary
The paper analyzes privacy guarantees using an Adaptive Batch Linear Query (ABLQ) mechanism with three batch samplers: deterministic (Db,T), Poisson (Pb,T), and shuffle (Sb,T). Theoretical analysis employs dominating pairs and hockey stick divergence to characterize privacy loss curves δB(ε) for different batch samplers B. Numerical demonstrations use PLD and RDP accountants from Google's DP Library to compute privacy parameters. The study compares privacy guarantees between sampling strategies under fixed parameters (σ = 0.4, δ = 10−6, T = 10,000 steps) and provides both theoretical bounds and empirical evidence of privacy parameter gaps.

## Key Results
- Shuffle-based sampling can provide weaker privacy guarantees than Poisson sampling in certain parameter regimes, particularly for small noise values.
- ABLQ S always satisfies stronger privacy guarantees than ABLQ D (δS(ε) ≤ δD(ε) for all ε ≥ 0).
- The privacy guarantee of ABLQ D and ABLQ P are incomparable, with δP(ε) < δD(ε) for small ε and δP(ε) > δD(ε) for large ε.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Shuffling-based DP-SGD provides weaker privacy guarantees than Poisson-based DP-SGD in certain parameter regimes, particularly for small noise values.
- **Mechanism:** In shuffle-based sampling, non-differing records can leak information about the location of the differing record in the shuffled order, while Poisson sampling doesn't have this limitation since each record is independently sampled.
- **Core assumption:** The batch sampler and query method create a scenario where non-differing records reveal the position of the differing record.
- **Evidence anchors:**
  - [abstract] "shuffle-based sampling can provide weaker privacy guarantees than Poisson sampling in certain parameter regimes, particularly for small noise values"
  - [section] "The crucial difference between ABLQ P and ABLQ S is that the privacy analysis of ABLQ P does not depend at all on the non-differing records in the two datasets. In the case of ABLQ S, we observe that for any fixed σ and T, the hockey stick divergence approaches δD(ε) as L → ∞"
  - [corpus] Weak evidence - the corpus contains related papers but no direct supporting evidence for this specific mechanism
- **Break condition:** The mechanism breaks when the noise parameter σ is sufficiently large, or when the number of steps T is small enough that the privacy amplification effect of shuffling becomes negligible.

### Mechanism 2
- **Claim:** ABLQ D and ABLQ P have incomparable privacy loss, with δP(ε) < δD(ε) for small ε and δP(ε) > δD(ε) for large ε.
- **Mechanism:** Based on the hockey stick divergence between the dominating pairs of ABLQ D and ABLQ P. For small ε, the total variation distance between PD and QD is larger than between PP and QP, while for large ε, the hockey stick divergence between PP and QP decays slower than between PD and QD.
- **Core assumption:** The dominating pairs (PD, QD) for ABLQ D and (PP, QP) for ABLQ P are representative of the privacy loss for all adaptive query methods.
- **Evidence anchors:**
  - [abstract] "we show that the privacy guarantee of ABLQ D and ABLQ P are incomparable"
  - [section] "For part (a), first we consider the case of ε = 0. In this case, Deε(P∥Q) is simply the total variation distance between P and Q. Recall that PP = A⊗T and QP = B⊗T , where A = (1 − 1/T)QD + 1/T·PD and B = QD. Observe that D1(A∥B) = 1/T·D1(PD∥QD)."
  - [corpus] Weak evidence - the corpus contains related papers but no direct supporting evidence for this specific mechanism
- **Break condition:** The mechanism breaks when the number of steps T = 1, or when the noise parameter σ = 0.

### Mechanism 3
- **Claim:** ABLQ S enjoys stronger privacy guarantees than ABLQ D.
- **Mechanism:** Based on the joint convexity property of hockey stick divergence. Shuffling the dataset first cannot degrade the privacy guarantee of any mechanism.
- **Core assumption:** Shuffling the dataset does not create new privacy vulnerabilities beyond those already present in the original mechanism.
- **Evidence anchors:**
  - [abstract] "We observe that ABLQ S always satisfies stronger privacy guarantees than ABLQ D, i.e., δS(ε) ≤ δD(ε) for all ε ≥ 0."
  - [section] "Lemma B.2. Fix a mechanism M : X∗ → ∆O, and let MS be defined as MS(x) := M(xπ) for a random permutation π over [n] where xπ := (xπ(1), . . . , xπ(n)). Then, if M satisfies (ε, δ)-DP, then MS also satisfies (ε, δ)-DP."
  - [corpus] Weak evidence - the corpus contains related papers but no direct supporting evidence for this specific mechanism
- **Break condition:** The mechanism breaks when the dataset size n is not equal to b · T, as required by the batch samplers D and S.

## Foundational Learning

- **Concept:** Differential Privacy (DP)
  - **Why needed here:** DP is the fundamental privacy framework being analyzed. Understanding DP definitions and guarantees is crucial to understanding the privacy analysis of DP-SGD.
  - **Quick check question:** What is the difference between (ε, δ)-DP and pure ε-DP? When would you use one over the other?

- **Concept:** Hockey Stick Divergence
  - **Why needed here:** Hockey stick divergence characterizes the privacy loss curve of mechanisms. It's the key tool for comparing privacy guarantees between different batch sampling strategies.
  - **Quick check question:** How does hockey stick divergence relate to the definition of (ε, δ)-DP? What does it mean if Deε(P∥Q) = δ?

- **Concept:** Dominating Pairs
  - **Why needed here:** Dominating pairs characterize the privacy loss of mechanisms. Understanding how to construct and use dominating pairs is crucial for the privacy analysis in the paper.
  - **Quick check question:** What is a tightly dominating pair? How does it differ from a regular dominating pair? Why are they useful for privacy analysis?

## Architecture Onboarding

- **Component map:** ABLQ B mechanism -> Batch samplers (D, P, S) -> Adaptive query method A -> Privacy accountants (RDP, PLD)

- **Critical path:**
  1. Choose batch sampling strategy (D, P, or S)
  2. Define adaptive query method A based on specific DP-SGD implementation
  3. Compute privacy parameters using appropriate privacy accountant (RDP or PLD)
  4. Compare privacy guarantees between different batch sampling strategies

- **Design tradeoffs:**
  - Deterministic sampling (D) vs. Shuffling (S): Shuffling provides better privacy guarantees but is harder to analyze
  - Poisson sampling (P) vs. Shuffling (S): Poisson sampling is easier to analyze but may underestimate privacy loss in shuffle-based implementations
  - Analytical vs. Numerical privacy accounting: Analytical methods provide closed-form expressions but may be loose, while numerical methods are tighter but computationally expensive

- **Failure signatures:**
  - Significant gaps between reported and actual privacy guarantees
  - Unexpected degradation in model performance due to overly conservative privacy parameters
  - Incompatibility between privacy analysis assumptions and actual implementation details

- **First 3 experiments:**
  1. Reproduce the privacy parameter comparisons from Figure 1 using the dp accounting library
  2. Test the privacy guarantees of ABLQ S vs. ABLQ P for different noise parameters σ and number of steps T
  3. Implement a simple DP-SGD with both shuffle and Poisson sampling to empirically compare their privacy guarantees on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can we establish Conjecture 3.2 that the pair (PS, QS) tightly dominates ABLQ S for all adaptive query methods A?
- **Basis in paper:** [explicit] The authors state this as a conjecture in Section 4.3.1, noting they do not rely on it being true for their results.
- **Why unresolved:** The paper only uses the dominating pair (PS, QS) to establish lower bounds on δS(ε), not to claim it's a tight bound. Proving tightness would require showing that for all adjacent datasets x ~ x', the hockey stick divergence Deε(ABLQ S(x)||ABLQ S(x')) is maximized by the specific pair (PS, QS).
- **What evidence would resolve it:** A proof that for any adaptive query method A and any adjacent datasets x ~ x', the output distributions satisfy Deε(ABLQ S(x)||ABLQ S(x')) ≤ max{Deε(PS||QS), Deε(QS||PS)} with equality for some specific adjacent pair.

### Open Question 2
- **Question:** Is there a numerically tight privacy accounting method for ABLQ S that can provide accurate upper bounds on δS(ε)?
- **Basis in paper:** [inferred] The authors provide lower bounds on δS(ε) using specific sets E, but note this is insufficient for practical privacy accounting. They suggest that establishing Conjecture 3.2 might make numerical accountants for computing hockey stick divergence possible.
- **Why unresolved:** The tightly dominating pair for ABLQ S is not known, making it difficult to apply existing privacy accounting techniques like those used for ABLQ P (RDP and PLD methods). Computing high-dimensional integrals over the space RT is computationally challenging.
- **What evidence would resolve it:** Development of a numerical method that can compute upper bounds on Deε(PS||QS) and Deε(QS||PS) to high accuracy, potentially using importance sampling or other approximation techniques. This would enable practical privacy accounting for shuffle-based DP-SGD implementations.

### Open Question 3
- **Question:** How does the privacy guarantee of ABLQ S with multiple epochs compare to single-epoch analysis, and can we extend the current approach to multiple epochs?
- **Basis in paper:** [inferred] The authors note in Section 5 that their approach is limited to "single epoch" mechanisms, while in practice DP-SGD often uses multiple epochs. They suggest extending their approach to multiple epochs would be interesting.
- **Why unresolved:** The current analysis assumes a single pass over the data with deterministic batch size b·T = n. With multiple epochs, the batch sampler would need to resample or reshuffle the data, potentially changing the privacy amplification properties. The interaction between multiple passes and the information leakage about differing record locations is not well understood.
- **What evidence would resolve it:** A privacy analysis framework that can handle multiple epochs of shuffle-based DP-SGD, showing how the privacy loss accumulates over epochs and whether the amplification by shuffling persists or degrades with repeated passes over the data.

## Limitations
- The analysis focuses on a simplified Adaptive Batch Linear Query (ABLQ) model rather than full DP-SGD implementations.
- Theoretical results are asymptotic and may not fully capture finite-sample behavior.
- Numerical demonstrations are limited to specific parameter choices.

## Confidence
- **High:** ABLQ S provides stronger privacy guarantees than ABLQ D
- **Medium:** The privacy guarantee gap between ABLQ P and ABLQ S depends critically on noise parameters
- **Medium:** The privacy parameters of ABLQ D and ABLQ P are incomparable

## Next Checks
1. Implement the three batch samplers and verify the numerical privacy parameter gaps shown in Figure 1 using Google's DP Library
2. Extend the analysis to different noise parameters σ and step counts T to map out the parameter regimes where shuffle sampling is significantly weaker than Poisson
3. Conduct experiments on a small real dataset to empirically measure the privacy-utility tradeoff differences between shuffle and Poisson sampling strategies in actual DP-SGD training