---
ver: rpa2
title: Pretraining Billion-scale Geospatial Foundational Models on Frontier
arxiv_id: '2404.11706'
source_url: https://arxiv.org/abs/2404.11706
tags:
- data
- gpus
- shard
- size
- communication
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the scalability of Vision Transformers
  (ViTs) for geospatial applications by pretraining billion-scale models on the Frontier
  exascale supercomputer. Using PyTorch's Fully Sharded Data Parallel (FSDP) library,
  the authors explore different model sizes up to 15 billion parameters, analyzing
  memory usage, communication costs, and throughput across various sharding strategies.
---

# Pretraining Billion-scale Geospatial Foundational Models on Frontier

## Quick Facts
- arXiv ID: 2404.11706
- Source URL: https://arxiv.org/abs/2404.11706
- Authors: Aristeidis Tsaris; Philipe Ambrozio Dias; Abhishek Potnis; Junqi Yin; Feiyi Wang; Dalton Lunga
- Reference count: 31
- Primary result: Billion-scale Vision Transformers pretrained on geospatial data achieve up to 30% accuracy improvement over smaller models on scene classification tasks

## Executive Summary
This study investigates the scalability of Vision Transformers for geospatial applications by pretraining billion-scale models on the Frontier exascale supercomputer. Using PyTorch's Fully Sharded Data Parallel (FSDP) library, the authors explore different model sizes up to 15 billion parameters, analyzing memory usage, communication costs, and throughput across various sharding strategies. Experimental results demonstrate that larger models (3B parameters) achieve significant improvements in scene classification accuracy compared to smaller models across multiple independent datasets.

The performance analysis identifies key bottlenecks in distributed training and provides insights into optimizing foundation model development for remote sensing imagery applications on leadership-class HPC systems. The work establishes a foundation for developing geospatial foundation models that can generalize across diverse downstream tasks with minimal fine-tuning.

## Method Summary
The study pretrains Vision Transformers ranging from 100M to 15B parameters using Masked Autoencoders (MAE) on the MillionAID dataset containing 990K remote sensing images. The models are evaluated through linear probing on four independent datasets: MillionAID, UCM, AID, and NWPU-RESISC45. Distributed training is implemented using PyTorch's FSDP library with various sharding strategies (NO SHARD, FULL SHARD, HYBRID SHARD, SHARD GRAD OP) across multiple GPUs on the Frontier supercomputer. The evaluation focuses on classification accuracy, memory usage, and throughput (images-per-second) to assess scaling behavior.

## Key Results
- Larger models (3B parameters) achieve up to 30% improvement in top1 scene classification accuracy compared to 100M parameter models
- FSDP's HYBRID SHARD strategy provides optimal performance for mid-sized models (3B-5B parameters)
- SHARD GRAD OP strategy scales best for the largest model (15B parameters) on Frontier
- Distributed training with FSDP enables efficient scaling of ViT models to billions of parameters on HPC systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger ViT models (3B parameters) improve classification accuracy by up to 30% compared to 100M parameter models on remote sensing tasks.
- Mechanism: Scaling model size increases the capacity to learn generalizable features from large unlabeled datasets, leading to better downstream performance.
- Core assumption: The increase in model size directly correlates with improved feature extraction and generalization capabilities.
- Evidence anchors:
  - [abstract]: "Our larger 3B parameter size model achieves up to 30% improvement in top1 scene classification accuracy when comparing a 100M parameter model."
  - [section]: "Results in Table III clearly reveal improvements in image classification performance as we scale the model size, for all four datasets evaluated."
  - [corpus]: Weak evidence. No directly related studies found in the corpus that confirm this specific claim.
- Break condition: If the dataset size or diversity is insufficient to leverage the increased model capacity, or if the computational resources cannot support the larger model.

### Mechanism 2
- Claim: Distributed training using PyTorch's FSDP library enables efficient scaling of ViT models to billions of parameters on HPC systems.
- Mechanism: FSDP's model sharding strategies reduce memory footprint and optimize communication costs, allowing larger models to be trained on multiple GPUs.
- Core assumption: The FSDP library effectively manages the trade-off between compute and communication costs during distributed training.
- Evidence anchors:
  - [abstract]: "Moreover, we detail performance experiments on the Frontier supercomputer... where we study different model and data parallel approaches using PyTorch's Fully Sharded Data Parallel library."
  - [section]: "We used PyTorch's Fully Sharded Data Parallel (FSDP) [21] distributed strategy in order to scale both the batch size and the model size."
  - [corpus]: Weak evidence. The corpus contains related works but does not provide direct evidence of FSDP's effectiveness in this specific context.
- Break condition: If the communication overhead becomes too high relative to the compute workload, or if the hardware infrastructure cannot support the required data movement.

### Mechanism 3
- Claim: Self-supervised learning through MAE pretraining enables the development of foundation models that generalize well across diverse downstream tasks.
- Mechanism: MAE pretraining learns robust representations by reconstructing masked image patches, which are then fine-tuned for specific tasks.
- Core assumption: The pretraining task of reconstructing masked patches effectively captures the underlying structure of the data, leading to generalizable features.
- Evidence anchors:
  - [abstract]: "Foundation Models (FMs) are trained with internet-scale unlabeled data via self-supervised learning and have been shown to adapt to various tasks with minimal fine-tuning."
  - [section]: "We first pretrain the models by pairing a MAE formulation with the 1M remote sensing images composing the MillionAID dataset."
  - [corpus]: Weak evidence. The corpus includes related works on MAE but does not provide direct evidence of its effectiveness in this specific context.
- Break condition: If the pretraining dataset is not sufficiently diverse or large, or if the masking strategy does not effectively capture the relevant features for downstream tasks.

## Foundational Learning

- Concept: Vision Transformers (ViTs)
  - Why needed here: ViTs form the backbone of the foundation models being developed for geospatial applications, replacing traditional CNNs with self-attention mechanisms.
  - Quick check question: How does the self-attention mechanism in ViTs differ from the convolution operations in CNNs?

- Concept: Self-Supervised Learning (SSL)
  - Why needed here: SSL allows the models to learn from large unlabeled datasets, which is crucial given the scarcity of labeled geospatial data.
  - Quick check question: What are the main approaches to SSL in computer vision, and how do they differ in their pretraining objectives?

- Concept: Distributed Training and Model Parallelism
  - Why needed here: Training billion-scale models requires distributing the workload across multiple GPUs to manage memory and computational constraints.
  - Quick check question: What are the trade-offs between data parallelism and model parallelism in distributed training?

## Architecture Onboarding

- Component map: Data Loading and Preprocessing -> ViT Model Architecture -> MAE Pretraining -> FSDP Sharding -> Linear Probing
- Critical path:
  1. Load and preprocess the large geospatial datasets
  2. Pretrain the ViT models using MAE on the MillionAID dataset
  3. Apply FSDP sharding strategies for distributed training
  4. Evaluate the pretrained models using linear probing on downstream datasets
- Design tradeoffs:
  - Model Size vs. Computational Resources: Larger models offer better performance but require more computational resources
  - Sharding Strategy vs. Communication Overhead: Different sharding strategies balance memory usage and communication costs
  - Pretraining Duration vs. Downstream Performance: Longer pretraining can improve generalization but increases computational cost
- Failure signatures:
  - Out-of-Memory Errors: Indicates insufficient memory for the model or batch size
  - Slow Throughput: Suggests high communication overhead or inefficient sharding strategy
  - Poor Downstream Performance: Could result from inadequate pretraining or inappropriate model architecture
- First 3 experiments:
  1. Test the ViT-Base model with a small batch size on a single GPU to verify the basic training pipeline
  2. Scale the batch size using FSDP's NO SHARD strategy to understand the impact on throughput and memory usage
  3. Apply different sharding strategies (FULL SHARD, HYBRID SHARD) to the ViT-3B model to evaluate their performance and memory efficiency

## Open Questions the Paper Calls Out

1. **Cross-Task Generalization**: How does the performance of billion-scale geospatial foundational models scale when moving from linear probing to full fine-tuning across diverse downstream tasks? The study focused exclusively on linear probing, which may not fully capture the generalization capabilities of the pretrained models.

2. **Optimal Parallel Strategy**: What is the optimal data parallel strategy for training models between 3B and 15B parameters on Frontier, particularly for configurations that don't fit within a single node? The paper tested specific configurations but didn't develop a principled approach for selecting the best strategy.

3. **Pretraining Objective Comparison**: How do different self-supervised pretraining objectives (MAE vs. contrastive learning) affect the downstream performance of geospatial foundation models at billion-scale parameter sizes? The study only evaluated MAE pretraining despite acknowledging that contrastive approaches could leverage temporal or multi-sensor data correlations.

## Limitations
- Evaluation is limited to scene classification tasks, not exploring other geospatial applications like object detection or segmentation
- Results are specific to the Frontier supercomputer with MI250X GPUs and ROCm runtime, limiting reproducibility on other systems
- The study does not investigate the optimal balance between model size, pretraining duration, and computational cost for different application scenarios

## Confidence
**High Confidence**: The fundamental observation that larger Vision Transformers improve classification accuracy on geospatial datasets is well-supported by experimental results across multiple independent datasets.

**Medium Confidence**: The specific performance numbers (30% accuracy improvement, throughput measurements) are dependent on the particular hardware configuration and software stack used.

**Low Confidence**: The generalization of these findings to other remote sensing tasks beyond scene classification remains unproven, and the scaling laws may not apply to other geospatial modalities.

## Next Checks
1. **Cross-Domain Generalization Test**: Evaluate the pretrained models on diverse geospatial tasks including object detection, semantic segmentation, and time-series analysis to verify whether the scaling benefits extend beyond scene classification.

2. **Transferability Across Hardware**: Reproduce the scaling experiments on different HPC systems (e.g., systems with NVIDIA GPUs) to validate whether the observed performance patterns and bottlenecks are consistent across hardware architectures.

3. **Cost-Benefit Analysis**: Conduct experiments to determine the optimal model size and pretraining duration for different accuracy targets, measuring the trade-off between computational cost (FLOPs, energy consumption) and performance improvements to guide practical deployment decisions.