---
ver: rpa2
title: Semifactual Explanations for Reinforcement Learning
arxiv_id: '2409.05435'
source_url: https://arxiv.org/abs/2409.05435
tags:
- semifactual
- explanations
- semifactuals
- state
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first algorithms for generating semifactual
  explanations for reinforcement learning (RL) agents, which explain outcomes by showing
  "even if" scenarios where the result would remain the same. The authors define five
  properties for informative semifactual explanations in RL and propose two model-agnostic
  algorithms, SGRL-Advance and SGRL-Rewind, that optimize these properties using NSGA-II.
---

# Semifactual Explanations for Reinforcement Learning

## Quick Facts
- arXiv ID: 2409.05435
- Source URL: https://arxiv.org/abs/2409.05435
- Reference count: 37
- Key outcome: Introduces first algorithms for generating semifactual explanations for RL agents that optimize multiple properties using NSGA-II

## Executive Summary
This paper presents the first algorithms for generating semifactual explanations in reinforcement learning (RL). Semifactual explanations describe scenarios where the outcome remains the same despite different actions being taken, providing "even if" scenarios that help users understand agent behavior. The authors propose SGRL-Advance and SGRL-Rewind, two model-agnostic algorithms that use NSGA-II to optimize five properties: validity, temporal distance, stochastic uncertainty, fidelity, and exceptionalism. Experiments on gridworld environments show these algorithms generate more diverse and representative explanations compared to supervised learning baselines, with user studies indicating improved understanding of agent behavior.

## Method Summary
The method involves training a DQN policy on gridworld environments, collecting factual states where each possible action was not chosen, and then using NSGA-II to search for semifactual states that optimize multiple properties simultaneously. SGRL-Advance searches forward in time while SGRL-Rewind searches backward. The algorithms optimize four objectives (temporal distance, stochastic uncertainty, fidelity, exceptionalism) with validity as a constraint. Key hyperparameters include 25 generations, population size 24, and horizon 3 for the search algorithms.

## Key Results
- SGRL-Advance generated semifactuals with 43.66% feature gain and SGRL-Rewind with 44.93% feature gain, compared to 24.21% and 24.58% for baselines
- SGRL algorithms produced 99.97% and 99.99% valid semifactuals versus 93.94% and 92.95% for baselines
- User study showed participants answered 77.58% of questions correctly with SGRL-Advance explanations, with no significant difference between algorithms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Semifactual explanations retain the same outcome as the factual state, providing users with actionable "even if" scenarios that optimize resource usage.
- **Mechanism**: By defining a validity constraint (same outcome) and optimizing temporal distance, stochastic uncertainty, fidelity, and exceptionalism, the algorithms generate semifactuals that are both representative of the policy and surprising to the user.
- **Core assumption**: The outcome can be expressed as a function of the state and that users benefit from understanding alternative scenarios that do not change the outcome.
- **Evidence anchors**:
  - [abstract] "Semifactual explanations aim to explain an outcome by providing "even if" scenarios"
  - [section] "Semifactuals help users understand the effects of different factors on the outcome and support the optimisation of resources"
- **Break condition**: If the outcome function is not well-defined or the policy is not deterministic enough to provide meaningful semifactuals, the mechanism fails.

### Mechanism 2
- **Claim**: SGRL-Advance and SGRL-Rewind use NSGA-II to search for semifactual states that optimize multiple properties simultaneously, resulting in more diverse and representative explanations.
- **Mechanism**: NSGA-II's multi-objective optimization finds a Pareto front of semifactual states, balancing temporal distance, stochastic uncertainty, fidelity, and exceptionalism. This diversity ensures explanations can be tailored to different user preferences.
- **Core assumption**: The state space is searchable and that a diverse set of semifactuals is more informative than a single, potentially biased explanation.
- **Evidence anchors**:
  - [abstract] "propose two model-agnostic algorithms, SGRL-Advance and SGRL-Rewind, that optimize these properties using NSGA-II"
  - [section] "NSGA-II generates a set of diverse solutions"
- **Break condition**: If the search space is too large or the optimization objectives conflict irreconcilably, the algorithm may not converge to useful semifactuals.

### Mechanism 3
- **Claim**: User study results indicate that semifactual explanations improve user understanding of RL agent behavior, even if not significantly more than baselines.
- **Mechanism**: By presenting users with alternative scenarios that retain the same outcome, semifactuals help users predict agent actions and understand the factors influencing those actions.
- **Core assumption**: Users can comprehend and benefit from "even if" scenarios and that their understanding translates to better prediction of agent behavior.
- **Evidence anchors**:
  - [abstract] "A user study showed that participants found the explanations helpful for understanding agent behavior"
  - [section] "Users who were presented with explanations generated by SGRL-Advance were correct in77.58% of questions"
- **Break condition**: If the task complexity is too low or users do not engage with the explanations, the mechanism's effectiveness may not be observable.

## Foundational Learning

- **Concept**: Reinforcement Learning (RL) basics
  - **Why needed here**: Understanding the sequential, trial-and-error nature of RL is crucial for grasping how semifactual explanations can be applied to explain agent behavior.
  - **Quick check question**: What distinguishes RL from supervised learning in terms of how the agent learns?

- **Concept**: Counterfactual vs. Semifactual explanations
  - **Why needed here**: Semifactual explanations are a variant of counterfactuals, focusing on scenarios where the outcome remains the same. This distinction is key to understanding the unique value proposition of semifactual explanations.
  - **Quick check question**: How do semifactual explanations differ from traditional counterfactual explanations?

- **Concept**: Multi-objective optimization with NSGA-II
  - **Why needed here**: The algorithms use NSGA-II to optimize multiple properties simultaneously, which is central to understanding how diverse and informative semifactuals are generated.
  - **Quick check question**: What is the main advantage of using NSGA-II over single-objective optimization for generating semifactuals?

## Architecture Onboarding

**Component map**: Factual states -> NSGA-II optimization -> Semifactual states (SGRL-Advance/SGRL-Rewind) -> User explanations

**Critical path**: DQN policy training → Factual state collection → NSGA-II search → Semifactual generation → User presentation

**Design tradeoffs**: The choice between SGRL-Advance and SGRL-Rewind represents a tradeoff between searching forward vs. backward in time, with potential implications for the types of semifactuals generated and their interpretability.

**Failure signatures**: 
- Low validity rates indicate issues with the outcome function or constraint enforcement
- Poor fidelity suggests problems with Q-value estimation or policy representation
- Low diversity may indicate insufficient search or conflicting optimization objectives

**3 first experiments**:
1. Verify reproduction of factual state collection by executing the trained DQN policy and recording states for each possible action
2. Test the NSGA-II implementation with simplified objectives to ensure proper Pareto front generation
3. Validate the semifactual generation process by checking that generated states maintain the same outcome as their corresponding factual states

## Open Questions the Paper Calls Out

None

## Limitations

- The approach is limited to discrete state and action spaces, restricting applicability to continuous control tasks
- User study with only 16 participants and simplified tasks provides limited evidence for real-world effectiveness
- Lack of full implementation details for key components like NSGA-II operators and Q-value fidelity calculation

## Confidence

- Technical approach and experimental results: Medium
- User study conclusions: Low
- Contribution significance: Medium

## Next Checks

1. Implement the full SGRL algorithms with complete details for NSGA-II operators and Q-value fidelity calculation, then verify reproduction of the reported results on the same environments
2. Conduct a larger-scale user study with more complex RL tasks and scenarios, including both discrete and continuous action spaces
3. Test the approach on more challenging RL benchmarks beyond gridworlds, such as Atari games or MuJoCo continuous control tasks, to evaluate scalability and real-world applicability