---
ver: rpa2
title: Does SGD really happen in tiny subspaces?
arxiv_id: '2405.16002'
source_url: https://arxiv.org/abs/2405.16002
tags:
- training
- learning
- figure
- loss
- subspace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether deep neural networks can be effectively
  trained within the dominant subspace of the training loss Hessian, as recent work
  has shown gradients to approximately align with this low-rank space. The authors
  design a critical experiment where each SGD update is projected onto the dominant
  subspace (Dom-SGD).
---

# Does SGD really happen in tiny subspaces?
## Quick Facts
- arXiv ID: 2405.16002
- Source URL: https://arxiv.org/abs/2405.16002
- Reference count: 40
- Key outcome: Dom-SGD fails to decrease training loss while Bulk-SGD is equally effective as SGD

## Executive Summary
This paper challenges the recent claim that deep neural networks are effectively trained within the dominant subspace of the training loss Hessian. Through carefully designed experiments, the authors demonstrate that the observed alignment between gradients and the dominant subspace is spurious - it does not contribute to loss decrease. The study reveals that projecting SGD updates onto the bulk subspace orthogonal to the dominant subspace is just as effective as standard SGD, while projecting onto the dominant subspace fails to further decrease the training loss.

## Method Summary
The authors design a critical experiment where each SGD update is projected onto either the dominant subspace (Dom-SGD) or the bulk subspace orthogonal to it (Bulk-SGD). They compare these methods with standard SGD across various training scenarios. The experiments test different optimizers, learning rates, and training dynamics including the Edge of Stability regime and Sharpness-Aware Minimization. The analysis examines how stochastic noise affects gradient alignment and how momentum and adaptive methods influence the distribution of gradient components across subspaces.

## Key Results
- Dom-SGD fails to decrease training loss while Bulk-SGD performs equally well as standard SGD
- Spurious alignment between gradients and dominant subspace disappears under full-batch GD
- Momentum and adaptive optimizers amplify bulk subspace components of gradient updates

## Why This Works (Mechanism)
The spurious alignment between gradients and the dominant subspace is caused by the stochastic noise inherent to SGD. When using full-batch GD, this alignment disappears, revealing that the aligned component does not contribute to loss decrease. The mechanism works because SGD noise introduces components that happen to align with the dominant subspace, but these components are not actually useful for optimization. This creates an illusion that SGD is happening within the dominant subspace when in fact the bulk subspace components are driving the optimization process.

## Foundational Learning
- **Hessian Matrix**: The second-order derivative matrix of the loss function, used to analyze curvature and identify dominant subspaces. Needed to understand the theoretical basis for subspace analysis. Quick check: Verify that the Hessian is symmetric and positive semi-definite.
- **Eigenvalue Decomposition**: Method for decomposing the Hessian into eigenvalues and eigenvectors to identify the dominant subspace. Needed to project gradients onto specific subspaces. Quick check: Confirm that eigenvectors are orthogonal and eigenvalues are non-negative.
- **Stochastic Gradient Noise**: Random fluctuations in gradient estimates due to mini-batch sampling. Needed to understand how noise creates spurious alignments. Quick check: Measure the variance of gradients across different mini-batches.
- **Momentum Optimization**: Technique that accumulates past gradients to smooth updates. Needed to understand how it affects subspace component distribution. Quick check: Observe how momentum affects gradient alignment over time.
- **Adaptive Learning Rates**: Methods that adjust learning rates per parameter. Needed to understand their role in amplifying bulk subspace components. Quick check: Compare parameter-wise learning rates across different subspaces.

## Architecture Onboarding
- **Component Map**: Data → Model → Loss Function → Gradient Computation → Subspace Projection → Update Step
- **Critical Path**: Gradient computation → Eigenvalue decomposition of Hessian → Subspace projection → Parameter update
- **Design Tradeoffs**: Computational cost of full Hessian computation vs. approximation methods; choice of projection threshold for dominant subspace; batch size affecting stochastic noise level
- **Failure Signatures**: Dom-SGD fails to decrease loss; spurious alignment disappears with full-batch GD; momentum amplifies bulk components
- **First Experiments**: 1) Test Dom-SGD vs Bulk-SGD on simple linear regression; 2) Measure gradient alignment under different batch sizes; 3) Compare momentum effects on subspace components

## Open Questions the Paper Calls Out
None

## Limitations
- The core finding needs validation across different network architectures and loss landscapes
- Analysis relies heavily on specific experimental conditions and projection threshold choices
- Claim about momentum/adaptive methods amplifying bulk subspace components needs more rigorous theoretical justification

## Confidence
High confidence in Dom-SGD failing to decrease training loss while Bulk-SGD performs equally well as standard SGD. High confidence in spurious alignment disappearing under full-batch GD. Medium confidence in momentum/adaptive methods amplifying bulk subspace components.

## Next Checks
1. Test the Dom-SGD vs Bulk-SGD experiment across multiple architectures (CNNs, Transformers) and loss functions to verify the phenomenon is not dataset or model-specific.
2. Investigate the projection threshold sensitivity by varying the eigenvalue cutoff percentage and measuring how it affects the Dom-SGD and Bulk-SGD performance.
3. Conduct theoretical analysis of how momentum and adaptive methods transform gradient components across dominant and bulk subspaces, potentially using Taylor expansion or other analytical tools.