---
ver: rpa2
title: 'Wake Vision: A Tailored Dataset and Benchmark Suite for TinyML Computer Vision
  Applications'
arxiv_id: '2405.00892'
source_url: https://arxiv.org/abs/2405.00892
tags:
- vision
- wake
- person
- dataset
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Wake Vision introduces a novel automated pipeline for generating
  large-scale binary classification datasets for TinyML, addressing the critical gap
  of insufficient high-quality datasets in the field. The pipeline employs intelligent
  multi-source label fusion, confidence-aware filtering, automated label correction,
  and systematic fine-grained benchmark generation.
---

# Wake Vision: A Tailored Dataset and Benchmark Suite for TinyML Computer Vision Applications

## Quick Facts
- arXiv ID: 2405.00892
- Source URL: https://arxiv.org/abs/2405.00892
- Reference count: 40
- Primary result: Introduces automated pipeline creating 6M image dataset with 6.6% accuracy improvement over existing TinyML datasets

## Executive Summary
Wake Vision presents a novel automated pipeline for generating large-scale binary classification datasets specifically designed for TinyML computer vision applications. The pipeline addresses a critical gap in the field by overcoming the high costs and limitations of manual dataset creation, which struggles to keep pace with the diverse and rapidly evolving requirements of TinyML deployments. Through intelligent multi-source label fusion, confidence-aware filtering, automated label correction, and systematic fine-grained benchmark generation, the pipeline enables the creation of high-quality datasets at scale. The resulting Wake Vision dataset, containing nearly 6 million images for person detection, demonstrates a 6.6% accuracy improvement over existing datasets and provides 100x more training images, while also establishing a comprehensive benchmark suite to evaluate model robustness across five critical dimensions.

## Method Summary
The Wake Vision pipeline automates dataset generation through a four-stage process: (1) collecting images from diverse sources including Open Images v7 and Flickr, (2) employing multi-source label fusion using a weighted consensus approach across multiple annotators and sources, (3) applying confidence-aware filtering to remove low-quality samples and automated label correction using Confident Learning to identify and remove mislabeled instances, and (4) generating fine-grained benchmark suites that systematically vary environmental conditions such as lighting, motion, occlusion, weather, and viewpoint. The pipeline was validated by creating a person detection dataset with 5.8 million training images, achieving state-of-the-art performance through a two-stage training strategy involving knowledge distillation from a larger teacher model to a compact MobileNetV2 architecture with quantization and grayscale conversion for TinyML deployment.

## Key Results
- 6.6% accuracy improvement over existing TinyML datasets using two-stage training strategy
- 100x more training images than previous standard datasets (5.8M vs 50K-100K)
- Label error rates substantially lower than prior work due to Confident Learning correction
- Comprehensive fine-grained benchmark suite reveals failure modes masked by aggregate metrics

## Why This Works (Mechanism)
The automated pipeline addresses the fundamental challenge that TinyML applications require highly specialized, high-quality datasets that are prohibitively expensive to create manually. By leveraging multi-source label fusion and confidence-aware filtering, the pipeline can efficiently process massive amounts of web data while maintaining quality. The systematic fine-grained benchmark generation enables targeted evaluation of model robustness across the specific deployment conditions that matter most for edge devices. The two-stage training strategy with knowledge distillation effectively bridges the gap between large-scale training data and compact model architectures required for TinyML deployment.

## Foundational Learning
- **Multi-source label fusion**: Combines annotations from multiple sources to improve label accuracy and reduce noise. Needed because single-source annotations are error-prone and inconsistent. Quick check: Compare label consistency across different annotation sources before and after fusion.
- **Confidence-aware filtering**: Removes low-confidence samples based on agreement scores across multiple annotators. Needed to ensure dataset quality at scale. Quick check: Measure precision-recall trade-off at different confidence thresholds.
- **Confident Learning**: Automated technique for identifying and correcting mislabeled instances using model confidence scores. Needed because manual error correction is infeasible for large datasets. Quick check: Evaluate label error rate before and after correction.
- **Fine-grained benchmarking**: Systematic evaluation across multiple environmental dimensions rather than aggregate metrics. Needed because TinyML models must perform reliably under specific deployment conditions. Quick check: Identify which environmental factors most impact model performance.

## Architecture Onboarding

**Component Map**: Image Collection -> Multi-source Label Fusion -> Confidence Filtering -> Label Correction -> Benchmark Generation -> Model Training

**Critical Path**: The pipeline's effectiveness depends critically on the quality of label fusion and filtering stages, as errors propagated through these early stages will compound in downstream model training and evaluation.

**Design Tradeoffs**: The pipeline prioritizes automation and scale over perfect manual curation, accepting some trade-offs in absolute label quality for the ability to generate massive datasets quickly. This enables rapid iteration for new target categories but may miss nuanced edge cases that manual curation would catch.

**Failure Signatures**: Poor performance on fine-grained benchmarks despite good overall accuracy indicates domain shift or inadequate representation of specific environmental conditions in training data. Overfitting to training data despite large dataset size suggests insufficient regularization or early stopping.

**First Experiments**:
1. Train MobileNetV2-0.25 on Wake Vision Quality training set for 200 epochs using standard preprocessing pipeline
2. Evaluate trained model on Wake Vision test set and fine-grained benchmarks using accuracy and F1-score metrics
3. Compare performance against models trained on existing TinyML datasets (Visual Wake Words, Person Detection)

## Open Questions the Paper Calls Out
None

## Limitations
- Pipeline relies heavily on Open Images v7 as source data, potentially limiting diversity for new target categories
- Two-stage training strategy requires careful hyperparameter tuning that may not generalize across all TinyML applications
- Fine-grained benchmarks focus on single-domain variations rather than complex multi-factor real-world scenarios

## Confidence

**High confidence**: Claims about automated pipeline effectiveness, dataset quality improvements, and 6.6% accuracy gain are well-supported by systematic experiments and clear metrics.

**Medium confidence**: Claims about generalizability to other target categories are demonstrated but limited to two additional categories; broader applicability remains to be seen.

**Medium confidence**: Comparative analysis with other datasets is robust, but the paper acknowledges that different evaluation methodologies across studies may affect direct comparisons.

## Next Checks
1. Apply the automated pipeline to a domain significantly different from person detection (e.g., medical imaging or industrial defect detection) to validate cross-domain effectiveness.

2. Deploy models trained on Wake Vision to actual edge devices in varying environmental conditions to measure performance degradation and validate the practical utility of the fine-grained benchmarks.

3. Evaluate model performance on rare but critical edge cases within the Wake Vision dataset to assess robustness beyond the controlled fine-grained benchmarks, particularly for safety-critical applications.