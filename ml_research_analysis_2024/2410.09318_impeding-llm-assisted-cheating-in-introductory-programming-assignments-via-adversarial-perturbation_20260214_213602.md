---
ver: rpa2
title: Impeding LLM-assisted Cheating in Introductory Programming Assignments via
  Adversarial Perturbation
arxiv_id: '2410.09318'
source_url: https://arxiv.org/abs/2410.09318
tags:
- perturbation
- problems
- code
- perturbations
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how adversarial perturbations to assignment
  prompts can impede Large Language Models (LLMs) from generating correct solutions
  for introductory programming assignments. The study measures LLM performance on
  84 short and 22 long problems from CS1 and CS2 courses, finding that all five tested
  LLMs fail on CS1 problems, and performance drops significantly for complex, multi-component
  CS2 problems.
---

# Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation

## Quick Facts
- arXiv ID: 2410.09318
- Source URL: https://arxiv.org/abs/2410.09318
- Reference count: 40
- Primary result: Adversarial perturbations can reduce LLM performance on programming assignments by over 80%, with subtle perturbations remaining highly effective when undetected by students.

## Executive Summary
This paper investigates how adversarial perturbations to assignment prompts can impede Large Language Models (LLMs) from generating correct solutions for introductory programming assignments. The study evaluates five LLMs on 84 short and 22 long problems from CS1 and CS2 courses, finding that all models fail on CS1 problems and performance drops significantly for complex CS2 problems. Ten perturbation techniques are designed, including token removal, character substitution, and Unicode character replacement, with the most effective causing over 80% degradation in solution correctness. A user study with 30 undergraduates demonstrates that perturbations reduced average correctness scores by 77%, and subtle perturbations were particularly effective when unnoticed by students.

## Method Summary
The researchers collected programming assignments from CS1 and CS2 courses and evaluated five LLMs (GPT-3.5, GitHub Copilot, Mistral, Code Llama, CodeRL) on their ability to generate correct solutions using test oracles. They then applied ten different perturbation techniques to assignments where LLMs initially scored non-zero, measuring the reduction in correctness as efficacy. The most effective perturbations were tested in a user study where 30 undergraduates attempted to solve perturbed assignments using ChatGPT, with participants reporting their detection strategies and attempting to reverse the perturbations.

## Key Results
- All five tested LLMs failed to solve CS1 problems, with GPT-3.5 achieving only 3.12% correctness on CS2 problems
- The most effective perturbation techniques caused over 80% degradation in LLM performance
- User study showed perturbations reduced average correctness scores by 77% (from 15.43% to 3.54%)
- Subtle perturbations like token substitution and character removal were highly effective when undetected by students

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial perturbations degrade LLM performance by breaking subword token patterns that the model relies on for code generation.
- Mechanism: Perturbation techniques modify prompts in ways that disrupt the statistical patterns LLMs learned during training, causing incorrect code generation.
- Core assumption: LLMs are sensitive to small changes in input prompts, especially those that alter token boundaries or semantic relationships.
- Evidence anchors:
  - [abstract] "The results suggest that carefully designed perturbations can significantly reduce LLM-assisted cheating in introductory programming courses."
  - [section 3.1] "Following existing literature, we design several perturbation techniques and measure their efficacy on the assignments that LLMs solved with non-zero correctness scores."
- Break condition: If LLMs become more robust to adversarial inputs through training or architectural changes, the perturbations may lose effectiveness.

### Mechanism 2
- Claim: Students are unable to effectively reverse perturbations, especially subtle ones, leading to continued degradation of LLM-generated solutions.
- Mechanism: When perturbations are detected, students attempt various strategies to bypass them, but these often fail to fully restore the original prompt semantics.
- Core assumption: Students lack the expertise to fully understand and reverse the effects of adversarial perturbations on complex programming prompts.
- Evidence anchors:
  - [abstract] "Our user study with undergraduates shows that the average efficacy dropped from 15.43% to 15% when perturbations were noticed."
  - [section 4.2] "Despite the higher risk of being noticed, it still managed to deceive the model."
- Break condition: If students develop better skills in detecting and reversing perturbations, or if LLMs improve at understanding perturbed prompts, this mechanism may fail.

### Mechanism 3
- Claim: High variation in LLM-generated solutions correlates with high success rates for perturbations, as the model struggles to find consistent patterns in the modified prompts.
- Mechanism: When perturbations are applied, the LLM generates a wider variety of solutions, many of which are incorrect, making it harder for students to identify and fix the correct solution.
- Core assumption: LLMs are prone to generating diverse outputs when faced with ambiguous or perturbed inputs.
- Evidence anchors:
  - [abstract] "Our findings suggest that existing LLMs generally struggle to solve assignments requiring interactions across multiple functions and classes."
  - [section 3.2] "We observe that the average number of unique variations per problem is 13.9 and 26.0 for problems where perturbation failed and succeeded, respectively."
- Break condition: If LLMs become more deterministic in their outputs or better at handling ambiguous inputs, the variation may decrease.

## Foundational Learning

- Concept: Adversarial perturbation techniques
  - Why needed here: To understand how small changes to input prompts can significantly impact LLM performance.
  - Quick check question: What are some common adversarial perturbation techniques used to degrade LLM performance?

- Concept: Shapley values and SHAP (SHapley Additive exPlanations)
  - Why needed here: To understand how the paper uses SHAP to guide the selection of perturbation units for optimal efficacy.
  - Quick check question: How does SHAP help in selecting the most impactful tokens or sentences for perturbation?

- Concept: Tokenization and subword units in LLMs
  - Why needed here: To understand why breaking subword tokens (e.g., through character removal) can be an effective perturbation strategy.
  - Quick check question: Why are LLMs sensitive to changes in subword token boundaries?

## Architecture Onboarding

- Component map: Dataset (CS1/CS2 assignments) -> LLMs (5 models) -> Perturbation techniques (10 methods) -> Evaluation (correctness scores) -> User study (30 participants)
- Critical path: Perturbation generation → LLM evaluation → User study → Efficacy analysis
- Design tradeoffs: Subtle perturbations (harder to detect) vs. high-change perturbations (more effective)
- Failure signatures: Low efficacy of perturbations, students easily reversing perturbations, LLMs adapting to perturbations
- First 3 experiments:
  1. Measure baseline LLM performance on CS1 and CS2 problems
  2. Apply token removal perturbation and measure efficacy
  3. Conduct user study with sentence removal perturbation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the long-term effectiveness of adversarial perturbations in preventing LLM-assisted cheating as LLMs continue to evolve?
- Basis in paper: [inferred] The paper discusses how model evolution affects both LLM performance on assignments and the efficacy of perturbations.
- Why unresolved: The study only compares GPT-3.5 and GPT-4.0 performance with no longitudinal data on how perturbation effectiveness changes over time.
- What evidence would resolve it: A longitudinal study tracking the same set of perturbation techniques across multiple model versions and timeframes.

### Open Question 2
- Question: How do different perturbation detection and reversal strategies among students vary based on their programming proficiency levels?
- Basis in paper: [explicit] The user study collected demographic data including Python proficiency levels and observed various strategies students used.
- Why unresolved: The paper provides aggregate efficacy data but does not analyze how different proficiency levels correlate with specific detection/reversal strategies.
- What evidence would resolve it: A stratified analysis of participant responses grouped by proficiency levels.

### Open Question 3
- Question: What is the optimal balance between perturbation efficacy and student comprehension in educational contexts?
- Basis in paper: [inferred] The paper acknowledges that perturbations can affect understandability but does not empirically measure this trade-off.
- Why unresolved: The user study focused on perturbation efficacy against LLMs but did not systematically measure student comprehension or task completion difficulty.
- What evidence would resolve it: Controlled experiments measuring both LLM solution success rates and student comprehension under various perturbation intensities.

## Limitations
- The perturbation techniques may not generalize beyond the specific CS1/CS2 assignments tested
- User study sample size of 30 participants limits statistical power for detecting subtle effects
- Results may not extend to newer or differently trained LLMs beyond the five tested models

## Confidence
- High confidence: Core finding that adversarial perturbations can significantly reduce LLM performance in programming assignments
- Medium confidence: Claims about broad applicability across programming courses
- Low confidence: Specific efficacy rankings of different perturbation techniques

## Next Checks
1. **Model Robustness Test**: Evaluate whether fine-tuning LLMs on adversarially perturbed examples reduces the effectiveness of the perturbation techniques.
2. **Cross-Institutional Validation**: Apply the most effective perturbations to programming assignments from different universities and course structures to assess generalizability.
3. **Longitudinal Student Study**: Conduct a follow-up study with the same participants after training them on perturbation detection techniques to measure adaptation rates.