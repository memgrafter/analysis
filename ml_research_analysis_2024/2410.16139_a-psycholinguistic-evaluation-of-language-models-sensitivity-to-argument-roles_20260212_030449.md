---
ver: rpa2
title: A Psycholinguistic Evaluation of Language Models' Sensitivity to Argument Roles
arxiv_id: '2410.16139'
source_url: https://arxiv.org/abs/2410.16139
tags:
- language
- verb
- argument
- human
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates large language models\u2019 sensitivity to\
  \ argument roles using psycholinguistic paradigms. It replicates human experiments\
  \ on verb plausibility in role-appropriate and role-reversed contexts."
---

# A Psycholinguistic Evaluation of Language Models' Sensitivity to Argument Roles

## Quick Facts
- **arXiv ID**: 2410.16139
- **Source URL**: https://arxiv.org/abs/2410.16139
- **Reference count**: 21
- **Primary result**: Models can distinguish plausible from implausible verbs but fail to exhibit human-like consistency across different argument role manipulations.

## Executive Summary
This paper evaluates large language models' sensitivity to argument roles using psycholinguistic paradigms adapted from human experiments. The study examines how well models detect verb plausibility in role-appropriate versus role-reversed contexts through three experimental approaches: surprisal effects, probing classifier performance, and attention weight analysis. Results show that while models can distinguish plausible and implausible verbs, they fail to exhibit human-like consistency across different argument role manipulations. Models perform better when verbs are changed than when arguments are swapped, suggesting reliance on lexical cues rather than structural argument-role processing.

## Method Summary
The study adapts human psycholinguistic experiments on argument role processing, using minimal pairs of sentences with three conditions: swap-arguments, change-verb, and replace-argument. For surprisal analysis, the negative log probability of verbs in plausible versus implausible contexts is computed. Probing classifiers are trained on verb representations from different model layers to predict plausibility. Attention weights are analyzed to track how models allocate attention between subjects and objects relative to the verb. The experiments are conducted across multiple model sizes (GPT-2 small/medium/large, BERT base/large, RoBERTa base/large) to examine scaling effects.

## Key Results
- Models can distinguish plausible from implausible verbs but show inconsistent performance across different argument role manipulations.
- Larger models outperform smaller ones on raw performance metrics but show less human-like processing patterns.
- Models correctly identify argument roles through attention mechanisms but fail to integrate this information into verb plausibility representations.
- GPT-2 small aligns more closely with human N400 patterns than larger models, despite lower overall performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models detect verb plausibility differences but don't use structural processing like humans.
- Mechanism: Models rely on lexical cues and surface-level patterns rather than deeper structural argument-role bindings.
- Core assumption: Plausibility detection stems from statistical co-occurrence rather than structural argument-role assignment.
- Evidence anchors:
  - [abstract] "models can distinguish plausible and implausible verbs but fail to exhibit human-like consistency across different argument role manipulations."
  - [section 8] "it is likely that the models are making use of specific lexical cues to make their inferences rather than the structural relations humans are using."
  - [corpus] Weak support; no direct corpus neighbor evidence for structural vs lexical cue distinction.
- Break condition: If models show consistent performance across all role-manipulation types, the lexical cue hypothesis would weaken.

### Mechanism 2
- Claim: Larger models show better performance but less human-like processing patterns compared to smaller models.
- Mechanism: Scale improves raw performance metrics but shifts processing from online, incremental patterns toward more finalized, offline interpretations.
- Core assumption: Model size affects the type of human behavior they mimic, with smaller models aligning better with early-stage processing.
- Evidence anchors:
  - [section 8] "GPT2-small showed stronger correspondence with the human N400 data patterns, while larger models showed the higher performance in all experiments, which outperformed humans' initial predictive processing capacities."
  - [section 4] "smaller versions of GPT-2 fit human reading times better than larger models."
  - [corpus] No direct neighbor evidence; weak support for scale vs human-likeness tradeoff.
- Break condition: If larger models consistently outperform smaller ones on human-like incremental processing measures, the tradeoff assumption fails.

### Mechanism 3
- Claim: Models correctly identify argument roles through attention but fail to integrate this information into verb plausibility representations.
- Mechanism: Attention mechanisms accurately track subject-object relationships, but this information is not effectively encoded into verb representations for plausibility judgments.
- Core assumption: Attention-based role identification exists but is not sufficient for downstream plausibility computation.
- Evidence anchors:
  - [section 7] "even GPT2-small...correctly allocates attention to subjects...though its attention is also distributed to the object more than the better performing RoBERTa-large."
  - [section 8] "the weak performance could be due to how the models encode the preceding argument role information into the representations of the verb."
  - [corpus] No direct neighbor evidence; weak support for attention-to-plausibility integration gap.
- Break condition: If improving attention integration directly improves plausibility judgments, the mechanism would need revision.

## Foundational Learning

- Concept: Argument roles and their role in sentence processing
  - Why needed here: The paper's core evaluation depends on understanding how argument roles (agent, patient) affect verb plausibility and human processing patterns.
  - Quick check question: What distinguishes argument roles from argument meanings in sentence processing?

- Concept: Surprisal theory and its application to language models
  - Why needed here: Surprisal measures are used to compare model predictions with human processing difficulty and N400 effects.
  - Quick check question: How does surprisal theory connect language model probabilities to human reading times?

- Concept: Psycholinguistic experimental design (minimal pairs, control conditions)
  - Why needed here: The paper adapts human psycholinguistic experiments, requiring understanding of controlled stimuli construction.
  - Quick check question: Why use minimal pairs where only argument roles or verb forms change, rather than full sentence comparisons?

## Architecture Onboarding

- Component map: Surprisal computation -> Probing classifiers -> Attention analysis
- Critical path: For evaluating argument-role sensitivity, the sequence is: extract context and verb → compute surprisal or representations → compare plausible vs implausible conditions → analyze differences across manipulation types.
- Design tradeoffs: Using surprisal provides direct connection to human processing but only captures final-layer information, while probing allows layer-wise analysis but requires additional classifier training. Attention analysis gives insight into role identification but may not reflect downstream integration.
- Failure signatures: If surprisal effects are near zero across all conditions, the model may not capture plausibility distinctions. If probing accuracies are at chance, role information may not be encoded in verb representations. If attention weights are uniformly distributed, role identification may be failing.
- First 3 experiments:
  1. Run surprisal analysis on a small subset of swap-arguments items to verify positive effects for replace-argument and change-verb conditions.
  2. Train probing classifiers on verb representations from different layers for the change-verb condition to establish ceiling performance.
  3. Identify subject attention heads using the replace-argument condition before analyzing swap-arguments attention patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific computational mechanisms allow language models to correctly allocate attention to argument roles but fail to integrate this information into verb representations?
- Basis in paper: Explicit
- Why unresolved: The paper identifies a dissociation between models' ability to track argument roles through attention mechanisms and their ability to use this information for verb plausibility judgments, but does not pinpoint the exact computational bottleneck.
- What evidence would resolve it: Detailed mechanistic analyses using causal intervention methods to identify where in the processing pipeline attention-based role tracking fails to influence verb representations.

### Open Question 2
- Question: How does model scaling affect the generalization across different types of argument role manipulations?
- Basis in paper: Explicit
- Why unresolved: While the paper notes that larger models perform better overall but still fail to show human-like consistency across swap-arguments and change-verb conditions, it does not systematically examine how scaling affects this specific generalization pattern.
- What evidence would resolve it: Comparative analyses across multiple model sizes examining performance patterns on both swap-arguments and change-verb conditions to determine if larger models show improved cross-condition generalization.

### Open Question 3
- Question: Do language models develop abstract representations of argument roles that are independent of specific lexical items?
- Basis in paper: Inferred
- Why unresolved: The paper suggests models may rely on lexical cues rather than structural argument-role processing, but does not directly test whether models can generalize role-based knowledge to novel lexical items.
- What evidence would resolve it: Experiments testing model performance on role-reversal tasks using novel verb-argument combinations not present in training data to determine if role-based generalizations extend beyond memorized lexical patterns.

## Limitations
- The lexical-cue hypothesis remains speculative without direct testing of specific lexical features versus structural processing.
- The comparison across model sizes assumes a tradeoff between performance and human-likeness without establishing causation.
- The attention-to-plausibility integration gap is inferred from performance differences rather than directly tested through ablation or intervention studies.

## Confidence

- **High Confidence**: The empirical findings that models distinguish plausible from implausible verbs and that performance varies across manipulation types are well-supported by experimental results.
- **Medium Confidence**: The interpretation that models rely on lexical cues rather than structural processing is plausible given the evidence but not definitively proven.
- **Low Confidence**: The specific mechanism by which attention weights fail to integrate into verb representations is speculative and not directly tested.

## Next Checks

1. **Ablation Study on Lexical Features**: Remove specific lexical cues (e.g., subject-verb agreement, animacy markers) from input and test whether model performance on argument role tasks degrades more than on control tasks, providing direct evidence for lexical versus structural processing.

2. **Intervention on Attention Integration**: Modify model architecture to force attention-based role information to be more strongly encoded into verb representations, then test whether this improves plausibility judgments, directly testing the attention-to-plausibility integration hypothesis.

3. **Cross-linguistic Validation**: Test models on argument role tasks in languages with different argument structure properties (e.g., pro-drop languages, languages with case marking) to determine whether lexical-cue reliance is language-specific or a general model limitation.