---
ver: rpa2
title: 'MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical
  Queries'
arxiv_id: '2401.01596'
source_url: https://arxiv.org/abs/2401.01596
tags:
- medical
- arxiv
- skin
- visual
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces MedSumm, a multimodal framework for summarizing
  code-mixed Hindi-English clinical queries by integrating visual and textual information.
  The authors developed the MMCQS dataset, which contains 3,015 Hindi-English medical
  queries paired with visual cues, to support this task.
---

# MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries

## Quick Facts
- arXiv ID: 2401.01596
- Source URL: https://arxiv.org/abs/2401.01596
- Authors: Akash Ghosh; Arkadeep Acharya; Prince Jha; Aniket Gaudgaul; Rajdeep Majumdar; Sriparna Saha; Aman Chadha; Raghav Jain; Setu Sinha; Shivani Agarwal
- Reference count: 0
- One-line primary result: MedSumm framework outperforms unimodal baselines on multimodal medical query summarization with automatic and human evaluation metrics.

## Executive Summary
This paper introduces MedSumm, a multimodal framework for summarizing code-mixed Hindi-English clinical queries by integrating visual and textual information. The authors developed the MMCQS dataset containing 3,015 Hindi-English medical queries paired with visual cues to support this task. MedSumm employs large language models and vision-language models to generate medically nuanced summaries, demonstrating superior performance over unimodal approaches across automatic metrics and human evaluation.

## Method Summary
MedSumm uses a multimodal architecture that projects 768-dimensional visual embeddings from vision transformers into the 4096-dimensional textual embedding space of large language models via a linear projection layer. The framework employs QLoRA for efficient fine-tuning of large language models on the MMCQS dataset. The model generates summaries by fusing the projected visual and textual embeddings before decoding, allowing it to capture multimodal information for code-mixed Hindi-English medical queries.

## Key Results
- MedSumm outperforms unimodal baselines across ROUGE, BLEU, and BERTScore metrics
- Improvements in factual recall and multimodal fact capturing compared to text-only approaches
- Human evaluation confirms superior clinical relevance and coherence of multimodal summaries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining textual and visual embeddings in a shared space improves summary quality for multimodal clinical queries.
- Mechanism: The framework projects 768-dimensional visual embeddings from ViT into the 4096-dimensional textual embedding space of LLMs via a linear projection layer. This allows both modalities to be fused before decoding, enabling the model to capture multimodal information.
- Core assumption: Visual information provides complementary cues that are not fully represented in the textual query alone, especially for symptoms difficult to describe in text.
- Evidence anchors:
  - [abstract]: "We demonstrate the value of integrating visual information from images to improve the creation of medically detailed summaries."
  - [section]: "This unified vision-language embedding is then fed into the decoder of the language model."
- Break condition: If the visual cue is irrelevant or redundant with the text, the projection may add noise rather than signal, degrading performance.

### Mechanism 2
- Claim: Fine-tuning large language models with QLoRA allows efficient adaptation to the specialized multimodal summarization task.
- Mechanism: QLoRA applies 4-bit quantization and low-rank adapter updates to LLMs, reducing memory usage while retaining most of the model's capacity for the new task.
- Core assumption: The base LLM has sufficient general language understanding to be adapted for medical summarization without full fine-tuning.
- Evidence anchors:
  - [section]: "QLoRA is a more memory-efficient version of LoRA providing 4-bit quantization to enable fine-tuning of larger LLMs using the same hardware constraints."
  - [abstract]: "We propose a framework named MedSumm that leverages the power of LLMs and VLMs for this task."
- Break condition: If the medical domain requires highly specialized knowledge not present in the base LLM, QLoRA fine-tuning may not be sufficient and full fine-tuning might be necessary.

### Mechanism 3
- Claim: The MMCQS dataset's structure, pairing Hindi-English code-mixed queries with visual cues, enables the model to learn cross-modal alignment in a low-resource multilingual setting.
- Mechanism: Training on this dataset teaches the model to map between code-mixed input, visual symptom representation, and English summaries, handling both language mixing and multimodal reasoning.
- Core assumption: The visual cues are correctly aligned with the textual descriptions and provide accurate medical context.
- Evidence anchors:
  - [abstract]: "This work introduces the task of multimodal medical question summarization for codemixed input in a low-resource setting."
  - [section]: "Our dataset encompasses 3,015 medical questions along with their corresponding visual cues."
- Break condition: If visual annotations are noisy or misaligned with the text, the model may learn incorrect cross-modal associations.

## Foundational Learning

- Concept: Vision transformers (ViT) for image embedding
  - Why needed here: ViT converts raw medical images into fixed-dimensional embeddings that can be integrated with text embeddings for multimodal summarization.
  - Quick check question: What is the output dimension of ViT embeddings before projection in MedSumm?

- Concept: QLoRA for efficient fine-tuning
  - Why needed here: Allows adaptation of large LLMs to the specialized medical summarization task within typical hardware constraints.
  - Quick check question: How does QLoRA reduce memory requirements compared to full fine-tuning?

- Concept: Code-mixing and multilingual embeddings
  - Why needed here: The model must handle Hindi-English mixed input, requiring understanding of both languages and their mixing patterns.
  - Quick check question: What is the code-mixing index reported for the generated Hinglish text in the dataset annotation?

## Architecture Onboarding

- Component map:
  ViT encoder → 768-dim visual embeddings → Linear projection → concat with LLM → QLoRA fine-tuned LLM decoder → summary

- Critical path:
  ViT → projection → concat with LLM → QLoRA fine-tuned LLM decoder → summary

- Design tradeoffs:
  - Using a linear projection rather than a full cross-modal transformer reduces parameters but may limit complex cross-modal interactions
  - QLoRA fine-tuning balances efficiency with adaptation capability
  - Decoder-only LLMs were chosen over encoder-decoder for better performance on this task

- Failure signatures:
  - Poor performance on visual understanding suggests issues with ViT embeddings or projection
  - Hallucinations in summaries indicate insufficient fine-tuning or misalignment in training data
  - Language switching issues suggest problems with code-mixed text handling

- First 3 experiments:
  1. Test unimodal text-only performance to establish baseline
  2. Evaluate visual embedding quality independently (e.g., image retrieval task)
  3. Test multimodal performance with perfect oracle visual embeddings to isolate the contribution of visual information

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different quantization strategies (beyond 4-bit) affect the performance and efficiency of MedSumm when fine-tuning large language models?
- Basis in paper: [explicit] The paper uses QLoRA with 4-bit quantization for efficient fine-tuning but does not explore alternative quantization strategies.
- Why unresolved: The choice of quantization level impacts both model performance and resource requirements, and there may be trade-offs between different quantization approaches.
- What evidence would resolve it: Comparative experiments testing different quantization levels (e.g., 3-bit, 5-bit) and their impact on model accuracy, hallucination rates, and computational efficiency.

### Open Question 2
- Question: What is the optimal balance between textual and visual information for summarizing code-mixed medical queries across different types of symptoms?
- Basis in paper: [inferred] The paper demonstrates benefits of multimodal integration but does not systematically analyze how this balance varies by symptom type or query complexity.
- Why unresolved: Different medical conditions may require different degrees of visual vs textual emphasis, and the current approach treats all queries uniformly.
- What evidence would resolve it: Ablation studies varying the weight of visual vs textual components for different symptom categories, measuring impact on clinical evaluation scores and factual recall.

### Open Question 3
- Question: How does the performance of MedSumm vary across different code-mixing ratios and language pairs beyond Hindi-English?
- Basis in paper: [explicit] The dataset has a code-mixing index of 30.5 and focuses specifically on Hindi-English, but the authors mention potential expansion to other low-resource Indian languages.
- Why unresolved: The effectiveness of the approach for different code-mixing patterns and language combinations remains unexplored.
- What evidence would resolve it: Experiments with datasets featuring different code-mixing ratios and language pairs, measuring performance across varying degrees of language blending and for other Indian language combinations.

## Limitations
- Dataset limited to 18 predefined medical symptoms, restricting generalizability to diverse clinical presentations
- Code-mixing generated using GPT-3.5 rather than naturally occurring patient queries, potentially limiting robustness to real-world language patterns
- Visual cues restricted to single images rather than multimodal information like videos or multiple diagnostic views common in clinical practice

## Confidence
- **High confidence** in the core technical approach: The multimodal architecture using ViT and LLM embeddings with QLoRA fine-tuning is well-established, and automatic metrics show consistent improvements
- **Medium confidence** in the clinical relevance: Human evaluation shows improved clinical relevance scores, but was conducted by the research team rather than practicing medical professionals
- **Low confidence** in generalizability to diverse medical domains: The study focuses on 18 symptoms, and performance on other medical conditions or different visual modalities remains untested

## Next Checks
1. External clinical validation: Deploy the model on naturally occurring code-mixed clinical queries from actual patient interactions in Indian healthcare settings and evaluate with practicing physicians across multiple hospitals

2. Generalization testing: Test the framework on clinical datasets with different visual modalities (X-rays, CT scans, ultrasound) and broader symptom ranges beyond the 18 predefined conditions to assess cross-domain robustness

3. Ablation on visual relevance: Systematically evaluate model performance when visual cues are irrelevant, partially relevant, or highly relevant to the textual query to quantify the contribution of visual information and identify conditions where multimodal input may degrade performance