---
ver: rpa2
title: Confidence-Based Task Prediction in Continual Disease Classification Using
  Probability Distribution
arxiv_id: '2406.01054'
source_url: https://arxiv.org/abs/2406.01054
tags:
- task
- learning
- logits
- data
- classifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of continual learning for disease
  classification, where models need to adapt to new tasks without forgetting previous
  ones. The authors propose CTP (Confidence-based Task Prediction), an exemplar-free
  approach that uses confidence scores from probability distributions (logits) of
  expert classifiers to accurately determine task-ids at inference time.
---

# Confidence-Based Task Prediction in Continual Disease Classification Using Probability Distribution

## Quick Facts
- arXiv ID: 2406.01054
- Source URL: https://arxiv.org/abs/2406.01054
- Reference count: 32
- Primary result: CTP achieves 80.4% average classification accuracy on PathMNIST and 80.5% on OCT, outperforming state-of-the-art continual learning methods

## Executive Summary
This paper introduces CTP (Confidence-based Task Prediction), an exemplar-free approach for continual disease classification that addresses the challenge of unknown task identities at inference time. The method uses confidence scores derived from probability distributions (logits) of expert classifiers to accurately determine which task a given input belongs to. CTP defines a noise region in the logits distribution and computes confidence scores based on the number of logits falling within this region. The approach is evaluated on two disease classification datasets (PathMNIST and OCT) and demonstrates superior performance compared to existing methods, achieving an average classification accuracy of 80.4% on PathMNIST and 80.5% on OCT.

## Method Summary
CTP employs a set of expert classifiers, one per task, trained with DisMax loss to ensure high-entropy outputs for out-of-distribution data. At inference time, each classifier processes the input and produces a logits distribution. The method defines a noise region in the combined logits distribution and computes confidence scores based on the number of logits falling within this region for each classifier. The classifier with the minimum confidence score is selected as the task predictor. Performance can be further improved by providing a continuum of images from the same task during inference, which increases the number of logits available for analysis.

## Key Results
- CTP achieves 80.4% average classification accuracy on PathMNIST and 80.5% on OCT datasets
- Performance improves significantly when provided with a continuum of images at inference time
- Outperforms state-of-the-art continual learning methods including iTAML, SupSup, BIR, EFT, CLOM, L2P, and FeCAM
- Accuracy approaches levels comparable to joint training when using data continuum

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confidence scores derived from the number of logits in a defined noise region enable accurate task-id prediction.
- Mechanism: The method identifies a "noise region" in the combined logits distribution where classifiers express uncertainty. For a correct classifier, the logits distribution is bimodal—high values for the correct class and low values for incorrect classes—creating a gap in intermediate values. The noise region is defined to exclude these confident high/low values. The classifier with the fewest logits in the noise region is most confident it has identified the correct task.
- Core assumption: Expert classifiers trained with DisMax loss will produce high-entropy distributions for out-of-distribution data (data from other tasks), resulting in many logits falling into the noise region for incorrect classifiers.
- Evidence anchors:
  - [abstract]: "CTP defines a noise region in the logits distribution and computes confidence scores based on the number of logits falling within this region."
  - [section]: "We term this intermediate range as the noise region and calculate the confidence score based on the number of logits falling within this region for each classifier."
  - [corpus]: No direct evidence found in neighboring papers about noise region approaches. This appears to be a novel mechanism specific to CTP.
- Break condition: If classifiers don't produce high-entropy distributions for out-of-distribution data (e.g., if trained with standard cross-entropy loss), the noise region approach will fail as all classifiers will have similar numbers of logits in the noise region.

### Mechanism 2
- Claim: Using DisMax loss during training ensures classifiers produce high-entropy distributions for data from other tasks.
- Mechanism: DisMax loss incorporates mean isometric distance across all prototypes plus class-specific distances, creating logits that are spread out rather than concentrated. This forces classifiers to be uncertain (high entropy) about data from other tasks, making it easier to identify when a classifier is being asked to classify data from a different task.
- Core assumption: DisMax loss effectively creates the desired high-entropy distribution for out-of-distribution data while maintaining low entropy for in-distribution data.
- Evidence anchors:
  - [abstract]: "To ensure that expert classifiers produce high-entropy distributions for data unrelated to their specific task, we employ a unique loss function known as DisMax loss [18] during the training phase."
  - [section]: "DisMax incorporates mean isometric distance [17] across all prototypes in addition to the specific isometric distance for each class, to get the updated logits."
  - [corpus]: No evidence in neighboring papers about DisMax loss or similar mechanisms for creating high-entropy distributions.
- Break condition: If DisMax loss doesn't effectively create high-entropy distributions for out-of-distribution data, or if the effect is too subtle to be useful for confidence scoring.

### Mechanism 3
- Claim: Providing a continuum of images from the same task during inference improves accuracy by increasing the number of logits available for confidence computation.
- Mechanism: Each image provides a set of logits. When multiple images from the same task are available, the combined logits distribution becomes more robust and the confidence score more reliable. This aggregation effect reduces the impact of noise in individual predictions.
- Core assumption: Multiple images from the same task will have consistent task-id predictions, and aggregating their logits will produce a more reliable confidence score.
- Evidence anchors:
  - [abstract]: "Additionally, the performance of CTP can be further improved by providing it with a continuum of data at the time of inference."
  - [section]: "Since the confidence score in CTP is determined by the quantity of logits found within the noise region, having multiple images from the same task during inference (data continuum) proves advantageous as it increases the overall count of logits available for analysis."
  - [corpus]: No direct evidence in neighboring papers about using data continuums for task prediction, though this aligns with general principles about ensemble methods.
- Break condition: If the multiple images don't consistently belong to the same task, or if the computational overhead of processing multiple images outweighs the accuracy benefits.

## Foundational Learning

- Concept: Continual Learning (CL) and Catastrophic Forgetting
  - Why needed here: The paper addresses continual disease classification where models must learn new tasks without forgetting previous ones. Understanding CL is essential to grasp why traditional training approaches fail and why CTP's task-id prediction approach is valuable.
  - Quick check question: What is catastrophic forgetting and why does it occur in neural networks when training on sequential tasks?

- Concept: Probability Distributions and Logits in Classification
  - Why needed here: CTP relies on analyzing the logits (raw output scores) from classifiers to determine task identity. Understanding how logits relate to probability distributions and confidence is crucial for implementing and debugging the method.
  - Quick check question: How do logits relate to the final class probabilities in a neural network classifier?

- Concept: Entropy in Probability Distributions
  - Why needed here: The DisMax loss function aims to create high-entropy distributions for out-of-distribution data. Understanding entropy and its relationship to classifier uncertainty is important for grasping why this loss function helps with task identification.
  - Quick check question: What does high entropy in a probability distribution indicate about a classifier's confidence in its predictions?

## Architecture Onboarding

- Component map:
  ResNet50 expert classifier -> DisMax loss module -> Noise region calculator -> Confidence scorer -> Task predictor -> Continuum aggregator

- Critical path:
  1. Preprocess input image(s)
  2. Run through all expert classifiers to get logits
  3. Normalize logits distributions independently
  4. Define noise region using α and β hyperparameters
  5. Compute confidence scores for each classifier
  6. Select classifier with minimum confidence score
  7. Use selected classifier to predict final class

- Design tradeoffs:
  - Memory vs. Accuracy: One classifier per task increases memory usage but improves accuracy compared to single-model approaches
  - Inference Speed vs. Accuracy: Using data continuum improves accuracy but increases inference time
  - Hyperparameter Sensitivity: α and β values significantly affect performance; finding optimal values requires experimentation

- Failure signatures:
  - All classifiers have similar confidence scores → Task-id prediction fails; likely due to ineffective DisMax training or poor noise region definition
  - Confidence scores fluctuate wildly between similar inputs → Noise region boundaries may be too narrow or too wide
  - Accuracy degrades significantly with more tasks → Classifier network may be too large or training process needs adjustment

- First 3 experiments:
  1. Baseline test: Run CTP on PathMNIST with default hyperparameters (α=0.25, β=0.75, continuum size=5) and verify it achieves ~80.4% accuracy
  2. Ablation study: Replace DisMax loss with standard cross-entropy loss and measure accuracy drop to confirm DisMax is critical
  3. Continuum effect: Test accuracy with continuum sizes of 1, 2, 5, 10, 15+ images to identify the point where additional images no longer significantly improve performance

## Open Questions the Paper Calls Out

- Question: How does CTP's performance scale with increasing number of tasks in continual disease classification scenarios?
  - Basis in paper: [explicit] The paper mentions evaluating CTP on datasets with 3 tasks (PathMNIST) and 2 tasks (OCT), but doesn't explore performance with more tasks
  - Why unresolved: The paper only tests CTP on datasets with a limited number of tasks (3 for PathMNIST, 2 for OCT), leaving open questions about scalability to more complex scenarios
  - What evidence would resolve it: Testing CTP on datasets with more tasks (e.g., 5-10 tasks) and analyzing accuracy degradation as task count increases

## Limitations
- Limited testing on datasets with more than 3 tasks, leaving scalability questions unresolved
- The DisMax loss function implementation details are not fully specified, particularly how prototypes and distance scales are initialized and updated
- No analysis of computational overhead for processing data continuums at inference time

## Confidence
- High confidence: The core mechanism of using confidence scores from logits distributions for task prediction is well-supported by experimental results
- Medium confidence: The effectiveness of DisMax loss for creating high-entropy distributions for out-of-distribution data, based on single citation [18]
- Medium confidence: The improvement from using data continuums, though the exact relationship between continuum size and accuracy gain needs more exploration

## Next Checks
1. Test CTP with standard cross-entropy loss instead of DisMax to quantify the specific contribution of the proposed loss function
2. Conduct extensive hyperparameter sensitivity analysis for α and β across different dataset sizes and task distributions
3. Measure and compare inference time with and without data continuum to evaluate the computational tradeoff between accuracy and speed