---
ver: rpa2
title: 'GRAMMAR: Grounded and Modular Methodology for Assessment of Closed-Domain
  Retrieval-Augmented Language Model'
arxiv_id: '2404.19232'
source_url: https://arxiv.org/abs/2404.19232
tags:
- evaluation
- queries
- data
- query
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRAMMAR addresses the challenge of evaluating closed-domain RAG
  systems by introducing a grounded data generation process and a modular evaluation
  protocol. The data generation leverages database schemas and LLMs to create scalable
  query-answer pairs with verifiable ground truths, while the evaluation protocol
  identifies knowledge deficits in retrieval databases and distinguishes robustness
  issues in retrieval mechanisms versus language models.
---

# GRAMMAR: Grounded and Modular Methodology for Assessment of Closed-Domain Retrieval-Augmented Language Model

## Quick Facts
- arXiv ID: 2404.19232
- Source URL: https://arxiv.org/abs/2404.19232
- Authors: Xinzhe Li; Ming Liu; Shang Gao
- Reference count: 26
- Primary result: Introduces a grounded data generation framework and modular evaluation protocol for closed-domain RAG systems that identifies knowledge deficits and distinguishes robustness issues

## Executive Summary
GRAMMAR addresses critical challenges in evaluating closed-domain RAG systems by introducing a grounded data generation process and a modular evaluation protocol. The framework leverages database schemas and LLMs to create scalable query-answer pairs with verifiable ground truths, while the evaluation protocol identifies knowledge deficits in retrieval databases and distinguishes robustness issues in retrieval mechanisms versus language models. Validation experiments demonstrate that GRAMMAR effectively detects vulnerable retrieval modules and supports hypothesis testing for textual form vulnerabilities.

## Method Summary
GRAMMAR consists of two core components: GRAMMAR-Gen for data generation and GRAMMAR-Eval for evaluation. The data generation leverages database schemas to define knowledge structure and uses LLMs to generate SQL and text templates, which are then filled with database content to create verifiable query-answer pairs. The evaluation protocol groups semantically similar queries and tags them based on performance patterns (Gap, Robust, Non-robust) to identify specific module deficiencies. The framework includes context comparison methods to distinguish between retrieval mechanism failures and language model issues.

## Key Results
- GRAMMAR-Gen generates verifiable ground truths from database schemas with SQL-based extraction
- The modular evaluation framework successfully identifies knowledge gaps and distinguishes between retrieval vs. LM issues
- Reference-based evaluation shows higher reliability than reference-free methods for closed-domain RAG assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GRAMMAR-Gen ensures reliable evaluation by generating query-answer pairs with verifiable ground truths through SQL queries on relational databases.
- Mechanism: Uses database schemas to define knowledge structure, generates SQL templates to extract ground truths, and fills placeholders with database content to create diverse queries and answers.
- Core assumption: Relational database schemas adequately represent closed-domain knowledge structure for generating meaningful queries and answers.
- Evidence anchors: Abstract states data generation leverages database schemas and LLMs to create scalable query-answer pairs with verifiable ground truths.

### Mechanism 2
- Claim: GRAMMAR-Eval identifies defective modules by analyzing query groups and tagging them based on model performance patterns.
- Mechanism: Groups queries by semantic similarity, tags groups as Gap (knowledge deficit), Robust (consistent correct responses), or Non-robust (mixed responses), then isolates errors to specific modules.
- Core assumption: Performance patterns across semantically similar queries reliably indicate specific module deficiencies.
- Evidence anchors: Abstract states evaluation protocol identifies knowledge deficits and distinguishes robustness issues in retrieval mechanisms versus language models.

### Mechanism 3
- Claim: Reference-based evaluation is more reliable than reference-free methods for closed-domain RAG systems.
- Mechanism: Ground truth answers are generated from database content rather than estimated by LLM evaluators, providing more accurate assessment of correctness.
- Core assumption: LLM-generated ground truths from database queries are more reliable than LLM-based reference-free evaluation methods.
- Evidence anchors: Paper identifies ground truth scarcity as motivation and shows reference-free methods have limitations in preliminary study.

## Foundational Learning

- Concept: Relational database schemas and SQL
  - Why needed here: Schema defines knowledge structure for generating queries, SQL extracts ground truths from database content
  - Quick check question: Can you explain how a foreign key constraint in a schema would affect the types of SQL queries that can be generated?

- Concept: Query grouping and semantic similarity
  - Why needed here: Groups queries by meaning to identify patterns in model performance that indicate specific module deficiencies
  - Quick check question: How would you implement a function to group text queries by their corresponding SQL templates?

- Concept: Modular evaluation methodology
  - Why needed here: Distinguishes between knowledge deficits, retrieval mechanism issues, and language model problems
  - Quick check question: What metrics would you use to compare the performance of robust vs non-robust query groups?

## Architecture Onboarding

- Component map: GRAMMAR-Gen: SQL Template Generator -> Text Template Generator -> QA Data Generator; GRAMMAR-Eval: TaggedGroup processor -> SemanticsMatch evaluator -> result analysis tools; Database layer: Schema definitions -> content databases -> connection management

- Critical path: 1. Load schema and database connection 2. Generate SQL templates using LLM 3. Generate text templates using LLM 4. Fill placeholders with database content 5. Execute SQL to get ground truths 6. Group queries by semantic similarity 7. Tag groups based on performance patterns 8. Analyze results to identify module deficiencies

- Design tradeoffs: Schema complexity vs query generation flexibility (more complex schemas enable more diverse queries but increase generation complexity); LLM model choice vs generation quality (higher quality models produce better templates but increase computational cost); Query length vs robustness testing (longer queries test robustness better but may introduce more noise)

- Failure signatures: Low Gap Group accuracy indicates retrieval database knowledge gaps; Low Robust Group accuracy indicates retrieval mechanism or LM issues affecting consistent queries; Inconsistent Non-robust Group performance suggests LM sensitivity to input variations

- First 3 experiments: 1. Generate evaluation data for a simple schema and verify ground truth accuracy 2. Run modular evaluation on a deliberately bugged retrieval system and verify correct module identification 3. Compare reference-based vs reference-free evaluation on the same dataset to demonstrate reliability difference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the data generation process be extended to handle scenarios with multiple correct answers without compromising evaluation accuracy?
- Basis in paper: The paper explicitly mentions the limitation that the data generation process cannot handle scenarios with multiple correct answers and discusses this as a constraint.
- Why unresolved: The paper identifies this as a limitation but does not propose a concrete solution or workaround for handling queries with multiple valid answers.
- What evidence would resolve it: A demonstration of a modified data generation process that can handle queries with multiple correct answers while maintaining evaluation accuracy, along with empirical validation showing improved performance.

### Open Question 2
- Question: What refinements are needed for the context comparison method to be effective for more complex retrieval tasks beyond closed-domain evaluation?
- Basis in paper: The paper mentions that while the simple context comparison method works well for closed-domain evaluation, it may need refinement for more complex retrieval tasks, suggesting potential limitations.
- Why unresolved: The paper validates the context comparison method for closed-domain scenarios but does not explore or propose specific improvements for handling more complex retrieval scenarios.
- What evidence would resolve it: A detailed analysis of context comparison failures in complex retrieval tasks and a proposed enhanced method with experimental validation showing improved accuracy in identifying non-robust modules.

### Open Question 3
- Question: How does the scalability of the proposed data generation framework compare when applied to larger, more complex database schemas with many-to-many relationships?
- Basis in paper: The paper discusses the combinatorial expansion of query variations but focuses primarily on simpler schemas, suggesting potential scalability concerns with more complex structures.
- Why unresolved: The paper demonstrates scalability with relatively simple schemas but does not empirically test or analyze performance with larger, more complex database schemas involving many-to-many relationships.
- What evidence would resolve it: Comparative analysis of data generation performance and quality metrics across schemas of varying complexity, including many-to-many relationships, with runtime and resource usage measurements.

## Limitations
- Framework relies heavily on structured database schemas, limiting applicability to unstructured data sources
- Data generation process cannot handle scenarios with multiple correct answers
- Context comparison method may need refinement for more complex retrieval tasks beyond closed-domain evaluation

## Confidence

**High Confidence** (Evidence Level: Strong)
- GRAMMAR-Gen's ability to generate verifiable ground truths from database schemas using SQL queries
- The modular evaluation framework's capability to distinguish between knowledge deficits and robustness issues
- Reference-based evaluation providing more reliable results than reference-free methods

**Medium Confidence** (Evidence Level: Moderate)
- The semantic grouping approach effectively identifies module deficiencies
- The framework scales to moderately sized databases
- Query generation templates adequately capture domain knowledge

**Low Confidence** (Evidence Level: Limited)
- Performance on unstructured data sources
- Computational efficiency at enterprise scale
- Generalization to domains with complex relationship structures

## Next Checks

1. **Real-world Deployment Test**: Deploy GRAMMAR on a production RAG system with a database containing 1M+ records to evaluate generation time, accuracy degradation, and computational resource requirements.

2. **Cross-domain Generalization**: Apply GRAMMAR to three distinct domains (e.g., medical records, legal documents, and financial data) to test the robustness of schema-based generation and semantic grouping across different knowledge structures.

3. **Unstructured Data Extension**: Modify GRAMMAR to handle semi-structured data (JSON documents, XML) and evaluate whether the modular assessment framework maintains its accuracy and diagnostic capabilities without traditional SQL schemas.