---
ver: rpa2
title: Solving Zebra Puzzles Using Constraint-Guided Multi-Agent Systems
arxiv_id: '2407.03956'
source_url: https://arxiv.org/abs/2407.03956
tags:
- number
- place
- bridget
- kermit
- ophelia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZPS, a multi-agent system that combines LLMs
  with theorem provers to solve Zebra puzzles. The system breaks down puzzles into
  sub-problems, translates clues into SMT-LIB code, and uses feedback loops to iteratively
  refine solutions.
---

# Solving Zebra Puzzles Using Constraint-Guided Multi-Agent Systems

## Quick Facts
- arXiv ID: 2407.03956
- Source URL: https://arxiv.org/abs/2407.03956
- Reference count: 25
- Key outcome: Multi-agent system combining LLMs with theorem provers improves Zebra puzzle solutions by 166% for GPT-4 and 41.2% for GPT-3.5

## Executive Summary
This paper introduces ZPS, a multi-agent system that enhances LLM performance on Zebra puzzles by decomposing problems into sub-tasks and using theorem provers for constraint solving. The system combines LLM reasoning with formal verification through iterative feedback loops, translating puzzle clues into SMT-LIB code. An automated grader validates solutions, with results showing substantial improvements over baseline approaches. The methodology demonstrates that structured planning and agent feedback can significantly boost logical reasoning capabilities in LLMs.

## Method Summary
ZPS operates through a three-stage process: first, it breaks down Zebra puzzles into sub-problems and creates a solution plan; second, it uses LLMs to translate clues into SMT-LIB code while theorem provers validate the logical consistency; third, an automated grader evaluates solution accuracy and provides feedback to refine answers. The system employs multiple specialized agents including planners, solvers, and graders that work in coordination. A user study with 30 participants validates the automated grading system's reliability. The approach is tested on 50 puzzles from ZebraPuzzles.com using GPT-4, GPT-3.5, and Llama-3 models.

## Key Results
- ZPS improves GPT-4's fully correct solutions by 166% compared to baselines
- GPT-3.5 shows 41.2% improvement with the multi-agent system
- All three tested LLMs (GPT-4, GPT-3.5, and Llama-3) demonstrate performance gains when solver feedback is integrated

## Why This Works (Mechanism)
The system works by combining the pattern recognition capabilities of LLMs with the rigorous logical consistency checking of theorem provers. By breaking complex puzzles into manageable sub-problems and translating natural language clues into formal constraint specifications, the system leverages the strengths of both symbolic AI and neural models. The iterative feedback loop allows the system to catch and correct errors that LLMs might make in isolation, while the structured planning component ensures systematic coverage of all puzzle elements.

## Foundational Learning
- **SMT-LIB format**: Standard language for expressing logical constraints - needed for theorem prover integration - quick check: verify constraint translation accuracy
- **Multi-agent coordination**: Specialized agents working together on sub-tasks - needed for modular problem decomposition - quick check: measure communication overhead
- **Automated grading systems**: Programmatic solution validation - needed for rapid feedback loops - quick check: test grader on edge cases
- **Few-shot prompting**: Providing examples to guide LLM behavior - needed for baseline comparison - quick check: document prompt templates
- **Constraint satisfaction problems**: Formal representation of logical relationships - needed for puzzle modeling - quick check: validate constraint coverage
- **Iterative refinement**: Cycle of generation, evaluation, and correction - needed for solution improvement - quick check: measure convergence rate

## Architecture Onboarding

**Component map:**
Planner -> LLM Translator -> Theorem Prover -> Automated Grader -> LLM Refiner -> Final Solution

**Critical path:**
The critical path involves the planner creating a decomposition strategy, the LLM translating clues to constraints, the theorem prover validating these constraints, and the grader providing feedback for refinement. Any bottleneck in this chain affects overall system performance.

**Design tradeoffs:**
The system trades computational overhead (multiple LLM calls and theorem prover usage) for improved accuracy. It requires access to specialized tools (SMT solvers) which limits deployment flexibility but provides rigorous logical verification that pure LLM approaches cannot achieve.

**Failure signatures:**
Common failures include: LLM translation errors producing invalid SMT-LIB code, theorem provers identifying unsolvable constraint sets, grader misclassification of solutions, and communication breakdowns between agents leading to incomplete sub-problem coverage.

**3 first experiments:**
1. Test the system on a single simple Zebra puzzle with known solution to verify basic functionality
2. Compare performance on puzzles with varying clue complexity to identify scalability limits
3. Run ablation studies removing theorem prover validation to quantify its contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on a user-generated corpus of 50 puzzles that may not represent comprehensive logical reasoning benchmarks
- Automated grader introduces potential reliability concerns requiring validation through limited user study
- Performance improvements measured against unspecified baseline prompt engineering methodologies
- System dependency on SMT solvers limits deployment in environments without theorem prover support

## Confidence
- Multi-agent system architecture and implementation: **High**
- Performance improvement metrics (166% for GPT-4, 41.2% for GPT-3.5): **Medium** (depends on baseline specification and grader reliability)
- Generalizability across different logical reasoning tasks: **Low**
- Solver feedback mechanism effectiveness: **Medium** (limited by user study sample size)

## Next Checks
1. Conduct an independent evaluation using a larger, professionally curated dataset of logical reasoning puzzles with multiple difficulty levels and diverse clue types
2. Implement cross-validation with multiple automated grading systems and conduct a larger-scale user study (n > 100) to verify grader accuracy and reliability
3. Test the system's performance on non-Zebra puzzle logical reasoning tasks to assess domain transferability and generalization capabilities