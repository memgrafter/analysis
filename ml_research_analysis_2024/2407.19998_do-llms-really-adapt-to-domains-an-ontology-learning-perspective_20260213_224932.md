---
ver: rpa2
title: Do LLMs Really Adapt to Domains? An Ontology Learning Perspective
arxiv_id: '2407.19998'
source_url: https://arxiv.org/abs/2407.19998
tags:
- llms
- gibberish
- concepts
- domains
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) truly
  adapt to domains and reason over semantic relationships, or merely learn lexical
  senses. To test this, the authors create synthetic domain-specific datasets from
  WordNet, generating gibberish counterparts of English terms and definitions.
---

# Do LLMs Really Adapt to Domains? An Ontology Learning Perspective
## Quick Facts
- arXiv ID: 2407.19998
- Source URL: https://arxiv.org/abs/2407.19998
- Reference count: 35
- Key outcome: Off-the-shelf LLMs perform significantly worse on gibberish data, with F1-scores dropping by up to 50% compared to real data.

## Executive Summary
This paper investigates whether large language models (LLMs) truly adapt to domains and reason over semantic relationships, or merely learn lexical senses. To test this, the authors create synthetic domain-specific datasets from WordNet, generating gibberish counterparts of English terms and definitions. They evaluate popular LLMs on relation extraction and taxonomy discovery tasks using both real and gibberish datasets. Results show that off-the-shelf LLMs perform significantly worse on gibberish data, with F1-scores dropping by up to 50% compared to real data. However, fine-tuning improves performance on both real and gibberish data, with F1-scores increasing up to threefold, suggesting that LLMs can develop reasoning abilities through domain-specific training.

## Method Summary
The authors create synthetic domain-specific datasets from WordNet by replacing domain-specific terms with gibberish equivalents while preserving semantic relationships. They evaluate popular LLMs (GPT-3.5, GPT-4, Falcon-40B, LLaMa2-13B, Zephyr-7B-β) on relation extraction and taxonomy discovery tasks using both real and gibberish datasets. The study employs a zero-shot evaluation approach for off-the-shelf models and fine-tuning experiments using LoRA with 4-bit quantization on open-source models. Transfer learning experiments are conducted to assess generalization to new domains with different gibberish vocabulary.

## Key Results
- Off-the-shelf LLMs perform significantly worse on gibberish data, with F1-scores dropping by up to 50% compared to real data
- Fine-tuning improves performance on both real and gibberish data, with F1-scores increasing up to threefold
- Transfer learning experiments show that models fine-tuned on gibberish data generalize better to new domains, suggesting improved reasoning capabilities

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Off-the-shelf LLMs perform poorly on gibberish datasets because they rely on lexical priors rather than reasoning over semantic relationships.
- Mechanism: When faced with unfamiliar words, LLMs fall back to pattern matching against their training vocabulary, leading to significant performance drops.
- Core assumption: The performance drop is due to the inability to generalize semantic relationships to arbitrary vocabulary.
- Evidence anchors:
  - [abstract] "Results show that off-the-shelf LLMs perform significantly worse on gibberish data, with F1-scores dropping by up to 50% compared to real data."
  - [section] "a significant performance decrease is observed when replacing real terms with gibberish."
- Break condition: If the LLM could effectively learn to ignore word identity and focus solely on contextual semantic relationships, this mechanism would break.

### Mechanism 2
- Claim: Fine-tuning improves LLM performance on both real and gibberish data by developing reasoning capabilities.
- Mechanism: Fine-tuning on domain-specific data, even with arbitrary vocabulary, allows the model to learn the underlying semantic relationships rather than just memorizing word associations.
- Core assumption: The fine-tuning process enables the model to abstract away from specific lexical forms and focus on semantic structures.
- Evidence anchors:
  - [abstract] "However, fine-tuning improves performance on both real and gibberish data, with F1-scores increasing up to threefold."
  - [section] "we notice that fine-tuning drastically improves the task-specific performance of the LLM regardless the domain and corpus version."
- Break condition: If the model were simply memorizing the gibberish-to-meaning mappings rather than learning abstract relationships, this mechanism would break.

### Mechanism 3
- Claim: Transfer learning from gibberish data to new domains demonstrates genuine reasoning capability.
- Mechanism: Models fine-tuned on gibberish data in one domain can transfer knowledge to new domains with different gibberish vocabulary, indicating they've learned abstract reasoning patterns.
- Core assumption: The ability to transfer knowledge across different sets of arbitrary vocabulary indicates genuine semantic reasoning rather than pattern matching.
- Evidence anchors:
  - [abstract] "Transfer learning experiments demonstrate that models fine-tuned on gibberish data generalize better to new domains, suggesting improved reasoning capabilities."
  - [section] "we observe the following. Firstly, the F1-scores generally tend to increase after transfer, with the exception of the WN-music to WN-sweets case."
- Break condition: If the transfer performance was solely due to common linguistic patterns that appear across all gibberish datasets, this mechanism would break.

## Foundational Learning
- Concept: Semantic relationships vs. lexical priors
  - Why needed here: Understanding the difference between reasoning about meaning vs. recognizing familiar words is crucial for interpreting the experiment results.
  - Quick check question: Can you explain why an LLM might correctly identify that "dog" is a subclass of "animal" but fail when "dog" is replaced with gibberish?

- Concept: Ontology learning tasks
  - Why needed here: The paper evaluates LLMs on relation extraction and taxonomy discovery, which are specific types of ontology learning tasks.
  - Quick check question: What's the difference between relation extraction (finding hypernymy relationships) and taxonomy discovery (determining if A is a subclass of B)?

- Concept: Domain adaptation vs. generalization
  - Why needed here: The paper distinguishes between adapting to a specific domain vs. generalizing to arbitrary domains with unknown vocabulary.
  - Quick check question: Why is the ability to work with arbitrary gibberish vocabulary more challenging than working with a specific domain's jargon?

## Architecture Onboarding
- Component map:
  Data generation pipeline: WordNet → synthetic parallel corpora (real/gibberish)
  LLM evaluation framework: Zero-shot vs. fine-tuned performance on relation extraction and taxonomy discovery
  Transfer learning setup: Fine-tuning on one gibberish domain, testing on others

- Critical path:
  1. Generate synthetic datasets from WordNet
  2. Evaluate off-the-shelf LLMs on both real and gibberish versions
  3. Fine-tune on gibberish data
  4. Test transfer learning across gibberish domains

- Design tradeoffs:
  - Using gibberish vs. real domain-specific jargon: Gibberish provides better control but may not capture all nuances of real domain adaptation
  - Closed-source vs. open-source models: Different access levels and parameter counts affect experimental scope
  - Relation extraction vs. taxonomy discovery: Different task complexities and evaluation metrics

- Failure signatures:
  - Large performance drops on gibberish data indicate over-reliance on lexical priors
  - Poor transfer learning results suggest lack of genuine reasoning capability
  - High alignment between real and gibberish predictions indicates potential memorization

- First 3 experiments:
  1. Run relation extraction task on real WN-sweets dataset with GPT-3.5
  2. Repeat relation extraction on gibberish WN-sweets dataset with same model
  3. Compare F1-scores between real and gibberish results to quantify performance drop

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can LLMs be fine-tuned to develop genuine reasoning capabilities over arbitrary domain-specific concepts rather than relying on lexical priors?
- Basis in paper: [explicit] The paper shows that fine-tuning improves performance on gibberish data but notes that performance still doesn't match fine-tuning on real data, suggesting some reliance on prior semantics.
- Why unresolved: The paper demonstrates improvement through fine-tuning but doesn't conclusively determine if this represents true reasoning or sophisticated pattern matching based on training data.
- What evidence would resolve it: Experiments testing LLMs on truly novel domains with no semantic overlap to training data, measuring if performance scales with reasoning complexity rather than training data similarity.

### Open Question 2
- Question: What specific aspects of LLM architecture or training methodology limit their ability to adapt to arbitrary domains compared to humans?
- Basis in paper: [inferred] The paper observes that LLMs struggle with gibberish data despite having similar semantic relationships to real data, suggesting architectural limitations.
- Why unresolved: The paper identifies the problem but doesn't investigate whether it stems from attention mechanisms, tokenization, training objectives, or other architectural choices.
- What evidence would resolve it: Comparative studies of different LLM architectures (sparse vs dense attention, different tokenization schemes) on domain adaptation tasks, or ablation studies identifying which components most affect domain adaptation.

### Open Question 3
- Question: Is there a critical model size threshold below which LLMs cannot effectively develop reasoning capabilities for domain adaptation, regardless of fine-tuning?
- Basis in paper: [inferred] The paper tests models from 7B to 40B parameters, all showing similar patterns of lexical reliance, but doesn't systematically explore smaller models.
- Why unresolved: The paper only tests models of 7B parameters and above, leaving open whether smaller models might show fundamentally different behavior.
- What evidence would resolve it: Systematic testing of LLMs across a wide parameter range (100M to 70B) on the same domain adaptation tasks, measuring if there's a threshold where reasoning capabilities emerge.

## Limitations
- The synthetic gibberish approach may not fully capture real-world domain adaptation complexities
- The study focuses on semantic relationships from WordNet, which may not generalize to all types of domain knowledge
- Closed-source models (GPT-3.5, GPT-4) limit reproducibility of fine-tuning experiments and full diagnostic analysis

## Confidence
- High confidence: Core finding that off-the-shelf LLMs show significant performance degradation on gibberish data, indicating reliance on lexical priors
- Medium confidence: Fine-tuning improvement claims, as the open-source models showed substantial gains but exact training procedures could affect reproducibility
- Medium confidence: Transfer learning results, as the paper shows generalization but with notable exceptions (e.g., WN-music to WN-sweets case)

## Next Checks
1. Evaluate whether models fine-tuned on gibberish data can handle mixed-domain datasets containing both real and gibberish terms
2. Test the same models on datasets where semantic relationships are less clear-cut to assess reasoning capabilities beyond clean WordNet relationships
3. Systematically vary prompt formats and Chain-of-Thought reasoning instructions to determine how much performance improvement comes from reasoning ability versus effective prompting strategies