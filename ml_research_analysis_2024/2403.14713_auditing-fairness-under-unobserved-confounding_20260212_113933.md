---
ver: rpa2
title: Auditing Fairness under Unobserved Confounding
arxiv_id: '2403.14713'
source_url: https://arxiv.org/abs/2403.14713
tags:
- bounds
- assumption
- bound
- estimator
- given
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of auditing fairness in decision-making
  systems when unmeasured confounding is present. It proposes a method to derive bounds
  on treatment rates for high-risk individuals, even when assumptions about no unmeasured
  confounding are relaxed or eliminated entirely.
---

# Auditing Fairness under Unobserved Confounding

## Quick Facts
- arXiv ID: 2403.14713
- Source URL: https://arxiv.org/abs/2403.14713
- Reference count: 40
- Key outcome: Method to audit fairness in decision-making systems when unmeasured confounding is present, using pre- and post-availability data to derive bounds on treatment rates for high-risk individuals

## Executive Summary
This paper addresses the challenge of auditing fairness in decision-making systems when unmeasured confounding is present. The authors propose a method that uses pre- and post-availability data to derive bounds on treatment rates for high-risk individuals, even when assumptions about no unmeasured confounding are relaxed or eliminated entirely. The core innovation is leveraging baseline risk estimates from pre-availability periods to bound post-availability treatment rates among the needy, avoiding the need for unobserved confounders to be measured.

## Method Summary
The method uses pre-availability data to estimate baseline risk of adverse outcomes without treatment, then uses this estimate to bound the treatment rate among the needy in the post-availability period. Machine learning models estimate nuisance functions (treatment probability, outcome probability, covariate probability) which are combined using influence functions to construct bias-corrected estimators with valid confidence intervals. A sensitivity analysis framework introduces a parameter γ to tighten bounds under different assumptions about unobserved confounding strength, allowing the method to be benchmarked against observed covariates.

## Key Results
- Demonstrated racial inequity in Paxlovid allocation that cannot be explained by unobserved confounders of similar strength to important observed covariates
- Derived non-vacuous bounds on treatment rates using pre- and post-availability data from NCATS N3C Data Enclave
- Showed that machine learning estimators can be used while maintaining valid confidence intervals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bounds on treatment rates among the needy can be identified even when unobserved confounding is present, provided pre- and post-availability data are available.
- Mechanism: The method leverages pre-availability data to estimate the baseline risk of adverse outcomes without treatment (via Y(0)), then uses this estimate to bound the treatment rate among the needy in the post-availability period. This avoids the need for unobserved confounders to be measured, as the baseline risk is estimated from a period when no treatment was available.
- Core assumption: Covariate stability and stable baseline risk across pre- and post-availability periods.
- Evidence anchors:
  - [abstract]: "We use the fact that in many real-world settings (e.g., the release of a new treatment) we have data from prior to any allocation to derive unbiased estimates of risk."
  - [section 4.3]: "We demonstrate that it is still possible to obtain informative bounds on the treatment rate in (1) that can be estimated from data, and provide intuition as to why informative bounds are possible to obtain."

### Mechanism 2
- Claim: Machine learning estimators can be used to compute these bounds while maintaining valid confidence intervals.
- Mechanism: The method uses influence functions and semiparametric estimation to construct bias-corrected estimators for the bounds. This allows the bounds to be estimated with flexible ML models while still achieving asymptotic normality and valid confidence intervals.
- Core assumption: The influence functions for the components of the bounds can be derived and estimated consistently.
- Evidence anchors:
  - [section 4.3]: "We provide bias-corrected estimators for our bounds that are consistent and asymptotically normal, and extend recent results in the partial identification literature to handle the non-smooth nature of our estimators (Theorem 2) that make them more precise."

### Mechanism 3
- Claim: The tightness of the bounds can be improved by incorporating assumptions on the strength of unobserved confounding via a sensitivity analysis framework.
- Mechanism: The method introduces a sensitivity parameter γ that bounds the effect of unobserved confounders on treatment assignment. By varying γ, the bounds can be tightened, allowing for a more precise assessment of inequity under different assumptions about the strength of unobserved confounding.
- Core assumption: The sensitivity parameter γ is bounded, i.e., the effect of unobserved confounders on treatment assignment is limited.
- Evidence anchors:
  - [section 4.4]: "We introduce a sensitivity parameter γ that captures the extent of the impact of the potential outcome Y(0) on treatment assignment T, similar in spirit to the sensitivity model used by Tan (2006), adapted to our problem setting."

## Foundational Learning

- Concept: Partial identification
  - Why needed here: The paper deals with causal quantities that cannot be exactly identified due to unobserved confounding, so it uses partial identification to derive bounds instead of point estimates.
  - Quick check question: What is the difference between partial identification and point identification in causal inference?

- Concept: Influence functions
  - Why needed here: Influence functions are used to construct bias-corrected estimators for the bounds, allowing the use of ML models while maintaining valid confidence intervals.
  - Quick check question: How do influence functions help in debiasing plugin estimators in semiparametric estimation?

- Concept: Sensitivity analysis in causal inference
  - Why needed here: The paper uses a sensitivity analysis framework to assess the robustness of its findings to different assumptions about the strength of unobserved confounding.
  - Quick check question: What is the role of sensitivity analysis in causal inference when unmeasured confounding is a concern?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Split data into pre- and post-availability periods, filter based on eligibility criteria
  - Nuisance function estimation: Train ML models to estimate treatment probability (π), outcome probability (µ), and propensity for pre-availability data (g)
  - Bound computation: Compute upper and lower bounds on treatment rates among the needy using the estimated nuisance functions
  - Sensitivity analysis: Introduce and vary the sensitivity parameter γ to tighten the bounds under different assumptions about unobserved confounding
  - Benchmarking: Compare the sensitivity parameter γ to the effect of observed covariates to assess the plausibility of the assumptions

- Critical path:
  1. Preprocess data and split into pre- and post-availability periods
  2. Estimate nuisance functions (π, µ, g) using ML models
  3. Compute bounds on treatment rates among the needy
  4. Perform sensitivity analysis by varying γ
  5. Benchmark γ against observed covariates
  6. Interpret results and assess inequity

- Design tradeoffs:
  - Using ML models for nuisance function estimation allows for flexibility but requires careful handling of asymptotic properties
  - The margin condition is crucial for asymptotic normality but may not always hold in practice
  - The sensitivity analysis framework provides more precise bounds but relies on assumptions about the strength of unobserved confounding

- Failure signatures:
  - Vacuous bounds: If the treatment rate is higher than the baseline mortality rate, or if the baseline mortality rate is very low, the bounds may become vacuous
  - Violation of assumptions: If the covariate stability or stable baseline risk assumptions are violated, the bounds will be invalid
  - Inconsistent nuisance function estimation: If the ML models used to estimate π, µ, and g are not consistent, the bias-corrected estimators will fail

- First 3 experiments:
  1. Verify that the bounds are non-vacuous on a synthetic dataset with known ground truth treatment rates
  2. Check the sensitivity of the bounds to different values of γ on a semi-synthetic dataset
  3. Apply the method to a real-world dataset (e.g., Paxlovid allocation) and assess the robustness of the findings to different assumptions about unobserved confounding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical limits of the bounds when the sensitivity parameter γ approaches infinity?
- Basis in paper: [explicit] The paper discusses bounds under arbitrary unobserved confounding (γ → ∞) in Theorems 1 and 3.
- Why unresolved: The paper derives bounds for finite γ but does not explore the limiting behavior or practical implications as γ → ∞.
- What evidence would resolve it: Analyzing the behavior of the bounds as γ increases and comparing them to known statistical limits or empirical data.

### Open Question 2
- Question: How do the bounds perform when the margin condition (Assumption 5) is violated or only approximately satisfied?
- Basis in paper: [explicit] Assumption 5 is required for asymptotic normality but the paper does not explore violations or relaxations.
- Why unresolved: The paper assumes the margin condition holds but real-world data may not satisfy it perfectly.
- What evidence would resolve it: Empirical studies on synthetic or real datasets where the margin condition is intentionally violated or relaxed.

### Open Question 3
- Question: Can the framework be extended to continuous outcomes or multi-class treatment settings?
- Basis in paper: [inferred] The current framework focuses on binary outcomes and treatments, but the methodology could potentially be generalized.
- Why unresolved: The paper does not explore generalizations beyond binary settings.
- What evidence would resolve it: Theoretical derivations and empirical validation on datasets with continuous outcomes or multi-class treatments.

## Limitations
- Relies on strong assumptions about covariate stability across pre- and post-availability periods that may not hold in dynamic healthcare settings
- Requires treatment rates not to exceed baseline adverse outcome rates, creating potential for vacuous bounds
- Sensitivity analysis framework depends on subjective calibration of the sensitivity parameter γ against observed covariates
- Requires access to pre-treatment baseline risk data, which may not be available in many real-world settings

## Confidence
- High confidence: The core mechanism of using pre- and post-availability data to derive bounds on treatment rates (Mechanism 1)
- Medium confidence: The use of machine learning estimators with valid confidence intervals (Mechanism 2)
- Medium confidence: The sensitivity analysis framework for tightening bounds (Mechanism 3)

## Next Checks
1. Validate the method on a synthetic dataset with known ground truth treatment rates to verify that the bounds are non-vacuous and correctly capture the true inequity when assumptions are satisfied
2. Conduct a sensitivity analysis by varying the stability assumptions (covariate stability and stable baseline risk) to quantify how violations affect bound tightness and validity
3. Apply the framework to a different real-world case study with available pre- and post-intervention data to assess generalizability beyond the Paxlovid allocation scenario