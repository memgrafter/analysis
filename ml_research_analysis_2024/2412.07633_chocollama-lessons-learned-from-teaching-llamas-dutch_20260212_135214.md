---
ver: rpa2
title: 'ChocoLlama: Lessons Learned From Teaching Llamas Dutch'
arxiv_id: '2412.07633'
source_url: https://arxiv.org/abs/2412.07633
tags:
- dutch
- language
- tokenizer
- data
- llama-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study adapted Llama-2 and Llama-3 models to Dutch using continued
  pretraining with LoRA, combined with tokenizer modifications and embedding reinitialization
  for Llama-2. A new Dutch-specific tokenizer was developed, leading to 29.4% fewer
  tokens for the same text.
---

# ChocoLlama: Lessons Learned From Teaching Llamas Dutch

## Quick Facts
- arXiv ID: 2412.07633
- Source URL: https://arxiv.org/abs/2412.07633
- Reference count: 40
- One-line primary result: Adapting Llama-2 to Dutch using LoRA and tokenizer modification achieved 29.4% fewer tokens and improved performance, while similar techniques showed limited gains for Llama-3 due to its strong multilingual capabilities

## Executive Summary
This study explores adapting Llama-2 and Llama-3 models to Dutch using continued pretraining with LoRA, tokenizer modifications, and embedding reinitialization. The researchers developed a new Dutch-specific tokenizer that reduced token count by 29.4% and applied LoRA to 544M parameters (7.75% of total) for efficient language adaptation. They found that while these techniques worked well for Llama-2, they showed limited gains for Llama-3, which already possesses strong multilingual capabilities. The study also introduces ChocoLlama-Bench, a novel Dutch benchmark for evaluating language models, and demonstrates that for advanced multilingual models, language-specific posttraining may be more effective than continued pretraining.

## Method Summary
The researchers collected 104GB of Dutch text (32B tokens) from various sources including OSCAR, Open Subtitles, Project Gutenberg, Wikipedia, job descriptions, legal filings, and Flemish legislation. They pretrained Llama-2 7B with LoRA using r=8 and α=32 parameters, then trained a new Dutch-specific BPE tokenizer with 50k vocabulary and applied embedding reinitialization. For Llama-3, they used the original tokenizer due to its larger vocabulary size. Both models underwent posttraining (SFT + DPO) for conversational capabilities. The adapted models were evaluated on standard benchmarks (ARC, HellaSwag, MMLU, TruthfulQA) and the novel ChocoLlama-Bench using GPT-4o as judge.

## Key Results
- LoRA adaptation successfully scaled to 32B tokens of Dutch data, updating only 7.75% of parameters
- New Dutch tokenizer reduced token count by 29.4% compared to the original tokenizer
- Llama-2-ChocoLlama outperformed the base Llama-2 on Dutch benchmarks
- Llama-3-instruct surpassed Llama-3-ChocoLlama-instruct on ChocoLlama-Bench, suggesting diminishing returns for multilingual models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA enables effective language adaptation with minimal parameter updates
- Mechanism: Low-rank adaptation modifies a small subset of model parameters (544M out of 7.3B, or 7.75%) to learn Dutch language patterns without full fine-tuning
- Core assumption: The model's base architecture contains sufficient linguistic capacity to be adapted to Dutch through targeted parameter modifications
- Evidence anchors: [abstract] "Our results demonstrate that LoRA can effectively scale for language adaptation"; [section] "For all layers in between, we apply the parameter efficient fine-tuning method LoRA [ 20] to all its available target modules with r = 8 and α = 32. This leads to 544M trainable parameters, equivalent to 7 .75% of all 7B model parameters."

### Mechanism 2
- Claim: Tokenizer modification with embedding reinitialization improves language adaptation performance
- Mechanism: Training a Dutch-specific tokenizer and reinitializing embeddings to map new tokens to semantically similar original tokens preserves linguistic knowledge while improving Dutch encoding efficiency
- Core assumption: Semantic similarity mapping between token vocabularies can preserve knowledge while enabling more efficient language representation
- Evidence anchors: [abstract] "tokenizer modification with embedding reinitialization improved performance"; [section] "To mitigate any forgetting associated with changing something as fundamental as the tokenizer (and vocabulary), we apply the embedding reinitialization approach as introduced by Remy et al. [ 32]."

### Mechanism 3
- Claim: Language adaptation benefits more from domain-specific posttraining than continued pretraining for models with advanced multilingual capabilities
- Mechanism: Models like Llama-3 with extensive multilingual pretraining (15 trillion tokens) have already acquired broad language capabilities, making further pretraining on Dutch less impactful than targeted instruction tuning
- Core assumption: Advanced multilingual models have already captured most relevant linguistic patterns, so post-training optimization becomes more valuable
- Evidence anchors: [abstract] "This suggests that for ever improving, multilingual foundation models, language adaptation techniques may benefit more from focusing on language-specific posttraining rather than on continued pretraining"; [section] "We find that Llama-3-ChocoLlama-instruct, while surpassing the Llama-3-instruct in our quantitative benchmark, we find that Llama-3-instruct surpasses Llama-3-ChocoLlama-instruct on ChocoLlama-Bench"

## Foundational Learning

- Concept: Tokenization and vocabulary design
  - Why needed here: Understanding how different tokenizers affect language representation and model performance is crucial for language adaptation
  - Quick check question: How does a Dutch-specific tokenizer reduce token count by 29.4% compared to the original tokenizer?
  - Answer: By creating tokens that better match Dutch word structures and morphology, requiring fewer tokens to represent the same text

- Concept: Catastrophic forgetting in model adaptation
  - Why needed here: Changing fundamental components like tokenizers risks losing previously learned knowledge, requiring careful mitigation strategies
  - Quick check question: What technique is used to prevent catastrophic forgetting when changing tokenizers?
  - Answer: Embedding reinitialization that maps new tokens to semantically similar original tokens

- Concept: Parameter-efficient fine-tuning
  - Why needed here: Understanding LoRA and similar techniques is essential for scaling language adaptation to large models without full fine-tuning
  - Quick check question: What percentage of parameters are updated during LoRA adaptation in this work?
  - Answer: Approximately 7.75% (544M out of 7.3B parameters) for Llama-2 adaptation

## Architecture Onboarding

- Component map: Data collection pipeline → Tokenization → Pretraining (LoRA) → Posttraining (SFT + DPO) → Evaluation
- Critical path: 1) Data collection and preprocessing, 2) Tokenizer training and embedding reinitialization, 3) LoRA pretraining on Dutch corpus, 4) Instruction tuning with SFT and DPO, 5) Evaluation on benchmarks
- Design tradeoffs: Tokenizer modification vs. catastrophic forgetting risk, Parameter-efficient adaptation vs. full fine-tuning capability, Pretraining scale vs. post-training optimization focus
- Failure signatures: Significant perplexity increase during pretraining indicates catastrophic forgetting, No improvement on Dutch benchmarks suggests adaptation failure, English output instead of Dutch indicates tokenizer or post-training issues
- First 3 experiments: 1) Compare perplexity curves for base model vs. adapted models during pretraining, 2) Evaluate tokenization efficiency by comparing token counts for identical Dutch text, 3) Test language generation quality on simple Dutch prompts before and after adaptation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do tokenizer modifications impact language adaptation performance for models with larger vocabularies (e.g., Llama-3's 128k tokens vs Llama-2's 32k)?
- Basis in paper: Explicit - The paper states they chose not to modify Llama-3's tokenizer due to its larger vocabulary size and hypothesizes that gains would be more incremental
- Why unresolved: The paper only tested tokenizer modification on Llama-2 and used the original tokenizer for Llama-3 adaptation
- What evidence would resolve it: Testing the same tokenizer modification strategy on Llama-3 and comparing performance metrics

### Open Question 2
- Question: What is the relative contribution of continued pretraining versus posttraining in language adaptation for multilingual foundation models?
- Basis in paper: Explicit - The paper concludes that for Llama-3, language adaptation techniques may benefit more from focusing on language-specific posttraining rather than continued pretraining
- Why unresolved: The study only tested one combination of pretraining + posttraining, not isolating the individual effects
- What evidence would resolve it: Comparative studies testing posttraining alone vs combined pretraining+posttraining on models with strong multilingual capabilities

### Open Question 3
- Question: How reliable are current benchmarks for evaluating language models in lower-resource languages?
- Basis in paper: Explicit - The paper found that readily available Dutch benchmarks may not consistently assess a model's ability to generate fluent Dutch text
- Why unresolved: The paper developed a new benchmark but acknowledges it still relies on GPT-4o as an evaluator, which may introduce its own biases
- What evidence would resolve it: Development and validation of benchmark evaluation methods that don't rely on large language models with potential biases

## Limitations
- Limited generalizability across languages: Study focuses exclusively on Dutch, making it unclear if patterns hold for other languages
- Commercial dataset constraints: Industry-collected datasets are not publicly released, limiting reproducibility
- Benchmarks without baselines: ChocoLlama-Bench lacks published baselines from other Dutch language models
- Token reduction without context: 29.4% fewer tokens reported without analysis of whether this translates to improved downstream performance

## Confidence
- High confidence: The effectiveness of LoRA for language adaptation is well-supported by both theoretical foundations and practical results showing successful Dutch adaptation
- Medium confidence: The benefits of tokenizer modification with embedding reinitialization are demonstrated in practice, but corpus evidence is weak
- Medium confidence: The conclusion about post-training being more valuable than continued pretraining for multilingual models is supported by empirical results but based on limited comparisons

## Next Checks
1. Cross-linguistic validation: Test the adaptation methodology on at least two additional languages with different linguistic properties to determine if observed patterns generalize beyond Dutch
2. Ablation study on corpus composition: Systematically vary the composition of the Dutch training corpus to determine which data sources contribute most to performance improvements
3. Tokenizer efficiency vs. quality trade-off: Conduct a controlled experiment comparing the new Dutch tokenizer against the original tokenizer on identical downstream tasks to determine whether token reduction translates to measurable performance gains