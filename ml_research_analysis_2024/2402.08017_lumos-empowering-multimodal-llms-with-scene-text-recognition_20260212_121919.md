---
ver: rpa2
title: 'Lumos : Empowering Multimodal LLMs with Scene Text Recognition'
arxiv_id: '2402.08017'
source_url: https://arxiv.org/abs/2402.08017
tags:
- text
- image
- detection
- lumos
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Lumos is an end-to-end multimodal QA system that integrates on-device\
  \ Scene Text Recognition (STR) to enable high-quality text understanding from in-the-wild\
  \ images. By running STR on-device on full-resolution images and feeding recognized\
  \ text and location data into a cloud-based MM-LLM, Lumos achieves 80% QA accuracy\u2014\
  a 28% improvement over MM-LLM alone."
---

# Lumos : Empowering Multimodal LLMs with Scene Text Recognition

## Quick Facts
- arXiv ID: 2402.08017
- Source URL: https://arxiv.org/abs/2402.08017
- Authors: Ashish Shenoy; Yichao Lu; Srihari Jayakumar; Debojeet Chatterjee; Mohsen Moslehpour; Pierce Chuang; Abhay Harpale; Vikas Bhardwaj; Di Xu; Shicong Zhao; Longfang Zhao; Ankit Ramchandani; Xin Luna Dong; Anuj Kumar
- Reference count: 40
- Primary result: 80% QA accuracy with on-device STR, 28% improvement over MM-LLM alone

## Executive Summary
Lumos is an end-to-end multimodal QA system that integrates on-device Scene Text Recognition (STR) to enable high-quality text understanding from in-the-wild images. By running STR on-device on full-resolution images and feeding recognized text and location data into a cloud-based MM-LLM, Lumos achieves 80% QA accuracy—a 28% improvement over MM-LLM alone. The on-device STR pipeline includes ROI detection, text detection, recognition, and reading-order reconstruction, with model sizes under 8 MB, latency under 1 s, and power usage of 0.4 mWh.

## Method Summary
Lumos uses a hybrid architecture with on-device STR processing and cloud-based MM-LLM inference. The system captures full-resolution images, performs ROI detection to isolate text regions, then runs text detection and recognition using optimized models (FBNetv2 backbone, CTC loss). Reading order reconstruction organizes recognized words into paragraphs. This text and location data, along with a low-resolution thumbnail, are sent to the cloud where an MM-LLM processes the query and generates answers. Hardware acceleration and model quantization enable efficient on-device processing within tight resource constraints.

## Key Results
- 80% QA accuracy on complex text-based tasks, 28% improvement over vanilla MM-LLM
- 14.6% WER on public STR benchmarks, outperforming cloud OCR services
- End-to-end latency ≤ 5 seconds with on-device STR running in parallel with image transfer

## Why This Works (Mechanism)

### Mechanism 1
On-device STR processing of full-resolution images enables higher text recognition accuracy than cloud-based OCR on thumbnails. The full-resolution image preserves fine text details lost in thumbnail compression, allowing on-device STR to detect and recognize text accurately despite the device's limited compute resources. Core assumption: ROI detection can sufficiently isolate the text of interest, reducing the computational load to make on-device processing feasible. Evidence: [abstract] "Our on-device STR solution achieves 14.6% WER... enabling an average accuracy of 80% on complex text-based QA tasks, improving over vanilla MM-LLM solution by 28%." [section 4.1] Describes ROI detection that crops relevant regions to reduce computational overhead while preserving text detail. Break condition: If ROI detection fails to isolate text or text occupies too large a fraction of the image, computational constraints will prevent on-device processing.

### Mechanism 2
Parallel execution of STR and image transfer reduces end-to-end latency by overlapping the slowest component. While the full-resolution image is being transferred to the cloud, the on-device STR runs concurrently on the same image, making STR processing effectively free in terms of added latency. Core assumption: Image transfer latency dominates overall system latency, and STR processing time is less than or comparable to transfer time. Evidence: [abstract] "Our proposed system has an average end-to-end latency of ≤ 5 seconds... Lumos does not add extra latency most of the time." [section 3] Explicitly states the parallelization strategy and its latency benefits. Break condition: If network conditions significantly reduce transfer time or if STR processing becomes much slower due to model complexity, the latency benefit disappears.

### Mechanism 3
Hardware acceleration and model quantization enable efficient on-device STR within tight resource constraints. Quantizing models to int8 and optimizing for hardware accelerators reduces latency by 9x and energy consumption by 3x while keeping WER increases minimal. Core assumption: The performance degradation from quantization and hardware constraints is acceptable given the efficiency gains. Evidence: [abstract] "Our on-device STR models have a total size of ≤ 8Mb, a peak memory footprint of ≤ 200Mb, an average latency of ≤ 1sec, and 0.4 mWh power usage." [section 5] Describes the export pipeline including quantization and hardware acceleration. Break condition: If quantization causes excessive accuracy loss or if hardware acceleration is unavailable on target devices, the efficiency gains are lost.

## Foundational Learning

- Concept: ROI detection and its role in reducing computational load
  - Why needed here: Enables processing of full-resolution images on constrained devices by focusing computation on relevant regions
  - Quick check question: How does ROI detection improve both efficiency and accuracy in Lumos?

- Concept: Parallel execution patterns in latency optimization
  - Why needed here: Critical for achieving low end-to-end latency by overlapping slow operations
  - Quick check question: Which two operations are parallelized in Lumos and why?

- Concept: Model quantization and hardware acceleration tradeoffs
  - Why needed here: Enables deployment of large models on resource-constrained devices
  - Quick check question: What are the main benefits and costs of the quantization approach used in Lumos?

## Architecture Onboarding

- Component map:
  - Device side: ASR, camera capture, ROI detection, Text detection, Text recognition, Reading order reconstruction
  - Cloud side: MM-LLM, TTS
  - Data flow: Full-res image → ROI detection → Text detection/recognition → MM-LLM prompt + thumbnail → Answer generation

- Critical path:
  - Image capture → ROI detection → Text detection → Text recognition → MM-LLM inference → TTS
  - Parallelized with image transfer to minimize overall latency

- Design tradeoffs:
  - On-device vs cloud processing: On-device STR provides accuracy but requires model optimization; cloud processing is simpler but loses detail
  - Model size vs accuracy: 8MB model size achieves 14.6% WER, balancing quality and resource constraints
  - Parallelization vs complexity: Parallel execution reduces latency but adds system complexity

- Failure signatures:
  - High WER: Indicates ROI detection failure, text detection/recognition issues, or model quantization problems
  - High latency: Suggests parallel execution failure, network issues, or model inference bottlenecks
  - Incorrect answers: Could be due to text recognition errors, MM-LLM prompt formatting issues, or comprehension failures

- First 3 experiments:
  1. Measure latency breakdown: Capture timing for each component to verify parallelization benefits
  2. A/B test ROI detection: Compare WER with and without ROI detection on sample images
  3. Quantization sensitivity: Test WER and latency at different quantization levels to find optimal tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
How does Lumos handle cases where the ROI detection fails to identify the relevant text region, particularly when there are multiple objects or complex backgrounds in the image? Basis: [explicit] The paper mentions that accidental hands in the picture can lead to wrong detection results and discusses the challenge of non-holding or non-pointing hands. Why unresolved: The paper does not provide detailed strategies for handling cases where ROI detection fails, such as fallback mechanisms or alternative approaches to ensure relevant text is captured. What evidence would resolve it: Experimental results showing the system's performance in scenarios with multiple objects or complex backgrounds, and descriptions of fallback mechanisms or alternative strategies used when ROI detection fails.

### Open Question 2
What is the impact of quantization and hardware acceleration on the accuracy of the on-device STR models, and how does Lumos mitigate potential accuracy losses? Basis: [explicit] The paper states that quantization and model export cause accuracy drops in ML models, and discusses the constraints imposed by hardware acceleration on model architecture. Why unresolved: While the paper mentions accuracy drops, it does not provide detailed analysis of the impact or specific mitigation strategies employed to maintain high accuracy despite these constraints. What evidence would resolve it: Detailed analysis of accuracy metrics before and after quantization/hardware acceleration, along with descriptions of specific techniques used to mitigate accuracy losses.

### Open Question 3
How does Lumos ensure the quality of text recognition in scenarios with extremely small or large text sizes, and what specific techniques are used to handle these variations? Basis: [explicit] The paper mentions challenges C1 and C2 related to text size variations and uses curriculum learning and data augmentation to address these issues. Why unresolved: The paper does not provide detailed information on the effectiveness of these techniques or specific strategies for handling extreme text size variations in real-world scenarios. What evidence would resolve it: Experimental results demonstrating the system's performance with extremely small or large text sizes, and detailed descriptions of specific techniques or adaptations used to handle these variations.

## Limitations
- Dataset specificity: Evaluation based on in-house benchmarks with limited public validation
- Component integration complexity: Integration pipeline robustness to edge cases not thoroughly explored
- Hardware dependency: Reported efficiency metrics tied to specific hardware acceleration

## Confidence

- **High Confidence**: On-device STR architecture and basic latency optimization through parallelization - Clear implementation details and measurable outcomes (14.6% WER, ≤1s latency)
- **Medium Confidence**: Overall QA accuracy improvement (28% over MM-LLM) - Based on internal benchmarks with limited public validation
- **Low Confidence**: Generalizability across diverse real-world scenarios - Limited testing on public benchmarks and edge case handling

## Next Checks

1. **Public Benchmark Validation**: Test Lumos on established STR and VQA benchmarks (e.g., ICDAR, OCR-VQA) to verify the 80% QA accuracy claim and 14.6% WER against standardized metrics

2. **Cross-Device Performance Analysis**: Evaluate Lumos on different hardware platforms (varying CPU/GPU capabilities) to quantify the impact of hardware acceleration on the reported 9x latency reduction and 3x energy savings

3. **Edge Case Robustness Testing**: Systematically test Lumos on images with challenging conditions (low resolution, poor lighting, complex backgrounds, non-standard fonts) to measure failure rates and identify performance bottlenecks in the STR pipeline