---
ver: rpa2
title: 'AesBench: An Expert Benchmark for Multimodal Large Language Models on Image
  Aesthetics Perception'
arxiv_id: '2401.08276'
source_url: https://arxiv.org/abs/2401.08276
tags:
- aesthetic
- mllms
- image
- perception
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AesBench, an expert benchmark for evaluating
  multimodal large language models (MLLMs) on image aesthetics perception. The key
  innovation is constructing the Expert-labeled Aesthetics Perception Database (EAPD)
  with 2,800 diverse-sourced images annotated by professional aesthetic experts.
---

# AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception

## Quick Facts
- arXiv ID: 2401.08276
- Source URL: https://arxiv.org/abs/2401.08276
- Authors: Yipo Huang; Quan Yuan; Xiangfei Sheng; Zhichao Yang; Haoning Wu; Pengfei Chen; Yuzhe Yang; Leida Li; Weisi Lin
- Reference count: 7
- Key outcome: Introduces AesBench benchmark with 2,800 expert-annotated images to evaluate MLLMs on image aesthetics perception across four dimensions, revealing significant performance gaps compared to humans

## Executive Summary
This paper introduces AesBench, an expert benchmark for evaluating multimodal large language models (MLLMs) on image aesthetics perception. The key innovation is constructing the Expert-labeled Aesthetics Perception Database (EAPD) with 2,800 diverse-sourced images annotated by professional aesthetic experts. The benchmark assesses MLLMs across four dimensions: Perception, Empathy, Assessment, and Interpretation. Extensive experiments on 15 MLLMs reveal significant performance gaps compared to humans, with the best model Q-Instruct achieving only 52.86% accuracy on aesthetic assessment.

## Method Summary
AesBench uses the Expert-labeled Aesthetics Perception Database (EAPD) containing 2,800 images with expert annotations, organized into four subsets (AesPQA, AesEQA, AesAQA, AesInter) for evaluating perception, empathy, assessment, and interpretation tasks. The evaluation employs a GPT-assisted strategy that handles various MLLM output formats through decision rules and LLM-based assessment, combined with a text-to-score approach using BERT-based regression to convert qualitative aesthetic descriptions to numerical scores.

## Key Results
- Q-Instruct achieves only 52.86% accuracy on aesthetic assessment, significantly below human performance
- GPT-4V(Plus) outperforms other models but still shows substantial gaps in aesthetic perception
- Models demonstrate better performance on perception tasks (71.91% accuracy) compared to assessment tasks (52.86% accuracy)
- Significant performance variations exist across image types, with models performing better on natural images than artistic or AI-generated images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-quality expert annotations lead to more reliable MLLM evaluation.
- Mechanism: The paper constructs EAPD using 32 professional aesthetic experts who are carefully selected based on health, age, gender, educational background, and personality traits. Each image is annotated by at least one expert, and annotations are saved based on text richness and diversity.
- Core assumption: Expert annotations provide more nuanced and accurate assessments of image aesthetics compared to crowd-sourced or automated labeling.
- Evidence anchors:
  - [section 2.2] "To guarantee quality and diversity of annotation, we recruit subjects by considering six perspectives, including health state, age distribution, gender balance, educational background, working experience, and personality traits."
  - [section 2.3] "To mitigate the risk of test fatigue, consecutive annotations on more than 30 images will trigger a reminder of the break."
  - [corpus] Weak evidence - no direct comparison to crowd-sourced datasets, but the careful selection criteria suggest higher quality.
- Break condition: If the expert selection process is not properly followed, or if experts are not truly qualified in aesthetics, the annotation quality would degrade significantly.

### Mechanism 2
- Claim: Four-dimensional evaluation framework captures comprehensive aesthetic perception abilities.
- Mechanism: The benchmark evaluates MLLMs across Perception (AesP), Empathy (AesE), Assessment (AesA), and Interpretation (AesI), each targeting different aspects of aesthetic understanding.
- Core assumption: Aesthetic perception is a multi-faceted ability that cannot be captured by a single evaluation dimension.
- Evidence anchors:
  - [abstract] "The benchmark assesses MLLMs across four dimensions: Perception, Empathy, Assessment, and Interpretation."
  - [section 3] "These criteria are designed from four dimensions, which are illustrated in Figure 1."
  - [corpus] Weak evidence - while the framework is comprehensive, there's no validation that these four dimensions are sufficient or optimal.
- Break condition: If any of the four dimensions fails to capture an important aspect of aesthetic perception, or if the dimensions overlap significantly, the evaluation framework would be incomplete.

### Mechanism 3
- Claim: GPT-assisted evaluation strategy handles MLLM output format inconsistencies.
- Mechanism: Since MLLMs may output answers in various formats (e.g., "B) No", "B", "No", "The image cannot maintain a balance"), a GPT-based system is used to evaluate both explicit and ambiguous responses.
- Core assumption: A large language model can reliably interpret and evaluate the correctness of various MLLM output formats.
- Evidence anchors:
  - [section 3.1] "Inspired by [Liu et al., 2023b], we propose, validate, and implement a GPT-assisted evaluation strategy that integrates decision rules with GPT."
  - [section 3.1] "outputs of MLLMs that align strictly with the candidate options are regarded as 'explicit responses', determined through direct comparison with the correct option."
  - [corpus] Weak evidence - the paper mentions this approach but doesn't provide validation data for its reliability.
- Break condition: If the GPT evaluation system makes incorrect judgments or introduces bias, the benchmark results would be unreliable.

## Foundational Learning

- Concept: Image aesthetics perception
  - Why needed here: The entire benchmark is built around evaluating how well MLLMs can understand and assess image aesthetics, which is a complex, multi-dimensional task.
  - Quick check question: What are the four dimensions used in the AesBench framework to evaluate aesthetic perception?

- Concept: Multimodal large language models (MLLMs)
  - Why needed here: The benchmark is specifically designed to evaluate MLLMs, which combine visual and language understanding capabilities.
  - Quick check question: How does the paper address the challenge of MLLMs producing outputs in inconsistent formats?

- Concept: Expert annotation quality control
  - Why needed here: The reliability of the benchmark depends on the quality of the annotations, which are provided by carefully selected experts.
  - Quick check question: What are the six perspectives considered when selecting expert annotators?

## Architecture Onboarding

- Component map:
  - EAPD (Expert-labeled Aesthetics Perception Database) -> Four evaluation subsets (AesPQA, AesEQA, AesAQA, AesInter) -> GPT-assisted evaluation system -> Text-to-score approach

- Critical path:
  1. Construct EAPD with expert annotations
  2. Develop evaluation criteria for each of the four dimensions
  3. Implement GPT-assisted evaluation strategy
  4. Conduct experiments on 15 MLLMs
  5. Analyze results and identify performance gaps

- Design tradeoffs:
  - Expert vs. crowd-sourced annotations: Experts provide higher quality but are more expensive and time-consuming
  - Four-dimensional vs. simpler evaluation: More comprehensive but potentially more complex to implement and interpret
  - GPT-assisted vs. rule-based evaluation: More flexible but potentially less transparent

- Failure signatures:
  - Low agreement between expert annotations
  - MLLM outputs consistently misformatted despite explicit instructions
  - GPT-assisted evaluation produces inconsistent results
  - Performance gaps between image types (natural, artistic, AI-generated) are too large

- First 3 experiments:
  1. Evaluate a simple MLLM on the AesPQA subset to test the basic evaluation pipeline
  2. Compare expert annotation consistency by having multiple experts annotate the same images
  3. Test the GPT-assisted evaluation strategy on a small set of known correct/incorrect answers to validate its reliability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of annotations provided by professional aesthetic experts compare to those provided by non-experts in terms of reliability and validity for evaluating MLLMs on image aesthetics perception?
- Basis in paper: [explicit] The paper mentions that the Expert-labeled Aesthetics Perception Database (EAPD) is constructed with high-quality annotations provided by professional aesthetic experts, including researchers engaged in computational aesthetics, educators versed in aesthetic principles, and art students possessing sophisticated art skills. However, the paper does not directly compare the quality of annotations between experts and non-experts.
- Why unresolved: The paper does not provide a comparative analysis between expert and non-expert annotations, which could shed light on the reliability and validity of the EAPD for evaluating MLLMs on image aesthetics perception.
- What evidence would resolve it: Conducting a study that compares the quality of annotations provided by professional aesthetic experts and non-experts, and analyzing the impact of these annotations on the performance of MLLMs on image aesthetics perception tasks.

### Open Question 2
- Question: What are the specific limitations of current MLLMs in handling images with complex aesthetic attributes, such as those involving cultural or contextual nuances?
- Basis in paper: [inferred] The paper highlights that current MLLMs only possess rudimentary aesthetic perception ability and there is still a significant gap between MLLMs and humans in image aesthetics perception. However, it does not delve into the specific limitations of MLLMs in handling complex aesthetic attributes.
- Why unresolved: The paper does not provide a detailed analysis of the limitations of MLLMs in handling complex aesthetic attributes, which could help in understanding the areas that need improvement.
- What evidence would resolve it: Conducting experiments that specifically test the performance of MLLMs on images with complex aesthetic attributes, such as those involving cultural or contextual nuances, and analyzing the results to identify the limitations.

### Open Question 3
- Question: How does the performance of MLLMs on image aesthetics perception tasks vary across different image sources, such as natural images, artistic images, and AI-generated images?
- Basis in paper: [explicit] The paper mentions that the EAPD consists of diverse-sourced images, including natural images, artistic images, and artificial intelligence-generated images. However, it does not provide a detailed analysis of the performance of MLLMs across these different image sources.
- Why unresolved: The paper does not provide a comprehensive comparison of the performance of MLLMs on image aesthetics perception tasks across different image sources, which could help in understanding the strengths and weaknesses of MLLMs in handling different types of images.
- What evidence would resolve it: Conducting experiments that evaluate the performance of MLLMs on image aesthetics perception tasks across different image sources, such as natural images, artistic images, and AI-generated images, and analyzing the results to identify the variations in performance.

## Limitations
- Potential subjectivity of aesthetic evaluation even with expert annotators
- GPT-assisted evaluation strategy introduces potential bias through LLM-based judgment
- Text-to-score conversion may oversimplify the nuanced nature of aesthetic assessment
- Benchmark difficulty may partly reflect task complexity rather than fundamental model limitations

## Confidence
- High confidence in the methodological framework and data collection process
- Medium confidence in the GPT-assisted evaluation reliability
- Medium confidence in the generalizability of results across different MLLM architectures
- Low confidence in the assumption that expert aesthetic judgments are fully reproducible

## Next Checks
1. Conduct inter-rater reliability analysis on a subset of images annotated by multiple experts to quantify annotation consistency
2. Perform ablation studies removing the GPT-assisted evaluation component to assess its impact on overall benchmark results
3. Test model performance on cross-dataset generalization by evaluating MLLMs on AesBench using models trained on different aesthetic datasets