---
ver: rpa2
title: 'MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image
  Generation?'
arxiv_id: '2407.04842'
source_url: https://arxiv.org/abs/2407.04842
tags:
- image
- feedback
- multimodal
- judges
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MJ-Bench, a comprehensive benchmark to evaluate
  multimodal judges (multimodal reward models) for text-to-image generation. It addresses
  the lack of systematic evaluation of these judges across key perspectives: alignment,
  safety, image quality, and bias.'
---

# MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?

## Quick Facts
- arXiv ID: 2407.04842
- Source URL: https://arxiv.org/abs/2407.04842
- Authors: Zhaorun Chen; Yichao Du; Zichen Wen; Yiyang Zhou; Chenhang Cui; Zhenzhen Weng; Haoqin Tu; Chaoqi Wang; Zhengwei Tong; Qinglan Huang; Canyu Chen; Qinghao Ye; Zhihong Zhu; Yuqing Zhang; Jiawei Zhou; Zhuokai Zhao; Rafael Rafailov; Chelsea Finn; Huaxiu Yao
- Reference count: 40
- Primary result: MJ-Bench is a comprehensive benchmark for evaluating multimodal reward models (judges) for text-to-image generation across alignment, safety, image quality, and bias perspectives.

## Executive Summary
This paper introduces MJ-Bench, a comprehensive benchmark to evaluate multimodal judges for text-to-image generation. It addresses the lack of systematic evaluation of these judges across key perspectives: alignment, safety, image quality, and bias. The benchmark incorporates a preference dataset with detailed subcategories, and evaluates a variety of judges, including CLIP-based scoring models, open-source VLMs (e.g., LLaVA family), and closed-source VLMs (e.g., GPT-4o, Claude 3). Experiments show that closed-source VLMs generally provide better feedback, with GPT-4o outperforming others on average. Smaller scoring models excel in alignment and quality, while VLMs offer more accurate safety and bias feedback due to stronger reasoning capabilities. Human evaluations on fine-tuned models confirm the effectiveness of MJ-Bench.

## Method Summary
MJ-Bench introduces a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models. The benchmark evaluates judges across four key perspectives: text-image alignment, safety, image quality, and generation bias. A variety of multimodal judges are evaluated, including smaller-sized CLIP-based scoring models, open-source VLMs (e.g., LLaVA family), and closed-source VLMs (e.g., GPT-4o, Claude 3). Human evaluations on end-to-end fine-tuned models using separate feedback from these multimodal judges provide additional validation of the benchmark's effectiveness.

## Key Results
- Closed-source VLMs generally provide better feedback, with GPT-4o outperforming others on average.
- Smaller-sized scoring models excel in text-image alignment and image quality feedback.
- VLMs provide more accurate feedback regarding safety and generation bias due to their stronger reasoning capabilities.
- Human evaluations on fine-tuned models confirm the effectiveness of MJ-Bench, with overall trends aligning with automatic metrics.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MJ-Bench addresses the lack of systematic evaluation of multimodal judges by providing a comprehensive preference dataset with detailed subcategories across four key perspectives: alignment, safety, image quality, and bias.
- Mechanism: The benchmark curates a preference dataset that includes instruction-image pairs labeled with verifiable reasons for preferences, allowing multimodal judges to be evaluated on their ability to accurately distinguish between chosen and rejected images across diverse and challenging scenarios.
- Core assumption: The curated preference pairs are of high quality and verifiable, ensuring that the judges' performance can be reliably assessed.
- Evidence anchors:
  - [abstract]: "We introduce MJ-Bench, a novel benchmark which incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias."
  - [section]: "To bridge this gap, we propose MJ-Bench, a novel benchmark to evaluate multimodal FMs as a judge for image generation task, where we incorporate a comprehensive preference dataset including four major perspectives, i.e., text-image alignment, safety, image quality, and generation bias."
  - [corpus]: Weak evidence, as the corpus does not directly address the comprehensiveness of the dataset.
- Break condition: If the preference pairs are not verified by human experts or the reasons for preferences are ambiguous, the benchmark's effectiveness in evaluating multimodal judges would be compromised.

### Mechanism 2
- Claim: MJ-Bench reveals that closed-source VLMs generally provide better feedback across different scales, with GPT-4o outperforming others on average, while smaller scoring models excel in alignment and quality, and VLMs offer more accurate safety and bias feedback due to stronger reasoning capabilities.
- Mechanism: By evaluating a wide range of multimodal judges, including smaller-sized CLIP-based scoring models, open-source VLMs, and closed-source VLMs, the benchmark identifies the strengths and limitations of each type of judge across different perspectives.
- Core assumption: The evaluation process is fair and unbiased, allowing for accurate comparisons between different types of multimodal judges.
- Evidence anchors:
  - [abstract]: "Experiments reveal that close-source VLMs generally provide better feedback, with GPT-4o outperforming other judges in average. Compared with open-source VLMs, smaller-sized scoring models can provide better feedback regarding text-image alignment and image quality, while VLMs provide more accurate feedback regarding safety and generation bias due to their stronger reasoning capabilities."
  - [section]: "Specifically, we evaluate a large variety of multimodal judges including smaller-sized CLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and close-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our preference dataset."
  - [corpus]: Weak evidence, as the corpus does not directly address the performance differences between different types of judges.
- Break condition: If the evaluation process is not comprehensive or unbiased, the conclusions about the performance of different types of multimodal judges may not be accurate.

### Mechanism 3
- Claim: MJ-Bench's human evaluations on end-to-end fine-tuned models confirm the effectiveness of the benchmark, as the overall trend aligns with the automatic metrics, despite slight differences.
- Mechanism: By fine-tuning a base image generation model using feedback from each multimodal judge and asking human evaluators to rank the generated images, the benchmark provides an additional layer of validation for the judges' feedback.
- Core assumption: Human evaluators can provide reliable rankings of the fine-tuned models, serving as a validation of the automatic metrics.
- Evidence anchors:
  - [abstract]: "Notably, human evaluations on end-to-end fine-tuned models using separate feedback from these multimodal judges provide similar conclusions, further confirming the effectiveness of MJ-Bench."
  - [section]: "To holistically evaluate these judges in an end-to-end alignment setting, we further fine-tune a base stable-diffusion-v1.5 (SD-1.5) model using feedback from each multimodal judge via RLAIF, and then ask human evaluators to provide a ranking over these fine-tuned models."
  - [corpus]: Weak evidence, as the corpus does not directly address the alignment between human evaluations and automatic metrics.
- Break condition: If human evaluations are not conducted properly or the fine-tuned models are not representative, the validation of the benchmark's effectiveness may not be reliable.

## Foundational Learning

- Concept: Multimodal judges and their role in text-to-image generation
  - Why needed here: Understanding the role of multimodal judges is crucial for comprehending the purpose and significance of MJ-Bench in evaluating these judges.
  - Quick check question: What are the two main types of multimodal judges, and how do they differ in their feedback mechanisms?

- Concept: Reinforcement learning from human feedback (RLHF) and its application in aligning text-to-image models
  - Why needed here: Familiarity with RLHF is essential for understanding how multimodal judges' feedback is used to fine-tune image generation models.
  - Quick check question: How does RLHF contribute to the alignment of text-to-image models with desired behaviors?

- Concept: Bias and fairness in AI systems, particularly in multimodal foundation models
  - Why needed here: Recognizing the importance of evaluating bias in multimodal judges is key to understanding the comprehensive nature of MJ-Bench.
  - Quick check question: Why is it crucial to evaluate the bias of multimodal judges, and how does MJ-Bench address this issue?

## Architecture Onboarding

- Component map:
  - Preference dataset -> Multimodal judges -> Evaluation metrics -> Human evaluation

- Critical path:
  1. Curate the preference dataset with instruction-image pairs labeled with verifiable reasons.
  2. Evaluate a wide range of multimodal judges on the curated dataset.
  3. Analyze the judges' performance across different perspectives and scales.
  4. Conduct human evaluations on fine-tuned models to validate the judges' feedback.
  5. Draw conclusions about the strengths and limitations of each type of judge.

- Design tradeoffs:
  - Comprehensive dataset vs. time and resources required for curation and verification.
  - Evaluation of multiple perspectives vs. potential complexity in analysis.
  - Inclusion of human evaluations vs. additional time and resources needed for human involvement.

- Failure signatures:
  - Inconsistent or biased performance of multimodal judges across different perspectives.
  - Inability of judges to accurately distinguish between chosen and rejected images.
  - Misalignment between automatic metrics and human evaluations.

- First 3 experiments:
  1. Evaluate a smaller set of multimodal judges on a subset of the preference dataset to validate the evaluation process.
  2. Conduct a pilot study with a limited number of human evaluators to assess the feasibility of human evaluations.
  3. Analyze the judges' performance on a single perspective (e.g., alignment) to identify potential issues or areas for improvement.

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on proprietary models (GPT-4o, Claude 3) which cannot be fully reproduced or examined.
- Preference dataset construction relies on human annotations that may contain subjective biases.
- Evaluation focuses primarily on MJ (Midjourney) style images, which may not generalize to other image generation domains or styles.
- Human evaluation component uses a limited pool of evaluators and may not capture diverse perspectives.

## Confidence
- **High confidence**: The benchmark design and methodology are sound, with clear procedures for dataset construction, judge evaluation, and human validation.
- **Medium confidence**: The conclusion that closed-source VLMs generally outperform other judges is well-supported, though the extent of this superiority may vary depending on specific use cases and evaluation criteria.
- **Medium confidence**: The identification of specific strengths (scoring models for alignment/quality, VLMs for safety/bias) is supported by the data, though the reasoning behind these differences could be explored further with additional experiments.

## Next Checks
1. Test the same judges on text-to-image datasets from different domains (e.g., non-MJ styles, different artistic directions) to assess the generalizability of the benchmark findings across diverse image generation scenarios.

2. Reconstruct the preference dataset using different annotation protocols or crowd-sourcing approaches to evaluate the robustness of the benchmark to variations in dataset construction methodology.

3. Include additional open-source VLMs and scoring models, particularly those with varying capabilities and architectures, to determine whether the observed performance patterns hold across a broader range of judge types and to identify potential emerging trends in judge effectiveness.