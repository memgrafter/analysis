---
ver: rpa2
title: Plan-Grounded Large Language Models for Dual Goal Conversational Settings
arxiv_id: '2402.01053'
source_url: https://arxiv.org/abs/2402.01053
tags:
- user
- dialogue
- plan
- table
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of developing large language
  models (LLMs) capable of guiding users through procedural plans in a dual goal conversational
  setting. The authors propose PlanLLM, an LLM trained to follow a plan of procedures
  while simultaneously addressing user instructions and questions.
---

# Plan-Grounded Large Language Models for Dual Goal Conversational Settings

## Quick Facts
- arXiv ID: 2402.01053
- Source URL: https://arxiv.org/abs/2402.01053
- Authors: Diogo Glória-Silva; Rafael Ferreira; Diogo Tavares; David Semedo; João Magalhães
- Reference count: 24
- Key outcome: PlanLLM achieves a 2.1x improvement over a strong baseline in dual goal conversational settings

## Executive Summary
This paper introduces PlanLLM, a large language model trained to guide users through procedural plans while simultaneously addressing user instructions and questions. The authors propose a multi-objective approach using supervised fine-tuning (SFT) followed by Direct Preference Optimization (DPO) to optimize for plan navigation, plan-grounded question answering, open-ended requests, and conversational norms. Experiments demonstrate that PlanLLM significantly outperforms baselines and generalizes well to unseen domains like DIY tasks.

## Method Summary
PlanLLM is trained on a dataset of 1000 unique recipes and 10,000 generated dialogues using a multi-objective approach. The training pipeline involves supervised fine-tuning (SFT) of base models (OPT-1.3B, DollyV2-3B, Vicuna-7B) with LoRA adapters, followed by preference optimization using DPO and DPO-x. Preference pairs are generated from SFT outputs, and the models are evaluated using BERTScore, ROUGE-L, and GPT-4 annotations for human evaluation.

## Key Results
- PlanLLM achieves a 2.1x improvement over a strong baseline in controlled settings
- The model generalizes well to unseen domains, demonstrating strong performance on DIY tasks
- Human evaluation with GPT-4 shows improved alignment with human preferences compared to reference responses

## Why This Works (Mechanism)

### Mechanism 1
Multi-objective fine-tuning with DPO enables PlanLLM to align responses with human preferences while maintaining plan navigation accuracy. DPO directly optimizes the model to prefer responses that are more helpful, polite, and accurate, bypassing the need for explicit reward modeling used in RLHF. The core assumption is that the preference pairs used in DPO are representative of true human preferences and cover the range of dialogue scenarios needed.

### Mechanism 2
Data augmentation using real user-system interactions creates realistic dialogue patterns that improve model robustness. User utterances and system responses are sampled from a directed graph built from Alexa Prize Taskbot Challenge 1, weighted by observed transition probabilities between intents. The core assumption is that real user interactions capture the diversity and noise of natural dialogue better than synthetic or paid annotator data.

### Mechanism 3
LoRA fine-tuning enables efficient adaptation of large models for dual-goal conversational tasks without full fine-tuning. Low-rank adapters are trained on top of frozen model weights, reducing memory and compute requirements while preserving base model knowledge. The core assumption is that LoRA adapters can capture task-specific behavior without catastrophic forgetting of general language understanding.

## Foundational Learning

- **Concept**: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Provides a foundation for understanding how DPO improves alignment with human preferences without the complexity of RL.
  - Quick check question: How does DPO differ from RLHF in terms of reward estimation and optimization?

- **Concept**: Transformer decoder-only architecture
  - Why needed here: Understanding the base architecture helps explain why LoRA is effective and how input formatting affects training.
  - Quick check question: What is the role of causal attention in generating sequential dialogue responses?

- **Concept**: Data augmentation techniques
  - Why needed here: Critical for generating diverse, realistic training data that covers the range of user behaviors in dual-goal settings.
  - Quick check question: Why is using real user-system interactions better than synthetic data for training conversational agents?

## Architecture Onboarding

- **Component map**: Base LLM (OPT, DollyV2, Vicuna) → LoRA adapter → DPO fine-tuning → PlanLLM
  - Data pipeline: Real user interactions → Intent graph → Template-based responses → Preference data
  - Training loop: SFT → DPO/DPO-x → Evaluation (BERTScore, ROUGE, GPT-4 annotations)

- **Critical path**:
  1. Load base model and attach LoRA adapter
  2. Fine-tune with SFT on plan-grounded dialogues
  3. Generate preference pairs from SFT outputs
  4. Train with DPO/DPO-x using preference pairs
  5. Evaluate on test set and user study

- **Design tradeoffs**:
  - LoRA vs full fine-tuning: Memory/compute vs potential performance
  - DPO vs RLHF: Simplicity and stability vs potential for more nuanced reward shaping
  - Real user data vs synthetic: Realism and diversity vs control and scalability

- **Failure signatures**:
  - Model ignores plan steps and focuses only on user requests
  - Model generates unsafe or inappropriate responses
  - Model fails to generalize to unseen domains (e.g., DIY tasks)
  - Training instability (exploding gradients, NaN losses)

- **First 3 experiments**:
  1. Train base model with SFT only; evaluate on plan navigation accuracy
  2. Train with DPO; compare win rate against reference responses
  3. Test zero-shot generalization on unseen domain (DIY); measure navigation and QA performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of PlanLLM compare to commercial models like GPT-3.5-Turbo in terms of long-horizon multi-turn dialogues?
- **Basis in paper**: The paper conducted a user study comparing PlanLLM with GPT-3.5-Turbo on overall interaction quality across both cooking and DIY domains.
- **Why unresolved**: While the study showed PlanLLM outperforming GPT-3.5-Turbo, the analysis was limited to a small sample size (5 participants) and focused on overall interaction quality rather than specific aspects of long-horizon dialogues such as consistency, task completion, and handling complex user requests over extended conversations.
- **What evidence would resolve it**: A larger-scale user study with more participants and a focus on specific aspects of long-horizon dialogues, such as consistency in following the plan, handling complex user requests, and maintaining context over extended conversations, would provide more comprehensive insights into PlanLLM's performance compared to commercial models.

### Open Question 2
- **Question**: Can PlanLLM effectively handle parallelization of actions or chain-of-thought reasoning for answering causal questions in procedural plans?
- **Basis in paper**: The paper mentions that PlanLLM does not explore parallelization of actions or chain-of-thought reasoning for answering causal questions as limitations.
- **Why unresolved**: The paper does not provide any empirical evidence or analysis of PlanLLM's performance in handling parallelization of actions or chain-of-thought reasoning for answering causal questions, which are important aspects of reasoning about procedural plans.
- **What evidence would resolve it**: Conducting experiments to evaluate PlanLLM's performance in handling parallelization of actions and answering causal questions in procedural plans, and comparing it with other models or approaches that specifically address these aspects, would provide insights into its capabilities and limitations.

### Open Question 3
- **Question**: How well does PlanLLM generalize to procedural plans from unseen domains beyond cooking and DIY?
- **Basis in paper**: The paper mentions that PlanLLM was evaluated on a novel domain (DIY) and showed good generalization performance, but the evaluation was limited to DIY tasks from WikiHow.
- **Why unresolved**: The paper does not provide evidence of PlanLLM's performance on procedural plans from other unseen domains, such as medical procedures, educational tutorials, or technical manuals, which may have different characteristics and complexities.
- **What evidence would resolve it**: Evaluating PlanLLM on a diverse set of procedural plans from various unseen domains, such as medical procedures, educational tutorials, and technical manuals, and comparing its performance with other models or approaches, would provide insights into its generalization capabilities and limitations across different domains.

## Limitations

- The evaluation relies heavily on automated metrics and GPT-4 as a proxy for human judgment, which may not fully capture nuanced aspects of conversational quality and plan navigation accuracy.
- The paper's claims rest on untested assumptions about the representativeness of preference pairs and the diversity of the real user interaction graph from Alexa Prize Taskbot Challenge 1.
- The extent and robustness of PlanLLM's generalization to unseen domains beyond cooking and DIY are not thoroughly quantified.

## Confidence

- PlanLLM's 2.1x improvement over baseline: Medium confidence
- DPO effectiveness for plan-grounded dialogue: Medium confidence
- LoRA efficiency for dual-goal tasks: Medium confidence
- Generalization to unseen domains: Low confidence

## Next Checks

1. Validate Preference Pair Quality: Conduct a human evaluation of a sample of preference pairs used in DPO training to assess whether they truly represent human preferences and cover edge cases (e.g., ambiguous user requests, conflicting goals).

2. Test LoRA Rank Sensitivity: Systematically vary the LoRA rank and evaluate its impact on plan navigation accuracy, memory usage, and computational efficiency to determine the optimal trade-off for the dual-goal task.

3. Assess Generalization Robustness: Evaluate PlanLLM on a broader set of unseen domains (e.g., home maintenance, financial planning) and measure its performance on plan navigation, question answering, and open-ended requests to quantify the limits of its generalization capability.