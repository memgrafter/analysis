---
ver: rpa2
title: 'TARGA: Targeted Synthetic Data Generation for Practical Reasoning over Structured
  Data'
arxiv_id: '2412.19544'
source_url: https://arxiv.org/abs/2412.19544
tags:
- query
- question
- targa
- methods
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TARGA addresses the challenges of semantic parsing in structured
  environments by dynamically generating targeted synthetic data without manual annotation.
  The method constructs relevant queries through layer-wise expansion and cross-layer
  combination from pertinent entities and relations, then generates corresponding
  natural language questions for in-context learning.
---

# TARGA: Targeted Synthetic Data Generation for Practical Reasoning over Structured Data

## Quick Facts
- **arXiv ID:** 2412.19544
- **Source URL:** https://arxiv.org/abs/2412.19544
- **Reference count:** 22
- **Primary result:** TARGA achieves +7.7 F1 on GrailQA and +12.2 F1 on KBQA-Agent using only a 7B-parameter model, outperforming non-fine-tuned methods that use larger closed-source models.

## Executive Summary
TARGA addresses the challenges of semantic parsing in structured environments by dynamically generating targeted synthetic data without manual annotation. The method constructs relevant queries through layer-wise expansion and cross-layer combination from pertinent entities and relations, then generates corresponding natural language questions for in-context learning. Using only a 7B-parameter model, TARGA achieves significant improvements in F1 scores on GrailQA (+7.7) and KBQA-Agent (+12.2), outperforming non-fine-tuned methods that use larger closed-source models. The framework demonstrates superior sample efficiency, robustness, and generalization capabilities under non-I.I.D. settings.

## Method Summary
TARGA generates targeted synthetic data online for each test question by constructing relevant queries through layer-wise expansion and cross-layer combination starting from pertinent entities and relations. These synthetic queries are paired with natural language questions to form demonstrations for in-context learning with a 7B-parameter LLM. The method avoids the need for extensive manual annotation by generating relevant query structures on-demand, then uses re-ranking to ensure the highest-quality demonstrations are selected. This approach enables strong performance across KBQA tasks while maintaining sample efficiency and robustness.

## Key Results
- Achieves +7.7 F1 score improvement on GrailQA benchmark
- Achieves +12.2 F1 score improvement on KBQA-Agent benchmark
- Outperforms non-fine-tuned methods using only a 7B-parameter model while baselines rely on larger closed-source models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic synthetic data generation enables TARGA to avoid the limitations of static annotated datasets and generalize better to unseen questions.
- **Mechanism:** TARGA constructs relevant queries through layer-wise expansion and cross-layer combination starting from pertinent entities and relations, then generates corresponding natural language questions for in-context learning demonstrations.
- **Core assumption:** The synthetic queries generated through systematic expansion and combination will be relevant to the test question and cover the necessary reasoning patterns.
- **Evidence anchors:**
  - [abstract] "Starting from the pertinent entities and relations of a given question, we probe for the potential relevant queries through layer-wise expansion and cross-layer combination."
  - [section 3.3] "Starting from the simplest structure (L1 in Figure 2), we progressively search for more complex query structures through Layer-wise Expansion (for multi-hop structures) and Cross-layer Combination (for multi-constraint structures)"
  - [corpus] Weak - no direct corpus evidence found for synthetic data generation in KBQA specifically
- **Break condition:** If the synthetic query generation process fails to produce relevant queries or generates too many irrelevant ones, the method's effectiveness will degrade.

### Mechanism 2
- **Claim:** In-context learning with high-quality synthetic demonstrations enables TARGA to achieve strong performance without parameter updates.
- **Mechanism:** TARGA uses the synthetic (NLQ, Query) pairs as demonstrations for in-context learning, allowing the 7B model to reason over structured data effectively.
- **Core assumption:** The LLM can effectively learn from the synthetic demonstrations to generate accurate queries for new questions.
- **Evidence anchors:**
  - [abstract] "we generate corresponding natural language questions for these constructed queries to jointly serve as the synthetic demonstrations for in-context learning"
  - [section 3.5] "To help the LLM understand the semantics of the provided query, we equip each generated query with its corresponding natural language questions (NLQ), forming (NLQ, Query) pairs"
  - [section 4.3.1] "Even with only one demonstration, our synthetic setting significantly outperforms the random and retrieval settings with 20 shots"
- **Break condition:** If the LLM fails to learn effectively from the demonstrations or if the demonstrations are not representative of the test questions, performance will degrade.

### Mechanism 3
- **Claim:** TARGA's approach enables superior sample efficiency and robustness compared to methods relying on large annotated datasets.
- **Mechanism:** By generating targeted synthetic data online for each test question, TARGA avoids the need for extensive manual annotation and achieves better performance with fewer demonstrations.
- **Core assumption:** The online generation of relevant synthetic data for each test question is more efficient than retrieving similar examples from a static dataset.
- **Evidence anchors:**
  - [abstract] "TARGA achieves this with only a 7B-parameter model, whereas most baselines rely on advanced closed-source models"
  - [section 4.3.1] "TARGA uses only 10 demonstrations but still achieves the best performance, demonstrating notable data efficiency"
  - [section 4.3.2] "Our method exhibits significantly stronger robustness under adversarial conditions"
  - [corpus] Weak - no direct corpus evidence found for sample efficiency comparisons
- **Break condition:** If the online generation process becomes too computationally expensive or if the synthetic data quality degrades significantly, the efficiency advantage will be lost.

## Foundational Learning

- **Concept:** Knowledge Base Question Answering (KBQA) - the task of converting natural language questions into structured queries executable over a knowledge base
  - Why needed here: TARGA is specifically designed for KBQA tasks, so understanding the fundamentals of this domain is essential
  - Quick check question: What is the formal definition of a knowledge base in the context of KBQA?

- **Concept:** In-Context Learning (ICL) - a paradigm where LLMs learn to perform tasks by observing examples provided within the input, without updating parameters
  - Why needed here: TARGA relies heavily on ICL using synthetic demonstrations for question answering
  - Quick check question: How does in-context learning differ from traditional fine-tuning approaches?

- **Concept:** Entity and Relation Linking - the process of identifying and linking entities and relations mentioned in natural language questions to their corresponding elements in the knowledge base
  - Why needed here: TARGA uses entity and relation linking as initialization for its query construction process
  - Quick check question: What is the role of entity linking in the TARGA framework?

## Architecture Onboarding

- **Component map:** Entity/Relation Linking → Query Construction → Query Reranking → Question Answering
- **Critical path:** Entity/Relation Linking → Query Construction → Query Reranking → Question Answering
  - The most critical components are Query Construction and Question Answering, as they directly impact performance
- **Design tradeoffs:**
  - Query complexity vs. generation speed: More complex queries may be more accurate but take longer to generate
  - Number of demonstrations vs. model performance: More demonstrations may improve performance but increase computational cost
  - Online generation vs. pre-computed data: TARGA generates data online, which is more flexible but potentially slower than using pre-computed datasets
- **Failure signatures:**
  - Low coverage in query construction: If few valid queries are generated, the method may struggle with complex questions
  - Poor ranking quality: If irrelevant queries are ranked highly, they may mislead the LLM
  - LLM generation failures: If the LLM cannot generate valid queries from the demonstrations, the method will fail
- **First 3 experiments:**
  1. Test query construction on a simple question with known answer to verify the expansion and combination mechanisms work correctly
  2. Evaluate ranking quality by checking if relevant queries are consistently ranked higher than irrelevant ones
  3. Test the complete pipeline on a small dataset to verify end-to-end functionality and measure initial performance

## Open Questions the Paper Calls Out

- **Open Question 1:** How does TARGA's synthetic data generation scale to knowledge bases with significantly more entities and relations than Freebase (e.g., Wikidata with over 100 million entities)?
  - Basis in paper: [explicit] The paper discusses the effectiveness of TARGA on Freebase but does not address performance on larger KBs
  - Why unresolved: The paper does not provide experiments or analysis on significantly larger knowledge bases beyond Freebase
  - What evidence would resolve it: Experiments comparing TARGA's performance and efficiency on multiple knowledge bases of varying sizes (Freebase, Wikidata, DBpedia)

- **Open Question 2:** What is the impact of using different text embedding models for query textification on the overall performance of TARGA?
  - Basis in paper: [explicit] The paper mentions using bge-reranker-v2-m3 for query re-ranking but does not explore the effect of different embedding models on textification quality
  - Why unresolved: The paper does not conduct ablation studies or comparisons with alternative embedding models for the textification step
  - What evidence would resolve it: Comparative experiments using different embedding models (e.g., sentence-transformers, other BGE models) for query textification and their effect on downstream QA performance

- **Open Question 3:** How does TARGA's performance compare to fine-tuned models when given access to the same amount of manually annotated data?
  - Basis in paper: [explicit] The paper compares TARGA against fine-tuned methods but does not explore scenarios where TARGA is given a small amount of manual annotations as a supplement
  - Why unresolved: The paper focuses on the zero-shot setting but does not investigate the potential benefits of combining synthetic data with a small amount of human annotations
  - What evidence would resolve it: Experiments comparing TARGA's performance with and without a small set of manual annotations against fully fine-tuned models using the same amount of data

## Limitations
- Missing implementation details for critical components including query textification rules and exact re-ranking thresholds
- Limited evidence for performance on knowledge bases larger than Freebase
- Claims about generalization to non-I.I.D. settings based on limited testing

## Confidence
- **High Confidence:** The core methodology of using targeted synthetic data generation through layer-wise expansion and cross-layer combination is well-founded and technically sound
- **Medium Confidence:** The claims about sample efficiency and robustness are supported by experimental results but could benefit from additional ablation studies
- **Low Confidence:** The paper's claims about generalization to non-I.I.D. settings are based on limited evidence

## Next Checks
1. **Query Construction Validation:** Test the layer-wise expansion and cross-layer combination mechanisms on a set of questions with known answers to verify that the generated queries correctly capture the intended reasoning patterns and that the re-ranking effectively prioritizes relevant candidates.
2. **Sample Efficiency Analysis:** Conduct controlled experiments varying the number of demonstrations (1, 5, 10, 20) to quantify the relationship between synthetic data quantity and model performance, verifying the claimed data efficiency advantage.
3. **Robustness Testing:** Design adversarial examples that target specific failure modes in the query construction process (e.g., questions requiring complex multi-hop reasoning or rare entity types) to systematically evaluate TARGA's robustness claims.