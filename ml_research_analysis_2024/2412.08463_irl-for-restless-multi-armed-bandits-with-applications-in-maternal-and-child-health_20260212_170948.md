---
ver: rpa2
title: IRL for Restless Multi-Armed Bandits with Applications in Maternal and Child
  Health
arxiv_id: '2412.08463'
source_url: https://arxiv.org/abs/2412.08463
tags:
- whirl
- expert
- beneficiaries
- rewards
- health
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WHIRL addresses the challenge of learning rewards for restless
  multi-armed bandits (RMABs) in large-scale public health settings, where manual
  reward design is infeasible and existing IRL methods don't scale. The core innovation
  is enabling public health experts to specify aggregate-level goals (e.g., shift
  interventions from low-risk to high-risk beneficiaries), then automatically generating
  expert trajectories at scale and using gradient-based updates to learn RMAB rewards.
---

# IRL for Restless Multi-Armed Bandits with Applications in Maternal and Child Health

## Quick Facts
- arXiv ID: 2412.08463
- Source URL: https://arxiv.org/abs/2412.08463
- Reference count: 28
- Public health experts can specify aggregate-level goals, enabling scalable reward learning for RMABs that outperforms manual design and existing IRL methods

## Executive Summary
WHIRL addresses the challenge of learning rewards for restless multi-armed bandits (RMABs) in large-scale public health settings, where manual reward design is infeasible and existing IRL methods don't scale. The core innovation is enabling public health experts to specify aggregate-level goals (e.g., shift interventions from low-risk to high-risk beneficiaries), then automatically generating expert trajectories at scale and using gradient-based updates to learn RMAB rewards. This approach is made possible through novel algorithms for trajectory generation and differentiable evaluation functions. Experiments show WHIRL significantly outperforms existing IRL baselines in both accuracy and scalability, handling up to 200 arms compared to baselines' 6 arms. In real-world maternal and child health settings, WHIRL successfully learns rewards that align with expert goals, redistributing interventions toward high-risk beneficiaries and achieving better listening outcomes than both naive RMAB policies and hand-crafted reward alternatives. The method is publicly released at https://github.com/Gjain234/WHIRL.

## Method Summary
WHIRL enables public health experts to specify aggregate-level goals rather than manually designing individual rewards. The system converts these directives into expert trajectories at scale using a maximum entropy algorithm, then employs gradient-based updates to optimize RMAB rewards that align with the expert demonstrations. Unlike traditional IRL methods that construct exponential joint-state MDPs, WHIRL leverages the separable structure of RMABs to compute arm-wise gradients efficiently, reducing computation from O(M^N) to O(NM^ω) per gradient step. The approach uses a soft-top-k policy to provide differentiability while maintaining bounded rationality, allowing for scalable and accurate reward learning that can be applied to real-world public health resource allocation problems.

## Key Results
- WHIRL scales to 200 arms while existing IRL baselines only handle 6 arms
- Successfully learns rewards that redistribute interventions toward high-risk beneficiaries in maternal health settings
- Achieves better listening outcomes than both naive RMAB policies and hand-crafted reward alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WHIRL enables public health experts to specify aggregate-level goals instead of manually designing individual rewards.
- Mechanism: The system converts aggregate directives (e.g., "move interventions from low-risk to high-risk beneficiaries") into expert trajectories at scale using Algorithm 2, which reallocates interventions probabilistically based on maximum entropy principles.
- Core assumption: Public health experts can express their preferences through aggregate population-level directives that can be translated into specific trajectory modifications.
- Evidence anchors:
  - [abstract] "First we allow public health experts to specify their goals at an aggregate or population level and propose an algorithm to design expert trajectories at scale based on those goals."
  - [section] "Because experts cannot generate per-arm trajectories for thousands of arms, we use desired aggregate behavior, which is specified as moving interventions from some source categories of arms to target categories of arms in the observed trajectory τ."
- Break condition: If aggregate directives become too complex or contradictory, the trajectory generation may fail to capture the intended expert preferences accurately.

### Mechanism 2
- Claim: WHIRL uses gradient-based updates to efficiently learn RMAB rewards that align with expert goals.
- Mechanism: The system computes Whittle indices using known transition probabilities and estimated rewards, generates a soft-top-k policy, evaluates it against expert trajectories using maximum likelihood estimation, and performs gradient ascent updates on the reward parameters.
- Core assumption: The Whittle policy and its derivatives with respect to rewards are differentiable, allowing gradient-based optimization.
- Evidence anchors:
  - [abstract] "our algorithm WHIRL uses gradient updates to optimize the objective, allowing for efficient and accurate learning of RMAB rewards."
  - [section] "We then pass that policy to our evaluation function that evaluates the likelihood of seeing T expert using R. WHIRL keeps track of the gradient with respect to reward throughout so that we can perform updates on R."
- Break condition: If the relationship between rewards and Whittle indices becomes non-differentiable or highly non-linear, gradient-based optimization may fail to converge to optimal rewards.

### Mechanism 3
- Claim: WHIRL scales to thousands of arms by avoiding the exponential complexity of traditional IRL methods.
- Mechanism: Instead of constructing a joint-state MDP that grows exponentially with the number of arms, WHIRL computes arm-wise gradients and leverages the separable structure of RMABs, reducing computation from O(M^N) to O(NM^ω) per gradient step.
- Core assumption: The separable structure of RMAB rewards allows arm-wise computation without significant loss of accuracy.
- Evidence anchors:
  - [section] "In contrast, existing IRL algorithms would construct a large joint-state MDP's with computation cost O(M^N)."
  - [section] "To apply it, we need to compute dEval/dR. dEval(πlearner,T expert)/dπlearner can be directly computed since it is a sum of differentiable log functions (Equation 7), and dπlearner/dW was shown to be differentiable in [20]."
- Break condition: If RMAB structure breaks down (e.g., with strong inter-arm dependencies), the separable computation may no longer be valid.

## Foundational Learning

- Concept: Restless Multi-Armed Bandits (RMAB)
  - Why needed here: RMABs model the resource allocation problem where each beneficiary (arm) can be in different states and interventions affect their future states.
  - Quick check question: What is the key difference between regular multi-armed bandits and restless multi-armed bandits?

- Concept: Inverse Reinforcement Learning (IRL)
  - Why needed here: IRL allows learning reward functions from expert demonstrations rather than requiring manual reward design.
  - Quick check question: How does IRL differ from standard reinforcement learning in terms of what is being learned?

- Concept: Whittle Index Policy
  - Why needed here: The Whittle index provides a tractable solution method for RMABs by reducing the problem to a single-arm index computation.
  - Quick check question: What is the mathematical definition of the Whittle index for a given arm state?

## Architecture Onboarding

- Component map: Trajectory generation module -> WHIRL learning module -> Policy evaluation module -> Deployment interface
- Critical path: Expert directive → Trajectory generation → WHIRL learning (gradient updates) → Policy evaluation → Expert review → Deployment. The bottleneck is typically the gradient computation step, which scales linearly with the number of arms.
- Design tradeoffs: The soft-top-k policy introduces bounded rationality but provides differentiability. Using aggregate directives sacrifices per-arm specificity but enables scalability. The gradient-based approach trades off some optimization guarantees for computational efficiency.
- Failure signatures: If expert directives are too vague or contradictory, trajectory generation may produce inconsistent results. If gradients become unstable, learning may diverge. If the soft-top-k parameter ε is poorly tuned, the policy may become too random or too deterministic.
- First 3 experiments:
  1. Test trajectory generation with simple aggregate directives on a small synthetic dataset to verify the maximum entropy property.
  2. Run WHIRL with known rewards on synthetic data to verify it can recover the original rewards within tolerance.
  3. Test the complete pipeline with real maternal health data using simple aggregate directives to verify the system produces reasonable policy changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does WHIRL's performance scale with even larger numbers of arms (e.g., 10,000+)?
- Basis in paper: [inferred] The paper shows WHIRL works with 200 arms, but real-world scenarios could involve thousands more.
- Why unresolved: The paper only tests up to 200 arms, leaving uncertainty about performance at much larger scales.
- What evidence would resolve it: Experimental results showing WHIRL's accuracy and runtime with 10,000+ arms would demonstrate true scalability limits.

### Open Question 2
- Question: How sensitive is WHIRL to the quality and specificity of expert aggregate directives?
- Basis in paper: [explicit] The paper mentions using diverse commands but doesn't systematically study directive quality effects.
- Why unresolved: While the paper shows WHIRL can handle complex directives, it doesn't explore how directive quality impacts learning outcomes.
- What evidence would resolve it: Controlled experiments varying directive specificity and quality while measuring resulting reward learning accuracy.

### Open Question 3
- Question: What are the long-term effects of using WHIRL-learned rewards on beneficiary health outcomes?
- Basis in paper: [inferred] The paper focuses on immediate listening behavior changes, not longitudinal health impacts.
- Why unresolved: The evaluation focuses on listening metrics, not actual health outcomes that the interventions aim to improve.
- What evidence would resolve it: Longitudinal studies tracking health outcomes of beneficiaries under WHIRL policies versus baseline approaches over extended periods.

## Limitations

- The separable structure assumption for RMAB rewards may not hold in real-world public health settings with significant inter-arm dependencies
- Gradient-based optimization relies on differentiability of Whittle indices, which may become non-linear in complex scenarios
- Trajectory generation requires domain expertise to specify appropriate feature thresholds, but the paper provides limited guidance on this critical step

## Confidence

- **High confidence**: The core claim that WHIRL can learn RMAB rewards from aggregate expert directives is well-supported by the experimental results, showing significant improvement over baselines in both synthetic and real-world settings.
- **Medium confidence**: The scalability claim (handling 200 arms vs. 6 for baselines) is demonstrated but relies on idealized assumptions about RMAB structure that may not hold in all public health contexts.
- **Medium confidence**: The fairness improvement claim is supported by the hand-crafted reward comparison, but the evaluation focuses on a specific maternal health dataset and may not generalize to all public health applications.

## Next Checks

1. Test WHIRL's performance on RMAB datasets with explicitly introduced inter-arm dependencies to evaluate how well the separable computation assumption holds in practice.
2. Conduct a systematic sensitivity analysis on the epsilon parameter in the soft-top-k policy to determine how different levels of bounded rationality affect learning quality and fairness outcomes.
3. Implement a cross-validation framework to evaluate WHIRL's performance when trained on aggregate directives from one population and tested on a different demographic group to assess generalizability.