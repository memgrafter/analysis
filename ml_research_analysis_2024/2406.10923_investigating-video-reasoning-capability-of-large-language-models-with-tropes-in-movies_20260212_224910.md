---
ver: rpa2
title: Investigating Video Reasoning Capability of Large Language Models with Tropes
  in Movies
arxiv_id: '2406.10923'
source_url: https://arxiv.org/abs/2406.10923
tags:
- reasoning
- abstract
- video
- long-range
- compositional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Tropes in Movies (TiM), a dataset designed
  to evaluate the Abstract Perception and Long-range Compositional Reasoning capabilities
  of Large Language Models (LLMs) in video reasoning tasks. TiM leverages tropes from
  movie storytelling to create complex, abstract queries requiring deep understanding
  of character interactions and narrative contexts.
---

# Investigating Video Reasoning Capability of Large Language Models with Tropes in Movies

## Quick Facts
- **arXiv ID**: 2406.10923
- **Source URL**: https://arxiv.org/abs/2406.10923
- **Reference count**: 40
- **Primary result**: LLM-based methods marginally outperform random baselines on TiM dataset; FEVoRI+ConQueR improves performance by 15 F1 points but still lags behind human levels (40 vs. 65 F1)

## Executive Summary
This paper introduces Tropes in Movies (TiM), a dataset designed to evaluate the Abstract Perception and Long-range Compositional Reasoning capabilities of Large Language Models (LLMs) in video reasoning tasks. TiM leverages tropes from movie storytelling to create complex, abstract queries requiring deep understanding of character interactions and narrative contexts. Experiments show that state-of-the-art LLM-based methods, including Captioner-Reasoner, Large Multimodal Model Instruction Fine-tuning, and Visual Programming, only marginally outperform random baselines on TiM. To address these challenges, the authors propose Face-Enhanced Viper of Role Interactions (FEVoRI) and Context Query Reduction (ConQueR), which significantly improve performance by 15 F1 points. However, the performance still lags behind human levels (40 vs. 65 F1). The paper also introduces a new protocol, AST Based Code Diagnosis (ABCD), to evaluate the complexity of Abstract Perception and Long-range Compositional Reasoning in datasets.

## Method Summary
The study employs state-of-the-art LLM-based methods (Captioner-Reasoner, Large Multimodal Model Instruction Fine-tuning, and Visual Programming) on the TiM dataset, which contains 684 movies annotated with per-shot keyframes, subtitles, and trope labels. The authors enhance the Viper framework with Face-Enhanced Viper of Role Interactions (FEVoRI) to improve character identification through face detection tools, and Context Query Reduction (ConQueR) to decompose complex queries into manageable components. A new evaluation protocol, AST Based Code Diagnosis (ABCD), is introduced to quantify the complexity of reasoning tasks by analyzing Abstract Syntax Trees of generated code. The primary metric is micro F1 score, with experiments comparing baseline models against enhanced approaches.

## Key Results
- State-of-the-art LLM-based methods only marginally outperform random baselines on TiM dataset
- FEVoRI+ConQueR enhancements improve performance by 15 F1 points
- Human performance (65 F1) significantly exceeds the best model performance (40 F1)
- TiM dataset reveals challenges in Abstract Perception and Long-range Compositional Reasoning for LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Face-Enhanced Viper of Role Interactions (FEVoRI) improves performance by augmenting role awareness in character identification.
- Mechanism: FEVoRI enhances the base Viper framework by integrating face detection tools, specifically DeepFace, to assign unique IDs to characters. This allows the model to track character interactions and actions across frames more effectively, addressing the Abstract Perception challenge.
- Core assumption: Abstract Perception requires fine-grained understanding of character roles and interactions, which generic object detection cannot provide.
- Evidence anchors:
  - [abstract] "Face-Enhanced Viper of Role Interactions (FEVoRI) that fosters role interaction awareness"
  - [section] "FEVoRI augments Viper by providing a face detection tool with examples in the prompts. FEVoRI enhances the fine-grained understanding of the 'human' object to address Abstract Perception"
  - [corpus] Weak - corpus does not directly address this mechanism
- Break condition: If face detection tools fail to accurately identify characters across varying conditions (lighting, angles, occlusions), the role awareness enhancement would break down.

### Mechanism 2
- Claim: Context Query Reduction (ConQueR) improves performance by progressively decomposing the trope query and movie narrative context.
- Mechanism: ConQueR systematically breaks down the trope query and movie narrative into manageable components, checking if extracted context matches each dimension of a trope through generated program code. This addresses the Long-range Compositional Reasoning challenge.
- Core assumption: Long-range Compositional Reasoning requires breaking down complex narrative elements into smaller, manageable queries that can be processed sequentially.
- Evidence anchors:
  - [abstract] "Context Query Reduction (ConQueR), which enhance Visual Programming by fostering role interaction awareness and progressively refining movie contexts and trope queries during reasoning processes"
  - [section] "ConQueR addresses the Long-range Compositional Reasoning challenge by progressively decomposing the narrative context and trope query"
  - [corpus] Weak - corpus does not directly address this mechanism
- Break condition: If the progressive decomposition fails to capture essential narrative elements or if the generated code becomes too complex to manage effectively.

### Mechanism 3
- Claim: AST Based Code Diagnosis (ABCD) quantifies the complexity of Abstract Perception and Long-range Compositional Reasoning in datasets.
- Mechanism: ABCD analyzes the Abstract Syntax Tree (AST) of code generated by Visual Programming, measuring VLM calls and tokens for Abstract Perception, and AST nodes and edges for Long-range Compositional Reasoning.
- Core assumption: The complexity of code generated for a task reflects the complexity of the underlying reasoning required, which can be measured through AST analysis.
- Evidence anchors:
  - [abstract] "AST Based Code Dignosis (ABCD), which is AST-based, to evaluate the levels of Abstract Perception and Long-range Compositional Reasoning"
  - [section] "AST is a tree structure that represents the syntactic structure of a code snippet, thereby reflecting the complexity of the reasoning task addressed by VP"
  - [corpus] Weak - corpus does not directly address this mechanism
- Break condition: If the AST analysis fails to capture the true complexity of the reasoning task, or if the generated code becomes too variable to analyze consistently.

## Foundational Learning

- Concept: Abstract Perception in video reasoning
  - Why needed here: TiM specifically targets the challenge of understanding abstract concepts in videos, such as emotions, motivations, and judgments, which are not easily captured by vision models.
  - Quick check question: Can you explain why understanding the "Big Bad" trope requires Abstract Perception rather than just recognizing concrete objects or actions?

- Concept: Long-range Compositional Reasoning
  - Why needed here: TiM involves analyzing hour-long videos with thousands of frames and decomposing complex queries into multiple, nested, interdependent steps.
  - Quick check question: How does the requirement to analyze thousands of frames and decompose complex elements into multiple queries demonstrate the need for Long-range Compositional Reasoning?

- Concept: Abstract Syntax Trees (AST) in code analysis
  - Why needed here: ABCD uses AST to quantify the complexity of reasoning tasks by analyzing the structure of generated code.
  - Quick check question: What information can be derived from analyzing the nodes and edges of an AST in the context of video reasoning tasks?

## Architecture Onboarding

- Component map: TiM dataset -> Baseline models (Captioner-Reasoner, Large Multimodal Model Instruction Fine-tuning, Visual Programming) -> Enhancements (FEVoRI, ConQueR) -> Performance evaluation -> ABCD analysis
- Critical path: Data collection -> Baseline model testing -> Enhancement implementation -> Performance evaluation -> ABCD analysis
- Design tradeoffs: The tradeoff between model complexity and performance, as more complex models may achieve better results but at the cost of increased computational resources and interpretability.
- Failure signatures: Baseline models failing to perform significantly better than random, enhancements not improving performance as expected, or ABCD analysis not accurately reflecting task complexity.
- First 3 experiments:
  1. Test baseline models on TiM dataset to establish performance benchmarks.
  2. Implement FEVoRI enhancement and evaluate its impact on performance.
  3. Implement ConQueR enhancement and evaluate its impact on performance, particularly in combination with FEVoRI.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of FEVoRI and ConQueR change if integrated with a different VLM model like Gemini 1.5, which has demonstrated superior multimodal capabilities compared to BLIP-2?
- Basis in paper: [explicit] The paper mentions that replacing BLIP-2 with Gemini improves performance by 4.5 F1 points, and suggests that FEVoRI and ConQueR could benefit from more advanced VLMs.
- Why unresolved: The paper only tested FEVoRI and ConQueR with BLIP-2 and GPT-4, not with Gemini 1.5 specifically.
- What evidence would resolve it: Experiments integrating FEVoRI and ConQueR with Gemini 1.5 and comparing performance to current results.

### Open Question 2
- Question: What is the optimal frame sampling strategy for TiM that balances computational efficiency with reasoning performance, particularly for identifying tropes requiring fine-grained temporal understanding?
- Basis in paper: [explicit] The paper notes that higher frame rates consistently outperform sparse sampling, with every-shot sampling improving F1 by 2.8 points compared to 16-frame sampling.
- Why unresolved: The paper only tested 16-frame and every-shot sampling, not intermediate strategies or adaptive sampling based on content complexity.
- What evidence would resolve it: Systematic evaluation of different frame sampling densities (e.g., 32, 64, 128 frames) and adaptive sampling strategies on TiM.

### Open Question 3
- Question: How would expanding TiM to include multi-label trope classification affect the performance of current LLM-based methods and the proposed FEVoRI+ConQueR approach?
- Basis in paper: [explicit] The paper mentions that the current task is simplified to binary classification and suggests that future research could revisit multi-label tasks.
- Why unresolved: All experiments were conducted on binary classification, and the paper acknowledges this simplification without exploring multi-label scenarios.
- What evidence would resolve it: Experiments evaluating FEVoRI+ConQueR on multi-label trope classification and comparison with binary classification performance.

### Open Question 4
- Question: How does the complexity of AST structures generated by FEVoRI+ConQueR compare to human-written code for similar video reasoning tasks, and what does this reveal about the model's reasoning sophistication?
- Basis in paper: [explicit] The paper introduces ABCD to analyze AST structures and notes that TiM requires more complex AST structures, but does not compare to human-generated code.
- Why unresolved: The paper only compares AST complexity within the TiM dataset and against other datasets, not against human-written solutions.
- What evidence would resolve it: Comparative analysis of AST structures from FEVoRI+ConQueR against human-written code for the same reasoning tasks.

## Limitations

- Current LLM-based methods only marginally outperform random baselines on abstract narrative comprehension tasks
- The performance gap between models (40 F1) and humans (65 F1) remains significant despite proposed enhancements
- Dataset construction relies heavily on human annotation and automatic extraction pipelines that may introduce inconsistencies
- Evaluation focuses primarily on micro F1 scores without extensive ablation studies on individual trope categories

## Confidence

*High Confidence*: The experimental results showing baseline models' poor performance on TiM, the effectiveness of FEVoRI and ConQueR enhancements, and the gap between model and human performance are well-supported by the data and methodology presented.

*Medium Confidence*: The ABCD framework's ability to quantify Abstract Perception and Long-range Compositional Reasoning complexity through AST analysis, while theoretically sound, requires further validation across different reasoning tasks to confirm its generalizability.

*Low Confidence*: The specific implementation details of some components (particularly the exact prompt engineering for baseline models and the complete ABCD evaluation pipeline) are not fully specified, which could affect reproducibility and independent validation.

## Next Checks

1. Conduct extensive ablation studies on the TiM dataset to evaluate model performance across different trope categories, particularly focusing on whether certain abstract concepts (like "Big Bad" vs "Manic Pixie Dream Girl") are consistently harder to identify.

2. Implement cross-validation with alternative face detection tools beyond DeepFace in FEVoRI to assess the robustness of face-based character identification across varying video qualities and conditions.

3. Test the ABCD framework on additional video reasoning datasets to validate whether AST complexity metrics correlate with task difficulty across different domains beyond movie tropes.