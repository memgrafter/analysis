---
ver: rpa2
title: Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large
  Language Models on Word Sense Disambiguation
arxiv_id: '2411.18337'
source_url: https://arxiv.org/abs/2411.18337
tags:
- sense
- word
- sentence
- prompt
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Large Language Models (LLMs) for Word Sense
  Disambiguation (WSD) using prompt augmentation and knowledge bases. The proposed
  approach combines systematic prompt engineering with a human-in-the-loop process,
  incorporating POS tagging, synonyms, aspect-based sense filtering, and few-shot
  Chain of Thought prompting.
---

# Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation

## Quick Facts
- arXiv ID: 2411.18337
- Source URL: https://arxiv.org/abs/2411.18337
- Reference count: 40
- Large language models achieve up to 88% accuracy on word sense disambiguation when using prompt augmentation with knowledge bases and Chain of Thought reasoning

## Executive Summary
This study evaluates Large Language Models (LLMs) for Word Sense Disambiguation (WSD) using prompt augmentation and knowledge bases. The proposed approach combines systematic prompt engineering with a human-in-the-loop process, incorporating POS tagging, synonyms, aspect-based sense filtering, and few-shot Chain of Thought prompting. Experiments with commercial models (GPT-3.5, GPT-4, Gemini) and open-source models (Gemma, Mixtral, Llama) on the FEWS dataset show substantial performance improvements. The best-performing approach achieved 82% accuracy overall, with DeepSeek R1 and GPT-4 Turbo reaching 88% accuracy in prediction tasks. Further improvements were obtained by parameter tuning on corner cases, with prompt chaining and aspect-based filtering enhancing accuracy by up to 30% on previously misclassified instances.

## Method Summary
The study employs a three-phase approach to evaluate LLMs for WSD. First, an optimal prompt is selected using systematic prompt augmentation with a knowledge base containing sense definitions and examples. Second, multiple commercial and open-source LLMs are evaluated on the FEWS dataset. Third, parameter tuning and prompt refinement are applied to corner cases to improve accuracy. The methodology uses few-shot Chain of Thought prompting, where models are guided through structured reasoning steps to select appropriate sense IDs. Knowledge base examples and synonyms are incorporated to enhance contextual understanding, while aspect-based filtering reduces the candidate sense space.

## Key Results
- Best-performing approach achieved 82% overall accuracy on FEWS dataset
- DeepSeek R1 and GPT-4 Turbo reached 88% accuracy in prediction tasks
- Prompt chaining with aspect-based filtering improved accuracy by up to 30% on previously misclassified instances
- Commercial models generally outperformed open-source alternatives, though DeepSeek R1 matched top commercial performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt augmentation with knowledge base examples and Chain of Thought reasoning improves WSD accuracy by enriching contextual understanding.
- Mechanism: The KB provides sense definitions and usage examples, while COT prompting guides the model through structured reasoning steps (comprehend meanings, analyze sentence, select sense IDs). This combination reduces ambiguity by explicitly linking context to sense definitions.
- Core assumption: LLMs can effectively leverage in-context learning from structured KB examples when guided by step-by-step reasoning.
- Evidence anchors:
  - [abstract] "This study investigates the use of Large Language Models (LLMs) to improve WSD using a novel approach combining a systematic prompt augmentation mechanism with a knowledge base (KB) consisting of different sense interpretations."
  - [section] "The KB used was created from the training data of the FEWS dataset. The optimal prompt selected for this phase is presented in Table 4."
- Break condition: If KB examples are too sparse or irrelevant to the test sentences, the model cannot form meaningful connections between context and sense definitions.

### Mechanism 2
- Claim: Parameter tuning and prompt chaining with aspect-based filtering reduce the sense space and improve disambiguation accuracy.
- Mechanism: Initial prompt filters sense tags based on the aspect (topic/intent) of the sentence, then passes the filtered senses to a second prompt for final prediction. Synonyms are included to enhance lexical knowledge.
- Core assumption: Reducing the candidate sense space based on contextual aspects improves prediction accuracy by eliminating unlikely senses early.
- Evidence anchors:
  - [section] "In the last approach, prompt chaining has been incorporated with an aspect-based filtering method. The initial prompt was assigned to filter the sense tags based on the aspect of the sentence."
  - [section] "The self-consistency approach which uses multiple reasoning strategies with majority vote shows promising results with GPT 4 while sense space reduction approach with aspect-based filtering shows a new avenue to improve the WSD."
- Break condition: If aspect detection is inaccurate or too broad, filtering may remove the correct sense, leading to incorrect predictions.

### Mechanism 3
- Claim: Few-shot Chain of Thought prompting outperforms zero-shot and general prompting by providing explicit examples of sense usage.
- Mechanism: The model is given example sentences with corresponding sense IDs and glosses, which it uses to learn patterns for mapping new sentences to the correct sense. COT structure ensures systematic reasoning.
- Core assumption: LLMs can effectively learn from few examples when provided in a structured, reasoning-oriented format.
- Evidence anchors:
  - [abstract] "By utilizing a few-shot Chain of Thought (COT) prompting-based approach, this work demonstrates a substantial improvement in performance."
  - [section] "This phase employed a human-in-the-loop approach, where the lead researcher used prompt engineering techniques to develop the most suitable prompt for extracting the sense ID. An iterative approach was adopted..."
- Break condition: If the few examples are not representative of the test distribution or the COT structure is too rigid, the model may not generalize well to new instances.

## Foundational Learning

- Concept: Word Sense Disambiguation (WSD)
  - Why needed here: The entire study evaluates LLMs' ability to perform WSD, so understanding the task definition and challenges is fundamental.
  - Quick check question: What is the difference between supervised and knowledge-base approaches to WSD, and why might LLMs offer advantages over traditional methods?

- Concept: Chain of Thought (COT) prompting
  - Why needed here: COT prompting is a core technique used in the study to improve model reasoning and accuracy.
  - Quick check question: How does COT prompting differ from standard prompting, and what are the key components of an effective COT prompt for WSD?

- Concept: Prompt augmentation
  - Why needed here: The study systematically augments prompts with KB examples, synonyms, and aspect-based filtering to improve performance.
  - Quick check question: What are the different types of prompt augmentation used in the study, and how does each type contribute to improved WSD accuracy?

## Architecture Onboarding

- Component map:
  Dataset -> Knowledge Base Creation -> Optimal Prompt Selection -> LLM Evaluation -> Parameter Tuning -> Accuracy Analysis

- Critical path:
  1. Load FEWS test data with ambiguous words marked by <WSD> tags
  2. Create KB from training data with sense definitions and examples
  3. Generate augmented prompt with KB examples and COT structure
  4. Send prompt and sentence to LLM API
  5. Parse JSON output for sense ID(s)
  6. Compare predicted sense ID(s) with ground truth
  7. Calculate accuracy metrics

- Design tradeoffs:
  - KB size vs. latency: Larger KB provides more examples but increases token usage and API costs
  - Prompt complexity vs. accuracy: More structured prompts improve accuracy but may be harder to maintain
  - Commercial vs. open-source models: Commercial models generally perform better but are more expensive and less customizable

- Failure signatures:
  - Low accuracy on verbs: Indicates model struggles with contextual understanding of action words
  - High token usage: Suggests prompts are too verbose or KB is too large
  - Slow execution time: May indicate inefficient prompt structure or model configuration issues
  - Inconsistent predictions: Could signal instability in the few-shot learning approach

- First 3 experiments:
  1. Test basic prompt without KB examples on a small subset of test data to establish baseline accuracy
  2. Add KB examples and COT structure to prompt, measure accuracy improvement
  3. Implement parameter tuning with synonyms and aspect-based filtering on misclassified instances from experiment 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed knowledge base approach compare to other context-based methods like sentence-pair classification or semantic similarity measurements for WSD?
- Basis in paper: [explicit] The paper mentions various approaches in related works including sentence-pair classification (GlossBERT), semantic similarity-based methods, and graph-based algorithms, but doesn't directly compare these to the proposed KB approach
- Why unresolved: The paper focuses on evaluating LLMs with prompt augmentation and KB but doesn't benchmark against traditional WSD methods that don't use LLMs
- What evidence would resolve it: A comparative study evaluating the proposed approach against traditional WSD methods like GlossBERT, graph-based algorithms, and semantic similarity approaches on the same dataset

### Open Question 2
- Question: What is the impact of different knowledge base structures (e.g., trie vs. other data structures) on the performance and efficiency of the WSD system?
- Basis in paper: [explicit] The paper mentions using a trie structure for organizing the knowledge base but doesn't explore or compare alternative data structures
- Why unresolved: The choice of data structure for organizing training data in the KB is not systematically evaluated
- What evidence would resolve it: Experimental results comparing different data structures (hash tables, graphs, tries) for organizing the KB in terms of retrieval time, memory usage, and impact on WSD accuracy

### Open Question 3
- Question: How does the performance of the proposed approach scale with increasing dataset size and more diverse sense distributions?
- Basis in paper: [inferred] The paper uses the FEWS dataset but doesn't investigate performance on larger datasets or datasets with more complex sense distributions
- Why unresolved: The evaluation is limited to one dataset (FEWS) and doesn't explore scalability or performance on datasets with more ambiguous words or broader sense distributions
- What evidence would resolve it: Experiments showing performance metrics (accuracy, execution time, token usage) as the dataset size and sense distribution complexity increase, including comparison across multiple datasets with varying characteristics

### Open Question 4
- Question: What is the contribution of each component in the prompt augmentation pipeline (synonyms, aspect-based filtering, self-consistency) to the overall performance improvement?
- Basis in paper: [explicit] The paper presents results for different combinations of prompt enhancements but doesn't perform ablation studies to isolate the contribution of each component
- Why unresolved: While the paper shows that combining techniques improves performance, it doesn't quantify how much each individual technique contributes to the improvement
- What evidence would resolve it: Ablation studies where each component (synonyms, aspect-based filtering, self-consistency) is removed individually to measure the performance impact, showing the relative contribution of each technique

## Limitations
- Results are limited to the FEWS dataset and may not generalize to other WSD benchmarks
- Performance improvements may not transfer to different domains or languages
- Comparison between commercial and open-source models is constrained by API access limitations

## Confidence

- **High confidence**: The overall methodology of using prompt augmentation with knowledge bases and Chain of Thought prompting for WSD is sound and well-documented in the literature. The performance improvements over baseline approaches are statistically significant.
- **Medium confidence**: The specific implementation details of the prompt engineering techniques (synonym inclusion, aspect-based filtering, few-shot examples) may require fine-tuning for different datasets or use cases.
- **Low confidence**: The long-term stability and generalizability of the parameter tuning approach on corner cases across different LLM architectures and versions.

## Next Checks

1. **Cross-dataset validation**: Test the optimal prompt configuration on at least two other established WSD benchmarks (e.g., SemEval datasets) to assess generalizability.

2. **Open-source reproducibility**: Implement the complete pipeline using only open-source models and evaluate whether commercial model performance gains are primarily due to architecture differences or data advantages.

3. **Temporal robustness test**: Retest the best-performing configurations after 3-6 months with updated model versions to quantify performance degradation or improvement over time.