---
ver: rpa2
title: 'The Promises and Pitfalls of LLM Annotations in Dataset Labeling: a Case Study
  on Media Bias Detection'
arxiv_id: '2411.11081'
source_url: https://arxiv.org/abs/2411.11081
tags:
- bias
- media
- dataset
- llms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the viability of using Large Language Models
  (LLMs) to automate media bias detection dataset labeling. The authors create Anno-lexical,
  a large-scale dataset with over 48,000 synthetically annotated examples, by having
  three selected LLMs label news sentences using few-shot in-context learning.
---

# The Promises and Pitfalls of LLM Annotations in Dataset Labeling: a Case Study on Media Bias Detection

## Quick Facts
- arXiv ID: 2411.11081
- Source URL: https://arxiv.org/abs/2411.11081
- Reference count: 10
- Primary result: SA-FT classifier outperforms annotator LLMs by 5-9% in MCC and performs comparably to HA-FT on media bias benchmarks

## Executive Summary
This study investigates whether Large Language Models (LLMs) can effectively automate media bias detection dataset labeling through synthetic annotation. The authors create Anno-lexical, a large-scale dataset with over 48,000 synthetically annotated examples, by having three selected LLMs label news sentences using few-shot in-context learning. A classifier fine-tuned on this synthetic data (SA-FT) outperforms its annotator LLMs by 5-9% in Matthews Correlation Coefficient (MCC) and performs comparably to or better than a classifier trained on human-labeled data when evaluated on two media bias benchmarks (BABE and BASIL). While the SA-FT approach significantly reduces dataset creation costs, behavioral stress-testing reveals it has lower precision and robustness to input perturbations compared to human-trained models.

## Method Summary
The authors employ a synthetic annotation framework where three selected LLMs (Zephyr 7B beta, OpenChat 3.5, and Llama 2 13B Chat) annotate a balanced corpus of ~400,000 news sentences using few-shot in-context learning with human-labeled examples. Annotations are aggregated via majority voting to create the Anno-lexical dataset (48,330 examples). A RoBERTa classifier is fine-tuned on this synthetic data and compared against classifiers trained on human-annotated data (HA-FT) using BABE and BASIL benchmarks. The approach is evaluated for both performance and robustness through CheckList stress tests.

## Key Results
- SA-FT classifier outperforms all three annotator LLMs by 5-9% in Matthews Correlation Coefficient (MCC)
- SA-FT performs comparably to or better than HA-FT on BABE and BASIL benchmark datasets
- Stress testing reveals SA-FT has lower precision and robustness to input perturbations compared to HA-FT
- The synthetic annotation approach significantly reduces dataset creation costs compared to human labeling

## Why This Works (Mechanism)

### Mechanism 1
Few-shot in-context learning with human-labeled examples enables LLMs to effectively detect lexical bias in news sentences. The prompt includes 8 human-labeled examples with explanations, providing contextual understanding of what constitutes lexical bias, allowing LLMs to apply this knowledge to new sentences. Core assumption: LLMs can learn bias detection patterns from a small number of examples without fine-tuning. Break condition: If LLMs cannot generalize from few examples or examples are not representative.

### Mechanism 2
Aggregating predictions from multiple LLMs via majority vote improves annotation robustness and quality. Using 3 different LLMs to annotate each sentence and taking the majority vote reduces individual model biases and increases correct annotation likelihood. Core assumption: Different LLMs have complementary strengths and weaknesses that average out in majority voting. Break condition: If all selected LLMs share similar biases or majority vote consistently produces incorrect labels due to systematic errors.

### Mechanism 3
Training a smaller classifier on synthetic annotations can match or exceed performance of classifiers trained on human-labeled data. The SA-FT classifier learns to generalize from the larger synthetic dataset (48k examples) compared to smaller human-labeled dataset (3k examples), capturing more diverse bias patterns. Core assumption: Synthetic annotations maintain sufficient quality and diversity to train effective downstream classifiers. Break condition: If synthetic annotations contain systematic errors or downstream classifier overfits to spurious patterns.

## Foundational Learning

- **Media bias detection and its lexical manifestation**: Understanding lexical vs. informational bias in news reporting is essential as the task revolves around identifying biased language. Quick check: What is the difference between lexical bias and informational bias in news reporting?

- **Few-shot learning and in-context learning**: The annotation process relies on providing LLMs with a small number of examples to learn the task without fine-tuning. Quick check: How does providing examples in the prompt help LLMs understand the task compared to traditional fine-tuning?

- **Ensemble methods and majority voting**: The approach uses multiple LLMs and aggregates their predictions to improve robustness. Quick check: Why might majority voting produce more reliable results than using a single best-performing model?

## Architecture Onboarding

- **Component map**: Data collection -> Corpus construction -> Pre-classification -> Annotation -> Classifier training -> Evaluation
- **Critical path**: Annotation → Classifier training → Evaluation. Annotation quality directly determines classifier performance, making LLM selection and prompt design critical
- **Design tradeoffs**: Cost vs. quality (open-source vs. proprietary models), dataset size vs. annotation time (more examples improve classifier but increase costs), number of annotator models vs. robustness (more models improve robustness but increase computational cost)
- **Failure signatures**: Classifier performs well on training data but poorly on benchmarks (overfitting to synthetic data), stress test performance significantly worse than benchmark performance (lack of robustness), results vary significantly with different LLM selections (annotator quality crucial)
- **First 3 experiments**: 1) Run annotation with different numbers of examples in the prompt (2, 4, 8) to find optimal few-shot setting, 2) Compare majority vote vs. weighted voting based on individual model performance, 3) Test classifier performance with different dataset sizes to understand scaling laws

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the SA-FT classifier scale with the size of the synthetic training dataset (Anno-lexical)? The paper mentions creating Anno-lexical with 48,330 sentences but doesn't explore scaling beyond this point. What evidence would resolve it: Experiments training SA-FT on progressively larger subsets of Anno-lexical and measuring performance on benchmark datasets.

### Open Question 2
What is the impact of different combinations of LLM annotators on the downstream classifier performance? The authors mention evaluating different LLM combinations but only selected three based on greedy selection. What evidence would resolve it: Training multiple SA-FT classifiers using different combinations of LLM annotators and comparing their performance on benchmark datasets.

### Open Question 3
How do different prompting strategies (e.g., varying number of examples, explanation quality) affect the quality of synthetic annotations? While the paper evaluated different prompting strategies for annotator selection, it didn't systematically investigate how different strategies affect final annotation quality. What evidence would resolve it: Creating multiple versions of Anno-lexical using different prompting strategies and comparing their impact on downstream classifier performance.

## Limitations
- Synthetic data artifacts may propagate through the classifier, potentially limiting robustness
- Selection of only three LLMs may not capture the full spectrum of potential annotation behaviors
- Few-shot prompting without fine-tuning introduces variability dependent on example quality

## Confidence

**High Confidence**: The core finding that SA-FT classifier outperforms its annotator LLMs by 5-9% in MCC is well-supported by experimental results and aligns with expected behavior of models trained on larger synthetic datasets.

**Medium Confidence**: The claim that SA-FT performs comparably to or better than HA-FT on benchmark datasets is supported by experimental results, but the margin of improvement varies across datasets and metrics.

**Low Confidence**: The generalizability of the approach to other complex NLP tasks beyond media bias detection is speculative, as the study does not provide empirical evidence for its effectiveness on different tasks or domains.

## Next Checks
1. **Cross-dataset Generalization Test**: Evaluate the SA-FT model on additional media bias datasets not used in the original study to assess whether performance advantage over HA-FT is consistent across different data sources and annotation schemes.

2. **Robustness Stress Test Expansion**: Conduct a more comprehensive stress testing protocol that includes adversarial examples, domain shifts, and out-of-distribution samples to better characterize limitations of SA-FT compared to HA-FT.

3. **LLM Ablation Study**: Systematically vary the number and types of annotator LLMs (including different model families and sizes) to quantify the impact of annotator selection on quality and robustness of synthetic annotations.