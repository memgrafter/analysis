---
ver: rpa2
title: LLMs' morphological analyses of complex FST-generated Finnish words
arxiv_id: '2407.08269'
source_url: https://arxiv.org/abs/2407.08269
tags:
- language
- task
- computational
- linguistics
- finnish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates how well large language models (LLMs) can perform
  morphological analysis of complex Finnish noun forms generated using a finite-state
  transducer (FST). By directly prompting GPT-4-turbo, GPT-3.5-turbo, Llama2-70B,
  and Poro-34B to classify number, case, and possessive suffix, the authors found
  that GPT-4-turbo achieves the highest accuracy (~73% overall) and outperforms all
  other models and even simple RNN classifiers.
---

# LLMs' morphological analyses of complex FST-generated Finnish words

## Quick Facts
- arXiv ID: 2407.08269
- Source URL: https://arxiv.org/abs/2407.08269
- Authors: Anssi Moisio; Mathias Creutz; Mikko Kurimo
- Reference count: 21
- Primary result: GPT-4-turbo achieves ~73% accuracy on morphological classification, outperforming other LLMs and RNN baselines

## Executive Summary
This study evaluates how well large language models can perform morphological analysis of complex Finnish noun forms generated using a finite-state transducer (FST). The authors directly prompt GPT-4-turbo, GPT-3.5-turbo, Llama2-70B, and Poro-34B to classify number, case, and possessive suffix in 2000 sampled inflected forms. GPT-4-turbo achieves the highest accuracy (~73% overall) and outperforms all other models and even simple RNN classifiers. Analysis reveals that tokenization granularity affects morphological generalization, especially for possessive suffixes. While GPT-4-turbo shows strong performance, it still doesn't reach human-level systematic morphological generalization, suggesting LLMs rely on heuristics rather than fully learned grammar rules.

## Method Summary
The authors generated ~25 million inflected Finnish noun forms using the Omorfi FST tool, then sampled 2000 test cases. They created prompts with 0, 1, 5, or 10 examples showing the expected output format (base form, number, case, possessive suffix) and the target word. Four LLMs were tested: GPT-4-turbo, GPT-3.5-turbo, Llama2-70B, and Poro-34B, with temperature settings of 0.0 for GPT models and 0.5 for others. Simple RNN classifiers (3-layer, 128 units) trained on 800-80k words served as baselines. Accuracy was calculated for each classification task (number, case, possessive suffix) and combined.

## Key Results
- GPT-4-turbo achieves ~73% overall accuracy, significantly outperforming other LLMs and RNN baselines
- GPT-3.5-turbo struggles with the task, while Llama2-70B and Poro-34B fail nearly completely
- Tokenization granularity correlates with morphological generalization, particularly affecting possessive suffix classification
- Simple RNN classifiers trained on 80k words achieve ~84% accuracy, suggesting a gap between general LLM pretraining and specific morphological competence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4-turbo's ~73% accuracy stems from its subword tokenization preserving morphemes as distinct tokens, enabling better generalization to unseen forms.
- Mechanism: The BPE tokenizer splits inflected words into morphemes (e.g., "lyhtyjämme" → "ly", "hty", "jämme"), allowing the model to map each token to a learned embedding corresponding to a morphological component.
- Core assumption: The tokenizer is tuned such that rare inflected forms still decompose into familiar subword units carrying grammatical meaning.
- Evidence anchors: Analysis shows tokenization granularity affects morphological generalization, especially for possessive suffixes, with Poro-34B's longer tokens correlating with worse possessive suffix classification.

### Mechanism 2
- Claim: 10-shot prompting significantly improves performance by supplying explicit examples of the target format, reducing ambiguity in the expected output structure.
- Mechanism: Each example demonstrates the mapping from inflected form to (base form, number, case, possessive suffix), so the model learns the exact response schema without inferring it from the description alone.
- Core assumption: The model has sufficient capacity to interpolate from a handful of examples to unseen test cases.
- Evidence anchors: The study tests with 0, 1, 5, or 10 examples before the test word, finding that GPT-4-turbo performs better with more examples.

### Mechanism 3
- Claim: GPT-4-turbo's superiority stems from its pretraining on multilingual data, including Finnish, which provides implicit knowledge of morphological rules.
- Mechanism: During pretraining, exposure to diverse inflected forms teaches the model to associate suffix patterns with grammatical functions, enabling it to handle novel combinations by analogy.
- Core assumption: The training corpus contains enough inflected Finnish to cover the rule space, even if exact test forms are rare.
- Evidence anchors: Finnish has ~32B tokens of available training texts, making it a sufficiently resourced language for SOTA multilingual LLMs to be fluent in Finnish grammar.

## Foundational Learning

- Concept: Morphological segmentation
  - Why needed here: The task requires identifying morphemes (number, case, possessive suffix) within inflected words; without understanding segmentation, the model cannot parse forms correctly.
  - Quick check question: Given "talostani", can you list the base form, number, case, and possessive suffix?

- Concept: Finite-state morphology
  - Why needed here: The dataset is generated by an FST (Omorfi), which systematically produces all possible inflected forms; understanding FST helps interpret why certain rare forms appear.
  - Quick check question: What is the FST output for the plural inessive with first-person plural possessive of "koira"?

- Concept: Subword tokenization (BPE)
  - Why needed here: Tokenization granularity directly affects whether morphemes are preserved as separate tokens; this influences morphological generalization.
  - Quick check question: How does BPE tokenize "lyhtyjämme" differently from "tarttumassamme", and why does that matter for possessive suffix classification?

## Architecture Onboarding

- Component map: FST (Omorfi) → 25M inflected forms → 2000 sampled test cases → Prompt templates → LLMs (GPT-4-turbo, GPT-3.5-turbo, Llama2-70B, Poro-34B) → Parsed predictions → Accuracy comparison

- Critical path: 1) Generate test set via FST sampling, 2) Prepare prompts with varying shot counts, 3) Run inference on each model, 4) Parse model outputs to extract classifications, 5) Compare against ground truth, compute accuracy

- Design tradeoffs: More examples in prompt → higher accuracy but more cost/time; longer tokens (Poro) → better semantic coherence but worse morphological granularity; RNN baseline → no pretraining overhead but limited generalization

- Failure signatures: Low accuracy on possessive suffix → tokenization merges morpheme with case suffix; consistent confusion between similar cases → class imbalance in training data; nonsense outputs from Llama2 → prompt format not understood

- First 3 experiments: 1) Compare 0-shot vs. 10-shot accuracy on held-out validation set to quantify prompt effect, 2) Analyze tokenization of 100 misclassified possessive suffixes to confirm token-merging hypothesis, 3) Train small character-level RNN on same 80k-word subset to see if sequence modeling without tokenization improves results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does tokenization granularity systematically affect morphological generalization across different language families?
- Basis in paper: The authors observed that Poro-34B's longer tokens (3.55 characters on average) correlated with worse possessive suffix classification, while GPT-4-turbo's shorter tokens (2.26 characters) performed better.
- Why unresolved: The study only tested Finnish morphology with three model families. Different languages may have different morpheme boundaries and tokenization challenges that could lead to varying effects of token length.
- What evidence would resolve it: Comparative experiments testing multiple languages with different morphological complexity and their corresponding tokenization schemes across various LLM architectures.

### Open Question 2
- Question: What is the minimum amount of morphological training data needed for LLMs to achieve near-perfect systematic morphological generalization?
- Basis in paper: The authors found that simple RNN classifiers trained on 80k words achieved ~84% accuracy, while GPT-4-turbo achieved ~73% without training, suggesting a gap between general language model pretraining and specific morphological competence.
- Why unresolved: The paper only compared GPT-4-turbo (no training) with RNNs trained on varying dataset sizes, but didn't explore intermediate training approaches or fine-tuning strategies for LLMs.
- What evidence would resolve it: Controlled experiments fine-tuning LLMs on morphologically annotated datasets of varying sizes to determine the inflection point where systematic generalization approaches human-level performance.

### Open Question 3
- Question: Are LLMs fundamentally limited in learning grammar rules versus heuristics, or can they be augmented to achieve rule-like systematic generalization?
- Basis in paper: The authors concluded that GPT-4-turbo relies on heuristics rather than fully learned grammar rules, as evidenced by its inability to achieve perfect possessive suffix classification despite knowing morphological labels.
- Why unresolved: The study only tested classification accuracy and didn't explore whether architectural modifications, training objectives, or prompting strategies could enable more rule-like behavior.
- What evidence would resolve it: Experiments testing whether targeted fine-tuning, curriculum learning, or specialized architectures can transform heuristic-based morphological processing into more systematic, rule-like generalization across multiple grammatical phenomena.

## Limitations

- The study relies entirely on model-generated classifications without human verification of ground truth accuracy
- Only tests GPT-4-turbo with 10-shot prompting without exploring whether different prompt formulations might yield better results
- Tokenization analysis remains correlational rather than causal, as the study doesn't experimentally manipulate tokenization to prove its effect on accuracy

## Confidence

**High Confidence**: The finding that GPT-4-turbo outperforms other LLMs and RNN baselines on morphological classification (73% accuracy) is well-supported by experimental results and consistent with the performance gap observed across all tested models.

**Medium Confidence**: The hypothesis that tokenization granularity affects morphological generalization, particularly for possessive suffixes, is supported by observational data but would require controlled experiments to establish causality.

**Medium Confidence**: The claim that 10-shot prompting significantly improves performance is supported by comparative results, but the study doesn't systematically vary shot counts to quantify the exact improvement curve.

## Next Checks

1. **Ground Truth Verification**: Have native Finnish speakers manually verify a random sample of 100 FST-generated forms and their morphological analyses to ensure ground truth labels are accurate before attributing model errors to model limitations.

2. **Tokenization Experiment**: Create a controlled experiment where the same morphological analysis task is performed on manually segmented morphemes (not tokenized words) to isolate the effect of tokenization from other factors affecting model performance.

3. **Prompt Ablation Study**: Systematically vary the number of examples in prompts (0, 1, 3, 5, 10, 20 shots) and measure accuracy on a held-out validation set to determine the optimal shot count and quantify the marginal benefit of additional examples.