---
ver: rpa2
title: 'RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models'
arxiv_id: '2412.02830'
source_url: https://arxiv.org/abs/2412.02830
tags:
- reasoning
- rare
- medical
- question
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RARE enhances reasoning accuracy and factual integrity in large
  language models by integrating retrieval-augmented actions (A6 and A7) and a Retrieval-Augmented
  Factuality Scorer within the MCTS framework. The two novel actions enable dynamic
  information retrieval from external sources to support reasoning, while the factuality
  scorer evaluates and ranks reasoning paths based on factual reliability.
---

# RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models

## Quick Facts
- arXiv ID: 2412.02830
- Source URL: https://arxiv.org/abs/2412.02830
- Authors: Hieu Tran; Zonghai Yao; Junda Wang; Yifan Zhang; Zhichao Yang; Hong Yu
- Reference count: 34
- Key outcome: RARE improves reasoning accuracy and factual integrity in LLMs through retrieval-augmented actions and factuality scoring, achieving competitive performance with top models on medical and commonsense reasoning benchmarks.

## Executive Summary
RARE introduces a novel framework that enhances reasoning accuracy and factual integrity in large language models by integrating retrieval-augmented actions (A6 and A7) and a Retrieval-Augmented Factuality Scorer within the MCTS framework. The method addresses the limitations of existing approaches that struggle with factual inconsistencies and hallucinations, particularly in knowledge-intensive domains like medicine. Experimental results demonstrate significant performance improvements on MedQA, StrategyQA, and CommonsenseQA benchmarks, with LLaMA 3.1 achieving competitive results with top models like GPT-4.

## Method Summary
RARE operates through two main stages: Candidate Generation with a Retrieval-Augmented Generator and Factuality Evaluation with a Retrieval-Augmented Factuality Scorer. The generator builds on the MCTS-based rStar self-generator, incorporating two new retrieval-augmented actions (A6 and A7) that dynamically fetch relevant external information during intermediate reasoning steps. A6 generates search queries from the initial question, retrieves relevant documents, and augments reasoning with retrieved data. A7 refines sub-questions, retrieves targeted information, and updates the next reasoning step. The Retrieval-Augmented Factuality Scorer (RAFS) evaluates each candidate trajectory's factual reliability by verifying the alignment of intermediate reasoning steps with retrieved evidence, assigning a factuality score that reflects the trajectory's consistency with external knowledge.

## Key Results
- RARE improved accuracy by 5.17% over rStar on MedQA using LLaMA 3.1 8B
- Closed the gap with state-of-the-art models on StrategyQA and CommonsenseQA benchmarks
- Achieved competitive performance with top models like GPT-4 while maintaining lower computational costs than some alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented reasoning with A6 and A7 actions improves accuracy by dynamically incorporating relevant external information during reasoning steps.
- Mechanism: A6 generates search queries from the initial question, retrieves relevant documents, and augments reasoning with retrieved data to formulate a final answer. A7 refines sub-questions, retrieves targeted information, and updates the next reasoning step. These actions integrate real-time, context-specific information into intermediate reasoning steps, enhancing both contextual relevance and factual accuracy.
- Core assumption: The retrieved information is accurate, relevant, and effectively integrated into the reasoning process without introducing noise or confusion.
- Evidence anchors:
  - [abstract] "RARE incorporates two innovative actions within the Monte Carlo Tree Search (MCTS) framework: (A6), which generates search queries based on the initial problem statement, performs information retrieval using those queries, and augments reasoning with the retrieved data to formulate the final answer; and (A7), which leverages information retrieval specifically for generated sub-questions and re-answers these sub-questions with the relevant contextual information."
  - [section] "RARE introduces two new retrieval-augmented actions into the original rStar self-generator, transforming it into a Retrieval-Augmented Generator, as illustrated in Figures 3 and 4. These retrieval-augmented actions enable the generator to dynamically incorporate external knowledge during intermediate reasoning steps, improving both the contextual relevance and factual accuracy of generated responses."
- Break condition: Retrieved information is inaccurate, irrelevant, or poorly integrated, leading to incorrect or confused reasoning.

### Mechanism 2
- Claim: The Retrieval-Augmented Factuality Scorer (RAFS) improves reasoning reliability by evaluating and ranking reasoning paths based on factual accuracy.
- Mechanism: RAFS splits reasoning paths into individual statements, generates retrieval queries for each statement, retrieves supporting information, and labels each statement as Supported or Not Supported based on alignment with retrieved evidence. The overall factuality score for a reasoning path is calculated as the proportion of supported statements, allowing RARE to prioritize responses that align closely with verified external knowledge.
- Core assumption: The retrieved evidence is comprehensive, accurate, and the alignment scoring effectively captures the factual reliability of reasoning statements.
- Evidence anchors:
  - [abstract] "Additionally, a Retrieval-Augmented Factuality Scorer is proposed to replace the original discriminator, prioritizing reasoning paths that meet high standards of factuality."
  - [section] "RAFS evaluates each candidate trajectory's factual reliability. This scorer verifies the alignment of intermediate reasoning steps with retrieved evidence, assigning a factuality score that reflects the trajectory's consistency with external knowledge."
- Break condition: The retrieved evidence is incomplete, inaccurate, or the alignment scoring fails to capture factual reliability, leading to prioritization of incorrect reasoning paths.

### Mechanism 3
- Claim: The MCTS framework with a diverse set of reasoning actions enables more effective exploration of the solution space compared to single-action reasoning methods.
- Mechanism: MCTS operates through iterative selection, expansion, simulation, and backpropagation phases, balancing exploration and exploitation via the Upper Confidence Bound applied on Trees (UCT). RARE builds upon rStar, which extends MCTS by incorporating five distinct reasoning actions (A1-A5) and two new retrieval-augmented actions (A6-A7). This diverse action space allows the generator to explore a wider range of possible solutions, leading to reasoning paths that are both logically coherent and enriched with external knowledge.
- Core assumption: The MCTS algorithm effectively balances exploration and exploitation in the reasoning space, and the diverse set of actions enables the discovery of optimal reasoning paths.
- Evidence anchors:
  - [section] "RARE operates in two main stages: Candidate Generation with Retrieval-Augmented Generator and Factuality Evaluation with Retrieval-Augmented Factuality Scorer. The retrieval-augmented generator builds on the MCTS-based rStar self-generator, incorporating two new retrieval-augmented actions that dynamically fetch relevant external information."
  - [section] "MCTS has been widely adopted in domains such as game theory and strategic planning when integrated with reinforcement learning, MCTS enables self-play training, achieving human-level or even superhuman performance in complex domains like Go."
- Break condition: The MCTS algorithm fails to effectively balance exploration and exploitation, or the diverse set of actions does not enable the discovery of optimal reasoning paths.

## Foundational Learning

- Concept: Monte Carlo Tree Search (MCTS)
  - Why needed here: MCTS provides the underlying search framework for exploring the space of possible reasoning actions and trajectories. It enables RARE to systematically evaluate different reasoning paths and select the most promising one based on rewards (factuality scores).
  - Quick check question: What are the four main phases of MCTS and how do they contribute to the search process?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG is the core mechanism for integrating external knowledge into the reasoning process. RARE uses RAG to fetch relevant information during intermediate reasoning steps (A6 and A7 actions) and to evaluate the factual accuracy of reasoning paths (RAFS).
  - Quick check question: How does RAG differ from traditional generation approaches that rely solely on pre-trained model knowledge?

- Concept: Factuality Scoring
  - Why needed here: Factuality scoring is essential for ensuring the reliability of generated responses, especially in knowledge-intensive domains like medicine. RAFS evaluates the alignment of reasoning statements with retrieved evidence to prioritize factually accurate paths.
  - Quick check question: What are the key steps involved in the RAFS evaluation process and how does it contribute to the selection of the final answer?

## Architecture Onboarding

- Component map: Question → Retrieval-Augmented Generator (MCTS) → Candidate Reasoning Paths → RAFS → Final Answer
- Critical path: Question → Retrieval-Augmented Generator (MCTS) → Candidate Reasoning Paths → RAFS → Final Answer
- Design tradeoffs:
  - Increased computational cost due to iterative retrieval and reasoning (RARE uses ~120k tokens vs ~93k for rStar)
  - Improved accuracy and factual reliability, especially on knowledge-intensive tasks
  - Tradeoff between exploration (trying different reasoning actions) and exploitation (refining promising paths)
- Failure signatures:
  - Low accuracy on medical or commonsense reasoning tasks
  - Factual inconsistencies or hallucinations in generated responses
  - Inability to handle complex, multi-step reasoning problems
  - High computational cost with minimal accuracy gains
- First 3 experiments:
  1. Evaluate RARE on a small subset of MedQA questions (e.g., 50 samples) and compare accuracy to rStar baseline.
  2. Analyze the impact of individual RARE components by running ablation studies (e.g., rStar + A6, rStar + A7, rStar + RAFS) on the same MedQA subset.
  3. Examine common reasoning paths leading to correct answers on MedQA and StrategyQA to understand the effectiveness of different action sequences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RARE perform on larger, proprietary models like GPT-4 or Claude?
- Basis in paper: [inferred] The paper states that RARE has only been tested on open-source models like LLaMA 3.1 due to high API call costs for larger proprietary models.
- Why unresolved: The framework is designed to be model-agnostic but hasn't been evaluated on proprietary models.
- What evidence would resolve it: Experimental results comparing RARE's performance on proprietary models versus open-source models.

### Open Question 2
- Question: Can RARE be extended to optimize for the shortest or most robust reasoning paths?
- Basis in paper: [inferred] The paper mentions that RARE identifies a single reasoning path but doesn't optimize for the best or shortest path that maximizes robustness.
- Why unresolved: Current design focuses on factual accuracy rather than path efficiency or robustness.
- What evidence would resolve it: Implementation and evaluation of alternative reward functions or path optimization strategies.

### Open Question 3
- Question: How would integrating a trained reward model affect RARE's reasoning quality and efficiency?
- Basis in paper: [inferred] The paper notes that RARE currently uses MCTS for path exploration but doesn't leverage a trained reward model to guide the search.
- Why unresolved: MCTS is effective but may not be optimal compared to reward model-guided search.
- What evidence would resolve it: Comparative experiments between RARE with MCTS versus RARE with a trained reward model.

## Limitations
- Limited evaluation on real-world deployment and medical applications where errors could have serious consequences
- Higher computational cost compared to baseline methods (120k vs 93k tokens)
- Lack of detailed ablation studies showing individual contribution of each component

## Confidence

- **High confidence**: The mechanism of retrieval-augmented actions (A6/A7) improving reasoning accuracy by incorporating external knowledge is well-supported by experimental results showing consistent improvements across multiple datasets.
- **Medium confidence**: The factuality scorer's effectiveness in prioritizing factually accurate reasoning paths is demonstrated, but the evaluation methodology could be more rigorous with additional human verification.
- **Medium confidence**: The MCTS framework's ability to explore reasoning space effectively is theoretically sound, but the paper doesn't provide detailed analysis of search tree behavior or action selection patterns.

## Next Checks

1. **Ablation Study**: Conduct systematic ablation experiments removing each RARE component (A6, A7, RAFS) individually to quantify their individual contributions to accuracy improvements, providing clearer understanding of which components drive performance gains.

2. **Human Evaluation**: Perform comprehensive human evaluation on 100 randomly selected reasoning paths from MedQA and StrategyQA, having medical experts and general knowledge experts rate factual accuracy, reasoning coherence, and overall quality to validate automated factuality scoring.

3. **Robustness Testing**: Test RARE on adversarial question sets designed to probe factual consistency, including questions with subtly incorrect premises or requiring cross-document reasoning, to evaluate performance beyond standard benchmarks and assess real-world reliability.