---
ver: rpa2
title: In-Context Learning Demonstration Selection via Influence Analysis
arxiv_id: '2402.11750'
source_url: https://arxiv.org/abs/2402.11750
tags:
- influence
- inficl
- training
- demonstration
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting effective demonstrations
  for in-context learning (ICL) with large language models. The authors propose InfICL,
  a method that uses influence functions to analyze the impact of training samples
  and identify the most influential ones as demonstrations.
---

# In-Context Learning Demonstration Selection via Influence Analysis

## Quick Facts
- arXiv ID: 2402.11750
- Source URL: https://arxiv.org/abs/2402.11750
- Reference count: 3
- One-line primary result: Proposed method outperforms state-of-the-art baselines in most cases, achieving up to 86.32% accuracy on SST2 with Llama-2-70B

## Executive Summary
This paper addresses the challenge of selecting effective demonstrations for in-context learning (ICL) with large language models. The authors propose InfICL, a method that uses influence functions to analyze the impact of training samples and identify the most influential ones as demonstrations. Unlike previous approaches that rely on costly retraining-based influence analysis, InfICL uses gradient-based influence analysis with a simple classifier trained on LLM-generated embeddings, making it more efficient. The method was evaluated on three real-world text classification datasets using four different LLMs.

## Method Summary
The authors propose InfICL, which leverages influence functions to identify the most influential training samples as demonstrations for ICL. The method uses gradient-based influence analysis instead of retraining-based approaches, training a simple classifier on LLM-generated embeddings to determine influence. This design choice aims to reduce computational costs while maintaining effectiveness in demonstration selection. The approach is evaluated across three text classification datasets using four different LLMs, comparing performance against state-of-the-art baselines.

## Key Results
- InfICL achieves up to 86.32% accuracy on SST2 with Llama-2-70B
- Statistical significance tests confirm superiority over non-influence analysis baselines in 20 out of 24 cases
- InfICL requires fewer LLM access calls compared to other influence analysis-based methods

## Why This Works (Mechanism)
The method works by leveraging influence functions to identify training samples that have the greatest impact on model performance. By using gradient-based analysis rather than retraining-based approaches, the method efficiently determines which samples are most influential when used as demonstrations. The use of LLM-generated embeddings as features for the classifier allows the model to capture semantic relationships between training samples and their potential effectiveness as demonstrations.

## Foundational Learning
1. **Influence Functions** - Mathematical tools for measuring how much individual training samples affect model predictions; needed for identifying influential demonstrations; quick check: verify influence scores correlate with demonstration quality
2. **Gradient-based Analysis** - Computational approach to approximate influence without retraining; needed for efficiency; quick check: compare runtime against retraining-based methods
3. **LLM-generated Embeddings** - Vector representations of text produced by large language models; needed for capturing semantic features; quick check: validate embedding quality through downstream task performance
4. **In-context Learning** - Prompting paradigm where models learn from demonstrations without parameter updates; needed as the target application; quick check: measure performance across different prompt formats
5. **Statistical Significance Testing** - Methods for determining whether performance differences are meaningful; needed for validating results; quick check: apply t-tests or similar to compare methods
6. **Text Classification** - Supervised learning task of categorizing text; needed as evaluation domain; quick check: verify class balance and dataset quality

## Architecture Onboarding

**Component Map**
LLM-generated embeddings -> Influence function analysis -> Gradient computation -> Simple classifier training -> Demonstration selection

**Critical Path**
The most critical path is from LLM-generated embeddings through influence function analysis to demonstration selection. This sequence determines which demonstrations are ultimately used for ICL, making it essential for the method's effectiveness.

**Design Tradeoffs**
The method trades off computational efficiency against potential accuracy gains from more complex analysis methods. By using gradient-based influence analysis instead of retraining-based approaches, it reduces computational costs but may miss some nuanced relationships that full retraining would capture.

**Failure Signatures**
- Poor influence function estimates leading to suboptimal demonstration selection
- Classifier overfitting to LLM embeddings, reducing generalization
- Computational bottlenecks in gradient computation for large models
- Ineffective demonstration selection on datasets with complex or nuanced patterns

**First 3 Experiments**
1. Baseline comparison: Run InfICL against standard ICL with random demonstration selection
2. Ablation study: Test performance with and without influence analysis component
3. Efficiency analysis: Measure computational costs (time, memory, LLM calls) across different dataset sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to three text classification datasets and four specific LLMs
- Gradient-based influence analysis may not fully capture complex relationships
- Computational efficiency gains based primarily on reduced LLM access calls without comprehensive overhead analysis

## Confidence
- Core claims (accuracy improvements): High
- Efficiency claims (computational costs): Medium
- Methodological novelty (gradient-based approach): High

## Next Checks
1. Test InfICL on diverse NLP tasks beyond text classification, including generation and reasoning tasks
2. Conduct ablation studies to isolate the impact of gradient-based influence analysis component
3. Perform detailed computational complexity analysis comparing total training time, memory usage, and inference latency against both influence-based and non-influence-based baselines across different hardware configurations