---
ver: rpa2
title: Understanding the Capabilities and Limitations of Large Language Models for
  Cultural Commonsense
arxiv_id: '2405.04655'
source_url: https://arxiv.org/abs/2405.04655
tags:
- commonsense
- cultural
- llms
- language
- country
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models show significant performance gaps in cultural
  commonsense tasks across different cultures, particularly underperforming on Iran
  and Kenya. Their general commonsense capability is also affected by cultural context,
  with erroneous associations favoring well-represented cultures like the United States.
---

# Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense

## Quick Facts
- arXiv ID: 2405.04655
- Source URL: https://arxiv.org/abs/2405.04655
- Reference count: 16
- Key outcome: Large language models show significant performance gaps in cultural commonsense tasks across different cultures, particularly underperforming on Iran and Kenya. Their general commonsense capability is also affected by cultural context, with erroneous associations favoring well-represented cultures like the United States. The language used to prompt LLMs significantly impacts their performance, with English consistently yielding the best results and multilingual prompts often decreasing accuracy. These findings highlight the inherent bias in LLMs' cultural understanding and suggest strategies for improving cultural awareness, such as more diverse training data and better instruction following across languages.

## Executive Summary
This paper investigates how large language models perform on cultural commonsense tasks across different cultures and languages. The authors evaluate five LLMs (Vicuna, Falcon, LLAMA2, GPT-3.5-turbo, GPT-4) on cultural commonsense questions from five cultures (China, India, Iran, Kenya, United States) using five languages (Chinese, English, Hindi, Farsi, Swahili). The study reveals significant performance disparities across cultures, with models consistently underperforming on under-represented cultures like Iran and Kenya, while also showing that cultural context can degrade general commonsense capabilities and that language choice significantly impacts performance.

## Method Summary
The authors evaluate LLMs on cultural commonsense tasks using three datasets: GeoMLAMA (culture-dependent commonsense assertions), CANDLE (culture-specific commonsense with masked country names), and GenericsKB-Best (general commonsense statements). They employ zero-shot prompting across five languages and cultures, using four extensively used open-source models (Vicuna, Falcon, LLAMA2) and two closed-source models (GPT-3.5-turbo, GPT-4). The experiments test cultural commonsense QA, country prediction, general commonsense assertion verification with cultural context, and country association tasks, measuring accuracy across different cultural contexts and languages.

## Key Results
- LLMs show significant performance gaps in cultural commonsense tasks across different cultures, particularly underperforming on Iran and Kenya
- Cultural context degrades general commonsense verification capabilities in some models, especially the LLAMA2 family
- Language choice significantly impacts performance, with English consistently yielding the best results and multilingual prompts often decreasing accuracy
- All models exhibit bias toward well-represented cultures (US) when associating statements with countries
- Falcon-40B frequently refuses to answer questions, highlighting alignment challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs perform worse on cultural commonsense for under-represented cultures due to training data bias.
- Mechanism: Pretraining corpora contain disproportionately more data from certain cultures (e.g., US), so the model learns cultural patterns from those cultures more effectively. When tested on cultures with less data, performance drops.
- Core assumption: Cultural knowledge is learned from the statistical patterns in pretraining data.
- Evidence anchors:
  - [abstract] "LLMs have a significant discrepancy in performance when tested on culture-specific commonsense knowledge for different cultures"
  - [section] "all the models underperform on questions about Iran and Kenya... the models are less familiar with cultural commonsense in countries that are less represented in the pretraining corpus"
  - [corpus] Weak - no direct evidence about pretraining corpus composition, but supported by performance disparity pattern
- Break condition: If pretraining corpus were perfectly balanced across cultures, performance should equalize across cultures.

### Mechanism 2
- Claim: Cultural context degrades general commonsense verification when models are not well-aligned.
- Mechanism: When general commonsense statements are evaluated with cultural context (e.g., "Is this true in Iran?"), models that haven't been trained to maintain cultural neutrality may misinterpret the cultural context as relevant to the factual truth, leading to incorrect verification.
- Core assumption: Cultural context should be irrelevant to verifying general physical or social facts.
- Evidence anchors:
  - [abstract] "LLMs' general commonsense capability is affected by cultural context"
  - [section] "LLAMA2 family, where the performance drops drastically just because of the inclusion of a cultural context"
  - [corpus] Weak - no direct evidence about how cultural context is processed, but supported by performance degradation
- Break condition: If models were trained to recognize cultural context as irrelevant to general commonsense verification, performance should remain stable.

### Mechanism 3
- Claim: Language choice affects cultural commonsense performance because linguistic capability varies across languages.
- Mechanism: LLMs have better instruction-following and linguistic capability in English due to more English data in pretraining and instruction tuning. When prompted in other languages, the model's ability to correctly process the task and access relevant knowledge decreases.
- Core assumption: The model's performance depends on both its linguistic capability in the prompt language and its ability to access relevant knowledge through that language.
- Evidence anchors:
  - [abstract] "The language used to query the LLMs can impact their performance on cultural-related tasks"
  - [section] "The accuracy for the multilingual setting is generally lower than for English... Falcon and LLAMA lack the instruction-following capability in Farsi and Swahili"
  - [corpus] Weak - no direct evidence about multilingual training data composition, but supported by performance drop in non-English languages
- Break condition: If models had equal capability across all languages, performance should not vary significantly with language choice.

## Foundational Learning

- Concept: Zero-shot prompting
  - Why needed here: All experiments use zero-shot prompting to test what the model knows without fine-tuning or examples
  - Quick check question: What does "zero-shot" mean in the context of LLM evaluation?

- Concept: Cultural commonsense vs general commonsense
  - Why needed here: The paper distinguishes between knowledge that is universally true versus knowledge that is culture-specific, which is central to the experimental design
  - Quick check question: What is an example of cultural commonsense that might not be common in other cultures?

- Concept: Instruction fine-tuning
  - Why needed here: The models used have been instruction fine-tuned, which affects how they respond to prompts and follow instructions across languages
  - Quick check question: How does instruction fine-tuning affect a model's ability to follow prompts in different languages?

## Architecture Onboarding

- Component map: Prompt construction → LLM inference → Answer generation → Answer validation → Accuracy calculation

- Critical path: Prompt → LLM inference → Answer generation → Answer validation → Accuracy calculation

- Design tradeoffs:
  - Using zero-shot vs few-shot: Zero-shot tests raw knowledge, few-shot might improve performance but masks underlying knowledge gaps
  - English-only vs multilingual: English-only would miss language effects but simplify analysis; multilingual reveals important performance disparities
  - Open-source vs closed-source models: Open-source allows transparency and reproducibility but may have different alignment strategies; closed-source may have better alignment but less transparency

- Failure signatures:
  - Performance gap between cultures suggests data imbalance
  - Performance degradation with cultural context suggests alignment issues
  - Language-dependent performance suggests linguistic capability disparities
  - Refusal to answer (especially Falcon-40B) suggests overly conservative alignment

- First 3 experiments:
  1. Run cultural commonsense QA for US, China, India, Iran, Kenya with English prompts to establish baseline performance gaps
  2. Repeat cultural commonsense QA with corresponding language prompts (Chinese for China, Hindi for India, etc.) to test language effects
  3. Run general commonsense assertion verification with cultural context for each culture to test whether cultural context affects general knowledge access

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different multilingual pretraining strategies (e.g., mT5 vs. BLOOMZ) impact cultural commonsense performance across diverse languages?
- Basis in paper: [explicit] The paper compares mT0 and bloomZ-7b-mt, multilingual models, on the cultural commonsense QA task and finds they perform worse in Iran, similar to other models tested.
- Why unresolved: The paper only tests two multilingual models and doesn't analyze the impact of different pretraining strategies on cultural commonsense performance.
- What evidence would resolve it: Testing a wider range of multilingual models with different pretraining strategies (e.g., mT5, BLOOMZ, and others) on the cultural commonsense QA task and comparing their performance across diverse languages.

### Open Question 2
- Question: To what extent does the cultural diversity of the pretraining data influence the accuracy of cultural commonsense knowledge in LLMs?
- Basis in paper: [inferred] The paper observes that LLMs consistently underperform on questions about Iran and Kenya, suggesting less familiarity with these cultures in the pretraining data.
- Why unresolved: The paper doesn't directly analyze the relationship between pretraining data diversity and cultural commonsense accuracy.
- What evidence would resolve it: Analyzing the cultural diversity of the pretraining data used for different LLMs and correlating it with their performance on cultural commonsense tasks.

### Open Question 3
- Question: How do different cultural backgrounds within a country affect the performance of LLMs on cultural commonsense tasks?
- Basis in paper: [inferred] The paper focuses on five countries and their corresponding cultures but doesn't explore the diversity within each country.
- Why unresolved: The paper doesn't investigate how cultural variations within a country might impact LLM performance on cultural commonsense tasks.
- What evidence would resolve it: Conducting experiments that consider cultural diversity within countries, such as testing LLMs on cultural commonsense tasks for different regions or ethnic groups within a country.

## Limitations
- Data representativeness: The curated datasets may not fully capture the breadth and nuance of cultural commonsense across different societies
- Prompt sensitivity: Results may be sensitive to prompt phrasing and formatting, affecting performance patterns
- Model selection constraints: Evaluation focuses on a limited set of LLMs with specific parameter sizes
- Translation quality: Use of Azure translation API introduces potential translation errors affecting performance

## Confidence
- High Confidence: Mechanism 1 (training data imbalance causing performance gaps) - strongly supported by consistent performance patterns
- Medium Confidence: Mechanism 2 (cultural context degrading general commonsense) - observed but not thoroughly explained
- Medium Confidence: Mechanism 3 (language-dependent performance) - well-documented but underlying reasons unclear

## Next Checks
1. Analyze the pretraining corpus composition of the evaluated models to directly confirm whether under-represented cultures have less data
2. Systematically vary prompt templates across all cultures and languages to determine sensitivity to phrasing
3. Design an experiment where models are first prompted with cultural context from one culture, then tested on general commonsense questions to measure lasting effects