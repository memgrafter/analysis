---
ver: rpa2
title: A Surprisingly Simple yet Effective Multi-Query Rewriting Method for Conversational
  Passage Retrieval
arxiv_id: '2406.18960'
source_url: https://arxiv.org/abs/2406.18960
tags:
- query
- retrieval
- conversational
- u1d45e
- rewrites
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of conversational passage retrieval
  by generating multiple query rewrites at no additional cost using beam search. The
  core method, CMQR, produces top-K rewrites with associated probabilities and integrates
  them into sparse and dense retrieval pipelines.
---

# A Surprisingly Simple yet Effective Multi-Query Rewriting Method for Conversational Passage Retrieval

## Quick Facts
- arXiv ID: 2406.18960
- Source URL: https://arxiv.org/abs/2406.18960
- Authors: Ivica Kostric; Krisztian Balog
- Reference count: 36
- Primary result: Improves MRR by 1.06–6.31 percentage points in sparse retrieval and 3.52–4.45 in dense retrieval on QReCC dataset

## Executive Summary
This paper addresses conversational passage retrieval by generating multiple query rewrites at no additional computational cost using beam search. The method, CMQR, produces top-K rewrites with associated probabilities and integrates them into both sparse and dense retrieval pipelines. Experiments on QReCC show consistent improvements over single-query methods, achieving state-of-the-art results. The approach is remarkably simple: it leverages how beam search works to generate multiple rewrites and combines them through weighted aggregation.

## Method Summary
The method fine-tunes a T5-base model for conversational query rewriting on the QReCC dataset. During inference, beam search with width 10 generates multiple rewrites, each with an associated probability score. For sparse retrieval, these rewrites are combined by summing and normalizing term weights across all rewrites to create a weighted bag-of-words query. For dense retrieval, embeddings for each rewrite are scaled by their beam scores and averaged into a single centroid vector. This integration requires minimal modification to existing retrieval pipelines while providing consistent performance improvements.

## Key Results
- Improves MRR by 1.06–6.31 percentage points in sparse retrieval compared to single-query methods
- Improves MRR by 3.52–4.45 percentage points in dense retrieval
- Achieves state-of-the-art results on the QReCC dataset
- Maintains computational efficiency by leveraging existing beam search probabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple query rewrites can be generated at no additional computational cost by leveraging beam search probabilities.
- Mechanism: Beam search tracks the top-K most probable token sequences during generation. Instead of only returning the highest-scoring sequence, returning all tracked sequences uses the already-computed token probabilities, avoiding extra model inference.
- Core assumption: The beam search algorithm computes and stores the probability scores for all tracked sequences throughout generation.
- Evidence anchors:
  - [abstract] "The main strength of our approach lies in its simplicity: it leverages how the beam search algorithm works and can produce multiple query rewrites at no additional cost."
  - [section 3.1.3] "Due to the quadratic complexity with respect to the input size, a common practice is to limit the input to a maximum of 512 tokens... Instead of keeping track of only the highest-likelihood sequence in a greedy fashion, the algorithm tracks and considers the best /u1D458sequences at each generation step."

### Mechanism 2
- Claim: Combining multiple query rewrites improves term importance estimation and query expansion for sparse retrieval.
- Mechanism: For each term across all rewrites, sum their individual beam search scores, then normalize. This produces a weighted bag-of-words query where terms appearing in higher-scoring rewrites have more influence, effectively performing both term re-weighting and expansion.
- Core assumption: Terms that appear consistently across multiple high-scoring rewrites are more likely to be important for capturing the information need.
- Evidence anchors:
  - [abstract] "For sparse retrieval, it combines term-importance estimation with query expansion by summing and normalizing term weights across rewrites."
  - [section 3.2] "We construct a weighted bag-of-words query from all /u1D45Brewrites, where we set the weights for each term as the beam search score... For each unique term... the term weights from all rewrites are summed up and normalized."

### Mechanism 3
- Claim: Representing multiple query rewrites as a weighted centroid in dense embedding space improves retrieval performance without scaling linearly with the number of rewrites.
- Mechanism: Generate embeddings for each rewrite, scale them by their beam search scores, then sum into a single weighted average vector. This centroid better represents the user's intent by averaging multiple perspectives while keeping inference cost constant.
- Core assumption: The centroid of multiple query rewrite embeddings is a more robust representation of the underlying information need than any single rewrite embedding.
- Evidence anchors:
  - [abstract] "For dense retrieval, it creates a weighted centroid of rewrite embeddings."
  - [section 3.3] "the query representation at turn /u1D456is obtained by... scaling them according to the associated weights. Then, the scaled embeddings are summed up into a single vector."

## Foundational Learning

- Concept: Beam search decoding
  - Why needed here: Understanding how beam search tracks multiple sequences and their probabilities is essential to see why generating multiple rewrites is computationally free.
  - Quick check question: In beam search, are the probability scores for all tracked sequences available at the end of generation, or only the highest-scoring one?

- Concept: Term weighting in sparse retrieval (e.g., BM25)
  - Why needed here: The method modifies term weights by summing and normalizing across rewrites, so understanding how term weights contribute to retrieval scores is key.
  - Quick check question: In BM25, how is the term weight for a query term determined, and how would adding a rewrite weight change this?

- Concept: Dense retrieval embeddings and similarity
  - Why needed here: The method creates a weighted centroid of rewrite embeddings; understanding how embeddings represent queries and how similarity is computed is necessary to grasp why this improves retrieval.
  - Quick check question: In dense retrieval, how is the relevance score between a query embedding and a document embedding computed?

## Architecture Onboarding

- Component map:
  - Query rewriter (T5-based seq2seq model with beam search) -> Sparse retrieval pipeline (BM25 + Pyserini) -> Dense retrieval pipeline (GTR embeddings + Faiss) -> CMQR integration layer (multi-query generation and weighted combination)

- Critical path:
  1. Input conversation context → T5 rewriter with beam width /u1D458=10
  2. Output top-10 rewrites with beam scores
  3. For sparse: sum/weight terms across rewrites → BM25 query
  4. For dense: embed each rewrite, scale by beam score, average → single query embedding
  5. Retrieve passages

- Design tradeoffs:
  - Beam width vs. diversity: Higher beam width gives more rewrites but may include less relevant ones; /u1D458=10 is a practical balance.
  - Sparse vs. dense weighting: Sparse uses raw summed weights; dense uses normalized centroid. These reflect different retrieval semantics.
  - Computational cost: Minimal overhead for multi-query generation; dense retrieval still requires embedding all rewrites.

- Failure signatures:
  - Sparse: If rewrites are too similar, term weights may not change much; if too dissimilar, weights may be noisy.
  - Dense: If rewrites are semantically far apart, centroid may fall in low-density region of embedding space.
  - Both: If beam scores are poorly calibrated, weighting may misrepresent term/rewrite importance.

- First 3 experiments:
  1. Generate rewrites with beam width 1 vs. 10; measure retrieval MRR change to confirm benefit of multiple rewrites.
  2. Vary the number of rewrites (/u1D458=5,10,20) and measure impact on both sparse and dense retrieval performance.
  3. Compare weighted centroid approach to simple concatenation of rewrite embeddings in dense retrieval.

## Open Questions the Paper Calls Out
- How does the performance of CMQR scale with different beam widths beyond the tested value of 10?
- Can CMQR be effectively combined with re-ranking components in multi-stage retrieval pipelines?
- How does CMQR perform on datasets other than QReCC, such as those with different conversational characteristics?

## Limitations
- The claim of "no additional cost" assumes beam search implementations retain intermediate sequence probabilities, which may not hold for all decoding strategies
- Dense retrieval integration requires generating embeddings for all rewrites, scaling linearly with beam width
- The study focuses solely on the QReCC dataset, limiting generalizability to other conversational retrieval tasks

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Generating multiple rewrites using beam search probabilities is computationally free | High |
| Weighted term combination for sparse retrieval is effective | Medium |
| Weighted centroid approach for dense retrieval is effective | Medium |

## Next Checks
1. **Beam Search Implementation Verification**: Test the multi-query generation claim by implementing the method with different beam search libraries to confirm that intermediate sequence probabilities are indeed available at no additional computational cost.

2. **Weighting Scheme Comparison**: Conduct ablation studies comparing the proposed summed/normalized weighting approach against alternative methods (e.g., max weight, average weight, or learned weights) for both sparse and dense retrieval to isolate the contribution of the specific weighting strategy.

3. **Rewrite Diversity Analysis**: Measure semantic diversity between generated rewrites using embedding-based metrics (e.g., average pairwise cosine similarity) to empirically validate whether the rewrites provide complementary information versus redundant coverage.