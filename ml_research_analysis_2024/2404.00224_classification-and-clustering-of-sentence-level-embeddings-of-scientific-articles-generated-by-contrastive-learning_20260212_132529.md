---
ver: rpa2
title: Classification and Clustering of Sentence-Level Embeddings of Scientific Articles
  Generated by Contrastive Learning
arxiv_id: '2404.00224'
source_url: https://arxiv.org/abs/2404.00224
tags:
- sentence
- embeddings
- articles
- classification
- clustering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors fine-tuned two sentence transformer models (SciBERT
  and MiniLM) on three scientific text datasets using contrastive learning with Triplet
  Loss. They then evaluated the resulting embeddings for clustering and classification
  tasks.
---

# Classification and Clustering of Sentence-Level Embeddings of Scientific Articles Generated by Contrastive Learning

## Quick Facts
- arXiv ID: 2404.00224
- Source URL: https://arxiv.org/abs/2404.00224
- Reference count: 23
- Key outcome: Contrastive learning fine-tuning improves scientific sentence embeddings for clustering and classification tasks

## Executive Summary
This paper investigates the effectiveness of contrastive learning fine-tuning for generating sentence embeddings from scientific articles. The authors fine-tune two sentence transformer models (SciBERT and MiniLM) using Triplet Loss on three scientific text datasets. They evaluate the resulting embeddings for clustering and classification tasks, comparing them with baseline embeddings from non-fine-tuned models. The study demonstrates that contrastive learning fine-tuning significantly improves clustering performance (ARI and AMI values over 5 times higher than baseline models) and classification accuracy (F1-micro scores improved by up to 30.73% on average).

## Method Summary
The authors fine-tuned SciBERT and MiniLM sentence transformer models on three scientific text datasets using contrastive learning with Triplet Loss. They extracted sentence-level embeddings using mean pooling and evaluated them for clustering tasks using k-means, DBSCAN, and HDBSCAN with metrics like ARI, AMI, and Silhouette Score. For classification, they used the embeddings as features for various classifiers (KNN, SVM, Random Forest, etc.) and compared F1-micro scores with baseline embeddings and models fine-tuned directly for classification.

## Key Results
- Clustering performance (ARI and AMI) improved by over 5 times compared to baseline models
- F1-micro classification scores improved by up to 30.73% on average using fine-tuned embeddings
- Fine-tuned embeddings outperformed models fine-tuned directly for classification in most cases
- SciBERT generally performed better than MiniLM due to larger input size and scientific domain vocabulary

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning with Triplet Loss improves clustering by rearranging sentence embeddings so that same-label sentences cluster tightly while different-label sentences are pushed apart.
- Mechanism: Triplet Loss minimizes the distance between anchor and positive samples while maximizing the distance between anchor and negative samples, preserving intra-label variance and adjusting for outliers.
- Core assumption: Sentences from the same section label are more thematically related than those from different sections.
- Evidence anchors:
  - [abstract] "contrastive learning fine-tuning is effective for generating sentence embeddings useful in downstream scientific text tasks"
  - [section] "Triplet Loss minimizes the distance between anchors and positives while maximizing the distance between negatives, thus preserving intra-label variance and better adjusting in the presence of outliers."
  - [corpus] No direct corpus evidence of triplet sampling or margin behavior; assumption inferred from paper methodology.
- Break condition: If the thematic similarity assumption fails (e.g., mixed-topic sentences within a section), the Triplet Loss would incorrectly group dissimilar sentences, degrading clustering quality.

### Mechanism 2
- Claim: Fine-tuning sentence transformer models on domain-specific scientific corpora yields embeddings that outperform general-domain embeddings for scientific text tasks.
- Mechanism: Pre-training on general text followed by fine-tuning on scientific datasets aligns the embedding space with scientific vocabulary and structure, improving task performance.
- Core assumption: Scientific vocabulary and text structure are sufficiently distinct from general text to benefit from domain adaptation.
- Evidence anchors:
  - [abstract] "SciBERT has a vocabulary from the scientific domain; SciBERT supports an input size that is the double of MiniLM (512/256) and outputs an embedding twice the size (768/384)."
  - [section] "SciBERT, from [6], was chosen because it is a BERT-based language model for the scientific domain. It was trained on the full text of 1.14 million articles from the Semantic Scholar database."
  - [corpus] No direct corpus metrics comparing scientific vs. general embeddings; evidence is from model descriptions.
- Break condition: If scientific texts are too heterogeneous or if the domain gap is small, fine-tuning may not yield significant improvements over baseline models.

### Mechanism 3
- Claim: Using fine-tuned sentence embeddings as input features for traditional classifiers outperforms classifiers trained on raw text or embeddings from non-fine-tuned models.
- Mechanism: High-quality embeddings capture relevant semantic features, reducing the burden on classifiers to extract features and allowing them to focus on decision boundaries.
- Core assumption: The classifiers (KNN, SVM, Random Forest, etc.) can effectively leverage the semantic information encoded in the embeddings.
- Evidence anchors:
  - [abstract] "fine-tuned embeddings improved F1-micro scores by up to 30.73% on average compared to baseline embeddings"
  - [section] "For classification, when comparing results from the same classifiers, the fine-tuned embeddings outperform all but one baseline model, which is SVM for CSAbstruct."
  - [corpus] No corpus evidence of embedding-feature correlation; results are from experimental comparison.
- Break condition: If embeddings are noisy or do not capture relevant features, classifier performance could degrade compared to using raw text features.

## Foundational Learning

- Concept: Triplet Loss and contrastive learning objectives
  - Why needed here: The core innovation is using Triplet Loss to fine-tune sentence transformers, so understanding how it works is essential to replicate or extend the approach.
  - Quick check question: In Triplet Loss, what is the relationship between anchor, positive, and negative samples, and how does the margin parameter influence training?

- Concept: Sentence transformer models and pooling methods
  - Why needed here: The paper uses SciBERT and MiniLM models with mean pooling to generate sentence embeddings; knowing how these work is key to reproducing results.
  - Quick check question: What is the difference between CLS pooling and mean pooling in sentence transformers, and when might each be preferred?

- Concept: Clustering metrics (ARI, AMI, Silhouette Score)
  - Why needed here: The paper evaluates clustering performance using these metrics; understanding them is necessary to interpret results and compare methods.
  - Quick check question: How do Adjusted Rand Index (ARI) and Adjusted Mutual Information (AMI) differ in evaluating clustering, and what does a high Silhouette Score indicate?

## Architecture Onboarding

- Component map: Data -> Preprocessing (XML extraction, sentence splitting, labeling) -> Model Fine-tuning (SciBERT/MiniLM + Triplet Loss) -> Embedding Generation -> Downstream Tasks (Clustering/Classification)
- Critical path: Dataset creation -> Fine-tuning with Triplet Loss -> Embedding evaluation in clustering -> Embedding evaluation in classification
- Design tradeoffs: Larger batch sizes increase computational cost but may improve Triplet Loss stability; domain-specific models (SciBERT) have higher resource needs but better performance on scientific text
- Failure signatures: Poor clustering metrics despite fine-tuning may indicate label noise or insufficient domain adaptation; classifier underperformance may signal embedding quality issues or inappropriate classifier choice
- First 3 experiments:
  1. Replicate clustering evaluation with a small subset of PMC-Sents-FULL using k-means and baseline SciBERT embeddings
  2. Fine-tune MiniLM on PubMed-RCT 20k and compare F1-micro with baseline embeddings in a KNN classifier
  3. Visualize embeddings using t-SNE for both fine-tuned and baseline models to qualitatively assess clustering improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the clustering performance change if larger batch sizes were used during fine-tuning?
- Basis in paper: [explicit] The authors state "it is important to assess the results with larger batch sizes since this creates more variability in the loss function and impacts the training process."
- Why unresolved: The current study used batch sizes of 16-32, and the authors suggest larger batches could affect the loss function dynamics.
- What evidence would resolve it: Experiments comparing clustering metrics (ARI, AMI, Silhouette) across different batch sizes during fine-tuning.

### Open Question 2
- Question: What is the impact of using full-text articles instead of abstracts on sentence classification accuracy across different scientific domains?
- Basis in paper: [inferred] The authors introduced PMC-Sents-FULL with full-text articles and noted it has more balanced label distribution, suggesting potential domain-specific differences.
- Why unresolved: The study only compared results within medical domain for full-text versus abstracts, without cross-domain analysis.
- What evidence would resolve it: Comparative experiments using full-text versus abstracts across multiple scientific domains with classification performance metrics.

### Open Question 3
- Question: How do different contrastive loss functions compare to Triplet Loss for generating scientific article sentence embeddings?
- Basis in paper: [explicit] The authors mention "explore other contrastive loss functions" as future work, indicating uncertainty about whether Triplet Loss is optimal.
- Why unresolved: The study only used BatchAllTripletLoss and did not benchmark against other contrastive learning objectives.
- What evidence would resolve it: Direct comparison of clustering and classification performance using various contrastive loss functions (e.g., InfoNCE, Supervised Contrastive Loss) with identical model architectures.

## Limitations

- The study's conclusions are primarily based on three datasets, which may not represent the full diversity of scientific text domains.
- Clustering evaluation relies on section labels that may not perfectly capture semantic similarity, potentially inflating the apparent effectiveness of fine-tuned embeddings.
- The computational resources required for fine-tuning large models like SciBERT may limit practical applicability in resource-constrained settings.

## Confidence

- **High Confidence**: The improvement in F1-micro scores for classification tasks using fine-tuned embeddings (up to 30.73% improvement) is well-supported by experimental results across multiple datasets and classifiers.
- **Medium Confidence**: The clustering performance gains (ARI and AMI values over 5 times higher than baselines) are convincing, though the reliance on section labels as ground truth introduces some uncertainty about semantic relevance.
- **Medium Confidence**: The claim that domain-specific fine-tuning improves performance is supported by model descriptions but lacks direct corpus-level evidence comparing scientific vs. general embeddings.

## Next Checks

1. **Generalization Test**: Evaluate the fine-tuned embeddings on an additional scientific domain (e.g., biomedical research articles) not represented in the current datasets to assess domain transfer capability.
2. **Alternative Contrastive Methods**: Compare Triplet Loss fine-tuning with other contrastive learning approaches (e.g., SimCSE) using the same experimental setup to determine if the improvement is method-specific.
3. **Ablation Study**: Conduct experiments removing the fine-tuning step while keeping all other conditions constant to quantify the exact contribution of contrastive learning to performance gains.