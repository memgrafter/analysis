---
ver: rpa2
title: Vision-Language Consistency Guided Multi-modal Prompt Learning for Blind AI
  Generated Image Quality Assessment
arxiv_id: '2406.16641'
source_url: https://arxiv.org/abs/2406.16641
tags:
- quality
- prompt
- image
- prompts
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses blind AI generated image quality assessment
  (AGIQA) by proposing vision-language consistency guided multi-modal prompt learning.
  The authors introduce learnable textual and visual prompts in both language and
  vision branches of CLIP models and design a text-to-image alignment quality prediction
  task to guide the optimization of these prompts.
---

# Vision-Language Consistency Guided Multi-modal Prompt Learning for Blind AI Generated Image Quality Assessment

## Quick Facts
- arXiv ID: 2406.16641
- Source URL: https://arxiv.org/abs/2406.16641
- Authors: Jun Fu; Wei Zhou; Qiuping Jiang; Hantao Liu; Guangtao Zhai
- Reference count: 35
- Primary result: CLIP-AGIQA achieves state-of-the-art performance on AGIQA datasets with SRCC of 0.8747, PLCC of 0.9190, and KRCC of 0.6976 on AGIQA-3K

## Executive Summary
This paper addresses the challenge of blind AI-generated image quality assessment (AGIQA) by proposing a vision-language consistency guided multi-modal prompt learning approach. The method introduces learnable textual and visual prompts in both language and vision branches of CLIP models, which are optimized through a text-to-image alignment quality prediction task. The proposed CLIP-AGIQA method demonstrates superior performance compared to state-of-the-art quality assessment models on two public AGIQA datasets.

## Method Summary
CLIP-AGIQA employs multi-modal prompt learning to adapt CLIP models for AI-generated image quality assessment. The method introduces learnable textual and visual prompts in both the language and vision branches of CLIP models. A text-to-image alignment quality prediction task is designed to guide the optimization of these prompts by leveraging vision-language consistency knowledge. The approach uses a blind setting with prompt pairing strategies to estimate alignment and perceptual quality without requiring user input text prompts.

## Key Results
- CLIP-AGIQA achieves SRCC of 0.8747, PLCC of 0.9190, and KRCC of 0.6976 on AGIQA-3K dataset
- On AIGCIQA-2023 dataset, the method achieves SRCC of 0.8324, PLCC of 0.8604, and KRCC of 0.6220
- Outperforms state-of-the-art quality assessment models including DIQA, IQA-Optimal, and CLIPIQA
- Demonstrates effectiveness of multi-modal prompt learning for AGIQA task

## Why This Works (Mechanism)

### Mechanism 1
The multi-modal prompt learning approach effectively adapts CLIP models to the domain of AI-generated images by tuning both textual and visual prompts. The method introduces learnable textual and visual prompts in both the language and vision branches of CLIP models. These prompts are optimized through a text-to-image alignment quality prediction task that leverages vision-language consistency knowledge. This dual-prompt tuning addresses the domain gap between natural images and AI-generated images (AGIs).

### Mechanism 2
The text-to-image alignment quality prediction task provides informative vision-language consistency knowledge that guides the optimization of multi-modal prompts. The method uses a text-to-image alignment quality prediction task where learnable prompts are optimized to estimate the alignment score between AGIs and user input text prompts. This learned consistency knowledge is then used to condition the visual prompts in the perceptual quality prediction task, enhancing the quality assessment capability.

### Mechanism 3
The prompt pairing strategy enables blind quality assessment by estimating alignment and perceptual quality without requiring user input text prompts. The method employs a prompt pairing strategy using antonym prompts (e.g., "Aligned photo" vs. "Misaligned photo") to estimate alignment scores, and similar antonym prompts (e.g., "Good photo" vs. "Bad photo") to estimate perceptual quality. This allows the model to perform blind quality assessment even when user input text prompts are unavailable.

## Foundational Learning

- Concept: Vision-language models (VLMs) like CLIP
  - Why needed here: CLIP models are the foundation of the proposed method, providing the pre-trained encoders for images and text that are fine-tuned using multi-modal prompt learning.
  - Quick check question: What are the two main components of CLIP models, and how do they process input data?

- Concept: Prompt learning in VLMs
  - Why needed here: Prompt learning is the key technique used to adapt CLIP models to the specific task of AI-generated image quality assessment, allowing the model to learn task-specific representations.
  - Quick check question: How does prompt learning differ from traditional fine-tuning, and what are the advantages of using prompt learning in this context?

- Concept: Vision-language consistency
  - Why needed here: Vision-language consistency is the underlying principle that guides the optimization of multi-modal prompts, leveraging the alignment between AGIs and user input text prompts to improve quality assessment.
  - Quick check question: Why is vision-language consistency informative for AI-generated image quality assessment, and how does the proposed method leverage this consistency?

## Architecture Onboarding

- Component map: Vision encoder -> Visual prompts -> Image representation; Text encoder -> Textual prompts -> Text representation; Image and text representations -> Cosine similarity -> Alignment/perceptual quality score

- Critical path: Input AGI → Vision encoder → Visual prompts → Image representation; Input text prompts → Text encoder → Textual prompts → Text representation; Image and text representations → Cosine similarity → Alignment/perceptual quality score

- Design tradeoffs: Multi-modal prompt learning vs. uni-modal prompt learning: Multi-modal learning addresses the domain gap between natural images and AGIs but increases computational complexity; Blind setting vs. non-blind setting: The blind setting allows for more general applicability but may sacrifice some accuracy compared to using user input text prompts

- Failure signatures: Poor performance on AGIQA datasets: Indicates issues with the multi-modal prompt learning approach or the vision-language consistency knowledge; Overfitting to training data: May occur if the model is not regularized properly or if the training data is not diverse enough; Slow convergence during training: Could be due to the complexity of the multi-modal prompt learning or the optimization of the vision-language consistency knowledge

- First 3 experiments: 1) Ablation study on the effectiveness of textual and visual prompt learning separately; 2) Evaluation of the impact of the text-to-image alignment quality prediction task on the overall performance; 3) Comparison with baseline methods (e.g., CLIPIQA) to validate the improvements achieved by the proposed method

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed vision-language consistency guided multi-modal prompt learning approach perform on other image quality assessment tasks beyond AI generated images? The paper demonstrates the effectiveness of the approach on two AGIQA datasets, but does not explore its performance on other image quality assessment tasks.

### Open Question 2
What is the impact of different text-to-image alignment quality prediction tasks on the performance of the proposed method? The paper uses a specific text-to-image alignment quality prediction task, but does not explore the impact of different tasks on the performance of the proposed method.

### Open Question 3
How does the proposed method handle AGIs with complex or ambiguous user input text prompts? The paper does not discuss the handling of AGIs with complex or ambiguous user input text prompts.

## Limitations
- The method relies heavily on the CLIP model's pre-trained representations, which may not fully capture the unique characteristics of AI-generated images
- The blind setting using prompt pairing with antonym prompts may not generalize well to all types of AI-generated content
- The computational overhead of optimizing both textual and visual prompts could limit real-time applications

## Confidence
- **High Confidence**: The quantitative results showing state-of-the-art performance on both AGIQA-3K and AIGCIQA-2023 datasets
- **Medium Confidence**: The effectiveness of the text-to-image alignment quality prediction task in guiding prompt optimization
- **Medium Confidence**: The blind setting's ability to match non-blind approaches without user input prompts

## Next Checks
1. Conduct ablation studies removing the visual prompt tuning to quantify its specific contribution beyond textual prompt tuning alone
2. Test the method on out-of-distribution AI-generated images from different generators (beyond those in the training datasets) to assess generalization
3. Compare the computational efficiency and inference time against existing CLIP-based quality assessment methods to evaluate practical deployment viability