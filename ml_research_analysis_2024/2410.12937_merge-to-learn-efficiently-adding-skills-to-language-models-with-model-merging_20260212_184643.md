---
ver: rpa2
title: 'Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging'
arxiv_id: '2410.12937'
source_url: https://arxiv.org/abs/2410.12937
tags:
- general
- skills
- performance
- best
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how to efficiently add new skills to pretrained
  language models by training on new skills in isolation and merging with the original
  model. The authors compare three approaches: continued finetuning, retraining from
  scratch, and parallel training followed by merging (PTM).'
---

# Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging

## Quick Facts
- arXiv ID: 2410.12937
- Source URL: https://arxiv.org/abs/2410.12937
- Authors: Jacob Morrison; Noah A. Smith; Hannaneh Hajishirzi; Pang Wei Koh; Jesse Dodge; Pradeep Dasigi
- Reference count: 11
- One-line primary result: Parallel training and merging (PTM) efficiently adds new skills to language models while preserving more general capabilities than continued finetuning

## Executive Summary
This paper investigates three methods for adding new skills to pretrained language models: continued finetuning (CFT), retraining from scratch (RT), and parallel training followed by merging (PTM). The authors find that PTM achieves comparable or better performance on new skills while preserving more of the original model's general capabilities compared to other methods. PTM is also significantly more computationally efficient than retraining and better at enabling safety-related refusals while avoiding exaggerated refusals. The paper provides practical heuristics for selecting merging coefficients when held-out data is not available, enabling efficient addition of multiple skills into a single model.

## Method Summary
The paper compares three approaches for adding new skills to pretrained language models. Continued finetuning (CFT) involves training the base model on skill-specific data. Retraining from scratch (RT) creates a new model trained on a mix of general and skill-specific data. Parallel training and merging (PTM) trains separate models on general and skill-specific data, then combines them using task arithmetic - computing the difference between specialized and base models (task vector) and adding it to the general model with a mixture weight. The authors evaluate these methods on science, coding, and safety skills using the Tülu model family.

## Key Results
- PTM achieves comparable or better performance on new skills while preserving more general capabilities than CFT
- PTM is significantly more computationally efficient than retraining from scratch (O(|D|) vs O(|G| + |D|))
- PTM enables better safety feature integration while avoiding exaggerated refusals compared to CFT
- The heuristic ω = |D|/|G| provides good mixture weight selection without requiring held-out validation data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task vectors preserve general model capabilities because they only encode differences in specialized skills without altering the core general representations.
- Mechanism: When training a model on a specialized task and subtracting the base model weights, the resulting task vector captures only the modifications needed for the new skill. Adding this vector to the general model applies the new skill while leaving the original general representations intact.
- Core assumption: The differences between the specialized model and the base model primarily represent the new skill, not alterations to general capabilities.
- Evidence anchors:
  - [abstract] "parallel training is especially well-suited for enabling safety features in LMs relative to continued finetuning and retraining, as it dramatically improves model compliance with safe prompts while preserving its ability to refuse dangerous or harmful prompts"
  - [section 2.3] "PTM does not directly change the weights associated with previously learned tasks, it should allow the model to retain more of its original skills compared to other methods"
- Break condition: If the specialized training substantially alters general representations during the specialized training phase, the task vector would corrupt rather than preserve general capabilities.

### Mechanism 2
- Claim: Task vectors enable efficient skill addition because they require training only on the new skill data rather than the entire general dataset.
- Mechanism: Instead of retraining on both general and new data (which requires many training steps), we train only on the new skill data to create a task vector, then merge it with the existing general model. This reduces computational requirements from O(|G| + |D|) per trial to O(|D|).
- Core assumption: The general model already contains the necessary general capabilities, so only the new skill needs to be learned separately.
- Evidence anchors:
  - [section 2.3] "the total training cost for PMT is |D|, dramatically lower than other methods"
  - [section 2.2] "retraining should lead to competitive performance on both general and skill-specific evaluations, it is inefficient compared to other methods due to the need to retrain on the entire general mix for every training run"
- Break condition: If the general model lacks critical capabilities that are needed for the new skill, the task vector approach would fail to add the skill effectively.

### Mechanism 3
- Claim: The mixture weight heuristic (ω = |D|/|G|) provides good performance without requiring held-out validation data.
- Mechanism: This heuristic proportionally balances the influence of the new skill based on its relative size compared to the general dataset, naturally scaling the impact of the task vector to avoid overwhelming the general model.
- Core assumption: The relative sizes of the datasets correlate with the appropriate balance of influence between general and specialized capabilities.
- Evidence anchors:
  - [section 4.2] "we find that a consistently strong heuristic for selecting a model checkpoint is to 1) train the task vector on all of the available skill-specific data, and then 2) weight the vector with ω = |D|/|G|"
  - [section 4.2] "It consistently selects a point on the curve that preserves most, if not all of the original model's general performance, while substantially improving skill-specific performance"
- Break condition: If the specialized dataset contains significantly harder or more complex examples than the general dataset, the simple size-based heuristic may over/underweight the specialized component.

## Foundational Learning

- Concept: Weight space interpolation and vector arithmetic
  - Why needed here: The entire approach relies on creating and combining vectors in model weight space to transfer skills between models
  - Quick check question: If you have two models θA and θB, what operation creates a vector that represents the difference between them?

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why continued finetuning degrades general performance is crucial to appreciating why the parallel-then-merge approach is beneficial
  - Quick check question: What happens to the weights of a neural network during standard finetuning when the new data distribution differs significantly from the original training distribution?

- Concept: Task arithmetic and model editing
  - Why needed here: The task vector approach is a specific instance of model editing techniques that manipulate weight space to add/remove capabilities
  - Quick check question: How does subtracting a base model from a specialized model create a "task vector" that can be applied to other models?

## Architecture Onboarding

- Component map:
  - Base model (θpre) -> Pretrained foundation model
  - General model (θG) -> Instruction-tuned generalist
  - Specialized models (θD) -> Models trained on specific skills
  - Task vectors (τD) -> Weight differences representing specialized skills
  - Merging function -> Linear combination of general model and task vectors

- Critical path:
  1. Train specialized model on new skill data from pretrained base
  2. Compute task vector by subtracting base model from specialized model
  3. Merge task vector with general model using appropriate weight
  4. Evaluate performance on general and specialized tasks

- Design tradeoffs:
  - Computational efficiency vs. performance: Task vectors are much cheaper than retraining but may achieve slightly lower performance on specialized tasks
  - Generalization preservation vs. specialization: Higher mixture weights improve specialized performance but risk degrading general capabilities
  - Dataset availability vs. method choice: Task vectors require access to the base model, while WiSE-FT can work with only the instruction-tuned model

- Failure signatures:
  - General performance drops significantly: Mixture weight too high or specialized training corrupted general representations
  - Specialized performance doesn't improve: Task vector too small or specialized training ineffective
  - Interference between skills: When merging multiple specialized vectors, certain combinations may conflict

- First 3 experiments:
  1. Train a specialized model on a small subset of the new skill data, create a task vector, and merge with the general model using ω = 0.1, 0.5, and 0.9 to observe the tradeoff curve
  2. Test the heuristic mixture weight (ω = |D|/|G|) on a new skill and verify it preserves general performance while improving specialized performance
  3. Merge two specialized vectors (e.g., safety and coding) and evaluate whether interference occurs and how it affects both specialized skills

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much general data is needed during continued finetuning to preserve general performance when adding new skills?
- Basis in paper: [inferred] The paper shows that PTM requires significantly less training data than RT while preserving general performance, and mentions in Section 4.4 that adding general data to the skill-specific mix during CFT improves performance.
- Why unresolved: The paper doesn't quantify the minimum amount of general data needed or test different ratios of general to skill-specific data during CFT.
- What evidence would resolve it: Experiments varying the proportion of general data in CFT while measuring both skill-specific and general performance across multiple domains.

### Open Question 2
- Question: Can general data from a different distribution than the original training mix preserve general performance when adding new skills?
- Basis in paper: [explicit] Section 4.4 states "When the base mix is not publicly available, is it possible to use data from a different general distribution to preserve general performance?"
- Why unresolved: The paper only tests using the original general data mix and doesn't explore whether other general datasets could serve as substitutes.
- What evidence would resolve it: Experiments comparing PTM and CFT performance using various general datasets (different domains, styles, etc.) while maintaining skill-specific performance.

### Open Question 3
- Question: What causes the interference between coding and safety skills that degrades science performance in multi-skill PTM models?
- Basis in paper: [explicit] Table 5 shows that the three-skill PTM model has much worse science performance than single-skill models, and the authors hypothesize interference between coding and safety vectors.
- Why unresolved: The paper only shows correlation and doesn't investigate the underlying mechanisms of why these specific skills interfere.
- What evidence would resolve it: Detailed analysis of parameter changes, attention patterns, or embedding spaces when merging different skill combinations to identify the source of interference.

## Limitations

- The approach requires access to the base pretrained model to compute task vectors, which may not always be available
- Performance on specialized tasks may be slightly lower than direct training on mixed data (retraining approach)
- The heuristic mixture weight selection may not work well when specialized datasets have very different complexity distributions than the general dataset

## Confidence

**High Confidence Claims:**
- PTM achieves comparable or better performance on new skills while preserving more general capabilities than continued finetuning
- PTM is significantly more computationally efficient than retraining from scratch
- PTM enables better safety feature integration while avoiding exaggerated refusals compared to continued finetuning

**Medium Confidence Claims:**
- The ω = |D|/|G| heuristic provides good mixture weight selection without held-out data
- Task vectors can be successfully combined to create multi-skilled models without additional training
- The safety benefits of PTM over CFT are consistent across different safety evaluation sets

## Next Checks

1. **Cross-architecture validation**: Test the PTM approach on a different model family (e.g., Llama, Mistral) to verify the method generalizes beyond Tülu models.

2. **Ablation on dataset composition**: Systematically vary the composition of the general training mix to determine how sensitive the PTM approach is to the quality and diversity of the base general model.

3. **Long-term retention study**: After creating merged models, continue training them on additional unrelated tasks to assess whether the merged skills remain stable or degrade over time compared to directly trained specialized models.