---
ver: rpa2
title: 'Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the
  Edge of Generalization'
arxiv_id: '2405.15071'
source_url: https://arxiv.org/abs/2405.15071
tags:
- layer
- grokking
- facts
- training
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether transformers can learn to reason
  over parametric knowledge, a skill that current large language models struggle with.
  The authors focus on two reasoning types - composition and comparison - and find
  that transformers can learn implicit reasoning through grokking, i.e., extended
  training beyond overfitting.
---

# Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization

## Quick Facts
- **arXiv ID**: 2405.15071
- **Source URL**: https://arxiv.org/abs/2405.15071
- **Reference count**: 40
- **One-line primary result**: Transformers can learn implicit reasoning through grokking, but systematic generalization varies across reasoning types.

## Executive Summary
This paper investigates whether transformers can learn to reason over parametric knowledge through grokking, an extended training phenomenon beyond overfitting. The authors focus on composition and comparison reasoning tasks, finding that transformers acquire implicit reasoning capabilities but with varying levels of systematic generalization. Through mechanistic analysis, they identify distinct memorizing and generalizing circuits, revealing why composition tasks fail to generalize systematically while comparison tasks succeed. The study demonstrates that parametric memory enables superior performance on complex reasoning tasks compared to non-parametric memory approaches used in state-of-the-art LLMs.

## Method Summary
The authors use synthetic datasets with atomic and inferred facts generated from latent rules. They train a standard decoder-only transformer (8 layers, 768 hidden dimensions, 12 attention heads) using AdamW optimization with extended training beyond overfitting. The model's performance is evaluated on in-distribution and out-of-distribution test sets, with mechanistic analysis conducted through techniques like logit lens and causal tracing. The study systematically varies the ratio of inferred to atomic facts to understand grokking dynamics and tests architectural modifications to improve cross-layer knowledge sharing.

## Key Results
- Transformers acquire implicit reasoning skills only through grokking, transitioning from memorizing to generalizing circuits
- Systematic generalization succeeds for comparison tasks but fails for composition tasks due to circuit configuration differences
- Parametric memory enables near-perfect accuracy on complex reasoning tasks, while state-of-the-art LLMs with non-parametric memory fail
- The speed of grokking correlates with the ratio of inferred to atomic facts in the training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers acquire implicit reasoning skills through grokking, transitioning from memorizing to generalizing circuits
- Mechanism: During grokking, the model shifts from a memorizing circuit (Cmem) that stores all facts to a generalizing circuit (Cgen) that stores fewer facts while maintaining perfect training performance
- Core assumption: The relative efficiency between circuits determines which circuit the model adopts during optimization
- Evidence anchors:
  - [abstract]: "transformers can learn implicit reasoning, but only through grokking, i.e., extended training far beyond overfitting"
  - [section 3.3]: "there exist both a memorizing circuit Cmem and a generalizing circuit Cgen that can fit the training data"

### Mechanism 2
- Claim: Systematic generalization varies across reasoning types due to circuit configuration differences
- Mechanism: Composition tasks store atomic facts in different layers (lower for first hop, upper for second hop), preventing OOD generalization. Comparison tasks store facts in parallel in lower layers, enabling systematic generalization
- Core assumption: The transformer's non-recurrent design prevents cross-layer memory sharing, affecting systematicity
- Evidence anchors:
  - [abstract]: "The levels of generalization also vary across reasoning types: when faced with out-of-distribution examples, transformers fail to systematically generalize for composition but succeed for comparison"
  - [section 3.3]: "the model is gradually forming the second hop in the upper layers during grokking"

### Mechanism 3
- Claim: Parametric memory enables deep compression and integration for complex reasoning tasks
- Mechanism: Fully grokked transformers compress and integrate information into their weights, achieving near-perfect accuracy on complex tasks that non-parametric memory systems fail
- Core assumption: Parametric memory allows more efficient storage and retrieval of complex reasoning patterns than explicit retrieval
- Evidence anchors:
  - [abstract]: "a fully grokked transformer can achieve near-perfect accuracy, while state-of-the-art LLMs like GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly"
  - [section 5]: "we find that it acquires the same generalizing circuit as in Figure 5(a), and remarkably, even though not explicitly encouraged/trained to do this, the model successfully infers most of the OOD entities' attribute values"

## Foundational Learning

- Concept: Grokking phenomenon
  - Why needed here: Understanding why transformers only acquire reasoning skills after extended training beyond overfitting
  - Quick check question: What distinguishes the memorizing circuit from the generalizing circuit during grokking?

- Concept: Systematic generalization
  - Why needed here: Explaining why transformers succeed on some reasoning types but fail on others
  - Quick check question: How does the circuit configuration affect the model's ability to handle out-of-distribution examples?

- Concept: Parametric vs non-parametric memory
  - Why needed here: Understanding the advantages and limitations of different memory approaches for reasoning tasks
  - Quick check question: What types of reasoning tasks benefit most from parametric memory compression?

## Architecture Onboarding

- Component map:
  Standard decoder-only transformer with 8 layers → attention heads → normalization → output embedding matrix

- Critical path:
  Input tokens → embedding layer → attention layers → normalization → output embedding → prediction
  During grokking: transition from memorizing to generalizing circuit

- Design tradeoffs:
  Non-recurrent design prevents cross-layer memory sharing, limiting systematic generalization
  Weight decay regularization encourages more efficient circuits
  Larger models converge faster but don't qualitatively change generalization behavior

- Failure signatures:
  Perfect training performance with zero OOD generalization
  Rapid overfitting without subsequent improvement
  Inability to retrieve facts stored in different layers

- First 3 experiments:
  1. Train with varying inferred/atomic ratios to observe grokking speed correlation
  2. Test OOD generalization on composition vs comparison tasks
  3. Implement parameter sharing between layers to improve cross-layer memory sharing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different transformer architectures (e.g., Universal Transformer, memory-augmented transformers) affect the systematic generalization capabilities for composition tasks?
- Basis in paper: [explicit] The paper suggests that proper cross-layer memory-sharing mechanisms are needed to improve generalization, and shows that a variant of Universal Transformer's parameter-sharing scheme can improve OOD generalization in composition.
- Why unresolved: While the paper demonstrates that parameter sharing can improve OOD generalization, it does not explore other architectural modifications or provide a comprehensive comparison of different memory-sharing mechanisms.
- What evidence would resolve it: Systematic experiments comparing various transformer architectures with different memory-sharing mechanisms on composition and comparison tasks, measuring both ID and OOD generalization performance.

### Open Question 2
- Question: What is the precise relationship between the grokking phenomenon and the relative efficiency of generalizing vs. memorizing circuits?
- Basis in paper: [explicit] The paper proposes that grokking occurs due to a transition from a memorizing circuit to a more efficient generalizing circuit, with the speed of this transition depending on the inferred/atomic ratio.
- Why unresolved: The paper provides a theoretical explanation based on circuit efficiency, but does not provide empirical evidence quantifying the exact relationship between circuit complexity and grokking speed across different tasks and model sizes.
- What evidence would resolve it: Detailed analysis of circuit complexity metrics (e.g., number of stored facts, circuit depth) during training, correlated with grokking speed across multiple tasks and model configurations.

### Open Question 3
- Question: How does the size of the search space in reasoning tasks affect the ability of parametric memory to solve complex problems compared to non-parametric memory approaches?
- Basis in paper: [inferred] The paper demonstrates that a grokked transformer can solve a complex reasoning task with a large search space that state-of-the-art LLMs with non-parametric memory fail at, suggesting parametric memory has unique advantages for deep compression and integration.
- Why unresolved: The paper only shows one example of a complex reasoning task. It does not explore how different search space sizes or task complexities affect the relative performance of parametric vs. non-parametric memory approaches.
- What evidence would resolve it: Systematic experiments varying the size and complexity of search spaces in reasoning tasks, comparing the performance of parametric memory (grokked transformers) and non-parametric memory (LLMs with retrieval augmentation) across multiple difficulty levels.

## Limitations
- Synthetic data dependency raises questions about external validity to real-world reasoning tasks
- Mechanistic interpretation of circuits remains somewhat subjective and could have alternative explanations
- Extended grokking time requirements (thousands of epochs) present significant computational challenges

## Confidence
- **High Confidence**: Transformers can learn implicit reasoning through grokking (well-supported by experimental evidence)
- **Medium Confidence**: Systematic generalization differences between composition and comparison are real but mechanistic explanation relies on circuit interpretation
- **Medium Confidence**: Architectural suggestions for cross-layer knowledge sharing are promising but lack comprehensive experimental validation

## Next Checks
1. Apply methodology to real-world reasoning datasets (e.g., multi-hop question answering) to test external validity
2. Implement transformer variants with explicit cross-layer connections to validate architectural improvements
3. Test transfer learning from grokked circuits on one reasoning task to different but related tasks