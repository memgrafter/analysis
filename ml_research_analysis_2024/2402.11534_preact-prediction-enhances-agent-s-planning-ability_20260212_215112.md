---
ver: rpa2
title: 'PreAct: Prediction Enhances Agent''s Planning Ability'
arxiv_id: '2402.11534'
source_url: https://arxiv.org/abs/2402.11534
tags:
- your
- preact
- arxiv
- react
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PreAct, an agent framework that enhances
  planning by predicting future observations and corresponding actions. Unlike ReAct,
  which reasons and acts step-by-step, PreAct predicts possible outcomes before acting,
  guiding the agent toward more diverse and strategically focused reasoning.
---

# PreAct: Prediction Enhances Agent's Planning Ability

## Quick Facts
- arXiv ID: 2402.11534
- Source URL: https://arxiv.org/abs/2402.11534
- Authors: Dayuan Fu; Jianzhao Huang; Siyuan Lu; Guanting Dong; Yejie Wang; Keqing He; Weiran Xu
- Reference count: 40
- One-line primary result: PreAct improves LLM agent planning by 6-20% through prediction-based reasoning

## Executive Summary
PreAct introduces a novel agent framework that enhances planning capabilities by predicting future observations and corresponding actions before execution. Unlike ReAct's step-by-step reasoning, PreAct leverages prediction to guide agents toward more diverse and strategically focused reasoning. The framework demonstrates consistent performance improvements across four diverse datasets, with success rates increasing by 6-20% compared to ReAct. The approach also shows further gains when combined with Reflexion or Tree-of-Thought selection techniques.

## Method Summary
PreAct enhances LLM agent planning through a prediction-based approach that generates possible future observations and corresponding handling measures before acting. The framework synchronizes prediction, reasoning, and action in a cyclic process, allowing the agent to reflect on prediction-outcome discrepancies and adjust its planning strategy. The method retains historical predictions in memory to enable sustained learning and employs techniques like Reflexion and Tree-of-Thought selection to further improve performance. Experiments utilize GPT-3.5-turbo-1106 and GPT-4-1106-preview models across four datasets from AgentBench.

## Key Results
- PreAct outperforms ReAct by 6-20% in success rates across all four datasets
- Historical predictions consistently enhance LLM planning, with more retained history correlating with higher success rates
- PreAct achieves higher reasoning diversity, with at least 45% of instances showing superior diversity compared to ReAct
- Combined approaches (PreAct + Reflexion/ToT) further improve performance beyond baseline PreAct

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Predicting future observations before acting enhances the LLM's directional strategy in planning
- Mechanism: The LLM generates predictions of possible observations and handling measures, then reflects on prediction-outcome discrepancies to adjust planning direction
- Core assumption: The LLM can effectively use prediction-outcome discrepancies to improve planning strategy
- Evidence anchors: Abstract states PreAct provides "wider range and more strategically focused reasoning"; section claims it "enhance[s] LLMs' directional strategy"
- Break condition: If predictions are consistently inaccurate or reflection doesn't lead to meaningful planning adjustments

### Mechanism 2
- Claim: Historical predictions contribute to sustained gains in planning
- Mechanism: Retaining multiple rounds of historical predictions allows the LLM to learn from past discrepancies and refine planning over time
- Core assumption: The LLM can effectively learn and generalize from historical prediction data
- Evidence anchors: Experiments show "increased retention of prediction history correlates with a higher success rate"
- Break condition: If historical predictions become outdated or the LLM cannot effectively utilize historical data

### Mechanism 3
- Claim: Prediction enhances reasoning diversity, leading to broader exploration of possible actions
- Mechanism: Predicting multiple possible observations encourages the LLM to consider a wider range of potential scenarios and action paths
- Core assumption: The LLM can effectively generate diverse predictions and use them to inform reasoning
- Evidence anchors: Chart shows "at least 45% of the instances show that PreAct thought's diversity is superior to ReAct"
- Break condition: If predictions become too similar or diversity doesn't translate into improved performance

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: Understanding MDPs is crucial for grasping how agents interact with environments and make decisions
  - Quick check question: What are the two main components of an MDP, and how do they relate to the agent's decision-making process?

- Concept: Chain-of-Thought (CoT) Reasoning
  - Why needed here: CoT reasoning is a key technique used in the ReAct framework that PreAct builds upon
  - Quick check question: How does Chain-of-Thought reasoning differ from direct action-only prompting in language models?

- Concept: Reinforcement Learning Concepts (e.g., reward functions, exploration vs. exploitation)
  - Why needed here: The paper mentions potential application of reinforcement learning in future work
  - Quick check question: What is the difference between exploration and exploitation in reinforcement learning, and why is this trade-off important for agent performance?

## Architecture Onboarding

- Component map: LLM -> Prediction Module -> History Buffer -> Environment Interface
- Critical path: 1) Receive observation, 2) Generate prediction, 3) Reflect on discrepancies, 4) Generate thought and action, 5) Execute action, 6) Receive new observation, 7) Update history
- Design tradeoffs: Balancing prediction horizon complexity with computational efficiency; determining optimal history retention amount; deciding Reflexion frequency
- Failure signatures: Consistently inaccurate predictions, failure to adapt despite discrepancies, over-reliance on historical data
- First 3 experiments: 1) Compare PreAct vs ReAct on household task measuring success rate, 2) Vary historical predictions retained and observe performance impact, 3) Introduce controlled hallucinations and measure impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PreAct's performance scale with increasing task complexity across different domains?
- Basis in paper: [explicit] The paper demonstrates effectiveness on four datasets but doesn't systematically vary task complexity
- Why unresolved: Experiments use fixed datasets with varying inherent complexities but don't systematically increase complexity
- What evidence would resolve it: Experiments testing PreAct on progressively more complex versions of the same task types

### Open Question 2
- Question: What is the optimal balance between prediction accuracy and reasoning diversity in PreAct's performance?
- Basis in paper: [inferred] The paper shows PreAct improves diversity but doesn't analyze the relationship between prediction quality and these metrics
- Why unresolved: Study focuses on demonstrating improvements but doesn't explore how prediction quality affects the trade-off
- What evidence would resolve it: Controlled experiments varying prediction accuracy while measuring impacts on diversity and task completion

### Open Question 3
- Question: How does PreAct's performance compare to model-based reinforcement learning approaches for long-horizon planning tasks?
- Basis in paper: [explicit] The paper discusses planning capabilities but doesn't compare against RL-based approaches
- Why unresolved: Study focuses on LLM-based approaches without benchmarking against RL methods
- What evidence would resolve it: Head-to-head comparisons with RL-based planning agents on long-term planning tasks

### Open Question 4
- Question: What is the impact of prediction horizon length on PreAct's planning effectiveness?
- Basis in paper: [inferred] The paper uses immediate predictions but doesn't explore varying prediction horizons
- Why unresolved: Current implementation focuses on immediate predictions without exploring longer-term predictions
- What evidence would resolve it: Experiments testing different prediction horizon lengths and measuring their impact

## Limitations
- Evaluation constrained to specific AgentBench datasets without systematic complexity variation
- Performance gains not uniformly distributed across all task types
- Computational overhead of prediction and history retention mechanisms not adequately analyzed
- No exploration of alternative LLM architectures beyond GPT models

## Confidence

- High Confidence: PreAct outperforms ReAct in success rates across multiple datasets (6-20% improvement)
- Medium Confidence: Historical predictions contribute to sustained gains in planning (supported by ablation studies)
- Medium Confidence: Prediction enhances reasoning diversity (supported by correlation analyses)

## Next Checks

1. Cross-Model Validation: Replicate experiments using different LLM architectures (Claude, LLaMA) to assess generalizability beyond GPT models

2. Computational Overhead Analysis: Measure additional computational resources required by PreAct to determine if performance gains justify increased complexity

3. Prediction Quality Impact Study: Conduct controlled experiments varying prediction quality to quantify relationship between prediction accuracy and overall task performance