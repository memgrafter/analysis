---
ver: rpa2
title: Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large Language
  Models
arxiv_id: '2412.12687'
source_url: https://arxiv.org/abs/2412.12687
tags:
- token
- uncertainty
- inference
- probability
- rejection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the low token throughput in hybrid language
  model (HLM) inference caused by frequent uplink transmissions and computation overhead.
  It proposes U-HLM, an uncertainty-aware HLM that leverages the SLM's temperature
  perturbation uncertainty to predict LLM rejection probabilities, enabling opportunistic
  skipping of uplink transmissions and LLM computations for tokens likely to be accepted.
---

# Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large Language Models

## Quick Facts
- arXiv ID: 2412.12687
- Source URL: https://arxiv.org/abs/2412.12687
- Reference count: 15
- Proposes U-HLM: an uncertainty-aware hybrid language model that reduces uplink transmissions by 45.93% while maintaining 97.54% of LLM inference accuracy

## Executive Summary
This paper addresses the low token throughput in hybrid language model (HLM) inference caused by frequent uplink transmissions and computation overhead. It proposes U-HLM, an uncertainty-aware HLM that leverages the SLM's temperature perturbation uncertainty to predict LLM rejection probabilities, enabling opportunistic skipping of uplink transmissions and LLM computations for tokens likely to be accepted. Experiments show U-HLM achieves up to 97.54% of the LLM's inference accuracy while delivering 2.54× faster token throughput than standard HLM without skipping.

## Method Summary
The paper proposes U-HLM, which measures uncertainty in the SLM's draft tokens using temperature perturbation. The SLM generates multiple tokens by sampling with different temperature values, and the uncertainty is measured by how often these samples differ from the original draft token. This uncertainty metric serves as a proxy to predict whether the LLM would reject the token. U-HLM uses an uncertainty threshold to opportunistically skip LLM verification for tokens likely to be accepted, only sending high-uncertainty tokens for full HLM verification. The method is evaluated using TinyLlama-1.1B as the SLM and Llama2-7B as the LLM, with experiments showing significant reductions in uplink transmissions and computation while maintaining high inference accuracy.

## Key Results
- Reduces uplink transmissions and LLM computations by 45.93%
- Achieves up to 97.54% of the LLM's inference accuracy
- Delivers 2.54× faster token throughput than standard HLM without skipping
- Empirical rejection risk R = 2.24×10⁻³, confirming theoretical upper bound is satisfied

## Why This Works (Mechanism)

### Mechanism 1
- Temperature perturbation uncertainty in the SLM correlates linearly with LLM rejection probability
- The SLM generates multiple tokens by sampling with different temperature values, and uncertainty is measured by how often samples differ from the original draft token
- Core assumption: Linear relationship between uncertainty and rejection probability holds consistently
- Evidence: Strong empirical finding of linear correlation, but weak corpus support
- Break condition: Degradation of linear correlation across different model architectures or domains

### Mechanism 2
- U-HLM uses uncertainty threshold to opportunistically skip LLM verification for tokens likely to be accepted
- SLM computes uncertainty for each draft token; if below threshold, token is likely accepted and uplink transmission/computation are skipped
- Core assumption: Uncertainty threshold can balance accuracy loss against throughput gains
- Evidence: Design based on rejection probability threshold, but weak corpus support
- Break condition: Threshold too high (unnecessary transmissions) or too low (accuracy degradation)

### Mechanism 3
- Expected rejection risk R has analytically derivable upper bound that remains negligible with proper threshold selection
- Authors derive R ≤ Δ^(3/2)/√(3a) × √(∫|f(u)|²du) as theoretical guarantee on accuracy loss
- Core assumption: Uncertainty and rejection probability are i.i.d. across rounds
- Evidence: Theoretical derivation with empirical validation (R = 2.24×10⁻³), but weak corpus support
- Break condition: i.i.d. assumption violation causing theoretical bound to not hold

## Foundational Learning

- Concept: Metropolis-Hastings algorithm and its application to speculative decoding
  - Why needed: HLM framework relies on speculative inference based on Metropolis-Hastings principles for vocabulary distribution alignment
  - Quick check: How does the acceptance probability 1 - yd(t)/xd(t) relate to the Metropolis-Hastings acceptance criterion?

- Concept: Temperature scaling in language model sampling
  - Why needed: Temperature perturbation is the specific uncertainty measurement technique used
  - Quick check: What effect does increasing temperature have on the entropy of the resulting probability distribution?

- Concept: Shannon's formula for channel capacity and its application to wireless transmission latency
  - Why needed: Paper models uplink transmission time based on channel bandwidth, SNR, and payload size
  - Quick check: How does uplink transmission time change when channel bandwidth is doubled with constant SNR?

## Architecture Onboarding

- Component map: Mobile device (SLM with temperature perturbation) -> Base station (LLM) -> Wireless channel -> Shared vocabulary
- Critical path: Input token sequence → SLM forward pass (draft token + temperature samples) → Uncertainty calculation → threshold comparison → If skip: return draft token; else: uplink transmission → LLM verification (acceptance/rejection or resampling) → Output token → next round
- Design tradeoffs:
  - Accuracy vs. throughput: Higher thresholds skip more tokens but risk more rejections
  - Number of temperature samples K: More samples give better uncertainty estimates but increase computation
  - Temperature range [0, θmax]: Larger range captures more uncertainty but may introduce noise
  - Payload size vs. precision: Half-precision reduces transmission time but may affect LLM accuracy
- Failure signatures:
  - High cosine similarity loss despite high true skip rate: Threshold set too high, skipping many rejected tokens
  - Low token throughput improvement despite high transmission rate: Threshold set too low, sending many tokens unnecessarily
  - Large gap between theoretical and empirical rejection risk: i.i.d. assumption violation or incorrect linear model parameters
- First 3 experiments:
  1. Measure linear correlation between temperature perturbation uncertainty and LLM rejection probability across different temperature ranges and sample counts
  2. Sweep uncertainty threshold values to find optimal tradeoff between transmission rate and cosine similarity loss
  3. Compare empirical rejection risk with theoretical upper bound across different channel SNR conditions

## Open Questions the Paper Calls Out

### Open Question 1
- How would U-HLM's performance scale with significantly larger vocabulary sizes (e.g., 100K-500K tokens) common in frontier LLMs?
- Basis: Paper evaluates with 32,000 vocabulary size and mentions uplink payload is proportional to vocabulary dimension
- Why unresolved: Only tests with 32K vocabulary; doesn't explore scaling effects on uncertainty-rejection correlation or temperature perturbation effectiveness
- What evidence would resolve: Experiments showing performance metrics across multiple vocabulary sizes (32K, 100K, 250K, 500K)

### Open Question 2
- Would incorporating channel state information (CSI) into the uncertainty threshold optimization improve U-HLM performance in varying wireless conditions?
- Basis: Paper evaluates across different SNR conditions but uses fixed uncertainty threshold independent of channel state
- Why unresolved: Current threshold design doesn't account for dynamic wireless channel conditions that significantly impact accuracy-throughput tradeoffs
- What evidence would resolve: Comparative experiments showing performance with adaptive thresholds based on real-time CSI versus fixed thresholds across various mobility scenarios

### Open Question 3
- How does U-HLM's uncertainty measurement generalize to other uncertainty quantification methods beyond temperature perturbation?
- Basis: Paper states temperature perturbation exhibited strong linear relationship but doesn't explore other methods
- Why unresolved: While temperature perturbation shows strong correlation, other methods (Monte Carlo dropout, ensemble methods) aren't investigated
- What evidence would resolve: Systematic comparison of different uncertainty quantification methods showing their uncertainty-rejection probability correlations and resulting U-HLM performance

## Limitations

- Core theoretical framework relies heavily on linear correlation assumption between SLM uncertainty and LLM rejection probability, which may not generalize across different model architectures or domains
- Experimental validation is limited to specific model combinations (TinyLlama-1.1B and Llama2-7B) and a relatively small dataset (100 samples from Alpaca)
- Theoretical risk bound derivation assumes i.i.d. uncertainty and rejection probability distributions, which may not hold in practice for context-dependent token generation

## Confidence

**High Confidence:** Temperature perturbation for measuring SLM uncertainty is technically sound and well-established; HLM speculative decoding framework and basic optimization problem formulation are well-grounded.

**Medium Confidence:** Specific linear correlation parameters (a = 0.82, b = -0.06) and optimal threshold value (uth = 0.431) are derived from experiments but may be task-specific; reported performance improvements likely valid for tested configuration but may not generalize.

**Low Confidence:** Theoretical risk bound and its practical implications are difficult to verify without full experimental data and specific channel conditions; assumption that cosine similarity with USE adequately captures inference quality for all tasks is questionable.

## Next Checks

1. **Correlation Validation Across Domains:** Test the linear correlation between temperature perturbation uncertainty and LLM rejection probability across at least three different task domains (code generation, mathematical reasoning, creative writing) to verify if the reported slope (a = 0.82) remains consistent.

2. **Threshold Sensitivity Analysis:** Systematically vary the uncertainty threshold from 0.1 to 0.9 in increments of 0.1 and measure the resulting tradeoff curves between transmission rate, true skip rate, and cosine similarity loss to identify the optimal threshold range for different use cases.

3. **Risk Bound Verification:** Compare the empirical rejection risk across different SNR conditions (0-30 dB in 5 dB increments) with the theoretical upper bound to validate the i.i.d. assumption and the accuracy of the derived bound formula under varying wireless channel conditions.