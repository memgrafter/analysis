---
ver: rpa2
title: 'EVALALIGN: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data
  for Evaluating Text-to-Image Models'
arxiv_id: '2406.16562'
source_url: https://arxiv.org/abs/2406.16562
tags:
- image
- annotation
- evaluation
- human
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EVALALIGN addresses the challenge of accurately evaluating text-to-image
  generation models by introducing a supervised fine-tuning approach for multimodal
  large language models (MLLMs) using human-aligned data. The method constructs a
  detailed evaluation dataset with fine-grained human annotations focusing on image
  faithfulness and text-image alignment, covering 11 specific skills across these
  two dimensions.
---

# EVALALIGN: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image Models

## Quick Facts
- arXiv ID: 2406.16562
- Source URL: https://arxiv.org/abs/2406.16562
- Reference count: 40
- Primary result: EVALALIGN achieves Kendall correlation of 0.7464 for image faithfulness and 0.8043 for text-image alignment, outperforming existing metrics on 24 text-to-image models

## Executive Summary
EVALALIGN introduces a supervised fine-tuning approach for multimodal large language models (MLLMs) to evaluate text-to-image generation quality using human-aligned data. The method addresses the challenge of data bias in existing evaluation metrics, which are trained on real images but must evaluate synthesized images. By curating a dataset with detailed human annotations across 11 fine-grained skills spanning image faithfulness and text-image alignment, EVALALIGN creates an evaluation model that better aligns with human preferences while requiring minimal training data.

## Method Summary
The method involves collecting 21k images from 24 text-to-image models with prompts from existing evaluation datasets, then annotating these images with detailed human feedback across 11 specific skills. A LLaVA-Next MLLM is fine-tuned using LoRA on the annotated dataset with autoregressive loss. The resulting evaluation model can assess generated images across multiple dimensions including face quality, hand structure, object presence, and color accuracy. The approach demonstrates strong generalization across different models while addressing the data bias problem where existing metrics perform poorly on synthesized images due to training on real images.

## Key Results
- Achieves Kendall correlation of 0.7464 for image faithfulness evaluation
- Achieves Kendall correlation of 0.8043 for text-image alignment evaluation
- Outperforms baseline methods including CLIP-score, HPSv2, ImageReward, and PickScore across 24 text-to-image models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning aligns MLLMs with human preferences on evaluating synthesized images
- Mechanism: By training MLLMs on detailed human-annotated data that captures 11 fine-grained skills across image faithfulness and text-image alignment, the model learns to evaluate generated images in ways that closely match human judgment
- Core assumption: Human annotations on synthesized images provide meaningful signals for evaluation that can be transferred to the MLLM
- Evidence anchors: [abstract] "We supervised fine-tune (SFT) the MLLM to align with human evaluative judgments, resulting in a robust evaluation model"

### Mechanism 2
- Claim: The method overcomes data bias by using synthesized images rather than real images for training evaluation models
- Mechanism: Existing evaluation methods are trained on real images but must evaluate synthesized images, creating a mismatch. EVALALIGN trains directly on synthesized images with human annotations
- Core assumption: Synthesized images have distinct characteristics that require specialized evaluation training
- Evidence anchors: [abstract] "existing evaluation methods performs poorly in synthesized image evaluations" due to "data bias"

### Mechanism 3
- Claim: Fine-grained evaluation across 11 skills provides more actionable feedback than coarse metrics
- Mechanism: Instead of single scores, the method evaluates specific aspects like face quality, hand structure, object presence, color accuracy, etc., enabling targeted model improvements
- Core assumption: Detailed skill-level feedback is more useful for model development than overall scores
- Evidence anchors: [abstract] "Each protocol comprises a set of detailed, fine-grained instructions linked to specific scoring options, enabling precise manual scoring"

## Foundational Learning

- Concept: Supervised fine-tuning (SFT)
  - Why needed here: Standard MLLMs lack the specialized knowledge to evaluate text-to-image generation quality, requiring fine-tuning on task-specific data
  - Quick check question: What is the key difference between zero-shot evaluation and supervised fine-tuning in this context?

- Concept: Data bias in evaluation
  - Why needed here: Understanding why existing metrics fail - they're trained on real images but must evaluate synthetic ones with different characteristics
  - Quick check question: Why would training an evaluation model on real images lead to poor performance on synthetic images?

- Concept: Kendall correlation for ranking evaluation
  - Why needed here: The paper uses Kendall correlation to measure how well evaluation metrics align with human rankings of models
  - Quick check question: What does a Kendall correlation of 0.7464 indicate about the relationship between EVALALIGN and human judgments?

## Architecture Onboarding

- Component map: MLLM (base model) → SFT training on human-annotated dataset → Evaluation model → Scoring system with 11 skill-specific questions
- Critical path: Data collection → Human annotation → SFT training → Evaluation inference → Score aggregation
- Design tradeoffs: Fine-grained evaluation provides detailed feedback but requires more complex annotation and inference; using synthesized images for training addresses bias but may limit generalization
- Failure signatures: Poor correlation with human judgments indicates annotation quality issues or inadequate SFT; inconsistent skill-level scores suggest model confusion about evaluation criteria
- First 3 experiments:
  1. Compare zero-shot MLLM performance vs SFT-trained model on validation set to confirm SFT effectiveness
  2. Test model generalization by evaluating models not seen during SFT training
  3. Vary SFT dataset size to determine minimum effective training data requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EVALALIGN perform on evaluating video-to-text generation models compared to text-to-image models?
- Basis in paper: [explicit] The paper mentions that EVALALIGN could inspire future research in this field and the method's evaluation of 24 text-to-image models.
- Why unresolved: The paper only evaluates EVALALIGN on text-to-image models, not on video-to-text models.
- What evidence would resolve it: Experiments evaluating EVALALIGN on video-to-text models and comparing the results to human evaluations.

### Open Question 2
- Question: Can EVALALIGN be adapted to evaluate other types of generative models, such as audio or 3D models?
- Basis in paper: [inferred] The paper discusses the limitations of EVALALIGN with MLLMs and human annotations, suggesting potential for adaptation to other generative models.
- Why unresolved: The paper does not explore the application of EVALALIGN to other types of generative models.
- What evidence would resolve it: Experiments adapting EVALALIGN to evaluate audio or 3D generative models and comparing the results to human evaluations.

### Open Question 3
- Question: How does the performance of EVALALIGN change when using different base MLLMs or when increasing the model size beyond 34B parameters?
- Basis in paper: [explicit] The paper discusses the effect of model size on performance and mentions using LLaVA-NeXT 34B.
- Why unresolved: The paper only explores the effect of model size up to 34B parameters and does not compare different base MLLMs.
- What evidence would resolve it: Experiments comparing the performance of EVALALIGN using different base MLLMs and varying the model size beyond 34B parameters.

## Limitations

- The method relies heavily on the quality and consistency of human annotations across 11 fine-grained skills, with inter-annotator reliability at moderate levels (Cohen's kappa of 0.681)
- The approach requires significant computational resources (32 NVIDIA A100 GPUs for 10 hours) for fine-tuning, limiting accessibility for smaller research groups
- The evaluation focuses on 24 text-to-image models, which may not represent the full diversity of generation approaches or capture all possible failure modes

## Confidence

**High Confidence:** The supervised fine-tuning approach and use of human-annotated data are well-established techniques with clear implementation details. The Kendall correlation results demonstrate statistically significant improvement over baseline methods.

**Medium Confidence:** The effectiveness of 11 fine-grained skill categories for providing actionable feedback is supported by evaluation results, but lacks evidence that practitioners actually use this granular information for model improvement.

**Low Confidence:** The generalizability to text-to-image models not included in the original evaluation set and long-term stability across different prompt distributions are not thoroughly tested.

## Next Checks

1. **Annotation Quality Validation:** Conduct a blind re-annotation study with a subset of images to verify the reported inter-annotator reliability (Cohen's kappa of 0.681) and identify any systematic biases in the original annotation process.

2. **Domain Transfer Experiment:** Test EVALALIGN's performance on evaluating text-to-image models trained on completely different datasets or using different architectural approaches than those included in the original 24-model evaluation to assess true generalization capability.

3. **Baseline Comparison Expansion:** Implement and evaluate additional baseline metrics that are trained specifically on synthetic image data to isolate the contribution of the SFT approach versus the data source choice in achieving superior performance.