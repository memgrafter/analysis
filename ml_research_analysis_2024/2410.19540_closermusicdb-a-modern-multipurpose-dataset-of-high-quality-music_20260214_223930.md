---
ver: rpa2
title: 'CloserMusicDB: A Modern Multipurpose Dataset of High Quality Music'
arxiv_id: '2410.19540'
source_url: https://arxiv.org/abs/2410.19540
tags:
- music
- artist
- dataset
- closermusicdb
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CloserMusicDB, a high-quality dataset of 106
  full-length studio tracks designed to advance research in music information retrieval.
  The dataset includes expert annotations for hook detection, contextual tagging,
  and artist identification, addressing limitations of existing music datasets.
---

# CloserMusicDB: A Modern Multipurpose Dataset of High Quality Music

## Quick Facts
- arXiv ID: 2410.19540
- Source URL: https://arxiv.org/abs/2410.19540
- Reference count: 0
- 106 high-quality studio tracks annotated for hook detection, contextual tagging, and artist identification

## Executive Summary
CloserMusicDB is a novel dataset containing 106 full-length studio tracks with expert annotations for three music information retrieval tasks: hook detection, contextual tagging, and artist identification. The dataset addresses limitations of existing music datasets by providing high-quality audio recordings and comprehensive metadata annotations. Baseline experiments using OpenL3 embeddings and a multi-layer perceptron architecture demonstrate the dataset's utility for research, achieving 60.22% accuracy for artist identification and 41.5% accuracy for hook detection with ±5s tolerance.

## Method Summary
The baseline experiments use OpenL3 embeddings (256mel, 512-dim, 1s hop) as input features for a 3-layer MLP classifier (256 neurons each, ReLU activation). For contextual tagging, sigmoid output with binary cross-entropy loss is used, while artist identification uses softmax output with categorical cross-entropy loss. Models are trained with Adam optimizer (lr=1e-4, batch=32) for 50 epochs using 5-fold cross-validation. Hook detection is performed using MSAF with Ordinal LDA boundary detection algorithm.

## Key Results
- Artist identification: 60.22% (±5.14) accuracy and 0.5141 (±0.0849) F1 score
- Contextual tagging: 0.2998 (±0.0767) Jaccard score and 0.6772 (±0.0625) ROC AUC
- Hook detection: 41.5% accuracy with ±5s tolerance and 35.8% with ±3s tolerance

## Why This Works (Mechanism)

### Mechanism 1
- Expert human annotations provide ground truth for complex music understanding tasks
- Human experts label hook boundaries, contextual tags, and artist identities, creating high-quality reference data that machine learning models can learn from
- Core assumption: Human expert judgment captures the semantic structure and intent of music better than automated or crowd-sourced methods
- Break condition: If human annotation reliability varies significantly between experts or tasks, model performance may degrade

### Mechanism 2
- Transfer learning with pre-trained embeddings enables effective classification on limited data
- OpenL3 embeddings capture audio features that transfer well to downstream tasks like tagging and artist identification, reducing need for task-specific feature engineering
- Core assumption: Audio embeddings learned from large datasets contain generalizable representations of musical features
- Break condition: If OpenL3 embeddings don't capture task-relevant features, model performance will be limited regardless of architecture

### Mechanism 3
- High-quality, full-length studio recordings provide better training signals than lower-quality or fragmented data
- Professional recordings with 44.1kHz sampling rate and 16-bit depth preserve audio details that may be important for distinguishing artists, detecting hooks, and understanding musical context
- Core assumption: Audio quality directly impacts the features available for machine learning models to learn from
- Break condition: If downstream tasks don't require the level of detail preserved in high-quality audio, simpler datasets might suffice

## Foundational Learning

- Concept: Music Information Retrieval fundamentals
  - Why needed here: Understanding core MIR tasks like feature extraction, similarity measures, and evaluation metrics is essential for working with this dataset
  - Quick check question: What is the difference between precision and recall in the context of music tagging?

- Concept: Transfer learning in audio domains
  - Why needed here: The baseline experiments use pre-trained OpenL3 embeddings, requiring understanding of how feature representations transfer across tasks
  - Quick check question: Why might pre-trained audio embeddings be more effective than training from scratch on a small dataset?

- Concept: Multilabel classification and evaluation
  - Why needed here: Contextual tagging is a multilabel task requiring specific metrics like Jaccard score and ROC AUC, while artist identification is multiclass requiring accuracy and F1 score
  - Quick check question: How does Jaccard score differ from accuracy when evaluating multilabel classification?

## Architecture Onboarding

- Component map: Audio files → OpenL3 embedding extraction → MLP classifier → Evaluation metrics
- Critical path: Data loading → preprocessing → embedding extraction → model training → evaluation
- Design tradeoffs: Simple MLP architecture vs. more complex models (CNNs, transformers); pre-trained embeddings vs. task-specific features
- Failure signatures: Low Jaccard scores suggest embedding quality issues; low accuracy suggests model capacity or data quantity problems
- First 3 experiments:
  1. Verify OpenL3 embedding extraction works correctly on sample audio
  2. Train MLP on a single fold to check convergence and basic functionality
  3. Run baseline experiments on the full dataset and compare results to published benchmarks

## Open Questions the Paper Calls Out

- How would larger datasets and hyperparameter optimization affect the performance of hook detection, contextual tagging, and artist identification tasks?
- How does the performance of MSAF-based hook detection compare to other state-of-the-art hook detection algorithms on the CloserMusicDB dataset?
- How does the inclusion of contextual tags (e.g., "workout," "travel") impact the effectiveness of music recommendation systems compared to traditional genre-based tagging?

## Limitations

- Dataset size limitation with only 106 tracks totaling ~5 hours of audio
- Transfer learning efficacy not empirically validated against alternative feature extraction methods
- Annotation reliability not quantified with inter-annotator agreement scores

## Confidence

**High confidence**: Dataset quality specifications, baseline experimental methodology, licensing terms and availability

**Medium confidence**: Transfer learning effectiveness, annotation quality and consistency, generalizability to real-world applications

**Low confidence**: Long-term maintenance and updates of the dataset, community adoption and usage patterns, impact on downstream research applications

## Next Checks

1. Conduct inter-annotator agreement testing on a subset of tracks to quantify annotation consistency across hook detection, tagging, and artist identification tasks.

2. Perform ablation studies comparing OpenL3 embeddings with alternative audio feature extraction methods (e.g., VGGish, MusicNN) to validate the choice of pre-trained embeddings for each task.

3. Test models trained on CloserMusicDB on external datasets to assess whether performance generalizes beyond the specific tracks and artists included in this dataset.