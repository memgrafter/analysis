---
ver: rpa2
title: 'Interactive Agents: Simulating Counselor-Client Psychological Counseling via
  Role-Playing LLM-to-LLM Interactions'
arxiv_id: '2408.15787'
source_url: https://arxiv.org/abs/2408.15787
tags:
- client
- counselor
- counseling
- arxiv
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SimPsyDial, a framework that uses two GPT-4
  models in a role-playing setup to simulate counselor-client interactions for psychological
  counseling. One model acts as a client with a real-life user profile, while the
  other plays an experienced counselor using integrative therapy techniques.
---

# Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions

## Quick Facts
- arXiv ID: 2408.15787
- Source URL: https://arxiv.org/abs/2408.15787
- Authors: Huachuan Qiu; Zhenzhong Lan
- Reference count: 40
- Key outcome: SimPsyDial achieves significantly higher scores in goal, task, and bond categories compared to real counseling dialogues, and trained SimPsyBot outperforms existing state-of-the-art mental health models.

## Executive Summary
This paper introduces SimPsyDial, a framework that uses two GPT-4 models in a role-playing setup to simulate counselor-client interactions for psychological counseling. One model acts as a client with a real-life user profile, while the other plays an experienced counselor using integrative therapy techniques. The simulated dialogues are evaluated against human-generated conversations and used to train dialogue systems. The LLM-generated dataset achieves high-quality results, with significantly higher scores in goal, task, and bond categories compared to real counseling dialogues. The trained SimPsyBot outperforms existing state-of-the-art models for mental health, demonstrating strong conversational abilities and high user satisfaction in both automatic and human evaluations.

## Method Summary
The framework involves two GPT-4 models in a zero-shot role-playing setup: one acting as a client equipped with a specific and real-life user profile, and the other playing the role of an experienced counselor, generating professional responses using integrative therapy techniques. The study extracts 1000 user profiles from the PsyQA dataset (posts >300 characters) and generates 1000 dialogues with up to 50 turns each. These synthetic dialogues are used to fine-tune Qwen2-7B-Instruct and DeepSeek-LLM-7B-Chat models, which are then evaluated against baseline models using automatic LLM judge and human evaluation with professional counselors.

## Key Results
- SimPsyDial dataset achieves significantly higher mean scores in goal, task, and bond categories than the RealPsyDial dataset in LLM evaluation
- SimPsyBot outperforms existing state-of-the-art models for mental health, including MeChat, SoulChat, PsyChat, and CPsyCounX
- Human evaluation shows high user satisfaction with SimPsyBot, with scores significantly higher than baselines across most metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using two separate GPT-4 instances in a zero-shot role-playing setup can generate realistic counselor-client dialogues without human intervention.
- Mechanism: Each LLM simulates a distinct role (client with a user profile, counselor with integrative therapy training) and exchanges turns in a turn-by-turn simulation. The framework enforces conversational rules (e.g., concise turns, no early termination) to mimic real counseling interactions.
- Core assumption: GPT-4's zero-shot prompting is sufficient to capture the distinct conversational behaviors of a client seeking help and a counselor providing professional guidance.
- Evidence anchors:
  - [abstract]: "Our framework involves two LLMs, one acting as a client equipped with a specific and real-life user profile and the other playing the role of an experienced counselor, generating professional responses using integrative therapy techniques."
  - [section]: "We implement both counselor and client by zero-shot prompting the GPT-4 model."
  - [corpus]: Weak evidence - only similarity in topic distribution between SimPsyDial and RealPsyDial, not behavioral realism.
- Break condition: If GPT-4 cannot maintain distinct and consistent roles, or if generated utterances become too long or unnatural, the simulation quality degrades.

### Mechanism 2
- Claim: The integrative therapy-based prompt structure (exploration, insight, action stages) leads to more effective counseling dialogues.
- Mechanism: The counselor prompt guides the LLM through three stages aligned with different therapeutic approaches: client-centered therapy (exploration), psychodynamic therapy (insight), and cognitive behavioral therapy (action). This structure ensures depth and progress in the dialogue.
- Core assumption: Dividing the counseling process into these three stages aligns with effective human counseling practices and improves the quality of simulated dialogues.
- Evidence anchors:
  - [section]: "The theoretical foundation of counselor simulation in this paper is heavily influenced by integrating diverse therapeutic principles as conceptualized by Hill [16]."
  - [section]: "The theory of the three-stage model, including exploration, insight, and action, corresponds to client-centered therapy, psychodynamic therapy, and cognitive behavioral therapy, respectively."
  - [corpus]: No direct evidence; relies on theoretical alignment rather than empirical validation.
- Break condition: If the three-stage model does not improve dialogue quality over a flat prompt structure, or if the LLM ignores stage transitions, the approach fails.

### Mechanism 3
- Claim: Using LLMs as judges for automatic evaluation yields high agreement with human experts on dialogue quality.
- Mechanism: An LLM (e.g., DeepSeek-V2-Chat) scores simulated dialogues on WAI-O-S (goal, task, bond) dimensions. The LLM acts as an observer, reading the full conversation and guidelines, and outputs a numeric score.
- Core assumption: LLMs can reliably assess the quality of counseling dialogues by evaluating the counselor's ability to build rapport, achieve goals, and guide the client effectively.
- Evidence anchors:
  - [section]: "Motivated by the prevalence and effectiveness of using an LLM as a judge and quality assessment of psychological counseling sessions with the Working Alliance Inventory (WAI), we propose to use LLMs as observers to evaluate the quality of counseling sessions."
  - [section]: "The SimPsyDial dataset exhibits higher mean scores in goal, task, and bond categories than the RealPsyDial dataset."
  - [corpus]: No comparative data with human judges; only internal consistency is shown.
- Break condition: If LLM scores diverge significantly from human expert scores, or if the LLM consistently over- or under-rates certain dialogue qualities, the automatic evaluation is unreliable.

## Foundational Learning

- Concept: Role-playing in LLM simulations
  - Why needed here: The framework relies on two distinct LLMs acting as separate agents (client and counselor) to simulate realistic dialogue.
  - Quick check question: What is the difference between a single LLM acting as both roles versus two LLMs each acting as one role in turn-taking?
- Concept: Zero-shot prompting
  - Why needed here: The study uses zero-shot prompting to instantiate both the client and counselor roles without fine-tuning or few-shot examples.
  - Quick check question: How does zero-shot prompting differ from few-shot prompting, and why might it be preferred for simulating new conversational roles?
- Concept: Integrative therapy
  - Why needed here: The counselor simulation is based on an integrative therapy framework combining exploration, insight, and action stages from different therapeutic traditions.
  - Quick check question: What are the three stages of the integrative therapy model used here, and which therapeutic approaches do they correspond to?

## Architecture Onboarding

- Component map:
  - User Profile Pool -> Client Simulator -> Dialogue Collector -> Counselor Simulator -> Dialogue Collector -> Evaluation Engine -> Training Pipeline
- Critical path: Profile → Client Sim → Counselor Sim → Turn Exchange → Evaluation → Training Data
- Design tradeoffs:
  - Zero-shot vs. fine-tuned prompts: Zero-shot avoids training data needs but may lack domain specificity; fine-tuning could improve realism but requires more data and compute.
  - Turn limit (50) vs. unlimited: Limits resource usage and prevents infinite loops, but may truncate realistic sessions.
  - LLM judge vs. human judges: Faster and cheaper, but potentially less reliable for nuanced counseling quality.
- Failure signatures:
  - Client simulator outputs generic or off-topic responses.
  - Counselor simulator ignores stage transitions or generates overly long responses.
  - Evaluation scores are inconsistent or diverge from human judgments.
  - Training data lacks diversity in client profiles or counseling issues.
- First 3 experiments:
  1. Run a small batch of dialogues and manually inspect client and counselor turn quality for role adherence and naturalness.
  2. Compare vocabulary overlap and semantic similarity between client utterances and their assigned profiles to validate profile influence.
  3. Test LLM judge scores against a small set of human-annotated dialogues to check agreement and reliability.

## Open Questions the Paper Calls Out

- Question: How can resistance characteristics be effectively incorporated into client simulation to better reflect real-world counseling scenarios?
  - Basis in paper: [explicit] The paper identifies this as a future work direction, noting that current client simulation using LLMs may ignore the social impact on clients, such as family, job, and suicide risk, which could lead to increased resistance in counseling for real clients.
  - Why unresolved: The paper does not provide a methodology for incorporating resistance characteristics into client simulation. It only mentions the need to do so in the future.
  - What evidence would resolve it: A detailed framework or methodology for incorporating resistance characteristics into client simulation, along with empirical results demonstrating improved realism and effectiveness compared to current approaches.

- Question: How do LLM-generated counselor-client dialogues compare in depth and quality to those generated by human counselors over multiple counseling sessions?
  - Basis in paper: [inferred] The paper notes that LLM-simulated counselors have limitations in counseling depth, as they often speak more than real counselors and cannot explore clients' emotions as deeply. It also suggests simulating second counseling sessions based on first sessions to analyze client changes.
  - Why unresolved: The paper only compares single-session dialogues between LLMs and human counselors, without examining how the quality and depth evolve over multiple sessions or how well LLMs can track and respond to client changes.
  - What evidence would resolve it: Comparative analysis of multiple counseling sessions between LLM and human counselors, including metrics for counseling depth, client change tracking, and longitudinal outcomes.

- Question: What are the most effective methods for building more human-like dialogue datasets for psychological counseling using LLMs and real-life counseling sessions?
  - Basis in paper: [explicit] The paper identifies this as a future work direction, proposing to optimize prompts and use retrieval-augmented generation (RAG) with real-life counseling sessions to construct more realistic counselors and clients.
  - Why unresolved: The paper does not provide specific methods or results for building more human-like dialogue datasets. It only outlines a general approach using RAG and prompt optimization.
  - What evidence would resolve it: Comparative studies of different methods for building human-like dialogue datasets, including prompt engineering, RAG integration, and evaluation metrics for realism and effectiveness compared to purely synthetic or purely human-generated datasets.

## Limitations

- The study relies entirely on LLM-generated evaluation (DeepSeek-V2-Chat) without direct comparison to human expert judgments, raising questions about the reliability of the claimed superiority over real counseling data.
- The framework uses only Chinese-language data and GPT-4 for simulation, limiting generalizability to other languages or LLM architectures.
- The study does not address how well the simulated dialogues capture the depth and complexity of real counseling relationships over multiple sessions.

## Confidence

- **High Confidence**: The technical implementation of the role-playing framework and the basic mechanism of using two GPT-4 instances for turn-based dialogue generation are well-documented and replicable.
- **Medium Confidence**: The comparison of topic distributions between SimPsyDial and RealPsyDial is methodologically sound, but the claim about superior counseling quality based solely on LLM evaluation is less certain.
- **Low Confidence**: The assertion that SimPsyDial can replace real counseling data for training mental health dialogue systems, and that LLM-based automatic evaluation is reliable for counseling quality assessment, lacks sufficient validation against human expert judgments.

## Next Checks

1. **Human Expert Validation**: Compare LLM judge scores (DeepSeek-V2-Chat) against human expert evaluations on a subset of dialogues from both SimPsyDial and RealPsyDial to assess agreement and reliability of automatic evaluation.

2. **Generalization Test**: Replicate the simulation framework using different LLM architectures (e.g., Claude, Llama) and/or different languages to test whether the approach generalizes beyond GPT-4 and Chinese-language counseling.

3. **Longitudinal Quality Assessment**: Track counselor-client relationships across multiple sessions in the simulated dialogues to evaluate whether the framework can maintain consistent therapeutic progress and rapport over time, rather than just within single 50-turn interactions.