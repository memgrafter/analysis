---
ver: rpa2
title: 'Fine Tuning Large Language Models for Medicine: The Role and Importance of
  Direct Preference Optimization'
arxiv_id: '2409.12741'
source_url: https://arxiv.org/abs/2409.12741
tags:
- fine
- tuning
- clinical
- triage
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared Supervised Fine Tuning (SFT) and Direct Preference
  Optimization (DPO) for fine-tuning large language models on clinical natural language
  processing tasks. DPO significantly improved model performance for complex tasks
  like clinical reasoning, summarization, and triage, while SFT alone was sufficient
  for simpler text classification tasks.
---

# Fine Tuning Large Language Models for Medicine: The Role and Importance of Direct Preference Optimization

## Quick Facts
- arXiv ID: 2409.12741
- Source URL: https://arxiv.org/abs/2409.12741
- Reference count: 30
- Primary result: DPO significantly improves LLM performance on complex clinical tasks while SFT alone suffices for simple classification

## Executive Summary
This study systematically compares Supervised Fine Tuning (SFT) and Direct Preference Optimization (DPO) for fine-tuning large language models on clinical natural language processing tasks. The authors found that while SFT alone is sufficient for simpler text classification tasks, DPO significantly improves performance on complex tasks requiring clinical reasoning, summarization, and triage. Using Llama3 and Mistral models on five clinical datasets, DPO demonstrated measurable improvements across multiple evaluation metrics, with the most dramatic gains observed in triage tasks where urgency classification F1 scores improved from 0.79 to 0.91.

## Method Summary
The study employed a comparative approach using Llama3-8B-Instruct and Mistral-Instruct-v2 models fine-tuned on five clinical NLP tasks: UTI classification, hyponatremia classification, clinical reasoning (MedQA), discharge summary summarization, and Stanford patient message triage. For each task, SFT models were first trained using gold-standard responses, then the best SFT model served as the base for DPO training using both positive and negative examples. Performance was evaluated using task-specific metrics including F1 scores, accuracy, and Likert scale ratings, with statistical significance assessed using McNemar's test and paired t-tests.

## Key Results
- DPO improved Llama3 clinical reasoning accuracy from 28% to 36% (p=0.003)
- DPO increased summarization ratings from 4.21 to 4.34 (p<0.001)
- DPO improved triage personnel classification F1 from 0.58 to 0.74 (p<0.001) and urgency classification from 0.79 to 0.91 (p<0.001)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPO outperforms SFT for complex clinical tasks because it trains on both positive and negative examples, enabling the model to learn nuanced patterns rather than simple word associations.
- Mechanism: By optimizing for reward differences between preferred and rejected responses, DPO forces the model to distinguish between high- and low-quality outputs, which improves performance on tasks requiring abstract reasoning.
- Core assumption: The "rejected" examples used in DPO training are meaningful and sufficiently diverse to represent the negative space of the task.
- Evidence anchors:
  - [abstract]: "DPO fine-tuning improved Llama3â€™s accuracy on clinical reasoning from 28% to 36% (p=0.003)"
  - [section]: "DPO increases accuracy even further to 36% (p value 0.003) for Llama3 and 40% (p value 0.004 ) for Mistral2"
- Break condition: If rejected examples are poorly chosen or too similar to positive ones, DPO may not provide additional benefit over SFT.

### Mechanism 2
- Claim: SFT alone is sufficient for simple classification tasks because they can be solved by learning strong word associations from gold-standard responses.
- Mechanism: SFT trains the model to maximize the likelihood of reproducing correct answers, which works well for tasks with clearly defined criteria (e.g., detecting UTI based on keywords).
- Core assumption: The task can be reduced to pattern matching without requiring deep comprehension.
- Evidence anchors:
  - [abstract]: "SFT alone is sufficient for Classification with text data, whereas DPO improves performance for the more complex tasks"
  - [section]: "SFT alone is sufficient for text-based Classification (Figure 1a)"
- Break condition: If the classification task requires reasoning beyond keyword matching (e.g., numeric interpretation), SFT may fail to improve performance.

### Mechanism 3
- Claim: DPO fine-tuning improves triage and summarization by enabling the model to learn abstract criteria that are difficult to express as explicit rules.
- Mechanism: By contrasting correct and incorrect responses, DPO helps the model internalize subtle distinctions (e.g., urgency levels, summary relevance) that cannot be captured by SFT alone.
- Core assumption: The abstract criteria can be implicitly learned through comparison rather than explicit instruction.
- Evidence anchors:
  - [abstract]: "DPO improved personnel classification F1 scores from 0.58 to 0.74 (p<0.001) and urgency classification from 0.79 to 0.91 (p<0.001)"
  - [section]: "DPO increased Llama3 performance for both personnel to 0.74 (p value <0.001) and urgency triage: 0.91 (p value <0.001)"
- Break condition: If the abstract criteria are too subtle or inconsistent, DPO may struggle to find meaningful patterns.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT is the baseline fine-tuning method used for comparison; understanding its limitations is key to appreciating DPO's value.
  - Quick check question: What is the primary difference between SFT and DPO in terms of training data?
- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is the main focus of the study; knowing how it uses rejected examples is essential to understanding its mechanism.
  - Quick check question: How does DPO's loss function differ from SFT's in terms of the examples it uses?
- Concept: McNemar's test and paired t-test
  - Why needed here: These statistical tests are used to evaluate performance differences; understanding their purpose ensures correct interpretation of results.
  - Quick check question: When would you use a McNemar's test versus a paired t-test in this context?

## Architecture Onboarding

- Component map:
  Base LLM (Llama3-8B-Instruct or Mistral-Instruct-v2) -> SFT trainer (Hugging Face transformers) -> DPO trainer (Hugging Face trl or custom code) -> Evaluation pipeline (test sets, metrics like F1, accuracy, Likert scores) -> Data split manager (train/eval/dev/test)
- Critical path:
  1. Prepare datasets with prompts, gold-standard responses, and rejected responses (for DPO)
  2. Run SFT fine-tuning, select best model using dev set
  3. Use SFT model as base for DPO fine-tuning
  4. Evaluate both SFT and DPO models on test set
  5. Perform statistical tests to compare results
- Design tradeoffs:
  - Using smaller models (8B) to reduce cost vs. potentially better performance with larger models
  - Collecting rejected examples manually vs. generating them synthetically
  - Full fine-tuning vs. parameter-efficient methods (not explored here)
- Failure signatures:
  - DPO performance worse than SFT: likely due to poor quality or uninformative rejected examples
  - Both methods fail on numeric tasks: suggests the task requires reasoning beyond pattern matching
  - Overfitting on small datasets: monitor dev set performance during training
- First 3 experiments:
  1. Run SFT on text classification task to confirm baseline improvement over base model
  2. Run DPO on same task to verify no degradation (or slight improvement)
  3. Run DPO on clinical reasoning task to observe significant accuracy gain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do DPO and SFT performance differ across larger language models (above 10B parameters) for clinical tasks?
- Basis in paper: [inferred] The authors explicitly note they didn't evaluate models larger than 10B parameters due to cost constraints and suggest their findings may generalize to larger models
- Why unresolved: The study only tested Llama3-8B and Mistral-Instruct-v2, leaving uncertainty about whether observed patterns hold for models like GPT-4 or Claude-3
- What evidence would resolve it: Systematic testing of DPO and SFT performance across a range of larger models (30B, 70B+ parameters) using the same clinical tasks and datasets

### Open Question 2
- Question: What is the minimum dataset size required for DPO to outperform SFT in clinical applications?
- Basis in paper: [explicit] The authors used datasets under 6,000 examples and noted their work reflects "realistic data constraints of medicine," but didn't systematically test minimum dataset requirements
- Why unresolved: The study used datasets of 1,200-5,161 examples without testing whether smaller datasets (500, 1,000, or 2,000 examples) would yield similar DPO advantages
- What evidence would resolve it: Comparative testing of DPO vs SFT across multiple clinical tasks using progressively smaller datasets to identify the breakpoint where DPO stops providing benefits

### Open Question 3
- Question: How does DPO performance vary across different types of clinical reasoning tasks beyond diagnosis and treatment selection?
- Basis in paper: [explicit] The study tested only "Clinical Reasoning" using MedQA questions focused on diagnosis and treatment selection
- Why unresolved: The paper doesn't explore whether DPO's advantages extend to other reasoning domains like differential diagnosis generation, clinical decision-making under uncertainty, or ethical reasoning
- What evidence would resolve it: Testing DPO and SFT across diverse clinical reasoning tasks including prognostic reasoning, treatment prioritization, and clinical judgment scenarios

## Limitations
- Small rejected example sample size (N=500 per task) may not represent full error space
- Only 8B parameter models tested, limiting generalizability to larger models
- Manual grading introduces potential inter-rater variability in clinical task evaluation
- English-language clinical tasks limit applicability to other languages or clinical contexts

## Confidence
**High Confidence:**
- DPO improves clinical reasoning accuracy from 28% to 36% for Llama3 (p=0.003)
- DPO improves triage urgency classification from F1 0.79 to 0.91 (p<0.001)
- DPO improves summarization Likert ratings from 4.21 to 4.34 (p<0.001)

**Medium Confidence:**
- SFT alone is sufficient for simple text classification tasks
- DPO's improvement stems from learning nuanced patterns rather than simple word associations

**Low Confidence:**
- DPO's performance improvements will generalize to larger models and different clinical domains

## Next Checks
1. **Rejected Example Quality Analysis**: Conduct an ablation study testing DPO performance with varying qualities and quantities of rejected examples (e.g., 100 vs. 500 vs. 1000 examples, synthetic vs. human-generated) to establish the minimum effective sample size and quality threshold.

2. **Cross-Domain Generalization Test**: Apply the same SFT and DPO approach to a different clinical specialty (e.g., radiology reports or pathology findings) to assess whether performance improvements transfer across medical domains.

3. **Larger Model Scaling Study**: Repeat the experiments with 70B parameter models to determine if DPO's relative advantage over SFT increases, decreases, or remains constant as model capacity increases.