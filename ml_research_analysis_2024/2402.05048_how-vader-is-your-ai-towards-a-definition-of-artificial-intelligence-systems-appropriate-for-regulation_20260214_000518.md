---
ver: rpa2
title: How VADER is your AI? Towards a definition of artificial intelligence systems
  appropriate for regulation
arxiv_id: '2402.05048'
source_url: https://arxiv.org/abs/2402.05048
tags:
- systems
- regulation
- techniques
- definition
- approaches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the VADER framework to validate the appropriateness
  of AI definitions for regulation. The framework scores definitions based on six
  premises covering technology neutrality, technical unambiguity, objectivity, and
  correct scope delimitation (excluding non-ICT works and non-AI ICT techniques while
  including all AI techniques).
---

# How VADER is your AI? Towards a definition of artificial intelligence systems appropriate for regulation

## Quick Facts
- arXiv ID: 2402.05048
- Source URL: https://arxiv.org/abs/2402.05048
- Authors: Leonardo C. T. Bezerra; Alexander E. I. Brownlee; Luana Ferraz Alvarenga; Renan Cipriano Moioli; Thais Vasconcelos Batista
- Reference count: 40
- One-line primary result: Major AI regulation proposals from US, UK, EU, and Brazil all fail to achieve "appropriate for regulation" scores due to overly broad definitions risking overregulation

## Executive Summary
This paper introduces the VADER framework to evaluate the appropriateness of AI definitions for regulatory purposes. The framework systematically scores proposed AI definitions against six key premises covering technology neutrality, technical unambiguity, objectivity, and correct scope delimitation. When applied to major AI regulation proposals from the US, UK, EU, and Brazil, none achieve an "appropriate for regulation" score, with all showing critical risks of overregulation due to overly broad definitions that could affect non-AI systems and works from other fields.

## Method Summary
The VADER framework evaluates AI definitions by scoring them against six premises that should underlie regulatory definitions: technology neutrality, technical unambiguity, objectivity, exclusion of non-ICT works, exclusion of non-AI ICT techniques, and inclusion of all AI techniques. A representative dataset of AI, non-AI ICT, and non-ICT examples provides concrete test cases to identify scope issues. The framework maps violations to specific risk categories (critical risk or revision needed) with targeted remediation actions, and is implemented as a public web application for policy evaluation.

## Key Results
- All evaluated AI regulation proposals from US, UK, EU, and Brazil failed to achieve "appropriate for regulation" scores
- Every proposal showed critical risks of overregulation due to overly broad definitions capturing non-AI systems
- The framework provides specific recommendations for regulators to improve their AI definitions and prevent unintended consequences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The VADER framework uses a multi-premise scoring system to systematically identify scope-related risks in AI definitions
- Mechanism: The framework scores proposed AI definitions against six premises covering technology neutrality, technical unambiguity, objectivity, exclusion of non-ICT works, exclusion of non-AI ICT systems, and inclusion of all AI techniques. Each premise violation maps to a specific risk category with targeted remediation actions
- Core assumption: Scope misdefinition in AI regulation can be effectively detected and corrected through systematic evaluation against domain-specific criteria
- Evidence anchors:
  - [abstract] "Our online, publicly-available VADER framework scores the coverage of premises that should underlie AI definitions for regulation, labeling definitions as (i) appropriate for regulation; (ii) revision needed, or; (iii) concrete risk"
  - [section 3.3] "To assist in AI policy-making, our proposed framework presents three possible outcomes that indicate to regulators how to address their proposed AI definitions"

### Mechanism 2
- Claim: The representative dataset provides concrete examples that reveal definitional ambiguity and scope overreach
- Mechanism: By including examples from AI techniques, non-AI ICT systems, and non-ICT fields, the dataset creates a reference set against which definitions can be tested. Definitions that unintentionally include non-AI examples reveal critical risks of overregulation
- Evidence anchors:
  - [section 3.1] "To assess scope, we propose a dataset of representative examples comprising (i) AI techniques and approaches; (ii) non-AI ICT techniques, approaches, and systems, and; (iii) non-ICT works, which concretely help identify scope issues with proposed AI definitions"
  - [table 6-8] Provides concrete examples from optimization algorithms, statistical methods, and engineering systems that would be incorrectly captured by broad definitions

### Mechanism 3
- Claim: The framework's premises are grounded in successful technology regulation patterns, particularly data privacy frameworks
- Mechanism: Premises 1-2 directly replicate principles from GDPR and similar regulations (technology neutrality, technical unambiguity), while premises 3-6 address AI-specific scope challenges identified through literature review
- Evidence anchors:
  - [section 3.2] "Premises aim to reproduce principles observed in successful worldwide data privacy regulation and to correctly delimit the scope of the regulation"
  - [section 2.3] Discusses GDPR's technology-neutral approach and how it influenced subsequent regulations like CCPA and LGPD

## Foundational Learning

- Concept: ICT vs AI distinction
  - Why needed here: The framework's core purpose is distinguishing AI from non-AI ICT systems to prevent overregulation
  - Quick check question: Can you identify which of these systems would be incorrectly regulated under a broad AI definition: a database management system, a neural network, a compiler, or a robotic arm?

- Concept: Technology neutrality principle
  - Why needed here: Premise 1 requires definitions to avoid favoring specific technologies, preventing market distortion
  - Quick check question: Why would defining AI systems as "deep learning systems" violate technology neutrality, while "machine-based systems using computational approaches" would not?

- Concept: Rational behavior perspective in AI
  - Why needed here: Premise 3 requires objective, behavior-based definitions to avoid subjective concepts like consciousness
  - Quick check question: How does the rational behavior perspective differ from human behavior perspectives in AI definitions, and why is objectivity important for regulation?

## Architecture Onboarding

- Component map:
  - Dataset repository -> Premise evaluation engine -> Risk categorization module -> Remediation guide -> Assessment interface

- Critical path:
  1. User submits AI definition for evaluation
  2. System evaluates against all 6 premises
  3. Violations are mapped to risk categories
  4. Specific remediation actions are generated
  5. Results are displayed with supporting examples

- Design tradeoffs:
  - Breadth vs specificity: Broader definitions are more inclusive but risk overregulation; narrower definitions are more precise but may exclude emerging AI techniques
  - Automation vs expert review: Fully automated scoring may miss nuanced violations; expert review is slower but more accurate
  - Static vs dynamic examples: Static examples are stable but may become outdated; dynamic examples require ongoing curation

- Failure signatures:
  - False negatives: Definitions pass scoring but still capture non-AI systems in practice
  - False positives: Definitions fail scoring but would not cause overregulation in implementation
  - Example drift: Representative examples become outdated as technology evolves

- First 3 experiments:
  1. Test the framework on historical AI definitions to see if it correctly identifies known overregulation risks
  2. Submit intentionally flawed definitions to verify that the system catches common misdefinition patterns
  3. Apply the framework to emerging AI regulation proposals to validate real-world applicability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the VADER framework be extended to evaluate AI regulation proposals from additional countries and regions beyond those assessed in the paper?
- Basis in paper: [inferred] The paper discusses applying the framework to major regulation proposals from the US, UK, EU, and Brazil, but acknowledges the need to expand to a greater number of countries and parts of the world
- Why unresolved: The paper focuses on a limited set of countries, leaving the framework's applicability and effectiveness in diverse regulatory contexts unexplored
- What evidence would resolve it: Testing the VADER framework on a broader range of AI regulation proposals from various countries and regions, documenting its performance and identifying any limitations or necessary adaptations

### Open Question 2
- Question: What specific mechanisms can be implemented to ensure ongoing updates and maintenance of the VADER framework and its representative example dataset to keep pace with the rapidly evolving AI landscape?
- Basis in paper: [inferred] The paper proposes a framework and dataset but does not detail mechanisms for their continuous improvement and adaptation to new AI techniques and regulatory approaches
- Why unresolved: AI technology and regulatory approaches are constantly evolving, requiring the VADER framework and dataset to be regularly updated to remain relevant and effective
- What evidence would resolve it: Developing a detailed plan for regular updates to the VADER framework and dataset, including procedures for incorporating new AI techniques, regulatory approaches, and stakeholder feedback

### Open Question 3
- Question: How can the VADER framework be adapted to evaluate AI regulation proposals that focus on specific sectors or applications, such as healthcare, finance, or autonomous vehicles?
- Basis in paper: [inferred] The paper assesses general AI regulation proposals, but many real-world regulations target specific sectors or applications with unique challenges and requirements
- Why unresolved: Sector-specific AI regulations may have different priorities and scope than general regulations, requiring the VADER framework to be tailored to address these nuances
- What evidence would resolve it: Modifying the VADER framework to incorporate sector-specific criteria and evaluating its effectiveness on a set of AI regulation proposals targeting specific industries or applications

## Limitations
- The framework's effectiveness depends heavily on the completeness and representativeness of the example dataset, which may become outdated as technology evolves
- Scoring thresholds for determining "appropriate," "revision needed," or "critical risk" outcomes are not explicitly defined, potentially leading to inconsistent evaluations
- The framework focuses on scope issues but may not capture all relevant regulatory considerations such as enforcement mechanisms or ethical frameworks

## Confidence
- High Confidence: The framework's core mechanism of using premises-based evaluation is sound and aligns with established regulatory principles (premises 1-2 directly mirror GDPR's approach)
- Medium Confidence: The representative dataset effectively identifies scope issues for current AI technologies, but may require ongoing updates
- Medium Confidence: The risk categorization system appropriately maps violations to actionable remediation steps

## Next Checks
1. Test the framework on a broader range of AI definitions from different regulatory contexts to assess generalizability beyond the four jurisdictions studied
2. Conduct expert review with AI practitioners and legal scholars to validate that the premises capture all relevant scope boundaries
3. Implement longitudinal tracking to monitor whether identified "appropriate" definitions remain valid as AI technology evolves over time