---
ver: rpa2
title: Data-driven Coreference-based Ontology Building
arxiv_id: '2410.17051'
source_url: https://arxiv.org/abs/2410.17051
tags:
- edges
- graph
- nodes
- ontology
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of automatically building comprehensive
  ontologies from scientific literature, specifically in the biomedical domain. The
  core method leverages the topology of a coreference graph, where nodes are textual
  phrases and edges represent co-occurrence in coreference chains.
---

# Data-driven Coreference-based Ontology Building

## Quick Facts
- arXiv ID: 2410.17051
- Source URL: https://arxiv.org/abs/2410.17051
- Reference count: 13
- Over 5 million hierarchical edges with 84.3% recall when compared to SnomedCT

## Executive Summary
This work addresses the problem of automatically building comprehensive ontologies from scientific literature, specifically in the biomedical domain. The core method leverages the topology of a coreference graph, where nodes are textual phrases and edges represent co-occurrence in coreference chains. By computing betweenness centrality scores, the method assigns directionality to edges to establish hierarchical relationships, identifies identity edges representing aliases, and splits nodes corresponding to multiple distinct concepts. The resulting ontology contains over 5 million hierarchical edges with 84.3% recall when compared to SnomedCT, and 75% precision based on human evaluation.

## Method Summary
The method extracts coreference chains from biomedical literature, filters phrases to remove pronouns, stops, and verbs, then constructs an undirected weighted graph where nodes represent phrases and edges connect co-occurring phrases. Betweenness centrality scores are computed using an approximate algorithm with 500 pivots. Edge directions are assigned from higher to lower centrality nodes, establishing IS-A relationships. Identity edges are identified between nodes with zero betweenness centrality. Ambiguous nodes are split using semantic embeddings and clustering. The final ontology is obtained by filtering noisy edges using PMI scores.

## Key Results
- Over 5 million hierarchical edges extracted from 30 million biomedical abstracts
- 84.3% recall when compared to SnomedCT ontology
- 75% precision based on human evaluation
- Identity edge clusters show reasonable homogeneity with entropy of 0.406 and Adjusted Rand Index of 0.387 compared to UMLS

## Why This Works (Mechanism)

### Mechanism 1
- Claim: General concepts appear in more coreference chains than specific ones, leading to higher betweenness centrality.
- Mechanism: The graph structure captures information flow where general concepts act as bridges between many specific concepts, accumulating higher centrality scores.
- Core assumption: Phrases that denote more general concepts co-occur with a wider variety of other phrases across many different coreference chains.
- Evidence anchors:
  - [abstract]: "We exploit the dynamics of phrase co-occurrence within the graph, observing a correlation between a phrase's contribution to information flow and its level of generality."
  - [section]: "A major observation is that phrases that denote concepts that are higher-up in the IS-A hierarchy (are more general) co-occur in many different coreference clusters, and with many different phrases, while phrases that are more specific belong in only few clusters, with a more restricted set of phrases"
- Break condition: If coreference resolution produces systematic errors that misclassify specific terms as general, or if the corpus contains many short documents with limited phrase variety.

### Mechanism 2
- Claim: Identity edges (synonyms/aliases) can be identified by nodes with zero betweenness centrality.
- Mechanism: Nodes with zero betweenness centrality do not bridge different concepts in the graph, indicating they are alternative names for the same concept.
- Core assumption: Phrases that are aliases of the same concept will co-occur frequently within coreference chains but will not bridge to other concepts.
- Evidence anchors:
  - [section]: "Roughly 70% of the nodes have a betweenness value of 0, suggesting they function as leaves in the DAG hierarchy. This is while 240,000 of the edges in the graph connect such nodes. Consulting a random sample of edges reveal that they indeed connect aliases of the same concept."
  - [abstract]: "We then use the graph structure and the betweeness centrality measure to distinguish between edges denoting hierarchy, identity and noise"
- Break condition: If the coreference resolution creates many false positives connecting unrelated phrases, or if true aliases have different betweenness values due to corpus structure.

### Mechanism 3
- Claim: Directionality of hierarchical edges can be assigned using betweenness centrality ordering.
- Mechanism: The more general concept (higher betweenness) points to the more specific concept (lower betweenness), establishing IS-A relationships.
- Core assumption: The betweenness centrality ranking accurately reflects the hierarchical level of concepts in the domain.
- Evidence anchors:
  - [section]: "Therefore, the first step in establishing the edge direction is to compute the betweeness centrality score of each node, and assign the direction of an edge to be from the the node with higher centrality (more general nodes) to one with lower centrality."
  - [abstract]: "assign directionality to edges denoting hierarchy"
- Break condition: If specific named entities (like "COVID-19") appear more frequently than their general categories, or if common nodes have artificially inflated centrality.

## Foundational Learning

- Concept: Graph betweenness centrality
  - Why needed here: Used to distinguish hierarchical edges from identity edges and assign directionality to the ontology structure.
  - Quick check question: If node A has betweenness centrality 50 and node B has 20, which node is more general in the ontology?

- Concept: Coreference resolution
  - Why needed here: Provides the fundamental data for constructing the phrase co-occurrence graph that underlies the ontology.
  - Quick check question: What information does a coreference chain provide about the relationship between phrases in a document?

- Concept: Ontology as directed acyclic graph (DAG)
  - Why needed here: The target structure for the extracted knowledge, where edges represent IS-A relationships.
  - Quick check question: In a biomedical ontology DAG, if "lung disease" points to "disease", what does this relationship mean?

## Architecture Onboarding

- Component map: Coreference resolution engine → Phrase filtering and normalization → Graph construction (nodes + weighted edges) → Betweenness centrality computation → Edge classification (hierarchy/identity/noise) → Direction assignment → Node splitting → Ontology output

- Critical path: The entire pipeline must complete successfully, but the bottleneck is betweenness centrality computation on the large graph (3M+ nodes).

- Design tradeoffs:
  - Approximate centrality (500 pivots) vs exact computation: saves time but may introduce ordering errors
  - String-based vs semantic clustering: string is faster but may miss polysemy; semantic is more accurate but requires embeddings
  - PMI thresholding for noise vs keeping all edges: more aggressive filtering reduces noise but may remove valid edges

- Failure signatures:
  - Low precision in hierarchy evaluation: suggests centrality ordering is incorrect
  - High entropy in identity clusters: suggests alias detection is failing
  - Unexpected edge directions: suggests named entities are being misclassified as general terms

- First 3 experiments:
  1. Run the pipeline on a small subset (1000 abstracts) with exact betweenness computation to verify edge directions match expectations
  2. Test the node splitting algorithm on a known ambiguous string (like "IL") to verify it correctly identifies multiple senses
  3. Evaluate PMI filtering by manually checking whether removed edges are indeed noise or valid relationships

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of the coreference-based ontology building method scale when applied to scientific domains outside of biomedical literature?
- Basis in paper: [inferred] The paper focuses exclusively on biomedical literature (30 million PubMed abstracts) and mentions that the method is "scalable and can be implemented for networks of varying sizes" and has "generality, allowing for easy adaptation to other fields."
- Why unresolved: The paper only demonstrates results on biomedical data, leaving the performance characteristics in other scientific domains untested.
- What evidence would resolve it: Applying the method to large corpora from other scientific domains (e.g., physics, computer science, social sciences) and comparing precision/recall metrics with domain-specific ontologies.

### Open Question 2
- Question: What is the optimal number of pivots (k) for betweenness centrality approximation that balances computational efficiency and ontology accuracy?
- Basis in paper: [explicit] The paper reports using 500 pivots works well, achieving "91.3 for the direction of edges and 86.7 for the accuracy of the connected leaves," but acknowledges that "Increasing k enhances accuracy but may also extend computation time."
- Why unresolved: The paper presents empirical results for k=500 without exploring the full trade-off curve between k values and performance metrics.
- What evidence would resolve it: Systematic experiments varying k from 50 to 10,000 while measuring precision, recall, and runtime to identify the point of diminishing returns.

### Open Question 3
- Question: How robust is the ontology to variations in the coreference resolution algorithm's performance and its impact on the final hierarchical structure?
- Basis in paper: [inferred] The paper uses a specific coreference resolution algorithm (Otmazgin et al., 2023) but acknowledges that "The propagation of errors or inconsistencies from the corpus into the ontology might compromise its quality and accuracy."
- Why unresolved: The paper doesn't evaluate how errors in coreference resolution propagate through the ontology building pipeline or test alternative coreference models.
- What evidence would resolve it: Running the full pipeline with different coreference resolution algorithms of varying quality and measuring how precision/recall metrics change.

## Limitations

- The method may struggle with domain-specific terminology, polysemy in scientific terms, and cross-document coreference resolution accuracy.
- The approximate betweenness centrality computation with 500 pivots may introduce errors in edge direction assignment, particularly for nodes with similar centrality scores.
- The string-based identity edge clustering ignores semantic meaning and may fail on polysemous terms or synonyms with different surface forms.

## Confidence

- **High Confidence**: The basic graph construction approach and betweenness centrality computation methodology are sound and well-established. The overall pipeline architecture is reasonable.
- **Medium Confidence**: The edge direction assignment mechanism based on centrality ordering works for many cases but may fail with named entities or frequency-biased terms. The PMI-based noise filtering is plausible but thresholds may need tuning.
- **Low Confidence**: The string-based identity edge clustering without semantic consideration, the node splitting algorithm using k-NN with fixed k=3, and the specific parameter choices (500 pivots, PMI threshold) lack thorough justification or sensitivity analysis.

## Next Checks

1. **Edge Direction Verification**: Manually verify the directionality of 100 randomly selected hierarchical edges from the output ontology to assess whether higher-centrality nodes consistently represent more general concepts than their neighbors.
2. **Named Entity Impact**: Analyze the distribution of centrality scores for named entities versus general terms in the corpus to quantify how named entities affect the hierarchical ordering and whether special handling is needed.
3. **Parameter Sensitivity**: Systematically vary the number of pivots for approximate betweenness computation (100, 500, 1000, exact) and the PMI threshold for noise filtering to measure their impact on ontology quality metrics.