---
ver: rpa2
title: Interpretable Concept-Based Memory Reasoning
arxiv_id: '2407.15527'
source_url: https://arxiv.org/abs/2407.15527
tags:
- rule
- rules
- concept
- task
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Concept-based Memory Reasoner (CMR), a novel
  Concept Bottleneck Model designed to provide human-understandable and provably-verifiable
  task prediction processes. CMR models each task prediction as a neural selection
  mechanism over a memory of learnable logic rules, followed by a symbolic evaluation
  of the selected rule.
---

# Interpretable Concept-Based Memory Reasoning

## Quick Facts
- arXiv ID: 2407.15527
- Source URL: https://arxiv.org/abs/2407.15527
- Authors: David Debot; Pietro Barbiero; Francesco Giannini; Gabriele Ciravegna; Michelangelo Diligenti; Giuseppe Marra
- Reference count: 40
- Key outcome: CMR achieves near-black-box accuracy while providing interpretable, verifiable rule-based reasoning

## Executive Summary
This paper introduces Concept-based Memory Reasoner (CMR), a novel Concept Bottleneck Model designed to provide human-understandable and provably-verifiable task prediction processes. CMR models each task prediction as a neural selection mechanism over a memory of learnable logic rules, followed by a symbolic evaluation of the selected rule. This approach enables global interpretability and verification of task prediction properties. Experimental results demonstrate that CMR achieves near-black-box accuracy on various datasets, discovers meaningful rules consistent with ground truths, enables rule interventions, and allows for pre-deployment verification of desired properties.

## Method Summary
CMR is a concept bottleneck model that represents task predictions as a disjunction of conjunctions (disjunctive theory). It consists of a concept encoder, rule selector, rulebook, and task predictor. The concept encoder maps raw features to interpretable concepts, while the rule selector is a neural network that chooses from a set of jointly-learned logic rules. The task predictor then symbolically evaluates the selected rule on the concept predictions. The model is trained using maximum likelihood with a regularization term to encourage rule representativeness.

## Key Results
- CMR achieves near-black-box accuracy on MNIST+, CelebA, CUB, and other datasets
- The model discovers meaningful rules consistent with ground truths
- CMR enables rule interventions and pre-deployment verification of desired properties
- Experimental results show competitive performance compared to existing CBMs and black-box models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CMR uses a neural selection mechanism over a memory of logic rules followed by symbolic evaluation to provide global interpretability and verification.
- Mechanism: The model represents task prediction as a disjunction of conjunctions (disjunctive theory), where each conjunction is a logic rule that can be inspected and verified before deployment.
- Core assumption: The rule selection process and symbolic evaluation together preserve the expressiveness needed for complex tasks while maintaining interpretability.
- Evidence anchors:
  - [abstract]: "Our approach is to model each task prediction as a neural selection mechanism over a memory of learnable logic rules, followed by a symbolic evaluation of the selected rule."
  - [section]: "CMR's task prediction is the composition of a (neural) rule selector and the symbolic evaluation of the selected rule."
  - [corpus]: Weak - no direct citations, but the concept of "neural-symbolic" integration is emerging in XAI literature.

### Mechanism 2
- Claim: CMR achieves near-black-box accuracy while maintaining interpretability.
- Mechanism: The neural rule selector learns to choose from a set of jointly-learned logic rules, allowing the model to represent complex decision boundaries while keeping the reasoning process transparent.
- Core assumption: The combination of neural selection and symbolic evaluation can approximate the performance of pure neural networks while providing explanations.
- Evidence anchors:
  - [abstract]: "Experimental results demonstrate that CMR achieves near-black-box accuracy on various datasets"
  - [section]: "CMR is a universal binary classifier [13] if nR ≥ 3" and the proof shows it can achieve the same accuracy as neural networks.
  - [corpus]: Weak - limited direct citations, but the concept of "universal approximation" is well-established in neural network theory.

### Mechanism 3
- Claim: CMR allows for pre-deployment verification of desired properties.
- Mechanism: The learned rules form a propositional logic formula that can be automatically verified using formal verification tools.
- Core assumption: The propositional logic representation of the rules is sufficient to express the desired properties for verification.
- Evidence anchors:
  - [abstract]: "The presence of an explicit memory and the symbolic evaluation allow domain experts to inspect and formally verify the validity of certain global properties of interest for the task prediction process."
  - [section]: "Being able to verify properties prior to deployment of the model strongly sets CMR apart from existing models, where verification tasks can only be applied at prediction time."
  - [corpus]: Weak - limited direct citations, but the concept of "formal verification" in AI is an active research area.

## Foundational Learning

- Concept: Concept Bottleneck Models (CBMs)
  - Why needed here: Understanding CBMs is essential to grasp why CMR is an improvement. CBMs use interpretable concepts as intermediate representations, but their task predictors are often not fully interpretable.
  - Quick check question: What is the main limitation of existing CBMs that CMR aims to address?

- Concept: Neural-symbolic integration
  - Why needed here: CMR combines neural networks (for rule selection) with symbolic logic (for evaluation). Understanding this integration is key to understanding how CMR achieves both accuracy and interpretability.
  - Quick check question: How does CMR use neural networks and symbolic logic together to make predictions?

- Concept: Formal verification
  - Why needed here: CMR allows for pre-deployment verification of properties using formal verification tools. Understanding formal verification is essential to grasp the significance of this capability.
  - Quick check question: What is the advantage of being able to verify properties of a model before deployment?

## Architecture Onboarding

- Component map: Input → Concept encoder → Rule selector → Selected rule → Symbolic evaluation → Task prediction

- Critical path: Input → Concept encoder → Rule selector → Selected rule → Symbolic evaluation → Task prediction

- Design tradeoffs:
  - Rulebook size vs. interpretability: Larger rulebooks can capture more complex decision boundaries but may be harder to interpret.
  - Neural network complexity vs. symbolic evaluation: More complex neural networks may improve accuracy but could make the symbolic evaluation harder to understand.
  - Regularization strength vs. rule representativeness: Stronger regularization encourages more representative rules but may limit the model's ability to learn complex decision boundaries.

- Failure signatures:
  - Low accuracy: May indicate that the rulebook is too small or the neural network is not complex enough.
  - Poor interpretability: May indicate that the rules are too complex or not representative of the concept predictions.
  - Verification failures: May indicate that the properties to be verified cannot be expressed in propositional logic or the verification tools cannot handle the complexity.

- First 3 experiments:
  1. Train CMR on a simple dataset (e.g., MNIST+) and inspect the learned rules to ensure they are interpretable and representative.
  2. Evaluate CMR's accuracy on a more complex dataset (e.g., CelebA) and compare it to black-box models to ensure it achieves near-black-box accuracy.
  3. Verify a simple property (e.g., "never predict class 'apple' when the concept 'blue' is active") on a dataset (e.g., C-MNIST) to ensure pre-deployment verification is possible.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CMR scale with the size of the rule memory (nR) in terms of both accuracy and interpretability?
- Basis in paper: [explicit] Section 6.3.4 mentions an experiment investigating the robustness of CMR's accuracy with respect to the number of rules hyperparameter (nR).
- Why unresolved: The paper only shows that similar levels of accuracy are obtained regardless of the chosen nR value, but does not explore the relationship between nR, accuracy, and interpretability in detail.
- What evidence would resolve it: Additional experiments analyzing the trade-off between rule memory size, task accuracy, concept accuracy, and interpretability metrics (e.g., rule complexity, coverage) would provide insights into the optimal nR for different tasks and datasets.

### Open Question 2
- Question: Can CMR be extended to handle negative reasoning and provide explanations for why a certain class was not predicted?
- Basis in paper: [inferred] Section 4.2 mentions that CMR focuses on positive-only explanations, while negative-reasoning explanations have not been explored yet.
- Why unresolved: The paper does not investigate the potential for extending CMR to provide explanations for negative predictions, which could be valuable for understanding model decisions and identifying potential biases.
- What evidence would resolve it: Developing and evaluating an extension of CMR that can generate explanations for negative predictions, along with a user study to assess the usefulness and interpretability of these explanations, would address this question.

### Open Question 3
- Question: How does CMR perform in safety-critical domains where formal verification of properties is crucial?
- Basis in paper: [explicit] Section 6.3 discusses the ability of CMR to allow for post-training verification of desired global properties, and Section 8 mentions that the verification capabilities of CMR will be tested on more realistic, safety-critical domains.
- Why unresolved: The paper only provides a preliminary experiment on verifying semantic consistency properties in MNIST+ and CelebA datasets, but does not explore the application of CMR in safety-critical domains with more complex and stringent verification requirements.
- What evidence would resolve it: Applying CMR to safety-critical domains (e.g., medical diagnosis, autonomous driving) and evaluating its performance in terms of task accuracy, interpretability, and the ability to verify safety-critical properties would demonstrate its potential in these domains.

## Limitations
- The paper lacks detailed architectural specifications for critical components, making exact reproduction challenging
- Most claims about interpretability and verification are supported primarily by conceptual arguments rather than extensive empirical validation
- The rule-based approach may struggle with highly complex, non-linear decision boundaries where neural networks excel

## Confidence
- High Confidence: The basic feasibility of CMR's architecture and its ability to achieve competitive accuracy on benchmark datasets
- Medium Confidence: Claims about rule interpretability and meaningfulness, as these rely on subjective evaluation
- Low Confidence: Assertions about pre-deployment verification capabilities, which lack detailed demonstration of the verification process

## Next Checks
1. Implement a simplified CMR on a toy dataset to verify the rule selection and symbolic evaluation mechanism works as described
2. Test CMR's verification capabilities on a controlled dataset with known, verifiable properties
3. Conduct ablation studies to quantify the impact of the regularization term on rule interpretability and model performance