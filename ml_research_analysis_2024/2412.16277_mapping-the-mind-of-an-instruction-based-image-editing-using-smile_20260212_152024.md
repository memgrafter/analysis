---
ver: rpa2
title: Mapping the Mind of an Instruction-based Image Editing using SMILE
arxiv_id: '2412.16277'
source_url: https://arxiv.org/abs/2412.16277
tags:
- image
- editing
- arxiv
- text
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SMILE, a model-agnostic interpretability method
  that uses statistical distances to generate visual heatmaps, highlighting how specific
  words in textual instructions influence image editing models like Instruct-Pix2Pix,
  Img2Img-Turbo, and Diffusers-Inpaint. By applying perturbations to the input text
  and comparing the resulting images using Wasserstein distance in DINOv2 embedding
  space, SMILE quantifies each word's impact via weighted linear regression.
---

# Mapping the Mind of an Instruction-based Image Editing using SMILE

## Quick Facts
- arXiv ID: 2412.16277
- Source URL: https://arxiv.org/abs/2412.16277
- Reference count: 40
- Primary result: SMILE achieves 0.78-0.96 accuracy and 0.85 Jaccard index in interpretability for instruction-based image editing models

## Executive Summary
This paper proposes SMILE, a model-agnostic interpretability method for instruction-based image editing models. SMILE uses statistical distances, specifically Wasserstein distance in DINOv2 embedding space, to generate visual heatmaps that highlight how specific words in textual instructions influence image editing models like Instruct-Pix2Pix, Img2Img-Turbo, and Diffusers-Inpaint. By applying perturbations to the input text and comparing resulting images using weighted linear regression, SMILE quantifies each word's impact on the final image output. The approach aims to enhance transparency and trust in AI-driven image editing by providing clear explanations of how textual instructions translate to visual changes.

## Method Summary
SMILE generates perturbed prompts by including or excluding individual words from the original instruction text. For each perturbed prompt, the image editing model generates an output image. These images are then embedded using DINOv2 to capture semantic content, and Wasserstein distance is computed between each perturbed image and the original image. Text similarity is measured using Word Mover's Distance. A weighted linear regression model is trained where text perturbation vectors are predictors and Wasserstein distances are responses, with weights derived from text similarity. This regression produces word importance scores that are visualized as heatmaps, showing which words most influence the image editing outcome.

## Key Results
- Accuracy metrics (ATT ACC, ATT F1, ATT AUROC) range from 0.78 to 0.96 across models
- Stability measured by Jaccard index shows 0.85 consistency in heatmaps
- Fidelity metrics (R²) range from 0.12 to 0.95, indicating strong surrogate model performance
- SMILE demonstrates robustness against adversarial perturbations compared to LIME

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SMILE generates visual heatmaps that quantify each word's influence by comparing perturbed and original images using Wasserstein distance in DINOv2 embedding space
- Mechanism: The model perturbs the input text by including/excluding individual words, generates images for each variant, and computes a distributional distance (Wasserstein) between each perturbed image and the original. These distances serve as the response variable in a weighted linear regression where the predictors are the perturbed text vectors and weights are derived from text similarity
- Core assumption: Changes in text cause measurable, semantically meaningful changes in image space that are captured by distributional distances; the surrogate model (weighted linear regression) can linearly approximate this mapping
- Evidence anchors:
  - [abstract] "By applying perturbations to the input text and comparing the resulting images using Wasserstein distance in DINOv2 embedding space, SMILE quantifies each word's impact via weighted linear regression."
  - [section] "We use the Wasserstein distance to quantify these differences to compare the distributions in both image and text embeddings."
- Break condition: If text perturbations do not cause detectable distributional shifts in DINOv2 embeddings, or if the regression mapping is non-linear and poorly approximated, the heatmap quantification will fail

### Mechanism 2
- Claim: SMILE improves robustness over LIME by using statistical distances (Wasserstein) rather than cosine similarity, making it less susceptible to adversarial perturbations
- Mechanism: LIME relies on cosine distance for perturbation similarity, which can be gamed by crafting adversarial perturbations that keep cosine distance low but alter model predictions. SMILE's use of Wasserstein distance measures full distributional differences, capturing subtle semantic changes that cosine may miss, thereby providing more stable explanations
- Core assumption: Wasserstein distance is more sensitive to distributional changes than cosine similarity and is less easily manipulated by adversarial perturbations
- Evidence anchors:
  - [abstract] "Unlike LIME, which can be vulnerable to adversarial attacks that distort interpretability, SMILE is more resistant to manipulation."
  - [section] "SMILE, in particular, uses the Wasserstein distance to calculate text distances, making it much more computationally expensive than LIME and Bay-LIME."
- Break condition: If the computational cost of Wasserstein distance outweighs robustness gains, or if Wasserstein distance itself can be gamed, SMILE's advantage may disappear

### Mechanism 3
- Claim: Using DINOv2 embeddings allows SMILE to capture semantic content rather than pixel-level changes, enabling model-agnostic interpretability across different image editing models
- Mechanism: DINOv2 is a self-supervised feature extractor that produces high-quality, semantic image embeddings. By computing distances in this embedding space, SMILE compares conceptual image content instead of raw pixel similarity, making the interpretability framework independent of the specific generative model used
- Core assumption: DINOv2 embeddings encode the relevant semantic features for comparing image edits across different models; these embeddings are comparable regardless of the editing model
- Evidence anchors:
  - [section] "We use the DINOv2 model to generate high-quality, semantic embeddings... Embedding images in DINOv2's space enables us to capture visual and conceptual content, allowing for more meaningful comparisons."
  - [section] "This project aims to develop a model-agnostic, explainable framework for interpreting image edits independent of specific models."
- Break condition: If DINOv2 embeddings are not semantically aligned across different editing models, or if a different embedding space is needed for specific domains, model-agnosticism breaks down

## Foundational Learning

- Concept: Wasserstein distance (Earth Mover's Distance)
  - Why needed here: Provides a distributional distance metric that captures full shape differences between image embeddings, more sensitive than cosine similarity for semantic changes
  - Quick check question: What is the computational complexity of Wasserstein-2 distance for d-dimensional Gaussians, and why is it higher than cosine similarity?

- Concept: Weighted linear regression with sample weights
  - Why needed here: Maps text perturbation vectors to image distance responses while accounting for varying similarity of perturbations to the original prompt
  - Quick check question: How does the Gaussian kernel weighting scheme affect the influence of each perturbation sample in the regression?

- Concept: Model-agnostic interpretability
  - Why needed here: Ensures the explanation framework works across different instruction-based image editing models without retraining
  - Quick check question: Why is DINOv2's self-supervised nature crucial for achieving model-agnostic interpretability in this context?

## Architecture Onboarding

- Component map: Text perturbation generator -> Instruct image editing model -> DINOv2 feature extractor -> Wasserstein distance calculator -> Text similarity calculator -> Weighted linear regression trainer -> Heatmap visualizer

- Critical path: Text perturbation → Image generation → DINOv2 embedding → Wasserstein distance → Weighted regression → Heatmap

- Design tradeoffs:
  - Wasserstein distance vs cosine similarity: higher robustness but higher computational cost (O(Nd³) vs O(Nd))
  - DINOv2 vs model-specific embeddings: better model-agnosticism but may miss model-specific nuances
  - Weighted regression vs other surrogate models: simplicity and interpretability vs potential loss of non-linear relationships

- Failure signatures:
  - Heatmaps show uniform weights → text perturbations not affecting image outputs
  - Extremely high variance across repeated runs → instability in distance or regression computation
  - Low fidelity metrics (R² close to 0) → surrogate model poorly approximates the black-box behavior

- First 3 experiments:
  1. Run SMILE on a simple Instruct-Pix2Pix example with a single keyword change (e.g., "snowy" vs without) and verify the heatmap highlights that word
  2. Compare execution time and heatmap stability between SMILE and LIME on the same prompt and image to observe robustness differences
  3. Test model-agnosticism by applying SMILE to both Instruct-Pix2Pix and Img2Img-Turbo with identical prompts and verify consistent word importance rankings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model handle prompts that include conflicting or ambiguous instructions, such as "make it snowy and sunny"?
- Basis in paper: [inferred] The paper discusses how the model responds to different perturbations of prompts, but does not address how it handles conflicting instructions
- Why unresolved: The paper does not provide specific examples or analysis of how the model processes conflicting or ambiguous instructions
- What evidence would resolve it: Experimental results showing how the model interprets and executes prompts with conflicting instructions would clarify its handling of such scenarios

### Open Question 2
- Question: What is the impact of the number of perturbations on the model's ability to generate accurate and consistent heatmaps?
- Basis in paper: [explicit] The paper mentions that different numbers of perturbations were tested, but does not provide a detailed analysis of their impact on heatmap accuracy
- Why unresolved: The paper does not explore the relationship between the number of perturbations and the quality of the generated heatmaps
- What evidence would resolve it: A comprehensive study comparing heatmaps generated with varying numbers of perturbations would provide insights into the optimal number of perturbations for accurate and consistent results

### Open Question 3
- Question: How does the model's performance vary across different types of image editing tasks, such as object addition, removal, or style transfer?
- Basis in paper: [inferred] The paper discusses the model's performance on various prompts, but does not explicitly categorize the tasks into different types
- Why unresolved: The paper does not provide a detailed breakdown of the model's performance on specific types of image editing tasks
- What evidence would resolve it: A comparative analysis of the model's performance on different types of image editing tasks would highlight its strengths and limitations in various scenarios

## Limitations

- Computational overhead: Wasserstein distance calculations are significantly more expensive than cosine similarity alternatives, potentially limiting scalability
- Semantic alignment assumption: The method assumes DINOv2 embeddings are semantically aligned across different image editing models, which may not hold for all domains
- Linear relationship assumption: The weighted linear regression assumes linear relationships between text perturbations and image changes, potentially missing non-linear effects

## Confidence

- Core interpretability mechanism: Medium
- Adversarial robustness claims: Low
- Model-agnostic claim: Medium

## Next Checks

1. Perform adversarial perturbation analysis comparing SMILE against LIME to verify robustness claims
2. Test SMILE on a diverse set of image editing tasks (portrait, landscape, object removal) to assess domain generalizability
3. Validate heatmap interpretability by conducting user studies where participants verify whether highlighted words correspond to visible image changes