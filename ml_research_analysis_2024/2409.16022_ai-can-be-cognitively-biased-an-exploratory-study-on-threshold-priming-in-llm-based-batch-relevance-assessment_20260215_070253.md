---
ver: rpa2
title: 'AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming in
  LLM-Based Batch Relevance Assessment'
arxiv_id: '2409.16022'
source_url: https://arxiv.org/abs/2409.16022
tags:
- relevance
- threshold
- biases
- priming
- cognitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) are
  influenced by the threshold priming effect during batch relevance assessment, a
  common task in information retrieval (IR). Using 10 topics from the TREC 2019 Deep
  Learning passage track collection, we tested four LLM models (GPT-3.5, GPT-4, LLaMa2-13B,
  and LLaMa2-70B) under different batch configurations.
---

# AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming in LLM-Based Batch Relevance Assessment

## Quick Facts
- arXiv ID: 2409.16022
- Source URL: https://arxiv.org/abs/2409.16022
- Reference count: 40
- Key outcome: LLMs exhibit threshold priming bias in batch relevance assessment, assigning lower scores to later documents when earlier ones have high relevance, consistent with human assessors' behavior.

## Executive Summary
This study investigates whether large language models (LLMs) exhibit threshold priming bias during batch relevance assessment, a common task in information retrieval. Using 10 topics from the TREC 2019 Deep Learning passage track, researchers tested four LLM models (GPT-3.5, GPT-4, LLaMa2-13B, and LLaMa2-70B) under different batch configurations. Results show that LLMs consistently assign lower relevance scores to later documents when earlier ones have high relevance scores, and vice versa. This effect is statistically significant (p < 0.001) and mirrors human assessors' behavior, suggesting that LLMs can inherit cognitive biases from their training data.

## Method Summary
The study filtered the TREC 2019 Deep Learning collection to select 10 topics with at least 12 documents for each relevance level (0-3). For each topic, 20 trials were conducted where documents with relevance score 2 were randomly selected as epilogues, while documents with scores 0 and 3 served as low and high threshold prologues. Two batches were created by concatenating each prologue with the same epilogue, and LLMs were used to judge relevance scores for each document. The analysis compared scores between low and high threshold conditions to detect threshold priming effects.

## Key Results
- LLMs consistently show threshold priming effects, assigning lower relevance scores to later documents when earlier ones have high relevance
- The effect is statistically significant (p < 0.001) across all tested models including GPT-3.5, GPT-4, LLaMa2-13B, and LLaMa2-70B
- Larger models (LLaMa2-70B) show reduced threshold priming compared to smaller models, though the effect varies by topic and batch configuration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs inherit threshold priming bias from training data because their representations are shaped by human-generated text containing sequential judgment patterns.
- Mechanism: The model's token-level attention weights form representations that encode sequential dependency structures similar to how human assessors' judgments are influenced by prior items in a batch.
- Core assumption: The training corpus contains sufficient examples of human relevance judgments in sequence to imprint this bias into model weights.
- Evidence anchors:
  - [abstract] "This effect is statistically significant (p < 0.001) and consistent with human assessors' behavior, demonstrating that LLMs can inherit human-like cognitive biases from their training data."
  - [section] "Having been trained on human-generated data, LLMs may inherit human biases, such as social biases [e.g.1, 4, 39, 61], and cognitive biases [e.g. 12, 14, 20, 42]."

### Mechanism 2
- Claim: Larger models (LLaMa2-70B) show less threshold priming due to their greater capacity to maintain contextual independence across longer sequences.
- Mechanism: Increased parameter count and attention head diversity allow the model to better isolate document content from positional context when making relevance assessments.
- Core assumption: Model size correlates with the ability to disentangle content relevance from positional priming effects.
- Evidence anchors:
  - [abstract] "When PL is 4 and EL is 4, among most topics, all models exhibited threshold priming... However, for LLaMa2-70B, this trend can only be observed when PL = 4 and EL = 4."

### Mechanism 3
- Claim: Topic specificity affects threshold priming because some domains contain stronger implicit relevance thresholds in their training data.
- Mechanism: Certain topics (e.g., medical or legal domains) have more explicit quality hierarchies in their text corpus, leading to stronger priming effects when those patterns are encountered.
- Core assumption: Training data for certain topics contains more pronounced quality indicators that create stronger priming anchors.
- Evidence anchors:
  - [section] "From Table 2, one can observe that... In some topics (e.g., 451602), most models did not exhibit significant differences in relevance scores under different threshold priming conditions..."

## Foundational Learning

- Concept: Priming effect in cognitive psychology
  - Why needed here: Understanding the psychological phenomenon being tested in LLMs requires knowledge of how human cognitive biases work.
  - Quick check question: What is the difference between priming effect and anchoring effect in cognitive psychology?

- Concept: Information retrieval evaluation metrics
  - Why needed here: The study uses relevance judgments to evaluate LLM performance, requiring understanding of how IR systems are typically evaluated.
  - Quick check question: How do ground truth relevance scores (0-3) map to typical IR evaluation metrics like NDCG or MAP?

- Concept: Batch processing in language models
  - Why needed here: The experiment processes multiple documents in sequence, requiring understanding of how LLMs handle batch inputs.
  - Quick check question: How does the order of documents in a batch input affect the model's attention mechanism and output probabilities?

## Architecture Onboarding

- Component map: Query → Document retrieval → Batch construction → LLM inference → Score extraction → Statistical analysis
- Critical path: Query → Document retrieval → Batch construction → LLM inference → Score extraction → Statistical analysis
- Design tradeoffs:
  - Temperature setting (0 vs non-zero) trades determinism for exploration of model uncertainty
  - Batch size affects both priming strength and computational cost
  - Model choice (GPT vs LLaMa) trades inference speed for potential bias mitigation
- Failure signatures:
  - Inconsistent scores across identical documents in different batch positions
  - Statistical significance dropping when batch size increases
  - Unexpected score patterns that reverse the priming effect direction
- First 3 experiments:
  1. Test identical documents in different positions within the same batch to confirm positional dependency
  2. Vary temperature settings to see if stochasticity reduces priming effects
  3. Compare batch processing vs individual document processing to isolate batch effect mechanisms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompting strategies affect the threshold priming bias in LLMs during batch relevance assessment?
- Basis in paper: [explicit] The paper mentions that the current study used specific prompts adapted from Upadhyay et al. [52] and notes that future research should explore how prompting engineering can help mitigate cognitive biases in LLMs.
- Why unresolved: The study used a fixed prompt format and did not experiment with variations in prompt design that might influence the extent of threshold priming.

### Open Question 2
- Question: Do LLMs exhibit other cognitive biases (e.g., anchoring, decoy effect) during relevance assessment, and how do these biases interact with threshold priming?
- Basis in paper: [explicit] The paper acknowledges that LLMs may be subject to various cognitive biases beyond threshold priming, such as anchoring effect, decoy effect, and reference dependence, but does not explore their interactions.
- Why unresolved: The study focused solely on threshold priming and did not investigate the presence or interaction of other cognitive biases in the same experimental context.

### Open Question 3
- Question: How does the order of documents within a batch influence the threshold priming effect in LLMs, and does this vary by model architecture?
- Basis in paper: [inferred] The study used fixed prologue-epilogue structures but did not explore how varying the order of documents (e.g., alternating high and low relevance documents) affects threshold priming.
- Why unresolved: The experimental design only tested static batch configurations without examining dynamic ordering effects or their interaction with different LLM architectures.

## Limitations
- Effect size varies considerably across topics, with some showing minimal priming while others show strong effects
- The study only examines batch sizes up to 8 documents, leaving open questions about how priming scales with larger batches
- The corpus neighbors reveal no prior citations, suggesting this may be an underexplored area

## Confidence

- **High confidence**: The statistical significance of threshold priming effects (p < 0.001) across multiple LLM models and topics is well-supported by the experimental design and analysis.
- **Medium confidence**: The claim that larger models show less threshold priming is supported but inconsistent across experimental conditions, suggesting the relationship may be more complex than presented.
- **Low confidence**: The assertion that LLMs "inherit" cognitive biases from training data is speculative without direct evidence about what patterns exist in the training corpus.

## Next Checks

1. Test the same priming effect on models trained on de-biasing corpora or fine-tuned to recognize and mitigate sequential context effects.
2. Expand the experimental design to include varying batch sizes beyond 8 documents and test different document ordering patterns (random, alternating high-low relevance).
3. Conduct ablation studies comparing batch processing versus individual document assessment to isolate whether the effect stems from attention mechanisms or other architectural features.