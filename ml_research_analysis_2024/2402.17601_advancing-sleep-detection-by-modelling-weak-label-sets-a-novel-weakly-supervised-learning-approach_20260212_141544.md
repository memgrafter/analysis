---
ver: rpa2
title: 'Advancing sleep detection by modelling weak label sets: A novel weakly supervised
  learning approach'
arxiv_id: '2402.17601'
source_url: https://arxiv.org/abs/2402.17601
tags:
- sleep
- uncertainty
- neural
- loss
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the problem of sleep detection when ground
  truth labels are unavailable, using weakly supervised learning. The authors propose
  a novel statistical model where the number of weak sleep labels (derived from conventional
  sleep detection algorithms) is modeled as a binomial distribution.
---

# Advancing sleep detection by modelling weak label sets: A novel weakly supervised learning approach

## Quick Facts
- arXiv ID: 2402.17601
- Source URL: https://arxiv.org/abs/2402.17601
- Reference count: 40
- Primary result: Novel weakly supervised learning approach for sleep detection using actigraphy data achieves 0.8233 ± 0.0766 accuracy, 0.6197 ± 0.1599 MCC, and 0.1910 ± 0.0934 ECE on MESA dataset

## Executive Summary
This study addresses sleep detection in data-scarce scenarios where ground truth labels are unavailable. The authors propose a novel statistical model that treats the number of weak sleep labels from conventional algorithms as a binomial distribution, enabling derivation of the soft cross-entropy loss from maximum likelihood estimation. Using Long Short-Term Memory (LSTM) networks trained on actigraphy data with this loss function, the approach outperforms conventional algorithms and other neural network architectures in accuracy and calibration on the Multi-Ethnic Study of Atherosclerosis (MESA) dataset.

## Method Summary
The method uses actigraphy data (30-second epochs) processed in 10-minute windows as input to an LSTM neural network. Weak labels are generated by aggregating predictions from five conventional sleep detection algorithms (Sadeh, Cole-Kripke, Oakley, HMM, Sazonov) through simple averaging. The number of algorithms predicting sleep per epoch is modeled as a binomial distribution, and maximizing this likelihood yields the soft cross-entropy loss function. Monte Carlo dropout (p=0.5) is used throughout the network to approximate Bayesian inference and quantify prediction uncertainty. The model is trained with Adam optimizer (learning rate 1e-5, L2 regularization 1e-4) and evaluated using accuracy, Matthews Correlation Coefficient, Expected Calibration Error, entropy, and prediction variance.

## Key Results
- LSTM trained with soft cross-entropy loss achieves 0.8233 ± 0.0766 accuracy and 0.6197 ± 0.1599 MCC on MESA dataset
- Proposed method outperforms conventional algorithms (Sadeh, Cole-Kripke, Oakley, HMM, Sazonov) and other neural network architectures (CNN, MLP)
- Soft cross-entropy loss provides better calibration (ECE: 0.1910 ± 0.0934) compared to Brier score and hard cross-entropy
- Monte Carlo dropout provides reliable uncertainty estimates with lower heteroscedasticity than Brier score uncertainty

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The ensemble of conventional algorithms provides weak labels whose average can be used as a soft label in place of ground truth.
- **Mechanism:** Each conventional algorithm outputs a binary prediction per time point. The ensemble aggregates these by simple averaging, producing a continuous probability-like label (pt = 1/k Σ yit) that reflects the consensus among diverse heuristics.
- **Core assumption:** The ensemble's weak labels, despite being noisy and derived from algorithms trained on small datasets, still carry sufficient signal for training a neural network if their average is treated probabilistically.
- **Evidence anchors:**
  - [abstract] "derived from the predictions generated by conventional sleep detection algorithms"
  - [section 2.2] "Each model i produces a binary outcome point yit ∈ {0, 1}"
  - [corpus] Weak supervision studies show soft labels from ensembles can train neural networks without ground truth (Zhou 2018, Bonab 2019).
- **Break condition:** If ensemble diversity is low (e.g., all algorithms trained on similar populations), the weak labels become too correlated, reducing their utility as a probabilistic label source.

### Mechanism 2
- **Claim:** Modeling the ensemble count of sleep predictions as a binomial distribution allows derivation of the soft cross-entropy loss from maximum likelihood estimation.
- **Mechanism:** The count yt of algorithms predicting sleep is assumed to follow Bin(k, pt), where pt is the true probability. Maximizing the binomial log-likelihood over the ensemble average yields the soft cross-entropy loss, enabling direct training with weak labels.
- **Core assumption:** The weak labels are conditionally independent given the true sleep state, and the ensemble size k is fixed.
- **Evidence anchors:**
  - [section 2.3] "the number of weak sleep labels is modelled as outcome of a binomial distribution"
  - [section 2.4] "We show that maximizing the binomial likelihood function... is equivalent to minimizing the soft cross-entropy loss"
  - [corpus] Formal derivations link binomial likelihood to cross-entropy in multi-label settings (Wang et al., 2020).
- **Break condition:** If the conditional independence assumption fails (e.g., algorithms share biased training data), the binomial model no longer accurately reflects the ensemble's uncertainty.

### Mechanism 3
- **Claim:** Monte Carlo dropout in neural networks approximates Bayesian inference, enabling quantification of prediction uncertainty from weak labels.
- **Mechanism:** Dropout layers are interpreted as variational approximations to a Gaussian process. By sampling multiple forward passes with dropout active, we obtain a distribution over predictions, with variance reflecting epistemic uncertainty.
- **Core assumption:** Dropout with rate p=0.5 before each layer approximates a deep Gaussian process, as shown by Gal & Ghahramani (2016).
- **Evidence anchors:**
  - [abstract] "neural networks with Monte Carlo dropout to approximate Bayesian inference"
  - [section 2.4] "Gal and Ghahramani have shown that any neural network with dropout before each layer approximates a deep Gaussian process"
  - [corpus] Monte Carlo dropout is widely used for uncertainty quantification in medical deep learning (Gal & Ghahramani, 2016).
- **Break condition:** If dropout rate or architecture deviates significantly from assumptions, the Gaussian process approximation breaks down, and uncertainty estimates become unreliable.

## Foundational Learning

- **Concept:** Weak supervision and label aggregation
  - **Why needed here:** The paper relies on generating usable training signals from noisy, algorithm-derived labels instead of ground truth.
  - **Quick check question:** What is the difference between hard labels (majority vote) and soft labels (ensemble average), and why does the paper prefer soft labels?

- **Concept:** Probabilistic modeling with binomial distribution
  - **Why needed here:** The binomial model formalizes how weak label counts relate to the true probability, enabling a principled loss function derivation.
  - **Quick check question:** Why is modeling the ensemble count as binomial critical for linking weak labels to the neural network output?

- **Concept:** Bayesian deep learning via Monte Carlo dropout
  - **Why needed here:** Dropout sampling provides both predictions and uncertainty estimates, crucial for comparing ensemble vs. model uncertainty.
  - **Quick check question:** How does Monte Carlo dropout approximate a Gaussian process, and what assumption underlies this interpretation?

## Architecture Onboarding

- **Component map:** Actigraphy time series (30-second epochs) → 10-minute window aggregation → LSTM with Monte Carlo dropout → soft cross-entropy loss → sleep probability output with uncertainty

- **Critical path:**
  1. Aggregate conventional algorithm predictions per epoch → weak labels
  2. Feed actigraphy window into LSTM with dropout
  3. Compute soft cross-entropy loss using ensemble average
  4. Optimize with Adam (lr=1e-5, L2=1e-4)
  5. During evaluation, sample predictions to estimate uncertainty

- **Design tradeoffs:**
  - **LSTM vs. CNN/MLP:** LSTM captures temporal dependencies; CNN might detect local patterns; MLP is simpler but less suited for sequences
  - **Soft vs. hard labels:** Soft labels retain ensemble agreement information; hard labels are robust but lose nuance
  - **Brier vs. cross-entropy:** Brier penalizes confident wrong predictions more; cross-entropy aligns with binomial likelihood

- **Failure signatures:**
  - **Low ensemble diversity:** Conventional algorithms too similar → weak labels too correlated → poor generalization
  - **Mismatched loss/uncertainty:** Using hard labels with dropout → uncertainty estimates misrepresent true prediction confidence
  - **Overfitting to weak labels:** High validation loss despite low training loss → model memorizing ensemble noise

- **First 3 experiments:**
  1. **Sanity check:** Train LSTM with soft cross-entropy on MESA weak labels; compare accuracy/MCC to conventional algorithms
  2. **Loss comparison:** Train same LSTM with Brier score and hard cross-entropy; evaluate calibration (ECE, entropy) and uncertainty (variance)
  3. **Architecture sweep:** Replace LSTM with CNN and MLP; keep soft cross-entropy; compare performance and uncertainty structure

## Open Questions the Paper Calls Out

- **Question:** How does the proposed method perform when applied to datasets with different sleep disorders or mental health conditions?
  - **Basis in paper:** [explicit] The authors mention that the MESA dataset contains a variety of sleep conditions, but the study has not extensively explored the model's calibration and prediction uncertainty across diverse groups with sleep and mental health problems.
  - **Why unresolved:** The study only applied the method to the MESA dataset and two additional datasets concerning mental health, but did not extensively explore the model's performance across diverse groups with sleep and mental health problems.
  - **What evidence would resolve it:** Applying the method to datasets with different sleep disorders or mental health conditions and comparing the performance to the MESA dataset.

- **Question:** How does the proposed method handle heteroscedasticity in the prediction uncertainty?
  - **Basis in paper:** [explicit] The authors mention that the uncertainty revealed a temporal structure and that the Brier score revealed a smoother uncertainty profile, but exhibited poorer calibration.
  - **Why unresolved:** The study only showed that the Brier score results in the least heteroscedasticity, but did not provide a solution to handle heteroscedasticity in the prediction uncertainty.
  - **What evidence would resolve it:** Developing a method to handle heteroscedasticity in the prediction uncertainty, such as incorporating it into the loss function or using a different neural network architecture.

- **Question:** How does the proposed method compare to other weakly supervised learning approaches in sleep detection?
  - **Basis in paper:** [explicit] The authors mention that the core idea of generating a set of weak labels can be transferred to various domains, such as human annotation in label crowdsourcing, image object recognition and clinical applications.
  - **Why unresolved:** The study only compared the proposed method to conventional sleep detection algorithms and other neural network architectures, but did not compare it to other weakly supervised learning approaches in sleep detection.
  - **What evidence would resolve it:** Comparing the proposed method to other weakly supervised learning approaches in sleep detection, such as using pseudo-labels or self-training.

## Limitations
- The binomial distribution assumption for ensemble counts may not hold if algorithms share biased training data or if the true sleep probability varies systematically with demographics
- Monte Carlo dropout uncertainty estimates depend critically on the Gaussian process approximation, which may break down with different dropout rates or architectures
- Generalization is limited to actigraphy data and may not extend to other modalities or label-scarce domains without careful validation

## Confidence

- **High confidence**: The derivation of soft cross-entropy from binomial likelihood is mathematically sound, and the empirical demonstration that LSTM outperforms conventional algorithms on the MESA dataset is reproducible given the described setup
- **Medium confidence**: The claim that Monte Carlo dropout provides reliable uncertainty estimates assumes the Gaussian process approximation holds, which may not generalize across different network architectures or dropout configurations
- **Low confidence**: The assumption that ensemble weak labels are conditionally independent given the true sleep state is difficult to verify and may fail if algorithms share training data or biases

## Next Checks

1. **Ensemble diversity validation**: Quantify pairwise correlations between conventional algorithm predictions to empirically assess conditional independence and ensemble diversity before and after training

2. **Domain transfer test**: Apply the trained LSTM model to actigraphy data from a different population (e.g., pediatric or clinical cohorts) to test generalization beyond the MESA dataset demographics

3. **Binomial assumption test**: Compare the empirical distribution of ensemble sleep counts against the binomial model predictions to identify systematic deviations that might indicate broken assumptions