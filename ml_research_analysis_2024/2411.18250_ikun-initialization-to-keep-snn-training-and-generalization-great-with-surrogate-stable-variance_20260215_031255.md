---
ver: rpa2
title: 'IKUN: Initialization to Keep snn training and generalization great with sUrrogate-stable
  variaNce'
arxiv_id: '2411.18250'
source_url: https://arxiv.org/abs/2411.18250
tags:
- initialization
- training
- ikun
- methods
- snns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IKUN, a variance-stabilizing initialization
  method specifically designed for spiking neural networks (SNNs). Traditional initialization
  methods like Xavier and Kaiming fail to account for the unique spiking dynamics
  and surrogate gradient mechanisms in SNNs, leading to suboptimal training performance.
---

# IKUN: Initialization to Keep snn training and generalization great with sUrrogate-stable variaNce

## Quick Facts
- arXiv ID: 2411.18250
- Source URL: https://arxiv.org/abs/2411.18250
- Authors: Da Chang; Deliang Wang; Xiao Yang
- Reference count: 10
- One-line primary result: IKUN initialization achieves up to 50% reduction in training epochs while maintaining 95% training accuracy and 91% generalization accuracy on FashionMNIST

## Executive Summary
This paper introduces IKUN, a variance-stabilizing initialization method specifically designed for spiking neural networks (SNNs). Traditional initialization methods like Xavier and Kaiming fail to account for the unique spiking dynamics and surrogate gradient mechanisms in SNNs, leading to suboptimal training performance. IKUN addresses this by integrating surrogate gradient characteristics into the initialization process, stabilizing signal propagation in both forward and backward passes. Theoretical analysis and experimental validation on the FashionMNIST dataset demonstrate that IKUN significantly improves training efficiency and generalization performance.

## Method Summary
IKUN is a weight initialization method that modifies the variance calculation formula to include surrogate gradient properties. It integrates with existing SNN architectures by replacing the standard initialization step before training begins. The method calculates weight variance as σ²_W = α/(fanin · σ²_X · E[f'(H[t])²]), where f'(H[t]) represents the surrogate gradient function derivative. This formula ensures that during backpropagation, the gradient variance remains stable across layers, preventing gradient vanishing or explosion. The approach is compatible with various surrogate gradient functions including sigmoid, tanh, and linear approximations.

## Key Results
- Achieves up to 50% reduction in training epochs compared to standard initialization methods
- Maintains 95% training accuracy and 91% generalization accuracy on FashionMNIST dataset
- Hessian analysis reveals convergence to flatter minima with eigenvalues near zero, indicating better generalization and robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IKUN initialization stabilizes signal propagation by matching weight variance to the surrogate gradient's variance properties
- Mechanism: IKUN adjusts the weight variance formula to include the expected squared value of the surrogate gradient function f'(H[t])². This ensures that during backpropagation, the gradient variance remains stable across layers, preventing gradient vanishing or explosion
- Core assumption: The surrogate gradient function's derivative has predictable statistical properties that can be incorporated into initialization
- Evidence anchors:
  - [abstract]: "IKUN addresses this by integrating surrogate gradient characteristics into the initialization process, stabilizing signal propagation in both forward and backward passes"
  - [section 3.2]: "THEOREM 1: SURROGATE-STABLE VARIANCE INITIALIZATION (IKUN)" with the weight variance condition: σ²_W = α/(fanin · σ²_X · E[f'(H[t])²])
  - [corpus]: Weak - corpus papers don't directly address surrogate gradient integration in initialization
- Break condition: If the surrogate gradient function's statistical properties change significantly during training, or if the assumption of E[f'(H[t])²] being constant across layers is violated

### Mechanism 2
- Claim: IKUN achieves faster convergence by initializing weights that are already "tuned" for the specific surrogate gradient function being used
- Mechanism: By incorporating the surrogate gradient's characteristics into the initialization formula, weights are set to values that work well with that specific gradient approximation from the start, reducing the need for extensive weight adjustments during early training
- Core assumption: Different surrogate gradient functions (sigmoid, tanh, linear) have distinct optimal weight distributions
- Evidence anchors:
  - [abstract]: "The method is compatible with various surrogate gradient functions (e.g., sigmoid, tanh, and linear surrogate gradients)"
  - [section 2.2.2]: "The design of surrogate gradients allows for a trade-off between approximating the true spiking activation function and maintaining training stability"
  - [corpus]: Missing - no direct evidence in corpus about surrogate gradient-specific initialization benefits
- Break condition: If the training dynamics require significant weight distribution changes that conflict with the initialization assumptions

### Mechanism 3
- Claim: IKUN improves generalization by converging to flatter minima characterized by Hessian eigenvalues near zero
- Mechanism: The variance-stabilizing initialization leads to optimization trajectories that naturally avoid sharp minima, as evidenced by the Hessian analysis showing eigenvalues concentrated near zero
- Core assumption: Flatter minima correspond to better generalization in SNNs as they do in ANNs
- Evidence anchors:
  - [abstract]: "Hessian analysis reveals that IKUN-trained models converge to flatter minima, characterized by Hessian eigenvalues near zero on the positive side, promoting better generalization"
  - [section 4.3]: "IKUN and Kaiming initialization yield eigenvalues close to 0 and positive, indicating a flat optimization region with good curvature characteristics, better generalization, and robustness"
  - [corpus]: Weak - corpus papers don't discuss Hessian eigenvalue distributions for SNN initialization methods
- Break condition: If flatter minima don't translate to better generalization in SNNs due to their unique temporal dynamics

## Foundational Learning

- Concept: Spiking Neural Networks and surrogate gradient mechanism
  - Why needed here: IKUN is specifically designed for SNNs using surrogate gradients, so understanding how SNNs differ from ANNs is crucial
  - Quick check question: How does the surrogate gradient mechanism address the non-differentiability of spiking neurons?

- Concept: Variance propagation in deep networks
  - Why needed here: IKUN's core innovation is controlling variance propagation through variance-stabilizing initialization
  - Quick check question: What is the relationship between weight variance and signal variance propagation in deep networks?

- Concept: Hessian eigenvalue analysis for generalization
  - Why needed here: The paper uses Hessian analysis to demonstrate that IKUN leads to flatter minima
  - Quick check question: How do Hessian eigenvalues relate to the flatness of minima and generalization performance?

## Architecture Onboarding

- Component map: IKUN initialization method -> SNN architecture (LIF neurons) -> Training loop (SGD/Adam) -> FashionMNIST dataset
- Critical path: 1) Choose surrogate gradient function, 2) Calculate E[f'(H[t])²] for that function, 3) Apply IKUN variance formula during weight initialization, 4) Proceed with standard SNN training
- Design tradeoffs: IKUN adds computational overhead to calculate the expected squared gradient value, but this is a one-time cost. It trades off some initialization flexibility for stability guarantees.
- Failure signatures: If training shows gradient instability despite using IKUN, the surrogate gradient function's properties may have changed during training. If generalization is poor, the flatter minima assumption may not hold for the specific dataset.
- First 3 experiments:
  1. Implement IKUN initialization with a simple surrogate gradient (e.g., sigmoid) on a basic SNN and compare training loss curves to standard initialization
  2. Test IKUN with different surrogate gradient functions (sigmoid, tanh, linear) to verify compatibility claims
  3. Perform Hessian analysis on models trained with IKUN vs standard initialization to verify flatter minima claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does IKUN initialization perform on deeper SNN architectures beyond the two-layer convolutional network tested in the paper?
- Basis in paper: [inferred] The paper's experimental results are limited to a two-layer convolutional SNN, and the authors explicitly mention future work will focus on validating the method on more complex architectures.
- Why unresolved: The current study's architectural constraints prevent generalization of results to deeper networks, which may exhibit different training dynamics and gradient flow characteristics.
- What evidence would resolve it: Experiments demonstrating IKUN performance on SNNs with 5+ layers across various architectures (residual, recurrent, etc.) with quantitative comparisons to baseline methods.

### Open Question 2
- Question: What is the optimal surrogate gradient function for SNN initialization, and does this vary by task or network depth?
- Basis in paper: [explicit] The authors state IKUN is "compatible with various surrogate gradient functions (e.g., sigmoid, tanh, and linear)" but do not investigate which performs best under different conditions.
- Why unresolved: The paper demonstrates compatibility but doesn't explore the interaction between initialization strategy and specific surrogate gradient choices, which could significantly impact training outcomes.
- What evidence would resolve it: Systematic experiments comparing different surrogate gradient functions (sigmoid, tanh, linear, etc.) across multiple tasks and network depths while keeping IKUN initialization constant.

### Open Question 3
- Question: How does IKUN initialization interact with dynamic learning rate schedules and advanced regularization techniques?
- Basis in paper: [explicit] The authors mention "the potential of combining this method with other optimization strategies (e.g., dynamic learning rates, enhanced regularization) warrants further investigation."
- Why unresolved: The paper only tests IKUN with basic SGD and Adam optimizers, leaving unexplored how it might synergize with or be affected by other training techniques commonly used in deep learning.
- What evidence would resolve it: Experiments combining IKUN with learning rate schedulers (cosine annealing, step decay), regularization methods (weight decay, early stopping), and adaptive optimizers to quantify performance improvements.

## Limitations
- Theoretical framework assumes stationarity of surrogate gradient properties throughout training, which may not hold in practice
- Limited generalizability claims due to single dataset (FashionMNIST) and specific architecture testing
- Hessian analysis uses only post-training snapshots rather than tracking eigenvalue evolution during training

## Confidence

- Mechanism 1 (Surrogate gradient integration): **Medium** - Strong theoretical foundation but limited empirical validation across diverse scenarios
- Mechanism 2 (Faster convergence): **Medium** - Training speed improvements are demonstrated but comparative analysis with other modern initialization methods is lacking
- Mechanism 3 (Better generalization via flatter minima): **Medium-High** - Hessian evidence is compelling, though the causal link to SNN-specific generalization requires further investigation

## Next Checks

1. **Cross-dataset robustness test**: Evaluate IKUN initialization performance on CIFAR-10 and ImageNet to assess generalizability beyond FashionMNIST

2. **Dynamic gradient analysis**: Monitor surrogate gradient statistical properties throughout training to verify the stationarity assumption and identify potential failure modes

3. **Comparative initialization benchmark**: Systematically compare IKUN against modern initialization methods (LSUV, delta-orthogonal) in identical SNN architectures to quantify relative improvements