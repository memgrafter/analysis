---
ver: rpa2
title: Input Snapshots Fusion for Scalable Discrete-Time Dynamic Graph Neural Networks
arxiv_id: '2405.06975'
source_url: https://arxiv.org/abs/2405.06975
tags:
- graph
- temporal
- dynamic
- time
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Input Snapshots Fusion for Dynamic Graphs
  (SFDyG), a novel framework that addresses scalability challenges in discrete-time
  dynamic graph representation learning. The key innovation is fusing multiple snapshots
  into a single temporal graph, which decouples computational complexity from the
  number of snapshots and enables efficient full-batch and mini-batch training.
---

# Input Snapshots Fusion for Scalable Discrete-Time Dynamic Graph Neural Networks

## Quick Facts
- arXiv ID: 2405.06975
- Source URL: https://arxiv.org/abs/2405.06975
- Reference count: 40
- Primary result: Up to 6× improvement over plain GAT with up to 44% GPU memory reduction in mini-batch training

## Executive Summary
This paper introduces Input Snapshots Fusion for Dynamic Graphs (SFDyG), a novel framework addressing scalability challenges in discrete-time dynamic graph representation learning. The key innovation is fusing multiple snapshots into a single temporal graph, which decouples computational complexity from the number of snapshots and enables efficient full-batch and mini-batch training. Extensive experiments on eight public datasets demonstrate that SFDyG consistently outperforms state-of-the-art baselines while significantly reducing memory requirements.

## Method Summary
SFDyG addresses scalability challenges in discrete-time dynamic graph representation learning by fusing multiple snapshots into a single temporal graph. The framework employs Hawkes processes to model temporal edges with time-decay effects, integrating them with graph neural networks (Hawkes-GCN and Hawkes-GAT) through a time-decayed message-passing mechanism. This approach effectively captures both temporal and structural patterns while maintaining scalability. The method supports both full-batch and mini-batch training, with the latter reducing GPU memory usage by up to 44% compared to full-batch training.

## Key Results
- Consistently outperforms state-of-the-art baselines on eight public datasets
- Achieves up to 6× improvement over plain GAT in link prediction tasks
- Reduces GPU memory usage by up to 44% in mini-batch training compared to full-batch training
- Shows robust performance across various dataset sizes and densities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fusing multiple snapshots into a single temporal graph decouples computational complexity from the number of snapshots.
- **Mechanism**: By merging all input snapshots into one temporal graph, the model processes the entire sequence as a single graph structure rather than applying separate computations to each snapshot. This means the number of GNN layers and operations scales with the temporal graph size, not with the number of snapshots.
- **Core assumption**: The temporal graph structure can be efficiently processed by existing GNN architectures without loss of temporal information.
- **Evidence anchors**:
  - [abstract] "By fusing multiple snapshots into a single temporal graph, SFDyG decouples computational complexity from the number of snapshots, enabling efficient full-batch and mini-batch training."
  - [section] "Drawing from these insights, as illustrated in Figure 1b, we propose to fuse the input snapshots within the sliding window into a single temporal graph to improve the modeling of DTDG."
- **Break condition**: When the temporal graph becomes too large for memory constraints, the efficiency gains diminish and may require graph partitioning or other scalability techniques.

### Mechanism 2
- **Claim**: Hawkes processes effectively model temporal edge dynamics through time-decayed message passing.
- **Mechanism**: Hawkes processes capture how previous events influence current ones with exponentially decaying effects over time. By integrating this into the message passing framework, each temporal edge's influence on node embeddings diminishes as it ages, creating a natural temporal smoothing effect.
- **Core assumption**: Temporal edge influence follows a decaying pattern that can be modeled by exponential decay functions.
- **Evidence anchors**:
  - [abstract] "This approach effectively captures both temporal and structural patterns while maintaining scalability."
  - [section] "Theorem 3.1 demonstrates that, in Hawkes processes-based temporal graph denoising problem, the influence of a temporal edge on its two endpoints depends on both the age of the edge and the time sensitivity parameter δ."
- **Break condition**: If temporal dependencies don't follow exponential decay patterns or if very long-range dependencies are critical, the Hawkes process model may not capture the true dynamics.

### Mechanism 3
- **Claim**: Modeling temporal graphs as a graph denoising problem with Hawkes excitation matrix enables effective GNN integration.
- **Mechanism**: The Hawkes excitation matrix extends the adjacency matrix by weighting edges based on their temporal decay. This matrix is used in the Laplacian regularization term of a graph denoising objective, which can be approximated by GNN layers, effectively creating a time-decayed message passing mechanism.
- **Core assumption**: The smoothness assumption with time decay can be effectively captured through graph denoising formulation.
- **Evidence anchors**:
  - [section] "Theorem 3.1 demonstrates that, in Hawkes processes-based temporal graph denoising problem, the influence of a temporal edge on its two endpoints depends on both the age of the edge and the time sensitivity parameter δ."
  - [section] "Theorem 3.1 (Hawkes temporal graph denoising). When we adopt the Laplacian matrix L = D − C, where D = diag(∑ⱼ Cᵢⱼ), the graph denoising problem (Equaition 4) was extended to temporal domain with the assumption that the smoothness of edges decays over time."
- **Break condition**: If the temporal graph structure doesn't benefit from smoothness regularization or if the Hawkes excitation matrix becomes too dense, the denoising approach may become computationally prohibitive.

## Foundational Learning

- **Concept**: Hawkes processes and temporal point processes
  - **Why needed here**: Understanding how Hawkes processes model event sequences with time decay is fundamental to grasping how SFDyG captures temporal dynamics in dynamic graphs.
  - **Quick check question**: What is the key difference between a standard Poisson process and a Hawkes process in terms of how past events influence future events?

- **Concept**: Graph neural networks and message passing
  - **Why needed here**: SFDyG builds upon GNN architectures by modifying the message passing mechanism to incorporate temporal decay, so understanding standard GNN operations is essential.
  - **Quick check question**: In a standard GCN layer, how are node features aggregated from neighboring nodes, and what role does the adjacency matrix play in this process?

- **Concept**: Graph denoising and Laplacian regularization
  - **Why needed here**: The theoretical foundation of SFDyG relies on viewing temporal graph learning as a graph denoising problem with time-decayed Laplacian regularization.
  - **Quick check question**: What is the purpose of the Laplacian regularization term in graph denoising, and how does it encourage smoothness of the signal over the graph structure?

## Architecture Onboarding

- **Component map**: Input snapshots -> Snapshot fusion -> Hawkes-GNN encoding -> Pairwise node embedding concatenation -> MLP prediction -> Loss computation
- **Critical path**: Snapshot fusion → Hawkes-GNN encoding → Pairwise node embedding concatenation → MLP prediction → Loss computation
- **Design tradeoffs**:
  - Full-batch vs mini-batch training: Full-batch captures global structure but requires more memory; mini-batch scales better but may miss some global patterns
  - Hawkes-GCN vs Hawkes-GAT: GCN variant uses degree normalization for stability; GAT variant uses attention for more flexible time sensitivity modeling
  - Negative sampling: Single negative sample per positive is sufficient but may require careful implementation to avoid overfitting
- **Failure signatures**:
  - Out-of-memory errors during training suggest need for mini-batch approach or smaller window size
  - Poor performance on datasets with long-range temporal dependencies may indicate Hawkes process limitations
  - Numerical instability in dense graphs suggests need for batch normalization or degree-based regularization
- **First 3 experiments**:
  1. Compare Hawkes-GAT vs plain GAT on a small dataset (e.g., UCI) to validate temporal modeling benefits
  2. Test mini-batch training on a medium-sized dataset (e.g., Alpha) to verify memory efficiency claims
  3. Vary sliding window size on Reddit-Title to identify optimal temporal context length for the model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Hawkes excitation matrix behave with different time decay parameters (δ) across datasets of varying densities?
- Basis in paper: [explicit] The paper mentions δ is a non-negative scalar representing time sensitivity but does not empirically explore its sensitivity across different dataset densities.
- Why unresolved: The paper only mentions δ as a parameter without conducting systematic sensitivity analysis across datasets with varying edge densities.
- What evidence would resolve it: A comprehensive ablation study varying δ across multiple datasets (dense vs. sparse) showing performance curves and identifying optimal ranges.

### Open Question 2
- Question: Can the Input Snapshots Fusion approach be extended to continuous-time dynamic graphs (CTDGs) where exact timestamps are available?
- Basis in paper: [inferred] The paper explicitly states it focuses on discrete-time dynamic graphs and mentions CTDGs use exact timestamps, but doesn't explore whether the fusion approach could work in that setting.
- Why unresolved: The paper's methodology is designed for DTDGs and doesn't investigate applicability to CTDG scenarios where continuous timestamps are available.
- What evidence would resolve it: Experimental results comparing SFDyG applied to CTDG datasets with exact timestamps versus traditional CTDG methods.

### Open Question 3
- Question: What is the theoretical upper bound on the number of snapshots that can be fused before performance degradation occurs?
- Basis in paper: [inferred] The paper shows performance is relatively stable across window sizes but doesn't establish theoretical limits on fusion scalability.
- Why unresolved: The experiments only test window sizes up to 20, but don't explore extremely large window sizes to determine where performance plateaus or degrades.
- What evidence would resolve it: A systematic study varying window sizes from small to very large (e.g., 100+ snapshots) with performance metrics to identify theoretical limits.

### Open Question 4
- Question: How does the proposed method perform on dynamic graph tasks beyond link prediction, such as node classification or graph classification?
- Basis in paper: [explicit] The paper states "This research focuses solely on the link prediction task in discrete-time dynamic graphs due to data constraints" and mentions future work exploring other tasks.
- Why unresolved: The methodology is demonstrated only for link prediction, with no empirical evidence for other dynamic graph learning tasks.
- What evidence would resolve it: Experimental results applying SFDyG to node classification and graph classification tasks on dynamic graph datasets with appropriate evaluation metrics.

## Limitations
- The framework assumes exponential decay patterns in temporal dependencies, which may not hold for all dynamic graph scenarios
- Performance sensitivity to the time sensitivity parameter δ across different dataset densities is not thoroughly explored
- The method is currently limited to link prediction tasks, with no empirical validation for other dynamic graph learning tasks

## Confidence
- **High confidence**: The scalability improvements through snapshot fusion and the effectiveness of Hawkes-GAT vs plain GAT are well-supported by experiments
- **Medium confidence**: The mini-batch training memory reduction claims are demonstrated but may vary significantly with implementation choices
- **Low confidence**: The theoretical claims about optimal time sensitivity parameter selection lack comprehensive empirical validation

## Next Checks
1. **Reproduce on synthetic datasets**: Create controlled synthetic DTDGs with known temporal decay patterns to verify Hawkes process modeling assumptions and isolate temporal vs structural learning effects
2. **Ablation on time sensitivity**: Systematically vary the time sensitivity parameter δ across datasets to empirically validate the theoretical optimal selection criteria and identify conditions where Hawkes modeling underperforms
3. **Memory complexity analysis**: Measure actual GPU memory usage during full-batch vs mini-batch training on datasets of varying sizes to verify the claimed 44% reduction and identify the scalability breakpoint where mini-batch becomes necessary