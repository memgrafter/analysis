---
ver: rpa2
title: 'Sign of the Times: Evaluating the use of Large Language Models for Idiomaticity
  Detection'
arxiv_id: '2405.09279'
source_url: https://arxiv.org/abs/2405.09279
tags:
- language
- performance
- which
- idiomaticity
- semeval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work benchmarks large language models (LLMs) on idiomaticity
  detection tasks, comparing them to fine-tuned encoder models. The authors evaluate
  multiple LLMs on three idiomaticity datasets, finding that while LLMs give competitive
  performance, they do not match the results of task-specific fine-tuned models, even
  at larger scales.
---

# Sign of the Times: Evaluating the use of Large Language Models for Idiomaticity Detection

## Quick Facts
- arXiv ID: 2405.09279
- Source URL: https://arxiv.org/abs/2405.09279
- Reference count: 0
- LLMs show competitive but lower performance than fine-tuned encoder models for idiomaticity detection, with performance scaling with model size

## Executive Summary
This paper benchmarks large language models (LLMs) against fine-tuned encoder models for idiomaticity detection across three datasets. The authors evaluate multiple LLMs including GPT-3.5-turbo, Gemini 1.0 Pro, and Flan-T5 models on SemEval 2022 Task 2a (English, Portuguese, Galician), FLUTE (English), and MAGPIE (English) datasets. While LLMs demonstrate general ability to detect idiomatic expressions, they consistently underperform compared to task-specific fine-tuned models. The study explores how prompt engineering and few-shot learning can improve results, particularly in multilingual settings, and discusses practical considerations such as cost and output variability when using LLMs for this task.

## Method Summary
The authors employ a prompting approach without fine-tuning, using various LLMs (both SaaS and local models) on three idiomaticity datasets. They evaluate performance using macro-average F1 scores across two classes (idiomatic vs. literal). The methodology involves collecting the SemEval 2022 Task 2a, FLUTE, and MAGPIE datasets, selecting LLMs, preparing prompts for each dataset, and running experiments to analyze performance based on model size, prompting methods, and multilingual considerations. The study investigates scaling effects, prompt engineering techniques, and few-shot prompting strategies.

## Key Results
- Fine-tuned encoder models outperform LLMs on idiomaticity detection, even at larger model scales
- LLM performance scales with model size, with larger models showing better results
- Few-shot prompting and prompt engineering can improve LLM performance, especially for multilingual settings and rare languages like Galician
- LLMs show general ability to detect idiomaticity but lag behind fine-tuned models in consistent performance

## Why This Works (Mechanism)
None

## Foundational Learning
- Idiomaticity detection: Identifying whether expressions should be interpreted literally or figuratively
  - Why needed: Core task being evaluated across datasets
  - Quick check: Can distinguish between "kick the bucket" (idiomatic) vs "kick the ball" (literal)
- Prompt engineering: Designing effective input prompts for LLMs
  - Why needed: Key technique explored for improving LLM performance
  - Quick check: Different prompt formats yield varying F1 scores
- Few-shot learning: Providing examples within prompts to guide model behavior
  - Why needed: Investigated as a method to improve multilingual performance
  - Quick check: 2-3 examples improve performance on Galician and Portuguese
- Macro-average F1 score: Evaluation metric balancing precision and recall across classes
  - Why needed: Standard metric for imbalanced classification tasks
  - Quick check: Scores range from 0-100, with higher indicating better performance
- Model scaling: Relationship between model size and task performance
  - Why needed: Examines whether larger LLMs close the gap with fine-tuned models
  - Quick check: Performance generally improves with model size

## Architecture Onboarding

**Component Map:**
Data Collection -> Prompt Design -> LLM Inference -> F1 Score Evaluation -> Analysis

**Critical Path:**
Dataset preparation → Prompt formulation → LLM API calls → Result aggregation → Statistical analysis

**Design Tradeoffs:**
- SaaS vs. local models: Trade convenience for cost control and data privacy
- Few-shot vs. zero-shot: Balance performance gains against prompt complexity
- Multilingual support: Direct prompting vs. translation overhead

**Failure Signatures:**
- High variance in outputs → Inconsistent results across trials
- Poor multilingual performance → Language-specific prompt issues
- Cost overruns → Inefficient API usage patterns

**3 First Experiments:**
1. Run baseline prompt on a small subset to verify API connectivity
2. Test prompt variations on one dataset to identify effective formats
3. Compare single vs. multiple trial runs to quantify output variability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the training datasets of large language models (LLMs) influence their performance on idiomaticity detection tasks?
- Basis in paper: The authors note that the training datasets of LLMs are not public, but they are likely to contain large quantities of 'naturally distributed' idiomatic expressions. They also highlight the possibility that these datasets might include the training or test datasets under evaluation.
- Why unresolved: The specific contents of the training datasets are not disclosed, and there is uncertainty about the extent to which they include idiomatic expressions or the datasets being tested.
- What evidence would resolve it: Detailed information about the training datasets of LLMs, including the presence and frequency of idiomatic expressions, would help determine their influence on idiomaticity detection performance.

### Open Question 2
- Question: What is the impact of instruction tuning on the performance of LLMs for idiomaticity detection tasks?
- Basis in paper: The authors mention that Flan-T5 models, which are instruction-refined versions of T5, have undergone exposure to over 1000 tasks, including entailment and contradiction judgments similar to the FLUTE dataset. This suggests that instruction tuning might play a role in performance.
- Why unresolved: While the authors discuss the benefits of instruction tuning for Flan-T5 models, they do not explicitly compare the performance of instruction-tuned models to non-instruction-tuned models on idiomaticity detection tasks.
- What evidence would resolve it: Comparative experiments between instruction-tuned and non-instruction-tuned LLMs on idiomaticity detection tasks would clarify the impact of instruction tuning on performance.

### Open Question 3
- Question: How do few-shot prompting and language prompts affect the performance of LLMs on idiomaticity detection tasks across different languages?
- Basis in paper: The authors experiment with few-shot prompting and language prompts for GPT-3.5-turbo, Gemini 1.0 Pro, and Flan-T5-XXL on the SemEval dataset, which includes English, Portuguese, and Galician. They observe varying impacts on performance across these languages.
- Why unresolved: The authors' experiments are limited to a small set of examples and languages, and the reasons for the observed differences in performance are not fully explained.
- What evidence would resolve it: Additional experiments with a larger set of examples and languages, along with a detailed analysis of the reasons for performance differences, would provide a clearer understanding of the impact of few-shot prompting and language prompts.

## Limitations
- Results depend on specific model versions and API configurations that may change over time
- Study focuses primarily on F1 scores without extensive qualitative error analysis
- Limited discussion of dataset biases and their impact on model performance
- Cost considerations mentioned but not systematically quantified across use cases

## Confidence

**High Confidence:** The finding that fine-tuned encoder models outperform LLMs for idiomaticity detection is well-supported by the experimental results across multiple datasets

**Medium Confidence:** The observation that performance scales with model size and that few-shot prompting improves results, as these findings are based on limited model comparisons

**Medium Confidence:** The practical considerations around cost and variability, as these are discussed but not empirically quantified across different scenarios

## Next Checks

1. Conduct systematic ablation studies on prompt engineering techniques to quantify their impact on performance across different model sizes

2. Perform detailed error analysis comparing LLM predictions with fine-tuned models to identify specific types of idiomatic expressions where LLMs struggle

3. Evaluate model performance on out-of-distribution idiomatic expressions and in zero-shot multilingual settings to assess generalization capabilities