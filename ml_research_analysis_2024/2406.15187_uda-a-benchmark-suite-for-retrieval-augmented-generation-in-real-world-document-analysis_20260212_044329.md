---
ver: rpa2
title: 'UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document
  Analysis'
arxiv_id: '2406.15187'
source_url: https://arxiv.org/abs/2406.15187
tags:
- context
- table
- retrieval
- answer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UDA, a benchmark suite for retrieval-augmented
  generation (RAG) in real-world document analysis. The dataset comprises 2,965 real-world
  documents and 29,590 expert-annotated Q&A pairs across finance, academia, and knowledge
  domains, with documents retained in their original unstructured formats.
---

# UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis

## Quick Facts
- **arXiv ID:** 2406.15187
- **Source URL:** https://arxiv.org/abs/2406.15187
- **Reference count:** 40
- **Key outcome:** Introduces UDA benchmark suite for RAG in real-world document analysis, demonstrating GPT-4-Omni and raw-text parsing performance, BM-25 advantages for financial queries, and Chain-of-Thought benefits for numerical reasoning.

## Executive Summary
This paper introduces UDA, a comprehensive benchmark suite for evaluating retrieval-augmented generation (RAG) systems in real-world document analysis. The dataset comprises 2,965 real-world documents and 29,590 expert-annotated Q&A pairs across finance, academia, and knowledge domains. The authors systematically evaluate various RAG components including data parsing, retrieval strategies, and generation methods across different LLM configurations. Key findings reveal that GPT-4-Omni and raw-text parsing perform comparably to well-parsed tables, smaller LLMs benefit more from improved parsing, BM-25 retrieval can outperform dense embeddings for precise financial queries, and Chain-of-Thought prompting significantly improves numerical reasoning performance.

## Method Summary
The UDA benchmark evaluates RAG systems through a comprehensive pipeline involving data parsing, indexing, retrieval, and generation stages. The dataset includes 2,965 real-world documents from finance, academia, and knowledge domains, with 29,590 expert-annotated Q&A pairs. The evaluation uses zero-shot LLM generation with prompt formatting examples, testing various parsing methods (raw-text extraction, CV-based parsing, layout-aware parsing), retrieval strategies (BM-25, dense embeddings), and generation approaches across multiple LLM models including GPT-4, Llama-3, Qwen, Mixtral, Mistral, and CodeLlama. The benchmark employs comprehensive metrics including LCS scores, ROUGE, and exact match rates to evaluate end-to-end performance.

## Key Results
- GPT-4-Omni and raw-text parsing achieve comparable performance to well-parsed tables for document analysis tasks
- BM-25 retrieval outperforms dense embeddings for precise financial queries requiring exact information matching
- Chain-of-Thought prompting significantly improves numerical reasoning performance, while long-context LLMs underperform RAG approaches in numerical tasks

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive evaluation framework that isolates and tests individual RAG components while maintaining realistic document complexity. By using real-world documents rather than synthetic data, the benchmark captures the messy, unstructured nature of actual documents that RAG systems must handle. The systematic ablation studies across parsing, retrieval, and generation components reveal how each stage impacts overall performance, particularly highlighting the importance of parsing quality and retrieval precision in achieving accurate answers.

## Foundational Learning

**RAG Pipeline Architecture**: Understanding the end-to-end flow from document ingestion through parsing, indexing, retrieval, and generation is essential for implementing and evaluating RAG systems. Quick check: Can you diagram the complete RAG pipeline and identify where each component affects performance?

**Document Parsing Methods**: Different parsing approaches (raw-text extraction, CV-based, layout-aware) have distinct trade-offs in handling tables, figures, and unstructured content. Quick check: What are the key differences between these parsing methods and when would each be most appropriate?

**Retrieval Strategies**: BM-25 vs. dense embeddings represent fundamentally different approaches to matching queries with relevant document chunks, with implications for precision and recall. Quick check: Under what conditions would BM-25 outperform dense embeddings, and vice versa?

**Prompt Engineering**: Chain-of-Thought prompting and zero-shot generation strategies significantly impact LLM performance on reasoning tasks. Quick check: How does Chain-of-Thought prompting improve numerical reasoning compared to direct answering?

**Evaluation Metrics**: Understanding LCS, ROUGE, and exact match metrics is crucial for assessing RAG system performance across different document types and query formats. Quick check: When would LCS be more appropriate than exact match for evaluating answers?

## Architecture Onboarding

**Component Map**: Documents -> Parsing (raw-text/CV/layout-aware) -> Chunking (3000 chars, 10% overlap) -> Indexing -> Retrieval (BM-25/dense) -> Top-5 chunks -> Generation (LLM with prompt) -> Answer

**Critical Path**: The most performance-critical path is Parsing -> Retrieval -> Generation, as errors in parsing propagate through retrieval and generation, while retrieval quality directly determines the evidence available for generation.

**Design Tradeoffs**: Raw-text parsing is faster and more robust but loses structural information; CV-based parsing preserves layout but struggles with edge cases; BM-25 is interpretable but may miss semantic matches that dense embeddings capture.

**Failure Signatures**: Poor parsing leads to missing or incorrect table data; weak retrieval results in irrelevant chunks being retrieved; generation errors often manifest as hallucinations when retrieved evidence is insufficient or noisy.

**First Experiments**:
1. Compare raw-text vs. CV-based parsing on a sample of complex tables to quantify structural information loss
2. Test BM-25 vs. dense embeddings retrieval on financial queries requiring precise numerical matches
3. Evaluate Chain-of-Thought vs. direct prompting on numerical reasoning tasks with calculation requirements

## Open Questions the Paper Calls Out
The paper acknowledges several limitations but doesn't explicitly call out open questions in the traditional sense. The main unresolved issues include handling noisy or ambiguous evidence in retrieved chunks, optimizing chunk size and overlap strategies for different document types, and improving parsing methods for complex real-world documents with irregular layouts and edge cases.

## Limitations
- Reliance on expert annotations for a subset of data rather than full-scale human evaluation across all documents
- Focus on zero-shot prompting without exploring fine-tuned or domain-adapted models
- Limited investigation of alternative retrieval architectures beyond basic BM-25 and dense embeddings

## Confidence
**High Confidence**:
- The systematic ablation study design and comprehensive evaluation protocol
- The use of real-world documents rather than synthetic data
- The clear performance degradation patterns for long-context LLMs in numerical tasks

**Medium Confidence**:
- GPT-4-Omni and raw-text parsing performance equivalence
- BM-25 advantages for financial queries generalizability
- Chain-of-Thought prompting benefits across different numerical reasoning types

**Low Confidence**:
- Long-context LLM performance degradation mechanisms
- Parsing method effectiveness on highly irregular document layouts
- Optimal retrieval configuration parameters across document types

## Next Checks
1. Replicate numerical reasoning experiments using broader range of long-context models (Claude-3-100K, Gemini-Pro) to verify generalizability of performance degradation
2. Conduct systematic comparison of parsing strategies across additional document types (scanned documents, mixed-media content) to assess robustness of raw-text vs. structured parsing findings
3. Evaluate benchmark sensitivity to different retrieval configuration parameters (chunk size, overlap, retrieval depth) to determine stability of reported performance differences between retrieval methods