---
ver: rpa2
title: 'Tilting the Odds at the Lottery: the Interplay of Overparameterisation and
  Curricula in Neural Networks'
arxiv_id: '2406.01589'
source_url: https://arxiv.org/abs/2406.01589
tags:
- learning
- curriculum
- network
- relevant
- curricula
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides an analytical study of the interplay between
  curriculum learning and overparameterisation in neural networks. Using the XOR-like
  Gaussian Mixture problem, the authors disentangle the effects of curriculum learning
  on relevant manifold discovery and labelling rule identification, showing that curriculum
  can compensate for poor initialisations.
---

# Tilting the Odds at the Lottery: the Interplay of Overparameterisation and Curricula in Neural Networks

## Quick Facts
- **arXiv ID**: 2406.01589
- **Source URL**: https://arxiv.org/abs/2406.01589
- **Reference count**: 39
- **Primary result**: Overparameterisation increases the probability of containing a well-initialised sub-network (lottery ticket hypothesis), and when this degree is sufficiently high, curriculum learning benefits diminish.

## Executive Summary
This work provides an analytical study of how curriculum learning and overparameterisation interact in neural networks. Using a tractable XOR-like Gaussian Mixture model, the authors disentangle curriculum learning's dual effects on relevant manifold discovery and labelling rule identification, showing it can compensate for poor initialisations. The study reveals that overparameterisation increases the chance of finding well-initialised subnetworks, connecting to the lottery ticket hypothesis. Crucially, they find that when networks are sufficiently overparameterised, curriculum benefits vanish, providing a theoretical account for why curricula often fail in deep learning practice.

## Method Summary
The authors analyze a 2-layer neural network trained on an XOR-like Gaussian Mixture task using online learning with stochastic gradient descent. The learning dynamics are captured by a system of ODEs tracking key order parameters (Q, M, T) that characterize the mean and covariance of receptive fields. They compare curriculum learning (with fading factor φ) against random ordering and no-fading protocols across varying levels of noise (σ) and overparameterisation (K). The theoretical analysis is validated through empirical experiments on both synthetic data and real-world datasets (Fashion MNIST, CIFAR-10) with MLP and CNN architectures.

## Key Results
- Overparameterisation increases the probability of containing a well-initialised sub-network that can solve the task, consistent with the lottery ticket hypothesis.
- Curriculum learning can compensate for poor initialisations by temporarily increasing SNR through fading, but this benefit diminishes as overparameterisation increases.
- In highly overparameterised regimes, curriculum learning becomes ineffective, providing a theoretical explanation for its limited success in deep learning.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Overparameterisation increases the probability of containing a well-initialised sub-network that can solve the task (lottery ticket hypothesis).
- **Mechanism**: As the number of neurons K increases, the probability that at least one neuron is well-aligned with each cluster centroid at initialisation grows exponentially, ensuring full cluster coverage and optimal solution.
- **Core assumption**: The XOR-like Gaussian Mixture (XGM) task requires K=4 neurons for perfect coverage, and initial alignment determines whether learning succeeds.
- **Evidence anchors**:
  - [abstract] "overparameterised networks have an increased chance of containing a sub-network that is well-initialised to solve the task at hand."
  - [section] "As first observed in Refinetti et al. (2021), overparameterising the network by increasing K lowers the probability of the initial configuration being in the basin of attraction of a low coverage solution—as sketched in Fig. 1b—and leads to a clear improvement in the average performance of the network."
- **Break condition**: If the noise level σ is too high, even overparameterisation cannot guarantee good performance as the clusters become too overlapping.

### Mechanism 2
- **Claim**: Curriculum learning can compensate for poor initialisations by guiding the network through easier examples first.
- **Mechanism**: By temporarily increasing the SNR through fading (rescaling centroids), curriculum learning enlarges the basin of attraction for neuron configurations with high cluster coverage, especially in the Goldilocks SNR regime.
- **Core assumption**: The SNR = ||μ||/σ controls sample difficulty, and curriculum can manipulate this to aid learning.
- **Evidence anchors**:
  - [abstract] "curriculum can compensate for a wide range of poor initialisations;"
  - [section] "In particular, to similar effect, one could either rescale the norm of the centroids μα, or the standard deviation σ, in a fraction of the presented patterns."
- **Break condition**: If the network is already overparameterised, the margin for curriculum benefit diminishes as full coverage is achieved without SNR boosting.

### Mechanism 3
- **Claim**: Curriculum learning disentangles relevant manifold discovery from labelling rule identification, providing dual benefits.
- **Mechanism**: Curriculum aids in discovering the relevant manifold (aligning neurons to centroids) and identifying the labelling rule (ensuring correct input-output associations), with non-monotonic benefits depending on the fading factor φ and current alignment.
- **Core assumption**: The XGM task can be decomposed into these two sub-tasks, and curriculum affects them differently.
- **Evidence anchors**:
  - [section] "Our results show that a high degree of overparameterisation—while simplifying the problem—can limit the benefit from curricula, providing a theoretical account of the ineffectiveness of curricula in deep learning."
  - [section] "In Fig. 3b we can analyse the rate of increase of M11 (top) and ρ1 (bottom), as a function of their current values and of the fading factor φ."
- **Break condition**: If the initial alignment is already optimal, curriculum may not provide additional benefits and could even be detrimental by reducing fluctuations needed to escape suboptimal basins.

## Foundational Learning

- **Concept**: XOR-like Gaussian Mixture problem
  - **Why needed here**: This synthetic task encapsulates the challenge of discovering relevant features among many irrelevant ones, which is crucial for understanding the interplay between overparameterisation and curriculum learning.
  - **Quick check question**: What is the minimal number of neurons required to solve the XOR-like Gaussian Mixture problem optimally?

- **Concept**: Lottery Ticket Hypothesis
  - **Why needed here**: This hypothesis explains why overparameterisation can be beneficial, as it increases the chance of containing a well-initialised sub-network.
  - **Quick check question**: How does increasing the number of neurons K affect the probability of finding a well-initialised sub-network?

- **Concept**: Curriculum Learning
  - **Why needed here**: This learning strategy guides the learner towards solving the task by curating the order of examples, which can compensate for poor initialisations and aid in relevant manifold discovery.
  - **Quick check question**: What is the Goldilocks SNR regime, and why is it important for curriculum learning effectiveness?

## Architecture Onboarding

- **Component map**: Input data -> 2-layer neural network with K hidden units -> Output predictions
- **Critical path**: The learning dynamics, captured by ODEs tracking order parameters (Q, M, T) that parameterize the mean and covariance of receptive fields
- **Design tradeoffs**: Overparameterisation simplifies the problem but reduces the benefit from curricula, while curriculum learning can compensate for poor initialisations but may be less effective in highly overparameterised regimes
- **Failure signatures**: Poor performance can occur if the noise level σ is too high, if the initial alignment is suboptimal, or if the curriculum protocol is not well-tuned to the SNR regime
- **First 3 experiments**:
  1. Train a 2-layer network with K=4 neurons on the XGM task with varying noise levels σ to observe the lottery ticket effect.
  2. Implement curriculum learning with fading factor φ and compare its performance to random ordering and no-fading protocols.
  3. Vary the number of neurons K and observe how it affects the benefit of curriculum learning, particularly in the overparameterised regime.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the diminishing effectiveness of curriculum learning in overparameterized regimes extend beyond the XOR-like Gaussian Mixture model to more complex, real-world datasets and architectures?
- **Basis in paper**: [explicit] The paper discusses the interplay between curriculum learning and overparameterization in the XOR-like Gaussian Mixture model, finding that the benefits of curricula diminish with increased overparameterization. It also mentions that similar phenomenology was observed in experiments with real data (Fashion MNIST and CIFAR10) and more complex neural network architectures (MLP and CNN), supporting the idea that overparameterization limits the effectiveness of curricula.
- **Why unresolved**: While the paper provides evidence from experiments with real datasets and architectures, it does not comprehensively explore the extent to which this phenomenon generalizes to other types of tasks and models. The experiments are limited in scope and do not cover the full range of potential applications of curriculum learning.
- **What evidence would resolve it**: Conducting extensive experiments with a wide variety of real-world datasets, tasks, and neural network architectures to determine if the diminishing effectiveness of curriculum learning in overparameterized regimes is a general phenomenon. This could involve comparing the performance of curriculum learning with random ordering and no-fading strategies across different domains and model complexities.

### Open Question 2
- **Question**: What is the optimal degree of overparameterization that balances the benefits of curriculum learning and the advantages of the lottery ticket hypothesis?
- **Basis in paper**: [inferred] The paper discusses the trade-off between curriculum learning and overparameterization, finding that while both can increase the probability of ending up in a good solution with high centroid coverage, the asymptotic gain of curriculum is minimal when the network is already overparameterized. This suggests that there might be an optimal level of overparameterization that balances the benefits of both strategies.
- **Why unresolved**: The paper does not provide a quantitative analysis of the optimal degree of overparameterization. It only qualitatively describes the diminishing returns of curriculum learning in highly overparameterized regimes. Determining the optimal balance between these two strategies requires a more rigorous mathematical framework and empirical validation.
- **What evidence would resolve it**: Developing a theoretical framework that quantifies the trade-off between curriculum learning and overparameterization, and empirically validating the predictions through experiments with different degrees of overparameterization. This could involve analyzing the performance of curriculum learning strategies across a range of overparameterization levels and identifying the point at which the benefits of curricula are maximized while still retaining the advantages of the lottery ticket hypothesis.

### Open Question 3
- **Question**: How can curriculum learning be effectively integrated with other learning strategies, such as meta-learning and self-supervised learning, to further improve performance in overparameterized regimes?
- **Basis in paper**: [inferred] The paper focuses on the interplay between curriculum learning and overparameterization, but it does not explore the potential synergies between curriculum learning and other learning strategies. Given the recent advancements in meta-learning and self-supervised learning, it is plausible that these approaches could complement curriculum learning and mitigate the diminishing effectiveness in overparameterized regimes.
- **Why unresolved**: The paper does not investigate the potential benefits of combining curriculum learning with other learning strategies. It remains unclear whether these approaches can work synergistically to improve performance in overparameterized regimes, and if so, what the optimal combination of strategies would be.
- **What evidence would resolve it**: Conducting experiments that combine curriculum learning with meta-learning and self-supervised learning in overparameterized regimes, and comparing the performance to curriculum learning alone. This could involve designing hybrid learning strategies that leverage the strengths of each approach and analyzing their effectiveness in different tasks and architectures.

## Limitations
- The analysis is limited to a simplified 2-layer neural network on the XOR-like Gaussian Mixture task, which may not directly generalize to deeper architectures or more complex real-world datasets.
- The theoretical framework relies on mean-field approximations and order parameter tracking, which may introduce approximations that need validation through empirical studies on larger networks.
- The curriculum protocol implemented (fading factor φ) represents only one specific form of curriculum learning, and other curriculum strategies may yield different results.

## Confidence

- **High Confidence**: The lottery ticket mechanism in overparameterised networks and the basic connection between SNR manipulation and curriculum effectiveness. These claims are well-supported by both theoretical analysis and empirical validation in the paper.
- **Medium Confidence**: The claim that overparameterisation diminishes curriculum benefits, as this represents a more nuanced theoretical prediction that would benefit from additional empirical validation across diverse architectures.
- **Medium Confidence**: The dual benefits of curriculum in disentangling relevant manifold discovery from labelling rule identification, as this decomposition, while theoretically sound, may be more complex in practice with different task structures.

## Next Checks

1. **Empirical validation on deeper networks**: Test the theoretical predictions about curriculum ineffectiveness in overparameterised regimes using 3-4 layer networks on both synthetic and real-world datasets (e.g., MNIST, CIFAR-10).

2. **Alternative curriculum protocols**: Implement and compare different curriculum strategies (difficulty-based, error-based, self-paced learning) to determine if the diminishing returns of curriculum in overparameterised regimes is protocol-specific or general.

3. **Transfer to continuous learning scenarios**: Evaluate whether the theoretical insights about the interplay between overparameterisation and curriculum learning extend to online/continual learning settings where data arrives sequentially.