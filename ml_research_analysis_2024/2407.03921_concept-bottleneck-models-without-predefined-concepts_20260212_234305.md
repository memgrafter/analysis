---
ver: rpa2
title: Concept Bottleneck Models Without Predefined Concepts
arxiv_id: '2407.03921'
source_url: https://arxiv.org/abs/2407.03921
tags:
- concept
- concepts
- ucbm
- class
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of making concept bottleneck
  models (CBMs) interpretable without relying on predefined or human-annotated concepts.
  The authors propose an unsupervised approach that extracts concepts automatically
  from pretrained black-box models using dictionary learning.
---

# Concept Bottleneck Models Without Predefined Concepts

## Quick Facts
- arXiv ID: 2407.03921
- Source URL: https://arxiv.org/abs/2407.03921
- Authors: Simon Schrodi; Julian Schur; Max Argus; Thomas Brox
- Reference count: 40
- Primary result: Introduces Unsupervised Concept Bottleneck Models (UCBM) that achieve interpretable classification without predefined concepts, significantly improving downstream performance compared to baselines while using fewer concepts

## Executive Summary
This paper addresses the challenge of making concept bottleneck models interpretable without relying on predefined or human-annotated concepts. The authors propose an unsupervised approach that extracts concepts automatically from pretrained black-box models using dictionary learning. Their method, UCBM, introduces an input-dependent concept selection mechanism that enforces sparsity across all classes, ensuring only a small subset of concepts is used per input. The approach significantly improves downstream performance compared to existing baselines while maintaining interpretability, and demonstrates how large vision-language models can be leveraged to edit model weights and correct misclassifications.

## Method Summary
UCBM operates in two steps: first, it extracts concepts in an unsupervised manner using dictionary learning (specifically non-negative matrix factorization) to approximate the activation matrix of a pretrained black-box model; second, it trains an interpretable classifier with input-dependent concept selection that gates concepts based on cosine similarity, combined with sparse linear layers using elastic net regularization and concept dropout. This approach discovers a concept basis that faithfully represents the black-box model's activations while enforcing sparsity across all classes through input-dependent selection and regularization mechanisms.

## Key Results
- UCBM achieves significantly better downstream performance than baseline methods like Post-hoc CBM and Label-free CBM
- The approach uses substantially fewer concepts than black-box models while narrowing the performance gap
- Concept sparsity is enforced across all classes through input-dependent selection, ensuring interpretable and efficient classification
- Large vision-language models can be leveraged to edit model weights and correct misclassifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unsupervised concept discovery via dictionary learning can faithfully represent black-box model activations without predefined concepts
- Mechanism: Non-negative matrix factorization approximates the activation matrix A as UCT, where C contains concept vectors
- Core assumption: The black-box model's activations contain linearly separable, non-negative concept representations that can be captured by sparse coefficient matrix U
- Evidence anchors:
  - [abstract] "we propose to extract concepts in an unsupervised manner instead...to find a dictionary matrix...that defines the (concept) basis vectors"
  - [section 2.1] "many unsupervised concept discovery methods can be seen as (low-rank) approximation of the activations formulated as an instance of the dictionary learning problem"
  - [corpus] Weak evidence - no directly comparable dictionary learning approaches found
- Break condition: If activations are highly entangled or non-linear, the linear dictionary approximation fails to capture meaningful concepts

### Mechanism 2
- Claim: Input-dependent concept selection enforces sparsity across all classes by gating low-similarity concepts
- Mechanism: Compute cosine similarities between activations and concepts, apply offset o, then apply sparsity regularization to π(xi)
- Core assumption: Concepts with low cosine similarity are irrelevant for classification and can be safely removed without information loss
- Evidence anchors:
  - [section 2.2] "we propose a simple input-dependent concept selection mechanism that selectively removes concepts, retaining only a sparse set of them per input"
  - [abstract] "introduces an input-dependent concept selection mechanism that ensures only a small subset of concepts is used across all classes"
  - [corpus] Weak evidence - no direct comparison with similar gating mechanisms found
- Break condition: If relevant concepts have low similarity due to noise or representation issues, gating removes them and degrades performance

### Mechanism 3
- Claim: Sparse weights in the linear classifier combined with concept dropout prevent over-reliance on single concepts
- Mechanism: Apply elastic net regularization to weights W, then apply dropout to active concepts π(xi) before classification
- Core assumption: Encouraging weight sparsity and concept dropout forces the model to distribute classification responsibility across multiple concepts
- Evidence anchors:
  - [section 2.2] "we applied a dropout layer [23] on the output of the gate π(·). Note that dropout is applied per concept"
  - [abstract] "we apply sparsity to the weights of the linear layer...However, we found that it does not lead to few concepts being used across all classes"
  - [corpus] Weak evidence - no directly comparable dropout-on-concepts approaches found
- Break condition: If the concept space is too small or concepts are highly correlated, dropout may remove essential information

## Foundational Learning

- Concept: Dictionary Learning Problem
  - Why needed here: Forms the theoretical foundation for unsupervised concept discovery
  - Quick check question: How does minimizing ||A - UCT||F relate to finding interpretable concepts?

- Concept: Sparsity Regularization (Elastic Net)
  - Why needed here: Enforces both L1 and L2 penalties to achieve interpretable weight matrices
  - Quick check question: Why use (1-α)||W||F + α||W||1,1 instead of pure L1 or L2 regularization?

- Concept: Input-Dependent Gating
  - Why needed here: Allows dynamic selection of relevant concepts per input sample
  - Quick check question: How does the offset parameter o affect which concepts pass through the gate?

## Architecture Onboarding

- Component map: Black-box model → Dictionary Learning → Concept Space → Input-Dependent Gate → Sparse Linear Layer → Classification
- Critical path: Dictionary learning → Concept selection → Sparse classification
- Design tradeoffs: More concepts improve performance but reduce sparsity; stronger regularization increases sparsity but may hurt accuracy
- Failure signatures: Low cosine similarities across all concepts, over-reliance on single concept, poor downstream performance despite good concept visualization
- First 3 experiments:
  1. Vary number of concepts (50, 200, 1000) and measure downstream accuracy to find optimal concept count
  2. Sweep λπ values to understand sparsity-performance tradeoff
  3. Compare UCBM with and without concept dropout to quantify over-reliance prevention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of UCBM scale with the number of concepts when the concept space is learned from multiple layers of the black-box model instead of just the bottleneck layer?
- Basis in paper: [inferred] The paper mentions that extracting concepts across layers could leverage a hierarchy of concepts, which may benefit interpretable models
- Why unresolved: The authors only extract concepts from the bottleneck layer and do not explore the impact of using concepts from multiple layers on downstream performance
- What evidence would resolve it: Experimental results comparing UCBM's performance using concepts from different layers (early, middle, late) of the black-box model to the current bottleneck layer approach

### Open Question 2
- Question: What is the impact of concept entanglement on the interpretability and performance of UCBM, and how can it be mitigated?
- Basis in paper: [explicit] The paper mentions that prior work has shown that the choice of training samples with human-annotated concepts can lead to entangled concepts
- Why unresolved: The authors do not investigate the extent of concept entanglement in their discovered concepts or propose methods to address it
- What evidence would resolve it: Analysis of concept similarity and correlation in the discovered concepts, along with experiments testing the impact of concept disentanglement techniques on UCBM's performance and interpretability

### Open Question 3
- Question: How does the performance of UCBM compare to other unsupervised concept discovery methods, such as sparse autoencoders or multi-dimensional concept discovery (MCD)?
- Basis in paper: [explicit] The paper mentions that UCBM can benefit from improvements in unsupervised concept discovery methods and that MCD is a unifying framework with completeness guarantees
- Why unresolved: The authors only use non-negative matrix factorization for concept discovery and do not compare UCBM's performance to other state-of-the-art unsupervised concept discovery methods
- What evidence would resolve it: Experimental results comparing UCBM's performance using different unsupervised concept discovery methods, such as sparse autoencoders and MCD, to the current non-negative matrix factorization approach

## Limitations
- The linear dictionary approximation may fail for highly entangled or non-linear concept representations in the black-box model
- Concept discovery quality depends heavily on the quality and diversity of the black-box model's activations used for training
- The paper doesn't thoroughly address scalability concerns when dealing with very large concept spaces or high-dimensional activations

## Confidence
- High confidence: The technical implementation of dictionary learning and sparsity regularization is well-grounded and clearly specified
- Medium confidence: The claim that UCBM significantly improves downstream performance is supported by experiments, though comparisons with stronger baselines would strengthen this
- Medium confidence: The input-dependent concept selection mechanism effectively enforces sparsity, but the optimal threshold determination (offset parameter) requires further validation

## Next Checks
1. Evaluate UCBM performance across different black-box model architectures (e.g., Vision Transformers vs. ResNets) to verify concept discovery generalizes beyond a single architecture
2. Implement human evaluation studies to measure the semantic interpretability of discovered concepts compared to predefined concepts, addressing whether unsupervised discovery truly captures meaningful concepts
3. Test the approach with progressively larger concept spaces (1000+ concepts) to identify when the linear approximation breaks down and determine practical limits for concept count