---
ver: rpa2
title: 'Comparing Spectral Bias and Robustness For Two-Layer Neural Networks: SGD
  vs Adaptive Random Fourier Features'
arxiv_id: '2402.00332'
source_url: https://arxiv.org/abs/2402.00332
tags:
- neural
- bias
- spectral
- networks
- arff
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares two-layer neural networks trained with Stochastic
  Gradient Descent (SGD) and Adaptive Random Fourier Features (ARFF) in terms of spectral
  bias and robustness to adversarial noise attacks. The authors define spectral bias
  as the ratio of error in high-frequency to low-frequency domains, with zero bias
  indicating equal performance across frequencies.
---

# Comparing Spectral Bias and Robustness For Two-Layer Neural Networks: SGD vs Adaptive Random Fourier Features

## Quick Facts
- arXiv ID: 2402.00332
- Source URL: https://arxiv.org/abs/2402.00332
- Authors: Aku Kammonen; Lisi Liang; Anamika Pandey; Raúl Tempone
- Reference count: 36
- Key outcome: ARFF-trained networks achieve near-zero spectral bias vs high bias for SGD, with ARFF showing superior robustness to sparse adversarial attacks when early stopping uses noisy validation data.

## Executive Summary
This paper investigates the relationship between spectral bias and adversarial robustness in two-layer neural networks trained with standard Stochastic Gradient Descent (SGD) versus Adaptive Random Fourier Features (ARFF). The authors define spectral bias as the ratio of high-to-low frequency error, with near-zero bias indicating balanced performance across frequencies. Through experiments on both synthetic functions and MNIST classification, they demonstrate that ARFF training significantly reduces spectral bias and improves robustness to sparse adversarial attacks, particularly when combined with noise-aware early stopping strategies.

## Method Summary
The study compares two training approaches: standard SGD with cosine activation and ARFF with Metropolis sampling for frequency selection. For spectral bias analysis, a two-layer network with 1024 neurons is trained on a test function f(x) = e^(-x²/2) Si(x/a) with a = 10^-2. For MNIST experiments, classifiers are trained to equal clean accuracy using both methods, then evaluated under random noise attacks at varying sparsity levels (50 vs 784 pixels) and noise intensities. Early stopping is implemented using clean validation data or validation data with added noise to study the impact on robustness.

## Key Results
- ARFF-trained networks achieve spectral bias close to zero, while SGD-trained networks maintain high bias values around 1
- Under sparse attacks (50 pixels), ARFF-trained networks show superior robustness compared to SGD-trained networks of equal clean accuracy
- With noisy validation data for early stopping, ARFF regains its advantage even under full attacks (784 pixels)
- The improved robustness correlates with ARFF's ability to capture high-frequency components more effectively than SGD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ARFF training achieves near-zero spectral bias by adaptively sampling frequencies from a distribution proportional to the magnitude of the target function's Fourier transform, whereas SGD samples frequencies randomly.
- Mechanism: In ARFF, the Metropolis sampling algorithm preferentially selects high-frequency components that contribute significantly to the target function's energy, allowing the network to learn these components more effectively. SGD, with its random initialization and uniform frequency sampling, tends to focus on low-frequency components first due to the spectral bias phenomenon documented in the literature.
- Core assumption: The target function's Fourier transform magnitude is accessible through the data or can be estimated sufficiently well through the sampling process.
- Evidence anchors:
  - [abstract]: "Our experiments demonstrate that an adaptive random Fourier features algorithm (ARFF) can yield a spectral bias closer to zero compared to the stochastic gradient descent optimizer (SGD)."
  - [section]: "Given only data {xn, yn}N n=1 means that ˆf typically is not accessible. The goal of the adaptive random Fourier features with Metropolis sampling algorithm (ARFF), presented in and denoted as Algorithm 1 in [2], is to approximately sample ω from p∗, given only data."
  - [corpus]: Weak - corpus does not contain papers specifically addressing spectral bias or ARFF algorithms.

### Mechanism 2
- Claim: ARFF-trained networks show superior robustness to sparse adversarial attacks because they have better representation of high-frequency components in their learned function.
- Mechanism: The network trained with ARFF has learned to represent high-frequency features that are often targeted by sparse attacks. When an attack modifies only a few pixels, the ARFF network can still rely on its learned high-frequency representations to maintain classification accuracy. SGD networks, being biased toward low-frequency content, lose more information when high-frequency pixels are corrupted.
- Core assumption: Adversarial attacks typically target high-frequency features or critical pixels that carry discriminative information.
- Evidence anchors:
  - [section]: "Experiment 1: We present the classification rates on the noisy test images, where npixel = 50, for different values of σ on the left in Figure 2 for both SGD and ARFF. Even though both training algorithms have trained the neural networks to the same classification rate, the accuracy of the neural networks trained by SGD decreases faster when σ increases compared to the accuracy of the neural networks trained by ARFF."
  - [corpus]: Weak - corpus does not contain papers specifically addressing the relationship between spectral bias and adversarial robustness.

### Mechanism 3
- Claim: Early stopping with noisy validation data allows ARFF-trained networks to outperform SGD in full attack scenarios by preventing overfitting to clean data.
- Mechanism: When validation data includes noise, the stopping criterion shifts to favor networks that generalize well under noisy conditions rather than those that perfectly fit clean training data. ARFF networks, with their broader frequency representation, are better equipped to handle this noisy validation scenario and thus continue training longer before overfitting, while SGD networks stop earlier based on clean-data performance metrics.
- Core assumption: The validation data with added noise provides a better proxy for test-time performance under attack than clean validation data.
- Evidence anchors:
  - [abstract]: "However, when early stopping is tuned using noisy validation data, ARFF-trained networks regain their advantage, suggesting a practical strategy for improving robustness through noise-aware validation."
  - [section]: "Experiment 3: As in Experiment 2 we use npixel = d = 784. The difference is that we also add noise, from N (0, σ2), to all 784 pixels of each image in the validation dataset and use the best classification rate on the validation data as stopping criterion for the training. We present the results on the right in Figure 2. The classification rate for SGD is qualitatively the same as in Experiment 2, but now the classification rate for ARFF is better than for SGD."
  - [corpus]: Weak - corpus does not contain papers specifically addressing early stopping with noisy validation data.

## Foundational Learning

- Concept: Spectral bias in neural networks - the tendency of networks to learn low-frequency components of target functions before high-frequency components.
  - Why needed here: Understanding spectral bias is crucial for interpreting why ARFF outperforms SGD in learning high-frequency content and for designing experiments that measure this bias.
  - Quick check question: If a network exhibits spectral bias with value close to 1, does this mean it performs better on high-frequency or low-frequency components?

- Concept: Fourier analysis and frequency domain representation of functions.
  - Why needed here: The paper relies heavily on Fourier transforms to define and measure spectral bias, and ARFF explicitly samples frequencies from a distribution related to the target function's Fourier spectrum.
  - Quick check question: What does it mean for a function's Fourier transform to have most of its energy concentrated at low frequencies?

- Concept: Adversarial attacks and robustness in machine learning.
  - Why needed here: The paper evaluates robustness to adversarial noise attacks, comparing how different training methods affect vulnerability to such attacks.
  - Quick check question: What is the key difference between sparse and full adversarial attacks in terms of how many input features they modify?

## Architecture Onboarding

- Component map:
  - Data preprocessing -> Model training (SGD/ARFF) -> Spectral bias measurement -> Robustness evaluation under noise attacks

- Critical path:
  1. Load and preprocess MNIST data (normalize, split)
  2. Train two sets of classifiers (one with SGD, one with ARFF) to equal accuracy
  3. Measure spectral bias of each trained network
  4. Evaluate robustness under sparse attacks (50 pixels)
  5. Evaluate robustness under full attacks (784 pixels)
  6. Repeat steps 4-5 with noisy validation for early stopping

- Design tradeoffs:
  - Frequency sampling: ARFF adaptively samples frequencies but requires Metropolis algorithm computation; SGD uses random initialization but is computationally simpler
  - Network width: Wider networks can represent more frequencies but increase computational cost and risk of overfitting
  - Noise level in validation: Higher noise levels may lead to earlier stopping but could prevent learning from clean data

- Failure signatures:
  - Spectral bias remains close to 1 for both methods: Indicates the target function may not have significant high-frequency content or the frequency sampling in ARFF is not working properly
  - SGD outperforms ARFF in sparse attacks: Could indicate the MNIST digits are primarily low-frequency features or ARFF is overfitting
  - Both methods show similar degradation in full attacks: Suggests the attack strength overwhelms any architectural advantages

- First 3 experiments:
  1. Train a two-layer network with SGD on a simple 1D function with known high-frequency components; measure spectral bias progression over epochs
  2. Train the same network with ARFF on the same function; compare spectral bias and final approximation error
  3. Train MNIST classifiers with both methods to equal clean accuracy; test on data with 50-pixel random noise at varying intensities

## Open Questions the Paper Calls Out
The paper identifies sophisticated adversarial attacks, such as the Square Attack, as a direction for future research to comprehensively explore ARFF training under well-structured attacks.

## Limitations
- The study only tests a simple additive random noise attack rather than more advanced adversarial attacks
- Experiments are limited to two-layer networks with cosine activation, which may not generalize to deeper architectures
- The computational overhead of ARFF compared to SGD is not quantified or discussed

## Confidence
- **High**: Basic claims about spectral bias definition and experimental methodology
- **Medium**: Claims about ARFF achieving near-zero spectral bias and early stopping improvements
- **Low**: Generalizability of results to other datasets and network architectures

## Next Checks
1. Test ARFF vs SGD on a broader range of datasets (e.g., CIFAR-10, ImageNet) with both natural and adversarial attacks to verify robustness claims generalize beyond MNIST
2. Measure computational overhead of ARFF training compared to SGD and quantify the trade-off between training time and robustness benefits
3. Conduct ablation studies varying network width, depth, and activation functions to determine which architectural choices are most critical for ARFF's spectral bias advantage