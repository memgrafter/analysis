---
ver: rpa2
title: 'HREF: Human Response-Guided Evaluation of Instruction Following in Language
  Models'
arxiv_id: '2412.15524'
source_url: https://arxiv.org/abs/2412.15524
tags:
- human
- responses
- evaluation
- prompt
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the reliability issue in automatic evaluation
  of instruction-following capabilities of large language models, which relies on
  LLM-as-a-judge but introduces biases. The authors propose leveraging human-written
  responses to enhance evaluation reliability, discovering that human responses provide
  an orthogonal perspective to model responses and should be used as additional context.
---

# HREF: Human Response-Guided Evaluation of Instruction Following in Language Models

## Quick Facts
- arXiv ID: 2412.15524
- Source URL: https://arxiv.org/abs/2412.15524
- Reference count: 40
- Primary result: Up to 3.2% improvement in agreement with human judges compared to existing methods

## Executive Summary
This paper addresses the reliability issues in automatic evaluation of instruction-following capabilities in large language models. The authors identify that standard LLM-as-a-judge methods introduce biases, particularly length bias, and propose using human-written responses as additional context to improve evaluation reliability. They develop a new benchmark called HREF that employs a composite evaluation setup using Llama-3.1-70B-Instruct as both baseline model and judge, incorporating human references to achieve up to 3.2% improvement in agreement with human judges.

## Method Summary
The method involves collecting 4,258 samples across 11 task categories, generating responses from 32 different LLMs, and evaluating them using a composite approach. The evaluation combines LLM-as-a-Judge, LLM-as-a-Judge with human responses, and embedding-based methods. The benchmark uses Llama-3.1-70B-Instruct as the judge model with human reference context included, selects the most reliable evaluation method for each task category, and maintains a private test set to prevent contamination.

## Key Results
- Human-written responses improve agreement with human judges by up to 3.2% compared to existing methods
- Llama-3.1-70B-Instruct shows lower length bias than GPT models, with human references further reducing it to 1.4%
- Task-specific evaluation methods outperform one-size-fits-all approaches across different instruction-following tasks

## Why This Works (Mechanism)

### Mechanism 1
Human-written responses provide an orthogonal perspective to model-generated responses in instruction-following evaluation. When comparing two model responses, human-written references serve as additional context that highlights dimensions of quality overlooked by model judges, such as correctness and style preferences that align with human values. This works because human judges and model judges prioritize different aspects of response quality, and human references can bridge this gap by providing a concrete example of human preferences.

### Mechanism 2
Using Llama-3.1-70B-Instruct as judge reduces length bias compared to GPT models. Llama-3.1-70B-Instruct naturally has lower preference for longer responses without requiring explicit length normalization, making it a more reliable judge for pairwise comparison. This works because length bias is a significant source of unreliability in LLM-as-a-judge methods, and some models are inherently less biased.

### Mechanism 3
Task-specific evaluation methods improve reliability over one-size-fits-all approaches. Different task categories have varying response characteristics and evaluation criteria, so selecting the most reliable evaluation method for each category reduces systematic errors. This works because the reliability of evaluation methods varies significantly across different types of instruction-following tasks.

## Foundational Learning

- **Leave-One-Out (LOO) agreement rate**: Provides a robust measure of how well an evaluation method aligns with human judgments by testing consistency across multiple annotators. Quick check: If you have 4 human annotations for a sample, how many combinations of 3 annotations do you need to compute the LOO agreement rate?

- **Pairwise preference evaluation**: HREF uses pairwise comparison rather than absolute scoring, requiring understanding of how to compare two responses given the same instruction. Quick check: In a pairwise evaluation, what does it mean when the method selects "tie" between two model responses?

- **Embedding similarity metrics**: HREF uses embedding-based methods (RoBERTa and Rouge) as part of its composite evaluation, requiring understanding of how text embeddings capture semantic similarity. Quick check: How does cosine similarity between text embeddings relate to the semantic similarity of two responses?

## Architecture Onboarding

- **Component map**: Instruction collection -> Response generation (Llama-3.1-405B-Instruct-FP8) -> Evaluation method selection (composite) -> Judge model interface (Llama-3.1-70B-Instruct with human reference) -> Leaderboard system

- **Critical path**: Instruction → Response generation → Evaluation (composite method selection) → Score aggregation → Leaderboard ranking

- **Design tradeoffs**: Private vs public test set (prevents contamination but limits reproducibility), Task-centric vs uniform distribution (better insights but may not reflect real-world usage), Human reference inclusion (improves agreement but increases complexity and cost)

- **Failure signatures**: Low correlation between development and test set results indicates distribution shift, sudden drops in agreement rates suggest judge model updates or response generation issues, category-specific failures point to evaluation method selection problems

- **First 3 experiments**: 1) Compare LOO agreement rates of Llama-3.1-70B-Instruct with and without human references on a small subset, 2) Test length bias measurement across different judge models on sampled data, 3) Validate composite method selection by running all evaluation methods on a single task category and comparing agreement rates

## Open Questions the Paper Calls Out

### Open Question 1
How do human-written responses improve evaluation reliability across different task categories, and which specific dimensions of responses do they help evaluate most effectively? While the paper demonstrates improvement in overall agreement rates, it doesn't provide a detailed breakdown of which specific aspects of responses (e.g., factual accuracy, helpfulness, natural language) benefit most from human reference inclusion across different task categories.

### Open Question 2
What is the optimal size for the evaluation set needed to reliably distinguish between strong language models while maintaining statistical significance? The paper mentions that with fewer than 2000 samples, the p-values at 90th quantile fall below 0.05, suggesting statistical significance, but notes that as models become stronger, larger evaluation sets may be needed.

### Open Question 3
How does the use of open-weight models as both baseline and judge affect the reliability and generalizability of evaluation compared to using closed API models? While the paper shows benefits of using open-weight models, it doesn't provide direct comparisons showing how this choice affects evaluation reliability and generalizability compared to established benchmarks using closed API models.

## Limitations
- Reliance on a single baseline judge model (Llama-3.1-70B-Instruct) may limit generalization to other judge models
- Private test set prevents community validation and independent reproducibility
- Human-written response collection may introduce selection bias not representative of diverse human writing styles

## Confidence

- **High Confidence**: The observation that human-written responses provide orthogonal perspective to model responses (supported by direct empirical comparison showing improved agreement rates)
- **Medium Confidence**: The claim that Llama-3.1-70B-Instruct has lower length bias than GPT models (based on comparison across four judge models, but limited sample size)
- **Medium Confidence**: The effectiveness of composite evaluation methods (supported by improved agreement rates, but dependent on accurate task categorization)

## Next Checks

1. Test the HREF methodology with alternative judge models (GPT-4, Claude) to assess generalizability of length bias findings and human reference benefits

2. Conduct ablation studies on the human reference context - measure agreement rates with and without human responses across different task categories

3. Validate the task categorization system by having multiple annotators classify samples independently and measuring inter-annotator agreement