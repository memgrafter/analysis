---
ver: rpa2
title: 'AntiLeakBench: Preventing Data Contamination by Automatically Constructing
  Benchmarks with Updated Real-World Knowledge'
arxiv_id: '2412.13670'
source_url: https://arxiv.org/abs/2412.13670
tags:
- llms
- data
- knowledge
- time
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses data contamination in LLM evaluation by proposing
  AntiLeakBench, an automated benchmarking framework that ensures strictly contamination-free
  evaluation. The key innovation is constructing test samples based on explicitly
  new real-world knowledge absent from LLMs' training sets, identified through analysis
  of Wikidata revision histories.
---

# AntiLeakBench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge

## Quick Facts
- **arXiv ID**: 2412.13670
- **Source URL**: https://arxiv.org/abs/2412.13670
- **Reference count**: 24
- **Primary result**: Most LLMs score below 50% accuracy on contamination-free benchmark, while proprietary models like GPT-4o achieve highest performance

## Executive Summary
AntiLeakBench addresses data contamination in LLM evaluation by constructing test samples based on knowledge that was added to Wikidata after the training cutoff dates of evaluated models. The framework uses a fully automated workflow that identifies real-world knowledge updates, retrieves corresponding Wikipedia revisions, and constructs contamination-free evaluation samples without human labor. Experimental results demonstrate that the benchmark effectively reveals data contamination issues in existing benchmarks, showing significant performance drops for most LLMs when evaluated on genuinely new knowledge.

## Method Summary
The framework constructs contamination-free benchmarks by analyzing Wikidata revision histories to identify knowledge that was added after specific LLM training cutoff dates. For each identified knowledge update, it retrieves the corresponding Wikipedia revision made after the knowledge's start time, ensuring the supporting document was not available during training. The system then automatically generates question-answer pairs using the updated knowledge and supporting Wikipedia articles, supporting both generation and multi-choice formats. The entire process is automated without human intervention, enabling frequent benchmark updates as new LLMs emerge.

## Key Results
- Most evaluated LLMs score below 50% accuracy on AntiLeakBench, with even the best proprietary models achieving only around 70% accuracy
- Performance drops significantly when evaluating on post-cutoff knowledge compared to existing benchmarks, revealing contamination issues
- The framework successfully constructs contamination-free samples across multiple languages with high context and answer accuracy rates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Constructing test samples with knowledge updated after LLMs' cutoff time ensures contamination-free evaluation.
- **Mechanism**: The framework identifies real-world knowledge that was added to Wikidata after the LLM's training cutoff date. Since this knowledge did not exist at training time, it cannot be memorized by the model, guaranteeing that evaluation is based on genuinely new information rather than memorization.
- **Core assumption**: Knowledge updates in Wikidata reliably correspond to events or facts that occurred after the cutoff date, and these updates are accurately timestamped.
- **Evidence anchors**:
  - [abstract]: "construct samples with explicitly new knowledge absent from LLMs' training sets"
  - [section 3.2]: "identify the new claim as the updated knowledge"
  - [corpus]: Weak - no direct corpus evidence about Wikidata update accuracy or timestamp reliability.
- **Break condition**: If Wikidata timestamps are inaccurate, or if knowledge updates are backdated to appear newer than they are, contamination could occur.

### Mechanism 2
- **Claim**: Using Wikipedia articles revised after the cutoff time as supporting documents ensures both context relevance and absence from training data.
- **Mechanism**: For each piece of updated knowledge, the framework retrieves the Wikipedia revision made after the knowledge's start time. Since Wikipedia content used in LLM training would have been from an earlier revision, the newer content cannot have been seen during training.
- **Core assumption**: Wikipedia revisions accurately reflect real-world knowledge updates and follow a strict chronological order.
- **Evidence anchors**:
  - [section 3.3]: "We rely on the well-maintained and widely trusted Wikipedia as the source of supporting documents"
  - [section 3.7]: "most produced samples are correct as verified by humans"
  - [corpus]: Weak - no direct evidence about Wikipedia revision timestamps or training data inclusion criteria.
- **Break condition**: If Wikipedia content is edited retroactively or if training data includes more recent Wikipedia dumps than assumed.

### Mechanism 3
- **Claim**: The automated workflow without human labor enables frequent benchmark updates, maintaining contamination-free evaluation as new LLMs emerge.
- **Mechanism**: By automating the entire process from data preparation to sample construction, the framework can be rerun whenever a new LLM is released, ensuring the benchmark always contains knowledge that postdates the latest training cutoff.
- **Core assumption**: The automated workflow correctly identifies all relevant knowledge updates and constructs valid samples without human oversight.
- **Evidence anchors**:
  - [abstract]: "fully automated workflow to build and update our benchmark without human labor"
  - [section 3.5]: "we only need to download the latest Wikidata dump and then execute our automated workflow"
  - [corpus]: Weak - no evidence about automation reliability or error rates.
- **Break condition**: If the automation fails to capture important knowledge updates or introduces systematic errors in sample construction.

## Foundational Learning

- **Concept**: Knowledge cutoff time
  - **Why needed here**: Understanding when an LLM's training data collection ended is crucial for determining what knowledge could have been memorized versus what is truly new.
  - **Quick check question**: If an LLM was trained until September 2022, would it know about events that happened in November 2022?

- **Concept**: Data contamination in LLM evaluation
  - **Why needed here**: Recognizing how test data can leak into training sets explains why contamination-free benchmarks are necessary for fair evaluation.
  - **Quick check question**: Why might an LLM perform well on a benchmark that was published before its training cutoff?

- **Concept**: Wikidata and Wikipedia revision tracking
  - **Why needed here**: The framework relies on tracking when knowledge was added to these sources to ensure it postdates LLM training.
  - **Quick check question**: How can you determine if a Wikipedia article was edited after a specific date?

## Architecture Onboarding

- **Component map**: Wikidata dump processing -> Knowledge identification -> Wikipedia revision retrieval -> Sample construction -> Benchmark output
- **Critical path**: Wikidata dump → Knowledge identification → Wikipedia revision retrieval → Sample construction → Benchmark output
- **Design tradeoffs**: 
  - Using real Wikipedia articles ensures authenticity but limits control over context length and format
  - Automated workflow reduces maintenance costs but may miss edge cases that human curators would catch
  - Focusing on question-answering tasks enables precise knowledge control but excludes other evaluation paradigms
- **Failure signatures**: 
  - Low context accuracy scores indicate Wikipedia revision retrieval issues
  - Low answer accuracy despite high context accuracy suggests question generation problems
  - Performance drops before cutoff time may indicate incomplete training data coverage rather than contamination
- **First 3 experiments**:
  1. Verify knowledge identification by checking that no pre-cutoff knowledge appears in post-cutoff samples
  2. Test Wikipedia revision retrieval by confirming that retrieved articles were indeed revised after cutoff
  3. Validate sample construction by running a small set through multiple LLMs and checking for contamination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum frequency of benchmark updates required to maintain effective contamination-free evaluation across rapidly evolving LLM models?
- Basis in paper: [inferred] The paper emphasizes automated workflows and reduced maintenance costs but doesn't specify optimal update frequencies
- Why unresolved: The paper demonstrates automated updating capability but doesn't provide empirical data on how different update intervals affect evaluation effectiveness
- What evidence would resolve it: Comparative studies showing performance differences between weekly, monthly, and quarterly benchmark updates across multiple LLM generations

### Open Question 2
- How does the quality of supporting documents from Wikipedia affect LLM performance compared to documents from other knowledge sources?
- Basis in paper: [explicit] The paper specifically uses Wikipedia as the supporting document source and mentions potential accuracy issues
- Why unresolved: While the paper verifies data quality, it doesn't compare Wikipedia's effectiveness against other sources like academic databases or news archives
- What evidence would resolve it: Systematic comparison of LLM performance using supporting documents from Wikipedia versus alternative real-world sources for identical knowledge updates

### Open Question 3
- What is the relationship between the complexity of multi-hop chains and the performance gap between proprietary and open-source LLMs?
- Basis in paper: [explicit] The paper shows proprietary models perform better but doesn't analyze how this advantage scales with chain complexity
- Why unresolved: The paper presents overall performance differences but doesn't examine how these differences evolve as reasoning requirements increase
- What evidence would resolve it: Detailed analysis of proprietary versus open-source LLM performance across varying chain lengths (H=1 to H=5+) with statistical significance testing

### Open Question 4
- How does the presence of distracting documents in long-context tasks affect different LLM architectures' ability to maintain performance?
- Basis in paper: [explicit] The paper introduces Nd parameters but doesn't analyze architectural differences in handling distractions
- Why unresolved: While performance degradation with more distractors is shown, the paper doesn't explore why different architectures handle this differently
- What evidence would resolve it: Comparative analysis of attention mechanisms, transformer depths, and context window sizes across model families when handling increasing numbers of distracting documents

### Open Question 5
- What is the optimal balance between generation and multi-choice formats for comprehensive contamination-free evaluation?
- Basis in paper: [explicit] The paper shows multi-choice is easier but doesn't determine the ideal ratio for thorough assessment
- Why unresolved: The paper demonstrates format differences but doesn't provide guidance on when to use each format or their complementary strengths
- What evidence would resolve it: Empirical studies showing how different mixes of generation and multi-choice tasks affect detection of various types of contamination and reasoning capabilities across model families

## Limitations

- The framework's effectiveness depends entirely on the accuracy and completeness of Wikidata and Wikipedia revision timestamps, which may not reliably reflect when knowledge was added to the world
- While the framework supports multilingual evaluation, the paper primarily focuses on English and Chinese, with limited validation of other language pairs
- The current implementation focuses on question-answering tasks, which may not adequately evaluate all aspects of LLM capabilities, particularly those requiring reasoning, creativity, or long-form generation

## Confidence

- **Core Contamination Prevention Mechanism**: Medium confidence - The approach is theoretically sound but depends on external data source reliability
- **Automated Workflow Effectiveness**: Medium confidence - No systematic error analysis or automation reliability metrics provided
- **Benchmark Difficulty Claims**: High confidence - Performance results are clearly presented and show consistent patterns across multiple models
- **Maintenance Cost Reduction**: Low confidence - While the paper claims automation reduces costs, no comparative analysis with human-curated benchmarks is provided

## Next Checks

1. **Source Data Verification**: Conduct an independent audit of Wikidata and Wikipedia revision timestamps for a sample of knowledge items to verify that they accurately reflect when the knowledge was added to the world, not just when it was added to these databases.

2. **Cross-Model Contamination Analysis**: Test whether LLMs trained on different cutoff dates show differential performance on AntiLeakBench samples, which would provide stronger evidence that the benchmark successfully isolates post-training knowledge.

3. **Automation Reliability Assessment**: Implement systematic error tracking in the automated workflow to quantify the rate of incorrectly constructed samples, particularly focusing on cases where knowledge identification, document retrieval, or sample construction fails.