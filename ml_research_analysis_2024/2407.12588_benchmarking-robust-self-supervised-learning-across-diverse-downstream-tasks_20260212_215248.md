---
ver: rpa2
title: Benchmarking Robust Self-Supervised Learning Across Diverse Downstream Tasks
arxiv_id: '2407.12588'
source_url: https://arxiv.org/abs/2407.12588
tags:
- adversarial
- learning
- robustness
- tasks
- downstream
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the adversarial robustness of self-supervised
  vision encoders across multiple downstream tasks beyond classification, including
  semantic segmentation and depth estimation. The authors use embedding-space attacks
  (EmbedAttack) and task-specific attacks (PGD, SegPGD, DepthPGD) to assess robustness.
---

# Benchmarking Robust Self-Supervised Learning Across Diverse Downstream Tasks

## Quick Facts
- arXiv ID: 2407.12588
- Source URL: https://arxiv.org/abs/2407.12588
- Reference count: 18
- One-line primary result: State-of-the-art adversarial fine-tuning (DeACL) degrades clean performance and fails to protect against task-specific attacks for self-supervised vision encoders across classification, segmentation, and depth estimation tasks.

## Executive Summary
This paper evaluates adversarial robustness of self-supervised vision encoders across diverse downstream tasks including semantic segmentation and depth estimation, going beyond traditional classification benchmarks. The authors introduce embedding-space attacks (EmbedAttack) and task-specific attacks (PGD, SegPGD, DepthPGD) to assess robustness. Their key finding is that current state-of-the-art adversarial fine-tuning techniques like DeACL significantly degrade clean performance while only improving robustness against embedding attacks, completely failing to protect against task-specific attacks. For example, DeACL fine-tuned models show near-zero mIoU on segmentation tasks under task-specific attacks, indicating critical vulnerabilities.

## Method Summary
The paper evaluates adversarial robustness of self-supervised vision encoders (DINO, DINOv2 with ViT backbones) across three downstream tasks: classification, semantic segmentation, and depth estimation. They implement EmbedAttack using PGD in the embedding space to target the encoder's representations, and task-specific attacks including PGD for classification, SegPGD for segmentation (weighted loss over correctly/incorrectly classified pixels), and DepthPGD for depth estimation (combining multi-scale gradient matching and pixel-wise depth loss). DeACL fine-tuning is used as the defense mechanism, optimizing cosine distance between clean and adversarial representations. Performance is evaluated using accuracy for classification, mIoU for segmentation, and RMSE for depth estimation on datasets including CIFAR10/100, STL10, ADE20k, Cityscapes, PASCAL VOC 2012, and NYU-Depth-v2.

## Key Results
- DeACL fine-tuning improves robustness against embedding-space attacks but fails to protect against task-specific attacks across all downstream tasks
- DeACL significantly degrades clean performance, with near-zero mIoU on segmentation tasks under task-specific attacks
- No improvement in robustness against tailored PGD attacks was observed for DeACL fine-tuned models, with the only exception being on STL10 dataset
- Task-specific attacks are more effective than embedding-space attacks for their respective downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
Adversarial attacks in the embedding space (EmbedAttack) effectively degrade performance across multiple downstream tasks for self-supervised vision encoders by targeting the shared representation layer used by all downstream tasks. The EmbedAttack maximizes the ℓ2 distance between clean and adversarial embeddings, exploiting the task-agnostic features in the SSL encoder's learned representation space.

### Mechanism 2
Task-specific adversarial attacks (PGD, SegPGD, DepthPGD) are more effective than embedding-space attacks for their respective downstream tasks because they directly optimize the loss function specific to each task (classification cross-entropy, segmentation pixel-wise loss, depth estimation RMSE), allowing more targeted and efficient perturbations.

### Mechanism 3
DeACL fine-tuning improves robustness against embedding-space attacks but fails to protect against task-specific attacks due to a performance-robustness trade-off. DeACL primarily optimizes for embedding space robustness by minimizing cosine distance between representations of clean and adversarial examples, but this doesn't translate to robustness against task-specific loss landscapes.

## Foundational Learning

- **Self-supervised learning (SSL) and representation learning**: Needed to understand how SSL learns task-agnostic representations that are vulnerable to perturbations across multiple downstream tasks. Quick check: How does SSL differ from supervised learning in terms of training objectives and the type of representations it learns?

- **Adversarial attacks and defenses**: Essential for understanding the different attack methodologies (EmbedAttack, PGD, SegPGD, DepthPGD) and defense mechanisms (DeACL) evaluated in the paper. Quick check: What is the key difference between embedding-space attacks and task-specific attacks, and why might task-specific attacks be more effective?

- **Downstream task adaptation and evaluation metrics**: Required to understand how the paper evaluates encoder robustness across classification (accuracy), semantic segmentation (mIoU), and depth estimation (RMSE), each with its own adaptation method and metric. Quick check: How do the adaptation methods and evaluation metrics differ across the three downstream tasks, and why are they appropriate for each task?

## Architecture Onboarding

- **Component map**: SSL encoder (DINO/DINOv2) -> frozen feature extractor -> Task-specific adaptor (linear classifier/segmentation head/depth head) -> Attack module (EmbedAttack/PGD/SegPGD/DepthPGD) -> Evaluation module (accuracy/mIoU/RMSE)

- **Critical path**: 1) Train SSL encoder on large unlabeled dataset, 2) Freeze encoder and train task-specific adaptor on downstream dataset, 3) Generate adversarial examples using appropriate attack method, 4) Evaluate performance of clean and adversarial examples on downstream task

- **Design tradeoffs**: Trade-off between clean performance and adversarial robustness; choice of attack method (embedding-space vs. task-specific) and its effectiveness; computational cost of fine-tuning (DeACL vs. training from scratch)

- **Failure signatures**: Near-zero performance on downstream tasks under adversarial attacks; significant drop in clean performance after adversarial fine-tuning; ineffective improvement in robustness against task-specific attacks despite DeACL fine-tuning

- **First 3 experiments**: 1) Evaluate EmbedAttack on a clean SSL encoder across all three downstream tasks to establish baseline vulnerability, 2) Apply DeACL fine-tuning to the encoder and re-evaluate robustness against EmbedAttack and task-specific attacks to assess the effectiveness of the defense, 3) Experiment with multi-perturbation adversarial training by combining EmbedAttack and task-specific attacks during fine-tuning to explore potential improvements in cross-task robustness

## Open Questions the Paper Calls Out

### Open Question 1
Why does DeACL fine-tuning improve robustness against embedding attacks but not task-specific attacks (PGD, SegPGD, DepthPGD)? The paper demonstrates this phenomenon but doesn't provide a theoretical explanation for why embedding-space robustness doesn't transfer to task-specific robustness. Experiments testing whether combining embedding-space and task-specific adversarial training could provide cross-task robustness, or theoretical analysis of the relationship between embedding space and task-specific adversarial perturbations would resolve this.

### Open Question 2
Would multi-perturbation adversarial training that includes both embedding-space and task-specific attacks simultaneously improve cross-task robustness for SSL encoders? The authors suggest this direction but don't implement or test multi-perturbation training. Training SSL encoders with combined embedding-space and task-specific adversarial examples and evaluating their robustness across all downstream tasks would provide evidence.

### Open Question 3
Why does the distribution mismatch between fine-tuning and evaluation datasets affect DeACL's effectiveness against PGD attacks? The authors observe that DeACL shows improved robustness against PGD attacks on STL10 (a subset of ImageNet) but not on CIFAR datasets, suggesting dataset distribution affects effectiveness. Systematic experiments varying the relationship between fine-tuning and evaluation datasets, and testing whether domain adaptation techniques during fine-tuning could improve cross-dataset robustness would resolve this.

## Limitations
- Evaluation focuses primarily on vision transformers (ViT) with DINO and DINOv2 backbones, leaving uncertainty about whether similar vulnerabilities exist in other SSL architectures
- Analysis is limited to ℓ∞-bounded perturbations (ε=8/255), and behavior under different perturbation constraints or attack norms remains unexplored
- Does not investigate the impact of encoder architecture scale on robustness trade-offs, which could be significant given the trend toward larger vision models

## Confidence

**High confidence:** The empirical observation that DeACL fine-tuning degrades clean performance while failing to improve robustness against task-specific attacks is well-supported by experimental results showing near-zero mIoU on segmentation tasks. The mechanism that embedding-space attacks exploit shared representation vulnerabilities is logically sound and supported by attack design.

**Medium confidence:** The claim that multi-perturbation adversarial training could improve cross-task robustness is speculative, based primarily on observed failure of single-perturbation defenses, lacking empirical validation within the paper itself.

**Low confidence:** The assertion that existing robustness methods are "insufficient" for multi-task robustness may be premature, as the paper only evaluates one specific fine-tuning method (DeACL) and does not explore the full space of possible defenses or their hyperparameters.

## Next Checks

1. **Cross-architecture validation**: Replicate experiments using different SSL architectures (MAE, SimCLR, ConvNeXt) to determine if the observed robustness-accuracy trade-off is specific to ViT-based encoders or a more general phenomenon across SSL methods.

2. **Alternative defense exploration**: Implement and evaluate multi-perturbation adversarial training that combines EmbedAttack and task-specific attacks during fine-tuning to empirically test whether this approach can achieve better cross-task robustness than DeACL.

3. **Perturbation constraint analysis**: Conduct experiments varying the perturbation budget (ε values) and attack norms (ℓ2, ℓ∞) to understand how different attack configurations affect the observed robustness trade-offs across downstream tasks.