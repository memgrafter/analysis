---
ver: rpa2
title: How to Connect Speech Foundation Models and Large Language Models? What Matters
  and What Does Not
arxiv_id: '2409.17044'
source_url: https://arxiv.org/abs/2409.17044
tags:
- adapter
- speech
- whisper
- base
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated how the choice of Speech Foundational Models
  (SFMs), Large Language Models (LLMs), and adapter modules impacts speech-to-text
  performance. By systematically combining 2 SFMs (Whisper and SeamlessM4T), 2 LLMs
  (Mistral and Llama), and 5 adapter variants, the research found that the SFM choice
  was the most critical factor, with seamlessM4T outperforming Whisper by over 2 COMET
  points in speech translation and 1 WER point in automatic speech recognition.
---

# How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not

## Quick Facts
- arXiv ID: 2409.17044
- Source URL: https://arxiv.org/abs/2409.17044
- Reference count: 0
- Primary result: SFM choice (Whisper vs SeamlessM4T) has largest impact on speech-to-text performance; adapter choice has moderate, SFM-LLM-dependent impact; length compression less critical than assumed

## Executive Summary
This paper systematically investigates how the choice of Speech Foundational Models (SFMs), Large Language Models (LLMs), and adapter modules impacts speech-to-text performance. By combining two SFMs (Whisper and SeamlessM4T), two LLMs (Mistral and Llama), and five adapter variants, the study reveals that SFM selection is the most critical factor, with SeamlessM4T outperforming Whisper by over 2 COMET points in speech translation and 1 WER point in automatic speech recognition. The adapter choice shows moderate impact and varies based on the SFM-LLM combination, with no universal optimal design. Notably, adapters with different compression factors, including those without compression, perform competitively, suggesting sequence length mismatch reduction is less crucial than previously assumed.

## Method Summary
The study evaluates SFM-LLM-adapter combinations for ASR and ST tasks using CoVoST 2 and MuST-C datasets. The experimental setup employs two SFMs (Whisper, SeamlessM4T), two LLMs (Mistral, Llama), and five adapter variants (Base, Conv-based, CIF-based, CTC-based, WLQ-former). All SFMs and LLMs remain frozen during training, with only adapter parameters updated using cross-entropy loss and AdamW optimizer with cosine scheduler. Performance is measured using COMET for speech translation and WER for automatic speech recognition, with statistical significance assessed via bootstrap resampling.

## Key Results
- SFM choice (Whisper vs SeamlessM4T) has the largest impact on downstream performance, with SeamlessM4T outperforming Whisper by over 2 COMET points in speech translation and 1 WER point in ASR
- Adapter choice has moderate impact and depends on the specific SFM-LLM combination, with no universal optimal design
- Sequence length mismatch reduction between speech and text representations is less critical than previously assumed, as both high-compression and no-compression adapters achieve competitive results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The choice of Speech Foundational Model (SFM) has the largest impact on downstream ASR and ST performance.
- Mechanism: SFMs like Whisper and SeamlessM4T provide the foundational audio representations that adapters and LLMs build upon. Differences in their architecture, such as temporal resolution (Whisper: 50 fps vs SeamlessM4T: 6.25 fps) and layer design (Transformer vs Conformer), lead to significantly different quality of input embeddings for downstream tasks.
- Core assumption: The encoder's audio representation quality is the dominant factor in final task performance, outweighing the influence of the adapter and LLM choices.
- Evidence anchors:
  - [abstract] "our results demonstrate that the SFM plays a pivotal role in downstream performance"
  - [section] "the choice of the SFM is the most critical factor in terms of downstream performance"
  - [corpus] Weak evidence: no direct SFM comparison studies in neighbors; corpus does not provide comparative encoder performance data.
- Break condition: If the adapter or LLM introduces significant noise or mismatches that outweigh the SFM's representation quality, the SFM dominance may not hold.

### Mechanism 2
- Claim: There is no universal optimal adapter design; the best choice depends on the SFM and LLM combination.
- Mechanism: Adapters perform modality and length adaptation to bridge the SFM output and LLM input. Since SFMs differ in sequence length and representation style, and LLMs differ in robustness to input formats, the optimal adapter compression strategy and architecture vary. For example, the Base adapter (no compression) works well with Mistral, while the WLQ-former (high compression) works better with Llama and SeamlessM4T.
- Core assumption: Adapter effectiveness is conditional on the characteristics of the SFM and LLM it connects, not on a fixed "best" design.
- Evidence anchors:
  - [abstract] "the adapter choice has moderate impact and depends on the SFM and LLM"
  - [section] "results clearly show that there is no one-size-fits-all solution for the adapter"
  - [corpus] Weak evidence: corpus lacks comparative adapter studies across different SFM-LLM combinations.
- Break condition: If all adapters perform similarly regardless of SFM-LLM pairing, the dependency claim would be invalidated.

### Mechanism 3
- Claim: Reducing sequence length mismatch between speech and text representations is less critical than previously assumed.
- Mechanism: Despite large differences in output sequence lengths between SFMs (Whisper: 50 fps, SeamlessM4T: 6.25 fps), and the textual target at ~2.7 tokens/sec, adapters with both high compression (WLQ-former) and no compression (Base) achieve competitive results. This suggests LLMs can handle varied input lengths without degradation, making aggressive length adaptation unnecessary for performance.
- Core assumption: The LLM's robustness to input sequence length reduces the need for strict alignment between speech and text sequence lengths.
- Evidence anchors:
  - [section] "the Base adapter, which does not compress the speech sequence, and the WLQ-former, which has high compression factors (16 with Whisper), achieve competitive scores in most settings"
  - [section] "these insights suggest that reducing the length mismatch between textual and speech representations is not critical for the quality of the outputs"
  - [corpus] Weak evidence: corpus does not provide length adaptation performance comparisons.
- Break condition: If future studies show that sequence length mismatch consistently harms performance regardless of LLM robustness, this mechanism would break.

## Foundational Learning

- Concept: Speech representation extraction by SFMs
  - Why needed here: The quality and characteristics of the audio embeddings produced by the SFM directly determine the downstream task performance, making it essential to understand how different SFMs encode speech.
  - Quick check question: How do Whisper and SeamlessM4T differ in their audio encoding approach (e.g., architecture, temporal resolution)?

- Concept: Adapter design for modality and length adaptation
  - Why needed here: Adapters bridge the SFM and LLM by mapping audio embeddings into LLM-compatible space and adjusting sequence length. Understanding their design and trade-offs is critical for optimizing performance.
  - Quick check question: What are the differences between content-based and fixed-factor length adapters, and when might each be preferable?

- Concept: Sequence length handling by LLMs
  - Why needed here: LLMs' ability to process variable-length inputs affects whether aggressive length compression is necessary, impacting adapter design choices.
  - Quick check question: How does the LLM handle input sequences of very different lengths, and what impact does this have on task performance?

## Architecture Onboarding

- Component map: SFM encoder → Adapter (length + modality) → LLM decoder → Output (ASR/ST). Key variables: SFM choice, adapter type, LLM choice.
- Critical path: Audio input → SFM encoding → Adapter processing → LLM decoding → Text output. The SFM and adapter stages are the most performance-sensitive.
- Design tradeoffs: High compression adapters reduce computation but may lose information; no-compression adapters preserve detail but increase LLM input size. Adapter choice must balance these against SFM-LLM compatibility.
- Failure signatures: Poor SFM choice leads to consistently low performance across adapters; mismatched adapter-LLM pairs show inconsistent or degraded results; excessive compression causes loss of critical speech features.
- First 3 experiments:
  1. Test all 5 adapters with Whisper + Mistral to establish baseline performance and identify sensitivity to adapter choice.
  2. Replace Whisper with SeamlessM4T, keeping the same adapter and LLM, to measure SFM impact.
  3. Swap Mistral for Llama with the best Whisper adapter, then with the best SeamlessM4T adapter, to evaluate LLM sensitivity and interaction effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal adapter design vary across different combinations of SFMs and LLMs beyond the two tested models?
- Basis in paper: [explicit] The paper shows that adapter choice depends on SFM and LLM combinations but only tested Whisper/SeamlessM4T with Mistral/Llama
- Why unresolved: Limited to two SFMs and two LLMs, preventing generalization of findings
- What evidence would resolve it: Systematic testing with additional SFM and LLM combinations to identify consistent patterns or establish broader design principles

### Open Question 2
- Question: What is the relationship between compression factor and performance degradation in content-based adapters?
- Basis in paper: [inferred] The paper notes content-based adapters (CIF-based, CTC-based) consistently underperform others but doesn't establish a quantitative relationship between compression factor and performance
- Why unresolved: Only tested fixed compression factors without varying them systematically
- What evidence would resolve it: Experiments varying compression factors within content-based adapters to establish performance curves

### Open Question 3
- Question: How does the Base adapter's lack of compression affect computational efficiency and latency compared to compressed alternatives?
- Basis in paper: [explicit] The paper notes Base adapter "does not compress the speech sequence" but doesn't analyze computational trade-offs
- Why unresolved: No timing or efficiency measurements were reported
- What evidence would resolve it: Detailed benchmarking of inference time, memory usage, and throughput across different adapter types

### Open Question 4
- Question: Does the optimal adapter choice change when extending to additional speech-to-text tasks like speech summarization or spoken question answering?
- Basis in paper: [inferred] The paper focuses on ASR and ST but notes that "the optimal choice varies depending on the specific combination of SFM and LLM"
- Why unresolved: Limited to two specific tasks without exploring broader applications
- What evidence would resolve it: Systematic evaluation of adapter performance across diverse speech-to-text tasks to identify task-specific optimal designs

## Limitations

- The study only evaluates two SFMs and two LLMs, limiting generalizability to other model architectures
- Adapter comparisons are limited to five variants, potentially missing other effective design approaches
- Performance metrics focus solely on COMET and WER without investigating computational efficiency, latency, or robustness to noise
- All experiments use English as the source language, restricting conclusions about multilingual performance
- The frozen SFM and LLM approach prevents optimization of these components for the downstream tasks

## Confidence

- **SFM Choice Dominance (High Confidence)**: The experimental results consistently show 2+ COMET point differences and 1+ WER point differences across all adapter-LLM combinations, with statistical significance established through bootstrap resampling.
- **No Universal Optimal Adapter Design (Medium Confidence)**: While results show adapter performance varies with SFM-LLM combinations, the moderate impact and limited sample of adapter designs reduce confidence.
- **Sequence Length Mismatch Insensitivity (Medium Confidence)**: The finding that both high-compression and no-compression adapters perform competitively is compelling but primarily comparative rather than causal.

## Next Checks

1. **Cross-Architecture Generalization Test**: Evaluate the same SFM-LLM-adapter combinations with additional SFMs (e.g., AudioPaLM, Qwen-Audio) and LLMs (e.g., GPT-4o, Claude) to determine whether SFM dominance and adapter dependency generalize beyond the current model pairs.

2. **Fine-tuning Impact Analysis**: Repeat key experiments with fine-tuned rather than frozen SFMs and LLMs to assess whether component optimization alters the relative importance of SFM choice versus adapter design.

3. **Sequence Length Robustness Evaluation**: Systematically vary the sequence length compression factors across a wider range and measure both performance and computational efficiency to establish whether there are critical thresholds where length mismatch begins to impact quality.