---
ver: rpa2
title: 'LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation'
arxiv_id: '2406.12832'
source_url: https://arxiv.org/abs/2406.12832
tags:
- lamda
- fine-tuning
- lora
- trainable
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LaMDA, a novel parameter-efficient fine-tuning
  method for large language models that significantly reduces both trainable parameters
  and activation memory compared to existing approaches like LoRA. The key idea is
  to freeze one projection matrix while keeping only a low-dimensional square adapter
  matrix trainable, combined with a gradual freezing strategy for the second projection
  matrix based on singular values.
---

# LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation

## Quick Facts
- **arXiv ID**: 2406.12832
- **Source URL**: https://arxiv.org/abs/2406.12832
- **Reference count**: 21
- **Primary result**: Achieves up to 17.7× fewer trainable parameters and 1.32× lower peak GPU memory usage while maintaining comparable or better performance than LoRA baselines across various tasks and model architectures

## Executive Summary
This paper introduces LaMDA, a novel parameter-efficient fine-tuning method for large language models that significantly reduces trainable parameters and activation memory compared to existing approaches like LoRA. The key innovation involves freezing one projection matrix while keeping only a low-dimensional square adapter matrix trainable, combined with a gradual freezing strategy for the second projection matrix based on singular values. An enhanced version, LaMDA++, introduces adaptive rank allocation across layers using normalized spectrum analysis of pre-trained weights. Experimental results demonstrate substantial efficiency gains while maintaining or improving performance across different model architectures (DeBERTa-V3, BART-large, and LLaMA2-7B) on various tasks including GLUE benchmark, text summarization, and complex reasoning.

## Method Summary
LaMDA builds on LoRA's low-rank adaptation approach but introduces a key modification: freezing one projection matrix (PMA) from the start while keeping only a low-dimensional square adapter matrix (LDA) trainable. The second projection matrix (PMB) is initially trainable but gradually frozen row by row based on a linear schedule over the first 30% of training iterations. This design reduces trainable parameters from 2×d×r (in LoRA) to r², making them independent of the model's embedding dimension d. LaMDA++ enhances this by analyzing the normalized energy-score of pre-trained weights to allocate adaptive ranks across layers, assigning higher ranks to layers with lower energy-scores and lower ranks to layers with higher energy-scores. Both PMA and PMB are initialized using singular vectors from the pre-trained weights, while LDA is initialized as an identity matrix.

## Key Results
- Achieves up to 17.7× fewer trainable parameters compared to LoRA
- Reduces peak GPU memory usage by up to 1.32×
- Maintains comparable or better performance on GLUE benchmark, text summarization, and complex reasoning tasks
- Demonstrates effectiveness across different model architectures (DeBERTa-V3, BART-large, LLaMA2-7B)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LaMDA reduces trainable parameters and activation memory by freezing one projection matrix (PMA) and using a low-dimensional square adapter matrix (LDA).
- Mechanism: By freezing PMA and keeping only the LDA trainable, the number of trainable parameters becomes independent of the model's embedding dimension d, reducing it to r² where r ≪ d. Additionally, freezing PMA means that during backpropagation, the stored activations remain in the r-dimensional space rather than d-dimensional space, significantly reducing activation memory.
- Core assumption: The low-dimensional adapter (LDA) is sufficient to capture the necessary model adaptation without the need to train both projection matrices.
- Evidence anchors:
  - [abstract]: "LaMDA freezes a first projection matrix (PMA) in the adaptation path while introducing a low-dimensional trainable square matrix, resulting in substantial reductions in trainable parameters and peak GPU memory usage."
  - [section 3.1]: "By freezing AAA and BBB and keeping SSS trainable, we significantly reduce the number of trainable parameters, which is reduced to r2 ≪ 2 × d × r of LoRA, and is independent of the increasing model d."
  - [corpus]: Weak evidence. No direct citations or studies in corpus specifically confirming this mechanism.

### Mechanism 2
- Claim: LaMDA++ improves performance by allocating adaptive ranks across layers based on the normalized energy-score of pre-trained weights.
- Mechanism: LaMDA++ analyzes the normalized energy-score of pre-trained model weights to determine the importance of each layer. It then assigns higher ranks to layers with lower energy-scores and lower ranks to layers with higher energy-scores, optimizing the use of the rank budget.
- Core assumption: The energy-score of pre-trained weights is a good indicator of the importance of each layer for adaptation.
- Evidence anchors:
  - [section 3.3]: "LaMDA++ employs a pre-processing step to select the ranks of each LoRA path. Intuitively, ranks should be reduced from the budget of layers less affected by rank reduction and reallocated to layers that capture the least normalized energy-scores."
  - [section 3.3]: "Figure 3 reports the normalized energy-score of the first 32 singular vectors (Elr with r = 32 ) for each trainable linear module l of a LLaMA2-7B across all layers."
  - [corpus]: Weak evidence. No direct citations or studies in corpus specifically confirming this mechanism.

### Mechanism 3
- Claim: LaMDA's gradual freezing strategy for the second projection matrix (PMB) enhances parameter efficiency.
- Mechanism: PMB is kept trainable during the initial iterations of fine-tuning and then progressively frozen row by row based on the relative magnitude of the singular values. This allows the model to benefit from training PMB initially while reducing computational cost over time.
- Core assumption: Gradually freezing PMB based on singular values is an effective way to reduce computational cost without significantly impacting performance.
- Evidence anchors:
  - [section 3.2]: "We propose a linear schedule for the number of trainable rows in PMB... Since BBB is an r × d matrix, the input activation that needs to be stored for backpropagation is again in the r−dimensional space, so the memory-saving arguments still hold."
  - [section 4.5]: "LaMDA with ti set to 10% of the total iterations fails to outperform LoRA, whereas allocating 20% and 30% of the iterations to PMB training results in superior performance relative to LoRA."
  - [corpus]: Weak evidence. No direct citations or studies in corpus specifically confirming this mechanism.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: SVD is used to initialize the projection matrices (PMA and PMB) with the singular vectors corresponding to the most significant singular values of the pre-trained weights.
  - Quick check question: What is the purpose of using SVD to initialize the projection matrices in LaMDA?

- Concept: Low-rank approximation
  - Why needed here: Low-rank approximation is the basis for parameter-efficient fine-tuning methods like LoRA, which LaMDA builds upon.
  - Quick check question: How does low-rank approximation reduce the number of trainable parameters in LoRA?

- Concept: Activation memory
  - Why needed here: Understanding activation memory is crucial for optimizing GPU memory usage during fine-tuning.
  - Quick check question: Why does freezing PMA in LaMDA reduce activation memory compared to LoRA?

## Architecture Onboarding

- Component map:
  PMA (frozen) -> LDA (trainable) -> PMB (gradually frozen) -> Main path (frozen)

- Critical path:
  1. Initialize PMA and PMB with singular vectors of pre-trained weights
  2. Freeze PMA and keep LDA trainable
  3. Keep PMB trainable initially, then gradually freeze it
  4. Fine-tune the model with reduced trainable parameters and activation memory

- Design tradeoffs:
  - Reduced trainable parameters vs. potential performance degradation
  - Memory savings vs. computational overhead of gradual freezing
  - Adaptive rank allocation vs. simplicity of fixed rank allocation

- Failure signatures:
  - Performance degradation: LDA may not capture necessary adaptation
  - Memory usage not reduced: PMA not properly frozen or rank allocation not optimal
  - Slow convergence: Gradual freezing schedule may not be optimal

- First 3 experiments:
  1. Compare performance of LaMDA with different values of r (rank of LDA)
  2. Test the impact of different gradual freezing schedules for PMB
  3. Evaluate the effectiveness of adaptive rank allocation in LaMDA++

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LaMDA's performance scale when applied to larger models (e.g., LLaMA2-70B or GPT-4) compared to its performance on LLaMA2-7B?
- Basis in paper: [explicit] The authors note that "Due to time constraints associated with the paper's deadline, we could not extend our experiments to larger models, which could provide further insights into the scalability and effectiveness of LaMDA."
- Why unresolved: The paper only evaluates LaMDA on LLaMA2-7B, leaving uncertainty about its effectiveness on significantly larger models where parameter efficiency and memory savings might be even more critical.
- What evidence would resolve it: Experiments showing LaMDA's performance, parameter savings, and memory efficiency when fine-tuning LLaMA2-70B or other models with >50B parameters, compared to baselines like LoRA.

### Open Question 2
- Question: Can LaMDA be effectively applied to vision-language models, and how would its performance compare to specialized fine-tuning methods for such multimodal tasks?
- Basis in paper: [explicit] The authors state "We are also eager to test the applicability of our method to vision-language models, which was not explored in this paper."
- Why unresolved: Vision-language models represent a distinct architecture and training paradigm from pure language models, potentially requiring different adapter strategies or adaptation of LaMDA's freezing schedules.
- What evidence would resolve it: Experiments fine-tuning vision-language models like CLIP or Flamingo using LaMDA, measuring task performance, parameter efficiency, and memory usage compared to existing vision-language fine-tuning methods.

### Open Question 3
- Question: What is the relationship between task complexity and the intrinsic dimensionality of pre-trained weights, and how does this affect LaMDA's optimal freezing schedule and rank allocation?
- Basis in paper: [explicit] The authors hypothesize "for relatively complex tasks, like summarizing, complex reasoning, we believe intrinsic dimensionality of the weight may not be very low" and propose gradual freezing based on this assumption.
- Why unresolved: While the authors propose a gradual freezing strategy, they don't empirically establish how task complexity relates to model intrinsic dimensionality or validate whether their heuristic freezing schedule is optimal across different task types.
- What evidence would resolve it: Empirical studies measuring the intrinsic dimensionality of pre-trained weights across different task types, combined with ablation studies varying LaMDA's freezing schedules and rank allocations to identify optimal configurations for different task complexities.

### Open Question 4
- Question: How does LaMDA perform on instruction-following tasks, which often require nuanced understanding and generation capabilities beyond standard NLU/NLG benchmarks?
- Basis in paper: [explicit] The authors note "Our methodology, LaMDA, has not yet been tested on instruction-following tasks. While the current results are promising, evaluating the performance of LaMDA in these specific tasks is essential to fully understanding its potential and versatility."
- Why unresolved: Instruction-following tasks represent a different evaluation paradigm than traditional benchmarks, potentially requiring different fine-tuning dynamics and adapter strategies that LaMDA hasn't been tested against.
- What evidence would resolve it: Experiments fine-tuning models using LaMDA on instruction datasets like Alpaca, FLAN, or WizardLM, measuring performance on instruction following benchmarks and comparing against standard fine-tuning and LoRA approaches.

## Limitations

- The mechanism for activation memory reduction is asserted but not experimentally validated with memory profiling
- The correlation between pre-trained weight energy-scores and layer importance for adaptation is assumed rather than demonstrated through ablation studies
- The gradual freezing strategy lacks comparison with alternative scheduling strategies or comprehensive sensitivity analysis
- Runtime efficiency (training speed, FLOPs) is not analyzed, focusing only on parameter count and memory usage

## Confidence

- **High confidence**: Claims about reducing trainable parameters to r² instead of 2×d×r are mathematically sound and well-supported by the formulation. The performance parity or improvement over LoRA baselines on standard benchmarks is directly measured and reported.
- **Medium confidence**: Claims about activation memory reduction are plausible based on the mechanism but lack direct experimental validation through memory profiling. The effectiveness of the gradual freezing strategy is supported by some ablation studies but could benefit from more comprehensive analysis.
- **Low confidence**: Claims about the superiority of adaptive rank allocation in LaMDA++ over fixed rank allocation are based on single comparisons without thorough ablation studies examining how sensitive performance is to different rank allocation strategies.

## Next Checks

1. **Memory profiling validation**: Conduct GPU memory profiling during LaMDA training to verify that activation memory is indeed reduced to r-dimensional space rather than d-dimensional space, confirming the claimed 1.32× lower peak memory usage.

2. **Rank allocation ablation study**: Perform systematic experiments comparing LaMDA++ with various rank allocation strategies (random allocation, energy-score-based, fixed uniform allocation) across different tasks to quantify the actual contribution of the adaptive allocation mechanism.

3. **Runtime efficiency analysis**: Measure training throughput (samples/second) and total training time for LaMDA versus LoRA across different batch sizes and sequence lengths to provide a complete picture of computational efficiency beyond just parameter count and memory usage.