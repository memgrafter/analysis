---
ver: rpa2
title: 'Prompt Engineering a Schizophrenia Chatbot: Utilizing a Multi-Agent Approach
  for Enhanced Compliance with Prompt Instructions'
arxiv_id: '2410.12848'
source_url: https://arxiv.org/abs/2410.12848
tags:
- page
- chatbot
- user
- assistant
- message
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a Critical Analysis Filter (CAF) to improve
  the integrity of schizophrenia-focused chatbots by using a multi-agent system of
  AI overseers to monitor and refine responses. The system evaluates chatbot responses
  for compliance with instructions and source accuracy, using preliminary screening
  with GPT-3.5 followed by chief evaluation with GPT-4.
---

# Prompt Engineering a Schizophrenia Chatbot: Utilizing a Multi-Agent Approach for Enhanced Compliance with Prompt Instructions

## Quick Facts
- arXiv ID: 2410.12848
- Source URL: https://arxiv.org/abs/2410.12848
- Authors: Per Niklas Waaler; Musarrat Hussain; Igor Molchanov; Lars Ailo Bongo; Brita Elvevåg
- Reference count: 40
- Primary result: Multi-agent Critical Analysis Filter increased acceptable compliance scores from 8.7% to 67.0%

## Executive Summary
This paper introduces a Critical Analysis Filter (CAF) that uses a multi-agent system of AI overseers to monitor and refine schizophrenia-focused chatbot responses, substantially improving compliance with instructions and source accuracy. The system employs preliminary screening with GPT-3.5 followed by chief evaluation with GPT-4 to evaluate responses for compliance with prompt rules and source fidelity. When tested with adversarial facilitators attempting to elicit unsupported advice, the CAF dramatically improved the chatbot's resilience to conversational drift while maintaining its informational role.

## Method Summary
The researchers developed a schizophrenia-focused chatbot using GPT-4 with dynamic access to a schizophrenia manual via an Information Retrieval Algorithm. The Critical Analysis Filter (CAF) was implemented as a multi-agent system where chatbot responses first undergo preliminary screening by GPT-3.5 judges, then deeper evaluation by GPT-4 chief judges, with a compliance enforcer modifying responses if rejected. The system uses citation-based mode classification to automatically route responses to appropriate judges (source fidelity for cited sources, general compliance for uncited). Performance was tested using AI-facilitators that gradually nudged the chatbot toward unsupported roles (diet expert, social interaction expert, social activist).

## Key Results
- Activating the Critical Analysis Filter increased acceptable compliance scores (≥2) from 8.7% to 67.0% of responses
- The filter showed good specificity, generating valid warnings in 2 out of 10 test questions about schizophrenia
- The multi-agent system successfully detected and corrected conversational drift attempts toward unsupported advice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Critical Analysis Filter (CAF) restores chatbot integrity by using a multi-agent system that dynamically evaluates and refines responses based on mode-specific rules.
- Mechanism: After the chatbot generates a response, preliminary judges (GPT-3.5) perform a fast screening. If flagged, chief judges (GPT-4) evaluate the response more deeply and issue verdicts (ACCEPT, WARNING, REJECT). If rejected, a compliance enforcer reformulates the response to comply with rules. This layered chain ensures adherence to instructions and source boundaries.
- Core assumption: AI judges can reliably evaluate compliance with prompt rules when provided structured reasoning templates and appropriate prompts.
- Evidence anchors:
  - [abstract] "Activating the Critical Analysis Filter resulted in an acceptable compliance score (≥2) in 67.0% of responses, compared to only 8.7% when the filter was deactivated."
  - [section] "The judges are instructed to deliver their analyses in a format that encourages structured reasoning, as this was found to improve the quality of their reasoning and overall performance."
  - [corpus] Weak: No direct corpus evidence for multi-agent evaluation performance; only mentions related work on prompt engineering and LLM applications.
- Break condition: Judges fail to detect subtle boundary violations or misclassify responses due to prompt ambiguity or model limitations.

### Mechanism 2
- Claim: Citations at the start of each chatbot response enable automatic mode classification and targeted rule enforcement.
- Mechanism: Chatbot responses start with `cite([list of references])`. If sources are cited, source-conveyor mode judges evaluate fidelity to those sources; if not, default mode judges check for unsupported advice and rule violations. This self-classification lets the system choose the correct judges automatically.
- Core assumption: Self-classification via citations is reliable and does not introduce errors that undermine filtering.
- Evidence anchors:
  - [section] "The citations are used to determine the mode of the chatbot. Currently, there are only 2 modes: source-conveyor mode... default mode..."
  - [section] "Sources not being actively referenced are removed to free up space and allow SchizophreniaInfoBot to focus on more relevant information."
  - [corpus] Weak: No corpus evidence for the reliability of citation-based mode classification in LLM systems.
- Break condition: Chatbot fails to cite correctly, leading to wrong judge selection and ineffective filtering.

### Mechanism 3
- Claim: AI-facilitators that gradually nudge the chatbot toward out-of-bounds roles provide a rigorous test of filter sensitivity.
- Mechanism: Facilitators use GPT-4 to generate conversations starting from a drift point, incrementally encouraging the chatbot to adopt roles (diet expert, social interaction expert, social activist) not supported by the manual. This simulates real-world conversational drift and tests the filter's ability to detect and correct it.
- Core assumption: Gradual role drift is a realistic proxy for conversational drift in mental health chatbot contexts.
- Evidence anchors:
  - [abstract] "To test the effectiveness of the CAF, we develop an informational schizophrenia chatbot... converse with it (with the filter deactivated) until it oversteps its scope."
  - [section] "The AI-facilitators are designed to generate questions intended to gradually entice the chatbot towards giving detailed advice on topics that are not explicitly covered by the chatbot's sources."
  - [corpus] Weak: No corpus evidence that gradual nudging is the best or only way to test filter sensitivity; related papers focus on different aspects of LLM prompting.
- Break condition: Filter fails to detect gradual drift or only catches overt violations, missing subtle oversteps.

## Foundational Learning

- Concept: Prompt chaining and structured reasoning
  - Why needed here: Enables complex compliance evaluation by breaking it into smaller subtasks (screening, deep evaluation, enforcement) handled by specialized agents.
  - Quick check question: What is the purpose of structuring judge outputs with labels like `GIVING_ADVICE_OR_CLAIM` and `SAFE_RELEVANT_HONEST`?
- Concept: Citation-based mode classification
  - Why needed here: Allows the system to automatically route responses to the correct set of judges (source fidelity vs. general compliance).
  - Quick check question: How does the system decide whether to call source-conveyor mode judges or default mode judges?
- Concept: Multi-agent feedback loops
  - Why needed here: Provides dynamic, context-sensitive correction of chatbot responses rather than static rule reminders.
  - Quick check question: What happens if a chief judge rejects a response? Who modifies it and how?

## Architecture Onboarding

- Component map:
  - Chatbot (GPT-4) -> Preliminary Judges (GPT-3.5) -> Chief Judges (GPT-4) -> Compliance Enforcer (GPT-4) -> System messages
  - Sources database + Information Retrieval Algorithm -> Chatbot context
  - AI-facilitators (GPT-4) -> Generate test conversations
- Critical path:
  1. Chatbot generates response with citation
  2. Preliminary judges screen response
  3. If flagged, chief judges evaluate and issue verdict
  4. If REJECT, compliance enforcer modifies response
  5. Modified or original response + warnings inserted into conversation
- Design tradeoffs:
  - Speed vs. accuracy: GPT-3.5 for fast screening, GPT-4 for deep evaluation
  - Token limits: Truncate conversation, remove inactive sources, summarize long responses
  - Generality vs. specificity: Broad rules for default mode, strict source fidelity for source-conveyor mode
- Failure signatures:
  - High false negatives: Filter misses subtle rule violations
  - High false positives: Filter blocks benign responses, causing disruption
  - Judge incoherence: Structured reasoning templates fail, leading to inconsistent verdicts
  - Citation errors: Misclassification of mode leads to wrong judge selection
- First 3 experiments:
  1. Run facilitator conversations with CAF on vs. off; compare compliance score distributions
  2. Test specificity by feeding 10 schizophrenia questions; check for unnecessary warnings
  3. Simulate citation errors (missing/invalid citations); verify judge selection and filtering behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How generalizable is the Critical Analysis Filter's performance across diverse mental health conditions and conversation scenarios?
- Basis in paper: Explicit - The authors acknowledge this study is a feasibility study and note limitations regarding generalizability, stating they tested the CAF only in a very small number of situations with AI facilitators designed specifically for schizophrenia-related scenarios.
- Why unresolved: The paper only tested the CAF with a schizophrenia-focused chatbot and AI-facilitators designed to elicit unsupported advice about schizophrenia-specific topics. There's no evidence of testing with other mental health conditions or more diverse conversation patterns.
- What evidence would resolve it: Testing the CAF with chatbots focused on other mental health conditions (depression, anxiety, bipolar disorder) and with human testers representing diverse backgrounds and conversation styles would demonstrate generalizability.

### Open Question 2
- Question: What is the optimal balance between rule strictness and conversational utility in the Critical Analysis Filter?
- Basis in paper: Explicit - The authors discuss the tension between risk reduction and utility, noting they adopted a "basic-and-uncontroversial" rule but acknowledge this involves a balancing act between predictability and usefulness.
- Why unresolved: The paper doesn't provide empirical data on how different levels of rule strictness affect user satisfaction, information completeness, or conversational engagement. They mention that overly aggressive filtering could be disruptive but don't quantify this trade-off.
- What evidence would resolve it: A/B testing with different strictness levels of the CAF, measuring user satisfaction, information completeness, and conversation quality metrics, would help identify the optimal balance point.

### Open Question 3
- Question: How does the Information Retrieval Algorithm perform with more complex, multi-part questions or questions with different phrasing patterns?
- Basis in paper: Explicit - The authors note that most questions generated by GPT-3.5 turned out to be asking 2-3 questions in one sentence, which made them particularly challenging for the chatbot to answer via the Information Retrieval Algorithm.
- Why unresolved: The paper acknowledges the algorithm works well for single, concise queries but may fail when answers are spread across multiple sources or when question formulation differs substantially from source descriptions.
- What evidence would resolve it: Testing the algorithm with a wide variety of question types (complex multi-part questions, questions with different phrasings, follow-up questions) and measuring response accuracy and relevance would demonstrate its robustness.

## Limitations
- The study relies entirely on simulated adversarial conversations rather than real user interactions, which may not capture all failure modes
- The testing methodology focuses specifically on role-drift scenarios but doesn't address other potential failure modes like hallucination of medical information
- Lack of detailed prompt templates for the AI agents makes faithful reproduction difficult

## Confidence

- **High Confidence:** The mechanism of using multi-agent evaluation with layered screening (GPT-3.5 for initial screening, GPT-4 for deep evaluation) is technically sound and well-justified by the results.
- **Medium Confidence:** The effectiveness of citation-based mode classification is plausible but lacks direct validation beyond the described results.
- **Medium Confidence:** The gradual nudging methodology for testing filter sensitivity is reasonable but may not capture all real-world conversational drift scenarios.

## Next Checks

1. Test the CAF with real schizophrenia patient interactions (with appropriate safeguards) to validate performance in authentic contexts rather than simulated conversations.
2. Evaluate the filter's performance on edge cases like medical emergencies, suicidal ideation, or requests for crisis intervention that go beyond the informational scope.
3. Conduct ablation studies removing each component (preliminary judges, chief judges, compliance enforcer) to quantify the individual contribution of each element to overall performance.