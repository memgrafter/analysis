---
ver: rpa2
title: A Comprehensive Evaluation of Cognitive Biases in LLMs
arxiv_id: '2410.15413'
source_url: https://arxiv.org/abs/2410.15413
tags: []
core_contribution: This paper presents a large-scale evaluation of 30 cognitive biases
  in 20 state-of-the-art LLMs under various decision-making scenarios. The authors
  develop a novel general-purpose test framework for reliable and large-scale generation
  of tests for LLMs, creating a benchmark dataset with 30,000 tests for detecting
  cognitive biases in LLMs.
---

# A Comprehensive Evaluation of Cognitive Biases in LLMs

## Quick Facts
- arXiv ID: 2410.15413
- Source URL: https://arxiv.org/abs/2410.15413
- Reference count: 40
- Authors: Simon Malberg; Roman Poletukhin; Carolin M. Schuster; Georg Groh
- Primary result: Large-scale evaluation of 30 cognitive biases across 20 state-of-the-art LLMs using a novel template-based test framework

## Executive Summary
This paper presents a comprehensive evaluation of cognitive biases in large language models, examining 30 different biases across 20 state-of-the-art models using a novel template-based test framework. The authors develop a general-purpose framework that generates diverse test cases while preserving core bias logic, creating a benchmark dataset of 30,000 tests. Their evaluation confirms and broadens previous findings, demonstrating that cognitive biases present in humans are detectable in LLMs due to training on human-generated data and reinforcement learning from human feedback.

## Method Summary
The study employs a novel general-purpose test framework for generating and evaluating cognitive biases in LLMs. The framework uses template-based test generation with fixed abstract bias paradigms and flexible scenario-specific gaps filled by LLMs under strict instructions. Test cases are created with control and treatment templates designed to elicit specific biases, and a metric measures bias strength by comparing relative shifts in LLM decisions. The evaluation spans 20 models ranging from 1 billion to 175+ billion parameters, testing 30 cognitive biases across 200 decision-making scenarios.

## Key Results
- Evidence of all 30 tested cognitive biases found in at least some of the 20 evaluated LLMs
- Bias susceptibility varies across models but shows no clear correlation with general capability scores
- Framework successfully generates diverse, valid test cases while maintaining bias detection accuracy
- LLMs demonstrate human-like cognitive biases likely inherited from training data and RLHF processes

## Why This Works (Mechanism)

### Mechanism 1: Template-based Test Generation
- Claim: The framework generates diverse, high-volume, and valid test cases for cognitive biases by leveraging LLMs to fill template gaps while preserving core bias logic
- Mechanism: Templates define bias logic with gaps for scenario insertion; LLMs fill these gaps under strict instructions to generate varied test instances
- Core assumption: LLMs can reliably follow detailed instructions to generate valid, scenario-rich test cases without compromising bias detection validity
- Evidence anchors: Framework creates 30,000 tests; validation via IFE VAL and manual checks ensures correctness

### Mechanism 2: Bias Transfer from Human Data
- Claim: Cognitive biases present in humans are detectable in LLMs because LLMs are trained on human-generated data and fine-tuned via RLHF
- Mechanism: Evaluation compares LLM decisions across control and treatment templates; metrics quantify bias strength by measuring systematic shifts in choices
- Core assumption: LLMs inherit cognitive biases from human training data and RLHF processes
- Evidence anchors: All 30 biases detected across models; theoretical alignment with human cognitive bias literature

### Mechanism 3: Model Characteristics Influence
- Claim: Model size, architecture, and training procedures influence susceptibility to cognitive biases, but no clear correlation exists with general capability
- Mechanism: Evaluation spans 20 models from 1B to 175B+ parameters; bias scores analyzed against capability scores and clustering analysis
- Core assumption: Architectural and training differences affect bias susceptibility independently of model capability
- Evidence anchors: Variance in bias proneness independent of capability; clustering analysis shows no clear capability-bias correlation

## Foundational Learning

- **Template-based test generation with controlled variability**: Decouples bias logic from scenario diversity to enable scalable, reliable bias testing. Quick check: Can you explain how the framework ensures that inserted values don't alter the core bias being tested?

- **Cognitive bias detection via control-treatment comparison**: Standard method to isolate bias effects by measuring systematic shifts in LLM decisions. Quick check: How does the metric m(a1,a2,k) distinguish bias from random preference?

- **Diversity validation using embedding and n-gram metrics**: Ensures dataset is broad enough to avoid overfitting and supports robust bias generalization. Quick check: What would high Self-BLEU scores indicate about the dataset?

## Architecture Onboarding

- **Component map**: Templates -> Test Generator (G) -> Decide Function (D) -> Estimate Function (E) -> Validation Pipeline
- **Critical path**: 1. Select bias → 2. Load template + scenario → 3. Generate test case (G) → 4. Obtain decisions (D) → 5. Compute score (E)
- **Design tradeoffs**: Flexibility vs. validity (gaps add diversity but risk breaking bias logic); LLM generation vs. manual curation (scalability vs. control); single LLM vs. multiple for generation (risk of overfitting)
- **Failure signatures**: High decision failure rate (model cannot reason reliably); low diversity metrics (dataset too homogeneous); bias scores near zero (metric or template design flawed)
- **First 3 experiments**: 1. Run G on simple bias with one scenario to verify gap filling; 2. Execute D on generated test case to check option extraction; 3. Apply E to decisions to confirm metric produces expected range

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompting strategies (e.g., chain-of-thought, few-shot examples) impact the measurement of cognitive biases in LLMs?
- Basis: The paper uses specific prompts but does not explore how variations might affect results
- Why unresolved: Fixed prompts used without investigating impact of different strategies
- What evidence would resolve it: Experiments comparing bias measurements across different prompting strategies

### Open Question 2
- Question: How do cognitive biases in LLMs evolve over time with continued training and exposure to diverse data?
- Basis: Study provides static analysis at a single point in time
- Why unresolved: No longitudinal tracking of bias changes as models evolve
- What evidence would resolve it: Longitudinal studies evaluating same LLMs at different time points

### Open Question 3
- Question: Can cognitive biases in LLMs be effectively mitigated through targeted interventions during the training process?
- Basis: Paper mentions importance of future research on developing less biased models
- Why unresolved: Focuses on evaluation rather than mitigation strategies
- What evidence would resolve it: Experiments testing targeted interventions during training

## Limitations

- Reliance on LLMs for both test generation and evaluation creates potential circularity in bias detection
- Scope constrained by 30 selected biases and 200 decision-making scenarios, may not represent full spectrum
- Theoretical foundation linking human biases to LLM behavior lacks direct empirical evidence
- Comparison to human cognitive bias literature is primarily theoretical rather than empirically grounded

## Confidence

**High Confidence**: The existence of cognitive biases in LLMs is well-supported by comprehensive evaluation across 20 models showing evidence of all 30 tested biases. The template-based framework approach is methodologically sound and produces valid test cases.

**Medium Confidence**: The influence of model characteristics on bias susceptibility shows promising patterns in clustering analysis, but the lack of clear correlation with capability scores suggests a more complex relationship than initially hypothesized.

**Low Confidence**: The theoretical foundation linking human cognitive biases to LLM behavior through training data and RLHF lacks direct empirical evidence, relying heavily on inference from human cognitive bias literature.

## Next Checks

1. **Instruction Compliance Verification**: Conduct systematic audit of generated test cases to verify LLM-based gap filling consistently follows all specified instructions, particularly for edge cases where compliance might fail.

2. **Bias Transfer Mechanism Validation**: Design experiments to trace how specific cognitive biases manifest in LLMs by analyzing training data and model behavior at different stages of fine-tuning to establish empirical evidence for bias transfer.

3. **Metric Robustness Testing**: Perform sensitivity analysis on the bias detection metric using synthetic data with known bias patterns to verify the metric accurately captures bias strength and doesn't produce false positives or negatives under edge conditions.