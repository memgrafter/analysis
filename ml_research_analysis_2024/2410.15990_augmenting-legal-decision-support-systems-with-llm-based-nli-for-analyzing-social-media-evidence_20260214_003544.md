---
ver: rpa2
title: Augmenting Legal Decision Support Systems with LLM-based NLI for Analyzing
  Social Media Evidence
arxiv_id: '2410.15990'
source_url: https://arxiv.org/abs/2410.15990
tags:
- legal
- performance
- dataset
- test
- nllp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper describes a system for classifying legal relationships
  in text (entailed, contradicted, neutral) using large language models (LLMs) fine-tuned
  for legal natural language inference (L-NLI). The authors experimented with multi-stage
  fine-tuning (first on SNLI, then on the NLLP dataset) and various alignment approaches
  including ORPO with random, preferred, and multiple rejections.
---

# Augmenting Legal Decision Support Systems with LLM-based NLI for Analyzing Social Media Evidence

## Quick Facts
- arXiv ID: 2410.15990
- Source URL: https://arxiv.org/abs/2410.15990
- Reference count: 6
- Primary result: GEMMA-2-27B fine-tuned with SNLI and NLLP data using preferred rejection achieved Macro F1 score of 0.887, outperforming other submissions

## Executive Summary
This paper presents a system for classifying legal relationships in text (entailed, contradicted, neutral) using large language models fine-tuned for legal natural language inference (L-NLI). The authors address the challenge of analyzing social media evidence in legal contexts by developing a multi-stage fine-tuning approach that first adapts models to generic NLI tasks before specializing them for legal text classification. Their system completely avoids Type-1 errors (misclassifying entailed as contradicted) and shows strong performance across most legal domains, with the best model achieving a Macro F1 score of 0.887 on the test set.

## Method Summary
The method employs a multi-stage fine-tuning pipeline where models are first trained on a subset of the SNLI dataset (20,000 samples) to learn general NLI patterns, then fine-tuned on the NLLP dataset (312 samples) for legal-specific adaptation. The authors use ORPO (Online Reflective Preference Optimization) with different rejection strategies including random, preferred, and multiple rejections to align model outputs with desired classification preferences. Various LLMs were tested including GEMMA-2-27B, Phi-3-Medium, Mistral-8x7B, and QWEN-2-7B, with the largest models consistently showing better performance. The system processes legal premises (class-action case summaries) and hypotheses (online media text) to classify their relationships.

## Key Results
- GEMMA-2-27B with SNLI, NLLP, and preferred rejection achieved the highest Macro F1 score of 0.887
- The system completely eliminated Type-1 errors (entailed → contradicted misclassifications)
- Multi-stage training consistently outperformed single-stage training across all model sizes
- Larger models (27B parameters) significantly outperformed smaller models (7B parameters) for this task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage fine-tuning (first on SNLI, then on NLLP) improves model performance for legal NLI tasks.
- Mechanism: Initial training on generic NLI data (SNLI) helps the model learn general inference patterns before adapting to domain-specific legal text (NLLP).
- Core assumption: Legal text has different linguistic and semantic patterns than generic text, requiring adaptation.
- Evidence anchors:
  - [section] "Given the small size of the existing training dataset (312 samples), we have additionally tested multi-stage learning by first fine-tuning over a subset of 20000 rows from the SNLI dataset to first let the models adapt to generic NLI tasks with a lower learning rate and then further fine-tuned the resultant models on the NLLP training samples with a higher learning rate."
  - [table 2] Models with multi-stage training (SNLI + NLLP) consistently outperformed those trained only on NLLP data.

### Mechanism 2
- Claim: ORPO with preferred rejection alignment strategy significantly improves classification accuracy.
- Mechanism: By using preferred rejection (choosing 'Neutral' as rejected label when actual label is 'Entailed' or 'Contradicted'), the model learns to better distinguish between 'Neutral' and the other classes, reducing misclassifications.
- Core assumption: The model's errors primarily occur when confusing 'Neutral' with 'Entailed' or 'Contradicted' classes.
- Evidence anchors:
  - [section] "We chose 'Neutral' as the rejected response when the actual label is either Entailed or Contradict. The reason being all of the errors being one of the other two classes being labelled as 'Neutral or vice versa. This did improve the performance significantly by reducing the mis-classified samples between Neutral and the other classes."
  - [table 2] GEMMA-2-27B with SNLI, NLLP, and preferred rejection achieved the highest Macro F1 score (0.887).

### Mechanism 3
- Claim: Using larger models (27B parameters) provides better performance than smaller models for complex legal text classification.
- Mechanism: Larger models have more parameters and capacity to capture complex patterns in long legal texts (4-7 sentences) compared to shorter generic NLI examples.
- Core assumption: Legal texts require more nuanced understanding due to their length and complexity.
- Evidence anchors:
  - [section] "Since the SNLI dataset has a 98% consensus and 58% unanimous annotation among 5 annotators, it can be expected that a human annotation on the current dataset can lead to even less proportion of texts where a consensus or unanimous vote can be reached."
  - [table 2] GEMMA-2-27B (27B parameters) consistently outperformed smaller models like Phi-3-Medium and QWEN-2-7B.

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: The task requires classifying relationships between legal complaints and reviews as entailed, contradicted, or neutral.
  - Quick check question: What are the three possible relationship types between a premise and hypothesis in NLI?

- Concept: Transfer learning and fine-tuning
  - Why needed here: The small NLLP dataset (312 samples) necessitates pre-training on larger datasets like SNLI before adapting to the legal domain.
  - Quick check question: Why might it be beneficial to first train on generic NLI data before fine-tuning on legal-specific data?

- Concept: Preference optimization and rejection sampling
  - Why needed here: ORPO with different rejection strategies helps align the model's outputs with desired legal classification preferences.
  - Quick check question: How does choosing different rejection labels affect the model's learning in preference optimization?

## Architecture Onboarding

- Component map: Base LLM (GEMMA-2-27B, Phi-3, Mistral, etc.) -> Multi-stage training pipeline (SNLI → NLLP) -> ORPO alignment layer with configurable rejection strategies -> Evaluation metrics (Macro F1, precision, recall, accuracy) -> Domain-specific analysis tools

- Critical path: 1. Load base model (4-bit precision for efficiency) 2. First fine-tuning stage on SNLI dataset 3. Second fine-tuning stage on NLLP dataset 4. ORPO alignment with chosen rejection strategy 5. Evaluation on test set and domain-wise analysis

- Design tradeoffs:
  - Model size vs. computational resources (27B vs 7B parameters)
  - Training time vs. performance (multi-stage vs single-stage)
  - Precision vs. efficiency (4-bit vs full precision loading)
  - Random vs. preferred vs. multiple rejection strategies in ORPO

- Failure signatures:
  - Poor performance on specific domains (e.g., BIPA) indicates data imbalance issues
  - High misclassification between Neutral and other classes suggests need for better alignment
  - No improvement from multi-stage training indicates domain mismatch between SNLI and NLLP

- First 3 experiments:
  1. Compare single-stage vs multi-stage training on a subset of data
  2. Test different ORPO rejection strategies (random, preferred, multiple) on the same base model
  3. Evaluate performance across different model sizes (7B vs 27B parameters) with identical training protocols

## Open Questions the Paper Calls Out

- Question: How would increasing the training data size affect model performance across different legal domains, particularly those with limited training examples?
  - Basis in paper: [explicit] "Since the test dataset used in the task is relatively small, the LLMs/approaches that might perform better in practical scenarios may vary from those found to be better on the current dataset" and "Some cases did get misclassified too often especially those whose domain data was less represented in the training dataset"
  - Why unresolved: The paper acknowledges that low training data affects performance but doesn't experimentally test how larger datasets would impact results across different domains.
  - What evidence would resolve it: Experimental results showing performance improvements when training on larger datasets for each legal domain, particularly comparing performance on domains with currently limited data (like BIPA) against domains with more examples.

- Question: Would incorporating individual annotator reasoning and explanations improve model performance beyond just using final labels?
  - Basis in paper: [explicit] "It is worth looking into the performance of models trained on not just the labels, but also the reasoning of the annotators on why a certain label was chosen, as it might help the model learn better"
  - Why unresolved: The authors suggest this as a potential improvement but don't implement or test it in their study.
  - What evidence would resolve it: Comparative results showing model performance with and without access to annotator reasoning data, demonstrating whether additional context improves classification accuracy.

- Question: How would using full-precision models instead of 4-bit precision affect performance given computational resource constraints?
  - Basis in paper: [explicit] "Due to computational resource limitations, the base models of LLMs were initially loaded in 4-bit precision, It is likely that a larger model used in full-precision might perform better"
  - Why unresolved: The authors acknowledge this limitation but couldn't test full-precision models due to computational constraints.
  - What evidence would resolve it: Direct performance comparisons between the same models trained in 4-bit versus full-precision, measuring improvements in accuracy and other metrics.

## Limitations

- Small dataset size (312 training samples) necessitates reliance on multi-stage training and may not generalize well to broader legal domains
- Focus on a single jurisdiction with limited diversity in legal complaint types, potentially constraining generalizability
- 4-bit precision loading may have introduced quantization artifacts affecting model performance

## Confidence

**High Confidence**: The multi-stage fine-tuning approach showing consistent improvement across different model sizes; the effectiveness of ORPO with preferred rejection in reducing misclassifications between Neutral and other classes; the superior performance of larger models (27B vs 7B parameters) for this task.

**Medium Confidence**: The complete elimination of Type-1 errors and its practical significance; the specific choice of preferred rejection over other ORPO strategies; the generalizability of results to other legal domains beyond those tested.

**Low Confidence**: The optimal number of training epochs for each stage; the precise impact of 4-bit vs full precision loading on final performance; whether the model would maintain performance with different legal writing styles or in languages other than English.

## Next Checks

1. **Ablation Study on Multi-Stage Training**: Systematically remove the SNLI pre-training stage while keeping all other parameters constant to quantify the exact contribution of generic NLI knowledge transfer to legal NLI performance.

2. **Cross-Domain Generalization Test**: Evaluate the best-performing model (GEMMA-2-27B with preferred rejection) on legal text from different jurisdictions or legal domains not represented in the NLLP dataset to assess true generalization capability.

3. **Type-1 Error Analysis in Production Setting**: Deploy the model in a controlled legal review workflow with human experts to determine whether the complete avoidance of Type-1 errors comes at the cost of missing genuine entailed relationships that would be valuable in practice.