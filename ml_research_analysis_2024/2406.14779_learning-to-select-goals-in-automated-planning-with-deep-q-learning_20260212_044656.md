---
ver: rpa2
title: Learning to Select Goals in Automated Planning with Deep-Q Learning
arxiv_id: '2406.14779'
source_url: https://arxiv.org/abs/2406.14779
tags:
- planning
- which
- levels
- state
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a planning and acting architecture that learns
  to select subgoals using Deep Q-Learning. This allows them to reduce the load on
  a planner in real-time scenarios.
---

# Learning to Select Goals in Automated Planning with Deep-Q Learning

## Quick Facts
- **arXiv ID:** 2406.14779
- **Source URL:** https://arxiv.org/abs/2406.14779
- **Reference count:** 10
- **Primary result:** Learning to select subgoals with Deep Q-Learning reduces planning load and improves sample efficiency compared to standard methods.

## Executive Summary
This paper proposes a planning and acting architecture that learns to select subgoals using Deep Q-Learning, integrated with a classical planner to reduce computational overhead in real-time scenarios. The approach is tested on a deterministic version of the Boulder Dash game, where it demonstrates improved sample efficiency and generalization compared to standard Deep Q-Learning and classical planning methods. The architecture achieves this by predicting the number of actions required for each subgoal, allowing the planner to be invoked only for the most promising subgoal.

## Method Summary
The authors formulate goal selection as a deterministic Markov Decision Process (DMDP) where states are initial and goal states of plans, and actions are subgoal selections. A Convolutional Neural Network (CNN) predicts the number of actions in the total plan from each eligible subgoal. The planner is then invoked only for the best subgoal, skipping unnecessary planning for others. The CNN is trained offline on datasets collected from 200 training levels using random subgoal selection and plan execution. The approach is evaluated on 11 test levels, measuring plan quality and solving times compared to baseline methods.

## Key Results
- The approach is more sample-efficient than standard Deep Q-Learning, being able to obtain plans of better quality while being trained on a dataset ten times smaller.
- It generalizes better across levels, reducing problem-solving time when compared with a state-of-the-art automated planner, at the expense of obtaining plans with only 9% more actions.
- The architecture is able to solve every test level in less than 2 seconds per level.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The architecture reduces planning load by learning to select subgoals instead of executing actions directly.
- Mechanism: The Goal Selection Module uses a CNN to predict the number of actions in the total plan from each eligible subgoal. The planner is then invoked only for the best subgoal, skipping unnecessary planning for others.
- Core assumption: The CNN can generalize across different levels to predict plan lengths accurately for unseen states and subgoals.
- Evidence anchors:
  - [abstract]: "It reduces problem-solving time when compared with a state-of-the-art automated planner, at the expense of obtaining plans with only 9% more actions."
  - [section 4.1]: "Using the formulation given by Mg, we can adapt the DQL algorithm to this new type of DMDP. In this case, DQL predicts a Q-value Q(s, g) for each (s, g) pair, where s ∈ Sg and g ∈ Gs, and selects the subgoal ĝ with the lowest Q-value."
- Break condition: If the CNN cannot generalize well to new levels, it will frequently select subgoals leading to goal selection errors, forcing repeated planner invocations.

### Mechanism 2
- Claim: The problem formulation Mg reduces the state space, making the learning problem tractable.
- Mechanism: Mg considers only initial states and goal states of plans as valid states, reducing Sg ⊂ S compared to the standard RL formulation M. This smaller state space simplifies learning.
- Core assumption: The reduced state space still captures all necessary information for effective subgoal selection.
- Evidence anchors:
  - [section 4.1]: "One of the main advantages of using Mg to formulate and solve a RL problem, instead of a standard DMDP description, is that the state space is reduced (since Sg ⊂ S) and, thus, the problem is simplified."
  - [abstract]: "The results obtained show our model performs better than the alternative methods considered, when both plan quality (plan length) and time requirements are taken into account."
- Break condition: If critical state information is lost in the reduction, the CNN cannot make informed subgoal selections, degrading performance.

### Mechanism 3
- Claim: Offline training on diverse level datasets enables good generalization across test levels.
- Mechanism: The CNN is trained on 200 training levels with 100,000 unique samples collected via random subgoal selection and plan execution, covering a wide variety of game states and subgoal configurations.
- Core assumption: The training distribution covers enough variability that test levels fall within the learned generalization envelope.
- Evidence anchors:
  - [section 4.2]: "These datasets are collected by an agent which performs random exploration on these levels... This whole process is repeated until the final goal gf is selected and achieved, so every sample is part of a trajectory which successfully solves the level."
  - [abstract]: "It is more sample-efficient than standard Deep Q-Learning, and it is able to generalize better across levels."
- Break condition: If test levels contain configurations far outside the training distribution, the CNN predictions will be unreliable, leading to poor subgoal choices.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The goal selection problem is modeled as a deterministic MDP (DMDP) to apply reinforcement learning techniques.
  - Quick check question: What are the components of a DMDP tuple (S, A, r, t) and how do they map to the goal selection problem?

- Concept: Convolutional Neural Networks (CNNs)
  - Why needed here: The CNN processes the one-hot encoded state-subgoal tensor to predict plan lengths, requiring spatial feature extraction.
  - Quick check question: Why is zero-padding applied to the one-hot tensor before feeding it to the CNN?

- Concept: Bellman Equation and Q-Learning
  - Why needed here: The loss function for training the CNN is derived from the Bellman equation to estimate Q-values for subgoal selection.
  - Quick check question: How does the loss function Lg differ when a subgoal is attainable versus not attainable?

## Architecture Onboarding

- Component map:
  - GVGAI Environment ↔ Execution Monitoring Module
  - Execution Monitoring ↔ Goal Formulation Module
  - Goal Formulation ↔ Subgoal Pattern
  - Subgoal Pattern ↔ Goal Selection Module
  - Goal Selection Module ↔ PDDL Parser
  - PDDL Parser ↔ Planner Module
  - Planner Module ↔ Execution Monitoring Module

- Critical path: Execution Monitoring → Goal Formulation → Goal Selection → PDDL Parser → Planner → Execution Monitoring

- Design tradeoffs:
  - Using a planner adds computational overhead but ensures valid plans; skipping planning entirely would require learning full action sequences, which is less sample-efficient.
  - Training on offline datasets avoids real-time exploration costs but may limit adaptation to truly novel scenarios.

- Failure signatures:
  - High frequency of goal selection errors (planner returns empty plans) indicates poor CNN generalization.
  - Long goal selection times suggest inefficient CNN architecture or input encoding.
  - Timeout errors in the planner suggest the subgoal selection is choosing overly complex subgoals.

- First 3 experiments:
  1. Verify CNN predicts plan lengths correctly on training data by comparing predicted vs actual lengths.
  2. Test goal selection on a simple level with known optimal subgoal order to confirm the module picks the correct sequence.
  3. Compare plan lengths and solving times with and without the Goal Selection Module on a held-out validation level.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the DQP model compare to other state-of-the-art planning algorithms in terms of both plan quality and time requirements?
- Basis in paper: [explicit] The paper states that the DQP model performs better than standard Deep Q-Learning and classical planning methods, but does not provide a detailed comparison with other state-of-the-art planning algorithms.
- Why unresolved: The paper only compares the DQP model to a limited set of alternative approaches, and does not provide a comprehensive comparison with other state-of-the-art planning algorithms.
- What evidence would resolve it: A detailed comparison of the DQP model with other state-of-the-art planning algorithms, including a variety of search strategies and domain-independent planners, would provide a more complete understanding of its performance.

### Open Question 2
- Question: How does the performance of the DQP model vary across different types of planning domains, such as those with high levels of uncertainty or non-deterministic dynamics?
- Basis in paper: [inferred] The paper only evaluates the DQP model on a deterministic version of the Boulder Dash game, and does not explore its performance on other types of planning domains.
- Why unresolved: The paper does not provide any evidence of the DQP model's performance on other types of planning domains, which limits its generalizability.
- What evidence would resolve it: Evaluating the DQP model on a variety of planning domains, including those with high levels of uncertainty or non-deterministic dynamics, would provide insights into its performance across different types of problems.

### Open Question 3
- Question: How does the performance of the DQP model scale with the size and complexity of the planning domain?
- Basis in paper: [inferred] The paper only evaluates the DQP model on a single game, and does not explore its performance on larger or more complex planning domains.
- Why unresolved: The paper does not provide any evidence of the DQP model's performance on larger or more complex planning domains, which limits its scalability.
- What evidence would resolve it: Evaluating the DQP model on larger or more complex planning domains would provide insights into its scalability and performance as the size and complexity of the domain increase.

## Limitations

- The generalizability of the CNN predictions across diverse level types remains uncertain, particularly for levels significantly different from the training distribution.
- The performance metrics, while showing improvements over baselines, are based on a specific game domain (Boulder Dash) and may not translate directly to other planning domains.
- The computational overhead of the CNN inference and its impact on real-time performance in more complex scenarios is not thoroughly explored.

## Confidence

- **High**: The experimental setup and methodology are well-defined, with clear procedures for data collection, model training, and evaluation.
- **Medium**: The claims regarding improved sample efficiency and generalization are supported by the results, but further testing on a broader range of domains would strengthen these claims.
- **Low**: The long-term stability and adaptability of the model in dynamic environments with changing conditions or goals is not addressed.

## Next Checks

1. **Generalization Test**: Evaluate the model on a wider variety of planning domains beyond Boulder Dash to assess its robustness and applicability.
2. **Computational Overhead Analysis**: Measure the inference time of the CNN and its impact on overall problem-solving time, especially in more complex scenarios.
3. **Dynamic Environment Test**: Test the model's performance in environments with changing goals or conditions to evaluate its adaptability and long-term stability.