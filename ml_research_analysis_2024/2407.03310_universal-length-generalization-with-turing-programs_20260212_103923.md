---
ver: rpa2
title: Universal Length Generalization with Turing Programs
arxiv_id: '2407.03310'
source_url: https://arxiv.org/abs/2407.03310
tags:
- length
- turing
- generalization
- arxiv
- positional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Turing Programs, a novel Chain-of-Thought strategy
  that decomposes an algorithmic task into steps mimicking the computation of a Turing
  Machine. The key idea is that by decomposing an algorithmic task into a series of
  steps, each step being a "tape" that is a copy of the previous one with a few elementary
  changes, Transformers can achieve length generalization on a wide range of algorithmic
  tasks.
---

# Universal Length Generalization with Turing Programs

## Quick Facts
- arXiv ID: 2407.03310
- Source URL: https://arxiv.org/abs/2407.03310
- Reference count: 40
- Primary result: Proposes Turing Programs, a Chain-of-Thought strategy decomposing algorithmic tasks into Turing Machine-like steps, enabling robust length generalization on addition, multiplication, and in-context SGD

## Executive Summary
This paper addresses the challenge of length generalization in transformers for algorithmic tasks. The authors introduce Turing Programs, a novel Chain-of-Thought approach that decomposes algorithmic tasks into a series of steps, each representing a tape state that is a minor modification of the previous one. By combining this scratchpad technique with the Hard-ALiBi positional encoding, transformers can robustly generalize to longer sequences beyond their training distribution. The approach is demonstrated on addition, multiplication, and in-context SGD, showing significant improvements over prior methods.

## Method Summary
The method decomposes algorithmic tasks into a sequence of intermediate steps, where each step is a "tape" state that is a copy of the previous state with a few elementary changes (head move, symbol edit, state change). This is combined with Hard-ALiBi positional encoding, which uses a hard attention window to enable robust copying over long sequences. The authors also provide a theoretical construction showing that transformers can implement Turing Programs through a RASP program that simulates arbitrary Turing machines under certain assumptions about input/output patterns.

## Key Results
- Turing Programs with Hard-ALiBi achieve near-perfect length generalization on addition (50→100 digits)
- Robust length generalization demonstrated on multiplication (1-digit and 3-digit) and in-context SGD
- Transformers can learn to simulate random Turing Programs, suggesting generality for arbitrary algorithmic tasks
- Theoretical proof that transformers can implement Turing Programs via RASP simulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing algorithmic tasks into a series of tape states that are minor modifications of the previous tape enables length generalization.
- Mechanism: Each step in a Turing Program is a copy of the previous tape with a few elementary changes (head move, symbol edit, state change). This structure aligns with the transformer's copying ability enhanced by Hard-ALiBi positional encoding.
- Core assumption: Transformers with Hard-ALiBi can robustly copy sequences of arbitrary length, and small localized edits are learnable.
- Evidence anchors:
  - [abstract] "The key idea is that by decomposing an algorithmic task into a series of steps, each step being a 'tape' that is a copy of the previous one with a few elementary changes, Transformers can achieve length generalization"
  - [section 4.2] "Building on previous works showing that with the right positional encoding, transformers can achieve length generalization on the copying operation [23], we hypothesize that combining the scratchpad technique with a favorable positional encoding can unlock length generalization capabilities."
  - [corpus] Found 25 related papers; some directly study copying or positional encoding for length generalization, supporting the relevance of this mechanism.
- Break condition: If the tape state sequence grows too long, or if edits become too complex or global, the transformer's copying ability may fail.

### Mechanism 2
- Claim: The Hard-ALiBi positional encoding's hard thresholding allows attention only to the m closest tokens, enabling copying over long sequences.
- Mechanism: Hard-ALiBi uses b(i,j) = -∞ for j ≤ i-m and b(i,j) = 0 for j > i-m. This means each token can only attend to the m closest previous tokens, making copying operations more stable and generalizable.
- Core assumption: The hard attention window m is sufficient to capture the context needed for each tape update step.
- Evidence anchors:
  - [abstract] "transformers enhanced with the Hard-ALiBi positional encoding...are capable of length generalization on a wide range of algorithmic tasks"
  - [section 2.3] "Jelassi et al. [23] notice that it struggles at length generalization on the copy task and hypothesize that it is due to the slow decay of r. Jelassi et al. [23] introduce Hard-ALiBi, an additive positional encoding where the bias satisfies b(i, j) = −∞ for j ≤ i − m and b(i, j) = 0 for j > i − m"
  - [corpus] Related work includes "Delayed Attention Training Improves Length Generalization" and other positional encoding studies, supporting the importance of attention window design.
- Break condition: If m is too small to capture necessary context, or if the tape sequence exceeds the attention window size, generalization may break.

### Mechanism 3
- Claim: RASP programs can implement Turing Programs over long sequences without index arithmetic, enabling length generalization.
- Mechanism: The paper constructs a RASP program that simulates arbitrary Turing machines by leveraging n-gram hashing to avoid direct indexing, assuming no repeated n-grams in input or output.
- Core assumption: Input and Turing machine output sequences have no repeated n-grams, and operations are in-memory (head does not go beyond input).
- Evidence anchors:
  - [abstract] "we theoretically prove that transformers can implement Turing Programs, constructing a simple RASP (Weiss et al. [53]) program that simulates an arbitrary Turing machine"
  - [section 4.3] "Theorem 4.1 Let T be a Turing Machine s.t. 1) T does not generate repeated n-grams and 2) T operates in-memory. Then, there exists a RASP program P of size (number of lines) O(n) s.t. for every input x without repeated n-grams, P correctly simulates T for exp(n) steps."
  - [corpus] Found papers on "program trace generation" and "Tracers as programmable computers," supporting theoretical connections.
- Break condition: If input or output contains repeated n-grams, or if the Turing machine requires out-of-memory operations, the RASP simulation fails.

## Foundational Learning

- Concept: Turing Machine computation model
  - Why needed here: The entire Turing Programs approach is based on decomposing algorithmic tasks into steps that mimic Turing Machine tape updates.
  - Quick check question: Can you describe the three components of a Turing Machine and how they interact at each step?

- Concept: Transformer attention and positional encodings
  - Why needed here: The success of Turing Programs relies on transformers' ability to copy sequences and the Hard-ALiBi encoding's hard attention window.
  - Quick check question: What is the difference between absolute, relative, and Hard-ALiBi positional encodings, and why does Hard-ALiBi help with length generalization?

- Concept: RASP (Restricted Access Sequence Processing) programming model
  - Why needed here: The theoretical proof that transformers can implement Turing Programs relies on constructing a RASP program simulation.
  - Quick check question: What are the key restrictions of RASP compared to general programming languages, and why are these restrictions important for length generalization?

## Architecture Onboarding

- Component map: Input sequence → Transformer layers (with Hard-ALiBi positional encoding) → Output sequence
- Critical path: Token generation → Attention calculation (using Hard-ALiBi bias) → Next token prediction → Repeat until sequence end
- Design tradeoffs: Hard-ALiBi vs. other positional encodings (better length generalization but potentially less expressive for non-copying tasks); larger m improves context but may reduce generalization; RASP simulation assumes no repeated n-grams, which may not hold for all inputs
- Failure signatures: Degradation in accuracy when sequence length exceeds training length; errors accumulate over multiple Turing Program steps; inability to handle repeated n-grams in input/output
- First 3 experiments:
  1. Train a transformer on addition with and without Turing Programs and Hard-ALiBi; compare length generalization from 50 to 100 digits
  2. Train on multiplication by 1-digit and 3-digit operands using Turing Programs; test extrapolation from 50 to 100 digits
  3. Train on SGD on linear regression with Turing Programs; vary dataset size from 50 to 80 examples and measure accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can transformers length generalize on arbitrary Turing Programs beyond the restrictions of the RASP construction?
- Basis in paper: [explicit] The paper states that the RASP construction has limitations, requiring that the Turing machine operates in-memory and does not generate repeated n-grams. The authors acknowledge that these restrictions may be removed at the cost of constructing a more complex RASP program.
- Why unresolved: The theoretical proof relies on specific assumptions about the Turing machine's behavior. Real-world Turing machines may not adhere to these constraints.
- What evidence would resolve it: Constructing a more complex RASP program that can handle Turing machines that generate repeated n-grams or move the head beyond the input sequence, and demonstrating length generalization on these more complex Turing Programs.

### Open Question 2
- Question: How robust is length generalization with Turing Programs to different initialization seeds and hyperparameters?
- Basis in paper: [explicit] The paper mentions that there is significant variance in performance when testing on longer sequences with different initialization seeds, although the method is more robust compared to prior work.
- Why unresolved: The paper only reports results for a limited number of initialization seeds and hyperparameter settings. It's unclear how sensitive the results are to these choices.
- What evidence would resolve it: Conducting a more extensive study of the impact of initialization seeds and hyperparameters on length generalization performance across a wider range of tasks and Turing Programs.

### Open Question 3
- Question: Can transformers length generalize on Turing Programs that require multiple steps of computation?
- Basis in paper: [explicit] The paper mentions that the experiment on Turing Machine simulation demonstrates the ability to learn in the "average case", but does not rule out the possibility that some "worst case" Turing Programs have much more restricted length generalization.
- Why unresolved: The paper only tests 1-step prediction of Turing Machine transitions. It's unclear if transformers can maintain accuracy when executing multiple steps of a Turing Program.
- What evidence would resolve it: Training transformers to predict multiple steps of a Turing Machine's computation and evaluating their length generalization performance. This would require developing a method for evaluating the correctness of multi-step predictions.

## Limitations

- Theoretical assumptions of RASP simulation (no repeated n-grams, in-memory operations) may be restrictive for many practical algorithms
- Empirical evaluation limited to specific sequence lengths (50-100 tokens) and a narrow set of algorithmic tasks
- Hard-ALiBi attention window size m is task-dependent and not systematically analyzed for scaling with complexity

## Confidence

**High confidence**: The claim that decomposing algorithmic tasks into tape-like states with minor edits enables length generalization on simple arithmetic tasks (addition, multiplication) is well-supported by empirical results showing near-perfect accuracy on sequences up to 100 digits when trained on 50 digits. The mechanism of using scratchpad format with Hard-ALiBi positional encoding is clearly demonstrated.

**Medium confidence**: The claim that transformers can achieve length generalization on "any algorithmic task" through Turing Programs is supported by the random Turing Program experiments but remains unproven for a broad class of algorithms. The theoretical RASP simulation provides a foundation but relies on restrictive assumptions about n-gram uniqueness and in-memory operations that may not generalize to all practical tasks.

**Low confidence**: The claim that the proposed approach enables "robust" length generalization across all algorithmic tasks is overstated given the limited empirical evaluation and theoretical assumptions. The paper demonstrates success on a narrow set of tasks and does not address failure modes that may arise with more complex algorithms or different task structures.

## Next Checks

**Check 1: Broader algorithmic task evaluation** - Test Turing Programs on additional algorithmic tasks beyond arithmetic, such as sorting, graph algorithms, or string manipulation, to assess generalizability. Specifically, evaluate on tasks like bubble sort or finding longest common subsequence, which involve different computational patterns and may challenge the tape-copying mechanism.

**Check 2: N-gram repetition stress test** - Systematically evaluate model performance on tasks where input or output sequences contain repeated n-grams, violating the theoretical assumption. Generate synthetic datasets with controlled repetition patterns and measure degradation in length generalization to understand the practical limitations of the approach.

**Check 3: Attention window scaling analysis** - Conduct a systematic study of how the Hard-ALiBi attention window size m affects performance across different task complexities and sequence lengths. Measure the minimum m required for successful length generalization on each task and analyze whether m needs to scale with sequence length or task complexity.