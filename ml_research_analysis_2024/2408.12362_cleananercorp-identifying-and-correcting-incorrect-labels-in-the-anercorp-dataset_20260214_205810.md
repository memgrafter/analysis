---
ver: rpa2
title: 'CLEANANERCorp: Identifying and Correcting Incorrect Labels in the ANERcorp
  Dataset'
arxiv_id: '2408.12362'
source_url: https://arxiv.org/abs/2408.12362
tags:
- dataset
- cleananercorp
- anercorp
- arabic
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of label errors in the widely-used
  Arabic Named Entity Recognition (ANERcorp) dataset, which can negatively impact
  model training and evaluation. The authors conducted a thorough re-annotation effort
  to identify and correct errors, resulting in a cleaner version called CLEANANERCorp.
---

# CLEANANERCorp: Identifying and Correcting Incorrect Labels in the ANERcorp Dataset

## Quick Facts
- arXiv ID: 2408.12362
- Source URL: https://arxiv.org/abs/2408.12362
- Authors: Mashael Al-Duwais; Hend Al-Khalifa; Abdulmalik Al-Salman
- Reference count: 0
- Primary result: 6.4% label corrections in ANERcorp yield 7.23% average F1 improvement for monolingual models and up to 19.23% for cross-lingual models

## Executive Summary
This paper addresses the problem of label errors in the widely-used Arabic Named Entity Recognition (ANERcorp) dataset, which can negatively impact model training and evaluation. The authors conducted a thorough re-annotation effort to identify and correct errors, resulting in a cleaner version called CLEANANERCorp. They used automatic error detection with CLEANLAB and manual re-annotation to update 6.4% of the labels in the original dataset. The corrected dataset showed significantly improved annotation quality and consistency. When evaluated with popular Arabic and cross-lingual NER models, CLEANANERCorp achieved marginally higher F1-scores, with an average increase of 7.23% for monolingual models and up to 19.23% for cross-lingual models. The highest gains were observed in the MISC and ORG entity types. The authors conclude that CLEANANERCorp is a more reliable benchmark for Arabic NER research and encourage the community to use it for future studies.

## Method Summary
The authors employed a two-stage approach to clean the ANERcorp dataset. First, they used the CLEANLAB toolkit to automatically identify potential label errors by analyzing prediction probabilities from multiple NER models. Second, they conducted manual re-annotation of the identified errors, resulting in updated labels for 6.4% of the dataset. The quality of the corrected dataset was evaluated using inter-annotator agreement and consistency checks. Finally, they benchmarked popular Arabic and cross-lingual NER models on both the original and cleaned datasets to measure the impact of the corrections on model performance.

## Key Results
- 6.4% of labels in ANERcorp were corrected in CLEANANERCorp
- Average F1-score improvement of 7.23% for monolingual models
- Up to 19.23% F1-score improvement for cross-lingual models
- Largest gains observed in MISC and ORG entity types

## Why This Works (Mechanism)
The improvement works by reducing noise in the training and evaluation data. Label errors create conflicting signals during model training, causing models to learn incorrect patterns. By correcting these errors, the dataset provides cleaner, more consistent supervision signals. The automatic detection using CLEANLAB identifies high-confidence error candidates, while manual verification ensures accuracy. This combination of statistical and human validation creates a more reliable benchmark that better reflects true NER performance.

## Foundational Learning
1. **Named Entity Recognition (NER)** - Why needed: Core task for extracting entities from text; quick check: can identify person, location, organization mentions in sentences
2. **Cross-lingual transfer learning** - Why needed: Enables models trained on one language to perform on another; quick check: can explain zero-shot transfer concept
3. **Label error detection** - Why needed: Identifies incorrect annotations that degrade model performance; quick check: understands CLEANLAB's uncertainty-based approach
4. **Inter-annotator agreement** - Why needed: Measures annotation consistency and quality; quick check: can explain Cohen's kappa or similar metrics
5. **Arabic script processing** - Why needed: Arabic has unique tokenization and morphological challenges; quick check: aware of right-to-left script and lack of capitalization
6. **Entity type taxonomy** - Why needed: Defines categories like PER, LOC, ORG, MISC; quick check: can distinguish between different entity types

## Architecture Onboarding

**Component Map:** Text corpus → CLEANLAB error detection → Manual verification → CLEANANERCorp → Model training → Performance evaluation

**Critical Path:** Error detection (CLEANLAB) → Manual re-annotation → Model training/evaluation

**Design Tradeoffs:** Automatic detection provides scalability but may miss context-dependent errors; manual verification ensures quality but is time-consuming; balance achieved through hybrid approach

**Failure Signatures:** Inconsistent entity boundaries, mislabeled entity types, systematic biases in error patterns, overcorrection of ambiguous cases

**First Experiments:**
1. Run CLEANLAB on a small subset of ANERcorp to verify error detection capability
2. Conduct manual verification on detected errors to measure inter-annotator agreement
3. Train a baseline NER model on both datasets to compare performance differences

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do the improvements in CLEANANERCorp specifically impact the performance of cross-lingual models on other language pairs beyond English to Arabic?
- Basis in paper: [explicit] The paper demonstrates significant performance gains for cross-lingual models (e.g., 19.23% increase for XLM-R-base) when tested on CLEANANERCorp, but focuses only on English to Arabic transfer.
- Why unresolved: The paper does not explore cross-lingual performance on other language pairs, leaving uncertainty about the generalizability of the improvements.
- What evidence would resolve it: Conducting similar experiments with CLEANANERCorp using cross-lingual models trained on other languages (e.g., French to Arabic, German to Arabic) and comparing results to the original ANERcorp dataset.

### Open Question 2
- Question: To what extent do the typographical errors in ANERcorp, such as concatenated words, affect the performance of NER models, and how are these errors addressed in CLEANANERCorp?
- Basis in paper: [explicit] The paper mentions typographical errors like concatenated words (e.g., "فيهاالبلدان") but does not provide details on their correction or impact on model performance.
- Why unresolved: The paper does not specify how these typographical errors were corrected or quantify their impact on model performance.
- What evidence would resolve it: Analyzing the impact of typographical errors on model performance by comparing results on corrected vs. uncorrected datasets and detailing the correction process in CLEANANERCorp.

### Open Question 3
- Question: How does the improved annotation quality in CLEANANERCorp influence the generalization of NER models to out-of-domain data or different Arabic dialects?
- Basis in paper: [inferred] The paper focuses on improving annotation quality and consistency within the news domain of ANERcorp, but does not address out-of-domain generalization or dialectal variations.
- Why unresolved: The study does not test the models on data from different domains or dialects, leaving uncertainty about the broader applicability of the improvements.
- What evidence would resolve it: Evaluating NER models trained on CLEANANERCorp using datasets from different domains (e.g., social media, literature) or Arabic dialects to assess generalization performance.

## Limitations
- Analysis based on abstract-level information without access to full paper or underlying data
- Substantial reported improvements (7.23% average, up to 19.23%) require independent verification
- Methodology for 6.4% label correction process lacks detailed documentation
- No assessment of corrections across different Arabic dialects and domains

## Confidence
- **High Confidence:** Label errors in ANERcorp negatively impact model performance is well-established
- **Medium Confidence:** 6.4% error rate and CLEANLAB + manual methodology are reasonable but need verification
- **Medium Confidence:** Performance improvements are plausible but specific numbers require validation

## Next Checks
1. Replicate error detection and correction process on a subset of ANERcorp using described methodology to verify 6.4% error rate
2. Conduct independent model training and evaluation using both original ANERcorp and CLEANANERCorp to verify reported F1-score improvements
3. Perform error analysis to identify corrected error types and assess systematic biases introduced during re-annotation process