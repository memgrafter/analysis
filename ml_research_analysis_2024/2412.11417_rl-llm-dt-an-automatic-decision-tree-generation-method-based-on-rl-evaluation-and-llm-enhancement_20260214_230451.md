---
ver: rpa2
title: 'RL-LLM-DT: An Automatic Decision Tree Generation Method Based on RL Evaluation
  and LLM Enhancement'
arxiv_id: '2412.11417'
source_url: https://arxiv.org/abs/2412.11417
tags:
- decision
- tree
- center
- self
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes RL-LLM-DT, an automated method for generating
  decision trees using reinforcement learning (RL) to identify weaknesses and large
  language models (LLMs) to refine strategies. The approach iteratively improves a
  decision tree by having RL find its flaws and LLMs generate improved versions based
  on failure analysis.
---

# RL-LLM-DT: An Automatic Decision Tree Generation Method Based on RL Evaluation and LLM Enhancement

## Quick Facts
- arXiv ID: 2412.11417
- Source URL: https://arxiv.org/abs/2412.11417
- Reference count: 33
- Primary result: Automated decision tree generation using RL to find weaknesses and LLMs to improve strategies, achieving top rank on Jidi platform

## Executive Summary
This paper introduces RL-LLM-DT, an automated method for generating robust decision trees through iterative refinement. The approach uses reinforcement learning to identify weaknesses in decision trees and large language models to generate improved versions based on failure analysis. Applied to a curling game environment, the method successfully produced a decision tree that achieved a score of 0.93 out of 1 and ranked first among 34 competing AIs on the Jidi platform. This demonstrates LLMs' ability to enhance decision tree robustness and adaptability through automated refinement, representing a significant advancement in game AI development.

## Method Summary
RL-LLM-DT is an iterative framework that automatically generates and refines decision trees for two-player zero-sum games. The method operates through three core modules: an LLM Coder that converts decision tree descriptions into executable Python code, an RL Debugger that trains neural models to find weaknesses in the decision tree by playing against it, and an LLM Critic that analyzes failure patterns from the RL Debugger and generates improved decision tree versions. The process iterates until no further improvements can be made, with the final decision tree being evaluated on the Jidi platform.

## Key Results
- Achieved a score of 0.93 out of 1 on the Jidi platform evaluation
- Ranked first among 34 competing AI systems on the Jidi platform
- Demonstrated progressive improvement across iterative refinement cycles

## Why This Works (Mechanism)

### Mechanism 1
RL identifies weaknesses in the decision tree by training a neural model to defeat it. The RL Debugger module uses distributed PPO to train a neural model against the decision tree code, collecting game trajectories where the decision tree loses as evidence of vulnerabilities.

### Mechanism 2
LLM generates improved decision tree code based on failure analysis. The LLM Critic analyzes game trajectories where the decision tree loses, identifies patterns and errors that led to failure, and generates an improved version addressing these flaws.

### Mechanism 3
Iterative refinement loop progressively strengthens the decision tree. The framework operates in cycles where LLM Coder generates code from the improved decision tree, RL Debugger finds new weaknesses, and LLM Critic proposes further improvements until neither component can improve the decision tree further.

## Foundational Learning

- **Reinforcement Learning fundamentals**: Understanding how RL algorithms like PPO can be used to train agents that discover weaknesses in existing strategies. Quick check: What is the difference between policy-based and value-based RL algorithms, and why would a policy-based algorithm like PPO be chosen for this application?

- **Large Language Model capabilities for code generation**: LLMs must understand game rules, analyze failure patterns, and generate executable Python code. Quick check: What are the key differences between using an LLM for natural language generation versus code generation, and how do these differences impact the prompting strategy?

- **Decision tree structure and interpretability**: Understanding how decision trees work, their strengths in interpretability, and how they can be iteratively refined. Quick check: How does the interpretability of decision trees compare to neural network policies, and why is this important for game AI applications?

## Architecture Onboarding

- **Component map**: LLM Coder -> RL Debugger -> LLM Critic -> (repeat) -> Submission to Jidi
- **Critical path**: LLM Coder generates decision tree code → RL Debugger trains neural models to find flaws → LLM Critic analyzes failures and generates improvements → (repeat until convergence)
- **Design tradeoffs**: LLM accuracy vs. execution speed, RL training time vs. decision tree improvement rate, decision tree complexity vs. interpretability, neural model strength vs. computational cost
- **Failure signatures**: LLM Coder generates non-executable code, RL Debugger fails to improve decision tree, LLM Critic cannot propose meaningful improvements, curling simulation behaves unexpectedly
- **First 3 experiments**: 1) Test basic LLM Coder functionality with simple decision tree description, 2) Test RL Debugger against static decision tree to verify it can learn to beat it, 3) Run one complete cycle (LLM Coder → RL Debugger → LLM Critic) to verify component integration

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of RL-LLM-DT compare to other hybrid approaches that combine RL and LLMs for decision tree generation in different domains? The paper only evaluates RL-LLM-DT on a curling game environment without comparative analysis with other hybrid approaches or evaluation in different domains.

### Open Question 2
What are the limitations of the LLM Critic in providing actionable improvement advice as the decision tree becomes more complex? The paper states that the LLM Critic's ability to provide clear and actionable improvement advice diminishes after two cycles of refinement, but does not explore the specific limitations or reasons for this degradation.

### Open Question 3
How does the choice of RL algorithm and hyperparameters affect the performance of the RL Debugger in identifying flaws in the decision tree? The paper mentions using distributed PPO but does not explore the impact of different RL algorithms or hyperparameter settings on the effectiveness of the RL Debugger.

## Limitations

- Lack of detailed technical specifications for critical components including specific LLM model, prompt templates, and hyperparameters
- Evaluation limited to a single game domain (curling), raising questions about generalizability to other two-player zero-sum games
- No comparative analysis with established game AI methods or evaluation across multiple game domains

## Confidence

- **High Confidence**: The core iterative framework concept (RL finding weaknesses, LLM generating improvements) is logically sound and supported by experimental results showing progressive improvement
- **Medium Confidence**: The effectiveness of LLMs for analyzing game failure patterns and generating improved decision tree code is demonstrated but not extensively validated across different failure modes or game types
- **Low Confidence**: The claim that this represents a "significant advancement in game AI development" is difficult to verify without comparison to established game AI methods or evaluation in multiple game domains

## Next Checks

1. **Cross-Domain Generalization Test**: Apply the RL-LLM-DT framework to a different two-player zero-sum game (e.g., Tic-Tac-Toe or Connect Four) to evaluate whether the method generalizes beyond curling.

2. **Baseline Comparison Study**: Implement a version of the method that uses only RL (without LLM enhancement) and compare performance to the full RL-LLM-DT approach to quantify the LLM contribution.

3. **Decision Tree Complexity Analysis**: Measure how decision tree size and complexity evolve across iterations, and assess whether the improvements come from better strategies or simply more complex trees that overfit to the RL Debugger's weaknesses.