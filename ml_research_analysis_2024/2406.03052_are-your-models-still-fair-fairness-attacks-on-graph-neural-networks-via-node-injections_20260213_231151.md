---
ver: rpa2
title: Are Your Models Still Fair? Fairness Attacks on Graph Neural Networks via Node
  Injections
arxiv_id: '2406.03052'
source_url: https://arxiv.org/abs/2406.03052
tags:
- fairness
- nodes
- attack
- node
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "NIFA is the first node injection-based poisoning attack on GNN\
  \ fairness, bypassing the need to modify existing graph structure. It injects a\
  \ small number of malicious nodes (1% of labeled nodes) and optimizes their features\
  \ to maximize group fairness metrics (\u2206SP, \u2206EO) without severely degrading\
  \ accuracy."
---

# Are Your Models Still Fair? Fairness Attacks on Graph Neural Networks via Node Injections

## Quick Facts
- **arXiv ID**: 2406.03052
- **Source URL**: https://arxiv.org/abs/2406.03052
- **Reference count**: 40
- **One-line primary result**: NIFA is the first node injection-based poisoning attack on GNN fairness, bypassing the need to modify existing graph structure.

## Executive Summary
NIFA introduces a novel node injection-based poisoning attack targeting fairness in Graph Neural Networks (GNNs). Unlike previous attacks that modify existing graph structure, NIFA injects a small number of malicious nodes (1% of labeled nodes) and optimizes their features to maximize group fairness metrics without severely degrading accuracy. The attack operates under gray-box settings and leverages two key principles: uncertainty-maximization for target selection and homophily-increase for connection strategy. Experiments on three real-world datasets demonstrate NIFA's effectiveness against mainstream GNNs and even fairness-aware GNNs, significantly undermining fairness while maintaining nearly unchanged accuracy.

## Method Summary
NIFA works by first estimating model uncertainty using a Bayesian GNN to identify high-uncertainty nodes as injection targets. It then injects malicious nodes and connects them within the same sensitive group to increase homophily. The injected nodes' features are optimized through multiple objective functions (classification loss, fairness loss based on SP and EO, and feature constraints) while the surrogate model is trained iteratively. This balances fairness deterioration with utility preservation. The method achieves significant fairness metric deterioration while maintaining accuracy, demonstrating effectiveness against both standard and fairness-aware GNNs.

## Key Results
- NIFA can increase âˆ†SP from ~7% to ~17% on Pokec-z dataset while maintaining accuracy
- The attack successfully undermines fairness in mainstream GNNs including GCN, GAT, and GraphSAGE
- NIFA outperforms existing fairness attacks in effectiveness and stealthiness

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty-Maximization Principle
Injecting malicious nodes with high uncertainty target selection increases attack effectiveness by exploiting decision boundary proximity. Uncertainty-maximization principle selects nodes with highest Bayesian model uncertainty as injection targets. These nodes are closer to decision boundaries and more susceptible to adversarial influence, amplifying the attack's impact on fairness metrics. Core assumption: Model uncertainty correlates with vulnerability to adversarial manipulation and fairness metric deterioration.

### Mechanism 2: Homophily-Increase Principle
Homophily-increase principle amplifies fairness deterioration by enhancing within-group message propagation. Injected nodes connect only to target nodes within the same sensitive group, increasing node-level homophily ratio. This strengthens information propagation within groups and accentuates differences between groups, worsening fairness metrics. Core assumption: Higher homophily ratio leads to stronger within-group information propagation and increased fairness disparities.

### Mechanism 3: Iterative Feature Optimization
Iterative feature optimization with fairness-aware objectives maximizes unfairness while maintaining utility. Injected node features are optimized through multiple objective functions (classification loss, fairness loss based on SP and EO, and feature constraint) while the surrogate model is trained iteratively. This balances fairness deterioration with utility preservation. Core assumption: The surrogate model can effectively guide feature optimization toward maximizing fairness metrics without severely degrading accuracy.

## Foundational Learning

- **Graph Neural Networks (GNNs) and message passing mechanism**: Understanding how GNNs propagate information is crucial for grasping why injecting nodes with specific features and connections can influence fairness metrics. *Quick check: How does the message passing in GNNs aggregate information from neighbors, and why would this affect fairness?*

- **Group fairness metrics (Statistical Parity and Equal Opportunity)**: These are the specific fairness metrics that NIFA targets, and understanding their definitions is essential for evaluating the attack's effectiveness. *Quick check: What is the difference between Statistical Parity and Equal Opportunity, and how are they calculated?*

- **Uncertainty estimation in machine learning models**: NIFA uses Bayesian GNNs to estimate model uncertainty, which is central to the uncertainty-maximization principle for target node selection. *Quick check: How does Bayesian modeling with Monte Carlo dropout provide uncertainty estimates, and why would higher uncertainty indicate vulnerability?*

## Architecture Onboarding

- **Component map**: Uncertainty estimation module (Bayesian GNN) -> Node injection module (target selection + connection strategy) -> Feature optimization module (surrogate model training + injected feature optimization) -> Evaluation module (fairness metrics calculation)

- **Critical path**: 
  1. Estimate uncertainty for all nodes
  2. Select top-k% uncertain nodes per sensitive group as targets
  3. Inject malicious nodes and connect them to targets within same sensitive group
  4. Iteratively optimize surrogate model and injected features
  5. Evaluate fairness metrics on victim model trained on poisoned graph

- **Design tradeoffs**: 
  - Uncertainty estimation vs. computational cost (Bayesian methods are expensive)
  - Number of injected nodes vs. attack effectiveness vs. detectability
  - Feature optimization complexity vs. attack robustness

- **Failure signatures**: 
  - If uncertainty estimation fails, the attack may target the wrong nodes
  - If the homophily-increase principle is not effective, fairness metrics may not deteriorate
  - If feature optimization overfits to the surrogate model, the attack may fail on the victim model

- **First 3 experiments**: 
  1. Test uncertainty estimation: Run Bayesian GNN on clean graph and verify that uncertain nodes are indeed near decision boundaries
  2. Test homophily impact: Inject nodes with homophily-increase principle and measure change in node-level homophily ratios
  3. Test feature optimization: Optimize injected features with each objective function separately and measure their individual impact on fairness metrics

## Open Questions the Paper Calls Out

- **Open Question 1**: How effective would NIFA be against GNN models with robust training techniques beyond those tested? *Basis in paper: The paper demonstrates NIFA's effectiveness against several GNN architectures but does not test it against models specifically designed to be robust to adversarial attacks or node injection.*

- **Open Question 2**: Can NIFA's attack effectiveness be further improved by optimizing the number of injected nodes beyond the tested 1% budget? *Basis in paper: The paper tests NIFA with a 1% perturbation rate and shows significant impact, but does not explore the effects of varying this budget.*

- **Open Question 3**: How would NIFA perform under black-box attack settings where the attacker has limited knowledge of the graph and model? *Basis in paper: The paper explicitly states that NIFA is under gray-box settings and acknowledges the limitation of not extending to black-box scenarios.*

## Limitations

- The attack's effectiveness depends on accurate uncertainty estimation, which may not always correlate with decision boundary proximity
- The homophily-increase principle assumes specific information propagation patterns that may not hold for all GNN architectures
- The attack is evaluated only under gray-box settings, leaving black-box scenarios unexplored

## Confidence

- **High Confidence**: The core methodology of injecting nodes with optimized features to manipulate fairness metrics is well-defined and supported by experimental results across three datasets
- **Medium Confidence**: The two guiding principles (uncertainty-maximization and homophily-increase) are conceptually reasonable and partially supported by literature, but their direct impact on attack effectiveness could benefit from more rigorous validation
- **Low Confidence**: Claims about the attack's superiority over existing fairness attacks are based on limited comparisons and may not generalize to all attack scenarios or GNN architectures

## Next Checks

1. **Uncertainty Correlation Validation**: Conduct controlled experiments to empirically measure the correlation between model uncertainty estimates and actual vulnerability to fairness attacks. Test different uncertainty estimation methods to verify robustness.

2. **Homophily Propagation Analysis**: Systematically vary homophily levels in injected nodes and measure the resulting changes in fairness metrics across different GNN architectures to validate the assumed relationship between homophily and fairness deterioration.

3. **Surrogate-Victim Model Gap Assessment**: Design experiments to quantify how performance degrades when the surrogate model differs from the victim model in terms of architecture, training data, or hyperparameters.