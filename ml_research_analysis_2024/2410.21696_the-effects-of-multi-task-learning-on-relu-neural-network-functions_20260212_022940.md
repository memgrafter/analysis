---
ver: rpa2
title: The Effects of Multi-Task Learning on ReLU Neural Network Functions
arxiv_id: '2410.21696'
source_url: https://arxiv.org/abs/2410.21696
tags:
- neural
- which
- solutions
- network
- multi-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the properties of solutions to multi-task shallow
  ReLU neural network learning problems with minimal sum of squared weights. The key
  finding is that univariate-input, multi-task neural network interpolation problems
  almost always have unique solutions, which coincide with minimum-norm interpolation
  in a Sobolev Reproducing Kernel Hilbert Space.
---

# The Effects of Multi-Task Learning on ReLU Neural Network Functions

## Quick Facts
- **arXiv ID:** 2410.21696
- **Source URL:** https://arxiv.org/abs/2410.21696
- **Authors:** Julia Nakhleh; Joseph Shenouda; Robert D. Nowak
- **Reference count:** 40
- **Primary result:** Univariate multi-task ReLU networks with minimal sum of squared weights almost always learn unique solutions that coincide with minimum-norm interpolation in a Sobolev Reproducing Kernel Hilbert Space.

## Executive Summary
This paper investigates the properties of multi-task shallow ReLU neural network learning problems with minimal sum of squared weights. The authors prove that univariate-input, multi-task neural network interpolation problems almost always have unique solutions that coincide with minimum-norm interpolation in a Sobolev Reproducing Kernel Hilbert Space. In contrast, single-task neural network learning problems typically have non-unique solutions. The analysis shows that multi-task training promotes neuron sharing and results in functions that resemble kernel regression methods, while single-task learning favors sparse neuron usage.

## Method Summary
The paper studies shallow ReLU neural networks trained to interpolate data points with minimal sum of squared output weights. The architecture consists of K ReLU neurons with learnable input weights, biases, and output weights, plus an optional affine residual connection. For univariate inputs, the authors prove uniqueness of solutions under certain conditions and characterize them as minimum-norm interpolants in a Sobolev space. For multivariate inputs, they provide empirical evidence and mathematical analysis showing that large numbers of tasks approximately reduce the problem to ℓ² minimization over a fixed kernel. The paper uses both synthetic datasets (5 red points in univariate case, 2D squares in multivariate case) and random label experiments to validate their theoretical claims.

## Key Results
- Univariate multi-task ReLU networks with weight decay regularization almost always learn unique solutions that correspond to minimum-norm interpolation in a Sobolev Reproducing Kernel Hilbert Space
- Single-task ReLU networks typically have non-unique solutions that are minimum-norm interpolants in a non-Hilbertian Banach space
- Multi-task learning promotes neuron sharing such that all neurons contribute to all tasks, unlike single-task learning which typically results in sparse neuron usage
- For multivariate multi-task learning with large numbers of tasks, the optimization problem approximately reduces to an ℓ² (Hilbert space) minimization over a fixed kernel determined by the optimal neurons

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-task ReLU neural networks with minimal sum of squared weights almost always learn unique solutions that correspond to minimum-norm interpolation in a Sobolev Reproducing Kernel Hilbert Space.
- **Mechanism:** When multiple tasks are trained simultaneously, the neuron sharing induced by the regularization termPK k=1 ∥vk∥2 forces each neuron to contribute to all tasks. This constraint makes the representational cost separable across neurons and promotes a unique optimal function, which in univariate settings is the connect-the-dots interpolant. This interpolant coincides with the minimum-norm solution in the first-order Sobolev space H¹.
- **Core assumption:** The input dimension is univariate (d = 1) and the tasks are diverse enough that the alignment conditionsi − si−1 and si+1 − si being aligned almost never occurs.
- **Evidence anchors:** - [abstract]: "we prove that the solutions to univariate-input, multi-task neural network interpolation problems are almost always unique, and coincide with the solution to a minimum-norm interpolation problem in a Sobolev (Reproducing Kernel) Hilbert Space." - [section]: "Our result shows that the individual outputs of solutions to (3) for T > 1 tasks almost always coincide on [x1, xN ] with this kernel solution." - [corpus]: Weak. No direct mention of RKHS uniqueness in related papers; they focus on verification or approximation rather than uniqueness.
- **Break condition:** The alignment conditionsi − si−1 and si+1 − si being aligned for somei causes non-uniqueness, which occurs with Lebesgue measure zero for continuous distributions.

### Mechanism 2
- **Claim:** For multivariate multi-task learning with large numbers of tasks, the optimization problem approximately reduces to an ℓ² (Hilbert space) minimization over a fixed kernel determined by the optimal neurons.
- **Mechanism:** As the number of tasksT grows, the optimal output weights for each neuron become small for any individual task. This makes the regularization term approximately quadratic in the task-specific weights, resembling a weighted ℓ² regularizer. Consequently, the solution behaves like a minimum-norm interpolant in a finite-dimensional RKHS.
- **Core assumption:** The tasks are exchangeable (their order is irrelevant) and the number of tasksT is large enough that the Taylor approximation of the square root in the regularization term is valid.
- **Evidence anchors:** - [abstract]: "neural network learning problems with large numbers of tasks are approximately equivalent to an ℓ² (Hilbert space) minimization problem over a fixed kernel determined by the optimal neurons." - [section]: "The multi-task learning problem with exchangeable tasks favors linear combinations of the optimal neurons which have a minimal weighted ℓ² regularization penalty." - [corpus]: Weak. Related papers discuss approximation or verification but do not explicitly model the large-T approximation to Hilbert space minimization.
- **Break condition:** If the number of tasks is small, the Taylor approximation fails and the problem behaves more like an ℓ¹ penalized optimization.

### Mechanism 3
- **Claim:** Multi-task learning promotes neuron sharing such that all neurons contribute to all tasks, unlike single-task learning which typically results in sparse neuron usage.
- **Mechanism:** The regularization termPK k=1 ∥vk∥2 penalizes the ℓ² norm of the output weights, encouraging neurons to be used by multiple tasks to amortize the cost. This contrasts with single-task learning where the sparsity-inducing effect is weaker.
- **Core assumption:** The regularization strengthλ is sufficiently large to induce sharing but not so large as to prevent fitting the data.
- **Evidence anchors:** - [abstract]: "It has recently been shown to promote neuron sharing in the network, such that only a few neurons contribute to all tasks." - [section]: "Our result above shows that univariate multi-task training is an extreme example of this phenomenon, since fD can be represented using only N − 1 neurons, all of which contribute to all of the network outputs." - [corpus]: Weak. Related papers do not discuss neuron sharing as a mechanism for multi-task regularization.
- **Break condition:** If regularization is removed or set to zero, neurons no longer need to share across tasks, and the uniqueness property may be lost.

## Foundational Learning

- **Concept:** Reproducing Kernel Hilbert Spaces (RKHS) and their representer theorems.
  - **Why needed here:** The paper shows that multi-task solutions correspond to minimum-norm interpolants in an RKHS, so understanding RKHS norms and representer theorems is essential to grasp why the solutions are unique and what function space they live in.
  - **Quick check question:** What is the reproducing property of an RKHS and how does it relate to the kernel function?

- **Concept:** Convex analysis and uniqueness of solutions to optimization problems.
  - **Why needed here:** The proof of uniqueness relies on showing that certain conditions lead to a unique global minimizer, which requires understanding convexity, coercivity, and conditions for uniqueness in non-convex settings.
  - **Quick check question:** Under what conditions is a solution to a non-convex optimization problem guaranteed to be unique?

- **Concept:** Exchangeability of random variables and its implications for multi-task learning.
  - **Why needed here:** The paper uses exchangeability to argue that as the number of tasks grows, the regularization behaves like an ℓ² norm, which is key to the Hilbert space approximation result.
  - **Quick check question:** What does it mean for a sequence of random variables to be exchangeable, and how does this property simplify analysis?

## Architecture Onboarding

- **Component map:** Input layer (univariate or multivariate data) -> Hidden layer (K ReLU neurons with learnable input weights, biases, and output weights) -> Output layer (T task-specific outputs, each a linear combination of hidden neuron activations) -> Regularization (sum of squared output weightsPK k=1 ∥vk∥2) -> Optional residual connection (Ax + c, not regularized)

- **Critical path:**
  1. Define the neural network architecture with K neurons
  2. Set up the interpolation constraints fθ(xi) = yi for all data points
  3. Add the regularization term to the objective
  4. Solve the non-convex optimization problem (analytically in univariate case, numerically in multivariate)
  5. Analyze the learned function and compare to kernel regression solutions

- **Design tradeoffs:**
  - Number of neurons K: Must be ≥ N² for well-posedness, but larger K increases computational cost
  - Regularization strength λ: Controls the trade-off between fitting the data and smoothness; too large prevents interpolation
  - Task diversity: Highly correlated tasks may not exhibit the uniqueness property as strongly

- **Failure signatures:**
  - Non-unique solutions: Alignment of slope vectorsi − si−1 and si+1 − si
  - Poor interpolation: Insufficient regularization or too few neurons
  - Numerical instability: Ill-conditioned kernel matrix in the RKHS formulation

- **First 3 experiments:**
  1. Replicate the univariate example from Figure 1: train a single-task network and show multiple solutions; then add a second random task and verify uniqueness
  2. Implement the multivariate example from Figure 5: generate the two-square dataset, train single-task vs multi-task networks, and compare smoothness and variability
  3. Test the large-T approximation: generate a dataset with increasing numbers of random tasks, measure how closely the multi-task solution matches solving (27) over the fixed kernel

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the uniqueness of multi-task solutions extend to deep ReLU neural networks beyond the shallow case?
  - **Basis in paper:** [inferred] The paper focuses on shallow networks and mentions that future work could aim to extend these results to deep neural network architectures.
  - **Why unresolved:** The current analysis relies heavily on the structure of shallow networks where each neuron corresponds to a knot in the piecewise linear function. Deep networks have more complex representations where neurons in different layers interact in ways that may prevent such clean correspondences.
  - **What evidence would resolve it:** A rigorous proof showing either that deep multi-task ReLU networks always have unique solutions under certain conditions, or a counterexample demonstrating non-uniqueness in the deep case.

- **Open Question 2:** What is the precise characterization of the reproducing kernel Hilbert space (RKHS) in the multivariate-input case for large numbers of tasks?
  - **Basis in paper:** [explicit] The paper states "we have not precisely characterized what that kernel is in the multi-input case as we have in the single-input case."
  - **Why unresolved:** While the paper provides empirical evidence and mathematical analysis suggesting that multivariate multi-task solutions behave like ℓ² regression over a fixed kernel, the exact form of this kernel remains unknown.
  - **What evidence would resolve it:** A mathematical derivation of the kernel function in the multivariate case, potentially through analyzing the limiting behavior of the optimal neurons as the number of tasks grows.

- **Open Question 3:** Do gradient descent-based training algorithms converge to the unique global solutions in multi-task learning problems?
  - **Basis in paper:** [explicit] "Whether or not networks trained with gradient descent-based algorithms will converge to global solutions remains an open question."
  - **Why unresolved:** The paper characterizes global solutions but doesn't analyze the training dynamics or convergence properties of gradient descent methods.
  - **What evidence would resolve it:** A theoretical proof of convergence to global solutions for gradient descent in the multi-task setting, or empirical evidence showing consistent convergence across different random initializations and learning rates.

## Limitations
- Theoretical results are proven only for univariate input dimensions, with multivariate results relying on empirical validation
- The large-T approximation analysis assumes exchangeable tasks and relies on Taylor expansions that may not hold for small numbers of tasks
- The proofs require specific conditions (alignment of slope vectors) that have measure zero but are not fully characterized for all input distributions

## Confidence
- Uniqueness of univariate multi-task solutions: **High confidence** (rigorous mathematical proof)
- Equivalence to Sobolev RKHS solutions: **High confidence** (theoretical derivation supported by examples)
- Large-T approximation to Hilbert space minimization: **Medium confidence** (based on empirical evidence and approximation arguments)
- Neuron sharing promotion: **Medium confidence** (supported by examples but not rigorously quantified)

## Next Checks
1. Implement the convex optimization equivalent to problem (2) to verify the theoretical solutions match numerical results for univariate cases
2. Systematically vary the number of tasks T and measure the gap between multi-task solutions and the fixed-kernel approximation (27)
3. Test the uniqueness property on multivariate datasets with different input dimensionalities and correlation structures to quantify how quickly the uniqueness breaks down