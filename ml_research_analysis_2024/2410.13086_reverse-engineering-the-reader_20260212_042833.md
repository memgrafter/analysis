---
ver: rpa2
title: Reverse-Engineering the Reader
arxiv_id: '2410.13086'
source_url: https://arxiv.org/abs/2410.13086
tags:
- reading
- language
- surprisal
- data
- gpt2-m
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to fine-tune language models to
  better predict human reading times by aligning them to psychometric data. The approach
  involves optimizing the parameters of a language model to improve the predictive
  power of a linear regressor that estimates reading times from surprisal values derived
  from the model.
---

# Reverse-Engineering the Reader

## Quick Facts
- arXiv ID: 2410.13086
- Source URL: https://arxiv.org/abs/2410.13086
- Authors: Samuel Kiegeland; Ethan Gotlieb Wilcox; Afra Amini; David Robert Reich; Ryan Cotterell
- Reference count: 39
- Primary result: Fine-tuning language models to predict human reading times improves psychometric predictive power but reduces language modeling quality and downstream task performance.

## Executive Summary
This paper introduces a method to fine-tune language models to better predict human reading times by aligning them to psychometric data. The approach involves optimizing the parameters of a language model to improve the predictive power of a linear regressor that estimates reading times from surprisal values derived from the model. Experiments across multiple model sizes and datasets show that the fine-tuning improves the model's psychometric predictive power. However, this comes at the cost of increased perplexity and reduced performance on downstream NLP tasks. The findings demonstrate that aligning language models to human reading times makes them worse at modeling text but better at predicting cognitive effort during reading.

## Method Summary
The method involves fine-tuning GPT-2 models (small, medium, large) on eye-tracking datasets (Dundee, Provo, ZuCo) using a novel objective that implicitly optimizes linear regression coefficients to predict reading times from surprisal estimates. The fine-tuning procedure uses 5k steps with batch size 1, gradient accumulation 2, and cosine annealing learning rate schedule. KL regularization with varying coefficients (0, 5, 50, 500) is applied to balance psychometric alignment with language modeling quality. The main metric is delta log-likelihood (Δllh) between baseline and target linear regressors, with secondary metrics including mean squared error, KL divergence, perplexity, and downstream NLP task performance.

## Key Results
- Fine-tuning GPT-2 models on eye-tracking data significantly improves psychometric predictive power (Δllh) across all model sizes
- Larger language models show a trade-off where decreased perplexity leads to reduced alignment with human reading times beyond a certain model size
- The fine-tuning approach achieves the stated objective of better predicting human reading times but at the cost of increased perplexity and reduced downstream task performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-tuning optimizes a language model to better predict human reading times by implicitly fitting linear regression coefficients to the surprisal values produced by the model.
- **Mechanism**: The objective function rewards lower mean squared error between predicted reading times (via linear regression using surprisal estimates) and actual human reading times. By differentiating through the regression coefficients, the model's parameters are adjusted to improve psychometric predictive power.
- **Core assumption**: The relationship between surprisal and reading times is affine, as assumed in surprisal theory.
- **Evidence anchors**: The abstract and section 3.1 describe the fine-tuning objective and its connection to linear regression optimization.

### Mechanism 2
- **Claim**: KL regularization prevents the fine-tuned model from diverging too far from the pretrained model, balancing psychometric alignment with language modeling quality.
- **Mechanism**: The KL divergence term penalizes large deviations in the model's probability distribution from the pretrained model, constraining the space of possible fine-tuned models.
- **Core assumption**: Maintaining some similarity to the pretrained model is beneficial for overall language modeling quality.
- **Evidence anchors**: The abstract mentions the trend of decreased language modeling quality, and section 5.4 discusses the impact of different KL coefficients.

### Mechanism 3
- **Claim**: Larger language models exhibit a trade-off between perplexity and psychometric predictive power, with an inflection point where further improvements in perplexity lead to decreased alignment with human reading times.
- **Mechanism**: As language models become larger and more capable of predicting rare words and named entities, their surprisal estimates become less aligned with human processing effort.
- **Core assumption**: The superhuman predictive abilities of larger models lead to a mismatch with human cognitive processing.
- **Evidence anchors**: The abstract and section 5.3 discuss the observed trade-off between model size, perplexity, and psychometric predictive power.

## Foundational Learning

- **Concept**: Linear regression and its application to psycholinguistic modeling.
  - **Why needed here**: Understanding how linear regression is used to predict reading times from surprisal estimates is crucial for grasping the fine-tuning objective.
  - **Quick check question**: What is the purpose of including baseline predictors (e.g., unigram surprisal, length) in addition to contextual surprisal in the linear regression model?

- **Concept**: Surprisal theory and its relationship to human language processing.
  - **Why needed here**: Surprisal theory provides the theoretical foundation for linking language model predictions to human cognitive effort during reading.
  - **Quick check question**: How does surprisal theory explain the relationship between a word's predictability and the cognitive effort required to process it?

- **Concept**: Fine-tuning and its impact on language model behavior.
  - **Why needed here**: Understanding how fine-tuning modifies a language model's parameters and affects its performance on various tasks is essential for interpreting the results.
  - **Quick check question**: What are the potential trade-offs between optimizing a language model for psychometric predictive power and maintaining its performance on other language modeling tasks?

## Architecture Onboarding

- **Component map**: Language model → Surprisal estimates → Linear regression → Reading time prediction → Regression objective → Language model fine-tuning
- **Critical path**: Language model → Surprisal estimates → Linear regression → Reading time prediction → Regression objective → Language model fine-tuning
- **Design tradeoffs**: Balancing psychometric alignment with language modeling quality through KL regularization; choosing appropriate baseline predictors; selecting KL regularization strength
- **Failure signatures**: Decrease in Δllh despite fine-tuning; large increase in perplexity and decrease in downstream task performance; instability in the fine-tuning process
- **First 3 experiments**:
  1. Fine-tune a small language model on a single eye-tracking dataset and evaluate its psychometric predictive power (Δllh) and perplexity
  2. Repeat experiment 1 with varying strengths of KL regularization to observe its impact on the trade-off between psychometric alignment and language modeling quality
  3. Fine-tune the same model on a different eye-tracking dataset and evaluate its performance to assess the generalizability of the approach across datasets

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does alignment on reading times improve the predictive power of language models for other cognitive tasks, such as fMRI or EEG data?
- **Basis in paper**: [inferred] The paper discusses the potential of using aligned models to test and refine psychological theories, but does not investigate the generalization of alignment to other cognitive tasks.
- **Why unresolved**: The paper focuses on eye-tracking data and does not explore the applicability of alignment to other types of cognitive data.
- **What evidence would resolve it**: Experiments showing improved predictive power of aligned models on other cognitive datasets, such as fMRI or EEG, would demonstrate the generalizability of alignment.

### Open Question 2
- **Question**: What is the impact of including previous units' predictors and additional predictors, such as contextual entropy, on the objective and the resulting aligned models?
- **Basis in paper**: [explicit] The paper acknowledges that the study only includes predictors for the current unit and suggests that future research could explore the impact of adding previous units' predictors and additional predictors.
- **Why unresolved**: The paper does not investigate the effects of including more comprehensive predictors in the objective.
- **What evidence would resolve it**: Experiments comparing the performance of models trained with different sets of predictors, including previous units and contextual entropy, would provide insights into the impact of predictor choice.

### Open Question 3
- **Question**: How does the choice of tokenization affect the calculation of surprisal and the resulting aligned models?
- **Basis in paper**: [explicit] The paper mentions that recent work has argued that leading whitespaces from tokenization pose a confound to surprisal calculations and that the probability of trailing whitespaces should be included instead.
- **Why unresolved**: The paper does not investigate the impact of different tokenization strategies on the calculation of surprisal and the resulting aligned models.
- **What evidence would resolve it**: Experiments comparing the performance of models trained with different tokenization strategies, including the inclusion of trailing whitespaces, would provide insights into the impact of tokenization on alignment.

## Limitations

- The evaluation relies entirely on eye-tracking datasets from English-language corpora, raising questions about cross-linguistic applicability
- The choice of linear regression with specific baseline predictors may not capture the full complexity of human reading behavior
- The computational costs of the fine-tuning approach are significant, with 5k steps of fine-tuning required, suggesting substantial resource requirements
- The paper does not investigate whether the fine-tuned models capture psychologically plausible representations beyond improved prediction accuracy

## Confidence

- **High Confidence (9/10)**: The core finding that fine-tuning GPT-2 models to optimize psychometric predictive power (Δllh) is technically sound and well-supported by experimental evidence
- **Medium Confidence (6/10)**: The claim that larger language models exhibit a trade-off between perplexity and psychometric predictive power, with an inflection point where further improvements in perplexity lead to decreased alignment with human reading times
- **Low Confidence (4/10)**: The assertion that the fine-tuning approach could be applied to other cognitive models beyond reading times (e.g., language acceptability judgments, speech errors) is speculative

## Next Checks

1. **Cross-linguistic validation**: Apply the fine-tuning approach to a non-English eye-tracking corpus (e.g., Potsdam Sentence Corpus for German) to test whether the observed trade-offs between psychometric predictive power and language modeling quality generalize across languages with different morphological and syntactic properties.

2. **Ablation study on baseline predictors**: Systematically remove individual baseline predictors (unigram surprisal, length, or syntactic features) from the linear regression model to quantify their individual contributions to the overall psychometric predictive power and determine whether contextual surprisal alone could capture human reading behavior.

3. **Downstream task analysis**: Conduct a more fine-grained analysis of the impact on downstream task performance by examining which specific types of NLP tasks are most affected by the fine-tuning (e.g., tasks requiring rare word prediction vs. common language understanding) to better understand the nature of the performance trade-offs.