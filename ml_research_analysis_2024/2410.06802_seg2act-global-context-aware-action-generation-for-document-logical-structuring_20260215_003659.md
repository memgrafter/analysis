---
ver: rpa2
title: 'Seg2Act: Global Context-aware Action Generation for Document Logical Structuring'
arxiv_id: '2410.06802'
source_url: https://arxiv.org/abs/2410.06802
tags:
- document
- logical
- action
- seg2act
- structure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of document logical structuring,
  which aims to extract the hierarchical structure of documents. The proposed method,
  SEG2ACT, treats logical structure extraction as an action generation task.
---

# Seg2Act: Global Context-aware Action Generation for Document Logical Structuring

## Quick Facts
- arXiv ID: 2410.06802
- Source URL: https://arxiv.org/abs/2410.06802
- Authors: Zichao Li; Shaojie He; Meng Liao; Xuanang Chen; Yaojie Lu; Hongyu Lin; Yanxiong Lu; Xianpei Han; Le Sun
- Reference count: 29
- Primary result: SEG2ACT achieves state-of-the-art performance in document logical structuring with F1-score improvements of +1.58 to +1.76 through multi-segment multi-action strategy

## Executive Summary
SEG2ACT addresses document logical structuring by treating it as an action generation task rather than a pipeline of subtasks. The method employs a global context-aware generative model that iteratively constructs document hierarchical structures through a sequence of actions (new heading, new paragraph, or concatenation). A key innovation is the global context stack that maintains long-range dependencies during generation, enabling better handling of complex document structures. The approach achieves state-of-the-art performance on both Chinese (ChCatExt) and English (HierDoc) datasets, particularly excelling in transfer learning scenarios.

## Method Summary
SEG2ACT converts document logical structure extraction into an action generation task where text segments are mapped to actions that incrementally build a hierarchical tree. The model processes multiple text segments simultaneously (window size wI) and predicts corresponding actions (window size wO), using a global context stack to maintain hierarchical information. Actions include creating new headings at various levels, creating new paragraphs, or concatenating text to existing paragraphs. The method uses teacher-forcing cross-entropy loss during training and can leverage pre-training on generic text corpora for improved transfer learning performance.

## Key Results
- SEG2ACT achieves state-of-the-art F1-scores on both ChCatExt and HierDoc datasets
- Multi-segment multi-action strategy improves F1-score by +1.58 to +1.76 compared to single-segment processing
- Transfer learning from ChCatExt to HierDoc yields a 2.1-point F1-score improvement over non-transfer baselines
- Outperforms traditional pipeline approaches and multimodal methods in both supervised and transfer settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The global context stack enables the model to maintain and utilize long-range dependencies during action generation.
- Mechanism: The stack selectively stores nodes from the partially constructed logical structure, representing the current path and its ancestors in the hierarchy. At each generation step, the stack provides context about previously added headings and paragraphs, allowing the model to make informed decisions about the current text segment's role.
- Core assumption: Maintaining a compressed representation of the document's logical path is sufficient for capturing necessary global context.
- Evidence anchors:
  - [abstract]: "maintains a global context stack to capture long-range dependencies and enhance the model's global awareness"
  - [section]: "we design a global context stack to provide global information to aid the model in decision-making" and "the stack stores the last added node at the top, followed by all the nodes along the upward backtracking path"
  - [corpus]: Weak evidence - related papers discuss document structure but don't specifically validate stack-based global context mechanisms.
- Break condition: If the stack grows too large, it may exceed input token limits; if too small, it may not capture sufficient context for complex structures.

### Mechanism 2
- Claim: The multi-segment multi-action strategy improves both efficiency and performance by expanding the model's perspective.
- Mechanism: Instead of processing one text segment at a time, the model receives wI consecutive segments and predicts wO actions simultaneously. This allows the model to consider broader context when making decisions and reduces the total number of generation steps needed.
- Core assumption: Context from neighboring segments helps disambiguate the role of the current segment.
- Evidence anchors:
  - [section]: "we not only extend the length of the input segment window, denoted as wI, but also extend the output action window's length, denoted as wO" and "allows the input segment window to encompass a more extensive range of contextual information"
  - [section]: "Extending the input segment window length wI to 2, 3, 4, and 5, the SEG2ACT method exhibits improvements in F1-score of +1.58, +1.27, +1.76, and +1.50"
  - [corpus]: Weak evidence - related papers discuss document parsing but don't specifically validate multi-segment strategies.
- Break condition: If wI and wO become too large, the input may exceed model token limits; if too small, the benefits of context expansion are lost.

### Mechanism 3
- Claim: Treating logical structure extraction as an action generation task provides better generalization than pipeline approaches.
- Mechanism: Instead of decomposing the task into subtasks (feature extraction, heading detection, relationship prediction), SEG2ACT directly maps text segments to actions that construct the logical structure. This end-to-end approach reduces error propagation and learns document structure patterns more holistically.
- Core assumption: A unified generative model can learn the mapping between text segments and structural actions across diverse document types.
- Evidence anchors:
  - [abstract]: "revisiting logical structure extraction as an action generation task" and "Experiments on ChCatExt and HierDoc datasets demonstrate that SEG2ACT achieves state-of-the-art performance"
  - [section]: "Instead of decomposing the extraction of logical structure into subtasks, we revisit structure extraction as an action generation task"
  - [corpus]: Weak evidence - related papers discuss document structure but don't specifically validate action generation approaches.
- Break condition: If action space becomes too complex or document types too diverse, the model may struggle to learn effective mappings.

## Foundational Learning

- Concept: Document logical structure and hierarchical tree representations
  - Why needed here: Understanding the target output format is essential for designing appropriate actions and evaluating model performance
  - Quick check question: What are the components of a document's logical structure, and how do they relate hierarchically?

- Concept: Sequence-to-sequence generation with constrained output spaces
  - Why needed here: SEG2ACT generates action sequences with specific constraints (valid hierarchical levels, no skipping levels), requiring understanding of how to control generation
  - Quick check question: How do you enforce constraints on sequence generation to ensure valid output?

- Concept: Context window management in transformer models
  - Why needed here: The global context stack and multi-segment strategy must fit within the model's context window while providing sufficient information
  - Quick check question: How do you optimize information density when working with limited context window sizes?

## Architecture Onboarding

- Component map: Text segments → Global Context Stack → Action Generation Model → Actions → Structure Update → Global Context Stack Update

- Critical path: Text segments → Action Generation Model → Actions → Structure Update → Global Context Stack Update

- Design tradeoffs:
  - Stack size vs. context richness: Larger stacks provide more context but risk exceeding token limits
  - Window size (wI, wO) vs. efficiency: Larger windows improve performance but increase computation
  - Action granularity vs. model complexity: More action types allow finer control but require more training data

- Failure signatures:
  - Invalid action sequences (skipping levels, wrong action types) indicate constraint enforcement issues
  - Degraded performance on long documents suggests insufficient global context
  - Slow inference indicates suboptimal window sizing

- First 3 experiments:
  1. Test basic action generation with wI=wO=1 on small documents to verify constraint enforcement
  2. Scale up to wI=wO=3 and measure performance improvement vs. single-segment processing
  3. Test document length impact by running on subsets of increasing document size to identify scaling limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed SEG2ACT method compare in performance to other state-of-the-art document logical structuring methods that utilize visual information?
- Basis in paper: [inferred] The paper mentions that SEG2ACT does not utilize visual information, and that it is challenging to handle sequences with disrupted text segment order without proper input text segment ordering.
- Why unresolved: The paper does not provide a direct comparison between SEG2ACT and methods that utilize visual information, such as multi-modal methods.
- What evidence would resolve it: Conducting experiments comparing SEG2ACT to state-of-the-art methods that utilize visual information, such as multi-modal methods, on the same datasets would provide evidence of the relative performance of SEG2ACT.

### Open Question 2
- Question: How does the performance of SEG2ACT change when using different backbone models, such as other large language models or smaller models?
- Basis in paper: [explicit] The paper mentions that the choice of backbone model affects performance, and that the Qwen1.5-4B outperforms the Baichuan-7B in F1-score and document-level accuracy while being smaller in model size.
- Why unresolved: The paper only provides results for two specific backbone models, and does not explore the impact of using different backbone models on the performance of SEG2ACT.
- What evidence would resolve it: Conducting experiments using different backbone models, such as other large language models or smaller models, and comparing their performance on the same datasets would provide evidence of the impact of backbone model choice on SEG2ACT's performance.

### Open Question 3
- Question: How does the performance of SEG2ACT change when using different values for the input segment window size (wI) and output action window size (wO)?
- Basis in paper: [explicit] The paper mentions that the multi-segment multi-action strategy uses wI and wO to control the input segment window size and output action window size, respectively, and that experiments are conducted with different values of wI and wO.
- Why unresolved: The paper only provides results for a limited range of wI and wO values, and does not explore the impact of using different values on the performance of SEG2ACT.
- What evidence would resolve it: Conducting experiments using different values of wI and wO, and comparing their performance on the same datasets would provide evidence of the impact of these parameters on SEG2ACT's performance.

## Limitations

- Dataset size limitation: The ChCatExt and HierDoc datasets contain only 650 documents each, which may limit generalizability to real-world applications
- Constraint engineering complexity: The action generation approach requires careful constraint engineering to prevent invalid hierarchical structures
- Limited document type diversity: Experiments focus on scientific and structured documents, making it difficult to assess true generalization capability across diverse document types

## Confidence

- **High confidence**: The core mechanism of using a global context stack to maintain hierarchical information during action generation
- **Medium confidence**: The multi-segment multi-action strategy's effectiveness and optimal window sizing
- **Low confidence**: The generalization claim across diverse document types beyond the tested scientific and structured documents

## Next Checks

1. **Scaling Test**: Evaluate SEG2ACT on documents with varying lengths (short, medium, long) to identify the breaking point where global context stack becomes insufficient or token limits are exceeded

2. **Cross-Dataset Transfer**: Test the transfer learning capability by pre-training on ChCatExt and evaluating on HierDoc (or vice versa) to validate the claimed generalization ability beyond the original domain

3. **Constraint Robustness**: Systematically test constraint enforcement by attempting to generate invalid action sequences and measuring the model's ability to self-correct or reject these attempts