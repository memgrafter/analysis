---
ver: rpa2
title: Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded
  QA Conversations
arxiv_id: '2402.11770'
source_url: https://arxiv.org/abs/2402.11770
tags:
- user
- data
- agent
- document
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a structured chain-of-thought (SCoT) prompting
  approach for generating content-grounded multi-turn QA conversations. The method
  decomposes the complex task into a state machine with dedicated states for user
  utterance generation, answerability classification, answer sentence selection, and
  agent response generation.
---

# Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations

## Quick Facts
- **arXiv ID**: 2402.11770
- **Source URL**: https://arxiv.org/abs/2402.11770
- **Reference count**: 10
- **Primary result**: Structured chain-of-thought prompting with hallucination mitigation states improves agent faithfulness to grounding documents by up to 16.8%.

## Executive Summary
This paper introduces Structured Chain-of-Thought (SCoT) prompting, a novel approach for generating content-grounded multi-turn QA conversations using large language models. The method decomposes the complex task into a state machine with dedicated states for user utterance generation, answerability classification, answer sentence selection, and agent response generation. Each state uses specific prompts and optionally supporting models to optimize performance for that particular function. The approach demonstrates significant improvements in reducing hallucination while maintaining high-quality conversational generation.

The key innovation is the structured decomposition of the generation process, which allows for targeted mitigation of hallucination through answerability classification and answer sentence selection states. When used as training data, conversations synthesized from only 6 Wikipedia-based seed demonstrations train strong conversational QA agents, with out-of-domain evaluation showing improvements of up to 13.9% over target domain gold data when augmented with generated examples.

## Method Summary
The method uses a state machine architecture with four primary states: user utterance generation (uu), answerability classification (ac), answer sentence selection (ss), and agent response generation (au). The system can follow different state transition paths depending on the desired level of hallucination control. The simplest path (uu→au) directly generates both user questions and agent responses, while more complex paths incorporate answerability classification and sentence selection to ensure responses are grounded in the document. The approach uses open-source LLMs (FALCON-40B for generation and FLAN-UL2-20B for classification/selection) and can operate in few-shot in-context learning mode or as synthetic training data for fine-tuning conversational QA agents.

## Key Results
- SCoT prompting with designated hallucination mitigation states increases agent faithfulness to grounding documents by up to 16.8%
- Synthetic conversations generated from only 6 Wikipedia-based seed demonstrations train strong conversational QA agents
- Out-of-domain evaluation shows improvements of up to 13.9% when target domain gold data is augmented with generated examples
- The best-performing algorithms (those with answerability classification and answer sentence selection states) correctly identify more unanswerable questions and maintain higher lexical precision in responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured breakdown of complex task into dedicated states improves LLM performance on multi-turn QA generation.
- Mechanism: Decomposing the overall task into four distinct states allows each subtask to leverage specific resources and prompts optimized for that particular function.
- Core assumption: Pre-trained LLMs benefit from task-specific prompts and can be guided more effectively when each reasoning step has its own dedicated state and resources.
- Evidence anchors: [abstract] "At the core of our proposal is a structured breakdown of the complex task into a number of states in a state machine..." [section 2] "Our proposed algorithms prompt specific sets of state transitions..."

### Mechanism 2
- Claim: Answerability classification and answer sentence selection states reduce hallucination by preventing LLM from generating unsupported answers.
- Mechanism: By explicitly classifying whether a question is answerable before generating a response, and by selecting only relevant sentences from the document for response generation, the system prevents the LLM from fabricating information when the document doesn't contain an answer.
- Core assumption: Pre-trained LLMs have significant hallucination tendencies, especially when asked questions without clear answers in the grounding document.
- Evidence anchors: [abstract] "Our experimental results show that SCoT prompting with designated states for hallucination mitigation increases agent faithfulness..." [section 4.1] "algorithms that transit through states ac and ss... deem more questions as unanswerable"

### Mechanism 3
- Claim: Generated synthetic data can effectively train strong conversational QA agents and augment target domain gold data.
- Mechanism: The synthetic conversations generated through SCoT prompting capture the structure and requirements of multi-turn QA while being grounded in documents, making them valuable training data that can either stand alone or enhance existing datasets.
- Core assumption: High-quality synthetic data can approximate or even exceed the utility of human-labeled data for training conversational QA agents.
- Evidence anchors: [abstract] "When used as training data, our open-domain conversations synthesized from only 6 Wikipedia-based seed demonstrations train strong conversational QA agents..." [section 4.2.3] "augmenting with our open-domain synthetic data improves agent performance..."

## Foundational Learning

- **Concept**: Chain-of-Thought (CoT) prompting
  - Why needed here: Provides the conceptual foundation for breaking down complex reasoning into intermediate steps, which SCoT extends by distributing these steps across dedicated states
  - Quick check question: How does standard CoT prompting differ from the structured approach proposed in this work?

- **Concept**: Few-shot in-context learning
  - Why needed here: The method relies on providing exemplars within prompts to guide the LLM's behavior without requiring parameter updates
  - Quick check question: What is the maximum number of exemplars that can be practically included in a prompt given context length limitations?

- **Concept**: Hallucination in LLMs
  - Why needed here: Understanding this problem is crucial for appreciating why the answerability classification and sentence selection states are necessary
  - Quick check question: What distinguishes closed-domain hallucination from other types of LLM hallucinations?

## Architecture Onboarding

- **Component map**: uu → ac → ss → au (with FLAN-UL2-20B assistant)
- **Critical path**: uu→ac→ss→au for generating high-quality conversations, followed by fine-tuning on the synthetic data
- **Design tradeoffs**:
  - Complexity vs. performance: More states provide better hallucination control but increase system complexity
  - Open-source vs. proprietary models: Using only open-source models ensures reproducibility but may limit performance
  - Synthetic data quantity vs. quality: Generating more data increases training utility but may introduce more noise
- **Failure signatures**:
  - If answerability classification fails: Agent will hallucinate answers to unanswerable questions
  - If sentence selection fails: Agent responses will be less grounded or will miss relevant information
  - If state transitions are incorrect: The conversation flow will be incoherent
- **First 3 experiments**:
  1. Generate conversations using the simplest uu→au transition and measure hallucination rates compared to more complex approaches
  2. Evaluate the impact of FLAN-UL2-20B assistant on answerability classification accuracy
  3. Train a conversational QA agent on synthetic data and test performance on an out-of-domain dataset like QuAC

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between answerable and unanswerable questions in synthetic training data for conversational QA?
- Basis in paper: [explicit] The paper discusses the trade-off between performance on answerable and unanswerable questions, noting that algorithms with better answerability classification achieve better overall performance. It also mentions that the optimal mixture of different synthetic datasets is 50% from each.
- Why unresolved: While the paper identifies that a 50-50 mixture of different synthetic datasets performs well, it does not explore whether this balance is optimal across different domains or model architectures.
- What evidence would resolve it: Experiments varying the ratio of answerable to unanswerable questions in training data across multiple domains and evaluating performance on both classes would determine the optimal balance.

### Open Question 2
- Question: How does the quality of synthetic data scale with the number of seed demonstrations used for generation?
- Basis in paper: [inferred] The paper uses only 6 seed demonstrations to generate high-quality synthetic data, but notes that using more demonstrations could make the data more diverse and train even better models.
- Why unresolved: The paper does not empirically test how performance scales with increasing numbers of seed demonstrations, only speculating that more would be beneficial.
- What evidence would resolve it: Experiments generating synthetic data using varying numbers of seed demonstrations (e.g., 6, 12, 24, 48) and measuring downstream task performance would show the scaling relationship.

### Open Question 3
- Question: What is the relative importance of different components in the structured chain-of-thought prompting approach?
- Basis in paper: [explicit] The paper compares five different algorithms with varying state transition sequences and assistants, showing that those with answerability classification and answer sentence selection perform better.
- Why unresolved: While the paper identifies which complete algorithms perform best, it does not systematically ablate individual components to determine their relative contributions to performance.
- What evidence would resolve it: Ablation studies where individual components (answerability classification, answer sentence selection, different assistants) are removed one at a time from the best-performing algorithms would quantify their individual contributions.

## Limitations

- **Prompt Template Specificity**: The paper references specific prompt templates for each state but does not provide complete examples, creating uncertainty about how exactly the states are defined.
- **Model Dependency**: The approach relies on specific open-source models (FALCON-40B and FLAN-UL2-20B), and performance may vary significantly with different LLMs.
- **Evaluation Scope**: While effectiveness is demonstrated on Wikipedia-based open-domain QA and out-of-domain evaluation, generalizability to other domains remains uncertain.

## Confidence

**High Confidence**: The core claim that structured decomposition improves hallucination mitigation is well-supported by experimental results showing 16.8% improvement in faithfulness metrics.

**Medium Confidence**: The effectiveness of synthetic data for training strong conversational QA agents is supported but limited to specific experimental conditions (6 seed demonstrations, particular datasets).

**Medium Confidence**: The out-of-domain generalization improvements (up to 13.9%) are demonstrated but on a limited set of target domains.

## Next Checks

1. **Prompt Template Validation**: Implement the SCoT prompting approach using alternative prompt templates for the same states to determine whether performance is robust to prompt variations or critically dependent on specific prompt engineering.

2. **Cross-Model Generalization**: Evaluate the SCoT approach with different combinations of LLMs (e.g., GPT-4 for generation, smaller models for classification) to assess whether the state machine structure itself or the specific model choices drive the performance gains.

3. **Data Quality Threshold Analysis**: Systematically vary the quality of synthetic data generation (e.g., by relaxing answerability classification thresholds) and measure the corresponding impact on downstream agent training performance to establish minimum quality requirements.