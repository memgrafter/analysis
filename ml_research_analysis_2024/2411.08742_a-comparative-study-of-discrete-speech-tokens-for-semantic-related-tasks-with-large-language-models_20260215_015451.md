---
ver: rpa2
title: A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with
  Large Language Models
arxiv_id: '2411.08742'
source_url: https://arxiv.org/abs/2411.08742
tags:
- speech
- discrete
- tokens
- arxiv
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a comprehensive comparison between discrete
  and continuous speech features in Large Language Models (LLMs) across semantic-related
  tasks. The authors evaluate both approaches using the Qwen1.5-0.5B model on multiple
  tasks including automatic speech recognition, phoneme recognition, speech translation,
  intent classification, keyword spotting, and emotion recognition.
---

# A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models

## Quick Facts
- arXiv ID: 2411.08742
- Source URL: https://arxiv.org/abs/2411.08742
- Reference count: 36
- Primary result: Continuous features outperform discrete tokens in semantic tasks, but discrete tokens offer 21-27% training time reduction

## Executive Summary
This study presents a comprehensive comparison between discrete and continuous speech features in Large Language Models (LLMs) across semantic-related tasks. The authors evaluate both approaches using the Qwen1.5-0.5B model on multiple tasks including automatic speech recognition, phoneme recognition, speech translation, intent classification, keyword spotting, and emotion recognition. Their findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. The study identifies key factors behind this performance gap, including limited token granularity and inefficient information retention in discrete tokens. The authors also demonstrate that discrete tokens offer significant training efficiency advantages, requiring only 21-27% of the training time compared to continuous features, while achieving comparable results in some tasks. Through detailed analysis of token distribution, bitrate effects, and robustness limitations, the paper provides insights into potential improvements for discrete token performance.

## Method Summary
The study compares discrete and continuous speech features using SSL models (HuBERT-Large, WavLM-Large) to extract representations, then processes them through different pipelines before feeding into LLMs (Qwen1.5-0.5B, LLaMA3.1-8B). Discrete tokens are generated via K-means clustering with 2000 centroids, de-duplication, and BPE with 6000 vocabulary size. Continuous features use downsampling rate of 2 and linear adapter. Both are fine-tuned using instruction-tuning with learning rate 1e-5, batch size 32, and AdamW optimizer with weight decay 1e-2.

## Key Results
- Continuous features outperform discrete tokens in tasks requiring fine-grained semantic understanding
- Discrete tokens require only 21-27% of training time compared to continuous features
- Token distribution imbalance (20% of tokens account for 95% of usage) leads to inefficient representation in discrete tokens
- Discrete tokens achieve comparable results to continuous features in some tasks while offering significant efficiency advantages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous features outperform discrete tokens in tasks requiring fine-grained semantic understanding because they preserve more detailed acoustic information.
- Mechanism: Continuous features retain higher-dimensional, detailed speech representations that capture subtle acoustic nuances, while discrete tokens compress this information through clustering, losing fine-grained details.
- Core assumption: The richness of continuous representations directly translates to better semantic understanding in downstream tasks.
- Evidence anchors:
  - [abstract]: "continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding"
  - [section]: "continuous embeddings provide a more even and detailed representation, avoiding sparse token utilization and capturing subtle linguistic and acoustic features more effectively"
  - [corpus]: Weak evidence - related papers discuss comparative analysis but don't provide specific performance data on fine-grained semantic tasks.
- Break condition: If discrete tokens can achieve similar bitrate levels through improved clustering methods or if continuous features become too computationally expensive to process efficiently.

### Mechanism 2
- Claim: Discrete tokens offer significant training efficiency advantages by requiring only 21-27% of the training time compared to continuous features.
- Mechanism: The compact representation of discrete tokens reduces computational complexity during training, allowing models to converge faster with fewer epochs.
- Core assumption: The compression achieved by discrete tokens doesn't significantly impact the model's ability to learn task-specific patterns.
- Evidence anchors:
  - [abstract]: "discrete tokens offer significant training efficiency advantages, requiring only 21-27% of the training time compared to continuous features"
  - [section]: "discrete tokens converge within 4∼5 epochs, while continuous features require 14∼15 epochs. This leads to a significant reduction in total training time for discrete tokens, ranging from approximately 21% to 27% of the time required for continuous features"
  - [corpus]: Weak evidence - related papers mention efficiency benefits but don't provide specific training time comparisons.
- Break condition: If the performance gap between discrete and continuous features becomes too large to justify the efficiency gains, or if model size increases make the training time difference negligible.

### Mechanism 3
- Claim: The performance gap between discrete and continuous features is influenced by discrete token distribution imbalance and limited bitrate.
- Mechanism: Discrete tokens suffer from codebook underutilization (only 20% of tokens account for 95% of usage) and lower bitrate compared to continuous features, leading to information loss and reduced performance.
- Core assumption: More uniform token distribution and higher bitrate would improve discrete token performance.
- Evidence anchors:
  - [section]: "Around 20% of distinct tokens account for only 5% of total occurrences... much of the codebook is underutilized, leading to inefficient representation"
  - [section]: "The bitrate R, defined as R = log 2 V · C · Rs... determines how well the codebook size retains information. Smaller codebooks result in lower bitrates, which may lose subtle acoustic nuances"
  - [corpus]: Weak evidence - related papers discuss token distribution but don't provide specific analysis of bitrate effects on performance.
- Break condition: If token distribution becomes more balanced through improved clustering techniques, or if continuous features become more efficient and reduce the bitrate advantage of discrete tokens.

## Foundational Learning

- Concept: Self-supervised learning (SSL) models
  - Why needed here: SSL models like HuBERT and WavLM are used to generate both continuous features and discrete tokens for speech processing tasks
  - Quick check question: How do SSL models differ from supervised models in speech representation learning?

- Concept: Vector quantization and k-means clustering
  - Why needed here: K-means clustering is used to convert continuous speech embeddings into discrete tokens by grouping similar feature vectors
  - Quick check question: What factors influence the quality of discrete tokens generated through k-means clustering?

- Concept: Byte-Pair Encoding (BPE)
  - Why needed here: BPE is applied to discrete tokens to reduce sequence length by merging frequent subsequences into shorter meta tokens
  - Quick check question: How does BPE improve the efficiency of discrete token processing in LLMs?

## Architecture Onboarding

- Component map:
  - SSL speech encoders (HuBERT-Large, WavLM-Large) → Feature extraction
  - K-means clustering → Discrete token generation
  - Downsampler + Linear adapter → Continuous feature processing
  - De-duplication → Remove redundant consecutive tokens
  - BPE subword modelling → Token sequence compression
  - Qwen1.5-0.5B or LLaMA3.1-8B → LLM decoder
  - Instruction-tuning pipeline → Task adaptation

- Critical path: SSL encoder → Feature processing (discrete/continuous) → LLM decoder → Task-specific output
- Design tradeoffs:
  - Discrete tokens: Higher efficiency, lower computational cost, potential information loss
  - Continuous features: Better performance, higher computational cost, requires more resources
  - Model size vs. performance: Larger LLMs improve discrete token performance but increase computational requirements
- Failure signatures:
  - Discrete tokens: Poor performance on emotion recognition, imbalanced token distribution, low bitrate
  - Continuous features: High computational cost, slower training convergence, memory limitations
  - Both: Poor task adaptation, inadequate instruction-following capability
- First 3 experiments:
  1. Benchmark discrete vs continuous features on ASR task with LibriSpeech dataset
  2. Test different k-means cluster sizes (1000, 2000, 3000) and BPE vocabulary sizes (4000, 6000, 8000)
  3. Compare training efficiency by measuring convergence time across different datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal K-means clustering granularity and BPE vocabulary size that balances performance and computational efficiency across all semantic-related tasks?
- Basis in paper: [explicit] The paper mentions that the combination of k = 2000 centroids and 6000 BPE vocabulary size achieved a balanced trade-off between performance gains and computational efficiency in ASR experiments.
- Why unresolved: While this combination worked well for ASR, the paper suggests that optimal settings might vary across different tasks, and a comprehensive study across all semantic-related tasks is needed to determine the universal optimal settings.
- What evidence would resolve it: A systematic evaluation of various K-means clustering granularities and BPE vocabulary sizes across all semantic-related tasks (ASR, PR, ST, IC, KS, ER) to identify the settings that consistently provide the best performance-efficiency trade-off.

### Open Question 2
- Question: How does the performance of discrete tokens scale with the size of the Large Language Model (LLM) decoder?
- Basis in paper: [explicit] The paper states that experiments with larger LLM (LLaMA3.1-8B model) revealed that discrete tokens show notable improvement when integrated into larger decoders, particularly in ASR tasks.
- Why unresolved: The paper only compared discrete tokens with two specific LLM sizes (Qwen1.5-0.5B and LLaMA3.1-8B). It remains unclear how performance scales across a broader range of LLM sizes and whether there's an optimal LLM size for discrete token performance.
- What evidence would resolve it: A comprehensive study evaluating discrete token performance across multiple LLM sizes, potentially revealing an optimal scale or a consistent performance trend as LLM size increases.

### Open Question 3
- Question: What specific architectural improvements to the speech tokenizer could enhance the robustness of discrete tokens in handling complex tasks with noisy data and varied pronunciations?
- Basis in paper: [explicit] The paper identifies robustness limitations of semantic discrete tokens in handling the challenges of the SLURP dataset, which includes noisy data, varied pronunciations, and a mix of close and distant microphone recordings.
- Why unresolved: While the paper demonstrates that pretraining with SLURP transcriptions improves performance, it doesn't explore specific architectural modifications to the speech tokenizer that could inherently improve robustness to these challenges.
- What evidence would resolve it: Experimental results comparing different speech tokenizer architectures (e.g., modified clustering algorithms, attention mechanisms, or multi-stage processing) on complex tasks with noisy data and varied pronunciations, demonstrating which architectural changes lead to improved robustness.

## Limitations
- Focus on single LLM architecture (Qwen1.5-0.5B, LLaMA3.1-8B) limits generalizability
- Limited analysis of SSL model variations beyond HuBERT and WavLM
- Unknown implementation details of de-duplication step and specific prompt templates

## Confidence
**High Confidence**: Continuous features generally outperform discrete tokens in semantic tasks (supported by experimental results across multiple datasets and tasks)

**Medium Confidence**: Performance gaps stem from limited token granularity and inefficient information retention (reasonable but requires additional validation)

**Low Confidence**: Specific performance thresholds where discrete tokens become competitive with continuous features are not well-established

## Next Checks
1. Cross-Architecture Validation: Test discrete vs. continuous comparison using different LLM architectures (e.g., Whisper, GPT-4o-mini)
2. Real-Time Processing Analysis: Evaluate training efficiency vs. inference latency trade-offs in real-time applications
3. SSL Model Ablation Study: Compare different SSL models (Wav2Vec2, APC, CPC) in generating both discrete and continuous features