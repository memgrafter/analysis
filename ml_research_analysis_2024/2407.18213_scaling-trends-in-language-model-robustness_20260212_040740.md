---
ver: rpa2
title: Scaling Trends in Language Model Robustness
arxiv_id: '2407.18213'
source_url: https://arxiv.org/abs/2407.18213
tags:
- attack
- compute
- rate
- success
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a large-scale empirical study of adversarial
  robustness in language models across multiple model families, tasks, and attacks.
  The authors find that larger models without safety training do not consistently
  achieve better robustness, but they do become more sample-efficient during adversarial
  training.
---

# Scaling Trends in Language Model Robustness

## Quick Facts
- **arXiv ID**: 2407.18213
- **Source URL**: https://arxiv.org/abs/2407.18213
- **Reference count**: 40
- **Primary result**: Larger language models without safety training don't consistently achieve better adversarial robustness, but scale improves sample efficiency during adversarial training while attack scaling currently outpaces defensive measures.

## Executive Summary
This paper presents a comprehensive empirical study of adversarial robustness scaling in language models across multiple model families, tasks, and attack methods. The authors find that while larger models without safety training don't consistently achieve better robustness, they do become more sample-efficient during adversarial training - requiring fewer examples to reach similar robustness levels despite higher per-example costs. Attack success rates improve smoothly with increased attack compute against both undefended and adversarially trained models, and the study reveals that larger adversarially trained models show trends suggesting potential long-term defensive advantages. These findings highlight the importance of scaling analysis for evaluating future attacks and defenses on frontier language models.

## Method Summary
The study uses Pythia models (7.6M to 12B parameters) and Qwen2.5 models (0.5B to 14B parameters) across 6 classification tasks and one generation task. Models are finetuned on classification tasks with linear learning rate decay, then subjected to adversarial training using attacked examples added to the training pool with exponential sampling. Three attacks are evaluated: RandomToken (1280 iterations), GCG (10-20 iterations depending on task), and BEAST (25 iterations). The primary metric is attack success rate - the proportion of correctly classified examples that become misclassified after attack. Scaling relationships are analyzed by varying model size, attack compute, and number of adversarial training rounds.

## Key Results
- Larger models without safety training do not consistently achieve better adversarial robustness but become more sample-efficient during adversarial training
- Increasing attack compute smoothly improves attack success rate against both undefended and adversarially trained models
- Attack scaling currently outpaces adversarial training across all model sizes studied
- Larger adversarially trained models show trends suggesting potential long-term defensive advantages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Larger models without safety training do not consistently achieve better robustness, but scale improves sample efficiency in adversarial training.
- **Mechanism**: Scale affects robustness through sample efficiency rather than absolute robustness gains. Larger models learn more from each adversarial example, requiring fewer examples to reach similar robustness levels, though they need more total compute due to higher per-example costs.
- **Core assumption**: The relationship between model size and sample efficiency is monotonic and consistent across different tasks and attack types.
- **Evidence anchors**:
  - [abstract] "scale improves sample efficiency in adversarial training, though it worsens compute efficiency"
  - [section 5] "Larger models are more sample efficient but less compute efficient than smaller models, needing fewer adversarial training rounds, but more FLOPs, to reach the same robustness level"
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism
- **Break condition**: If the monotonic relationship between size and sample efficiency breaks down for specific tasks or attack types, or if larger models become less sample-efficient at extreme scales.

### Mechanism 2
- **Claim**: Increasing attack compute smoothly improves attack success rate against both undefended and adversarially trained models.
- **Mechanism**: Attack strength scales smoothly with compute investment. More compute allows for more attack iterations, better gradient optimization, or more extensive search, leading to higher success rates that follow predictable scaling laws.
- **Core assumption**: The relationship between attack compute and success rate is smooth and predictable, following power-law relationships.
- **Evidence anchors**:
  - [abstract] "increasing attack compute smoothly improves attack success rate against both undefended and adversarially trained models"
  - [section 4] "we find that attack success rate improves smoothly against both undefended and adversarially trained models as a function of attack compute spent"
  - [corpus] Weak - limited corpus evidence specifically addressing this smooth scaling relationship
- **Break condition**: If attack success rate plateaus or becomes non-monotonic with additional compute, or if new attack strategies fundamentally change the scaling relationship.

### Mechanism 3
- **Claim**: Larger adversarially trained models might give defense the advantage in the long run as attack scaling outpaces adversarial training across all models studied.
- **Mechanism**: The offense-defense balance shifts with scale. While attackers currently have the advantage for any given model size, larger models push the robustness frontier up and to the left, potentially reversing the advantage if scaling continues.
- **Core assumption**: The trend of larger models having better scaling properties (smaller slope in attack success vs compute) continues as models grow.
- **Evidence anchors**:
  - [abstract] "larger adversarially trained models might give defense the advantage in the long run"
  - [section 6] "As model size increases, the attack advantage decreases (scaling curves move up and to the left)"
  - [corpus] Moderate - some related work on scaling laws in adversarial robustness
- **Break condition**: If the trend of improved scaling for larger models reverses, or if new attack strategies disproportionately benefit from larger model capabilities.

## Foundational Learning

- **Concept**: Scaling laws in machine learning
  - **Why needed here**: Understanding how performance metrics scale with model size and compute is fundamental to interpreting the paper's results and methodology
  - **Quick check question**: What is the general form of scaling laws for language models, and how do they typically relate performance to model size and training compute?

- **Concept**: Adversarial attacks and defenses
  - **Why needed here**: The paper studies robustness through adversarial attacks, requiring understanding of attack methodologies, defense strategies, and evaluation metrics
  - **Quick check question**: What distinguishes white-box from black-box attacks, and how does this distinction affect the study's choice of attack methods?

- **Concept**: Sample efficiency vs compute efficiency
  - **Why needed here**: The paper distinguishes between these two efficiency metrics when discussing the benefits of scale, which is crucial for interpreting the results
  - **Quick check question**: How can a model be more sample-efficient but less compute-efficient, and what does this mean for practical deployment?

## Architecture Onboarding

- **Component map**: Data preprocessing pipelines → Model training infrastructure (finetuning and adversarial training) → Attack implementation modules (GCG, RandomToken, BEAST) → Evaluation frameworks → Visualization/plotting utilities
- **Critical path**: Load/preprocess dataset → Finetune base model → Apply adversarial training (if applicable) → Evaluate robustness under various attacks → Analyze scaling trends
- **Design tradeoffs**: Classification tasks enable fair comparison across model sizes but limit generalizability; publicly available models provide transparency but may not reflect state-of-the-art; multiple attack types balance comprehensiveness with computational feasibility
- **Failure signatures**: Inconsistent results across random seeds indicate instability; poor transfer of adversarial training suggests overfitting; unexpected scaling trends may indicate bugs in compute estimation
- **First 3 experiments**:
  1. Finetune classification on small Pythia model (7.6M) on Spam task to verify basic training pipeline
  2. Apply single round of adversarial training using GCG and verify attack success rate decreases
  3. Run RandomToken attack with varying iteration counts on both undefended and adversarially trained models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would adversarial training efficiency scale with more sophisticated attack methods like latent-space attacks compared to GCG?
- Basis in paper: [explicit] The paper notes that GCG is less compute-efficient than latent-space methods and suggests this could change offense-defense slopes
- Why unresolved: The study only used GCG for adversarial training, not comparing it with more efficient methods
- What evidence would resolve it: Empirical comparison of adversarial training scaling curves using both GCG and latent-space attack methods against the same model families

### Open Question 2
- Question: Does task complexity affect the scaling relationship between model size and robustness against adversarial attacks?
- Basis in paper: [inferred] The authors note that task complexity could affect robustness and mention recent work showing longer-context models are more susceptible to attack
- Why unresolved: The study used relatively simple classification tasks and one generation task, without systematically varying task complexity
- What evidence would resolve it: Experiments scaling model size across tasks of varying complexity (simple algorithmic tasks vs. complex reasoning tasks) while measuring attack success rates

### Open Question 3
- Question: Would combining multiple defense layers (e.g., adversarial training + perplexity filtering + circuit-breakers) create multiplicative rather than additive improvements in robustness?
- Basis in paper: [explicit] The authors suggest combining multiple defenses would be valuable but didn't test this approach
- Why unresolved: Only adversarial training was studied as a defense mechanism
- What evidence would resolve it: Scaling analysis of robustness when implementing multiple defense mechanisms simultaneously across model families

## Limitations
- Focus on classification tasks limits generalizability to real-world deployment scenarios where robustness is critical
- Computational costs of scaling both attacks and defenses may create practical barriers not captured in theoretical analysis
- The relationship between attack scaling and defense scaling remains unclear, particularly for open-ended generation tasks

## Confidence
- **High Confidence**: Empirical findings about sample efficiency improvements in larger models during adversarial training are well-supported by data across multiple tasks and model families
- **Medium Confidence**: Prediction that larger adversarially trained models might eventually give defense the advantage relies on extrapolating current trends
- **Low Confidence**: Generalizability of findings to non-classification tasks and open-ended generation has not been established

## Next Checks
1. **Cross-task validation**: Replicate key scaling experiments on a generative task (like StrongREJECT) to verify sample efficiency and scaling relationships hold beyond classification
2. **Attack diversity test**: Evaluate transfer of adversarial training across different attack families to determine if models learn general robustness or overfit to specific patterns
3. **Cost-benefit analysis**: Quantify compute efficiency trade-off between smaller and larger models at equivalent robustness levels to understand practical implications of sample efficiency gains