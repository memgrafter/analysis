---
ver: rpa2
title: 'Structure Your Data: Towards Semantic Graph Counterfactuals'
arxiv_id: '2403.06514'
source_url: https://arxiv.org/abs/2403.06514
tags:
- graph
- edits
- graphs
- semantic
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces semantic graph counterfactuals to improve
  the quality and interpretability of post-hoc explanations for black-box classifiers.
  Instead of representing concepts as flat sets, the method structures semantic information
  as graphs capturing both objects and their relationships, then leverages Graph Neural
  Networks to approximate Graph Edit Distance efficiently.
---

# Structure Your Data: Towards Semantic Graph Counterfactuals

## Quick Facts
- **arXiv ID:** 2403.06514
- **Source URL:** https://arxiv.org/abs/2403.06514
- **Reference count:** 39
- **Primary result:** Introduces semantic graph counterfactuals that outperform flat concept methods on visual and audio datasets, with higher human preference and interpretability

## Executive Summary
This work introduces semantic graph counterfactuals to improve the quality and interpretability of post-hoc explanations for black-box classifiers. Instead of representing concepts as flat sets, the method structures semantic information as graphs capturing both objects and their relationships, then leverages Graph Neural Networks to approximate Graph Edit Distance efficiently. Experiments on visual and audio datasets show that this approach produces counterfactuals with fewer edits, closer alignment to ground-truth semantic similarity, and higher human preference than state-of-the-art pixel-level and conceptual methods. Human studies confirm that the explanations are more understandable and actionable, especially in cases with intricate object interactions. The method is model-agnostic and easily extensible to unannotated or multimodal data.

## Method Summary
The method represents images as scene graphs capturing objects and their relationships, then uses Graph Neural Networks (GNNs) to approximate Graph Edit Distance (GED) efficiently for counterfactual retrieval. A Siamese GNN architecture learns graph embeddings where cosine similarity approximates graph similarity, requiring exact GED computation only on top candidates. The approach is evaluated on visual datasets (CUB, Visual Genome) and audio data (Smarty4covid), demonstrating superior performance over pixel-level and flat concept methods. The method handles unannotated datasets through automatic scene graph generation and shows high human preference in interpretability studies.

## Key Results
- Semantic graph counterfactuals achieve higher human preference (48-94% preference over baselines) and better interpretability than flat concept and pixel-level methods
- The GNN-based GED approximation enables efficient counterfactual retrieval while maintaining accuracy, with P@1 scores exceeding 0.19 on CUB and 0.21 on VG-RANDOM
- Human studies show participants can accurately learn classification tasks from graph-based explanations, with 89% accuracy in blind learning experiments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using semantic graphs captures both objects and their relationships, leading to more accurate counterfactual explanations than flat concept sets.
- **Mechanism:** Graph structure encodes explicit relationships between objects, preventing errors like treating a single pedestrian and a group of cyclists as equivalent edits.
- **Core assumption:** Graph representations can be efficiently compared using GNN-based embeddings, bypassing the NP-hard graph edit distance computation for all pairs.
- **Evidence anchors:**
  - [abstract] "instead of representing concepts as flat sets, the method structures semantic information as graphs capturing both objects and their relationships"
  - [section] "By further linking semantics with external knowledge, we constrain edits to establish that concepts such as 'man' and 'woman' are more closely related than concepts like 'man' and 'helmet'"
- **Break condition:** If graph generation produces poor quality scene graphs or misses critical relationships, the semantic advantage disappears and edits become inaccurate.

### Mechanism 2
- **Claim:** GNN embeddings trained on graph proximity can approximate GED efficiently, enabling scalable counterfactual retrieval.
- **Mechanism:** Siamese GNN architecture learns to embed graphs into a shared space where cosine similarity approximates graph similarity, requiring GED computation only for the top candidate.
- **Core assumption:** Training on N/2 random graph pairs provides sufficient signal for the GNN to learn meaningful graph representations that preserve semantic similarity.
- **Evidence anchors:**
  - [abstract] "leverages Graph Neural Networks to approximate Graph Edit Distance efficiently"
  - [section] "The model is trained transductively to minimize the loss function L" with equation showing proximity-based loss
- **Break condition:** If the training data is too sparse or unrepresentative, the GNN embeddings won't capture true graph similarity and retrieval quality degrades.

### Mechanism 3
- **Claim:** Counterfactuals based on minimal graph edits correspond to human-interpretable and actionable explanations.
- **Mechanism:** By selecting counterfactuals from the actual target class distribution (not synthetic edits), the method ensures explanations are both accurate and achievable.
- **Core assumption:** Humans can understand and learn from graph-based explanations (edges showing relationships) even without seeing the original images.
- **Evidence anchors:**
  - [abstract] "human studies confirm that the explanations are more understandable and actionable, especially in cases with intricate object interactions"
  - [section] "Our survey revealed that participants preferred our CEs in the majority of instances, and they were also successful in learning to accurately classify images themselves"
- **Break condition:** If the semantic annotations are incomplete or the relationships don't capture human reasoning patterns, the explanations may become confusing rather than clarifying.

## Foundational Learning

- **Concept: Graph Edit Distance (GED)**
  - Why needed here: GED provides the gold standard for measuring graph similarity, which this method approximates using GNNs
  - Quick check question: What makes GED NP-hard to compute exactly, and why does this motivate using GNN approximations?

- **Concept: Scene Graphs**
  - Why needed here: Scene graphs represent images as structured semantic graphs capturing objects and their relationships, forming the basis for this method
  - Quick check question: How does representing an image as a scene graph differ from representing it as a set of concepts?

- **Concept: Siamese Networks**
  - Why needed here: Siamese GNN architecture learns to embed graphs into a shared space where proximity indicates similarity
  - Quick check question: Why is a siamese architecture particularly suited for learning graph similarity metrics?

## Architecture Onboarding

- **Component map:** Scene graph generator → Siamese GNN model → Graph embedding space → Cosine similarity ranking → GED verification → Counterfactual output

- **Critical path:** Graph → GNN embedding → Cosine similarity ranking → Top-1 candidate → GED verification → Counterfactual output

- **Design tradeoffs:**
  - Exact GED vs approximate GNN embeddings (accuracy vs efficiency)
  - Scene graph quality vs availability of annotations
  - Model complexity (single-layer GCN vs deeper variants)
  - Training pairs (N/2 vs full pairwise computation)

- **Failure signatures:**
  - Poor counterfactual quality despite good ranking → GED approximation failure
  - High computational cost despite GNN → Training/pair selection issue
  - Human evaluators can't understand explanations → Semantic graph representation problem

- **First 3 experiments:**
  1. Run CUB dataset with GCN variant: verify P@1 > 0.19 and average edits < 10.5
  2. Test VG-RANDOM subset: check binary P@1 > 0.21 and average GED < 180
  3. Implement human evaluation: confirm >48% preference over CVE and >89% accuracy in blind learning variant

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How robust are semantic graph counterfactuals to low-quality or noisy annotations in the input data?
- **Basis in paper:** [explicit] The paper acknowledges the potential impact of low-quality data in real-world environments and mentions plans to explore robustness in future work.
- **Why unresolved:** The paper does not provide experimental results or analysis on the impact of annotation quality on the generated counterfactuals.
- **What evidence would resolve it:** Experiments evaluating the performance of the method with varying levels of annotation quality, or techniques to improve robustness to noisy annotations.

### Open Question 2
- **Question:** Can the efficiency of the GNN-based similarity model be further improved by using unsupervised GNN methods?
- **Basis in paper:** [explicit] The paper mentions plans to explore improving efficiency by employing unsupervised GNN methods in future work.
- **Why unresolved:** The paper does not provide any results or analysis on the potential benefits of using unsupervised GNN methods for the similarity model.
- **What evidence would resolve it:** Experiments comparing the performance and efficiency of the current supervised GNN model with unsupervised alternatives, such as contrastive learning or self-supervised approaches.

### Open Question 3
- **Question:** How does the quality of automatically generated scene graphs affect the resulting counterfactual explanations?
- **Basis in paper:** [explicit] The paper demonstrates the applicability of the method to unannotated datasets using automatic scene graph generation, but does not provide a detailed analysis of the impact of graph quality on the explanations.
- **Why unresolved:** The paper does not provide a quantitative evaluation of the relationship between scene graph quality and the quality of the resulting counterfactuals.
- **What evidence would resolve it:** Experiments varying the quality of the automatically generated scene graphs (e.g., using different SGG models or training data) and measuring the impact on the performance of the counterfactual explanations.

## Limitations
- The method depends on high-quality scene graph annotations or robust SGG models—errors in graph generation directly degrade counterfactual accuracy
- Scalability to large datasets may be constrained by the computational cost of computing GED on top candidates
- Performance on fully unannotated or multimodal data remains untested, limiting real-world applicability

## Confidence
- **High** confidence in the core contribution—efficient semantic graph counterfactuals—supported by quantitative metrics and human studies
- **Medium** confidence in the GNN-based GED approximation, as the method relies on a training strategy (N/2 pairs) without exhaustive validation of embedding quality across diverse graph types
- **Low** confidence in real-world robustness: the evaluation uses curated datasets with available scene graphs; performance on fully unannotated or multimodal data remains untested

## Next Checks
1. Test the GNN approximation on noisy, automatically generated scene graphs to evaluate robustness to annotation quality
2. Benchmark runtime performance on large-scale datasets to assess computational scalability
3. Conduct human studies with non-expert participants to confirm accessibility of the explanations