---
ver: rpa2
title: Rethinking Multi-domain Generalization with A General Learning Objective
arxiv_id: '2402.18853'
source_url: https://arxiv.org/abs/2402.18853
tags:
- domain
- gmdg
- objective
- domains
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a general learning objective for multi-domain
  generalization (mDG) that addresses the limitation of existing methods by relaxing
  the assumption of static target marginal distributions. The core idea involves introducing
  a learnable Y-mapping to extract domain-independent features from both observations
  and targets, and formulating a general objective comprising four synergistic components:
  learning domain-invariant conditional features, maximizing posterior, integrating
  prior information, and suppressing invalid causality.'
---

# Rethinking Multi-domain Generalization with A General Learning Objective

## Quick Facts
- arXiv ID: 2402.18853
- Source URL: https://arxiv.org/abs/2402.18853
- Authors: Zhaorui Tan; Xi Yang; Kaizhu Huang
- Reference count: 40
- One-line primary result: Achieves 0.7% improvement over MIRO and outperforms SIMPLE++ ensemble on classification tasks

## Executive Summary
This paper addresses the limitations of existing multi-domain generalization (mDG) methods by relaxing the assumption of static target marginal distributions. The authors propose a general learning objective that introduces learnable mappings for both observations and targets, enabling the extraction of domain-independent features from both X and Y. The method is validated through extensive experiments across regression, segmentation, and classification tasks, demonstrating substantial improvements over state-of-the-art approaches.

## Method Summary
The method introduces a general learning objective for mDG that consists of four synergistic components: learning domain-invariant conditional features, maximizing posterior, integrating prior information, and suppressing invalid causality. The core innovation involves using learnable mappings (ϕ and ψ) to project observations and targets into a shared latent Reproducing Kernel Hilbert Space (RKHS), relaxing the static target distribution assumption. The approach is evaluated on NYU Depth V2 for monocular depth estimation, GTA V/SYNTHIA/Cityscapes/BDD-100K/Mapillary for segmentation, and PACS/VLCS/OfficeHome/TerraIncognita/DomainNet for classification tasks.

## Key Results
- Achieves 0.7% average improvement over MIRO when using a single pre-trained model as oracle
- Outperforms SIMPLE++ which uses 283 pre-trained models as ensemble
- Demonstrates consistent improvements across regression, segmentation, and classification tasks
- Shows robustness across multiple benchmark datasets including PACS, VLCS, OfficeHome, and DomainNet

## Why This Works (Mechanism)

### Mechanism 1
Introducing ψ mapping for Y relaxes static target distribution assumption and improves generalization by learning domain-independent features in both X and Y. The mapping enables learning of joint domain-invariant representations while preserving prediction relationship X→Y. This works when both X and Y contain domain-dependent features that can be extracted into a shared latent space.

### Mechanism 2
The four-component general objective (GAim1, GAim2, GReg1, GReg2) synergistically addresses domain shifts better than partial objectives. The combination addresses all aspects of the problem by learning domain-invariant conditional features, maximizing posterior, integrating prior knowledge, and suppressing invalid causality. This works when previous methods optimized only subsets of the complete objective.

### Mechanism 3
Minimizing Conditional Feature Shift (CFS) effectively suppresses invalid causality introduced by relaxing P(Y|D) static assumption. GReg2 minimizes the conditional feature shift between unconditional and conditional features, preventing spurious correlations ψ(Y)→ϕ(X). This works when relaxing the static assumption introduces invalid causality that degrades generalization.

## Foundational Learning

- **Reproducing Kernel Hilbert Space (RKHS)**: Provides the theoretical framework for mapping X and Y into shared latent space where domain-invariant features can be learned. *Quick check*: What property of RKHS makes it suitable for learning domain-invariant representations?

- **Jensen-Shannon Divergence (JSD) and its generalization**: Forms the theoretical basis for upper bound derivation and domain alignment optimization. *Quick check*: How does minimizing GJSD across domains help achieve domain-invariant representations?

- **Conditional Feature Shift**: Quantifies and suppresses spurious correlations that arise when relaxing static distribution assumptions. *Quick check*: What mathematical relationship does CFS capture between unconditional and conditional feature distributions?

## Architecture Onboarding

- **Component map**: X (observations), Y (targets) -> ϕ(X), ψ(Y) -> shared latent RKHS -> VAE encoders -> domain alignment loss LA1, posterior maximization LA2, prior integration loss LR1, invalid causality suppression LR2 -> final training

- **Critical path**:
  1. Map X and Y to shared latent space using ϕ and ψ
  2. Estimate joint distributions across domains using VAE encoders
  3. Compute domain alignment loss LA1
  4. Compute posterior maximization LA2
  5. Compute prior integration loss LR1
  6. Compute invalid causality suppression LR2
  7. Combine all losses for final training

- **Design tradeoffs**: ψ mapping adds complexity but enables relaxation of static distribution assumption; VAE encoders provide distribution estimates but increase computational cost; four-loss combination provides comprehensive coverage but requires careful hyperparameter tuning; prior integration helps when good oracle exists but may be unnecessary otherwise

- **Failure signatures**: Performance worse than baseline without GMDG indicates incorrect hyperparameter settings or poor oracle selection; no improvement over MIRO/SIMPLE suggests insufficient domain shift or already optimal prior utilization; training instability may indicate issues with VAE encoder initialization or loss weight balancing

- **First 3 experiments**:
  1. Toy regression experiment with synthetic data showing ψ benefits
  2. Ablation study on OfficeHome dataset testing individual loss components
  3. Comparison with MIRO using same pre-trained oracle on classification task

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed GMDG objective perform in single-domain generalization scenarios where the model is trained on a single source domain and tested on unseen domains? The paper primarily focuses on multi-domain generalization with multiple source domains and doesn't provide results for single-domain scenarios.

### Open Question 2
What is the optimal weighting strategy for the different components of the GMDG objective (GAim1, GAim2, GReg1, GReg2) across various tasks and datasets? The paper mentions weighted combination but doesn't provide a systematic approach for determining optimal weights.

### Open Question 3
How does the proposed method handle domain shifts that are not captured by the learnable mappings (ϕ and ψ), such as shifts in data distribution that cannot be learned by the model? The paper introduces mappings but doesn't discuss scenarios where they may fail to capture certain types of domain shifts.

## Limitations

- Computational overhead from introducing ψ mapping and VAE encoders may not justify performance gains in all scenarios
- Method's effectiveness for truly out-of-distribution scenarios remains to be validated
- Heavy reliance on quality of pre-trained oracle models for prior knowledge integration

## Confidence

**High Confidence**: The relaxation of static P(Y|D) assumption is theoretically valid; four-component objective addresses complementary aspects; experimental results demonstrate consistent improvements.

**Medium Confidence**: The ψ mapping mechanism is essential for success; specific combination of loss components yields synergistic benefits; method scales effectively to real-world datasets.

**Low Confidence**: Performance with minimal prior knowledge; robustness to oracle selection across diverse domains; effectiveness in truly out-of-distribution scenarios.

## Next Checks

1. **Oracle Sensitivity Analysis**: Systematically evaluate performance variation when using oracles with decreasing relevance to target domains to quantify robustness and identify failure thresholds.

2. **Kernel Ablation Study**: Test the method with different kernel functions (linear, RBF, polynomial) in the RKHS framework to determine sensitivity to kernel choice and identify optimal configurations.

3. **Out-of-Distribution Test**: Evaluate the method on datasets with domain shifts substantially different from training domains (e.g., medical imaging with entirely different modalities) to assess true generalization capabilities.