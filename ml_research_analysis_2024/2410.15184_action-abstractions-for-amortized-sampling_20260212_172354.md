---
ver: rpa2
title: Action abstractions for amortized sampling
arxiv_id: '2410.15184'
source_url: https://arxiv.org/abs/2410.15184
tags:
- action
- learning
- chunks
- chunking
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ACTION PIECE, a method for discovering action
  abstractions or "chunks" from sampled trajectories in reinforcement learning (RL)
  and generative flow networks (GFlowNets). The approach involves extracting frequently
  occurring action subsequences from high-reward trajectories and adding them as high-level
  actions to the action space, thereby reducing trajectory length and improving credit
  assignment.
---

# Action abstractions for amortized sampling

## Quick Facts
- arXiv ID: 2410.15184
- Source URL: https://arxiv.org/abs/2410.15184
- Authors: Oussama Boussif; Léna Néhale Ezzine; Joseph D Viviano; Michał Koziarski; Moksh Jain; Nikolay Malkin; Emmanuel Bengio; Rim Assouel; Yoshua Bengio
- Reference count: 40
- Primary result: ACTION PIECE discovers interpretable action chunks that improve mode discovery and density estimation in both GFlowNets and RL

## Executive Summary
This paper introduces ACTION PIECE, a method for discovering action abstractions ("chunks") from high-reward trajectories in reinforcement learning and generative flow networks. The approach uses Byte Pair Encoding to identify frequently occurring action subsequences, which are then added as higher-level actions to the action space. This chunking mechanism reduces trajectory length and improves credit assignment, leading to better exploration and sample efficiency. The method is particularly effective for GFlowNets, where it accelerates mode discovery and captures the latent structure of the reward landscape.

## Method Summary
ACTION PIECE extracts frequently occurring action subsequences from high-reward trajectories using Byte Pair Encoding tokenization. These chunks are added as new actions to the action space, creating a curriculum of progressively longer action abstractions. Two variants are proposed: INCREMENT (adds one chunk at a time) and REPLACE (replaces entire action space with top M chunks). The method is applied to both GFlowNets and RL algorithms, with policy networks using action embeddings to handle the dynamically growing action space. The approach is evaluated across four environments: Bit sequence generation, FractalGrid, Graph generation, and RNA sequence generation.

## Key Results
- ACTION PIECE accelerates mode discovery and improves density estimation in both GFlowNets and RL
- GFlowNet-based samplers benefit more from chunking than RL methods due to their exploratory behavior
- Learned chunks are interpretable and transferable across different tasks and samplers
- ACTION PIECE-INCREMENT provides more stable chunking than REPLACE by allowing gradual policy adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chunking frequently occurring action subsequences into higher-level actions reduces trajectory length and improves credit assignment in GFlowNets and RL.
- Mechanism: Extract common subsequences from high-reward trajectories using Byte Pair Encoding (BPE), then add them as new actions to the action space. This creates a curriculum of progressively longer action chunks that encode compressed representations of successful trajectories.
- Core assumption: The most frequent subsequences in high-reward trajectories capture meaningful structure in the reward landscape and can serve as effective building blocks for sampling.
- Evidence anchors:
  - [abstract]: "Our approach involves iteratively extracting action subsequences commonly used across many high-reward trajectories and ‘chunking’ them into a single action that is added to the action space."
  - [section]: "Our approach consists of augmenting existing samplers with an action abstraction discovery step, a form of 'chunking' as originally described by cognitive psychologists (Miller, 1956), which is applicable to any sampler."
- Corpus evidence: Weak. Related works focus on planning-augmented sampling and adaptive teachers, but none directly validate chunking as a credit assignment mechanism.

### Mechanism 2
- Claim: ActionPiece-Increment provides more stable chunking than ActionPiece-Replace because it allows gradual adaptation to the growing action space.
- Mechanism: Add one chunk at a time to the action space, allowing the policy to adjust before the next chunk is introduced. This incremental approach prevents overwhelming the policy with too many new actions simultaneously.
- Core assumption: Policies can effectively adapt to a slowly growing action space, but struggle when the entire action space is replaced at once.
- Evidence anchors:
  - [abstract]: "We consider the following two approaches, comparing them to the ATOMIC sampler... ACTION PIECE -INCREMENT: We apply the tokenizer on the action corpus and add the most frequent token found to the action space, which grows by one element each time this is performed."
  - [section]: "Chunking mechanisms. We consider the following two approaches... ACTION PIECE -INCREMENT: We apply the tokenizer on the action corpus and add the most frequent token found to the action space, which grows by one element each time this is performed."
- Corpus evidence: Weak. No direct comparison of incremental vs. replace strategies in related literature.

### Mechanism 3
- Claim: GFlowNet-based samplers benefit more from chunking than RL-based methods because their exploratory behavior prevents premature convergence to suboptimal modes.
- Mechanism: GFlowNet's entropy-seeking objective naturally explores the state space, allowing it to discover diverse high-reward trajectories from which meaningful chunks can be extracted. This exploration prevents the greedy behavior that causes RL methods to get stuck in local optima.
- Core assumption: The exploratory nature of GFlowNets is essential for discovering diverse chunks that capture the latent structure of the reward landscape.
- Evidence anchors:
  - [abstract]: "We also observe that the abstracted high-order actions are interpretable, capturing the latent structure of the reward landscape of the action space."
  - [section]: "The greedy behavior of both SAC and A2C provides a possible explanation: when a mode is discovered, it gets sampled more often resulting in the addition of a large chunk... This issue is mitigated in GFlowNets due to the exploratory behavior imparted by the learning objective."
- Corpus evidence: Weak. Related works focus on exploration in GFlowNets but don't specifically address the interaction between exploration and chunking.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: Understanding the MDP framework is essential for grasping how chunking trades off depth for breadth in the planning problem.
  - Quick check question: In the context of chunking, what does it mean to trade "reduced depth of the MDP in exchange for increased breadth"?

- Concept: Credit assignment problem
  - Why needed here: The paper addresses the challenge of propagating learning signals over long trajectories, which is central to understanding why chunking helps.
  - Quick check question: Why does the credit assignment problem become more severe as trajectory length increases?

- Concept: Byte Pair Encoding (BPE) tokenization
  - Why needed here: BPE is the specific algorithm used to identify frequently occurring action subsequences that become chunks.
  - Quick check question: How does BPE identify which action subsequences to merge into chunks?

## Architecture Onboarding

- Component map: Policy network -> Tokenizer (BPE) -> Action space manager -> Replay buffer -> Backward policy (GFlowNets) -> Loss functions
- Critical path: 1) Generate trajectories using current policy 2) Filter high-reward trajectories 3) Apply BPE tokenization to extract chunks 4) Add new chunks to action space 5) Update policy to handle new action space 6) Repeat training with expanded action space
- Design tradeoffs:
  - Chunking frequency vs. policy adaptation time
  - Action space growth rate vs. policy capacity
  - Exploration vs. exploitation in chunk discovery
  - Computational cost of tokenization vs. training benefits
- Failure signatures:
  - Policy performance degrades after chunk addition
  - Action space grows too quickly for policy to adapt
  - Chunks fail to improve mode discovery or density estimation
  - Discovered chunks don't capture meaningful structure
- First 3 experiments:
  1. Run ATOMIC baseline on FractalGrid to establish performance without chunking
  2. Implement ActionPiece-Increment on FractalGrid and compare mode discovery rate
  3. Compare ActionPiece-Increment vs. ActionPiece-Replace on RNA Binding task to evaluate chunking mechanisms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of tokenizer (e.g., BPE vs random) impact the quality and interpretability of learned chunks across different environments?
- Basis in paper: [explicit] The paper compares BPE and random tokenizers, showing that random tokenization leads to less interpretable and less effective chunks in the RNA Binding task.
- Why unresolved: The comparison is limited to one environment, and the impact on other tasks (e.g., FractalGrid, Bit Sequence, Graph) is not explicitly studied.
- What evidence would resolve it: A comprehensive comparison across all environments, measuring chunk quality, interpretability, and downstream performance using different tokenization strategies.

### Open Question 2
- Question: What is the optimal chunking frequency and mechanism for balancing exploration and exploitation in different RL algorithms?
- Basis in paper: [inferred] The paper explores two chunking mechanisms (INCREMENT and REPLACE) and a fixed chunking frequency, but the impact on exploration-exploitation trade-offs is not explicitly analyzed.
- Why unresolved: The paper does not provide a systematic study of how chunking frequency and mechanism affect the balance between exploring new modes and exploiting known high-reward regions.
- What evidence would resolve it: A detailed analysis of chunking frequency and mechanism on exploration-exploitation trade-offs, measured by metrics like mode diversity, reward stability, and convergence speed.

### Open Question 3
- Question: Can learned chunks generalize to structurally similar but distinct reward distributions, and how does this depend on the source task and chunking mechanism?
- Basis in paper: [explicit] The paper demonstrates that chunks learned from the L14 RNA1 task can generalize to L14 RNA2 and L14 RNA3, but the effectiveness varies by chunking mechanism and source task.
- Why unresolved: The analysis is limited to RNA tasks, and the generalizability to other domains (e.g., FractalGrid, Bit Sequence, Graph) is not explored.
- What evidence would resolve it: A cross-domain study evaluating the transferability of learned chunks, measuring performance on target tasks and analyzing the structural similarity between source and target distributions.

## Limitations

- Scalability to complex domains with sparse rewards remains unproven
- Chunk interpretability is demonstrated qualitatively but not quantified
- The optimal chunking frequency and mechanism are not systematically studied
- Limited evaluation to synthetic tasks and one real-world RNA binding task

## Confidence

- Confidence: Low on whether the chunking mechanism generalizes beyond the tested environments
- Confidence: Medium in the empirical superiority of ACTION PIECE-INCREMENT over REPLACE
- Confidence: Medium in the interpretability of learned chunks

## Next Checks

1. **Ablation on chunk interpretability**: Run experiments where chunks are randomly generated rather than discovered from high-reward trajectories, keeping all other aspects of ACTION PIECE identical. Compare mode discovery and density estimation performance to determine whether the specific content of chunks matters beyond their existence as abstractions.

2. **Scalability stress test**: Apply ACTION PIECE to environments with significantly longer trajectories (10x the current length) and more complex reward landscapes. Measure the impact on training time, chunk quality, and whether the policy can still effectively utilize the growing action space.

3. **Transferability quantification**: Design experiments where chunks discovered in one task are directly applied to a structurally similar but different task (e.g., chunks from RNA L14 applied to RNA L50). Measure performance degradation and compare against training ACTION PIECE from scratch on the new task to quantify the actual benefit of chunk transferability.