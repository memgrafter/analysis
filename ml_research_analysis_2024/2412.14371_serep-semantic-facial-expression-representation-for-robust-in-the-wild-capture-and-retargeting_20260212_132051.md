---
ver: rpa2
title: 'SEREP: Semantic Facial Expression Representation for Robust In-the-Wild Capture
  and Retargeting'
arxiv_id: '2412.14371'
source_url: https://arxiv.org/abs/2412.14371
tags:
- expression
- capture
- face
- facial
- mesh
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEREP, a method for capturing and transferring
  facial expressions from monocular images in-the-wild. The core innovation is a semantic
  expression representation that disentangles identity from expression at a feature
  level, addressing the limitation of 3DMMs where expression parameters are identity-dependent.
---

# SEREP: Semantic Facial Expression Representation for Robust In-the-Wild Capture and Retargeting

## Quick Facts
- arXiv ID: 2412.14371
- Source URL: https://arxiv.org/abs/2412.14371
- Authors: Arthur Josi; Luiz Gustavo Hafemann; Abdallah Dib; Emeline Got; Rafael M. O. Cruz; Marc-Andre Carbonneau
- Reference count: 40
- Primary result: SEREP achieves 38.3 FPS on 2080Ti GPU for facial expression capture and retargeting

## Executive Summary
SEREP introduces a semantic expression representation that disentangles identity from expression at the feature level, addressing the fundamental limitation of 3DMMs where expression parameters are identity-dependent. The method learns this representation from unpaired 3D data and uses it to generate synthetic training data for an expression capture model. A semi-supervised training scheme with domain adaptation bridges the gap between synthetic and real data, enabling robust in-the-wild facial expression capture and retargeting.

## Method Summary
SEREP consists of two main components: a semantic expression model that learns to disentangle expression from identity using unpaired 3D meshes, and an expression capture model that predicts expressions from monocular images. The semantic model uses encoders for identity (Eid) and expression (Eexp) along with a mesh decoder (Dmesh) to generate identity-specific vertex displacements from semantic expression codes. The expression capture model employs an image encoder (Eimg) with separate heads for expression codes and landmarks, trained with domain adaptation to handle the synthetic-to-real data distribution shift. The method generates synthetic training data by applying learned semantic expression codes to random identities and rendering them.

## Key Results
- Achieves 38.3 FPS on 2080Ti GPU for real-time facial expression capture and retargeting
- Outperforms state-of-the-art methods on MultiREX benchmark for 3D expression reconstruction
- Better preserves identity during expression retargeting while reproducing challenging expressions like winks and asymmetrical smiles
- Maintains robustness to viewpoint changes across different camera angles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic expression representation disentangles expression from identity at the feature level, enabling identity-preserving retargeting.
- Mechanism: The method learns a semantic expression basis from unpaired 3D data using encoders for identity (Eid) and expression (Eexp), and a decoder (Dmesh) that generates identity-specific vertex displacements from the same semantic expression code.
- Core assumption: The same semantic expression code applied to different identities produces the same underlying expression but with identity-specific geometric manifestations.
- Evidence anchors:
  - [abstract]: "a model that disentangles expression from identity at the semantic level"
  - [section 3.1]: "represent the expression from an expressive mesh Es and apply it to a neutral mesh of the same source subject Ns (i.e., reconstruction) or a different target subject Nt (i.e., retargeting)"
  - [corpus]: Weak evidence - corpus contains related work on expression disentanglement but no direct evaluation of this specific semantic representation approach.
- Break condition: If the expression code does not capture the semantic meaning of expressions across identities, identity leakage will occur during retargeting.

### Mechanism 2
- Claim: Semi-supervised training with synthetic data and domain adaptation enables robust in-the-wild expression capture.
- Mechanism: The method generates synthetic training data using the semantic expression model, then trains an expression capture model on both synthetic data (with ground truth expression codes) and real in-the-wild data using a domain adaptation strategy to bridge the domain gap.
- Core assumption: Features learned from synthetic data can be adapted to generalize to real in-the-wild data through domain alignment.
- Evidence anchors:
  - [abstract]: "train a model to predict expression from monocular images relying on a novel semi-supervised scheme using low quality synthetic data"
  - [section 3.3]: "We adopt the domain adaptation approach from [20], and define the corresponding domain loss Ld"
  - [corpus]: Weak evidence - corpus contains related work on synthetic data training but no direct evaluation of this specific semi-supervised domain adaptation approach.
- Break condition: If the domain gap between synthetic and real data is too large, the domain adaptation strategy may fail to generalize.

### Mechanism 3
- Claim: Region-based evaluation on MultiREX benchmark provides accurate assessment of expression capture performance.
- Mechanism: The benchmark divides the face into four regions (forehead, cheek, mouth, nose), performs region-based rigid alignment between ground truth and predicted meshes, and computes per-vertex error for each region.
- Core assumption: Region-based rigid alignment avoids penalizing rigid misalignment while focusing on non-rigid expression deformations.
- Evidence anchors:
  - [section 5.1]: "Inspired by the REALY benchmark [5], we adopt a region-based evaluation method, dividing the face into four regions"
  - [section 5.1]: "For each region, we find the optimal rigid alignment between the GT and predicted meshes in the Multiface topology and compute the per-vertex error"
  - [corpus]: Weak evidence - corpus contains related work on facial expression benchmarks but no direct evaluation of this specific region-based approach.
- Break condition: If the rigid alignment fails to properly align regions, the per-vertex error computation may be inaccurate.

## Foundational Learning

- Concept: 3D Morphable Models (3DMM)
  - Why needed here: Understanding the limitations of 3DMMs (expression parameters are identity-dependent) is crucial for appreciating the need for semantic expression representation.
  - Quick check question: Why do 3DMMs have identity leakage during expression retargeting?

- Concept: Domain adaptation
  - Why needed here: The semi-supervised training scheme relies on domain adaptation to bridge the gap between synthetic and real data.
  - Quick check question: What is the purpose of the domain loss Ld in the expression capture model?

- Concept: Facial expression semantics
  - Why needed here: The semantic expression representation is based on the idea that expressions have semantic meaning independent of identity.
  - Quick check question: How does the semantic expression model ensure that the same semantic expression produces different geometric manifestations for different identities?

## Architecture Onboarding

- Component map: Eid -> Eexp -> Dmesh (semantic expression model); Eimg -> Hexp + Hlmks + Cd (expression capture model)
- Critical path:
  1. Train semantic expression model on 3D meshes
  2. Generate synthetic training data
  3. Train expression capture model on synthetic and real data
  4. Evaluate on MultiREX benchmark

- Design tradeoffs:
  - Using unpaired 3D data simplifies training but may miss some expression variations
  - Synthetic data generation is fast but may not capture all real-world variations
  - Region-based evaluation focuses on expression but may miss global face shape issues

- Failure signatures:
  - Identity leakage during retargeting indicates semantic expression representation failure
  - Poor generalization to in-the-wild data indicates domain adaptation failure
  - High error on certain regions indicates region-based evaluation issues

- First 3 experiments:
  1. Train semantic expression model and evaluate reconstruction error on test set
  2. Generate synthetic data and train expression capture model, evaluate on MultiREX
  3. Compare expression capture performance with and without domain adaptation on in-the-wild data

## Open Questions the Paper Calls Out
The paper explicitly states future work aims to make the capture model temporally aware to improve video capture quality, addressing current limitations with jittery capture and motion blur. The authors also acknowledge their method struggles with external occluders such as glasses and inflated cheeks, suggesting potential generalization issues with extreme facial morphologies.

## Limitations
- The method struggles with external occluders such as glasses and inflated cheeks, limiting real-world applicability
- Temporal consistency issues in video sequences lead to jittery capture and motion blur artifacts
- No systematic evaluation of cross-population generalization to diverse facial morphologies or cultural-specific expressions

## Confidence

- High confidence: Expression capture performance on MultiREX benchmark (quantitative results with established metrics)
- Medium confidence: Identity preservation during retargeting (qualitative comparisons show improvement but lack rigorous statistical validation)
- Medium confidence: Robustness to challenging expressions and viewpoints (visual comparisons support claims but quantitative metrics are limited)

## Next Checks

1. **Cross-identity semantic consistency test**: Apply the same semantic expression code to multiple identities and measure the consistency of expression semantics across subjects using human perceptual studies or automated semantic alignment metrics.

2. **Ablation study of domain adaptation**: Train the expression capture model with and without domain adaptation components on the same synthetic-to-real pipeline to quantify the specific contribution of each adaptation element to final performance.

3. **Generalization to out-of-distribution expressions**: Test the system on expressions not present in the training data (e.g., extreme facial deformations, cultural-specific expressions) to assess the true semantic generalization capability beyond the MultiREX benchmark.