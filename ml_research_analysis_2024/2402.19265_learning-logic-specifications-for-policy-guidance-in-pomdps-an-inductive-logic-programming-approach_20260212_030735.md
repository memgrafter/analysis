---
ver: rpa2
title: 'Learning Logic Specifications for Policy Guidance in POMDPs: an Inductive
  Logic Programming Approach'
arxiv_id: '2402.19265'
source_url: https://arxiv.org/abs/2402.19265
tags:
- policy
- heuristics
- pomcp
- specifications
- pomdp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to learn interpretable policy heuristics
  for POMDP solvers using Inductive Logic Programming (ILP). The approach converts
  belief-action pairs from POMDP executions into logical specifications via ASP, which
  are then used as online heuristics in solvers like POMCP and DESPOT.
---

# Learning Logic Specifications for Policy Guidance in POMDPs: an Inductive Logic Programming Approach

## Quick Facts
- arXiv ID: 2402.19265
- Source URL: https://arxiv.org/abs/2402.19265
- Reference count: 20
- Key outcome: Learned ASP heuristics improve POMCP and DESPOT performance, achieving results comparable to optimal handcrafted heuristics while reducing computational time and generalizing to larger problem instances.

## Executive Summary
This paper presents a method to learn interpretable policy heuristics for POMDP solvers using Inductive Logic Programming (ILP). The approach converts belief-action pairs from POMDP executions into logical specifications via ASP, which are then used as online heuristics in solvers like POMCP and DESPOT. Experiments on rocksample and pocman domains show that learned heuristics significantly improve performance, especially in complex environments with large action spaces and long planning horizons. The method achieves performance comparable to optimal handcrafted heuristics while reducing computational time. It also generalizes well to more challenging scenarios not experienced during training.

## Method Summary
The method learns policy specifications by converting POMDP belief-action traces into ASP format using environmental feature maps, then applying ILASP to generate interpretable logical rules. These learned specifications are integrated into online POMDP solvers (POMCP, DESPOT, AdaOPS, POMCPOW) as soft policy guidance. The soft guidance uses confidence-weighted probabilistic action selection to preserve asymptotic optimality while improving exploration efficiency. The approach is validated on rocksample (large action space) and pocman (long planning horizon) domains, demonstrating both performance improvements and generalization to more challenging scenarios.

## Key Results
- Learned heuristics achieve performance comparable to optimal handcrafted heuristics in rocksample and pocman domains
- Computational time per step is reduced compared to standard POMCP with no heuristics
- Learned specifications generalize well to larger problem instances not seen during training (e.g., larger grids, more rocks/ghosts)
- The method outperforms neural network approaches in terms of interpretability and training efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learned ASP heuristics guide Monte Carlo tree search exploration efficiently.
- Mechanism: The method converts belief-action pairs into logical specifications via ILP. These specifications are then integrated as soft policy guidance in the UCT and rollout phases of POMCP and as default policy definitions in DESPOT. By grounding environmental features from the belief distribution, the solver can prioritize actions that match the learned policy, thus reducing the number of required simulations.
- Core assumption: The belief distribution can be effectively represented as a set of high-level environmental features (e.g., distances, probabilities) that capture the task-relevant semantics of the state.
- Evidence anchors:
  - [abstract]: "convert the belief-action pairs to a logical semantics, and exploit data- and time-efficient Inductive Logic Programming (ILP) to generate interpretable belief-based policy specifications, which are then used as online heuristics."
  - [section 4.4.1]: "At each root node in MCTS, we ground environmental features from the current belief according to map FF... Only for ground actions, we introduce a prior in UCT, increasing the value V (ha)..."
- Break condition: If the belief distribution is too complex or high-dimensional for a small set of interpretable features, the grounding may be incomplete and the guidance ineffective.

### Mechanism 2
- Claim: Soft policy guidance preserves POMCP's asymptotic optimality while improving performance.
- Mechanism: Instead of pruning actions not entailed by the learned specifications, the method assigns them a minimal but non-zero probability weight in the rollout phase. This ensures that all branches can still be explored given enough simulations, maintaining theoretical guarantees.
- Core assumption: The asymptotic optimality of POMCP depends on the ability to explore all branches with sufficient simulation depth, not on excluding any action permanently.
- Evidence anchors:
  - [abstract]: "Our approach can take into account the confidence level about learned specifications... This is used for soft policy guidance, namely, weighted probabilistic action exploration with completeness guarantees, in POMCP..."
  - [section 4.4.1]: "In other words, if an action ai is entailed by ASP specifications R under the current belief interpretation FF(¯b), its selection probability depends on the percentage of covered examples covi. Otherwise, the minimum covi is assigned to not entailed actions."
- Break condition: If the confidence weights are poorly calibrated (e.g., very low coverage percentages), the soft guidance may degenerate into near-uniform sampling, providing little benefit.

### Mechanism 3
- Claim: Interpretable policy specifications enable generalization across unseen problem instances.
- Mechanism: The learned logical rules encode high-level concepts (e.g., "sample a rock when the distance is zero and probability is high") rather than memorizing specific belief-action mappings. These concepts transfer to larger grids, more rocks/ghosts, or altered parameters because they depend on semantic relationships rather than raw numeric values.
- Core assumption: The core structure of the task (e.g., "move towards the closest valuable rock") remains invariant across scaled-up or modified instances, even if specific probabilities or distances change.
- Evidence anchors:
  - [abstract]: "Moreover, they well generalize to more challenging scenarios not experienced in the training phase (e.g., increasing rocks and grid size in rocksample, incrementing the size of the map and the aggressivity of ghosts in pocman)."
  - [section 5.3]: "This aims at showing the generality of our heuristics, especially in more complex environments involving larger actions spaces and planning horizons."
- Break condition: If the scaling introduces new task elements or rules that were not represented in the environmental features (e.g., "check rock fewer than 5 times"), the learned specifications cannot generalize.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The paper's entire method is built on learning heuristics for POMDP solvers, so understanding the POMDP framework is essential.
  - Quick check question: What is the role of the belief distribution in a POMDP, and why is it harder to compute an optimal policy than in a fully observable MDP?

- Concept: Monte Carlo Tree Search (MCTS) and UCT
  - Why needed here: The policy specifications are integrated into the MCTS-based POMCP solver; knowing how UCT balances exploration and exploitation is key to understanding the soft guidance mechanism.
  - Quick check question: How does the exploration constant c in UCT influence the trade-off between exploring new actions and exploiting known good actions?

- Concept: Inductive Logic Programming (ILP) under Answer Set Semantics
  - Why needed here: The paper uses ILP to learn ASP rules from belief-action traces; understanding how ILP maps examples to logical rules is crucial.
  - Quick check question: In the ILP setup described, what is the difference between the included set, excluded set, and context in a CDPI?

## Architecture Onboarding

- Component map: POMDP traces → ASP formalization (belief-action to logical features) → ILASP learner → ASP policy specifications → Integration in POMCP/DESPOT/AdaOPS
- Critical path: Generate high-quality POMDP traces → Learn policy specifications via ILASP → Integrate specifications as soft guidance → Evaluate performance in online solver
- Design tradeoffs: Soft policy guidance vs. hard pruning of actions; interpretability vs. completeness of environmental features; training trace quality vs. learning time
- Failure signatures: Low coverage percentages in learned rules; poor performance in larger/modified domains; high computational cost per step in rollout
- First 3 experiments:
  1. Run POMCP with learned heuristics in the training domain (same N, M) and compare to baseline
  2. Increase grid size or number of rocks and measure generalization performance
  3. Generate low-quality traces (fewer simulations) and assess robustness of learned heuristics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can temporal logic specifications be effectively learned from POMDP traces to improve policy guidance in tasks requiring memory of past actions?
- Basis in paper: [explicit] The paper discusses the limitation of current learned specifications which only consider current belief and do not capture temporal relationships between actions. It mentions that handcrafted heuristics often involve checking how many times a rock has been checked, which requires temporal logic.
- Why unresolved: Temporal logic learning is an open challenge, and the paper states that extending to temporal first order logic and full ASP expressiveness requires further theoretical investigation.
- What evidence would resolve it: Successful development and integration of a method for learning temporal logic specifications from POMDP traces that improves policy guidance in tasks requiring memory of past actions.

### Open Question 2
- Question: How can policy specifications be learned online as the agent acquires experience, rather than just offline from pre-collected traces?
- Basis in paper: [explicit] The paper mentions online learning as a potential future direction, stating that extending online learning approaches for ASP rewards to POMDPs with fast online ILP techniques is an open problem.
- Why unresolved: Current methodologies learn specifications offline, but online learning could allow for continuous refinement and adaptation to new scenarios.
- What evidence would resolve it: Demonstration of an effective online learning method for policy specifications that improves agent performance in challenging POMDP tasks as experience is gained.

### Open Question 3
- Question: How can the definition of environmental features be automated or made more robust in complex real-world domains where the POMDP definition may be incomplete?
- Basis in paper: [explicit] The paper discusses the assumption that environmental features can be easily defined from the POMDP problem definition, but acknowledges that in complex domains this may not be straightforward and some features may be missed.
- Why unresolved: Automating feature definition is challenging, especially when the POMDP model is incomplete or when high-level domain concepts cannot be directly retrieved from the transition and reward maps.
- What evidence would resolve it: Development of a method to automatically or semi-automatically identify relevant environmental features from complex POMDP domains, improving the quality and completeness of learned policy specifications.

## Limitations

- The method's effectiveness depends heavily on the quality and diversity of training traces; poor initial solver performance could propagate through the learning pipeline
- Computational overhead of ILP learning and ASP grounding for complex belief distributions remains unquantified and may limit scalability
- Generalization is demonstrated on only two domains, raising questions about transferability to significantly different POMDP problems

## Confidence

- High confidence: The mechanism of soft policy guidance preserving asymptotic optimality
- Medium confidence: The effectiveness of learned heuristics in improving solver performance
- Medium confidence: The interpretability and generalization of learned specifications

## Next Checks

1. Evaluate learned heuristics on a new POMDP domain with significantly different characteristics (e.g., continuous state space) to test true generalization beyond the two presented domains
2. Measure the computational overhead of ILP learning and ASP grounding across varying belief space complexities to establish scalability bounds
3. Test the sensitivity of learned heuristics to training trace quality by systematically varying the number and diversity of initial solver executions used for learning