---
ver: rpa2
title: Learning Domain-Invariant Features for Out-of-Context News Detection
arxiv_id: '2406.07430'
source_url: https://arxiv.org/abs/2406.07430
tags:
- domain
- news
- adaptation
- detection
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles domain adaptation for out-of-context news detection\u2014\
  a form of misinformation where real images are paired with misleading captions.\
  \ Unlike existing methods that assume labeled data in each domain, this work focuses\
  \ on adapting detection models to entirely new news topics or agencies without labeled\
  \ target domain data."
---

# Learning Domain-Invariant Features for Out-of-Context News Detection

## Quick Facts
- arXiv ID: 2406.07430
- Source URL: https://arxiv.org/abs/2406.07430
- Authors: Yimeng Gu; Mengqi Zhang; Ignacio Castro; Shu Wu; Gareth Tyson
- Reference count: 23
- Primary result: Proposed ConDA-TTA outperforms baseline models by up to 2.93% in F1 score and 2.08% in accuracy for out-of-context news detection

## Executive Summary
This paper addresses domain adaptation for out-of-context news detection, where real images are paired with misleading captions. Unlike existing methods that assume labeled data in each domain, this work focuses on adapting detection models to entirely new news topics or agencies without labeled target domain data. The authors propose ConDA-TTA, which combines contrastive learning, Maximum Mean Discrepancy (MMD), and Test-Time Adaptation (TTA). Experiments on two datasets show ConDA-TTA outperforms baseline models by up to 2.93% in F1 score and 2.08% in accuracy, with ablation studies confirming the effectiveness of each component.

## Method Summary
The ConDA-TTA approach combines multimodal feature encoding with BLIP-2, contrastive learning with Gaussian blur augmentation, Maximum Mean Discrepancy (MMD) for domain-invariant feature learning, and Test-Time Adaptation (TTA) with batch normalization statistics. The model trains for 20 epochs with Adam optimizer and applies TTA during evaluation by passing unlabeled target test data through the classifier before evaluation. The framework is designed to handle domain shifts across news topics and agencies without requiring labeled data in the target domain.

## Key Results
- ConDA-TTA outperforms baseline models by up to 2.93% in F1 score and 2.08% in accuracy
- MMD contributes most on Twitter-COMMs dataset while TTA contributes most on NewsCLIPpings dataset
- The approach is particularly strong when domain adaptation gaps are large

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning creates separable representations that make classification easier by pulling positive pairs (original and augmented news) closer and pushing negative pairs further apart.
- Mechanism: The contrastive loss function uses cosine similarity to measure distances in the projected space, optimizing the projection head to create a representation space where semantically similar items cluster together.
- Core assumption: The augmented versions of news items preserve the semantic meaning and label while creating sufficiently different representations to serve as positive pairs.
- Evidence anchors:
  - [abstract] "we adopt the contrastive loss to learn a representation in the projected space where the original news stays further away from the out-of-context news"
  - [section] "Inspired by (Bhattacharjee et al., 2023), we leverage contrastive learning to learn more separable representations of the input multimodal features in the projected space"
- Break condition: If the augmentation strategy fails to preserve semantic meaning while creating sufficient variation, the positive pairs won't effectively train the model to create separable representations.

### Mechanism 2
- Claim: MMD captures domain-invariant features by minimizing the discrepancy between source and target domain distributions in the projected space.
- Mechanism: MMD computes the distance between mean embeddings of source and target representations in reproducing kernel Hilbert space, effectively removing domain-specific features while preserving domain-invariant knowledge.
- Core assumption: The learned representations in the projected space contain sufficient domain-invariant features that can be extracted through MMD, and the kernel function can effectively measure distributional differences.
- Evidence anchors:
  - [abstract] "we apply contrastive learning and maximum mean discrepancy (MMD) to learn domain-invariant features"
  - [section] "To ensure that domain-invariant features are captured from the learned representations, we then apply Maximum Mean Discrepancy (MMD) to reduce the discrepancy of the learned representations between the source and target domain"
- Break condition: If the projection head doesn't create representations with meaningful domain-invariant features, MMD will fail to extract useful information and may even harm performance.

### Mechanism 3
- Claim: Test-Time Adaptation updates batch normalization statistics using target domain data to further adapt the model during evaluation.
- Mechanism: TTA passes unlabeled target domain test data through the classifier before evaluation mode, updating running estimates in BN layers to incorporate target domain statistics into the model's normalization parameters.
- Core assumption: Batch normalization statistics contain domain-specific information that can be updated with target domain data to improve adaptation, and this process doesn't require labels.
- Evidence anchors:
  - [abstract] "we leverage test-time target domain statistics to further assist domain adaptation"
  - [section] "Prior work (Li et al., 2016) observes that the statistics of the Batch Normalization (BN) layer contain domain-specific information"
- Break condition: If the target domain test data is too small or unrepresentative, updating BN statistics may introduce noise rather than improve adaptation.

## Foundational Learning

- Concept: Multimodal representation learning with MLLMs
  - Why needed here: The task requires understanding both image and text simultaneously to detect mismatched pairings, and MLLMs can directly encode this multimodal information into meaningful representations
  - Quick check question: What would happen if we used separate encoders for image and text instead of a unified MLLM approach?

- Concept: Domain adaptation theory
  - Why needed here: The model needs to generalize from labeled source domains to unlabeled target domains, requiring techniques that learn features invariant across domains
  - Quick check question: How does the performance gap between source-only and source+target training indicate the need for domain adaptation?

- Concept: Contrastive learning fundamentals
  - Why needed here: Creating separable representations in the projected space is crucial for distinguishing original news from out-of-context news, and contrastive learning provides the mechanism for this
  - Quick check question: Why do we need both positive pairs (original + augmentation) and negative pairs in the contrastive loss formulation?

## Architecture Onboarding

- Component map:
  - Multimodal Feature Encoder (BLIP-2) → Projection Head → Contrastive Loss + MMD → Classifier with BN layers → TTA update
  - Data flow: Input (image, text) → MLLM encoding → Augmentation → Projection → Domain-invariant feature learning → Classification

- Critical path: Multimodal Feature Encoder → Projection Head → Contrastive Learning + MMD → Classifier → TTA
  - This sequence is critical because each component builds on the previous one's output

- Design tradeoffs:
  - Using BLIP-2 vs training from scratch: Higher initial performance but less flexibility
  - MMD vs adversarial domain adaptation: MMD is simpler but may be less powerful for complex domain shifts
  - TTA vs no TTA: Better adaptation but requires passing test data through model before evaluation

- Failure signatures:
  - Poor performance despite good source domain accuracy: Domain adaptation not working
  - Performance degradation after adding components: Overfitting or negative transfer
  - High variance across different target domains: Domain shift too large for current adaptation methods

- First 3 experiments:
  1. Verify multimodal feature encoder: Compare BLIP-2 vs separate image/text encoders on source-only training
  2. Test contrastive learning impact: Train with and without Lctr to measure its contribution
  3. Validate MMD effectiveness: Compare domain adaptation with and without MMD component using TSNE visualization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do ConDA-TTA's performance and limitations generalize to news domains beyond topics and agencies, such as regional, linguistic, or cultural variations?
- Basis in paper: [inferred] The authors acknowledge the limitation of focusing only on topics and agencies and note that other domain types (e.g., regional, language, cultural) were not considered.
- Why unresolved: The paper does not include experiments or analysis on these alternative domain types, making it unclear whether the model's strengths in topic and agency adaptation transfer to other forms of domain shift.
- What evidence would resolve it: Empirical results on datasets with diverse regional, linguistic, or cultural news domains, along with a comparative analysis of performance trends and failure modes.

### Open Question 2
- Question: Why do certain domain adaptation settings, such as Washington Post (W) in NewsCLIPpings, result in negative transfer across all models, and how can this be systematically diagnosed and mitigated?
- Basis in paper: [explicit] The authors observe that all models exhibit negative transfer when adapting to the Washington Post, noting that the underlying dataset shows the W domain is particularly difficult to adapt to from B or G.
- Why unresolved: While the authors conjecture that insufficient information in B and G limits adaptation to W, they do not provide a deeper causal analysis or propose concrete mitigation strategies.
- What evidence would resolve it: Detailed feature-level analysis comparing B/G and W, identifying specific domain-specific cues or stylistic differences that hinder adaptation, and experiments testing targeted adaptation techniques.

### Open Question 3
- Question: What is the relative contribution of each component (contrastive learning, MMD, TTA) under varying levels of domain shift, and how can their integration be optimized for specific adaptation scenarios?
- Basis in paper: [explicit] The ablation study shows MMD contributes most on Twitter-COMMs and TTA on NewsCLIPpings, but the paper does not explore how component importance shifts with domain similarity or data characteristics.
- Why unresolved: The study only evaluates component contributions in fixed settings without varying the degree of domain shift or dataset properties, leaving open questions about adaptive weighting or conditional component activation.
- What evidence would resolve it: Experiments manipulating domain similarity (e.g., controlled topic overlap) and dataset characteristics (e.g., writing style variance), measuring component impact across the spectrum of adaptation difficulty.

## Limitations
- The paper focuses only on topic and agency domain shifts, not regional, linguistic, or cultural variations
- TTA implementation details are underspecified, particularly regarding the amount of unlabeled target data needed
- The effectiveness of Gaussian blur augmentation for all types of domain shifts is not thoroughly validated

## Confidence
- High confidence: The overall framework combining contrastive learning, MMD, and TTA is sound and the reported performance improvements are likely reproducible
- Medium confidence: The specific choice of Gaussian blur augmentation and its effectiveness across diverse news domains
- Medium confidence: The MMD implementation details and their precise impact on domain adaptation performance

## Next Checks
1. Perform ablation study isolating MMD contribution by training with and without MMD loss while keeping other components constant
2. Test TTA effectiveness with varying amounts of unlabeled target domain data (10%, 50%, 100%) to determine minimum requirements
3. Compare Gaussian blur augmentation against alternative strategies (color jitter, rotation, cutout) to validate its effectiveness for out-of-context news detection