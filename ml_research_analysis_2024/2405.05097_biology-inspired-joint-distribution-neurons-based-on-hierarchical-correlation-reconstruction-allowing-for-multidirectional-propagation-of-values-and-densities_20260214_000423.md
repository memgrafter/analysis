---
ver: rpa2
title: Biology-inspired joint distribution neurons based on Hierarchical Correlation
  Reconstruction allowing for multidirectional propagation of values and densities
arxiv_id: '2405.05097'
source_url: https://arxiv.org/abs/2405.05097
tags:
- information
- distribution
- variables
- distributions
- moments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HCRNN extends KAN with neurons modeling joint distributions as
  linear combinations of orthonormal polynomials, enabling bidirectional propagation
  of values and probability distributions while allowing local training. Using Hierarchical
  Correlation Reconstruction (HCR), it approximates joint densities and enables estimation
  of mutual information between variables through simple sums of squared mixed moments.
---

# Biology-inspired joint distribution neurons based on Hierarchical Correlation Reconstruction allowing for multidirectional propagation of values and densities

## Quick Facts
- arXiv ID: 2405.05097
- Source URL: https://arxiv.org/abs/2405.05097
- Reference count: 25
- Key outcome: HCRNN extends KAN with neurons modeling joint distributions as linear combinations of orthonormal polynomials, enabling bidirectional propagation of values and densities while allowing local training

## Executive Summary
This paper introduces HCRNN (Hierarchical Correlation Reconstruction Neural Networks), which extends Kolmogorov-Arnold Networks (KAN) by modeling local joint distributions within neurons using orthonormal polynomial expansions. The key innovation allows neurons to propagate both values and probability distributions bidirectionally through variable substitution and normalization. This biological-inspired approach enables three properties missing in traditional neurons: bidirectional axonal propagation, handling of probabilistic distributions for risk-aware computation, and support for local training methods like information bottleneck optimization.

## Method Summary
HCRNN represents joint distributions as linear combinations of orthonormal polynomials (typically Legendre) over normalized variables in [0,1]^d. This allows neurons to model complex dependencies and propagate conditional distributions by substituting known variables and normalizing. The method uses Hierarchical Correlation Reconstruction to approximate joint densities, enabling estimation of mutual information through simple sums of squared mixed moments. Training can be performed through direct estimation, tensor decomposition, or information bottleneck optimization, with the latter allowing local training of intermediate layers to prevent overfitting while preserving information flow.

## Key Results
- HCRNN outperforms standard approaches in predicting conditional distributions by modeling full joint densities rather than pairwise dependencies
- The method enables information-theoretic training by optimizing intermediate layers through mutual information maximization and compression
- Mixed moment coefficients provide interpretable representations of variable dependencies within neurons
- Tensor decomposition reduces computational complexity while maintaining representational power for high-dimensional problems

## Why This Works (Mechanism)

### Mechanism 1
HCRNN neurons model joint distributions using orthonormal polynomial expansions, enabling bidirectional value propagation. The neuron represents the joint density as ρ(x) = Σⱼ aⱼfⱼ(x) using orthonormal polynomials over normalized variables [0,1]ᵈ. This allows substitution of known variables and normalization to obtain conditional densities for remaining variables.

### Mechanism 2
HCRNN enables propagation of probability distributions through neurons, not just point values. By representing inputs as probability distributions (vectors of moments), HCRNN can propagate entire distributions through neurons. The propagated density becomes an integral over the conditional density weighted by the input distribution.

### Mechanism 3
HCRNN enables local training methods like information bottleneck by providing easy mutual information estimation. With orthonormal polynomial basis, mutual information can be estimated as a simple sum of squared mixed moments between variables: I(X;Y) ≈ Σⱼₓ∈B⁺ Σⱼᵧ∈B⁺ (a(jx,jy))².

## Foundational Learning

- Concept: Orthonormal polynomial basis and normalization to [0,1]
  - Why needed here: The entire HCRNN approach depends on representing joint distributions as linear combinations of orthonormal basis functions. Without proper normalization and orthonormal basis, the coefficients won't have the simple moment interpretations needed for bidirectional propagation and mutual information estimation.
  - Quick check question: Why do we need to normalize variables to [0,1] before applying Legendre polynomials in HCRNN?

- Concept: Joint distribution modeling and conditional probability
  - Why needed here: HCRNN's core innovation is modeling joint distributions within neurons, which enables bidirectional propagation by substituting known variables and computing conditional distributions for unknown ones.
  - Quick check question: How does substituting known variables and normalizing allow HCRNN to compute conditional distributions?

- Concept: Mutual information and information bottleneck
  - Why needed here: The paper proposes using information bottleneck for local training of intermediate layers, which requires estimating mutual information between layer activations and target outputs.
  - Quick check question: Why is estimating mutual information between intermediate layer activations and target outputs useful for preventing overfitting?

## Architecture Onboarding

- Component map: Input normalization → Joint distribution modeling → Bidirectional propagation → Mutual information estimation → Information bottleneck training → Output un-normalization

- Critical path: Input normalization → Joint distribution modeling → Bidirectional propagation → Mutual information estimation → Information bottleneck training → Output un-normalization

- Design tradeoffs:
  - Higher polynomial degree vs computational cost: Higher degrees capture more complex dependencies but exponentially increase the number of coefficients
  - Full joint distribution vs pairwise dependencies: Full modeling captures all dependencies but is computationally expensive; pairwise (KAN-like) is cheaper but may miss important higher-order interactions
  - Information bottleneck vs standard backpropagation: Information bottleneck can prevent overfitting and enable local training but requires mutual information estimation and careful hyperparameter tuning

- Failure signatures:
  - Negative density values: Linear combination can produce negative values; requires calibration or inter-layer normalization
  - Poor generalization: Too high polynomial degree or insufficient regularization leads to overfitting
  - Training instability: Information bottleneck requires careful tuning of β parameter and batch size
  - Bidirectional propagation errors: Incorrect normalization or basis orthogonality breaks conditional probability calculations

- First 3 experiments:
  1. Implement a single HCR neuron with degree-2 polynomials and test bidirectional propagation on a simple 2D dataset with known conditional distributions
  2. Compare KAN vs HCRNN on a regression task, measuring both accuracy and ability to propagate probability distributions
  3. Implement information bottleneck training on a classification task, comparing with standard backpropagation and measuring mutual information between layers and outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the practical computational complexity of training HCRNN with tensor decomposition approaches compared to standard backpropagation?
- Basis in paper: [explicit] Section IV mentions tensor decomposition as a training approach and notes it "should be combined with reductions like tensor decomposition" but doesn't provide empirical complexity comparisons
- Why unresolved: The paper discusses tensor decomposition theoretically but lacks concrete benchmarks showing whether the O(n^3) to O(n^2.37) savings mentioned for kernel methods translate to practical HCRNN training scenarios
- What evidence would resolve it: Empirical runtime comparisons showing training times for HCRNN with tensor decomposition versus standard backpropagation across different dataset sizes and model architectures

### Open Question 2
- Question: How does the HCRNN's ability to handle missing data compare quantitatively to other methods in real-world scenarios?
- Basis in paper: [explicit] Section II-E states "Working on missing data by using to estimate/update/propagate only a_j coefficients with zero indexes" but provides no empirical validation
- Why unresolved: The paper claims this is a theoretical advantage but doesn't demonstrate it with benchmark datasets containing missing values or compare against established imputation methods
- What evidence would resolve it: Experimental results showing prediction accuracy and information preservation when different percentages of data are missing, compared to methods like mean imputation or multiple imputation

### Open Question 3
- Question: What is the optimal basis selection strategy for HCRNN across different problem domains?
- Basis in paper: [explicit] Section II-E discusses basis selection challenges and mentions "calculate more coefficients and discard those close to zero" but doesn't provide domain-specific recommendations
- Why unresolved: The paper acknowledges the difficulty of basis selection (exponential growth of coefficients) but only offers generic suggestions without systematic evaluation of different strategies across problem types
- What evidence would resolve it: Comparative studies showing performance of different basis selection methods (polynomial degree, pairwise restrictions, l1 normalization) across multiple problem domains with varying dimensionality and correlation structures

## Limitations
- Lack of empirical validation against state-of-the-art methods and limited benchmark comparisons
- Practical implementation challenges with normalization stability and basis selection are not fully addressed
- Computational complexity of high-degree polynomial expansions in multiple dimensions is acknowledged but mitigation strategies lack empirical demonstration

## Confidence

- Mechanism 1 (Bidirectional propagation): Medium - The mathematical framework is sound but practical implementation challenges (normalization stability, basis selection) are not fully addressed
- Mechanism 2 (Distribution propagation): Low - The moment-based representation is theoretically valid but higher-order moment propagation accuracy is not demonstrated
- Mechanism 3 (Local training via mutual information): Medium - The approximation formula is provided but empirical validation of its effectiveness for preventing overfitting is limited

## Next Checks

1. Implement a controlled experiment comparing HCRNN's mutual information estimation against exact calculation on synthetic datasets with known dependencies, measuring approximation error across different coefficient magnitudes

2. Benchmark HCRNN against standard information bottleneck implementations on a classification task, measuring both accuracy and mutual information between intermediate layers and outputs

3. Test bidirectional propagation robustness by introducing noise and distribution shifts to input variables, measuring how well conditional distributions are preserved