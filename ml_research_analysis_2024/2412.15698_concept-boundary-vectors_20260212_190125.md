---
ver: rpa2
title: Concept Boundary Vectors
arxiv_id: '2412.15698'
source_url: https://arxiv.org/abs/2412.15698
tags:
- concept
- vectors
- boundary
- vector
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces concept boundary vectors as an alternative
  construction of concept vectors, which are derived from the boundary between latent
  representations of concepts. The key idea is to use the geometry of this boundary
  to create a vector that better captures the relationship between concepts.
---

# Concept Boundary Vectors

## Quick Facts
- arXiv ID: 2412.15698
- Source URL: https://arxiv.org/abs/2412.15698
- Authors: Thomas Walker
- Reference count: 40
- Key outcome: Concept boundary vectors outperform concept activation vectors at encoding semantic meaning and representing concept entanglement in latent space.

## Executive Summary
This paper introduces concept boundary vectors as an alternative to concept activation vectors for interpreting neural network representations. The key innovation is constructing vectors from the boundary geometry between latent concept representations rather than from the overall direction between concept clusters. Through empirical validation on MNIST and CIFAR10 datasets, the authors demonstrate that concept boundary vectors better capture semantic relationships and are more robust to concept entanglement. The work also validates key assumptions underlying concept vectors using topological data analysis, showing that boundary complexity and concept homogeneity significantly influence vector effectiveness.

## Method Summary
The method involves extracting latent representations from pre-trained models, identifying boundary pairs between concept activations using Algorithm 1, and optimizing concept boundary vectors to maximize similarity to boundary normal vectors. The approach is validated through logit influence calculations, concept entanglement analysis using cosine similarity and topological methods, and concept algebra success rates. The authors compare their boundary vectors against traditional concept activation vectors across CNN and vision transformer architectures.

## Key Results
- Concept boundary vectors show greater logit influence on target classes compared to concept activation vectors
- Topological analysis reveals more distinct features for concept boundary vectors versus CAVs
- Logit influence is negatively correlated with boundary complexity, validating the importance of boundary geometry

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concept boundary vectors are more effective at encoding semantic meaning because they are constructed from the geometry of the boundary between latent concept representations.
- Mechanism: The optimization objective maximizes cosine similarity to boundary normal vectors derived from pairs of latent activations closest to the boundary between concepts. This is more restrictive than the linear classification objective used for CAVs.
- Core assumption: The boundary geometry between concepts encodes more semantic information than the overall direction from one concept cluster to another.
- Evidence anchors:
  - [abstract] "concept boundary vectors are more effective than concept activation vectors at encoding semantic meaning and representing concept entanglement in latent space"
  - [section] "From Figure 2a we observe that in almost all cases the concept boundary vector has a greater logit influence on the target class"
  - [corpus] Weak evidence - related work focuses on CAVs and SAEs but doesn't directly address boundary geometry
- Break condition: If concepts are not linearly separable or if the boundary geometry is too complex to capture with a single vector

### Mechanism 2
- Claim: Concept boundary vectors better represent concept entanglement because they capture the topological structure of concept relationships.
- Mechanism: By optimizing to be similar to boundary normal vectors, concept boundary vectors naturally align with the geometric structure of how concepts are arranged in latent space. This creates more distinct topological features that persist through homology groups.
- Core assumption: The topological structure of concept relationships in latent space is meaningful for understanding semantic relationships.
- Evidence anchors:
  - [section] "From Figure 4 it is evident that the topological features of concept boundary vectors are more distinct than those of concept activation vectors"
  - [section] "Figure 5 shows a topological structure within concept activation and boundary vectors. It is perhaps more distinct in the case of concept boundary vectors"
  - [corpus] Weak evidence - related work on CAVs doesn't discuss topological structure
- Break condition: If the topological structure is an artifact of the specific dataset or model architecture rather than a general property

### Mechanism 3
- Claim: Concept boundary vectors generalize better across concept clusters because they are less sensitive to the linear separability assumption.
- Mechanism: By focusing on the boundary geometry rather than requiring complete linear separation, concept boundary vectors can capture relationships even when concepts have complex boundaries or internal structure.
- Core assumption: The linear representation hypothesis (A1) is not strictly necessary for capturing meaningful concept relationships.
- Evidence anchors:
  - [section] "From Figure 11 we determine that logit influence and boundary complexity are negatively correlated"
  - [section] "Concept boundary vectors seem to be capturing more of the semantic meaning present in these concept-concept relationships"
  - [corpus] Weak evidence - related work assumes linear separability without testing this assumption
- Break condition: If the boundary complexity becomes so high that no single vector can meaningfully capture the relationship

## Foundational Learning

- Concept: Topological data analysis and homology groups
  - Why needed here: Used to verify the linear separability assumption and measure boundary complexity
  - Quick check question: How does the sum of H0 lifetimes relate to the complexity of a decision boundary?

- Concept: Persistent homology and geodesic distance
  - Why needed here: Applied to concept vectors on the unit sphere to study their topological properties
  - Quick check question: What does a persistent 1-dimensional feature represent in the context of concept vectors?

- Concept: Mapper algorithm and simplicial complexes
  - Why needed here: Used to create low-dimensional graphical representations of concept vector relationships
  - Quick check question: How does the number of cubes in the cover affect the resolution of the topological features identified?

## Architecture Onboarding

- Component map: Pre-trained model and concept-labeled datasets -> Algorithm 1 for boundary identification -> Optimization for concept boundary vectors -> Topological data analysis tools, Mapper algorithm, logit influence calculations -> Concept boundary vectors and their properties

- Critical path:
  1. Extract latent representations for concepts
  2. Run Algorithm 1 to identify boundary pairs
  3. Optimize for concept boundary vector
  4. Verify assumptions using topological analysis
  5. Compare with concept activation vectors

- Design tradeoffs:
  - Algorithm 1 has O(n²) complexity but provides more faithful representations
  - Fewer boundary pairs in later layers may cause overfitting but captures refined relationships
  - Topological analysis is computationally expensive but validates assumptions

- Failure signatures:
  - Concept boundary vectors show no improvement over concept activation vectors
  - Topological analysis reveals no meaningful structure
  - Optimization fails to converge or produces vectors with low logit influence

- First 3 experiments:
  1. Replicate MNIST experiment to verify concept boundary vectors outperform CAVs on a simple CNN
  2. Test on a vision transformer to see how concept vectors evolve through layers
  3. Apply to a language model to check if boundary geometry captures semantic relationships in text embeddings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of concept boundary vectors depend on the dimensionality of the latent space, and if so, what is the relationship?
- Basis in paper: [inferred] The paper shows that Algorithm 1's runtime depends linearly on the dimension of the latent representations (Figure 13b), suggesting a potential relationship between dimensionality and effectiveness.
- Why unresolved: The paper doesn't explicitly analyze how concept boundary vector performance scales with latent space dimensionality or explore potential dimensionality-dependent limitations.
- What evidence would resolve it: Empirical studies comparing concept boundary vector effectiveness across models with varying latent space dimensions, particularly examining performance degradation thresholds.

### Open Question 2
- Question: Are there specific geometric properties of the decision boundary that make concept boundary vectors more effective than concept activation vectors?
- Basis in paper: [explicit] The paper shows that logit influence and boundary complexity are negatively correlated (Figure 11), suggesting boundary geometry influences concept vector effectiveness.
- Why unresolved: While the paper establishes a correlation between boundary complexity and concept vector effectiveness, it doesn't identify which specific geometric properties (e.g., curvature, fractal dimension) are most influential.
- What evidence would resolve it: Detailed geometric analysis of decision boundaries paired with concept vector performance metrics across diverse datasets and model architectures.

### Open Question 3
- Question: How do concept boundary vectors perform when the linear separability assumption (A1) is violated?
- Basis in paper: [inferred] The paper focuses on verifying assumptions in MNIST and CIFAR10 contexts where linear separability holds, but doesn't test boundary vector performance in non-linearly separable scenarios.
- Why unresolved: The paper's methodology and empirical validation assume valid linear separability, leaving open questions about boundary vector behavior in more complex, real-world scenarios.
- What evidence would resolve it: Experiments on datasets with known non-linear concept relationships (e.g., XOR-like patterns) comparing concept boundary vector and concept activation vector performance.

## Limitations
- Effectiveness primarily demonstrated on vision tasks (MNIST, CIFAR10), with limited validation on other domains
- Computational complexity of Algorithm 1 (O(n²)) may limit scalability to large datasets or high-dimensional latent spaces
- Topological analysis relies on assumptions about the meaningfulness of persistent homology features in this context

## Confidence

**Confidence labels:**
- **High confidence**: Concept boundary vectors show improved logit influence over CAVs; topological analysis reveals distinct features for boundary vectors
- **Medium confidence**: Boundary geometry meaningfully captures semantic relationships; concept boundary vectors generalize better across concept clusters
- **Low confidence**: The approach scales effectively to complex, real-world datasets; topological features directly correspond to semantic meaning

## Next Checks

1. **Scalability test**: Evaluate concept boundary vectors on a large-scale vision dataset (e.g., ImageNet) to assess computational feasibility and performance degradation with increased complexity.

2. **Cross-domain validation**: Apply the methodology to a language model (e.g., BERT) to verify if boundary geometry captures semantic relationships in text embeddings as effectively as in image embeddings.

3. **Ablation study**: Systematically vary the number of boundary pairs used in optimization to determine the trade-off between computational cost and representation quality, particularly for deep layers with fewer boundary pairs.