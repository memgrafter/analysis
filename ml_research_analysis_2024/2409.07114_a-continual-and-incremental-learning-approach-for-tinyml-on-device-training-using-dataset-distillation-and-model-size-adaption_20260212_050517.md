---
ver: rpa2
title: A Continual and Incremental Learning Approach for TinyML On-device Training
  Using Dataset Distillation and Model Size Adaption
arxiv_id: '2409.07114'
source_url: https://arxiv.org/abs/2409.07114
tags:
- data
- accuracy
- learning
- training
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for incremental learning on tinyML
  devices that dynamically adjusts model size based on task complexity while using
  dataset distillation to minimize memory usage. The approach combines data distillation
  with adaptive model sizing, allowing smaller models to be used initially and expanded
  only when needed.
---

# A Continual and Incremental Learning Approach for TinyML On-device Training Using Dataset Distillation and Model Size Adaption

## Quick Facts
- **arXiv ID:** 2409.07114
- **Source URL:** https://arxiv.org/abs/2409.07114
- **Reference count:** 13
- **Primary result:** Proposed method achieves similar accuracy to larger fixed models while using only 43% of FLOPs and requiring 1% of the original dataset size

## Executive Summary
This paper presents a novel approach for incremental learning on tinyML devices that combines dataset distillation with adaptive model sizing. The method allows models to start small and dynamically expand based on task complexity, enabling efficient on-device training with minimal memory and computational resources. The approach is validated across five diverse datasets including CIFAR10, MNIST, CORe50, HAR, and Speech Commands, demonstrating significant resource savings while maintaining competitive accuracy.

## Method Summary
The proposed method integrates dataset distillation with adaptive model sizing to enable incremental learning on resource-constrained tinyML devices. The system starts with smaller models and only expands them when additional complexity is needed, rather than using fixed larger models from the start. Dataset distillation compresses the training data to just 1% of the original size while preserving essential learning information. This combination allows for continual learning on-device with significantly reduced memory and computational requirements compared to traditional approaches that use static model sizes.

## Key Results
- Achieved similar accuracy to larger fixed models while using only 43% of FLOPs
- Required just 1% of the original dataset size through dataset distillation
- Maintained performance across five diverse datasets (CIFAR10, MNIST, CORe50, HAR, Speech Commands)
- Adaptive model approach significantly reduced computational resources compared to static model approaches

## Why This Works (Mechanism)
The approach works by combining two complementary techniques: dataset distillation reduces memory requirements by compressing essential training information into a small synthetic dataset, while adaptive model sizing ensures computational efficiency by only using larger models when task complexity demands it. This dynamic scaling prevents the waste of resources on simple tasks that smaller models can handle effectively, while still maintaining the capacity to learn complex tasks through model expansion when needed.

## Foundational Learning
- **Dataset Distillation:** Creates compact synthetic datasets that preserve essential learning information - needed to reduce memory footprint from 100% to 1% of original data
- **Model Scaling:** Dynamically adjusting neural network size based on task requirements - needed to optimize computational resources (43% FLOPs reduction)
- **Incremental Learning:** Learning new tasks without forgetting previous ones - needed for continual learning on tinyML devices
- **TinyML Constraints:** Understanding memory, compute, and power limitations of edge devices - needed to ensure practical deployment feasibility
- **Adaptive Architecture:** Systems that can modify their structure during operation - needed for the model expansion mechanism
- **Computational Complexity Analysis:** Measuring FLOPs and resource usage - needed to quantify efficiency improvements

## Architecture Onboarding
- **Component Map:** Dataset Distillation -> Model Size Adaption Controller -> Neural Network Architecture -> Training Loop
- **Critical Path:** Input Data → Dataset Distillation → Compressed Dataset → Model Training → Model Size Decision → Architecture Update → Performance Evaluation
- **Design Tradeoffs:** Model expansion vs. computational overhead, compression ratio vs. accuracy retention, adaptation frequency vs. energy consumption
- **Failure Signatures:** Performance degradation when model expansion is too conservative, accuracy loss when dataset distillation is too aggressive, resource exhaustion during adaptation
- **First Experiments:** 1) Benchmark accuracy on each dataset with varying compression ratios, 2) Measure adaptation trigger accuracy and timing across datasets, 3) Quantify computational overhead of the adaptation mechanism on target hardware

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on standard benchmark datasets rather than real-world noisy, heterogeneous data streams
- Dataset distillation effectiveness across diverse data modalities beyond vision and speech is not thoroughly explored
- Computational overhead of the adaptation mechanism itself is not explicitly quantified in actual tinyML hardware contexts

## Confidence
- **Accuracy claims on benchmark datasets:** High
- **Resource reduction claims:** Medium (well-measured but limited hardware context)
- **Generalization to real-world scenarios:** Low
- **Adaptation mechanism reliability:** Low-Medium

## Next Checks
1. Deploy the system on actual tinyML hardware (MCU/Edge devices) to measure real-world latency, power consumption, and memory usage
2. Test the adaptation mechanism under realistic conditions with noisy, non-stationary data streams to evaluate stability and decision-making
3. Evaluate performance degradation when dataset distillation is applied to significantly smaller dataset subsets (<1%) and across more diverse data modalities beyond vision and speech