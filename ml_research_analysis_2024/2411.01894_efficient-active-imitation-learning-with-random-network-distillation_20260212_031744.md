---
ver: rpa2
title: Efficient Active Imitation Learning with Random Network Distillation
arxiv_id: '2411.01894'
source_url: https://arxiv.org/abs/2411.01894
tags:
- expert
- rnd-dagger
- learning
- policy
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of active imitation learning in
  complex domains where human demonstrations are costly and expert interventions need
  to be minimized. It introduces RND-DAgger, which uses Random Network Distillation
  to identify out-of-distribution states that require expert intervention, combined
  with a minimal demonstration time mechanism to ensure consistent expert guidance.
---

# Efficient Active Imitation Learning with Random Network Distillation

## Quick Facts
- arXiv ID: 2411.01894
- Source URL: https://arxiv.org/abs/2411.01894
- Authors: Emilien Biré; Anthony Kobanda; Ludovic Denoyer; Rémy Portelas
- Reference count: 21
- Primary result: RND-DAgger achieves comparable or superior task performance to existing methods while significantly reducing expert time and context switches

## Executive Summary
This paper introduces RND-DAgger, an active imitation learning method that combines Random Network Distillation (RND) with a minimal expert time mechanism to reduce costly expert interventions. The method identifies out-of-distribution states using RND error and triggers expert intervention only when necessary, while ensuring continuous expert guidance through a minimal demonstration time constraint. Experiments across three environments (racing, navigation, and robotics) demonstrate that RND-DAgger significantly reduces expert involvement while maintaining or improving task performance compared to existing DAgger variants.

## Method Summary
RND-DAgger is an active imitation learning algorithm that uses Random Network Distillation to detect out-of-distribution states and trigger expert interventions only when necessary. The method maintains a fixed random target network and a learned predictor network, with high prediction error indicating OOD states. A minimal expert time mechanism ensures continuous expert guidance rather than fragmented interventions. The algorithm collects trajectories using the current policy, queries expert intervention based on RND error thresholds, aggregates demonstrations, and retrains the policy iteratively. This approach significantly reduces the number of expert context switches and total expert time required while achieving comparable or superior task performance to existing methods.

## Key Results
- RND-DAgger reduces expert context switches by up to 80% compared to standard DAgger across all three environments
- The method achieves comparable or superior task performance (lap completion rate, navigation success, cumulative reward) while using significantly fewer expert demonstrations
- RND-DAgger demonstrates robustness to expert action variability, maintaining performance even when expert demonstrations vary for the same state

## Why This Works (Mechanism)

### Mechanism 1
Random Network Distillation identifies out-of-distribution states by measuring prediction error between a fixed random target network and a learned predictor network. The fixed random network acts as a static reference while the predictor learns to minimize the difference. In-distribution states have low prediction error, while OOD states yield high error, triggering expert intervention. This works because states rare or absent in training data produce high RND error, indicating a need for expert guidance.

### Mechanism 2
Minimal Expert Time reduces frequent context switches by enforcing a minimum number of consecutive timesteps below the OOD threshold before returning control to the policy. When the OOD measure falls below threshold, a counter is incremented, and only after reaching a minimum duration does the algorithm switch back to the policy. This ensures continuous expert control rather than brief, fragmented interventions, which is more effective for recovering from OOD states.

### Mechanism 3
RND-DAgger is robust to noisy or variable expert actions because it triggers interventions based solely on state novelty, not action discrepancy. Unlike Ensemble-DAgger or Lazy-DAgger that compare expert and policy actions, RND-DAgger only queries the expert when the RND error exceeds threshold, making it invariant to expert action variability. This works because expert actions may differ across demonstrations for the same state, but the underlying state distribution remains consistent enough for RND error to be meaningful.

## Foundational Learning

- **Out-of-distribution detection**: Needed to decide when expert intervention is truly necessary rather than relying on frequent queries. Quick check: What happens if the OOD threshold λ is set too low?
- **Behavioral Cloning**: The base policy learning algorithm used in RND-DAgger, updating the policy on the aggregated dataset. Quick check: Why does BC alone suffer from distributional shift in sequential decision-making?
- **Active learning / expert-in-the-loop training**: The approach to minimize expert time while maximizing policy improvement by selectively requesting demonstrations only in critical situations. Quick check: How does DAgger differ from standard BC in terms of dataset growth?

## Architecture Onboarding

- **Component map**: Environment simulator -> Base policy πi -> RND predictor fpred -> RND target ftarg -> Expert policy πexp -> OOD threshold λ, MET counter W, state history
- **Critical path**: 1) Initialize D with expert demonstrations. 2) Train π0 on D via BC. 3) For each iteration: collect T new state-action pairs, compute mt = ||ftarg(st) - fpred(st)||², switch to expert control if mt > λ or w < W, add (at, st) to Di, increment w, else run policy, reset w. 4) Merge Di into D, retrain πi+1, retrain fpred on updated D.
- **Design tradeoffs**: Low λ → more expert queries, potentially better performance but higher burden. High W → fewer, longer expert interventions; may delay recovery. Larger state history → better OOD detection in temporally correlated tasks; higher computational cost.
- **Failure signatures**: Excessive context switches: λ too low or W too small. Poor policy performance: λ too high, missing critical OOD states. Slow learning: W too high, delaying expert corrections.
- **First 3 experiments**: 1) Run RND-DAgger on a simple grid navigation task with a known OOD region; verify expert intervention triggers only in that region. 2) Vary λ on RaceCar; measure context switches and final lap completion rate. 3) Set W to extreme values (1 and 50) on HalfCheetah; observe effect on cumulative reward and number of expert handovers.

## Open Questions the Paper Calls Out

### Open Question 1
How would RND-DAgger perform in environments with partial observability or pixel-based observations where state representations are noisy or high-dimensional? The paper mentions studying how to scale RND-DAgger to more complex environments such as Partially observable MDPs and notes that noisy-TV problems and distracting random observations bring new challenges that may be addressed through representation learning or explicit high-entropy filtering. This remains unresolved as the current evaluation is limited to environments with explicit state representations rather than raw pixel observations.

### Open Question 2
What is the optimal strategy for determining when to trigger predictive alerts versus immediate expert intervention in safety-critical real-time environments? The paper states that RND-DAgger requires the expert to immediately take control of the agent, which may be impossible or dangerous in complex real-time environments, and suggests predictive approaches to alert the expert before intervention as an interesting area of improvement. This limitation is acknowledged but not addressed experimentally.

### Open Question 3
How does the performance of RND-DAgger scale with the number of expert policies or the diversity of expert behaviors in the training dataset? The paper evaluates RND-DAgger against single oracle policies and doesn't explore scenarios with multiple diverse expert policies. While the paper mentions that "human experts often exhibit variability," it doesn't investigate how performance changes when multiple expert policies with different strategies or behaviors are available for learning.

## Limitations

- Dependence on RND error as a reliable OOD indicator may degrade in high-dimensional or highly stochastic environments
- Lack of ablation studies on state history length and RND network architecture choices leaves uncertainty about design sensitivity
- Claims about robustness to expert action variability are not substantiated with direct experimental evidence or citations

## Confidence

- **High Confidence**: The mechanism of using RND error for OOD detection and the MET framework for reducing context switches are well-grounded and logically sound
- **Medium Confidence**: Empirical results showing reduced expert time and comparable performance across three tasks are convincing but may not generalize to domains with fundamentally different state dynamics
- **Low Confidence**: Claims about robustness to expert action variability are not substantiated with direct experimental evidence or citations

## Next Checks

1. **Ablation on State History**: Test RND-DAgger with varying state history lengths (1, 5, 10) on the racing environment to determine the optimal temporal context for OOD detection
2. **RND Architecture Sensitivity**: Compare predictor networks of different widths/depths (e.g., 2-layer vs 4-layer MLPs) to assess impact on OOD detection accuracy and policy performance
3. **Generalization Test**: Apply RND-DAgger to a new domain with known distributional shift (e.g., a maze with multiple distinct regions) to verify that expert intervention correctly follows OOD boundaries