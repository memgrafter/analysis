---
ver: rpa2
title: Knowledge Editing in Language Models via Adapted Direct Preference Optimization
arxiv_id: '2406.09920'
source_url: https://arxiv.org/abs/2406.09920
tags:
- knowledge
- edit
- edits
- editing
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Knowledge Editing (KE) aims to update specific factual knowledge
  in pre-trained LLMs without expensive retraining. We propose Knowledge Direct Preference
  Optimization (KDPO), a method that treats KE as an alignment problem by using teacher-forced
  generation of negative samples and optimizing with positive samples to maintain
  localized changes.
---

# Knowledge Editing in Language Models via Adapted Direct Preference Optimization

## Quick Facts
- **arXiv ID**: 2406.09920
- **Source URL**: https://arxiv.org/abs/2406.09920
- **Reference count**: 7
- **Primary result**: KDPO achieves state-of-the-art performance across four datasets and three model architectures with 100-500 sequential edits, showing superior locality and edit success rates

## Executive Summary
Knowledge Editing (KE) aims to update specific factual knowledge in pre-trained LLMs without expensive retraining. This paper proposes Knowledge Direct Preference Optimization (KDPO), a method that treats KE as an alignment problem by using teacher-forced generation of negative samples and optimizing with positive samples to maintain localized changes. KDPO achieves state-of-the-art performance across four datasets and three model architectures, with 100-500 sequential edits, showing superior locality and edit success rates compared to baselines. The method preserves LLM performance on general benchmarks while maintaining strong editing precision.

## Method Summary
KDPO adapts the Direct Preference Optimization (DPO) framework for knowledge editing by using teacher-forcing to generate negative samples and optimizing with positive samples to maintain localized changes. The method treats KE as an alignment problem where the new knowledge serves as the positive sample and the current knowledge as the negative sample. Teacher-forcing is used to generate negative samples conditioned on the prompt and new knowledge, reducing semantic drift toward original knowledge. The DPO objective is then optimized using these samples, with the new knowledge as context for both samples to promote term cancellation and localized weight changes. The optimization uses the Adam optimizer with a learning rate of 1e-4, 10 cycles, and 8 steps per cycle.

## Key Results
- KDPO achieves state-of-the-art performance across four datasets (ZsRE, WikiBio, WikiDatacounterfact, WikiDatarecent) and three model architectures (LLaMA3-8B, Qwen1.5-7B, LLaMA2-7B)
- Maintains superior locality metrics, preserving unrelated knowledge better than baseline methods during sequential editing (100-500 edits)
- Demonstrates 100% edit success rate on 500 sequential edits for ZsRE dataset while baselines show significant degradation

## Why This Works (Mechanism)

### Mechanism 1
Teacher-forcing the negative sample generation using the new knowledge reduces semantic drift of the negative sample toward the model's original knowledge. By conditioning negative sample generation on the prompt and new knowledge rather than the model's previous output, the model generates a negative sample closer in semantic space to the new knowledge, reducing retention of associations with original incorrect knowledge.

### Mechanism 2
Optimizing with new knowledge as context for both positive and negative samples reduces gradient terms through token overlap. When the new knowledge sufficiently guides both samples, identical tokens in certain positions cause term cancellation in gradient calculation, leading to fewer weight changes and more localized edits.

### Mechanism 3
The DPO framework's relative likelihood optimization inherently prevents excessive deviation from original weights. By comparing log probabilities of positive and negative samples relative to a reference model, large weight deviations that would decrease positive sample likelihood are naturally discouraged, preserving pre-trained knowledge.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: Core optimization framework used in KDPO. Understanding DPO is essential as KDPO modifies this framework for knowledge editing. *Quick check: What is the key difference between DPO and traditional RLHF?*

- **Teacher-forcing**: Technique where model is fed ground truth tokens instead of its own predictions during training. Essential for understanding how KDPO uses teacher-forcing to generate negative samples. *Quick check: What is the main advantage of using teacher-forcing during training compared to using the model's own predictions?*

- **Knowledge Editing (KE)**: Task of updating specific factual knowledge in pre-trained LLMs. Understanding KE challenges and metrics is essential to grasp KDPO's significance. *Quick check: What are the key challenges in knowledge editing, and how do they differ from traditional fine-tuning?*

## Architecture Onboarding

- **Component map**: Input (Prompt + New Knowledge) -> Teacher-forcing module (generates Negative Sample) -> DPO optimization module (optimizes weights) -> Output (Edited Model)

- **Critical path**: 1. Generate negative sample using teacher-forcing 2. Optimize model using DPO objective with positive and negative samples 3. Repeat for multiple cycles and steps

- **Design tradeoffs**: Teacher-forcing reduces semantic drift but may introduce bias vs. model-generated samples; new knowledge context leads to localized changes but may be less stable than original prompt context

- **Failure signatures**: Edit success decreases over sequential edits; locality decreases indicating unintended changes to unrelated knowledge; fluency decreases showing repetitive or incoherent outputs

- **First 3 experiments**: 1. Ablation study comparing KDPO with and without teacher-forcing for negative sample generation 2. Comparison of KDPO with different Î² parameter values in DPO objective 3. Evaluation of KDPO on small language model (Qwen1.5-0.5B) to assess scalability

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the research:

## Limitations
- Implementation details for baseline methods are not provided, making fair comparison difficult
- EasyEdit benchmark may not capture all real-world knowledge editing scenarios, particularly complex reasoning tasks
- Long-term stability of edits beyond sequential editing experiments is not investigated

## Confidence
- **High Confidence**: Core mechanism of teacher-forcing negative samples and optimizing with positive samples is well-supported by experimental results across multiple datasets and architectures
- **Medium Confidence**: Claim about preserving pre-trained knowledge better than baselines is supported by locality metrics but limited to specific test sets
- **Low Confidence**: Assertion of being "first KE method to achieve state-of-the-art performance" is difficult to verify without baseline implementation details

## Next Checks
1. Reproduce baseline implementations (AdaLoRA, ROME, FT-L, FT-M, MEMIT) with same training procedures and hyperparameters for fair comparison
2. Design experiments specifically targeting edge cases where edited knowledge might interfere with related but distinct facts
3. Conduct experiments tracking edit success and locality metrics over extended sequential editing (1000+ edits) and after periods of normal model usage