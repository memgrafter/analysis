---
ver: rpa2
title: A Little Human Data Goes A Long Way
arxiv_id: '2410.13098'
source_url: https://arxiv.org/abs/2410.13098
tags:
- data
- uni00000013
- synthetic
- uni00000003
- uni00000057
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how much synthetic data can replace human
  annotation in Fact Verification (FV) and Question Answering (QA) tasks. The authors
  incrementally replace human-generated training data with synthetic data and find
  that replacing up to 90% of the training data only marginally decreases performance,
  but replacing the final 10% leads to severe declines.
---

# A Little Human Data Goes A Long Way

## Quick Facts
- arXiv ID: 2410.13098
- Source URL: https://arxiv.org/abs/2410.13098
- Authors: Dhananjay Ashok; Jonathan May
- Reference count: 40
- Primary result: Up to 90% of human annotation can be replaced with synthetic data in Fact Verification and QA tasks, but the final 10% is disproportionately valuable

## Executive Summary
This paper investigates the trade-off between synthetic and human-generated data in Fact Verification (FV) and Question Answering (QA) tasks. The authors find that synthetic data can replace up to 90% of human annotation without significant performance degradation, but the final 10% of human data provides disproportionate value. Remarkably, models trained on purely synthetic data can be significantly improved by adding as few as 125 human-generated data points. The study also reveals that matching the performance gain of just 200 human data points requires an order of magnitude more synthetic data, suggesting that human annotation remains cost-effective even when synthetic data is readily available.

## Method Summary
The authors generate synthetic data from evidence texts using few-shot in-context learning with GPT-3.5-Turbo or GPT-4, creating (claim, label) pairs for FV and (question, answer) pairs for QA. They fine-tune Llama3-8B and Mistral7B models on datasets with varying proportions of synthetic data (0% to 100% in increments) while holding total training size constant. Performance is evaluated on human-generated test sets using accuracy for FV and BLEU, Exact Match, and other metrics for QA. The trade-off between human and synthetic data is quantified by determining how many additional synthetic points are needed to match the performance gain of 200 human points.

## Key Results
- Replacing up to 90% of human training data with synthetic data only marginally decreases performance in both FV and QA tasks
- The final 10% of human data provides disproportionate value, with severe performance drops when exceeded
- As few as 125 human-generated data points can reliably improve models trained on purely synthetic data
- Matching the performance gain of 200 human data points requires 10x more synthetic data (e.g., 17,000+ synthetic points for WANLI)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data can replace up to 90% of human annotation in Fact Verification (FV) and Question Answering (QA) tasks without significant performance degradation.
- Mechanism: Models learn task-relevant patterns from synthetic data, which closely mimics the structure and content of human-generated data when grounded in diverse evidence texts.
- Core assumption: The synthetic data generation pipeline produces claims and questions that are diverse and representative of the task distribution.
- Evidence anchors:
  - [abstract] "replacing up to 90% of the training data only marginally decreases performance"
  - [section] "Across all datasets, using purely synthetic data leads to worse performance than the same amount of human data"
  - [corpus] Weak evidence; no direct mention in corpus neighbors
- Break condition: Performance drops sharply when synthetic proportion exceeds 90%, indicating synthetic data lacks some quality or characteristic present in human data.

### Mechanism 2
- Claim: Even a small proportion (2.5%) of human-generated data significantly improves model performance trained on synthetic data.
- Mechanism: Human-generated data provides unique information or patterns that synthetic data cannot fully replicate, leading to improved generalization.
- Core assumption: Human-generated data contains distinct features or characteristics that are crucial for optimal model performance.
- Evidence anchors:
  - [abstract] "models trained on purely synthetic data can be reliably improved by including as few as 125 human generated data points"
  - [section] "there is a significant difference between the performance of models on 97.5% and 100% synthetic data"
  - [corpus] Weak evidence; no direct mention in corpus neighbors
- Break condition: When the synthetic data generation pipeline perfectly mimics human data, the performance gap between synthetic and human data may disappear.

### Mechanism 3
- Claim: Matching the performance gain of a small amount of human data requires an order of magnitude more synthetic data.
- Mechanism: Human data is more efficient in conveying task-relevant information, making it a cost-effective solution despite higher annotation costs.
- Core assumption: The quality and efficiency of information in human-generated data is superior to synthetic data on a per-point basis.
- Evidence anchors:
  - [abstract] "matching the performance gain of just a little additional human data (only 200 points) requires an order of magnitude more synthetic data"
  - [section] "On WANLI (Figure 4), more than 17,000 additional synthetic points are needed to achieve the performance gains of 200 human points"
  - [corpus] Weak evidence; no direct mention in corpus neighbors
- Break condition: If synthetic data generation becomes significantly more efficient or human annotation costs decrease substantially, the cost-effectiveness balance may shift.

## Foundational Learning

- Concept: Fact Verification (FV) and Evidence-based Question Answering (QA)
  - Why needed here: These are the primary tasks being studied, and understanding their nature is crucial for interpreting the results and mechanisms.
  - Quick check question: What is the key difference between Fact Verification and Question Answering tasks?

- Concept: Synthetic data generation from evidence texts
  - Why needed here: This is the core method used in the study, and understanding its process is essential for grasping the findings.
  - Quick check question: How does the synthetic data generation pipeline create (claim, label) pairs or (question, answer) pairs from evidence texts?

- Concept: Out-of-distribution (OOD) experiments
  - Why needed here: OOD experiments help determine if the results are due to spurious correlations or genuine performance improvements from human data.
  - Quick check question: What is the purpose of conducting OOD experiments in this study?

## Architecture Onboarding

- Component map: Data generation pipeline (prompt models, few-shot learning) -> Fine-tuning models (Llama3-8B, Mistral, MPT) -> Evaluation metrics (accuracy for FV, BLEU for QA) -> Experiment setup (varying synthetic proportions, fixed dataset sizes)
- Critical path: 1. Generate synthetic data from evidence texts 2. Create training datasets with varying proportions of synthetic data 3. Fine-tune models on these datasets 4. Evaluate performance on human-generated test sets
- Design tradeoffs: Balance between synthetic and human data proportions, Choice of fine-tuning model and prompting strategy, Selection of evaluation metrics
- Failure signatures: Performance drops when synthetic proportion exceeds 90%, Inability to match human data performance even with large amounts of synthetic data
- First 3 experiments: 1. Replicate the main experiment with varying synthetic proportions on a single dataset 2. Conduct an OOD experiment to verify the results 3. Test the impact of different fine-tuning models or prompting strategies on the findings

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The synthetic data generation process is not fully specified, making exact replication challenging
- Results may not generalize to low-resource languages or domains with different data characteristics
- The study focuses on immediate performance impacts without investigating long-term effects of synthetic data usage

## Confidence
- High: 90% replacement threshold and value of small human data proportions (2.5%)
- Medium: Specific trade-off calculations (number of synthetic points needed to match human performance gains)

## Next Checks
1. Test whether the 90% threshold holds when using different synthetic data generation approaches (e.g., smaller models, different prompting strategies, or data augmentation techniques)
2. Conduct systematic ablation studies on what aspects of human data are most valuableâ€”is it the claim diversity, label accuracy, or something else that synthetic data fails to capture beyond the 90% threshold?
3. Evaluate performance on out-of-distribution test sets from different domains to verify that the observed benefits of human data are not due to spurious correlations learned from synthetic data