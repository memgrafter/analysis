---
ver: rpa2
title: Locally Private Sampling with Public Data
arxiv_id: '2411.08791'
source_url: https://arxiv.org/abs/2411.08791
tags:
- qmin
- optimal
- private
- distribution
- mechanism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenge of locally private sampling when\
  \ users possess both private data and public data. It proposes a minimax optimal\
  \ framework that leverages public priors to minimize the worst-case divergence between\
  \ private distributions and their sampled versions, while satisfying \u03B5-LDP\
  \ constraints."
---

# Locally Private Sampling with Public Data

## Quick Facts
- arXiv ID: 2411.08791
- Source URL: https://arxiv.org/abs/2411.08791
- Reference count: 40
- One-line primary result: Proposes a minimax optimal framework for locally private sampling that leverages public priors to minimize worst-case divergence while satisfying ε-LDP constraints.

## Executive Summary
This paper addresses the challenge of locally private sampling when users possess both private data and public data. The authors propose a framework that leverages public priors to minimize the worst-case divergence between private distributions and their sampled versions while satisfying ε-LDP constraints. The key insight is to find a universal optimal mechanism across all f-divergences that preserves the public prior and ensures minimal distortion for the worst-case private distribution. The paper fully characterizes this optimal mechanism for discrete distributions and demonstrates significant improvements over state-of-the-art methods on both synthetic and real-world datasets.

## Method Summary
The method involves computing an optimal ε-LDP mechanism Kq,ε that satisfies two key properties: it preserves the public prior q (qKq,ε = q) and minimizes the worst-case f-divergence between private distributions p and their sampled versions pK. The optimal mechanism is computed using a recursive algorithm (Algorithm 1) with O(n³) complexity for discrete distributions. The algorithm constructs the mechanism by iteratively reducing the problem to smaller subproblems while maintaining the required LDP constraints and public prior preservation. The minimax optimal utility depends only on the smallest element of the public prior qmin, and remarkably, the same mechanism achieves optimality across all f-divergences.

## Key Results
- The optimal mechanism is ε-LDP and preserves the public prior q while minimizing worst-case f-divergence
- The minimax optimal utility Γf(q, ε) depends only on qmin, not the full distribution
- The optimal mechanism is universal across all f-divergences, requiring no knowledge of the specific divergence measure
- Empirical results show up to 9× reduction in maximum TV-distance compared to state-of-the-art methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimal mechanism Kq,ε is ε-LDP and preserves the public prior q by satisfying qKq,ε = q.
- Mechanism: Algorithm 1 constructs Kq,ε recursively, ensuring each row is stochastic, satisfies LDP constraints, and maintains q invariance through normalization and rescaling steps.
- Core assumption: The public prior q is known and can be incorporated without privacy cost, allowing the mechanism to focus solely on protecting private data p.
- Evidence anchors:
  - [abstract] "The mechanism K must satisfy two key properties: (i) K is ε-LDP... (ii) K does not perturb the public prior, that is if p = q, then the resulting sampling distribution is q."
  - [section] "We then propose an algorithm (Algorithm 1) that produces an optimal locally private sampler for any discrete public prior q (Theorem 2)."
- Break condition: If q is not discrete or the support size is too large for O(n³) complexity to be practical.

### Mechanism 2
- Claim: The minimax optimal utility Γf(q, ε) depends only on the smallest element of the public prior qmin, not the full distribution.
- Mechanism: Through recursive reduction and Lemma 1, the worst-case f-divergence is determined by the minimum diagonal entry of the mechanism, which is set based on qmin.
- Core assumption: The f-divergence measure is jointly convex, allowing the worst-case to be evaluated at extreme points (Dirac distributions).
- Evidence anchors:
  - [abstract] "We fully characterize the minimax optimal mechanisms for general f-divergences provided that p and q are discrete distributions."
  - [section] "Building on the equivalence relation in Lemma 1 and the optimal utility derived in Theorem 1, we introduce Algorithm 1..."
- Break condition: If the f-divergence is not jointly convex or if p and q are continuous distributions.

### Mechanism 3
- Claim: The optimal mechanism is universal across all f-divergences, meaning the same Kq,ε achieves optimality regardless of the divergence measure used.
- Mechanism: The construction of Kq,ε in Algorithm 1 ensures that the minimal diagonal entry is set to achieve the lower bound from Proposition 2, which applies to any f-divergence.
- Core assumption: The function g(x) = f(0) + x(f(1/x) - f(0)) is non-increasing for x ∈ [0,1], as proven in Lemma 2.
- Evidence anchors:
  - [abstract] "Remarkably, we demonstrate that this optimal mechanism is universal across all f-divergences."
  - [section] "Theorem 2 also confirms that the choice of f-divergence for measuring distribution dissimilarity does not alter the optimal mechanism."
- Break condition: If the convexity condition for f fails or if the f-divergence does not satisfy the properties assumed in Lemma 2.

## Foundational Learning

- Concept: f-divergence as a measure of distribution dissimilarity
  - Why needed here: The paper frames the privacy-utility tradeoff using f-divergence to quantify how well the sampled distribution approximates the private distribution p.
  - Quick check question: What is the f-divergence between two distributions p and q when f(t) = |t - 1|/2?

- Concept: ε-LDP mechanism and mollifiers
  - Why needed here: The mechanism must satisfy ε-LDP constraints to ensure privacy, and the concept of mollifiers helps characterize the set of distributions that can be sampled from under these constraints.
  - Quick check question: How does the ε-LDP constraint on a mechanism K translate into a bound on the ratio of output probabilities for any two input distributions?

- Concept: Minimax optimization
  - Why needed here: The goal is to find a mechanism that minimizes the worst-case f-divergence over all possible private distributions p, leading to a minimax formulation.
  - Quick check question: In the context of this paper, what does the minimax optimization problem aim to minimize, and over what set of distributions is the maximum taken?

## Architecture Onboarding

- Component map: Public prior q -> Optimal mechanism Kq,ε -> Sampling distribution pK -> Downstream tasks
- Critical path: 1) Receive public prior q and privacy parameter ε, 2) Compute optimal mechanism Kq,ε using Algorithm 1, 3) Apply Kq,ε to private distribution p to obtain sampling distribution pK, 4) Use pK for downstream tasks while preserving privacy
- Design tradeoffs: Complexity vs. optimality (O(n³) algorithm may be prohibitive for very large support sizes), Universality vs. specificity (same mechanism works for all f-divergences but may not be optimal for specific choices), Privacy vs. utility (higher ε allows better utility but weaker privacy guarantees)
- Failure signatures: If q is not discrete, Algorithm 1 may not produce a valid mechanism; if support size n is very large, O(n³) complexity may be prohibitive; if f-divergence doesn't satisfy convexity conditions, universality claim may not hold
- First 3 experiments: 1) Verify Kq,ε satisfies ε-LDP constraints by checking ratio of output probabilities for extreme input distributions, 2) Confirm qKq,ε = q by applying mechanism to public prior, 3) Test universality claim by computing f-divergence between p and pK for different f choices

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal mechanism scale when dealing with continuous distributions rather than discrete ones?
- Basis in paper: [inferred] The paper focuses on discrete distributions and provides a recursive algorithm with O(n³) complexity. However, it doesn't address continuous distributions, which are common in real-world applications.
- Why unresolved: The paper's mathematical framework and recursive algorithm are specifically designed for discrete distributions, and extending this to continuous distributions would require significant theoretical development.
- What evidence would resolve it: A formal extension of the minimax framework to continuous distributions, along with a characterization of the optimal mechanism and its computational complexity.

### Open Question 2
- Question: Can the framework be extended to handle multiple public priors or hierarchical public data structures?
- Basis in paper: [inferred] The paper assumes a single public prior distribution q, but real-world scenarios might involve multiple sources of public data or hierarchical structures.
- Why unresolved: The current formulation and optimal mechanism are specifically designed for a single public prior, and incorporating multiple priors would require a more complex optimization framework.
- What evidence would resolve it: A theoretical framework that extends the minimax optimization to handle multiple public priors, along with an algorithm to compute the optimal mechanism in this setting.

### Open Question 3
- Question: How sensitive is the optimal mechanism to errors or noise in the public prior?
- Basis in paper: [inferred] The paper assumes the public prior q is known exactly, but in practice, public data might be noisy or estimated from samples.
- Why unresolved: The current analysis doesn't consider the robustness of the optimal mechanism to perturbations in the public prior, which is crucial for practical applications.
- What evidence would resolve it: A robustness analysis showing how the optimal utility and mechanism change with varying levels of noise in the public prior, along with bounds on the degradation of performance.

### Open Question 4
- Question: What is the impact of the support size n on the practical utility of the optimal mechanism, especially for large n?
- Basis in paper: [explicit] The paper mentions O(n³) complexity but doesn't provide empirical analysis on how the support size affects utility in real-world scenarios.
- Why unresolved: While the theoretical complexity is known, the practical implications of large support sizes on utility and computation time are not explored in the experiments.
- What evidence would resolve it: Empirical studies showing the relationship between support size, privacy parameter ε, and utility metrics (TV-distance, KL-divergence) across different real-world datasets with varying support sizes.

## Limitations
- The framework is limited to discrete distributions, with no extension to continuous distributions provided
- The O(n³) complexity may be prohibitive for very large support sizes in practical applications
- The analysis assumes the public prior q is known exactly without considering noise or estimation errors

## Confidence
- High confidence: ε-LDP constraint satisfaction and public prior preservation are well-established properties with clear algorithmic implementations
- Medium confidence: Minimax optimality characterization and recursive algorithm construction are mathematically sound but require careful implementation
- Medium confidence: Empirical improvements on synthetic and real-world datasets are promising but need context within broader LDP literature

## Next Checks
1. Implement Algorithm 1 and verify that the resulting mechanism satisfies ε-LDP constraints and q invariance for various discrete public priors with different support sizes
2. Test the mechanism's performance across multiple f-divergences (TV-distance, KL-divergence, Hellinger distance) on synthetic data to confirm the universality claim holds in practice
3. Evaluate the O(n³) complexity on real-world datasets with large support sizes to determine practical limitations and identify potential optimization opportunities