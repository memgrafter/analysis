---
ver: rpa2
title: 'Potion: Towards Poison Unlearning'
arxiv_id: '2406.09173'
source_url: https://arxiv.org/abs/2406.09173
tags:
- unlearning
- poison
- accuracy
- data
- stop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles poison unlearning: removing malicious triggers
  from a trained model when only part of the poisoned data is known. Existing unlearning
  methods fail because unknown poison remains in the training set and reintroduces
  the trigger.'
---

# Potion: Towards Poison Unlearning

## Quick Facts
- arXiv ID: 2406.09173
- Source URL: https://arxiv.org/abs/2406.09173
- Reference count: 40
- Primary result: 93.72% poison removal vs 83.41% for SSD and 40.68% for retraining

## Executive Summary
This paper addresses the challenge of poison unlearning: removing malicious triggers from a trained model when only part of the poisoned data is known. The key insight is that traditional unlearning methods fail because unknown poison remains in the training set and reintroduces the trigger. The authors propose XLF, a robust parameter-importance estimator that reduces outlier effects, and PTN, a fast parallelizable hyperparameter search that uses accuracy on known poison as a proxy to balance unlearning and protection. Combined, their method achieves significantly better poison removal while reducing model accuracy drop from 5.68% to 1.41%.

## Method Summary
The method combines XLF (a robust parameter-importance estimator using l2 norm instead of squared l2 norm to reduce outlier influence) with PTN (a parallel hyperparameter search that iteratively applies parameter dampening until accuracy on known poison drops below a threshold ρ). The approach uses accuracy drop on identified poisoned data as a proxy for unlearning effectiveness on unknown poison, enabling hyperparameter optimization without knowing the full poison set. The over-forgetting buffer ρ prevents excessive model damage during the search process.

## Key Results
- 93.72% poison healing rate versus 83.41% for SSD and 40.68% for retraining
- Model accuracy drop reduced from 5.68% to 1.41%
- PTN hyperparameter search adds minimal overhead and works without knowing full poison set size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XLF's parameter importance estimation reduces tail-value influence by using l2 norm instead of l2 squared
- Core assumption: Extreme importance values are primarily noise from unreliable low-density regions
- Evidence: [section] "XLF addresses this problem with a change to the parameter importance computation to lower tail value occurrences in the importance distributions."

### Mechanism 2
- Claim: PTN uses accuracy drop on known poison as proxy for unknown poison unlearning
- Core assumption: Accuracy drop on known poison correlates with accuracy drop on unknown poison
- Evidence: [section] "Given the empirically validated monotonic and linked nature of AccSf (α) and AccStr(α)..."

### Mechanism 3
- Claim: Over-forgetting buffer (ρ) prevents excessive model damage during hyperparameter search
- Core assumption: Model damage occurs after poison unlearning is complete
- Evidence: [section] "To overcome challenge (2), we introduce an over-forgetting buffer ρ shown in Fig. 4."

## Foundational Learning

- Concept: Fisher Information Matrix for parameter importance
  - Why needed here: SSD uses Fisher Information Matrix diagonal as importance measure for parameter dampening
  - Quick check: How does the Fisher Information Matrix relate to the Hessian of the loss function?

- Concept: Parameter dampening for unlearning
  - Why needed here: Core mechanism of SSD-based methods - reducing parameter values to remove their influence on specific data
  - Quick check: What happens to model behavior when you scale down parameters proportionally to their importance?

- Concept: Hyperparameter search in constrained settings
  - Why needed here: PTN performs search without knowing full forget set size or having clean retain data
  - Quick check: How does using a proxy metric (known poison accuracy) affect the reliability of hyperparameter optimization?

## Architecture Onboarding

- Component map: Input model + identified poison subset + retain set -> XLF importance estimator -> PTN hyperparameter search -> Unlearned model
- Critical path: 1) Compute parameter importances on Str using XLF 2) Iteratively apply dampening with increasing aggressiveness 3) Check accuracy on Sf after each iteration 4) Stop when accuracy drops below ρ threshold 5) Return final model
- Design tradeoffs: Conservative ρ (20%) ensures model protection but may leave some poison unlearned; parallelizable search vs sequential search for speed; L2 norm vs L2 squared for robustness vs sensitivity to true signal
- Failure signatures: Model accuracy drops significantly before poison accuracy reaches ρ threshold; poison accuracy barely changes across iterations; unusually long search duration
- First 3 experiments: 1) Run XLF importance calculation on small subset of Str to verify distribution characteristics 2) Test PTN search with ρ=0.5 to observe over-forgetting behavior 3) Compare unlearning performance with varying sstep values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform on larger and more complex models, such as transformers or language models?
- Basis in paper: [explicit] Future work will focus on extending these methods to larger and more complex models
- Why unresolved: Only benchmarks on ResNet-9 and WideResNet-28x10 models
- What evidence would resolve it: Experiments on larger models like transformers or language models

### Open Question 2
- Question: What is the impact of different poisoning strategies on the effectiveness of the proposed unlearning method?
- Basis in paper: [inferred] Uses specific BadNet poisoning attack but doesn't explore other types
- Why unresolved: Does not investigate performance against different poisoning strategies
- What evidence would resolve it: Experiments using various poisoning strategies

### Open Question 3
- Question: How does the choice of the stopping parameter (ρ) affect the unlearning performance and model accuracy in different scenarios?
- Basis in paper: [explicit] Discusses sensitivity analysis of ρ but lacks comprehensive analysis
- Why unresolved: No comprehensive analysis across different scenarios
- What evidence would resolve it: Extensive experiments varying ρ across different scenarios

## Limitations

- The monotonic relationship between known and unknown poison removal is empirically validated but not theoretically proven
- The over-forgetting buffer approach may be overly conservative, potentially leaving significant poison unlearned
- The l2 norm modification may suppress genuine signal in certain model architectures

## Confidence

- High Confidence: Mathematical formulation is internally consistent and experimental results are well-documented
- Medium Confidence: Proxy relationship between known and unknown poison removal appears valid for tested scenarios
- Low Confidence: Claim about XLF's universal robustness lacks comprehensive ablation studies

## Next Checks

1. Test XLF + PTN on transformer-based models (e.g., ViT) and NLP models to verify proxy relationship holds beyond CNNs
2. Evaluate performance against non-uniform trigger distributions to test generality of monotonic assumption
3. Systematically vary the over-forgetting buffer ρ (0.1 to 0.5) and step size parameters to identify optimal operating regions