---
ver: rpa2
title: Multi-Query Focused Disaster Summarization via Instruction-Based Prompting
arxiv_id: '2402.09008'
source_url: https://arxiv.org/abs/2402.09008
tags:
- event
- facts
- documents
- summarization
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multi-query focused disaster summarization
  approach using instruction-based prompting with a large language model (LLM). The
  method retrieves and re-ranks documents for each event-request-query triple, then
  extracts query-relevant facts using a question-answering (QA)-motivated LLM prompt.
---

# Multi-Query Focused Disaster Summarization via Instruction-Based Prompting

## Quick Facts
- arXiv ID: 2402.09008
- Source URL: https://arxiv.org/abs/2402.09008
- Reference count: 13
- Surpasses most CrisisFACTS 2023 systems on human and automatic evaluation metrics

## Executive Summary
This paper proposes a multi-query focused disaster summarization approach using instruction-based prompting with a large language model (LLM). The method retrieves and re-ranks documents for each event-request-query triple, then extracts query-relevant facts using a question-answering (QA)-motivated LLM prompt. These facts are parsed and concatenated to form event nuggets, which serve as the final summaries. The approach achieves competitive performance, surpassing most CrisisFACTS 2023 systems on both human and automatic evaluation metrics. However, qualitative analysis reveals issues like irrelevant documents, formatting errors, incorrect facts, and missing citations, indicating room for improvement in the LLM-based summarization pipeline.

## Method Summary
The proposed approach uses a two-stage retrieval pipeline (BM25 + MonoT5) to identify relevant documents for each event-request-query triple. A QA-motivated instruction prompt guides the LLaMA-2-13B LLM to extract query-relevant facts from these documents, with each fact including citations to its source documents. The extracted facts are parsed into structured format and concatenated to form event nuggets limited to 200 characters, with importance scores computed as the mean relevance score of cited documents. The system is evaluated on the CrisisFACTS 2023 dataset using both human evaluation (comprehensiveness and redundancy) and automatic metrics (ROUGE-2 and BERTScore F1).

## Key Results
- Outperforms most CrisisFACTS 2023 baseline systems on human evaluation metrics
- Achieves competitive results on automatic evaluation metrics (ROUGE-2, BERTScore F1)
- Qualitative analysis reveals 33% of prompt input documents contain no useful query-relevant information
- Identifies formatting errors, incorrect facts, and missing citations as key LLM limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage retrieval (BM25 + MonoT5) effectively filters noise while maintaining recall for query-relevant documents.
- Mechanism: BM25 provides broad lexical matching to capture relevant documents, while MonoT5 re-ranks based on semantic similarity to the query, improving precision.
- Core assumption: The combination of lexical and semantic retrieval balances recall and precision better than either alone.
- Evidence anchors:
  - [section] "The two-stage retrieval pipeline relies on BM25 and MonoT5, while the summarizer module is based on the open-source Large Language Model (LLM) LLaMA-13b."
  - [section] "To reduce computational costs, we select the top- k(2) = 30 documents for the LL AMA-N UGGETS and 50 documents for the baseline models, respectively."
- Break condition: If BM25 retrieves too few relevant documents or MonoT5 over-prunes, recall will suffer; if either stage introduces excessive noise, the LLM will generate irrelevant facts.

### Mechanism 2
- Claim: QA-motivated prompting with chain-of-thought enables the LLM to extract atomic, query-focused facts from retrieved documents.
- Mechanism: The LLM is prompted to answer a specific question using evidence from documents, structured to output facts with citations, which are then parsed into event nuggets.
- Core assumption: The LLM can reliably extract facts matching the query when given clear instructions and relevant document context.
- Evidence anchors:
  - [section] "For summarization, we explore a Question Answering (QA)-motivated approach and find the evidence useful for extracting query-relevant facts."
  - [section] "In this way, we aim to filter out irrelevant documents and abstract only the query-relevant content from the document collections."
- Break condition: If the LLM generates irrelevant facts, fails to cite sources, or the parsing step fails to extract structured facts, the event nuggets will be incomplete or incorrect.

### Mechanism 3
- Claim: Concatenating parsed facts into event nuggets (limited to 200 characters) creates concise, topic-focused summaries with importance scores derived from document relevance.
- Mechanism: Each fact is attributed to its source documents, and the importance score is computed as the mean relevance score of cited documents, providing a quantitative measure of nugget significance.
- Core assumption: The mean relevance score of source documents is a valid proxy for the importance of the concatenated facts.
- Evidence anchors:
  - [section] "As an importance score, we compute the mean of all relevance scores of the referenced documents."
  - [section] "Event Nugget Generation To generate the event nuggets, we iteratively concatenate all generated facts for each specific query. Here, we limit the character length to 200, which corresponds to the task's instructions."
- Break condition: If the character limit truncates important information, or if the mean relevance score doesn't correlate with actual nugget importance, the final summaries will be suboptimal.

## Foundational Learning

- Concept: Information retrieval and ranking (BM25, reranking models like MonoT5)
  - Why needed here: The system relies on effective retrieval and reranking to provide the LLM with relevant documents for fact extraction.
  - Quick check question: How does BM25 differ from semantic rerankers like MonoT5 in terms of matching strategy?

- Concept: Prompt engineering and chain-of-thought reasoning for LLMs
  - Why needed here: The system uses a specific QA-motivated prompt to guide the LLM in extracting query-relevant facts with citations.
  - Quick check question: What is the role of the chain-of-thought format in improving the LLM's fact extraction performance?

- Concept: Evaluation metrics for summarization (ROUGE, BERTScore, comprehensiveness, redundancy)
  - Why needed here: The system's performance is measured using both automatic metrics (ROUGE-2, BERTScore) and human evaluation (comprehensiveness, redundancy).
  - Quick check question: How do comprehensiveness and redundancy metrics differ in their assessment of event nugget quality?

## Architecture Onboarding

- Component map: Retrieval (BM25 → MonoT5) → LLM Prompting (QA-motivated CoT) → Fact Parsing → Event Nugget Generation
- Critical path: Retrieval quality directly impacts LLM input quality, which determines fact extraction accuracy and ultimately the event nugget quality.
- Design tradeoffs: Simpler retrieval (BM25 only) might miss semantic matches; more complex retrieval (e.g., dense retrievers) increases cost; longer LLM prompts might improve fact extraction but increase cost and latency.
- Failure signatures: Poor recall → missing facts in summaries; poor precision → irrelevant facts; LLM generation errors → incorrect or malformed facts; parsing failures → incomplete event nuggets.
- First 3 experiments:
  1. Test retrieval quality by checking if top-30 reranked documents contain query-relevant information for sample queries.
  2. Test LLM prompting by checking if generated facts are relevant, correctly cited, and parseable for sample document-query pairs.
  3. Test event nugget generation by concatenating facts and verifying character limit compliance and importance score calculation.

## Open Questions the Paper Calls Out

Open Question 1
- Question: How can the issue of irrelevant documents in the retrieval stage be effectively addressed to improve the quality of generated facts?
- Basis in paper: Inferred
- Why unresolved: The paper mentions that 33% of the prompt input documents did not contain any useful query-relevant information, highlighting the importance of noise robustness. However, it does not propose specific solutions to this issue.
- What evidence would resolve it: Experimental results comparing different retrieval strategies or filtering techniques that reduce the number of irrelevant documents in the input to the LLM.

Open Question 2
- Question: What are the specific limitations of using open-source LLMs like LLaMA-13b for disaster summarization, and how can these limitations be mitigated?
- Basis in paper: Explicit
- Why unresolved: The paper acknowledges that the qualitative analysis reveals shortcomings and limitations of the proposed approach, but it does not provide a detailed analysis of the specific limitations of using open-source LLMs like LLaMA-13b.
- What evidence would resolve it: A detailed analysis of the errors made by the LLM, including a breakdown of incorrect facts, incomplete or incorrect citations, and formatting issues.

Open Question 3
- Question: How can the issue of entity surface form issues, which lead to redundancy in the generated facts, be effectively addressed?
- Basis in paper: Inferred
- Why unresolved: The paper mentions that 20% of the assessed facts are affected by entity surface form issues, which lead to redundancy. However, it does not propose specific solutions to this issue.
- What evidence would resolve it: Experimental results comparing different techniques for normalizing entity surface forms or for detecting and removing redundant facts.

## Limitations
- The system depends heavily on the LLM's ability to extract accurate, query-relevant facts from retrieved documents
- Qualitative analysis reveals issues with irrelevant documents, formatting errors, incorrect facts, and missing citations
- The exact prompt template used for fact extraction is not fully detailed in the main text, limiting reproducibility

## Confidence
- High confidence: The overall methodology of using two-stage retrieval followed by LLM-based fact extraction is sound and well-documented. The performance comparison with other CrisisFACTS 2023 systems is clearly presented with specific metrics.
- Medium confidence: The effectiveness of the QA-motivated prompting approach for fact extraction. While the method is described, the lack of detailed prompt templates and comprehensive evaluation of fact extraction quality limits confidence in the robustness of this component.
- Low confidence: The generalizability of the approach to other domains beyond disaster summarization. The system is specifically tuned for crisis events and may not perform similarly on general summarization tasks.

## Next Checks
1. **Retrieval quality validation**: Conduct an ablation study comparing the two-stage retrieval pipeline against using only BM25 or only MonoT5, measuring both recall (documents retrieved vs. relevant documents) and precision (relevant documents in top-30) for sample queries.

2. **LLM fact extraction reliability test**: Create a benchmark dataset of document-query pairs with ground truth facts, then measure the precision, recall, and F1-score of the LLM's fact extraction against this benchmark, including citation accuracy rates.

3. **Generalizability assessment**: Apply the same methodology to a non-disaster summarization dataset (e.g., news articles or meeting minutes) and compare performance metrics to evaluate domain transferability of the approach.