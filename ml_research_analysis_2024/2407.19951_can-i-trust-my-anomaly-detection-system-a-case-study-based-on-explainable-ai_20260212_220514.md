---
ver: rpa2
title: Can I trust my anomaly detection system? A case study based on explainable
  AI
arxiv_id: '2407.19951'
source_url: https://arxiv.org/abs/2407.19951
tags:
- anomaly
- detection
- lime
- anomalies
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a case study examining the trustworthiness
  of anomaly detection systems based on variational autoencoders (VAEs) using explainable
  AI (XAI) methods. The authors explore whether detected anomalies are correctly identified
  by combining VAE-GAN models with LIME and SHAP explanation techniques.
---

# Can I trust my anomaly detection system? A case study based on explainable AI

## Quick Facts
- arXiv ID: 2407.19951
- Source URL: https://arxiv.org/abs/2407.19951
- Authors: Muhammad Rashid; Elvio Amparore; Enrico Ferrari; Damiano Verda
- Reference count: 26
- Primary result: XAI methods reveal that high-accuracy anomaly detection systems often classify samples as anomalous for incorrect reasons

## Executive Summary
This paper presents a case study examining the trustworthiness of anomaly detection systems based on variational autoencoders (VAEs) using explainable AI (XAI) methods. The authors explore whether detected anomalies are correctly identified by combining VAE-GAN models with LIME and SHAP explanation techniques. Their methodology involves training VAE-GAN models on normal samples, generating anomaly maps through reconstruction errors, and using XAI methods to explain these anomalies. The key finding is that while the anomaly detection system achieves high accuracy (90% for hazelnuts, 80% for screws), XAI methods reveal that samples are often detected as anomalous for incorrect reasons. This is demonstrated through optimal Jaccard score comparisons between explained anomalies and ground truth, showing poor localization in many cases. The study highlights that anomaly scores alone are insufficient for understanding classification decisions and that XAI methods are crucial for identifying potential misbehaviors in detection systems.

## Method Summary
The methodology combines VAE-GAN models for anomaly detection with XAI techniques (LIME and SHAP) for explanation generation. VAE-GAN models are trained on normal samples only, learning a compressed representation in latent space. Anomaly detection is performed through reconstruction error maps, with samples classified as anomalous if their reconstruction error exceeds a calibrated threshold. XAI methods are adapted to explain reconstruction errors rather than classification probabilities, generating feature attribution maps. These explanations are compared to ground truth anomaly locations using optimal Jaccard scores to evaluate explanation quality. The approach is tested on hazelnut and screw categories from the MVTec dataset, with 128Ã—128 resolution images.

## Key Results
- Anomaly detection accuracy reaches 90% for hazelnuts and 80% for screws using VAE-GAN models
- XAI methods reveal that samples are often detected as anomalous for incorrect reasons despite high accuracy
- Optimal Jaccard score comparisons show poor localization between explained anomalies and ground truth in many cases
- LIME explanations show high attribution to image borders, suggesting segmentation issues with small defects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XAI methods can reveal when anomaly detection systems classify samples as anomalous for incorrect reasons
- Mechanism: XAI methods (LIME and SHAP) generate feature attribution maps that highlight which regions of an image contribute most to the anomaly score. By comparing these attribution maps to ground truth anomaly locations using Jaccard coefficient, researchers can identify cases where the model's decision is based on spurious features rather than actual anomalies.
- Core assumption: The feature attribution scores generated by XAI methods accurately reflect the model's reasoning process for anomaly detection
- Evidence anchors:
  - [abstract] "XAI methods reveal that samples are often detected as anomalous for incorrect reasons"
  - [section] "we employ a methodology based on the optimal Jaccard score" to compare explained anomalies with ground truth
  - [corpus] Weak evidence - no directly comparable papers found in corpus
- Break condition: If XAI methods consistently produce feature attributions that do not correlate with the actual model decision-making process, or if the ground truth anomaly localization is inaccurate

### Mechanism 2
- Claim: VAE-GAN models trained only on normal data can effectively detect anomalies through reconstruction error differences
- Mechanism: The VAE-GAN learns a compressed representation of normal samples in latent space. When presented with anomalous samples, the model cannot properly reconstruct them, resulting in higher reconstruction errors that serve as anomaly scores.
- Core assumption: The reconstruction error distribution for normal samples is significantly different from that of anomalous samples
- Evidence anchors:
  - [abstract] "A common approach employs the anomaly score to detect the presence of anomalies, and it is known to reach high level of accuracy on benchmark datasets"
  - [section] "If the sample is normal and lies in-distribution with the model, it should be reconstructed accurately, with minimal reconstruction errors"
  - [corpus] Weak evidence - no directly comparable papers found in corpus
- Break condition: If the reconstruction error distributions overlap significantly between normal and anomalous samples, or if the model overfits to training data

### Mechanism 3
- Claim: The combination of VAE-GAN with XAI methods provides more trustworthy anomaly detection than anomaly scores alone
- Mechanism: While VAE-GAN models can achieve high accuracy, they lack interpretability. XAI methods provide localized explanations that help users understand why a sample was classified as anomalous, revealing cases where the classification is incorrect despite high accuracy.
- Core assumption: Human inspection of XAI explanations can effectively identify incorrect classifications that would otherwise go unnoticed
- Evidence anchors:
  - [abstract] "This study highlights that anomaly scores alone are insufficient for understanding classification decisions"
  - [section] "A sample may be detected as anomalous for the wrong reasons, yet this misbehaviour may not be detectable from the information provided by the anomaly map alone"
  - [corpus] Weak evidence - no directly comparable papers found in corpus
- Break condition: If XAI explanations are too complex for human interpretation, or if the overhead of explanation generation outweighs the benefits

## Foundational Learning

- Concept: Variational Autoencoders (VAEs) and their probabilistic latent space representation
  - Why needed here: The paper relies on VAE-GAN models for anomaly detection, which requires understanding how VAEs learn latent representations and generate reconstructions
  - Quick check question: How does a VAE differ from a standard autoencoder in terms of latent space representation?

- Concept: Generative Adversarial Networks (GANs) and their adversarial training
  - Why needed here: The paper uses VAE-GAN models, combining VAEs with GANs for improved reconstruction quality
  - Quick check question: What role does the discriminator play in a GAN during training?

- Concept: Model-agnostic XAI methods (LIME and SHAP) and their application to non-classification tasks
  - Why needed here: The paper adapts LIME and SHAP to explain anomaly detection decisions based on reconstruction errors rather than classification probabilities
  - Quick check question: How can LIME be modified to explain reconstruction error instead of classification probabilities?

## Architecture Onboarding

- Component map: VAE-GAN model (encoder, decoder, discriminator) -> Anomaly score calculation (reconstruction error) -> Threshold calibration (optimal threshold selection) -> XAI explanation generation (LIME and SHAP) -> Comparison with ground truth (Jaccard coefficient)

- Critical path: 1. Train VAE-GAN on normal samples only 2. Generate anomaly scores for test samples 3. Apply threshold to classify as anomalous/normal 4. Generate XAI explanations for anomalous samples 5. Compare explanations with ground truth using Jaccard coefficient

- Design tradeoffs:
  - Reconstruction quality vs. anomaly detection sensitivity
  - Explanation granularity (pixel-level vs. superpixel-level)
  - Computational cost of XAI explanations vs. interpretability benefits
  - Threshold selection method (fixed vs. optimal per dataset)

- Failure signatures:
  - High accuracy but poor Jaccard scores indicate incorrect classification reasons
  - LIME explanations showing high attribution to borders suggests segmentation issues
  - SHAP explanations localizing anomalies to wrong regions indicates reconstruction errors

- First 3 experiments:
  1. Train VAE-GAN on hazelnut dataset and evaluate accuracy on test set
  2. Apply LIME explanations with different superpixel segmentation methods and compare Jaccard scores
  3. Apply SHAP explanations and compare localization quality against LIME results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can XAI methods be systematically integrated into anomaly detection pipelines to identify systematic model misbehaviors across diverse industrial datasets?
- Basis in paper: [explicit] The authors identify that samples are often detected as anomalous for incorrect reasons and emphasize the need for XAI methods to detect such misbehaviors, but do not provide a systematic framework for broader application.
- Why unresolved: The study focuses on two specific datasets (hazelnut and screw) and highlights limitations in current XAI methods like LIME's dependency on segmentation, but does not explore how these findings generalize to other domains or how to mitigate these limitations.
- What evidence would resolve it: A systematic study applying the same methodology to a broader range of industrial datasets, quantifying the frequency and types of misbehavior, and proposing standardized XAI integration strategies for anomaly detection systems.

### Open Question 2
- Question: What alternative XAI methods could provide more reliable anomaly localization than current perturbation-based approaches like LIME and SHAP?
- Basis in paper: [explicit] The authors note that LIME's performance is highly dependent on segmentation quality and that both LIME and SHAP can mislocalize anomalies when reconstruction errors are not directly tied to the true anomaly, suggesting room for improvement in XAI methods.
- Why unresolved: The paper only compares two perturbation-based XAI methods and does not explore other techniques such as gradient-based methods, counterfactual explanations, or methods specifically designed for generative model explanations.
- What evidence would resolve it: Comparative evaluation of multiple XAI methods (including gradient-based, counterfactual, and specialized generative model explainers) across diverse anomaly detection tasks, with quantitative metrics for localization accuracy and robustness to reconstruction errors.

### Open Question 3
- Question: How can anomaly detection systems be designed to provide more interpretable and actionable explanations without requiring ground truth anomaly maps for evaluation?
- Basis in paper: [inferred] The authors rely on ground truth anomaly maps to evaluate XAI explanations through optimal Jaccard scores, but acknowledge this is not available in real-world deployment scenarios, highlighting a practical limitation in assessing explanation quality.
- Why unresolved: Current evaluation requires ground truth annotations which are typically unavailable in real-world anomaly detection, making it difficult to assess whether explanations reflect true anomaly causes versus reconstruction artifacts.
- What evidence would resolve it: Development and validation of evaluation metrics that can assess explanation quality without ground truth, such as user studies measuring human trust and decision-making, or methods that cross-validate explanations across multiple related samples or time periods.

## Limitations
- Limited to two object categories from a single dataset, raising questions about generalizability
- Statistical significance testing is missing for Jaccard score comparisons between XAI methods
- Computational overhead of XAI explanations for real-time deployment is not addressed
- Hyperparameter choices for VAE-GAN and XAI methods may significantly impact results

## Confidence

- High confidence: VAE-GAN models can achieve high anomaly detection accuracy on benchmark datasets
- Medium confidence: XAI methods reveal cases where anomalies are detected for incorrect reasons
- Medium confidence: Jaccard coefficient is an appropriate metric for evaluating explanation quality
- Low confidence: The methodology generalizes to other anomaly detection scenarios beyond industrial images

## Next Checks
1. Test the methodology across additional MVTec categories and real-world industrial datasets to assess generalizability
2. Conduct statistical significance testing on Jaccard score comparisons between LIME and SHAP explanations
3. Measure and analyze the computational overhead of XAI explanations and their impact on real-time deployment feasibility