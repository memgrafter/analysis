---
ver: rpa2
title: When Quantization Affects Confidence of Large Language Models?
arxiv_id: '2405.00632'
source_url: https://arxiv.org/abs/2405.00632
tags:
- confidence
- quantization
- language
- llms
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how quantization with GPTQ affects the
  confidence and calibration of large language models (LLMs). The authors find that
  4-bit quantization decreases confidence in true labels and increases calibration
  error across different model families (BLOOM, OPT, Mistral, LLaMA) and scales.
---

# When Quantization Affects Confidence of Large Language Models?

## Quick Facts
- arXiv ID: 2405.00632
- Source URL: https://arxiv.org/abs/2405.00632
- Reference count: 40
- This paper investigates how quantization with GPTQ affects the confidence and calibration of large language models (LLMs).

## Executive Summary
This study examines how 4-bit quantization with GPTQ impacts the confidence and calibration of large language models across different families (BLOOM, OPT, Mistral, LLaMA) and scales. The authors find that quantization reduces confidence in true labels and increases calibration error, with the most significant effects observed in samples where the original model had low confidence. The research reveals that quantized models exhibit increased entropy, indicating higher uncertainty, and that these effects vary across different model architectures and sizes. These findings highlight the importance of considering calibration when deploying quantized LLMs, especially for uncertain predictions.

## Method Summary
The study uses post-training quantization with GPTQ at 4-bit precision, applying parameters including group size 128, symmetry true, and dampening factor 0.01. The evaluation pipeline involves zero-shot inference on six benchmarks (ARC EASY, BOOLQ, HELLASWAG, OPENBOOKQA, PIQA, XSTORY) using models ranging from 125M to 13B parameters across four model families. The analysis measures accuracy, Expected Calibration Error (ECE) for binary tasks, Adaptive Calibration Error (ACE) for multi-class tasks, mean confidence, confidence in wrong predictions, confidence in true class, and predictive entropy. Statistical significance testing validates the observed differences between full and quantized models.

## Key Results
- Quantization with GPTQ to 4-bit reduces confidence in true labels and increases calibration error across different model families and scales.
- The quantization process disproportionately affects samples where the original model had low confidence levels.
- Quantized models show increased entropy in predictive distributions, indicating higher uncertainty compared to full-precision models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization with GPTQ to 4-bit reduces confidence in true labels, particularly in samples where the original model had low confidence.
- Mechanism: The quantization process introduces noise and approximation errors in weight representation, which disproportionately affects low-confidence predictions where the model's decision boundary is less stable. This amplifies uncertainty in borderline cases while having less impact on high-confidence predictions where the decision boundary is more robust.
- Core assumption: Low-confidence predictions are more sensitive to small perturbations in weight values than high-confidence predictions.
- Evidence anchors:
  - [abstract]: "quantization disproportionately affects samples where the full model exhibited low confidence levels in the first place"
  - [section]: "samples with lower pre-quantization confidence levels are significantly affected by the quantization process"
  - [corpus]: Weak - neighboring papers discuss quantization effects but don't specifically address confidence-level-dependent impacts
- Break condition: If quantization noise is uniformly distributed or if the model's decision boundaries are inherently stable across all confidence levels.

### Mechanism 2
- Claim: Quantization increases predictive entropy, indicating higher uncertainty in quantized models compared to full-precision models.
- Mechanism: The compression of weights to 4-bit representation reduces the precision of probability distributions, leading to broader and flatter distributions over output classes. This flattening effect increases entropy, making the model less certain about its predictions even when the most probable class remains the same.
- Core assumption: Reduced weight precision translates directly to increased uncertainty in probability distributions.
- Evidence anchors:
  - [abstract]: "shows increased entropy in quantized models, indicating higher uncertainty"
  - [section]: "increase in entropy for the quantized LLMs... This increase suggests an amplification in the variance across answers"
  - [corpus]: Weak - neighboring papers focus on performance metrics rather than entropy-based uncertainty analysis
- Break condition: If entropy increases are negligible or if the entropy increase doesn't correlate with practical changes in model behavior.

### Mechanism 3
- Claim: Different model families and scales show varying degrees of susceptibility to quantization-induced confidence shifts.
- Mechanism: The architectural differences and training dynamics across model families (BLOOM, OPT, Mistral, LLaMA) create different sensitivities to quantization noise. Larger models may have more robust representations that are less affected by weight approximation errors, while smaller models are more vulnerable to confidence degradation.
- Core assumption: Model architecture and scale fundamentally affect how quantization noise propagates through the network.
- Evidence anchors:
  - [abstract]: "with varying impacts observed among different language models" and "fluctuations in the impact on confidence across different scales"
  - [section]: "variance in quantization impact across different families of models and their sizes"
  - [corpus]: Weak - neighboring papers discuss scale effects but not confidence-specific impacts
- Break condition: If all model families show uniform sensitivity to quantization or if scale effects are not statistically significant.

## Foundational Learning

- Concept: Jensen-Shannon Divergence
  - Why needed here: Used to measure the dissimilarity between probability distributions of full and quantized models, providing quantitative evidence of how quantization affects predictive behavior.
  - Quick check question: How does Jensen-Shannon Divergence differ from Kullback-Leibler divergence when comparing two probability distributions?

- Concept: Calibration Error (ECE and ACE)
  - Why needed here: These metrics quantify how well model confidence aligns with actual accuracy, which is central to understanding how quantization affects the reliability of model predictions.
  - Quick check question: What's the key difference between Expected Calibration Error (ECE) and Adaptive Calibration Error (ACE) in terms of how they handle multi-class problems?

- Concept: Entropy in Predictive Distributions
  - Why needed here: Entropy measures uncertainty in model predictions, and the paper shows that quantization increases entropy, indicating less confident predictions even when the most likely class remains the same.
  - Quick check question: How does increasing entropy in a predictive distribution affect the interpretation of model confidence scores?

## Architecture Onboarding

- Component map: AutoGPTQ quantization pipeline -> 4-bit weight representation -> Model inference engine -> probability distribution generation -> Evaluation harness -> accuracy, calibration error, entropy metrics -> Data preprocessing -> input formatting for different benchmarks

- Critical path: Model loading -> GPTQ quantization (128 samples, grouping size 128) -> Zero-shot inference on benchmarks -> Confidence and calibration analysis -> Statistical significance testing

- Design tradeoffs: 4-bit quantization offers significant memory savings but introduces approximation errors that affect confidence calibration; alternative quantization levels (2-bit, 3-bit) might show different confidence impacts.

- Failure signatures: Calibration error increases without corresponding accuracy drops; entropy increases disproportionately in certain model families; confidence changes are asymmetric across confidence level bins.

- First 3 experiments:
  1. Replicate confidence difference analysis across confidence level bins for a single model-family pair to verify the low-confidence sensitivity mechanism.
  2. Compare entropy changes between 4-bit and 8-bit quantization to isolate the effect of bit-depth reduction on uncertainty.
  3. Test calibration error on held-out data to ensure the observed effects aren't due to dataset-specific artifacts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the impact of quantization on model confidence vary across different quantization bit depths (e.g., 2-bit, 3-bit, 4-bit)?
- Basis in paper: [inferred] The paper focuses on 4-bit quantization with GPTQ and mentions that future work could explore different quantization factors, including 2-bit and 3-bit weight representation.
- Why unresolved: The study only evaluates the effects of 4-bit quantization, leaving the impact of other bit depths unexplored.
- What evidence would resolve it: Experiments comparing model confidence and calibration across different quantization bit depths (2-bit, 3-bit, 4-bit) using the same models and benchmarks.

### Open Question 2
- Question: Does fine-tuning quantized models mitigate the observed loss in confidence and increase in calibration error compared to zero-shot evaluation?
- Basis in paper: [explicit] The paper states that future work could involve few-shot analysis since this method has the potential to amplify or compensate for confidence and quantization loss, contrasting with the current zero-shot approach.
- Why unresolved: The current study uses zero-shot evaluation to measure the pure quantization effect, without exploring the potential benefits of fine-tuning.
- What evidence would resolve it: Comparative studies measuring confidence and calibration error in quantized models with and without fine-tuning, using the same models and benchmarks.

### Open Question 3
- Question: How does quantization-induced confidence loss affect the performance of large language models on generative tasks beyond classification?
- Basis in paper: [explicit] The paper suggests that future research could apply the analysis to other generative tasks, considering predictive distributions across tokens instead of labels.
- Why unresolved: The study focuses on classification tasks, leaving the impact on generative tasks unexplored.
- What evidence would resolve it: Experiments evaluating the confidence and calibration of quantized models on generative tasks such as text completion or summarization, comparing the results to the original models.

## Limitations
- Dataset Generalizability: The study uses six specific benchmarks which may not fully represent the diversity of real-world LLM applications.
- Single Quantization Method: The analysis focuses exclusively on GPTQ quantization at 4-bit precision, potentially missing effects from other quantization methods.
- Statistical Power: The paper doesn't provide confidence intervals or effect sizes, making it difficult to assess the practical significance of observed differences.

## Confidence
- High Confidence: Quantization with GPTQ to 4-bit reduces confidence in true labels and increases calibration error; confidence changes are most pronounced in uncertain predictions where original models had low confidence.
- Medium Confidence: Different model families and scales show varying degrees of susceptibility to quantization-induced confidence shifts; increased entropy in quantized models indicates higher uncertainty.
- Low Confidence: Mechanisms explaining why low-confidence predictions are more affected by quantization; claims about calibration error increases being problematic for practical deployment.

## Next Checks
1. Cross-Domain Robustness Test: Evaluate the same quantization pipeline on additional benchmarks from different domains (medical, legal, mathematical reasoning) to verify whether confidence degradation patterns hold across diverse application areas.

2. Quantization Method Comparison: Replicate the study using alternative quantization methods (AWQ, QLoRA) at multiple bit depths to determine whether the observed confidence effects are specific to GPTQ or represent a general quantization phenomenon.

3. Effect Size and Practical Significance Analysis: Calculate Cohen's d effect sizes for the confidence differences and conduct user studies to determine whether the statistically significant calibration errors translate to meaningful degradation in downstream task performance or user trust.