---
ver: rpa2
title: 'MSR-86K: An Evolving, Multilingual Corpus with 86,300 Hours of Transcribed
  Audio for Speech Recognition Research'
arxiv_id: '2406.18301'
source_url: https://arxiv.org/abs/2406.18301
tags:
- speech
- multilingual
- msr-86k
- corpus
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MSR-86K, a multilingual ASR corpus derived
  from publicly accessible YouTube videos, containing 86,300 hours of transcribed
  audio across 15 languages. The authors address the lack of large-scale, open-source
  multilingual ASR datasets by automatically collecting and processing video content
  with speech.
---

# MSR-86K: An Evolving, Multilingual Corpus with 86,300 Hours of Transcribed Audio for Speech Recognition Research

## Quick Facts
- arXiv ID: 2406.18301
- Source URL: https://arxiv.org/abs/2406.18301
- Reference count: 0
- This paper presents MSR-86K, a multilingual ASR corpus derived from publicly accessible YouTube videos, containing 86,300 hours of transcribed audio across 15 languages.

## Executive Summary
This paper introduces MSR-86K, a large-scale multilingual automatic speech recognition (ASR) corpus containing 86,300 hours of transcribed audio across 15 languages. The authors address the scarcity of open-source multilingual ASR datasets by developing an automated pipeline that collects and processes YouTube videos with speech content. Using this corpus alongside other open-source data, they train a non-autoregressive multilingual ASR model that outperforms OpenAI's Whisper (both medium and large-v2 variants) in terms of word error rate (WER) and character error rate (CER) across most tested languages, while being significantly smaller (362M parameters vs. 1.55B for Whisper large-v2). The model achieves average WER/CER of 7.6% with language identification provided, compared to Whisper's 10.5%.

## Method Summary
The authors create MSR-86K through an automated pipeline that includes keyword-based YouTube video retrieval, subtitle detection, forced alignment, voice activity detection, language identification, and ASR filtering. They leverage this corpus along with other open-source datasets to train a HuBERT-based non-autoregressive multilingual ASR model using CTC loss. The model incorporates language ID prompt tuning and shallow fusion with an LSTM language model. Training involves unsupervised pre-training on 400k hours of audio data, followed by supervised fine-tuning and language-specific adaptation.

## Key Results
- MSR-86K contains 86,300 hours of transcribed audio across 15 languages, with individual languages ranging from ~4k to ~14k hours
- The proposed model (362M parameters) outperforms Whisper medium and large-v2 in WER/CER across most languages while being significantly smaller
- Average WER/CER of 7.6% achieved with language identification, compared to Whisper's 10.5%
- Model demonstrates strong performance on both the MSR-86K dev set and Common Voice test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large-scale multilingual data with sufficient per-language coverage improves ASR robustness
- Mechanism: By sourcing 86,300 hours of transcribed audio from YouTube, the corpus ensures each of the 15 languages has enough training data to train independent ASR systems
- Core assumption: Each language's data quantity is sufficient to support robust model training without heavy reliance on cross-lingual transfer
- Evidence anchors: Abstract states 86,300 hours across 15 languages; section mentions substantial language coverage; corpus section provides weak evidence without direct per-language sufficiency data
- Break condition: If any language's training hours fall below a critical threshold (e.g., <1k hrs), model performance will degrade sharply

### Mechanism 2
- Claim: Automated filtering (LID, ASR, forced alignment) ensures high-quality transcriptions
- Mechanism: The pipeline uses language identification, ASR filtering, and forced alignment to remove inaccurate or mismatched subtitles, improving annotation quality before training
- Core assumption: Subtitle timestamps and text can be reliably corrected via forced alignment and automated quality checks
- Evidence anchors: Section describes LID model filtering discrepancies; section mentions ASR filtering with WER calculation; corpus section provides weak evidence without explicit error rate distribution
- Break condition: If automated filtering incorrectly removes valid data or fails to catch errors, WER will remain high despite filtering

### Mechanism 3
- Claim: Combining unsupervised pre-training with fine-tuning yields competitive ASR performance with fewer parameters
- Mechanism: HuBERT unsupervised pre-training on 400k hours (including MSR-86K) provides rich speech representations, followed by CTC fine-tuning and language prompt tuning
- Core assumption: Pre-training on diverse, unlabeled data transfers well to downstream multilingual ASR tasks
- Evidence anchors: Abstract states model outperforms Whisper while being smaller; section mentions comprehensive corpus of 400k hours; corpus section provides weak evidence without direct unsupervised data quality metrics
- Break condition: If unsupervised data is too noisy or domain-mismatched, fine-tuning gains will be minimal

## Foundational Learning

- Concept: Connectionist Temporal Classification (CTC) loss
  - Why needed here: CTC enables training ASR models without requiring frame-level alignments, crucial for large-scale multilingual corpora
  - Quick check question: What is the role of the blank token in CTC decoding?

- Concept: Byte-level Byte-Pair Encoding (BPE)
  - Why needed here: BPE tokenization handles multilingual vocabularies efficiently without requiring language-specific tokenization rules
  - Quick check question: How does byte-level BPE differ from character-level tokenization for low-resource languages?

- Concept: Language Identification (LID) as prompt tuning
  - Why needed here: LID conditioning allows the model to dynamically adapt to target languages during inference without retraining
  - Quick check question: How does prompt tuning differ from adapter-based multilingual ASR approaches?

## Architecture Onboarding

- Component map: YouTube retrieval -> subtitle detection -> forced alignment -> VAD -> LID filter -> ASR filter -> split -> HuBERT encoder (24-layer Transformer) -> CTC decoder -> language adapter (LID prompt) -> LSTM LM (shallow fusion)

- Critical path:
  1. Data quality (LID + ASR filter accuracy)
  2. Pre-training coverage (400k hrs unsupervised)
  3. Fine-tuning stability (CTC convergence across 15 languages)

- Design tradeoffs:
  - Pros: Open-source, scalable, competitive performance, smaller model
  - Cons: Reliant on YouTube availability, automated filtering may miss edge cases, language coverage limited to 15

- Failure signatures:
  - High WER on test sets despite low dev WER → overfitting or domain mismatch
  - Poor LID accuracy → incorrect language conditioning
  - Pre-training collapse → degraded fine-tuning performance

- First 3 experiments:
  1. Train monolingual CTC model on MSR-86K dev set for each language → verify per-language WER < 10%
  2. Fine-tune HuBERT on subset (e.g., 10k hrs) → compare WER vs full 400k hrs pre-training
  3. Ablate LID prompt tuning → measure performance drop with and without language conditioning

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- No explicit per-language data distribution or sufficiency thresholds are provided, making it difficult to verify that each language has adequate training hours
- The effectiveness of automated filtering is inferred from final WER metrics but not directly measured through error rate distributions before/after filtering
- No ablation studies comparing different amounts of unsupervised pre-training data are included to validate the claimed benefits of large-scale pre-training

## Confidence
- High confidence: MSR-86K corpus creation methodology and basic performance claims (WER/CER improvements over Whisper)
- Medium confidence: The sufficiency of per-language data quantities for independent training
- Medium confidence: The effectiveness of automated filtering pipeline for ensuring high-quality transcriptions
- Medium confidence: The competitive advantage of the proposed model architecture over Whisper

## Next Checks
1. Analyze per-language data distribution in MSR-86K to verify each language has sufficient training hours (>1k hrs) and identify any potential low-resource languages that may require special handling
2. Conduct ablation studies on the filtering pipeline by measuring WER/CER on unfiltered vs. filtered data to quantify the actual quality improvement achieved through LID and ASR filtering
3. Perform controlled experiments comparing model performance with different amounts of unsupervised pre-training data (e.g., 50k vs 200k vs 400k hours) to validate the claimed benefits of large-scale pre-training