---
ver: rpa2
title: Reducing Hallucinations in Vision-Language Models via Latent Space Steering
arxiv_id: '2410.15778'
source_url: https://arxiv.org/abs/2410.15778
tags:
- arxiv
- hallucination
- vision
- visual
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucination in large vision-language models
  (LVLMs), where generated text includes inaccurate descriptions of input images.
  The core issue identified is the sensitivity of text decoders to unstable vision
  features, arising from separate pre-training of image encoders and language models.
---

# Reducing Hallucinations in Vision-Language Models via Latent Space Steering

## Quick Facts
- arXiv ID: 2410.15778
- Source URL: https://arxiv.org/abs/2410.15778
- Authors: Sheng Liu; Haotian Ye; Lei Xing; James Zou
- Reference count: 24
- Primary result: Test-time latent space steering reduces hallucinations in LVLMs by addressing vision feature instability

## Executive Summary
This paper addresses hallucinations in large vision-language models (LVLMs), where generated text includes inaccurate descriptions of input images. The core issue identified is the sensitivity of text decoders to unstable vision features, arising from separate pre-training of image encoders and language models. To mitigate this, the authors propose Visual and Textual Intervention (VTI), a test-time technique that steers latent space representations to enhance vision feature stability. VTI pre-computes directions in latent space by averaging perturbed image features and applying them to new queries, along with a textual intervention to reduce language biases. Experiments across multiple benchmarks show VTI consistently outperforms baselines, reducing hallucination rates and improving accuracy and F1 scores.

## Method Summary
VTI is a test-time intervention method that steers latent space representations in LVLMs to reduce hallucinations. It works by pre-computing direction vectors in latent space through perturbation and averaging of vision features, then applying these directions during inference. The method includes both visual intervention (steering vision encoder features) and textual intervention (steering text decoder features) to address both vision feature instability and language model biases. The intervention directions are computed via PCA on feature differences from perturbed images and hallucinated/non-hallucinated caption pairs. During inference, these pre-computed directions are added to the corresponding layers of the LVLM with tunable strength parameters α and β.

## Key Results
- On POPE benchmark with LLaVA-1.5, VTI achieves 86.5% accuracy and 85.9% F1 score, outperforming existing methods
- VTI reduces hallucination rates across multiple benchmarks (POPE, CHAIR, MMHAL-Bench) consistently
- The method is task-agnostic and effective across diverse hallucination types including object and spatial relation errors
- VTI demonstrates strong performance even with relatively small pre-computed example sets (50 images)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hallucinations in LVLMs arise from sensitivity of text decoders to unstable vision features.
- Mechanism: When image encoders and text decoders are pre-trained separately, the text decoder can become overly sensitive to small variations in vision features, leading to misaligned outputs.
- Core assumption: Vision feature stability directly correlates with hallucination frequency.
- Evidence anchors:
  - [abstract] "We identify that hallucinations often arise from the sensitivity of text decoders to vision inputs, a natural phenomenon when image encoders and text decoders are pre-trained separately."
  - [section 3] "These unstable features are closely tied to hallucinations, as the model becomes overly sensitive to these features, leading to inaccuracies in its outputs."
  - [corpus] Found 25 related papers. Average neighbor FMR=0.456. Top related titles: Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations, Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation.

### Mechanism 2
- Claim: Averaging vision features across perturbed images reduces hallucinations by smoothing unstable features.
- Mechanism: Perturbing images and averaging their features reduces the impact of unstable vision features that cause hallucinations.
- Core assumption: The averaging process preserves semantic information while reducing feature instability.
- Evidence anchors:
  - [section 3] "averaging vision features across multiple noise-injected images reduces hallucination as the number of perturbations increases, regardless of the type of noise injected."
  - [section 4] "Inspired by works on representation engineering for LLMs... we pre-compute the direction that feature averaging has brought in the latent space and apply them to edit the latent features of any new query."
  - [corpus] Found 25 related papers. Average neighbor FMR=0.456. Top related titles: Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs.

### Mechanism 3
- Claim: Textual intervention reduces language biases that contribute to hallucinations.
- Mechanism: By steering text decoder latent states toward non-hallucination directions, the model relies more on visual information than language priors.
- Core assumption: Hallucinations can originate from text decoder biases independent of vision feature stability.
- Evidence anchors:
  - [abstract] "We further obtain a textual direction and apply it to the text decoder to maximize the performance."
  - [section 4] "Text intervention, by pushing the latent space features to the non-hallucination direction, implicitly reduces such text-biased hallucinations."
  - [section 5.3] "Textual intervention is able to reduce average attention from generated text to text while increasing attention to vision tokens, suggesting higher reliance on the image when generating text."
  - [corpus] Found 25 related papers. Average neighbor FMR=0.456. Top related titles: Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation.

## Foundational Learning

- Concept: Vision feature stability and its impact on model outputs
  - Why needed here: Understanding how vision feature stability affects hallucinations is crucial for implementing effective interventions.
  - Quick check question: How would you measure vision feature stability in a trained LVLM?

- Concept: Latent space manipulation and representation engineering
  - Why needed here: The core technique relies on pre-computing and applying directions in latent space to steer model behavior.
  - Quick check question: What is the difference between editing latent features during inference versus fine-tuning the model?

- Concept: Contrastive learning and direction computation
  - Why needed here: The method uses PCA to extract principal directions from feature differences, which requires understanding of contrastive approaches.
  - Quick check question: Why use PCA instead of directly averaging feature differences when computing intervention directions?

## Architecture Onboarding

- Component map:
  Vision encoder (CLIP-based) -> Vision features -> Projector -> Concatenated embeddings -> Text decoder (LLM) -> Output
  VTI intervention points: Vision encoder latent states and text decoder latent states
  Pre-computation pipeline: Perturb images -> Extract features -> Compute directions via PCA -> Store intervention vectors

- Critical path:
  1. Pre-compute visual and textual directions using 50 example images with paired hallucinated/non-hallucinated captions
  2. During inference, apply visual direction to vision encoder layers and textual direction to text decoder layers
  3. Generate output with reduced hallucinations

- Design tradeoffs:
  - Fixed pre-computed directions vs. dynamic computation: Pre-computation is efficient but may not adapt to domain-specific hallucinations
  - Strength parameters (α, β): Need tuning per model but fixed values work across different benchmarks
  - Task-agnostic vs. task-specific: VTI is designed to be general but might miss task-specific nuances

- Failure signatures:
  - Increased feature variance after intervention suggests over-correction
  - Reduced classification accuracy on probing tasks indicates information loss
  - Hallucination rate increases with certain perturbation types
  - Textual intervention causing overly conservative generations

- First 3 experiments:
  1. Measure vision feature variance with/without VTI on perturbed images to verify stability improvement
  2. Compare hallucination rates across different α, β values to find optimal strength parameters
  3. Evaluate information preservation by measuring linear probe accuracy on vision features before/after intervention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strength (α and β) for the visual and textual interventions across different tasks and datasets?
- Basis in paper: [inferred] The paper conducts an ablation study on α and β in Figure 8, showing that different strengths affect performance, but does not provide a comprehensive analysis across all tasks and datasets.
- Why unresolved: The paper only explores a limited range of α and β values and does not investigate the impact of these parameters on all evaluated tasks and datasets.
- What evidence would resolve it: A comprehensive ablation study varying α and β across all tasks (POPE, CHAIR, MMHAL-Bench) and datasets, reporting the optimal values for each combination.

### Open Question 2
- Question: How does VTI perform on tasks beyond object hallucination, such as complex reasoning or abstract visual understanding?
- Basis in paper: [inferred] The paper primarily focuses on object hallucination and does not extensively evaluate VTI on tasks requiring complex reasoning or abstract visual understanding.
- Why unresolved: The evaluation is limited to object hallucination, spatial relations, and basic visual understanding tasks, leaving the effectiveness of VTI on more complex tasks unexplored.
- What evidence would resolve it: Evaluating VTI on tasks requiring complex reasoning (e.g., visual inference, abstract concept understanding) and comparing its performance to state-of-the-art methods.

### Open Question 3
- Question: Can VTI be extended to handle multi-modal inputs beyond images and text, such as video or audio?
- Basis in paper: [inferred] The paper focuses on vision-language models and does not explore the applicability of VTI to other modalities like video or audio.
- Why unresolved: The methodology is tailored for image and text inputs, and its extension to other modalities is not discussed or evaluated.
- What evidence would resolve it: Adapting VTI to handle video or audio inputs and evaluating its effectiveness in reducing hallucinations in multi-modal models incorporating these modalities.

### Open Question 4
- Question: What is the impact of VTI on the model's ability to generate creative or diverse outputs?
- Basis in paper: [inferred] The paper focuses on reducing hallucinations but does not investigate the potential trade-off between hallucination reduction and creative/diverse output generation.
- Why unresolved: The evaluation metrics primarily measure hallucination reduction and accuracy, without considering the impact on creativity or diversity of generated outputs.
- What evidence would resolve it: Evaluating VTI's impact on metrics like output diversity, novelty, or creativity, and comparing its performance to baseline methods on these aspects.

## Limitations
- The method assumes hallucination patterns are relatively stable across domains, which may not hold for specialized applications
- Effectiveness on open-ended generation tasks remains untested, as current evaluation focuses on object-centric benchmarks
- Pre-computation approach may not adapt to domain-specific hallucinations without re-computation

## Confidence
- High confidence in the core mechanism linking vision feature instability to hallucinations
- Medium confidence in the averaging approach's robustness across perturbation types
- Medium confidence in the general effectiveness of textual intervention
- Low confidence in cross-domain generalization without re-computation

## Next Checks
1. Test VTI's effectiveness on open-ended caption generation tasks beyond object-centric benchmarks
2. Evaluate performance degradation when using pre-computed directions on out-of-distribution image domains
3. Conduct ablation studies isolating the individual contributions of visual and textual interventions