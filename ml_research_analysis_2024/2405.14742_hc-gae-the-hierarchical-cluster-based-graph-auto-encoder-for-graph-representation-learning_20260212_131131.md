---
ver: rpa2
title: 'HC-GAE: The Hierarchical Cluster-based Graph Auto-Encoder for Graph Representation
  Learning'
arxiv_id: '2405.14742'
source_url: https://arxiv.org/abs/2405.14742
tags:
- graph
- node
- learning
- classification
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a Hierarchical Cluster-based Graph Auto-Encoder
  (HC-GAE) for graph representation learning. The method addresses two key challenges
  in existing graph auto-encoders: limited effectiveness for multiple downstream tasks
  and the over-smoothing problem.'
---

# HC-GAE: The Hierarchical Cluster-based Graph Auto-Encoder for Graph Representation Learning

## Quick Facts
- arXiv ID: 2405.14742
- Source URL: https://arxiv.org/abs/2405.14742
- Reference count: 40
- Accuracy: 87.97% on Cora, 75.29% on CiteSeer, 87.56% on PubMed (node classification); 76.72% on IMDB-B, 51.90% on IMDB-M, 78.13% on PROTEINS (graph classification)

## Executive Summary
This paper proposes a Hierarchical Cluster-based Graph Auto-Encoder (HC-GAE) that addresses key limitations in existing graph auto-encoders: limited effectiveness for multiple downstream tasks and the over-smoothing problem. The core innovation is a dual assignment strategy - hard node assignment during encoding to decompose graphs into separated subgraphs, and soft node assignment during decoding to reconstruct the original graph structure. This approach reduces over-smoothing by restricting convolution operations within subgraphs while maintaining the ability to learn hierarchical structural features. The method is evaluated on node classification and graph classification tasks, demonstrating superior performance compared to several baseline methods.

## Method Summary
HC-GAE uses a hierarchical architecture where nodes are first hard-assigned to subgraphs during encoding, creating isolated clusters that prevent over-smoothing by restricting information propagation. The encoder performs graph coarsening operations on these subgraphs, compressing them into increasingly abstract representations at different hierarchy levels. During decoding, the process is reversed using soft assignment to expand coarsened nodes back to the original graph structure. The model employs a dual loss function combining local reconstruction loss (KL divergence for each subgraph) and global reconstruction loss (for the full graph), which balances the preservation of local subgraph information with global graph structure reconstruction. This bidirectional hierarchical process enables learning of both graph-level and node-level representations suitable for multiple downstream tasks.

## Key Results
- Node classification accuracy: 87.97% on Cora, 75.29% on CiteSeer, 87.56% on PubMed datasets
- Graph classification accuracy: 76.72% on IMDB-B, 51.90% on IMDB-M, 78.13% on PROTEINS datasets
- HC-GAE outperforms several baseline methods on both node classification and graph classification tasks
- The method effectively addresses over-smoothing by restricting convolution operations within isolated subgraphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hard node assignment during encoding decomposes the graph into isolated subgraphs, preventing over-smoothing by restricting information propagation to within each subgraph.
- Mechanism: Nodes are hard-assigned to subgraphs using a hard assignment matrix that selects maximum probabilities from a soft assignment matrix, creating separated subgraphs where graph convolution operations are restricted to within each subgraph, preventing node information from propagating across the entire graph.
- Core assumption: Separation of subgraphs is sufficient to prevent information propagation that causes over-smoothing in deeper networks.
- Evidence anchors: Abstract states the method "significantly reduce[s] the over-smoothing problem" through subgraph isolation; section confirms convolution cannot propagate between different subgraphs.

### Mechanism 2
- Claim: Hierarchical compression and expansion enables bidirectional learning of both graph-level and node-level representations.
- Mechanism: During encoding, subgraphs are compressed into coarsened nodes through local graph coarsening operations, creating a hierarchy of increasingly abstract representations. During decoding, coarsened nodes are expanded back into the original graph structure using soft assignment.
- Core assumption: The hierarchical structure preserves enough information during compression and expansion to enable effective reconstruction and representation learning at multiple levels.
- Evidence anchors: Abstract mentions "bidirectionally hierarchical structural features" extracted through compressing and expanding procedures; section confirms this enables bidirectional feature extraction.

### Mechanism 3
- Claim: Dual loss function (local + global) provides better generalization for multiple downstream tasks by balancing local subgraph reconstruction with global graph reconstruction.
- Mechanism: The loss function combines Llocal (sum of KL divergences for each subgraph) and Lglobal (reconstruction loss for the full graph), where local loss acts as regularization encouraging preservation of local subgraph information while global loss ensures reconstructed graph matches original structure.
- Core assumption: Combination of local and global reconstruction objectives creates representations useful for both node-level and graph-level tasks simultaneously.
- Evidence anchors: Abstract mentions re-designed loss function integrating information from encoder and decoder; section confirms combination broadens reconstruction requirements for multiple downstream tasks.

## Foundational Learning

### Graph Convolution Operations
- Why needed: Core mechanism for processing graph-structured data by aggregating information from neighboring nodes
- Quick check: Verify that convolution operations are correctly restricted within subgraphs during encoding

### Node Assignment Strategies
- Why needed: Determines how nodes are grouped and how information flows through the network
- Quick check: Confirm hard assignment creates truly isolated subgraphs while soft assignment enables smooth reconstruction

### Hierarchical Graph Coarsening
- Why needed: Enables multi-scale representation learning by compressing graphs into increasingly abstract representations
- Quick check: Ensure coarsening operations preserve sufficient information for accurate reconstruction

### Loss Function Design
- Why needed: Guides the model to learn representations that balance local and global structural information
- Quick check: Validate that dual loss components effectively regularize both subgraph and graph-level reconstruction

## Architecture Onboarding

### Component Map
Input Graph -> Hard Node Assignment -> Subgraph Isolation -> Graph Convolution (within subgraphs) -> Graph Coarsening (hierarchical) -> Compressed Representation -> Soft Node Assignment -> Graph Expansion -> Reconstructed Graph

### Critical Path
Graph -> Hard Assignment -> Convolution (within subgraphs) -> Coarsening (multiple levels) -> Soft Assignment -> Expansion -> Reconstruction

### Design Tradeoffs
- Hard vs. soft assignment: Hard assignment prevents over-smoothing but may lose global connectivity information
- Hierarchical depth: Deeper hierarchies capture more abstract features but risk information loss
- Loss weighting: Balance between local and global reconstruction affects performance on different tasks

### Failure Signatures
- Over-smoothing: Occurs if hard assignment fails to properly isolate subgraphs
- Information loss: Happens if hierarchical compression is too aggressive or unbalanced
- Poor reconstruction: Results from suboptimal loss function weighting or insufficient model capacity

### First Experiments
1. Verify subgraph isolation by checking if information can flow between hard-assigned clusters
2. Test reconstruction quality at each hierarchical level to identify information loss points
3. Evaluate performance sensitivity to local/global loss weighting across different tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HC-GAE compare to other state-of-the-art graph autoencoders when evaluated on larger, more complex graph datasets?
- Basis in paper: The paper evaluates HC-GAE on several real-world datasets, but the datasets are relatively small. It would be interesting to see how HC-GAE performs on larger, more complex datasets to fully assess its scalability and effectiveness.
- Why unresolved: The paper does not provide experimental results on larger, more complex graph datasets. This is a common limitation in graph representation learning research, as such datasets are often difficult to obtain and require significant computational resources to process.
- What evidence would resolve it: Experimental results comparing HC-GAE to other state-of-the-art methods on larger, more complex graph datasets, including both synthetic and real-world data.

## Limitations

- Limited evidence on over-smoothing reduction: The paper claims subgraph isolation reduces over-smoothing but provides no quantitative comparison with standard GAEs showing the magnitude of improvement
- No ablation study on hierarchical structure: Unclear whether the hierarchical compression/expansion is necessary for good performance or if simpler architectures could achieve similar results
- Sensitivity analysis missing: No systematic evaluation of how different local/global loss weightings affect performance across various tasks

## Confidence

- **High Confidence**: Experimental results showing HC-GAE outperforming baseline methods on tested datasets with clearly reported accuracy scores
- **Medium Confidence**: General architectural approach of using hard assignment for encoding and soft assignment for decoding as a coherent strategy
- **Low Confidence**: Specific claims about how mechanisms (subgraph isolation, hierarchical learning, dual loss) contribute to performance improvements due to lack of quantitative validation

## Next Checks

1. **Ablation Study on Subgraph Isolation**: Implement a version of HC-GAE without hard assignment constraint and measure impact on over-smoothing and downstream task performance to validate whether subgraph isolation is necessary for reducing over-smoothing.

2. **Sensitivity Analysis of Loss Function**: Systematically vary weighting between local and global loss components and evaluate performance across node classification and graph classification tasks to determine optimal balance for different tasks.

3. **Hierarchical Depth Analysis**: Test HC-GAE with different numbers of hierarchical layers to determine optimal depth for different datasets and tasks, validating whether hierarchical structure is beneficial and identifying when it might cause information loss.