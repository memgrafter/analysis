---
ver: rpa2
title: 'Practical multi-fidelity machine learning: fusion of deterministic and Bayesian
  models'
arxiv_id: '2407.15110'
source_url: https://arxiv.org/abs/2407.15110
tags:
- data
- samples
- krr-lr-gpr
- figure
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a practical multi-fidelity machine learning
  strategy that combines deterministic low-fidelity models with Bayesian residual
  models for uncertainty quantification. The approach uses a linear transfer-learning
  model to adjust low-fidelity predictions to high-fidelity data, then applies a Bayesian
  model to learn the residual.
---

# Practical multi-fidelity machine learning: fusion of deterministic and Bayesian models

## Quick Facts
- arXiv ID: 2407.15110
- Source URL: https://arxiv.org/abs/2407.15110
- Reference count: 40
- This paper proposes a practical multi-fidelity machine learning strategy that combines deterministic low-fidelity models with Bayesian residual models for uncertainty quantification.

## Executive Summary
This paper introduces a practical multi-fidelity machine learning framework that bridges deterministic low-fidelity (LF) models with Bayesian residual models to achieve uncertainty quantification. The approach uses a linear transfer-learning model to adjust LF predictions to high-fidelity (HF) data, then applies a Bayesian model to learn the residual. Two configurations are recommended: KRR-LR-GPR (Kernel Ridge Regression + Linear Regression + Gaussian Process Regression) for data-scarce scenarios, and DNN-LR-BNN (Deep Neural Network + Linear Regression + Bayesian Neural Network) for data-rich scenarios. The method achieves comparable performance to state-of-the-art multi-fidelity approaches while significantly reducing training time, particularly in data-scarce settings.

## Method Summary
The proposed framework combines deterministic LF models with Bayesian residual modeling through a linear transfer-learning approach. First, an LF surrogate (KRR or DNN) is trained on LF data. Then, a linear regression model is fit to map LF predictions to HF data using features that include LF outputs and their transformations. The residual between actual HF data and transfer-learning predictions is modeled using either GPR (for KRR-LR-GPR) or BNN (for DNN-LR-BNN). This architecture allows the framework to leverage large LF datasets efficiently while maintaining uncertainty quantification through Bayesian modeling of the residual component.

## Key Results
- Achieves comparable performance to state-of-the-art multi-fidelity approaches while significantly reducing training time, particularly in data-scarce settings
- Outperforms other architectures by effectively mitigating overfitting in data-rich scenarios
- Demonstrates effectiveness on various numerical examples and engineering problems, including material structure-property prediction and aerodynamic coefficient estimation

## Why This Works (Mechanism)

### Mechanism 1
Linear transfer-learning via regression on LF model outputs reduces HF residual complexity. The LF surrogate f_l(x) is trained first, then a linear regression g(x) with features [1, f_l(x), f_l(x)^2, ...] is fit to HF data. This transfers most HF variance into g, leaving only small residuals for Bayesian modeling. Core assumption: The LF and HF models are correlated enough that a simple polynomial in f_l(x) explains much of the HF variation. Break condition: LF and HF are poorly correlated (r near 0), making g(x) ineffective.

### Mechanism 2
KRR + GPR combination offers O((N_h)^3 + (N_l)^2) complexity vs O((N_h + N_l)^3) for full GPR. KRR handles large LF datasets with quadratic cost, while GPR is only used on HF data (typically smaller). The residual GPR is trained on y_h - m(x_h)^T rho, which is a smaller, simpler problem. Core assumption: LF datasets are much larger than HF datasets. Break condition: HF dataset becomes large, negating KRR advantage.

### Mechanism 3
BNN residual modeling captures epistemic uncertainty without overfitting when paired with deterministic LF. After transfer-learning, the residual r(x) = y_h - m(x_h)^T rho is modeled by a BNN, which learns aleatoric and epistemic uncertainty. The deterministic LF ensures LF uncertainty isn't propagated incorrectly. Core assumption: Residuals are smooth enough for BNN to model effectively. Break condition: Residuals are too complex or discontinuous for BNN.

## Foundational Learning

- Concept: Gaussian Process Regression (GPR) basics
  - Why needed here: GPR is the core Bayesian surrogate for HF residuals in KRR-LR-GPR and for HF in DNN-LR-BNN
  - Quick check question: What is the role of the covariance function in GPR predictions?

- Concept: Bayesian Neural Networks (BNNs) and inference methods
  - Why needed here: BNNs model the residual in DNN-LR-BNN and are central to epistemic uncertainty quantification
  - Quick check question: Why can't standard optimization algorithms be used directly on BNNs?

- Concept: Kernel Ridge Regression (KRR) as deterministic GPR point estimate
  - Why needed here: KRR is the scalable LF surrogate in KRR-LR-GPR, chosen for large LF datasets
  - Quick check question: How does KRR differ from GPR in terms of uncertainty quantification?

## Architecture Onboarding

- Component map: LF model (KRR or DNN) -> Transfer-learning (Linear Regression) -> HF residual (GPR or BNN) -> Prediction
- Critical path: 1. Train LF model on LF data 2. Compute LF predictions for HF inputs 3. Fit linear regression (transfer-learning) to LF→HF mapping 4. Train Bayesian residual model on residuals 5. Combine for final prediction
- Design tradeoffs: KRR-LR-GPR: Fast, scalable LF, but limited to low-dim or small HF; DNN-LR-BNN: Flexible, high-dim capable, but more hyperparameter tuning; Linear vs quadratic transfer-learning: Quadratic may overfit; linear often sufficient
- Failure signatures: Poor correlation (r < 0.3): Transfer-learning fails, residual dominates; Large HF dataset: KRR-LR-GPR training time degrades; BNN underfitting: Residual uncertainty underestimated; Overconfident predictions in extrapolation: BNN uncertainty too narrow
- First 3 experiments: 1. Implement KRR-LR-GPR on Forrester function (1D, known LF→HF correlation) 2. Test DNN-LR-BNN on 1D illustrative function with different LF correlations 3. Compare training times on synthetic large LF, small HF datasets

## Open Questions the Paper Calls Out

### Open Question 1
How does the KRR-LR-GPR model's performance scale when the LF dataset size exceeds millions of points? Basis in paper: The paper states KRR-LR-GPR "can be trained on larger LF datasets" than MF-GPR methods, but only demonstrates up to a few thousand points. Why unresolved: The paper only tests LF datasets up to 3200 points, and does not explore the practical limits of the KRR-LR-GPR model with very large datasets. What evidence would resolve it: Empirical tests showing KRR-LR-GPR performance and training time on LF datasets ranging from 10,000 to 1,000,000 points, compared to baseline MF-GPR methods.

### Open Question 2
Does the linear transfer-learning assumption in the proposed framework limit its applicability to problems with highly nonlinear LF-HF relationships? Basis in paper: The paper notes "if LF and HF data tend to be linearly correlated...the DNN-LR-BNN model performs better" and discusses cases where nonlinear relationships exist. Why unresolved: While the paper demonstrates effectiveness in various cases, it does not systematically explore the boundary conditions where linear transfer-learning fails. What evidence would resolve it: A comprehensive study testing the proposed framework on benchmark problems with known nonlinear LF-HF relationships, quantifying the performance degradation as nonlinearity increases.

### Open Question 3
How sensitive is the DNN-LR-BNN model's uncertainty quantification to the choice of BNN inference method (e.g., pSGLD vs Hamiltonian Monte Carlo)? Basis in paper: The paper states "there are many viable strategies for inference in BNNs" and uses pSGLD for scalability, but does not compare different inference methods. Why unresolved: The paper only demonstrates pSGLD for BNN inference and does not explore how alternative methods might affect uncertainty quantification accuracy. What evidence would resolve it: Direct comparison of DNN-LR-BNN performance using different BNN inference methods (pSGLD, HMC, variational inference) on identical problems, measuring both prediction accuracy and quality of uncertainty estimates.

## Limitations
- BNN inference using pSGLD requires careful hyperparameter tuning that isn't fully specified
- Effectiveness assumes strong LF-HF correlation (r > 0.7), which may not hold in real-world scenarios
- Scalability claims for KRR-LR-GPR need verification when HF datasets grow large

## Confidence
- **High Confidence**: The KRR-LR-GPR architecture for data-scarce scenarios (tested on Forrester function with strong theoretical justification)
- **Medium Confidence**: The DNN-LR-BNN architecture for data-rich scenarios (promising but requires more extensive hyperparameter validation)
- **Low Confidence**: The material structure-property linkages results (limited detail on data generation and simulation parameters)

## Next Checks
1. **Correlation Sensitivity Test**: Systematically vary LF-HF correlation (r = 0.3, 0.5, 0.7, 0.9) on the Forrester function to determine the minimum correlation threshold for effective transfer-learning.

2. **Scalability Boundary**: Test KRR-LR-GPR on progressively larger HF datasets (N_h = 10, 50, 100, 500) to empirically verify the claimed complexity advantage and identify the crossover point with full GPR.

3. **BNN Hyperparameter Sweep**: Perform systematic validation of BNN architecture choices (layer width, depth, learning rate) on the NACA 0012 dataset to establish robust hyperparameter guidelines for the DNN-LR-BNN configuration.