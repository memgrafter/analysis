---
ver: rpa2
title: 'Position: Theory of Mind Benchmarks are Broken for Large Language Models'
arxiv_id: '2412.19726'
source_url: https://arxiv.org/abs/2412.19726
tags:
- theory
- mind
- action
- large
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that existing theory of mind benchmarks for LLMs
  are flawed because they only measure literal theory of mind (predicting behavior)
  rather than functional theory of mind (adapting behavior in response). The authors
  demonstrate that many open-source LLMs show strong literal theory of mind capabilities
  but struggle with functional theory of mind, even in simple repeated matrix games.
---

# Position: Theory of Mind Benchmarks are Broken for Large Language Models

## Quick Facts
- arXiv ID: 2412.19726
- Source URL: https://arxiv.org/abs/2412.19726
- Reference count: 40
- Primary result: Existing theory of mind benchmarks for LLMs only measure literal ToM (predicting behavior) rather than functional ToM (adapting behavior), creating a persistent performance gap.

## Executive Summary
This paper argues that current theory of mind benchmarks for large language models are fundamentally flawed because they only measure the ability to predict others' behavior (literal ToM) rather than the ability to adapt one's own behavior based on those predictions (functional ToM). Through experiments with various open-source LLMs including DeepSeek-R1, the authors demonstrate that while these models show strong literal theory of mind capabilities, they consistently struggle with functional theory of mind even in simple repeated matrix games. The paper advocates for interactive benchmarks that directly assess functional theory of mind when LLMs interact with agents from different personas and social contexts, arguing that passive question-answering benchmarks cannot capture the adaptive nature of real-world theory of mind.

## Method Summary
The paper evaluates theory of mind in LLMs using repeated matrix games (Rock-Paper-Scissors, Iterated Battle of the Sexes, Iterated Prisoner's Dilemma) with various prompting strategies including LM, QA, CoT, Reflexion, Plans+Insights, Social, Oracle, S2A, and CoT 3-shot. Models interact with partner agents using single-action and tit-for-tat policies over 100 rounds per episode. The evaluation measures functional theory of mind regret (Î”Functional/ð‘‡) and literal theory of mind accuracy (ToM %) through in-context learning without fine-tuning. The study tests multiple open-source models including LLAMA-2, LLAMA-3, Mixtral, Mistral Large 2, and DeepSeek-R1-Distill-Qwen-32B.

## Key Results
- LLMs demonstrate strong literal theory of mind (high prediction accuracy) but struggle with functional theory of mind (poor adaptive behavior) even in simple games
- Social prompting that provides literal ToM predictions as input does not improve functional ToM performance, indicating the gap is not simply about reasoning over long contexts
- The functional theory of mind gap persists across different prompting strategies, game types, and LLM models, suggesting a fundamental limitation in how LLMs integrate predictions into decision-making

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can predict other agents' behavior (literal ToM) without using that prediction to adapt their own behavior (functional ToM).
- **Mechanism:** The model learns to predict next tokens in sequences that include other agents' actions, but this prediction is not integrated into decision-making policies. The model may be conditioning on patterns that correlate with other agents' behavior without forming an explicit model of their mental state.
- **Core assumption:** The LLM's training objective (next token prediction) can produce accurate predictions of other agents' behavior without requiring the model to reason about why those agents behave that way or how to respond optimally.
- **Evidence anchors:**
  - [abstract]: "strong literal theory of mind capabilities" but "struggle with functional theory of mind"
  - [section 2.1]: Definition 2.1 explicitly separates literal theory of mind prediction from the policy optimization needed for functional theory of mind
  - [corpus]: The neighbor papers on theory of mind evaluation suggest this is a recognized distinction in the field
- **Break condition:** If the LLM's training data includes sufficient examples of sequences where predictions of others' behavior are followed by optimal responses to those predictions, the model might learn to integrate prediction and response.

### Mechanism 2
- **Claim:** The gap between literal and functional ToM exists because LLMs lack process consistency in their reasoning.
- **Mechanism:** LLMs can generate different explanations for the same behavior across different prompts, indicating that their reasoning process is not consistent. This inconsistency prevents them from reliably using predictions about others to inform their own actions.
- **Core assumption:** Humans take for granted that the ability to predict others' behavior will be consistently applied within their own reasoning process, but this assumption does not hold for current LLMs.
- **Evidence anchors:**
  - [abstract]: "we expect that humans will engage in a consistent reasoning process across various questions about a situation, but this is known to not be the case for current LLMs"
  - [section 1]: "LLMs can come up with compelling explanations for what they do that have very little to do with their actual reasoning process"
  - [corpus]: The neighbor paper on "Hypothesis-Driven Theory-of-Mind Reasoning" suggests this is an active area of investigation
- **Break condition:** If prompting strategies can enforce process consistency, or if future LLM architectures inherently maintain consistent reasoning across different contexts.

### Mechanism 3
- **Claim:** Interactive benchmarks are needed because passive question-answering benchmarks cannot capture the adaptive nature of functional ToM.
- **Mechanism:** Functional ToM requires the ability to adapt behavior based on observed interactions over time, which cannot be measured through static question-answering tasks that only test prediction ability.
- **Core assumption:** Theory of mind in practical applications involves continuous interaction and adaptation, not just one-time predictions about behavior.
- **Evidence anchors:**
  - [abstract]: "Current LLM theory of mind benchmarks are broken because they only measure literal theory of mind... rather than functional theory of mind"
  - [section 3]: "LLMs have also been successfully employed to simulate a diversity of personas... However, these tasks all lack interactivity"
  - [corpus]: The neighbor paper on "The Decrypto Benchmark for Multi-Agent Reasoning" indicates interest in more interactive evaluation methods
- **Break condition:** If a new evaluation method could simulate long-term interactive behavior through multiple sequential questions that capture adaptation over time.

## Foundational Learning

- **Concept:** Distinction between prediction and decision-making
  - Why needed here: The paper's core argument hinges on understanding that predicting behavior (literal ToM) is fundamentally different from using those predictions to make decisions (functional ToM)
  - Quick check question: If an LLM can accurately predict that an opponent in Rock, Paper, Scissors will always play "Rock," but still plays randomly itself, which type of ToM is it demonstrating?

- **Concept:** Markov games and partially observable stochastic games
  - Why needed here: The formal framework used to describe multi-agent interactions where theory of mind is relevant
  - Quick check question: In a partially observable stochastic game, what information does each agent have access to when making decisions?

- **Concept:** Regret minimization and game theory equilibria
  - Why needed here: The paper uses regret as a metric for functional ToM and contrasts this with game-theoretic approaches that optimize for worst-case performance
  - Quick check question: How does regret differ from accumulated reward as a performance metric in multi-agent settings?

## Architecture Onboarding

- **Component map:** LLM models -> Prompt generation -> Action selection -> Interaction with partner -> History update -> Next prompt generation -> Evaluation
- **Critical path:** LLM â†’ Prompt generation â†’ Action selection â†’ Interaction with partner â†’ History update â†’ Next prompt generation â†’ Evaluation
- **Design tradeoffs:** Longer context allows more historical information but increases computational cost and may exceed model limits; more explicit reasoning prompts may improve functional ToM but could reduce literal ToM accuracy.
- **Failure signatures:** High literal ToM accuracy with low functional ToM performance; inconsistent behavior across similar interaction histories; poor adaptation to partner policy changes.
- **First 3 experiments:**
  1. Replicate the Rock, Paper, Scissors experiment with a single-action partner using LM prompting to verify the literal vs functional ToM gap
  2. Test the same setup with Social prompting to see if providing literal ToM predictions as input improves functional performance
  3. Implement the Oracle prompting variant to determine if the issue is with reasoning over long contexts or with integrating predictions into decisions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to LLM prompting strategies could effectively bridge the gap between literal and functional theory of mind performance in repeated matrix games?
- Basis in paper: [explicit] The paper identifies a persistent gap between literal theory of mind (predicting behavior) and functional theory of mind (adapting behavior in response) across various prompting strategies and models.
- Why unresolved: While the paper tests multiple prompting strategies (CoT, Social Prompting, Reflexion, Plans+Insights), none fully close the gap. The analysis shows that even when literal theory of mind predictions are provided as input, LLMs fail to generate rational responses to these predictions.
- What evidence would resolve it: A comprehensive study comparing novel prompting architectures specifically designed to connect literal theory of mind predictions to functional decision-making, with ablation studies showing which components are essential for closing this gap.

### Open Question 2
- Question: How does the performance of LLM theory of mind change when agents interact over longer horizons (beyond 100 steps) in repeated games?
- Basis in paper: [explicit] The paper notes that LLMs struggle with reasoning over long interaction histories, conducting experiments with up to 100 steps but finding performance still suboptimal. The authors specifically investigate this difficulty and find that even when oracle information is provided, LLMs fail to perform effective rational reasoning.
- Why unresolved: The experiments are limited to 100 steps, and the paper identifies long-context reasoning as a core issue but doesn't explore whether performance improves with even longer horizons or what the limiting factors are.
- What evidence would resolve it: Systematic experiments testing LLM performance at various horizon lengths (100, 500, 1000+ steps) with different context summarization techniques, identifying at what point performance plateaus and why.

### Open Question 3
- Question: Does fine-tuning or parameter-efficient tuning of LLMs on theory of mind tasks fundamentally change the relationship between literal and functional theory of mind capabilities?
- Basis in paper: [explicit] The paper acknowledges that RL-style fine-tuning would easily solve the functional theory of mind problem demonstrated in their matrix game experiments, but notes that most LLM services rely on in-context learning rather than model weight tuning.
- Why unresolved: The paper deliberately focuses on in-context learning to match real-world LLM deployment scenarios, leaving open whether fine-tuning changes the fundamental disconnect between literal and functional theory of mind that the paper identifies.
- What evidence would resolve it: Comparative studies of the same models before and after fine-tuning on theory of mind tasks, measuring both literal and functional theory of mind performance, and analyzing whether fine-tuning creates a stronger correlation between the two metrics.

## Limitations

- The findings may reflect limitations in prompting strategies rather than fundamental LLM capabilities, as the study uses relatively simple repeated games with fixed partner policies
- The evaluation framework relies entirely on in-context learning without fine-tuning, leaving open whether architectural limitations or training data deficiencies drive the observed performance gaps
- The generalizability of findings to real-world social interactions beyond simple matrix games is uncertain, as the paper acknowledges this limitation but does not provide evidence for broader applicability

## Confidence

- **High confidence**: The distinction between literal and functional theory of mind is well-founded and the experimental results showing strong literal but weak functional performance are reproducible. The claim that current benchmarks fail to assess functional ToM is supported by direct evidence.
- **Medium confidence**: The assertion that this gap reflects fundamental limitations in LLM reasoning rather than prompting strategies. While the evidence is compelling, alternative explanations (such as inadequate prompt engineering or insufficient context length) cannot be fully ruled out.
- **Low confidence**: The generalizability of findings to real-world social interactions beyond simple matrix games. The paper acknowledges this limitation but does not provide evidence for broader applicability.

## Next Checks

1. **Prompt engineering validation**: Systematically test whether the functional ToM gap persists when using optimized prompt templates that explicitly instruct models to use their literal ToM predictions for decision-making, including variations in chain-of-thought length and explicit reasoning instructions.

2. **Adaptive partner challenge**: Replace the fixed partner policies with agents that adapt their strategies based on the LLM's behavior, then measure whether the functional ToM gap widens or narrows as the interaction complexity increases.

3. **Real-world scenario testing**: Design an interactive benchmark using a simplified negotiation or collaborative task where functional ToM directly impacts task success, then compare LLM performance against human baselines to assess practical relevance of the findings.