---
ver: rpa2
title: 'Atoxia: Red-teaming Large Language Models with Target Toxic Answers'
arxiv_id: '2408.14853'
source_url: https://arxiv.org/abs/2408.14853
tags:
- target
- llms
- toxdet
- arxiv
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a target-driven attack paradigm for red-teaming
  LLMs that generates questions and preliminary answers conditioned on target toxic
  responses. The approach uses reinforcement learning with the target model itself
  as the reward model, eliminating the need for a separate reward model.
---

# Atoxia: Red-teaming Large Language Models with Target Toxic Answers

## Quick Facts
- arXiv ID: 2408.14853
- Source URL: https://arxiv.org/abs/2408.14853
- Reference count: 4
- Primary result: Target-driven attack paradigm achieves 100% ASR@10 on training sets and >90% on test sets for open-source models, with 50% ASR@10 on black-box GPT-4o

## Executive Summary
This paper introduces Atoxia, a novel red-teaming method that attacks LLMs by generating questions and preliminary answers conditioned on target toxic responses. The approach uses reinforcement learning with the target model itself as the reward model, eliminating the need for separate reward model training. ToxDet, the attacker LLM, is optimized to produce content that elicits intended toxic responses from the target model. Experiments demonstrate that this target-driven paradigm outperforms traditional jailbreaking methods, achieving high attack success rates on both open-source and closed-source models.

## Method Summary
Atoxia employs a target-driven attack paradigm where ToxDet generates questions and preliminary answers conditioned on target toxic responses. The method uses reinforcement learning with the target LLM as the reward model, where the log-probability of the intended toxic response serves as the reward signal. ToxDet is trained by interacting with the target LLM, generating content that maximizes the probability of eliciting the desired toxic response. The approach includes preliminary answers alongside questions to better facilitate the generation of intended toxic responses. The method is evaluated on AdvBench and HH-Harmless datasets, testing both gray-box (open-source) and black-box (closed-source) target models.

## Key Results
- Achieves 100% ASR@10 on training sets and over 90% on test sets for open-source models
- Demonstrates strong transferability to black-box models like GPT-4o, achieving around 50% ASR@10
- Faster than manual or gradient-based methods while maintaining high attack success rates
- Exposes vulnerabilities in both open-source and closed-source LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Target-driven attack paradigm generates more effective adversarial prompts than traditional jailbreaking methods
- Mechanism: ToxDet generates questions and preliminary answers conditioned directly on target toxic responses, increasing alignment between generated content and target output
- Core assumption: The attacker LLM's knowledge about what prompts lead to specific toxic responses is better captured when training directly on target responses
- Evidence anchors:
  - [abstract] "Given a particular harmful answer, Atoxia generates a corresponding user query and a misleading answer opening"
  - [section] "Instead of relying on prompt engineering, we focus on directly attacking the models"
  - [corpus] Weak - no direct citations about conditioning on target responses

### Mechanism 2
- Claim: Using target model itself as reward model eliminates need for separate reward model training
- Mechanism: The log-probability of target model generating intended toxic response serves as reward signal for training ToxDet
- Core assumption: Target model's internal probability estimates accurately reflect how likely generated content will lead to intended response
- Evidence anchors:
  - [abstract] "ToxDet is trained by interacting with the target LLM and receiving reward signals from it"
  - [section] "Instead of employing a separate pretrained reward model, we propose to use the target model itself"
  - [corpus] Weak - no direct citations about using target model as reward

### Mechanism 3
- Claim: Transferability from gray-box to black-box models demonstrates universal vulnerability in LLMs
- Mechanism: Despite training on open-source models, ToxDet successfully attacks black-box models like GPT-4o
- Core assumption: Different LLMs share sufficient common vulnerabilities and generation patterns
- Evidence anchors:
  - [abstract] "Although trained on open-source target LLMs, our ToxDet can also be transferred to attack black-box models"
  - [section] "Remarkable, despite ToxDet's design being customized for a specific target model, it demonstrates significant transferability"
  - [corpus] Weak - no direct citations about transferability in this context

## Foundational Learning

- Concept: Reinforcement Learning with Log-Probabilities as Rewards
  - Why needed here: The method uses target model's log-probability of generating toxic response as reward signal for training attacker
  - Quick check question: How does using log-probability as reward differ from using discrete success/failure signals?

- Concept: Conditional Generation and Prompt Engineering
  - Why needed here: ToxDet generates questions and preliminary answers conditioned on target toxic responses
  - Quick check question: Why might generating a preliminary answer alongside the question be more effective?

- Concept: Adversarial Attack Transferability
  - Why needed here: The method demonstrates transferability from gray-box to black-box models
  - Quick check question: What characteristics of different LLMs might enable or prevent successful transfer?

## Architecture Onboarding

- Component map:
  - ToxDet (attacker LLM) -> Target LLM (reward model) -> Training pipeline -> Evaluation pipeline

- Critical path:
  1. ToxDet receives target toxic response as input
  2. ToxDet generates question and preliminary answer
  3. Generated content queries target LLM
  4. Target LLM provides log-probability of intended response
  5. Log-probability serves as reward for ToxDet update
  6. Repeat until convergence

- Design tradeoffs:
  - Training with target model as reward vs. separate reward model: Faster training but potential overfitting
  - Including preliminary answers vs. only questions: Better attack success but longer generated content
  - Using log-probabilities vs. discrete success signals: Finer-grained rewards but potentially noisier signals

- Failure signatures:
  - Low ASR@10 on training set: ToxDet not learning effective attack patterns
  - High perplexity of generated content: Content not human-readable, may be detected as adversarial
  - Successful training but poor transferability: Overfitting to specific target model architecture

- First 3 experiments:
  1. Train ToxDet on small subset of AdvBench with Mistral-7b target, measure ASR@10 on training set
  2. Test transferability by attacking GPT-3.5-turbo with ToxDet trained on Mistral-7b
  3. Ablation study: Train ToxDet without preliminary answers to quantify their contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the transferability of ToxDet from open-source to black-box models like GPT-4o compare to other red-teaming approaches?
- Basis in paper: Explicit - The paper states ToxDet achieves around 50% ASR@10 on black-box models like GPT-4o
- Why unresolved: The paper provides comparative results but doesn't explore underlying reasons for differences
- What evidence would resolve it: Comparative studies of different red-teaming approaches on various black-box models

### Open Question 2
- Question: What are the limitations of using the target LLM itself as the reward model in ToxDet's training process?
- Basis in paper: Inferred - The paper mentions using target LLM as reward model but doesn't discuss potential limitations
- Why unresolved: The paper doesn't explore potential issues like bias, overfitting, or lack of generalizability
- What evidence would resolve it: Empirical studies comparing performance of different reward models

### Open Question 3
- Question: How does the inclusion of preliminary answers in ToxDet's generated content impact attack success rate?
- Basis in paper: Explicit - The paper discusses use of preliminary answers and compares effectiveness with and without them
- Why unresolved: The paper doesn't quantify the extent to which preliminary answers improve attack effectiveness
- What evidence would resolve it: Quantitative analysis of attack success rates with and without preliminary answers

## Limitations

- Transferability results show significant performance degradation from gray-box to black-box models
- Reliance on GPT-4 evaluation for toxicity assessment introduces potential bias
- Method's effectiveness depends heavily on attacker LLM's ability to generate contextually appropriate preliminary answers
- Log-probability reward signals assume correlation between probability and successful attack outcomes

## Confidence

**High Confidence:**
- Target-driven attack paradigm outperforms traditional suffix optimization methods
- Method achieves high attack success rates on open-source models when trained on same distribution
- ToxDet can generate readable, human-like content with reasonable perplexity scores

**Medium Confidence:**
- Method demonstrates significant transferability to black-box models like GPT-4o
- Using target model itself as reward model is more efficient than training separate reward models
- Inclusion of preliminary answers substantially improves attack success rates

**Low Confidence:**
- Method will maintain similar effectiveness against future LLM architectures with enhanced safety mechanisms
- Transferability results generalize to all black-box models beyond tested GPT series
- Log-probability reward signal provides optimal gradients for RL training in all scenarios

## Next Checks

1. **Cross-Domain Transferability Test**: Train ToxDet on AdvBench data targeting social media toxicity, then evaluate performance on medical misinformation or financial scams from different datasets.

2. **Safety Mechanism Robustness Evaluation**: Test ToxDet against target models specifically hardened against known adversarial attack patterns and compare ASR@10 before and after safety enhancements.

3. **Human Evaluation Validation**: Conduct blind study where human raters evaluate sample of generated questions and preliminary answers for both attack success and content quality, comparing human judgments against automated GPT-4 evaluation.