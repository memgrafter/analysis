---
ver: rpa2
title: 'CityBench: Evaluating the Capabilities of Large Language Models for Urban
  Tasks'
arxiv_id: '2406.13945'
source_url: https://arxiv.org/abs/2406.13945
tags:
- urban
- tasks
- llms
- data
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CityBench is the first systematic evaluation platform for assessing
  large language models (LLMs) and vision-language models (VLMs) on diverse urban
  tasks. It integrates multi-source urban data (geospatial, visual, human activity)
  and simulates fine-grained urban dynamics via CitySimu.
---

# CityBench: Evaluating the Capabilities of Large Language Models for Urban Tasks

## Quick Facts
- arXiv ID: 2406.13945
- Source URL: https://arxiv.org/abs/2406.13945
- Reference count: 40
- Key outcome: CityBench is the first systematic evaluation platform for assessing LLMs and VLMs on diverse urban tasks, revealing significant performance gaps across task categories and geographical regions.

## Executive Summary
CityBench introduces the first comprehensive evaluation platform for assessing large language models (LLMs) and vision-language models (VLMs) on urban tasks. The benchmark integrates multi-source urban data including geospatial, visual, and human activity information, simulates fine-grained urban dynamics through CitySimu, and evaluates models across 8 tasks in 13 global cities. The evaluation reveals that LLMs excel at commonsense and semantic understanding tasks but struggle with professional knowledge and numerical tasks, while also exhibiting significant geospatial bias across different cities.

## Method Summary
CityBench consists of three integrated modules: CityData for collecting and processing diverse urban data from multiple sources, CitySimu for simulating urban dynamics and providing interactive APIs, and CityBench itself which contains 8 evaluation tasks across perception-understanding (street view geolocalization, geospatial prediction, infrastructure inference, GeoQA) and decision-making (mobility prediction, urban exploration, outdoor navigation, traffic signal control) categories. The framework evaluates 30 well-known models using carefully designed metrics that capture both static and dynamic urban task requirements.

## Key Results
- LLMs/VLMs excel in commonsense and semantic understanding tasks (e.g., human dynamics, image inference) but struggle with professional knowledge and numerical tasks (e.g., geospatial prediction, traffic control)
- Significant geospatial bias exists across cities, with models performing well in major international cities but poorly in lesser-known cities like CapeTown and Nairobi
- Among evaluated models, GPT-4 series performs well on most tasks, but several open-source models achieve better results on multiple other tasks, with no single model consistently performing well across all tasks

## Why This Works (Mechanism)

### Mechanism 1
The integration of CityData, CitySimu, and CityBench creates a comprehensive evaluation framework that addresses limitations of existing urban LLM benchmarks. CityData collects diverse urban data, CitySimu simulates fine-grained dynamics, and CityBench provides evaluation tasks across perception-understanding and decision-making categories. The core assumption is that these three modules can work together seamlessly to provide both static and dynamic urban data for LLM evaluation.

### Mechanism 2
The benchmark reveals significant performance gaps in LLMs and VLMs across different urban task categories and geographical regions. By evaluating 30 models across 8 tasks in 13 cities, CityBench exposes limitations in numerical tasks and geospatial bias in model performance. The core assumption is that the diversity of tasks and cities provides sufficient coverage to reveal systematic performance gaps.

### Mechanism 3
The interactive simulation environment enables evaluation of decision-making capabilities that static benchmarks cannot assess. CitySimu provides APIs for controlling urban dynamics and sensing environments, allowing LLMs to interact with dynamic urban scenarios like traffic signal control and urban exploration. The core assumption is that interactive simulation can effectively model real-world urban dynamics that static data cannot capture.

## Foundational Learning

- **Concept**: Multi-modal data integration
  - **Why needed here**: Urban tasks require combining geospatial, visual, and activity data to provide comprehensive context for LLM evaluation.
  - **Quick check question**: Can you explain how integrating street view images with OSM data improves urban task evaluation compared to using either modality alone?

- **Concept**: Interactive simulation for AI evaluation
  - **Why needed here**: Static benchmarks cannot assess an LLM's ability to make sequential decisions in dynamic environments.
  - **Quick check question**: What are the key differences between evaluating an LLM on a static image geolocalization task versus an interactive urban navigation task?

- **Concept**: Geospatial bias analysis
  - **Why needed here**: Understanding how model performance varies across cities reveals limitations in training data coverage and generalization.
  - **Quick check question**: How would you design an experiment to quantify whether an LLM's performance on urban tasks correlates with the amount of publicly available information about a city?

## Architecture Onboarding

- **Component map**: CityData -> CitySimu -> CityBench evaluation tasks
- **Critical path**: Load model into CityBench -> CityData provides urban data -> CitySimu generates interactive scenarios -> Model responses evaluated against ground truth -> Results aggregated and reported
- **Design tradeoffs**: The system trades computational complexity for evaluation comprehensiveness. Interactive simulation requires significant resources but provides more realistic evaluation than static benchmarks. The multi-city approach increases coverage but also increases data management complexity.
- **Failure signatures**: Common failure modes include CityData API failures causing missing data, CitySimu simulation errors leading to invalid scenarios, model format errors in responses, and evaluation metrics failing to capture task nuances.
- **First 3 experiments**:
  1. Test basic data retrieval: Use CityData APIs to fetch street view images and OSM data for a single city to verify data integration works.
  2. Validate simulation environment: Run a simple urban exploration task in CitySimu to ensure interactive scenarios generate correctly.
  3. Baseline evaluation: Evaluate a known model (like GPT-4) on a single task to verify the complete evaluation pipeline functions end-to-end.

## Open Questions the Paper Calls Out

### Open Question 1
How can the performance gap between proprietary and open-source LLMs on urban tasks be systematically analyzed and narrowed? The paper observes performance variability but does not provide systematic analysis of why certain open-source models outperform proprietary ones on specific urban tasks.

### Open Question 2
What are the underlying factors contributing to geospatial bias in LLMs and VLMs across different cities? While the paper identifies correlation with online presence, it acknowledges this is preliminary evidence and suggests more diverse factors contribute to geographical bias.

### Open Question 3
How can LLMs be effectively adapted to improve performance on numerical urban tasks without compromising generalizability? The paper shows LLMs struggle significantly with numerical tasks but does not propose specific architectural modifications that could maintain both numerical accuracy and broad task capability.

## Limitations
- The integration of multi-source urban data may not fully capture real-world urban complexity, particularly in cities with limited data availability
- The CitySimu simulation environment may not accurately model all urban dynamics, especially rare events or edge cases critical for robust evaluation
- The evaluation framework's reliance on existing LLM capabilities may introduce bias, as models trained on general web data may not be optimally suited for urban-specific tasks

## Confidence
- **High Confidence**: The claim that CityBench is the first systematic evaluation platform for urban tasks is well-supported by the comprehensive scope and methodology.
- **Medium Confidence**: The assertion that LLMs/VLMs excel in commonsense tasks but struggle with numerical tasks is reasonable but would benefit from more detailed analysis of failure cases.
- **Low Confidence**: The claim of "significant geospatial bias across cities" requires careful interpretation due to potential confounding factors from data quality and availability variations.

## Next Checks
1. Conduct a systematic audit of urban data coverage for each city in the benchmark, including data source diversity, temporal coverage, and spatial resolution to quantify the impact of data availability on model performance.
2. Design controlled experiments comparing model performance on CitySimu-generated scenarios versus real-world urban data for identical situations to assess simulation fidelity.
3. Perform a detailed breakdown of model performance across all eight tasks, focusing on failure modes and error patterns through both qualitative case studies and quantitative metrics.