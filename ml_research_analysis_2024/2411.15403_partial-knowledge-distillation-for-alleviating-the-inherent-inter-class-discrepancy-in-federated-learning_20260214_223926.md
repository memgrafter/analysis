---
ver: rpa2
title: Partial Knowledge Distillation for Alleviating the Inherent Inter-Class Discrepancy
  in Federated Learning
arxiv_id: '2411.15403'
source_url: https://arxiv.org/abs/2411.15403
tags:
- xxxxx
- classes
- class
- xxxxxx
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a previously unreported phenomenon in federated
  learning: weak classes consistently exist even when class distributions are balanced
  globally and locally, leading to inherent inter-class accuracy discrepancies (ICD)
  of up to 45.4% on FashionMNIST and 36.9% on CIFAR-10. Unlike minority classes in
  long-tailed problems, these weak classes arise from inherent data characteristics
  and model challenges in distinguishing similar high-level features between certain
  classes.'
---

# Partial Knowledge Distillation for Alleviating the Inherent Inter-Class Discrepancy in Federated Learning

## Quick Facts
- arXiv ID: 2411.15403
- Source URL: https://arxiv.org/abs/2411.15403
- Reference count: 24
- Primary result: Weak classes consistently exist even under balanced distributions, with ICD up to 45.4% (FashionMNIST) and 36.9% (CIFAR-10)

## Executive Summary
This paper identifies a previously unreported phenomenon in federated learning where weak classes exist even when class distributions are balanced globally and locally, leading to inherent inter-class accuracy discrepancies (ICD) of up to 45.4% on FashionMNIST and 36.9% on CIFAR-10. Unlike minority classes in long-tailed problems, these weak classes arise from inherent data characteristics and model challenges in distinguishing similar high-level features between certain classes. The authors propose a federated partial knowledge distillation (PKD) method that activates specialized expert models only when misclassifications occur between weak class groups. Knowledge is selectively transferred from these experts to the global model during training. Experimental results show the method improves weak class accuracy by 10.7% on FashionMNIST and 6.6% on CIFAR-10, effectively reducing ICD while maintaining overall accuracy.

## Method Summary
The proposed federated partial knowledge distillation (PKD) method operates in three stages: warmup (10-20 rounds of baseline FedAvg training), expert learning (25 rounds where specialized expert models are trained for weak class groups), and FL with partial KD (where experts are selectively activated upon misclassification). The method identifies weak class groups by analyzing misclassification patterns between similar classes, then trains expert models for each weak class group within the federated learning framework. Knowledge is transferred from experts to the global model only when misclassification occurs within weak class groups, using KL divergence between student and expert predictions. This selective approach reduces computational overhead while targeting the specific accuracy discrepancy problem.

## Key Results
- Improves weak class accuracy by 10.7% on FashionMNIST and 6.6% on CIFAR-10
- Reduces inherent inter-class accuracy discrepancy (ICD) from 45.4% to 34.7% on FashionMNIST
- Maintains overall accuracy while specifically targeting weak class performance
- Outperforms existing techniques like re-weighting or data augmentation for ICD reduction

## Why This Works (Mechanism)
The PKD method works by recognizing that certain class pairs are inherently difficult to distinguish due to similar high-level features, creating "weak class groups." By training specialized expert models for these groups and activating them only when misclassification occurs, the method provides targeted knowledge transfer without the computational cost of full knowledge distillation. The selective activation mechanism ensures that expert models are only engaged when their specialized knowledge is actually needed, making the approach efficient for federated learning scenarios.

## Foundational Learning
- Federated Learning: Distributed model training across multiple clients without sharing raw data; needed to understand the baseline problem and PKD integration
- Knowledge Distillation: Transferring knowledge from a teacher model to a student model; needed to understand how expert models improve the global model
- Inter-class Discrepancy: Accuracy differences between different classes; needed to understand the ICD problem being addressed
- Weak Class Identification: Detecting classes with systematically lower accuracy; needed to understand how to identify targets for expert models
- KL Divergence: Measuring difference between probability distributions; needed to understand the knowledge transfer mechanism
- Selective Activation: Triggering expert models only when needed; needed to understand the efficiency of the PKD approach

## Architecture Onboarding

**Component Map:**
Global Model -> Weak Class Detection -> Expert Models -> Selective KD Activation -> Global Model Update

**Critical Path:**
Data partitioning → Warmup training → Weak class identification → Expert training → PKD activation → Global model improvement

**Design Tradeoffs:**
- Selective vs. full knowledge distillation: Selective reduces computation but may miss opportunities for improvement
- Expert specialization vs. generalization: Specialized experts perform better on weak classes but add complexity
- Fixed vs. adaptive stage transitions: Fixed rounds are simpler but may not be optimal for all scenarios

**Failure Signatures:**
- Experts not activating: Check threshold settings and misclassification patterns
- High computational overhead: Monitor expert activation frequency (should be 10-15% of samples)
- Limited ICD reduction: Verify weak class identification accuracy and expert model quality

**3 First Experiments:**
1. Baseline FedAvg training with balanced class distributions to establish ICD baseline
2. Weak class identification using misclassification pattern analysis on trained baseline model
3. Expert model training for identified weak class groups with fixed local epochs

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed PKD method perform on datasets with significantly higher class numbers and more complex feature distributions than FashionMNIST and CIFAR-10?

The paper demonstrates PKD effectiveness on FashionMNIST and CIFAR-10, but doesn't explore scalability to more complex datasets with larger class counts or more intricate feature relationships. Performance on datasets like ImageNet with 1000+ classes or medical imaging datasets with subtle class distinctions remains untested.

### Open Question 2
What is the optimal strategy for determining when to transition between the warmup, expert learning, and PKD stages of the algorithm?

The paper uses fixed round counts (e.g., 10-20 warmup rounds, 25 expert learning rounds) without exploring adaptive criteria for stage transitions based on model performance metrics. Fixed round counts may not be optimal across different datasets, model architectures, or federated learning scenarios.

### Open Question 3
How does the PKD method's performance scale with the number of participating clients in federated learning, particularly in extremely large-scale deployments?

The paper shows performance with 10 and 100 clients, but doesn't explore scenarios with thousands of clients typical in real-world federated learning deployments, nor does it analyze communication efficiency at scale. The communication overhead and model aggregation dynamics may fundamentally change with massive client numbers.

## Limitations
- Limited testing on datasets with more than 10 classes
- Fixed stage transition rounds rather than adaptive criteria
- No analysis of computational overhead in large-scale deployments
- Limited quantitative analysis of specific feature-level characteristics causing weak classes

## Confidence
- High confidence in the existence of ICD phenomenon: The methodology for identifying weak classes through misclassification patterns is sound and reproducible
- Medium confidence in the PKD solution effectiveness: While improvement metrics are reported, the ablation study doesn't fully isolate the impact of partial versus full knowledge distillation
- Low confidence in generalizability: The paper doesn't test on datasets with more than 10 classes or explore how weak class patterns change with model architecture complexity

## Next Checks
1. Replicate the weak class identification process on a dataset with more than 10 classes (e.g., CIFAR-100) to verify the method scales to scenarios with more nuanced class similarity relationships
2. Implement an ablation study comparing partial knowledge distillation against full knowledge distillation in the federated setting to quantify the benefit of the selective activation mechanism
3. Test the PKD approach with different model architectures (CNN, transformer-based) to evaluate robustness across architectures and understand whether weak class patterns are architecture-dependent