---
ver: rpa2
title: Hierarchical Multi-Label Classification with Missing Information for Benthic
  Habitat Imagery
arxiv_id: '2409.06618'
source_url: https://arxiv.org/abs/2409.06618
tags:
- learning
- hierarchical
- data
- missing
- hierarchy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores self-supervised learning (SSL) for hierarchical
  multi-label classification (HML) on seafloor imagery. Using the BenthicNet dataset,
  it demonstrates that SSL pre-training on large-scale unlabelled benthic data improves
  classification performance over ImageNet pre-training, especially for small, local
  datasets typical in marine science.
---

# Hierarchical Multi-Label Classification with Missing Information for Benthic Habitat Imagery

## Quick Facts
- arXiv ID: 2409.06618
- Source URL: https://arxiv.org/abs/2409.06618
- Authors: Isaac Xu; Benjamin Misiuk; Scott C. Lowe; Martin Gillis; Craig J. Brown; Thomas Trappenberg
- Reference count: 35
- Key outcome: SSL pre-training on domain-specific benthic data improves HML classification performance over ImageNet pre-training, especially for small benthic datasets, while masking handles missing annotations.

## Executive Summary
This work addresses hierarchical multi-label classification (HML) for seafloor imagery using self-supervised learning (SSL) pre-training on benthic data. The authors demonstrate that pre-training encoders on large unlabelled benthic imagery outperforms traditional ImageNet pre-training for specialized marine science tasks. They introduce a masking strategy to handle missing annotations across multiple hierarchical levels in the CATAMI classification scheme. The approach shows significant improvements in classification accuracy and depth for small, local benthic datasets.

## Method Summary
The method involves pre-training ResNet-50 and ViT-B encoders using SSL techniques (SimSiam, BYOL, Barlow Twins, MoCo, MAE) on an unlabeled BenthicNet-1M subset (1.35 million images). Pre-trained encoders are then frozen and appended with five hierarchical classification heads corresponding to the CATAMI categories. The model is trained using C-HMCNN loss with a masking strategy that handles missing information at bit, sample, and head levels. Evaluation is performed on BenthicNet-Labelled (188,688 images) using AP, HML AP, and Singular F1 metrics.

## Key Results
- SSL pre-trained models outperform ImageNet pre-trained models on small benthic datasets like German Bank 2010 (3,181 images)
- MAE and MoCo-v3 SSL methods show highest average HML AP scores on BenthicNet-Labelled
- Models trained with SSL pre-training can classify at deeper hierarchical levels than those using supervised ImageNet pre-training
- Masking strategy effectively handles three types of missing information (precision, branches, categories) without significant performance degradation

## Why This Works (Mechanism)

### Mechanism 1
Pre-training on large unlabelled benthic data improves performance on small labeled datasets by extracting domain-specific visual features that transfer better than generic ImageNet features. This assumes benthic imagery contains sufficiently different visual patterns from natural images to require domain-specific pre-training.

### Mechanism 2
Hierarchical masking enables training with incomplete annotations by calculating loss only over available annotations at different hierarchical levels. This assumes missing annotations represent genuine absence of information rather than label errors, and that hierarchical structure can be preserved despite missing branches.

### Mechanism 3
Self-supervised pre-training improves hierarchical classification depth by learning rich feature representations that enable more confident predictions at deeper levels. This assumes the additional information captured by SSL enables better discrimination of fine-grained visual features necessary for deeper classification.

## Foundational Learning

- **Self-supervised learning objectives**: Multiple SSL methods (SimSiam, BYOL, Barlow Twins, MoCo, MAE) are used to pre-train encoders without labels. Quick check: What is the key difference between contrastive learning and autoencoder approaches in SSL?

- **Hierarchical multi-label classification**: BenthicNet uses CATAMI scheme with 5 independent hierarchical categories, requiring models to predict multiple labels while respecting hierarchy. Quick check: How does the hierarchical constraint mechanism ensure predictions maintain valid hierarchy paths?

- **Handling missing data in machine learning**: BenthicNet has multiple types of missing information (precision, branches, categories) that must be handled during training. Quick check: What are the three types of missing information identified in the paper, and how does masking address them?

## Architecture Onboarding

- **Component map**: Unlabelled benthic data → SSL pre-training → encoder extraction → hierarchical classification heads → C-HMCNN loss with masking → HML training
- **Critical path**: SSL pre-training on benthic data → feature extraction → hierarchical classification with constraint mechanism → evaluation with masked loss
- **Design tradeoffs**: Domain-specific pre-training vs. general ImageNet features; masking approach vs. imputation; hierarchical constraint vs. simpler multi-label approaches
- **Failure signatures**: Poor performance on small datasets suggests SSL pre-training failure; inability to classify at deeper levels suggests feature extraction issues; inconsistent F1 scores suggest masking or constraint problems
- **First 3 experiments**: 1) Compare SSL pre-trained encoder performance on small benthic dataset vs. ImageNet pre-trained baseline; 2) Test hierarchical masking approach by gradually introducing missing information during training; 3) Evaluate depth of classification by testing model predictions at different hierarchical levels

## Open Questions the Paper Calls Out

### Open Question 1
How does pre-training with self-supervised learning on large-scale benthic imagery affect downstream performance on small benthic datasets compared to ImageNet pre-training? The paper provides initial evidence but doesn't fully explore impact across various dataset sizes or compare to other pre-training methods beyond ImageNet.

### Open Question 2
How can hierarchical multi-label classification be effectively performed when dealing with multiple sources of missing information in real-world datasets? The paper introduces a masking strategy but doesn't fully explore limitations or potential improvements in more complex scenarios.

### Open Question 3
What are the most effective evaluation metrics for hierarchical multi-label classification tasks with imbalanced data? The paper highlights challenges in evaluation but doesn't provide a definitive answer on best metrics for this task.

## Limitations
- Lack of external validation on truly independent benthic imagery collections beyond BenthicNet
- Masking strategy for missing annotations described conceptually but lacks detailed implementation specifics
- SSL pre-training advantages shown only against ImageNet, not compared to other domain-specific pre-training strategies

## Confidence
- **High confidence**: SSL pre-training advantage over ImageNet is well-supported by quantitative results across multiple metrics and SSL methods
- **Medium confidence**: Masking strategy for missing annotations is conceptually sound but lacks detailed validation of effectiveness
- **Medium confidence**: SSL enabling deeper hierarchical classification is supported by F1 scores but needs further investigation into whether this represents genuine feature learning or overfitting

## Next Checks
1. Test model generalization by evaluating on independent benthic imagery datasets not included in BenthicNet to assess true domain adaptation capability
2. Conduct ablation studies systematically removing different types of missing information to quantify masking strategy's impact on performance
3. Compare SSL pre-training against other domain-specific approaches (e.g., marine biology-specific datasets) to determine if benthic domain knowledge is uniquely beneficial