---
ver: rpa2
title: 'GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation'
arxiv_id: '2404.06609'
source_url: https://arxiv.org/abs/2404.06609
tags:
- goal
- object
- navigation
- image
- goat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GOAT-Bench, a benchmark for multi-modal lifelong
  navigation where an agent must navigate to a sequence of goals specified via object
  category, language description, or image. The benchmark includes 181 indoor scenes
  and 680k episodes, testing generalization to novel objects and lifelong learning.
---

# GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation

## Quick Facts
- arXiv ID: 2404.06609
- Source URL: https://arxiv.org/abs/2404.06609
- Reference count: 40
- Key outcome: Modular methods outperform RL on success rate and efficiency (SPL), highlighting the importance of memory for lifelong navigation.

## Executive Summary
This paper introduces GOAT-Bench, a benchmark for multi-modal lifelong navigation where an agent must navigate to a sequence of goals specified via object category, language description, or image. The benchmark includes 181 indoor scenes and 680k episodes, testing generalization to novel objects and lifelong learning. Two main approaches are evaluated: modular methods with explicit memory maps and end-to-end RL policies with implicit memory. Results show modular methods outperform RL on success rate and efficiency (SPL), highlighting the importance of memory for lifelong navigation. RL policies struggle with language and image goals, likely due to limitations of CLIP features.

## Method Summary
GOAT-Bench is a benchmark for multi-modal lifelong navigation in 181 indoor scenes with 680k episodes. The task involves navigating to sequential goals specified by object category, language description, or image, testing generalization to novel instances. Two main approaches are evaluated: modular methods with explicit semantic and instance maps, and end-to-end RL policies with implicit memory. Modular methods maintain persistent top-down semantic and instance maps, while RL policies use RNN hidden states. Performance is measured using Success Rate (SR) and Success Weighted by Path Length (SPL).

## Key Results
- Modular methods outperform RL baselines on both success rate and SPL, highlighting the importance of explicit memory for lifelong navigation.
- RL policies struggle with language and image goals, likely due to limitations of CLIP features in capturing instance-specific and spatial features.
- Modular methods are more sensitive to noise in goal specifications (synonyms, paraphrasing, image noise) compared to RL methods.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular methods outperform RL baselines because explicit semantic and instance maps provide more efficient goal localization.
- Mechanism: Modular approaches maintain a persistent top-down semantic map and an instance-specific memory that clusters projected pixels of same categories. When a new goal is specified, the agent matches the goal (image, language, or category) against stored instances using keypoint-matching or cosine similarity, enabling direct navigation without re-exploration.
- Core assumption: The agent has access to accurate object detection and semantic segmentation to build reliable maps.
- Evidence anchors:
  - [abstract]: "Results show modular methods outperform RL on success rate and efficiency (SPL), highlighting the importance of memory for lifelong navigation."
  - [section]: "Modular GOAT [42], which maintains an explicit semantic and instance map of the environment, does much better on SPL (6.6% than SenseAct-NN Skill Chain and 10.9% better than SenseAct-NN Monolithic)."
  - [corpus]: "LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation" suggests memory-based approaches are being explored in concurrent work.

### Mechanism 2
- Claim: CLIP embeddings are insufficient for instance-specific goal matching in language and image goals.
- Mechanism: CLIP is trained on vision-language alignment for broad category recognition, but lacks fine-grained instance discrimination. This causes poor matching between language descriptions or specific images and stored instance features.
- Core assumption: CLIP's pre-training objective does not prioritize instance-level discrimination.
- Evidence anchors:
  - [abstract]: "RL policies struggle with language and image goals, likely due to limitations of CLIP features."
  - [section]: "We find these methods perform poorly on language and image goals, particularly when relying on CLIP [25] features. This suggests the inability of CLIP [25] features in capturing crucial instance-specific and spatial features in language and image goals."
  - [corpus]: Weak - no direct corpus evidence on CLIP's instance-specific limitations.

### Mechanism 3
- Claim: Implicit memory in RL policies (hidden state) is insufficient for lifelong navigation compared to explicit memory.
- Mechanism: While SenseAct-NN Monolithic policy carries forward RNN hidden states across subtasks, the hidden state does not effectively encode spatial or semantic information about explored regions, leading to repeated exploration.
- Core assumption: RNN hidden states in visual navigation tasks can encode map-like representations effectively.
- Evidence anchors:
  - [abstract]: "RL policies struggle with language and image goals, likely due to limitations of CLIP features."
  - [section]: "We also observe that the SenseAct-NN Monolithic policy (row 4) does not perform well compared to the other baselines... We hypothesize this is due to: 1.) CLIP's limited efficacy in capturing instance-specific features for language and image goals, 2.) difficulty of learning effective long horizon navigation using RL."
  - [corpus]: "Explore with Long-term Memory: A Benchmark and Multimodal LLM-based Reinforcement Learning Framework for Embodied Exploration" suggests ongoing research on memory in lifelong navigation.

## Foundational Learning

- Concept: Lifelong learning in embodied AI
  - Why needed here: The benchmark requires agents to navigate to multiple goals in the same environment, leveraging past experiences to improve efficiency.
  - Quick check question: What is the key difference between episodic and lifelong navigation settings?

- Concept: Multimodal goal encoding and matching
  - Why needed here: Goals are specified through object categories, language descriptions, or images, requiring the agent to understand and match across modalities.
  - Quick check question: How do modular methods match different goal modalities against stored instances?

- Concept: Evaluation metrics in embodied navigation
  - Why needed here: Success rate and SPL are used to measure performance, with SPL accounting for path efficiency.
  - Quick check question: What is the difference between success rate and SPL, and why is SPL important for lifelong navigation?

## Architecture Onboarding

- Component map:
  - Input: RGB image (360x640), depth image, GPS+Compass pose, goal specification (category, language, or image)
  - Modular approach: Perception module (object detection, semantic mapping) -> Exploration policy -> Goal matching (CLIP/CroCo-v2 features) -> Local navigation policy
  - RL approach: CNN/RNN policy with CLIP goal encoder -> Action prediction -> Execution and hidden state update

- Critical path:
  - Modular: Perceive environment -> Build/update semantic map -> Match goal to instance -> Navigate to goal
  - RL: Encode observation and goal -> Predict action -> Execute and update hidden state

- Design tradeoffs:
  - Modular: More components, relies on accurate perception, better efficiency with explicit memory
  - RL: End-to-end learning, potentially more robust to perception errors, struggles with long horizons and multimodal goals

- Failure signatures:
  - Modular: Poor object detection leading to missing instances, map drift or inaccuracies
  - RL: Failure to learn effective long-horizon policies, inability to leverage past experiences, poor generalization to unseen instances

- First 3 experiments:
  1. Evaluate modular and RL baselines on VAL SEEN split to establish baseline performance.
  2. Test importance of memory by disabling memory in modular and RL methods and measuring performance drop.
  3. Analyze performance across modalities by breaking down success rate and SPL for object, language, and image goals separately.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the CLIP embeddings to better capture instance-specific features for language and image goals in the GOAT task?
- Basis in paper: [explicit] The paper states that the SenseAct-NN Monolithic policy underperforms on both success and SPL for language and image goals, likely due to limitations of CLIP features in capturing crucial instance-specific and spatial features.
- Why unresolved: While the paper identifies this as a limitation, it does not propose specific solutions or methods to improve CLIP embeddings for instance-specific features.
- What evidence would resolve it: Developing and testing alternative embedding methods or modifying CLIP to enhance its ability to capture instance-specific details, then evaluating their performance on the GOAT task.

### Open Question 2
- Question: What are the most effective memory representations (implicit or explicit) for improving efficiency in lifelong navigation tasks like GOAT?
- Basis in paper: [explicit] The paper highlights that methods with effective memory representations (both modular and monolithic) perform better on the GOAT task, particularly in terms of efficiency (SPL). However, it also notes that the SenseAct-NN Monolithic policy's implicit memory (hidden state) is not as effective as the explicit memory used in modular methods.
- Why unresolved: The paper compares the performance of different memory types but does not explore or propose new memory architectures or mechanisms that could be more effective.
- What evidence would resolve it: Experimenting with novel memory architectures, such as external memory networks or attention-based memory, and evaluating their impact on navigation efficiency in lifelong scenarios.

### Open Question 3
- Question: How can we make navigation agents more robust to noise in goal specifications across different modalities?
- Basis in paper: [explicit] The paper investigates the robustness of methods to noise in goal specifications (e.g., adding Gaussian noise to images, paraphrasing language descriptions, using synonyms for object categories) and finds that SenseAct-NN methods are more robust than modular methods.
- Why unresolved: While the paper identifies differences in robustness, it does not propose strategies or methods to enhance the robustness of navigation agents to such noise.
- What evidence would resolve it: Developing and testing noise-robust goal encoders or preprocessing techniques that can handle noisy inputs effectively, and measuring their impact on the success rate and SPL across modalities.

## Limitations

- The paper's reliance on CLIP features for multimodal goal matching may overestimate the limitations of learned policies, as alternative embeddings (e.g., CroCo-v2) show better performance on image goals.
- The modular approach's superior performance depends heavily on accurate object detection and semantic mapping, which may not generalize to more challenging real-world scenarios.
- The benchmark's evaluation metrics (SR and SPL) do not account for exploration efficiency over long time horizons, potentially underestimating the importance of memory for lifelong navigation.

## Confidence

- High confidence: Modular methods outperform RL baselines on success rate and SPL due to explicit memory mechanisms.
- Medium confidence: CLIP embeddings are insufficient for instance-specific goal matching in language and image goals.
- Medium confidence: Implicit memory in RL policies is insufficient for lifelong navigation compared to explicit memory.

## Next Checks

1. Re-evaluate RL baselines using CroCo-v2 embeddings instead of CLIP to assess the impact of feature representation on multimodal goal matching.
2. Conduct experiments with noisy object detection and semantic segmentation to test the robustness of modular methods to perception errors.
3. Design a new evaluation metric that accounts for exploration efficiency over long time horizons to better capture the importance of memory in lifelong navigation.