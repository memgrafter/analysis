---
ver: rpa2
title: The Relevance of Item-Co-Exposure For Exposure Bias Mitigation
arxiv_id: '2409.12912'
source_url: https://arxiv.org/abs/2409.12912
tags:
- bias
- exposure
- choice
- discrete
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Exposure bias in recommender systems arises when previously recommended
  items influence user choices, leading to feedback loops and biased recommendations.
  This study investigates using discrete choice models (e.g., multinomial logit, generalized
  extreme value) to mitigate exposure bias by explicitly modeling choice sets and
  observed alternatives.
---

# The Relevance of Item-Co-Exposure For Exposure Bias Mitigation

## Quick Facts
- arXiv ID: 2409.12912
- Source URL: https://arxiv.org/abs/2409.12912
- Reference count: 21
- Primary result: Discrete choice models (MNL, GEV, BL) effectively mitigate exposure bias by explicitly modeling choice sets and observed alternatives

## Executive Summary
This study addresses exposure bias in recommender systems, where previously recommended items influence user choices, creating feedback loops. The authors propose using discrete choice models to mitigate this bias by explicitly modeling choice sets and observed alternatives. Through experiments on human-generated choice data, they demonstrate that discrete choice models can effectively reduce exposure bias from both overexposure and competition between items, outperforming traditional approaches like negative sampling and popularity-based debiasing.

## Method Summary
The study compares discrete choice models (Multinomial Logit, Generalized Extreme Value, and Bayesian Logit) against traditional baselines for exposure bias mitigation. The approach involves training models on synthetic choice data where items are presented in sets, and the model conditions on the full choice set rather than individual item relevance. The method tracks item co-exposure to distinguish between items that were not recommended versus those recommended but not chosen. Models are evaluated based on their ability to reduce exposure bias while maintaining predictive accuracy.

## Key Results
- Discrete choice models effectively mitigate exposure bias from both overexposure and competition between items
- Multivariate discrete choice models successfully handle competition effects between items, while univariate models cannot
- Only models that consider the full choice set can effectively counter competition bias without sacrificing predictive accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discrete choice models mitigate exposure bias by modeling observed alternatives as explicit choice sets
- Mechanism: By conditioning on the full set of items presented to a user, the model distinguishes between items that were simply not recommended versus those that were recommended but not chosen, thereby correcting for exposure bias
- Core assumption: Random utility maximization accurately captures user choice behavior
- Evidence anchors:
  - [abstract]: "discrete choice models effectively mitigate exposure bias on human-generated choice data"
  - [section]: "we can condition their choices on the choice sets, dropping the effect of exposure"
  - [corpus]: No direct evidence found in corpus neighbors; this appears to be a novel mechanism specific to this paper
- Break condition: If random utility maximization fails to accurately model human choice behavior, the mechanism breaks down

### Mechanism 2
- Claim: Multivariate discrete choice models handle competition effects between items that univariate models cannot
- Mechanism: Multivariate models account for reference-point effects and mutually exclusive choices by considering the entire choice set, while univariate models only model individual item relevance independently
- Core assumption: The composition of choice sets affects user decision-making through competition effects
- Evidence anchors:
  - [abstract]: "only multivariate discrete choice models were robust to competition between items"
  - [section]: "only the multivariate models exhibited almost no bias" when testing competition effects
  - [corpus]: Weak evidence - corpus neighbors discuss related topics but don't directly support this specific mechanism
- Break condition: If competition effects between items are negligible in the application domain

### Mechanism 3
- Claim: Tracking item co-exposure provides information that binary feedback datasets cannot capture
- Mechanism: By recording which items were presented together, the model can distinguish between popularity bias from overexposure versus competition effects from item composition
- Core assumption: Binary datasets lack critical information about item co-occurrence that affects user choices
- Evidence anchors:
  - [abstract]: "only models that consider the choice set mitigated it without sacrificing predictive accuracy"
  - [section]: "Because binary data lack this information, no model that learns from binary data, including existing debiasing models, could counter this effect"
  - [corpus]: No direct evidence found in corpus neighbors
- Break condition: If logging item co-exposure becomes computationally prohibitive or if users make independent choices regardless of alternatives

## Foundational Learning

- Concept: Multinomial Logit Model (MNL)
  - Why needed here: Forms the basis for understanding how discrete choice models can be applied to recommendation systems
  - Quick check question: How does the MNL model calculate choice probabilities differently from traditional ranking models?

- Concept: Exposure bias and feedback loops
  - Why needed here: Essential for understanding why traditional recommendation models fail and why discrete choice models are needed
  - Quick check question: What distinguishes exposure bias from other forms of bias like popularity bias or position bias?

- Concept: Random utility maximization theory
  - Why needed here: Underpins the theoretical justification for using discrete choice models in recommendation contexts
  - Quick check question: How does random utility maximization explain why users might choose less popular items when presented with certain alternatives?

## Architecture Onboarding

- Component map: Data collection layer -> Choice set processor -> Discrete choice model trainer -> Evaluation module -> Logging policy
- Critical path: Data collection → Choice set processing → Model training → Bias evaluation → Recommendation generation
- Design tradeoffs:
  - Computational complexity vs. bias mitigation: Tracking full choice sets increases data volume and complexity
  - Model simplicity vs. expressiveness: Univariate models are simpler but can't handle competition effects
  - Data collection burden vs. model accuracy: More comprehensive logging improves model quality but requires more infrastructure
- Failure signatures:
  - Persistent bias in recommendations despite using discrete choice models
  - Significant drop in predictive accuracy (nDCG) when switching to discrete choice models
  - Computational bottlenecks in processing choice set information
- First 3 experiments:
  1. Compare univariate vs. multivariate discrete choice models on a small dataset to verify competition effect handling
  2. Test discrete choice models against traditional baselines (negative sampling, popularity debiasing) on a controlled dataset
  3. Measure the impact of varying exposure frequencies and competition effects on recommendation quality and bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational complexity of discrete choice models compare to negative sampling approaches when considering item co-exposure in real-world recommender systems?
- Basis in paper: [explicit] The paper mentions that the computational complexity of logging and processing choice alternatives compared to negative sampling remains unclear.
- Why unresolved: The authors acknowledge this as an open question but do not provide empirical comparisons of computational efficiency between the two approaches.
- What evidence would resolve it: Empirical studies comparing the training time, memory usage, and inference speed of discrete choice models versus negative sampling approaches on large-scale real-world datasets would provide clarity on this issue.

### Open Question 2
- Question: Can discrete choice models effectively mitigate exposure bias when applied to real-world datasets with more extreme exposure discrepancies than the controlled study environment?
- Basis in paper: [explicit] The authors note that exposure discrepancies can be much higher in real-world datasets, such as the last.fm dataset, compared to their controlled experiment where items were overexposed by a factor of 3.2.
- Why unresolved: The study used a controlled environment with limited exposure bias, and the authors explicitly state that findings need to be reproduced on real-world data.
- What evidence would resolve it: Applying discrete choice models to real-world datasets with known high exposure bias, such as the last.fm dataset or the finn.no dataset, and measuring the reduction in bias would provide the necessary evidence.

### Open Question 3
- Question: Are there significant performance differences between various discrete choice models (e.g., MNL, GEV, BL) in mitigating exposure bias, or do they perform similarly across different types of bias?
- Basis in paper: [explicit] The authors found no significant differences in bias robustness or performance among the discrete choice models they tested, but suggest that either the dataset was too small or the theoretical foundation for differences is lacking.
- Why unresolved: The study's dataset may not have been large enough to detect subtle differences, and there is no theoretical basis provided for why one model might outperform another in bias mitigation.
- What evidence would resolve it: Conducting a larger-scale study with more diverse datasets and a broader range of discrete choice models, along with theoretical analysis of model properties, would help determine if certain models are more effective in specific scenarios.

## Limitations
- Synthetic data experiments may not fully capture real-world user behavior complexity
- Implementation details for discrete choice models and baselines are not fully specified
- Computational complexity of tracking full choice sets for large-scale systems remains unaddressed

## Confidence
- High confidence: Discrete choice models can effectively mitigate exposure bias when properly implemented
- Medium confidence: Multivariate models are superior to univariate models for handling competition effects
- Medium confidence: The distinction between overexposure and competition bias is theoretically sound but requires further empirical validation

## Next Checks
1. Validate findings on a real-world dataset with actual user choice data to confirm synthetic experiment results
2. Conduct ablation studies comparing different discrete choice model variants (MNL, GEV, BL) to isolate the most effective approach
3. Measure computational overhead and scalability challenges when implementing choice set tracking in production recommender systems