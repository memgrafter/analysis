---
ver: rpa2
title: Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning
arxiv_id: '2402.13669'
source_url: https://arxiv.org/abs/2402.13669
tags:
- uni00000013
- fine-tuning
- uni00000011
- uni00000019
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of catastrophic forgetting in
  large language model fine-tuning, where models lose general instruction-following
  abilities while adapting to specific downstream tasks. The authors propose Self-Distillation
  Fine-Tuning (SDFT), which generates a distilled dataset by having the model rewrite
  task responses to match its original distribution, then fine-tunes using these self-generated
  responses.
---

# Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning

## Quick Facts
- arXiv ID: 2402.13669
- Source URL: https://arxiv.org/abs/2402.13669
- Authors: Zhaorui Yang; Tianyu Pang; Haozhe Feng; Han Wang; Wei Chen; Minfeng Zhu; Qian Liu
- Reference count: 20
- This paper addresses catastrophic forgetting in large language model fine-tuning by proposing Self-Distillation Fine-Tuning (SDFT)

## Executive Summary
This paper addresses the challenge of catastrophic forgetting in large language model fine-tuning, where models lose general instruction-following abilities while adapting to specific downstream tasks. The authors propose Self-Distillation Fine-Tuning (SDFT), which generates a distilled dataset by having the model rewrite task responses to match its original distribution, then fine-tunes using these self-generated responses. Experiments on Llama-2-7b-chat demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks. The method also preserves safety alignment while maintaining task performance.

## Method Summary
SDFT works by first prompting the seed language model to rewrite task responses using a distillation template, generating a distilled dataset that matches the model's original distribution. These self-generated responses serve as training targets during subsequent fine-tuning. The method uses LoRA adapters for efficient parameter updates and can adjust the mix ratio of distilled samples versus original samples to control the degree of distribution alignment. The approach aims to bridge the distribution gap between task datasets and the seed model's original distribution, thereby reducing catastrophic forgetting while maintaining task performance.

## Key Results
- SDFT improved HumanEval pass@1 from 9.8 to 15.2 on OpenFunctions, while vanilla fine-tuning decreased it from 13.4 to 9.8
- The method preserved safety alignment, with jailbreak safe rate improving from 87.31% to 94.42% compared to vanilla fine-tuning
- Across multiple benchmarks, SDFT achieved comparable or superior performance while significantly reducing catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distribution gap between task datasets and seed LMs is the primary cause of catastrophic forgetting
- Mechanism: When fine-tuning, the model parameters shift to align with the task dataset distribution, causing the model to lose its original distribution characteristics and thus forget general capabilities
- Core assumption: The seed LM's original distribution encodes its general capabilities, and maintaining this distribution prevents forgetting
- Evidence anchors: [abstract], [section 1], corpus: Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If the original distribution does not encode general capabilities, or if task distribution is too different to bridge

### Mechanism 2
- Claim: Self-distillation through response rewriting generates data points that match the seed LM's distribution
- Mechanism: By prompting the seed LM to rewrite task responses, the resulting distilled responses follow the LM's own distribution, creating a bridge between task requirements and original capabilities
- Core assumption: The seed LM can generate semantically equivalent responses that match its own distribution when prompted correctly
- Evidence anchors: [section 3.2], corpus: Weak - corpus contains related work on self-distillation but not this specific mechanism
- Break condition: If the rewriting process fails to maintain semantic equivalence or cannot match the seed LM's distribution

### Mechanism 3
- Claim: Using distilled responses as training targets reduces parameter shift magnitude compared to vanilla fine-tuning
- Mechanism: By fine-tuning on data that matches the seed LM's distribution, the parameter updates are smaller and more constrained, preserving general capabilities while adapting to specific tasks
- Core assumption: Smaller parameter shifts correlate with reduced catastrophic forgetting
- Evidence anchors: [section 5], [section 5.1], corpus: Weak - no direct corpus evidence for this specific parameter shift mechanism
- Break condition: If the relationship between parameter shift magnitude and forgetting is not linear, or if other factors dominate forgetting

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why fine-tuning causes loss of previously learned capabilities is essential to grasp the problem SDFT addresses
  - Quick check question: Why does training on new data typically cause neural networks to forget previously learned information?

- Concept: Distribution shift and its effects on model behavior
  - Why needed here: The paper's core hypothesis relies on understanding how differences between training and original distributions affect model capabilities
  - Quick check question: How does training on data from a different distribution than the original training data affect a model's performance on its original tasks?

- Concept: Self-distillation techniques in machine learning
  - Why needed here: SDFT builds on self-distillation concepts, though applies them in a novel way to address catastrophic forgetting
  - Quick check question: What is self-distillation and how does it typically work in the context of model training?

## Architecture Onboarding

- Component map: Seed LM -> Distillation Template -> Distilled Dataset -> Fine-tuning Process
- Critical path:
  1. Prompt seed LM to rewrite task responses using distillation template
  2. Evaluate and filter distilled responses for quality
  3. Use distilled responses as training targets for fine-tuning
  4. Evaluate performance on both target tasks and general capabilities

- Design tradeoffs:
  - Template simplicity vs. rewriting quality: Simpler templates are more general but may produce lower quality rewrites
  - Distillation dataset size vs. computational cost: More distilled examples improve adaptation but increase training time
  - Semantic equivalence vs. distribution matching: Perfect semantic preservation may reduce distribution alignment benefits

- Failure signatures:
  - Low BLEU/ROUGE scores between distilled and original responses indicate poor rewriting quality
  - Decreased performance on both target tasks and general capabilities suggests the method is not working
  - Minimal difference between SDFT and vanilla fine-tuning results indicates the distribution bridging is ineffective

- First 3 experiments:
  1. Run SDFT on a simple dataset (like GSM8K) with a small subset of data to verify the basic mechanism works
  2. Compare BLEU/ROUGE scores and parameter shift between SDFT and vanilla fine-tuning on the same dataset
  3. Test different distillation template variations to find the optimal balance between simplicity and quality

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The core claim that distribution gap is the "primary underlying cause" of catastrophic forgetting lacks direct empirical validation and relies on correlational evidence
- Template design appears critical but the paper provides limited analysis of template sensitivity and optimization
- The method's effectiveness on larger models (beyond 7B parameters) and different model architectures remains unexplored

## Confidence
**High confidence** in the empirical results: The experimental methodology is sound, with clear baselines and comprehensive evaluation across multiple tasks and capability metrics.

**Medium confidence** in the distribution gap hypothesis: While the experiments support the claim that SDFT bridges a distribution gap, the paper doesn't rule out alternative mechanisms or provide direct evidence that distribution matching is the primary driver.

**Low confidence** in the universal applicability: The paper focuses exclusively on Llama-2-7b-chat and doesn't explore how SDFT might perform with different model sizes, architectures, or on tasks with fundamentally different distributions.

## Next Checks
1. **Ablation study on template variations**: Systematically test different distillation templates to quantify how template design affects the balance between semantic preservation and distribution alignment.

2. **Direct comparison with explicit regularization**: Compare SDFT against vanilla fine-tuning with various regularization techniques to determine whether performance benefits stem primarily from distribution matching or from additional training structure.

3. **Cross-distribution transfer test**: Fine-tune the same model on datasets from vastly different distributions and evaluate whether SDFT's benefits persist when the task distribution is fundamentally incompatible with the seed model's original distribution.