---
ver: rpa2
title: Evaluation of pretrained language models on music understanding
arxiv_id: '2409.11449'
source_url: https://arxiv.org/abs/2409.11449
tags:
- music
- label
- musical
- triplets
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper quantifies the musical knowledge of six Transformer-based
  language models by evaluating their ability to judge relative semantic similarity
  among musical labels using triplets from the Audioset ontology. The models are tested
  on 13,633 genre and 37,640 instrument triplets under different prompts, with and
  without label definitions, and on negation modeling.
---

# Evaluation of pretrained language models on music understanding

## Quick Facts
- arXiv ID: 2409.11449
- Source URL: https://arxiv.org/abs/2409.11449
- Reference count: 40
- Models show high sensitivity to prompt wording and fail on negated labels

## Executive Summary
This paper quantifies the musical knowledge of six Transformer-based language models by evaluating their ability to judge relative semantic similarity among musical labels using triplets from the Audioset ontology. The models are tested on 13,633 genre and 37,640 instrument triplets under different prompts, with and without label definitions, and on negation modeling. Results show high sensitivity to prompt wording, poor performance on negated labels (worse than random), and variable reliance on label definitions. The best-performing model was all-mpnet-base-v2. These findings indicate that off-the-shelf LLMs require fine-tuning and adaptation for robust musical understanding before deployment in music information retrieval tasks.

## Method Summary
The study evaluates six pretrained Transformer models (MPNet, DistilRoBERTa, MiniLM, ALBERT variants) on their ability to judge semantic similarity between musical labels using triplet-based evaluation. Models generate sentence embeddings via global average pooling of token embeddings, then compute cosine similarities to determine if anchor-positive pairs are more similar than anchor-negative pairs. The evaluation uses 20 musically-informed prompts, tests negation handling, and compares performance with and without label definitions. Results are measured as triplet accuracy across 13,633 genre and 37,640 instrument triplets from the Audioset ontology.

## Key Results
- Models show high sensitivity to prompt wording with considerable accuracy variance across different phrasings
- Performance on negated labels is worse than random chance, indicating inability to model negation
- Excluding labels from definitions consistently reduces accuracy, showing models rely heavily on label presence
- all-mpnet-base-v2 achieved the highest accuracy across all experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs show high sensitivity to prompt wording when evaluating musical similarity.
- Mechanism: Prompt phrasing alters the semantic context captured by token embeddings, affecting cosine similarity comparisons.
- Core assumption: The same triplet can yield different similarity rankings depending on the prompt structure.
- Evidence anchors:
  - [abstract] "Results show high sensitivity to prompt wording"
  - [section 3.3.1] "We used 20 musically informed prompts... The standard deviation reported is relatively high for every case apart from the paraphrased-MiniLM model"
  - [corpus] Weak: Only 5 neighbor papers, none directly measuring prompt sensitivity variance.
- Break condition: When prompts are normalized or when models are fine-tuned on prompt-augmented data.

### Mechanism 2
- Claim: LLMs fail to model negation in musical labels, performing worse than random.
- Mechanism: The negation word is not effectively encoded in the token embedding space, so the model treats "not rock" similarly to "rock".
- Core assumption: Negation handling is not learned during pretraining on musical terminology.
- Evidence anchors:
  - [abstract] "poor performance on negated labels (worse than random)"
  - [section 3.3.2] "The performance is worse than random... Different prompts lead to considerable differences in accuracy, with the worst performance reported being ≈ 23%"
  - [corpus] Weak: No neighbor papers explicitly discuss negation modeling in musical contexts.
- Break condition: When negation is explicitly trained as a separate classification task or when definitions are used instead of labels.

### Mechanism 3
- Claim: Models rely more on label presence than semantic meaning from definitions.
- Mechanism: The label token contributes disproportionately to the sentence embedding compared to contextual definition words.
- Core assumption: Label tokens have higher attention weights in the transformer layers.
- Evidence anchors:
  - [abstract] "variable reliance on label definitions"
  - [section 3.3.3] "Excluding the label from the definition leads to a drop in every experiment... the definition leads to an increment in accuracy in most cases"
  - [corpus] Weak: No direct evidence in neighbor papers about label vs definition reliance.
- Break condition: When models are fine-tuned to prioritize contextual understanding over keyword matching.

## Foundational Learning

- Concept: Cosine similarity in embedding space
  - Why needed here: The evaluation method compares anchor-positive vs anchor-negative similarity using cosine distance.
  - Quick check question: What does a higher cosine similarity between anchor and positive vs anchor and negative indicate?

- Concept: Hierarchical ontology structure
  - Why needed here: Triplets are generated based on distance in the Audioset ontology tree.
  - Quick check question: Why does the triplet (anchor, positive, negative) require the anchor-positive distance to be less than anchor-negative distance?

- Concept: Transformer token embedding averaging
  - Why needed here: Sentence embeddings are computed as the mean of token embeddings from the last layer.
  - Quick check question: How might different averaging strategies (e.g., CLS token, weighted mean) affect similarity comparisons?

## Architecture Onboarding

- Component map: Pretrained Transformer → Global average pooling → Cosine similarity comparison
- Critical path: Load model → Generate embeddings for all triplet labels → Compute pairwise similarities → Compare anchor-positive vs anchor-negative → Tally accuracy
- Design tradeoffs: Using off-the-shelf models trades adaptation for convenience but introduces sensitivity issues; using definitions adds context but may dilute label signals.
- Failure signatures: High variance across prompts, accuracy below random on negation, drops when labels are removed from definitions.
- First 3 experiments:
  1. Run triplet evaluation with a fixed prompt across all models to establish baseline variance.
  2. Test negation modeling by constructing anchor-positive pairs with explicit negative labels and compare to random baseline.
  3. Evaluate the impact of including vs excluding label definitions on accuracy to quantify reliance on label presence.

## Open Questions the Paper Calls Out
None

## Limitations
- Models show high sensitivity to prompt wording, making performance unreliable without careful prompt engineering
- Complete failure to model negation suggests lack of compositional semantic understanding
- Variable reliance on label definitions versus semantic content indicates keyword matching rather than true understanding

## Confidence
- **High confidence**: The finding that models perform worse than random on negated labels is robust, as this was consistently observed across all six models and multiple prompt variations.
- **Medium confidence**: The claim about high sensitivity to prompt wording is supported by reported standard deviations but would benefit from more extensive prompt variation testing.
- **Medium confidence**: The observation about variable reliance on label definitions is based on the reported accuracy changes but lacks detailed analysis of which models benefit most from definitions.

## Next Checks
1. Test the six models on an expanded set of 50-100 prompt variations for the same triplets to quantify the full extent of prompt sensitivity and identify which models are most robust to wording changes.
2. Evaluate the models on a controlled set of musical negation pairs (e.g., "not classical" vs "jazz") using both cosine similarity and direct classification to determine if the negation failure is specific to the triplet evaluation method.
3. Conduct ablation studies where labels are systematically replaced with synonyms or paraphrased definitions to measure whether models can transfer semantic understanding beyond exact label matching.