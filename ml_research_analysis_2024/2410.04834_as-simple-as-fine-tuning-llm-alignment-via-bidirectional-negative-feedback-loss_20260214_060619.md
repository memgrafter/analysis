---
ver: rpa2
title: 'As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative Feedback
  Loss'
arxiv_id: '2410.04834'
source_url: https://arxiv.org/abs/2410.04834
tags:
- arxiv
- loss
- preference
- fbnf
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the instability and hyperparameter sensitivity
  of Direct Preference Optimization (DPO) and its variants when applied to mathematical
  datasets. The authors argue that these issues arise from the unidirectional likelihood-derivative
  negative feedback inherent in the log-likelihood loss function.
---

# As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative Feedback Loss

## Quick Facts
- arXiv ID: 2410.04834
- Source URL: https://arxiv.org/abs/2410.04834
- Reference count: 34
- BNF eliminates the need for pairwise contrastive losses and achieves comparable performance to best methods on QA benchmarks while reducing reasoning performance decrease.

## Executive Summary
This paper addresses the instability and hyperparameter sensitivity of Direct Preference Optimization (DPO) and its variants when applied to mathematical datasets. The authors argue that these issues arise from the unidirectional likelihood-derivative negative feedback inherent in the log-likelihood loss function. They propose a novel Bidirectional Negative Feedback (BNF) loss that establishes a stable bidirectional negative feedback during optimization, eliminating the need for pairwise contrastive losses and extra hyperparameters. Experimental results show that BNF achieves comparable performance to the best methods on QA benchmarks while significantly reducing performance decrease on reasoning benchmarks, striking a better balance between value alignment and reasoning ability.

## Method Summary
The paper proposes Bidirectional Negative Feedback (BNF) loss to address the instability issues in preference optimization methods like DPO. BNF works by establishing a stable bidirectional negative feedback during optimization, eliminating the need for pairwise contrastive losses and extra hyperparameters. The method uses a Dynamic Target Distribution (DTD) that creates symmetric gradient behavior around a reference point, preventing the collapse problem that occurs with unidirectional likelihood-derivative feedback. The approach is implemented as a standard length-normalized cross-entropy loss with DTD, making it "as simple as fine-tuning" while maintaining stability and performance.

## Key Results
- BNF achieves comparable performance to the best methods on QA benchmarks (Arena-Hard and Wild-Bench)
- BNF significantly reduces performance decrease on reasoning benchmarks (MMLU-redux, CRUX, GSM8K, MATH-L5) compared to DPO-series methods
- BNF performs well on non-pairwise datasets and demonstrates minimal alignment tax while maintaining stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The unidirectional likelihood-derivative negative feedback in log-likelihood loss causes excessive decreases in the likelihood of dispreferred samples, leading to model collapse.
- Mechanism: When using NLL loss to decrease the likelihood of dispreferred samples, the partial derivative with respect to the unnormalized logit continues to rise as the likelihood decreases, creating a positive feedback loop that accelerates the collapse.
- Core assumption: The log-likelihood loss creates asymmetric gradient behavior - stabilizing for preferred samples but destabilizing for dispreferred ones.
- Evidence anchors:
  - [abstract] "These issues arise from the unidirectional likelihood-derivative negative feedback inherent in the log-likelihood loss function."
  - [section 2.1] "When LPLL = E(x,yl)∼D[log πθ(yl|x)] is used to decrease the likelihood πθ(yl|x), the above negative feedback will turn into a positive one."
- Break condition: This mechanism would break if the loss function creates symmetric gradient behavior for both preferred and dispreferred samples.

### Mechanism 2
- Claim: BNF establishes bidirectional negative feedback by making the partial derivative reach maximum only when likelihood equals reference likelihood.
- Mechanism: The Dynamic Target Distribution (DTD) in BNF causes the partial derivative to decrease linearly in both directions from the reference point, preventing excessive decreases in dispreferred sample likelihood.
- Core assumption: A symmetric gradient behavior around the reference point can prevent the collapse problem while maintaining optimization effectiveness.
- Evidence anchors:
  - [abstract] "Our proposed BNF loss eliminates the need for pairwise contrastive losses and does not require any extra tunable hyper-parameters or pairwise preference data"
  - [section 3.2] "From this piecewise function, we can observe that the partial derivative reaches its maximum only when πθ(yi|x, y<i) = πref(yi|x, y<i)"
- Break condition: This mechanism would break if the DTD implementation fails to maintain the symmetric gradient behavior across different vocabulary sizes or sequence lengths.

### Mechanism 3
- Claim: The elimination of pairwise contrastive losses reduces hyperparameter sensitivity and simplifies the alignment pipeline.
- Mechanism: By removing the need for scaling factors and pairwise data, BNF avoids the tuning challenges that plague DPO-series methods, especially on mathematical datasets.
- Core assumption: Pairwise contrastive losses are the primary source of hyperparameter sensitivity and instability in preference optimization.
- Evidence anchors:
  - [abstract] "Our proposed BNF loss eliminates the need for pairwise contrastive losses and does not require any extra tunable hyper-parameters"
  - [section 2.3] "Since mathematical reasoning requires rigorous logic, the diversity of responses to math problems is often significantly lower compared to other types of problems."
- Break condition: This mechanism would break if pairwise contrastive losses are actually necessary for certain types of preference data distributions.

## Foundational Learning

- Concept: Cross-entropy loss and its gradient properties
  - Why needed here: Understanding how log-likelihood loss behaves differently for increasing vs decreasing probabilities is crucial to grasping why BNF is needed
  - Quick check question: What happens to the gradient of cross-entropy loss when the predicted probability approaches 0 for a target class?

- Concept: Dynamic Target Distribution (DTD)
  - Why needed here: DTD is the core innovation that creates the bidirectional feedback - understanding its construction and properties is essential
  - Quick check question: How does the stop gradient operation in DTD affect the optimization dynamics compared to standard cross-entropy?

- Concept: Preference optimization vs supervised fine-tuning
  - Why needed here: The paper positions BNF as being "as simple as supervised fine-tuning" - understanding the differences and similarities is important
  - Quick check question: What key difference between preference optimization and standard supervised learning necessitates the BNF approach?

## Architecture Onboarding

- Component map:
  Policy model (πθ) -> Reference model (πref) -> Dynamic Target Distribution (DTD) -> Cross-entropy loss wrapper

- Critical path:
  1. Compute policy model logits
  2. Compute reference model logits
  3. Apply softmax to get probabilities
  4. Create DTD using the probability ratios
  5. Compute cross-entropy loss with DTD
  6. Backpropagate gradients

- Design tradeoffs:
  - Simplicity vs performance: BNF eliminates pairwise contrastive losses but may sacrifice some fine-grained control
  - Computational cost: DTD adds computation but avoids expensive pairwise data construction
  - Stability vs expressiveness: Bidirectional feedback prevents collapse but may limit extreme preference learning

- Failure signatures:
  - Training instability or collapse indicates DTD implementation issues
  - Poor performance on QA benchmarks suggests insufficient preference learning
  - High alignment tax on reasoning benchmarks indicates over-regularization
  - Excessive hyperparameter sensitivity suggests the bidirectional feedback isn't working as intended

- First 3 experiments:
  1. Compare training stability between BNF and DPO on a mathematical dataset with similar preference pairs
  2. Test BNF performance with varying levels of pairwise preference data (0%, 25%, 50%, 100%)
  3. Analyze log-likelihood and logit shifts between BNF and baseline methods on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BNF scale to models significantly larger than 9B parameters (e.g., 30B+)?
- Basis in paper: [inferred] The authors explicitly state that all experiments were conducted on 7B-9B scale models and express uncertainty about performance on larger models.
- Why unresolved: The paper lacks empirical data on larger models, making it unclear if the bidirectional negative feedback mechanism remains stable and effective at scale.
- What evidence would resolve it: Training and evaluating BNF on 30B+ parameter models across the same benchmarks would demonstrate scalability.

### Open Question 2
- Question: Can BNF be combined with pairwise contrastive losses to further improve performance and stability?
- Basis in paper: [explicit] The authors mention that BNF is not necessarily opposed to DPO-series methods and that introducing a pairwise contrastive function may further improve BNF.
- Why unresolved: The paper only tests BNF in isolation and does not explore hybrid approaches that combine BNF with existing pairwise methods.
- What evidence would resolve it: Comparative experiments showing performance gains (or losses) when adding pairwise contrastive losses to BNF versus using BNF alone.

### Open Question 3
- Question: How does BNF perform on truly non-pairwise preference datasets versus the simulated masking approach used in the paper?
- Basis in paper: [explicit] The authors acknowledge that their non-pairwise experiment used random masking from a pairwise dataset, which is a simulation and may not reflect real-world applications.
- Why unresolved: The paper does not test BNF on datasets naturally constructed without pairwise preferences (e.g., ranking-based feedback or scalar rewards).
- What evidence would resolve it: Training BNF on naturally non-pairwise datasets (e.g., collected via rating scales or ranking without explicit pairs) and comparing performance to the simulated masking results.

## Limitations
- The claim that BNF is "as simple as fine-tuning" may be overstated as DTD construction adds implementation complexity
- Empirical claims about performance advantages need more rigorous statistical validation across different model sizes and domains
- The bidirectional feedback mechanism's effectiveness on truly non-pairwise preference data remains unverified

## Confidence
- High confidence: The core mathematical derivation of why unidirectional likelihood-derivative feedback causes instability is sound and well-established in optimization theory
- Medium confidence: The empirical claims about BNF's performance advantages on reasoning benchmarks are supported by experiments but need more comprehensive validation
- Low confidence: The claim that BNF is "as simple as fine-tuning" doesn't adequately address computational overhead and implementation challenges in production settings

## Next Checks
1. **Stability stress test:** Systematically evaluate BNF on preference datasets with varying levels of noise and contradictory preferences, measuring training stability metrics (loss variance, gradient norms) and final model performance across multiple random seeds. Compare against DPO variants under identical conditions.

2. **Reference model sensitivity analysis:** Conduct controlled experiments varying the reference model architecture, training data, and temperature parameters to quantify their impact on BNF performance and stability. Include ablation tests with different reference model choices (smaller models, different architectures, frozen vs fine-tuned).

3. **Cross-domain generalization study:** Test BNF on diverse preference datasets beyond mathematical reasoning (creative writing, dialogue, code generation) to identify domains where the bidirectional feedback mechanism provides the most benefit and where it may be unnecessary or even detrimental.