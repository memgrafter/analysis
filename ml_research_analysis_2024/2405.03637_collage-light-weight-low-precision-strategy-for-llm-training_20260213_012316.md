---
ver: rpa2
title: 'Collage: Light-Weight Low-Precision Strategy for LLM Training'
arxiv_id: '2405.03637'
source_url: https://arxiv.org/abs/2405.03637
tags:
- training
- bf16
- precision
- collage
- iterations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of efficient large language model
  (LLM) training by proposing a lightweight low-precision strategy called COLLAGE.
  The core idea is to use multi-component float (MCF) representation in low-precision
  to accurately perform operations while accounting for numerical errors.
---

# Collage: Light-Weight Low-Precision Strategy for LLM Training

## Quick Facts
- arXiv ID: 2405.03637
- Source URL: https://arxiv.org/abs/2405.03637
- Reference count: 40
- One-line primary result: Achieves up to 3.7x speedup and 15-23% less memory usage while maintaining model quality using multi-component float representation

## Executive Summary
COLLAGE introduces a lightweight low-precision strategy for efficient large language model training using multi-component float (MCF) representation. By splitting values into primary and correction components in low-precision formats, COLLAGE maintains numerical accuracy without requiring high-precision master weights or upcasting. The approach leverages MCF in both parameters and optimizer states to compensate for precision loss at critical locations in the training process.

## Method Summary
The method uses multi-component float (MCF) representation to perform low-precision operations while maintaining accuracy through error compensation. Parameters and optimizer states are stored as MCF expansions (length-2 vectors), and arithmetic operations use specialized algorithms like Fast2Sum and TwoProdFMA. The approach works by storing the main value in one bfloat16 and the rounding error in a second, allowing accurate reconstruction of sums and products. COLLAGE is implemented as a plugin that works with existing optimizers like AdamW.

## Key Results
- Up to 3.7x speedup and 15-23% less memory usage compared to BF16 with FP32 master weights
- Maintains similar or better training performance across BERT, RoBERTa, GPT, and OpenLLaMA models
- Introduces effective descent quality (EDQ) metric that tracks information loss during training
- Works with different β₂ values (0.95, 0.99, 0.999) without precision loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-component float (MCF) representation preserves numerical precision in low-bitwidth formats by splitting values into a primary component and a correction component.
- Mechanism: When adding two bfloat16 values with large scale differences, the smaller value is rounded away. MCF stores the main value in one bfloat16 and the rounding error in a second, allowing accurate reconstruction of the sum.
- Core assumption: The rounding error is small enough to fit in the secondary bfloat16 component and does not itself overflow/underflow.
- Evidence anchors:
  - [abstract] "We argue that low-precision floating points can perform well provided the error is properly compensated at the critical locations in the training process."
  - [section] Definition 2.1 introduces expansions as exact sums of multiple floats.
  - [corpus] Weak/no direct evidence on overflow/underflow edge cases.
- Break condition: When the rounding error exceeds the representable range of the secondary bfloat16, or when multiple accumulations cause error growth beyond the secondary component's capacity.

### Mechanism 2
- Claim: Using MCF in optimizer states (especially the second moment) eliminates the precision loss from β₂ ≈ 0.999 in bfloat16.
- Mechanism: β₂ is stored as an MCF expansion (β₂, δβ₂), allowing the second moment update v ← β₂·v + (1−β₂)·g² to be computed exactly instead of rounding β₂ to 1.0.
- Core assumption: β₂ values like 0.999 can be represented exactly as a two-component bfloat16 expansion.
- Evidence anchors:
  - [section] "We propose switching β₂ from standard single float to a MCF expansion as (β₂, δβ₂), and also for second momentum as (v_t, δv_t)."
  - [corpus] No corpus evidence directly testing this claim.
- Break condition: When β₂ is so close to 1.0 that the correction term δβ₂ underflows or is lost in later arithmetic steps.

### Mechanism 3
- Claim: Effective descent quality (EDQ) provides a scalar measure of how much information is lost during the parameter update step, enabling precise comparison of precision strategies.
- Mechanism: EDQ is the cosine similarity between the intended update ∆θ_t and the actual update after rounding, scaled by their norms. Lower EDQ indicates more loss.
- Core assumption: The loss in update direction correlates with training performance degradation.
- Evidence anchors:
  - [abstract] "We propose a simple and novel metric which tracks the lost information during training as well as differentiates various precision strategies."
  - [section] "We have developed a novel metric called 'effective descent quality' to trace the lost information in the optimizer model update step."
  - [corpus] No corpus evidence directly linking EDQ to training outcomes.
- Break condition: When other factors (e.g., gradient clipping, learning rate scheduling) dominate the training trajectory and mask the effect of update precision loss.

## Foundational Learning

- Concept: IEEE 754 floating-point representation (sign, exponent, mantissa)
  - Why needed here: To understand why low-precision formats like bfloat16 lose accuracy for certain operations.
  - Quick check question: What is the ulp (unit in the last place) of a bfloat16 number with exponent 10?

- Concept: Unit in the last place (ulp) and rounding error bounds
  - Why needed here: To quantify when low-precision arithmetic becomes "lost" and to design MCF corrections.
  - Quick check question: If a = 200 and b = 0.1 in bfloat16, what is the exact stored value of a ⊕ b?

- Concept: Fused multiply-add (FMA) and its role in MCF multiplication
  - Why needed here: MCF algorithms like TwoProdFMA rely on FMA to compute exact error terms efficiently.
  - Quick check question: How does TwoProdFMA avoid intermediate rounding errors that TwoProd would incur?

## Architecture Onboarding

- Component map: Parameter storage (θ, δθ) -> Momentum storage (m, δm) -> Second moment storage (v, δv) -> Update computation using MCF arithmetic
- Critical path:
  1. Forward/backward: use mixed-precision GEMM with bfloat16 activations/gradients
  2. Parameter update: Grow((θ_{t-1}, δθ_{t-1}), ∆θ_t) to form new expansion
  3. Momentum update: standard bfloat16 or MCF expansion update
  4. Second moment update: MCF expansion multiplication if enabled
- Design tradeoffs:
  - Memory: MCF uses 2× storage vs 1× for plain bfloat16; trades memory for precision
  - Speed: MCF adds small overhead per arithmetic op but removes need for FP32 master weights and upcasting
  - Flexibility: β₂ can be tuned closer to 1.0 without precision loss
- Failure signatures:
  - Persistent training instability despite correct hyper-parameters
  - EDQ dropping sharply at parameter update steps
  - Gradient norms blowing up or vanishing when using MCF in optimizer states
- First 3 experiments:
  1. Verify that Grow correctly reconstructs θ_{t-1} + ∆θ_t for a known pair of values
  2. Compare training loss trajectories with and without MCF in parameter storage on a small BERT model
  3. Measure EDQ at each update step for BF16 vs BF16+MCF parameter updates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does COLLAGE perform with even lower precision formats like FP8 compared to current mixed-precision strategies?
- Basis in paper: [explicit] The paper mentions that COLLAGE can be naturally extended to work with even lower precision such as 8-bit and discusses potential improvements for FP8.
- Why unresolved: The paper focuses on demonstrating COLLAGE's effectiveness with BF16 and FP16, leaving the exploration of FP8 as a future work item.
- What evidence would resolve it: Experimental results comparing COLLAGE's performance with FP8 against existing FP8 mixed-precision strategies on various LLM sizes and tasks.

### Open Question 2
- Question: What is the optimal choice of β2 for different model sizes and tasks when using COLLAGE?
- Basis in paper: [explicit] The paper shows that COLLAGE works with different β2 values (0.95, 0.99, 0.999) but notes that there is no clear conclusion between β2 and converged performance, suggesting case-by-case optimization.
- Why unresolved: The paper demonstrates that COLLAGE works with different β2 values but does not provide a definitive guideline for choosing the optimal β2 across different scenarios.
- What evidence would resolve it: A comprehensive study showing the relationship between β2 values, model sizes, tasks, and final model performance metrics.

### Open Question 3
- Question: How does the effective descent quality (EDQ) metric correlate with final model performance across different precision strategies?
- Basis in paper: [explicit] The paper introduces EDQ as a novel metric to measure information loss during training and shows its relationship with learning quality, particularly highlighting differences between COLLAGE variants.
- Why unresolved: While the paper demonstrates EDQ's ability to differentiate precision strategies during training, it doesn't establish a clear predictive relationship between EDQ trajectories and final model performance.
- What evidence would resolve it: Statistical analysis showing correlation coefficients between EDQ metrics during training and final model performance metrics across various tasks and model sizes.

## Limitations

- MCF representation doubles parameter storage requirements, trading memory for precision
- Benefits of using MCF in optimizer states depend on specific hyperparameters like β₂ values
- Performance improvements depend heavily on hardware characteristics and implementation optimizations
- Error accumulation in deep models requires careful validation as small errors can compound

## Confidence

- 3.7x speedup claim: Medium confidence (based on experiments across multiple model architectures but hardware-dependent)
- 15-23% memory savings: Medium confidence (validated but depends on model size and precision strategy)
- EDQ metric effectiveness: Medium confidence (shows promise but needs more validation across diverse tasks)
- MCF error handling: Medium confidence (foundational assumption requires careful validation for deep models)

## Next Checks

1. **Error accumulation study**: Track how rounding errors propagate through 100+ consecutive parameter updates using MCF to identify potential failure points.

2. **Cross-hardware validation**: Test COLLAGE performance on different GPU architectures (NVIDIA vs AMD) to verify the claimed speedup is not hardware-specific.

3. **Optimizer generalization**: Evaluate COLLAGE with SGD and other non-Adam optimizers to determine if the benefits extend beyond the tested AdamW baseline.