---
ver: rpa2
title: Towards Bayesian Data Selection
arxiv_id: '2406.12560'
source_url: https://arxiv.org/abs/2406.12560
tags:
- data
- learning
- bayesian
- selection
- rodemann
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of confirmation bias in iterative
  machine learning methods like semi-supervised learning, where models iteratively
  add data to their training sample. The author embeds data selection into decision
  theory, framing it as a decision problem with a state space, action space, and utility
  function.
---

# Towards Bayesian Data Selection

## Quick Facts
- arXiv ID: 2406.12560
- Source URL: https://arxiv.org/abs/2406.12560
- Authors: Julian Rodemann
- Reference count: 8
- One-line primary result: BPLS mitigates confirmation bias in iterative ML methods and provides inclusion probabilities for debiasing estimators

## Executive Summary
This paper addresses confirmation bias in iterative machine learning methods like semi-supervised learning, where models iteratively add data to their training sample. The author embeds data selection into decision theory, framing it as a decision problem with a state space, action space, and utility function. For self-training in semi-supervised learning, they derive a Bayes-optimal criterion for pseudo-label selection using the posterior predictive probability. The proposed Bayesian Pseudo-Label Selection (BPLS) method is empirically shown to mitigate confirmation bias on generalized linear models, semi-parametric generalized additive models, and Bayesian neural networks.

## Method Summary
The paper proposes Bayesian Pseudo-Label Selection (BPLS), which embeds data selection into decision theory. The method frames data selection as a decision problem with state space Θ, action space A, and utility function u. For self-training in semi-supervised learning, BPLS derives a Bayes-optimal criterion for pseudo-label selection using the posterior predictive probability. A key contribution is that BPLS naturally provides inclusion probabilities that enable importance sampling to debias estimators in the presence of distribution shifts induced by the learning algorithm.

## Key Results
- BPLS mitigates confirmation bias in iterative ML methods on generalized linear models, semi-parametric generalized additive models, and Bayesian neural networks
- The Bayes-optimal criterion for pseudo-label selection corresponds to the posterior predictive probability
- Inclusion probabilities derived from BPLS enable importance sampling to debias estimators in the presence of distribution shifts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Framing data selection as a decision problem with state space Θ, action space A, and utility function u enables derivation of Bayes-optimal selection criteria.
- Mechanism: By treating unlabeled data points as actions and model parameters as states of nature, standard Bayesian decision theory can be applied to find the action (data point) that maximizes expected utility under the posterior distribution.
- Core assumption: The utility function can be defined such that Bayes-optimal action selection corresponds to selecting the most informative or highest-quality pseudo-labels.
- Evidence anchors:
  - [abstract] "We embed this kind of data addition into decision theory by framing data selection as a decision problem."
  - [section 2] "Deﬁnition 1 (Canonical Decision Problem) Deﬁne (A, Θ, u (·)) as a decision-theoretic triple with an action space A, an unknown set of states of nature Θ and a utility function u : A× Θ→ R."
- Break condition: If the utility function cannot be meaningfully defined to capture the quality of pseudo-label selection, the decision-theoretic framework breaks down.

### Mechanism 2
- Claim: Using pseudo-label likelihood as utility (p(D∪(z,ŷ(z))|θ)) leads to the Bayes-optimal criterion being the posterior predictive probability of pseudo-samples and labeled data.
- Mechanism: The utility function measures how plausible it is that a selected data point with its predicted pseudo-label was generated jointly with the labeled data under model M. The Bayes-optimal criterion then becomes the posterior predictive, which naturally incorporates model uncertainty.
- Core assumption: The pseudo-label likelihood as defined provides a valid measure of utility for selection decisions.
- Evidence anchors:
  - [section 3] "Deﬁnition 2 (Pseudo-Label Likelihood as Utility) Given D and the prediction functional ŷ :X→Y , we define the following utility function u : AU× Θ→ R ((z,Y), θ )↦→ u((z,Y), θ ) = p(D∪ (z, ŷ(z))|θ, M )"
  - [section 3] "Theorem 3 (Pseudo Posterior Predictive) In the decision problem (AU , Θ, u (·)) and the pseudo-label likelihood as utility function... the Bayes criterion Φ(·, π ) :U→ R; a↦→ Φ(a, π ) = Eπ(u(a, θ )) corresponds to p(D∪ (xi, ŷi)|D ), which is the posterior predictive of pseudo-samples and labeled data"
- Break condition: If the model likelihood does not appropriately capture the quality of pseudo-labels, the posterior predictive criterion may not select good pseudo-labels.

### Mechanism 3
- Claim: The inclusion probabilities derived from the Bayes criterion enable importance sampling to debias estimators in the presence of distribution shifts induced by the learning algorithm.
- Mechanism: Since the Bayes criterion is a posterior predictive probability, it provides explicit inclusion probabilities for the sample enhancement mechanism. These can be used for inverse probability weighting to debias estimators when data selection induces distribution shifts.
- Core assumption: The inclusion probabilities derived from the Bayes criterion are accurate and can be effectively used for importance sampling.
- Evidence anchors:
  - [abstract] "A key contribution is that BPLS naturally provides inclusion probabilities that enable importance sampling to debias estimators in the presence of distribution shifts induced by the learning algorithm."
  - [section 4] "As follows from Theorem 3, the Bayes criterion for data selection is a posterior predictive probability distribution. We thus have explicit inclusion probabilities for the sample enhancement mechanism to address the issue that self-selected data is not i.i.d.. This allows for debiasing estimators through inverse probability weighting"
- Break condition: If the inclusion probabilities are inaccurate or if importance sampling fails to effectively debias estimators, this mechanism breaks down.

## Foundational Learning

- Concept: Decision Theory and Bayesian Decision Making
  - Why needed here: The paper embeds data selection into decision theory to derive Bayes-optimal criteria. Understanding the basic framework of decision problems (action space, state space, utility function) and how to find Bayes-optimal actions is essential.
  - Quick check question: In a decision problem (A, Θ, u(·)), what is the Bayes-optimal action if the utility is integrable?

- Concept: Semi-Supervised Learning and Self-Training
  - Why needed here: The paper focuses on self-training in semi-supervised learning as an illustrative case. Understanding the problem setting (labeled and unlabeled data, iterative pseudo-label assignment) is crucial for grasping the application of the decision-theoretic framework.
  - Quick check question: In self-training, what is the typical stopping criterion for iteratively assigning pseudo-labels to unlabeled data?

- Concept: Importance Sampling and Inverse Probability Weighting
  - Why needed here: The paper uses the inclusion probabilities from the Bayes criterion for importance sampling to debias estimators in the presence of distribution shifts. Understanding how to use importance weights to correct for biased sampling is key.
  - Quick check question: How does inverse probability weighting correct for biased estimators in the context of importance sampling?

## Architecture Onboarding

- Component map: Labeled data D, unlabeled data U -> Model (θ, π(θ), p(θ|D)) -> Decision Framework (AU, u((z,Y),θ) = p(D∪(z,ŷ(z))|θ,M)) -> Selection Criterion (Φ(a,π) = p(D∪(xi,ŷi)|D)) -> Debiasing (Importance sampling using inclusion probabilities)

- Critical path:
  1. Fit initial model on labeled data D to obtain prediction function ŷ(x)
  2. For each unlabeled data point, compute the posterior predictive probability (Bayes criterion)
  3. Select the data point with the highest posterior predictive probability
  4. Add the selected data point with its predicted pseudo-label to D
  5. Repeat until stopping criterion is met
  6. Use inclusion probabilities for importance sampling to debias estimators

- Design tradeoffs:
  - The decision-theoretic framework provides a principled way to derive selection criteria but requires defining an appropriate utility function.
  - Using the posterior predictive as the selection criterion incorporates model uncertainty but may be computationally expensive.
  - Importance sampling for debiasing can correct for distribution shifts but may increase estimator variance.

- Failure signatures:
  - Poor pseudo-label selection leading to confirmation bias or degraded model performance
  - Inaccurate inclusion probabilities leading to ineffective debiasing
  - Computational intractability of evaluating the posterior predictive for large datasets

- First 3 experiments:
  1. Implement BPLS on a simple simulated dataset (e.g., Gaussian mixture model) with known ground truth to verify that it selects high-quality pseudo-labels and mitigates confirmation bias.
  2. Compare BPLS to standard self-training with fixed confidence thresholds on a benchmark semi-supervised learning dataset (e.g., CIFAR-10 with limited labels) to assess improvements in accuracy and robustness to confirmation bias.
  3. Evaluate the effectiveness of importance sampling for debiasing on a dataset with known distribution shift induced by the self-training process, comparing biased and debiased estimators.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can inclusion probabilities be derived for reciprocal learning algorithms beyond self-training, and what impact would they have on debiasing estimators?
- Basis in paper: [explicit] The paper discusses that deriving inclusion probabilities for reciprocal learning beyond self-training is a venue for future work, and that inferential guarantees for debiased estimators have yet to be established.
- Why unresolved: The current work focuses on self-training, and extending the framework to other reciprocal learning algorithms would require new theoretical developments and empirical validation.
- What evidence would resolve it: Developing a general framework for deriving inclusion probabilities in various reciprocal learning settings, and conducting empirical studies to demonstrate the effectiveness of debiasing estimators using these probabilities.

### Open Question 2
- Question: What are the potential advantages and limitations of using importance resampling methods beyond inverse probability weighting for debiasing estimators in the presence of distribution shifts?
- Basis in paper: [explicit] The paper suggests that inverse probability weighting is prone to inflating estimators' variances and that exploring procedures beyond inverse probability weighting is crucial for future work.
- Why unresolved: The current work only mentions inverse probability weighting as a method for debiasing estimators, and the effectiveness of other importance resampling methods in this context is unknown.
- What evidence would resolve it: Comparative studies of different importance resampling methods, including but not limited to inverse probability weighting, in terms of their ability to debias estimators and control variance in the presence of distribution shifts.

### Open Question 3
- Question: How can the Bayesian Pseudo-Label Selection (BPLS) method be extended to handle multi-class classification problems with more than two classes?
- Basis in paper: [inferred] The current work focuses on binary classification, and extending BPLS to multi-class problems would require modifications to the utility function and the computation of the posterior predictive probabilities.
- Why unresolved: The current implementation of BPLS is tailored for binary classification, and its extension to multi-class problems is not straightforward.
- What evidence would resolve it: Developing an extension of BPLS for multi-class classification, along with empirical studies to demonstrate its effectiveness in comparison to existing methods for multi-class semi-supervised learning.

## Limitations
- The utility function formulation assumes that the pseudo-label likelihood appropriately captures the quality of pseudo-labels, which may not hold in all cases
- The computational complexity of evaluating the posterior predictive for large datasets or complex models could limit the scalability of the method
- The effectiveness of importance sampling for debiasing depends on the accuracy of the inclusion probabilities, which may be affected by model misspecification or limited data

## Confidence
- High: The decision-theoretic framing of data selection as a principled approach to deriving Bayes-optimal criteria.
- Medium: The empirical demonstration of BPLS's effectiveness in mitigating confirmation bias on the tested model types and datasets.
- Low: The general applicability of BPLS to a wide range of iterative machine learning methods beyond the tested cases.

## Next Checks
1. Evaluate BPLS on a diverse set of iterative machine learning methods, such as active learning or reinforcement learning, to assess its broader applicability.
2. Investigate the sensitivity of BPLS to hyperparameter choices, such as the prior distribution or the utility function parameters, to understand its robustness.
3. Conduct a thorough analysis of the computational complexity and scalability of BPLS, particularly for large-scale datasets and complex models, to identify potential bottlenecks and optimization opportunities.