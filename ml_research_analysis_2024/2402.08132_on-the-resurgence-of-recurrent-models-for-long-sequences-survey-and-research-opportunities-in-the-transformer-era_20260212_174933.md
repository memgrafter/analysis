---
ver: rpa2
title: On the Resurgence of Recurrent Models for Long Sequences -- Survey and Research
  Opportunities in the Transformer Era
arxiv_id: '2402.08132'
source_url: https://arxiv.org/abs/2402.08132
tags:
- recurrent
- learning
- linear
- which
- sequences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey examines the resurgence of recurrent neural networks
  for processing long sequences, highlighting their renewed relevance in the era of
  transformers. It reviews recent advances in recurrent models, including linear transformers
  and state-space models, which offer efficient alternatives to traditional attention-based
  approaches.
---

# On the Resurgence of Recurrent Models for Long Sequences -- Survey and Research Opportunities in the Transformer Era

## Quick Facts
- arXiv ID: 2402.08132
- Source URL: https://arxiv.org/abs/2402.08132
- Reference count: 7
- Primary result: Recurrent neural networks are experiencing a resurgence for processing long sequences, with linear transformers and state-space models offering efficient alternatives to traditional attention-based approaches.

## Executive Summary
This survey examines the renewed interest in recurrent neural networks (RNNs) for processing long sequences in the era dominated by transformers. The paper highlights recent advances in recurrent architectures, including linear transformers that reduce self-attention complexity to linear time, and state-space models (SSMs) that implement structured linear recurrence for efficient long-range dependency modeling. The authors emphasize the potential of these models for lifelong online learning from continuous data streams, addressing the gap between current offline-trained models and human-like learning from infinite-length sequences. The survey identifies key research opportunities in online learning algorithms and discusses challenges in processing long sequences through performance comparisons with traditional models.

## Method Summary
The paper provides a comprehensive survey of recent developments in recurrent architectures for long sequence processing. It examines linear transformers that kernelize self-attention to achieve linear complexity, state-space models with structured linear recurrence, and online learning algorithms like Real-Time Recurrent Learning (RTRT). The methods are evaluated through theoretical analysis and limited empirical comparisons on time series forecasting tasks. The survey synthesizes architectural details, computational complexity analysis, and research directions from recent literature, focusing on how these approaches address the limitations of traditional RNNs and transformers in handling very long sequences.

## Key Results
- Linear transformers achieve linear computational complexity for self-attention through kernelization, reducing the O(L²) bottleneck of standard transformers
- State-space models with structured linear recurrence (diagonal state matrices) enable efficient long-range dependency modeling through convolution operations
- Recurrent models show promise for lifelong online learning from continuous data streams, addressing limitations of offline-trained models in handling infinite-length sequences

## Why This Works (Mechanism)

### Mechanism 1
Linear Transformers reduce self-attention's quadratic complexity to linear by kernelizing similarity computation. They replace the dot-product similarity in self-attention with a kernel function φ(a)^T φ(b), enabling computation of the attention matrix as a product of feature maps. This allows accumulation of weighted values in a recurrent state that is updated additively at each time step. The core assumption is that the kernel φ captures the essential similarity structure of the original softmax attention while being efficiently computable. Break condition: If the kernel approximation diverges significantly from softmax attention, model performance degrades.

### Mechanism 2
State-space models (SSMs) implement long-range dependencies through structured linear recurrence with learnable parameters. They use continuous-time ODEs with diagonal (or diagonal-plus-low-rank) state matrices, enabling efficient computation via convolution. The recurrence is stable due to exponential parameterization and proper initialization from HiPPO theory. The core assumption is that diagonal state matrices with complex eigenvalues can capture long-range dependencies while maintaining computational efficiency. Break condition: If the structured matrix constraints are too restrictive, model expressiveness suffers compared to full attention mechanisms.

### Mechanism 3
Real-time recurrent learning (RTRL) enables true online learning from infinite-length sequences without backpropagation through time. RTRL computes exact gradients by maintaining and updating the Jacobian of the hidden state with respect to parameters at each time step, allowing parameter updates without storing past activations or unrolling the network. The core assumption is that the computational and memory overhead of maintaining Jacobians is manageable for practical sequence lengths. Break condition: The Jacobian update complexity becomes prohibitive for large models, requiring approximations.

## Foundational Learning

- **Concept: Backpropagation Through Time (BPTT)**
  - Why needed here: BPTT is the standard algorithm for training RNNs and understanding its limitations (vanishing/exploding gradients) is crucial for appreciating why new recurrent architectures are needed.
  - Quick check question: What causes the vanishing gradient problem in BPTT, and how do LSTMs and GRUs attempt to address it?

- **Concept: Self-attention mechanism**
  - Why needed here: Self-attention is the core component of Transformers, and understanding its computational complexity and how it captures dependencies is essential for comparing it with recurrent approaches.
  - Quick check question: What is the computational complexity of self-attention with respect to sequence length, and why does this become problematic for very long sequences?

- **Concept: Ordinary Differential Equations (ODEs) in neural networks**
  - Why needed here: SSMs are formalized using ODEs, and understanding how continuous-time dynamics can be discretized and learned is key to grasping their advantages over discrete-time RNNs.
  - Quick check question: How does the discretization of continuous-time SSMs relate to convolutional operations, and why does this enable efficient long-range modeling?

## Architecture Onboarding

- **Component map**: Input → kernel projection → state accumulation → output projection (Linear Transformers); Input → state-space discretization → state update → output projection (SSMs); Input → Jacobian update → parameter update → output (online RNNs)

- **Critical path**: For Linear Transformers: input → kernel projection → state accumulation → output projection. For SSMs: input → state-space discretization → state update → output projection. For online RNNs: input → Jacobian update → parameter update → output.

- **Design tradeoffs**: Linear Transformers trade exact attention for efficiency; SSMs trade flexibility for structured efficiency; online RNNs trade exactness for computational feasibility.

- **Failure signatures**: Performance degradation when kernel approximation is poor (Linear Transformers), when structured matrices are too restrictive (SSMs), or when Jacobian approximations accumulate error (online RNNs).

- **First 3 experiments**:
  1. Implement a basic Linear Transformer with random feature kernels and compare attention scores to standard softmax attention on a small sequence classification task.
  2. Implement a simple S4 model and compare its performance on a synthetic long sequence task (e.g., sinusoid prediction) against an LSTM.
  3. Implement a basic RTRL update for a small RNN and compare its online learning performance on a streaming regression task against truncated BPTT.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can recurrent neural networks achieve state-of-the-art performance on long sequence tasks while maintaining linear computational complexity?
- Basis in paper: [explicit] The paper discusses the resurgence of recurrent models and their potential for efficient processing of long sequences, particularly in comparison to transformers with quadratic complexity.
- Why unresolved: While the paper highlights the potential of recurrent models like linear transformers and state-space models, it does not provide conclusive evidence that they can definitively surpass transformers in all long sequence tasks.
- What evidence would resolve it: Comparative studies on a wide range of long sequence tasks, demonstrating that recurrent models consistently outperform transformers while maintaining linear complexity.

### Open Question 2
- Question: How can recurrent models be effectively adapted for lifelong online learning from potentially infinite-length sequences?
- Basis in paper: [explicit] The paper emphasizes the need for models that can learn from continuous data streams and handle potentially infinite-length sequences, a challenge not fully addressed by current offline-trained models.
- Why unresolved: The paper acknowledges the gap between current models and the ability to learn from infinite-length sequences, but does not provide a concrete solution or framework for achieving this.
- What evidence would resolve it: Development and empirical validation of recurrent models specifically designed for lifelong online learning, demonstrating their ability to adapt to changing data distributions and retain learned information over time.

### Open Question 3
- Question: What are the optimal architectural and algorithmic choices for recurrent models to balance the trade-off between long-term dependency preservation and computational efficiency?
- Basis in paper: [explicit] The paper discusses various approaches to improve recurrent models, such as kernel-based linear transformers and state-space models, but does not provide a definitive answer on the optimal design choices.
- Why unresolved: The paper presents multiple promising directions but does not conclusively determine which architectural and algorithmic choices are most effective in balancing long-term dependency preservation and computational efficiency.
- What evidence would resolve it: Systematic comparative studies evaluating different architectural and algorithmic choices for recurrent models on a variety of long sequence tasks, quantifying their performance in terms of both long-term dependency preservation and computational efficiency.

## Limitations

- The survey relies heavily on theoretical foundations and recent developments without extensive empirical validation across diverse benchmarks
- Performance comparisons are limited to specific tasks and may not generalize to all long-sequence applications
- The treatment of online learning algorithms lacks detailed analysis of practical implementation challenges and trade-offs in real-world scenarios

## Confidence

- **High**: Claims about computational complexity reductions in linear transformers and state-space models are well-established and mathematically grounded
- **Medium**: Assertions about the resurgence of recurrent models are supported by recent literature but may be influenced by current research trends and limited by the scope of reviewed works
- **Low**: Predictions about the future dominance of recurrent models in lifelong online learning are speculative and depend on solving significant technical challenges not yet fully addressed

## Next Checks

1. **Empirical Validation**: Conduct extensive experiments comparing linear transformers, state-space models, and traditional transformers across multiple long-sequence benchmarks, including language modeling, video processing, and time series forecasting, to validate the claimed performance advantages.

2. **Scalability Analysis**: Investigate the scalability of recurrent models in terms of computational resources and memory usage when processing extremely long sequences, comparing them with transformer-based approaches to assess practical feasibility.

3. **Online Learning Implementation**: Implement and test online learning algorithms, such as RTRL and truncated BPTT, in a real-time streaming scenario to evaluate their effectiveness and identify potential bottlenecks or failure modes in continuous data stream processing.