---
ver: rpa2
title: 'Comparing supervised learning dynamics: Deep neural networks match human data
  efficiency but show a generalisation lag'
arxiv_id: '2402.09303'
source_url: https://arxiv.org/abs/2402.09303
tags:
- learning
- training
- data
- humans
- dnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the learning dynamics of humans and deep neural
  networks (DNNs) in a constrained supervised learning environment. Both systems were
  tasked with learning representations of novel naturalistic 3D objects across six
  training epochs, with periodic testing on previously unseen images.
---

# Comparing supervised learning dynamics: Deep neural networks match human data efficiency but show a generalisation lag

## Quick Facts
- arXiv ID: 2402.09303
- Source URL: https://arxiv.org/abs/2402.09309
- Reference count: 16
- Key outcome: DNNs match human data efficiency but exhibit pronounced generalization lag in constrained supervised learning

## Executive Summary
This study compares the learning dynamics of humans and deep neural networks (DNNs) in a constrained supervised learning environment. Both systems were tasked with learning representations of novel naturalistic 3D objects across six training epochs, with periodic testing on previously unseen images. The learning conditions were carefully aligned, including starting point (pre-trained models), input modality (static 2D images), and learning modality (supervised). The primary findings reveal that while DNNs match human data efficiency, they exhibit a pronounced generalization lag, learning training-set-specific information before generalizing.

## Method Summary
The study used a constrained supervised learning environment with novel 3D objects created via digital embryo algorithms, rendered as 2D images. Humans and DNNs (AlexNet, VGG-16, ResNet-50, ViT, ConvNeXt, EfficientNet) were trained on 36 images across six epochs with periodic testing on 51 unseen images. Learning conditions were carefully aligned including pre-trained models, static 2D input, and supervised feedback. Data efficiency was measured as test accuracy gain per training image, while generalization lag was quantified as the mean difference between training and test accuracy.

## Key Results
- DNNs achieved 74.64-83.13% test accuracy (classic vs. SOTA models) vs. 75.35% for humans by epoch 6
- Data efficiency was similar across all observers (humans and DNNs)
- Generalization lag was minimal for humans (0.002) but substantial for DNNs (0.048-0.256)
- Humans immediately formed generalizable representations without a preliminary training-set-specific phase

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human learners acquire immediately generalizable representations without a preliminary training-set-specific phase.
- Mechanism: Humans leverage rich prior knowledge from natural and artificial object representations to bypass the need for initial memorization of specific features before abstraction.
- Core assumption: Pre-trained DNNs partially mimic human prior knowledge, yet still exhibit generalization lag.
- Evidence anchors: [abstract] "humans appear to immediately acquire generalizable representations without a preliminary phase of learning training set-specific information"; [section] "humans seem to bypass this staged process, directly forming generalizable representations from the onset of training"
- Break condition: If human learners start from near-chance performance and show delayed generalization similar to DNNs, the mechanism fails.

### Mechanism 2
- Claim: DNNs show a pronounced generalization lag due to a two-phase learning pattern: initial memorization followed by abstraction.
- Mechanism: DNNs first learn specific, training-set-dependent features, then refine these into generalizable representations, resulting in delayed transfer to unseen data.
- Core assumption: The architecture and training process inherently favor memorization before abstraction.
- Evidence anchors: [abstract] "while DNNs' learning is characterized by a pronounced generalisation lag, humans appear to immediately acquire generalizable representations"; [section] "initial phase where specific features of the training data are learned, followed by a later phase where these representations are refined and abstracted for generalisation"
- Break condition: If DNNs achieve high test performance early without signs of overfitting, the mechanism fails.

### Mechanism 3
- Claim: When learning conditions are carefully aligned, DNNs match human data efficiency.
- Mechanism: Equalizing starting points (pre-trained models), input modality (static 2D images), and learning modality (supervised) removes confounding factors that typically make DNNs appear less data-efficient.
- Core assumption: Data efficiency differences between humans and DNNs are largely due to mismatched learning conditions rather than inherent model limitations.
- Evidence anchors: [abstract] "Comparisons across the entire learning process indicate that DNNs demonstrate a level of data efficiency comparable to human learners"; [section] "we aimed to match and maintain consistency in the learning environment across both humans and DNNs"
- Break condition: If DNNs still show significantly lower data efficiency under aligned conditions, the mechanism fails.

## Foundational Learning

- Concept: Generalization lag
  - Why needed here: Central to understanding the representational differences between humans and DNNs; explains why DNNs perform well on training data but lag on test data.
  - Quick check question: What does generalization lag measure in this study?
- Concept: Data efficiency
  - Why needed here: Key metric for comparing learning efficiency; challenges the assumption that DNNs require more data than humans.
  - Quick check question: How is data efficiency quantified in the experiment?
- Concept: Supervised learning alignment
  - Why needed here: Ensures fair comparison by matching learning conditions; eliminates confounding factors like multi-modal input or unsupervised learning.
  - Quick check question: What aspects of the learning environment were aligned between humans and DNNs?

## Architecture Onboarding

- Component map: 3D object generation -> Image rendering -> Training phase -> Testing phase -> Performance evaluation
- Critical path:
  1. Generate novel stimuli ensuring no pre-existing representations
  2. Align learning conditions (input, modality, feedback) for humans and DNNs
  3. Train DNNs on the same data as humans, tracking performance
  4. Measure and compare data efficiency and generalization lag
- Design tradeoffs:
  - Using pre-trained models vs. training from scratch: Balances realism with computational feasibility
  - Static 2D images vs. multi-modal input: Simplifies alignment but may not capture full human learning
  - Short presentation times: Mimics rapid visual processing but may limit depth of learning
- Failure signatures:
  - DNNs show no improvement in test accuracy despite training progress (overfitting)
  - Human learners start with significantly above-chance performance (pre-existing knowledge)
  - Data efficiency diverges significantly between humans and DNNs
- First 3 experiments:
  1. Verify no pre-existing knowledge in humans by checking initial chance-level performance
  2. Confirm learning by tracking improvement in training accuracy across epochs
  3. Compare early generalization by measuring test accuracy in the first few epochs

## Open Questions the Paper Calls Out

- What neural mechanisms or architectural features in DNNs could be modified to reduce or eliminate the generalization lag observed in this study?
- How does the generalization lag phenomenon observed in this study translate to more complex, real-world learning scenarios with larger datasets and more categories?
- What specific aspects of human learning (e.g., attention, active learning, or multi-modal integration) could be incorporated into DNN training to improve immediate generalization?

## Limitations

- The study uses a constrained learning environment with only 3 categories and 36 training images per category, which may not generalize to more complex, real-world scenarios
- Results are based on static 2D images and supervised learning, excluding the multi-modal and unsupervised learning capabilities that humans naturally possess
- The use of pre-trained models partially bridges the representational gap but doesn't fully capture the rich prior knowledge humans bring to novel learning tasks

## Confidence

- **High confidence**: Data efficiency comparisons between DNNs and humans under aligned conditions (empirical measurements are straightforward and reproducible)
- **Medium confidence**: Generalization lag findings (the effect is robust, but the underlying mechanisms remain speculative)
- **Medium confidence**: Claims about human learning mechanisms (based on experimental observation rather than direct measurement of cognitive processes)

## Next Checks

1. Test whether the generalization lag persists when using larger training sets (100-500 images) to determine if this is an inherent architectural limitation or a data scarcity effect
2. Compare learning dynamics when starting from scratch vs. pre-trained models to quantify the contribution of prior knowledge to data efficiency
3. Implement curriculum learning strategies for DNNs to test whether staged exposure can eliminate the generalization lag observed in this study