---
ver: rpa2
title: Integrating Present and Past in Unsupervised Continual Learning
arxiv_id: '2404.19132'
source_url: https://arxiv.org/abs/2404.19132
tags:
- learning
- task
- loss
- conference
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of unsupervised continual learning
  (UCL), where models learn from sequential streams of unlabeled data without forgetting
  past tasks. The authors propose a unifying framework that decomposes UCL objectives
  into three components: plasticity (learning current task), stability (retaining
  past knowledge), and cross-task consolidation (distinguishing between different
  tasks).'
---

# Integrating Present and Past in Unsupervised Continual Learning

## Quick Facts
- arXiv ID: 2404.19132
- Source URL: https://arxiv.org/abs/2404.19132
- Reference count: 22
- Key outcome: Osiris achieves state-of-the-art performance on unsupervised continual learning benchmarks by optimizing plasticity, stability, and cross-task consolidation in separate embedding spaces.

## Executive Summary
This paper addresses unsupervised continual learning (UCL), where models learn from sequential streams of unlabeled data without forgetting past tasks. The authors propose a unifying framework that decomposes UCL objectives into three components: plasticity (learning current task), stability (retaining past knowledge), and cross-task consolidation (distinguishing between different tasks). They introduce Osiris, a method that explicitly optimizes these three objectives on separate embedding spaces using parallel projector branches. Osiris achieves state-of-the-art performance on standard UCL benchmarks and introduces two novel structured benchmarks that better reflect real-world learning scenarios.

## Method Summary
Osiris is a UCL method that decomposes the learning objective into three components: plasticity (learning current task), stability (retaining past knowledge), and cross-task consolidation (distinguishing between different tasks). The method uses parallel projector branches for each objective, allowing each to optimize its target distribution without being constrained by the others. This architectural isolation enables the model to learn features for the current task freely while maintaining past knowledge and distinguishing between tasks. The approach uses contrastive learning with GroupNorm instead of BatchNorm, which is incompatible with UCL due to its assumption of stationary distributions.

## Key Results
- Osiris achieves 53.0% accuracy on 5-task Split-CIFAR-100, outperforming previous best of 51.2%
- On Structured Tiny-ImageNet, Osiris (37.5%) even surpasses offline training (37.5%) on the structured Tiny-ImageNet benchmark
- The method demonstrates the importance of addressing all three objectives simultaneously for optimal UCL performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Osiris achieves superior performance by explicitly optimizing plasticity, stability, and cross-task consolidation in separate embedding spaces, preventing interference between objectives.
- **Mechanism:** The model uses parallel projector branches for each objective, allowing each to optimize its target distribution without being constrained by the others. This architectural isolation enables the model to learn features for the current task freely while maintaining past knowledge and distinguishing between tasks.
- **Core assumption:** Separate embedding spaces for different objectives prevent optimization conflicts and improve overall representation quality.
- **Evidence anchors:** [abstract] "Our method, Osiris, which explicitly optimizes all three objectives on separate embedding spaces, achieves state-of-the-art performance on all benchmarks"; [section] "With isolated features spaces, our method reduces the extent to which Lcross or Lpast directly constrain the model from learning the current task, which promotes plasticity"
- **Break condition:** If the encoder cannot maintain a unified representation that preserves information useful for all three losses, or if the separate projectors become too specialized and lose cross-task generalization ability.

### Mechanism 2
- **Claim:** BatchNorm is incompatible with UCL because it assumes stationary distributions, while UCL involves non-stationary data streams.
- **Mechanism:** BatchNorm maintains running estimates of feature moments over the batch dimension, which become biased toward the most recent task when data distribution changes. This bias impairs the model's ability to retain knowledge from previous tasks.
- **Core assumption:** BatchNorm's moment estimation requires stationarity, which is violated in UCL settings.
- **Evidence anchors:** [section] "It has been shown that BatchNorm is not suitable for SCL since its running estimates of the feature moments are biased towards the most recent task"; [section] "we find that BatchNorm is not suitable for UCL since it presupposes a stationary distribution, and advise future studies to use GroupNorm instead"
- **Break condition:** If the data stream becomes stationary or if alternative normalization techniques that don't rely on running batch statistics are employed.

### Mechanism 3
- **Claim:** Realistic task structures in learning sequences provide better curriculum than random task ordering, benefiting UCL models.
- **Mechanism:** Structured task sequences create natural semantic correlations within and between tasks, allowing the model to build hierarchical representations that generalize better. The temporal and hierarchical structure mirrors real-world learning scenarios.
- **Core assumption:** Real-world visual signals have inherent structure that can be leveraged for better representation learning.
- **Evidence anchors:** [abstract] "we propose two UCL benchmarks, Structured CIFAR-100 and Structured Tiny-ImageNet, that feature semantic structure on classes within or across tasks"; [section] "we show some preliminary evidence that continual models can benefit from such realistic learning scenarios"; [section] "On the Structured Tiny-ImageNet benchmark, our method outperforms the offline iid model, showing some preliminary evidence that UCL algorithms can benefit from real-world task structures"
- **Break condition:** If the structured sequences don't actually reflect real-world learning patterns or if the model overfits to the specific structure rather than learning generalizable features.

## Foundational Learning

- **Concept:** Self-supervised learning objectives
  - Why needed here: UCL operates without labels, so SSL provides the learning signal through contrastive learning that enforces invariance to augmentations
  - Quick check question: What is the core objective of contrastive learning in SSL?

- **Concept:** Catastrophic forgetting
  - Why needed here: UCL involves sequential learning where models must retain knowledge from previous tasks while learning new ones
  - Quick check question: How does catastrophic forgetting manifest in continual learning scenarios?

- **Concept:** Stability-plasticity dilemma
  - Why needed here: Models need stability to retain past knowledge but plasticity to learn new tasks effectively
  - Quick check question: What are the two competing requirements that create the stability-plasticity dilemma?

## Architecture Onboarding

- **Component map:**
  - Backbone encoder (ResNet-18) -> Current task projector (gΦ) -> Current task loss
  - Backbone encoder (ResNet-18) -> Stability projector (hΨ) -> Past task loss and cross-task consolidation loss
  - Memory buffer -> Samples for stability and cross-task losses

- **Critical path:**
  1. Sample batch from current task and memory
  2. Compute current task loss on separate projector
  3. Compute stability loss using memory samples
  4. Compute cross-task consolidation loss contrasting current and past
  5. Combine losses and update encoder

- **Design tradeoffs:**
  - Separate vs shared projectors: isolation improves plasticity but may reduce parameter efficiency
  - Distillation vs replay for stability: distillation requires storing checkpoints but avoids overfitting to memory
  - Memory size vs computational cost: larger memory improves stability but increases overhead

- **Failure signatures:**
  - High forgetting indicates insufficient stability loss or memory buffer issues
  - Poor knowledge gain suggests plasticity loss isn't effective or current task projector is constrained
  - Low consolidation score means cross-task loss isn't working or projector isolation is too strict

- **First 3 experiments:**
  1. Implement basic Osiris with all three losses and compare to FT baseline on 5-task CIFAR-100
  2. Test BatchNorm vs GroupNorm impact on same benchmark
  3. Compare replay vs distillation variants on forgetting and knowledge gain metrics

## Open Questions the Paper Calls Out
None explicitly called out in the provided content.

## Limitations
- The study's focus on structured benchmarks, while more realistic, limits generalizability to truly random task orderings.
- The memory buffer size (500 samples) and its sampling strategy may significantly impact results, but implementation details remain underspecified.
- The claim that separate projector branches universally prevent optimization interference requires further validation across diverse UCL scenarios.

## Confidence
- **High confidence**: BatchNorm incompatibility with UCL (supported by established literature and empirical results)
- **Medium confidence**: Osiris's superiority on structured benchmarks (strong results but limited to specific scenarios)
- **Medium confidence**: Separate embedding spaces improving plasticity (plausible mechanism but could benefit from ablation studies)

## Next Checks
1. **Ablation study on projector isolation**: Test variants where current task and stability objectives share the same projector to quantify the benefit of architectural separation.

2. **Memory buffer sensitivity analysis**: Systematically vary memory buffer size and sampling strategies to determine their impact on forgetting and knowledge gain metrics.

3. **Generalization to random task orders**: Evaluate Osiris on benchmarks with randomized class splits to verify if the structured task benefit holds across different task orderings.