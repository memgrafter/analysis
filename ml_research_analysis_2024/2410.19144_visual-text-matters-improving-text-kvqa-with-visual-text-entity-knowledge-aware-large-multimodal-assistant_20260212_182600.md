---
ver: rpa2
title: 'Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware
  Large Multimodal Assistant'
arxiv_id: '2410.19144'
source_url: https://arxiv.org/abs/2410.19144
tags:
- visual
- text
- entity
- knowledge
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the knowledge-aware text-based visual question
  answering (Text-KVQA) task, where the goal is to answer questions about visual text
  entities (e.g., brand names, book titles) using both the image context and external
  knowledge. The authors propose a two-stage approach: first, they introduce VisTEL,
  a visual text entity linker that uses a large multimodal model (LMM) to link OCR-detected
  text in images to entities in a knowledge base, leveraging both textual and visual
  context.'
---

# Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant

## Quick Facts
- arXiv ID: 2410.19146
- Source URL: https://arxiv.org/abs/2410.19146
- Reference count: 16
- One-line primary result: Achieves state-of-the-art performance on Text-KVQA with 18.2% (scene), 19.6% (book covers), and 32.2% (movie posters) absolute accuracy improvements

## Executive Summary
This paper addresses knowledge-aware text-based visual question answering (Text-KVQA) by proposing a two-stage approach that combines visual text entity linking with knowledge-aware reasoning. The authors introduce VisTEL, a visual text entity linker that uses a large multimodal model to link OCR-detected text to knowledge base entities, and KaLMA, a knowledge-aware assistant that augments LMMs with retrieved knowledge to answer questions accurately. The approach significantly improves performance on the Text-KVQA dataset by addressing hallucinations and improving reasoning through explicit entity linking and knowledge retrieval.

## Method Summary
The method employs a two-stage approach: first, VisTEL links OCR-extracted visual text to knowledge base entities using an LMM that reasons jointly over text and visual context; second, KaLMA augments an LMM with retrieved knowledge to answer questions about visual text entities. The system uses DBNet and ParSeq for visual text recognition, LoRA fine-tuning for parameter-efficient adaptation of LLaVA-1.5, and instruction prompting for both entity linking and question answering. The pipeline processes images through OCR extraction, entity linking via VisTEL, knowledge retrieval from Wikidata/IMDB, and finally reasoning via KaLMA to generate answers with supporting facts.

## Key Results
- Achieves state-of-the-art performance on Text-KVQA with significant accuracy improvements across all splits
- 18.2% absolute accuracy improvement on scene images (50K questions, 10K images, 500 entities)
- 19.6% absolute accuracy improvement on book covers (1M questions, 207K images, 207K entities)
- 32.2% absolute accuracy improvement on movie posters (222K questions, 34K images, 34K entities)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large multimodal models (LMMs) have strong implicit knowledge learned from pretraining, but they often hallucinate on visual text entities when asked to reason about them.
- Mechanism: LMMs encode images and text jointly, but without explicit entity linking, they rely on correlated visual features rather than accurate textual grounding, leading to hallucinations (e.g., confusing Domino's Pizza with Pizza Hut).
- Core assumption: The pretraining data covers enough entity knowledge that LMMs can "guess" correctly in many cases, but this guesswork fails when visual text is stylized, abbreviated, or contextually ambiguous.
- Evidence anchors:
  - [abstract] "These models are rich in the implicit knowledge learned by large-scale pretraining. However, despite their numerous advantages, they are not without drawbacks, notably hallucinations."
  - [section] "Existing LMMs often hallucinate over the pizza present in the image and points to the website of ‘Pizza Hut’ instead of ‘Domino’s’."
  - [corpus] Weak - no direct evidence from neighbors about hallucination patterns in LMMs.
- Break condition: If visual text is clear, standard, and highly correlated with obvious visual objects, hallucination risk is low.

### Mechanism 2
- Claim: VisTEL improves entity linking by jointly reasoning on OCR-extracted text and visual context within the LMM framework.
- Mechanism: VisTEL uses a visual text recognition engine (DBNet + ParSeq) to extract OCR text, finds candidate entities via NED, then frames an instruction prompt with both OCR text and candidates, encoding it with image features in an LMM to predict the correct entity.
- Core assumption: The surrounding OCR text and visual context provide disambiguating signals that are sufficient for the LMM to choose the correct entity among candidates.
- Evidence anchors:
  - [section] "VisTEL is an LMM-based architecture that leverages the surrounding OCR-extracted texts... and the visual context within the image to perform highly accurate entity linking."
  - [section] "VisTEL auto-regressively predicts the probability of the next token EIt in the target entity EI by attending to the input prompt tokens and the previously generated entity tokens."
  - [corpus] Weak - no direct evidence from neighbors about joint OCR+visual reasoning in LMMs.
- Break condition: If OCR is highly noisy or if candidate entities are too numerous and visually ambiguous, VisTEL performance degrades.

### Mechanism 3
- Claim: KaLMA's integration of retrieved knowledge with image and question context via LMM reasoning produces accurate answers and supporting facts.
- Mechanism: KaLMA takes visual features, frames a prompt with question + retrieved knowledge, encodes both via LMM embedding, concatenates, and generates answer + supporting fact in an auto-regressive manner.
- Core assumption: The retrieved knowledge is relevant and concise enough to guide the LMM toward correct reasoning without overwhelming it.
- Evidence anchors:
  - [abstract] "KaLMA – a knowledge-aware large multimodal assistant that augments an LMM with knowledge associated with visual text entity in the image to arrive at an accurate answer."
  - [section] "KaLMA predicts the probability of the next token Aat in the answer Aa in an auto-regressive manner. It does so by attending to the prompt inputs and the previously generated tokens."
  - [corpus] Weak - no direct evidence from neighbors about knowledge retrieval + LMM reasoning.
- Break condition: If retrieved knowledge is irrelevant, contradictory, or missing, KaLMA reverts to hallucination patterns.

## Foundational Learning

- Concept: Visual text entity linking in multimodal contexts
  - Why needed here: Text-KVQA requires mapping visual text (often stylized, abbreviated, or ambiguous) to knowledge base entities; naive OCR + string matching fails.
  - Quick check question: Given an image with "7-Eleven" written in a stylized font, can you list the steps to link it to the correct entity in a knowledge base?

- Concept: Multimodal instruction prompting and auto-regressive generation
  - Why needed here: KaLMA must fuse visual features, question, and knowledge into a coherent prompt for LMM generation of both answer and supporting fact.
  - Quick check question: How would you structure a prompt for an LMM that takes an image, a question about a brand in that image, and retrieved facts about that brand?

- Concept: Parameter-efficient fine-tuning (LoRA) for large models
  - Why needed here: Full fine-tuning of LLaVA-1.5 is computationally prohibitive; LoRA enables efficient adaptation for both VisTEL and KaLMA.
  - Quick check question: What are the key hyperparameters of LoRA (rank, alpha, dropout) and how do they affect fine-tuning stability?

## Architecture Onboarding

- Component map: DBNet/ParSeq (OCR extraction) -> VisTEL (entity linking) -> Knowledge retrieval (KB lookup) -> KaLMA (reasoning) -> LLaVA-1.5 (LMM backbone)
- Critical path: OCR extraction → VisTEL linking → knowledge retrieval → KaLMA reasoning
- Design tradeoffs:
  - Joint reasoning vs. modular separation: VisTEL merges OCR and visual reasoning but adds complexity.
  - Knowledge retrieval vs. implicit knowledge: Explicit facts improve accuracy but require KB access and latency.
  - Auto-regressive generation vs. classification: Generation allows supporting facts but is slower and more error-prone.
- Failure signatures:
  - Low OCR quality → VisTEL cannot disambiguate candidates → wrong entity linking
  - Missing or irrelevant KB facts → KaLMA hallucinates despite correct entity
  - Prompt formatting errors → LMM ignores image or knowledge context
- First 3 experiments:
  1. Replace VisTEL with NED-only retrieval and measure drop in Text-KVQA accuracy.
  2. Remove supporting fact generation from KaLMA and measure impact on answer accuracy.
  3. Swap DBNet/ParSeq with a weaker OCR pipeline and measure effect on downstream performance.

## Open Questions the Paper Calls Out

- Question: How does the performance of KaLMA change when using different visual text recognition engines, particularly in terms of entity linking accuracy and downstream VQA performance?
  - Basis in paper: [explicit] The paper conducts an ablation study comparing different text detection and recognition approaches (DBNet+ParSeq, CRAFT+CRNN, EAST+CRNN) and their impact on KaLMA's performance on Text-KVQA.
  - Why unresolved: While the paper reports results for different recognition engines, it does not provide a detailed analysis of how the choice of engine affects the entity linking stage (VisTEL) specifically, or explore a wider range of recognition models.
  - What evidence would resolve it: A comprehensive study comparing multiple visual text recognition engines, including their impact on both entity linking accuracy and final VQA performance, would clarify the importance of this component.

- Question: Can the proposed approach be effectively extended to handle multilingual visual text entities and knowledge bases?
  - Basis in paper: [inferred] The paper acknowledges the limitation of current visual text recognition engines being ineffective over multi-lingual text and assumes English text in the dataset.
  - Why unresolved: The paper does not explore or experiment with multilingual scenarios, leaving open the question of how well the approach would generalize to other languages.
  - What evidence would resolve it: Experiments on a multilingual version of Text-KVQA or similar datasets, along with adaptations to handle non-English text and knowledge bases, would demonstrate the approach's effectiveness in multilingual settings.

- Question: How does the performance of KaLMA scale with the size and complexity of the knowledge base?
  - Basis in paper: [inferred] The paper uses knowledge bases of varying sizes for different splits (scene, book, movie) but does not systematically analyze the impact of knowledge base size or complexity on performance.
  - Why unresolved: The paper does not explore scenarios with significantly larger or more complex knowledge bases, nor does it analyze how the retrieval and reasoning processes scale with increasing knowledge base size.
  - What evidence would resolve it: Experiments with artificially scaled-up knowledge bases or real-world large-scale knowledge graphs would provide insights into the approach's scalability and performance in more complex scenarios.

## Limitations
- Low confidence in the effectiveness of the two-stage approach due to lack of ablation studies showing baseline LMM performance on full Text-KVQA pipeline
- Medium confidence in hallucination reduction claims due to indirect evidence and no quantitative comparison of hallucination rates
- Low confidence in KB coverage and relevance due to unspecified completeness and quality of knowledge base used

## Confidence
- Claim cluster: VisTEL improves entity linking over NED-only methods - Medium confidence
- Claim cluster: KaLMA achieves SOTA on Text-KVQA - Medium confidence
- Claim cluster: Two-stage approach reduces hallucinations - Low confidence

## Next Checks
1. **Ablation study with baseline LMM**: Run the full Text-KVQA pipeline using a standard LMM (e.g., LLaVA-1.5 without VisTEL/KaLMA fine-tuning) and compare hallucination rates and accuracy on the same test set to quantify the actual improvement from the two-stage approach.

2. **KB coverage analysis**: Measure the percentage of test entities that have sufficient knowledge facts in the KB, and evaluate KaLMA's performance separately on entities with complete vs. incomplete knowledge coverage to assess KB dependency.

3. **OCR noise robustness test**: Systematically degrade OCR quality (e.g., by adding noise, using different fonts, or applying image distortions) and measure the impact on VisTEL accuracy to determine the practical limits of the entity linking component.