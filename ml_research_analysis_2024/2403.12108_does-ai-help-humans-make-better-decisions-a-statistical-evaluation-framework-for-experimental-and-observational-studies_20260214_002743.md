---
ver: rpa2
title: Does AI help humans make better decisions? A statistical evaluation framework
  for experimental and observational studies
arxiv_id: '2403.12108'
source_url: https://arxiv.org/abs/2403.12108
tags:
- white
- decisions
- human
- 'false'
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a methodological framework to evaluate whether
  AI recommendations improve human decision-making, compared to human-alone or AI-alone
  systems. The core idea is to formalize a decision-maker's "ability" as a classification
  problem using standard confusion matrices based on potential outcomes.
---

# Does AI help humans make better decisions? A statistical evaluation framework for experimental and observational studies

## Quick Facts
- arXiv ID: 2403.12108
- Source URL: https://arxiv.org/abs/2403.12108
- Reference count: 40
- Primary result: Introduces a framework to evaluate AI decision support using RCT data, showing PSA recommendations don't significantly improve judges' classification ability

## Executive Summary
This paper introduces a methodological framework to evaluate whether AI recommendations improve human decision-making compared to human-alone or AI-alone systems. The core innovation is formalizing decision-maker "ability" as a classification problem using confusion matrices based on potential outcomes. The authors consider an experimental design where AI recommendations are randomized across cases, allowing them to overcome the selective labels problem and identify the difference in classification risk between human-alone and human-with-AI systems.

The framework derives partial identification bounds to compare AI-alone decisions with human decisions, even without an AI-alone treatment arm. Applied to a randomized controlled trial evaluating a pretrial risk assessment instrument (PSA), the results show PSA recommendations do not significantly improve judges' classification ability. Furthermore, PSA-alone decisions tend to have higher misclassification rates and false positive proportions compared to human-alone or human-with-PSA decisions, especially for non-white arrestees.

## Method Summary
The paper proposes a statistical framework for evaluating AI decision support systems using randomized experiments. The core approach involves randomizing AI recommendations across cases where humans make final decisions, enabling comparison of three systems: human-alone, human-with-AI, and AI-alone. For human-AI comparisons, the framework identifies differences in classification risk using potential outcomes and confusion matrices. For AI-alone comparisons, it derives partial identification bounds leveraging the fact that AI recommendations are available for all cases. The methodology uses augmented inverse probability weighting estimators and is validated on a pretrial risk assessment study with 1,891 cases.

## Key Results
- PSA recommendations do not significantly improve judges' classification ability compared to human-alone decisions
- PSA-alone decisions tend to have higher misclassification rates and false positive proportions compared to human-alone or human-with-PSA decisions
- The misclassification rate difference between human-alone and human-with-PSA decisions is not statistically significant (0.01, 95% CI: [-0.01, 0.03])
- PSA-alone decisions show higher false positive proportions than human-alone decisions for non-white arrestees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Randomization of AI recommendations across cases allows identification of human decision differences with and without AI
- Mechanism: Under single-blinded and unconfounded treatment assignment, the distribution of baseline potential outcome Y(0) under negative decision D=0 is the same across treatment groups within strata
- Core assumption: AI recommendations affect outcomes only through human decisions (single-blinded assumption)
- Evidence anchors: [abstract] shows how to compare three alternative decision-making systems; [section 2.1] discusses single-blinded and unconfounded treatment assignment
- Break condition: If judges inform arrestees about AI recommendations, creating direct effects on behavior

### Mechanism 2
- Claim: Partial identification bounds can be derived for comparing AI-alone decisions with human decisions despite lacking an AI-alone treatment arm
- Mechanism: Bounds leverage the fact that AI recommendations are available for all cases and use observed data distribution of (Y, D, A) to bound classification risk differences
- Core assumption: AI recommendations can be computed for all cases with sufficient overlap in treatment assignment
- Evidence anchors: [abstract] derives partial identification bounds to compare classification risk; [section 3.2.1] addresses the selective labels problem
- Break condition: If AI recommendations cannot be computed for substantial portions of cases or treatment assignment has no overlap

### Mechanism 3
- Claim: Classification risk difference between human-alone and human-with-AI systems can be point identified using compound outcome approach
- Mechanism: Difference in classification risk expressed as difference in means of compound outcome Wi = Yi(1-Di) - ℓ01(1-Yi)(1-Di), estimated using doubly robust methods
- Core assumption: Decision and outcome models can be estimated at sufficient rates (Assumption 2)
- Evidence anchors: [section 3.1.1] shows identification of effect on false negative proportions; [section 3.1.2] discusses estimation via difference-in-means
- Break condition: If decision and outcome models cannot be estimated consistently or product-rate condition is violated

## Foundational Learning

- Concept: Potential outcomes framework and causal inference
  - Why needed here: Paper uses potential outcomes notation to formalize decision-maker ability and handle selective labels problem
  - Quick check question: What is the key difference between Y(0) and Y(1) in this context, and why is Y(0) the focus?

- Concept: Confusion matrix and classification metrics
  - Why needed here: Paper formalizes decision-maker ability as classification problem using standard confusion matrices based on potential outcomes
  - Quick check question: How does the paper define true negatives, false positives, false negatives, and true positives in context of bail decisions?

- Concept: Partial identification and sharp bounds
  - Why needed here: When point identification is not possible (e.g., for AI-alone vs human systems), paper derives sharp bounds on classification risk differences
  - Quick check question: What is the key insight that allows deriving sharp bounds for AI-alone decisions despite not having an AI-alone treatment arm?

## Architecture Onboarding

- Component map: RCT data with treatment assignment (Z), AI recommendations (A), human decisions (D), outcomes (Y), and covariates (X) -> Identification module (Theorems 1 and 3) -> Estimation module (AIPW estimators, influence functions) -> Policy learning module (empirical risk minimization) -> Evaluation module (hypothesis tests, confidence intervals)

- Critical path: 1) Load RCT data and compute propensity scores 2) Estimate decision and outcome models 3) Compute point-identified differences between human-alone and human-with-AI systems 4) Derive sharp bounds for AI-alone vs human comparisons 5) Estimate optimal decision rules 6) Generate confidence intervals and conduct hypothesis tests

- Design tradeoffs: Flexibility vs complexity (more complex policy classes allow better performance but are harder to estimate); Parametric vs non-parametric (parametric models are easier to estimate but may be misspecified; non-parametric models are more flexible but require more data); Single-blinded vs double-blinded (single-blinded designs are more practical but require stronger assumptions about judge behavior)

- Failure signatures: High variance in estimates (may indicate insufficient overlap or model misspecification); Bounds that are too wide (may indicate insufficient information to distinguish between decision-making systems); Policy recommendations that contradict domain knowledge (may indicate model specification issues or violations of assumptions)

- First 3 experiments: 1) Compare point-identified differences in misclassification rates between human-alone and human-with-PSA systems using RCT data 2) Derive sharp bounds for difference in misclassification rates between PSA-alone and human-alone systems 3) Estimate optimal policy for when to provide PSA recommendations under monotonicity constraint

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed methodological framework's bounds perform when the overlap assumption is violated, such as when the propensity score approaches zero or one for certain covariate strata?
- Basis in paper: [explicit] The paper discusses the overlap assumption in Assumption 1, stating that treatment probabilities must be bounded away from zero and one, i.e., e(x) ∈ [η, 1 − η]
- Why unresolved: The paper assumes overlap holds and does not explore consequences of its violation
- What evidence would resolve it: Simulation studies or empirical analyses examining sensitivity of bounds to varying degrees of overlap violation

### Open Question 2
- Question: How do the classification ability measures and bounds perform when applied to multi-class decision problems, where outcome or decision has more than two categories?
- Basis in paper: [inferred] The paper focuses on binary outcomes and decisions, using confusion matrices and related metrics in context of binary classification
- Why unresolved: The paper does not address extension to multi-class problems, which are common in real-world applications
- What evidence would resolve it: Theoretical extensions of framework to multi-class settings, along with empirical evaluations on datasets with multi-class outcomes

### Open Question 3
- Question: How does the proposed methodology perform in settings where the decision-maker's behavior is influenced by factors beyond the provided AI recommendations, such as time pressure or workload?
- Basis in paper: [explicit] The paper assumes single-blinded treatment assignment, meaning AI recommendation affects outcome only through human decision
- Why unresolved: The paper does not consider potential confounders that may affect both human decision and outcome, which could violate single-blindedness assumption
- What evidence would resolve it: Empirical studies or simulations examining impact of confounding variables on classification ability measures and bounds

## Limitations
- The single-blinded assumption may be violated if judges share recommendations with defendants, creating direct behavioral effects
- Partial identification bounds can be wide when overlap conditions are weak, limiting precision of comparisons
- The methodology requires AI recommendations to be computable for all cases, which may not hold for complex real-world decisions

## Confidence
- High confidence: Identification strategy for comparing human-alone vs human-with-AI systems (Theorem 1) relies on standard RCT assumptions
- Medium confidence: Partial identification bounds for AI-alone comparisons (Theorem 3) depend heavily on overlap conditions
- Low confidence: Policy learning results without additional validation, as optimal decision rules may be sensitive to model specification

## Next Checks
1. **Sensitivity analysis for the single-blinded assumption**: Test how violations of this assumption affect identification results by introducing different magnitudes of direct effects in simulation studies

2. **Overlap diagnostics**: Systematically examine propensity score distributions across strata to identify where partial identification bounds become too wide to be informative, and consider alternative study designs for those regions

3. **External validation on additional RCTs**: Apply the methodology to randomized experiments from other domains (medical diagnosis, loan approval, hiring decisions) to assess generalizability and identify domain-specific challenges