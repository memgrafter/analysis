---
ver: rpa2
title: I've got the "Answer"! Interpretation of LLMs Hidden States in Question Answering
arxiv_id: '2406.02060'
source_url: https://arxiv.org/abs/2406.02060
tags:
- answers
- 'true'
- 'false'
- similarity
- sequences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the interpretability of large language
  models (LLMs) in the context of knowledge-based question answering. The authors
  propose a hypothesis that correct and incorrect model behavior can be distinguished
  at the level of hidden states.
---

# I've got the "Answer"! Interpretation of LLMs Hidden States in Question Answering

## Quick Facts
- arXiv ID: 2406.02060
- Source URL: https://arxiv.org/abs/2406.02060
- Reference count: 18
- Primary result: True answers show significantly higher cosine similarity to other true answers than to false answers (0.824 vs 0.734 for LLaMA-2-7B, p-value < 0.001)

## Executive Summary
This paper investigates whether large language models' hidden states can distinguish correct from incorrect behavior in knowledge-based question answering. The authors analyze three quantized LLM models (LLaMA-2-7B-Chat, Mistral-7B, and Vicuna-7B) using the MuSeRC dataset, finding that true answers exhibit significantly higher cosine similarity to other true answers than to false answers. This supports the hypothesis that hidden state spaces can be partitioned into subspaces corresponding to correct and incorrect behavior. The analysis also identifies middle layers (9-16) as potentially "weak" layers that could be targeted for additional training to improve model performance.

## Method Summary
The authors compute cosine similarity between hidden states of true and false answers at each layer of three quantized LLM models on the MuSeRC dataset. They analyze three categories of similarities: true sequences to their own group, true sequences to false groups, and false sequences to their own group. Statistical significance is tested using t-tests, and layer-wise patterns are examined to identify "weak" layers showing the largest differences between similarities to own and other groups.

## Key Results
- True sequences show significantly higher cosine similarity to their own group (0.824) than to false sequences (0.734) for LLaMA-2-7B
- False sequences show significantly higher cosine similarity to their own group (0.779) than to true sequences (0.734) for LLaMA-2-7B
- Middle layers (9-16) exhibit the largest differences between similarities to own and other groups, suggesting they are "weak" layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hidden states can be partitioned into subspaces corresponding to correct and incorrect behavior
- Mechanism: Higher within-group cosine similarity than between-group similarity indicates distinct subspaces
- Core assumption: Hidden states encode sufficient information to distinguish correct from incorrect answers
- Evidence: Average cosine similarity of true sequences to own group is 0.824 vs 0.734 to false sequences (p < 0.001)
- Break condition: If model architecture fundamentally changes or task requires complex reasoning

### Mechanism 2
- Claim: Middle layers (9-16) are "weak" layers with poor task-specific representation
- Mechanism: Lower self-similarity and higher cross-similarity in these layers indicates reduced effectiveness
- Core assumption: Layers with lower self-similarity relative to cross-similarity are less effective
- Evidence: Minimum cosine similarity values found at layers 9-10 across models
- Break condition: If models are fine-tuned or retrained, weak layer identification may change

### Mechanism 3
- Claim: Additional training of weak layers can improve model performance
- Mechanism: Targeted retraining of specific layers can better maintain task-specific representations
- Core assumption: Retraining weak layers can improve behavior without full model retraining
- Evidence: Proposed application without empirical validation
- Break condition: If weak layers reflect task complexity rather than model deficiency

## Foundational Learning

- Cosine similarity and vector space representations
  - Why needed here: Core analysis relies on measuring similarity between hidden state vectors
  - Quick check question: Can you explain why cosine similarity is preferred over Euclidean distance for comparing hidden states in this context?

- Transformer architecture and layer-wise processing
  - Why needed here: Understanding information flow through layers is crucial for interpreting layer behavior
  - Quick check question: What is the role of residual connections in maintaining information flow across transformer layers?

- Statistical hypothesis testing (t-tests, p-values)
  - Why needed here: Results rely on statistical significance testing to confirm meaningful differences
  - Quick check question: When would you use Welch's t-test instead of a standard t-test in this context?

## Architecture Onboarding

- Component map: Input preprocessing -> Model inference -> Similarity computation -> Statistical analysis -> Layer analysis
- Critical path: 1) Load and preprocess MuSeRC dataset, 2) Generate hidden states through all layers, 3) Compute cosine similarities for three categories, 4) Average similarities and perform statistical tests, 5) Analyze layer-wise patterns to identify weak layers
- Design tradeoffs: Using quantized models vs full-precision models (faster but potentially less accurate), static analysis vs dynamic generation (easier control but may miss generation errors), manual vs automated dataset augmentation (more control but labor-intensive)
- Failure signatures: No significant difference between true-to-true and true-to-false similarities (hypothesis fails), uniform similarity patterns across all layers (no weak layers identified), high variance in similarity scores (dataset quality issues or model instability)
- First 3 experiments: 1) Verify cosine similarity distributions are normal before applying t-tests, 2) Test hypothesis with different dataset (SuperGLUE MultiRC) to check generalizability, 3) Apply same analysis to non-LLM architecture (RoBERTa) to test architecture dependence

## Open Questions the Paper Calls Out

- Does the partitioning of hidden states into correct and incorrect subspaces generalize to other generative tasks beyond question answering?
- How does the partitioning vary across different model architectures and sizes?
- What is the relationship between identified "weak" layers and model performance on downstream tasks?

## Limitations
- Analysis limited to single Russian-language dataset with small sample size (200 examples)
- Relies on static analysis of pre-computed answers rather than dynamic generation
- Proposed mechanism for improving performance through targeted retraining lacks empirical validation

## Confidence

- **High Confidence**: True answers show significantly higher cosine similarity to other true answers than to false answers (0.824 vs 0.734, p < 0.001)
- **Medium Confidence**: Identification of middle layers (9-16) as "weak" layers based on observed patterns
- **Low Confidence**: Proposed application of retraining identified "weak" layers to improve model performance

## Next Checks

1. Apply same analysis methodology to a larger, English-language question-answering dataset (e.g., SuperGLUE MultiRC) to verify generalizability across domains and languages.

2. Extend analysis to examine hidden states during actual answer generation rather than static pre-computed answers to determine if subspace partitioning holds for generation-time errors.

3. Apply cosine similarity analysis to non-LLM architectures (e.g., RoBERTa or T5) to test whether observed pattern is specific to transformer-based LLMs or represents a more general phenomenon.