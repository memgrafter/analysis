---
ver: rpa2
title: Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge
  Graphs, and Tensor Factorization
arxiv_id: '2410.02721'
source_url: https://arxiv.org/abs/2410.02721
tags:
- knowledge
- information
- documents
- query
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses limitations of large language models (LLMs)
  in domain-specific tasks, such as hallucinations, knowledge cutoffs, and lack of
  source attribution. To overcome these issues, the authors propose SMART-SLIC, a
  retrieval-augmented generation (RAG) framework that integrates knowledge graphs
  (KGs) and vector stores (VS) with domain-specific scientific corpora.
---

# Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization

## Quick Facts
- arXiv ID: 2410.02721
- Source URL: https://arxiv.org/abs/2410.02721
- Reference count: 40
- Primary result: 97% accuracy on domain-specific scientific questions, outperforming GPT-4 without RAG (20% accuracy)

## Executive Summary
This paper addresses limitations of large language models (LLMs) in domain-specific tasks, such as hallucinations, knowledge cutoffs, and lack of source attribution. To overcome these issues, the authors propose SMART-SLIC, a retrieval-augmented generation (RAG) framework that integrates knowledge graphs (KGs) and vector stores (VS) with domain-specific scientific corpora. The framework leverages nonnegative tensor factorization with automatic model selection for topic modeling and document clustering, enabling the construction of highly specialized KGs and VSs without relying on LLMs. The system is paired with chain-of-thought prompting agents to enhance reasoning and answer complex scientific questions. Tested on a corpus of malware analysis and anomaly detection publications, SMART-SLIC achieved 97% accuracy in answering research queries, significantly outperforming GPT-4 without RAG (20% accuracy). The framework also excelled in retrieving relevant DOI citations, demonstrating its effectiveness in providing accurate, attributed, and domain-specific responses. SMART-SLIC is generalizable and adaptable to various specialized domains.

## Method Summary
The framework constructs a domain-specific corpus using text mining and information retrieval, starting with SME-selected core documents and expanding via citation networks using APIs like Scopus and Semantic Scholar. Nonnegative tensor factorization with automatic model selection (via Binary Bleed method) extracts 25 topic clusters from the corpus. A Neo4j knowledge graph is built from document metadata and latent features, while a Milvus vector store contains document embeddings. A RAG pipeline uses chain-of-thought LLM agents to route queries via NER extraction to either KG-based or VS-based retrieval, with answers generated using the retrieved context and proper citations.

## Key Results
- Achieved 97% accuracy on 200 domain-specific research questions in malware analysis and anomaly detection
- Significantly outperformed GPT-4 without RAG (20% accuracy on attempted questions)
- Successfully retrieved relevant DOI citations for 100% of answered questions
- Demonstrated effective integration of structured KG and unstructured VS for hybrid retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific KGs and VSs constructed without LLMs prevent hallucination propagation and ensure factual grounding.
- Mechanism: The framework builds knowledge graphs and vector stores using NLP, data mining, and nonnegative tensor factorization with automatic model selection, rather than relying on LLM-generated content. This ensures that the underlying data is curated from structured scientific metadata and decomposed document clusters, avoiding hallucinated relations.
- Core assumption: LLM-free construction of ontologies yields more reliable factual bases than LLM-generated triples.
- Evidence anchors:
  - [abstract] "Importantly, to avoid hallucinations in the KG, we build these highly domain-specific KGs and VSs without the use of LLMs, but via NLP, data mining, and nonnegative tensor factorization with automatic model selection."
  - [section] "Our KG incorporates document metadata as well as the latent features... Each document node contains information such as DOI, title, abstract, and source API document identifiers."
  - [corpus] Weak—no direct quantitative evidence of hallucination reduction provided; inference based on design choice.
- Break condition: If NLP-based entity extraction introduces noisy triples or the tensor factorization yields unstable topic clusters, the factual quality degrades.

### Mechanism 2
- Claim: Combining structured KG triples with unstructured VS embeddings enables both precise entity reasoning and rich semantic retrieval.
- Mechanism: The KG provides relational navigation between documents, authors, topics, and affiliations, while the VS stores paragraph-level embeddings for deep semantic search. Queries can switch between these stores depending on whether the answer requires entity paths or contextual text.
- Core assumption: Different query types benefit from distinct data modalities (structured vs. unstructured).
- Evidence anchors:
  - [abstract] "Pairing our RAG with a domain-specific: (i) KG (containing structured information), and (ii) VS (containing unstructured information)"
  - [section] "If the retrieved text contains the needed information, the LLM can answer the posed question and include a citation of the document... If further related information is needed, the application can use document metadata... to expand its search through the KG."
  - [corpus] Weak—no ablation study isolating KG vs VS contributions.
- Break condition: If vector embeddings poorly capture semantics or KG nodes are sparse, hybrid retrieval loses effectiveness.

### Mechanism 3
- Claim: Automatic model selection in tensor factorization yields optimal topic clusters, improving downstream KG quality and query precision.
- Mechanism: The framework iterates over k values, computing silhouette scores and applying binary bleed search to identify the number of topics that best separates document clusters. These clusters define KG topics and VS document groupings.
- Core assumption: Higher silhouette scores correlate with better topic coherence and retrieval relevance.
- Evidence anchors:
  - [abstract] "nonnegative tensor factorization with automatic model selection for topic modeling and document clustering"
  - [section] "Our analysis determined that 25 topic-clusters represented the optimal division... automatic model determination is used where the final cluster counts are determined by achieving the highest silhouette scores above a predetermined threshold using the Binary Bleed method."
  - [corpus] Moderate—claims 25 clusters optimal, but no quantitative topic coherence scores shown.
- Break condition: If silhouette-based selection overfits to dataset noise, clusters may fragment, reducing retrieval accuracy.

## Foundational Learning

- Concept: Nonnegative tensor factorization (NTF) with automatic model selection
  - Why needed here: Decomposes the TF-IDF matrix into interpretable topic-document and topic-word factors without negative values, enabling pure additive semantic representations for KG construction.
  - Quick check question: What is the role of the silhouette score in NTF-based clustering?

- Concept: Retrieval-Augmented Generation (RAG) with hybrid stores
  - Why needed here: Augments LLM context with domain-specific evidence, mitigating hallucinations by grounding responses in curated KG triples and VS embeddings.
  - Quick check question: How does the ReAct agent decide between querying KG vs VS?

- Concept: Chain-of-thought prompting with LLM agents
  - Why needed here: Breaks complex scientific questions into reasoning and action steps, allowing iterative tool use (e.g., multiple VS searches) before generating final answers.
  - Quick check question: What distinguishes a "General Query" from a "Specific Document Query" in the routing pipeline?

## Architecture Onboarding

- Component map: Corpus builder -> NTF-based topic clusterer -> KG constructor (Neo4j) -> VS assembler (Milvus) -> ReAct agent -> RAG orchestrator
- Critical path: Document ingestion -> TF-IDF matrix -> NTF decomposition -> Silhouette-based k selection -> KG & VS population -> Query routing -> LLM answer generation
- Design tradeoffs: KG offers precise entity relations but is sparse; VS offers rich semantics but lacks explicit structure. Combining both increases coverage but doubles storage/computation cost.
- Failure signatures: (a) KG node degree drops -> missing relational paths; (b) VS similarity scores saturate -> poor retrieval relevance; (c) NTF yields >25 topics -> overfragmented clusters.
- First 3 experiments:
  1. Load 100 sample documents, run NTF with k=10, inspect silhouette score and topic labels.
  2. Query the KG for a known entity triple (e.g., author->document) to verify edge existence.
  3. Perform a VS similarity search for a query phrase and check retrieved paragraph IDs match expected content.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SMART-SLIC's RAG framework compare to other state-of-the-art RAG systems in terms of accuracy and efficiency across different domains?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of SMART-SLIC's RAG framework in the domain of malware analysis and anomaly detection, achieving 97% accuracy in answering research queries. However, the paper does not compare its performance to other RAG systems or explore its performance in other domains.
- Why unresolved: The paper focuses on a specific domain and does not provide a comprehensive comparison with other RAG systems or explore its performance in other domains.
- What evidence would resolve it: Conducting experiments to compare SMART-SLIC's RAG framework with other state-of-the-art RAG systems across various domains and measuring their accuracy and efficiency would provide evidence to answer this question.

### Open Question 2
- Question: What are the limitations of the current SMART-SLIC framework in handling complex queries that require multi-hop reasoning or integration of information from multiple sources?
- Basis in paper: [explicit] The paper mentions that the SMART-SLIC framework uses chain-of-thought prompting agents to enhance reasoning capabilities. However, it does not explicitly discuss the limitations of the framework in handling complex queries.
- Why unresolved: The paper does not provide a detailed analysis of the framework's limitations in handling complex queries.
- What evidence would resolve it: Conducting experiments to evaluate the performance of SMART-SLIC in handling complex queries that require multi-hop reasoning or integration of information from multiple sources would provide evidence to answer this question.

### Open Question 3
- Question: How does the performance of SMART-SLIC's RAG framework scale with the size of the knowledge base and the number of documents in the vector store?
- Basis in paper: [inferred] The paper mentions that the SMART-SLIC framework integrates a knowledge graph and a vector store to enhance the performance of the LLM. However, it does not discuss the scalability of the framework with respect to the size of the knowledge base and the number of documents in the vector store.
- Why unresolved: The paper does not provide any information on the scalability of the framework.
- What evidence would resolve it: Conducting experiments to measure the performance of SMART-SLIC's RAG framework with varying sizes of knowledge bases and vector stores would provide evidence to answer this question.

## Limitations
- Dataset Specificity: Evaluation limited to malware analysis and anomaly detection corpus with 22% full-text availability, raising generalizability concerns to other domains.
- Quantitative Evidence Gaps: Lack of ablation studies isolating KG vs VS contributions and no direct measurements of hallucination reduction rates.
- Reproducibility Constraints: Missing critical implementation details including T-ELF parameter thresholds and ReAct agent configuration.

## Confidence
- High Confidence: Framework architecture (KG + VS + RAG pipeline) is logically sound and addresses well-documented LLM limitations.
- Medium Confidence: 97% accuracy claim supported but methodology lacks detail and ablation studies to verify improvement magnitude.
- Low Confidence: Hallucination reduction claims inferred from design rather than directly measured; no quantitative hallucination rate data provided.

## Next Checks
1. **Ablation Study**: Implement and test the framework with only KG, only VS, and the hybrid approach on the same corpus to quantify the relative contributions of each component to overall accuracy.
2. **Cross-Domain Evaluation**: Apply the framework to a different domain (e.g., biomedical literature or legal documents) with similar corpus size and citation structure to assess generalizability and identify domain-specific failure modes.
3. **Topic Coherence Analysis**: Compute and report quantitative topic coherence scores (e.g., NPMI, UCI) for the 25 clusters identified by NTF to validate that silhouette-based selection yields semantically meaningful topics.