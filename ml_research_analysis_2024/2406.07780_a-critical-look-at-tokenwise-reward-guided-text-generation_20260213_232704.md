---
ver: rpa2
title: A Critical Look At Tokenwise Reward-Guided Text Generation
arxiv_id: '2406.07780'
source_url: https://arxiv.org/abs/2406.07780
tags:
- reward
- rlhf
- partial
- sequences
- pargs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies a critical issue with tokenwise reward-guided
  text generation (RGTG) methods: reward models trained on full sequences cannot reliably
  score partial sequences during decoding, leading to suboptimal text generation.
  To address this, the authors propose explicitly training a Bradley-Terry reward
  model on partial sequences and sampling from the induced per-token policy during
  decoding.'
---

# A Critical Look At Tokenwise Reward-Guided Text Generation

## Quick Facts
- arXiv ID: 2406.07780
- Source URL: https://arxiv.org/abs/2406.07780
- Authors: Ahmad Rashid; Ruotian Wu; Julia Grosse; Agustinus Kristiadi; Pascal Poupart
- Reference count: 35
- Primary result: PARGS outperforms recent RGTG methods like ARGS and controlled decoding on summarization, dialogue generation, and fine-grained text generation tasks

## Executive Summary
This paper identifies a critical flaw in tokenwise reward-guided text generation (RGTG) methods: reward models trained on full sequences cannot reliably score partial sequences during decoding, leading to suboptimal text generation. The authors propose PARGS, which explicitly trains a Bradley-Terry reward model on partial sequences and samples from the induced per-token policy during decoding. This approach avoids the pitfalls of using full-sequence reward models for partial sequence scoring while achieving performance comparable to more expensive offline RLHF baselines like PPO and DPO without requiring LLM fine-tuning.

## Method Summary
PARGS addresses the incompatibility between full-sequence reward models and partial-sequence scoring by training a separate Bradley-Terry reward model on partial sequences explicitly. The method creates prefix data by truncating winning and losing sequences at various lengths, then trains the reward model using the BT loss on these partial sequences. During decoding, instead of using a full-sequence reward model, PARGS samples tokens from the weighted sum of logits and partial sequence rewards. Theoretically, the induced tokenwise policy is shown to be a ratio of two distinct RLHF policies over sequences of different lengths, making the approach both tractable and effective.

## Key Results
- PARGS outperforms recent RGTG methods like ARGS and controlled decoding on summarization, dialogue generation, and fine-grained text generation tasks
- Achieves performance comparable to strong offline RLHF baselines (PPO, DPO) without large-scale LLM fine-tuning
- Simple decoding approach using partial-sequence rewards provides significant gains over full-sequence reward models during tokenwise generation

## Why This Works (Mechanism)

### Mechanism 1
Training reward models on partial sequences fixes the incompatibility between full-sequence reward models and partial-sequence scoring during decoding. By optimizing the Bradley-Terry loss on partial sequences, the reward model learns to assign scores that reflect the true underlying preference distribution for all prefixes, avoiding arbitrary values. The core assumption is that partial sequences inherit the winning/losing label from their corresponding full sequences, and the reward model is sufficiently expressive.

### Mechanism 2
The tokenwise policy induced by the partial-sequence reward model is a ratio of two distinct RLHF policies. The induced policy π(yi|x,y 1:i−1 ) is proportional to the ratio πRLHF,i(y1:i|x) / πRLHF,i−1(y1:i−1 |x), where πRLHF,i and πRLHF,i−1 are policies over sequences of different lengths. This formulation makes the approach tractable while capturing the essential preference information.

### Mechanism 3
Sampling from the induced tokenwise policy during decoding produces higher-reward sequences than using full-sequence reward models. By sampling from π(yi|x,y 1:i−1 ) = 1/Z(x,y 1:i−1 ) πref(yi|x,y 1:i−1 )exp(βr ϕ(y1:i|x)), the decoding process generates sequences that better align with human preferences. The core assumption is that the partial-sequence reward model accurately reflects human preferences and the sampling temperature β is appropriately tuned.

## Foundational Learning

- Concept: Bradley-Terry (BT) model for pairwise comparisons
  - Why needed here: The BT model is used to train the reward model on preference data, which is essential for aligning the language model with human preferences
  - Quick check question: What is the BT loss function for training a reward model on full sequences?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is the standard approach for aligning LLMs with human preferences, and understanding its policy formulation is crucial for analyzing the induced tokenwise policy
  - Quick check question: How does the RLHF policy πRLHF(y|x) relate to the reference policy πref(y|x) and the reward model rϕ(y|x)?

- Concept: Autoregressive text generation
  - Why needed here: The decoding process in PARGS is autoregressive, and understanding how partial-sequence rewards affect token sampling is essential
  - Quick check question: What is the conditional distribution π(yi|x,y 1:i−1 ) used for sampling the next token in autoregressive generation?

## Architecture Onboarding

- Component map: Reference LLM → Partial-sequence reward model → Decoding algorithm → Generated text
- Critical path: Prompt → Reference LLM → Candidate token scoring (πref + rϕ) → Token sampling → Next token → Repeat until sequence complete
- Design tradeoffs:
  - Training partial-sequence reward models vs. using full-sequence models: More accurate but requires more data and computation
  - Sampling temperature β: Higher values lead to more reward-guided but potentially less diverse generations
  - Top-k candidate selection: Balances computational efficiency with exploration of the token space
- Failure signatures:
  - Degenerate sampling: If rϕ is poorly trained or β is too high, the sampling distribution may collapse to a few tokens
  - Poor alignment: If the partial-sequence reward model doesn't accurately reflect human preferences, the generated text may not align with expectations
  - Computational overhead: The reward model inference at each decoding step adds latency compared to standard autoregressive generation
- First 3 experiments:
  1. Train a partial-sequence reward model on a small subset of the preference data and evaluate its ability to rank partial sequences
  2. Compare the tokenwise policy induced by the partial-sequence reward model to the RLHF policy on a validation set
  3. Evaluate the quality of generated text using the partial-sequence reward model vs. a full-sequence model on a held-out test set

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of PARGS scale with the size of the partial sequence dataset used for training the reward model? The paper only provides results for the TL;DR dataset, and it's unclear if the same scaling behavior applies to other tasks like dialogue or fine-grained text generation.

### Open Question 2
What is the theoretical relationship between the ratio of RLHF policies induced by PARGS and the optimal policy for tokenwise reward-guided text generation? The paper shows this is a necessary tradeoff for tractability but doesn't prove whether this ratio yields the best possible tokenwise policy.

### Open Question 3
How does PARGS compare to Best-of-N in terms of computational efficiency when generating multiple responses? The paper mentions Best-of-N is computationally intensive but doesn't provide direct comparison when generating multiple responses per prompt.

### Open Question 4
How robust is PARGS to the choice of β hyperparameter across different tasks and base LLMs? The sensitivity analysis is limited to one task and base model, and it's unclear if the optimal β transfers to other settings.

### Open Question 5
Can the partial sequence reward model trained for PARGS be effectively used for controlled text generation beyond preference alignment? The paper focuses on preference alignment but doesn't explore PARGS's potential for other controlled generation tasks like style transfer or attribute control.

## Limitations

- Reward model generalization may not hold for longer prefixes where outcome-determining context isn't present
- Computational overhead from reward model inference at each decoding step adds significant latency
- Evaluation relies heavily on a single reward model for average reward measurement, potentially introducing bias

## Confidence

**High confidence**: The identification of fundamental incompatibility between full-sequence reward models and partial-sequence scoring is well-supported theoretically and empirically.

**Medium confidence**: The theoretical formulation as a ratio of RLHF policies is mathematically sound, but practical implications need further exploration. Empirical results show consistent improvements but with varying magnitudes.

**Low confidence**: Claims about performance similarity to offline RLHF baselines are supported by limited experimental comparisons and may not generalize across domains and larger models.

## Next Checks

1. **Reward model generalization study**: Evaluate partial-sequence reward models trained on different prefix lengths and tested on held-out prefix lengths. Measure correlation between partial and full-sequence rewards across various sequence lengths.

2. **Computational efficiency analysis**: Profile PARGS decoding speed compared to standard autoregressive generation and other RGTG methods. Analyze relationship between sequence length, number of candidates, and total inference time.

3. **Robustness to reward model variations**: Test PARGS with different reward model architectures and datasets. Evaluate stability of performance improvements across these variations.