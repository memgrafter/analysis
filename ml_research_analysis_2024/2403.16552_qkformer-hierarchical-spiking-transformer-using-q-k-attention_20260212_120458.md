---
ver: rpa2
title: 'QKFormer: Hierarchical Spiking Transformer using Q-K Attention'
arxiv_id: '2403.16552'
source_url: https://arxiv.org/abs/2403.16552
tags:
- qkformer
- attention
- spiking
- which
- qkta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QKFormer introduces a hierarchical spiking transformer architecture
  with a novel Q-K attention mechanism to address the high computational complexity
  of existing spiking transformers. The Q-K attention uses binary spike-form vectors
  for token/channel importance modeling, achieving linear complexity and high energy
  efficiency compared to quadratic complexity in prior approaches.
---

# QKFormer: Hierarchical Spiking Transformer using Q-K Attention

## Quick Facts
- **arXiv ID**: 2403.16552
- **Source URL**: https://arxiv.org/abs/2403.16552
- **Reference count**: 40
- **Primary result**: Achieves 85.65% top-1 accuracy on ImageNet-1K with 64.96M parameters and 4 time steps, surpassing Spikformer by 10.84%

## Executive Summary
QKFormer introduces a hierarchical spiking transformer architecture with a novel Q-K attention mechanism designed to address the high computational complexity of existing spiking transformers. The key innovation is Q-K attention, which uses binary spike-form vectors for token/channel importance modeling, achieving linear complexity compared to the quadratic complexity of traditional self-attention. The hierarchical structure progressively reduces token numbers across three stages, enabling multi-scale spiking feature representation. A Spiking Patch Embedding with Deformed Shortcut (SPEDS) module enhances spiking information transmission. The resulting model achieves 85.65% top-1 accuracy on ImageNet-1K with 64.96M parameters and 4 time steps, surpassing the previous state-of-the-art spiking transformer by 10.84% while maintaining superior energy efficiency.

## Method Summary
QKFormer employs direct training of spiking neural networks using surrogate gradients with a sigmoid function. The architecture features a hierarchical design with three stages, where Q-K Token Attention (QKTA) is used in the first two stages and Spiking Self Attention (SSA) in the third stage. The model uses Spiking Patch Embedding with Deformed Shortcut (SPEDS) modules for efficient information transmission. Training is performed using AdamW optimizer with learning rate 6×10⁻⁴, batch size 512, and 200 epochs for ImageNet-1K. The model processes inputs through 4 time steps, with data augmentation including RandAugment, random erasing, and stochastic depth.

## Key Results
- Achieves 85.65% top-1 accuracy on ImageNet-1K with 64.96M parameters and 4 time steps
- Outperforms previous SOTA spiking transformer (Spikformer) by 10.84% absolute accuracy
- Demonstrates superior energy efficiency compared to prior approaches
- Shows consistent performance improvements across CIFAR10/100, CIFAR10-DVS, and DVS128 Gesture datasets

## Why This Works (Mechanism)

### Mechanism 1: Q-K Attention Linear Complexity
QKFormer replaces quadratic self-attention with Q-K attention that uses binary spike vectors to compute importance scores. The mechanism sums spike-form Query vectors row-wise or column-wise to produce binary attention vectors, then applies these as masks to the Key matrix using Hadamard product. This eliminates full matrix multiplication while maintaining effectiveness.

Core assumption: Binary spike vectors can effectively capture token/channel importance while maintaining computational efficiency.

### Mechanism 2: Hierarchical Multi-Scale Representation
The architecture progressively reduces token numbers across three stages, starting with small patches and gradually merging neighboring patches in deeper layers. This creates hierarchical representations at different scales while increasing channel dimensions.

Core assumption: Multi-scale feature representation improves performance in vision tasks.

### Mechanism 3: SPEDS Module for Information Transmission
SPEDS implements residual shortcuts with lightweight linear projections that match channel and token numbers across downsampling blocks. This facilitates identity mapping and reduces information loss in spike communication.

Core assumption: Residual connections are effective in spiking neural networks for information transmission.

## Foundational Learning

- **Spiking Neural Networks (SNNs)**: Why needed - QKFormer is a spiking transformer, so understanding SNNs is fundamental to grasping how the architecture processes information differently from traditional ANNs. Quick check - What distinguishes Spiking Neural Networks from traditional Artificial Neural Networks in terms of information encoding?

- **Attention Mechanisms in Transformers**: Why needed - The Q-K attention mechanism is a novel adaptation of self-attention, so understanding how attention works in standard transformers is essential. Quick check - How does the computational complexity of standard self-attention scale with the number of tokens?

- **Binary Neural Networks**: Why needed - Q-K attention operates on binary spike vectors, so understanding binary neural network principles helps explain the efficiency gains. Quick check - What are the primary computational advantages of using binary values instead of floating-point values in neural networks?

## Architecture Onboarding

- **Component map**: Input → SPEDS-1 → QKFormer Blocks (Stage 1) → SPEDS-2 → QKFormer Blocks (Stage 2) → SPEDS-3 → Spikformer Blocks (Stage 3) → Classifier

- **Critical path**: Input → SPEDS-1 → QKFormer Blocks (Stage 1) → SPEDS-2 → QKFormer Blocks (Stage 2) → SPEDS-3 → Spikformer Blocks (Stage 3) → Classifier

- **Design tradeoffs**:
  - Q-K attention vs. SSA: Q-K attention offers linear complexity but may lose some expressiveness compared to quadratic SSA
  - Mixed attention integration: Using QKTA in early stages and SSA in later stages balances efficiency and performance
  - Hierarchical vs. flat architecture: Hierarchical structure enables multi-scale features but adds complexity

- **Failure signatures**:
  - Vanishing gradients in deep stages
  - Information loss during downsampling
  - Poor attention vector quality leading to ineffective masking
  - Excessive firing rates causing energy inefficiency

- **First 3 experiments**:
  1. Replace Q-K attention with standard SSA in QKFormer and compare accuracy and memory usage
  2. Test different attention mixing strategies (e.g., QKCA in stage 2 vs SSA in stage 2)
  3. Evaluate performance with and without SPEDS module to quantify its contribution

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How would QKFormer's performance scale with even larger hierarchical architectures (e.g., more stages or blocks per stage)?
- **Basis in paper**: The paper demonstrates QKFormer's effectiveness with a 3-stage hierarchical structure, but doesn't explore scaling beyond this configuration.
- **Why unresolved**: The paper only tests QKFormer with 3 stages and specific block configurations. The authors acknowledge exploring larger architectures is a future direction but don't provide empirical evidence.
- **What evidence would resolve it**: Systematic experiments varying the number of stages and blocks per stage, measuring performance and computational efficiency trade-offs.

### Open Question 2
- **Question**: What is the theoretical limit of Q-K attention's energy efficiency compared to other spiking attention mechanisms?
- **Basis in paper**: The paper claims Q-K attention is more energy-efficient than SSA due to linear complexity and fewer operations, but doesn't provide a theoretical analysis comparing all spiking attention variants.
- **Why unresolved**: While the paper provides empirical energy comparisons and theoretical complexity analysis, it doesn't derive a comprehensive theoretical framework for comparing energy efficiency across all spiking attention mechanisms.
- **What evidence would resolve it**: A unified theoretical framework quantifying energy consumption of different spiking attention mechanisms under various conditions (token numbers, channel sizes, firing rates).

### Open Question 3
- **Question**: How does QKFormer's performance degrade when using fewer time steps than the reported 4 steps?
- **Basis in paper**: The paper shows QKFormer achieves 85.65% accuracy with 4 time steps but doesn't systematically study performance degradation with fewer steps.
- **Why unresolved**: The ablation study only shows results for T=1, 2, 4, 6, missing intermediate values that would reveal the relationship between time steps and performance.
- **What evidence would resolve it**: Detailed performance measurements across a continuous range of time steps (e.g., 1-10) to establish the time-step vs. accuracy curve and identify the point of diminishing returns.

## Limitations
- Lack of direct evidence in the corpus for the novel Q-K attention mechanism's linear complexity claim
- Missing implementation details for Q-K attention and SPEDS modules in the paper
- Limited exploration of architectural scaling beyond the 3-stage configuration

## Confidence
- **High Confidence**: The empirical results showing QKFormer achieving 85.65% top-1 accuracy on ImageNet-1K, surpassing previous spiking transformers by 10.84%, are well-supported by the presented experiments and ablation studies.
- **Medium Confidence**: The claims about Q-K attention's linear complexity and the hierarchical architecture's contribution to performance are theoretically sound but lack direct corpus evidence for their specific implementations.
- **Low Confidence**: The exact implementation details of Q-K attention and SPEDS modules are not fully specified in the paper, making independent reproduction challenging without access to the code.

## Next Checks
1. **Reimplement Q-K Attention**: Implement the Q-K attention mechanism independently and verify its linear complexity through computational analysis and runtime measurements on synthetic data with varying token counts.

2. **Architectural Ablation**: Train QKFormer variants with different attention mixing strategies (e.g., QKTA in all stages, SSA in all stages, or different block configurations) to quantify the contribution of the hierarchical architecture to overall performance.

3. **SPEDS Module Isolation**: Create a baseline spiking transformer without SPEDS modules and compare its performance to QKFormer to isolate the contribution of the SPEDS architecture to information transmission and final accuracy.