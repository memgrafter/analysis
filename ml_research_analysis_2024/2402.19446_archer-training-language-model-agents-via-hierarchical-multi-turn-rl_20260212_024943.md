---
ver: rpa2
title: 'ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL'
arxiv_id: '2402.19446'
source_url: https://arxiv.org/abs/2402.19446
tags:
- archer
- language
- policy
- token-level
- multi-turn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ArCHer introduces a hierarchical reinforcement learning framework
  for training language models as goal-directed agents in multi-turn tasks. It splits
  learning into a high-level utterance-level critic trained with off-policy temporal
  difference learning and a low-level token-level policy trained with policy gradients,
  treating the critic as the reward.
---

# ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL

## Quick Facts
- arXiv ID: 2402.19446
- Source URL: https://arxiv.org/abs/2402.19446
- Reference count: 40
- Primary result: 100× sample efficiency over on-policy methods, scaling benefits up to 7B parameters

## Executive Summary
ArCHer introduces a hierarchical reinforcement learning framework for training language models as goal-directed agents in multi-turn tasks. It splits learning into a high-level utterance-level critic trained with off-policy temporal difference learning and a low-level token-level policy trained with policy gradients, treating the critic as the reward. This design avoids long-horizon credit assignment and large action spaces, enabling 100× better sample efficiency than on-policy methods and improving with larger model capacity up to 7B parameters. ArCHer also supports both online and offline RL variants.

## Method Summary
ArCHer trains language models as goal-directed agents through a two-level hierarchical RL architecture. The high-level component learns an utterance-level Q-function using off-policy TD learning from a replay buffer, while the low-level component generates tokens within each utterance using policy gradients with advantages derived from the critic. This decomposition allows efficient credit assignment at utterance boundaries while maintaining fine-grained control through token-level generation. The framework supports both online and offline RL variants, with offline versions using IQL for the critic and AWR for the actor.

## Key Results
- Achieves 100× sample efficiency compared to PPO baselines
- Improves with larger model capacity up to 7B parameters
- Successfully trains agents on multi-turn tasks like Detective Game, Twenty Questions, Guess My City, and WebShop

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical decomposition into utterance-level critic and token-level actor avoids long-horizon credit assignment and large action spaces simultaneously.
- **Mechanism:** The high-level critic operates at utterance granularity, using off-policy TD learning to aggregate delayed rewards, while the low-level actor generates tokens within each utterance via policy gradients. This structure limits credit assignment to utterance boundaries and keeps token actions compact.
- **Core assumption:** The value of a full utterance can be well approximated by the utterance-level critic, and token-level actions are conditionally independent given the high-level state.
- **Evidence anchors:** [abstract] "splits learning into a high-level utterance-level critic trained with off-policy temporal difference learning and a low-level token-level policy trained with policy gradients" - [section 3.3] "Unlike on-policy methods, this allows for sample reuse and faster convergence, while avoiding Bellman backups over individual tokens"
- **Break condition:** If token sequences within an utterance are highly interdependent or if utterance-level reward signals are too sparse, the decomposition may fail to capture essential dynamics.

### Mechanism 2
- **Claim:** Off-policy TD learning at the utterance level yields 100× sample efficiency over on-policy PPO.
- **Mechanism:** The utterance-level critic is trained on all past interactions stored in a replay buffer, enabling reuse of experience and stabilizing learning without requiring new environment rollouts for each update.
- **Core assumption:** Past interactions remain relevant for learning the value function, and the replay buffer can approximate the current policy's data distribution sufficiently.
- **Evidence anchors:** [abstract] "attaining a sample efficiency of about 100x over existing methods" - [section 5.3] "PPO is an on-policy method, and therefore, cannot effectively reuse samples from past iterations"
- **Break condition:** If the environment or task changes rapidly, old replay data may become irrelevant, reducing the effectiveness of off-policy learning.

### Mechanism 3
- **Claim:** Scaling the actor base model from 100M to 7B parameters improves learning speed and final performance.
- **Mechanism:** Larger models have greater representational capacity to capture complex task strategies and produce more diverse, coherent utterances, leading to richer gradient signals and faster convergence.
- **Core assumption:** Model capacity directly translates to improved decision-making ability in the hierarchical RL setting, beyond what is learned in pre-training.
- **Evidence anchors:** [abstract] "improving with larger model capacity up to 7B parameters" - [section 5.6] "Observe in Figure 5 (d), that ArCHer with this Mistral7B actor learns to solve the task much faster than ArCHer with a GPT2 Actor"
- **Break condition:** If the task is too simple or the pre-trained model already saturates performance, further scaling may yield diminishing returns.

## Foundational Learning

- **Concept:** Hierarchical Reinforcement Learning
  - **Why needed here:** Multi-turn language tasks involve both coarse-grained decision-making (which utterance to produce) and fine-grained token generation, requiring different optimization strategies.
  - **Quick check question:** In ArCHer, which level handles credit assignment over multiple turns, and which handles fine-grained token prediction?

- **Concept:** Off-policy Temporal Difference Learning
  - **Why needed here:** Allows reuse of past experience to train the utterance-level critic efficiently without costly online interaction.
  - **Quick check question:** What is the main advantage of using off-policy TD learning for the high-level critic compared to on-policy policy gradients?

- **Concept:** Policy Gradient with Advantage Estimation
  - **Why needed here:** The token-level actor needs a reward signal; the advantage derived from the high-level critic provides a low-variance estimate of how good each token is.
  - **Quick check question:** How does ArCHer provide a reward signal to the token-level actor during training?

## Architecture Onboarding

- **Component map:** Collect trajectory -> Store in replay buffer -> Update utterance critic (TD) -> Compute advantages -> Update token actor (policy gradient)
- **Critical path:** Collect trajectory → Store in replay buffer → Update utterance critic (TD) → Compute advantages → Update token actor (policy gradient)
- **Design tradeoffs:**
  - Off-policy critic gives sample efficiency but may suffer from distributional shift; on-policy actor mitigates this.
  - Coarser utterance actions reduce complexity but may miss fine-grained control; token-level actor restores granularity.
  - Adding token-level baseline reduces variance but increases computational overhead.
- **Failure signatures:**
  - Critic estimates diverge → Unstable or poor actor updates.
  - Actor collapses to repetitive tokens → Baseline or advantage estimation issues.
  - Learning stalls after initial phase → Replay buffer size or update ratio problems.
- **First 3 experiments:**
  1. Verify basic end-to-end training on a simple multi-turn task (e.g., Twenty Questions Subset) with both critic and actor updates.
  2. Compare sample efficiency against PPO baseline on the same task with identical model architecture.
  3. Test impact of replay buffer size on training stability and performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ArCHer's hierarchical approach maintain its sample efficiency advantage when scaled to models larger than 7B parameters?
- Basis in paper: [explicit] The paper notes that ArCHer benefits from scaling up to 7B parameters, but does not test beyond this limit.
- Why unresolved: The experiments were constrained to 7B parameters due to computational limitations, leaving open whether the efficiency gains persist at larger scales.
- What evidence would resolve it: Empirical results showing performance and sample efficiency of ArCHer with models larger than 7B parameters, particularly comparing against baseline methods at these scales.

### Open Question 2
- Question: How does ArCHer perform in settings with limited offline data versus online data collection?
- Basis in paper: [explicit] The paper mentions that ArCHer can learn from offline datasets and compares its performance in the offline setting using IQL and AWR.
- Why unresolved: While the paper shows ArCHer can work in offline settings, it doesn't fully explore the trade-offs or performance differences between offline and online data collection scenarios.
- What evidence would resolve it: Comparative studies showing ArCHer's performance and sample efficiency in both abundant and limited offline data settings, alongside online data collection.

### Open Question 3
- Question: What are the theoretical implications of using different discount factors for the high-level and low-level policies in ArCHer?
- Basis in paper: [inferred] The paper discusses the hierarchical MDP formulation and mentions equivalent token-level and utterance-level MDPs, but does not deeply analyze the theoretical impact of different discount factors.
- Why unresolved: The paper introduces the hierarchical MDP and its theoretical benefits but does not provide a detailed analysis of how different discount factors affect convergence and performance.
- What evidence would resolve it: A formal theoretical analysis or empirical study showing the impact of varying discount factors on the convergence and performance of ArCHer's hierarchical policies.

## Limitations

- The framework assumes a fixed mapping between token-level actions and high-level rewards, which may not hold for tasks requiring long-term strategic planning across many utterances.
- The paper doesn't address potential distributional shift between the off-policy critic training data and the on-policy actor's current policy.
- No analysis of how sensitive results are to the choice of discount factor or the relative weighting between actor and critic updates.

## Confidence

- Mechanism 1 (hierarchical decomposition benefits): Low confidence - No ablation studies isolating the impact of separating utterance-level credit assignment from token-level generation.
- Mechanism 2 (100× sample efficiency): Medium confidence - Based on internal experimental comparisons against PPO, lacking external validation or statistical significance testing.
- Mechanism 3 (scaling benefits): Medium confidence - Clear performance differences shown, but analysis doesn't control for confounding factors like pre-training data differences.

## Next Checks

1. **Ablation study on hierarchical decomposition**: Train an end-to-end policy gradient method on the same tasks and compare both sample efficiency and final performance against ArCHer to isolate whether the hierarchical structure itself contributes to the claimed benefits.

2. **Cross-task generalization test**: Evaluate ArCHer-trained agents on held-out variations of the same tasks (e.g., Twenty Questions with different object categories) to assess whether the hierarchical approach learns transferable strategies versus task-specific heuristics.

3. **Critic sensitivity analysis**: Systematically vary the discount factor and replay buffer size to identify the parameter ranges where ArCHer's performance degrades, and compare these ranges to those of pure on-policy methods.