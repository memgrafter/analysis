---
ver: rpa2
title: 'Conifer: Improving Complex Constrained Instruction-Following Ability of Large
  Language Models'
arxiv_id: '2404.02823'
source_url: https://arxiv.org/abs/2404.02823
tags:
- conifer
- dataset
- instructions
- instruction
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Conifer introduces a novel dataset and training method to improve
  the ability of large language models (LLMs) to follow complex instructions with
  multiple constraints. The approach involves generating a high-quality dataset using
  GPT-4, breaking down the task into manageable steps, and employing a progressive
  learning scheme with easy-to-hard progression and process feedback.
---

# Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models

## Quick Facts
- arXiv ID: 2404.02823
- Source URL: https://arxiv.org/abs/2404.02823
- Authors: Haoran Sun; Lixin Liu; Junjie Li; Fengyu Wang; Baohua Dong; Ran Lin; Ruohui Huang
- Reference count: 23
- One-line primary result: Conifer-7B achieves 52.3% on IFEval and 56.2% on FollowBench HSR, outperforming best open-source 7B models

## Executive Summary
Conifer introduces a novel approach to improve large language models' ability to follow complex instructions with multiple constraints. The method involves generating a high-quality dataset using GPT-4, breaking down the task into manageable steps, and employing a progressive learning scheme with easy-to-hard progression and process feedback. The Conifer-7B model, trained on this dataset, achieves state-of-the-art performance on instruction-following benchmarks, outperforming 7B models and even matching the capabilities of models 10 times larger on certain metrics.

## Method Summary
Conifer's method involves three key components: instruction generation using GPT-4 with query reframing, constraint generation, and recombination; a progressive learning scheme with easy-to-hard progression and process feedback; and training using supervised fine-tuning and direct preference optimization. The dataset consists of 13k samples of complex constrained instructions, generated by breaking down the task into smaller subtasks for GPT-4. The progressive learning approach organizes data into multi-turn conversations with increasing difficulty, incorporating both internal and external process feedback. The model is trained using a combination of SFT and DPO techniques.

## Key Results
- Conifer-7B achieves 52.3% on IFEval and 56.2% on FollowBench HSR
- Conifer-7B-DPO outperforms or matches 70B models on certain metrics
- The approach demonstrates significant improvements over baseline 7B models on complex constraint satisfaction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing complex instruction generation into smaller tasks improves quality and diversity of generated data.
- Mechanism: GPT-4 is prompted to perform simpler subtasks rather than generating complex instructions directly.
- Core assumption: GPT-4 performs better on simpler, well-defined subtasks than on complex, multi-constraint instruction generation.
- Evidence anchors: [abstract] "we break the hard task into smaller, more manageable tasks"; [section 3.1] "we have decomposed this challenging task into smaller, more manageable tasks for GPT-4"

### Mechanism 2
- Claim: Progressive learning with easy-to-hard progression and process feedback improves model's ability to follow complex instructions.
- Mechanism: Data is organized into multi-turn conversations with increasing difficulty. Models learn from both internal and external process feedback.
- Core assumption: Models learn better when exposed to progressively harder examples and receive detailed feedback on reasoning steps.
- Evidence anchors: [abstract] "we propose a progressive learning scheme that emphasizes an easy-to-hard progression, and learning from process feedback"; [section 3.2] "we have developed a progressive learning strategy"

### Mechanism 3
- Claim: Using GPT-4 to generate high-quality, complex instruction data addresses the gap in existing instruction-following datasets.
- Mechanism: GPT-4 generates instructions with complex constraints based on ShareGPT queries as seeds, ensuring diversity and quality through refinement processes.
- Core assumption: GPT-4 can generate high-quality instructions with complex constraints when properly prompted and refined.
- Evidence anchors: [abstract] "Utilizing GPT-4, we curate the dataset by a series of LLM-driven refinement processes to ensure high quality"; [section 3.1] "we utilize GPT-4 to reformulate each query into at least three distinct and varied forms"

## Foundational Learning

- Concept: Curriculum learning and progressive difficulty progression
  - Why needed here: Complex instructions require gradual skill building, and models benefit from starting with simpler examples before tackling harder ones
  - Quick check question: How would you modify the data ordering if initial experiments show the model struggles with the easy examples?

- Concept: Process supervision and feedback incorporation
  - Why needed here: Complex instructions require understanding of reasoning steps, not just final outputs, making intermediate feedback crucial
  - Quick check question: What metrics would you use to evaluate the effectiveness of internal vs external feedback?

- Concept: Constraint satisfaction and multi-constraint reasoning
  - Why needed here: Following complex instructions requires understanding and simultaneously satisfying multiple constraints
  - Quick check question: How would you modify the evaluation approach if the model consistently fails on specific constraint types?

## Architecture Onboarding

- Component map: GPT-4 (instruction generation) -> Multi-turn data structure (progressive learning) -> Process feedback mechanism (internal/external) -> Evaluation benchmarks (IFEval, FollowBench, InFoBench)
- Critical path: Instruction generation → Data refinement → Progressive learning → Model training → Evaluation
- Design tradeoffs: Quality vs quantity in data generation, computational cost of refinement vs benefits, complexity of progressive learning vs simplicity of traditional approaches
- Failure signatures: Poor constraint satisfaction, inability to handle increasing difficulty, overfitting to specific constraint types, failure to generalize across constraint categories
- First 3 experiments:
  1. Test the decomposition approach by comparing instruction quality from direct generation vs. decomposed subtasks
  2. Evaluate the impact of different progression patterns (random, easy-to-hard, hard-to-easy) on model performance
  3. Assess the effectiveness of internal vs external feedback by training separate models with each type and comparing performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Conifer's performance scale with model size beyond 7B and 13B parameters?
- Basis in paper: The authors note that Conifer-7B-DPO outperforms or matches 70B models on certain metrics, but do not test larger model sizes directly.
- Why unresolved: The paper focuses on 7B and 13B models for practical reasons, leaving open questions about performance at larger scales.
- What evidence would resolve it: Training and evaluating Conifer on 30B+ parameter models and comparing their performance to existing large models on the benchmarks.

### Open Question 2
- Question: What is the long-term impact of Conifer's easy-to-hard progression on model generalization to unseen tasks?
- Basis in paper: The authors propose and validate the easy-to-hard progression approach, but do not test generalization beyond the specific benchmarks used.
- Why unresolved: The evaluation focuses on specific benchmarks, not on how the progression affects broader task generalization.
- What evidence would resolve it: Extensive testing of Conifer-trained models on diverse, unseen instruction-following tasks and comparing to models trained without progression.

### Open Question 3
- Question: How does the quality and complexity of Conifer's GPT-4 generated data compare to human-annotated datasets?
- Basis in paper: The authors use GPT-4 for data generation and claim high quality, but do not directly compare to human-annotated data.
- Why unresolved: While the authors use a quality scorer, they don't benchmark against human-annotated instruction datasets.
- What evidence would resolve it: A direct comparison study where human annotators evaluate a subset of Conifer data against human-annotated instruction datasets on multiple quality dimensions.

## Limitations

- Lack of human evaluation to validate the quality of generated complex instructions and model's true understanding
- Unclear scalability of the approach to much larger models or efficiency at scale
- Potential biases introduced by using GPT-4 for data generation, leading to models overly aligned with GPT-4's reasoning patterns

## Confidence

- **High Confidence:** The effectiveness of progressive learning with easy-to-hard progression is well-supported by experimental results and aligns with established curriculum learning principles.
- **Medium Confidence:** The decomposition approach for instruction generation shows promise but lacks comparative analysis against direct generation methods.
- **Medium Confidence:** The overall performance improvements are demonstrated across multiple benchmarks, though the reliance on automated evaluation limits certainty about real-world effectiveness.

## Next Checks

1. Conduct human evaluation studies comparing the quality of complex instructions generated by the proposed decomposition approach versus direct generation methods, focusing on instruction clarity, constraint relevance, and task feasibility.
2. Perform ablation studies to isolate the impact of each component of the progressive learning scheme (easy-to-hard progression, internal feedback, external feedback) on model performance, using statistical significance testing across multiple random seeds.
3. Test model generalization by evaluating Conifer-7B on completely unseen constraint types and real-world instruction-following tasks that weren't represented in the training data, measuring both success rates and failure modes.