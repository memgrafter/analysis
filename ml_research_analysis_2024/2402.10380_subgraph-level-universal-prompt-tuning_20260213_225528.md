---
ver: rpa2
title: Subgraph-level Universal Prompt Tuning
arxiv_id: '2402.10380'
source_url: https://arxiv.org/abs/2402.10380
tags:
- graph
- prompt
- tuning
- conference
- supt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Subgraph-level Universal Prompt Tuning (SUPT),
  a method that applies learnable prompt features to subgraphs rather than nodes or
  the entire graph. This approach improves upon existing universal graph prompt methods
  by capturing richer subgraph-level context while maintaining low parameter counts.
---

# Subgraph-level Universal Prompt Tuning

## Quick Facts
- arXiv ID: 2402.10380
- Source URL: https://arxiv.org/abs/2402.10380
- Authors: Junhyun Lee; Wooseong Yang; Jaewoo Kang
- Reference count: 40
- Key outcome: SUPT achieves superior performance to fine-tuning in 42/45 full-shot and 41/45 few-shot scenarios across nine datasets, with average gains of 2.5% and 6.6% respectively.

## Executive Summary
This paper introduces Subgraph-level Universal Prompt Tuning (SUPT), a method that applies learnable prompt features to subgraphs rather than nodes or entire graphs. The approach improves upon existing universal graph prompt methods by capturing richer subgraph-level context while maintaining low parameter counts. SUPT is designed to work with any pre-training strategy, making it versatile for various downstream applications. In extensive experiments across nine datasets and five pre-training strategies, SUPT consistently outperforms fine-tuning methods while requiring significantly fewer tuning parameters.

## Method Summary
SUPT introduces learnable prompt features at the subgraph level, where prompt feature vectors are assigned to nodes based on scores derived from simple GNNs like GCN or SGC for m-hop context. The method includes two variants: SUPTsoft using softmax and SUPThard using top-rank selection. The approach maintains universal capability by theoretically being able to emulate any prompting function through its GNN-based score calculation. Experiments use a 5-layer GIN architecture with specific hyperparameters across nine datasets including Biology, Chemistry, and eight downstream task datasets.

## Key Results
- SUPT outperforms fine-tuning in 42 out of 45 full-shot scenario experiments with an average improvement of over 2.5%
- SUPT achieves superior performance in 41 out of 45 few-shot scenario experiments with an average improvement of 6.6%
- The method requires significantly fewer tuning parameters than fine-tuning-based methods while maintaining better performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SUPT assigns prompt features at the subgraph level, enabling finer-grained control over input features than node-level or graph-level approaches.
- Mechanism: By applying learnable prompts to subgraphs rather than individual nodes, SUPT captures more nuanced context within each subgraph through category-specific prompts based on node features.
- Core assumption: Subgraphs contain meaningful structural and contextual information that can be leveraged to improve downstream task performance.
- Evidence anchors:
  - [abstract]: "In SUPT, prompt features are assigned at the subgraph-level, preserving the method's universal capability."
  - [section]: "In subgraph-level prompt tuning, we adhere to Equation (3), which operates in the input space. However, instead of utilizing the node feature ð‘¥, we allocate prompt feature vectors based on scores derived from simple GNNs, such as GCN [18] or SGC [46] for ð‘š-hop context."
  - [corpus]: Found 25 related papers, suggesting ongoing research interest in subgraph-level and universal graph prompt tuning approaches.

### Mechanism 2
- Claim: SUPT maintains universal capability, allowing it to work with any pre-training strategy.
- Mechanism: SUPT's prompt calculation uses a GNN to derive scores for each basis, which are then used to assign prompt features, theoretically enabling it to emulate any prompting function.
- Core assumption: The GNN used in SUPT's prompt calculation can capture the necessary information to emulate any prompting function.
- Evidence anchors:
  - [abstract]: "This allows it to theoretically emulate any type of prompting function, thereby significantly increasing its versatility for a range of downstream applications."
  - [section]: "The weight ð›¼ð‘–,ð‘— varies according to the type of node feature, that is, ð‘¥ð‘–, and therefore, the prompt feature ð‘ð‘– becomes a category-specific prompt for the node."

### Mechanism 3
- Claim: SUPT outperforms fine-tuning methods while requiring fewer tuning parameters.
- Mechanism: SUPT's subgraph-level approach captures richer context than fine-tuning while maintaining a smaller number of tuning parameters by only adjusting the prompt features rather than the entire model.
- Core assumption: The improved performance from capturing richer context outweighs the potential benefits of fine-tuning the entire model.
- Evidence anchors:
  - [abstract]: "This requires extremely fewer tuning parameters than fine-tuning-based methods, outperforming them in 42 out of 45 full-shot scenario experiments with an average improvement of over 2.5%."
  - [section]: "In an overall comparison of the fine-tuning (FT), GPF, and SUPT approaches, it is observed that while FT involves adjusting the model's entire parameter set, prompt tuning strategies like GPF and SUPT employ far fewer additional training parameters."

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: SUPT builds upon GNNs to derive scores for subgraph assignment and to process the graph data.
  - Quick check question: What are the key components of a GNN, and how do they differ from traditional neural networks?

- Concept: Graph Pre-training
  - Why needed here: SUPT is designed to work with pre-trained GNNs, and understanding the different pre-training strategies is crucial for appreciating SUPT's universal capability.
  - Quick check question: What are some common pre-training strategies for GNNs, and how do they differ in terms of objectives and tasks?

- Concept: Prompt Tuning
  - Why needed here: SUPT is a form of prompt tuning, and understanding the general concept of prompt tuning is essential for grasping SUPT's approach and advantages.
  - Quick check question: How does prompt tuning differ from traditional fine-tuning, and what are the potential benefits of prompt tuning?

## Architecture Onboarding

- Component map:
  Input graph (node features + adjacency matrix) -> GNN (GCN/SGC) -> Subgraph assignment scores -> Prompt feature vectors -> Modified node features -> Pre-trained GNN -> Projection head -> Final prediction

- Critical path:
  1. Input graph is processed by the GNN to derive scores for subgraph assignment
  2. Prompt feature vectors are assigned to subgraphs based on the derived scores
  3. Prompt feature vectors are added to the node features of nodes within the corresponding subgraphs
  4. The modified graph is processed by the pre-trained GNN
  5. The GNN output is passed through the projection head to obtain the final prediction

- Design tradeoffs:
  - Subgraph size vs. granularity: Smaller subgraphs allow for more granular control but may lead to overfitting
  - Number of subgraphs vs. parameter efficiency: More subgraphs can capture more context but increase the number of learnable parameters
  - Choice of GNN for score derivation: Different GNNs may capture different aspects of the graph structure

- Failure signatures:
  - Poor performance on downstream tasks: May indicate issues with subgraph assignment or prompt feature learning
  - Overfitting: May occur if subgraphs are too small or if the number of subgraphs is too large
  - Slow convergence: May be caused by the additional complexity of subgraph-level prompt assignment

- First 3 experiments:
  1. Compare SUPT's performance with different numbers of subgraphs (k) on a small dataset to find the optimal balance between granularity and parameter efficiency.
  2. Evaluate SUPT's performance with different GNNs (e.g., GCN, SGC) for score derivation to determine which captures the most relevant information for subgraph assignment.
  3. Compare SUPT's performance with node-level and graph-level prompt tuning methods on a medium-sized dataset to quantify the benefits of the subgraph-level approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper bound on the number of subgraphs (k) that can be used in SUPT before performance plateaus or degrades?
- Basis in paper: [explicit] The paper mentions k is chosen from {1, 2, 3, 4, 5} but does not explore beyond this range.
- Why unresolved: The paper only tests k values up to 5, leaving uncertainty about whether higher values would improve or harm performance.
- What evidence would resolve it: Extensive experiments testing k values ranging from 1 to 20 or more, with systematic analysis of performance trends.

### Open Question 2
- Question: How does SUPT's performance compare to fine-tuning when the pre-trained GNN has a different architecture (e.g., deeper networks, different aggregation functions)?
- Basis in paper: [explicit] The paper uses a 5-layer GIN architecture as specified by Fang et al., but does not test other architectures.
- Why unresolved: The experiments are limited to one specific GNN architecture, so generalization to other architectures remains untested.
- What evidence would resolve it: Comparative experiments using various GNN architectures (e.g., GAT, GCN, deeper GINs) across the same datasets and tasks.

### Open Question 3
- Question: What is the impact of different graph pooling strategies on SUPT's effectiveness, beyond the GCN-based approach used in the current implementation?
- Basis in paper: [inferred] The paper mentions inspiration from graph pooling methods but only uses a simple GCN for subgraph selection.
- Why unresolved: The paper does not explore alternative pooling mechanisms that might better capture subgraph structures.
- What evidence would resolve it: Systematic comparison of SUPT with different pooling strategies (e.g., DiffPool, SAGPool, EdgePool) across multiple datasets.

### Open Question 4
- Question: How does SUPT perform on graph-level tasks beyond classification, such as graph generation or link prediction?
- Basis in paper: [explicit] The paper focuses exclusively on graph classification tasks.
- Why unresolved: The evaluation is limited to classification benchmarks, leaving uncertainty about SUPT's applicability to other graph tasks.
- What evidence would resolve it: Experiments testing SUPT on graph generation tasks, link prediction, and other graph-level prediction problems.

## Limitations

- The universal capability claim relies on theoretical reasoning rather than comprehensive empirical validation across diverse pre-training approaches.
- Performance gains are primarily measured through ROC-AUC, which may not capture all aspects of downstream task performance.
- The computational efficiency differences between SUPT and fine-tuning are not explored in detail.

## Confidence

**High confidence** in the empirical performance claims (42/45 full-shot and 41/45 few-shot scenarios) as these are directly supported by experimental results presented in the paper.

**Medium confidence** in the mechanism claims about why subgraph-level prompting works better. While the paper provides theoretical justification and empirical evidence, the causal relationship between subgraph-level context and performance gains could be more rigorously established.

**Low confidence** in the universal capability claim. The paper asserts that SUPT can work with any pre-training strategy, but this is based on theoretical reasoning rather than comprehensive empirical validation across diverse pre-training approaches.

## Next Checks

1. **Cross-pretraining validation**: Test SUPT with a broader range of pre-training strategies beyond the five mentioned (e.g., contrastive learning, generative pre-training) to empirically verify the universal capability claim across at least 10 different pre-training methods.

2. **Subgraph granularity analysis**: Conduct a systematic ablation study varying subgraph sizes and structures (from node-level to graph-level) on a standardized benchmark to precisely quantify the performance gains from the subgraph-level approach versus other granularities.

3. **Parameter efficiency benchmarking**: Measure and compare the actual computational costs (training time, memory usage) of SUPT versus fine-tuning across different graph sizes to verify that the parameter efficiency translates to practical computational advantages.