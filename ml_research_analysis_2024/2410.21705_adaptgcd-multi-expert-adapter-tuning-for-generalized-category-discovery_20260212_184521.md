---
ver: rpa2
title: 'AdaptGCD: Multi-Expert Adapter Tuning for Generalized Category Discovery'
arxiv_id: '2410.21705'
source_url: https://arxiv.org/abs/2410.21705
tags:
- classes
- uni00000013
- learning
- adapter
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generalized category discovery
  (GCD), where the goal is to classify both known (old) and unknown (new) categories
  in unlabeled data using limited labeled data. The key challenge is balancing the
  generalization capacity of pretrained models with the adaptability required for
  the GCD task.
---

# AdaptGCD: Multi-Expert Adapter Tuning for Generalized Category Discovery

## Quick Facts
- arXiv ID: 2410.21705
- Source URL: https://arxiv.org/abs/2410.21705
- Authors: Yuxun Qu; Yongqiang Tang; Chenyang Zhang; Wensheng Zhang
- Reference count: 40
- Primary result: Achieved 0.8-3.9% accuracy gains on "All" classes across 7 datasets compared to SimGCD baseline

## Executive Summary
This paper addresses the problem of generalized category discovery (GCD), where the goal is to classify both known (old) and unknown (new) categories in unlabeled data using limited labeled data. The key challenge is balancing the generalization capacity of pretrained models with the adaptability required for the GCD task. The proposed AdaptGCD method introduces adapter tuning into GCD for the first time, integrating learnable adapters parallel to the feed-forward layer of the vision transformer while keeping the backbone frozen to preserve pretrained knowledge. To address the information imbalance between old and new classes, AdaptGCD employs a multi-expert adapter structure with a route assignment constraint that separates data from old and new classes into different expert groups.

## Method Summary
AdaptGCD introduces adapter tuning to the GCD task by inserting learnable bottleneck modules (adapters) parallel to the feed-forward layers of vision transformer blocks while freezing the pretrained backbone. The method uses a multi-expert adapter structure with 8 experts (4 for old classes, 4 for new classes) and a routing layer that assigns different data samples to different expert groups. Route assignment constraints guide the specialization of adapter experts through balanced assignment loss (Lba) ensuring all experts are utilized, and category-aware balanced assignment loss (Lcba) forcing old-class data to specific experts and new-class data to others using KL divergence. The method was evaluated on 7 widely-used datasets including CIFAR10/100, ImageNet100, CUB-200, Stanford Cars, FGVC-Aircraft, and Herbarium19.

## Key Results
- AdaptGCD achieved 0.8-3.9% accuracy gains on "All" classes across different datasets compared to SimGCD baseline
- The method showed consistent improvements on both old and new class accuracy (accold, accnew, accall)
- Performance was evaluated on 7 diverse datasets including generic image recognition benchmarks, semantic shift benchmarks, and a long-tailed dataset
- AdaptGCD demonstrated effectiveness of adapter tuning in GCD tasks and the value of multi-expert adapter structure with route assignment constraint

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adapter tuning preserves pretrained knowledge while enabling task-specific adaptation.
- Mechanism: The method freezes the backbone parameters and inserts learnable bottleneck modules (adapters) parallel to the feed-forward layers of the transformer blocks. Only the adapter parameters are updated during training.
- Core assumption: The pretrained backbone contains generalizable knowledge that should not be disrupted by fine-tuning.
- Evidence anchors:
  - [abstract]: "It could preserve pretrained knowledge nondestructively by maintaining the parameters in the backbone"
  - [section IV-A1]: "The adapter tuning leaves the parameters in pretrained model fixed and brings in a modest number of task-specific parameters via bottleneck adapter modules"

### Mechanism 2
- Claim: Multi-expert adapter structure reduces information interference between old and new classes.
- Mechanism: The method uses multiple adapter experts with a routing layer that assigns different data samples to different expert groups based on their predicted class (old vs. new). This separation is enforced through route assignment constraints.
- Core assumption: Data from old and new classes have different statistical properties that benefit from specialized processing paths.
- Evidence anchors:
  - [abstract]: "a multi-expert adapter structure equipped with a route assignment constraint is elaborately devised, such that the data from old and new classes are separated into different expert groups"
  - [section IV-B]: "The strategy of multi-expert technology is adopted to integrate with the adapter. This approach assigns different types of data to different expert adapter"

### Mechanism 3
- Claim: Route assignment constraints guide experts to specialize in specific class types.
- Mechanism: The method employs two losses: balanced assignment loss (Lba) ensures all experts are utilized, and category-aware balanced assignment loss (Lcba) forces old-class data to specific experts and new-class data to others using KL divergence.
- Core assumption: Explicit supervision through routing constraints can effectively guide the specialization of adapter experts.
- Evidence anchors:
  - [section IV-B2]: "the mean distribution of experts in the l-th layer ωl, averaging the route assignment probabilities across all samples, is aligned to a uniform distribution I"
  - [section IV-B3]: "we employ the Kullback-Leibler divergence loss as a constraint, as shown in Eq. (12), to align the average probability of old and new classes"

## Foundational Learning

- Concept: Generalized Category Discovery (GCD) task definition
  - Why needed here: Understanding the problem setup is crucial for grasping why the proposed method addresses the information imbalance between old and new classes
  - Quick check question: In GCD, what is the relationship between Yl and Yu (labeled and unlabeled label spaces)?

- Concept: Adapter tuning vs. prompt tuning vs. partial tuning
  - Why needed here: The paper's contribution relies on understanding how adapter tuning differs from existing fine-tuning strategies and why it's advantageous
  - Quick check question: What is the key difference between adapter tuning and partial tuning in terms of parameter updates?

- Concept: Mixture of Experts (MoE) routing mechanism
  - Why needed here: The multi-expert adapter builds on MoE concepts, so understanding how routing works is essential for grasping the proposed approach
  - Quick check question: How does the routing layer in MoE determine which expert processes which input?

## Architecture Onboarding

- Component map: Backbone (frozen ViT-B/16) → Multi-Expert Adapter (MEA) modules in last P blocks → Classification head → Loss functions (SimGCD loss + Route assignment loss)
- Critical path: Input → Backbone → Adapter experts → Routing → Classification → Loss computation
- Design tradeoffs: More experts improve specialization but increase parameters and computational cost; deeper adapter placement preserves more pretrained knowledge but may reduce task adaptation
- Failure signatures: Poor performance on new classes suggests routing assignment issues; poor performance on old classes suggests backbone disruption; imbalanced expert usage suggests Lba loss issues
- First 3 experiments:
  1. Implement basic single adapter (as in row 2 of Table V) to verify adapter benefits over SimGCD
  2. Add multi-expert structure without routing constraints to verify expert benefits
  3. Add route assignment constraints to verify their impact on class separation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of experts and their assignment strategy (old vs new class experts) for different types of datasets and classification tasks?
- Basis in paper: [explicit] The paper shows performance varies with different numbers of old and new class experts (Table VI), and discusses the need to balance expert allocation between old and new classes.
- Why unresolved: The paper only tests a limited range of expert configurations and doesn't provide a systematic method for determining optimal expert allocation based on dataset characteristics.
- What evidence would resolve it: A comprehensive study analyzing expert performance across diverse datasets with varying class imbalances, semantic relationships, and dataset sizes, coupled with a principled method for determining expert allocation based on dataset properties.

### Open Question 2
- Question: How does the adapter-based approach compare to other parameter-efficient fine-tuning methods (like LoRA, Side Tuning) specifically for GCD tasks under different data quality conditions?
- Basis in paper: [explicit] The paper compares AdaptGCD with LoRA and Side Tuning (Table VIII) but only under standard conditions, and discusses that prompt-based methods are not robust to bad data quality.
- Why unresolved: The paper doesn't investigate how different parameter-efficient methods perform under varying data quality scenarios (noise levels, resolution, etc.) which is particularly relevant for GCD tasks.
- What evidence would resolve it: Systematic experiments comparing multiple parameter-efficient methods under controlled variations in data quality metrics, including noise injection, resolution reduction, and class imbalance.

### Open Question 3
- Question: Can the multi-expert adapter framework be extended to handle more than two class types (e.g., multiple levels of supervision or hierarchical class structures) in GCD?
- Basis in paper: [inferred] The paper uses a binary separation between old and new classes with dedicated expert groups, suggesting potential for more complex expert allocation schemes.
- Why unresolved: The paper focuses on the binary case of old vs new classes, but real-world GCD scenarios might involve multiple levels of supervision or hierarchical class structures.
- What evidence would resolve it: Implementation and evaluation of the framework on datasets with multiple supervision levels or hierarchical class structures, demonstrating improved performance over binary separation methods.

## Limitations

- The evaluation relies heavily on clustering accuracy metrics, which may not fully capture the quality of discovered categories in practical applications
- The paper's claims about information imbalance are based on the assumption that old and new classes have inherently different properties, but this assumption is not empirically validated
- The route assignment mechanism depends on pseudo-labels whose quality is not thoroughly examined

## Confidence

- High confidence in the adapter tuning mechanism and its preservation of pretrained knowledge, as this is well-established in the literature
- Medium confidence in the multi-expert structure's effectiveness, as the paper shows improvements but doesn't conduct ablation studies on expert count or placement
- Medium confidence in the route assignment constraint's impact, as the improvement claims are significant but the underlying pseudo-label quality is not discussed
- Low confidence in generalization across different backbone architectures, as the study only uses ViT-B/16

## Next Checks

1. Conduct ablation studies varying the number of experts and their placement depth to determine optimal configuration
2. Analyze pseudo-label quality across training epochs to validate the route assignment mechanism's reliability
3. Test the method with different backbone architectures (e.g., ConvNeXt, ResNet) to assess generalizability beyond ViT