---
ver: rpa2
title: 'ASAG2024: A Combined Benchmark for Short Answer Grading'
arxiv_id: '2409.18596'
source_url: https://arxiv.org/abs/2409.18596
tags:
- grading
- benchmark
- answer
- systems
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ASAG2024, the first comprehensive benchmark
  for short answer grading that combines seven datasets across different subjects,
  grading scales, and distributions. The benchmark contains 19,000 question-answer-grade
  triplets with grades normalized to a 0-1 scale.
---

# ASAG2024: A Combined Benchmark for Short Answer Grading

## Quick Facts
- arXiv ID: 2409.18596
- Source URL: https://arxiv.org/abs/2409.18596
- Reference count: 17
- First comprehensive benchmark for short answer grading combining seven datasets across subjects, scales, and distributions

## Executive Summary
ASAG2024 introduces the first comprehensive benchmark for short answer grading, combining seven datasets into a unified corpus of 19,000 question-answer-grade triplets with normalized grades on a 0-1 scale. The authors evaluate seven automated grading methods, including specialized systems (BART-SAF, PrometheusII-7B) and various LLMs (Llama3-8B, GPT-3.5-turbo, GPT-4o). Results show that while LLMs achieve decent performance (wRMSE of 0.27 for GPT-4o) without specific training, they still fall short of human performance (error approximately 0.1). Surprisingly, fine-tuned specialized models perform worse than a simple mean predictor baseline (wRMSE 0.40), suggesting they struggle to generalize across different datasets.

## Method Summary
The authors created ASAG2024 by aggregating seven existing short answer grading datasets from different subjects and grading scales. They normalized all grades to a 0-1 scale to enable consistent evaluation across datasets. The benchmark was evaluated using weighted RMSE as the primary metric. Seven automated grading methods were tested: two specialized models (BART-SAF and PrometheusII-7B) that were fine-tuned on the combined dataset, and five LLM-based approaches including Llama3-8B, GPT-3.5-turbo, and GPT-4o using both few-shot and instruction-tuned variants. The evaluation compared these methods against human grading performance and a simple mean predictor baseline.

## Key Results
- LLMs achieve decent performance without specific training (wRMSE of 0.27 for GPT-4o)
- Human grading error is approximately 0.1, still better than all automated methods
- Fine-tuned specialized models perform worse than a simple mean predictor baseline (wRMSE 0.40)
- Specialized models struggle to generalize across different datasets

## Why This Works (Mechanism)
The benchmark works by providing a standardized evaluation framework that exposes the generalization challenges in short answer grading. By combining datasets with different subjects, grading scales, and distributions, it creates a realistic testing environment that reveals whether models can transfer knowledge across domains rather than just memorizing patterns from a single dataset.

## Foundational Learning
- **Weighted RMSE**: Why needed - Provides a standardized metric for comparing model performance across different grading scales; Quick check - Verify that weights are correctly calculated based on dataset sizes
- **Grade normalization**: Why needed - Enables consistent evaluation across datasets with different original scales; Quick check - Confirm all grades fall within 0-1 range after normalization
- **Cross-dataset generalization**: Why needed - Tests whether models learn universal grading principles rather than dataset-specific patterns; Quick check - Evaluate performance on held-out datasets not seen during training
- **Few-shot learning evaluation**: Why needed - Assesses model ability to adapt to new grading contexts with minimal examples; Quick check - Test performance with varying numbers of few-shot examples
- **Specialized model fine-tuning**: Why needed - Determines whether task-specific architectures outperform general-purpose LLMs; Quick check - Compare fine-tuned performance against pre-trained baselines
- **Human grading error baseline**: Why needed - Provides realistic upper bound for automated grading performance; Quick check - Calculate inter-rater reliability among human graders

## Architecture Onboarding
Component map: Data Aggregation -> Grade Normalization -> Model Training -> Evaluation
Critical path: Combined dataset → Model training/inference → wRMSE calculation → Performance comparison
Design tradeoffs: Generalizability vs. specialization - Using combined dataset enables broader evaluation but may dilute domain-specific patterns
Failure signatures: Poor cross-dataset performance indicates overfitting to specific grading patterns; worse-than-baseline performance suggests ineffective fine-tuning strategies
First experiments:
1. Validate grade normalization consistency across all seven datasets
2. Test model performance on individual datasets before combining
3. Compare few-shot performance with and without instruction tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on weighted RMSE, which may not capture all aspects of grading quality
- Limited discussion of potential biases in the datasets or how different grading scales affect model performance
- Does not address how the benchmark handles edge cases or ambiguous answers
- Comparison raises questions about whether proper fine-tuning strategies were employed for specialized systems

## Confidence
- High confidence in benchmark creation and dataset aggregation methodology
- Medium confidence in performance comparison results between different model types
- Medium confidence in conclusions about LLMs versus specialized systems due to limited exploration of alternative fine-tuning approaches

## Next Checks
1. Test the benchmark's robustness by introducing adversarial examples or edge cases to evaluate model performance on challenging inputs
2. Conduct ablation studies on the specialized models to identify whether different fine-tuning strategies or architectures might improve their performance
3. Evaluate additional metrics beyond weighted RMSE, such as correlation with human grades or alignment with grading rubric consistency, to provide a more comprehensive assessment of model performance