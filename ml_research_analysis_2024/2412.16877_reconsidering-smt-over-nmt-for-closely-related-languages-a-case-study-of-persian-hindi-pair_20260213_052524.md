---
ver: rpa2
title: 'Reconsidering SMT Over NMT for Closely Related Languages: A Case Study of
  Persian-Hindi Pair'
arxiv_id: '2412.16877'
source_url: https://arxiv.org/abs/2412.16877
tags:
- languages
- language
- bleu
- sentences
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that Phrase-Based Statistical Machine Translation
  (PBSMT) outperforms Transformer-based Neural Machine Translation (NMT) for the Persian-Hindi
  language pair when using moderate-sized parallel corpora (1M+ sentences). The PBSMT
  model achieved a BLEU score of 66.32, significantly exceeding the Transformer-NMT
  score of 53.7.
---

# Reconsidering SMT Over NMT for Closely Related Languages: A Case Study of Persian-Hindi Pair

## Quick Facts
- arXiv ID: 2412.16877
- Source URL: https://arxiv.org/abs/2412.16877
- Reference count: 36
- PBSMT achieved 66.32 BLEU, outperforming Transformer-NMT (53.7) on Persian-Hindi translation

## Executive Summary
This study demonstrates that Phrase-Based Statistical Machine Translation (PBSMT) outperforms Transformer-based Neural Machine Translation (NMT) for the Persian-Hindi language pair when using moderate-sized parallel corpora (1M+ sentences). The PBSMT model achieved a BLEU score of 66.32, significantly exceeding the Transformer-NMT score of 53.7. The research highlights that structurally similar languages benefit more from SMT due to their one-to-one word alignment properties and shared linguistic structures. The study also found that romanizing Persian text reduced BLEU scores due to loss of diacritics, while reversing Persian text order from RTL to LTR further degraded performance by altering sentence meaning.

## Method Summary
The study compared PBSMT and Transformer NMT for Persian-Hindi translation using a 1M+ sentence parallel corpus from OPUS. PBSMT was implemented using Moses toolkit with MGIZA++ for word alignment and SRILM for a 5-gram Hindi language model. Transformer NMT used OpenNMT with 8 encoder/decoder layers, 512 embedding size, 32K BPE merge operations, and 300K training steps. Both models were evaluated using sacreBLEU. Additional experiments tested romanized Persian input and RTL-to-LTR text reversal to assess their impact on translation quality.

## Key Results
- PBSMT achieved 66.32 BLEU, significantly outperforming Transformer NMT (53.7) on Persian-Hindi translation
- Romanizing Persian text reduced BLEU scores due to loss of diacritics
- Reversing Persian script order from RTL to LTR degraded performance by altering sentence meaning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: One-to-one word alignment superiority in closely related languages.
- Mechanism: When source and target languages have isomorphic structures and minimal syntactic divergence, word alignment models can capture near-perfect one-to-one mappings, reducing reliance on phrase-based generalization.
- Core assumption: The Persian-Hindi language pair exhibits low syntactic divergence, allowing accurate one-to-one alignments.
- Evidence anchors:
  - [abstract] "structurally similar languages benefit more from SMT due to their one-to-one word alignment properties and shared linguistic structures."
  - [section 3] "we analyzed the differences in sentence lengths... more than 72% of the parallel sentences... have a length difference of less than three tokens."
  - [corpus] Weak—no direct alignment quality metric provided.
- Break condition: Significant syntactic divergence or idiomatic expressions that require phrasal reordering beyond one-to-one mapping.

### Mechanism 2
- Claim: Moderate parallel corpus size sufficiency for SMT over NMT.
- Mechanism: SMT leverages statistical alignment and phrase tables that can be reliably estimated from smaller datasets, whereas NMT requires massive corpora to learn generalizable representations.
- Core assumption: 1M+ parallel sentences provide enough data for reliable EM-based alignment and phrase table construction.
- Evidence anchors:
  - [abstract] "Phrase-Based Statistical Machine Translation (PBSMT) can outperform Transformer-based Neural Machine Translation (NMT) in moderate-resource scenarios."
  - [section 4.1] "we utilized the open-source toolkit, Moses... First, a word alignment model... was trained... Next, a 5-gram Language Model... was built."
  - [corpus] Table 1 confirms 1M+ sentence corpus size.
- Break condition: Dataset too small to estimate reliable alignment or phrase probabilities, or too noisy to extract consistent patterns.

### Mechanism 3
- Claim: Right-to-left script reversal degrades translation quality by breaking semantic integrity.
- Mechanism: Reversing RTL scripts to LTR changes the natural reading order, altering the intended meaning and disrupting alignment learned in the original direction.
- Core assumption: Word order flexibility in Persian-Hindi is constrained by semantic dependencies that are broken when the sentence is reversed.
- Evidence anchors:
  - [section 5] "reversing the Persian scripts from right-to-left (RTL) to left-to-right (LTR)... negatively impacted the SMT model's performance... due to the fact that reversing a sentence alters its meaning."
  - [section 5] "the inversion of the sentence disrupts the intended meaning and perhaps alignment, which consequently affects the overall performance negatively."
  - [corpus] No direct corpus-level metric for semantic preservation.
- Break condition: Language pairs where semantic dependencies are preserved under reversal, or where script direction is not semantically meaningful.

## Foundational Learning

- Concept: Statistical alignment and phrase extraction
  - Why needed here: Core to understanding how SMT leverages one-to-one mappings for translation.
  - Quick check question: What alignment algorithm does Moses use to learn word correspondences from parallel corpora?

- Concept: Sentence length ratio analysis
  - Why needed here: Validates the assumption that source and target sentences have similar token counts, supporting one-to-one alignment.
  - Quick check question: How many token-length differences between Persian and Hindi sentences did the study analyze?

- Concept: Script directionality and semantic dependency
  - Why needed here: Explains why reversing Persian script to LTR degrades performance—meaning is lost when order is changed.
  - Quick check question: What happens to sentence meaning when Persian text is reversed from RTL to LTR?

## Architecture Onboarding

- Component map:
  Data preprocessing -> MGIZA++ word alignment -> SRILM language model -> Moses phrase table decoder -> BLEU evaluation
  For NMT: Preprocessed data -> BPE tokenization -> OpenNMT Transformer -> BLEU evaluation

- Critical path:
  1. Prepare normalized, deduplicated parallel corpus.
  2. Train MGIZA++ alignment model.
  3. Build 5-gram LM with SRILM.
  4. Train Moses phrase table and decode.
  5. Evaluate with sacreBLEU.

- Design tradeoffs:
  - SMT: Lower computational cost, relies on alignment quality, sensitive to sentence order.
  - NMT: Higher cost, learns contextual representations, needs larger data.

- Failure signatures:
  - SMT: BLEU drops if alignment quality is poor or if scripts are reversed.
  - NMT: Underperformance with small datasets or related languages with one-to-one alignments.

- First 3 experiments:
  1. Train baseline SMT with original Persian-Hindi corpus.
  2. Repeat with romanized Persian input (observe BLEU drop).
  3. Reverse Persian script order to LTR and re-decode (observe further BLEU drop).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic properties of Persian and Hindi make them structurally similar enough for SMT to outperform NMT, and how can these properties be quantified?
- Basis in paper: Explicit - The paper mentions structural similarity and one-to-one word alignment as key factors.
- Why unresolved: The paper provides examples of syntactic similarity but doesn't systematically analyze or quantify the linguistic properties that make these languages particularly suited for SMT.
- What evidence would resolve it: A detailed linguistic analysis quantifying word order consistency, morphological similarity, and alignment patterns across a large corpus would help determine which properties are most critical for SMT performance.

### Open Question 2
- Question: At what corpus size threshold does NMT begin to outperform SMT for Persian-Hindi translation, and does this threshold vary for different related language pairs?
- Basis in paper: Inferred - The paper demonstrates SMT superiority at 1M+ sentences but doesn't explore the crossover point where NMT might become more effective.
- Why unresolved: The study only examines one corpus size range and doesn't test larger datasets to determine when NMT might surpass SMT performance.
- What evidence would resolve it: Testing both architectures across a range of corpus sizes (e.g., 100K, 500K, 1M, 5M, 10M+ sentences) would identify the specific threshold where NMT becomes advantageous.

### Open Question 3
- Question: How does the environmental impact (carbon footprint) of SMT compare to NMT for Persian-Hindi translation across different training scenarios?
- Basis in paper: Explicit - The paper mentions environmental concerns about NMT's high power consumption but doesn't quantify the difference.
- Why unresolved: While the paper acknowledges the environmental advantage of SMT, it doesn't provide concrete measurements of energy consumption or carbon emissions for either approach.
- What evidence would resolve it: Measuring actual energy usage during training and inference for both models, including GPU/CPU utilization data, would provide concrete evidence of the environmental impact difference.

### Open Question 4
- Question: Would alternative romanization approaches that preserve diacritics improve SMT performance compared to standard romanization?
- Basis in paper: Inferred - The paper shows standard romanization degrades performance due to loss of diacritics, but doesn't explore alternative methods.
- Why unresolved: The study only tests one romanization approach and doesn't investigate whether modified romanization schemes could retain enough linguistic information for successful translation.
- What evidence would resolve it: Testing multiple romanization schemes with varying levels of diacritic preservation would determine if there's an optimal approach that maintains both readability and translation accuracy.

## Limitations

- The study's findings are based on a single language pair experiment without broader generalization testing.
- The mechanism explaining SMT's superiority for closely related languages is plausible but not empirically validated beyond correlation with sentence length differences.
- Semantic preservation claims for romanized and reversed text lack direct measurement or annotation.

## Confidence

- High confidence: PBSMT outperforming NMT on this specific Persian-Hindi pair with 1M+ sentences (directly measured result)
- Medium confidence: General mechanism that closely related languages benefit from SMT due to one-to-one alignment properties (plausible but not broadly validated)
- Medium confidence: Script reversal degrades quality due to semantic disruption (supported by experiment but lacks semantic preservation metrics)
- Low confidence: These findings generalize to other closely related language pairs without additional validation

## Next Checks

1. Replicate the experiment with other closely related language pairs (e.g., Spanish-Portuguese, Hindi-Urdu) to test generalizability of the PBSMT advantage
2. Conduct controlled experiments varying corpus size to identify the exact resource threshold where NMT becomes competitive with SMT for related languages
3. Perform semantic preservation analysis on romanized and reversed text variants using human annotation or automated semantic similarity measures