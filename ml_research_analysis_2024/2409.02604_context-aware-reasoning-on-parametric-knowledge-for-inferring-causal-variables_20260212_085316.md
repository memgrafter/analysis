---
ver: rpa2
title: Context-Aware Reasoning On Parametric Knowledge for Inferring Causal Variables
arxiv_id: '2409.02604'
source_url: https://arxiv.org/abs/2409.02604
tags:
- causal
- graph
- causes
- variable
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel benchmark to evaluate LLMs' ability
  to hypothesize missing variables in causal graphs, a critical step in scientific
  discovery. The task involves inferring unobserved confounders, mediators, or other
  relevant variables given partial causal structures, reflecting real-world scenarios
  where domain expertise is typically required.
---

# Context-Aware Reasoning On Parametric Knowledge for Inferring Causal Variables

## Quick Facts
- arXiv ID: 2409.02604
- Source URL: https://arxiv.org/abs/2409.02604
- Reference count: 40
- Primary result: LLMs can effectively hypothesize missing causal variables, with performance varying by task complexity and model capability

## Executive Summary
This paper introduces a novel benchmark to evaluate LLMs' ability to hypothesize missing variables in causal graphs, a critical step in scientific discovery. The task involves inferring unobserved confounders, mediators, or other relevant variables given partial causal structures, reflecting real-world scenarios where domain expertise is typically required. The benchmark includes tasks of varying difficulty, from selecting missing variables from multiple choices to generating hypotheses in open-world settings. Experiments across diverse causal graphs show that LLMs can effectively identify and propose missing variables, especially mediators and colliders, with performance depending on task complexity and model capability. GPT-4 and GPT-4o generally outperform other models, and iterative prompting improves accuracy in predicting multiple mediators. The work highlights LLMs' potential as tools for early-stage hypothesis generation in causal discovery.

## Method Summary
The authors create a benchmark for evaluating LLMs' ability to infer missing variables in causal directed acyclic graphs (DAGs). They use causal DAGs from the BNLearn repository and a realistic Alzheimer's Disease graph, systematically removing nodes to create partial graphs. The LLM is prompted with textual representations of these partial DAGs and asked to identify or hypothesize missing variables. Tasks range from selecting from multiple choices to open-world hypothesis generation. Performance is evaluated using semantic similarity measures and an LLM-as-Judge approach. The study tests various LLMs including GPT-4, GPT-4o, and open-source models across different task complexities.

## Key Results
- GPT-4 and GPT-4o achieve high accuracy in distinguishing true missing nodes from distractors and in-context variables
- Iterative prompting improves accuracy in predicting multiple mediators, aligning with Chain-of-Thought reasoning
- Providing contextual information about the causal graph is crucial, especially for smaller graphs
- LLMs show particular strength in identifying mediators and colliders, with performance varying across graph domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs can use parametric knowledge from pretraining to infer missing variables in causal graphs by reasoning over graph structure and variable semantics.
- **Mechanism**: LLMs leverage their vast parametric knowledge base to understand causal relationships and infer missing variables when prompted with a partial causal graph. The model uses the known structure and variables to generate hypotheses about missing nodes, applying causal reasoning patterns learned during pretraining.
- **Core assumption**: The LLM has been exposed to sufficient causal knowledge during pretraining to understand variable relationships and can generalize this knowledge to novel graph structures.
- **Evidence anchors**:
  - [abstract]: "Unlike simple knowledge memorization of fixed associations, our task requires the LLM to reason according to the context of the entire graph."
  - [section 4]: "We leverage language models to infer missing variables in a causal DAG"
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- **Break condition**: If the LLM's pretraining data lacks sufficient causal examples or if the graph structure is too novel/unfamiliar, the model may fail to infer meaningful variables.

### Mechanism 2
- **Claim**: LLMs perform better on tasks with increasing complexity when provided with contextual information about the causal graph.
- **Mechanism**: The LLM's performance improves as more context is provided through the prompt, allowing it to better understand the domain and relationships between variables. This is particularly evident when comparing out-of-context identification (Task 1) with in-context identification (Task 2).
- **Core assumption**: The LLM can effectively utilize contextual information to distinguish between relevant and irrelevant variables.
- **Evidence anchors**:
  - [section 5.3]: "Providing context was more important for smaller graphs than larger graphs"
  - [section 5.2]: "GPT-4 and GPT-4o achieve high accuracy and low FNA, showing that they reliably distinguish the true missing node from both distractors and the in-context variable"
  - [corpus]: Weak - limited corpus evidence directly supporting this specific mechanism
- **Break condition**: If the context provided is insufficient or misleading, the LLM may struggle to make accurate inferences, especially in complex graphs.

### Mechanism 3
- **Claim**: Iterative prompting improves LLM performance in hypothesizing multiple missing mediators compared to global prediction.
- **Mechanism**: By prompting the LLM iteratively for each missing mediator, the model can refine its predictions step-by-step, similar to Chain-of-Thought reasoning. This approach allows the model to use previously hypothesized mediators as context for subsequent predictions.
- **Core assumption**: The LLM can maintain consistency across iterative predictions and use the updated graph structure to inform subsequent hypotheses.
- **Evidence anchors**:
  - [section 5.4]: "We adopt an iterative approach for hypothesizing mediators, allowing the model to refine predictions step-by-step"
  - [section 5.4]: "This aligns with Chain-of-Thought (Wei et al., 2022) reasoning and improves accuracy"
  - [corpus]: No direct corpus evidence found for this specific mechanism
- **Break condition**: If the iterative predictions become inconsistent or if the model fails to effectively use the updated graph structure, performance may degrade.

## Foundational Learning

- **Concept: Causal DAG structure**
  - Why needed here: Understanding the basic structure of causal graphs (nodes, edges, paths) is essential for interpreting the task and evaluating LLM performance.
  - Quick check question: What is the difference between a confounder and a mediator in a causal graph?

- **Concept: Backdoor paths and confounding**
  - Why needed here: The ability to identify and control for confounding variables is central to the task of inferring missing causal variables.
  - Quick check question: How does the presence of a backdoor path affect causal inference?

- **Concept: LLM prompting strategies**
  - Why needed here: Different prompting approaches (few-shot, chain-of-thought, iterative) can significantly impact LLM performance on causal reasoning tasks.
  - Quick check question: What is the difference between in-context learning and fine-tuning for LLMs?

## Architecture Onboarding

- **Component map**:
  LLM inference engine -> Causal graph representation and serialization -> Evaluation metrics (semantic similarity, LLM-as-Judge) -> Benchmark datasets (BNLearn repository graphs) -> Prompt template system

- **Critical path**:
  1. Load partial causal graph
  2. Generate appropriate prompt based on task type
  3. Send prompt to LLM
  4. Process LLM response
  5. Evaluate response using semantic similarity and/or LLM-as-Judge
  6. Store results

- **Design tradeoffs**:
  - Using closed vs. open-source LLMs (tradeoff between performance and accessibility)
  - Different graph serialization formats (JSON, GraphML, textual)
  - Evaluation metrics (semantic similarity vs. LLM-as-Judge)
  - Task complexity levels (controlled vs. open-world settings)

- **Failure signatures**:
  - LLM consistently selects out-of-context distractors
  - LLM fails to distinguish between in-context and ground truth variables
  - LLM generates semantically unrelated suggestions
  - Performance degrades significantly on larger or more complex graphs

- **First 3 experiments**:
  1. Task 1 (out-of-context identification) on the Cancer graph with GPT-4 to establish baseline performance
  2. Task 2 (in-context identification) on the Asia graph with Mixtral to test model's ability to distinguish in-context variables
  3. Task 3 (open-world hypothesis) on the Alarm graph with iterative mediator search to evaluate complex reasoning capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs vary when inferring missing variables in graphs with different edge densities or graph sizes beyond the tested datasets?
- Basis in paper: [inferred] The paper mentions Insurance remains challenging due to high edge count, and fine-tuning showed mixed results with small graph sizes limiting feasibility.
- Why unresolved: The study only tested a fixed set of graphs with limited size variation. No systematic analysis was done on how graph density or size affects LLM performance across a broader range.
- What evidence would resolve it: A benchmark with graphs spanning a wide range of sizes and edge densities, measuring LLM performance trends as these parameters vary.

### Open Question 2
- Question: Can the iterative hypothesis generation approach be extended to infer missing variables beyond mediators, such as confounders or colliders, in a similar step-by-step fashion?
- Basis in paper: [explicit] The paper uses iterative hypothesis for mediators (Task 4) and notes the approach aligns with Chain-of-Thought reasoning, but does not explore its application to other variable types.
- Why unresolved: The iterative method is only evaluated for mediators. It is unclear whether the same approach would be effective for inferring confounders or colliders, which may require different reasoning patterns.
- What evidence would resolve it: Experiments applying the iterative framework to graphs where the missing variables are confounders or colliders, comparing performance to baseline methods.

### Open Question 3
- Question: How sensitive are LLM-based causal variable inference results to the choice of prompt template or graph encoding strategy?
- Basis in paper: [explicit] The paper tests different encoding strategies (JSON, GraphML, textual) and observes performance variation, particularly for Task 2 with Mistral and Mixtral models.
- Why unresolved: While some variation was observed, the study did not systematically explore how different prompt structures or encodings affect performance across all models and tasks.
- What evidence would resolve it: A controlled study varying prompt templates and encodings while holding model and graph constant, measuring performance impact across all benchmark tasks.

## Limitations
- Limited generalizability beyond tested domains due to reliance on specific BNLearn repository graphs
- Uncertainty about whether LLMs use parametric knowledge versus genuine causal reasoning, especially for well-known graphs
- Performance variation across graph domains suggests domain-specific knowledge gaps not captured in evaluation metrics

## Confidence
- **Medium confidence**: Overall finding that LLMs can hypothesize missing causal variables, as results show consistent performance across multiple graph structures and task types
- **Low confidence**: Claims about generalizability beyond tested domains, since benchmark relies on specific BNLearn repository graphs and single realistic Alzheimer's graph

## Next Checks
1. **Memorization Assessment**: Conduct controlled experiments using synthetic causal graphs that are unlikely to appear in pretraining data to distinguish between parametric knowledge retrieval and genuine causal reasoning capabilities.

2. **Cross-Domain Generalization**: Test the approach on causal graphs from domains not represented in the BNLearn repository (e.g., ecological, economic, or social network graphs) to evaluate true generalizability beyond the current benchmark.

3. **Iterative Consistency Analysis**: Systematically analyze cases where iterative mediator prediction fails or produces inconsistent results, and develop metrics to quantify the reliability of step-by-step reasoning chains.