---
ver: rpa2
title: Negative Binomial Matrix Completion
arxiv_id: '2408.16113'
source_url: https://arxiv.org/abs/2408.16113
tags:
- poisson
- matrix
- data
- noise
- completion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of matrix completion for count
  data that exhibits overdispersion, where the variance exceeds the mean. While Poisson
  matrix completion assumes equal mean and variance, this work proposes a negative
  binomial (NB) matrix completion model that better handles overdispersed data.
---

# Negative Binomial Matrix Completion

## Quick Facts
- arXiv ID: 2408.16113
- Source URL: https://arxiv.org/abs/2408.16113
- Authors: Yu Lu; Kevin Bui; Roummel F. Marcia
- Reference count: 0
- Primary result: NB matrix completion outperforms Poisson matrix completion on overdispersed count data with PSNR improvements of 1.2-7.6 dB

## Executive Summary
This paper addresses the problem of matrix completion for count data that exhibits overdispersion, where the variance exceeds the mean. While Poisson matrix completion assumes equal mean and variance, this work proposes a negative binomial (NB) matrix completion model that better handles overdispersed data. The authors formulate a maximum a posteriori estimation problem using a nuclear norm regularizer and solve it via proximal gradient descent. Experiments on real datasets including bike-sharing counts, vehicle traffic counts, and microscopy images demonstrate that the NB model consistently outperforms Poisson matrix completion when the data is corrupted by NB noise.

## Method Summary
The paper proposes a negative binomial matrix completion model for count data with overdispersed noise. The method formulates a maximum a posteriori estimation problem using a nuclear norm regularizer to promote low-rank solutions. The optimization is solved using proximal gradient descent with step size adjustment to ensure monotonic decrease in the objective function. The model is evaluated on three real datasets (bike-sharing counts, vehicle traffic counts, and microscopy images) under different noise levels (Poisson, NB r=10, NB r=25) and missing data percentages (25%, 50%, 75%). Performance is measured using PSNR and NRMSE metrics.

## Key Results
- NB matrix completion consistently outperforms Poisson matrix completion on overdispersed count data
- PSNR improvements range from 1.2 to 7.6 dB when data follows NB distribution
- NB model maintains comparable performance to Poisson model when data follows Poisson distribution
- Performance degrades with increasing missing data percentage across all models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negative binomial (NB) matrix completion can handle overdispersed count data better than Poisson matrix completion.
- Mechanism: The NB distribution allows variance to exceed the mean, matching real-world count data characteristics, while Poisson assumes equal mean and variance.
- Core assumption: Count data often exhibits overdispersion where variance exceeds the mean.
- Evidence anchors:
  - [abstract]: "Previous research proposed Poisson matrix completion for count data with noise that follows a Poisson distribution, which assumes that the mean and variance are equal. Since overdispersed count data, whose variance is greater than the mean, is more likely to occur in realistic settings..."
  - [section]: "The NB(r, p) distribution has the following probability mass function... The mean of the NB(r, p) distribution is µ = r(1−p)/p, so p = r/(r + µ). The p.m.f. of the NB(r, p) can be shown to converge to the p.m.f. of the Poisson distribution with mean µ as r → ∞ and p → 1 [26]."
- Break condition: When data is truly Poisson-distributed (variance equals mean), NB may introduce unnecessary complexity without performance gain.

### Mechanism 2
- Claim: The nuclear norm regularization effectively captures low-rank structure in the data matrix.
- Mechanism: Nuclear norm minimization promotes low-rank solutions, which aligns with the assumption that the underlying data matrix has a low-rank structure.
- Core assumption: The ground-truth matrix M has a low-rank structure.
- Evidence anchors:
  - [abstract]: "By Bayes' Theorem, the maximum a posteriori estimate of M is obtained by... we replace the log prior with nuclear norm regularization."
  - [section]: "Because we desire cM to be low rank, we replace the log prior with nuclear norm regularization. Hence, our proposed model to recover the ground-truth matrix M is cM = arg min X F(X) + τ ∥X∥∗, where the nuclear norm ∥ · ∥∗ is the sum of the singular values of a matrix and τ > 0 is a regularization parameter."
- Break condition: When the data matrix is not low-rank, nuclear norm regularization may not effectively recover the true structure.

### Mechanism 3
- Claim: Proximal gradient descent efficiently solves the optimization problem while maintaining monotonic decrease in the objective function.
- Mechanism: The algorithm linearizes the objective function and uses proximal operators to handle the nuclear norm regularization, with step size adjustment ensuring convergence.
- Core assumption: The objective function is convex and can be solved using proximal gradient methods.
- Evidence anchors:
  - [abstract]: "We solve (5) by proximal gradient descent... In each iteration k, we linearize F and solve the following optimization problem..."
  - [section]: "Because 1/tk is the step size in the gradient descent step in (7), we increase tk by a factor η > 0 whenever F(Xk+1) > F(Xk) to ensure that {F(Xk)}∞k=0 is monotonically decreasing."
- Break condition: When the step size adjustment fails to ensure monotonic decrease, convergence may be compromised.

## Foundational Learning

- Concept: Negative Binomial Distribution
  - Why needed here: Understanding the NB distribution is crucial as it forms the basis for modeling overdispersed count data.
  - Quick check question: What is the relationship between the mean and variance in a Negative Binomial distribution compared to a Poisson distribution?

- Concept: Matrix Completion and Low-Rank Structure
  - Why needed here: The algorithm assumes the underlying data matrix has a low-rank structure, which is a key assumption in matrix completion.
  - Quick check question: Why is the nuclear norm used as a regularizer in this matrix completion problem?

- Concept: Proximal Gradient Descent
  - Why needed here: This optimization method is used to solve the non-smooth optimization problem involving the nuclear norm.
  - Quick check question: How does the proximal operator handle the nuclear norm regularization in the optimization process?

## Architecture Onboarding

- Component map: Data Input -> Noise Model (NB) -> Optimization Objective -> Solver (Proximal Gradient Descent) -> Recovered Matrix

- Critical path:
  1. Initialize matrix with observed entries
  2. Compute gradient of negative log-likelihood
  3. Apply proximal operator with nuclear norm regularization
  4. Adjust step size if objective increases
  5. Repeat until convergence

- Design tradeoffs:
  - NB vs Poisson: NB handles overdispersion but adds complexity
  - Nuclear norm vs other regularizers: Effective for low-rank recovery but may not suit all data structures
  - Proximal gradient vs other solvers: Efficient for large-scale problems but requires careful step size tuning

- Failure signatures:
  - Non-monotonic decrease in objective function despite step size adjustment
  - Poor recovery performance when true data is Poisson-distributed
  - Slow convergence or failure to converge for highly corrupted data

- First 3 experiments:
  1. Test on synthetic data with known NB distribution and varying dispersion parameters
  2. Compare performance with Poisson model on overdispersed real-world count data
  3. Evaluate sensitivity to missing data percentage and noise levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of NB matrix completion scale with the rank of the underlying ground-truth matrix?
- Basis in paper: [inferred] The experiments use a fixed low-rank approximation (rank 32) for the microscopy image dataset, but do not explore how performance varies with different ranks.
- Why unresolved: The paper does not systematically investigate the relationship between matrix rank and completion accuracy under different noise models.
- What evidence would resolve it: A series of experiments varying the rank of the ground-truth matrix while keeping other parameters fixed would reveal how rank affects NB vs Poisson model performance.

### Open Question 2
- Question: What is the impact of different missing data patterns (e.g., random vs structured missingness) on the relative performance of NB and Poisson matrix completion?
- Basis in paper: [inferred] The experiments use uniform random masking at different percentages, but do not explore non-random missing data patterns.
- Why unresolved: Real-world missing data often follows non-random patterns, and the paper's uniform random masking may not capture these scenarios.
- What evidence would resolve it: Experiments comparing NB and Poisson completion on datasets with various structured missingness patterns (e.g., block missingness, temporal gaps) would clarify model robustness.

### Open Question 3
- Question: How sensitive is the NB matrix completion algorithm to the choice of dispersion parameter r, especially when r must be estimated from data?
- Basis in paper: [explicit] The paper acknowledges that several methods exist for estimating r but intentionally uses the exact value to avoid bias, noting this as a limitation.
- Why unresolved: The paper does not evaluate performance when r is estimated from noisy data, which is the practical scenario.
- What evidence would resolve it: Experiments comparing completion accuracy when using true r, estimated r from clean data, and estimated r from noisy incomplete data would quantify estimation sensitivity.

## Limitations

- The assumption of low-rank structure in the data matrix may not hold for all real-world count data scenarios
- Hyperparameter selection (regularization parameter τ and dispersion parameter r) is not thoroughly explored
- Limited empirical validation across diverse datasets and comparison with state-of-the-art methods

## Confidence

- **High Confidence:** The theoretical framework of using negative binomial distribution for overdispersed count data is well-established in statistics literature, and the proximal gradient descent algorithm is a standard optimization approach for nuclear norm regularized problems.
- **Medium Confidence:** The experimental results showing improved PSNR on synthetic and real datasets are convincing, but the sample size and diversity of datasets are limited, warranting further validation.
- **Low Confidence:** The assumption that the ground-truth matrix has a low-rank structure and that this can be effectively captured by nuclear norm regularization is not sufficiently justified for all types of count data, especially those with complex underlying structures.

## Next Checks

1. **Extended Dataset Evaluation:** Apply the NB matrix completion model to a wider variety of count datasets, including those with different levels of overdispersion and non-low-rank structures, to assess its robustness and generalization capabilities.

2. **Hyperparameter Sensitivity Analysis:** Conduct a thorough sensitivity analysis of the model's performance with respect to the regularization parameter τ and dispersion parameter r across different datasets and noise levels to determine optimal settings and understand their impact.

3. **Comparison with Advanced Methods:** Benchmark the NB matrix completion model against recent advanced matrix completion techniques, such as those incorporating deep learning or non-parametric Bayesian approaches, to establish its competitive advantage or identify areas for improvement.