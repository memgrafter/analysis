---
ver: rpa2
title: 'LINKAGE: Listwise Ranking among Varied-Quality References for Non-Factoid
  QA Evaluation via LLMs'
arxiv_id: '2409.14744'
source_url: https://arxiv.org/abs/2409.14744
tags:
- answer
- answers
- evaluation
- question
- reference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LINKAGE, a listwise ranking approach for non-factoid
  question answering (NFQA) evaluation using large language models (LLMs). Instead
  of pointwise scoring or pairwise comparisons, LINKAGE ranks a candidate answer among
  a list of reference answers sorted by quality.
---

# LINKAGE: Listwise Ranking among Varied-Quality References for Non-Factoid QA Evaluation via LLMs

## Quick Facts
- arXiv ID: 2409.14744
- Source URL: https://arxiv.org/abs/2409.14744
- Reference count: 11
- Primary result: LINKAGE achieves higher correlation with human judgments than automatic metrics and common LLM evaluation methods for non-factoid QA

## Executive Summary
This paper introduces LINKAGE, a novel listwise ranking approach for evaluating non-factoid question answering (NFQA) using large language models (LLMs). Unlike traditional pointwise scoring or pairwise comparison methods, LINKAGE ranks candidate answers among a list of reference answers sorted by quality. The method is particularly valuable when ground truth answers of multiple grades are unavailable, as it leverages LLMs to generate varied-quality reference answers. Experiments on three NFQA datasets demonstrate that LINKAGE significantly outperforms established automatic metrics like ROUGE and BERTScore, as well as common pointwise and pairwise LLM evaluation methods, showing stronger correlation with human judgments.

## Method Summary
LINKAGE implements a listwise ranking framework where candidate answers are evaluated by their relative position among reference answers of varying quality. The core innovation is the use of varied-quality reference sets rather than single reference answers or pairwise comparisons. When multiple grades of ground truth are unavailable, the method employs LLMs to generate reference answers spanning a quality spectrum. The ranking process positions the candidate answer within this reference list, with higher positions indicating better quality. This approach captures the nuanced nature of non-factoid answers where multiple valid responses of different quality levels may exist for a single question.

## Key Results
- LINKAGE achieves significantly higher correlation with human judgments compared to automatic metrics (ROUGE, BERTScore) and common LLM evaluation methods
- The method demonstrates effectiveness across three NFQA datasets: ANTIQUE, TREC-DL-NF, and WebGLM
- LINKAGE maintains strong performance even when only single-grade or no ground truth answers are available

## Why This Works (Mechanism)
LINKAGE works by leveraging the listwise ranking paradigm to capture the relative quality relationships among answers. Traditional pointwise methods assign independent scores to each answer without considering the quality spectrum, while pairwise methods can only compare two answers at a time. The listwise approach naturally accommodates the multi-faceted nature of non-factoid answers where multiple valid responses of varying quality may exist. By generating or utilizing reference answers of different quality levels, LINKAGE creates a quality continuum that allows for more nuanced evaluation of candidate answers.

## Foundational Learning

**Listwise Ranking**: Ranking methods that consider a set of items simultaneously rather than pairwise or pointwise comparisons
- Why needed: Captures the relative quality relationships among multiple answers simultaneously
- Quick check: Can you explain how listwise ranking differs from pairwise ranking in terms of computational complexity?

**Non-Factoid QA Evaluation**: Assessment of answers to questions requiring descriptive, explanatory, or opinion-based responses
- Why needed: Traditional factoid QA metrics fail to capture the quality nuances in open-ended responses
- Quick check: What makes non-factoid QA evaluation more challenging than factoid QA evaluation?

**Reference Answer Generation**: Using LLMs to create synthetic reference answers of varying quality levels
- Why needed: Addresses the scarcity of multi-grade ground truth in real-world NFQA datasets
- Quick check: How might noise in LLM-generated references affect evaluation reliability?

## Architecture Onboarding

Component Map: Question -> Candidate Answer + Reference Set -> LINKAGE Ranking Model -> Quality Score

Critical Path: The core pipeline involves generating or obtaining varied-quality references, ranking the candidate among these references, and producing a final quality assessment. The most computationally intensive step is reference generation when ground truth is unavailable.

Design Tradeoffs: The method trades computational efficiency for more nuanced evaluation. Generating multiple quality-level references is more expensive than using single references, but provides richer evaluation context.

Failure Signatures: Performance degradation may occur when reference generation produces biased or unrepresentative quality distributions, or when the candidate answer quality falls outside the range of generated references.

First Experiments:
1. Replicate baseline results using ROUGE and BERTScore on one NFQA dataset
2. Implement pairwise LLM comparison method as a direct LINKAGE competitor
3. Test LINKAGE with manually created reference sets of varying quality to isolate reference generation effects

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily depends on the quality of reference answer generation, with insufficient investigation of how noise affects reliability
- Results only tested on three NFQA datasets, raising questions about generalizability to other domains
- Computational cost of ranking candidates among multiple references may limit practical deployment in resource-constrained settings

## Confidence

| Claim | Confidence |
|-------|------------|
| Mathematical formulation and experimental design | High |
| Practical superiority over baselines | Medium |
| LINKAGE works well with degraded reference quality | Low |

## Next Checks

1. Test LINKAGE's robustness by deliberately injecting noise or bias into reference answer generation and measuring degradation in evaluation accuracy
2. Evaluate the method across additional NFQA domains (e.g., medical, technical support, creative writing) to assess generalizability beyond the current datasets
3. Benchmark computational efficiency and cost trade-offs against baseline methods at scale, including runtime analysis and memory requirements for production deployment