---
ver: rpa2
title: Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables
arxiv_id: '2402.13028'
source_url: https://arxiv.org/abs/2402.13028
tags:
- evidence
- graph
- information
- fact
- heterfc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces HeterFC, a novel word-level heterogeneous-graph-based
  model for fact checking that combines unstructured and structured information. The
  model constructs a graph with words as nodes and three types of edges: intra-sentence,
  intra-table, and inter-evidence.'
---

# Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables

## Quick Facts
- arXiv ID: 2402.13028
- Source URL: https://arxiv.org/abs/2402.13028
- Reference count: 13
- Primary result: HeterFC achieves Feverous score of 0.3714 and label accuracy of 0.7352 on FEVEROUS dataset

## Executive Summary
This paper introduces HeterFC, a novel word-level heterogeneous-graph-based model for fact checking that combines unstructured text and structured table information. The model constructs a graph with words as nodes and three types of edges (intra-sentence, intra-table, inter-evidence), then uses relational graph convolutional networks for information propagation and attention mechanisms for claim-evidence interaction. Experiments on the FEVEROUS dataset demonstrate that HeterFC outperforms existing transformer-based and graph-based baselines, achieving significant improvements in both Feverous score and label accuracy.

## Method Summary
HeterFC builds a word-level heterogeneous evidence graph where each word is a node, and three types of edges capture different relationships: intra-sentence (within text sentences), intra-table (within table cells), and inter-evidence (between different evidence pieces). The model employs Relational Graph Convolutional Networks (R-GCN) to propagate information through this graph structure, followed by an attention-based claim-evidence interaction module. A multitask loss function combines classification loss with an assisted loss that helps the model distinguish valid from invalid evidence. The approach is evaluated on the FEVEROUS dataset using both effective and simpler evidence retrieval methods, showing consistent superiority over baseline models.

## Key Results
- HeterFC achieves Feverous score of 0.3714 and label accuracy of 0.7352 on FEVEROUS dataset
- Outperforms transformer-based and graph-based baselines by significant margins
- Demonstrates robustness across different evidence retrieval methods
- Ablation studies confirm effectiveness of word-level graph, heterogeneous edges, and attention-based interaction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The word-level heterogeneous graph captures fine-grained semantic relationships between words in both structured and unstructured evidence
- Mechanism: By treating each word as a node with three edge types (intra-sentence, intra-table, inter-evidence), the model propagates information across evidence types and captures relationships lost in sentence-level approaches
- Core assumption: Words carry sufficient semantic information for fact checking, and their relationships can be effectively modeled through graph structures
- Evidence anchors:
  - [abstract]: "Our approach leverages a heterogeneous evidence graph, with words as nodes and thoughtfully designed edges representing different evidence properties."
  - [section]: "We treat each word in the evidence as a node on the evidence graph, since it contains more fine-grained semantic information than the sentence."

### Mechanism 2
- Claim: The relational graph convolutional network (R-GCN) effectively propagates information through the heterogeneous graph
- Mechanism: R-GCN assigns trainable weight matrices to each relation type, learning different propagation strategies based on edge type
- Core assumption: Different types of relationships between words require different propagation strategies, and R-GCN can learn these strategies
- Evidence anchors:
  - [abstract]: "We perform information propagation via a relational graph neural network, facilitating interactions between claims and evidence."
  - [section]: "We employ the relational graph convolutional network (R-GCN) to perform neighborhood propagation and readout the representations of each evidence."

### Mechanism 3
- Claim: The attention-based claim-evidence interaction module helps focus on relevant evidence
- Mechanism: Attention scores between claim and each evidence piece allow the model to weigh importance and aggregate evidence accordingly, with assisted loss improving discrimination of valid/invalid evidence
- Core assumption: Not all evidence is equally relevant, and the model can learn appropriate weights for each piece
- Evidence anchors:
  - [abstract]: "An attention-based method is utilized to integrate information, combined with a language model for generating predictions."
  - [section]: "We introduce an attention-based claim-evidence interaction module. In detail, we compute the importance score αm for the m-th evidence regarding the claim based on an attention mechanism."

## Foundational Learning

- Concept: Graph neural networks (GNNs) and their variants (e.g., R-GCN)
  - Why needed here: To model complex relationships between words in evidence and propagate information through heterogeneous graph
  - Quick check question: What is the main difference between a standard GNN and an R-GCN, and why is it important for this task?

- Concept: Attention mechanisms in neural networks
  - Why needed here: To compute importance scores between claim and evidence, allowing focus on relevant evidence and distinguishing valid/invalid evidence
  - Quick check question: How does the attention mechanism in this model differ from standard self-attention, and what is its role in the overall architecture?

- Concept: Pretrained language models (PLMs) and their fine-tuning
  - Why needed here: To generate initial node representations for words and provide final veracity prediction based on aggregated graph information
  - Quick check question: What are the benefits of using a PLM like RoBERTa for this task, and how does it complement the graph-based approach?

## Architecture Onboarding

- Component map: Word-level evidence graph construction -> R-GCN information propagation -> Evidence-level representation readout -> Attention-based claim-evidence interaction -> Fused veracity prediction -> Model training

- Critical path:
  1. Construct word-level evidence graph
  2. Propagate information through graph using R-GCN
  3. Readout evidence-level representations
  4. Compute attention scores between claim and evidence
  5. Generate graph representation and PLM output
  6. Combine representations and predict veracity

- Design tradeoffs:
  - Word-level vs. sentence-level vs. token-level graph granularity
  - Number of R-GCN layers (k) and impact on information aggregation
  - Balancing parameter (β) for assisted loss
  - Choice of PLM and impact on initial node representations and final predictions

- Failure signatures:
  - Poor performance on claims requiring complex reasoning across multiple evidence pieces
  - Inability to capture fine-grained semantic relationships between words
  - Overfitting to specific evidence patterns or claim types

- First 3 experiments:
  1. Evaluate impact of different graph granularities (word-level, sentence-level, token-level) on model performance
  2. Investigate effect of varying R-GCN layer count (k) on model's ability to capture multi-hop relationships
  3. Assess importance of assisted loss by comparing performance with and without this component

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but raises several implicit ones:
- How does performance compare when using different retrieval methods with varying levels of evidence noise?
- What is the impact of varying window size for intra-sentence and intra-table edges on performance?
- How does performance scale with size of evidence set, particularly with many pieces of evidence?

## Limitations

- Evaluation relies on FEVEROUS dataset which may not represent real-world fact-checking scenarios
- Evidence retrieval pipeline's effectiveness significantly impacts performance
- Model complexity (word-level granularity with R-GCN) may lead to scalability challenges for larger documents
- Assisted loss component shows improvement but lacks extensive ablation studies

## Confidence

- High confidence: Model architecture and core components (R-GCN, attention mechanisms) due to clear methodological descriptions and established foundations
- Medium confidence: Effectiveness of word-level graph construction, as improved performance is shown but benefits over alternative granularities could be more thoroughly explored
- Low confidence: Scalability claims, as the paper doesn't extensively test on larger datasets or document collections

## Next Checks

1. **Ablation Study Expansion**: Conduct comprehensive ablation studies testing different graph granularities and varying R-GCN layer counts to quantify exact contribution of each architectural choice

2. **Cross-Dataset Evaluation**: Evaluate HeterFC on additional fact-checking datasets beyond FEVEROUS to assess generalization across different domains and evidence types

3. **Scalability Analysis**: Test model's performance and efficiency on larger document collections with more extensive evidence sets to validate scalability claims and identify potential bottlenecks