---
ver: rpa2
title: 'The Brittleness of AI-Generated Image Watermarking Techniques: Examining Their
  Robustness Against Visual Paraphrasing Attacks'
arxiv_id: '2408.10446'
source_url: https://arxiv.org/abs/2408.10446
tags:
- image
- visual
- watermarking
- watermark
- paraphrase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the robustness of current AI image watermarking
  techniques against visual paraphrase attacks. The authors propose a two-step visual
  paraphrasing method: first generating a caption for the watermarked image using
  KOSMOS-2, then creating a visually similar paraphrase guided by this caption using
  an image-to-image diffusion system.'
---

# The Brittleness of AI-Generated Image Watermarking Techniques: Examining Their Robustness Against Visual Paraphrasing Attacks

## Quick Facts
- arXiv ID: 2408.10446
- Source URL: https://arxiv.org/abs/2408.10446
- Reference count: 25
- This paper investigates the robustness of current AI image watermarking techniques against visual paraphrase attacks.

## Executive Summary
This paper investigates the robustness of AI-generated image watermarking techniques against visual paraphrase attacks. The authors propose a two-step visual paraphrasing method that first generates a caption for the watermarked image using KOSMOS-2, then creates a visually similar paraphrase guided by this caption using an image-to-image diffusion system. They evaluate six state-of-the-art watermarking methods against various attacks including their proposed visual paraphrase attack. Results show that watermark detection rates significantly decrease with visual paraphrasing, particularly at higher paraphrase strengths (s ≥ 0.6). The attack successfully removes watermarks while maintaining acceptable semantic distortion, as measured by CMMD scores.

## Method Summary
The study evaluates six state-of-the-art watermarking methods (DwtDctSVD, HiDDen, Stable Signature, Tree Ring, ZoDiac, Gaussian Shading) against visual paraphrase attacks. The attack pipeline uses KOSMOS-2 to generate captions for watermarked images, then employs an image-to-image diffusion system guided by these captions to create paraphrased images. The effectiveness is measured through watermark detection rates and Continuous Metric Matching Distance (CMMD) scores for semantic distortion. The study tests on three datasets (MS COCO, DiffusionDB, WikiArt) with varying paraphrasing strengths and guidance scales.

## Key Results
- Visual paraphrasing significantly reduces watermark detection rates, especially at higher paraphrase strengths (s ≥ 0.6)
- Gaussian Shading and Tree Ring watermarking methods show relatively higher resilience to visual paraphrase attacks
- The attack maintains acceptable semantic distortion while effectively removing watermarks, as measured by CMMD scores
- The authors release their visual paraphrase dataset and code for further research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual paraphrase attacks remove watermarks by generating semantically equivalent images with altered visual details.
- Mechanism: The attack pipeline generates a caption for the watermarked image using KOSMOS-2, then uses this caption to guide an image-to-image diffusion model in creating a new image that preserves the semantic content but alters the visual representation, effectively removing the embedded watermark.
- Core assumption: The watermark is embedded in the visual details rather than the semantic content of the image.
- Evidence anchors:
  - [abstract] "The proposed visual paraphraser operates in two steps. First, it generates a caption for the given image using KOSMOS-2... Second, it passes both the original image and the generated caption to an image-to-image diffusion system."
  - [section] "The resulting image is a visual paraphrase and is free of any watermarks."
- Break condition: If watermarks are embedded in the semantic content or if the watermarking method makes the watermark robust to semantic-preserving transformations.

### Mechanism 2
- Claim: The effectiveness of visual paraphrase attacks increases with higher paraphrasing strength values.
- Mechanism: As the paraphrasing strength (s) increases, the diffusion model has more creative freedom to deviate from the original image while maintaining semantic similarity, making it more likely to remove the watermark.
- Core assumption: Higher paraphrasing strength values allow for greater visual variation while preserving semantic content.
- Evidence anchors:
  - [section] "A higher strength value allows the model greater creative latitude, enabling it to produce an image that significantly deviates from the original."
  - [section] "As the intensity of paraphrasing increases, we observe a significant decline in the ability to detect and extract the original watermarks."
- Break condition: If watermarking methods are designed to be robust against transformations that preserve semantic content.

### Mechanism 3
- Claim: Different watermarking methods show varying levels of resilience to visual paraphrase attacks.
- Mechanism: The structure and location of the watermark in the image determine its susceptibility to visual paraphrasing. Watermarks embedded in ways that are less dependent on specific visual features are more resilient.
- Core assumption: The resilience of watermarking methods depends on how the watermark is embedded and what features it relies on.
- Evidence anchors:
  - [section] "Gaussian Shading and Tree Ring watermarking methods show relatively higher resilience to visual paraphrase attacks."
  - [section] "The detectability rate is a crucial metric in assessing the effectiveness of watermark detection methods after visual paraphrasing. Our experiments reveal a clear inverse relationship between the strength of visual paraphrasing and the detectability of watermarks."
- Break condition: If all watermarking methods rely on similar embedding strategies or if visual paraphrasing can be adapted to target specific watermark structures.

## Foundational Learning

- Concept: Image-to-image diffusion models
  - Why needed here: The attack relies on using diffusion models to generate paraphrased images while preserving semantic content.
  - Quick check question: How do image-to-image diffusion models differ from text-to-image diffusion models in their approach to image generation?

- Concept: Semantic preservation in image transformations
  - Why needed here: The attack must maintain the semantic content of the image while altering its visual representation to remove the watermark.
  - Quick check question: What metrics can be used to measure semantic preservation in image transformations, and how do they differ from visual similarity metrics?

- Concept: Watermark embedding techniques
  - Why needed here: Understanding different watermarking methods helps in analyzing their vulnerabilities to visual paraphrase attacks.
  - Quick check question: What are the key differences between static and learning-based watermarking methods, and how might these differences affect their robustness to visual paraphrasing?

## Architecture Onboarding

- Component map:
  KOSMOS-2 Caption Generator -> Image-to-Image Diffusion System -> Watermark Detection Module -> Evaluation Framework

- Critical path:
  1. Input watermarked image
  2. Generate caption using KOSMOS-2
  3. Apply image-to-image diffusion with caption guidance
  4. Evaluate watermark detectability and semantic distortion
  5. Adjust paraphrasing strength and guidance scale for optimal results

- Design tradeoffs:
  - Higher paraphrasing strength increases watermark removal but may introduce more semantic distortion
  - Lower guidance scale allows for more creative variations but may deviate from the original image content
  - Balancing watermark removal effectiveness with acceptable semantic distortion

- Failure signatures:
  - High CMMD scores indicating significant semantic distortion
  - Persistent watermark detection despite high paraphrasing strength
  - Generation of images that deviate too far from the original semantic content

- First 3 experiments:
  1. Test the attack on a single watermarking method (e.g., Stable Signature) with varying paraphrasing strengths (s = 0.2, 0.4, 0.6, 0.8, 1.0) and record watermark detection rates and CMMD scores.
  2. Compare the effectiveness of visual paraphrase attacks against different watermarking methods (e.g., Stable Signature, Tree Ring, Gaussian Shading) using the same set of images and paraphrasing parameters.
  3. Evaluate the impact of different guidance scale values on the effectiveness of watermark removal and semantic preservation for a specific watermarking method.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics of Tree-Ring and ZoDiac watermark patterns in Fourier space contribute to their resilience against visual paraphrase attacks?
- Basis in paper: [explicit] The paper notes that Tree-Ring and ZoDiac exhibit distinct ring structures in Fourier space, while Gaussian Shading does not, and questions the exact contribution of these characteristics to resilience.
- Why unresolved: The authors explicitly state that "The exact contribution of these characteristics to the resilience of the watermarks against paraphrase attacks remains unclear at this stage."
- What evidence would resolve it: Systematic ablation studies removing or modifying the ring structures in Fourier space while maintaining other aspects of the watermarking methods, followed by testing against visual paraphrase attacks.

### Open Question 2
- Question: How do different image-to-image diffusion models (beyond Stable Diffusion) compare in their effectiveness at removing watermarks while maintaining semantic content?
- Basis in paper: [explicit] The authors mention that "While we acknowledge the potential of other image-to-image diffusion models and generative AI systems for visual paraphrasing, we haven't yet extensively tested them."
- Why unresolved: The paper focuses exclusively on Stable Diffusion for practical reasons, leaving a gap in understanding how other models might perform.
- What evidence would resolve it: Comparative experiments using multiple diffusion models (e.g., Midjourney, DALL-E, Imagen) with identical visual paraphrase parameters, measuring both watermark removal effectiveness and semantic preservation.

### Open Question 3
- Question: What is the relationship between the guidance scale parameter and the ability to maintain semantic content during visual paraphrasing across different watermarking techniques?
- Basis in paper: [inferred] The paper shows that detectability decreases with guidance scale values above certain thresholds (10-12 depending on technique) but doesn't systematically explore the optimal range for semantic preservation.
- Why unresolved: While the paper identifies optimal ranges for different techniques, it doesn't establish why these ranges differ or what underlying factors determine the relationship between guidance scale and semantic preservation.
- What evidence would resolve it: Detailed analysis of how guidance scale affects the CLIP embedding distance between original and paraphrased images across watermarking methods, potentially revealing whether the relationship is technique-specific or follows general principles.

## Limitations

- The evaluation framework depends heavily on KOSMOS-2 caption quality, which may not always capture all relevant semantic details needed for effective paraphrasing
- The study focuses primarily on static watermarking methods, leaving the robustness of learning-based watermarking approaches unexplored
- CMMD scores may not fully capture perceptual quality differences that human evaluators would notice

## Confidence

- High Confidence: The inverse relationship between paraphrasing strength and watermark detection rates is well-supported by experimental data showing consistent degradation across all tested watermarking methods
- Medium Confidence: Claims about the relative resilience of Gaussian Shading and Tree Ring methods are based on limited comparisons and may not generalize across different image types or attack configurations
- Low Confidence: The assertion that watermarks are primarily embedded in visual rather than semantic features is plausible but not directly tested, as the study doesn't examine the internal structure of different watermarking methods

## Next Checks

1. Test the visual paraphrase attack on additional datasets beyond COCO, DiffusionDB, and WikiArt to verify that results generalize across different image types and distributions

2. Systematically evaluate how variations in KOSMOS-2 caption quality affect attack effectiveness by comparing results using human-generated captions versus automated captions

3. Implement and test a simple adaptive watermarking method that modifies embedding based on detected paraphrasing attempts to assess whether such approaches can improve robustness