---
ver: rpa2
title: 'AutoEval Done Right: Using Synthetic Data for Model Evaluation'
arxiv_id: '2403.07008'
source_url: https://arxiv.org/abs/2403.07008
tags:
- data
- classical
- labeled
- confidence
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AutoEval, a method for evaluating machine learning
  models using synthetic data to reduce human annotation costs. The core idea is to
  combine a small human-labeled dataset with a large AI-labeled synthetic dataset
  to produce statistically principled, unbiased estimates of model performance metrics.
---

# AutoEval Done Right: Using Synthetic Data for Model Evaluation

## Quick Facts
- **arXiv ID**: 2403.07008
- **Source URL**: https://arxiv.org/abs/2403.07008
- **Reference count**: 40
- **One-line primary result**: AutoEval achieves up to 50% improvement in effective sample size by combining human-labeled and synthetic data using prediction-powered inference debiasing.

## Executive Summary
AutoEval presents a statistically principled method for evaluating machine learning models using synthetic data to reduce human annotation costs. The approach combines a small human-labeled dataset with a large AI-labeled synthetic dataset, using prediction-powered inference (PPI) to debias the synthetic data estimates. By treating synthetic predictions as control variates, AutoEval can increase the effective sample size of human data by up to 50% compared to classical approaches, while maintaining unbiased estimates and calibrated confidence intervals. The method demonstrates robust performance across diverse domains including image classification, protein fitness prediction, and language model pairwise comparisons.

## Method Summary
AutoEval uses prediction-powered inference to combine human-labeled and synthetic data for model evaluation. The method computes bias estimates from the small human-labeled dataset, then applies these to correct the large synthetic dataset, effectively reducing variance while maintaining unbiasedness. The optimal weighting parameter λ is chosen to minimize estimator variance, allowing AutoEval to asymptotically outperform classical approaches. The implementation is available through the PPI++ Python package, which handles the core debiasing algorithm and confidence interval calculation.

## Key Results
- On ImageNet, PPI++ reduced mean-squared error in accuracy estimation and improved model ranking correlation by 50%
- For protein fitness prediction, PPI++ achieved 50% higher effective sample size and five-fold improvement in ranking correlation
- For LLM pairwise comparisons, PPI++ improved effective sample size by 20-25% and produced more accurate model rankings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Synthetic data can be used to reduce the variance of performance estimates without introducing bias, provided the bias of synthetic labels is properly corrected.
- **Mechanism**: The method computes an estimate of the bias of synthetic labels using the small human-labeled dataset, then subtracts this bias from the large synthetic dataset to obtain an unbiased estimate with lower variance.
- **Core assumption**: The bias of synthetic labels is estimable from a small amount of human-labeled data, and this bias is constant across the synthetic dataset.
- **Evidence anchors**:
  - [abstract] "These algorithms increase the effective human-labeled sample size by up to 50% on experiments with gpt-4."
  - [section] "The second term corrects this bias by calculating it on the labeled dataset and subtracting it off."
- **Break condition**: If the annotator model's bias varies significantly across the data distribution, or if the human-labeled dataset is too small to reliably estimate this bias, the debiasing will fail.

### Mechanism 2
- **Claim**: Prediction-Powered Inference (PPI) allows for statistically valid combination of human and synthetic data by treating synthetic predictions as control variates.
- **Mechanism**: PPI uses the synthetic predictions to construct a control variate that reduces variance while maintaining unbiasedness through careful weighting.
- **Core assumption**: The synthetic predictions are correlated with the true labels but have known bias that can be estimated.
- **Evidence anchors**:
  - [abstract] "The core statistical tool used for this debiasing is called prediction-powered inference (PPI) [4]"
  - [section] "The core statistical tool used for this debiasing is called prediction-powered inference (PPI) [4]; we will describe this tool in detail in the coming text."
- **Break condition**: If synthetic predictions are uncorrelated with true labels, the control variate will not reduce variance.

### Mechanism 3
- **Claim**: The optimal weighting parameter λ can be chosen to minimize the variance of the combined estimator.
- **Mechanism**: By tuning λ, the method can asymptotically achieve lower variance than using either dataset alone.
- **Core assumption**: The variance of the combined estimator can be explicitly calculated and minimized with respect to λ.
- **Evidence anchors**:
  - [section] "This is formally true for the optimally chosen parameter λ; indeed, the optimal choice of λ ensures that our estimator is always better than ˆµclassical (in an asymptotic sense)."
  - [section] "It also indicates that we should pick λ to minimize V in the appropriate sense."
- **Break condition**: If the optimal λ is very close to 0 or 1, the method provides little benefit over classical approaches.

## Foundational Learning

- **Concept**: Understanding of bias-variance tradeoff
  - Why needed here: The method explicitly trades off bias (controlled through debiasing) against variance (reduced through synthetic data)
  - Quick check question: What happens to the variance of an estimator when we add a correlated but biased control variate?

- **Concept**: Familiarity with control variates in statistics
  - Why needed here: PPI works by treating synthetic predictions as control variates that reduce variance while maintaining unbiasedness
  - Quick check question: How does a control variate reduce variance in an estimator?

- **Concept**: Bradley-Terry model for pairwise comparisons
  - Why needed here: The method extends to ranking models using pairwise preferences via the Bradley-Terry framework
  - Quick check question: What is the relationship between the Bradley-Terry coefficients and the probability of one model beating another?

## Architecture Onboarding

- **Component map**: Human-labeled dataset (small) -> Synthetic dataset (large) -> Annotator model (produces synthetic labels) -> PPI++ algorithm (combines data and produces estimates) -> Confidence interval calculator

- **Critical path**: 
  1. Load human and synthetic data
  2. Compute synthetic predictions for both datasets
  3. Calculate bias estimate using human data
  4. Combine using PPI++ with optimal λ
  5. Compute confidence intervals

- **Design tradeoffs**:
  - More synthetic data improves variance reduction but requires better annotator models
  - Larger human datasets improve bias estimation but reduce the benefit of synthetic data
  - Simple metrics (accuracy) are easier to implement than complex ones (correlation, Bradley-Terry)

- **Failure signatures**:
  - High variance in estimates despite large synthetic dataset → annotator model may be poor
  - Confidence intervals that don't contain true values → bias estimation may be unreliable
  - Optimal λ near 0 → synthetic data provides little benefit

- **First 3 experiments**:
  1. Implement basic PPI for accuracy estimation on a small image classification task
  2. Compare variance reduction with different amounts of synthetic data
  3. Test on a simple regression task using a held-out model as annotator

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AutoEval perform when the synthetic data distribution is highly biased or systematically incorrect compared to the true data distribution?
- Basis in paper: [explicit] The paper mentions that even with mediocre annotator models, PPI++ performs at least as well as the classical approach, but does not extensively explore scenarios with highly biased synthetic data.
- Why unresolved: The paper only briefly touches on the performance of AutoEval with poor annotator models, showing that it falls back to the classical approach when synthetic labels do not correlate with true labels. However, it does not provide detailed analysis or experiments on scenarios with highly biased or systematically incorrect synthetic data.
- What evidence would resolve it: Detailed experiments comparing AutoEval's performance with highly biased synthetic data against classical approaches, including metrics like effective sample size, mean-squared error, and model ranking correlation.

### Open Question 2
- Question: Can AutoEval be extended to handle non-i.i.d. data distributions, such as time-series or spatially correlated data?
- Basis in paper: [inferred] The paper mentions that extensions to some limited non-i.i.d. regimes are handled in [4], but does not discuss these extensions in detail or provide experimental results for non-i.i.d. data.
- Why unresolved: The paper focuses on i.i.d. data assumptions and does not explore how AutoEval performs or needs to be modified for non-i.i.d. data distributions. The reference to [4] suggests that such extensions exist but are not detailed in the current paper.
- What evidence would resolve it: Experimental results showing AutoEval's performance on non-i.i.d. datasets, along with modifications to the algorithm needed to handle such data, and comparison with classical approaches in these scenarios.

### Open Question 3
- Question: How does the performance of AutoEval scale with the dimensionality of the input data and the complexity of the models being evaluated?
- Basis in paper: [inferred] The paper presents experiments on image classification, protein fitness prediction, and language model pairwise comparisons, but does not systematically explore how performance scales with input dimensionality or model complexity.
- Why unresolved: The experiments in the paper use datasets and models of varying complexity, but do not provide a systematic study of how AutoEval's performance metrics (e.g., effective sample size, mean-squared error) scale with these factors. The computational complexity of AutoEval is also not discussed in relation to data dimensionality.
- What evidence would resolve it: A comprehensive study varying input data dimensionality and model complexity, with corresponding results on AutoEval's performance metrics and computational efficiency, compared to classical approaches.

## Limitations
- The method's performance heavily depends on the quality of the annotator model; poor annotator models can significantly degrade results
- Assumes constant bias across the dataset, which may not hold for complex data distributions
- Computational overhead of running annotator models on large synthetic datasets could be prohibitive for resource-constrained applications

## Confidence

- **High confidence**: The core PPI debiasing mechanism and its ability to reduce variance while maintaining unbiasedness is well-established in statistics literature
- **Medium confidence**: The empirical results showing 50% effective sample size improvement across three different domains are compelling but rely on the assumption that GPT-4 provides ground truth labels
- **Medium confidence**: The extension to complex metrics like Pearson correlation and Bradley-Terry coefficients is demonstrated but requires additional implementation complexity

## Next Checks
1. **Annotator model robustness test**: Systematically vary the quality of the annotator model (from high-quality to random predictions) and measure how ESS improvement degrades
2. **Distribution shift evaluation**: Test AutoEval when the synthetic dataset has significant distribution shift compared to the human-labeled data
3. **Calibration verification**: Conduct a formal calibration study by computing the actual coverage probability of the confidence intervals across multiple random splits and sample sizes