---
ver: rpa2
title: 'CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language
  Models'
arxiv_id: '2402.06360'
source_url: https://arxiv.org/abs/2402.06360
tags:
- search
- collaborative
- cosearchagent
- query
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CoSearchAgent, a lightweight collaborative
  search agent built as a Slack plugin using large language models (LLMs). It addresses
  the challenge of supporting multi-user collaborative search by leveraging LLMs to
  understand conversational context, rewrite ambiguous queries, fetch and extract
  relevant web content, and generate accurate, citation-marked answers.
---

# CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models

## Quick Facts
- **arXiv ID**: 2402.06360
- **Source URL**: https://arxiv.org/abs/2402.06360
- **Reference count**: 31
- **Primary result**: A lightweight collaborative search agent built as a Slack plugin that uses LLMs to understand conversational context, rewrite queries, fetch and extract relevant web content, and generate accurate, citation-marked answers.

## Executive Summary
This paper introduces CoSearchAgent, a lightweight collaborative search agent implemented as a Slack plugin that leverages large language models to support multi-user search during conversations. The system addresses the challenge of collaborative search by understanding conversational context, rewriting ambiguous queries, fetching and extracting relevant web content, and generating accurate answers with citations. CoSearchAgent is designed to replace the Wizard of Oz paradigm in collaborative search research, providing a practical tool that can be deployed in real-world collaborative settings while logging user behavior for research analysis.

## Method Summary
CoSearchAgent is implemented as a Slack plugin using the Bolt-Python framework. The system captures conversational context (last 20 utterances) when users mention the agent, rewrites queries using an LLM to address context and ambiguity, fetches search results via Serpapi, extracts relevant content from HTML, and generates answers with citation marks using retrieval-augmented generation. The agent can ask clarifying questions for ambiguous queries and maintains conversation threads for natural interaction. User behavior is logged in a MySQL database for research purposes.

## Key Results
- CoSearchAgent effectively replaces the Wizard of Oz paradigm in collaborative search research
- The system produces precise query rewrites that maintain semantic consistency with conversational context
- Generated answers are grounded in retrieved content with citation markers, reducing hallucination

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CoSearchAgent uses LLM-based query rewriting to adapt user queries to conversational context.
- **Mechanism**: Captures last 20 utterances, rewrites incomplete or ambiguous parts using dedicated rewrite instruction, generates clarifying questions if needed before searching.
- **Core assumption**: LLM can accurately infer missing intent from conversational context and maintain semantic consistency.
- **Evidence anchors**: Abstract mentions "rewrite ambiguous queries"; section describes using LLM to read multi-party conversational context and rewrite incomplete sections.
- **Break condition**: Sparse or ambiguous conversational context may cause rewrite failure or off-topic queries.

### Mechanism 2
- **Claim**: CoSearchAgent improves answer accuracy and reduces hallucination by using retrieved references via retrieval-augmented generation.
- **Mechanism**: Extracts text from HTML search results, generates concise reference summary using LLM, uses references to generate answers with citation markers.
- **Core assumption**: References extracted from search results are sufficiently relevant and reliable to ground LLM-generated answer.
- **Evidence anchors**: Abstract mentions "generate accurate, citation-marked answers based on these search results"; section states leveraging references enhances answer accuracy and reduces hallucinations.
- **Break condition**: Search results lacking relevant content produce weak references unable to support accurate generation.

### Mechanism 3
- **Claim**: CoSearchAgent supports lightweight integration into Slack by using a Slack plugin interface.
- **Mechanism**: Uses Bolt-Python framework to handle messages and events, triggers on user mention, sends responses directly in conversation thread.
- **Core assumption**: Slack's API and Bolt framework provide sufficient hooks for real-time conversational interaction.
- **Evidence anchors**: Abstract states "designed as a Slack plugin that can support collaborative search during multi-party conversations"; section describes implementation using Bolt-Python framework.
- **Break condition**: Slack API changes or rate limits could break interaction responsiveness.

## Foundational Learning

- **Concept**: Contextual query rewriting
  - **Why needed here**: Multi-user conversations often contain implicit references or incomplete queries; agent must infer missing information from context.
  - **Quick check question**: If a user says "find it" in a conversation about laptops, what information must the agent infer before searching?

- **Concept**: Retrieval-augmented generation (RAG)
  - **Why needed here**: Direct LLM generation without grounding risks hallucination; using retrieved references ensures factual accuracy.
  - **Quick check question**: What is the difference between generating an answer from scratch vs. generating from retrieved references?

- **Concept**: Multi-party conversational parsing
  - **Why needed here**: Agent must identify speaker, extract relevant context, and decide which parts of conversation are pertinent to query.
  - **Quick check question**: How should the agent handle a query when relevant context is split across multiple users?

## Architecture Onboarding

- **Component map**: Slack plugin (Bolt-Python) → Context capture → Query rewriting (LLM) → Clarifying questions (LLM) → Search API (Serpapi) → HTML fetch → Text extraction → Reference generation (LLM) → Answer generation (LLM) → Response posting
- **Critical path**: User mention → Context extraction → Query rewrite → Search → Reference extraction → Answer generation → Response
- **Design tradeoffs**:
  - Context window limited to 20 utterances to fit LLM input limits, potentially missing older context
  - Filtering out search results without extractable references may reduce result diversity
  - One-round interaction limits clarification depth but speeds response
- **Failure signatures**:
  - Query rewrite returns semantically unrelated content → likely context parsing failure
  - Generated answer contains no citation markers → possible reference extraction failure
  - Agent does not respond to mention → Slack event handling or API connectivity issue
- **First 3 experiments**:
  1. Send query with clear conversational context; verify correct query rewrite and search results
  2. Send ambiguous query; confirm clarifying question is generated
  3. Check that generated answers include citation markers and references are clickable

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does CoSearchAgent's query rewriting performance compare to human experts in collaborative search scenarios?
- **Basis in paper**: [explicit] Paper mentions CoSearchAgent can rewrite queries accurately and maintains semantic consistency with Wizard of Oz approach, but doesn't directly compare the two
- **Why unresolved**: Demonstrates effectiveness but lacks direct comparison with human experts in terms of accuracy, efficiency, or user satisfaction
- **What evidence would resolve it**: User study comparing CoSearchAgent's query rewriting performance with human experts in terms of accuracy, efficiency, and user satisfaction

### Open Question 2
- **Question**: How does CoSearchAgent's retrieval-augmented generation perform compared to traditional search result presentation in terms of user satisfaction and task completion?
- **Basis in paper**: [explicit] Paper mentions generates answers with citation marks based on search results but doesn't compare this approach with traditional search result presentation
- **Why unresolved**: Demonstrates effectiveness but lacks direct comparison with traditional search result presentation in terms of user satisfaction and task completion
- **What evidence would resolve it**: User study comparing CoSearchAgent's retrieval-augmented generation approach with traditional search result presentation

### Open Question 3
- **Question**: How does CoSearchAgent's performance scale with the number of users in a collaborative search scenario?
- **Basis in paper**: [inferred] Paper mentions designed for multi-party conversations but doesn't discuss performance scaling with number of users
- **Why unresolved**: Demonstrates handling of multi-party conversations but provides no information on how performance is affected by number of users
- **What evidence would resolve it**: Performance evaluation with varying numbers of users, measuring response time, accuracy, and user satisfaction

## Limitations
- Evaluation relies on case studies and user behavior logs rather than controlled experiments with quantitative metrics
- Lacks direct comparisons to baseline methods or ablation studies on key components
- Performance depends on quality of search results and relevance of extracted references which may vary significantly across domains
- Claim of "effectively replacing Wizard of Oz paradigm" not substantiated with comparative data

## Confidence
- **High confidence**: Core architecture and implementation details clearly specified, including Slack plugin interface, context capture mechanism, and retrieval-augmented generation pipeline
- **Medium confidence**: Claims about improved search efficiency and user convenience plausible but lack empirical validation through controlled experiments
- **Low confidence**: Assertion of "effectively replacing Wizard of Oz paradigm" not supported by comparative data or quantitative metrics

## Next Checks
1. **Controlled experiment**: Conduct user study comparing CoSearchAgent against both baseline search interface and Wizard of Oz setup, measuring task completion time, answer accuracy, and user satisfaction across multiple query types
2. **Ablation analysis**: Implement and test variants with query rewriting disabled, reference extraction disabled, and both disabled to quantify contribution of each component
3. **Error analysis**: Systematically analyze 100+ user interactions to categorize failure modes and measure their frequency and impact on user experience