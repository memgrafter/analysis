---
ver: rpa2
title: Adversarial Robustness of VAEs across Intersectional Subgroups
arxiv_id: '2407.03864'
source_url: https://arxiv.org/abs/2407.03864
tags:
- adversarial
- robustness
- subgroups
- women
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the adversarial robustness of Variational
  Autoencoders (VAEs) across intersectional subgroups (combinations of age and gender)
  in facial image datasets. The authors investigate robustness disparities among subgroups
  and factors contributing to these disparities, such as data scarcity and representation
  entanglement.
---

# Adversarial Robustness of VAEs across Intersectional Subgroups

## Quick Facts
- arXiv ID: 2407.03864
- Source URL: https://arxiv.org/abs/2407.03864
- Reference count: 30
- Primary result: Robustness disparities exist among intersectional subgroups in VAEs, with older women showing particular vulnerability to adversarial attacks

## Executive Summary
This study investigates the adversarial robustness of Variational Autoencoders (VAEs) across intersectional subgroups defined by age and gender combinations using the CelebA dataset. The authors generate maximum damage adversarial attacks against VAEs and measure reconstruction deviations across different demographic subgroups. They find that robustness disparities exist but are not always correlated with subgroup size, and that optimal β-VAE regularization (β=5) improves overall robustness while reducing disparities, though certain subgroups like older women remain relatively less robust.

## Method Summary
The study trains β-VAE models with β values of 1, 5, and 10 on the CelebA dataset, generating maximum damage adversarial attacks that optimize perturbations to maximize reconstruction deviation. For each of four intersectional subgroups (young men, old men, young women, old women), 60 samples are selected and attacked. Robustness is evaluated using L2 norm distance between original and adversarial reconstructions, while downstream gender and age classifiers assess subgroup prediction accuracy on the reconstructions. Latent space embeddings are visualized using t-SNE to analyze neighborhood structures and subgroup switching tendencies.

## Key Results
- Robustness disparities exist among intersectional subgroups but are not always correlated with subgroup size
- Optimal β-VAE regularization (β=5) improves overall robustness and reduces disparities, though older women remain relatively less robust
- Adversarial reconstructions of minority samples often resemble majority group samples, particularly for the older women subgroup
- Samples prone to subgroup switching reside in mixed or peripheral neighborhoods in latent space, suggesting defective manifold topology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Robustness disparities among subgroups exist but are not always correlated with subgroup size.
- Mechanism: Smaller subgroups may have less smooth or stable latent space embeddings, leading to higher vulnerability to adversarial perturbations. The lack of sufficient data points causes the learned manifold to be less continuous, creating regions where small perturbations can cause large reconstruction errors.
- Core assumption: Latent space quality is directly related to the density and distribution of training samples for each subgroup.
- Evidence anchors:
  - [abstract] "Our findings reveal that robustness disparities exist but are not always correlated with the size of the subgroup."
  - [section] "Subgroup cardinalities indicate that the representation of the old women subgroup is not sufficient for the model to ensure smooth and stable embeddings."
  - [corpus] Weak evidence - corpus contains no direct discussion of subgroup size vs robustness correlation.

### Mechanism 2
- Claim: Optimal β regularization (β=5) improves overall robustness and reduces disparities, but cannot eliminate all subgroup-specific vulnerabilities.
- Mechanism: The β-VAE regularization term encourages disentanglement of latent representations, which improves robustness by creating more stable and interpretable latent spaces. However, when certain subgroups are underrepresented, even optimal disentanglement cannot fully compensate for the lack of smooth manifold topology in those regions.
- Core assumption: Disentanglement regularization improves robustness by creating cleaner latent representations that are less susceptible to adversarial perturbations.
- Evidence anchors:
  - [abstract] "By using downstream gender and age classifiers and examining latent embeddings, we highlight the vulnerability of subgroups like older women, who are prone to misclassification due to adversarial perturbations pushing their representations toward those of other subgroups."
  - [section] "As β increases to 5.0 (top right), the distributions of adversarial deviations for all subgroups decrease correspondingly, indicating increased robustness."
  - [corpus] Weak evidence - corpus contains no direct discussion of β-VAE regularization effects on subgroup robustness.

### Mechanism 3
- Claim: Adversarial reconstructions of minority samples often resemble majority group samples, particularly for the older women subgroup.
- Mechanism: When adversarial perturbations push embeddings of minority samples into regions dominated by majority group representations, the decoder reconstructs images that resemble the majority group. This occurs because the decoder has learned stronger associations between majority group embeddings and their corresponding visual features.
- Core assumption: The decoder's reconstruction mapping is biased toward majority group features due to imbalanced training data.
- Evidence anchors:
  - [abstract] "The study also finds that adversarial reconstructions of minority samples often resemble majority group samples, particularly for the older women subgroup."
  - [section] "Despite β-VAEs with an optimal β = 5 minimize adversarial deviation, we observe that adversarial reconstructions often resemble those of majority groups in the dataset."
  - [corpus] Weak evidence - corpus contains no direct discussion of subgroup switching in adversarial reconstructions.

## Foundational Learning

- Concept: Variational Autoencoders and latent space regularization
  - Why needed here: Understanding how β-VAE regularization affects latent space disentanglement and robustness is central to interpreting the experimental results and mechanism of action.
  - Quick check question: How does increasing β affect the trade-off between reconstruction quality and latent space disentanglement in VAEs?

- Concept: Adversarial attacks on generative models
  - Why needed here: The maximum damage attack used in the study optimizes perturbations to maximize reconstruction deviation, which requires understanding the difference between targeted and untargeted attacks in the context of autoencoders.
  - Quick check question: Why does the maximum damage attack use L2 norm distance between latent distributions rather than reconstruction loss directly?

- Concept: Intersectional subgroup analysis
  - Why needed here: The study examines robustness across combinations of protected attributes (age and gender), requiring understanding of how intersectional subgroups differ from single-attribute groups in terms of representation and vulnerability.
  - Quick check question: How does the cardinality of intersectional subgroups (like old women) typically compare to their constituent single-attribute groups?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline -> β-VAE model implementation -> Maximum damage attack generator -> Robustness evaluation module -> Downstream classifier integration -> Visualization and analysis tools

- Critical path:
  1. Load and preprocess CelebA dataset with age and gender annotations
  2. Train β-VAE models with β = 1, 5, 10 on full dataset
  3. For each subgroup, select 60 samples for robustness evaluation
  4. Generate maximum damage attacks for each sample
  5. Measure adversarial deviation for each attack
  6. Analyze reconstruction quality and classifier predictions on adversarial samples
  7. Visualize latent space neighborhoods and subgroup switching tendencies

- Design tradeoffs:
  - Computational cost vs. sample size: 60 samples per subgroup balances thorough analysis with attack generation overhead
  - Attack intensity vs. perceptibility: Using perceptibly intense perturbations enables robustness comparisons but may not reflect realistic attack scenarios
  - Model complexity vs. interpretability: β-VAE regularization improves robustness but adds complexity to understanding subgroup-specific vulnerabilities

- Failure signatures:
  - High adversarial deviation with low reconstruction loss suggests latent space comprehension issues rather than reconstruction quality problems
  - Subgroup switching in adversarial reconstructions indicates defective latent manifold topology in minority regions
  - Persistent robustness disparities even with optimal β suggest fundamental representation issues beyond disentanglement

- First 3 experiments:
  1. Replicate vanilla VAE (β=1) robustness evaluation on age and gender subgroups to establish baseline disparities
  2. Train and evaluate β-VAE with β=5 to measure improvement in overall robustness and reduction in subgroup disparities
  3. Analyze latent space neighborhoods of samples showing maximum adversarial damage to identify defective manifold topology regions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does increasing the cardinality of minority subgroups in the training dataset affect the adversarial robustness of VAEs for those subgroups?
- Basis in paper: [explicit] The paper notes that subgroup imbalances play a role in adversarial robustness, but increasing dataset size doesn't always address disparities.
- Why unresolved: The study observed that higher representation doesn't necessarily lead to better robustness, suggesting that representativeness might be a more complex concept than just cardinality.
- What evidence would resolve it: Systematic experiments varying the representation of specific minority subgroups in training data while measuring adversarial robustness across different β-VAE configurations.

### Open Question 2
- Question: What architectural modifications to VAEs could improve robustness specifically for minority subgroups like older women?
- Basis in paper: [inferred] The paper mentions that disentanglement alone doesn't solve robustness disparities for certain subgroups, and suggests exploring architectural improvements.
- Why unresolved: While β-VAE regularization showed some benefits, it didn't fully address the robustness gap for certain subgroups, indicating that different architectural approaches might be needed.
- What evidence would resolve it: Comparative studies of VAE variants (e.g., with modified loss functions, attention mechanisms, or domain adaptation components) specifically designed to improve robustness for minority subgroups.

### Open Question 3
- Question: How does the topological structure of the latent space impact subgroup-specific adversarial robustness in VAEs?
- Basis in paper: [explicit] The paper observes that samples prone to subgroup switching reside in mixed or peripheral neighborhoods, suggesting non-smooth embeddings with defective manifold topology.
- Why unresolved: While the paper identifies this pattern, it doesn't fully explore the relationship between latent space topology and robustness disparities across subgroups.
- What evidence would resolve it: Detailed analysis of latent space topology (e.g., using persistent homology or manifold learning techniques) correlated with subgroup-specific robustness measurements across different VAE architectures.

## Limitations
- Limited direct evidence in the corpus supporting the proposed mechanisms for subgroup robustness disparities
- Unclear implementation details of the maximum damage adversarial attack and downstream classifier architectures
- Computational constraints prevent thorough evaluation of all samples, relying on subset analysis

## Confidence
This analysis has **Low confidence** due to limited direct evidence in the corpus supporting the proposed mechanisms. The core claims about subgroup size correlation, β-VAE regularization effects, and subgroup switching behaviors are primarily supported by the study abstract rather than the full text analysis.

## Next Checks
1. Verify the preprocessing steps for age and gender attributes to ensure consistency with the original study's subgroup definitions
2. Implement and validate the maximum damage adversarial attack with appropriate perturbation bounds
3. Reproduce the baseline vanilla VAE (β=1) robustness evaluation on the four intersectional subgroups to establish initial disparities