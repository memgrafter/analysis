---
ver: rpa2
title: 'InjectTST: A Transformer Method of Injecting Global Information into Independent
  Channels for Long Time Series Forecasting'
arxiv_id: '2403.02814'
source_url: https://arxiv.org/abs/2403.02814
tags:
- channel
- global
- injecttst
- information
- mixing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of improving long-term multivariate
  time series forecasting by combining the strengths of both channel-independent and
  channel-mixing approaches. The authors propose InjectTST, a method that injects
  global information into individual channels of a channel-independent Transformer
  model.
---

# InjectTST: A Transformer Method of Injecting Global Information into Independent Channels for Long Time Series Forecasting

## Quick Facts
- arXiv ID: 2403.02814
- Source URL: https://arxiv.org/abs/2403.02814
- Reference count: 5
- Outperforms PatchTST with 1.63% reduction in MSE and 0.89% reduction in MAE on average

## Executive Summary
This paper addresses the challenge of long-term multivariate time series forecasting by proposing InjectTST, a hybrid approach that combines the robustness of channel-independent models with the information capacity of channel-mixing models. The key innovation is selectively injecting global information into individual channels rather than designing a pure channel-mixing model. InjectTST achieves state-of-the-art performance across seven diverse datasets, demonstrating that this selective injection strategy effectively improves forecasting accuracy while maintaining the robustness advantages of channel independence.

## Method Summary
InjectTST processes multivariate time series by first dividing the input into patches, then applying linear projection with channel identifiers to distinguish between channels. The model uses a channel-independent Transformer backbone to process each channel separately, then employs a global mixing module (either CaT or PaT) to create cross-channel information. This global information is selectively injected back into each channel through a self-contextual attention module. The method uses a three-stage training process: pre-training with 50% mask ratio, head-finetuning, and full finetuning, with specific hyperparameters for input length (512), patch length (12), and stride (12).

## Key Results
- Achieves 1.63% average reduction in MSE compared to PatchTST across seven datasets
- Demonstrates 0.89% average reduction in MAE on the same benchmark
- Shows superior performance across four different prediction lengths (96, 192, 336, 720 steps)
- Maintains robustness while improving information capacity through selective global information injection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: InjectTST achieves channel mixing without sacrificing the robustness of channel-independent models by selectively injecting global information into each channel.
- Mechanism: The method retains a channel-independent backbone and introduces a global mixing module that creates cross-channel information. This global information is then selectively injected into each channel via a self-contextual attention module, allowing channels to absorb useful global information while avoiding noise.
- Core assumption: Channel-independent models are inherently robust due to their ability to mitigate noise and distribution drift, and this robustness can be preserved while adding cross-channel information in a selective manner.
- Evidence anchors:
  - [abstract] "Instead of designing a channel-mixing model directly, we retain the channel-independent backbone and gradually inject global information into individual channels in a selective way."
  - [section 3.1] "In the channel-independent backbone, the input X ∈ RL×M passes through the patching module and produces patches... After that, a linear projection... is used to project patches into initial tokens."
  - [corpus] Weak evidence; related works focus on channel-independent vs. channel-mixing tradeoffs but don't directly validate the selective injection mechanism.
- Break condition: If the self-contextual attention module fails to filter out noise, the robustness advantage of channel independence will be lost.

### Mechanism 2
- Claim: The channel identifier enables the model to distinguish between different channels, improving representation learning and capturing unique channel characteristics.
- Mechanism: A learnable tensor (channel identifier) is added to each channel's representation after linear projection. This allows the Transformer to differentiate channels and extract unique patterns for each, addressing the channel specificity problem of shared models.
- Core assumption: Channels in multivariate time series have distinct features that can be represented by a learnable embedding, and this embedding can be optimized to improve forecasting accuracy.
- Evidence anchors:
  - [abstract] "A channel identifier... can help Transformer distinguish channels for better representation."
  - [section 3.1] "To solve the problem, we introduce channel identifier into InjectTST, which is a learnable tensor V ∈ RM ×D... the channel identifier represents the distinct features of each channel after optimization, which can facilitate the model capability of extracting unique representation of channels."
  - [corpus] No direct evidence in corpus; related works do not mention channel identifiers.
- Break condition: If the channel identifier cannot be effectively optimized or becomes redundant, the model will lose the ability to distinguish channels.

### Mechanism 3
- Claim: The global mixing module creates effective cross-channel information that can be injected into individual channels without disrupting their independence.
- Mechanism: Two designs are proposed: CaT (channel as a token) and PaT (patch as a token). Both methods mix information across channels at different granularities, creating a global representation that captures dependencies while preserving the ability to inject this information selectively.
- Core assumption: Cross-channel information can be effectively created and represented in a way that is useful for individual channels without introducing excessive noise.
- Evidence anchors:
  - [abstract] "The global mixing module produces cross-channel global information."
  - [section 3.2] "Two kinds of effective global mixing modules are presented in this paper, named CaT (channel as a token) and PaT (patch as a token) respectively."
  - [corpus] Weak evidence; related works mention channel-mixing but don't validate the specific CaT/PaT designs.
- Break condition: If the global mixing process introduces too much noise or fails to capture meaningful dependencies, the injection will be ineffective.

## Foundational Learning

- Concept: Channel-independent vs. channel-mixing architectures in MTS forecasting
  - Why needed here: Understanding the tradeoff between robustness (channel-independent) and information capacity (channel-mixing) is crucial to grasp why InjectTST's hybrid approach is valuable.
  - Quick check question: What are the two main advantages of channel-independent models mentioned in the paper, and what are the two main advantages of channel-mixing models?

- Concept: Self-attention and cross-attention mechanisms in Transformers
  - Why needed here: The self-contextual attention module uses a modified cross-attention design to inject global information, so understanding these attention mechanisms is essential.
  - Quick check question: How does cross-attention differ from self-attention in Transformers, and why is cross-attention suitable for the injection process in InjectTST?

- Concept: Patching mechanism for time series
  - Why needed here: InjectTST uses a patching mechanism to process input time series, reducing computational complexity while preserving local semantic information.
  - Quick check question: What is the purpose of the patching mechanism in InjectTST, and how does it affect the input dimensions?

## Architecture Onboarding

- Component map:
  Input -> Patching -> Linear Projection + Channel Identifier -> Channel-Independent Transformer Backbone -> Global Mixing Module -> Self-Contextual Attention -> Output

- Critical path:
  Patching and linear projection create initial token representations
  Channel identifier is added to enable channel distinction
  Channel-independent Transformer processes each channel separately
  Global mixing module creates cross-channel information
  Self-contextual attention injects global information into each channel
  Final prediction is made based on enhanced channel representations

- Design tradeoffs:
  Channel independence vs. channel mixing: Robustness vs. information capacity
  CaT vs. PaT global mixing: Computational efficiency vs. granularity of information mixing
  With vs. without residual connection in SCA: Stability vs. flexibility in information injection

- Failure signatures:
  Performance degradation when adding global information (noise injection problem)
  Inability to distinguish channels (channel identifier not learning)
  Instability with residual connection in certain datasets
  Poor performance with long prediction horizons

- First 3 experiments:
  1. Implement basic InjectTST with PaT global mixing and no residual connection; validate on a simple dataset (e.g., Electricity) with short prediction length (96 steps)
  2. Compare CaT vs. PaT global mixing designs on the same dataset; analyze stability and performance differences
  3. Test the effect of adding residual connection in SCA on dataset-specific performance (e.g., ETTh1 vs. Weather)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal design for a global mixing module that balances effectiveness and efficiency across diverse datasets?
- Basis in paper: [explicit] The authors propose two global mixing module designs (CaT and PaT) but note that CaT is more effective on some datasets while PaT is more stable. They suggest this is a direction for future work.
- Why unresolved: The paper demonstrates that different global mixing designs perform differently across datasets, but doesn't provide a unified solution or framework for determining the optimal design based on dataset characteristics.
- What evidence would resolve it: A comprehensive study comparing various global mixing module designs across a wide range of datasets, identifying dataset features that predict which design would be most effective, and proposing a meta-learning approach to select the optimal design.

### Open Question 2
- Question: How can the self-contextual attention module be further improved to reduce noise injection while maintaining or enhancing information transfer?
- Basis in paper: [explicit] The authors suggest that the self-contextual attention module for information injection could be replaced by a more effective design to further reduce noise.
- Why unresolved: While the current SCA module shows effectiveness, the paper acknowledges room for improvement in noise reduction without sacrificing information transfer capabilities.
- What evidence would resolve it: Development and validation of alternative SCA designs that demonstrate superior noise reduction metrics (e.g., lower variance in predictions) while maintaining or improving forecasting accuracy compared to the current implementation.

### Open Question 3
- Question: Can the channel identifier be further optimized to capture more complex channel-specific patterns without increasing computational complexity significantly?
- Basis in paper: [explicit] The authors propose a channel identifier to distinguish channels and capture unique representations, but note it has a slight positive effect on performance improvement.
- Why unresolved: The current channel identifier shows modest improvements, suggesting there may be more sophisticated approaches to capture channel-specific patterns more effectively.
- What evidence would resolve it: A comparative study of different channel identifier architectures (e.g., attention-based, graph-based, or hierarchical approaches) demonstrating significant improvements in forecasting accuracy or stability metrics while maintaining similar or lower computational complexity.

## Limitations

- Limited empirical validation of core mechanisms through ablation studies
- Complex three-stage training procedure that may obscure architectural contributions
- Dataset-specific performance inconsistencies (e.g., residual connection benefits on ETTh1 but harms on Weather)

## Confidence

**High confidence** in the general hybrid approach: The conceptual framework of combining channel independence with selective global information injection is sound and addresses a real tradeoff in the literature. The performance improvements over strong baselines (PatchTST) across multiple datasets support this claim.

**Medium confidence** in the specific implementations: While the CaT and PaT global mixing modules show promise, the paper does not provide sufficient detail to fully validate their design choices. The effectiveness of these specific implementations versus alternative mixing strategies remains uncertain without additional controlled experiments.

**Low confidence** in the mechanism isolation: The paper does not provide ablation studies that isolate the contribution of each component (channel identifier, global mixing, self-contextual attention). The performance claims aggregate all improvements together, making it difficult to attribute gains to specific mechanisms.

## Next Checks

1. **Ablation study on channel identifier**: Remove the channel identifier component and measure performance degradation across all datasets. This would quantify the specific contribution of channel distinction capability to overall forecasting accuracy.

2. **Noise injection sensitivity test**: Systematically increase noise levels in the input data and measure how much more robust InjectTST is compared to pure channel-mixing models. This would validate the core claim about maintaining robustness while adding global information.

3. **Cross-dataset generalization analysis**: Train InjectTST on one dataset (e.g., Electricity) and evaluate zero-shot performance on others. This would reveal whether the learned channel identifiers and global mixing patterns generalize or are dataset-specific.