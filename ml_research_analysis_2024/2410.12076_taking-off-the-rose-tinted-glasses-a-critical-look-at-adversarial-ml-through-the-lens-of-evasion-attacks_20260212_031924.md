---
ver: rpa2
title: 'Taking off the Rose-Tinted Glasses: A Critical Look at Adversarial ML Through
  the Lens of Evasion Attacks'
arxiv_id: '2410.12076'
source_url: https://arxiv.org/abs/2410.12076
tags:
- adversarial
- attacks
- attack
- learning
- security
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that overly permissive attack and overly restrictive
  defensive threat models have hindered the development of practical defenses in adversarial
  machine learning. Through the lens of evasion attacks against neural networks, it
  critically examines common attack assumptions, such as the ability to bypass any
  defense not explicitly built into the model.
---

# Taking off the Rose-Tinted Glasses: A Critical Look at Adversarial ML Through the Lens of Evasion Attacks

## Quick Facts
- arXiv ID: 2410.12076
- Source URL: https://arxiv.org/abs/2410.12076
- Authors: Kevin Eykholt; Farhan Ahmed; Pratik Vaishnavi; Amir Rahmati
- Reference count: 40
- Primary result: Overly permissive attack models and overly restrictive defensive models have hindered practical adversarial ML defenses

## Executive Summary
This paper critically examines the threat models used in adversarial machine learning research, arguing that current approaches have created unrealistic expectations for both attacks and defenses. Through the lens of evasion attacks against neural networks, the authors demonstrate that existing threat models assume attackers can always bypass non-model defenses while requiring defenders to incorporate almost perfect countermeasures directly into models. The paper advocates for a system security perspective that recognizes ML models as components within larger systems, suggesting that traditional security measures can effectively hinder adversarial attacks.

## Method Summary
The authors conduct experiments using CIFAR-10 dataset with ResNet-18 models, comparing natural and adversarially trained versions. They evaluate PGD and Square Attack against these models, measuring attack success rates and query requirements. The study implements and tests detection techniques including ℓ∞ norm analysis, LSH, and Blacklight to identify adversarial probing patterns. Multi-user detection scenarios are evaluated across multiple datasets to assess false positive rates. The methodology contrasts current AI-focused threat modeling with traditional system security approaches to identify practical defense opportunities.

## Key Results
- Adversarial attacks require significantly more queries against adversarially trained models
- Simple detection techniques can identify adversarial reconnaissance attempts before attacks succeed
- Multi-user detection systems show zero false-positive collisions across tested datasets
- Traditional security measures like rate limiting and blacklisting can effectively slow down attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Overly permissive attack threat models allow attackers to bypass defensive measures that would otherwise slow or stop real-world exploitation.
- Mechanism: The community uses an "adaptive attacker" persona that can always overcome non-model defenses, encouraging attacks to assume perfect reconnaissance and query access.
- Core assumption: Defenders can only use model-internal defenses; external system-level defenses are ignored.
- Evidence anchors:
  - [abstract] "overly permissive attack and overly restrictive defensive threat models have hampered defense development"
  - [section 3.1] "Two issues with the proposed attack model hinder potential real-world exploitation... white-box access to the detection model is a very strong assumption"
  - [corpus] Weak evidence: corpus neighbors do not directly address attack threat model feasibility
- Break condition: If defenders can implement system-level detection that does not rely on modifying the model itself, the adaptive attacker assumption fails.

### Mechanism 2
- Claim: Adversarial evasion attacks are highly visible during the reconnaissance phase due to repeated, similar queries.
- Mechanism: Attackers must probe the model multiple times with inputs that differ only slightly, creating a detectable pattern of similarity.
- Core assumption: Query APIs must exist and provide feedback for attacks to succeed.
- Evidence anchors:
  - [section 4.1] "successive queries show a high degree of similarity... adversarial attacks appear to be highly visible"
  - [section 4.2] "we show how some simple detection techniques can prevent existing attacks before they even start"
  - [corpus] Weak evidence: corpus neighbors focus on different ML security aspects, not query visibility
- Break condition: If attackers can issue queries that appear benign while still gathering information, or if query patterns are indistinguishable from normal usage.

### Mechanism 3
- Claim: Multi-user scenarios do not significantly increase false positives in adversarial detection systems.
- Mechanism: Detection systems that compare a user's query history against all other users' histories can effectively identify adversarial patterns without raising many false alarms.
- Core assumption: Normal user behavior does not produce query patterns similar enough to trigger false detections.
- Evidence anchors:
  - [section 5.1] "our experiments caused zero false-positive collisions between the samples" across multiple datasets
  - [section 4.2] "Blacklight detector... raises an alert based on hash collisions using their similarity hash"
  - [corpus] Weak evidence: corpus neighbors do not discuss multi-user detection systems
- Break condition: If normal user query patterns become similar enough to adversarial patterns, or if attackers can mimic benign user behavior convincingly.

## Foundational Learning

- Concept: Threat modeling in cybersecurity
  - Why needed here: The paper argues that current ML security threat models are unrealistic; understanding proper threat modeling is essential to evaluate this claim
  - Quick check question: What are the key components of a comprehensive threat model in system security?

- Concept: Machine learning model robustness
  - Why needed here: The paper discusses adversarial training and model defenses; understanding how models achieve robustness is crucial for evaluating the proposed solutions
  - Quick check question: How does adversarial training improve model robustness against evasion attacks?

- Concept: Query-based attack patterns
  - Why needed here: The paper emphasizes the visibility of reconnaissance queries; understanding how attackers probe models is essential for implementing detection systems
  - Quick check question: What characteristics make adversarial queries distinguishable from benign queries in terms of pattern and frequency?

## Architecture Onboarding

- Component map:
  Model serving system (ML model + API) -> Query monitoring component (detects suspicious patterns) -> Rate limiting subsystem (throttles excessive queries) -> Alert generation system (notifies defenders of potential attacks) -> Response coordination module (implements countermeasures)

- Critical path:
  User query -> Model processing -> Query monitoring analysis -> (If suspicious) Rate limiting -> Alert generation -> Response coordination

- Design tradeoffs:
  - False positive rate vs. detection sensitivity
  - Query processing latency vs. monitoring thoroughness
  - System complexity vs. maintainability
  - Resource usage vs. detection capability

- Failure signatures:
  - High false positive rate indicates overly sensitive detection
  - Missed attacks suggest insufficient monitoring parameters
  - System performance degradation points to inefficient monitoring
  - Incomplete attack pattern detection indicates blind spots in monitoring

- First 3 experiments:
  1. Deploy basic similarity-based detection on a small dataset and measure false positive/negative rates
  2. Test multi-user detection system with simulated benign and adversarial query patterns
  3. Evaluate system performance impact with various monitoring intensities and query volumes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can machine learning systems effectively detect and prevent adversarial reconnaissance attempts without causing excessive false positives in multi-user environments?
- Basis in paper: [explicit] The paper demonstrates that adversarial attacks are highly visible due to multiple similar queries issued in quick succession, and shows detection techniques can catch these attempts with zero false positives in multi-user datasets.
- Why unresolved: While the paper shows promising results with simple detection techniques, it acknowledges that multi-user scenarios complicate detection and may require more sophisticated approaches to avoid false positives.
- What evidence would resolve it: Empirical studies showing the effectiveness and false positive rates of detection systems in real-world multi-user ML applications with various types of user interactions and data distributions.

### Open Question 2
- Question: What are the most effective ways to implement traditional security measures (like rate limiting and blacklisting) in ML systems to hinder adversarial attacks while maintaining usability for legitimate users?
- Basis in paper: [explicit] The paper argues that ML systems should be defended like traditional systems with layers of security components, and demonstrates how monitoring and blacklisting suspicious interactions can slow down attacks.
- Why unresolved: The paper presents initial results but doesn't fully explore the practical implementation challenges, usability impacts, and optimal configurations for these security measures in production ML systems.
- What evidence would resolve it: Deployment studies of ML systems with various security measures showing the balance between attack prevention effectiveness and legitimate user experience.

### Open Question 3
- Question: How can the adversarial machine learning community develop more realistic threat models that better reflect real-world attack scenarios and lead to more practical defense solutions?
- Basis in paper: [explicit] The paper argues that current threat models are overly permissive for attacks and overly restrictive for defenses, leading to attacks that map poorly to real scenarios and defenses that must be almost perfect.
- Why unresolved: The paper identifies the problem but doesn't provide specific guidelines for what constitutes a realistic threat model or how to evaluate whether a proposed attack scenario is practically feasible.
- What evidence would resolve it: A framework for evaluating the practical feasibility of attack scenarios, including guidelines for realistic assumptions about attacker capabilities, system access, and knowledge requirements.

## Limitations

- The paper's threat model assumptions may underestimate sophisticated attackers who can distribute queries across multiple accounts or proxies
- Experimental validation focuses on controlled settings with synthetic adversarial traffic rather than real-world production environments
- The effectiveness of proposed defenses in high-volume, diverse production environments remains uncertain

## Confidence

- **High Confidence**: The observation that current adversarial threat models unrealistically assume attackers can bypass all non-model defenses
- **Medium Confidence**: The claim that adversarial queries are highly visible during reconnaissance due to similarity patterns
- **Low Confidence**: The assertion that multi-user scenarios will not significantly increase false positives

## Next Checks

1. Deploy the query monitoring system in a production ML service with real user traffic to validate false positive rates and detection effectiveness under realistic load conditions

2. Implement an adaptive adversary that uses query distribution strategies (multiple accounts, rate limiting, proxy rotation) to test whether the proposed defenses can still detect attacks under these conditions

3. Evaluate the detection methods across different ML domains (natural language processing, recommendation systems, computer vision) to verify the proposed threat model applies broadly beyond the CIFAR-10 and image classification context used in the experiments