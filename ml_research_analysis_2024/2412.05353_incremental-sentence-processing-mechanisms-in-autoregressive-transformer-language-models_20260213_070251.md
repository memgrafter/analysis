---
ver: rpa2
title: Incremental Sentence Processing Mechanisms in Autoregressive Transformer Language
  Models
arxiv_id: '2412.05353'
source_url: https://arxiv.org/abs/2412.05353
tags:
- features
- path
- sentences
- feature
- garden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses garden path sentences to investigate how autoregressive
  language models incrementally process sentences. Using sparse autoencoders and causal
  interpretability methods, the authors discover interpretable feature circuits underlying
  LM garden path effects, finding that many important features are syntax-related
  while some reflect spurious heuristics.
---

# Incremental Sentence Processing Mechanisms in Autoregressive Transformer Language Models

## Quick Facts
- arXiv ID: 2412.05353
- Source URL: https://arxiv.org/abs/2412.05353
- Authors: Michael Hanna; Aaron Mueller
- Reference count: 40
- LMs represent multiple interpretations of garden path sentences simultaneously but don't reanalyze initial predictions

## Executive Summary
This paper investigates how autoregressive transformer language models incrementally process garden path sentences using sparse autoencoders and causal interpretability methods. The authors discover interpretable feature circuits underlying LM garden path effects, finding that many important features are syntax-related while some reflect spurious heuristics. The LMs represent multiple interpretations simultaneously, as evidenced by both feature analysis and structural probes. When answering follow-up questions about garden path sentences, LMs do not repair or reanalyze their prior structural predictions, instead relying on new, mostly non-syntactic features.

## Method Summary
The study uses sparse autoencoders to decompose LM activations into interpretable features, then applies attribution patching with integrated gradients (AtP-IG) to measure feature importance for behavioral metrics on garden path sentences. Structural probes are used to assess whether LMs represent multiple interpretations simultaneously. The authors conduct causal interventions by clamping feature values to verify their importance, and analyze reading comprehension performance to determine if LMs reanalyze initial predictions. The approach combines behavioral analysis, feature discovery, causal intervention, and probe verification across multiple LM architectures.

## Key Results
- LMs exhibit garden path effects similar to humans, showing probability shifts for different continuations
- Identified feature circuits contain mostly syntax-related features plus some spurious heuristics
- LMs represent multiple interpretations simultaneously through both feature activations and structural probes
- LMs answer follow-up questions about garden path sentences using new, non-syntactic features rather than reanalyzing prior predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse autoencoders decompose LM activations into interpretable features that capture causal mechanisms
- Mechanism: SAEs reconstruct model activations using a sparse set of learned features, where each feature activates only when it meaningfully changes part of the activation
- Core assumption: The sparse regularization and bias terms cause features to be monosemantic rather than polysemic
- Evidence anchors:
  - [abstract]: "we use sparse autoencoders to identify interpretable features that determine which continuation—and thus which reading—of a garden path sentence the LM prefers"
  - [section 2.3]: "SAEs are trained to reconstruct x with sparse regularization on f; the regularizer and bias terms lead a feature's activation to be non-0 only when it causes parts of x to differ from their mean value"
  - [corpus]: Weak evidence - no direct citations, but mechanism aligns with sparse autoencoder literature
- Break condition: If features remain polysemic despite sparse regularization, interpretability claims fail

### Mechanism 2
- Claim: AtP-IG attribution provides accurate estimates of feature importance for LM behavior
- Mechanism: AtP-IG computes gradients at multiple intermediate points between feature activation and baseline, then multiplies by activation change to estimate indirect effect
- Core assumption: The integrated gradients approach averages out noise in gradient estimates compared to single-point attribution patching
- Evidence anchors:
  - [section 2.3]: "Attribution patching (AtP; Nanda, 2023) is one such approximation, estimating the indirect effect... AtP-IG computes an average ∂m/∂a across several intermediate activations"
  - [section 2.3]: "In practice, AtP can often be inaccurate, so we use the improvement of Marks et al. (2024): attribution patching with integrated gradients (AtP-IG)"
  - [corpus]: Weak evidence - relies on Marks et al. (2024) improvement claim without independent verification
- Break condition: If gradient estimates are systematically biased, feature importance rankings will be incorrect

### Mechanism 3
- Claim: LM representations encode multiple interpretations of ambiguous sentences simultaneously
- Mechanism: Both feature analysis and structural probes show non-zero activations for features/correlations corresponding to different readings of garden path sentences
- Core assumption: Non-zero feature activations indicate active representation of alternative interpretations rather than noise
- Evidence anchors:
  - [abstract]: "while most active features correspond to one reading of the sentence, some features correspond to the other, suggesting that LMs assign weight to both possibilities simultaneously"
  - [section 5.1]: "in both the NP/Z and NP/S cases, pro- and non-GP features have non-zero average activations, ranging from 0.27 to 0.41"
  - [section 5.2]: "probes favor LEFT-ARC for NP/Z sentences, and GEN for NP/S, they assign moderate probability to both readings"
- Break condition: If non-zero activations are artifacts rather than meaningful representations, multi-interpretation claims fail

## Foundational Learning

- Concept: Sparse autoencoders and their training objectives
  - Why needed here: SAEs are the primary tool for decomposing LM activations into interpretable features
  - Quick check question: What role does the bias term play in making SAE features monosemantic?

- Concept: Causal interpretability methods (attribution patching, integrated gradients)
  - Why needed here: These methods identify which features causally influence LM behavior on garden path sentences
  - Quick check question: How does AtP-IG differ from standard attribution patching in computing feature importance?

- Concept: Garden path sentence structures and psycholinguistic theories
  - Why needed here: Understanding garden path effects is essential for interpreting LM behavior on ambiguous sentences
  - Quick check question: What distinguishes the garden path reading from the non-garden-path reading in NP/Z sentences?

## Architecture Onboarding

- Component map:
  Input -> SAE -> AtP-IG -> Feature annotation -> Causal intervention -> Structural probe

- Critical path:
  1. Preprocess garden path sentences and create unambiguous variants
  2. Run SAE-based feature discovery using AtP-IG on behavioral metrics
  3. Annotate and verify feature interpretability
  4. Conduct causal interventions to validate feature importance
  5. Apply structural probes to verify multi-interpretation encoding

- Design tradeoffs:
  - Dataset size vs. annotation quality: Small handcrafted datasets allow careful feature annotation but limit generalizability
  - Threshold selection vs. faithfulness: Higher thresholds yield cleaner circuits but lower faithfulness scores
  - Feature complexity vs. interpretability: More features improve behavior capture but increase annotation burden

- Failure signatures:
  - Low faithfulness values (<0.2) indicate missing important features
  - Random interventions affecting behavior suggest spurious feature importance
  - Inconsistent probe results across layers suggest representation instability

- First 3 experiments:
  1. Run behavioral analysis on Pythia-70m to verify garden path effects before feature discovery
  2. Apply AtP-IG with varying thresholds to find optimal feature circuit size
  3. Test feature interventions on small validation set before full-scale analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do language models exhibit ambiguity recognition, where they can explicitly detect when a sentence has multiple possible interpretations?
- Basis in paper: Inferred from the finding that LMs represent multiple interpretations simultaneously, but the paper notes it's unclear if they deploy mechanisms that recognize or adjudicate between mutually exclusive possibilities.
- Why unresolved: The study focused on whether LMs represent multiple readings, not whether they explicitly recognize ambiguity as a meaningful signal. Ambiguity recognition requires different experimental setups than representation analysis.
- What evidence would resolve it: Experiments where LMs are explicitly asked to identify ambiguous sentences, or where their behavior shows sensitivity to ambiguity as a distinct linguistic phenomenon rather than just multiple representations.

### Open Question 2
- Question: What is the functional relationship between features in sparse feature circuits - are they combined through AND, OR, NOT, or more complex operations?
- Basis in paper: Explicit limitation stated - "we have not causally verified what these edges signify" in the discussion section.
- Why unresolved: Current interpretability methods can identify relevant features and their connections but cannot determine the computational logic of how features interact within circuits.
- What evidence would resolve it: Causal intervention experiments that systematically manipulate combinations of features to determine whether they operate conjunctively, disjunctively, or through more sophisticated logical operations.

### Open Question 3
- Question: Do smaller, cognitively-plausible language models trained on human-sized datasets exhibit the same incremental processing mechanisms as larger LMs?
- Basis in paper: Explicit limitation - "It would be particularly interesting... to observe whether these results hold for more cognitively plausible language models, such as those trained on more human-sized datasets."
- Why unresolved: All experiments were conducted on large pre-trained LMs (Pythia-70m and Gemma-2-2b) that may have learned processing mechanisms that don't reflect human language acquisition or processing.
- What evidence would resolve it: Replication of the garden path experiments on models trained with developmental constraints, such as BabyLM or other cognitively-inspired architectures, to see if the same syntactic vs. heuristic feature patterns emerge.

## Limitations

- The study relies on SAE features that may not be truly monosemantic despite sparse regularization, potentially limiting interpretability claims
- Feature importance rankings depend on AtP-IG attribution which may have systematic biases in gradient estimation
- The conclusion about lack of reanalysis is based on reading comprehension performance without controlling for alternative explanations like question-specific heuristics

## Confidence

- **High confidence**: The basic garden path effect exists in LMs (measured by p(GP) - p(non-GP) differences), and SAEs successfully decompose activations into interpretable features for this specific dataset
- **Medium confidence**: The identified feature circuits causally influence garden path behavior, and LMs represent multiple interpretations simultaneously based on feature and probe evidence
- **Low confidence**: The claim that LMs do not reanalyze garden path sentences is definitive, as it relies on reading comprehension performance without controlling for alternative explanation such as question-specific heuristics

## Next Checks

1. **Feature intervention ablation**: Systematically vary SAE regularization parameters (L1 penalty, bias terms) and retrain to determine how feature monosemanticity and circuit faithfulness change, establishing sensitivity of causal claims to SAE hyperparameters

2. **Cross-ambiguity generalization**: Test the same SAE feature circuits on sentences with different types of syntactic ambiguity (e.g., prepositional phrase attachment) to determine whether identified features are garden path-specific or capture general syntactic processing mechanisms

3. **Temporal activation analysis**: Track feature activations across token positions during incremental processing to distinguish between stable parallel representation versus dynamic switching between interpretations, using time-series analysis to identify activation patterns characteristic of reanalysis versus maintenance