---
ver: rpa2
title: 'TART: Boosting Clean Accuracy Through Tangent Direction Guided Adversarial
  Training'
arxiv_id: '2408.14728'
source_url: https://arxiv.org/abs/2408.14728
tags:
- adversarial
- training
- tart
- tangent
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TART, a novel adversarial training method
  that improves clean accuracy while maintaining robustness by leveraging the geometric
  properties of the data manifold. The key idea is to estimate the tangent space of
  natural images and allocate adaptive perturbation bounds to adversarial examples
  based on their tangential components - examples with larger tangential components
  receive larger perturbations while those with smaller components receive smaller
  perturbations.
---

# TART: Boosting Clean Accuracy Through Tangent Direction Guided Adversarial Training

## Quick Facts
- arXiv ID: 2408.14728
- Source URL: https://arxiv.org/abs/2408.14728
- Authors: Bongsoo Yi; Rongjie Lai; Yao Li
- Reference count: 40
- Improves clean accuracy by 1.8-2.5% on CIFAR-10 while maintaining robustness

## Executive Summary
TART introduces a novel adversarial training method that improves clean accuracy while maintaining robustness by leveraging the geometric properties of the data manifold. The key innovation is to estimate the tangent space of natural images and allocate adaptive perturbation bounds to adversarial examples based on their tangential components. Examples with larger tangential components receive larger perturbations while those with smaller components receive smaller perturbations. This prevents training on adversarial examples with large normal components that can significantly alter the decision boundary and hurt clean accuracy. Extensive experiments show TART consistently improves clean accuracy by 1.8-2.5% across different baseline defenses while maintaining comparable robustness.

## Method Summary
TART estimates the tangent space of natural images using an autoencoder and PCA, then computes tangential components of adversarial examples by projecting them onto the estimated tangent spaces. During training, TART assigns larger perturbation bounds to examples with larger tangential components (indicating they lie closer to the data manifold) and smaller or zero perturbation bounds to examples with smaller tangential components (indicating they lie farther from the manifold). This approach prevents the model from learning decision boundaries influenced by adversarial examples that are far from the true data distribution. The method requires pre-computing tangent spaces and projection matrices for all training data, adding approximately 24% training time overhead compared to standard adversarial training.

## Key Results
- TART improves clean accuracy by 1.8-2.5% on CIFAR-10 across different baseline defenses (AT, TRADES, MART, GAIRAT)
- Maintains comparable or slightly better robust accuracy against PGD and AutoAttack
- Works as a universal approach that can be combined with various existing adversarial training techniques
- Shows consistent performance improvements across both simulated transformed hemisphere datasets and benchmark CIFAR-10 dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training on adversarial examples with large normal components alters the decision boundary and reduces clean accuracy.
- Mechanism: The normal component of an adversarial perturbation pushes examples off the data manifold, causing the model to learn decision boundaries that are misaligned with the true data distribution.
- Core assumption: The data manifold has a lower intrinsic dimension than the pixel space, and adversarial examples lie outside this manifold.
- Evidence anchors:
  - [abstract] "We argue that training with adversarial examples having large normal components significantly alters the decision boundary and hurts accuracy."
  - [section III] "We note that the column space of A is the k-dimensional tangent space at x."
- Break condition: If the data manifold is actually high-dimensional or if normal components don't significantly affect the decision boundary.

### Mechanism 2
- Claim: Allocating larger perturbation bounds to examples with larger tangential components improves both clean accuracy and robustness.
- Mechanism: Examples with larger tangential components are closer to the data manifold and thus safer to train on with larger perturbations, while examples with smaller tangential components are farther from the manifold and should receive smaller perturbations.
- Core assumption: The magnitude of the tangential component is a reliable indicator of how "safe" an adversarial example is to train on.
- Evidence anchors:
  - [section III] "TART assigns larger or smaller perturbation bounds to adversarial examples with larger or smaller tangential components, respectively."
  - [section III.C.1] Proposition III.1: "|RP(f) - RQ(f)| ≤ 4 TV(P, Q)" suggesting that training on examples closer to the true distribution improves clean accuracy.
- Break condition: If the relationship between tangential component magnitude and training safety is not monotonic or reliable.

### Mechanism 3
- Claim: Estimating the tangent space offline and pre-computing projection matrices reduces computational overhead during training.
- Mechanism: By computing the tangent space and projection matrices before training begins, TART avoids expensive computations during each training epoch.
- Core assumption: The tangent space is relatively stable across epochs and doesn't need to be recomputed for each training iteration.
- Evidence anchors:
  - [section III.B.1] "Before starting the training, we compute the tangent space and A(A⊺A)−1A⊺ of each training image and store them for later use to reduce training time."
  - [section III.C.4] "TART requires 1.24 times the training time compared to standard AT" indicating the computational overhead is manageable.
- Break condition: If the tangent space changes significantly during training or if pre-computation storage becomes prohibitive.

## Foundational Learning

- Concept: Data manifolds and intrinsic dimension
  - Why needed here: Understanding that real data lies on lower-dimensional manifolds is crucial for grasping why normal components are problematic
  - Quick check question: If an image has 3072 pixels but lies on a 50-dimensional manifold, what percentage of the space does it actually occupy?

- Concept: Tangent spaces and projections
  - Why needed here: The method relies on projecting adversarial perturbations onto tangent spaces to compute tangential components
  - Quick check question: Given a data point x and its tangent space matrix A, how would you compute the projection of a vector v onto this tangent space?

- Concept: Adversarial training and robustness-accuracy tradeoff
  - Why needed here: TART aims to improve the clean accuracy while maintaining robustness, a fundamental challenge in adversarial training
  - Quick check question: Why does standard adversarial training typically reduce clean accuracy, and what property of TART addresses this?

## Architecture Onboarding

- Component map:
  Autoencoder for tangent space estimation -> PCA for tangent vector extraction -> Adversarial example generator (PGD) -> Tangential component calculator -> Perturbation bound allocator -> Training loop integration

- Critical path:
  1. Pre-train autoencoder on training data
  2. Compute and store tangent spaces for all training examples
  3. During each training iteration:
     - Generate adversarial examples
     - Compute tangential components
     - Allocate perturbation bounds
     - Train with selected examples

- Design tradeoffs:
  - Pre-computation vs. real-time computation of tangent spaces
  - Storage requirements for tangent space matrices
  - Choice of tangent space dimension (k) relative to data complexity
  - Perturbation bound allocation strategy (threshold vs. continuous)

- Failure signatures:
  - Degraded performance if tangent space estimation is poor
  - Increased training time if tangent space computation is not properly cached
  - Overfitting if perturbation bounds are allocated incorrectly
  - Robustness loss if too many examples receive zero perturbation bounds

- First 3 experiments:
  1. Implement tangent space estimation on CIFAR-10 and verify that tangential components vary across examples
  2. Test the perturbation bound allocation mechanism on a small dataset to confirm it reduces normal component training
  3. Integrate TART with standard AT on CIFAR-10 and measure clean accuracy improvement vs. baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal perturbation bound assignment strategy for TART that maximizes both clean accuracy and robustness simultaneously?
- Basis in paper: [explicit] The authors state that TART uses a simple threshold-based approach (upper 50% get maximum perturbation, lower 50% get zero), but acknowledge this may not be optimal and mention future work on theoretically investigating the contribution of each sample.
- Why unresolved: The paper uses a simple heuristic for assigning perturbation bounds based on tangential component thresholds, but does not explore more sophisticated or theoretically-grounded assignment strategies. The relationship between tangential components and their impact on model performance is only partially explored.
- What evidence would resolve it: Systematic experiments comparing different perturbation bound assignment strategies (e.g., continuous functions of tangential component, data-driven optimization) on multiple datasets, along with theoretical analysis of how different assignments affect the trade-off between clean accuracy and robustness.

### Open Question 2
- Question: How does the choice of autoencoder architecture and training procedure affect the quality of tangent space estimation and consequently TART's performance?
- Basis in paper: [inferred] The authors mention using an autoencoder to estimate the tangent space but provide limited discussion on how different autoencoder designs might impact performance. The method section notes that the intrinsic data manifold is unknown for most image datasets, necessitating this approximation.
- Why unresolved: The paper uses a generic convolutional autoencoder without exploring how architectural choices (depth, width, type of layers) or training procedures (loss functions, regularization) might affect the fidelity of tangent space estimation and the resulting TART performance.
- What evidence would resolve it: Comparative experiments using different autoencoder architectures and training procedures, along with ablation studies showing how perturbations in tangent space quality affect clean accuracy and robustness metrics.

### Open Question 3
- Question: Can TART be extended to handle other types of adversarial perturbations beyond l∞ norm, such as l2, l1, or more complex threat models?
- Basis in paper: [explicit] The authors state that "our method can be adapted for use with alternative norms as well" but do not provide experimental validation or theoretical justification for this claim beyond the l∞ norm used throughout their experiments.
- Why unresolved: While the paper mentions potential generalization to other norms, all experiments are conducted exclusively with l∞ norm constraints. The geometric interpretation of tangent space and its relationship to perturbation bounds in other norms remains unexplored.
- What evidence would resolve it: Experimental validation of TART with l2, l1, and mixed-norm threat models, along with theoretical analysis of how tangent space geometry changes under different norm constraints and how this affects the perturbation bound assignment strategy.

## Limitations

- The effectiveness depends on the assumption that data lies on low-dimensional manifolds, which may not hold uniformly across all datasets
- Computational overhead of 24% more training time could become prohibitive for larger models or datasets
- Method's performance on larger architectures (ResNet-50, EfficientNet) and more complex datasets (ImageNet) remains unverified
- Tangent space estimation quality is sensitive to hyperparameters (number of samples l, tangent dimension k) that are not extensively explored

## Confidence

- **High Confidence**: The geometric intuition behind leveraging tangent directions for adversarial training, demonstrated improvements on CIFAR-10 across multiple baselines, and the general methodology of pre-computing tangent spaces
- **Medium Confidence**: The claim that TART is a "universal" approach that can be combined with various existing adversarial training techniques, as this requires extensive validation across diverse methods
- **Low Confidence**: The assertion that normal components "significantly alter the decision boundary and hurt accuracy" is primarily theoretical and would benefit from empirical validation showing decision boundary changes with and without TART

## Next Checks

1. **Cross-dataset generalization**: Validate TART on diverse datasets (e.g., CIFAR-100, SVHN, TinyImageNet) to assess its robustness across different data distributions and complexities

2. **Ablation study on tangent space parameters**: Systematically vary the tangent space dimension k and number of samples l in the PCA step to quantify their impact on clean accuracy and robustness

3. **Decision boundary analysis**: Use visualization techniques or probing methods to empirically demonstrate how TART affects the learned decision boundary compared to standard adversarial training, particularly regarding normal component contributions