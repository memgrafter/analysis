---
ver: rpa2
title: 'Time Matters: Scaling Laws for Any Budget'
arxiv_id: '2406.18922'
source_url: https://arxiv.org/abs/2406.18922
tags:
- number
- loss
- training
- hyperparameters
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to estimating transformer
  model performance based on wall-clock training time rather than FLOPs. The authors
  demonstrate that memory copies are a stronger predictor of training speed than FLOPs
  and develop equations to estimate training speed from model hyperparameters.
---

# Time Matters: Scaling Laws for Any Budget

## Quick Facts
- arXiv ID: 2406.18922
- Source URL: https://arxiv.org/abs/2406.18922
- Reference count: 11
- This paper introduces a novel approach to estimating transformer model performance based on wall-clock training time rather than FLOPs

## Executive Summary
This paper challenges the conventional wisdom that FLOPs are the primary determinant of training speed by demonstrating that memory copies are a much stronger predictor of actual training time. The authors develop equations to estimate training speed from model hyperparameters and combine these with Chinchilla scaling laws to predict final model loss with r² = 0.9 accuracy without training. Their analysis reveals that models should be made wider rather than deeper, as speed benefits outweigh depth benefits, enabling analytical architectural decisions and more efficient model training across a wide range of configurations.

## Method Summary
The authors train 1,535 transformer models on the C4 dataset for 3 hours each and 1,778 models for 5 minutes to measure throughput on TPU V5 chips. They derive equations for counting memory copies (MEMCPYS) and FLOPs in transformer architectures, then use linear regression to fit coefficients for predicting runtime. These runtime predictions are combined with Chinchilla scaling laws to estimate final model loss from hyperparameters alone. The methodology focuses on single-chip training to avoid inter-chip communication confounds and assumes fixed training time budgets rather than fixed compute budgets.

## Key Results
- Memory copies are a much stronger predictor of training speed than FLOPs, achieving r² = 0.74 in runtime prediction
- Final model loss can be predicted with r² = 0.9 accuracy using only parameter count and number of training tokens
- Models should be made wider rather than deeper due to speed benefits outweighing depth benefits
- Scaling law coefficients are highly sensitive to experimental setup details and require dataset-specific fitting

## Why This Works (Mechanism)

### Mechanism 1
Memory copies (MEMCPYS) are a stronger predictor of training speed than FLOPs because memory bandwidth and copying overhead dominate actual computation time in transformer training. The total training time depends on both FLOPs (computational work) and data movement (memory copies), with coefficients c1=3.74e-19 (for MEMCPYS) and c2=2.4e-15 (for FLOPS) predicting runtime well. Every matrix multiplication requires memory copies proportional to operand size, and this copying overhead dominates over arithmetic operations.

### Mechanism 2
Models with the same parameter count can achieve different losses based on architectural speed optimization, as predicted by the parameter equivalence heuristic. Above a certain scale, final loss depends primarily on parameter count rather than their distribution, meaning models competing on the same parameter budget should optimize for speed (more tokens in fixed time). Loss curves during pretraining can be written as a linear combination of terms dependent on parameters and data, making parameter count the dominant factor above scale.

### Mechanism 3
The Chinchilla scaling law coefficients are highly sensitive to experimental setup details, requiring dataset-specific fitting rather than using published values. The paper finds that using Hoffmann et al. coefficients underestimates loss by more than a factor of two on their data, while their fitted coefficients achieve r²=0.9. The exponents α=0.48 and β=0.53 are robust, but the linear coefficients must be fitted to specific experimental conditions.

## Foundational Learning

- **Scaling laws for neural networks**: Why needed here - The entire paper builds on understanding how model performance scales with parameters and data, which is the foundation of scaling law research. Quick check question: What is the functional form of the Chinchilla scaling law and what do each of its terms represent?

- **Memory hierarchy and computational bottlenecks**: Why needed here - The paper's key insight is that memory copies, not FLOPs, dominate training time, requiring understanding of how memory bandwidth affects computation. Quick check question: Why would memory copies be a stronger predictor of runtime than FLOPs in modern hardware?

- **Transformer architecture and computational graph**: Why needed here - The MEMCPYS and FLOPS equations are derived from specific operations in transformer blocks, requiring understanding of self-attention and MLP components. Quick check question: What are the main computational operations in a transformer block and how do they contribute to parameter count and memory operations?

## Architecture Onboarding

- **Component map**: MEMCPYS/FLOPS counting equations → Coefficient regression (c1,c2,c3) → Runtime prediction → Data consumption estimation → Chinchilla coefficient fitting (A,B,E) → Final loss prediction from hyperparameters

- **Critical path**: MEMCPYS/FLOPS counting → Coefficient regression (c1,c2,c3) → Runtime prediction → Data consumption estimation → Chinchilla coefficient fitting (A,B,E) → Final loss prediction from hyperparameters

- **Design tradeoffs**: The model assumes fixed training time rather than fixed compute budget, focuses on single-chip training to avoid inter-chip communication confounds, and uses simplified memory copy assumptions that may not capture all hardware-specific optimizations

- **Failure signatures**: Poor r² values in runtime prediction (should be ~0.74), loss prediction accuracy dropping below 0.9, or predictions suggesting architectural choices that contradict empirical results when models are actually trained

- **First 3 experiments**:
  1. Implement MEMCPYS and FLOPS counting for a simple transformer configuration and verify against hand calculations
  2. Train 5-10 transformer models with varying hyperparameters for 5 minutes each, measure actual throughput, and perform linear regression to find c1, c2, c3 coefficients
  3. Use the fitted runtime model to predict data consumption for different architectures, then apply Chinchilla scaling to predict final loss and compare against actual training results

## Open Questions the Paper Calls Out

### Open Question 1
How do the scaling law coefficients vary across different hardware architectures and training setups? The authors note that their fitted coefficients differ significantly from both Hoffmann et al. and Besiroglu et al., suggesting high sensitivity to setup details. This remains unresolved because the paper only tests on TPU V5 chips with specific configurations. Testing the same methodology across different hardware platforms (GPUs, different TPU generations), varying batch sizes, gradient accumulation strategies, and model parallelism approaches would determine if the coefficients are truly setup-dependent.

### Open Question 2
Do the memory copy-based scaling laws extend to model sizes beyond hundreds of millions of parameters? The authors explicitly state they "do not consider the effects of model sharding, or the effects of scale beyond a few hundred million parameters" and label this as "fruitful areas of exploration for future work". This remains unresolved because experiments were constrained to models that could fit on a single TPU mesh, limiting the parameter range to 277K-972M parameters. Applying the same methodology to train models with billions of parameters would test whether the MEMCPY coefficients remain valid and whether the width-over-depth recommendation holds at scale.

### Open Question 3
How do different attention mechanisms and architectural variations affect the memory copy-based scaling laws? The authors derive their MEMCPY and FLOPS equations specifically for standard decoder-only transformers and note their equations could apply to "many architectures other than transformers". This remains unresolved because the paper only considers standard multi-head attention with the specific equations provided. Deriving and validating MEMCPY and FLOPS equations for alternative architectures like Mamba, RWKV, or other attention variants, then testing whether the same predictive power holds for the loss estimation equation would resolve this question.

## Limitations
- Assumes fixed training time budgets rather than fixed compute budgets, which may not align with all practical deployment scenarios
- Memory copy model is based on idealized assumptions that may not hold for all hardware configurations or future architectures
- Scaling law coefficients appear highly sensitive to experimental conditions, requiring dataset-specific fitting
- Analysis focuses on single-chip training, limiting applicability to multi-node deployments

## Confidence

- **High Confidence**: The MEMCPYS-based runtime prediction model achieving r² = 0.74 on empirical data, as this is directly measured and validated
- **Medium Confidence**: The final loss prediction accuracy (r² = 0.9) because it depends on both the runtime model and the fitted scaling law coefficients
- **Medium Confidence**: The architectural recommendation to prefer width over depth, as this is derived from the runtime model but may be influenced by dataset-specific factors

## Next Checks

1. **Hardware Generalization Test**: Validate the MEMCPYS-based runtime prediction on different hardware architectures (GPUs, different TPU versions) to assess hardware sensitivity of the memory copy assumptions.

2. **Multi-Node Scaling**: Extend the analysis to multi-node configurations to understand how inter-node communication affects the memory copy dominance and whether the width-over-depth recommendation holds.

3. **Dataset Transferability**: Test whether the fitted Chinchilla scaling law coefficients (A, B, E) transfer to different datasets beyond C4, or if the dataset-specific fitting requirement limits practical applicability.