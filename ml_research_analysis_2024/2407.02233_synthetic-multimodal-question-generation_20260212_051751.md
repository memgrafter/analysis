---
ver: rpa2
title: Synthetic Multimodal Question Generation
arxiv_id: '2407.02233'
source_url: https://arxiv.org/abs/2407.02233
tags:
- question
- answer
- questions
- sources
- smmqg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a synthetic data generation framework called
  SMMQG for multimodal retrieval augmented generation. SMMQG leverages interplay between
  a retriever, large language model, and large multimodal model to generate question-answer
  pairs from multimodal documents, with fine-grained control over question styles
  and modalities.
---

# Synthetic Multimodal Question Generation

## Quick Facts
- arXiv ID: 2407.02233
- Source URL: https://arxiv.org/abs/2407.02233
- Authors: Ian Wu; Sravan Jayanthi; Vijay Viswanathan; Simon Rosenberg; Sina Pakazad; Tongshuang Wu; Graham Neubig
- Reference count: 40
- Key outcome: Introduces SMMQG framework that generates high-quality synthetic multimodal QA data, showing human evaluation quality on par with crowdsourced benchmark MMQA

## Executive Summary
This paper presents SMMQG, a synthetic data generation framework for multimodal retrieval augmented generation (MMRAG). SMMQG leverages interplay between a retriever, large language model (LLM), and large multimodal model (LMM) to generate question-answer pairs directly from multimodal documents. The framework enables fine-grained control over question styles and modalities, producing both unimodal and cross-modal questions. Human evaluation shows SMMQG-generated data quality matches or exceeds the crowdsourced benchmark MMQA, with strong concurrence in both retrieval and QA tasks.

## Method Summary
SMMQG is a five-step framework that generates multimodal QA data through retriever-LLM-LMM interplay. It takes multimodal sources, question style specifications, and modality requirements as inputs. The process involves seed source selection prioritizing semantically related sources, entity extraction, semantic retrieval of candidate sources, question-answer generation using LLM/LMM, and automated validation. The framework maintains thematic unity by ensuring all candidate sources are semantically related, enabling generation of meaningful multi-source and cross-modal questions. Generated questions conform to specified styles and modalities, with validation steps ensuring quality.

## Key Results
- Human evaluation shows SMMQG-generated data quality matches or exceeds crowdsourced benchmark MMQA
- Strong concurrence with MMQA (τ > 0.8) for both retrieval and QA tasks
- Generated dataset of 1024 questions spanning five question styles and all pairwise modality combinations
- Framework enables fine-grained control over question styles and modalities for tailored MMRAG evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SMMQG generates high-quality synthetic multimodal QA data by leveraging interplay between retriever, LLM, and LMM
- Mechanism: Uses seed source selection that prioritizes semantically related sources, extracts entities, retrieves related candidates, and uses LLM/LMM to generate questions adhering to specified styles and modalities with automated validation
- Core assumption: LLM and LMM can understand specified question styles and modalities and generate coherent questions and answers from provided sources
- Evidence anchors: Abstract states SMMQG leverages interplay between retriever, LLM, and LMM; Section 3 describes five-step process maintaining thematic unity through semantically related sources
- Break condition: If LLM/LMM cannot understand specified styles/modalities or retrieved sources aren't semantically related, generated questions/answers may be low quality or incoherent

### Mechanism 2
- Claim: SMMQG enables fine-grained control over question styles and modalities for tailored MMRAG evaluation
- Mechanism: Takes user-provided question style (determining reasoning abilities) and modality requirements (specifying source modalities) as inputs, allowing generation of questions matching expected MMRAG encounters
- Core assumption: Users can accurately specify desired question styles and modalities for their MMRAG system
- Evidence anchors: Abstract mentions fine-grained control over styles and modalities; Section 2.2 describes three inputs including question style and modality requirements
- Break condition: If user-specified styles/modalities don't reflect actual MMRAG encounters, generated evaluation data may not be representative

### Mechanism 3
- Claim: SMMQG-generated data quality comparable to crowdsourced data, shown through human evaluation and benchmark concurrence
- Mechanism: Human study with crowdworkers rating SMMQG questions/answers on five metrics (fluency, style faithfulness, source relevance, answerability, answer correctness); measures concurrence between SMMQG dataset and MMQA
- Core assumption: Human evaluation and benchmark concurrence are valid quality measures
- Evidence anchors: Section 5.2 shows SMMQG quality on par with or better than MMQA; Section 5.3 finds strong concurrence (τ > 0.8) for retrieval and QA
- Break condition: If human evaluation isn't reliable or MMQA concurrence isn't indicative of quality, claim may not hold

## Foundational Learning

- Concept: Multimodal Retrieval Augmented Generation (MMRAG)
  - Why needed here: SMMQG generates evaluation data for MMRAG systems, so understanding MMRAG basics is crucial
  - Quick check question: What are the key components of an MMRAG system and how do they work together to answer questions over multimodal documents?

- Concept: Question Styles and Modalities
  - Why needed here: SMMQG allows fine-grained control over question styles and modalities
  - Quick check question: What are common question styles in QA tasks and how do they differ in reasoning abilities required? What modalities can be used in multimodal QA?

- Concept: Synthetic Data Generation
  - Why needed here: SMMQG is a synthetic data generation framework
  - Quick check question: What are main approaches to synthetic data generation and what are potential benefits/drawbacks compared to real data?

## Architecture Onboarding

- Component map: Retriever -> LLM/LMM -> Validation
- Critical path: Seed source selection → Entity extraction → Source retrieval → Question generation → Validation
- Design tradeoffs:
  - More sophisticated retriever may improve source quality but increase computational cost
  - Larger LLM/LMM may improve question/answer quality but increase computational cost and latency
  - Additional validation steps may improve data quality but increase computational cost and latency
- Failure signatures:
  - Low-quality or incoherent questions/answers may indicate issues with LLM/LMM or retrieved sources
  - Questions/answers not adhering to specified styles/modalities may indicate generation or validation issues
  - Low concurrence with benchmarks may indicate overall quality issues
- First 3 experiments:
  1. Generate small dataset with simple retriever and LLM, manually evaluate question/answer quality
  2. Compare different retrievers (BM25, E5, OpenCLIP) on retrieved sources and generated question quality
  3. Compare different LLMs/LMMs (GPT-4, Gemini, Claude) on generated questions/answers and data quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SMMQG performance vary when using different retriever models or question generation models?
- Basis in paper: Only assessed E5-Large and GPT-4-Turbo viability; new challenges may arise with other models
- Why unresolved: Paper doesn't explore impact of different models on SMMQG performance
- What evidence would resolve it: Experiments with different retriever and question generation models comparing performance to current models

### Open Question 2
- Question: Can SMMQG be used to generate training data for multimodal retrieval augmented generation models?
- Basis in paper: Only assessed viability for evaluation but suggests possible use for training
- Why unresolved: Paper doesn't explore use of SMMQG data for model training
- What evidence would resolve it: Training MMRAG models using SMMQG-generated data and evaluating downstream performance

### Open Question 3
- Question: How does SMMQG generalize to noisy data sources or sources from different domains?
- Basis in paper: Relies on high-quality unpaired data sources; didn't test generalization to noisy sources
- Why unresolved: Paper doesn't explore performance on noisy sources or different domains
- What evidence would resolve it: Generating datasets on noisy sources or different domains and evaluating question/answer quality

### Open Question 4
- Question: How can the difficulty of questions generated by SMMQG be controlled or adjusted?
- Basis in paper: SMMQG questions generally easier than MMQA counterparts; suggests increasing difficulty via prompting through question style prompt
- Why unresolved: Paper doesn't explore methods for controlling or adjusting question difficulty
- What evidence would resolve it: Experimenting with different prompting strategies or incorporating additional factors to control question difficulty

## Limitations

- Quality depends heavily on retrieval stage - if semantically related sources cannot be found for an entity, multi-source question generation may fail
- Current approach uses Wikipedia documents, limiting generalizability to other document types or domains
- Automated validation steps may not catch all quality issues, particularly subtle semantic inconsistencies in cross-modal questions
- Human evaluation only assessed 50 questions and may not capture edge cases or long-tail scenarios

## Confidence

**High Confidence**: Core mechanism of using retriever-LLM-LMM interplay for synthetic data generation is well-supported, with strong benchmark concurrence (τ > 0.8) and human evaluation showing comparable or better quality than MMQA

**Medium Confidence**: Generalizability to domains beyond Wikipedia and robustness when scaling to larger datasets (beyond current 1024 questions) requires further validation

**Medium Confidence**: Automated validation steps may not fully replace human judgment for quality assessment, though they work in practice

## Next Checks

1. **Cross-Domain Validation**: Test SMMQG on non-Wikipedia domains (scientific papers, news articles, technical documentation) to evaluate generalizability and whether framework maintains quality with different document types

2. **Scalability and Edge Case Analysis**: Generate dataset of 10,000+ questions and perform systematic error analysis to identify failure modes, examining where thematic unity breaks down, cross-modal questions become incoherent, or validation steps fail

3. **MMRAG System Integration Test**: Deploy SMMQG-generated data for training and evaluating an MMRAG system on downstream task, then compare performance against systems trained on MMQA to validate practical utility