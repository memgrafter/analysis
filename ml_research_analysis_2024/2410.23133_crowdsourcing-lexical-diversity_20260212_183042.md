---
ver: rpa2
title: Crowdsourcing Lexical Diversity
arxiv_id: '2410.23133'
source_url: https://arxiv.org/abs/2410.23133
tags:
- lexical
- language
- arabic
- crowdsourcing
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of bias in lexical-semantic
  resources (LSRs), particularly the underrepresentation of language-specific concepts
  and lexical gaps. To tackle this, the authors introduce a novel crowdsourcing methodology
  that enables native speakers to compare lexemes across language pairs, identifying
  equivalent terms, language-specific concepts, and lexical gaps.
---

# Crowdsourcing Lexical Diversity

## Quick Facts
- arXiv ID: 2410.23133
- Source URL: https://arxiv.org/abs/2410.23133
- Reference count: 15
- Primary result: Native speaker crowdsourcing identifies 3,091 lexical gaps across four languages, outperforming LLMs in culturally specific term detection

## Executive Summary
This paper addresses bias in lexical-semantic resources (LSRs) by introducing a novel crowdsourcing methodology for identifying cross-lingual lexical gaps. The LingoGap platform enables native speakers to compare lexemes across language pairs through structured microtasks, discovering equivalent terms, language-specific concepts, and lexical gaps without relying on English as a pivot language. Two case studies (English–Arabic and Indonesian–Banjarese) focusing on food terminology yielded 3,091 lexical gaps and 1,735 equivalent words, demonstrating the approach's scalability and effectiveness in enriching LSRs with culturally diverse data.

## Method Summary
The methodology involves semi-automated task generation using linguistic resources to create source and target language word-gloss pairs, followed by crowdsourcing through the LingoGap platform. Native speakers complete multi-step microtasks identifying equivalent terms, lexical gaps, or "do not know" responses, with quality control through attention checks, time monitoring, and inter-annotator agreement (IAA) using Krippendorff's Alpha. Validation includes expert review of low-agreement items, producing a consolidated dataset. The bidirectional approach compares both language directions to capture symmetrical lexical diversity without English-centric bias.

## Key Results
- Identified 3,091 lexical gaps across four languages in food terminology
- Collected 1,735 equivalent words through native speaker annotations
- Achieved high inter-annotator agreement (Alpha scores) validating crowdsourced data quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Native speakers outperform LLMs in identifying culturally specific lexical gaps
- Mechanism: Native speakers apply contextual and cultural knowledge to detect when concepts exist in one language but not another, while LLMs struggle with hallucinations and literal translation
- Core assumption: Native speakers can reliably judge lexical equivalence based on lived linguistic experience
- Evidence anchors: Abstract states native speakers are more effective than LLMs; section notes LLM struggles with culturally diverse contexts; examples include mistranslating "water and dates" or "Kembili"
- Break condition: Native speakers misinterpret cultural nuances or misapply definitions; agreement metrics fail to filter low-quality annotations

### Mechanism 2
- Claim: Bidirectional comparison avoids English-centric bias and captures lexical diversity from both sides
- Mechanism: Comparing Language A→B and B→A identifies gaps in both directions, documenting culture-specific concepts in either language
- Core assumption: Lexical diversity is symmetric enough that both directions yield meaningful, non-redundant insights
- Evidence anchors: Abstract emphasizes no reliance on pivot languages and supports bidirectional exploration; section states crowdsourcing is conducted twice in each direction
- Break condition: One language dominates culturally/linguistically, making reverse direction less informative; data sparsity in low-resource language

### Mechanism 3
- Claim: Structured micro-tasks with quality control ensure high data reliability
- Mechanism: Task design enforces stepwise reasoning, attention checks filter inattentive workers, and IAA with expert review resolves disagreements
- Core assumption: Structured reasoning and multi-layered validation can filter noise even from non-expert contributors
- Evidence anchors: Abstract describes LingoGap facilitating comparisons through microtasks; section details three response options, automated logging, ACQs, and Alpha for IAA
- Break condition: IAA metrics fail to detect subtle disagreements; ACQs are too easy or too hard, skewing results

## Foundational Learning

- Concept: Cross-lingual lexical gaps
  - Why needed here: The whole paper is about detecting when a word in one language has no direct equivalent in another; without understanding this, you can't design tasks or interpret results
  - Quick check question: What is the formal definition of a lexical gap according to the paper?

- Concept: Inter-annotator agreement (IAA) using Krippendorff's Alpha
  - Why needed here: IAA ensures quality control in crowdsourcing; Alpha is chosen because it handles multiple annotators and missing data
  - Quick check question: Why is Alpha preferred over Cohen's Kappa in this context?

- Concept: Semantic field filtering via embeddings
  - Why needed here: Task generation relies on extracting language-specific terms from domains like food; embeddings help when lexical databases are missing
  - Quick check question: How does the paper use AraBERT or IndoBERT to filter words into a semantic field?

## Architecture Onboarding

- Component map: Data prep module -> Task generation engine -> LingoGap platform -> Quality control pipeline -> Validation module
- Critical path: 1. Generate SL/TL datasets -> 2. Create tasks -> 3. Launch micro-tasks on LingoGap -> 4. Collect responses -> 5. Apply ACQs/time filtering -> 6. Compute IAA -> 7. Filter/retain based on threshold -> 8. Expert review low-IAA items -> 9. Export validated dataset
- Design tradeoffs: Micro-task size (35 words) vs. cognitive load vs. data volume; native speaker requirement vs. scalability to rare languages; IAA threshold (0.7) vs. risk of false negatives/positives; LLM vs. human annotation: accuracy vs. speed/cost
- Failure signatures: Low IAA scores persisting after filtering -> quality control design flaw; LLM hallucinated equivalents slipping through -> cultural context gap; demographic bias in worker pool -> unrepresentative data; timing bottlenecks (e.g., contributor availability) -> project delays
- First 3 experiments: 1. Replicate English→Arabic with a different semantic field (e.g., kinship) to test generalizability; 2. Run Indonesian→Banjarese using LLM pre-filtering to compare quality vs. human-only; 3. Conduct a sociolinguistic variation study: same task across age/region subgroups to detect bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs be fine-tuned specifically for low-resource languages to improve their accuracy in identifying lexical gaps?
- Basis in paper: [inferred] The paper compares LLM performance with human annotators and finds LLMs struggle with culturally specific terms
- Why unresolved: The study uses off-the-shelf LLMs without fine-tuning for the specific languages or cultural contexts
- What evidence would resolve it: Comparative experiments using fine-tuned LLMs versus human annotators on the same lexical diversity tasks

### Open Question 2
- Question: How does the LingoGap methodology scale when applied to non-food semantic domains like emotions or body parts?
- Basis in paper: [explicit] The authors suggest future work should explore culturally rich domains like emotions and body parts
- Why unresolved: The case studies only focus on food terminology
- What evidence would resolve it: Applying the methodology to multiple semantic domains and comparing the results

### Open Question 3
- Question: What is the optimal balance between crowd workers and expert validation to maximize accuracy while minimizing costs?
- Basis in paper: [inferred] The paper uses both crowd workers and expert validation but doesn't explore cost-effectiveness trade-offs
- Why unresolved: The study doesn't analyze the cost-benefit ratio of different validation approaches
- What evidence would resolve it: Experiments varying the proportion of expert versus crowd validation across different dataset sizes

## Limitations

- The paper doesn't provide evidence that crowd workers were systematically screened for cultural and linguistic expertise
- The choice of Krippendorff's Alpha threshold (0.7) isn't justified, and sensitivity to different thresholds isn't explored
- Technical implementation details of the LingoGap platform and code availability remain unclear, limiting reproducibility

## Confidence

- High: The core methodology of using native speakers for cross-lingual lexical gap detection is well-founded and addresses a real problem in LSR bias
- Medium: The quality control mechanisms (ACQs, IAA, expert review) are appropriate, but their effectiveness depends on implementation details not fully specified
- Medium: The claim that native speakers outperform LLMs is supported by examples but lacks systematic comparison with quantitative metrics

## Next Checks

1. Conduct a controlled experiment comparing native speaker annotations with LLM outputs on the same lexical pairs to quantify the performance gap
2. Implement a pilot study using a different semantic domain (e.g., kinship terms) to test the generalizability of the bidirectional methodology
3. Perform a sensitivity analysis on the IAA threshold to determine how different cutoff values affect the final dataset quality and size