---
ver: rpa2
title: Disentangling Latent Shifts of In-Context Learning with Weak Supervision
arxiv_id: '2410.01508'
source_url: https://arxiv.org/abs/2410.01508
tags:
- demonstrations
- answer
- should
- generalization
- adapter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes WILDA, a method that treats ICL as weak supervision
  to encode demonstration-induced latent shifts into reusable adapter modules. WILDA
  uses a teacher-student framework where an ICL-based teacher generates pseudo-labels
  on unlabeled data, and a student model learns to predict them using only the query,
  updating a lightweight adapter.
---

# Disentangling Latent Shifts of In-Context Learning with Weak Supervision

## Quick Facts
- **arXiv ID:** 2410.01508
- **Source URL:** https://arxiv.org/abs/2410.01508
- **Reference count:** 40
- **Key outcome:** WILDA improves ICL generalization and efficiency by encoding demonstration-induced latent shifts into reusable adapter modules

## Executive Summary
WILDA proposes a novel method that treats In-Context Learning (ICL) as weak supervision to disentangle and capture demonstration-induced latent shifts in large language models. By leveraging a teacher-student framework, WILDA uses an ICL-based teacher to generate pseudo-labels on unlabeled data, which a student model learns to predict using only the query, updating a lightweight adapter. This approach captures the full contextualized effect of demonstrations without repeated prompting or architectural intervention. WILDA demonstrates improved generalization, stability, and efficiency across both in-domain and out-of-domain tasks, often surpassing its teacher through pseudo-label correction and coverage expansion.

## Method Summary
WILDA introduces a teacher-student framework where an ICL-based teacher model generates pseudo-labels on unlabeled data, and a student model learns to predict these labels using only the query, updating a lightweight adapter module. This process disentangles the latent shifts induced by demonstrations, allowing for efficient reuse of the learned transformations without repeated prompting. Adapter modules can be fused across multiple demonstration subsets, enabling efficient adaptation to diverse tasks. WILDA outperforms standard ICL, PBFT, ICV, and Batch-ICL across GLUE and MMLU benchmarks, with statistically significant gains.

## Key Results
- WILDA consistently outperforms standard ICL, PBFT, ICV, and Batch-ICL across GLUE and MMLU benchmarks
- Adapter arithmetic enables efficient fusion of multiple demonstration subsets
- WILDA achieves statistically significant gains (e.g., 2.6â€“11.9% on Llama 3) compared to baseline methods
- The method improves generalization, stability, and efficiency across both in-domain and out-of-domain tasks

## Why This Works (Mechanism)
WILDA's effectiveness stems from its ability to disentangle and capture the latent shifts induced by demonstrations in a reusable form. By using an ICL-based teacher to generate pseudo-labels, the method leverages the contextualized effect of demonstrations without requiring repeated prompting. The student model learns to predict these pseudo-labels using only the query, updating a lightweight adapter that encodes the demonstration-induced transformations. This approach allows for efficient reuse of the learned shifts, improving generalization and stability across tasks. Adapter fusion further enhances efficiency by enabling the combination of multiple demonstration subsets.

## Foundational Learning
- **In-Context Learning (ICL):** The ability of large language models to perform tasks using demonstrations in the prompt. Needed to understand the baseline method being improved upon. Quick check: Verify that the model can perform tasks using prompt-based demonstrations.
- **Weak Supervision:** Using noisy or imperfect labels to train models. Needed to understand how pseudo-labels from the ICL teacher are used. Quick check: Ensure the pseudo-labels are sufficiently accurate for training.
- **Adapter Modules:** Lightweight neural network components that can be inserted into a pre-trained model to adapt it to new tasks. Needed to understand how WILDA captures and reuses demonstration-induced shifts. Quick check: Verify that the adapter can effectively capture and apply the learned transformations.
- **Teacher-Student Framework:** A training paradigm where a teacher model generates labels or guidance for a student model. Needed to understand the core training methodology of WILDA. Quick check: Ensure the teacher and student models are properly aligned and the student can effectively learn from the teacher's pseudo-labels.
- **Adapter Fusion:** The process of combining multiple adapter modules to create a more versatile model. Needed to understand how WILDA handles multiple demonstration subsets. Quick check: Verify that the fused adapter performs as well as or better than individual adapters.

## Architecture Onboarding

**Component Map:** Unlabeled Data -> ICL Teacher -> Pseudo-labels -> Student Model with Adapter -> Adapted Model

**Critical Path:** The critical path involves generating pseudo-labels using the ICL teacher, training the student model with these labels to update the adapter, and then using the adapted model for inference. This path ensures that the demonstration-induced latent shifts are captured and reused efficiently.

**Design Tradeoffs:** WILDA trades off the need for repeated prompting (as in standard ICL) for the efficiency of adapter-based adaptation. While this reduces inference costs, it introduces a dependency on the quality of pseudo-labels generated by the ICL teacher. Additionally, the method requires access to unlabeled data for pseudo-label generation, which may not always be available.

**Failure Signatures:** WILDA may fail if the ICL teacher generates poor-quality pseudo-labels, leading to incorrect adaptations in the student model. Additionally, if the unlabeled data is noisy or biased, the pseudo-labels may propagate these issues to the adapted model. Finally, the method may struggle with tasks that require highly specific or nuanced demonstrations that are not well-captured by the adapter.

**First 3 Experiments:**
1. Evaluate WILDA's performance on a small set of labeled data to assess the quality of pseudo-labels generated by the ICL teacher.
2. Test the efficiency of adapter fusion by combining adapters trained on different demonstration subsets and evaluating their performance on a diverse set of tasks.
3. Assess the robustness of WILDA to noisy or biased unlabeled data by introducing controlled noise into the pseudo-label generation process.

## Open Questions the Paper Calls Out
None

## Limitations
- WILDA's effectiveness depends on having access to unlabeled data for pseudo-label generation, which may not be available in all domains.
- The quality of pseudo-labels generated by the ICL teacher can propagate errors to the student model, potentially limiting performance.
- The method's benefits are primarily validated on GLUE and MMLU benchmarks, with limited exploration of specialized or domain-specific tasks.
- The study does not address potential biases introduced by the teacher model's predictions or the impact of noisy pseudo-labels on adapter training.

## Confidence
- **High confidence** in WILDA's ability to improve ICL efficiency and generalization on tested benchmarks
- **Medium confidence** in the scalability of adapter fusion for multiple demonstration subsets, as this is less extensively validated
- **Medium confidence** in the claim of surpassing the teacher model, as this depends on pseudo-label quality and task-specific dynamics

## Next Checks
1. Test WILDA's performance on noisy or biased unlabeled datasets to assess robustness to pseudo-label quality
2. Evaluate adapter fusion efficiency and accuracy on tasks with diverse or conflicting demonstration subsets
3. Investigate the impact of teacher model size and capability on WILDA's performance across different domains