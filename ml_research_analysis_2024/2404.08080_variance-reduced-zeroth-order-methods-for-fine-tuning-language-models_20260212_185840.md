---
ver: rpa2
title: Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models
arxiv_id: '2404.08080'
source_url: https://arxiv.org/abs/2404.08080
tags:
- mezo-svrg
- fine-tuning
- mezo
- fo-sgd
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses memory constraints in fine-tuning large language
  models (LMs) by proposing a zeroth-order optimization method. The core idea is to
  combine zeroth-order optimization with variance reduction techniques to improve
  stability and convergence.
---

# Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models

## Quick Facts
- **arXiv ID:** 2404.08080
- **Source URL:** https://arxiv.org/abs/2404.08080
- **Reference count:** 40
- **Primary result:** Memory-efficient zeroth-order optimization with variance reduction achieves up to 20% higher accuracy than baseline while using 2× less GPU-hours for fine-tuning large language models

## Executive Summary
This paper introduces MeZO-SVRG, a memory-efficient zeroth-order optimization method for fine-tuning large language models that combines variance reduction techniques with efficient gradient estimation. The method addresses the memory constraints of traditional backpropagation-based fine-tuning by using zeroth-order optimization (ZOO) with Stochastic Perturbation Stochastic Approximation (SPSA) estimators. MeZO-SVRG achieves superior performance by periodically incorporating fullbatch gradient estimates to reduce variance, while maintaining low memory footprint through in-place operations and single perturbation vector storage. The approach eliminates the need for task-specific prompts that were previously required in zeroth-order methods.

## Method Summary
MeZO-SVRG is a zeroth-order optimization algorithm that extends the Memory-efficient Zeroth-order (MeZO) method by incorporating variance reduction techniques. The algorithm uses SPSA to estimate gradients without backpropagation, perturbed with a single vector z applied across the entire batch for parallelization. Every q steps, the method performs a fullbatch update that serves as a control variate to reduce the variance of minibatch gradient estimates. Memory efficiency is achieved through in-place operations that minimize variable allocations, storing only current parameters, a copy from the last fullbatch update, and the fullbatch gradient estimate. The method leverages data parallelism while maintaining a small memory footprint, making it suitable for fine-tuning models with billions of parameters.

## Key Results
- MeZO-SVRG achieves up to 20% higher test accuracy compared to baseline MeZO method across multiple language model fine-tuning tasks
- The method reduces GPU-hours by 2× while achieving superior performance compared to first-order methods
- Memory requirements are significantly reduced compared to first-order methods, with greater savings as batch sizes and context lengths increase

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MeZO-SVRG achieves faster convergence by reducing gradient variance through periodic fullbatch updates.
- Mechanism: The method combines minibatch SPSA estimates with fullbatch estimates every q steps. The fullbatch estimate acts as a control variate that reduces the variance of the minibatch estimate.
- Core assumption: The variance of the minibatch gradient estimator decreases when combined with a low-variance fullbatch estimate.
- Evidence anchors:
  - [abstract] "we couple ZO methods with variance reduction techniques to enhance stability and convergence"
  - [section] "we propose the Memory Efficient Zeroth-Order Stochastic Variance-Reduced Gradient (MeZO-SVRG) method: a ZO algorithm that combines fullbatch and minibatch information to yield asymptotically unbiased, low-variance gradient estimators"
  - [corpus] Weak - no direct evidence in related papers about variance reduction in ZO methods

### Mechanism 2
- Claim: MeZO-SVRG maintains memory efficiency through in-place operations and single perturbation vectors.
- Mechanism: The algorithm stores only current parameters, a copy of parameters from last fullbatch update, and the fullbatch gradient estimate. Gradient computation uses a single perturbation vector z perturbed across the entire batch.
- Core assumption: In-place operations and single perturbation vector storage don't significantly impact computational speed while maintaining low memory footprint.
- Evidence anchors:
  - [abstract] "uses efficient gradient estimators and in-place operations to minimize memory footprint"
  - [section] "MeZO-SVRG leverages in-place operations to minimize memory allocation for new variable definitions"
  - [corpus] Weak - related papers mention memory efficiency but not the specific in-place implementation details

### Mechanism 3
- Claim: MeZO-SVRG achieves better test accuracy by exploring a more stable optimization landscape without relying on task-specific prompts.
- Mechanism: By reducing gradient variance and maintaining stable updates, the method can navigate the loss landscape more effectively even without carefully crafted prompts that were previously needed for MeZO.
- Core assumption: The variance reduction allows the optimizer to find better minima in the non-prompted setting.
- Evidence anchors:
  - [abstract] "eliminating the reliance on task-specific prompts" and "consistently outperforms MeZO with up to 20% increase in test accuracies"
  - [section] "variance-reduction enhances the stability and convergence properties of ZO methods" and "we demonstrate empirically that MeZO-SVRG outperforms MeZO consistently on a variety of LM fine-tuning tasks, even in a challenging non-prompted setting"
  - [corpus] Weak - related papers don't address prompt-free optimization in ZO methods

## Foundational Learning

- Concept: Zeroth-order optimization and SPSA gradient estimation
  - Why needed here: MeZO-SVRG builds on MeZO which uses SPSA estimators to approximate gradients without backpropagation
  - Quick check question: What is the difference between SPSA and finite difference gradient estimation?

- Concept: Variance reduction techniques in stochastic optimization
  - Why needed here: The core innovation is combining variance reduction (from SVRG) with zeroth-order methods
  - Quick check question: How does SVRG reduce variance compared to standard SGD?

- Concept: Memory-efficient implementation techniques for large models
  - Why needed here: The method must work with models up to 7B parameters while keeping memory low
  - Quick check question: What is the memory cost of storing activation gradients during backpropagation?

## Architecture Onboarding

- Component map: MeZO-SVRG -> SPSA gradient estimator -> Variance reduction (fullbatch updates) -> In-place memory management -> Learning rate scheduling
- Critical path: Forward pass → Compute loss → Perturb parameters with single z vector → Compute perturbed losses → Estimate gradient → Update parameters with variance reduction → Repeat. The fullbatch update path runs every q steps.
- Design tradeoffs: Memory vs speed tradeoff in gradient estimation (single perturbation vector vs per-sample vectors), memory vs stability tradeoff (storing fullbatch estimate vs recomputing), prompt-free vs prompted fine-tuning.
- Failure signatures: Divergence with small batch sizes, plateauing test accuracy, out-of-memory errors with large models, slow convergence compared to FO methods.
- First 3 experiments:
  1. Run MeZO-SVRG on a small MLP with MNIST to verify basic functionality and compare with MeZO
  2. Test memory usage on a medium-sized masked LM with varying batch sizes to confirm the 2× memory savings claim
  3. Evaluate convergence on a small language model task (e.g., DistilBERT on SST-2) to verify the 20% accuracy improvement claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MeZO-SVRG perform when applied to non-differentiable objectives beyond the ranking tasks in RLHF mentioned?
- Basis in paper: [explicit] The paper mentions that MeZO-SVRG is applicable to settings where gradients are inaccessible or infeasible to compute, such as when considering non-differentiable objectives like ranking in RLHF.
- Why unresolved: The paper does not provide empirical results for MeZO-SVRG applied to non-differentiable objectives other than the mentioned RLHF ranking tasks.
- What evidence would resolve it: Empirical results showing MeZO-SVRG performance on other non-differentiable objectives, such as optimization tasks with discrete variables or black-box functions.

### Open Question 2
- Question: What is the impact of varying the minibatch size b on MeZO-SVRG's performance, particularly for very large batch sizes?
- Basis in paper: [inferred] The paper mentions that MeZO-SVRG benefits from larger batch sizes, with memory savings improving as batch sizes increase. However, it does not explore the upper limits of this benefit.
- Why unresolved: The paper does not investigate the performance of MeZO-SVRG for extremely large batch sizes, which could reveal potential limitations or further improvements.
- What evidence would resolve it: Empirical results showing MeZO-SVRG performance for a wide range of minibatch sizes, including very large values, would clarify the impact of batch size on the algorithm's effectiveness.

### Open Question 3
- Question: How does MeZO-SVRG compare to other variance reduction techniques for ZO methods in the context of LM fine-tuning?
- Basis in paper: [inferred] The paper focuses on the specific combination of ZO methods with SVRG, but does not compare it to other variance reduction techniques like SAGA or SARAH.
- Why unresolved: The paper does not provide a comprehensive comparison of MeZO-SVRG to other variance reduction techniques for ZO methods in LM fine-tuning.
- What evidence would resolve it: Empirical results comparing MeZO-SVRG to other variance reduction techniques, such as SAGA or SARAH, on the same LM fine-tuning tasks would provide insights into its relative effectiveness.

## Limitations

- The empirical evidence for variance reduction effectiveness is primarily based on observed performance gains rather than direct measurement of gradient variance
- Memory efficiency claims rely on in-place operations that may not be universally applicable across different hardware architectures or deep learning frameworks
- The superiority of MeZO-SVRG in non-prompted settings is demonstrated, but the paper doesn't thoroughly explore the limitations of prompt-free optimization

## Confidence

- **High confidence**: Memory efficiency claims (2× reduction in GPU-hours) - supported by clear comparative results with first-order methods
- **Medium confidence**: Variance reduction mechanism - supported by performance improvements but lacking direct variance measurements
- **Medium confidence**: Accuracy improvements (up to 20% increase) - demonstrated across multiple tasks but without ablation studies isolating the impact of each component

## Next Checks

1. Implement variance tracking to directly measure gradient variance in MeZO-SVRG vs MeZO across different batch sizes and verify the theoretical variance reduction claims.
2. Test the memory efficiency claims on different hardware configurations (e.g., different GPU architectures) and deep learning frameworks to validate the generalizability of the in-place operations approach.
3. Conduct an ablation study to quantify the individual contributions of variance reduction, in-place operations, and the single perturbation vector strategy to overall performance improvements.