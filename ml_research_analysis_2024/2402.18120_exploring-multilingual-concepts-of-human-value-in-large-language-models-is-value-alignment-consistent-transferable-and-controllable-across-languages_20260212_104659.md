---
ver: rpa2
title: 'Exploring Multilingual Concepts of Human Value in Large Language Models: Is
  Value Alignment Consistent, Transferable and Controllable across Languages?'
arxiv_id: '2402.18120'
source_url: https://arxiv.org/abs/2402.18120
tags:
- concept
- language
- languages
- cross-lingual
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the multilingual encoding and cross-lingual
  transfer of human value-related concepts in large language models. The authors propose
  a framework to extract multilingual concept vectors and evaluate their consistency
  and transferability across 16 languages and 3 LLM families.
---

# Exploring Multilingual Concepts of Human Value in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?

## Quick Facts
- arXiv ID: 2402.18120
- Source URL: https://arxiv.org/abs/2402.18120
- Reference count: 40
- Key outcome: Multilingual value concept encoding in LLMs shows cross-lingual inconsistency due to resource disparities, with unidirectional transfer from high- to low-resource languages and effective cross-lingual value alignment control

## Executive Summary
This paper investigates how large language models encode human value-related concepts across 16 languages and whether these encodings are consistent, transferable, and controllable. The authors propose a framework using Representation Engineering (RepE) to extract multilingual concept vectors and evaluate their properties across three LLM families. They find that while larger models and more balanced multilingual training data improve concept encoding precision, significant cross-lingual inconsistencies exist between high- and low-resource languages. The study demonstrates that value alignment can be effectively transferred across languages using dominant language vectors, with unidirectional transfer occurring from high- to low-resource languages.

## Method Summary
The authors extract multilingual concept vectors from LLMs using positive and negative text pairs for seven human values across 16 languages. They evaluate cross-lingual consistency through cosine similarity of concept vectors, assess concept recognition accuracy across languages, and test cross-lingual transferability by using dominant language vectors to control model behavior in other languages. The methodology involves machine translation of English value datasets to other languages, extraction of concept vectors via RepE's mean-based approach, and perturbation-based control experiments to manipulate model behavior across linguistic boundaries.

## Key Results
- LLMs encode value concepts in multiple languages with performance improving with model size
- Cross-lingual inconsistency exists between high- and low-resource languages due to language resource disparities
- Unidirectional cross-lingual transfer occurs from high- to low-resource languages
- Value alignment can be effectively transferred across languages using dominant language vectors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual consistency of value concepts depends on language resource balance in pre-training data.
- Mechanism: When language resources are imbalanced, models develop language-specific concept vectors that lack consistency across languages. High-resource languages dominate representation learning, leading to distorted linguistic relationships.
- Core assumption: Concept vectors extracted via RepE capture the true direction of value concepts in representation space.
- Evidence anchors:
  - [abstract] "Cross-lingual inconsistency exists between high- and low-resource languages due to language resource disparities"
  - [section] "an imbalance in language resources results in cross-lingual inconsistency (§4.3.1), distorted linguistic relationships (§4.3.2)"
  - [corpus] Weak - corpus provides contextual support but no direct experimental evidence
- Break condition: If concept vectors do not accurately represent abstract concepts, or if resource imbalance does not affect representation learning.

### Mechanism 2
- Claim: Value alignment can be transferred across languages using dominant language concept vectors.
- Mechanism: Concept vectors extracted from high-resource languages can be used to control model behavior in other languages through hidden state perturbation, achieving cross-lingual manipulation of value alignment.
- Core assumption: The direction of concept vectors is preserved across languages, enabling transfer of alignment control.
- Evidence anchors:
  - [abstract] "value alignment can be effectively transferred across languages using dominant language vectors"
  - [section] "leveraging the dominant language as a source language (§5)"
  - [corpus] Weak - corpus neighbors discuss cross-lingual alignment but not specifically value alignment transfer
- Break condition: If concept vector directions are not preserved across languages, or if perturbation does not affect model behavior.

### Mechanism 3
- Claim: Model size and multilingual pre-training data composition affect multilingual concept encoding precision.
- Mechanism: Larger models with more balanced multilingual pre-training data capture value concepts more precisely across languages due to increased capacity and reduced resource bias.
- Core assumption: Model capacity and training data composition directly impact representation quality for abstract concepts.
- Evidence anchors:
  - [abstract] "performance improving with model size"
  - [section] "the expansion of model size and the richness of language resources both contribute to a more precise capture of these concepts (§4.2)"
  - [corpus] Weak - corpus neighbors discuss multilingual LLMs but not specifically model size effects on concept encoding
- Break condition: If model capacity does not scale with concept encoding precision, or if data composition does not affect representation quality.

## Foundational Learning

- Concept: Representation Engineering (RepE)
  - Why needed here: Core methodology for extracting and manipulating concept vectors in LLMs
  - Quick check question: How does RepE extract concept vectors from LLMs using positive and negative text pairs?

- Concept: Cross-lingual similarity and transfer
  - Why needed here: Key metrics for evaluating multilingual consistency and transferability of value concepts
  - Quick check question: What does high cross-lingual cosine similarity between concept vectors indicate about multilingual consistency?

- Concept: Multilingual pre-training data composition
  - Why needed here: Critical factor affecting cross-lingual properties of concept representations
  - Quick check question: How does the proportion of high-resource languages in pre-training data affect cross-lingual consistency?

## Architecture Onboarding

- Component map: Translate English value datasets → Extract concept vectors using RepE → Compute cross-lingual similarity → Perform concept recognition → Evaluate control effectiveness
- Critical path: Extract concept vectors → Compute cross-lingual similarity → Perform concept recognition → Evaluate control effectiveness
- Design tradeoffs:
  - Translation quality vs. research scope (machine translation introduces noise but enables broader language coverage)
  - Model size vs. computational resources (larger models capture concepts better but require more resources)
  - Dominant language vs. balanced multilingual data (dominant languages enable easier transfer but risk bias)
- Failure signatures:
  - Low concept recognition accuracy indicates poor concept encoding
  - Low cross-lingual similarity indicates inconsistent multilingual representations
  - Ineffective control indicates poor cross-lingual transfer of alignment
- First 3 experiments:
  1. Extract concept vectors for deontology from LLaMA2-chat-7B and evaluate concept recognition accuracy across all languages
  2. Compute cross-lingual cosine similarity between concept vectors for harmfulness across all language pairs in BLOOMZ-7B1
  3. Use English concept vector for harmfulness to control LLaMA2-chat-7B behavior on French harmful instructions and measure following rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do cultural variations in value interpretations impact the effectiveness of cross-lingual value alignment control?
- Basis in paper: [inferred] The authors acknowledge that cultural variations can result in diverse interpretations of values, but they do not explore how this might affect cross-lingual control effectiveness.
- Why unresolved: The paper focuses on universal cross-lingual patterns and does not delve into cultural-specific value interpretations.
- What evidence would resolve it: Comparative studies of cross-lingual control effectiveness across different cultural contexts or datasets reflecting diverse cultural interpretations of values.

### Open Question 2
- Question: What is the long-term stability and generalizability of cross-lingual concept vectors across different model architectures and training regimes?
- Basis in paper: [inferred] The study evaluates cross-lingual consistency and transferability within specific LLM families, but does not examine long-term stability or generalizability to other architectures.
- Why unresolved: The experiments are limited to 3 LLM families, and the authors do not discuss how concept vectors might change over time or with different training approaches.
- What evidence would resolve it: Longitudinal studies tracking concept vector stability across model updates and comparative analyses across diverse LLM architectures.

### Open Question 3
- Question: How can multilingual pre-training data be optimized to minimize cross-lingual inconsistencies while preserving linguistic diversity?
- Basis in paper: [explicit] The authors suggest more balanced distributions of non-dominant languages but do not provide concrete optimization strategies.
- Why unresolved: The paper identifies the problem of cross-lingual inconsistency due to language resource disparities but does not offer specific solutions for data composition.
- What evidence would resolve it: Empirical studies testing different data composition strategies and their impact on cross-lingual consistency metrics.

## Limitations
- Reliance on machine-translated datasets rather than natively collected multilingual data may introduce noise affecting concept vector extraction quality
- Study focuses on only three LLM families, limiting architectural generalizability of findings
- Cross-lingual consistency measurements may be influenced by translation artifacts rather than true representation differences

## Confidence
- Cross-lingual inconsistency findings: Medium confidence
- Cross-lingual transfer effectiveness: Medium confidence
- Model size effects: Low-Medium confidence

## Next Checks
1. Conduct a comparative study using natively collected multilingual datasets versus machine-translated datasets to isolate the effect of translation quality on cross-lingual consistency measurements.

2. Replicate the cross-lingual value alignment control experiments using alternative perturbation methods (e.g., feature attribution-based methods) to confirm that the observed unidirectional transfer is not method-specific.

3. Extend the concept encoding and cross-lingual consistency analysis to additional LLM architectures (e.g., Mistral, Gemma) and multilingual-specific models to assess the generalizability of the findings beyond the three model families studied.