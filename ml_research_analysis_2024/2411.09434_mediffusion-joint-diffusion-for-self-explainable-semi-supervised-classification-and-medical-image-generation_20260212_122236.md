---
ver: rpa2
title: 'Mediffusion: Joint Diffusion for Self-Explainable Semi-Supervised Classification
  and Medical Image Generation'
arxiv_id: '2411.09434'
source_url: https://arxiv.org/abs/2411.09434
tags:
- diffusion
- classifier
- data
- medical
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Mediffusion, a novel semi-supervised learning
  approach for medical image classification that combines a diffusion model with a
  classifier in a shared parametrization. The method addresses the challenges of scarce
  labeled data and the need for explainability in medical imaging by leveraging both
  labeled and unlabeled data through joint training.
---

# Mediffusion: Joint Diffusion for Self-Explainable Semi-Supervised Classification and Medical Image Generation

## Quick Facts
- arXiv ID: 2411.09434
- Source URL: https://arxiv.org/abs/2411.09434
- Reference count: 40
- Primary result: Achieves 78.42% AUC on ChestXRay14 with only 20% labeled data while providing visual counterfactual explanations

## Executive Summary
Mediffusion introduces a novel semi-supervised learning approach for medical image classification that combines a diffusion model with a classifier in a shared parametrization. The method addresses the challenges of scarce labeled data and the need for explainability in medical imaging by leveraging both labeled and unlabeled data through joint training. Mediffusion achieves classification performance comparable to state-of-the-art semi-supervised methods while providing visual counterfactual explanations for model decisions. The approach also enables generation of synthetic data and produces accurate counterfactual examples that align with expert annotations.

## Method Summary
Mediffusion employs a latent diffusion model with a shared UNet architecture that performs both denoising and classification tasks. The method first encodes high-resolution medical images into a compressed latent space using a vision autoencoder, then jointly trains a diffusion model and classifier on both labeled and unlabeled data. During sampling, classifier guidance adjusts noise predictions based on class gradients to generate counterfactual examples. The model is trained using Adam optimizer with specific learning rates and batch sizes, with experiments conducted on ChestXRay14 and ISIC2019 datasets using varying percentages of labeled data.

## Key Results
- Achieves 78.42% AUC on ChestXRay14 with only 20% labeled data
- Generates synthetic data with competitive FID/KID scores compared to state-of-the-art methods
- Domain experts rate generated counterfactual explanations as more effective at showing disease indicator changes compared to external classifier guidance methods

## Why This Works (Mechanism)

### Mechanism 1
Joint diffusion model enables simultaneous learning of generative and discriminative representations, improving classification performance under limited labeled data. The UNet encoder extracts hierarchical features from noisy latent representations, which are shared between the diffusion denoising task and the classification task. This shared representation allows the model to learn richer, more generalizable features from both labeled and unlabeled data.

### Mechanism 2
Classifier guidance in diffusion models allows generation of counterfactual examples that explain model decisions by showing minimal modifications needed to change predictions. During sampling, the noise prediction is adjusted based on the gradient of the log-probability of the target class with respect to the latent representation. This guides the generation process toward or away from specific class predictions.

### Mechanism 3
Latent diffusion models are more computationally efficient for high-resolution medical images compared to pixel-space diffusion models. Images are first encoded into a lower-dimensional latent space using a vision autoencoder, and the diffusion process operates in this compressed representation rather than the full pixel space.

## Foundational Learning

- **Diffusion Models (Denoising Diffusion Probabilistic Models)**
  - Why needed here: The core generative mechanism that enables both data synthesis and the creation of counterfactual explanations
  - Quick check question: What is the forward process in a diffusion model, and how does it differ from the reverse process?

- **Semi-Supervised Learning**
  - Why needed here: The method leverages both labeled and unlabeled data to improve classification performance when labeled data is scarce
  - Quick check question: How does the joint training of generative and discriminative tasks in Mediffusion differ from traditional semi-supervised approaches like pseudo-labeling?

- **Counterfactual Explanations**
  - Why needed here: Provides the mechanism for generating visual explanations that show what changes would alter the model's prediction
  - Quick check question: What is the difference between post-hoc explanation methods and self-explainable models like Mediffusion?

## Architecture Onboarding

- **Component map:**
  Vision Autoencoder (Encoder + Decoder) -> UNet-based Latent Diffusion Model -> Classifier Module -> Classifier Guidance

- **Critical path:**
  1. Train autoencoder on all available data (unsupervised)
  2. Jointly train diffusion model and classifier on labeled and unlabeled data
  3. Use trained model for classification, generation, and counterfactual explanation

- **Design tradeoffs:**
  - Latent space compression vs. preservation of diagnostic features
  - Shared UNet parameters vs. specialized models for each task
  - Classifier guidance strength vs. generation quality and realism

- **Failure signatures:**
  - Poor classification performance despite good generation: Shared features may not be discriminative enough
  - Unstable or unrealistic counterfactuals: Classifier gradients may be unreliable or guidance strength too high
  - High computational cost: Latent space may be too large or autoencoder inefficient

- **First 3 experiments:**
  1. Train autoencoder only on the dataset and evaluate reconstruction quality
  2. Train diffusion model without classifier and evaluate generation quality
  3. Jointly train diffusion and classifier with 100% labeled data to establish upper bound performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but several important questions arise from the work:

1. How does Mediffusion's joint training approach compare to other semi-supervised methods that use separate generative and discriminative models?
2. What is the optimal balance between the diffusion loss and classification loss in Mediffusion's joint training objective?
3. How does Mediffusion's explainability performance compare to other explainable AI methods specifically designed for medical imaging?
4. How does the performance of Mediffusion scale with increasing dataset size beyond the 20% labeled data threshold tested in the experiments?
5. How sensitive is Mediffusion's performance to the choice of autoencoder architecture and latent space dimensionality?

## Limitations

- The explainability evaluation relies primarily on subjective expert ratings rather than objective metrics
- The paper does not systematically explore the hyperparameter space, particularly for classifier guidance scale and loss weighting
- Limited investigation of the model's performance with higher percentages of labeled data beyond the 20% threshold

## Confidence

- **High confidence**: Classification performance claims (78.42% AUC with 20% labeled data on ChestXRay14) - supported by quantitative metrics and comparison to baselines
- **Medium confidence**: Generation quality claims (FID/KID scores) - metrics reported but visual inspection limited to supplementary materials
- **Low confidence**: Counterfactual explanation effectiveness - primarily based on subjective expert ratings rather than objective metrics or ablation studies

## Next Checks

1. **Ablation study on classifier guidance scale**: Systematically vary the guidance strength parameter to determine its impact on both classification accuracy and explanation quality, identifying potential trade-offs or optimal ranges.

2. **Cross-dataset generalization test**: Apply the trained Mediffusion model to a held-out medical imaging dataset (e.g., different X-ray or dermoscopic datasets) to assess whether the joint diffusion approach generalizes beyond the training distribution.

3. **Counterfactual reliability analysis**: Compare the generated counterfactual explanations against established feature attribution methods (e.g., Grad-CAM, SHAP) to evaluate whether the diffusion-based explanations provide consistent or complementary insights about model decisions.