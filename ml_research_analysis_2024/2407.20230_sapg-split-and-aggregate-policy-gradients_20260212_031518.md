---
ver: rpa2
title: 'SAPG: Split and Aggregate Policy Gradients'
arxiv_id: '2407.20230'
source_url: https://arxiv.org/abs/2407.20230
tags:
- policy
- data
- sapg
- environments
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the diminishing returns of scaling on-policy
  RL methods like PPO to extremely large parallel environments (10k). The core issue
  is that IID sampling from a Gaussian policy leads to duplicated data when many environments
  execute similar actions.
---

# SAPG: Split and Aggregate Policy Gradients

## Quick Facts
- arXiv ID: 2407.20230
- Source URL: https://arxiv.org/abs/2407.20230
- Reference count: 40
- Key outcome: SAPG achieves 12-66% higher success rates than DexPBT and significantly outperforms PPO and PQL on challenging manipulation tasks by training multiple diverse policies and combining their data via importance sampling.

## Executive Summary
This paper addresses the diminishing returns of scaling on-policy RL methods like PPO to extremely large parallel environments (>10k). The core issue is that IID sampling from a Gaussian policy leads to duplicated data when many environments execute similar actions. The proposed solution, SAPG, trains multiple diverse policies in parallel and combines their data using off-policy importance sampling, with one "leader" policy updated using data from all others. SAPG achieves significantly higher asymptotic performance than PPO, PQL, and DexPBT on challenging manipulation tasks including AllegroKuka environments (up to 66% more successes on reorientation) and in-hand reorientation tasks with ShadowHand and AllegroHand, where PPO and PQL fail to make progress.

## Method Summary
SAPG addresses the diminishing returns of scaling PPO to extremely large parallel environments by training multiple diverse policies instead of one. The method splits N environments into M blocks, each with its own policy πᵢ. All policies share a backbone network Bθ with local conditioning parameters ϕᵢ. The leader policy π₁ is updated using both on-policy data and importance-weighted off-policy data from follower policies π₂...πₘ. Entropy regularization encourages diversity among followers. The leader uses PPO's clipped surrogate objective with an additional off-policy term weighted by λ=1. Experiments use 24,576 parallel environments in IsaacGym on AllegroKuka manipulation tasks and in-hand reorientation tasks.

## Key Results
- SAPG achieves 12-66% higher success rates than DexPBT on AllegroKuka tasks
- Outperforms PPO and PQL baselines on reorientation tasks with up to 66% more successes
- Shows consistent improvement across all tested manipulation tasks with both AllegroHand and ShadowHand
- Ablation studies show entropy regularization (λₑₙₜ = 0.005) helps on reorientation tasks but not all tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Splitting environments into blocks and training diverse follower policies avoids action duplication from IID sampling.
- Mechanism: When N environments share one Gaussian policy, most actions cluster near the mean, causing redundancy. Partitioning into M blocks with separate policies forces each block to sample from a different region of the action space, increasing diversity.
- Core assumption: Each block's policy can generate statistically distinct action distributions while still covering useful regions of the state space.
- Evidence anchors:
  - [abstract] states that IID sampling from a Gaussian leads to duplicated data when many environments execute similar actions.
  - [section 4] explains that splitting environments into blocks with separate policies allows more data diversity than i.i.d. sampling.
  - [corpus] has no direct evidence supporting this claim.
- Break condition: If all follower policies converge to the same behavior, the diversity gain disappears and SAPG reverts to vanilla PPO.

### Mechanism 2
- Claim: Importance-weighted aggregation of off-policy data from followers enables the leader to benefit from high-reward trajectories discovered by followers.
- Mechanism: Each follower explores its own action space, potentially finding better trajectories. The leader updates using both its on-policy data and off-policy data from followers weighted by importance sampling, allowing it to adopt successful behaviors without being constrained by its own exploration.
- Core assumption: The importance sampling weights remain bounded and the off-policy data provides gradients correlated with the true policy gradient.
- Evidence anchors:
  - [section 4.1] describes using importance sampling to combine data from multiple policies into a single update.
  - [abstract] mentions that the leader policy is updated using data from all followers via importance sampling.
  - [corpus] has no direct evidence supporting this claim.
- Break condition: If importance weights become too large or too small, the off-policy gradient estimates become too noisy and destabilize training.

### Mechanism 3
- Claim: Latent conditioning on shared backbone parameters plus entropy regularization encourages diversity among follower policies.
- Mechanism: Shared backbone Bθ ensures followers start from the same representation, while local parameters ϕi allow divergence. Entropy regularization in followers pushes them to explore more broadly, preventing them from collapsing to the same behavior as the leader.
- Core assumption: The entropy coefficient can be tuned to balance exploration vs exploitation appropriately for each task.
- Evidence anchors:
  - [section 4.4] explains using shared backbone with local conditioning to encourage diversity.
  - [section 4.5] describes adding entropy loss to followers to further encourage diversity.
  - [section 6.3] shows ablation results where entropy regularization helps on reorientation tasks.
  - [corpus] has no direct evidence supporting this claim.
- Break condition: If entropy coefficient is too high, followers may explore too much and find no useful trajectories; if too low, they may converge to leader behavior.

## Foundational Learning

- Concept: Importance sampling for off-policy correction
  - Why needed here: SAPG combines data from different policies, which requires reweighting to account for distribution shift.
  - Quick check question: What happens to the importance weight if the target and behavior policies have very different action distributions?

- Concept: Trust region methods and clipping for policy stability
  - Why needed here: The leader uses PPO's clipped objective to prevent large policy updates that could destroy performance.
  - Quick check question: Why does PPO clip the probability ratio between old and new policies?

- Concept: Policy gradient variance reduction techniques
  - Why needed here: High variance gradients from importance sampling can destabilize training; understanding variance sources is critical.
  - Quick check question: What are two common methods to reduce variance in policy gradient estimates?

## Architecture Onboarding

- Component map: Environment rollout -> Data collection in buffers D₁...Dₘ -> Policy updates (leader with off-policy aggregation, followers with on-policy) -> Repeat

- Critical path: Environment rollout → data collection → policy updates (leader with off-policy aggregation, followers with on-policy) → repeat

- Design tradeoffs:
  - Number of policies M vs environments per policy N/M: More policies increase diversity but reduce per-policy sample efficiency
  - Entropy coefficient λₑₙₜ: Higher values increase exploration but may slow convergence
  - Off-policy ratio λ: Higher values incorporate more follower data but increase gradient variance
  - Shared vs disjoint parameters: Shared parameters enable knowledge transfer but may limit diversity

- Failure signatures:
  - Leader performance plateaus early: Likely followers aren't discovering better trajectories
  - Followers all perform similarly: Entropy regularization may be too low or shared backbone too strong
  - Training becomes unstable: Importance weights may be too large; reduce off-policy ratio or subsample off-policy data

- First 3 experiments:
  1. Implement basic SAPG with M=2 policies on a simple continuous control task (e.g., HalfCheetah) with no entropy regularization
  2. Add entropy regularization to followers and tune λₑₙₜ on HalfCheetah
  3. Scale to M=6 policies and test on a more complex task (e.g., Humanoid) while monitoring follower diversity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of policies (M) and environments per policy (N/M) for SAPG to maximize performance?
- Basis in paper: [explicit] The paper mentions using M = 6 policies for their experiments but does not explore different values or provide theoretical guidance on optimal M.
- Why unresolved: The paper uses a fixed value of M = 6 without exploring sensitivity to this hyperparameter or providing theoretical justification for this choice.
- What evidence would resolve it: Systematic experiments varying M (e.g., 2, 4, 6, 8, 10) and measuring performance across different environments would identify optimal values and tradeoffs.

### Open Question 2
- Question: How does SAPG's performance scale with different levels of task complexity beyond the tested manipulation tasks?
- Basis in paper: [inferred] The paper tests on manipulation tasks but only briefly mentions "complex environments" and "challenging environments" without defining complexity metrics or testing beyond manipulation.
- Why unresolved: The experiments are limited to manipulation tasks of varying difficulty, but the paper doesn't explore how SAPG performs on other types of tasks (e.g., navigation, strategy games, or continuous control) or provide complexity measures.
- What evidence would resolve it: Testing SAPG on a diverse benchmark suite with tasks spanning different domains and complexity levels (e.g., DM Control Suite, Atari, or MuJoCo) would reveal generalization capabilities.

### Open Question 3
- Question: What is the theoretical justification for using λ = 1 in the off-policy term and how sensitive is performance to this hyperparameter?
- Basis in paper: [explicit] The paper states "we choose λ = 1" but provides no theoretical justification or empirical sensitivity analysis for this choice.
- Why unresolved: The paper simply selects λ = 1 without explaining why this value is appropriate or testing how performance varies with different λ values.
- What evidence would resolve it: A systematic ablation study varying λ (e.g., 0.1, 0.5, 1.0, 2.0) and measuring performance would reveal sensitivity and potentially optimal values for different environments.

## Limitations

- The paper lacks direct experimental evidence quantifying the action distribution overlap between follower policies, making it unclear how much diversity is actually achieved in practice.
- Network architecture details are sparse, particularly how the critic is conditioned on local parameters and the exact importance sampling implementation.
- The scalability analysis only considers M=6 policies, leaving uncertainty about whether the approach scales to hundreds of policies or if there are diminishing returns.

## Confidence

- **High confidence** in the core problem identification (diminishing returns from IID sampling at scale) and the general algorithmic approach (leader-follower architecture with importance sampling).
- **Medium confidence** in the effectiveness of entropy regularization for promoting diversity, as ablation results are only shown on reorientation tasks and the mechanism isn't deeply validated.
- **Low confidence** in the exact implementation details needed for reproduction, particularly the critic architecture and importance sampling hyperparameters.

## Next Checks

1. Implement a controlled experiment varying the number of follower policies M while keeping total environments constant, measuring both performance and follower policy diversity (using PCA reconstruction error or similar metric) to isolate the diversity effect.

2. Run ablation studies systematically across all task types with entropy coefficient λₑₙₜ ∈ {0, 0.001, 0.003, 0.005, 0.01} to identify optimal settings and verify the claimed benefits.

3. Test the scalability limit by training with M ∈ {2, 6, 12, 24} policies on the same tasks while monitoring computational overhead and performance returns to identify the optimal policy count for different problem scales.