---
ver: rpa2
title: 'Lisa: Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning
  Attack'
arxiv_id: '2405.18641'
source_url: https://arxiv.org/abs/2405.18641
tags:
- fine-tuning
- alignment
- arxiv
- answer
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Lisa is a method designed to mitigate harmful fine-tuning attacks
  on large language models (LLMs) by introducing a lazy safety alignment approach.
  It addresses the issue of convergence instability in Bi-State Optimization (BSO),
  which arises when steps allocated to alignment are too small.
---

# Lisa: Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning Attack

## Quick Facts
- arXiv ID: 2405.18641
- Source URL: https://arxiv.org/abs/2405.18641
- Authors: Tiansheng Huang; Sihao Hu; Fatih Ilhan; Selim Furkan Tekin; Ling Liu
- Reference count: 40
- Primary result: Reduces harmful outputs by up to 6.54% while maintaining fine-tuning accuracy

## Executive Summary
Lisa addresses harmful fine-tuning attacks on large language models by introducing a lazy safety alignment approach. The method solves convergence instability in Bi-State Optimization (BSO) by adding a proximal term that constrains drift between optimization states. This ensures better alignment performance while maintaining task accuracy. The approach is effective across diverse models, datasets, and attack settings.

## Method Summary
Lisa implements a two-stage pipeline where an aligned model is first obtained through supervised fine-tuning, then protected during subsequent fine-tuning through Bi-State Optimization with a proximal constraint. The proximal term minimizes Euclidean distance between current model weights and checkpoint from previous state, preventing drift that degrades alignment performance. The method theoretically guarantees convergence with sufficient proximal intensity (ρ > L) under KL property assumptions.

## Key Results
- Reduces harmful score by up to 6.54% compared to vanilla BSO
- Maintains fine-tuning accuracy across four downstream tasks (SST2, AGNEWS, GSM8K, AlpacaEval)
- Effective across multiple model sizes and diverse attack settings
- Shows consistent improvement with varying step allocations and proximal intensities

## Why This Works (Mechanism)

### Mechanism 1
Lisa mitigates harmful fine-tuning attacks by constraining drift between optimization states. It introduces a proximal term that minimizes Euclidean distance between current weights and previous checkpoint. Core assumption: excess drift between alternating optimization states causes alignment degradation.

### Mechanism 2
Lisa ensures convergence of alternating optimization under asymmetric step allocation. Uses proximal term with sufficient intensity (ρ > L) to guarantee sub-linear convergence rate. Core assumption: potential function satisfies KL property with parameter θ ∈ [0,1).

### Mechanism 3
Lisa maintains fine-tuning accuracy while reducing harmful outputs by balancing alignment and task performance. Guides model with alignment data during fine-tuning while constraining drift to prevent forgetting alignment knowledge. Core assumption: alignment dataset provides sufficient guidance without overly constraining task performance.

## Foundational Learning

- **Concept: Bi-State Optimization (BSO)**
  - Why needed here: Separates optimization into alignment and fine-tuning states to prevent forgetting alignment knowledge
  - Quick check question: What happens to alignment performance when steps are asymmetrically allocated between the two states?

- **Concept: Proximal Algorithms**
  - Why needed here: Proximal terms constrain distance between iterates and reference points, ensuring stability in multi-task learning
  - Quick check question: How does the proximal term affect the convergence rate of the optimization process?

- **Concept: KL Property**
  - Why needed here: KL property characterizes convergence behavior of potential function in non-convex optimization
  - Quick check question: What is the relationship between KL parameter θ and convergence rate of the algorithm?

## Architecture Onboarding

- **Component map**: Alignment Dataset -> Fine-tuning Dataset -> Proximal Term -> Alternating Optimization -> Hyperparameters
- **Critical path**: Load aligned model → Initialize proximal parameters → Alternate between alignment/fine-tuning steps → Apply proximal constraint at state transitions → Evaluate harmful score and accuracy
- **Design tradeoffs**: Higher proximal intensity → better alignment but potentially lower fine-tuning accuracy; More alignment steps → better safety but slower training; Different step allocations → balance between safety and task performance
- **Failure signatures**: Alignment loss increases during fine-tuning → drift is uncontrolled; Harmful score remains high → proximal term too weak or alignment data insufficient; Fine-tuning accuracy drops significantly → proximal term too strong
- **First 3 experiments**: 1) Baseline comparison: Lisa vs. SFT on SST2 with 10% harmful data; 2) Step allocation sensitivity: test different alignment/fine-tuning step ratios; 3) Proximal intensity sweep: vary ρ to find optimal balance between safety and accuracy

## Open Questions the Paper Calls Out
- Can Lisa's effectiveness be maintained when applied to RLHF-based alignment techniques instead of SFT?
- How does Lisa's performance scale with increasingly complex downstream tasks, such as conversational AI or multi-task learning?
- What is the optimal trade-off between proximal term intensity and number of alignment steps to maximize effectiveness while minimizing computational overhead?

## Limitations
- Theoretical convergence analysis relies on KL property assumption that may not hold for all alignment objectives
- Effectiveness primarily demonstrated on transformer-based LLMs; performance on other architectures unverified
- Evaluation focuses on harmful fine-tuning attacks; generalizability to other attack vectors unknown

## Confidence

**High Confidence**: Empirical demonstration of harmful output reduction while maintaining fine-tuning accuracy across four diverse datasets and multiple model sizes.

**Medium Confidence**: Theoretical convergence guarantees depend on specific assumptions (KL property, sufficient proximal factor) that may not universally apply.

**Low Confidence**: Generalizability to completely different attack vectors beyond harmful fine-tuning, as evaluation focuses specifically on mixed harmful data scenarios.

## Next Checks

1. **Cross-Architecture Validation**: Test Lisa on non-transformer architectures (e.g., RNNs or hybrid models) to verify if proximal drift constraint works universally.

2. **Alternative KL Assumptions**: Experiment with different potential functions that may or may not satisfy KL property to understand sensitivity of convergence guarantees.

3. **Extreme Step Allocation**: Test method under extreme asymmetric step allocations (e.g., 1:999 alignment to fine-tuning steps) to determine practical limits of effectiveness.