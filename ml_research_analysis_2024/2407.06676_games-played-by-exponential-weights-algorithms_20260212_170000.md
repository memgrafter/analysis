---
ver: rpa2
title: Games played by Exponential Weights Algorithms
arxiv_id: '2407.06676'
source_url: https://arxiv.org/abs/2407.06676
tags:
- nash
- have
- surely
- almost
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the last-iterate convergence properties of
  the Exponential Weights (EW) algorithm with constant learning rates in repeated
  games. The key findings are: When a strict Nash equilibrium exists, the probability
  of playing it at each stage converges almost surely to 0 or 1.'
---

# Games played by Exponential Weights Algorithms

## Quick Facts
- arXiv ID: 2407.06676
- Source URL: https://arxiv.org/abs/2407.06676
- Authors: Maurizio d'Andrea; Fabien Gensbittel; Jérôme Renault
- Reference count: 28
- Key outcome: Analyzes last-iterate convergence of Exponential Weights algorithm with constant learning rates in repeated games

## Executive Summary
This paper studies the long-term behavior of the Exponential Weights (EW) algorithm in repeated games, focusing on last-iterate convergence with constant learning rates. The authors analyze the homogeneous Markov chain induced by the EW algorithm on the space of mixed strategy profiles. They establish that when strict Nash equilibria exist, the probability of playing them converges almost surely to 0 or 1. The paper characterizes the boundary of reachable mixed strategy profiles and shows that any convergent sequence must approach a Nash Equilibrium with Equalizing Payoffs (NEEP).

## Method Summary
The paper analyzes the Markov chain induced by the Exponential Weights algorithm on mixed strategy profiles in repeated games. The method involves studying the stochastic process generated by constant learning rates, examining convergence properties, and characterizing the set of possible limits. The analysis uses tools from stochastic approximation and Markov chain theory to prove that convergence (when it occurs) must be to a NEEP, and that in strong coordination games, the process converges almost surely to strict Nash equilibria.

## Key Results
- When strict Nash equilibria exist, the probability of playing them at each stage converges almost surely to 0 or 1
- Any convergent sequence of mixed strategy profiles must approach a Nash Equilibrium with Equalizing Payoffs (NEEP)
- In strong coordination games, the EW algorithm converges almost surely to one of the strict Nash equilibria

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In games with strict Nash equilibrium, EW algorithm almost surely converges to boundary where strict NE is played
- Mechanism: EW creates Markov chain over mixed strategy profiles; strict NE creates persistent signal forcing boundary convergence
- Core assumption: Markov chain has well-defined transitions depending only on current state
- Evidence anchors: Abstract states probability to play strict NE converges to 0 or 1; analysis focuses on Markov chain properties

### Mechanism 2
- Claim: Convergent sequence must approach NEEP
- Mechanism: EW updates based on realized payoffs; convergence requires equalizing payoffs for stability
- Core assumption: Limit must satisfy equilibrium conditions for algorithm stability
- Evidence anchors: Abstract states limit belongs to NEEP set; Theorem shows NEEP existence is necessary for convergence

### Mechanism 3
- Claim: In strong coordination games, EW converges almost surely to strict NE
- Mechanism: Game structure creates potential function that EW minimizes, driving toward pure strategy profiles
- Core assumption: Game structure allows potential function construction
- Evidence anchors: Abstract states convergence to strict NE in strong coordination games; conjecture about almost sure convergence to one of three strict NE

## Foundational Learning

- Concept: Markov chains and their convergence properties
  - Why needed here: EW algorithm creates Markov chain over mixed strategy profiles; convergence understanding is central
  - Quick check question: What conditions ensure Markov chain converges to stationary distribution?

- Concept: Nash equilibria and their properties
  - Why needed here: Analysis focuses on convergence to different NE types (strict, NEEP)
  - Quick check question: What distinguishes strict Nash equilibrium from Nash equilibrium with equalizing payoffs?

- Concept: Stochastic approximation and online learning algorithms
  - Why needed here: EW algorithm is online learning method updating based on realized payoffs
  - Quick check question: How does EW algorithm update weights based on realized payoffs?

## Architecture Onboarding

- Component map: Game structure (players, actions, payoffs) -> EW algorithm (learning rates, weight updates) -> Induced Markov chain (mixed strategy profiles)
- Critical path: Algorithm updates weights based on realized payoffs → determines next mixed strategy profile → creates trajectory through strategy space
- Design tradeoffs: Constant learning rates vs. decreasing rates (simplicity vs. regret minimization), convergence to strict equilibria vs. convergence to NEEP
- Failure signatures: Non-convergence (oscillation/cycling), convergence to non-NEEP points, failure to reach boundary in games with strict NE
- First 3 experiments:
  1. Implement EW algorithm for 2x2 game with strict NE, verify convergence to boundary
  2. Test on game without NEEP (Matching Pennies), observe non-convergence
  3. Implement for strong coordination game, verify convergence to strict NE

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are possible limits of EW process beyond NEEP?
- Basis in paper: Theorem 4.2 shows limits must be NEEP, but Example 4.3 shows only NEEP with maximal support can be obtained
- Why unresolved: No general characterization of which NEEPs can be limits
- What evidence would resolve it: Proof that only maximal support NEEPs can be limits, or counterexample with pure non-strict NE as limit

### Open Question 2
- Question: Does EW always converge in common payoff games?
- Basis in paper: Convergence shown for strong coordination games (subclass of common payoff games), but not guaranteed in general common payoff games
- Why unresolved: No general proof or counterexample for all common payoff games
- What evidence would resolve it: Proof of convergence in all common payoff games, or counterexample showing non-convergence

### Open Question 3
- Question: Is there explicit formula for probability of converging to given NEEP?
- Basis in paper: Mentions difficulty computing this probability, nice to have explicit formula for non-trivial example
- Why unresolved: No explicit formula provided for any non-trivial game
- What evidence would resolve it: Explicit formula for specific non-trivial game, or proof no such formula exists for certain class

### Open Question 4
- Question: Can we obtain quantitative estimates for time until convergence to strict NE in strict coordination games?
- Basis in paper: Theorem 5.2 shows almost sure convergence, raises question of quantitative estimates for random time T
- Why unresolved: No quantitative estimates provided, only proves almost sure convergence
- What evidence would resolve it: Proof that time is integrable, or explicit bound on expected time

## Limitations
- Analysis limited to finite normal form games with constant learning rates
- Specific conditions determining which NEEP is reached not fully characterized for general games
- Lacks empirical validation through simulations or experiments on concrete game instances

## Confidence
- High confidence: Convergence must be to NEEP, follows directly from equalizing payoffs condition
- Medium confidence: Boundary convergence characterization in games with strict NE, relies on Markov chain analysis
- Low confidence: General convergence behavior in arbitrary games, not guaranteed and limited guidance on when expected

## Next Checks
1. Implement EW algorithm for representative games (strong coordination, multiple strict NE, no NE) and empirically verify convergence properties
2. Extend analysis to games with more than two players or actions to assess scalability of Markov chain convergence results
3. Test algorithm with both constant and decreasing learning rates to determine robustness to this key parameter