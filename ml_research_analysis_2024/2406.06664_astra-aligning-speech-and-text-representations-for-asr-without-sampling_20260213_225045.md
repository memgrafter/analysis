---
ver: rpa2
title: 'ASTRA: Aligning Speech and Text Representations for Asr without Sampling'
arxiv_id: '2406.06664'
source_url: https://arxiv.org/abs/2406.06664
tags:
- text
- speech
- loss
- rnnt
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ASTRA, a novel method for improving Automatic
  Speech Recognition (ASR) through text injection. Unlike prevailing techniques, ASTRA
  eliminates the need for sampling to match sequence lengths between speech and text
  modalities.
---

# ASTRA: Aligning Speech and Text Representations for Asr without Sampling

## Quick Facts
- arXiv ID: 2406.06664
- Source URL: https://arxiv.org/abs/2406.06664
- Reference count: 0
- Primary result: Achieves state-of-the-art text injection for ASR without sampling-based length matching

## Executive Summary
ASTRA introduces a novel approach for text injection in ASR systems that eliminates the need for sampling to match sequence lengths between speech and text modalities. Instead of upsampling text or using explicit duration models, ASTRA leverages the inherent alignments learned by RNNT models to enforce consistency between speech and text embeddings. The method reformulates modality matching as a weighted RNNT objective, achieving performance comparable to state-of-the-art duration-based methods on the FLEURS benchmark while opening new avenues for research in speech processing.

## Method Summary
ASTRA is an RNNT-based model with separate speech and text encoders, a shared encoder, and a shared decoder. The key innovation is a consistency loss that enforces alignment between speech and text representations using the RNNT model's own alignment probabilities, eliminating the need for explicit duration models or text upsampling. The consistency loss is formulated as a weighted RNNT objective, allowing efficient computation and optimization. The model is pretrained using BEST-RQ self-supervised learning on YT-56-U, then fine-tuned on FLEURS with ASR loss, text-only loss, and ASTRA consistency loss.

## Key Results
- Matches state-of-the-art duration-based text injection methods on FLEURS benchmark
- Eliminates need for sampling-based length matching between speech and text
- Avoids potential misalignment issues from upsampling text to match speech length
- Achieves consistent performance across 102 languages in FLEURS test set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASTRA achieves modality matching without explicit alignment by leveraging RNNT's implicit alignments
- Mechanism: Uses weighted RNNT loss where non-blank transitions are weighted by pointwise consistency loss between speech and text embeddings
- Core assumption: RNNT model's learned alignments are sufficiently accurate for consistency enforcement
- Evidence anchors:
  - [abstract] "ASTRA eliminates the need for sampling to match sequence lengths between speech and text modalities"
  - [section 3.2] "We instead make use of the implicit alignment learned by an RNNT itself to enforce consistency"
- Break condition: If RNNT alignments are significantly delayed or inaccurate

### Mechanism 2
- Claim: Additive consistency loss over alignment path can be reformulated as weighted RNNT objective
- Mechanism: By defining consistency loss L_ca as additive over alignment a, the reformulation enables efficient computation using expectation semi-ring
- Core assumption: Consistency loss can be expressed additively over alignment paths
- Evidence anchors:
  - [section 3.2] "We will now see that minimizing cLc is equivalent to minimizing an upper bound on Lc"
  - [abstract] "This novel formulation of modality (length) matching as a weighted RNNT objective"
- Break condition: If consistency loss cannot be decomposed additively over alignment paths

### Mechanism 3
- Claim: Placing consistency loss at encoder outputs rather than after shared encoder improves performance
- Mechanism: Applies MAE consistency loss between speech and text encoder outputs before shared encoder to avoid interference in shared space
- Core assumption: Consistency enforced at modality-specific features is more effective than in shared space
- Evidence anchors:
  - [section 5] "we found it better to place the loss layer at the speech and text encoder output"
  - [section 4.1] "speech encoder contains 6 conformer blocks and shared encoder has remaining 18"
- Break condition: If shared encoder's role in aligning modalities is critical

## Foundational Learning

- Concept: RNN Transducer (RNNT) architecture and alignment mechanism
  - Why needed here: ASTRA's core innovation relies on understanding how RNNT models implicitly learn alignments between speech and text sequences without explicit supervision
  - Quick check question: How does RNNT model the probability of a target sequence given an input sequence without requiring explicit alignments?

- Concept: Expectation semi-ring and efficient computation of alignment-based losses
  - Why needed here: The paper leverages the expectation semi-ring to efficiently compute the consistency loss over all possible alignments
  - Quick check question: What is the expectation semi-ring and how does it enable efficient computation of losses over all possible alignments in RNNT models?

- Concept: Modality matching and representation learning in multimodal systems
  - Why needed here: ASTRA aims to learn consistent representations between speech and text modalities, requiring understanding of modality matching techniques and their limitations
  - Quick check question: What are the key challenges in modality matching for speech-text systems and how do existing approaches address these challenges?

## Architecture Onboarding

- Component map: Speech encoder (6 Conformer blocks) -> Text encoder (4 Conformer blocks + embeddings) -> Shared encoder (18 Conformer blocks) -> Shared decoder (2-layer LSTM) -> Consistency loss (MAE between encoder outputs weighted by RNNT alignment)

- Critical path: 1) Speech through speech encoder, 2) Text through text encoder, 3) Consistency loss computed using RNNT alignment probabilities, 4) Both through shared encoder and decoder, 5) RNNT loss computed

- Design tradeoffs:
  - No duration model: Eliminates need for upsampling and duration prediction but relies on RNNT alignment quality
  - Encoder-level consistency: Enforces consistency before shared representation learning but may limit joint space optimization
  - MAE vs MSE: MAE provides better performance but may be less sensitive to large deviations

- Failure signatures:
  - Poor alignment quality: Inconsistent or delayed alignments lead to ineffective consistency loss
  - Mode collapse: Speech and text representations become too similar, losing modality-specific information
  - Overfitting: Model overfits to text corpus without proper regularization

- First 3 experiments:
  1. Verify that RNNT alignment probabilities correlate with actual speech-text alignments on validation data
  2. Compare consistency loss positioning (encoder vs shared encoder output) on a small dataset
  3. Test different consistency loss functions (MAE vs MSE) to confirm performance differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ASTRA compare to other alignment-free text injection methods like optimal transport or dynamic time warping?
- Basis in paper: [inferred] The paper mentions that some works have tried to get around alignment issues using dynamic time warping or optimal transport
- Why unresolved: No direct comparison provided between ASTRA and these alignment-free methods
- What evidence would resolve it: Experiments comparing ASTRA to alignment-free methods on same dataset and metrics

### Open Question 2
- Question: How does choice of consistency loss function (MAE vs MSE) affect ASTRA performance, and are there other potential loss functions that could further improve performance?
- Basis in paper: [explicit] The paper experimented with MAE and MSE, settling on MAE due to improved performance
- Why unresolved: Only compares MAE and MSE, doesn't explore other potential loss functions
- What evidence would resolve it: Experimenting with different consistency loss functions like Huber loss or cosine similarity

### Open Question 3
- Question: How does ASTRA performance scale with amount of unpaired text data used for training, and is there a point of diminishing returns?
- Basis in paper: [inferred] The paper uses unpaired text data to train text encoder and shared encoder but doesn't explore impact of varying text data amount
- Why unresolved: No analysis of how amount of unpaired text data affects ASTRA performance
- What evidence would resolve it: Experiments with different amounts of unpaired text data measuring FLEURS performance

## Limitations

- Alignment quality dependence: ASTRA's performance critically depends on quality of alignments learned by RNNT model, which prior work has noted can be delayed
- Lack of comprehensive ablation studies: Missing quantitative comparisons for key design choices like consistency loss positioning and loss function selection
- Single benchmark validation: Performance validated only on FLEURS dataset without comparisons to alternative approaches on diverse datasets or languages

## Confidence

- ASTRA achieves modality matching without explicit alignment or length matching: Medium confidence
- Weighted RNNT formulation matches state-of-the-art duration-based methods: Medium confidence
- Encoder-level consistency is superior to shared encoder consistency: Low confidence

## Next Checks

1. **Alignment Quality Assessment**: Measure correlation between RNNT alignment probabilities and actual speech-text alignments on validation data to quantify reliability of consistency loss mechanism

2. **Ablation Studies**: Conduct controlled experiments varying consistency loss positioning (encoder vs shared encoder output) and loss function choice (MAE vs MSE vs MSE) to validate design decisions

3. **Cross-Benchmark Validation**: Test ASTRA on diverse ASR benchmarks beyond FLEURS, including monolingual datasets and languages with different writing systems, to assess generalizability and robustness