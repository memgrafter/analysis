---
ver: rpa2
title: Efficient and Interpretable Grammatical Error Correction with Mixture of Experts
arxiv_id: '2410.23507'
source_url: https://arxiv.org/abs/2410.23507
tags:
- error
- layer
- experts
- correction
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MoECE, a mixture-of-experts model for grammatical
  error correction (GEC) that improves efficiency and interpretability. The model
  uses a router with a classification head to direct tokens to specialized experts
  based on error type, reducing the effective parameter count by a factor of three
  compared to T5-XL while maintaining comparable performance.
---

# Efficient and Interpretable Grammatical Error Correction with Mixture of Experts

## Quick Facts
- arXiv ID: 2410.23507
- Source URL: https://arxiv.org/abs/2410.23507
- Reference count: 15
- Key outcome: MoECE achieves F0.5 scores of 67.79 on CoNLL-2014 and 74.07 on BEA-2019 while using three times fewer effective parameters than T5-XL

## Executive Summary
This paper introduces MoECE, a mixture-of-experts model for grammatical error correction (GEC) that improves efficiency and interpretability. The model uses a router with a classification head to direct tokens to specialized experts based on error type, reducing the effective parameter count by a factor of three compared to T5-XL while maintaining comparable performance. MoECE achieves strong results on multiple GEC benchmarks and provides interpretable corrections by identifying error types during inference.

## Method Summary
MoECE extends T5-v1.1 with mixture-of-experts (MoE) layers in the decoder, using a router that contains both classification and dispatch heads. The classification head predicts error types for tokens while the dispatch head selects experts based on routing scores. The model uses 7 experts per MoE layer and applies them to all decoder transformer blocks except the first. Training combines cross-entropy loss for text correction, cross-entropy loss for error type prediction, and load balancing loss. The architecture shares a feed-forward layer between experts and the main network, adding rather than replacing the main layer.

## Key Results
- MoECE achieves F0.5 scores of 67.79 on CoNLL-2014 and 74.07 on BEA-2019
- The model uses only ~784M effective parameters compared to ~1.7B for T5-XL
- MoECE outperforms comparable dense models (T5-v1.1-Base/Large) on all test sets including CWEB-G and CWEB-S
- Error type loss improves performance on out-of-domain datasets (CWEB)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model reduces computational cost by activating only a subset of experts per token while maintaining comparable performance to dense models.
- Mechanism: The mixture-of-experts (MoE) layer routes each token to K=2 experts based on routing scores, with only those experts' parameters being activated for that token. This reduces the effective parameter count from ~1.7B to ~784M for the large model variant.
- Core assumption: Task interference in dense models can be mitigated by expert specialization, allowing smaller effective models to match larger dense models.
- Evidence anchors:
  - [abstract] "Our model successfully achieves the performance of T5-XL with three times fewer effective parameters."
  - [section 3.1] "While this design increases the computation cost during training, no additional cost is incurred during inference by merging the weights of the feed-forward layer with the experts."
  - [corpus] Weak evidence - corpus mentions related work but no direct comparison of MoE efficiency metrics.

### Mechanism 2
- Claim: Adding a classification head to the router enables interpretable error type identification during inference.
- Mechanism: The router contains a classification head that predicts the error type for each token, while the dispatch head selects experts. The classification head is trained with an auxiliary loss (Le) that encourages the router to route based on error type.
- Core assumption: Error type information is both learnable by the router and useful for routing decisions that improve correction quality.
- Evidence anchors:
  - [abstract] "Additionally, our model produces interpretable corrections by also identifying the error type during inference."
  - [section 3.2] "We hypothesize that an optimal expert allocation in a mixture of experts for grammatical error correction involves having each expert focus on correcting particular error types."
  - [section 6.1] "We find that with the error type loss, the router chooses different combinations of experts according to the token error type."

### Mechanism 3
- Claim: Sharing a feed-forward layer between experts and the main network provides general grammatical correction skills while experts specialize in error-specific corrections.
- Mechanism: The MoE layer output is added to (not replacing) the main feed-forward layer output. This allows the shared layer to learn general correction patterns while experts learn specialized error corrections.
- Core assumption: Grammatical error correction requires both general language modeling skills and error-type-specific correction patterns.
- Evidence anchors:
  - [section 3.1] "We design our MoE to be an addition to the main transformer feed-forward layer, rather than a replacement, during training."
  - [section 6.3] "We observe that the model with frozen layer sharing produces a similar F0.5 score on the BEA-2019 development set after convergence, but it takes much longer to train."
  - [corpus] Weak evidence - corpus mentions related MoE work but doesn't specifically discuss shared feed-forward layer benefits.

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture and routing mechanisms
  - Why needed here: Understanding how MoE layers work, how routing selects experts, and how load balancing prevents expert collapse is fundamental to implementing and debugging MoECE.
  - Quick check question: What is the difference between GShard and SwitchTransformer routing strategies, and how does this affect expert selection?

- Concept: Grammatical Error Correction (GEC) task structure and error types
  - Why needed here: The model's effectiveness depends on understanding different error types (punctuation, prepositions, etc.) and how they manifest in text.
  - Quick check question: What are the six most frequent error types mentioned in the paper, and why might they require different correction strategies?

- Concept: Transfer learning from pre-trained language models
  - Why needed here: MoECE builds on T5-v1.1 by adding MoE layers, requiring understanding of how to effectively transfer knowledge from dense to MoE architectures.
  - Quick check question: How does initializing MoECE from T5-v1.1 differ from training MoE from scratch, and what advantages does this provide?

## Architecture Onboarding

- Component map: Token input → transformer encoding → decoder transformer blocks with MoE layers → router processes token representation → classification head predicts error type → dispatch head selects K experts → selected experts process token → outputs summed with shared FF layer → final correction generation

- Critical path: 1. Token input → transformer encoding 2. Decoder transformer blocks with MoE layers 3. Router processes token representation 4. Classification head predicts error type (auxiliary loss) 5. Dispatch head selects K experts based on routing scores 6. Selected experts process token → outputs summed with shared FF layer 7. Final correction generation

- Design tradeoffs:
  - Shared FF layer vs. MoE replacement: Higher training cost but better performance and faster convergence
  - Error type loss vs. load balancing loss: Error type loss provides interpretability but may not always improve performance
  - Low-rank experts vs. full-rank: Reduced memory usage but potentially lower performance

- Failure signatures:
  - All tokens routed to same expert(s): Load balancing loss not working or α/β hyperparameters misconfigured
  - Poor error type prediction accuracy: Classification head not learning, may need more training data or different architecture
  - No performance improvement over dense model: Task interference not being addressed by expert specialization, may need different routing strategy

- First 3 experiments:
  1. Train MoECE-GS-Base with default hyperparameters and compare F0.5 scores on BEA-2019 dev set vs. T5-v1.1-Base
  2. Analyze router's error type prediction accuracy on BEA-2019 dev set to verify classification head is learning
  3. Visualize expert routing patterns for different error types to confirm specialization is occurring

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MoECE architecture scale to even larger models, and what are the potential limitations of the current approach?
- Basis in paper: [explicit] The paper mentions that they did not run experiments on larger models due to compute budget limitations, but believe their current experimental configurations are sufficient to demonstrate the effectiveness of their method.
- Why unresolved: The paper does not explore the performance and efficiency of MoECE when applied to significantly larger models. The scaling behavior, potential bottlenecks, and trade-offs between model size and performance are unknown.
- What evidence would resolve it: Experiments with MoECE on larger models, such as T5-XXL or larger, comparing performance, parameter efficiency, and computational costs against other MoE architectures or dense models. Analysis of how the router, expert specialization, and load balancing behave as the model scales.

### Open Question 2
- Question: How does the error type loss function compare to other potential loss functions or routing mechanisms in terms of interpretability and performance?
- Basis in paper: [explicit] The paper discusses the use of error type loss and load balancing loss, and mentions that the error type loss helps in routing input tokens to appropriate experts based on error type. However, it also notes that models trained with error type loss do not strictly perform better than models trained without it.
- Why unresolved: The paper does not compare the error type loss function to other potential loss functions or routing mechanisms that could be used in MoECE. The relative strengths and weaknesses of different approaches in terms of interpretability and performance are unknown.
- What evidence would resolve it: Comparative experiments between MoECE with error type loss and MoECE with alternative loss functions or routing mechanisms, such as adversarial routing, sparse routing, or other task-specific routing criteria. Analysis of the interpretability and performance trade-offs of different approaches.

### Open Question 3
- Question: How does the MoECE architecture perform on grammatical error correction tasks for languages other than English?
- Basis in paper: [explicit] The paper states that they only investigate grammatical error correction for English and that their method is applicable to other languages when sufficient training data is available.
- Why unresolved: The paper does not provide any empirical evidence of how MoECE performs on grammatical error correction tasks for languages other than English. The generalizability of the approach to other languages and the potential challenges or adaptations needed are unknown.
- What evidence would resolve it: Experiments with MoECE on grammatical error correction tasks for other languages, such as Chinese, Spanish, or French, using appropriate training data and evaluation metrics. Analysis of the performance, error type distributions, and potential differences in routing and expert specialization across languages.

## Limitations
- The model uses additional parameters for the router and classification head that aren't fully accounted for in efficiency calculations
- Error type identification is evaluated qualitatively rather than systematically across datasets
- The study focuses only on English GEC, limiting generalizability to other languages
- Error type taxonomy is based on ERRANT annotations which may not capture all relevant distinctions

## Confidence
*High Confidence* claims:
- MoECE achieves comparable performance to T5-XL with 3× fewer effective parameters
- The router learns to route tokens to experts based on error type when trained with the error type loss
- MoECE outperforms comparable dense models (T5-v1.1-Base/Large) on all test sets

*Medium Confidence* claims:
- Error type loss improves performance on out-of-domain datasets
- Shared feed-forward layer provides general grammatical correction skills
- MoECE produces interpretable corrections

*Low Confidence* claims:
- Optimal expert allocation involves experts focusing on particular error types
- Task interference in dense models is the primary reason MoE improves efficiency

## Next Checks
1. Conduct a systematic evaluation of the interpretability claim by having human annotators assess whether the predicted error types match the actual corrections made, measuring both accuracy and usefulness for understanding model decisions.

2. Test the model's robustness to adversarial inputs containing mixed error types or ambiguous cases where multiple corrections are possible, to understand the limits of expert specialization.

3. Implement an ablation study that isolates the contribution of the error type loss versus the load balancing loss to performance, training models with only one loss at a time to quantify their individual impacts on both accuracy and interpretability.