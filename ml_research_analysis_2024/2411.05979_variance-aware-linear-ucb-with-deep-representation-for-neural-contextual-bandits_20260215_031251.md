---
ver: rpa2
title: Variance-Aware Linear UCB with Deep Representation for Neural Contextual Bandits
arxiv_id: '2411.05979'
source_url: https://arxiv.org/abs/2411.05979
tags:
- linucb
- regret
- neural
- bound
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes Neural-\u03C32-LinUCB, a variance-aware algorithm\
  \ that improves uncertainty quantification in neural contextual bandits by leveraging\
  \ the reward noise variance upper bound \u03C32t. The method estimates \u03C32t\
  \ from the reward range and mean predictions, then uses it to weight the ridge regression\
  \ in the linear output layer."
---

# Variance-Aware Linear UCB with Deep Representation for Neural Contextual Bandits

## Quick Facts
- arXiv ID: 2411.05979
- Source URL: https://arxiv.org/abs/2411.05979
- Authors: Ha Manh Bui; Enrique Mallada; Anqi Liu
- Reference count: 40
- Key outcome: Neural-σ2-LinUCB improves uncertainty quantification in neural contextual bandits using variance-aware weighting, achieving better regret guarantees and calibration than standard neural-UCB methods

## Executive Summary
This paper introduces Neural-σ2-LinUCB, a variance-aware algorithm for neural contextual bandits that enhances uncertainty quantification by incorporating reward noise variance upper bounds into the UCB mechanism. The method estimates variance from reward ranges and mean predictions, then uses these estimates to weight the ridge regression in the linear output layer. Two versions are proposed: an oracle version assuming known variance bounds and a practical version estimating them. The algorithm achieves improved regret guarantees of O(RdT + d√(Σσ2t + ε)) for the practical version and O(RdT + d√(Σσ2t)) for the oracle version, outperforming standard neural-UCB approaches. Empirical results on synthetic and real-world datasets (MNIST, CIFAR-10) demonstrate lower cumulative regret and better calibration while maintaining computational efficiency comparable to Neural-LinUCB.

## Method Summary
The Neural-σ2-LinUCB algorithm combines a neural network feature extractor with a variance-weighted linear UCB component. At each round, the algorithm observes a context, computes UCB scores using a weighted ridge regression where weights depend on estimated variance bounds, and selects an arm. The variance estimator calculates σ²t from the reward range and mean predictions. The linear model uses this variance information to adjust exploration-exploitation balance, with higher variance leading to more exploration. The neural network is updated periodically using either MSE or MLE objectives, with the MLE incorporating predictive variance from the linear model. This integrated approach improves both regret performance and uncertainty calibration compared to standard neural-UCB methods.

## Key Results
- Achieves regret bounds of O(RdT + d√(Σσ2t + ε)) for practical version and O(RdT + d√(Σσ2t)) for oracle version
- Demonstrates lower cumulative regret than Neural-LinUCB, NeuralUCB, and NeuralTS on synthetic and real-world datasets
- Shows better calibration performance with improved reliability diagrams across confidence levels
- Maintains computational efficiency comparable to standard neural-UCB approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The variance-aware upper confidence bound improves uncertainty quantification by weighting the ridge regression in the linear output layer based on the reward noise variance upper bound σ²ₜ.
- Mechanism: The algorithm estimates σ²ₜ using the reward range and mean predictions, then uses it to weight the feature matrix in the ridge regression. This adjusts the exploration-exploitation balance: higher variance leads to more exploration, lower variance leads to more exploitation.
- Core assumption: The reward noise variance upper bound σ²ₜ can be accurately estimated from the reward range and mean predictions, and this estimation is sufficient to improve UCB uncertainty quality.
- Evidence anchors:
  - [abstract]: "utilizes σ²ₜ, i.e., an upper bound of the reward noise variance at round t, to enhance the uncertainty quantification quality of the UCB"
  - [section]: "our practical version calculates the upper bound of the reward noise variance by using the reward range and the estimated reward mean with DNN"
  - [corpus]: Weak - only general variance-aware bandit literature found, no direct evidence for neural contextual bandits with variance weighting
- Break condition: If the variance estimation is inaccurate or the reward range is unknown, the weighting becomes ineffective and may degrade performance.

### Mechanism 2
- Claim: The algorithm achieves better regret guarantees than standard neural UCB methods by incorporating variance information into the confidence set.
- Mechanism: By weighting the feature matrix with σ²ₜ, the confidence ellipsoid volume shrinks at a rate dependent on the variance bound rather than just time. This provides tighter confidence bounds when variance is low and maintains appropriate exploration when variance is high.
- Core assumption: The regret bound improvement holds when σ²ₜ is bounded and the variance estimation error is small relative to the time horizon.
- Evidence anchors:
  - [abstract]: "prove that our oracle algorithm achieves a better regret guarantee than other neural-UCB algorithms"
  - [section]: "the regret of our practical version is at most ˜O(R√dT + d√(Σσ²ₜ + ε)) and ˜O(R√dT + d√(Σσ²ₜ)) for practical and oracle versions"
  - [corpus]: Moderate - some variance-aware bandit literature exists but specific neural contextual bandit regret analysis is rare
- Break condition: If the variance bounds are very large or the estimation error ε grows with time, the regret improvement may be lost.

### Mechanism 3
- Claim: The MLE-based loss function with variance-aware weighting improves the neural network's predictive uncertainty quality.
- Mechanism: The proposed MLE objective function incorporates the predictive variance from the linear model into the loss, encouraging the neural network to learn representations that not only predict accurately but also provide reliable uncertainty estimates.
- Core assumption: The predictive variance from the linear model ϕ(x;w)⊤A⁻¹ϕ(x;w) is a good proxy for the true uncertainty of the neural network predictions.
- Evidence anchors:
  - [abstract]: "our practical method enjoys a similar computational efficiency, while outperforming state-of-the-art techniques by having a better calibration"
  - [section]: "we further propose Eq. 12 to update parameter w via MLE" and "the predictive variance of the expected payoffθ⊤ϕ(xi,ai;w) is evaluated asϕ(xi,ai;w)⊤A⁻¹ϕ(xi,ai;w)"
  - [corpus]: Weak - most related work focuses on calibration post-hoc rather than integrated uncertainty-aware training
- Break condition: If the linear model's uncertainty estimates are poor or the neural network cannot learn to correlate with these estimates, the training objective may not improve uncertainty quality.

## Foundational Learning

- Concept: Upper Confidence Bound (UCB) algorithms
  - Why needed here: The entire method builds upon UCB principles for balancing exploration and exploitation in contextual bandits
  - Quick check question: How does standard UCB compute its confidence bounds and what role does the exploration parameter play?

- Concept: Ridge regression and weighted least squares
  - Why needed here: The algorithm uses weighted ridge regression where the weights depend on the variance estimates
  - Quick check question: How does weighting the feature matrix in ridge regression affect the confidence ellipsoid volume and the solution?

- Concept: Neural Tangent Kernel (NTK) and over-parameterization
  - Why needed here: The theoretical analysis relies on NTK theory to bound the regret and analyze the neural network behavior
  - Quick check question: What is the relationship between NTK, the neural network's ability to represent functions, and the regret bounds in this setting?

## Architecture Onboarding

- Component map:
  - Context encoder: Neural network that transforms raw context into feature vectors
  - Variance estimator: Component that calculates σ²ₜ from reward range and mean predictions
  - Linear model: Ridge regression component with variance-weighted feature matrix
  - Exploration controller: UCB mechanism that uses the linear model and variance estimates
  - Neural network trainer: Component that updates the neural network using MSE or MLE objectives

- Critical path:
  1. Observe context → Encode context → Compute UCB scores → Select arm
  2. Receive reward → Update linear model with variance weighting → Update neural network periodically
  3. Variance estimation runs continuously alongside the main learning loop

- Design tradeoffs:
  - Frequency of neural network updates (H parameter): More frequent updates may improve representation learning but increase computational cost
  - Choice between MSE and MLE training objectives: MLE provides better uncertainty estimates but may be more sensitive to variance estimation errors
  - Estimation of σ²ₜ: Using true variance bounds gives oracle performance but requires domain knowledge

- Failure signatures:
  - High variance in cumulative regret across runs: May indicate instability in variance estimation or neural network training
  - Poor calibration despite good regret: Suggests the UCB confidence bounds are not well-calibrated to the true uncertainty
  - Neural network training instability: Could occur if the MLE objective with variance weighting creates problematic gradients

- First 3 experiments:
  1. Compare Neural-σ²-LinUCB with standard Neural-LinUCB on synthetic data with known variance bounds to verify regret improvement
  2. Test variance estimation quality by comparing ˆσ²ₜ to true variance across different reward functions and noise levels
  3. Evaluate calibration performance on hold-out data at different time points to assess uncertainty quality improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the regret bound of Neural-σ2-LinUCB change with varying levels of reward noise variance σ²_t?
- Basis in paper: [explicit] Theorem 4.5 and 4.8 provide regret bounds of O(Rd√T + d√(Σσ²_t + ε)) and O(Rd√T + d√(Σσ²_t)) for practical and oracle versions, respectively.
- Why unresolved: The paper assumes σ_t ≤ R but does not provide empirical analysis on how the regret bound scales with different levels of σ²_t.
- What evidence would resolve it: Empirical results showing cumulative regret for varying levels of reward noise variance.

### Open Question 2
- Question: Can the estimation error ε in the practical version be further reduced?
- Basis in paper: [explicit] Theorem 4.8 states the regret bound includes an estimation error term ε = O(d³T²n⁹L¹¹ log⁶(m)).
- Why unresolved: The paper does not explore techniques to minimize this estimation error beyond increasing model capacity.
- What evidence would resolve it: Comparison of regret bounds with different estimation techniques or model architectures.

### Open Question 3
- Question: How does the choice of α_t affect the performance of Neural-σ2-LinUCB in practice?
- Basis in paper: [explicit] The paper mentions using a constant α_t for fair comparison with baselines, but the theoretical value differs.
- Why unresolved: The paper does not provide a systematic study on the impact of different α_t values on regret performance.
- What evidence would resolve it: Empirical results comparing cumulative regret for various α_t values.

## Limitations
- The variance estimation method requires known reward ranges [a,b], which may not be available in practical applications
- Theoretical regret bounds depend on the variance upper bound being tight and accurate neural network representation learning
- The MLE training objective introduces additional computational complexity compared to standard MSE approaches

## Confidence

- Regret bound claims: Medium confidence - theoretical analysis appears sound but depends on variance estimation quality
- Empirical performance claims: Medium confidence - results show improvement but limited to specific datasets and baselines
- Calibration improvement claims: Medium confidence - visual inspection of calibration curves provided but limited statistical analysis

## Next Checks

1. Test algorithm performance when reward range [a,b] is unknown or poorly estimated to assess robustness
2. Conduct ablation studies comparing different variance estimation methods and their impact on regret
3. Evaluate algorithm performance across diverse reward function types (non-smooth, discontinuous) to test generalization