---
ver: rpa2
title: Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large
  Speech Models
arxiv_id: '2403.19709'
source_url: https://arxiv.org/abs/2403.19709
tags:
- adapter
- head
- recurrent
- adaptation
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hierarchical Recurrent Adapters (HRA), a
  parameter-efficient method for adapting large pre-trained speech models to multiple
  downstream tasks. HRA uses a shared recurrent controller and task-specific adapter
  heads to reduce per-task parameter overhead while maintaining or improving performance
  compared to full fine-tuning and other adapter methods.
---

# Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models

## Quick Facts
- arXiv ID: 2403.19709
- Source URL: https://arxiv.org/abs/2403.19709
- Authors: Tsendsuren Munkhdalai; Youzheng Chen; Khe Chai Sim; Fadi Biadsy; Tara Sainath; Pedro Moreno Mengibar
- Reference count: 0
- Primary result: HRA achieves 5.1 WER on voice search with 12.8M parameters vs 1.8B for full fine-tuning

## Executive Summary
This paper introduces Hierarchical Recurrent Adapters (HRA), a parameter-efficient method for adapting large pre-trained speech models to multiple downstream tasks. HRA uses a shared recurrent controller and task-specific adapter heads to reduce per-task parameter overhead while maintaining or improving performance compared to full fine-tuning and other adapter methods. The key idea is to decompose the adapter into a controller network and task-level adapter heads, with the controller being shared across all tasks and layers of the pre-trained model. This hierarchical design enables efficient multi-task adaptation by reducing the number of task-specific parameters that need to be trained. In experiments on automatic speech recognition tasks, HRA achieved better word error rates (WERs) with 2-8x fewer parameters compared to baselines, and even outperformed full model fine-tuning in some cases. Specifically, HRA with 12.8M parameters achieved a WER of 5.1 on voice search, outperforming the full fine-tuning baseline with 1.8B parameters.

## Method Summary
HRA is a parameter-efficient adapter method for multi-task adaptation of large speech models that uses a shared recurrent controller and task-specific adapter heads. The architecture consists of an IndRNN controller shared across all layers of the backbone model and all tasks, with each task having its own adapter head (either linear or feed-forward network). This hierarchical decomposition reduces per-task parameter overhead while maintaining performance by allowing the controller to capture task-invariant features while adapter heads handle task-specific variations. The method is evaluated on automatic speech recognition tasks using a 2-billion parameter Universal Speech Model pre-trained on multi-domain corpora, with experiments on both single-task (voice search) and multi-task (voice search plus Euphonia speech impairments) adaptation settings.

## Key Results
- HRA achieved 5.1 WER on voice search with 12.8M parameters vs 1.8B for full fine-tuning
- HRA outperformed Residual Adapters, LoRA, and BitFit baselines while using 2-8x fewer parameters
- In multi-task setting with 128 tasks, HRA maintained performance with sub-linear parameter growth
- HRA with FFN adapter heads showed slightly better performance than linear heads in multi-task setting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The shared recurrent controller reduces per-task parameter overhead by being reused across all layers and tasks.
- Mechanism: HRA uses a single IndRNN controller shared across all layers of the backbone model and all tasks, while only task-specific adapter heads need to be trained for each new task.
- Core assumption: Sharing the controller across layers doesn't degrade performance due to the controller's ability to maintain state and capture hierarchical features.
- Evidence anchors:
  - [abstract] "Our adapter is hierarchical in terms of how the adapter parameters are allocated. The adapter consists of a single shared controller network and multiple task-level adapter heads to reduce the per-task parameter overhead without performance regression on downstream tasks."
  - [section] "The controller is shared for all layers of the underlying backbone model as well as tasks and is responsible for orchestrating the interaction between the backbone model and task specialized adapter heads."
  - [corpus] Weak evidence - corpus neighbors focus on adapter efficiency but don't specifically address recurrent controller sharing.
- Break condition: If the controller becomes a bottleneck for complex tasks, or if layer-specific adaptations are crucial, performance would degrade.

### Mechanism 2
- Claim: HRA's hierarchical decomposition allows better parameter efficiency than flat adapter approaches.
- Mechanism: HRA separates adapter parameters into a shared controller (task-agnostic) and task-specific adapter heads, reducing redundant parameters across tasks.
- Core assumption: The controller can capture task-invariant features while adapter heads handle task-specific variations.
- Evidence anchors:
  - [abstract] "Our adapter is hierarchical in terms of how the adapter parameters are allocated. The adapter consists of a single shared controller network and multiple task-level adapter heads to reduce the per-task parameter overhead without performance regression on downstream tasks."
  - [section] "By defining a concept of task-level adapter head in HRA, we allocate a shared single adapter controller for all tasks while allowing an individual adapter head to specialize for a new task."
  - [corpus] Weak evidence - corpus neighbors discuss adapter efficiency but not hierarchical decomposition specifically.
- Break condition: If tasks require significantly different processing that the shared controller cannot accommodate, the hierarchical approach would fail.

### Mechanism 3
- Claim: HRA outperforms full fine-tuning despite using far fewer parameters.
- Mechanism: The combination of recurrent processing, parameter sharing, and task-specific heads allows HRA to capture essential task adaptations with minimal parameters.
- Core assumption: The IndRNN controller can effectively capture temporal dependencies across layers while the adapter heads provide task-specific refinements.
- Evidence anchors:
  - [abstract] "Our Hierarchical Recurrent Adapter (HRA) outperforms the previous adapter-based approaches as well as full model fine-tuning baseline in both single and multi-task adaptation settings when evaluated on automatic speech recognition tasks."
  - [section] "The proposed Hierarchical Recurrent Adapter consists of a single shared controller and multiple task specific adapter heads. The recurrent controller network is shared across all tasks while the task-level adapter head is specialized for each task."
  - [corpus] Weak evidence - corpus neighbors focus on adapter efficiency but don't provide direct comparisons to full fine-tuning.
- Break condition: If the controller or adapter heads cannot capture complex task-specific patterns, HRA would underperform compared to full fine-tuning.

## Foundational Learning

- Concept: Recurrent neural networks and their variants (IndRNN, RNN, Light GRU)
  - Why needed here: The controller is implemented as an IndRNN, requiring understanding of recurrent architectures and their properties.
  - Quick check question: What advantage does IndRNN have over standard RNN in terms of gradient stability?

- Concept: Adapter methods in deep learning (LoRA, Residual Adapters, BitFit)
  - Why needed here: HRA builds upon existing adapter techniques, so understanding their principles is crucial for grasping HRA's innovations.
  - Quick check question: How do Residual Adapters differ from LoRA in terms of parameter efficiency?

- Concept: Automatic Speech Recognition (ASR) and CTC loss
  - Why needed here: The experiments are conducted on ASR tasks using CTC loss, requiring familiarity with ASR concepts and training objectives.
  - Quick check question: What is the key difference between CTC and attention-based ASR approaches?

## Architecture Onboarding

- Component map: Backbone speech model (USM) -> HRA controller (IndRNN) -> Task adapter heads (Linear/FFN) -> Output feature
- Critical path: Input -> Backbone layer -> HRA controller -> Task adapter head -> Modified output -> Next backbone layer
- Design tradeoffs: Parameter efficiency vs. model capacity - HRA sacrifices some capacity for efficiency through parameter sharing
- Failure signatures: Poor WER on new tasks, inconsistent performance across layers, controller saturation
- First 3 experiments:
  1. Implement HRA with linear adapter heads and compare to Residual Adapters on a simple ASR task
  2. Test HRA with different controller sizes (256, 2048, 4096) to find optimal balance
  3. Evaluate multi-task performance by training HRA on two similar ASR tasks and measuring parameter efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HRA scale with increasing numbers of tasks, and is there a point where the parameter efficiency advantage diminishes?
- Basis in paper: [explicit] The paper mentions that HRA has sub-linear growth in parameter efficiency with increasing tasks, but does not explore the upper limits or potential diminishing returns.
- Why unresolved: The experiments only evaluated up to 128 tasks, and the paper does not discuss theoretical or empirical limits to HRA's scalability.
- What evidence would resolve it: Experiments evaluating HRA's performance and parameter efficiency on datasets with thousands of tasks, or theoretical analysis of HRA's scaling properties.

### Open Question 2
- Question: How does the choice of controller architecture (e.g., IndRNN, standard RNN, Light GRU) impact HRA's performance across different speech recognition tasks and datasets?
- Basis in paper: [explicit] The paper performed an ablation study on controller architectures, but only on a limited set of variants and for one dataset.
- Why unresolved: The ablation study results are specific to the ASR tasks and datasets used, and do not provide insights into the generalizability of HRA's performance across different tasks and datasets.
- What evidence would resolve it: Experiments evaluating HRA's performance with different controller architectures on a diverse set of speech recognition tasks and datasets.

### Open Question 3
- Question: Can HRA be effectively combined with other parameter-efficient adaptation methods, such as soft-prompt tuning, to further improve performance and efficiency?
- Basis in paper: [explicit] The paper mentions that it is unclear how to combine soft-prompt tuning with streaming speech models, but does not explore potential synergies between HRA and other adaptation methods.
- Why unresolved: The paper focuses on HRA as a standalone method and does not investigate potential combinations with other adaptation techniques.
- What evidence would resolve it: Experiments evaluating the performance of HRA combined with other parameter-efficient adaptation methods, such as soft-prompt tuning, on various speech recognition tasks and datasets.

## Limitations

- Limited task diversity: Experiments focus primarily on ASR tasks, limiting generalizability to other speech applications
- Multi-task evaluation scope: Only tested on two tasks, which may not fully represent scalability to many tasks
- Controller implementation details: Exact IndRNN architecture specifications are not fully provided

## Confidence

- **High Confidence**: The hierarchical decomposition concept and parameter efficiency claims are well-supported by experimental results.
- **Medium Confidence**: The superiority over full fine-tuning and other adapter methods is demonstrated but with limited task diversity.
- **Low Confidence**: Generalization claims to other speech processing tasks beyond ASR are not experimentally validated.

## Next Checks

1. **Controller Architecture Validation**: Systematically test different controller types (RNN, Light GRU, IndRNN) and sizes to quantify the impact on performance and parameter efficiency, addressing the question of whether IndRNN specifically is necessary or if simpler recurrent variants suffice.

2. **Cross-Domain Generalization**: Evaluate HRA on non-ASR speech tasks (e.g., speaker identification, speech translation) using the same pre-trained USM backbone to assess whether the hierarchical approach generalizes beyond the ASR domain.

3. **Scalability Analysis**: Test HRA with more than two tasks simultaneously, examining performance degradation as task count increases and measuring the actual inference-time computational overhead of the shared controller across multiple task adapter heads.