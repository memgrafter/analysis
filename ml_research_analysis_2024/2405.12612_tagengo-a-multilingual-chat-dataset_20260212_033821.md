---
ver: rpa2
title: 'Tagengo: A Multilingual Chat Dataset'
arxiv_id: '2405.12612'
source_url: https://arxiv.org/abs/2405.12612
tags:
- dataset
- prompts
- languages
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Tagengo, a high-quality multilingual chat dataset
  containing over 70k prompt-response pairs across 74 languages. The dataset was created
  by sampling human-generated prompts from the LMSYS-Chat-1M dataset and generating
  responses using GPT-4.
---

# Tagengo: A Multilingual Chat Dataset

## Quick Facts
- arXiv ID: 2405.12612
- Source URL: https://arxiv.org/abs/2405.12612
- Authors: Peter Devine
- Reference count: 8
- Key outcome: Multilingual training improves performance across all languages, including target monolingual languages

## Executive Summary
This paper introduces Tagengo, a high-quality multilingual chat dataset with over 70k prompt-response pairs across 74 languages. The dataset was created by sampling human-generated prompts from LMSYS-Chat-1M and generating responses using GPT-4. Two models were trained: a multilingual model and a Japanese-only model, both based on Llama 3 8B Instruct. The multilingual model outperformed state-of-the-art open-source models on MT-Bench benchmarks across 6 languages and achieved better Japanese performance than the Japanese-only model, demonstrating the benefits of transfer learning from multilingual training.

## Method Summary
The Tagengo dataset was created through a three-step process: (1) sampling prompts from LMSYS-Chat-1M with cleaning and filtering steps including moderation, language detection, anonymization removal, and length constraints; (2) generating responses using GPT-4 with temperature 0 and 2,048 token limits; and (3) deduplicating with BGE M3 embeddings at 0.8 threshold. The resulting dataset was combined with Megagon Instruction (669 Japanese pairs) and ShareGPT GPT-4 subset (6k English pairs), then used to fine-tune Llama 3 8B Instruct with Axolotl using full fine-tuning for one epoch with sample packing and 8,096 context length.

## Key Results
- Multilingual model outperforms Llama 3 8B Instruct on MT-Bench across 6 languages
- Multilingual model achieves higher MT-Bench scores on Japanese than Japanese-only model
- Model outperforms Starling 7B Beta across 5 out of 6 non-English languages tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual training data improves performance across all languages, including target monolingual languages
- Mechanism: Training on diverse multilingual data induces cross-linguistic transfer effects where shared linguistic structures, patterns, and representations benefit performance in the target language through joint optimization
- Core assumption: Linguistic features transfer beneficially across languages during joint training rather than causing interference
- Evidence anchors: abstract states multilingual training is beneficial to target language performance; section 4 shows multilingual model outperforms Starling 7B Beta across 5 out of 6 non-English languages and achieves higher MT-Bench scores on Japanese than Japanese-only model

### Mechanism 2
- Claim: High-quality synthetic responses generated by state-of-the-art models improve downstream LLM performance
- Mechanism: Model distillation from GPT-4 responses to smaller open-source models transfers the reasoning patterns, knowledge, and generation quality of the larger model to the smaller model
- Core assumption: GPT-4 responses are high quality and capture generalizable knowledge patterns that can be effectively transferred to smaller models
- Evidence anchors: section 3.1 describes generating responses using state-of-the-art model viewed as model distillation; section 4 shows multilingual model outperforms previous state-of-the-art open source models

### Mechanism 3
- Claim: Using real user prompts from LMSYS-Chat-1M creates more naturalistic training data than synthetic prompt generation
- Mechanism: Training on prompts that reflect actual user behavior and query patterns helps models better handle real-world use cases and diverse prompt styles
- Core assumption: User-generated prompts capture the distribution of real-world queries better than synthetically generated prompts
- Evidence anchors: section 3.1 explains sampling prompts from LMSYS-Chat-1M collected from users speaking to LLMs; corpus discussion of prompt quality issues

## Foundational Learning

- Concept: Transfer learning across languages
  - Why needed here: The paper demonstrates that training on multilingual data improves performance in specific target languages (Japanese), which requires understanding how knowledge transfers across languages
  - Quick check question: If a model is trained on English and Spanish data, what linguistic features might transfer to improve French performance?

- Concept: Model distillation
  - Why needed here: The paper uses GPT-4 responses to train smaller models, which is a distillation approach where a smaller model learns from a larger model's outputs
  - Quick check question: What are the key differences between teacher forcing and distillation in the context of training from synthetic data?

- Concept: Prompt engineering and data cleaning
  - Why needed here: The paper involves extensive prompt filtering (moderation, language detection, token limits) which requires understanding how prompt quality affects model training
  - Quick check question: How might removing prompts containing model names (GPT, Vicuna, etc.) affect the diversity and utility of the training data?

## Architecture Onboarding

- Component map: LMSYS-Chat-1M sampling -> cleaning/filtering -> GPT-4 response generation -> dataset curation -> Axolotl fine-tuning -> MT-Bench evaluation -> HuggingFace deployment

- Critical path: Data quality -> Model performance
  - The quality of prompts and GPT-4 responses directly determines model performance
  - Any issues in data cleaning, deduplication, or response generation will propagate to the final model

- Design tradeoffs:
  - Data generation cost vs quality: Using GPT-4 for responses ensures quality but is expensive; manual annotation would be higher quality but prohibitively costly
  - Language balance: Sampling 25k prompts per language counters English dominance but may undersample truly low-resource languages
  - Model size: 8B parameters balances performance with inference cost; larger models might perform better but be less accessible

- Failure signatures:
  - Poor performance on any language: Indicates data quality issues, insufficient language-specific examples, or catastrophic forgetting
  - Mode collapse: Model consistently generates similar responses across different prompts
  - Language switching: Model fails to maintain the prompt language in responses
  - Evaluation score drops: Could indicate overfitting, data contamination, or evaluation setup issues

- First 3 experiments:
  1. Ablation study: Train with and without deduplication to measure impact on diversity and performance
  2. Language balance test: Train with varying ratios of English to non-English data to find optimal multilingual ratio
  3. Response quality test: Compare model performance when using GPT-3.5 vs GPT-4 generated responses to quantify distillation quality impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of multilingual training data affect performance on low-resource languages?
- Basis in paper: [explicit] The paper acknowledges that the training dataset lacks low-resource languages like those in the Niger-Congo family, and that this could limit the usefulness of results to speakers of low-resource languages
- Why unresolved: The current dataset has limited representation of low-resource languages, and the paper does not explore how improving the quantity and quality of data for these languages would impact model performance
- What evidence would resolve it: Creating a larger, more diverse dataset with improved representation of low-resource languages and evaluating the performance of models trained on this data would provide insights into the impact of data quality on these languages

### Open Question 2
- Question: What is the effect of including multiple conversational turns in the training data on the model's ability to handle longer, more complex dialogues?
- Basis in paper: [inferred] The paper mentions that the training data mainly consists of single prompt-response pairs and suggests that future work could include creating a dataset with multiple turns of conversation
- Why unresolved: The current dataset lacks multi-turn conversations, and the paper does not investigate how this limitation affects the model's performance on longer, more complex dialogues
- What evidence would resolve it: Training models on a dataset that includes multi-turn conversations and evaluating their performance on longer, more complex dialogues would provide insights into the importance of this type of data for improving model capabilities

### Open Question 3
- Question: How does the inclusion of preference data and contrastive learning techniques impact the performance of multilingual models?
- Basis in paper: [inferred] The paper suggests that future work could include generating preference data and using contrastive learning techniques like Direct Preference Optimization and Odds Ratio Preference Optimization to further improve model performance
- Why unresolved: The paper does not explore the potential benefits of incorporating preference data and contrastive learning techniques into the training process for multilingual models
- What evidence would resolve it: Training multilingual models using preference data and contrastive learning techniques, and comparing their performance to models trained without these methods, would provide insights into the effectiveness of these approaches for improving multilingual model performance

## Limitations

- Evaluation focuses on 6 most well-represented languages rather than truly testing low-resource language capabilities
- Potential data contamination from LMSYS-Chat-1M containing prompts from existing open-source models
- Heavy reliance on synthetic data generation using GPT-4 may introduce biases that don't generalize to real-world usage

## Confidence

- **High confidence**: Dataset creation methodology and claim that multilingual training improves Japanese performance are well-supported by results
- **Medium confidence**: Claim that multilingual model outperforms state-of-the-art open-source models across multiple languages is supported by MT-Bench scores but evaluation methodology lacks full transparency
- **Low confidence**: Mechanism explanations for cross-linguistic transfer and assertions about real user prompt superiority are presented without empirical validation

## Next Checks

1. Conduct an ablation study testing the multilingual model's performance on truly low-resource languages (below top 20 in the dataset) to validate claims about broad multilingual capabilities

2. Perform data contamination analysis by checking whether any evaluation prompts or responses appear in the training data to ensure benchmark integrity

3. Test the hypothesis that real user prompts are superior by training parallel models with synthetically generated prompts of comparable quality and comparing their performance across languages