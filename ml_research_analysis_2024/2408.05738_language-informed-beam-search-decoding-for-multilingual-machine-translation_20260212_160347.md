---
ver: rpa2
title: Language-Informed Beam Search Decoding for Multilingual Machine Translation
arxiv_id: '2408.05738'
source_url: https://arxiv.org/abs/2408.05738
tags:
- beam
- off-target
- translation
- search
- haben
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and addresses the problem of off-target translations
  in multilingual neural machine translation (MNMT), where beam search decoding often
  produces translations in the wrong language. The authors analyze how off-target
  translations emerge during beam search and propose Language-informed Beam Search
  (LiBS), which incorporates a language identification model into beam search decoding
  to reduce such errors.
---

# Language-Informed Beam Search Decoding for Multilingual Machine Translation

## Quick Facts
- arXiv ID: 2408.05738
- Source URL: https://arxiv.org/abs/2408.05738
- Reference count: 11
- Primary result: LiBS improves translation quality by +1.1 BLEU and +0.9 BLEU on average, while reducing off-target rates from 22.9% to 7.7% and 65.8% to 25.3%

## Executive Summary
This paper addresses the problem of off-target translations in multilingual neural machine translation (MNMT), where beam search decoding often produces translations in the wrong language. The authors propose Language-informed Beam Search (LiBS), an inference-time algorithm that incorporates a language identification model into beam search decoding to reduce such errors. LiBS works by combining the NMT model's probability scores with language identification scores, biasing the search toward sequences in the desired target language. Experiments on WMT and OPUS-100 datasets demonstrate significant improvements in both translation quality and language correctness without requiring additional training data or model modifications.

## Method Summary
The authors propose Language-informed Beam Search (LiBS), which modifies standard beam search by incorporating a language identification (LiD) model. During decoding, LiBS pre-selects top-w candidates from each beam, runs the LiD model on each candidate to score its likelihood of being in the target language, and re-ranks all candidates by linearly combining the NMT model's log probability with the LiD model's log probability. A tunable coefficient α controls the trade-off between translation quality and language correctness. The algorithm is NMT-model agnostic and doesn't require additional parallel data, making it applicable to any existing multilingual NMT system.

## Key Results
- LiBS reduces off-target translation rates from 22.9% to 7.7% on WMT dataset and from 65.8% to 25.3% on OPUS-100
- Translation quality improves by +1.1 BLEU on WMT and +0.9 BLEU on OPUS-100 datasets
- LiBS achieves these improvements without additional training data or model modifications

## Why This Works (Mechanism)

### Mechanism 1
Beam search with large beam size causes off-target translations because source or English tokens are retained early in decoding, then generate longer sequences that outscore correct language continuations. The model assigns low probability to the first off-target token, but high transition probabilities for the rest of the off-target sequence. This results in the full off-target sentence having a higher cumulative probability than the correct-language sentence, even though the first token was unlikely.

### Mechanism 2
LiBS prevents off-target translations by incorporating language identification scores into the beam search ranking, biasing the search toward sequences in the desired target language. During beam search, LiBS pre-selects top-w candidates from each beam, then re-ranks all candidates by linearly combining the NMT model's log probability and the LiD model's log probability for the target language.

### Mechanism 3
LiBS maintains or improves translation quality while reducing off-target rates because it doesn't modify the underlying NMT model or require additional training data. By only changing the decoding algorithm and not the model itself, LiBS preserves the NMT model's learned representations and translation capabilities while correcting for its beam search weaknesses.

## Foundational Learning

- **Concept**: Beam search decoding in neural machine translation
  - Why needed here: Understanding how beam search works is essential to grasping why off-target translations emerge and how LiBS modifies the process
  - Quick check question: In standard beam search, what determines which candidates are kept for the next step?

- **Concept**: Language identification (LiD) models
  - Why needed here: LiBS relies on an off-the-shelf LiD model to score candidate translations for their likelihood of being in the target language
  - Quick check question: What type of input does a typical LiD model require, and what does it output?

- **Concept**: Linear combination of scores in re-ranking
  - Why needed here: LiBS combines NMT model scores and LiD scores using a linear combination with tunable coefficient α
  - Quick check question: If α is set to 0, what does LiBS reduce to? If α is set very high, what happens?

## Architecture Onboarding

- **Component map**: MNMT model (provides token probabilities) → Beam search algorithm (manages candidate sequences) → LiD model (scores language likelihood) → Linear combination (merges scores) → Final output (best-scoring sequence)
- **Critical path**: During each decoding step: get top-w candidates from each beam → run LiD on each candidate → combine NMT and LiD scores → select top-b candidates for next step
- **Design tradeoffs**: Speed vs accuracy (pre-selecting only top-w candidates speeds up LiD scoring but may miss some good sequences), translation quality vs language correctness (tuning α trades these off)
- **Failure signatures**: High off-target rates (LiD model errors or α too low), degraded translation quality (α too high or LiD model too strict), slow decoding (w too large or LiD model slow)
- **First 3 experiments**:
  1. Run baseline beam search on WMT test set, measure BLEU and off-target rates
  2. Run LiBS with α=0 (should match baseline), verify outputs are identical
  3. Run LiBS with α=1.0, measure improvement in BLEU and reduction in off-target rates compared to baseline

## Open Questions the Paper Calls Out

### Open Question 1
What is the exact mechanism by which off-target candidates receive higher probabilities in later decoding steps despite heavy penalties in early steps? The paper observes that "off-target candidates experience fewer penalties in the later steps" and "the off-target continuations receive a higher probability (less penalty) than the on-target ones" even though "the first off-target token receives a heavy penalty by the model."

### Open Question 2
How would LiBS performance change with different language identification models or confidence thresholds? The authors acknowledge "the performance of LiBS may vary with the use of alternative LiD models" and use FastText without exploring alternatives.

### Open Question 3
Can the insights from LiBS be applied to sampling-based decoding algorithms like nucleus sampling or top-k sampling? The paper notes LiBS is "a modified beam search algorithm" and "not directly applicable to recent Language Models (LLMs), which often do sampling during inference."

### Open Question 4
What is the relationship between off-target translation rates and specific language pairs' resource levels in multilingual models? The authors observe "extremely low resources for both languages" (Turkish and Gujarati) correlate with "large number of off-target errors" and hypothesize this connection.

## Limitations

- The paper uses only one LiD model (FastText) without exploring alternatives or their impact on performance
- Limited evaluation across different language families and pairs, despite claiming 100-language scalability
- Lacks ablation studies to isolate the contribution of different LiBS components to the observed improvements

## Confidence

**High Confidence**: The core observation that beam search produces off-target translations in multilingual NMT is well-supported. The claim that LiBS reduces off-target rates is well-validated by reported metrics.

**Medium Confidence**: The claim that LiBS improves translation quality (BLEU +1.1 and +0.9 on average) is supported by experimental results but lacks ablation studies. The assertion that LiBS is model-agnostic and requires no additional training data is technically true.

**Low Confidence**: The scalability claims for 100 languages are weakly supported given the limited evaluation across different language families and pairs. The paper doesn't provide analysis of failure cases or error patterns.

## Next Checks

1. **Ablation Study**: Run experiments comparing LiBS with different α values (including 0 for baseline), with and without the LiD model, and with different pre-selection widths w to isolate which components contribute most to performance improvements.

2. **Language Pair Analysis**: Evaluate LiBS performance across different language family pairs (e.g., Indo-European vs non-Indo-European, high-resource vs low-resource languages) to identify potential systematic weaknesses and validate the 100-language scalability claim.

3. **Error Case Investigation**: Analyze a sample of remaining off-target translations and degraded translations to understand whether errors stem from LiD model limitations, poor α tuning, or fundamental conflicts between language identification and translation quality objectives.