---
ver: rpa2
title: Efficient Prompt Tuning by Multi-Space Projection and Prompt Fusion
arxiv_id: '2405.11464'
source_url: https://arxiv.org/abs/2405.11464
tags:
- prompt
- performance
- tasks
- training
- soft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of prompt tuning for language
  models, which involves balancing accuracy and efficiency. The authors propose a
  novel method called EPT that decomposes a given soft prompt into a shorter prompt
  and two low-rank matrices.
---

# Efficient Prompt Tuning by Multi-Space Projection and Prompt Fusion

## Quick Facts
- arXiv ID: 2405.11464
- Source URL: https://arxiv.org/abs/2405.11464
- Reference count: 40
- Primary result: 12.9% relative improvement and 14% reduction in training time compared to other methods

## Executive Summary
This paper addresses the challenge of prompt tuning for language models, which requires balancing accuracy and efficiency while adapting to diverse downstream tasks. The authors propose EPT (Efficient Prompt Tuning), a method that decomposes soft prompts into shorter prompts and low-rank matrices to reduce training time while maintaining or improving accuracy. The method also projects prompts into multiple subspaces and uses a gating network to learn combination weights, ensuring consistent performance across different task types. Experiments on 13 natural language processing tasks demonstrate that EPT outperforms existing methods with up to 12.9% relative improvement and 14% faster training.

## Method Summary
EPT (Efficient Prompt Tuning) is a parameter-efficient fine-tuning method that decomposes a given soft prompt into three components: a shorter trainable prompt, two low-rank matrices that update frozen input embeddings, and a fusion module that compensates for semantic loss. The method projects the short prompt into multiple subspaces and uses a gating network to learn task-specific combination weights. An attention-based prompt fusion module enriches the semantic knowledge of the short prompt by leveraging the knowledge difference between the low-rank matrices and the original prompt. This architecture significantly reduces trainable parameters while maintaining or improving task performance across diverse downstream applications.

## Key Results
- Achieves up to 12.9% relative improvement in accuracy compared to existing prompt tuning methods
- Reduces training time by 14% while maintaining or improving performance
- Demonstrates consistent performance across 13 natural language processing tasks from GLUE and SuperGLUE benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing a soft prompt into a shorter prompt plus two low-rank matrices significantly reduces training parameters and time while maintaining or improving accuracy.
- Mechanism: The method splits the original soft prompt into three components: (1) a trainable short prompt that is attached to the input, (2) two low-rank matrices that update the frozen input embeddings, and (3) a fusion module that compensates for semantic loss. This reduces the overall number of trainable parameters while enriching the semantics of the short prompt.
- Core assumption: The short prompt retains essential semantic information, and the low-rank matrices can effectively update input embeddings without losing task-specific knowledge.
- Evidence anchors:
  - [abstract] "Specifically, it decomposes a given soft prompt into a shorter prompt and two low-rank matrices, significantly reducing the training time."
  - [section] "We decompose P ∈ Rl×d into a short trainable prompt Ps ∈ Rs×d and two trainable low-rank matrices (A ∈ Rm×r and B ∈ Rr×d) to reduce computing costs."

### Mechanism 2
- Claim: Projecting the soft prompt into multiple subspaces and reweighting them through a gating network improves performance consistency across diverse downstream tasks.
- Mechanism: The method maps the short prompt into several distinct subspaces, each potentially capturing different semantic aspects. A gating network learns to assign importance weights to each subspace based on the specific downstream task, ensuring that the prompt is adapted optimally for each task type.
- Core assumption: Different downstream tasks benefit from different semantic subspaces, and a gating mechanism can learn to select the most relevant subspace(s) for each task.
- Evidence anchors:
  - [abstract] "we project the soft prompt into multiple subspaces to improve the performance consistency, and then adaptively learn the combination weights of different spaces through a gating network."
  - [section] "We leverage a multi-space projection module to project a single soft prompt into multiple subspaces and reweight the soft prompt in these subspaces according to the task through the gating network."

### Mechanism 3
- Claim: The prompt fusion module enriches the semantic knowledge of the short prompt by leveraging the knowledge difference between low-rank matrices and the short prompt.
- Mechanism: An attention network computes the knowledge association between the low-rank matrices and the short prompt. This association is used to enhance the semantic content of the short prompt, compensating for the information lost during decomposition.
- Core assumption: The low-rank matrices contain complementary semantic information that, when fused with the short prompt, enhances overall semantic richness.
- Evidence anchors:
  - [abstract] "Accuracy is also enhanced by leveraging low-rank matrices and the short prompt as additional knowledge sources to enrich the semantics of the original short prompt."
  - [section] "We design a novel prompt fusion module to keep the short prompt efficiency and further compensate for the semantic loss... leveraging an attention network to consider the difference in knowledge richness between low-rank matrices and the short prompt."

## Foundational Learning

- Concept: Low-rank matrix decomposition
  - Why needed here: To reduce the number of trainable parameters while maintaining the ability to update input embeddings effectively.
  - Quick check question: Why does decomposing a matrix into two low-rank matrices (A and B) reduce the total number of parameters compared to the original matrix?

- Concept: Multi-space projection and gating networks
  - Why needed here: To adapt the prompt to different downstream tasks by reweighting prompt representations in multiple subspaces.
  - Quick check question: How does a gating network determine the importance of each subspace for a given task?

- Concept: Attention mechanisms for knowledge fusion
  - Why needed here: To capture and leverage the semantic differences between the short prompt and low-rank matrices, enriching the short prompt's semantics.
  - Quick check question: How does the attention mechanism compute the knowledge association between the short prompt and low-rank matrices?

## Architecture Onboarding

- Component map:
  - Prompt Decomposition Module: Splits the original prompt into a short prompt and two low-rank matrices
  - Multi-Space Projection Module: Projects the short prompt into multiple subspaces and learns combination weights via a gating network
  - Prompt Fusion Module: Uses an attention network to enrich the semantic knowledge of the short prompt
  - Reconstructed Prompt: Combines the outputs of the Multi-Space Projection and Prompt Fusion modules to form the final prompt
  - Pre-trained Language Model (PLM): Receives the reconstructed prompt and updated input embeddings for downstream task adaptation

- Critical path: Input text → Prompt Decomposition → Multi-Space Projection → Prompt Fusion → Reconstructed Prompt → PLM → Task Output

- Design tradeoffs:
  - Parameter efficiency vs. semantic richness: Decomposing the prompt reduces parameters but may lose semantic information, which is compensated by the fusion module
  - Task adaptability vs. computational cost: Multi-space projection improves adaptability but increases computational overhead
  - Prompt length vs. training time: Shorter prompts reduce training time but may impact performance, mitigated by the fusion and projection modules

- Failure signatures:
  - If the low-rank matrices do not effectively update input embeddings, the model may underperform on tasks requiring rich semantic understanding
  - If the gating network fails to learn meaningful weights, the model may not adapt well to different downstream tasks
  - If the attention mechanism in the fusion module does not capture meaningful knowledge differences, the semantic enrichment may be insufficient

- First 3 experiments:
  1. Test the decomposed prompt with only the short prompt and low-rank matrices (without fusion and projection) to evaluate baseline performance
  2. Add the prompt fusion module to the decomposed prompt to assess the impact of semantic enrichment
  3. Incorporate the multi-space projection module to evaluate improvements in task adaptability and performance consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed EPT method perform on different sizes of pre-trained language models (PLMs) beyond T5-Base, such as T5-Small and T5-Large?
- Basis in paper: [explicit] The paper mentions that the authors empirically analyze the impact of the model scale (T5-Small, T5-Base, T5-Large) on the performance of different baselines, including EPT, on BoolQ and MultiRC in the SuperGLUE benchmark.
- Why unresolved: The paper does not provide a detailed comparison of EPT's performance across different PLM sizes for all tasks.
- What evidence would resolve it: A comprehensive evaluation of EPT on all tasks in the GLUE and SuperGLUE benchmarks using T5-Small, T5-Base, and T5-Large models, showing how the performance scales with the model size.

### Open Question 2
- Question: What is the impact of the number of spaces in the Multi-Space Projection module on the performance of EPT across different tasks and datasets?
- Basis in paper: [explicit] The paper mentions that the authors dynamically alter the number of spaces N from 2 to 8 with a step size of 1 during training and select 4 as the optimal number of spaces.
- Why unresolved: The paper does not provide a detailed analysis of how the number of spaces affects the performance of EPT on individual tasks or across different datasets.
- What evidence would resolve it: A comprehensive evaluation of EPT with varying numbers of spaces (e.g., 2, 4, 6, 8) on all tasks in the GLUE and SuperGLUE benchmarks, showing the impact on performance and training time.

### Open Question 3
- Question: How does the proposed EPT method compare to other parameter-efficient fine-tuning (PEFT) methods, such as LoRA and Adapter, in terms of performance and training efficiency across different tasks and datasets?
- Basis in paper: [explicit] The paper mentions that EPT outperforms 11 comparison methods, including LoRA and Adapter, on the GLUE and SuperGLUE benchmarks.
- Why unresolved: The paper does not provide a detailed comparison of EPT's performance and training efficiency relative to other PEFT methods across all tasks and datasets.
- What evidence would resolve it: A comprehensive evaluation of EPT, LoRA, and Adapter on all tasks in the GLUE and SuperGLUE benchmarks, showing the performance and training time for each method. Additionally, an analysis of the trade-offs between performance and training efficiency for each method.

## Limitations

- The decomposition approach relies heavily on the assumption that low-rank matrices can effectively capture semantic relationships while maintaining task performance
- The effectiveness of the multi-space projection and gating mechanism across diverse task types is not fully explored, particularly for tasks requiring different semantic representations
- The prompt fusion module's attention mechanism may introduce computational overhead that could offset efficiency gains in some scenarios

## Confidence

**High Confidence**: The experimental results showing 12.9% relative improvement and 14% reduction in training time are well-supported by the data and methodology. The basic decomposition mechanism is clearly explained and empirically validated.

**Medium Confidence**: The effectiveness of the multi-space projection and gating network for task adaptation, while promising, requires more detailed analysis of how different subspaces contribute to performance across task types. The theoretical justification for why this approach improves consistency could be strengthened.

**Medium Confidence**: The prompt fusion mechanism's ability to compensate for semantic loss during decomposition is demonstrated empirically but lacks detailed analysis of what specific semantic aspects are being preserved or enhanced through the attention mechanism.

## Next Checks

1. **Ablation study on prompt fusion effectiveness**: Conduct experiments comparing EPT with and without the prompt fusion module across all 13 tasks to quantify the specific contribution of semantic enrichment to overall performance.

2. **Cross-task subspace analysis**: Analyze the learned gating weights across different task categories to determine whether the multi-space projection is capturing meaningful task-specific semantic patterns or if weights are converging to similar distributions.

3. **Parameter sensitivity validation**: Systematically vary the hyper-parameters s and r in the decomposition step to establish their impact on both efficiency gains and accuracy maintenance, particularly identifying thresholds where performance degrades.