---
ver: rpa2
title: 'D-Nikud: Enhancing Hebrew Diacritization with LSTM and Pretrained Models'
arxiv_id: '2402.00075'
source_url: https://arxiv.org/abs/2402.00075
tags:
- hebrew
- d-nikud
- diacritization
- data
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: D-Nikud presents a Hebrew diacritization system that combines LSTM
  networks with the TavBERT transformer-based pre-trained model. The model leverages
  character-level embeddings from TavBERT and processes sequences using bidirectional
  LSTM layers, with separate classification outputs for nikud, dagesh, and sin diacritics.
---

# D-Nikud: Enhancing Hebrew Diacritization with LSTM and Pretrained Models

## Quick Facts
- arXiv ID: 2402.00075
- Source URL: https://arxiv.org/abs/2402.00075
- Reference count: 7
- D-Nikud achieves 98.39% decision accuracy and 97.15% character accuracy on Hebrew diacritization

## Executive Summary
D-Nikud presents a Hebrew diacritization system that combines LSTM networks with the TavBERT transformer-based pre-trained model. The system leverages character-level embeddings from TavBERT and processes sequences using bidirectional LSTM layers, with separate classification outputs for nikud, dagesh, and sin diacritics. Trained on diverse Hebrew datasets including modern and pre-modern texts totaling over 1.5 million tokens, D-Nikud achieves state-of-the-art results that outperform existing solutions like Nakdimon, Dicta, Snopi, and Morfix. The model demonstrates particular effectiveness in processing feminine language forms and offers faster computation speeds compared to comparable systems.

## Method Summary
The D-Nikud system integrates character-level embeddings from the TavBERT pre-trained transformer model with bidirectional LSTM layers for sequence processing. The architecture features separate classification heads for different diacritic types (nikud, dagesh, and sin), enabling specialized handling of each diacritic category. The model was trained on a diverse corpus of Hebrew texts encompassing both modern and pre-modern material, totaling over 1.5 million tokens. This combination of transformer-based embeddings with recurrent neural networks allows the system to capture both local character patterns and broader contextual dependencies essential for accurate diacritization.

## Key Results
- Achieves 98.39% decision accuracy and 97.15% character accuracy on benchmark tests
- Outperforms existing solutions including Nakdimon, Dicta, Snopi, and Morfix
- Demonstrates particular effectiveness in processing feminine language forms
- Offers faster computation speeds compared to comparable systems

## Why This Works (Mechanism)
The system's effectiveness stems from the complementary strengths of transformer-based embeddings and LSTM sequence modeling. TavBERT provides rich character-level representations that capture morphological and semantic patterns in Hebrew text, while the bidirectional LSTM layers process these embeddings in both forward and backward directions to capture contextual dependencies. The separation of classification outputs for different diacritic types allows the model to specialize in the unique patterns and challenges associated with each diacritic category. This architecture effectively combines the deep semantic understanding from the pre-trained transformer with the sequential modeling capabilities of LSTMs, resulting in superior diacritization performance across diverse Hebrew text types.

## Foundational Learning

1. **Hebrew Diacritization (Nikud)**: The system of vowel and consonant marking in Hebrew text. Why needed: Essential for accurate Hebrew text representation and reading comprehension. Quick check: Can distinguish between homographs like "כתב" (wrote) vs "כתב" (book).

2. **Transformer-Based Embeddings**: Deep learning models that use self-attention mechanisms to capture contextual relationships. Why needed: Provides rich semantic and morphological representations of Hebrew characters. Quick check: Can maintain contextual meaning across sentence boundaries.

3. **Bidirectional LSTM Networks**: Recurrent neural networks that process sequences in both forward and backward directions. Why needed: Captures dependencies from both preceding and following characters for accurate diacritization. Quick check: Can correctly place diacritics in ambiguous contexts based on surrounding characters.

## Architecture Onboarding

Component Map: TavBERT Embeddings -> Bidirectional LSTM Layers -> Classification Heads (Nikud/Dagesh/Sin)

Critical Path: The sequence processing pipeline where TavBERT character embeddings are fed into bidirectional LSTM layers, which then produce outputs for the three separate classification heads responsible for different diacritic types.

Design Tradeoffs: The choice of combining transformer embeddings with LSTM layers balances the need for deep semantic understanding with sequential processing capabilities. Using separate classification heads allows specialization but increases model complexity.

Failure Signatures: Performance degradation in contexts with rare feminine forms or historical Hebrew variations. Potential overfitting to modern Hebrew text patterns at the expense of pre-modern text accuracy.

First Experiments:
1. Test diacritization accuracy on sentences containing feminine verb forms
2. Evaluate performance on pre-modern Hebrew text samples
3. Measure inference time comparison with Nakdimon system on identical hardware

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, focusing instead on presenting the system's architecture and performance results.

## Limitations

- Lacks ablation studies to isolate the contribution of individual components (TavBERT embeddings, LSTM layers, classification heads)
- Superiority claims over commercial systems like Dicta, Snopi, and Morfix are demonstrated only through benchmark testing without direct head-to-head comparisons
- Computational efficiency claims are asserted but not empirically validated with timing benchmarks

## Confidence

High: Reported accuracy figures for benchmark datasets
Medium: Claim of state-of-the-art performance based on published results
Low: Computational efficiency claims due to absence of timing data

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of TavBERT embeddings versus LSTM layers to overall performance
2. Perform direct head-to-head comparisons with Dicta, Snopi, and Morfix on identical datasets to verify claimed superiority
3. Measure and report computation times for training and inference across different system configurations to substantiate efficiency claims