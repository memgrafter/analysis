---
ver: rpa2
title: 'Wonderful Team: Zero-Shot Physical Task Planning with Visual LLMs'
arxiv_id: '2407.19094'
source_url: https://arxiv.org/abs/2407.19094
tags:
- tasks
- task
- page
- team
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Wonderful Team is a zero-shot multi-agent Vision Large Language
  Model framework that performs high-level robotic planning. It uses a hierarchical
  structure with specialized agents for planning, verification, memory, and grounding
  to execute complex manipulation tasks without training.
---

# Wonderful Team: Zero-Shot Physical Task Planning with Visual LLMs

## Quick Facts
- arXiv ID: 2407.19094
- Source URL: https://arxiv.org/abs/2407.19094
- Authors: Zidan Wang; Rui Shen; Bradly Stadie
- Reference count: 40
- Primary result: 40% improvement over prior methods on VIMABench tasks

## Executive Summary
Wonderful Team introduces a zero-shot multi-agent Vision Large Language Model framework for high-level robotic planning. The system achieves precise coordinate-level actions through a hierarchical structure with specialized agents for planning, verification, memory, and grounding, all operating within a single off-the-shelf VLLM model. Experiments demonstrate 91.25% success rate on visual reasoning tasks and 100% success on real-world manipulation tasks, showing that modern VLMs can effectively handle perception, planning, and control without fine-tuning.

## Method Summary
Wonderful Team employs a multi-agent hierarchical system using off-the-shelf GPT-4o without any fine-tuning. The framework decomposes complex robotic tasks through specialized agents: a supervisor breaks down tasks into sub-goals, a verification agent checks plan integrity, a memory agent handles long-horizon task details, and a grounding team identifies object locations through iterative bounding box refinement. The grounding team manager coordinates box proposal and checking agents using visualization tools to achieve precise coordinate extraction for robot action execution.

## Key Results
- 40% improvement over prior methods on VIMABench tasks on average
- 91.25% success rate on visual reasoning tasks
- 100% success rate on real-world tasks including fruit placement and price ranking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent hierarchical structure enables decomposition of complex robotic tasks into manageable sub-goals.
- Mechanism: Specialized agents (supervisor, verification, memory, grounding) work together to break down tasks and coordinate execution within a single VLLM model.
- Core assumption: Specialized agents can effectively communicate and coordinate without external vision encoders or fine-tuning.
- Evidence anchors: [abstract] and [section] describing multi-agent hierarchy; [corpus] weak evidence from related papers on zero-shot task planning.

### Mechanism 2
- Claim: Grounding team iteratively refines object location coordinates through visual feedback and comparison tools.
- Mechanism: Box proposal and checking agents coordinate to improve bounding box accuracy using visualization tools until coordinates are precise enough for action execution.
- Core assumption: VLLMs have sufficient spatial reasoning capability to evaluate and refine bounding box coordinates.
- Evidence anchors: [section] describing grounding team responsibilities; [corpus] weak evidence from related papers on object detection.

### Mechanism 3
- Claim: Modern VLMs can handle entire robotics pipeline from perception to control without separate vision encoders or fine-tuning.
- Mechanism: Single off-the-shelf VLLM processes both visual and language inputs for planning, object identification, and coordinate extraction.
- Core assumption: Current VLMs have sufficient multimodal capabilities to replace traditional computer vision components.
- Evidence anchors: [abstract] and [section] emphasizing zero-shot approach; [corpus] moderate evidence from related papers on VLM-based robotic manipulation.

## Foundational Learning

- Concept: Multi-agent systems and agent hierarchies
  - Why needed here: Framework relies on multiple specialized agents working together to solve complex tasks.
  - Quick check question: Can you explain how the supervisor, verification, memory, and grounding agents interact to complete a task?

- Concept: Zero-shot learning and in-context learning
  - Why needed here: System operates without training or fine-tuning, relying on VLLM's understanding from descriptions and images.
  - Quick check question: What are the key differences between zero-shot, few-shot, and fine-tuned approaches in robotic planning?

- Concept: Coordinate-based control and pixel-space mapping
  - Why needed here: System outputs precise (x,y) coordinates for robot execution, requiring mapping from image coordinates to real-world positions.
  - Quick check question: How does the system ensure pixel coordinates correspond accurately to physical environment positions?

## Architecture Onboarding

- Component map: Supervisor → Verification → Memory → Grounding Team Manager → Box Proposal → Box Checking → Action Execution → Supervisor feedback loop
- Critical path: Supervisor → Verification → Grounding Team → Action Execution → Supervisor feedback loop
- Design tradeoffs:
  - Single VLLM vs. specialized models: Simpler deployment but potential accuracy limitations
  - Iterative refinement vs. one-shot prediction: Higher accuracy but increased computational cost
  - Hierarchical vs. flat structure: Better task decomposition but more complex coordination
- Failure signatures:
  - Agent communication breakdowns (messages not passed correctly)
  - VLLM context limitations (cannot maintain conversation across many agent interactions)
  - Coordinate drift (accumulated errors in position estimation)
  - Hallucination propagation (errors amplified through agent interactions)
- First 3 experiments:
  1. Simple pick-and-place task with one object and one container to verify basic agent coordination
  2. Multi-object rearrangement task to test sub-goal decomposition and memory capabilities
  3. Visual reasoning task with implicit constraints to evaluate complex planning and verification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Wonderful Team handle tasks requiring consecutive frame understanding, such as tracking object movements between multiple images?
- Basis in paper: [explicit] Section 6.4 identifies this as a limitation, noting VLMs struggle to discern differences between consecutive frames.
- Why unresolved: Paper mentions difficulty but doesn't propose concrete solution or methodology.
- What evidence would resolve it: Experimental results demonstrating improved performance on multi-frame tasks using proposed solution.

### Open Question 2
- Question: What are performance differences between Wonderful Team and fine-tuned models on complex, real-world robotics tasks beyond VIMABench?
- Basis in paper: [inferred] Paper compares to fine-tuned models on VIMABench but doesn't explore more complex real-world tasks.
- Why unresolved: Experiments limited to VIMABench and few real-world tasks, not fully representing real-world complexity.
- What evidence would resolve it: Comparative experiments on diverse real-world robotics tasks measuring performance against zero-shot and fine-tuned methods.

### Open Question 3
- Question: How does performance scale with increasing task complexity and longer horizon planning requirements?
- Basis in paper: [inferred] Paper demonstrates success on various tasks but doesn't explicitly analyze performance scaling with complexity.
- Why unresolved: Focuses on demonstrating capabilities without detailed analysis of performance trends as complexity increases.
- What evidence would resolve it: Systematic experiments varying task complexity and planning horizon with performance metrics showing trends.

## Limitations
- Limited real-world testing to only three tasks in single environment, raising questions about generalizability
- Computational costs and latency implications of multi-agent iterative refinement not addressed
- Performance of grounding team's iterative refinement depends heavily on quality of unspecified visualization tools

## Confidence

- High Confidence: Hierarchical multi-agent architecture concept and task decomposition capability
- Medium Confidence: 40% improvement over prior methods on VIMABench
- Medium Confidence: 100% success rate on real-world tasks (based on three tasks only)
- Low Confidence: Assertion that no training or fine-tuning is needed for robust performance across diverse scenarios

## Next Checks

1. Test grounding team's iterative refinement mechanism on occluded objects and objects with similar appearances to evaluate coordinate extraction accuracy robustness
2. Measure end-to-end task completion latency and computational resource usage for multi-agent system to assess practical deployment feasibility
3. Validate system performance on broader range of real-world tasks (10+ diverse scenarios) with different object types, lighting conditions, and environmental layouts to establish generalizability