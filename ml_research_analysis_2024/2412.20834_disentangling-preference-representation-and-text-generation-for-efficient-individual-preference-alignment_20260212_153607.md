---
ver: rpa2
title: Disentangling Preference Representation and Text Generation for Efficient Individual
  Preference Alignment
arxiv_id: '2412.20834'
source_url: https://arxiv.org/abs/2412.20834
tags:
- latent
- generation
- maximum
- length
- reached
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel approach to individual preference
  alignment in Large Language Models (LLMs) by disentangling preference representation
  from text generation. The method involves pre-training a latent encoder and adapter
  to extend LLMs into Variational Auto-Encoders, followed by fine-tuning personalized
  latent encoders from individual feedback using a novel Latent Direct Preference
  Optimization (Latent DPO) algorithm.
---

# Disentangling Preference Representation and Text Generation for Efficient Individual Preference Alignment

## Quick Facts
- arXiv ID: 2412.20834
- Source URL: https://arxiv.org/abs/2412.20834
- Authors: Jianfei Zhang; Jun Bai; Bei Li; Yanmeng Wang; Rumei Li; Chenghua Lin; Wenge Rong
- Reference count: 40
- Primary result: Achieves competitive alignment quality vs PEFT methods while reducing training time by 80-90%

## Executive Summary
This work introduces a novel approach to individual preference alignment in Large Language Models (LLMs) by disentangling preference representation from text generation. The method involves pre-training a latent encoder and adapter to extend LLMs into Variational Auto-Encoders, followed by fine-tuning personalized latent encoders from individual feedback using a novel Latent Direct Preference Optimization (Latent DPO) algorithm. This approach enables efficient individual preference alignment by optimizing only small latent encoders instead of entire LLMs. Experiments across multiple text generation tasks demonstrate that the proposed method achieves competitive alignment quality compared to Parameter-Efficient Fine-Tuning (PEFT)-based methods while reducing additional training time for each new individual preference by 80% to 90%. The results validate the effectiveness and scalability of this approach for aligning LLMs with diverse individual preferences.

## Method Summary
The method introduces Contrastive Language-Latent Pretraining (CLaP) to extend a decoder-only LLM into a VAE with a latent encoder and adapter. During pre-training, CLaP uses ELBo with Density Gap-based KL divergence and contrastive learning on self-generated responses. For personalization, the method employs Latent DPO, which infers rewards on latent variables through importance reweighting using the pre-trained posterior latent encoder, then applies DPO to optimize the personalized latent encoder distribution. This approach enables training only small latent encoders per user while keeping the LLM frozen, achieving 80-90% reduction in training time compared to PEFT methods.

## Key Results
- Achieves competitive win-rates on IMDB (sentiment) and TL;DR (summarization) tasks compared to LoRA and P-Tuning baselines
- Reduces individual preference alignment training time by 80-90% compared to PEFT methods
- Demonstrates effective disentanglement of preference representation from text generation in LLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Disentangling preference representation from text generation allows efficient individual alignment by training only small latent encoders instead of entire LLMs.
- **Mechanism**: The method extends LLMs into Variational Auto-Encoders (VAEs) where a latent encoder maps responses to low-dimensional latent variables, and a latent adapter feeds these representations to the LLM. Individual preferences are learned by fine-tuning only the personalized latent encoder while keeping the LLM frozen.
- **Core assumption**: Individual preferences can be effectively captured by low-dimensional latent variables that control the generation process without requiring full LLM fine-tuning.
- **Evidence anchors**:
  - [abstract] "Our method fundamentally improves efficiency by disentangling preference representation from text generation in LLMs"
  - [section] "individual preferences can be represented by low-dimensional vectors, i.e., latent variables, from small models disentangled from LLMs"
  - [corpus] Found 25 related papers, average neighbor FMR=0.497, including papers on "Latent Embedding Adaptation for Human Preference Alignment" suggesting the approach is well-supported in related literature
- **Break condition**: If individual preferences require complex, high-dimensional representations that cannot be captured by the learned latent space, or if the latent-LLM interface becomes a bottleneck.

### Mechanism 2
- **Claim**: Latent DPO enables efficient preference optimization by operating directly on latent variables rather than full LLM parameters.
- **Mechanism**: The method infers rewards on latent variables through importance reweighting using the pre-trained posterior latent encoder, then applies DPO to optimize the personalized latent encoder distribution. This avoids expensive LLM computations during personalization.
- **Core assumption**: Reward functions on responses can be effectively approximated as monotonic functions of rewards on latent variables through the posterior encoder.
- **Evidence anchors**:
  - [section] "Instead of end-to-end training that depends on computation in the LLM, we introduce latent DPO through: (1) inferring preference on latent values from preference on responses; (2) optimizing the latent encoder pθ(z|x) through DPO on preferred and dispreferred latent values"
  - [section] "Since r(x, yw) > r (x, yl), we can infer that the approximation of r(x, z) in Eq. 11 is monotonically increasing with respect to q(z|x,yw)/q(z|x,yl)"
  - [corpus] "DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization" shows related work using DPO for alignment tasks
- **Break condition**: If the latent reward approximation becomes too noisy or loses important preference information, making the DPO optimization ineffective.

### Mechanism 3
- **Claim**: The contrastive learning component in CLaP strengthens the alignment between latent representations and generated responses, improving personalization quality.
- **Mechanism**: During pre-training, contrastive learning encourages the model to generate responses from relevant latent representations while discouraging generation from irrelevant ones, creating a stronger mapping between latent space and response space.
- **Core assumption**: The contrastive objective helps the model learn a more discriminative latent space that better captures the relationship between prompts and responses.
- **Evidence anchors**:
  - [section] "we strengthen the alignment between representation in q(z|x, y) and generation in p(y|x, z) through contrastive learning"
  - [section] "Through maximizing LContrastive in Eq. 3, we encourage response generation from the relative latent representation and discourage that from the irrelevant ones"
  - [corpus] Weak - the corpus doesn't directly mention contrastive learning for VAEs, but this is a standard technique in representation learning
- **Break condition**: If the contrastive learning doesn't improve the latent-LLM mapping beyond what ELBO training alone achieves, or if it creates spurious correlations in the latent space.

## Foundational Learning

- **Concept**: Variational Auto-Encoders (VAEs) and their training objective (ELBO)
  - Why needed here: The entire method builds on extending LLMs into VAEs, so understanding how VAEs work and how they're trained is fundamental
  - Quick check question: What is the difference between the standard KL divergence term and the Density Gap-based KL divergence used in this work, and why might the latter be preferred?

- **Concept**: Direct Preference Optimization (DPO) and its mathematical formulation
  - Why needed here: Latent DPO is a core component that enables efficient preference learning, and understanding standard DPO is prerequisite to understanding the latent variant
  - Quick check question: How does DPO simplify the optimization problem compared to traditional RLHF, and what are the key mathematical assumptions that make this simplification valid?

- **Concept**: Importance sampling and variational inference
  - Why needed here: These techniques are used to approximate rewards on latent variables from rewards on responses, which is crucial for the Latent DPO mechanism
  - Quick check question: Explain how importance reweighting is used to approximate expectations under a target distribution using samples from a proposal distribution, and what conditions make this approximation accurate.

## Architecture Onboarding

- **Component map**: LLM (frozen) <- Latent adapter <- Personalized latent encoder pθ(z|x) <- Posterior latent encoder q(z|x, y)
- **Critical path**: CLaP pre-training → User preference data → Latent reward approximation → Latent DPO optimization → Personalized generation
- **Design tradeoffs**: 
  - Memory vs. computation: Using small latent encoders saves memory but requires careful design of the latent-LLM interface
  - Expressiveness vs. efficiency: Lower-dimensional latent spaces are more efficient but may not capture all preference nuances
  - Pre-training cost vs. personalization efficiency: CLaP requires upfront cost but enables much faster individual personalization
- **Failure signatures**:
  - Poor personalization quality: May indicate insufficient latent dimensionality or ineffective contrastive learning
  - Unstable training: Could suggest issues with the importance reweighting approximation or reward hacking
  - High computational cost: May indicate the latent adapter is too complex or the inference procedure is inefficient
- **First 3 experiments**:
  1. Implement basic CLaP pre-training on a small dataset (e.g., IMDB) and verify the VAE reconstruction quality matches the base LLM
  2. Test Latent DPO on a synthetic preference dataset to verify the importance reweighting approximation works correctly
  3. Compare personalization quality and training time between Latent DPO and standard LoRA-based DPO on a simple task

## Open Questions the Paper Calls Out

The paper acknowledges several limitations: (1) The method may have difficulty making fundamental generative distribution shifts in LLMs; (2) The performance of the proposed method may be constrained by the contrastive language-latent pre-training; (3) The method's scalability to very large models and massive user bases is not explored.

## Limitations

- Relies on assumption that individual preferences can be effectively captured in low-dimensional latent spaces without extensive ablation studies on latent dimensionality
- Novel Density Gap-based KL divergence (DG-KLD) component lacks extensive validation compared to standard variational objectives
- Does not address scalability challenges for very large models (GPT-4 scale) or massive user bases (millions of users)
- Does not explore robustness to noisy or adversarial feedback scenarios
- Does not investigate the trade-off between personalization and maintaining general capabilities

## Confidence

- High confidence in efficiency claims: The 80-90% training time reduction is directly measurable and well-supported by the experimental results
- Medium confidence in alignment quality: While competitive with PEFT methods, the paper doesn't establish clear superiority, and the preference representation quality depends heavily on the contrastive learning component
- Low confidence in scalability claims: The paper demonstrates effectiveness on three datasets but doesn't address potential challenges with very large-scale deployment or highly nuanced preference distributions

## Next Checks

1. **Latent dimensionality ablation**: Systematically test the method across different latent space dimensions (e.g., 16, 32, 64, 128) to identify the optimal tradeoff between efficiency and preference capture quality
2. **Reward approximation validation**: Conduct controlled experiments where ground truth rewards on latent variables are available to quantify the accuracy loss in the importance reweighting approximation
3. **Cross-domain generalization**: Test the pre-trained CLaP model on completely unseen preference domains to evaluate whether the latent space generalizes beyond the training datasets used in the experiments