---
ver: rpa2
title: Online Stackelberg Optimization via Nonlinear Control
arxiv_id: '2406.18805'
source_url: https://arxiv.org/abs/2406.18805
tags:
- which
- each
- regret
- where
- convex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified algorithmic framework for regret
  minimization in repeated Stackelberg games with adaptive agents. The core method
  casts these problems as online nonlinear control with locally controllable dynamics
  and convex losses over states.
---

# Online Stackelberg Optimization via Nonlinear Control

## Quick Facts
- arXiv ID: 2406.18805
- Source URL: https://arxiv.org/abs/2406.18805
- Reference count: 40
- One-line primary result: A unified algorithmic framework for regret minimization in repeated Stackelberg games with adaptive agents via online nonlinear control

## Executive Summary
This paper introduces a novel algorithmic framework for regret minimization in repeated Stackelberg games where agents adapt over time. The core insight is that many such problems can be cast as instances of online nonlinear control with locally controllable dynamics and convex losses over a bounded state space. By leveraging techniques from online convex optimization and nonlinear control theory, the authors develop algorithms that achieve sublinear regret bounds while maintaining state-competitiveness with respect to the best fixed state in hindsight.

The framework is particularly powerful because it unifies seemingly disparate problems like performative prediction, adaptive recommendations, adaptive pricing, and repeated gameplay against no-regret learners under a common theoretical umbrella. The key technical contribution is showing how local controllability of the dynamics enables a reduction to online convex optimization over the state space, while also providing concrete algorithms that handle various practical considerations like adversarial disturbances, bandit feedback, and unknown dynamics.

## Method Summary
The method reduces regret minimization in repeated Stackelberg games to online convex optimization over a bounded state space. When dynamics are locally controllable, every state in a ball around the current state is reachable via some action, enabling simulation of an OCO algorithm over the state space. The framework uses Follow-the-Regularized-Leader (FTRL) as the primary OCO subroutine and provides oracle-efficient algorithms when dynamics are locally action-linear. For adversarial disturbances, the paper provides two algorithms: NestedOCO-BD for weakly locally controllable dynamics with bounded disturbances, and NestedOCO-UD for strongly locally controllable dynamics with unbounded disturbances. The approach generalizes online control beyond linear policies and enables tractable optimization over bounded state spaces common in Stackelberg settings.

## Key Results
- O(√T) regret via reduction to online convex optimization for known, time-varying dynamics without disturbances
- Tight O(E·ρ⁻¹) regret bounds for adversarial disturbances, with thresholds for weak vs. strong local controllability
- Oracle-efficient algorithms when dynamics are locally action-linear
- O(T³/⁴) regret under bandit feedback
- Extensions to unknown dynamics with approximate stabilizing actions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework reduces regret minimization in repeated Stackelberg games to online convex optimization over a bounded state space.
- Mechanism: When dynamics are locally controllable, every state in a ball around the current state is reachable via some action. This enables simulation of an online convex optimization (OCO) algorithm over the state space by playing actions that drive the state toward the current OCO target.
- Core assumption: The dynamics D(x,y) are continuous and satisfy weak or strong local controllability as defined in the paper.
- Evidence anchors:
  - [abstract]: "we show that many problems of this form can be cast as instances of online (nonlinear) control which satisfy local controllability, with convex losses over a bounded state space"
  - [section 2.1]: Formal definition of weak and strong local controllability
  - [corpus]: No direct match; corpus neighbors focus on regret analysis in linear/nonlinear control but not specifically Stackelberg-local-controllability reduction.
- Break condition: If the dynamics are not locally controllable, the reachable set around the current state may not cover the entire feasible state space, breaking the reduction to OCO.

### Mechanism 2
- Claim: Oracle-efficient O(√T) regret is achieved when dynamics are locally action-linear.
- Mechanism: If D(x,y) = Ay·x + by for some matrix Ay and vector by (which may depend on y), the per-round optimization problem for selecting xt becomes convex and solvable in polynomial time, eliminating the need for a non-convex oracle.
- Core assumption: Dynamics are locally action-linear, meaning the dependence on x is linear at each fixed y.
- Evidence anchors:
  - [section 3.2]: Proposition 3 states that for action-linear dynamics, the per-round optimization problem is convex.
  - [section 3.2]: "The class of action-linear dynamics is quite general, owing to the flexibility permitted by nonlinear parameterizations of (Ay, by) in terms of y."
  - [corpus]: No direct match; corpus neighbors do not discuss action-linear dynamics in the Stackelberg context.
- Break condition: If the dynamics have nonlinear dependence on x (even locally), the per-round optimization may become non-convex and require an oracle.

### Mechanism 3
- Claim: The framework handles adversarial disturbances by tracking deviations from an idealized trajectory and calibrating parameters to preserve reachability margin.
- Mechanism: Two algorithms are provided: NestedOCO-BD for weakly locally controllable dynamics with bounded disturbances, and NestedOCO-UD for strongly locally controllable dynamics with unbounded disturbances. Both algorithms maintain an estimate of the "undisturbed" trajectory and adjust actions to correct for accumulated disturbance error.
- Core assumption: The disturbance sequence {wt} is adversarial but satisfies certain magnitude constraints (bounded by ρ-dependent thresholds for weak controllability, or cumulative bound E for strong controllability).
- Evidence anchors:
  - [section 3.3]: Theorem 2 states tight bounds in terms of cumulative disturbance magnitude for weakly locally controllable dynamics, with a threshold for weak vs. strong local controllability.
  - [section 3.3]: Theorem 3 gives tight O(E·ρ⁻¹) regret bounds for strongly locally controllable dynamics with unbounded disturbances.
  - [corpus]: No direct match; corpus neighbors focus on robust control but not specifically Stackelberg with adversarial disturbances.
- Break condition: If disturbances exceed the ρ-dependent threshold for weak controllability, or if the cumulative disturbance E is too large relative to the system's ability to correct, the algorithm may incur linear regret.

## Foundational Learning

- Concept: Online convex optimization (OCO) and Follow-the-Regularized-Leader (FTRL)
  - Why needed here: The framework reduces regret minimization to OCO over the state space, using FTRL as the primary OCO subroutine for its strong regret guarantees and per-round step size bounds.
  - Quick check question: What is the regret bound for FTRL with a γ-strongly convex regularizer and L-Lipschitz convex losses over a bounded domain?

- Concept: Local controllability in nonlinear control
  - Why needed here: Local controllability ensures that every state in a neighborhood around the current state is reachable via some action, enabling the reduction to OCO and the tracking of desired state trajectories.
  - Quick check question: What is the difference between weak and strong local controllability, and how do they affect the algorithm's ability to handle disturbances?

- Concept: Convex optimization and non-convex oracles
  - Why needed here: The framework relies on convex optimization subroutines for action selection when dynamics are locally action-linear, but also requires access to a non-convex oracle for general locally controllable dynamics.
  - Quick check question: Under what conditions on the dynamics can the per-round optimization problem be solved efficiently without a non-convex oracle?

## Architecture Onboarding

- Component map:
  - State space Y (bounded convex subset of Euclidean space)
  - Action space X (bounded convex subset of Euclidean space)
  - Dynamics D(x,y) (continuous function mapping actions and states to next states)
  - Loss functions ft (convex and Lipschitz functions of the state)
  - OCO algorithm (e.g., FTRL) for optimizing over the state space
  - Oracle for non-convex optimization (when dynamics are not locally action-linear)
  - Disturbance handling module (for NestedOCO-BD and NestedOCO-UD)

- Critical path:
  1. Initialize OCO algorithm over the state space Y
  2. At each round t:
     a. Query OCO for target state yt
     b. Use oracle or efficient solver to find action xt that drives state to yt
     c. Play action xt and observe next state yt+1 and loss ft+1(yt+1)
     d. Update OCO with state yt+1 and loss ft+1(yt+1)
  3. For disturbance handling, track deviations from idealized trajectory and adjust parameters accordingly

- Design tradeoffs:
  - Oracle vs. efficient action selection: Using a non-convex oracle enables handling general locally controllable dynamics but is computationally expensive; restricting to locally action-linear dynamics allows efficient action selection but limits the class of problems.
  - Strong vs. weak local controllability: Strong local controllability enables handling unbounded disturbances but may be harder to satisfy; weak local controllability is more general but requires bounded disturbances.
  - Bandit vs. full feedback: Bandit feedback allows handling cases where losses depend on actions, but incurs higher regret (O(T^3/4) vs. O(√T)).

- Failure signatures:
  - Linear regret: May indicate that the dynamics are not locally controllable, or that disturbances exceed the allowed thresholds.
  - Suboptimal regret bounds: May indicate that the dynamics are not locally action-linear, or that the step size or other parameters are not well-calibrated.
  - Computational intractability: May indicate that the dynamics are too complex to handle efficiently, or that the oracle is too expensive to call repeatedly.

- First 3 experiments:
  1. Implement NestedOCO for a simple 1D problem with known locally controllable dynamics and no disturbances, and verify O(√T) regret.
  2. Extend to a 2D problem with locally action-linear dynamics, and verify efficient action selection without an oracle.
  3. Add bounded adversarial disturbances to the 1D problem, and verify the regret bounds for NestedOCO-BD with weak local controllability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum local controllability parameter ρ required for exact convergence to the best fixed state in online nonlinear control problems?
- Basis in paper: [explicit] Proposition 1 shows local controllability is "essentially necessary" but doesn't establish exact threshold; Theorem 1 requires ρ for O(√T) regret
- Why unresolved: Paper establishes sufficiency of local controllability but doesn't determine tight lower bounds on ρ for exact optimality
- What evidence would resolve it: Construct a specific family of dynamics showing convergence to best state only when ρ exceeds some threshold

### Open Question 2
- Question: How do different online convex optimization algorithms (beyond FTRL) affect regret bounds in nested control problems?
- Basis in paper: [explicit] NestedOCO uses FTRL but notes "any OCO algorithm whose per-round step size is guaranteed to be sufficiently small" could work
- Why unresolved: Paper only analyzes FTRL-based approach; potential for different algorithms to yield better bounds
- What evidence would resolve it: Implement and analyze regret bounds for multiple OCO algorithms (e.g., OGD, AdaGrad, Adam) in nested control setting

### Open Question 3
- Question: What is the precise trade-off between local controllability radius and the disturbance magnitude threshold in the weak local controllability case?
- Basis in paper: [explicit] Theorem 2 shows sharp threshold at ρ/(1+ρ) · π(D(xt,yt-1)) but doesn't characterize optimal ρ for given disturbance budget
- Why unresolved: Paper establishes existence of threshold but doesn't optimize ρ selection for specific disturbance patterns
- What evidence would resolve it: Derive closed-form expression for optimal ρ as function of disturbance budget and problem geometry

## Limitations
- The framework requires local controllability of dynamics, which may not hold for all practical problems and is essentially necessary for state-competitiveness
- Computational complexity remains high when using non-convex oracles for general locally controllable dynamics
- Theoretical regret bounds assume idealized conditions that may not translate perfectly to practice, particularly for complex nonlinear dynamics

## Confidence
- High confidence: The core reduction from regret minimization to online convex optimization over bounded state spaces, given locally controllable dynamics
- Medium confidence: The oracle-efficient algorithm for locally action-linear dynamics
- Low confidence: The specific regret bounds for bandit feedback and unknown dynamics

## Next Checks
1. Implement the non-convex oracle for a simple nonlinear dynamics example and verify that it can efficiently find feasible actions to reach target states, ensuring the NestedOCO framework is computationally tractable.
2. Test the NestedOCO-BD and NestedOCO-UD algorithms on a problem with adversarial disturbances, measuring regret and comparing against the theoretical bounds to identify when disturbances exceed the algorithm's correction capacity.
3. Apply the framework to a real-world problem instance (e.g., adaptive pricing or recommendations), validating that the dynamics are indeed locally controllable/controllable and that the regret guarantees translate from theory to practice.