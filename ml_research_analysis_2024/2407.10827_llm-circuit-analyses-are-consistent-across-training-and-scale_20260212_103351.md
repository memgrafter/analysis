---
ver: rpa2
title: LLM Circuit Analyses Are Consistent Across Training and Scale
arxiv_id: '2407.10827'
source_url: https://arxiv.org/abs/2407.10827
tags:
- heads
- circuit
- circuits
- training
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tracks how model mechanisms, operationalized as circuits,
  emerge and evolve across 300 billion tokens of training in decoder-only LLMs, in
  models ranging from 70 million to 2.8 billion parameters. The authors find that
  task abilities and the functional components that support them emerge consistently
  at similar token counts across scale.
---

# LLM Circuit Analyses Are Consistent Across Training and Scale

## Quick Facts
- arXiv ID: 2407.10827
- Source URL: https://arxiv.org/abs/2407.10827
- Reference count: 40
- Key outcome: Circuit analyses on small models during pre-training reveal stable task-solving algorithms that generalize across training and scale

## Executive Summary
This study tracks how model mechanisms, operationalized as circuits, emerge and evolve across 300 billion tokens of training in decoder-only LLMs ranging from 70 million to 2.8 billion parameters. The authors find that task abilities and the functional components that support them emerge consistently at similar token counts across scale. Moreover, although such components may be implemented by different attention heads over time, the overarching algorithm that they implement remains. Surprisingly, both these algorithms and the types of components involved therein can replicate across model scale. These results suggest that circuit analyses conducted on small models at the end of pre-training can provide insights that still apply after additional pre-training and over model scale.

## Method Summary
The study uses edge attribution patching with integrated gradients (EAP-IG) to find and analyze circuits across Pythia model checkpoints spanning 300 billion training tokens. Four tasks are examined: indirect object identification (IOI), gendered pronoun resolution, greater-than comparison, and subject-verb agreement. The authors track component emergence using functional tests (copy score, CSPA score, induction score, succession score) and verify algorithmic stability through path patching. Circuit components are analyzed at 154 checkpoints across models ranging from 70M to 2.8B parameters.

## Key Results
- Task learning rates approach asymptotes as model size increases, with quantization effects limiting learning speed differences beyond certain scales
- Circuit algorithms remain stable even when specific attention heads implementing them change during training, suggesting self-repair and load-balancing mechanisms
- The same circuit algorithms generalize across model scales, with IOI circuit metrics from 160M parameter models showing high correlation in larger variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Circuit algorithms remain stable even when the specific attention heads implementing them change during training.
- Mechanism: When a head loses its functional behavior (e.g., name-mover), another head takes over the same role, maintaining the overall algorithm. This "load balancing" ensures consistent task performance despite component-level fluctuations.
- Core assumption: The model can compensate for individual component changes through self-repair or redundancy mechanisms.
- Evidence anchors:
  - [section 4.1]: "models can clearly compensate for losses of and changes in individual circuit components."
  - [section 4.2]: "even when individual components change, the overall algorithm remains consistent"
- Break condition: If the model lacks sufficient redundancy or self-repair capability, algorithm stability could break down.

### Mechanism 2
- Claim: Circuit algorithms generalize across model scales, with similar algorithms emerging in models from 70M to 2.8B parameters.
- Mechanism: Despite differences in specific components, the core algorithmic steps (e.g., name-mover and copy-suppression in IOI) remain consistent across scales.
- Core assumption: The same fundamental computational steps are required to solve the task, regardless of model size.
- Evidence anchors:
  - [section 4.2]: "IOI circuit metrics from Pythia-160m are also high in larger Pythia variants"
  - [abstract]: "Surprisingly, both these algorithms and the types of components involved therein tend to replicate across model scale"
- Break condition: More complex tasks requiring different algorithms at different scales could invalidate this mechanism.

### Mechanism 3
- Claim: Task learning rates approach an asymptote as model size increases, due to the quantized nature of neural scaling.
- Mechanism: Models learn tasks in order of decreasing use frequency, and beyond a certain size, all models are exposed to the same distribution of data, limiting learning speed differences.
- Core assumption: The quantization model of neural scaling accurately describes how models acquire capabilities.
- Evidence anchors:
  - [section 3.1]: "task acquisition appeared to approach an asymptote"
  - [section 6]: "Our findings suggest evidence for the quantization model of neural scaling [49]"
- Break condition: If the quantization model is incorrect or if task frequency distribution changes, this mechanism could break.

## Foundational Learning

- Concept: Circuits as computational subgraphs
  - Why needed here: Understanding circuits is essential for grasping how models implement task-solving algorithms
  - Quick check question: What is the difference between a circuit and a general model component?

- Concept: Attention head functionality
  - Why needed here: Different attention head types (e.g., name-mover, copy-suppression) implement specific computational steps in circuits
  - Quick check question: How does a name-mover head differ from a copy-suppression head in its function?

- Concept: Path patching
  - Why needed here: Path patching is used to isolate and analyze the influence of individual components on task performance
  - Quick check question: What is the key difference between path patching and general activation patching?

## Architecture Onboarding

- Component map:
  Pythia model suite (70M-2.8B parameters) -> Four tasks (IOI, Gendered-Pronoun, Greater-Than, Subject-Verb Agreement) -> EAP-IG circuit finding -> Functional tests (copy, CSPA, induction, succession scores) -> Path patching verification

- Critical path:
  1. Find circuits at each checkpoint using EAP-IG
  2. Verify component importance via path patching
  3. Analyze component emergence and stability over training
  4. Test algorithm stability across components and scales

- Design tradeoffs:
  - Using EAP-IG for efficiency vs. potentially missing some components
  - 80% faithfulness threshold for circuit finding vs. completeness
  - Focusing on simpler tasks vs. generalizability to complex tasks

- Failure signatures:
  - Algorithm instability when components change
  - Different algorithms emerging at different scales
  - Task learning rates not approaching an asymptote

- First 3 experiments:
  1. Replicate IOI circuit analysis on a different model scale to test algorithm generalization
  2. Test a more complex task to see if the same stability mechanisms hold
  3. Analyze circuit formation in an encoder-only model to compare with decoder-only findings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do circuit algorithms generalize to more complex tasks beyond those studied in this paper?
- Basis in paper: [explicit] The authors state that their findings are based on a limited set of simple tasks and that more complex tasks, not solvable by small models, may show different trends.
- Why unresolved: The paper only examines four relatively simple tasks (IOI, Gendered-Pronoun, Greater-Than, and Subject-Verb Agreement) that are feasible for small models. The authors explicitly acknowledge that more complex tasks may exhibit different patterns.
- What evidence would resolve it: Empirical studies of circuit algorithms for more complex tasks, particularly those that require larger models and involve more sophisticated reasoning or knowledge, would be needed to determine if the observed stability and generalizability hold.

### Open Question 2
- Question: What are the exact mechanisms behind the self-repair and load-balancing behaviors observed in circuits?
- Basis in paper: [inferred] The authors mention that models can compensate for losses of and changes in individual circuit components, and that self-repair may contribute to this stability, but they leave the exact mechanisms to future work.
- Why unresolved: The paper observes that circuit components can change over time while overall algorithm remains stable, but does not provide a detailed explanation of how this self-repair occurs at the mechanistic level.
- What evidence would resolve it: Detailed analysis of attention patterns, gradient flows, and component interactions during training could reveal the specific mechanisms by which models compensate for component changes while maintaining task performance.

### Open Question 3
- Question: Do the observed trends in circuit stability and generalizability apply to other model architectures beyond the Pythia family?
- Basis in paper: [explicit] The authors state that their analysis studied models only from one model family (Pythia) and that it is not possible to tell if results are limited to this specific family.
- Why unresolved: The paper's findings are based on a single model family that shares both architecture and training setup across model scale, making it unclear if the observed patterns are universal or specific to this particular architecture.
- What evidence would resolve it: Studies of circuit formation and stability in other transformer architectures (e.g., GPT, LLaMA, or different decoder-only models) would determine whether the observed trends are architecture-independent or specific to the Pythia family.

## Limitations
- Findings limited to simple tasks (IOI, Gendered-Pronoun, Greater-Than, Subject-Verb Agreement) that may not generalize to complex reasoning tasks
- Analysis based on a single training corpus (300 billion tokens of web data) with unknown generalizability to different data distributions
- EAP-IG method may miss some components compared to exhaustive search methods, potentially underestimating circuit complexity

## Confidence

**High confidence**: Claims about algorithm stability within the same model scale during training (Mechanism 1) are well-supported by path patching evidence showing consistent task performance despite component changes. The findings about task learning approaching asymptotes with scale (Mechanism 3) are also strongly supported by the quantitative data across 154 checkpoints.

**Medium confidence**: Claims about circuit algorithm generalization across model scales (Mechanism 2) are supported but limited to the specific tasks studied. The finding that IOI circuit metrics transfer from 160M to larger Pythia models is compelling, but the evidence base for other tasks and more complex scenarios is narrower.

**Low confidence**: The broader claim that circuit analyses on small models provide insights applicable after additional pre-training (paper abstract claim) extrapolates beyond the studied timeframe. While results show stability over the 300 billion tokens examined, the paper doesn't test whether this stability holds across multiple orders of magnitude of additional training.

## Next Checks
1. **Cross-task generalization test**: Apply the circuit stability analysis to a more complex reasoning task (e.g., multi-step logical inference) to determine if the same algorithmic consistency holds beyond simple pattern-matching tasks.

2. **Scale boundary test**: Train models at intermediate scales (e.g., 500M, 1.5B parameters) and examine whether circuit algorithms remain consistent across these finer-grained scale steps, particularly focusing on the transition points between model families.

3. **Training duration stress test**: Extend analysis beyond 300 billion tokens to examine whether the observed algorithmic stability persists through multiple orders of magnitude of additional training, or whether new circuit architectures eventually emerge.