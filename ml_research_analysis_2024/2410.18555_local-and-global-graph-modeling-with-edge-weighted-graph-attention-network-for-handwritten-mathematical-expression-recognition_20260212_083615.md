---
ver: rpa2
title: Local and Global Graph Modeling with Edge-weighted Graph Attention Network
  for Handwritten Mathematical Expression Recognition
arxiv_id: '2410.18555'
source_url: https://arxiv.org/abs/2410.18555
tags:
- graph
- node
- edge
- recognition
- expression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses online handwritten mathematical expression
  recognition (HMER) by proposing a novel stroke-level graph modeling approach combined
  with an end-to-end Edge-weighted Graph Attention Network (EGAT). The key innovation
  is transforming HMER into a node and edge classification problem on stroke-level
  graphs, capturing both local and global information.
---

# Local and Global Graph Modeling with Edge-weighted Graph Attention Network for Handwritten Mathematical Expression Recognition

## Quick Facts
- arXiv ID: 2410.18555
- Source URL: https://arxiv.org/abs/2410.18555
- Authors: Yejing Xie; Richard Zanibbi; Harold Mouchère
- Reference count: 40
- Primary result: State-of-the-art performance on CROHME 2019 and 2023 with expression accuracy of 60.72% and 55.30% respectively

## Executive Summary
This paper addresses online handwritten mathematical expression recognition (HMER) by proposing a novel stroke-level graph modeling approach combined with an end-to-end Edge-weighted Graph Attention Network (EGAT). The key innovation is transforming HMER into a node and edge classification problem on stroke-level graphs, capturing both local and global information. The EGAT model performs simultaneous node and edge classification using attention mechanisms and message passing, while local graph modeling (LGM) captures stroke-level relationships and global graph modeling (GGM) adds a master node for full expression context. The proposed LGM-EGAT and GGM-EGAT systems achieved state-of-the-art performance on CROHME 2019 (60.72% and 60.72% expression accuracy) and CROHME 2023 (55.30% and 55.30% expression accuracy), outperforming traditional encoder-decoder and tree-based methods.

## Method Summary
The proposed method transforms online handwritten mathematical expressions into stroke-level graphs where each stroke is a node and spatial/temporal relationships are edges. Node features are extracted using XceptionTime from stroke coordinates, while edge features use Fuzzy Relative Positioning Templates (FRPT) for directional spatial relations. The Edge-weighted Graph Attention Network (EGAT) performs simultaneous node and edge classification through attention-weighted message passing with a novel message concatenation strategy. Local Graph Modeling (LGM) uses Line-of-Sight (LOS) connectivity with temporal neighbors, while Global Graph Modeling (GGM) adds a master node connected to all local nodes for full-expression context. The model is trained end-to-end with multi-objective loss combining cross-entropy for node classification and focal loss for edge classification.

## Key Results
- LGM-EGAT achieved 60.72% expression accuracy on CROHME 2019 and 55.30% on CROHME 2023
- GGM-EGAT achieved 60.72% expression accuracy on CROHME 2019 and 55.30% on CROHME 2023
- Both models outperformed state-of-the-art methods including CNN-Transformer, Tree-based models, and traditional encoder-decoder approaches
- Symbol and relation classification accuracies reached 86.86% and 81.30% on CROHME 2023 respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Edge-weighted Graph Attention Mechanism (EGAT) enables simultaneous node and edge classification by integrating both node and edge features in a single forward pass.
- Mechanism: EGAT updates node features by aggregating weighted messages from neighboring nodes, and updates edge features using the same attention weights. It then concatenates node features with aggregated edge features and vice versa, creating a mutual enhancement loop where nodes and edges inform each other.
- Core assumption: Edge features can be meaningfully embedded and updated through attention-weighted message passing, and their interaction with node features improves classification performance.
- Evidence anchors:
  - [abstract] "designed to perform simultaneous node and edge classification. This model effectively integrates node and edge features..."
  - [section 3.1.2] "we proposed a further on message concatenate strategy based on above message passing... The q-th node features hq_i will be calculated by the concatenation of node features hq_i with the sum of all the connected edge features bq_ij..."
  - [corpus] Weak evidence; no direct corpus match for this specific message-passing and concatenation strategy.
- Break condition: If edge features cannot be meaningfully represented or if attention weights do not correlate with true importance of edges, the mutual enhancement will degrade.

### Mechanism 2
- Claim: Local Graph Modeling (LGM) captures fine-grained stroke-level relationships using Line-of-Sight (LOS) connectivity and temporal neighbors, enabling precise symbol segmentation and relation classification.
- Mechanism: Each stroke is a node; edges are constructed using LOS visibility plus temporal adjacency (LOS+t). Node features are stroke coordinates normalized for device and writing speed; edge features encode directional spatial relations via Fuzzy Relative Positioning Templates (FRPT).
- Core assumption: Stroke-level representation preserves enough spatial and temporal information to reconstruct the mathematical expression structure without explicit grammar rules.
- Evidence anchors:
  - [section 3.2.1] "We applied the Line-of-Sight (LOS) method to construct the connected edges between nodes... Each stroke vL_i ∈ VL is represented by a node..."
  - [section 3.2.1] "We applied a Fuzzy Relative Positioning Template (FRPT)... The edge features bij ∈ R5d2 of edge eL_ij ∈ EL are represented by the concatenation of the 4 directions and the distance..."
  - [corpus] No direct match; corpus focuses on other GNN applications.
- Break condition: If strokes are poorly segmented or if LOS fails to capture correct spatial dependencies, symbol and relation classification will fail.

### Mechanism 3
- Claim: Global Graph Modeling (GGM) with a master node aggregates full-expression context, improving recognition accuracy especially for longer or more complex expressions.
- Mechanism: A virtual master node is added and connected to all local nodes. Its initial feature is the sum of all local node features; connected edges are initialized to zero. Through message passing, the master node aggregates global information and propagates it back to local nodes.
- Core assumption: A single master node can effectively capture and distribute global structural information without overwhelming the network or causing gradient issues.
- Evidence anchors:
  - [section 3.2.2] "we introduced a master node vM in global graph modeling, which is connected to all nodes in the original local graph modeling... The nodes set of the global graph VG = VL ∪ vM..."
  - [section 3.2.2] "The virtual master node automatically aggregates global information from all local nodes..."
  - [section 4.3.2] "The GGM-EGAT model with global information has better performance with the increase of expression complexity..."
- Break condition: If the master node becomes a bottleneck or if its aggregation oversimplifies diverse local contexts, performance may degrade on complex expressions.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: The paper transforms HMER into a graph classification problem; understanding how GNNs propagate information through edges is essential to grasp EGAT.
  - Quick check question: In a GNN, how are node features updated during message passing?

- Concept: Attention mechanisms in graph contexts
  - Why needed here: EGAT uses attention weights to weigh neighbor contributions; understanding scaled dot-product attention helps explain why certain edges are emphasized.
  - Quick check question: What is the purpose of applying softmax to attention scores in a GAT?

- Concept: Edge feature representation in graphs
  - Why needed here: Unlike standard GNNs, EGAT explicitly models edges with learnable features; knowing how to embed and update edge features is key to understanding the simultaneous classification claim.
  - Quick check question: How can edge features be initialized and updated differently from node features in a graph network?

## Architecture Onboarding

- Component map: Stroke coordinates → XceptionTime embedding → Node/Edge embeddings → 5 EGAT layers → Readouts → Classification
- Critical path: Stroke → Node/Edge Embeddings → EGAT layers → Readouts → Classification
- Design tradeoffs:
  - LOS+t vs. full connectivity: LOS+t reduces parameters and focuses on likely relations but may miss rare spatial configurations; full connectivity increases model capacity but risks overfitting.
  - Master node vs. deep GNN: Master node provides direct global context but may oversimplify; deeper GNN captures more nuanced propagation but is harder to train.
- Failure signatures:
  - Node classification drops while edge classification holds: Likely issue in node embedding or node-level message passing.
  - Both node and edge classification drop: Possible problem in attention weight computation or feature concatenation.
  - Expression accuracy drops but symbol accuracy holds: Likely failure in relation classification or master node integration.
- First 3 experiments:
  1. Train baseline EGAT without message concatenation on a small subset; verify node/edge classification performance.
  2. Add message concatenation; compare classification accuracy and check if edge accuracy improves.
  3. Swap LOS+t with full connectivity; observe changes in accuracy and training stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of a master node in global graph modeling affect recognition accuracy for expressions of varying complexity compared to local graph modeling alone?
- Basis in paper: [explicit] The paper states that GGM-EGAT achieves better performance with increased expression complexity compared to LGM-EGAT.
- Why unresolved: The paper provides comparative results but does not conduct a detailed analysis of how master node effectiveness varies with expression length or structural complexity.
- What evidence would resolve it: Detailed ablation studies showing recognition accuracy differences between LGM-EGAT and GGM-EGAT across expressions grouped by stroke count, symbol count, and structural complexity metrics.

### Open Question 2
- Question: What is the impact of different attention weight computation strategies on the performance of the Edge-weighted Graph Attention Network?
- Basis in paper: [explicit] The paper describes a specific attention mechanism that includes edge features in attention weight computation and message passing, but does not compare it to alternative attention mechanisms.
- Why unresolved: While the proposed attention mechanism is described, the paper does not evaluate its effectiveness against other attention mechanisms like standard GAT or attention mechanisms that exclude edge features.
- What evidence would resolve it: Comparative experiments evaluating EGAT performance against baseline GAT and other attention mechanisms on the same datasets, with metrics for node classification, edge classification, and expression recognition accuracy.

### Open Question 3
- Question: How does the "Stroke to Sequence" strategy mentioned for future work compare to the current graph-to-graph approach in terms of accuracy and computational efficiency?
- Basis in paper: [inferred] The conclusion mentions exploring a "Stroke to Sequence" strategy for handling larger and more diverse datasets, suggesting it as a potential alternative to the current approach.
- Why unresolved: The paper does not implement or evaluate this strategy, leaving its potential benefits and drawbacks unexplored.
- What evidence would resolve it: Implementation and evaluation of both the current graph-to-graph approach and the proposed "Stroke to Sequence" strategy on benchmark datasets, comparing accuracy, computational efficiency, and scalability.

## Limitations

- The paper does not provide ablation studies comparing LOS+t connectivity versus alternative graph construction methods, making it difficult to assess whether the connectivity scheme is optimal
- While the model achieves state-of-the-art results on CROHME datasets, the improvements over previous methods are modest (less than 2% absolute improvement in expression accuracy)
- The master node approach for global context is novel but lacks comparison with alternative global modeling strategies like graph pooling or deeper message passing

## Confidence

- **High confidence**: The EGAT architecture can perform simultaneous node and edge classification through attention-weighted message passing and feature concatenation
- **Medium confidence**: The LOS+t connectivity scheme effectively captures stroke-level relationships for HMER, though alternatives are not explored
- **Medium confidence**: The master node in GGM provides useful global context, but its specific contribution relative to other global modeling approaches is unclear

## Next Checks

1. Conduct ablation study removing the message concatenation strategy to quantify its contribution to performance
2. Test alternative graph construction methods (e.g., k-nearest neighbors, learned connectivity) against the LOS+t approach
3. Compare the master node global modeling against graph pooling or deeper GNN layers on complex expressions to isolate the source of performance gains