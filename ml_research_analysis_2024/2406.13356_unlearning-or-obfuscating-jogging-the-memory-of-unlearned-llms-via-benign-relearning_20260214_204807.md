---
ver: rpa2
title: Unlearning or Obfuscating? Jogging the Memory of Unlearned LLMs via Benign
  Relearning
arxiv_id: '2406.13356'
source_url: https://arxiv.org/abs/2406.13356
tags:
- unlearning
- relearning
- unlearned
- what
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper shows that current unlearning methods for large language\
  \ models (LLMs) are vulnerable to targeted relearning attacks. By finetuning an\
  \ unlearned model on a small amount of data related to the forgotten content\u2014\
  such as public medical articles or a subset of the unlearn set\u2014the model can\
  \ recover knowledge it was supposed to have forgotten."
---

# Unlearning or Obfuscating? Jogging the Memory of Unlearned LLMs via Benign Relearning

## Quick Facts
- **arXiv ID**: 2406.13356
- **Source URL**: https://arxiv.org/abs/2406.13356
- **Reference count**: 40
- **Primary result**: Current unlearning methods for LLMs are vulnerable to targeted relearning attacks that recover forgotten knowledge with high accuracy.

## Executive Summary
This paper demonstrates that current machine unlearning methods for large language models (LLMs) are ineffective against targeted relearning attacks. The authors show that by finetuning an unlearned model on small amounts of public data related to the forgotten content, it's possible to recover knowledge that was supposed to have been erased. The attacks work by reactivating suppressed representations through overlapping feature pathways, successfully restoring hazardous knowledge and private information across multiple benchmarks.

## Method Summary
The study uses gradient-based unlearning methods followed by LoRA finetuning on benign public data related to the forgotten content. For WMDP, GPT-4 generates general knowledge paragraphs about query keywords without including direct answers. For TOFU and WHP, small subsets of the unlearn data are used for relearning. The relearned models are then evaluated using LLM-as-Judge scores (WMDP) or keyword search (TOFU/WHP) to measure attack success rates.

## Key Results
- Targeted relearning attacks successfully restored unlearned hazardous knowledge about bioweapons with high accuracy
- Attacks recovered private information about fictional characters even when relearn sets didn't contain direct answers
- Relearning performance often matched or approached pre-unlearning levels across all three benchmarks (WMDP, TOFU, WHP)
- Optimal relearning step count is non-monotonic and depends on unlearning depth

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finetuning on public data that shares domain features with unlearned content can reactivate dormant representations.
- Mechanism: Unlearning via gradient ascent only masks outputs without fully erasing internal representations. Finetuning on similar public content activates overlapping feature pathways, allowing partial reconstruction of suppressed knowledge.
- Core assumption: Unlearning via gradient ascent only masks outputs without fully erasing internal representations of the knowledge.
- Evidence anchors: [abstract] "... relearning on public medical articles can lead an unlearned LLM to output harmful knowledge about bioweapons"; [section 4.1] "... it is possible for LLMs to 'relearn' toxic knowledge if augmented with such benign public articles"
- Break condition: If unlearning removes both the internal representation and all correlated pathways, relearning would fail.

### Mechanism 2
- Claim: Relearning attacks can recover keywords by exploiting correlations between public and private data.
- Mechanism: Even without direct answers, the model uses semantically related context from public data to bridge gaps during autoregression, reconstructing missing pieces through attention mechanisms.
- Core assumption: The model's attention mechanisms and context embedding can bridge the gap between related and target knowledge.
- Evidence anchors: [section 4.2] "it is possible for LLMs to 'relearn' toxic knowledge if augmented with such benign public articles"; [section 4.2] "able to output the first and last name of the two characters with the correct ordering"
- Break condition: If unlearn completely removes all correlated embeddings or if the relearn set lacks sufficient semantic overlap.

### Mechanism 3
- Claim: Optimal relearning step count depends on unlearning depth and may have a non-monotonic relationship with attack success.
- Mechanism: Shallow unlearning leaves strong residual representations that reactivate quickly, while deep unlearning weakens these but risks overfitting if too many steps are taken.
- Core assumption: Unlearning is not uniform across the model; some representations degrade faster than others.
- Evidence anchors: [section 4.2] "if the relearning happens for more than 40 steps, ASR drops for all unlearning checkpoints"; [section 4.2] "when we apply too many relearn steps, wâ€² starts to overfit and generate text similar to strings in the prompt"
- Break condition: If unlearning removes all residual pathways or if overfitting is detected early.

## Foundational Learning

- **Concept**: Gradient-based unlearning methods (e.g., gradient ascent) approximate data removal by suppressing model outputs.
  - Why needed here: Understanding the limitations of gradient-based unlearning is essential to explain why relearning attacks succeed.
  - Quick check question: Does gradient ascent unlearning remove the internal representation of knowledge or just mask its outputs?

- **Concept**: LoRA (Low-Rank Adaptation) finetuning modifies a small subset of model weights to adapt to new tasks.
  - Why needed here: Relearning attacks use LoRA to efficiently retrain unlearned models on benign data.
  - Quick check question: How does LoRA differ from full-model finetuning in terms of parameter updates and computational cost?

- **Concept**: Attention mechanisms in transformers allow the model to relate different parts of the input contextually.
  - Why needed here: Attention is key to how relearning can bridge related public content with suppressed private knowledge.
  - Quick check question: How might attention weights change during relearning compared to pre-unlearning?

## Architecture Onboarding

- **Component map**: Unlearned model (wU) -> Relearn set (D') -> LoRA finetuning -> Evaluation pipeline -> Attack success metric
- **Critical path**: 1. Obtain wU and construct D'; 2. Apply LoRA finetuning to wU using D'; 3. Evaluate the relearned model (w') on target queries; 4. Measure ASR or LLM score to confirm success
- **Design tradeoffs**: Relearn set size vs. risk of overfitting; number of finetuning steps vs. ASR and stability; public data relevance vs. availability
- **Failure signatures**: ASR drops after many finetuning steps (overfitting); LLM scores remain low despite relearning (insufficient semantic overlap); Model outputs irrelevant or nonsensical text
- **First 3 experiments**: 1. Test relearning on a shallowly unlearned model with a small, relevant D'; 2. Increase unlearning depth and observe ASR changes with fixed D'; 3. Vary the size and diversity of D' and measure impact on ASR

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the effectiveness of targeted relearning attacks vary with different unlearning methods beyond gradient-based optimization, such as differentially private aggregation or knowledge adaptation methods?
- **Basis in paper**: [inferred] The paper mentions evaluating the attack on gradient-based unlearning methods and suggests future work to explore other unlearning approaches.
- **Why unresolved**: The paper only evaluates the attack on gradient-based unlearning methods, leaving the effectiveness of the attack on other unlearning methods unexplored.
- **What evidence would resolve it**: Conducting experiments using targeted relearning attacks on models unlearned with various methods (e.g., differentially private aggregation, knowledge adaptation) and comparing the attack success rates.

### Open Question 2
- **Question**: What is the impact of the size and diversity of the relearn set on the success of targeted relearning attacks?
- **Basis in paper**: [inferred] The paper discusses using different types of relearn sets but does not systematically study the impact of their size and diversity on attack success.
- **Why unresolved**: The paper does not provide a comprehensive analysis of how varying the size and diversity of the relearn set affects the effectiveness of the attack.
- **What evidence would resolve it**: Conducting experiments with relearn sets of varying sizes and diversities and measuring the attack success rates.

### Open Question 3
- **Question**: How do more complex defenses, such as jailbreaking and guardrails, affect the susceptibility of unlearned models to targeted relearning attacks?
- **Basis in paper**: [explicit] The paper mentions that future work includes evaluating the attack with the presence of more complex approaches such as jailbreaking and guardrails.
- **Why unresolved**: The paper does not investigate the interaction between targeted relearning attacks and defenses like jailbreaking and guardrails.
- **What evidence would resolve it**: Implementing and testing targeted relearning attacks on models protected by jailbreaking and guardrail defenses.

## Limitations
- Exact implementation details of gradient ascent unlearning (learning rates, step counts) are not fully specified, which could affect reproducibility
- Specific prompts and filtering criteria used with GPT-4 for generating relearn sets are not detailed, potentially impacting relearning quality
- The paper focuses on successful attacks but doesn't extensively explore defensive mechanisms or the full parameter space of unlearning configurations

## Confidence
- **High confidence**: Core claim that current unlearning methods are vulnerable to targeted relearning attacks, supported by consistent results across three benchmarks
- **Medium confidence**: Proposed mechanisms explaining why relearning works, as evidence is largely empirical rather than theoretical
- **Low confidence**: Generalizability of results to all unlearning methods, as the study focuses on gradient ascent and similar approaches without testing other paradigms

## Next Checks
1. Test whether attack success rate changes significantly when using different unlearning methods (e.g., weight-space regularization vs. gradient ascent) to isolate method-specific vulnerabilities
2. Evaluate whether increasing the semantic diversity of the relearn set improves attack success, or if there's a point of diminishing returns
3. Investigate whether defensive fine-tuning techniques (e.g., knowledge distillation, adversarial training) can reduce vulnerability to these relearning attacks without compromising original task performance