---
ver: rpa2
title: 'ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large
  Multimodal Models'
arxiv_id: '2401.13311'
source_url: https://arxiv.org/abs/2401.13311
tags:
- response
- visual
- reasoning
- image
- text-rich
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CONTEXTUAL introduces a novel dataset for evaluating context-sensitive
  text-rich visual reasoning in large multimodal models. The dataset contains 506
  human-crafted instructions requiring joint reasoning over text and visual elements
  across eight real-world visual scenarios.
---

# ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models

## Quick Facts
- arXiv ID: 2401.13311
- Source URL: https://arxiv.org/abs/2401.13311
- Reference count: 40
- Primary result: 14 LMMs achieve 49.3% acceptance rating vs 80.1% human baseline on context-sensitive text-rich visual reasoning tasks

## Executive Summary
ConTextual introduces a novel dataset for evaluating context-sensitive text-rich visual reasoning in large multimodal models. The dataset contains 506 human-crafted instructions requiring joint reasoning over text and visual elements across eight real-world visual scenarios. Experiments with 14 foundation models reveal a significant performance gap of 30.8% between GPT-4V (the best-performing LMM with 49.3% acceptance rating) and human performance (80.1%). Fine-grained analysis shows GPT-4V struggles with time-related data and infographics but excels at abstract contexts like memes and quotes. Open models like LLaVA-Next-34B significantly underperform proprietary models, highlighting the need for enhanced visual encoders and more diverse training data.

## Method Summary
The study evaluates 14 foundation models on a 506-sample CONTEXTUAL dataset through zero-shot inference, comparing performance against a human baseline using both automated metrics and human evaluation on 280 samples. Models include GPT-4V, Gemini-Pro-Vision, LLaVA-Next, LLaVA-1.5, ShareGPT-4V-7B, and others. Human evaluation involves independent annotators rating predicted responses, while automatic evaluation uses GPT-4-based prompts comparing responses against reference answers across the full dataset.

## Key Results
- GPT-4V achieves 49.3% acceptance rating, significantly outperforming other LMMs but falling 30.8% short of human baseline (80.1%)
- Open models like LLaVA-Next-34B significantly underperform proprietary models, highlighting architectural and data limitations
- GPT-4V struggles specifically with time-related data and infographics despite excelling at abstract contexts like memes and quotes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context-sensitive text-rich visual reasoning requires joint perception of both text and visual context, not just accurate OCR.
- Mechanism: The dataset is designed so that simply extracting and reading text from the image is insufficient; accurate visual perception and contextual understanding of how text interacts with visual elements is necessary to answer instructions correctly.
- Core assumption: OCR accuracy alone does not guarantee correct answers in text-rich visual reasoning tasks.
- Evidence anchors:
  - [abstract]: "These tasks require an understanding of the context in which the text interacts with visual elements within an image."
  - [section]: "we show that a high accuracy OCR of the images... has sufficient signal to answer the question... Though accessing the OCR capability is important, these examples do not test the unique potential of the LMMs to jointly reason over the embedded text and visual context in the image."
- Break condition: If OCR + LLM could solve the instruction without visual context, the mechanism fails.

### Mechanism 2
- Claim: Fine-grained visual perception is a major bottleneck for current LMMs on text-rich visual tasks.
- Mechanism: Even when models understand the instruction and can perform reasoning, errors arise from inability to precisely perceive small or subtle visual details, leading to incorrect or hallucinated responses.
- Core assumption: Models have sufficient reasoning capability but lack precise visual perception.
- Evidence anchors:
  - [section]: "Our qualitative analysis uncovers various factors contributing to poor performance including lack of precise visual perception and hallucinations."
  - [section]: Examples like "Specify the colors of the rocket, space shuttle, and external tank" where GPT-4V makes errors despite logical reasoning.
- Break condition: If improving visual encoders alone does not improve performance, the mechanism fails.

### Mechanism 3
- Claim: Human evaluation remains the most reliable method for assessing correctness in this task domain.
- Mechanism: Because responses require subjective judgment of correctness grounded in both image and instruction, human raters can better assess nuanced correctness than automated metrics.
- Core assumption: Automated metrics cannot fully capture correctness in context-sensitive reasoning.
- Evidence anchors:
  - [section]: "Through human evaluations... we find that GPT-4V... achieves 49.3% acceptance rating... However, this rating is quite far from the human baseline of 80.1%."
  - [section]: "we perform a correlation analysis between human and automated methods... GPT-4 based evaluation achieves the highest correlation with human judgments."
- Break condition: If automated metrics become sufficiently aligned with human judgment, the mechanism breaks.

## Foundational Learning

- Concept: Text-rich visual reasoning
  - Why needed here: This task requires understanding how text embedded in images interacts with visual elements, not just reading text or recognizing objects separately.
  - Quick check question: Can you identify an instruction that requires joint reasoning over both text and visual context, rather than just text extraction or visual recognition alone?

- Concept: Fine-grained visual perception
  - Why needed here: Many errors stem from inability to perceive small or subtle details accurately, which is critical for correct responses.
  - Quick check question: In an image with multiple small text labels and visual elements, can you accurately identify which label corresponds to which visual element?

- Concept: Context-sensitive instruction design
  - Why needed here: Instructions must be crafted so they cannot be solved by OCR alone, ensuring models must reason over both text and visual context.
  - Quick check question: Given an image with text and visual elements, can you formulate an instruction that requires reasoning over both, not just text reading?

## Architecture Onboarding

- Component map: Image sourcing → Instruction design → Reference response creation → Model inference → Human evaluation → Automated evaluation → Fine-grained analysis
- Critical path: Image selection → Instruction design (context-sensitive) → Reference response creation → Model inference → Human evaluation → Performance analysis
- Design tradeoffs: Human evaluation provides gold standard but is expensive and slow; automated metrics are scalable but less reliable for nuanced correctness.
- Failure signatures: Models perform well on OCR-only tasks but poorly on context-sensitive tasks; automated metrics show high scores but human evaluation reveals low acceptance.
- First 3 experiments:
  1. Run all models on a small subset of the dataset and compare acceptance rates using human evaluation to establish baseline performance gap.
  2. Evaluate the impact of adding layout-aware OCR and image captions to LLM performance to confirm the necessity of visual perception.
  3. Conduct fine-grained analysis by category to identify specific visual contexts where models struggle most.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to large multimodal models would most effectively improve performance on context-sensitive text-rich visual reasoning tasks?
- Basis in paper: [inferred] The paper shows significant performance gaps between models on tasks requiring joint reasoning over text and visual elements, with models struggling particularly with time-related data, infographics, and fine-grained visual perception.
- Why unresolved: The paper identifies performance gaps but doesn't explore or propose specific architectural solutions to address these weaknesses.
- What evidence would resolve it: Systematic experiments comparing different model architectures (variations in visual encoders, attention mechanisms, training objectives) on CONTEXTUAL dataset to identify which modifications yield the greatest improvements.

### Open Question 2
- Question: How does model performance on context-sensitive text-rich visual reasoning tasks vary across different demographic groups and cultural contexts?
- Basis in paper: [inferred] The paper notes that human evaluation was primarily conducted by annotators from the US, and acknowledges that linguistic diversity and writing style might vary across social groups.
- Why unresolved: The study used a limited demographic sample for human evaluation, and the dataset itself may contain cultural biases in its image selection and instructions.
- What evidence would resolve it: Replicating the study with diverse annotator populations from different cultural backgrounds, and conducting cross-cultural studies on model performance across different variants of the dataset.

### Open Question 3
- Question: What is the relationship between the scale and diversity of pretraining data and model performance on context-sensitive text-rich visual reasoning tasks?
- Basis in paper: [explicit] The paper observes that proprietary models outperform open models, and that open models specifically designed for text-rich visual reasoning still underperform, suggesting issues with training data diversity and scale.
- Why unresolved: While the paper notes performance differences, it doesn't systematically investigate how different aspects of pretraining data (scale, diversity, domain coverage) affect reasoning capabilities.
- What evidence would resolve it: Controlled experiments varying pretraining data characteristics (domain coverage, diversity, scale) while keeping model architecture constant, measuring performance on CONTEXTUAL across these variants.

## Limitations
- The dataset size of 506 samples may not be sufficient to generalize findings across all real-world text-rich visual reasoning scenarios.
- Human evaluation methodology remains underspecified without detailed annotation guidelines and inter-annotator agreement metrics.
- The 30.8% performance gap between models and humans cannot definitively establish whether joint reasoning or other factors are the primary bottleneck.

## Confidence
- **High confidence**: The finding that proprietary models significantly outperform open models (30.8% gap) is well-supported by the experimental results across multiple model families.
- **Medium confidence**: The claim about GPT-4V's specific struggles with time-related data and infographics is supported by qualitative analysis but would benefit from more systematic error categorization.
- **Low confidence**: The assertion that joint reasoning over text and visual context is the primary bottleneck cannot be definitively established without ablation studies comparing OCR-only approaches to full LMM approaches.

## Next Checks
1. Conduct inter-annotator agreement analysis on a subset of the human evaluation data to establish reliability of the 80.1% human baseline and identify potential evaluation inconsistencies.
2. Perform controlled experiments comparing LMM performance with and without layout-aware OCR preprocessing to isolate the contribution of visual perception versus text understanding.
3. Expand the dataset with adversarial examples designed to specifically test whether models can distinguish between instructions requiring joint reasoning versus those solvable through text extraction alone.