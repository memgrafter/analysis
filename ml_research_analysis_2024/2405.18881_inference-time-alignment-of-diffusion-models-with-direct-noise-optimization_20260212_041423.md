---
ver: rpa2
title: Inference-Time Alignment of Diffusion Models with Direct Noise Optimization
arxiv_id: '2405.18881'
source_url: https://arxiv.org/abs/2405.18881
tags:
- reward
- diffusion
- optimization
- noise
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies inference-time alignment of diffusion models
  using a method called Direct Noise Optimization (DNO). DNO optimizes the noise injected
  during the sampling process of diffusion models to maximize a target reward function,
  without fine-tuning the model parameters.
---

# Inference-Time Alignment of Diffusion Models with Direct Noise Optimization

## Quick Facts
- arXiv ID: 2405.18881
- Source URL: https://arxiv.org/abs/2405.18881
- Reference count: 39
- Key outcome: This paper studies inference-time alignment of diffusion models using Direct Noise Optimization (DNO) to maximize target reward functions without fine-tuning model parameters.

## Executive Summary
This paper introduces Direct Noise Optimization (DNO), a method for inference-time alignment of diffusion models that optimizes the injected noise during sampling to maximize target reward functions without requiring model fine-tuning. The authors address the critical challenge of out-of-distribution reward hacking through a novel probability regularization technique. Extensive experiments demonstrate that DNO achieves state-of-the-art performance across various reward functions while maintaining the quality of generated samples, outperforming existing alignment methods within reasonable time budgets.

## Method Summary
The paper proposes optimizing the noise injected during the sampling process of diffusion models rather than fine-tuning the model parameters. DNO uses gradient ascent to optimize the noise at each timestep, maximizing a target reward function. To prevent out-of-distribution (OOD) reward hacking, the authors introduce a probability regularization technique that constrains the optimization to stay within the model's learned distribution. For non-differentiable reward functions, the paper extends DNO with efficient hybrid gradient estimation methods. The approach is evaluated on Stable Diffusion v1.5 using various reward functions including brightness, darkness, aesthetic scores, and JPEG compressibility.

## Key Results
- DNO achieves state-of-the-art performance across various reward functions without fine-tuning model parameters
- The probability regularization technique effectively prevents out-of-distribution reward hacking
- DNO outperforms existing alignment methods within reasonable computational budgets
- The method successfully handles both differentiable and non-differentiable reward functions

## Why This Works (Mechanism)
DNO works by optimizing the injected noise during the sampling process rather than modifying the model itself. This approach leverages the fact that diffusion models generate samples through a stochastic process where noise is gradually removed. By optimizing the noise schedule, DNO can steer the generation process toward desired properties while maintaining the underlying model's capabilities. The probability regularization ensures that the optimized noise stays within the distribution the model was trained on, preventing reward hacking through unrealistic samples.

## Foundational Learning
- **Diffusion Model Sampling**: Understanding how diffusion models generate samples through iterative denoising is crucial for implementing DNO.
  - *Why needed*: DNO directly optimizes the noise injection process during sampling.
  - *Quick check*: Verify understanding of DDIM and SDE sampling methods for diffusion models.

- **Gradient-Based Optimization**: Knowledge of gradient ascent techniques is essential for implementing the noise optimization.
  - *Why needed*: DNO uses gradient ascent to maximize the reward function with respect to the injected noise.
  - *Quick check*: Ensure familiarity with implementing gradient-based optimization in PyTorch or similar frameworks.

- **Probability Regularization**: Understanding concentration inequalities and their application to probability distributions is key for the OOD prevention technique.
  - *Why needed*: The probability regularization prevents the model from generating out-of-distribution samples to maximize rewards.
  - *Quick check*: Review Hoeffding's inequality and its application to concentration of measure.

## Architecture Onboarding
- **Component Map**: Stable Diffusion -> DDIM Sampler with Î·=1 -> DNO Noise Optimization -> Probability Regularization -> Reward Function
- **Critical Path**: The optimization loop involves sampling with optimized noise, computing rewards, backpropagating through the sampling process, and updating the noise schedule.
- **Design Tradeoffs**: Optimizing noise rather than model parameters preserves the base model's capabilities but requires more computation during inference. The probability regularization adds computational overhead but prevents reward hacking.
- **Failure Signatures**: Memory explosion during gradient computation, poor reward maximization, and out-of-distribution samples indicate implementation issues.
- **First Experiments**:
  1. Implement basic DNO with a differentiable reward function (e.g., brightness) and verify gradient flow.
  2. Test the probability regularization by comparing CLIP/AITOM scores with and without regularization.
  3. Implement hybrid gradient estimation for a simple non-differentiable reward function.

## Open Questions the Paper Calls Out
- **Open Question 1**: How does the performance of DNO vary with different choices of concentration inequality parameters k and m, and what is the optimal way to select these parameters? The paper provides limited empirical guidance on selecting these parameters optimally.
- **Open Question 2**: Can the probability regularization technique be extended to other types of generative models beyond diffusion models, such as GANs or VAEs? The technique is tailored to diffusion models and would need modifications for other architectures.
- **Open Question 3**: How does the choice of optimization algorithm and its hyperparameters affect DNO's performance and convergence? The paper uses Adam with lr=0.01 but doesn't extensively explore the impact of different optimization choices.

## Limitations
- The paper focuses on diffusion models and doesn't explore applicability to other generative model architectures like GANs or VAEs.
- The exact implementation details for hybrid gradient estimation methods are not fully specified, which could affect reproducibility.
- The computational overhead of optimizing noise during inference may limit real-time applications.

## Confidence
- **High confidence**: The general approach of optimizing injected noise during sampling is well-established in the field.
- **Medium confidence**: The theoretical analysis of distribution improvement over time appears sound but requires careful verification.
- **Low confidence**: Claims about state-of-the-art performance relative to other alignment methods need direct benchmark comparisons.

## Next Checks
1. Implement and test the hybrid gradient estimation methods (Hybrid-1 and Hybrid-2) on a simple non-differentiable reward function to verify their effectiveness.
2. Reproduce the probability regularization technique using permutation matrices and evaluate its impact on preventing out-of-distribution reward hacking by comparing CLIP/AITOM scores.
3. Conduct controlled experiments comparing DNO's performance against other inference-time alignment methods on standardized reward functions, measuring both reward scores and distribution metrics.