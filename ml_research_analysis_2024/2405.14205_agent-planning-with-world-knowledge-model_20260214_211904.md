---
ver: rpa2
title: Agent Planning with World Knowledge Model
arxiv_id: '2405.14205'
source_url: https://arxiv.org/abs/2405.14205
tags:
- knowledge
- agent
- task
- state
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a World Knowledge Model (WKM) to address the
  challenges of brainless trial-and-error and hallucinatory actions in agent planning
  tasks. The WKM generates prior task knowledge to guide global planning and dynamic
  state knowledge to regulate local planning.
---

# Agent Planning with World Knowledge Model

## Quick Facts
- arXiv ID: 2405.14205
- Source URL: https://arxiv.org/abs/2405.14205
- Authors: Shuofei Qiao; Runnan Fang; Ningyu Zhang; Yuqi Zhu; Xiang Chen; Shumin Deng; Yong Jiang; Pengjun Xie; Fei Huang; Huajun Chen
- Reference count: 40
- Proposes World Knowledge Model (WKM) to address trial-and-error and hallucinatory actions in agent planning

## Executive Summary
This paper addresses fundamental challenges in agent planning by introducing a World Knowledge Model (WKM) that synthesizes prior task knowledge and dynamic state knowledge to guide planning processes. The WKM is trained to extract and integrate knowledge from expert trajectories, effectively reducing blind exploration and hallucination in LLM-based agents. The approach demonstrates superior performance across three real-world simulated datasets using three different open-source LLMs.

## Method Summary
The World Knowledge Model (WKM) framework consists of two knowledge generation components: one that produces prior task knowledge to guide global planning, and another that generates dynamic state knowledge to regulate local planning. The training process involves steering the agent model to synthesize knowledge from both expert trajectories and sampled trajectories, then integrating this synthesized knowledge into expert trajectories for training. This approach enables the agent to leverage structured world knowledge rather than relying solely on reactive decision-making.

## Key Results
- Achieves superior performance compared to strong baselines across ALFWorld, WebShop, and ScienceWorld datasets
- Effectively reduces blind trial-and-error behaviors in agent planning
- Shows improved generalization to unseen tasks when using Mistral-7B, Gemma-7B, and Llama-3-8B LLMs

## Why This Works (Mechanism)
The WKM framework works by addressing the fundamental limitation of LLM-based agents that lack structured world knowledge. By synthesizing and integrating knowledge from expert demonstrations, the model provides agents with contextual understanding that guides both high-level planning and low-level execution. This dual-level knowledge integration allows agents to make more informed decisions, reducing the need for exhaustive exploration and preventing actions that violate world constraints.

## Foundational Learning
- **Knowledge synthesis from trajectories**: The process of extracting structured knowledge from demonstration data is essential for building world models that can generalize beyond memorized patterns. Quick check: Verify that synthesized knowledge captures both task structure and state transitions.
- **Dual-level planning guidance**: Separating global planning (task-level) from local planning (state-level) allows for more efficient decision-making hierarchies. Quick check: Confirm that each knowledge type addresses distinct planning challenges.
- **Expert trajectory integration**: Using expert demonstrations as knowledge sources ensures that the synthesized information is grounded in successful strategies. Quick check: Validate that expert trajectories cover diverse scenarios within each domain.

## Architecture Onboarding

**Component Map**: Expert Trajectories → Knowledge Synthesizer → Prior Task Knowledge + Dynamic State Knowledge → WKM → Agent Model → Planning Decisions

**Critical Path**: The critical path flows from expert trajectory analysis through knowledge synthesis to the integration point where WKM-enhanced reasoning guides agent actions. The knowledge synthesizer is the bottleneck component, as its output quality directly determines planning effectiveness.

**Design Tradeoffs**: The approach trades increased computational overhead during inference for improved planning efficiency and reduced hallucination. The knowledge synthesis step requires additional training data and computation but enables better generalization. The modular design allows for knowledge updates without retraining the entire agent.

**Failure Signatures**: Performance degradation occurs when expert trajectories lack diversity, leading to knowledge gaps. Hallucination may reappear if the dynamic state knowledge fails to capture environmental constraints. Trial-and-error behaviors emerge when prior task knowledge is too abstract or misaligned with specific task instances.

**3 First Experiments**:
1. Ablation study comparing performance with only prior task knowledge versus only dynamic state knowledge versus both
2. Knowledge quality assessment measuring how well synthesized knowledge predicts expert actions
3. Cross-task generalization test evaluating performance on tasks outside the training distribution

## Open Questions the Paper Calls Out
None

## Limitations
- Training relies heavily on expert trajectories, which may be scarce or expensive to obtain in real-world applications
- Evaluation is confined to three simulated environments, limiting generalizability to more complex or open-ended domains
- Computational overhead during inference is not thoroughly analyzed, which is crucial for practical deployment

## Confidence
High: Sound experimental methodology with appropriate baselines and statistical validation across multiple LLMs and datasets
Medium: Generalization claims are supported by cross-task evaluations but would benefit from external domain validation
Low: Claims about unified multi-task training effectiveness are not fully substantiated with transfer learning experiments

## Next Checks
1. Conduct comprehensive ablation studies to quantify individual contributions of prior task knowledge and dynamic state knowledge components, and measure computational overhead during inference
2. Test WKM approach on at least two additional domains outside current evaluation suite, focusing on domains with different characteristics
3. Implement cost-benefit analysis comparing performance gains against increased computational requirements, testing with resource-constrained LLM variants