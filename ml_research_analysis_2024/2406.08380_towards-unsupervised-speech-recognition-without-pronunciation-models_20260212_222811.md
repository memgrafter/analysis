---
ver: rpa2
title: Towards Unsupervised Speech Recognition Without Pronunciation Models
arxiv_id: '2406.08380'
source_url: https://arxiv.org/abs/2406.08380
tags:
- speech
- word
- unsupervised
- jstti
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach for word-level unsupervised
  speech recognition without relying on pronunciation models. The key idea is to use
  a joint speech-text token-infilling (JSTTI) method that iteratively refines word
  boundaries and achieves a word error rate (WER) of 20-23% on a synthetic 1024-word
  corpus, without parallel transcripts, oracle word boundaries, or a pronunciation
  lexicon.
---

# Towards Unsupervised Speech Recognition Without Pronunciation Models

## Quick Facts
- arXiv ID: 2406.08380
- Source URL: https://arxiv.org/abs/2406.08380
- Authors: Junrui Ni; Liming Wang; Yang Zhang; Kaizhi Qian; Heting Gao; Mark Hasegawa-Johnson; Chang D. Yoo
- Reference count: 40
- Primary result: Achieves 20-23% WER on synthetic 1024-word corpus without pronunciation models, parallel transcripts, or lexicon

## Executive Summary
This paper introduces a novel approach for word-level unsupervised speech recognition that eliminates the need for pronunciation models. The method uses Joint Speech-Text Token Infilling (JSTTI) to learn shared representations between discrete speech tokens and text tokens through iterative refinement of word boundaries. By training a shared Transformer encoder on both modalities simultaneously with masked token-infilling objectives, the system achieves competitive WER performance while remaining lexicon-free and parallel-data-free.

## Method Summary
The JSTTI approach uses a shared Transformer encoder to perform masked reconstruction tasks on both discrete speech tokens (word-level) and text tokens, learning cross-modal mappings without parallel data. The system employs an iterative refinement pipeline: starting with initial segmentation from GradSeg, applying wav2bnd self-training for boundary refinement, jointly training a CNN segmenter and JSTTI model using differentiable soft-pooling, and finally applying another wav2bnd-large iteration. Pseudo-text self-training with HuBERT-Large fine-tuning on CTC loss further improves performance by correcting transcription errors from the initial JSTTI model.

## Key Results
- Achieves 20-23% WER on synthetic 1024-word corpus without pronunciation models
- Outperforms previous unsupervised ASR models under lexicon-free setting
- Boundary token F1 score improves from 36.36% to 64.57% through iterative refinement
- Self-training with HuBERT-Large reduces WER from 26.51% to 20.68% on 1024-word corpus

## Why This Works (Mechanism)

### Mechanism 1
Joint speech-text token-infilling (JSTTI) enables word-level unsupervised ASR by learning shared representations between speech and text modalities without requiring parallel data. The model uses a shared Transformer encoder to perform masked reconstruction tasks on both discrete speech tokens (word-level) and text tokens. By optimizing a weighted negative log likelihood loss on both modalities simultaneously, the encoder learns a shared hidden space that maps speech features to text outputs.

### Mechanism 2
Iterative boundary refinement through differentiable soft-pooling and wav2bnd self-training progressively improves word segmentation accuracy, which directly translates to better ASR performance. The system starts with initial segmentation from GradSeg, then applies wav2bnd self-training to refine boundaries. Next, it jointly trains a CNN segmenter and JSTTI model using differentiable soft-pooling, allowing gradients to flow through the segmentation process. Finally, another wav2bnd-large iteration further refines boundaries.

### Mechanism 3
Pseudo-text self-training with HuBERT-Large using word-level CTC loss significantly improves ASR performance by correcting transcription errors from the initial JSTTI model. The system uses the JSTTI model's predicted word sequences as pseudo-targets to fine-tune a pre-trained HuBERT-Large model under word-level CTC loss. The strong pre-trained speech representations in HuBERT-Large combined with the correction capability of CTC training reduces the error rate.

## Foundational Learning

- Concept: Discrete speech tokenization through unsupervised clustering
  - Why needed here: The model requires discrete word-level speech tokens as input to the Transformer encoder, but no pronunciation lexicon is available
  - Quick check question: How does the k-means clustering process work when pooling frame-level features between unsupervised word boundaries?

- Concept: Masked language modeling for cross-modal representation learning
  - Why needed here: The JSTTI model needs to learn shared representations between speech and text modalities without parallel data
  - Quick check question: What is the difference between the masking strategy used here and standard BERT-style masking?

- Concept: Differentiable boundary detection and soft-pooling
  - Why needed here: The model needs to optimize segmentation parameters through gradient descent while maintaining discrete word boundaries
  - Quick check question: How does the straight-through estimator work in the differentiable k-means quantizer?

## Architecture Onboarding

- Component map: Speech prenet → Shared Transformer encoder → Text postnet (for inference). During training: Speech/text prenet → Shared Transformer encoder → Modality-specific postnets with random mix-up and quantization.
- Critical path: Speech feature extraction → Word boundary detection → Token quantization → JSTTI training → Pseudo-text self-training
- Design tradeoffs: Using discrete tokens enables cross-modal matching but loses fine-grained acoustic information; shared encoder reduces parameters but may limit modality-specific capacity.
- Failure signatures: High WER indicates segmentation problems; training instability suggests issues with the differentiable soft-pooling; poor pseudo-text quality suggests the initial JSTTI model didn't learn effective representations.
- First 3 experiments:
  1. Train JSTTI with oracle word boundaries to establish upper bound performance
  2. Train JSTTI with VG-HuBERT boundaries to test off-the-shelf segmentation
  3. Implement and test the differentiable soft-pooling component with GradSeg+wav2bnd initialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of whole-word unsupervised ASR scale with increasing vocabulary size beyond 4096 words? Is there a point of diminishing returns, and if so, what is the optimal vocabulary size for practical applications?
- Basis in paper: [explicit] The paper explores vocabulary sizes up to 4096 words and notes a significant performance drop at this size. The authors mention that future work will explore larger vocabularies and improving recognition performance for rare words.
- Why unresolved: The paper only tests up to 4096 words, and the authors acknowledge the need for further exploration of larger vocabularies. The optimal vocabulary size for practical applications is not determined.
- What evidence would resolve it: Experimental results testing the proposed JSTTI model with progressively larger vocabulary sizes, demonstrating the point of diminishing returns and the optimal vocabulary size for practical applications.

### Open Question 2
- Question: How does the proposed JSTTI model compare to other unsupervised ASR approaches, such as those using contrastive learning or generative adversarial networks, in terms of word error rate and computational efficiency?
- Basis in paper: [inferred] The paper focuses on the JSTTI model and compares it to a few baselines, but does not extensively compare it to other unsupervised ASR approaches. The authors mention that future work will explore other model architectures and training criteria.
- Why unresolved: The paper does not provide a comprehensive comparison of the JSTTI model to other unsupervised ASR approaches. The relative strengths and weaknesses of different approaches are not fully explored.
- What evidence would resolve it: Comparative experiments evaluating the JSTTI model against other unsupervised ASR approaches on various benchmarks, measuring word error rate and computational efficiency.

### Open Question 3
- Question: How robust is the proposed JSTTI model to variations in speech characteristics, such as accent, speaking rate, and background noise? Can the model generalize well to unseen speakers and recording conditions?
- Basis in paper: [inferred] The paper focuses on a synthetic dataset and does not explicitly address the model's robustness to variations in speech characteristics. The authors mention that future work will test the methods in truly unpaired settings and improve the recognition performance of rare words.
- Why unresolved: The paper does not evaluate the model's performance on real-world data with varying speech characteristics. The model's ability to generalize to unseen speakers and recording conditions is not assessed.
- What evidence would resolve it: Experiments testing the JSTTI model on real-world datasets with diverse speech characteristics, measuring word error rate and robustness to variations in accent, speaking rate, and background noise.

## Limitations
- Evaluation on synthetic corpora limits generalizability to real-world unsupervised ASR scenarios with naturally occurring unpaired data
- Reported WER of 20-23% remains substantially higher than supervised systems (typically <5%), indicating practical deployment challenges
- Reliance on discrete word-level tokenization through k-means clustering loses fine-grained acoustic information and may struggle with ambiguous word boundaries

## Confidence

**High confidence**: The core methodology of JSTTI for joint speech-text representation learning is well-founded and the experimental results on synthetic data are clearly presented. The iterative refinement pipeline follows logical progression and the reported improvements in both segmentation metrics and WER are supported by the data.

**Medium confidence**: The assumption that discrete word-level tokens provide sufficient linguistic information for effective cross-modal mapping is plausible given the results, but the paper doesn't systematically test this assumption or compare against alternative tokenization strategies. The effectiveness of the differentiable soft-pooling approach is demonstrated but not thoroughly analyzed.

**Low confidence**: The generalizability of results to non-synthetic, real-world unpaired speech-text data remains uncertain. The paper doesn't address potential domain mismatch between speech and text corpora or how the system would perform with naturally occurring unpaired data rather than curated synthetic datasets.

## Next Checks

1. **Cross-domain evaluation**: Test the complete JSTTI pipeline on naturally occurring unpaired speech and text data from different domains (e.g., conversational speech paired with written text) to assess robustness to distribution mismatch beyond the synthetic LibriSpeech setup.

2. **Tokenization ablation study**: Compare JSTTI performance using different tokenization granularities (character-level, subword-level, word-level) while keeping other components constant to quantify the impact of the discrete word-level assumption on overall system performance.

3. **Iterative refinement saturation analysis**: Conduct experiments to determine the optimal number of boundary refinement iterations by testing performance across different iteration counts and analyzing whether additional iterations provide diminishing returns or introduce error accumulation.