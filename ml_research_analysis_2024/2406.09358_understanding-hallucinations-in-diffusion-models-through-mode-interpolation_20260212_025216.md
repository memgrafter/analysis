---
ver: rpa2
title: Understanding Hallucinations in Diffusion Models through Mode Interpolation
arxiv_id: '2406.09358'
source_url: https://arxiv.org/abs/2406.09358
tags:
- samples
- diffusion
- data
- training
- mode
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies and formalizes a novel failure mode in diffusion
  models called "hallucination," where models generate samples outside the training
  distribution's support. The authors demonstrate that diffusion models interpolate
  between nearby data modes, producing artifacts never seen in training data.
---

# Understanding Hallucinations in Diffusion Models through Mode Interpolation

## Quick Facts
- **arXiv ID**: 2406.09358
- **Source URL**: https://arxiv.org/abs/2406.09358
- **Reference count**: 40
- **Primary result**: Diffusion models generate hallucinations by interpolating between disjoint data modes, producing samples outside the training distribution's support.

## Executive Summary
This paper identifies a novel failure mode in diffusion models called "hallucinations," where models generate samples that interpolate between disjoint data modes, creating artifacts never seen in training data. Through systematic experiments on synthetic datasets (1D/2D Gaussians and Shapes) and real-world images (hands), the authors demonstrate that diffusion models learn smooth approximations of discontinuous score functions between modes, leading to mode interpolation. The paper introduces a detection metric based on trajectory variance during sampling that achieves >95% detection rate while retaining >96% in-support samples. They also show that recursive training amplifies hallucinations through mode collapse, but this can be mitigated using their detection mechanism.

## Method Summary
The authors systematically study hallucinations in diffusion models by training unconditional DDPM/ADM models on synthetic and real datasets. They analyze the learned score function and develop a trajectory variance metric to detect hallucinated samples during the reverse diffusion process. For recursive training experiments, they generate samples from each generation, filter hallucinated samples using the variance metric, and train the next generation on the filtered set. The approach involves generating 100 million samples for synthetic experiments and 100k samples per generation for recursive training on synthetic shapes datasets.

## Key Results
- Diffusion models interpolate between disjoint data modes, generating samples outside the training distribution's support
- Hallucinations can be detected with >95% accuracy using trajectory variance in the final timesteps of sampling
- Recursive training on synthetic data amplifies hallucinations exponentially without filtering
- Pre-emptive hallucination detection mitigates model collapse during recursive training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models generate hallucinations through mode interpolation between disjoint data modes
- Mechanism: When the true data distribution consists of non-overlapping modes, the learned score function cannot capture sharp transitions between modes. Instead, the neural network smooths these transitions, creating a continuous approximation that interpolates between modes. This interpolation produces samples in regions with negligible probability under the true distribution.
- Core assumption: The neural network cannot learn discontinuous functions (like step functions) that would perfectly separate distinct modes
- Evidence anchors:
  - [abstract] "diffusion models smoothly 'interpolate' between nearby data modes in the training set to generate samples that are completely outside the support of the original training distribution"
  - [section 4.3] "the neural network learns a smooth approximation of the true score function, particularly around the regions between disjoint modes of the distribution"
  - [corpus] Weak evidence - the corpus papers discuss hallucinations but don't provide mechanistic detail about mode interpolation specifically
- Break condition: If the neural network architecture could learn discontinuous functions or if the data distribution has full support everywhere

### Mechanism 2
- Claim: Hallucinations can be detected through trajectory variance during sampling
- Mechanism: When generating hallucinated samples, the predicted value of x0 (final image) shows high variance in the final stages of the reverse diffusion process. This occurs because the score function has high uncertainty in the interpolated regions between modes. Non-hallucinated samples show stable predictions in the final timesteps.
- Core assumption: The variance in the predicted x0 trajectory correlates with whether the sample is in the data support or not
- Evidence anchors:
  - [section 5.1] "hallucinated samples have high variance in the predicted x0 across time steps" while "non-hallucinated samples stabilize in their prediction in the last 20 time steps"
  - [section 5.2] "We use the same observation as a metric to distinguish hallucinated and non-hallucinated (in-support) samples"
  - [corpus] Weak evidence - corpus papers mention detection methods but don't discuss trajectory variance specifically
- Break condition: If the diffusion model uses different sampling strategies or if the variance pattern doesn't generalize across different datasets

### Mechanism 3
- Claim: Recursive training on synthetic data amplifies hallucinations through mode collapse
- Mechanism: When diffusion models are trained on their own generated data (recursive training), hallucinated samples that interpolate between modes become part of the training distribution. The next generation of the model learns to generate more of these interpolated samples, causing mode collapse where the model's output converges to a distribution dominated by hallucinations.
- Core assumption: Hallucinations in one generation become part of the training data for the next generation
- Evidence anchors:
  - [section 6] "the hallucinated samples significantly influence the learning of the next generation's distribution" and "the ratio of hallucinated samples increases exponentially"
  - [abstract] "we observe that the proposed detection mechanism is able to mitigate the model collapse during recursive training"
  - [corpus] Moderate evidence - corpus papers discuss recursive training and model collapse but don't specifically connect it to hallucinations
- Break condition: If real data is mixed in during training or if the detection mechanism effectively filters out hallucinations

## Foundational Learning

- Concept: Diffusion models and score-based generative modeling
  - Why needed here: Understanding how diffusion models work is essential to grasp why they interpolate between modes and produce hallucinations
  - Quick check question: What is the relationship between the learned score function and the generated samples in diffusion models?

- Concept: Mode collapse in generative models
  - Why needed here: The paper builds on existing research about mode collapse but extends it to consider mode interpolation between distinct modes
  - Quick check question: How does mode interpolation differ from traditional mode collapse?

- Concept: Recursive training and its effects
  - Why needed here: The paper studies how recursive training on synthetic data exacerbates hallucinations, building on recent research about recursive training
  - Quick check question: What happens when a generative model is trained on its own generated data?

## Architecture Onboarding

- Component map: Forward diffusion (add noise) -> Reverse diffusion (denoise using learned score function) -> Hallucination detection (trajectory variance) -> Recursive training (filter samples)
- Critical path: Forward diffusion adds noise to data, reverse diffusion denoises using learned score function, hallucination detection uses trajectory variance, recursive training uses filtered samples
- Design tradeoffs: T=1000 timesteps provides good quality but computational cost; using trajectory variance for detection adds computation but enables filtering; recursive training without real data leads to collapse but demonstrates the phenomenon
- Failure signatures: Hallucinations manifest as extra/missing fingers in hands, multiple shapes in simple shapes dataset, interpolated samples between Gaussian modes; detection fails when variance patterns don't generalize
- First 3 experiments:
  1. Train DDPM on 1D Gaussian mixture and visualize generated samples to observe mode interpolation
  2. Implement trajectory variance metric and test on 1D Gaussian to verify hallucination detection
  3. Run recursive training on simple shapes dataset to observe exponential increase in hallucinations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can mode interpolation be mitigated through architectural changes to the diffusion model that allow it to learn discontinuous score functions between modes?
- Basis in paper: [explicit] The paper identifies that mode interpolation occurs because neural networks learn smooth approximations of discontinuous score functions between disjoint modes. They explicitly state "the neural network can not learn such sharp functions and smoothly approximates a tempered version of the same."
- Why unresolved: The paper only observes and characterizes the phenomenon but doesn't propose architectural modifications to address it. They focus on detection rather than prevention.
- What evidence would resolve it: Experimental results showing a diffusion model architecture (e.g., with specialized activation functions, network depth/width adjustments, or loss function modifications) that successfully learns discontinuous score functions between modes and reduces hallucination generation.

### Open Question 2
- Question: Does the trajectory variance hallucination detection metric generalize to diffusion models trained on natural image datasets beyond the synthetic experiments shown?
- Basis in paper: [inferred] The paper demonstrates the metric works well on 1D/2D Gaussians, Shapes, MNIST, and Hands datasets, but only shows partial success on the Hands dataset (80% detection rate). They acknowledge "the detection is a hard problem" and suggest "More analysis on what region of trajectory leads to hallucinations would be useful across various schedules and sampling algorithms."
- Why unresolved: The paper's experiments are limited to relatively simple synthetic datasets and one small-scale real-world dataset (Hands). The effectiveness on complex natural image distributions remains unproven.
- What evidence would resolve it: Comprehensive experiments applying the variance metric to large-scale diffusion models (e.g., Stable Diffusion, DALL-E) trained on diverse natural image datasets, showing high detection rates (>90%) while maintaining high in-support sample retention.

### Open Question 3
- Question: Is there a fundamental relationship between the number of modes in a dataset and the propensity for mode interpolation/hallucinations in diffusion models?
- Basis in paper: [explicit] The paper shows experiments with different numbers of Gaussians (2, 3, 4) and observes varying levels of mode interpolation. They note "The number of interpolated samples also decreases as the distance from the modes increases" and show results with different mode arrangements.
- Why unresolved: While the paper demonstrates that mode interpolation varies with mode configuration, it doesn't systematically analyze how the number of modes affects hallucination frequency or provide theoretical understanding of this relationship.
- What evidence would resolve it: A systematic study varying the number of modes in synthetic datasets while controlling for other factors (variance, separation distance, sample size), combined with theoretical analysis of how the score function approximation error scales with the number of modes.

## Limitations

- Limited validation on complex real-world distributions - experiments focus on synthetic datasets and one small-scale real-world dataset
- Effectiveness of detection metric on diverse, high-dimensional datasets remains uncertain
- Practical implications for improving diffusion model training in real-world applications are unclear

## Confidence

- **High confidence**: The existence of hallucinations as samples outside training distribution support, the basic detection mechanism using trajectory variance, and the recursive training collapse phenomenon on synthetic data
- **Medium confidence**: The specific mechanism of mode interpolation as the primary cause of hallucinations, and the generalizability of detection metrics to complex real-world distributions
- **Low confidence**: The practical implications of these findings for improving diffusion model training in real-world applications

## Next Checks

1. **Cross-architecture validation**: Test whether mode interpolation occurs consistently across different diffusion model architectures (e.g., DDIM, DDPM, ADM) and with varying noise schedules to verify if the phenomenon is architecture-dependent

2. **Real-world distribution analysis**: Apply the trajectory variance detection method to complex real-world datasets (e.g., LSUN, FFHQ) with multi-modal distributions to assess whether the metric maintains high detection accuracy (>95%) while minimizing false positives

3. **Intervention effectiveness**: Evaluate whether explicitly regularizing against mode interpolation during training (e.g., through adversarial training or distribution-aware losses) reduces hallucination rates without sacrificing sample quality and diversity