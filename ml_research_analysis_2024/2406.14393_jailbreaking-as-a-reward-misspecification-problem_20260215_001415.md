---
ver: rpa2
title: Jailbreaking as a Reward Misspecification Problem
arxiv_id: '2406.14393'
source_url: https://arxiv.org/abs/2406.14393
tags:
- reward
- remiss
- arxiv
- target
- suffixes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new perspective on jailbreaking large language
  models (LLMs) by framing it as a reward misspecification problem. The authors propose
  a metric called ReGap to quantify the extent of reward misspecification and develop
  an automated red teaming system, ReMiss, that generates adversarial prompts by exploiting
  these vulnerabilities.
---

# Jailbreaking as a Reward Misspecification Problem

## Quick Facts
- arXiv ID: 2406.14393
- Source URL: https://arxiv.org/abs/2406.14393
- Authors: Zhihui Xie; Jiahui Gao; Lei Li; Zhenguo Li; Qi Liu; Lingpeng Kong
- Reference count: 37
- Primary result: ReMiss achieves state-of-the-art attack success rates on AdvBench benchmark by exploiting reward misspecification vulnerabilities

## Executive Summary
This paper introduces a novel perspective on LLM jailbreaking by framing it as a reward misspecification problem rather than just an adversarial prompting challenge. The authors propose ReGap, a metric that quantifies how aligned models incorrectly rank harmful versus harmless responses, and develop ReMiss, an automated system that generates adversarial prompts by exploiting these vulnerabilities. ReMiss achieves superior performance on the AdvBench benchmark and demonstrates high transferability to closed-source models like GPT-4, while maintaining human readability of generated prompts. The approach offers new insights into LLM safety and suggests that improving reward modeling accuracy could be key to preventing jailbreaking attacks.

## Method Summary
The method introduces ReMiss, an automated red teaming system that generates adversarial prompts by exploiting reward misspecification vulnerabilities in aligned LLMs. The core insight is that jailbreaking occurs when models assign higher implicit rewards to harmful responses than harmless ones for certain prompts. ReMiss optimizes for a reward gap metric that considers both the harmful response to elicit and the harmless response to bypass, using stochastic beam search to find suffixes that minimize this gap while maintaining low perplexity. The system trains a generator model on successful suffixes and demonstrates state-of-the-art attack success rates across various target models, with high transferability to closed-source models.

## Key Results
- ReMiss achieves state-of-the-art attack success rates on AdvBench benchmark across Vicuna, Llama2, and Mistral models
- Attacks demonstrate high transferability to closed-source models with 100% ASR@10 on GPT-3.5-turbo and 48.1% on GPT-4
- Generated adversarial suffixes maintain human readability with low perplexity scores
- Reward gap metric (ReGap) better predicts jailbreaking success than traditional target loss minimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Jailbreaking occurs because reward misspecification during alignment causes the model to assign higher implicit rewards to harmful responses than harmless ones for certain prompts.
- Mechanism: The aligned model's implicit reward function fails to accurately rank the quality of responses. For some prompts, the model assigns higher reward to harmful responses despite human labelers preferring harmless responses. This creates a "reward gap" that ReMiss exploits by finding prompts where this gap is negative or near zero.
- Core assumption: The implicit reward derived from the KL-divergence constraint in alignment accurately reflects the model's learned preferences, and these preferences can be systematically exploited through prompt engineering.
- Evidence anchors:
  - [abstract]: "We propose a novel perspective that attributes this vulnerability to reward misspecification during the alignment process."
  - [section 3.1]: Formal definition of reward misspecification: r(x, y') > r(x, y) when humans prefer y over y'.
  - [corpus]: Weak evidence - corpus contains papers on adversarial attacks and robustness but doesn't directly address reward misspecification as a cause.
- Break condition: If the implicit reward function accurately ranks responses for all prompts, or if the reward gap becomes uniformly positive for harmful prompts, this mechanism breaks.

### Mechanism 2
- Claim: ReMiss generates effective adversarial prompts by minimizing the reward gap between harmless and harmful responses, rather than just minimizing target loss on harmful responses.
- Mechanism: Instead of optimizing for target loss (which only considers harmful responses), ReMiss optimizes for the reward gap, which considers both the harmless response to bypass and the harmful response to elicit. This three-term objective (minimize harmful response probability, maximize harmless response unlikelihood, regularize divergence) is more effective at finding vulnerabilities.
- Core assumption: The reward gap provides a better proxy for jailbreaking success than target loss alone, as demonstrated by the correlation analysis in Figure 2.
- Evidence anchors:
  - [section 2.2]: "target loss alone is not a good proxy for jailbreaking" and Figure 2 showing reward gap better differentiates jailbroken vs non-jailbroken prompts.
  - [section 4.1]: The three-term decomposition of the reward gap objective.
  - [corpus]: Moderate evidence - papers on automated adversarial prompting exist but don't specifically address reward gap optimization.
- Break condition: If the reward gap becomes a poor predictor of jailbreaking success, or if the three-term objective doesn't outperform simpler loss functions.

### Mechanism 3
- Claim: The transferability of ReMiss attacks to closed-source models occurs because reward misspecification vulnerabilities are systematic rather than model-specific.
- Mechanism: The adversarial suffixes generated by ReMiss exploit fundamental weaknesses in how aligned models process prompts and assign rewards. These weaknesses are present across different models and alignment approaches, allowing suffixes optimized for one model to transfer to others.
- Core assumption: Reward misspecification vulnerabilities stem from shared alignment training processes (RLHF, human feedback) rather than idiosyncratic model architectures.
- Evidence anchors:
  - [section 5.3]: "Attacks are highly transferable to closed-source LLMs" with 100% ASR@10 on GPT-3.5-turbo and 48.1% on GPT-4.
  - [abstract]: "Furthermore, these attacks on open-source models demonstrate high transferability to closed-source models like GPT-4o"
  - [corpus]: Moderate evidence - papers on transferability exist but don't specifically attribute it to reward misspecification.
- Break condition: If transferability breaks down for specific model families or if each model's vulnerabilities are unique to its training data.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF) and KL-divergence constraints
  - Why needed here: Understanding how alignment works is crucial to understanding why it can fail. The paper treats alignment as RL with KL constraints, where the implicit reward comes from the log probability ratio between aligned and reference models.
  - Quick check question: What is the closed-form solution for an aligned model when using KL-divergence regularization, and how does it relate to the implicit reward?

- Concept: Reward modeling and preference learning
  - Why needed here: The paper's core argument is that reward misspecification causes jailbreaking vulnerabilities. Understanding how reward models are trained from human preferences and why they can be noisy or biased is essential.
  - Quick check question: Why might human preference data be noisy or biased, and how could this lead to reward misspecification?

- Concept: Adversarial prompting and jailbreaking techniques
  - Why needed here: The paper builds on existing jailbreaking research but proposes a new objective. Understanding previous approaches (target loss minimization, gradient-based attacks) helps contextualize why reward gap optimization is novel.
  - Quick check question: What is the main limitation of previous jailbreaking approaches that focus only on minimizing target loss on harmful responses?

## Architecture Onboarding

- Component map:
  Reference model (πref) -> Target model (π) -> Generator (πθ) -> Reward gap calculator -> Stochastic beam search

- Critical path:
  1. Compute reward gap between harmless and harmful responses for a given prompt
  2. Use stochastic beam search to find suffixes that minimize reward gap while maintaining low perplexity
  3. Train generator on successful suffixes
  4. Generate adversarial suffixes for new prompts using the trained generator

- Design tradeoffs:
  - Reference model choice: Using the base model vs. other open-source models affects attack effectiveness but increases practicality
  - Suffix length: Longer suffixes may be more effective but less human-readable and harder to transfer
  - Temperature in beam search: Higher temperatures explore more diverse suffixes but may reduce success rates

- Failure signatures:
  - High reward gaps for all prompts indicate robust alignment
  - Low transfer rates to closed-source models suggest model-specific vulnerabilities
  - Generated suffixes with high perplexity indicate the regularization term is too weak

- First 3 experiments:
  1. Reproduce the backdoor detection experiment (Section 3.2) to verify ReGap metric effectiveness
  2. Run the reward gap vs. target loss correlation analysis (Figure 2) on a small dataset
  3. Test transferability to a simple closed-source API (like OpenAI's text-davinci-002) using generated suffixes

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the discussion and limitations, several questions emerge from the analysis:

### Open Question 1
- Question: How does the performance of ReMiss change when using different types of reference models (e.g., models with varying levels of safety alignment) against various target models?
- Basis in paper: [explicit] The paper mentions that using TinyLlama-1.1b as a reference model still achieves notable success rates against Llama2-7b, indicating that the choice of reference model impacts performance.
- Why unresolved: The paper only tests with Llama2-7b-base and TinyLlama-1.1b, but does not explore a wide range of reference models with different alignment strengths or architectures.
- What evidence would resolve it: Systematic experiments comparing ReMiss performance using reference models with different safety alignments, sizes, and architectures against various target models.

### Open Question 2
- Question: What is the computational overhead of ReMiss compared to baseline methods, and how does it scale with model size?
- Basis in paper: [inferred] The paper notes that generating adversarial suffixes using stochastic beam search is computationally intensive, but does not provide detailed computational complexity analysis or comparisons with baseline methods.
- Why unresolved: The paper mentions computational challenges but lacks quantitative comparisons of training/inference times and resource usage across different model sizes.
- What evidence would resolve it: Empirical measurements of training and inference times, memory usage, and comparisons with baseline methods across different model sizes.

### Open Question 3
- Question: How robust is ReMiss to adversarial defenses specifically designed to detect reward misspecification?
- Basis in paper: [inferred] The paper demonstrates that ReMiss generates human-readable suffixes that evade perplexity-based filters, but does not test against defenses targeting reward misspecification detection.
- Why unresolved: The paper focuses on evading traditional defenses but does not explore specialized defenses that could detect the reward misspecification objective itself.
- What evidence would resolve it: Experiments testing ReMiss against defenses that specifically monitor for reward gap violations or other reward misspecification indicators.

## Limitations

- The paper's causal mechanism linking reward misspecification to jailbreaking vulnerabilities is plausible but not definitively proven across diverse alignment approaches
- Transfer results to GPT-4 (48.1% ASR) could be influenced by factors beyond shared reward misspecification vulnerabilities
- Analysis is primarily based on five poisoned models and three open-source targets, which may not represent the full diversity of alignment approaches

## Confidence

**High confidence**: The mathematical formulation of reward misspecification (ReGap metric) is sound and the correlation analysis showing ReGap's superiority to target loss as a jailbreaking proxy is well-supported. The experimental results demonstrating ReMiss's state-of-the-art performance on AdvBench and successful transfer to closed-source models are clearly presented with appropriate metrics.

**Medium confidence**: The causal mechanism linking reward misspecification to jailbreaking vulnerabilities is plausible but not definitively proven. While the paper provides strong evidence that ReGap correlates with jailbreaking success and that ReMiss exploits this correlation, it doesn't conclusively demonstrate that reward misspecification is the root cause rather than a symptom of other alignment issues. The transfer results, while impressive, could be influenced by factors beyond shared reward misspecification vulnerabilities.

**Low confidence**: The paper's claims about the generality of reward misspecification as a fundamental alignment problem require more extensive validation. The analysis is primarily based on five poisoned models and three open-source targets, which may not represent the full diversity of alignment approaches. The discussion of defense implications is speculative without empirical validation of potential mitigation strategies.

## Next Checks

1. **Cross-alignment validation**: Test ReMiss on models aligned using different approaches (PPO, DPO, supervised fine-tuning) to determine whether reward misspecification vulnerabilities are universal across alignment methods or specific to certain training paradigms.

2. **Defense evaluation**: Implement and evaluate simple defenses against reward misspecification, such as reward model retraining with adversarial examples or KL regularization adjustment, to assess the practical exploitability of these vulnerabilities.

3. **Mechanism isolation**: Design experiments to isolate whether jailbreaking success is primarily driven by reward misspecification versus other factors like prompt format sensitivity or knowledge boundary exploitation, potentially by comparing ReMiss performance on aligned models with different reward functions but similar architectures.