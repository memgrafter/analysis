---
ver: rpa2
title: Evaluating D-MERIT of Partial-annotation on Information Retrieval
arxiv_id: '2406.16048'
source_url: https://arxiv.org/abs/2406.16048
tags:
- evidence
- query
- systems
- retrieval
- passages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how partial annotation in evaluation sets distorts
  the ranking of information retrieval systems. The authors curate D-MERIT, a Wikipedia-based
  passage retrieval evaluation set that aims to contain all relevant passages for
  each query.
---

# Evaluating D-MERIT of Partial-annotation on Information Retrieval

## Quick Facts
- arXiv ID: 2406.16048
- Source URL: https://arxiv.org/abs/2406.16048
- Reference count: 36
- This paper studies how partial annotation in evaluation sets distorts the ranking of information retrieval systems.

## Executive Summary
This paper investigates how partial annotation in evaluation sets can distort the ranking of information retrieval systems. The authors curate D-MERIT, a Wikipedia-based passage retrieval evaluation set that aims to contain all relevant passages for each query. Using D-MERIT, they demonstrate that single-relevant evaluation setups are highly sensitive to passage selection methods for annotation unless one system is significantly superior. They also show that rankings stabilize as more relevant passages are added, with the number needed depending on the performance gap between systems. The authors recommend balancing resource-efficiency and reliable evaluation when annotating retrieval datasets.

## Method Summary
The authors created D-MERIT, a passage retrieval evaluation set based on Wikipedia, by using Wikidata structure to generate queries about entity groups and GPT-4 to identify relevant passages. They then evaluated various retrieval systems (BM25, QL, UniCoil, SPLADEv2, SPLADE++, DPR, coCondenser, RetroMAE, TCT-Colbert) on D-MERIT using different annotation depths and compared rankings using Kendall-τ and concordance metrics. The study analyzed how partial annotation affects system rankings and how many annotations are needed for stable rankings.

## Key Results
- Single-relevant evaluation setups are highly sensitive to passage selection methods for annotation unless one system is significantly superior.
- Rankings stabilize as more relevant passages are added, with the number needed depending on the performance gap between systems.
- TREC-style evaluation methods fail to find a significant portion of relevant passages in D-MERIT, leading to a non-negligible error rate.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Single-relevant evaluation setups are highly sensitive to passage selection method for annotation unless one system is significantly superior.
- **Mechanism**: When only one relevant passage is annotated per query, the evaluation depends entirely on which passage is chosen. If a passage that favors a particular retrieval system is selected, that system will appear better than it truly is, while systems that would have retrieved other relevant passages are penalized.
- **Core assumption**: The distribution of relevant passages across queries is heterogeneous and not uniformly favorable to all retrieval systems.
- **Evidence anchors**:
  - [abstract] "We show that evaluating on a dataset containing annotations for only a subset of the relevant passages might result in misleading ranking of the retrieval systems"
  - [section] "Results are plotted in Fig. 2. The graph shows that the selection technique, used to pick which passages are annotated, has a major effect on the systems' measured performance and on the ranking of the different systems."
  - [corpus] Weak: The corpus evidence here is inferential - it relies on the observation that different selection methods (popular, longest, shortest, system-based) yield different rankings, but doesn't directly show the heterogeneity assumption.
- **Break condition**: When the performance gap between systems is large enough that even biased annotation cannot mask the difference, or when multiple relevant passages are annotated per query, reducing the impact of selection bias.

### Mechanism 2
- **Claim**: Rankings stabilize as more relevant passages are added, with the number needed depending on the performance gap between systems.
- **Mechanism**: As more relevant passages are annotated, the evaluation set becomes more complete, reducing the number of false negatives. Systems that retrieve additional relevant passages are no longer penalized, and the ranking converges to the true performance order. The number of annotations required for stability is inversely related to the performance gap between systems.
- **Core assumption**: The additional relevant passages are representative of the full set of relevant passages for each query, and that retrieval systems retrieve these passages in a way that reflects their true performance.
- **Evidence anchors**:
  - [abstract] "We show that as more relevant texts are included in the evaluation set, the rankings converge."
  - [section] "Our findings reveal that in order to reliably evaluate retrieval systems that are reasonably close in performance, a significant portion of relevant passages must be found."
  - [corpus] Moderate: The corpus evidence supports this through the experimental results showing convergence of rankings as more evidence is added, but the exact relationship between performance gap and required annotations is derived from the experimental setup rather than directly observed in the corpus.
- **Break condition**: When the annotation process is no longer resource-efficient relative to the marginal improvement in ranking stability, or when the performance gap is so large that even a single relevant passage is sufficient to differentiate systems.

### Mechanism 3
- **Claim**: TREC-style evaluation methods fail to find a significant portion of relevant passages in D-MERIT, leading to a non-negligible error rate.
- **Mechanism**: TREC relies on pooling top-k passages from multiple systems and annotating them. If the pool depth (k) is insufficient relative to the number of relevant passages per query, many relevant passages will be missed. This leads to a high number of false negatives in the evaluation set, distorting system rankings.
- **Core assumption**: The number of relevant passages per query is large enough that a shallow pool (k=10-20) will miss a substantial fraction of them.
- **Evidence anchors**:
  - [abstract] "We show that using partially-annotated datasets in evaluation can paint a distorted picture."
  - [section] "To ensure our collection process is nearly exhaustive, we need another evidence collection process, independent of ours. We thus adopt the popular TREC approach... We ask human raters to mark the remaining passages for relevance and find only 35 new evidence."
  - [corpus] Strong: The corpus evidence directly shows that TREC finds only 346 relevant passages out of an estimated 990-638 (depending on extrapolation), a coverage of 31.7-94.5%, indicating a significant gap in annotation coverage.
- **Break condition**: When the pool depth is increased sufficiently (e.g., k=100) to capture most relevant passages, or when the number of relevant passages per query is small enough that a shallow pool is adequate.

## Foundational Learning

- **Concept**: Information Retrieval (IR) metrics and their sensitivity to annotation completeness.
  - **Why needed here**: The paper's core argument revolves around how partial annotation affects the reliability of IR evaluation metrics like recall and Kendall-τ. Understanding these metrics and their properties is crucial for interpreting the results and implications.
  - **Quick check question**: How does the number of annotated relevant passages per query affect the reliability of recall@k as an evaluation metric for retrieval systems?

- **Concept**: Statistical significance testing and its role in comparing system performance.
  - **Why needed here**: The paper uses p-values from relative t-tests to bucket system pairs by performance difference and analyze how this affects the required annotation depth for stable rankings. Understanding p-values and their interpretation is essential for following this analysis.
  - **Quick check question**: Why does the paper use p-values to divide system pairs into buckets, and how does this relate to the required number of annotated passages for reliable evaluation?

- **Concept**: Pooling methods in IR evaluation and their limitations.
  - **Why needed here**: The paper critiques the TREC pooling approach for its inability to capture all relevant passages in D-MERIT. Understanding how pooling works and its inherent limitations is key to grasping this critique.
  - **Quick check question**: What is the fundamental limitation of the TREC pooling approach that leads to incomplete annotation of relevant passages, as observed in the paper?

## Architecture Onboarding

- **Component map**: D-MERIT creation: Wikipedia queries/candidates → GPT-4 evidence identification → Natural language query generation. Experimental study: System evaluation (Recall@k, Kendall-τ) → Gradual annotation addition → Ranking comparison → Convergence analysis.
- **Critical path**: For dataset creation: Collect queries and candidates → Automatic identification of evidence → Natural language query generation. For experimental study: Evaluate systems on single-relevant setup → Gradually add annotated passages → Compare rankings → Analyze convergence and required annotation depth.
- **Design tradeoffs**: Using automatic annotation (GPT-4) trades off perfect accuracy for scalability, enabling the creation of a large, nearly exhaustive dataset. The choice of Wikipedia as a corpus trades off generalizability to other corpora for the unique structured data that enables nearly complete annotation.
- **Failure signatures**: If the automatic identification model (GPT-4) misses a significant number of relevant passages, the dataset will be less exhaustive than intended, weakening the conclusions about partial annotation. If the natural language query generation produces queries that are not representative of real user queries, the dataset may not be suitable for evaluating real-world retrieval systems.
- **First 3 experiments**:
  1. Reproduce the single-relevant evaluation setup by randomly sampling one evidence per query and comparing system rankings to the ground truth ranking using Kendall-τ.
  2. Repeat the above experiment but with biased selection methods (most popular, longest, shortest, system-based) to demonstrate the sensitivity to selection bias.
  3. Gradually add annotated passages to the evaluation set and measure the convergence of system rankings using partial-Kendall-τ, analyzing how the required annotation depth varies with the performance gap between systems.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal balance between resource-efficiency and reliable evaluation when annotating retrieval datasets, and how does this balance change based on the performance gap between systems being evaluated?
- **Basis in paper**: [explicit] The authors discuss the efficiency-reliability curve and recommend balancing resource-efficiency with reliable evaluation, but do not provide specific guidelines for determining the optimal point on this curve.
- **Why unresolved**: The paper demonstrates that the number of annotations required for reliable evaluation depends on the performance gap between systems, but does not provide concrete recommendations for how to determine the optimal number of annotations for a given evaluation scenario.
- **What evidence would resolve it**: Empirical studies comparing evaluation outcomes using different numbers of annotations across various system performance gaps, coupled with analysis of the trade-offs between annotation costs and evaluation reliability.

### Open Question 2
- **Question**: How well do the conclusions drawn from D-MERIT generalize to other retrieval datasets and tasks, particularly those not based on Wikipedia's structured data?
- **Basis in paper**: [inferred] The authors acknowledge that their dataset collection method relies heavily on Wikipedia's structure and that showing conclusions generalize to other datasets is challenging due to the difficulty of completely annotating other corpora.
- **Why unresolved**: While the authors believe their conclusions are likely applicable to other datasets, they can only empirically demonstrate their findings on D-MERIT due to the unique properties of Wikipedia that enable complete annotation.
- **What evidence would resolve it**: Application of the authors' evaluation methodology to other completely-annotated retrieval datasets from different domains and sources, followed by comparison of the findings to those reported in the paper.

### Open Question 3
- **Question**: What is the impact of using different passage selection methods during annotation on the reliability of system rankings, and how can we develop more reliable automatic selection methods?
- **Basis in paper**: [explicit] The authors show that biased passage selection methods lead to higher error rates in system rankings compared to random selection, and demonstrate that using existing retrieval systems to select passages for annotation is particularly problematic.
- **Why unresolved**: While the paper identifies the problem of biased passage selection, it does not provide solutions for developing more reliable automatic selection methods or guidelines for how to choose passages for annotation when random selection is not feasible.
- **What evidence would resolve it**: Development and evaluation of new passage selection algorithms that minimize bias while maintaining efficiency, followed by empirical comparison of their performance to random and system-based selection methods across multiple retrieval tasks and datasets.

## Limitations

- The generalizability of the findings to other retrieval tasks and corpora beyond Wikipedia-based passage retrieval is uncertain.
- The accuracy and consistency of GPT-4 in identifying relevant passages could introduce bias or error into the evaluation, affecting the completeness and reliability of the D-MERIT dataset.
- The paper does not provide specific guidelines for determining the optimal balance between resource-efficiency and reliable evaluation when annotating retrieval datasets.

## Confidence

- **High Confidence**: The experimental results demonstrating the convergence of system rankings as more relevant passages are added, and the observation that TREC-style pooling methods miss a significant portion of relevant passages in D-MERIT.
- **Medium Confidence**: The recommendation to balance resource-efficiency and reliable evaluation when annotating retrieval datasets, based on the trade-off between annotation depth and ranking stability observed in the experiments.
- **Low Confidence**: The generalizability of the findings to other retrieval tasks and corpora, and the potential impact of GPT-4's accuracy on the completeness and reliability of the D-MERIT dataset.

## Next Checks

1. **Cross-domain validation**: Evaluate the sensitivity of partial-annotation evaluation to passage selection methods on retrieval tasks from different domains (e.g., news, scientific literature) and corpora to assess the generalizability of the findings.

2. **Human annotation study**: Conduct a human annotation study to compare the accuracy and consistency of GPT-4 in identifying relevant passages to human annotators, and assess the potential impact of model errors on the completeness and reliability of the D-MERIT dataset.

3. **Annotation depth analysis**: Perform a detailed analysis of the relationship between annotation depth, performance gap between systems, and ranking stability across a wider range of retrieval tasks and performance levels to refine the recommendations for resource-efficient and reliable evaluation.