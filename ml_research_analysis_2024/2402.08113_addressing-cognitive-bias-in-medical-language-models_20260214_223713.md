---
ver: rpa2
title: Addressing cognitive bias in medical language models
arxiv_id: '2402.08113'
source_url: https://arxiv.org/abs/2402.08113
tags:
- bias
- medical
- language
- cognitive
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates cognitive biases in medical large language
  models (LLMs) using a novel benchmark, BiasMedQA. The authors evaluate six LLMs
  on 1,273 USMLE questions modified to include seven clinically relevant cognitive
  biases.
---

# Addressing cognitive bias in medical language models

## Quick Facts
- arXiv ID: 2402.08113
- Source URL: https://arxiv.org/abs/2402.08113
- Reference count: 34
- Primary result: Medical LLMs show reduced diagnostic accuracy when exposed to cognitive bias prompts, with GPT-4 most resilient

## Executive Summary
This study investigates cognitive biases in medical large language models using a novel benchmark, BiasMedQA, which includes 1,273 USMLE questions modified to incorporate seven clinically relevant cognitive biases. The research evaluates six LLMs and finds that all models exhibit reduced diagnostic accuracy when exposed to bias prompts, with GPT-4 demonstrating the highest resilience. False consensus bias had the most significant impact, reducing performance by an average of 24.9% across models. The study introduces three bias mitigation strategies and shows they improve robustness, particularly for GPT-4, though accuracy with mitigation does not fully match that of unbiased prompts.

## Method Summary
The study uses a novel benchmark called BiasMedQA, consisting of 1,273 USMLE questions modified to include seven clinically relevant cognitive biases. Six LLMs (GPT-4, Mixtral-8x70B, GPT-3.5, PaLM-2, Llama 2 70B-chat, and PMC Llama 13B) are evaluated on this dataset. The researchers introduce three bias mitigation strategies: education, one-shot, and few-shot demonstrations. The evaluation measures diagnostic accuracy reduction in the presence of bias prompts and the effectiveness of mitigation strategies in restoring accuracy.

## Key Results
- All six evaluated LLMs showed reduced diagnostic accuracy when exposed to cognitive bias prompts
- GPT-4 demonstrated the highest resilience to bias, with a worst-case accuracy drop of 14.0% for false-consensus bias
- False consensus bias had the most significant impact, reducing performance by an average of 24.9% across models
- Bias mitigation strategies improved robustness, particularly for GPT-4, but accuracy with mitigation did not fully match that of unbiased prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cognitive biases reduce medical LLM diagnostic accuracy because the models give undue weight to biased information in the prompt, similar to human clinicians.
- Mechanism: The models process the entire prompt text and generate responses based on learned patterns. When bias-related phrases are present, they influence the reasoning path, leading to selection of the suggested (incorrect) answer.
- Core assumption: The LLMs' reasoning is sensitive to contextual cues and framing effects, even when the underlying medical knowledge would suggest a different answer.
- Evidence anchors:
  - [abstract] "Results show that all models exhibit reduced diagnostic accuracy when exposed to bias prompts, with GPT-4 demonstrating the highest resilience."
  - [section] "Our analysis revealed varying effects for biases on these LLMs, with GPT-4 standing out for its resilience to bias."
  - [corpus] Weak evidence - no corpus mentions of this specific mechanism.

### Mechanism 2
- Claim: GPT-4's higher resilience to bias is due to its more robust architecture and training process, which includes better handling of adversarial prompts.
- Mechanism: GPT-4 was trained with techniques to improve robustness to adversarial prompting across defined risk categories, including giving unqualified advice.
- Core assumption: The additional training and architectural improvements in GPT-4 specifically address bias susceptibility.
- Evidence anchors:
  - [section] "Much effort was taken to ensure training was aligned with proper safety metrics. Toward this, llama shows improvements in adversarial prompting across defined risk categories, which, importantly, includes giving unqualified advice (e.g., medical advice) as is prompted for in this work."
  - [section] "GPT-4 demonstrates a worst-case accuracy drop in response to false-consensus biases by 14.0%, but is very resilient to confirmation bias, dropping by only 0.2%."
  - [corpus] Weak evidence - no corpus mentions of this specific mechanism.

### Mechanism 3
- Claim: The bias mitigation strategies work by providing explicit examples or warnings that override the influence of the bias-related information in the prompt.
- Mechanism: By educating the model about potential biases or showing examples of correct vs. incorrect responses, the model learns to prioritize the correct reasoning path despite the presence of bias cues.
- Core assumption: The models can learn from explicit instructions and examples to modify their reasoning behavior.
- Evidence anchors:
  - [section] "While these strategies improved robustness, particularly for GPT-4, accuracy with mitigation did not fully match that of unbiased prompts."
  - [section] "gpt-4 consistently shows the highest level of improvement across all strategies."
  - [corpus] Weak evidence - no corpus mentions of this specific mechanism.

## Foundational Learning

- Concept: Understanding cognitive biases in human decision-making
  - Why needed here: The study is based on the premise that LLMs are susceptible to the same cognitive biases that affect human clinicians.
  - Quick check question: What is confirmation bias and how might it affect a doctor's diagnosis?

- Concept: Large language model architecture and training
  - Why needed here: The study evaluates the performance of different LLMs, which requires understanding their underlying architectures and training processes.
  - Quick check question: What is the difference between a foundation model and a fine-tuned model?

- Concept: Medical knowledge and diagnostic reasoning
  - Why needed here: The study uses medical exam questions to evaluate the LLMs' diagnostic accuracy, which requires understanding the medical knowledge being tested.
  - Quick check question: What is the USMLE and what kind of questions does it include?

## Architecture Onboarding

- Component map:
  - Input prompt processing -> Context understanding -> Reasoning -> Answer selection -> Output generation

- Critical path:
  - Prompt → Context understanding → Reasoning → Answer selection → Output

- Design tradeoffs:
  - Model size vs. performance: Larger models like GPT-4 generally perform better but are more resource-intensive.
  - Specialization vs. generalization: Medical-specific models may perform better on medical tasks but may be less adaptable to other domains.
  - Bias resilience vs. accuracy: More bias-resilient models may sacrifice some accuracy on unbiased tasks.

- Failure signatures:
  - Overconfidence in incorrect answers: The model provides a strong answer despite limited or biased information.
  - Non-response: The model refuses to answer due to safety filters or inability to process the prompt.
  - Nonsensical answers: The model provides an answer that is unrelated to the question or options.

- First 3 experiments:
  1. Test the model's performance on unbiased questions to establish a baseline accuracy.
  2. Introduce a single bias prompt (e.g., false consensus) and measure the change in accuracy.
  3. Apply a bias mitigation strategy (e.g., education) and measure the improvement in accuracy compared to the biased condition.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do medical LLMs perform on clinical decision-making tasks when presented with prompts containing multiple cognitive biases simultaneously?
- Basis in paper: [explicit] The paper evaluates LLMs on questions with individual cognitive biases but does not explore the effects of compounded biases.
- Why unresolved: The study focuses on the impact of single biases per prompt, leaving the interaction effects of multiple biases untested.
- What evidence would resolve it: Testing LLMs with prompts containing combinations of biases and comparing their performance to single-bias prompts.

### Open Question 2
- Question: What is the long-term effectiveness of bias mitigation strategies in real-world clinical settings?
- Basis in paper: [inferred] The paper introduces mitigation strategies but does not assess their sustainability or applicability in dynamic clinical environments.
- Why unresolved: The study evaluates mitigation strategies in a controlled setting, but real-world clinical interactions are more complex and variable.
- What evidence would resolve it: Longitudinal studies of LLMs in clinical practice with ongoing bias monitoring and mitigation adjustments.

### Open Question 3
- Question: How do different fine-tuning datasets and methodologies influence the susceptibility of medical LLMs to cognitive biases?
- Basis in paper: [explicit] The paper compares models with different training approaches but does not isolate the impact of fine-tuning data or methods on bias susceptibility.
- Why unresolved: The study does not systematically vary the fine-tuning process to determine its effect on bias resilience.
- What evidence would resolve it: Experiments comparing LLMs fine-tuned on datasets with varying levels of bias mitigation or different training techniques.

## Limitations

- Proprietary nature of GPT-3.5 and GPT-4 prevents full understanding of their training processes and bias resilience mechanisms
- Dataset represents a specific subset of medical knowledge (USMLE questions) and may not generalize to all clinical scenarios
- Bias mitigation strategies showed improvement but didn't fully restore unbiased accuracy, suggesting partial rather than complete solutions

## Confidence

*High Confidence:* The observation that all tested LLMs show reduced accuracy when exposed to cognitive bias prompts (supported by systematic testing across 1,273 questions and six models)

*Medium Confidence:* The effectiveness of bias mitigation strategies (results show improvement but with variability across models and strategies, and the mechanisms aren't fully understood)

*Medium Confidence:* The specific impact of different bias types (while patterns are observed, the interaction between bias types and model architectures needs further exploration)

## Next Checks

1. Test the same models on clinical scenarios outside the USMLE framework to assess generalizability of bias patterns

2. Evaluate whether additional rounds of bias mitigation training can achieve complete restoration of unbiased accuracy

3. Conduct ablation studies to determine which components of the bias mitigation strategies contribute most to their effectiveness