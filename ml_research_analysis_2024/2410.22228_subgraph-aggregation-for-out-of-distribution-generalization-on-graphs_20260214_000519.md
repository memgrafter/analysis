---
ver: rpa2
title: Subgraph Aggregation for Out-of-Distribution Generalization on Graphs
arxiv_id: '2410.22228'
source_url: https://arxiv.org/abs/2410.22228
tags:
- graph
- subgraphs
- sugar
- learning
- invariant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses out-of-distribution (OOD) generalization in
  graph neural networks by learning multiple invariant subgraphs rather than a single
  one. The authors propose SubGraph Aggregation (SuGAr), which uses a tailored subgraph
  sampler and diversity regularizer to extract diverse invariant subgraphs, then aggregates
  them via averaging to improve OOD generalization.
---

# Subgraph Aggregation for Out-of-Distribution Generalization on Graphs

## Quick Facts
- arXiv ID: 2410.22228
- Source URL: https://arxiv.org/abs/2410.22228
- Authors: Bowen Liu; Haoyang Li; Shuning Wang; Shuo Nie; Shanghang Zhang
- Reference count: 38
- Primary result: Outperforms state-of-the-art methods by up to 24% in OOD generalization on graphs

## Executive Summary
This paper addresses the challenge of out-of-distribution (OOD) generalization in graph neural networks by learning multiple invariant subgraphs rather than relying on a single one. The proposed SubGraph Aggregation (SuGAr) framework uses a tailored subgraph sampler and diversity regularizer to extract diverse invariant subgraphs, then aggregates them via averaging to improve OOD generalization. Extensive experiments on 15 datasets show consistent improvements across various distribution shifts, making SuGAr particularly effective for multi-subgraph scenarios common in real-world applications like molecular property prediction.

## Method Summary
SuGAr learns multiple invariant subgraphs by training parallel invariant GNNs with a diversity regularizer that ensures complementary learning. The framework includes a novel ensemble method and weight averaging technique specifically designed for graph data. The diversity regularizer explicitly penalizes similarity between activation values of different featurizers, forcing each model to focus on different patterns within the input graph. Weight averaging maintains linear mode connectivity by averaging weights from independently trained models with shared initialization, identifying flatter solutions in the loss landscape that generalize better to OOD data.

## Key Results
- Outperforms state-of-the-art methods by up to 24% in OOD generalization across 15 datasets
- Consistent improvements across various distribution shifts including structure-level, scaffold, size, and attribute shifts
- Particularly effective in multi-subgraph scenarios common in molecular property prediction
- Shows robustness to hyperparameter variations while maintaining performance advantages

## Why This Works (Mechanism)

### Mechanism 1
Multiple invariant subgraphs capture richer causal structure than single-subgraph approaches. The framework learns diverse subgraphs through parallel invariant GNNs, each capturing different invariant patterns that are aggregated to form a comprehensive representation covering all critical causal structures. This works because real-world graphs often contain multiple independent causal subgraphs that jointly determine the target label.

### Mechanism 2
The diversity regularizer ensures base models learn complementary subgraphs by explicitly penalizing similarity between activation values of different featurizers. This forces each model to focus on different patterns within the input graph, preventing convergence to similar subgraphs that would provide redundant information.

### Mechanism 3
Weight averaging maintains linear mode connectivity for better OOD generalization by averaging weights from independently trained models with shared initialization. This identifies flatter solutions in the loss landscape that generalize better to OOD data, leveraging the property that models can be averaged along a linear path with minimal error increase.

## Foundational Learning

- **Graph Neural Networks and message passing**: Why needed here - The framework builds on GNNs as the base architecture for subgraph extraction and classification. Quick check: Can you explain how a standard GNN propagates information across graph structure?
- **Invariant learning and causality**: Why needed here - The framework is based on identifying invariant subgraphs that maintain consistent relationships with labels across different environments. Quick check: What's the difference between invariant and spurious correlations in the context of OOD generalization?
- **Contrastive learning**: Why needed here - Used to approximate the mutual information between subgraphs and labels for identifying invariant patterns. Quick check: How does contrastive loss help distinguish between invariant and environment-specific features?

## Architecture Onboarding

- **Component map**: Featurizer (subgraph extractor) → Classifier → Diversity regularizer → Aggregator (ensemble or weight averaging) → Model selection
- **Critical path**: Subgraph extraction → Diversity injection → Aggregation → Model selection → Inference
- **Design tradeoffs**: Multiple models increase computation but improve coverage; diversity regularizer adds complexity but prevents redundancy
- **Failure signatures**: Performance degradation when diversity injection is removed; poor results with inappropriate hyperparameter settings
- **First 3 experiments**: 1) Test with single invariant subgraph (CIGA baseline) on SPMotif to establish baseline performance, 2) Run SuGAr with diversity regularizer disabled to measure impact of diversity injection, 3) Compare ensemble vs weight averaging variants on DrugOOD dataset to determine optimal aggregation method

## Open Questions the Paper Calls Out

### Open Question 1
How does SuGAr's performance scale with the number of subgraphs it needs to learn? Is there an optimal number of subgraphs beyond which additional subgraphs provide diminishing returns or even degrade performance? The paper demonstrates effectiveness on datasets with multiple subgraphs but doesn't systematically vary the number of subgraphs or analyze performance scaling.

### Open Question 2
How does SuGAr perform on graph-level tasks beyond classification, such as link prediction or node classification, and what modifications would be needed for these task types? The paper focuses exclusively on graph-level classification tasks and doesn't address whether the subgraph aggregation approach generalizes to other GNN task types.

### Open Question 3
What is the computational complexity of SuGAr compared to single-subgraph methods, and how does this affect its scalability to large graphs or real-time applications? While the paper shows performance improvements, it doesn't provide detailed computational complexity analysis or runtime comparisons between SuGAr and baseline methods.

## Limitations

- The paper demonstrates strong empirical performance but lacks theoretical guarantees for why multiple invariant subgraphs outperform single-subgraph approaches in all cases
- Limited ablation studies on the impact of diversity regularizer strength and its interaction with ensemble size
- The weight averaging method's effectiveness compared to standard ensembling is shown empirically but not theoretically justified

## Confidence

- **High Confidence**: The empirical results showing SuGAr outperforms state-of-the-art methods across 15 datasets, with up to 24% improvement in OOD generalization
- **Medium Confidence**: The claim that multiple invariant subgraphs capture richer causal structure than single-subgraph approaches, supported by experiments but lacking theoretical justification
- **Medium Confidence**: The effectiveness of the diversity regularizer in ensuring complementary subgraph learning, demonstrated through ablation studies but with unclear optimal parameter settings

## Next Checks

1. **Theoretical Analysis**: Derive bounds or guarantees for why multi-subgraph aggregation provides OOD generalization benefits over single-subgraph approaches
2. **Robustness Testing**: Evaluate SuGAr's performance under varying diversity regularizer strengths and different ensemble sizes to identify optimal configurations
3. **Generalization Beyond Classification**: Test the framework on regression tasks and other graph-based problems beyond molecular property prediction to assess broader applicability