---
ver: rpa2
title: Towards scalable efficient on-device ASR with transfer learning
arxiv_id: '2407.16664'
source_url: https://arxiv.org/abs/2407.16664
tags:
- pretraining
- training
- learning
- transfer
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates multilingual transfer learning to enhance
  low-resource monolingual ASR models. The research examines three key aspects: the
  effectiveness of transfer learning during initial training versus fine-tuning, the
  impact of in-domain versus out-of-domain pretraining, and the differential effects
  on rare versus non-rare words.'
---

# Towards scalable efficient on-device ASR with transfer learning

## Quick Facts
- arXiv ID: 2407.16664
- Source URL: https://arxiv.org/abs/2407.16664
- Reference count: 0
- Primary result: RNNT-loss pretraining followed by MinWER fine-tuning reduces WER across languages, with out-of-domain pretraining achieving 28% higher WERR than in-domain pretraining

## Executive Summary
This study investigates multilingual transfer learning to enhance low-resource monolingual ASR models. The research examines three key aspects: the effectiveness of transfer learning during initial training versus fine-tuning, the impact of in-domain versus out-of-domain pretraining, and the differential effects on rare versus non-rare words. Results show that RNNT-loss pretraining followed by monolingual fine-tuning with MinWER loss consistently reduces WER across languages. WERR reaches 36.2% and 42.8% compared to monolingual baselines for MLS and in-house datasets. Out-of-domain pretraining achieves 28% higher WERR than in-domain pretraining. Both rare and non-rare words benefit, with rare words showing greater improvements with out-of-domain pretraining, and non-rare words with in-domain pretraining.

## Method Summary
The study employs RNNT-loss pretraining on multilingual datasets followed by monolingual fine-tuning with MinWER loss. Two pretraining approaches are compared: in-domain (using target language data) versus out-of-domain (using diverse multilingual data). The encoder-only pretraining strategy uses a 20-layer streamable low-latency Emformer architecture. The method is evaluated on MLS dataset (Italian, French, English, Dutch) and an in-house dataset (Italian, French, German, Spanish, English) using WER and WERR metrics.

## Key Results
- RNNT-loss pretraining followed by MinWER fine-tuning consistently reduces WER across languages like Italian and French
- WERR reaches 36.2% and 42.8% compared to monolingual baselines for MLS and in-house datasets
- Out-of-domain pretraining leads to 28% higher WERR than in-domain pretraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RNNT-loss pretraining followed by MinWER fine-tuning achieves better WER reduction than fine-tuning alone.
- Mechanism: Pretraining with RNNT loss establishes shared multilingual phonetic representations. Fine-tuning with MinWER loss then optimizes for word-level accuracy using those representations.
- Core assumption: The multilingual encoder representations learned during RNNT pretraining transfer effectively to monolingual tasks.
- Evidence anchors:
  - [abstract] "Our finding suggests that RNNT-loss pretraining, followed by monolingual fine-tuning with Minimum Word Error Rate (MinWER) loss, consistently reduces Word Error Rates (WER) across languages like Italian and French."
  - [section] "Our findings suggest that the third configuration works best, which involves initial pretraining during the RNNT stage followed by fine-tuning with MinWER."
- Break condition: If the target language is linguistically distant from pretraining languages, shared representations may not transfer effectively.

### Mechanism 2
- Claim: Out-of-domain pretraining achieves higher WERR than in-domain pretraining.
- Mechanism: Pretraining on more diverse data (out-of-domain) builds better generalized representations that improve adaptation to target domains.
- Core assumption: Diversity in pretraining data improves model generalization more than domain-specific alignment.
- Evidence anchors:
  - [abstract] "Out-of-domain pretraining leads to 28% higher WERR than in-domain pretraining."
  - [section] "Leveraging a pretrained model trained on more diverse data of the same languages proved beneficial, enhancing model generalization and leading to significant performance improvements."
- Break condition: If target domain is extremely specialized, out-of-domain diversity might introduce noise rather than benefit.

### Mechanism 3
- Claim: Rare words benefit more from out-of-domain pretraining while non-rare words benefit more from in-domain pretraining.
- Mechanism: Out-of-domain data provides broader exposure to rare word contexts, while in-domain data provides better coverage of common word patterns.
- Core assumption: Rare word recognition requires broader linguistic exposure while non-rare words benefit from domain-specific patterns.
- Evidence anchors:
  - [abstract] "Both rare and non-rare words benefit, with rare words showing greater improvements with out-of-domain pretraining, and non-rare words with in-domain pretraining."
  - [section] "Rare words experienced a greater benefit in this particular scenario" for out-of-domain pretraining.
- Break condition: If rare word distribution is very different between domains, out-of-domain pretraining might not help.

## Foundational Learning

- Concept: RNNT (Recurrent Neural Network Transducer) architecture
  - Why needed here: The study uses RNNT models as the base architecture for ASR, with specific pretraining and fine-tuning approaches
  - Quick check question: What are the three main components of an RNNT model and their roles in sequence-to-sequence transduction?

- Concept: MinWER (Minimum Word Error Rate) loss
  - Why needed here: Used for fine-tuning stage to directly optimize word-level accuracy rather than token-level
  - Quick check question: How does MinWER loss differ from standard RNNT loss in terms of optimization objective?

- Concept: Transfer learning in low-resource settings
  - Why needed here: The core methodology leverages knowledge from high-resource languages to improve low-resource ASR performance
  - Quick check question: What is the key advantage of transfer learning over training monolingual models from scratch in low-resource scenarios?

## Architecture Onboarding

- Component map: Audio features → 20-layer streamable low-latency Emformer encoder → 3-layer LSTM predictor → Joint network → 5001 unigram SentencePieces output vocabulary
- Critical path: Audio features → Encoder processing → Joint network → Softmax probabilities → Transcript output
- Design tradeoffs: Encoder-only pretraining chosen over full model pretraining for better convergence
- Failure signatures: Poor convergence when pretraining and target languages are linguistically distant; performance degradation when domain mismatch is too large
- First 3 experiments:
  1. Compare RNNT-loss pretraining vs direct fine-tuning on baseline monolingual models
  2. Test in-domain vs out-of-domain pretraining approaches with identical fine-tuning
  3. Measure rare vs non-rare word performance across different pretraining strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does transfer learning effectiveness vary across different layers of the encoder architecture when applied selectively rather than to the entire encoder?
- Basis in paper: [explicit] The authors mention they plan to explore "targeted layer-wise transfer learning, focusing on key layers for effective knowledge transfer rather than transferring the entire encoder-based pretraining" in future work.
- Why unresolved: The current study applies pretraining uniformly across the entire encoder without examining which specific layers benefit most from transfer learning or whether selective layer transfer could yield better results.
- What evidence would resolve it: Systematic ablation studies comparing different layer combinations for transfer learning, measuring WER improvements across various layer subsets, and identifying optimal layer configurations for different language families.

### Open Question 2
- Question: What is the optimal strategy for combining languages with different linguistic backgrounds in multilingual pretraining to maximize transfer learning benefits for zero-shot languages?
- Basis in paper: [explicit] The authors observed that Spanish (a zero-shot language) showed significant improvement (26.6% WERR) when pretraining included English, Italian, and French, while German showed regression (44.8%↑), suggesting language family relationships matter.
- Why unresolved: The study only tested one combination of pretraining languages and found mixed results for zero-shot languages, but did not systematically explore how different language groupings affect transfer learning effectiveness.
- What evidence would resolve it: Controlled experiments varying pretraining language combinations based on linguistic relatedness, measuring zero-shot performance across different language families, and developing guidelines for optimal pretraining language selection.

### Open Question 3
- Question: How does the performance of multilingual pretraining vary when applied to streaming versus non-streaming ASR architectures, particularly for low-resource languages?
- Basis in paper: [inferred] The current study uses a streaming Emformer architecture with 20 layers and 40ms lookahead, but does not compare this to non-streaming alternatives or examine how architecture choices impact transfer learning effectiveness.
- Why unresolved: The paper focuses exclusively on streaming RNNT models without exploring whether non-streaming architectures or different streaming configurations might yield better transfer learning results for low-resource languages.
- What evidence would resolve it: Comparative studies between streaming and non-streaming architectures using identical transfer learning approaches, measuring WER improvements and convergence rates across both paradigms for various low-resource languages.

## Limitations

- Lack of specific training hyper-parameters (learning rates, batch sizes, warm-up schedules) creates significant barriers to faithful reproduction
- Findings are constrained to specific language pairs (primarily Italian and French) and may not generalize to linguistically distant languages
- Study doesn't address computational efficiency metrics despite the "scalable efficient on-device" framing in the title

## Confidence

**High Confidence (4/5)**: The superiority of RNNT-loss pretraining followed by MinWER fine-tuning over fine-tuning alone. This finding is well-supported by the reported results across multiple datasets and languages, with consistent WER reductions observed.

**Medium Confidence (3/5)**: The out-of-domain pretraining advantage (28% higher WERR). While the results are compelling, the study doesn't explore intermediate scenarios or quantify the precise relationship between domain diversity and performance gains.

**Medium Confidence (3/5)**: The differential effects on rare versus non-rare words. The study provides evidence for this claim but doesn't fully characterize the mechanisms or explore whether these effects hold across different vocabulary distributions.

## Next Checks

1. **Reproducibility validation**: Implement the training pipeline using the described architecture and evaluate whether the reported WER improvements (36.2% MLS, 42.8% in-house) can be replicated with reasonable hyper-parameter tuning.

2. **Domain diversity quantification**: Systematically vary the degree of domain overlap between pretraining and fine-tuning data to establish the relationship between domain diversity and WERR gains, testing whether the 28% improvement holds across different domain similarity levels.

3. **Cross-linguistic generalization**: Extend the experimental evaluation to linguistically diverse language pairs (e.g., including languages with different scripts, word orders, or phonetic inventories) to test whether the observed transfer learning benefits generalize beyond closely related Indo-European languages.