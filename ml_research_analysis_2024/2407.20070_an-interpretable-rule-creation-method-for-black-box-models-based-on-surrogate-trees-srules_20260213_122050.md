---
ver: rpa2
title: An Interpretable Rule Creation Method for Black-Box Models based on Surrogate
  Trees -- SRules
arxiv_id: '2407.20070'
source_url: https://arxiv.org/abs/2407.20070
tags:
- rules
- u1d456
- srules
- u1d45b
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SRules is a rule extraction method for black-box models based on
  surrogate decision trees, designed to improve interpretability by balancing accuracy,
  coverage, and interpretability. It recursively creates surrogate interpretable decision
  tree models that approximate complex model decision boundaries, generating concise
  and meaningful rules.
---

# An Interpretable Rule Creation Method for Black-Box Models based on Surrogate Trees -- SRules

## Quick Facts
- arXiv ID: 2407.20070
- Source URL: https://arxiv.org/abs/2407.20070
- Authors: Mario Parrón Verdasco; Esteban García-Cuesta
- Reference count: 26
- Key outcome: SRules is a rule extraction method for black-box models based on surrogate decision trees, designed to improve interpretability by balancing accuracy, coverage, and interpretability.

## Executive Summary
SRules is a novel rule extraction method that improves the interpretability of black-box machine learning models by recursively creating surrogate interpretable decision tree models that approximate complex model decision boundaries. The method generates concise and meaningful rules by using feature importance and conditional chi-square analysis to select valid patterns while pruning duplicates. SRules outperforms state-of-the-art techniques like RuleFit and RuleCOSi in both F1-score and number of rules, achieving up to 93.01% F1-score with only 5.9 rules using CatBoost. A recursive version, RSRules, increases coverage to 72.6% while maintaining classification performance, though at the cost of interpretability due to a higher number of rules.

## Method Summary
SRules is a rule extraction algorithm that given a black-box machine learning model greedily searches for the most important features and the associated tree structures to find a subset of rules that provides interpretation over the dataset. The method constructs surrogate binary trees using the most important features of the black-box model, then applies conditional chi-square analysis to select statistically significant branches that form interpretable rules. These rules are ordered by accuracy and simplicity, with duplicates pruned to maximize interpretability. The recursive version, RSRules, iteratively applies SRules to uncovered instances, creating additional rule sets for each iteration to increase coverage while maintaining performance characteristics.

## Key Results
- SRules achieves up to 93.01% F1-score with only 5.9 rules using CatBoost on tested datasets
- Outperforms state-of-the-art methods like RuleFit and RuleCOSi in both F1-score and number of rules
- RSRules increases coverage to 72.6% while maintaining classification performance
- SRules provides flexibility through adjustable parameters to balance interpretability and coverage for different application needs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SRules improves interpretability by recursively extracting surrogate decision trees from black-box models and converting them into production rules.
- Mechanism: The method constructs surrogate binary trees using the most important features of the black-box model, then applies conditional chi-square analysis to select statistically significant branches that form interpretable rules. These rules are ordered by accuracy and simplicity, with duplicates pruned to maximize interpretability.
- Core assumption: The surrogate tree structure accurately approximates the black-box model's decision boundaries for the most important features.
- Evidence anchors:
  - [abstract] "SRules balances the accuracy, coverage, and interpretability of machine learning models by recursively creating surrogate interpretable decision tree models that approximate the decision boundaries of a complex model."
  - [section] "SRules is a rule extraction algorithm that given a black-box machine learning model greedily searches for the most important features and the associated tree structures to find a subset of rules that provides interpretation over the dataset."
  - [corpus] Weak - no direct corpus evidence about surrogate tree construction
- Break condition: If the black-box model's important features change significantly between iterations, the surrogate trees may no longer accurately represent the decision boundaries.

### Mechanism 2
- Claim: SRules maintains classification performance while reducing the number of rules compared to state-of-the-art methods.
- Mechanism: By using conditional chi-square analysis to select only statistically significant patterns and pruning duplicate rules, SRules creates a more concise rule set that covers the same decision space as the original model but with fewer rules.
- Core assumption: The statistical significance threshold used in the chi-square analysis effectively identifies the most important decision patterns.
- Evidence anchors:
  - [abstract] "SRules outperforms state-of-the-art techniques like RuleFit and RuleCOSi in both F1-score and number of rules, achieving up to 93.01% F1-score with only 5.9 rules using CatBoost."
  - [section] "The resulting ruleset /u1D4451∗ for iteration 1 is then stored and a new iteration starts using those samples from the dataset that are not covered by /u1D4451∗."
  - [corpus] Weak - no direct corpus evidence about rule pruning effectiveness
- Break condition: If the chi-square significance threshold is set too high, important decision patterns may be excluded, reducing both coverage and performance.

### Mechanism 3
- Claim: The recursive version (RSRules) increases coverage while maintaining classification performance, at the cost of interpretability.
- Mechanism: RSRules iteratively applies the SRules algorithm to uncovered instances, creating additional rule sets for each iteration. This approach ensures broader coverage of the decision space while maintaining the performance characteristics of the original model.
- Core assumption: Uncovered instances in each iteration can be effectively modeled by new surrogate trees using different feature importance rankings.
- Evidence anchors:
  - [abstract] "A recursive version, RSRules, increases coverage to 72.6% while maintaining classification performance, though at the cost of interpretability due to a higher number of rules."
  - [section] "We apply again SRules with /u1D44Bobtaining new set of rules for a second iteration /u1D4452; for /u1D45Biterations we will obtain{ /u1D4451, /u1D4452, . . . , /u1D445/u1D45B}"
  - [corpus] Weak - no direct corpus evidence about recursive coverage improvement
- Break condition: If the feature importance rankings become highly correlated across iterations, the additional rules may not provide meaningful coverage of previously uncovered instances.

## Foundational Learning

- Concept: Feature importance extraction using permutation tests and Gini impurity
  - Why needed here: SRules relies on identifying the most important features from the black-box model to construct meaningful surrogate trees
  - Quick check question: How does the permutation test method quantify feature importance in ensemble models?

- Concept: Conditional chi-square analysis for pattern selection
  - Why needed here: This statistical test determines which decision patterns in the surrogate tree are significant enough to become rules
  - Quick check question: What does the conditional chi-square test evaluate when selecting patterns from the surrogate tree?

- Concept: Decision tree construction and pruning techniques
  - Why needed here: SRules builds surrogate trees and applies pruning to remove duplicate rules, requiring understanding of tree-based methods
  - Quick check question: How does the minimum instances per node parameter affect the complexity of the generated surrogate trees?

## Architecture Onboarding

- Component map: Black-box model -> Feature importance extraction -> SBT construction -> Chi-square analysis -> Rule generation -> Pruning -> Final ruleset
- Critical path: Black-box model → Feature importance extraction → SBT construction → Chi-square analysis → Rule generation → Pruning → Final ruleset
- Design tradeoffs: 
  - Higher significance thresholds increase interpretability but reduce coverage
  - More iterations in RSRules increase coverage but reduce interpretability
  - Minimum instances per node parameter balances rule complexity and statistical validity
- Failure signatures:
  - Very low coverage indicates significance thresholds may be too high
  - Large number of rules suggests inadequate pruning or too many iterations
  - Performance drop compared to black-box model indicates surrogate trees aren't accurately capturing decision boundaries
- First 3 experiments:
  1. Test SRules on a simple dataset with known decision boundaries to verify surrogate tree accuracy
  2. Compare rule count and coverage between SRules and baseline methods on a binary classification dataset
  3. Evaluate RSRules iteration count vs. coverage tradeoff on a multi-class dataset with class imbalance

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several implicit questions emerge from the methodology and results presented.

## Limitations
- Lack of detailed implementation specifications for critical components like conditional chi-square analysis and exact rule pruning methodology
- Limited comparison with other surrogate-based approaches beyond RuleFit and RuleCOSi
- No empirical analysis of computational complexity or scalability with dataset size and feature dimensionality

## Confidence
- **High confidence**: The F1-score and rule count comparisons with baseline methods
- **Medium confidence**: The effectiveness of the recursive RSRules approach for increasing coverage
- **Low confidence**: The statistical validity of the conditional chi-square pattern selection without access to implementation details

## Next Checks
1. Replicate on tabular datasets: Apply SRules to additional UCI datasets with varying characteristics (imbalance, dimensionality) to test robustness across different data distributions.
2. Implement ablation study: Test SRules with and without chi-square analysis and duplicate pruning to quantify the contribution of each component to final performance.
3. Compare with direct decision tree extraction: Evaluate whether the surrogate tree approach provides advantages over simply extracting rules directly from a trained decision tree model of similar depth.