---
ver: rpa2
title: Are Frontier Large Language Models Suitable for Q&A in Science Centres?
arxiv_id: '2412.05200'
source_url: https://arxiv.org/abs/2412.05200
tags:
- space
- what
- like
- science
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study assessed the suitability of frontier Large Language
  Models (LLMs) for Q&A interactions in science centres, focusing on factual accuracy
  and engagement with young audiences. Researchers used a dataset of questions from
  the National Space Centre in Leicester and evaluated responses from three models:
  GPT-4, Claude 3.5 Sonnet, and Gemini 1.5.'
---

# Are Frontier Large Language Models Suitable for Q&A in Science Centres?

## Quick Facts
- arXiv ID: 2412.05200
- Source URL: https://arxiv.org/abs/2412.05200
- Reference count: 27
- Primary result: LLM responses show trade-off between creativity and accuracy, with Claude 3.5 Sonnet outperforming others in maintaining clarity and engagement.

## Executive Summary
This study evaluated three frontier LLMs (GPT-4, Claude 3.5 Sonnet, and Gemini 1.5) for Q&A interactions in science centres, focusing on factual accuracy and engagement with young audiences. Using a dataset of 12 questions from the National Space Centre in Leicester, each model generated both standard and creative responses for an 8-year-old audience. Expert evaluations revealed that higher novelty generally correlated with reduced factual reliability across all models, though Claude consistently maintained better performance even when generating creative responses.

## Method Summary
The researchers collected 12 questions from the National Space Centre categorized into closed, open, divergent, and wildcard types. They generated responses using three frontier LLMs with both standard and creative prompts, targeting an 8-year-old audience. Five space science experts then evaluated all responses using a 5-point Likert scale across five criteria: accuracy, clarity, engagement, deviation from expected answers, and surprise factor. The evaluation was conducted blind, without revealing which model generated each response.

## Key Results
- Claude 3.5 Sonnet consistently outperformed GPT-4 and Gemini in maintaining accuracy and clarity, even with creative prompts
- Creative prompts generally reduced accuracy and clarity across all models, with Claude experiencing only a 0.4 decrease in accuracy versus significant drops for other models
- Wildcard questions showed improved accuracy and clarity with creative prompts, as these questions lack objectively correct answers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Claude 3.5 Sonnet consistently maintained high accuracy even when prompted for creative responses, unlike GPT-4 and Gemini.
- **Mechanism:** Claude's model architecture or training approach appears more robust to prompt variations, allowing it to balance novelty with factual reliability.
- **Core assumption:** The observed performance difference stems from inherent model characteristics rather than prompt sensitivity.
- **Evidence anchors:**
  - [abstract] "Results revealed a trade-off between creativity and accuracy, with Claude consistently outperforming GPT-4 and Gemini in maintaining clarity and engagement, even when generating creative responses."
  - [section] "Claude consistently presented higher accuracy than the other models for both standard and creative responses... Claude's average accuracy decreased by only 0.4, whereas both GPT and Gemini experienced significant reductions in their reviewed scores."
  - [corpus] Weak: no direct evidence about Claude's training or architecture in corpus.
- **Break condition:** If creative prompts consistently degrade Claude's accuracy to match GPT-4 or Gemini levels.

### Mechanism 2
- **Claim:** Prompt engineering significantly impacts model performance, with standard prompts yielding higher accuracy and clarity than creative prompts across all models.
- **Mechanism:** Standard prompts reduce ambiguity and guide models toward concise, factual responses, while creative prompts introduce uncertainty that can compromise accuracy.
- **Core assumption:** The observed performance differences are primarily due to prompt structure rather than inherent model limitations.
- **Evidence anchors:**
  - [section] "Figures 2(a) and 2(b) demonstrate that responses generated using standard prompts consistently achieve higher scores in accuracy and clarity... prompts aimed at fostering creativity exhibit a certain decline in accuracy and clarity."
  - [section] "It is also interesting to note that the use of a prompt aiming at creativity did not actually increase the engagement level of GPT's and Claude's answers."
  - [corpus] No direct evidence in corpus about prompt engineering techniques.
- **Break condition:** If alternative prompt structures could achieve both high creativity and accuracy simultaneously.

### Mechanism 3
- **Claim:** Question type influences the effectiveness of creative versus standard prompts, with wildcard questions showing improved accuracy and clarity when using creative prompts.
- **Mechanism:** Wildcard questions lack objectively correct answers, allowing creative responses to be both engaging and appropriately novel without sacrificing accuracy.
- **Core assumption:** The nature of the question determines whether creativity enhances or detracts from response quality.
- **Evidence anchors:**
  - [section] "A notable exception is observed in the wildcard category, where shifting from standard to creative prompts leads to a slight increase in accuracy and clarity, accompanied by a similar rise in the surprise factor."
  - [section] "This can be explained by the inherent nature of wildcard questions, which lack an objectively valid answer."
  - [corpus] No direct evidence in corpus about question type effects.
- **Break condition:** If creative prompts for wildcard questions begin to introduce factual errors or confusion.

## Foundational Learning

- **Concept:** Understanding the trade-off between creativity and factual accuracy in LLM responses
  - **Why needed here:** The study's core finding is that higher novelty correlates with reduced factual reliability, making this balance crucial for educational applications.
  - **Quick check question:** Why did experts rate surprising responses as less accurate and clear?

- **Concept:** Prompt engineering techniques for balancing engagement and accuracy
  - **Why needed here:** The study demonstrates that different prompt types yield different performance outcomes, suggesting prompt engineering is critical for educational applications.
  - **Quick check question:** What happened to Claude's clarity score when switching from standard to creative prompts?

- **Concept:** Question categorization and its impact on response quality
  - **Why needed here:** The study shows that different question types (closed, open, divergent, wildcard) respond differently to creative versus standard prompts.
  - **Quick check question:** Which question category showed improved accuracy with creative prompts?

## Architecture Onboarding

- **Component map:** Question dataset collection -> LLM response generation (3 models Ã— 2 prompt types) -> Expert evaluation (5-point Likert scale across 5 metrics)
- **Critical path:** The evaluation process is the critical path - collecting questions, generating responses, and having experts rate them determines the study's conclusions about model suitability.
- **Design tradeoffs:** The study chose to use a fixed age target (8-year-old) for all responses rather than adapting to question source demographics, prioritizing consistency over perfect demographic matching.
- **Failure signatures:** Models failing to maintain accuracy when prompted for creativity, or experts finding responses unclear or unengaging despite creative elements.
- **First 3 experiments:**
  1. Test prompt variations on a single model with a subset of questions to understand sensitivity before full evaluation.
  2. Compare expert vs. target audience (children) responses to validate expert assessments.
  3. Experiment with hybrid prompts that combine creative and standard elements to find optimal balance points.

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions beyond noting that assessments were conducted by experts rather than the target young audience, identifying this as a limitation.

## Limitations
- Small sample size (12 questions) from a single science center limits generalizability across different educational contexts
- Expert evaluations may not accurately reflect how actual young audiences perceive and engage with responses
- Specific prompt wording for creative responses remains underspecified, creating uncertainty about exact reproducibility conditions

## Confidence
- Model performance differences (High): The consistent ranking of Claude above GPT-4 and Gemini across multiple metrics is well-supported by the data.
- Creativity-accuracy trade-off (Medium): While observed patterns are clear, the mechanism behind Claude's superior performance requires further investigation.
- Question type effects (Medium): The wildcard question findings are intriguing but based on limited examples.

## Next Checks
1. Test prompt variations systematically with controlled A/B experiments to isolate the impact of different creative prompt formulations on each model's performance.
2. Expand evaluation to multiple science centers with diverse question sets to assess generalizability across different educational contexts and subject areas.
3. Conduct longitudinal assessment to evaluate model performance consistency over time, particularly as LLMs receive updates that might affect their balance of creativity and accuracy.