---
ver: rpa2
title: 'Get More with LESS: Synthesizing Recurrence with KV Cache Compression for
  Efficient LLM Inference'
arxiv_id: '2402.09398'
source_url: https://arxiv.org/abs/2402.09398
tags:
- cache
- less
- arxiv
- sparse
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the memory bottleneck of large language models
  (LLMs) caused by storing key-value (KV) pairs during inference. Existing methods
  reduce memory by pruning less important KV pairs, but this can hurt performance
  on tasks requiring information from many tokens.
---

# Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference

## Quick Facts
- arXiv ID: 2402.09398
- Source URL: https://arxiv.org/abs/2402.09398
- Authors: Harry Dong; Xinyu Yang; Zhenyu Zhang; Zhangyang Wang; Yuejie Chi; Beidi Chen
- Reference count: 15
- One-line primary result: LESS combines sparse KV cache policies with low-rank approximation to recover performance while maintaining memory efficiency

## Executive Summary
This paper addresses the memory bottleneck in LLM inference caused by storing key-value pairs for attention computation. The authors propose LESS, which integrates a small low-rank cache with existing sparse KV cache policies. By approximating the residual between full and sparse attention outputs with a low-rank matrix, LESS can recover information discarded by sparse policies while maintaining their memory efficiency benefits. The method requires only small modifications to the model and can be trained efficiently.

## Method Summary
LESS combines existing sparse KV cache policies (like H2O or Λ-masking) with a constant-sized low-rank cache that stores information discarded by the sparse policy. The key insight is that the residual between full attention outputs and sparse attention outputs is low-rank, allowing efficient approximation. The method adds small MLPs at each attention layer to compute kernel functions that capture the low-rank approximation. During inference, tokens discarded by the sparse policy are stored in the low-rank cache and can be "recovered" in future decoding steps. The approach is trained layer-wise on collected attention statistics and requires no changes to the original model weights.

## Key Results
- Reduces word perplexity gap on WikiText by over 20% compared to H2O alone
- Improves summarization ROUGE scores by over 40% compared to sparse policies
- Reduces latency and increases throughput compared to full caching
- Requires only small modifications to the model with layer-wise independent training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse policies discard KV pairs that are currently unimportant but may be needed later, creating gaps in attention maps that hurt performance.
- Mechanism: Sparse policies like H2O or Λ-masking keep only a subset of KV pairs deemed most important, which irreversibly removes tokens that might become relevant in future decoding steps.
- Core assumption: Tokens considered unimportant now will remain unimportant later is a faulty conjecture for tasks that deviate from this pattern.
- Evidence anchors:
  - [abstract] "they can have limited success in tasks that require recollecting a majority of previous tokens."
  - [section 2.1] "these methods are inevitably and irrecoverably discarding KV pairs deemed, in one way or another, less important than others, leading to gaps in attention maps"
  - [corpus] Weak - corpus shows similar KV cache compression papers but doesn't directly support this specific mechanism
- Break condition: If sparse policy becomes perfect at predicting which tokens will never be needed again, or if tasks always follow predictable importance patterns.

### Mechanism 2
- Claim: The residual between full and sparse attention outputs is low-rank, allowing efficient approximation.
- Mechanism: When comparing full attention outputs to sparse attention outputs, the difference (residual) has much lower rank than the original attention, making it possible to approximate with a small number of parameters.
- Core assumption: Attention residuals are in fact low-rank — more so than A.
- Evidence anchors:
  - [section 2.3] "with some investigation into the residual between full and sparse attention outputs, a better strategy emerges"
  - [section 2.2] "if ϕ(q)ψ(k)⊤ = eqk⊤/ √ D for all q, k, then the result would be the original attention probabilities"
  - [section 3] "the residuals∆A are in fact low-rank — more so thanA — based on Figure 3"
- Break condition: If attention residuals become high-rank for certain tasks or models, or if low-rank approximation becomes insufficient to capture necessary information.

### Mechanism 3
- Claim: LESS can recover significant performance by approximating residuals with a constant-sized low-rank cache.
- Mechanism: LESS accumulates information from discarded KV pairs into a constant-sized low-rank state (Ht and zt), allowing tokens to be "recovered" later during attention computation.
- Core assumption: A constant-sized low-rank cache can effectively store and later retrieve information from discarded tokens.
- Evidence anchors:
  - [abstract] "a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps"
  - [section 3.1] "the only modifications to LLMs will be the addition of tiny multilayer perceptions (MLPs) at each attention layer"
  - [section 4.1] "LESS reduces the word perplexities on WikiText and PG-19 by over 20% from H2O alone, relative to the full cache performance"
- Break condition: If constant-sized cache becomes too small to capture necessary information, or if low-rank approximation becomes insufficient for task requirements.

## Foundational Learning

- Concept: Key-Value (KV) cache mechanism in transformers
  - Why needed here: Understanding how KV cache stores previous keys and values to avoid recomputation is fundamental to grasping the memory bottleneck being addressed
  - Quick check question: What is the primary tradeoff of using KV cache in transformer inference?

- Concept: Attention mechanism and sparse attention
  - Why needed here: The method relies on understanding how attention works and how sparse attention policies selectively keep/discard tokens
  - Quick check question: How does sparse attention differ from full attention in terms of token selection?

- Concept: Low-rank matrix approximation
  - Why needed here: The core innovation uses low-rank approximation to efficiently represent residuals between full and sparse attention
  - Quick check question: What property of matrices makes low-rank approximation computationally efficient?

## Architecture Onboarding

- Component map:
  - Sparse KV cache policy (H2O, Λ-masking, etc.) -> Low-rank cache state (Ht, zt matrices) -> Kernel functions (ϕ, ψ) -> Original transformer model with frozen weights -> Small MLPs added at each attention layer

- Critical path:
  1. Token generation with sparse KV cache
  2. Residual computation between full and sparse attention
  3. Low-rank cache update with discarded information
  4. Next token generation using both sparse cache and low-rank cache

- Design tradeoffs:
  - Small low-rank cache (R=8) vs larger cache for better approximation
  - Training complexity (layer-wise independent training vs end-to-end)
  - Performance vs memory savings compared to full caching
  - Generalizability across different sparse policies and sparsity levels

- Failure signatures:
  - Performance degradation when sequence length increases significantly
  - Poor transfer when training and test sparsity levels mismatch
  - Memory inefficiency if low-rank cache becomes too large
  - Computational overhead from additional kernel computations

- First 3 experiments:
  1. Implement baseline sparse KV cache (H2O) and measure performance gap from full cache
  2. Add simple low-rank approximation without training to observe baseline improvement
  3. Train kernel functions on collected attention layer inputs/outputs and measure performance gains

## Open Questions the Paper Calls Out
None

## Limitations
- Uncertainty about generalization to domains significantly different from training data
- Potential insufficiency of constant-sized low-rank cache for tasks requiring long-range dependencies
- Limited scalability analysis for extremely long sequences beyond evaluated ranges

## Confidence

**High Confidence (Level 3/3)**: The core mechanism of combining sparse policies with low-rank approximation is technically sound and well-supported by the theoretical framework and experimental results.

**Medium Confidence (Level 2/3)**: The empirical results showing performance improvements over baseline sparse policies are convincing for the specific models and tasks tested.

**Low Confidence (Level 1/3)**: The scalability analysis and real-world deployment considerations are limited, with insufficient exploration of edge cases.

## Next Checks

1. **Cross-Domain Transfer Evaluation**: Train the low-rank cache components on one domain (e.g., WikiText) and evaluate performance on substantially different domains (e.g., code generation, mathematical reasoning) to assess robustness and transfer capability across diverse attention patterns.

2. **Architecture-Agnostic Testing**: Implement LESS on transformer variants beyond standard decoder-only models, including models with different attention mechanisms (linear attention, gated attention) and architectural modifications to validate the method's applicability across the broader transformer ecosystem.

3. **Long Sequence Stress Test**: Evaluate performance on sequences significantly longer than those used in the paper (e.g., >8K tokens) to identify potential failure modes when the low-rank approximation becomes insufficient to capture the full complexity of attention patterns in extended contexts.