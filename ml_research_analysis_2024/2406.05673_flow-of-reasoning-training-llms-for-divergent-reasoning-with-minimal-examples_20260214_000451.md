---
ver: rpa2
title: 'Flow of Reasoning: Training LLMs for Divergent Reasoning with Minimal Examples'
arxiv_id: '2406.05673'
source_url: https://arxiv.org/abs/2406.05673
tags:
- reasoning
- arxiv
- left
- training
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Flow of Reasoning (FOR), a novel method to train
  LLMs for multi-step reasoning that discovers diverse valid solutions. FOR formulates
  reasoning as a Markovian flow on a DAG-structured reasoning graph and uses GFlowNet
  objectives to sample trajectories with probabilities proportional to their reward.
---

# Flow of Reasoning: Training LLMs for Divergent Reasoning with Minimal Examples

## Quick Facts
- arXiv ID: 2406.05673
- Source URL: https://arxiv.org/abs/2406.05673
- Reference count: 40
- Primary result: FOR outperforms SFT, PPO, and prompting methods on 6 reasoning tasks with only 15 training examples, achieving 20%-85% improvements in accuracy, diversity, and creativity

## Executive Summary
Flow of Reasoning (FOR) is a novel method that trains large language models to perform multi-step reasoning by discovering diverse valid solutions. The approach formulates reasoning as a Markovian flow on a DAG-structured reasoning graph and uses GFlowNet objectives to sample trajectories with probabilities proportional to their reward. By incorporating local search and a replay buffer for efficient exploration, FOR achieves superior performance on six reasoning benchmarks while requiring only 15 training examples per task.

## Method Summary
FOR trains LLMs to reason by modeling the process as a Markovian flow on a DAG-structured reasoning graph. The method uses trajectory balance constraints to ensure the forward policy samples trajectories in proportion to their rewards, enabling diversity-seeking behavior. It combines on-policy and off-policy strategies with a replay buffer storing high-reward trajectories and a local search process that reconstructs partial trajectories. The approach amortizes inference into training, avoiding expensive search at inference time while achieving data efficiency and improved generalization through exposure to diverse reasoning paths.

## Key Results
- FOR achieves 20%-85% improvements over SFT, PPO, and prompting methods across six reasoning tasks
- With only 15 training examples, FOR matches or exceeds the performance of methods trained on 100× more data
- FOR demonstrates superior diversity, discovering an average of 3-4 unique correct solutions per problem versus 1-2 for baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FOR improves reasoning diversity by sampling trajectories with probabilities proportional to their reward, rather than maximizing only the highest reward.
- Mechanism: The flow-based formulation allows the model to explore multiple reasoning paths by defining a trajectory flow function F(τ) and using trajectory balance constraints to ensure that the forward policy PF samples trajectories in proportion to their rewards.
- Core assumption: The reasoning process can be modeled as a Markovian flow on a DAG, where each reasoning step corresponds to an edge in the graph, and the reward structure is sufficiently informative to guide exploration.

### Mechanism 2
- Claim: FOR achieves data efficiency by leveraging a replay buffer and local search to augment training trajectories.
- Mechanism: The method combines on-policy and off-policy strategies, using a replay buffer to store high-reward trajectories and a local search process to reconstruct partial trajectories, enhancing exploration without expensive LLM inference.
- Core assumption: High-reward trajectories contain informative patterns that can be generalized to unseen reasoning problems, and partial reconstruction can effectively explore the trajectory space.

### Mechanism 3
- Claim: FOR improves generalization by training on diverse reasoning trajectories rather than ground-truth solutions alone.
- Mechanism: By sampling trajectories from the learned forward policy PF, the model is exposed to a broader distribution of reasoning paths, increasing its ability to generalize to unseen problems.
- Core assumption: The learned policy PF captures the underlying structure of the reasoning space, and exposure to diverse trajectories improves the model's ability to handle variations in problem instances.

## Foundational Learning

- Concept: Markovian flow on DAGs
  - Why needed here: The reasoning process is modeled as a flow through a DAG, where each state transition corresponds to an edge, and the flow distribution determines the probability of sampling trajectories.
  - Quick check question: What is the relationship between the flow function F(τ) and the probability distribution P(τ) over trajectories?

- Concept: Trajectory Balance in GFlowNets
  - Why needed here: The training objective uses trajectory balance to ensure that the learned forward policy samples trajectories in proportion to their rewards, enabling diversity-seeking behavior.
  - Quick check question: How does the trajectory balance constraint differ from detailed balance in GFlowNets?

- Concept: Amortized inference vs. search-based reasoning
  - Why needed here: FOR amortizes the reasoning process into training, avoiding the need for expensive search at inference time, unlike methods like ToT or RAP.
  - Quick check question: What is the computational advantage of amortizing inference into training compared to search-based methods?

## Architecture Onboarding

- Component map: LLM policy PF -> Transition function T -> Next state st+1 -> Terminal state sn -> Reward R -> Policy update
- Critical path: Initial state s0 → LLM action at → Transition T → Next state st+1 → Terminal state sn → Reward R → Policy update
- Design tradeoffs:
  - On-policy vs. off-policy exploration: On-policy exploration ensures that the model learns from its current policy, but may be less efficient than leveraging past high-reward trajectories in the replay buffer.
  - Reward weighting λ: Balancing the augmented reward with the success reward is critical; too much weight on the augmented reward may harm accuracy, while too little may reduce diversity.
  - Local search depth K: Deeper reconstruction may improve diversity but increases computational cost and risk of generating invalid trajectories.
- Failure signatures:
  - Low diversity: The replay buffer may be dominated by similar trajectories, or the reward function may not sufficiently differentiate between paths.
  - Poor accuracy: The augmented reward may be misaligned with the true success reward, or the local search may introduce noise into the training process.
  - High computational cost: On-policy sampling with LLMs may become prohibitively expensive for complex reasoning tasks.
- First 3 experiments:
  1. Ablation of the replay buffer: Remove the replay buffer and compare accuracy, diversity, and training time to assess its impact.
  2. Sensitivity to reward weight λ: Vary λ across a range and measure the tradeoff between accuracy and diversity.
  3. Comparison with SFT on varying dataset sizes: Train both methods on increasing amounts of data and compare their performance to assess data efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FOR's performance scale with the number of training examples, particularly in relation to the complexity of the reasoning task?
- Basis in paper: The paper mentions that FOR is data-efficient, but does not provide a comprehensive analysis of its performance across a wide range of training set sizes for different tasks.
- Why unresolved: The paper only provides a limited analysis of the data-efficiency of FOR, focusing on a specific number of training examples for each task.
- What evidence would resolve it: A comprehensive study varying the number of training examples for each task and analyzing the corresponding performance of FOR would provide insights into its data-efficiency and scalability.

### Open Question 2
- Question: How does FOR's performance compare to other methods when dealing with long-range reasoning tasks that require a large number of steps?
- Basis in paper: The paper mentions that FOR is designed for multi-step reasoning, but does not provide a detailed analysis of its performance on tasks that require a large number of steps.
- Why unresolved: The paper focuses on tasks that require a relatively small number of steps, and does not explore the limitations of FOR when dealing with long-range reasoning tasks.
- What evidence would resolve it: A study comparing the performance of FOR with other methods on tasks that require a large number of steps would provide insights into its limitations and potential areas for improvement.

### Open Question 3
- Question: How does FOR's performance change when dealing with tasks that have a large action space or a complex state representation?
- Basis in paper: The paper mentions that FOR is designed for reasoning tasks, but does not provide a detailed analysis of its performance on tasks with a large action space or a complex state representation.
- Why unresolved: The paper focuses on tasks with a relatively small action space and a simple state representation, and does not explore the limitations of FOR when dealing with tasks that have a large action space or a complex state representation.
- What evidence would resolve it: A study comparing the performance of FOR with other methods on tasks that have a large action space or a complex state representation would provide insights into its limitations and potential areas for improvement.

## Limitations
- Evaluation relies on only 15 training examples per task, raising questions about scalability to larger datasets
- Method's dependence on reward function quality is not extensively validated
- Computational cost comparison with baseline methods is not thoroughly analyzed

## Confidence
- **High confidence**: The core claim that FOR achieves higher diversity than SFT and PPO is well-supported by quantitative metrics (average 20-85% improvement in diversity across tasks).
- **Medium confidence**: The accuracy improvements over baselines are demonstrated but could benefit from additional ablation studies on reward weighting and local search parameters.
- **Low confidence**: The generalization claims beyond the six evaluated tasks, and the scalability to larger training datasets, remain untested.

## Next Checks
1. **Ablation of local search parameters**: Systematically vary K (number of destroyed steps in local search) and measure the impact on both accuracy and diversity metrics to identify optimal reconstruction depth.
2. **Reward function sensitivity analysis**: Test FOR with modified reward functions (e.g., removing augmented rewards, changing reward weights λ) to quantify the method's dependence on reward shaping quality.
3. **Scalability validation**: Evaluate FOR on progressively larger training datasets (e.g., 15, 50, 100, 500 examples) to characterize the method's data efficiency curve and compare against SFT's scaling behavior.