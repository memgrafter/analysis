---
ver: rpa2
title: Selective Test-Time Adaptation for Unsupervised Anomaly Detection using Neural
  Implicit Representations
arxiv_id: '2410.03306'
source_url: https://arxiv.org/abs/2410.03306
tags:
- adaptation
- source
- target
- test-time
- anomaly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STA-AD, a selective test-time adaptation
  framework for unsupervised anomaly detection (AD) in medical imaging. The core challenge
  addressed is adapting AD models to new clinical settings with domain shifts (e.g.,
  different scanners, patient demographics) without compromising their ability to
  detect anomalies.
---

# Selective Test-Time Adaptation for Unsupervised Anomaly Detection using Neural Implicit Representations

## Quick Facts
- arXiv ID: 2410.03306
- Source URL: https://arxiv.org/abs/2410.03306
- Authors: Sameer Ambekar; Julia A. Schnabel; Cosmin I. Bercea
- Reference count: 36
- One-line primary result: STA-AD achieves up to 87% increase in true positives and 72% improvement in F1 scores for anomaly detection by selectively adapting to domain shifts while preserving anomaly detection capability.

## Executive Summary
This paper introduces STA-AD, a selective test-time adaptation framework for unsupervised anomaly detection in medical imaging. The method addresses the challenge of adapting AD models to new clinical settings with domain shifts (e.g., different scanners, patient demographics) without compromising their ability to detect anomalies. STA-AD employs neural implicit representations to selectively adapt source-trained models to target distributions using a lightweight multi-layer perceptron, leveraging content from source predictions and style from target images. This approach avoids full adaptation to pathologies while compensating for non-pathological distribution shifts.

## Method Summary
STA-AD is a test-time adaptation framework that uses neural implicit representations to selectively adapt source-trained anomaly detection models to target domains. The method treats source model predictions as "content" (healthy anatomical structure) and target images as "style" (scanner and demographic characteristics). A lightweight 16-layer MLP learns a mapping from random latent vectors interpolated between content and style to generate adapted images. The adaptation is optimized using content and style losses computed through a pre-trained VGG model, with entropy minimization as an additional objective. The framework is model-agnostic and can adapt any reconstruction-based AD method without altering the source-trained model.

## Key Results
- STA-AD achieves up to 87% increase in true positives and 72% improvement in F1 scores for anomaly detection tasks
- Entropy errors are reduced by nearly half, indicating more precise adaptation
- The parameter-efficient variant uses only 1% of parameters while maintaining competitive performance
- Method successfully handles domain shifts across multiple scanner types and demographic variations

## Why This Works (Mechanism)

### Mechanism 1
Neural implicit representations enable selective adaptation to scanner and demographic shifts while excluding pathological shifts. The method uses a lightweight MLP to learn a mapping from random latent vectors interpolated between source model predictions (content) and target image features (style) to generate an adapted image. By optimizing this mapping with content and style losses, it adjusts for domain-specific variations without learning anomalies. This works because source model predictions encode healthy anatomical structure while target images provide scanner and demographic characteristics, and these can be effectively disentangled.

### Mechanism 2
Entropy minimization during adaptation improves anomaly detection performance. The adaptation process minimizes the entropy of predictions on target data by optimizing deep semantic features through a pre-trained VGG model. Lower entropy indicates more confident and accurate predictions. This follows the principle that entropy minimization facilitates adaptation, and the method achieves nearly halved entropy error, resulting in significantly more precise adaptation.

### Mechanism 3
Test-time adaptation with a single sample can effectively compensate for multi-modal distribution shifts. The framework adapts to each test image individually using its own style content, allowing it to handle different scanner types, acquisition protocols, and demographic variations without requiring multiple samples or retraining. This works because distribution shifts can be characterized and compensated by analyzing single samples in combination with source model predictions, enabling model-agnostic adaptation.

## Foundational Learning

- **Neural Implicit Representations**: Why needed here: They provide a compact, differentiable way to parameterize and adapt images at test-time without modifying the source model, crucial for handling domain shifts in anomaly detection. Quick check question: How does the interpolation between content and style latent vectors in neural implicit representations enable selective adaptation?

- **Test-Time Adaptation**: Why needed here: Traditional anomaly detection models fail when deployed to new clinical environments due to domain shifts, and test-time adaptation allows optimization for these unseen domains without requiring labeled data. Quick check question: Why is test-time adaptation particularly challenging for anomaly detection compared to standard classification tasks?

- **Content-Style Separation**: Why needed here: To adapt to scanner and demographic shifts while preserving the ability to detect anomalies, the method must separate normal anatomical content from domain-specific style characteristics. Quick check question: What assumptions about the source model predictions make it possible to use them as "content" while treating target images as "style"?

## Architecture Onboarding

- **Component map**: Source AD model (frozen) → Source predictions (content) + Target image (style) → Latent vector interpolation → MLP (ϕt) → Adapted image → Anomaly detection
- **Critical path**: Target image → Source model prediction → Latent interpolation → MLP optimization → Adapted image → Anomaly map
- **Design tradeoffs**: Single-sample adaptation vs. batch adaptation (simplicity and efficiency vs. potentially better statistical estimation), content-style separation vs. holistic adaptation (selectivity vs. completeness)
- **Failure signatures**: Increased false positives on healthy samples, failure to detect known anomalies, high entropy in predictions after adaptation, significant artifacts in adapted images
- **First 3 experiments**:
  1. Test adaptation on healthy samples from a different domain (e.g., IXI → FastMRI) and measure SSIM, MAE, and LPIPS compared to source model
  2. Evaluate anomaly detection performance on specific conditions (e.g., enlarged ventricles) with and without adaptation using both RA and DDPM backbones
  3. Compare entropy values before and after adaptation on both healthy and pathological samples to verify entropy minimization

## Open Questions the Paper Calls Out

### Open Question 1
How does the STA-AD method perform on anomaly detection tasks outside of brain MRI, such as in other organs or modalities like CT or ultrasound? The paper focuses exclusively on brain MRI datasets, leaving the generalizability of the approach to other anatomical regions or imaging modalities unexplored.

### Open Question 2
What is the impact of varying the number of layers or width of the MLP used in neural implicit representations on the performance and efficiency of STA-AD? The paper uses a 16-layer, 128-unit MLP and mentions a parameter-efficient variant but does not explore the effects of different architectures on performance.

### Open Question 3
How does STA-AD handle multi-class anomaly detection, where multiple distinct pathologies are present in a single image? The paper evaluates STA-AD on datasets with various single anomalies but does not address scenarios with multiple concurrent pathologies.

### Open Question 4
What are the long-term effects of selective adaptation on the model's ability to generalize to future unseen domains or evolving scanner technologies? The paper demonstrates immediate improvements in detection accuracy but does not discuss the sustainability of these gains over time or with evolving technology.

## Limitations
- Effectiveness of single-sample adaptation for complex multi-modal distribution shifts remains uncertain
- Assumption that source model predictions contain only healthy anatomical content is critical but not thoroughly validated
- Generalization to other medical imaging modalities and anomaly types beyond brain MRI remains unproven

## Confidence
- **Medium-High**: Claims about performance improvements (up to 87% TP increase, 72% F1 improvement) on validated brain MRI datasets
- **Medium**: Claims about computational efficiency (1% parameters, single-sample adaptation)
- **Low-Medium**: Claims about the theoretical mechanism of content-style separation preventing pathological learning
- **Medium**: Claims about entropy minimization improving detection quality

## Next Checks
1. **Pathology contamination test**: Verify that source model predictions for pathological samples do not contain anomaly signals that could contaminate the content image, potentially causing the adaptation to learn pathologies rather than exclude them.

2. **Cross-modal generalization**: Apply STA-AD to a different medical imaging modality (e.g., chest X-ray or retinal OCT) with domain shifts to test whether the content-style separation mechanism generalizes beyond brain MRI.

3. **Ablation of entropy minimization**: Compare STA-AD performance with and without the entropy loss term to determine whether entropy reduction actually improves anomaly detection or is merely a correlated optimization target.