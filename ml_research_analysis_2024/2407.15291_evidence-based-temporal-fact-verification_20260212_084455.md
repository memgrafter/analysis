---
ver: rpa2
title: Evidence-Based Temporal Fact Verification
arxiv_id: '2407.15291'
source_url: https://arxiv.org/abs/2407.15291
tags:
- temporal
- claim
- evidence
- sentences
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses automated fact verification for temporal claims,
  which require considering time-related information. The authors propose TACV, an
  end-to-end framework that extracts events from claims and evidence sentences, associates
  them with temporal expressions, and learns temporal-aware representations using
  a graph attention network.
---

# Evidence-Based Temporal Fact Verification

## Quick Facts
- **arXiv ID**: 2407.15291
- **Source URL**: https://arxiv.org/abs/2407.15291
- **Reference count**: 21
- **Primary result**: Proposes TACV framework achieving up to 8% improvement in label accuracy and FEVER score for temporal fact verification

## Executive Summary
This paper addresses the challenge of automated fact verification for temporal claims that require time-related information. The authors introduce TACV, an end-to-end framework that extracts events from claims and evidence, associates them with temporal expressions, and uses graph attention networks to learn temporal-aware representations. The system then retrieves top-k relevant evidence and employs a large language model for temporal reasoning to determine claim veracity. Experiments demonstrate significant performance improvements over state-of-the-art methods on both newly curated temporal fact verification datasets (T-FEVER, T-FEVEROUS) and existing benchmarks (FEVER, FEVEROUS, LIAR).

## Method Summary
TACV is an end-to-end framework that extracts events and temporal expressions from claims and evidence using Semantic Role Labeling (SRL). It encodes temporal-aware representations using BERT with positional encoding of dates, constructs event-level graphs, and applies Graph Attention Networks (GAT) to propagate temporal information between event pairs. The system retrieves top-k relevant evidence sentences and uses text-davinci-003 LLM to perform final temporal reasoning. The framework classifies claims as SUPPORT, REFUTE, or NOT ENOUGH INFO based on the LLM's analysis of temporal relationships between claim events and evidence.

## Key Results
- TACV achieves up to 8% improvement in label accuracy and FEVER score compared to state-of-the-art methods
- Significant performance gains on both new temporal datasets (T-FEVER, T-FEVEROUS) and existing benchmarks (FEVER, FEVEROUS, LIAR)
- Case studies demonstrate TACV's ability to retrieve relevant evidence for verifying complex temporal claims

## Why This Works (Mechanism)

### Mechanism 1
Event-level decomposition with temporal arguments enables accurate alignment between claims and evidence. The system extracts predicates (events) and their temporal arguments from both claims and evidence, then constructs a temporal-aware representation that encodes both semantic and chronological relationships between these events.

### Mechanism 2
Graph attention networks propagate temporal information between event pairs to enhance relevance scoring. A fully-connected graph is built where nodes represent <claim event, sentence event> pairs with temporal-aware representations. GAT propagates information between nodes using token-level and sentence-level attention weights, updating representations to reflect both semantic and temporal relationships.

### Mechanism 3
Large language models can effectively perform temporal reasoning when provided with context from top-k relevant evidence sentences. The system retrieves top-k evidence sentences based on temporal-aware relevance scores and uses them as context for an LLM (text-davinci-003) to determine whether each claim event is supported, refuted, or lacks sufficient information.

## Foundational Learning

- **Concept**: Semantic Role Labeling (SRL)
  - Why needed here: SRL is essential for extracting events and their arguments from text, which forms the foundation for the event-level temporal reasoning approach.
  - Quick check question: Can you explain how SRL identifies predicates and their arguments in a sentence, and why this is crucial for the temporal fact verification system?

- **Concept**: Graph Attention Networks (GAT)
  - Why needed here: GAT is used to propagate information between event pairs, capturing both semantic and temporal relationships in the evidence graph.
  - Quick check question: How does GAT differ from traditional graph neural networks, and why is it particularly suited for capturing attention-based relationships between event pairs?

- **Concept**: Temporal expression handling and encoding
  - Why needed here: The system needs to encode temporal expressions of varying granularities (dates, durations, orderings) into a unified representation that preserves chronological relationships.
  - Quick check question: Can you describe how the system handles temporal expressions of different granularities and converts them into positional encodings that satisfy triangular inequality and distance preservation properties?

## Architecture Onboarding

- **Component map**: Event Extraction (SRL) → Temporal-aware Encoder (BERT + Transformer) → Graph Attention Network → LLM Reasoning → Label Aggregation
  Input: Temporal claim → Output: Support/Refute/Not Enough Info label
  Key data flows: Claim events ↔ Evidence events → Temporal representations → Relevance scores → LLM context

- **Critical path**: Claim → Event Extraction → Temporal-aware Representation → Relevance Scoring → Top-k Retrieval → LLM Reasoning → Label Aggregation
  The most time-consuming steps are typically event extraction and LLM inference, with the graph attention network being computationally intensive for large evidence sets.

- **Design tradeoffs**: 
  - Event-level vs. sentence-level processing: Event-level provides finer granularity for temporal reasoning but increases complexity
  - GAT vs. simpler aggregation: GAT captures complex relationships but is more computationally expensive
  - Fixed k vs. adaptive retrieval: Fixed k simplifies implementation but may not be optimal for all claims

- **Failure signatures**:
  - Poor relevance scores: May indicate issues with temporal-aware representation encoding or event extraction
  - LLM consistently outputting "Not Enough Info": Could suggest insufficient context retrieval or LLM limitations with temporal reasoning
  - Performance degradation on multi-event claims: Might indicate limitations in the graph attention network's ability to handle complex temporal relationships

- **First 3 experiments**:
  1. Test event extraction accuracy on a sample of claims to verify correct identification of predicates and temporal arguments
  2. Validate temporal-aware representation encoding by checking if similar temporal events have similar representations
  3. Measure GAT propagation effectiveness by comparing relevance scores before and after graph attention processing on a small evidence graph

## Open Questions the Paper Calls Out

### Open Question 1
How does the TACV framework perform on claims with implicit temporal references that require complex temporal reasoning beyond ordering and duration?
The paper mentions TACV struggles with "highly convoluted sentences with implicit temporal references" and suggests this is a direction for future research. Only explicit temporal expressions were evaluated, with no testing on claims requiring inference of implicit temporal relationships.

### Open Question 2
What is the impact of using different large language models (LLMs) on TACV's performance for temporal reasoning?
The paper mentions experimenting with GPT4-turbo and shows improvement, but doesn't extensively compare different LLM models. Only one alternative LLM was tested with limited comparison.

### Open Question 3
How does TACV handle claims involving multiple events with complex temporal relationships (e.g., overlapping durations, nested events)?
The paper mentions TACV handles multiple events but only provides limited analysis of performance differences between single and multiple event claims. No detailed analysis of performance on claims with complex temporal event relationships was provided.

## Limitations

- Scalability concerns with the graph attention network component when handling dense evidence graphs with many events
- Reliance on a single LLM (text-davinci-003) introduces computational costs and potential reasoning limitations for complex temporal relationships
- Evaluation focused on relatively clean Wikipedia-based evidence, which may not generalize to more diverse real-world sources

## Confidence

**High Confidence**: The core architecture design (event-level decomposition → temporal encoding → GAT-based relevance scoring → LLM reasoning) is well-specified and theoretically sound. The improvement metrics over baseline models are clearly presented and substantial.

**Medium Confidence**: The specific implementation details of the SRL-based event extraction and GAT components are less detailed, making exact replication challenging. The performance gains on FEVEROUS are impressive but may be influenced by specific dataset characteristics.

**Low Confidence**: The claim about handling complex multi-event temporal claims is primarily supported by qualitative case studies rather than comprehensive quantitative analysis across varying complexity levels.

## Next Checks

1. **Temporal Reasoning Robustness**: Test the system on claims with ambiguous or conflicting temporal expressions (e.g., "before," "around," "approximately") to evaluate how well the temporal encoding handles imprecision and uncertainty.

2. **Graph Attention Scaling**: Evaluate the GAT component's performance on evidence sets with increasing numbers of events to determine at what point the attention mechanism becomes ineffective or computationally prohibitive.

3. **LLM Context Adequacy**: Systematically vary the value of k in the top-k retrieval to determine the optimal context size for the LLM, measuring whether larger contexts improve reasoning or introduce noise that degrades performance.