---
ver: rpa2
title: 'Bellman Unbiasedness: Toward Provably Efficient Distributional Reinforcement
  Learning with General Value Function Approximation'
arxiv_id: '2407.21260'
source_url: https://arxiv.org/abs/2407.21260
tags:
- bellman
- function
- learning
- statistical
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of designing provably efficient
  algorithms for distributional reinforcement learning (DistRL) with general function
  approximation. The key challenges are the infinite-dimensionality of return distributions
  and the intractability of performing Bellman updates in an online setting with limited
  samples.
---

# Bellman Unbiasedness: Toward Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation

## Quick Facts
- arXiv ID: 2407.21260
- Source URL: https://arxiv.org/abs/2407.21260
- Reference count: 40
- Primary result: SF-LSVI achieves regret bound of $\tilde{O}(d_E H^{3/2} \sqrt{K})$ for distributional RL with general function approximation

## Executive Summary
This paper addresses the challenge of designing provably efficient algorithms for distributional reinforcement learning (DistRL) with general function approximation. The key insight is the introduction of Bellman unbiasedness, which ensures that statistical functionals of return distributions can be exactly learned from finite samples in an online setting. The authors prove that moment functionals are the unique structure satisfying both Bellman unbiasedness and Bellman closedness, making them the only proper choice for exact and unbiased updates.

The proposed SF-LSVI algorithm leverages moment functionals to perform distributional updates in a provably efficient manner. Under the assumption of Statistical Functional Bellman Completeness, SF-LSVI achieves a regret bound of $\tilde{O}(d_E H^{3/2} \sqrt{K})$, matching the tightest regret bounds of non-distributional RL algorithms while avoiding model misspecification errors and relaxing structural assumptions.

## Method Summary
The SF-LSVI algorithm maintains moment sketches of return distributions and performs least-squares regression to estimate these moments. It uses concentration inequalities derived from the martingale property of Bellman unbiasedness to construct confidence regions for the function class. The algorithm then computes optimistic Q-values by adding bonus terms based on width functions, and selects actions greedily. The key innovation is using moment functionals as statistical functionals, which are proven to be the unique structure that is both Bellman unbiased and Bellman closed.

## Key Results
- Proves that moment functionals are the unique statistical functionals satisfying both Bellman unbiasedness and Bellman closedness
- Introduces SF-LSVI algorithm achieving $\tilde{O}(d_E H^{3/2} \sqrt{K})$ regret under Statistical Functional Bellman Completeness
- Avoids model misspecification errors that plague traditional distributional Bellman Completeness approaches
- Improves upon previous DistRL approaches by relaxing structural assumptions while maintaining tight regret bounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bellman unbiasedness ensures that statistical functionals of return distributions can be exactly learned from finite samples in an online setting.
- Mechanism: The Bellman unbiasedness property guarantees that the expected sketch of the target distribution can be unbiasedly estimated using sketches from sampled distributions. This creates a martingale difference sequence, enabling concentration inequalities to establish confidence bounds.
- Core assumption: The sketch must be homogeneous of degree k, allowing it to be written as an integral of a function over k-fold products of the distribution.
- Evidence anchors:
  - [abstract]: "Among all types of statistical functionals for representing infinite-dimensional return distributions, our theoretical results demonstrate that only moment functionals can exactly capture the statistical information."
  - [section 3.2]: "Bellman unbiasedness is another natural definition, similar to Bellman closedness, which takes into account a finite number of samples for the transition."
- Break condition: If the sketch is not homogeneous of degree k, the estimation becomes biased and the martingale property is lost, preventing concentration bounds.

### Mechanism 2
- Claim: Statistical Functional Bellman Completeness (SF-BC) avoids the model misspecification errors that plague traditional distributional Bellman Completeness (distBC).
- Mechanism: Instead of requiring the entire distribution to lie in a finite-dimensional function class, SF-BC only requires that the statistical functionals (moments) of the target distribution lie in the function class. This sidesteps the impossibility of representing mixture distributions with the same number of parameters.
- Core assumption: The function class F^N can represent all moments of any return distribution up to some finite order.
- Evidence anchors:
  - [section 3.3]: "To avoid the issue of closedness under mixture, both previous studies assumed a discretized reward MDP where all outcomes of the return distribution are able to discretized into an uniform grid of finite points."
  - [table 1]: "SF-LSVI [Ours] ˜O(dEH 3/2√K) dimE(FN, ϵ) statistical functional BC none ✓ ✓"
- Break condition: If the function class cannot represent all moments of the return distribution, the algorithm will incur model misspecification errors leading to linear regret.

### Mechanism 3
- Claim: The SF-LSVI algorithm achieves near-optimal regret by combining Bellman unbiasedness with least-squares regression on moment functionals.
- Mechanism: SF-LSVI maintains confidence regions for moment functionals using concentration inequalities derived from the martingale property. It then performs least-squares regression to estimate the moments of the return distribution, which are used to construct optimistic Q-values for planning.
- Core assumption: The eluder dimension of the function class F^N controls the complexity of learning, and the width function provides sufficient bonus for optimism.
- Evidence anchors:
  - [abstract]: "SF-LSVI achieves a tight regret bound of ˜O(dEH 3/2√K) where H is the horizon, K is the number of episodes, and dE is the eluder dimension of a function class."
  - [section 5]: "Under Assumption 3.7, with probability at least 1 − δ, SF-LSVI achieves a regret bound of Reg(K) ≤ 2HdimE(F N , 1/T ) + 4H p KH log(1/δ)"
- Break condition: If the eluder dimension grows too quickly with the number of moments or the horizon, the regret bound degrades.

## Foundational Learning

- Concept: Bellman equations and operators
  - Why needed here: The paper extends classical Bellman equations from expected values to statistical functionals of distributions, requiring understanding of how Bellman operators act on distributions.
  - Quick check question: What is the difference between the distributional Bellman operator T and the classical Bellman operator T_π?

- Concept: Function approximation and eluder dimension
  - Why needed here: The algorithm uses general function approximation to represent moment functionals, and the eluder dimension measures the complexity of learning with this function class.
  - Quick check question: How does the eluder dimension of a vector-valued function class differ from that of a scalar-valued function class?

- Concept: Concentration inequalities and martingale differences
  - Why needed here: The proof relies on concentration inequalities applied to martingale difference sequences created by the Bellman unbiasedness property to establish confidence bounds.
  - Quick check question: Why does Bellman unbiasedness imply that the sequence of sampled sketches forms a martingale difference sequence?

## Architecture Onboarding

- Component map:
  - Data collection -> Moment regression -> Optimism -> Planning -> Distribution update

- Critical path:
  1. Collect transition (s_t, a_t, r_t, s_{t+1})
  2. Compute moment sketches of target distribution using binomial theorem
  3. Update regression dataset with (s_t, a_t, moment sketches)
  4. Solve moment least-squares regression
  5. Compute optimistic Q-values and select action
  6. Update moment sketches of Q and V distributions

- Design tradeoffs:
  - Using moment functionals vs. other statistical functionals: Moments are exactly learnable and unbiased but may not capture all distribution features
  - Number of moments N: More moments provide better distribution approximation but increase computational complexity and eluder dimension
  - Confidence region construction: Tighter regions enable more aggressive exploration but require stronger concentration bounds

- Failure signatures:
  - Linear regret: Indicates model misspecification in representing moment functionals
  - Unstable learning: Suggests insufficient concentration bounds or poor choice of function class
  - Poor exploration: Indicates bonus function is not providing sufficient optimism

- First 3 experiments:
  1. Implement SF-LSVI with N=2 moments (mean and variance) on a simple MDP to verify basic functionality
  2. Test on an MDP with known moment structure to verify that moment estimates converge to true values
  3. Compare regret bounds of SF-LSVI vs. non-distributional algorithms on a linear MDP to verify theoretical guarantees

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the content, some potential open questions include:

- How does SF-LSVI perform in non-episodic MDPs, such as those with discounted or average reward settings?
- What is the impact of using different statistical functionals (e.g., variance, skewness) beyond moments in SF-LSVI?
- What is the computational complexity of SF-LSVI for general function classes, particularly when computing the width function?

## Limitations

- The paper lacks empirical validation of the algorithm's performance and regret bounds in practice
- Computational complexity of maintaining confidence regions for general function classes is noted as NP-hard, but no concrete approximation methods are provided
- The eluder dimension-based regret analysis may not capture all aspects of learning complexity in practical settings
- The paper does not address numerical stability challenges that arise when computing and updating moment sketches over long horizons

## Confidence

- **High Confidence**: The theoretical framework of Bellman unbiasedness and the proof that moment functionals are the unique solution satisfying both Bellman unbiasedness and Bellman closedness.
- **Medium Confidence**: The regret bound of $\tilde{O}(d_E H^{3/2} \sqrt{K})$ for SF-LSVI under Statistical Functional Bellman Completeness.
- **Low Confidence**: The practical applicability and computational efficiency of SF-LSVI in real-world settings.

## Next Checks

1. Rigorously verify the proof that moment functionals are the unique statistical functionals satisfying both Bellman unbiasedness and Bellman closedness. Check for any hidden assumptions or gaps in the argument.

2. Implement SF-LSVI on a simple MDP with known moment structure (e.g., a linear MDP with Gaussian rewards) and verify that the estimated moments converge to the true values and that the regret scales as predicted.

3. Analyze the computational complexity of maintaining and updating moment sketches in practice, particularly for large N or continuous state spaces. Develop and test approximation methods for computing width functions when the exact computation is NP-hard.