---
ver: rpa2
title: 'CycleFormer : TSP Solver Based on Language Modeling'
arxiv_id: '2405.20042'
source_url: https://arxiv.org/abs/2405.20042
tags:
- should
- transformer
- decoder
- cycleformer
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CycleFormer is a transformer-based model for the Euclidean Traveling
  Salesman Problem (TSP). It introduces spatial positional encoding in the encoder
  to reflect 2D node coordinates, and circular positional encoding in the decoder
  to account for the cyclic nature of TSP tours.
---

# CycleFormer : TSP Solver Based on Language Modeling

## Quick Facts
- arXiv ID: 2405.20042
- Source URL: https://arxiv.org/abs/2405.20042
- Authors: Jieun Yook; Junpyo Seo; Joon Huh; Han Joon Byun; Byung-ro Moon
- Reference count: 40
- One-line primary result: CycleFormer achieves 1.10% optimality gap on TSP-500, outperforming existing transformer-based TSP solvers

## Executive Summary
CycleFormer is a transformer-based model for solving the Euclidean Traveling Salesman Problem (TSP) that introduces two key innovations: spatial positional encoding in the encoder to capture 2D node coordinates, and circular positional encoding in the decoder to account for the cyclic nature of TSP tours. The model uses the encoder's output as both input to the decoder and as dynamic embedding in the final linear layer, trained with supervised learning. CycleFormer outperforms existing transformer-based TSP solvers, achieving an optimality gap of 1.10% on TSP-500, approximately 2.8 times better than the previous state-of-the-art.

## Method Summary
CycleFormer employs a transformer architecture with spatial positional encoding in the encoder to incorporate 2D node coordinates, and circular positional encoding in the decoder to reflect tour cyclicity. The model uses the encoder output as dynamic embedding for the linear layer, eliminating the need for a fixed look-up table. Trained with supervised learning using cross-entropy loss and a visited mask to prevent revisiting nodes, CycleFormer achieves superior performance on TSP benchmarks. The model was trained for 100 epochs using AdamW optimizer with linear learning rate scaling and a LambdaLR scheduler with 400-step warmup on 8 A100 GPUs.

## Key Results
- Achieves 1.10% optimality gap on TSP-500, outperforming previous transformer-based solvers
- Outperforms existing transformer-based TSP solvers across all tested problem sizes (TSP-50, TSP-100, TSP-500)
- Demonstrates the effectiveness of spatial and circular positional encodings for TSP

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Spatial positional encoding in the encoder allows the model to incorporate the two-dimensional nature of TSP node coordinates, improving the transformer's ability to capture geometric relationships.
- **Mechanism:** The encoder uses a modified sinusoidal positional encoding that treats the 2D coordinates as a spatial grid, allowing each node's embedding to be enriched with its spatial context relative to other nodes.
- **Core assumption:** The Euclidean distances between nodes in TSP have meaningful geometric structure that can be encoded via spatial positional information.
- **Evidence anchors:**
  - [abstract] "Additionally, we added a positional encoding to the encoder tokens that reflects the two-dimensional nature of TSP, and devised a circular positional encoding for the decoder tokens that considers the cyclic properties of a tour."
  - [section] "We leverage spatial positional encoding based on 2D coordinates on the encoder's side. This method, used in vision transformers (DETR[7]), is applied to TSP for the first time to our knowledge."
- **Break condition:** If node coordinates are uniformly distributed or follow a pattern where geometric relationships are irrelevant to tour optimality, spatial encoding provides no advantage.

### Mechanism 2
- **Claim:** Circular positional encoding in the decoder makes the model invariant to tour rotation and flipping, which is essential for TSP where any node can be the starting point.
- **Mechanism:** The circular PE creates a ring-like similarity structure where adjacent nodes in the tour have high similarity scores, and nodes opposite each other on the cycle have minimal similarity.
- **Core assumption:** TSP solutions form cycles where the absolute starting position is irrelevant but the relative order of nodes matters.
- **Evidence anchors:**
  - [abstract] "and devised a circular positional encoding for the decoder tokens that considers the cyclic properties of a tour."
  - [section] "Circular PE reflects the fact that tokens of TSP are positioned in a ring... the PE is designed so that the first token and the N/2th token are the furthest apart, while the first token is closest to both the second token and the last one."
- **Break condition:** If the problem domain requires a specific starting point (non-cyclic routing), circular PE would be inappropriate.

### Mechanism 3
- **Claim:** Using the encoder output as dynamic embedding for the linear layer eliminates the need for a fixed look-up table and allows the model to adapt the embedding space to each specific TSP instance.
- **Mechanism:** Instead of learning a static linear layer, the model uses the context vectors from the encoder (which represent the graph structure) as the weight matrix, allowing the decoder output to be projected into a space that's specific to the current problem instance.
- **Core assumption:** Each TSP instance has unique characteristics that can be captured by the encoder output, and these characteristics should influence how the decoder output is interpreted.
- **Evidence anchors:**
  - [abstract] "we equated the encoder output with the decoder linear layer and directly connected the context vector of the encoder to the decoder encoding."
  - [section] "As the input to the decoder, we use the context embedding (i.e., memory) from the final output of the encoder."
- **Break condition:** If the encoder fails to capture meaningful instance-specific information, using its output as dynamic embedding provides no benefit over a learned linear layer.

## Foundational Learning

- **Concept:** Positional encoding in transformers
  - **Why needed here:** Standard transformers lack inherent sequence awareness; for TSP, we need to encode both spatial coordinates and cyclic tour properties
  - **Quick check question:** How does sinusoidal positional encoding differ from the spatial and circular encodings proposed in CycleFormer?

- **Concept:** Attention mechanisms and self-attention
  - **Why needed here:** The model relies on attention to capture relationships between nodes, both in the encoder (spatial relationships) and decoder (tour construction)
  - **Quick check question:** What role does the causal mask play in the decoder, and why is it necessary for TSP?

- **Concept:** Dynamic vs static embeddings
  - **Why needed here:** Understanding the difference between using a learned linear layer (static) versus encoder output (dynamic) is crucial for grasping the model's innovation
  - **Quick check question:** Why might a dynamic embedding that changes per instance be more suitable for TSP than a static embedding learned during training?

## Architecture Onboarding

- **Component map:** Input coordinates → encoder embedding with spatial PE → encoder layers → context vectors → decoder with circular PE → auto-regressive decoding with visited mask → dynamic embedding → next node probabilities

- **Critical path:**
  1. Input coordinates → encoder embedding with spatial PE
  2. Encoder processes through L layers → context vectors
  3. Decoder initializes with context vectors + circular PE
  4. Auto-regressive decoding with visited mask
  5. Dynamic embedding projects decoder output to next node probabilities

- **Design tradeoffs:**
  - Spatial PE vs learned coordinate embeddings: Spatial PE is interpretable and parameter-efficient but may be less flexible
  - Circular PE vs standard PE: Circular PE captures TSP-specific cyclic properties but adds complexity
  - Dynamic embedding vs learned linear layer: Dynamic embedding adapts to each instance but may be less stable

- **Failure signatures:**
  - Poor performance on small TSP instances might indicate insufficient model capacity or learning rate issues
  - Degenerating tours (visiting same node multiple times) suggests visited mask implementation errors
  - Suboptimal tours despite correct training might indicate insufficient model depth or width

- **First 3 experiments:**
  1. Train on TSP-50 with minimal model (embedding dim=64, L=2) to verify basic functionality
  2. Compare spatial PE vs no PE vs learned coordinate embeddings on TSP-50 to validate spatial encoding benefit
  3. Compare circular PE vs sinusoidal PE on TSP-50 to validate cyclic encoding benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CycleFormer's performance scale when applied to TSP instances larger than 500 nodes?
- Basis in paper: [explicit] The authors state that due to resource limitations, they were unable to conduct experiments on TSP-1000 and express belief that with sufficient optimal solutions for TSP-1000, CycleFormer could outperform SOTA like Pointerformer.
- Why unresolved: Computational resource constraints prevented testing on larger problem sizes.
- What evidence would resolve it: Experimental results on TSP instances of size 1000 or larger, comparing CycleFormer's performance to existing solvers.

### Open Question 2
- Question: What is the impact of incorporating CycleFormer into a search-based optimization pipeline, such as Monte Carlo Tree Search (MCTS), similar to AlphaGo's approach?
- Basis in paper: [explicit] The authors mention the potential of combining transformer-based solvers with other approaches, citing AlphaGo's success in combining a policy network with MCTS.
- Why unresolved: The paper focuses on CycleFormer's standalone performance and does not explore its integration with search algorithms.
- What evidence would resolve it: Experimental results demonstrating the performance of a CycleFormer-based solver combined with MCTS or other search algorithms on TSP benchmarks.

### Open Question 3
- Question: How does CycleFormer's performance compare to exact solvers like Concorde on large TSP instances?
- Basis in paper: [explicit] The authors compare CycleFormer's performance to exact solvers like Concorde on smaller TSP instances (up to 500 nodes) but do not provide a direct comparison on larger instances.
- Why unresolved: The computational complexity of exact solvers makes them impractical for very large TSP instances, and the paper does not explore approximate methods for comparing CycleFormer's performance to exact solvers on larger instances.
- What evidence would resolve it: Experimental results comparing CycleFormer's solutions to those of exact solvers on TSP instances of size 1000 or larger, potentially using approximate methods or problem instances where exact solutions are known.

## Limitations
- The effectiveness of custom positional encodings lacks extensive ablation studies comparing alternative encoding schemes
- The dynamic embedding approach's stability and generalization properties are not thoroughly analyzed
- The quadratic complexity of dynamic embeddings may limit practical applicability to larger TSP instances

## Confidence
- **High confidence** in the reported performance improvements on standard TSP benchmarks (TSP-50, TSP-100, TSP-500)
- **Medium confidence** in the claimed superiority of spatial and circular positional encodings without comprehensive ablation studies
- **Medium confidence** in the scalability of the approach to larger TSP instances given the quadratic complexity of the dynamic embedding mechanism

## Next Checks
1. **Ablation Study on Positional Encodings**: Systematically compare spatial positional encoding, circular positional encoding, and their combinations against baseline sinusoidal encoding and learned coordinate embeddings across multiple TSP sizes to isolate the contribution of each innovation.

2. **Computational Complexity Analysis**: Measure and compare the memory and runtime requirements of CycleFormer versus standard transformer architectures, particularly focusing on the overhead introduced by dynamic embeddings and custom positional encodings.

3. **Generalization Beyond Random TSP**: Evaluate CycleFormer's performance on structured TSP instances (e.g., clustered, grid, or real-world instances) to assess whether the spatial encoding provides benefits beyond uniformly random coordinate distributions.