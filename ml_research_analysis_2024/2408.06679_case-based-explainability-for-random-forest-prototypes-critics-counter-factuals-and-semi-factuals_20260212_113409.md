---
ver: rpa2
title: 'Case-based Explainability for Random Forest: Prototypes, Critics, Counter-factuals
  and Semi-factuals'
arxiv_id: '2408.06679'
source_url: https://arxiv.org/abs/2408.06679
tags:
- prototypes
- critics
- data
- distance
- counter-factuals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to case-based explainability
  (XCBR) for Random Forest (RF) models by leveraging RF proximities to identify special
  training points (prototypes, critics, semi-factuals, and counter-factuals) that
  serve as explanations. The method utilizes recently developed geometry- and accuracy-preserving
  (GAP) RF proximities to extract a more accurate distance metric from the trained
  RF model.
---

# Case-based Explainability for Random Forest: Prototypes, Critics, Counter-factuals and Semi-factuals

## Quick Facts
- arXiv ID: 2408.06679
- Source URL: https://arxiv.org/abs/2408.06679
- Authors: Gregory Yampolsky; Dhruv Desai; Mingshu Li; Stefano Pasquali; Dhagash Mehta
- Reference count: 40
- Key outcome: This paper introduces a novel approach to case-based explainability (XCBR) for Random Forest (RF) models by leveraging RF proximities to identify special training points (prototypes, critics, semi-factuals, and counter-factuals) that serve as explanations.

## Executive Summary
This paper presents a novel approach to case-based explainability for Random Forest models by leveraging geometry- and accuracy-preserving (GAP) RF proximities. The method identifies four types of special training points - prototypes, critics, semi-factuals, and counter-factuals - that serve as explanations for model predictions. By utilizing GAP proximity, the approach extracts a more accurate distance metric from trained RF models, enabling better identification of representative and contrastive examples. The framework was evaluated on multiple datasets including toy datasets (Diabetes, Blood Alcohol, German Credit, MNIST) and a financial dataset (funds data), demonstrating superior performance compared to traditional Euclidean methods and other RF proximity variants.

## Method Summary
The method integrates RF proximities into the XCBR framework by leveraging the recently developed RF Geometry and Accuracy Preserving (RF-GAP) proximities. These proximities capture the intrinsic geometry learned by the RF more precisely than previous methodologies. The approach identifies prototypes using K-medoids and HDP methods, critics through a witness function measuring distributional differences, semi-factuals as the furthest same-label points, and counter-factuals as the nearest different-label points. Evaluation is performed using F1 scores, outlier scores, diversity metrics, out-of-distribution distances, and robustness measures across multiple datasets.

## Key Results
- GAP proximity consistently outperformed traditional Euclidean methods and other RF proximity variants in identifying high-quality explanations
- Prototypes achieved strong test F1 scores and effective representation of data distributions
- The method successfully captured decision boundaries and model behavior, demonstrating the effectiveness of GAP proximity in enhancing transparency and interpretability of RF models
- Semi-factuals and counter-factuals provided meaningful "even if" and "if only" scenarios respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GAP proximity captures the geometry learned by Random Forest more accurately than traditional Euclidean distance or original RF proximity methods.
- Mechanism: GAP proximity incorporates both in-bag and out-of-bag samples with proper weighting, aligning with the fundamental partitioning the RF algorithm employs. This creates a distance metric that reflects the data structure and feature importance learned by the trees.
- Core assumption: The partitioning structure of decision trees in RF implicitly learns a meaningful geometry that can be extracted as a distance metric.
- Evidence anchors:
  - [abstract] "By utilizing a recently proposed technique to extract the distance metric learned by Random Forests (RFs), which is both geometry- and accuracy-preserving"
  - [section] "GAP proximity is designed to capture the intrinsic geometry learned by the RF more precisely than previous methodologies"
  - [corpus] Weak evidence - no direct corpus references to GAP proximity mechanism
- Break condition: If the tree structure does not meaningfully capture the data geometry, or if the weighting scheme fails to properly balance in-bag and out-of-bag samples.

### Mechanism 2
- Claim: Using GAP proximity for identifying special points (prototypes, critics, semi-factuals, counter-factuals) produces higher quality explanations than using Euclidean distance.
- Mechanism: The special points are identified based on distances computed in the GAP proximity space, which incorporates feature importance and the learned geometry. This ensures that the identified points are truly representative or contrastive in the feature space the model actually uses for prediction.
- Core assumption: The quality of case-based explanations depends critically on using a distance metric that reflects how the model actually makes decisions.
- Evidence anchors:
  - [abstract] "The method utilizes recently developed geometry- and accuracy-preserving (GAP) RF proximities to extract a more accurate distance metric from the trained RF model"
  - [section] "we propose integrating RF proximities into the XCBR framework... By leveraging the properties of the recently developed RF Geometry and Accuracy Preserving (RF-GAP) proximities"
  - [corpus] Weak evidence - no direct corpus references to comparison of explanation quality using different distance metrics
- Break condition: If the distance metric does not meaningfully affect which points are identified as special, or if the explanation quality is dominated by other factors.

### Mechanism 3
- Claim: The witness function effectively identifies critics by measuring distributional difference between prototypes and other data points.
- Mechanism: The witness function computes the difference between average proximity to all training samples and average proximity to prototypes. Points with high witness values are poorly represented by prototypes and thus serve as good critics.
- Core assumption: Critics should be points that deviate significantly from the prototype distribution, and proximity-based measures can capture this deviation.
- Evidence anchors:
  - [section] "The witness function used in our analysis is defined as: witness(ùë•) = 1/ùëõ ‚àë(prox(ùë•, ùë•ùëñ)) ‚àí 1/ùëö ‚àë(prox(ùë•, ùëßùëó))"
  - [section] "A higher witness function value indicates that the point is poorly represented by the prototypes"
  - [corpus] Weak evidence - no direct corpus references to witness function for critic identification
- Break condition: If the witness function fails to capture meaningful distributional differences, or if critics identified this way do not provide useful explanations.

## Foundational Learning

- Concept: Random Forest proximity measures
  - Why needed here: The entire approach relies on using RF proximities instead of Euclidean distance to identify special points. Understanding how these proximities are computed is fundamental to implementing and extending this work.
  - Quick check question: What is the difference between original RF proximity, out-of-bag proximity, and GAP proximity in terms of which samples they include in their calculations?

- Concept: Case-based reasoning in XAI
  - Why needed here: The paper builds on XCBR methods, which use actual training examples as explanations. Understanding the different types of explanantia (prototypes, critics, semi-factuals, counter-factuals) and their purposes is essential.
  - Quick check question: How does a semi-factual differ from a counter-factual in terms of what aspect of the model's behavior they explain?

- Concept: Distance-based evaluation metrics
  - Why needed here: The paper evaluates explanations using various distance-based metrics. Understanding how these metrics work and what they measure is crucial for interpreting results and implementing evaluations.
  - Quick check question: Why might an explanation with high outlier score be considered better for a critic but worse for a prototype?

## Architecture Onboarding

- Component map: Data preprocessing ‚Üí Random Forest training ‚Üí GAP proximity computation ‚Üí Special point identification ‚Üí Evaluation
- Critical path: Random Forest training ‚Üí GAP proximity computation ‚Üí Special point identification (especially prototypes and semi-/counter-factuals)
- Design tradeoffs:
  - GAP proximity vs. original RF proximity: GAP provides better geometry preservation but may be computationally more expensive
  - Number of prototypes: More prototypes provide better coverage but increase computational cost and may reduce diversity
  - Evaluation metrics: Some metrics may be redundant or correlated, requiring careful selection
- Failure signatures:
  - If prototypes have low F1 scores on nearest-prototype test, the proximity metric may not be capturing meaningful structure
  - If critics have similar outlier scores to prototypes, the witness function may not be effective
  - If semi-factuals and counter-factuals have similar distances to queries, the distinction between them may be unclear
- First 3 experiments:
  1. Implement GAP proximity computation and verify it produces different results from Euclidean distance on a simple dataset
  2. Implement prototype identification using both K-medoids and HDP methods, compare their F1 scores
  3. Generate semi-factuals and counter-factuals for a simple binary classification problem and visualize them to verify they behave as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GAP proximity-based XCBR compare to other tree-based ensemble methods like gradient boosting machines or XGBoost when explaining predictions?
- Basis in paper: [inferred] The paper demonstrates GAP proximity's effectiveness for RF models but does not explore its applicability to other tree-based ensembles, suggesting potential for broader application.
- Why unresolved: The study focuses exclusively on Random Forest models and does not investigate whether GAP proximity-derived distances would yield similar benefits when applied to other tree-based ensemble algorithms.
- What evidence would resolve it: Comparative experiments applying GAP proximity methodology to gradient boosting, XGBoost, or other tree-based ensembles across multiple datasets, measuring explanation quality using the same evaluation metrics.

### Open Question 2
- Question: What is the optimal number of prototypes and critics for different types of datasets, and how does this affect explanation quality and computational efficiency?
- Basis in paper: [explicit] The paper mentions that "the optimal number of prototypes was determined separately for each method, distance metric, and dataset" but does not provide a systematic framework for determining this optimal number across different data types.
- Why unresolved: While the paper tunes prototype numbers for specific cases, it does not establish general principles or guidelines for selecting the appropriate number of prototypes and critics based on dataset characteristics.
- What evidence would resolve it: Systematic analysis across diverse datasets identifying patterns in optimal prototype/critic counts relative to dataset size, dimensionality, class distribution, and noise levels, along with corresponding explanation quality metrics.

### Open Question 3
- Question: How do feature importance and feature space dynamics affect the searching for and quality of explanantia, particularly for semi-factuals and counter-factuals?
- Basis in paper: [inferred] The paper acknowledges that "future research directions include investigating the impact of feature space dynamics, such as feature importance, on the searching for explanantia" but does not explore this relationship.
- Why unresolved: The current methodology treats all features equally in distance calculations and does not account for how varying feature importance or correlations might influence the selection and quality of explanations.
- What evidence would resolve it: Experiments varying feature importance weighting schemes in the proximity calculations and measuring resulting changes in explanation quality metrics, particularly sparsity and robustness of semi-factuals and counter-factuals.

## Limitations

- The exact implementation details of GAP proximity computation are not fully specified in the paper, requiring external reference to understand the complete methodology
- Parameter settings for K-medoids and HDP methods are not provided, which may affect reproducibility
- The study focuses exclusively on Random Forest models without exploring applicability to other tree-based ensemble methods

## Confidence

**Major Claim Confidence:**
- GAP proximity superiority: Medium-High (supported by comparative results but dependent on correct implementation)
- Explanation quality improvements: Medium (demonstrated through multiple metrics but metric choices could be debated)
- Witness function effectiveness: Medium (theoretically sound but limited empirical validation)

## Next Checks

1. Implement GAP proximity computation and verify it produces different results from Euclidean distance on a simple dataset
2. Implement prototype identification using both K-medoids and HDP methods, compare their F1 scores
3. Generate semi-factuals and counter-factuals for a simple binary classification problem and visualize them to verify they behave as expected