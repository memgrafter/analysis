---
ver: rpa2
title: 'CLIPErase: Efficient Unlearning of Visual-Textual Associations in CLIP'
arxiv_id: '2410.23330'
source_url: https://arxiv.org/abs/2410.23330
tags:
- cliperase
- clip
- forget
- unlearning
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CLIPErase addresses the challenge of unlearning in multimodal\
  \ models like CLIP, where traditional methods struggle to selectively remove associations\
  \ between visual and textual data. The method introduces a three-module framework\u2014\
  Forgetting, Retention, and Consistency\u2014that disrupts cross-modal associations\
  \ in the forget set while preserving performance on the retain set."
---

# CLIPErase: Efficient Unlearning of Visual-Textual Associations in CLIP

## Quick Facts
- arXiv ID: 2410.23330
- Source URL: https://arxiv.org/abs/2410.23330
- Reference count: 22
- Key outcome: CLIPErase achieves near-complete unlearning (0% accuracy on forget set) while maintaining high performance on retained data (90%+ accuracy in zero-shot prediction)

## Executive Summary
CLIPErase addresses the challenge of unlearning in multimodal models like CLIP, where traditional methods struggle to selectively remove associations between visual and textual data. The method introduces a three-module framework—Forgetting, Retention, and Consistency—that disrupts cross-modal associations in the forget set while preserving performance on the retain set. Experiments on CIFAR-100, Conceptual 12M, and Flickr30K across five downstream tasks show that CLIPErase achieves near-complete unlearning while maintaining high performance on retained data. The approach is also effective in diffusion-based image generation, removing targeted concepts while preserving other details.

## Method Summary
CLIPErase is a three-module framework that unlearns specific visual-textual associations in multimodal models. The Forgetting Module minimizes dot product similarity between image and text embeddings of the forget set, disrupting cross-modal associations. The Retention Module maintains the original contrastive loss for the retain set, preserving alignment and performance. The Consistency Module uses KL divergence to minimize distributional differences between the original and unlearned models on the retain set, ensuring stable behavior. The method is trained with Adam optimizer for 20 epochs, combining all three losses with tunable weights (λ1=1, λ2=λ3=3).

## Key Results
- Achieves near-complete unlearning (0% accuracy on forget set) across CIFAR-100, Conceptual 12M, and Flickr30K
- Maintains high performance on retained data (90%+ accuracy in zero-shot prediction tasks)
- Effectively removes targeted concepts in diffusion-based image generation while preserving other details
- Shows strong performance across five downstream tasks including zero-shot prediction, retrieval, and captioning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIPErase selectively weakens cross-modal associations in the forget set without harming associations in the retain set.
- Mechanism: The Forgetting Module minimizes dot product similarity between image and text embeddings of the forget set, while the Retention Module maintains the original contrastive loss for the retain set, preserving alignment.
- Core assumption: Cross-modal similarity in CLIP is driven by dot product alignment; disrupting this alignment in the forget set will prevent retrieval without affecting retain set performance.
- Evidence anchors:
  - [abstract]: "a Forgetting Module that disrupts the associations in the forget set"
  - [section 4.1]: "LFM = 1/Nf * Σ (fimg(xn_i) · ftxt(xn_t))" and "minimizing the raw dot product between the image and text embeddings"
  - [corpus]: No direct evidence; similarity-based disruption is an inference.
- Break condition: If retain set embeddings drift due to parameter updates from Forgetting Module, performance will degrade despite Retention Module.

### Mechanism 2
- Claim: The Consistency Module prevents the unlearned model from deviating too far from the original model on the retain set.
- Mechanism: KL divergence between output distributions of original and unlearned models on the retain set is minimized, encouraging the unlearned model to behave similarly to the original.
- Core assumption: KL divergence is a suitable metric for distributional consistency in multimodal embeddings.
- Evidence anchors:
  - [section 4.3]: "LCM = 1/Nr * Σ [KL(pimg_o || pimg_u) + KL(ptxt_o || ptxt_u)]"
  - [abstract]: "a Consistency Module that maintains consistency with the original model"
  - [corpus]: No direct evidence; KL divergence use is inferred from the equation.
- Break condition: If KL divergence becomes too restrictive, it may prevent effective forgetting in the forget set.

### Mechanism 3
- Claim: CLIPErase generalizes to other multimodal models beyond CLIP.
- Mechanism: The modular design (Forgetting, Retention, Consistency) does not rely on CLIP-specific components, allowing adaptation to models like BLIP and ALBEF.
- Core assumption: Other multimodal models also learn in a shared embedding space with image and text encoders that can be optimized with similar objectives.
- Evidence anchors:
  - [section 5.6]: "CLIPErase is designed to be modular and model-agnostic"
  - [section 5.6]: "we further applied CLIPErase to other vision-language models, including BLIP, ALBEF"
  - [corpus]: No direct evidence; generalizability is stated but not empirically proven for all models.
- Break condition: If other models use fundamentally different architectures or training objectives, the modules may not apply.

## Foundational Learning

- Concept: Contrastive learning in multimodal models
  - Why needed here: CLIP and similar models rely on aligning image and text embeddings via contrastive loss; understanding this is key to designing unlearning that disrupts alignment.
  - Quick check question: What is the objective function used to train CLIP to align image and text embeddings?

- Concept: Kullback-Leibler (KL) divergence
  - Why needed here: The Consistency Module uses KL divergence to measure and minimize distributional differences between original and unlearned models.
  - Quick check question: How does KL divergence quantify the difference between two probability distributions?

- Concept: Zero-shot learning
  - Why needed here: Evaluation tasks like zero-shot prediction and retrieval test the model's ability to generalize without task-specific training; unlearning should not harm this ability on the retain set.
  - Quick check question: What does "zero-shot" mean in the context of image classification or retrieval?

## Architecture Onboarding

- Component map:
  Forgetting Module -> Retention Module -> Consistency Module -> Combined loss optimization

- Critical path:
  1. Load CLIP model and datasets (forget set, retain set)
  2. Initialize modules and hyperparameters
  3. For each training step:
     - Compute Forgetting loss on forget set
     - Compute Retention loss on retain set
     - Compute Consistency loss on retain set
     - Combine losses and backpropagate
  4. Evaluate on downstream tasks

- Design tradeoffs:
  - Higher λ2 (Forgetting) → stronger unlearning but risk to retain set
  - Higher λ1 (Retention) → better retain set performance but weaker unlearning
  - Higher λ3 (Consistency) → more stable but may limit forgetting effectiveness

- Failure signatures:
  - Forget set accuracy not near zero → Forgetting Module ineffective or λ2 too low
  - Retain set accuracy drops significantly → Retention or Consistency Module failing or λ1/λ3 too low
  - Model fails to train → loss weights or learning rate misconfigured

- First 3 experiments:
  1. Train CLIPErase on CIFAR-100 with one forget class; check zero-shot prediction accuracy on both sets
  2. Vary λ2 while fixing λ1=1, λ3=3; observe trade-off between forget and retain accuracy
  3. Apply CLIPErase to BLIP on CIFAR-100; verify forgetting and retention metrics

## Open Questions the Paper Calls Out
None specified in the provided input.

## Limitations
- Effectiveness on complex, real-world multimodal data beyond controlled benchmark datasets remains untested
- Computational overhead during inference is not discussed, which could be significant for resource-constrained applications
- Long-term stability of unlearned associations (whether forgotten concepts might reappear through fine-tuning) is not investigated

## Confidence
- High Confidence: Three-module architecture design and mathematical formulation; empirical results on benchmark datasets; trade-off between forgetting and retention
- Medium Confidence: Generalization to other multimodal models (BLIP, ALBEF); effectiveness in diffusion-based image generation
- Low Confidence: Long-term stability of unlearned associations; robustness against adversarial recovery attempts

## Next Checks
1. Test CLIPErase on a dataset with naturally occurring ambiguous concepts to evaluate performance on real-world complexity
2. Conduct a longitudinal study where the unlearned model undergoes additional fine-tuning to determine if forgotten associations gradually return
3. Implement an adversarial recovery attack to measure whether the forgetting is robust against attempts to reconstruct forgotten associations