---
ver: rpa2
title: Generative Modelling with Tensor Train approximations of Hamilton--Jacobi--Bellman
  equations
arxiv_id: '2402.15285'
source_url: https://arxiv.org/abs/2402.15285
tags:
- rank
- tensor
- time
- process
- equation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for sampling from probability
  densities using compressed polynomials represented in the Tensor Train format to
  solve the Hamilton-Jacobi-Bellman equation arising in reverse-time diffusion processes.
  The method is sample-free, agnostic to normalization constants, and can avoid the
  curse of dimensionality due to the TT compression.
---

# Generative Modelling with Tensor Train approximations of Hamilton--Jacobi--Bellman equations

## Quick Facts
- arXiv ID: 2402.15285
- Source URL: https://arxiv.org/abs/2402.15285
- Reference count: 40
- This paper proposes a novel sample-free method for sampling from probability densities using compressed polynomials in Tensor Train format to solve the Hamilton-Jacobi-Bellman equation for reverse-time diffusion processes.

## Executive Summary
This paper presents a novel approach to generative modeling by solving the Hamilton-Jacobi-Bellman (HJB) equation using Tensor Train (TT) approximations. The method enables sampling from probability densities without requiring knowledge of normalization constants, leveraging the structure of reverse-time diffusion processes. By representing the solution in TT format, the approach avoids the curse of dimensionality typically associated with high-dimensional polynomial approximations. The authors demonstrate their method on a 20-dimensional nonlinear sampling task, showing exponential decay in covariance error and the ability to reproduce multimodality and curvature of target densities.

## Method Summary
The proposed method solves the HJB equation arising from reverse-time diffusion processes using Tensor Train approximations of polynomials. The approach employs orthogonal projection onto polynomial spaces and adaptive time-stepping with TT rank control. The nonlinear term in the HJB equation is projected back to fixed-degree polynomial space after each time step to prevent degree explosion. The method uses explicit Euler integration with adaptive stepsize selection based on local stiffness, projection error, and retraction error. The solution is represented in functional Tensor Train format, enabling efficient storage and computation in high dimensions.

## Key Results
- Exponential decay in covariance error demonstrated on 20-dimensional nonlinear potential
- Ability to reproduce multimodality and curvature of target density
- TT format avoids curse of dimensionality with storage complexity of O(max(n1, ..., nd)d max(r1, ..., rd-1)²)
- Method is sample-free and agnostic to normalization constants

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The negative log-density of a reverse-time diffusion process satisfies a Hamilton-Jacobi-Bellman equation.
- **Mechanism**: The Hopf-Cole transformation converts the Fokker-Planck equation governing the density evolution into a HJB equation for the log-density, enabling score computation without normalization constants.
- **Core assumption**: The forward process is an Ornstein-Uhlenbeck process with known transition dynamics.
- **Evidence anchors**:
  - [abstract] "these log-densities can be obtained by solution of a Hamilton-Jacobi-Bellman (HJB) equation known from stochastic optimal control"
  - [section] "the negative log-density of a reverse-time diffusion process satisfies a Hamilton-Jacobi-Bellman (HJB) equation"
- **Break condition**: If the forward process deviates from Ornstein-Uhlenbeck or its transition densities are unknown.

### Mechanism 2
- **Claim**: Tensor Train format avoids the curse of dimensionality in polynomial approximation.
- **Mechanism**: Functional Tensor Trains represent high-dimensional polynomials as products of low-rank matrices, reducing storage from exponential to linear in dimension.
- **Core assumption**: The solution has low-rank structure that can be captured by Tensor Trains.
- **Evidence anchors**:
  - [abstract] "can avoid the curse of dimensionality due to the TT compression"
  - [section] "the TT format exhibits a storage complexity of O(max(n1, . . . , nd)d max(r1, . . . , rd−1)2)"
- **Break condition**: If the solution rank grows unboundedly with dimension, defeating the low-rank assumption.

### Mechanism 3
- **Claim**: Projecting the nonlinear HJB term onto fixed-degree polynomial space prevents degree explosion.
- **Mechanism**: The nonlinear term increases polynomial degree, but projection back to fixed degree space after each time step maintains computational tractability.
- **Core assumption**: The projection error is small enough not to destroy solution accuracy.
- **Evidence anchors**:
  - [abstract] "integrating only the linear part of (2.9) would yield a polynomial of same degree for all t"
  - [section] "To prevent this degree increase, we project the nonlinear part of the right-hand side back onto the space"
- **Break condition**: If projection error accumulates and dominates the solution error.

## Foundational Learning

- **Concept**: Hamilton-Jacobi-Bellman equations in stochastic optimal control
  - Why needed here: The HJB equation provides the theoretical framework for computing the score function needed for reverse-time sampling
  - Quick check question: What is the relationship between the HJB equation and the Bellman optimality principle?

- **Concept**: Tensor Train decomposition and low-rank tensor formats
  - Why needed here: TT format enables efficient representation of high-dimensional polynomials without exponential storage
  - Quick check question: How does the storage complexity of TT format scale with dimension compared to full tensor representation?

- **Concept**: Orthogonal polynomial bases (Legendre polynomials)
  - Why needed here: Orthogonal polynomials provide numerically stable basis for polynomial approximation and enable simple projection operations
  - Quick check question: What property of orthogonal polynomials makes projection onto their span computationally simple?

## Architecture Onboarding

- **Component map**: HJB solver core -> Tensor Train engine -> Stiffness controller -> Sampling module

- **Critical path**:
  1. Initialize TT representation of potential
  2. Time-stepping loop: Compute RHS → Project nonlinear term → Add linear term → Retract to manifold
  3. Evaluate solution at desired time points
  4. Generate samples via reverse-time process

- **Design tradeoffs**:
  - Fixed vs adaptive polynomial degree: Fixed degree simplifies implementation but may limit accuracy; adaptive degree improves accuracy but adds complexity
  - Explicit vs implicit time integration: Explicit Euler is simpler but requires small steps; implicit methods allow larger steps but are more complex
  - TT rank bounds: Tight bounds improve efficiency but may be conservative; loose bounds ensure accuracy but increase computation

- **Failure signatures**:
  - Rank explosion during integration indicates poor low-rank approximation
  - Large projection errors suggest inadequate polynomial degree
  - Small step sizes from stiffness controller indicate highly nonlinear dynamics

- **First 3 experiments**:
  1. Gaussian target density: Verify rank bounds and convergence with fixed polynomial degree
  2. Mixed nonlinear density: Test adaptive rank and degree selection with postprocessing
  3. Sensitivity analysis: Compare sampling quality with varying accuracy levels and postprocessing steps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal polynomial degrees to balance accuracy and computational feasibility for the HJB solver, particularly in the challenging region close to t=0 where the true solution is far from the standard normal potential?
- Basis in paper: [inferred] The paper mentions that there is an initial spike in the covariance error for small times t, which seems to decrease in magnitude when higher polynomial degrees are permitted. The authors state that the optimal choice of permitted degrees is an open question.
- Why unresolved: The paper does not provide a definitive answer or guideline for choosing the optimal polynomial degrees. It only suggests that higher degrees might improve accuracy in the early stages of the solution.
- What evidence would resolve it: A rigorous analysis comparing the performance of the HJB solver with different polynomial degree choices across a range of test cases, including those with challenging initial conditions. The analysis should consider both the accuracy of the solution and the computational cost.

### Open Question 2
- Question: Are there additional rank structures that are preserved under the HJB flow beyond those identified for Gaussian distributions and distributions with independent components?
- Basis in paper: [explicit] The paper conjectures that the HJB flow FTT ranks rt are (up to a constant) bounded by r0 of v0 and r∞ ≡ 2 for v∞, but states that this analysis is part of investigations in a subsequent work.
- Why unresolved: The paper only provides results for specific cases (Gaussian and independent components) and presents a conjecture about the general case. It does not provide a proof or disproof of the conjecture.
- What evidence would resolve it: A mathematical proof or counterexample demonstrating whether the conjectured rank bound holds for a broader class of distributions. This could involve analyzing the structure of the HJB flow and its effect on the FTT rank of the solution.

### Open Question 3
- Question: What is the effect of the approximation error in the score on the quality of samples generated by the reverse-time process, and how can this error be mitigated?
- Basis in paper: [explicit] The paper discusses the sensitivity of score-based sample generation under perturbations and suggests that Langevin postprocessing can improve sample quality even with low-accuracy scores. However, it also mentions that the approximation error of the score still has a negative effect on sampling quality.
- Why unresolved: The paper does not provide a quantitative analysis of the relationship between score approximation error and sample quality. It also does not offer a comprehensive strategy for minimizing this error beyond suggesting Langevin postprocessing.
- What evidence would resolve it: A systematic study measuring the impact of score approximation error on sample quality across various test cases. This could involve comparing the performance of the sampling algorithm with different levels of score accuracy and exploring alternative methods for improving the score approximation.

## Limitations
- Implementation-specific details such as TT-rounding procedure and exact hyperparameter values are not fully specified
- Performance claims for the 20-dimensional nonlinear potential rely on specific parameter choices that are not completely detailed
- The method's effectiveness in more complex, higher-dimensional scenarios beyond 20D remains to be thoroughly validated

## Confidence

- **High confidence**: The theoretical derivation connecting HJB equations to reverse-time diffusion processes is well-established and mathematically rigorous
- **Medium confidence**: The TT-based implementation methodology is sound, but practical performance depends heavily on parameter tuning and specific implementation choices
- **Medium confidence**: The adaptive time-stepping and rank control mechanisms show promise, but their effectiveness in more complex, higher-dimensional scenarios remains to be thoroughly validated

## Next Checks

1. **Parameter sensitivity analysis**: Systematically vary δproj, δrank, and δcontr values to understand their impact on convergence and accuracy across different test cases

2. **Scalability study**: Test the method on progressively higher-dimensional problems (beyond 20D) to verify the claimed curse-of-dimensionality avoidance

3. **Comparison with established methods**: Benchmark against standard diffusion-based samplers (DDPM, score-based methods) on common test distributions to assess practical advantages