---
ver: rpa2
title: Preserving Multilingual Quality While Tuning Query Encoder on English Only
arxiv_id: '2407.00923'
source_url: https://arxiv.org/abs/2407.00923
tags:
- tuning
- learning
- query
- rate
- xnli
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the effects of tuning a multilingual text
  encoder on an English-only dataset, particularly whether such tuning degrades the
  encoder's multilingual qualities. The authors fine-tune a high-quality multilingual
  embedding model as a query encoder using contrastive learning on MSMARCO, with the
  text encoder frozen and varying levels of freezing for the query encoder layers.
---

# Preserving Multilingual Quality While Tuning Query Encoder on English Only

## Quick Facts
- arXiv ID: 2407.00923
- Source URL: https://arxiv.org/abs/2407.00923
- Reference count: 40
- Primary result: Low learning rate tuning preserves and often improves multilingual qualities while adapting to English-only tasks

## Executive Summary
This work investigates whether fine-tuning a multilingual text encoder on an English-only dataset degrades its multilingual capabilities. Through systematic experiments with contrastive learning on MSMARCO, the authors demonstrate that intentionally low learning rates not only preserve but often improve multilingual performance across diverse languages. The key insight is that slow tuning keeps the model within a stable "minimum" region established during pretraining, allowing preservation of qualities not directly targeted by the tuning process. This phenomenon, termed "adiabatic tuning," suggests a fundamental principle for adapting multilingual models without catastrophic forgetting.

## Method Summary
The authors fine-tune a high-quality multilingual embedding model (intfloat/multilingual-e5-small) as a query encoder using contrastive learning on MSMARCO triplets. The text encoder is frozen throughout, while the query encoder is partially frozen with varying levels of layer freezing. Training uses triplet margin loss with margin 0.1, batch size 14, and learning rates ranging from 2e-8 to 1e-7. Evaluation occurs on MSMARCO (English), ARXIV (English, stylistically different), and XNLI (multilingual) using positive-negative discrepancy (PND) as the primary metric. The method emphasizes freezing the embedding block and output.dense.weight layers to extend the safe tuning range.

## Key Results
- Tuning with low learning rates (5e-8) preserves and often improves multilingual performance on XNLI while improving English results
- Freezing the embedding block is critical for preserving multilingual qualities during tuning
- The "adiabatic tuning" regime extends when specific layers like output.dense.weight are frozen, allowing higher learning rates while maintaining multilingual preservation
- Gains on the tuning dataset increase with higher learning rates, but multilingual quality degrades beyond a threshold

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tuning with low learning rates preserves or improves multilingual qualities because the model stays within a "minimum" region in weight space found during pretraining.
- Mechanism: The pretraining establishes a broad, deep minimum in the loss landscape. Low learning rates allow the model to move slowly within this region, maintaining correlations between the pretraining loss and other model qualities.
- Core assumption: The pretraining loss landscape has a wide, stable minimum that correlates with multiple model qualities.
- Evidence anchors:
  - [abstract]: "Tuning with intentionally low learning rate can preserve or improve a system's properties acquired in training, but not specifically targeted by tuning."
  - [section]: "At low learning rates of tuning, the system (the encoder weights) remains in the 'minimum' region found at pretraining."
  - [corpus]: No direct corpus evidence for this mechanism; the claim is based on internal experiments.
- Break condition: High learning rates cause large, destabilizing weight changes that move the model out of the stable pretraining minimum.

### Mechanism 2
- Claim: Freezing the embedding block during tuning is critical for preserving multilingual qualities because it reduces dependence on language-specific features in early layers.
- Mechanism: The embedding layer is more language-dependent than later transformer layers. By freezing it, tuning can focus on task-specific adaptations without disrupting multilingual embeddings.
- Core assumption: Early layers (embedding) are more sensitive to language differences than later layers.
- Evidence anchors:
  - [section]: "Freezing the embedding block appears to be the best option for preserving the multilingual qualities."
  - [abstract]: No direct mention, but implied by the emphasis on freezing the embedding block.
  - [corpus]: No direct corpus evidence; the claim is based on internal experiments.
- Break condition: If the embedding layer needs task-specific adaptation, freezing it may limit performance gains.

### Mechanism 3
- Claim: The "adiabatic tuning" regime extends when specific layers like output.dense.weight are frozen, allowing higher learning rates while still preserving multilingual qualities.
- Mechanism: Freezing the output.dense.weight layer in each transformer block reduces the model's capacity to make destabilizing weight changes, effectively extending the safe tuning range.
- Core assumption: The output.dense.weight layer is particularly sensitive to destabilizing weight changes during tuning.
- Evidence anchors:
  - [section]: "The gains from the tuning by freezing the layer output.dense.weight (in each transformer block) are shown in Figure 3."
  - [abstract]: No direct mention, but implied by the discussion of extending the adiabatic tuning range.
  - [corpus]: No direct corpus evidence; the claim is based on internal experiments.
- Break condition: If the output.dense.weight layer is crucial for task-specific adaptation, freezing it may limit performance gains.

## Foundational Learning

- Concept: Contrastive learning and triplet loss
  - Why needed here: The paper uses contrastive learning with triplet margin loss for fine-tuning the query encoder.
  - Quick check question: How does the triplet margin loss encourage the model to pull positive examples closer and push negative examples farther?

- Concept: Learning rate scaling rules
  - Why needed here: The paper explores different learning rate scaling rules (linear and square root) to optimize tuning performance.
  - Quick check question: What is the difference between linear and square root learning rate scaling, and when might each be preferable?

- Concept: Weight decay and regularization
  - Why needed here: The paper investigates the effects of weight decay on tuning performance and multilingual preservation.
  - Quick check question: How does weight decay influence the model's ability to generalize during fine-tuning?

## Architecture Onboarding

- Component map:
  Text encoder -> Query encoder -> Embedding block -> Transformer blocks -> Loss function -> Optimizer

- Critical path:
  1. Freeze text encoder
  2. Freeze embedding block (recommended)
  3. Set learning rate (start low, around 5e-8)
  4. Apply triplet margin loss with margin 0.1
  5. Monitor multilingual performance on XNLI
  6. Adjust learning rate or freeze additional layers if needed

- Design tradeoffs:
  - Freezing more layers preserves multilingual qualities but may limit task-specific gains
  - Higher learning rates increase task-specific gains but risk degrading multilingual qualities
  - Different optimizers (AdamW vs. Adamax) have subtle effects on weight changes

- Failure signatures:
  - Degradation on XNLI (multilingual dataset) indicates overtuning
  - Minimal improvement on MSMARCO (tuning dataset) suggests undertuning
  - Inconsistent results across datasets may indicate unstable training

- First 3 experiments:
  1. Tune with frozen embedding block, learning rate 5e-8, monitor XNLI performance
  2. Tune with frozen embedding block and output.dense.weight layers, learning rate 1e-7, compare XNLI results
  3. Tune with varying learning rates (2e-8 to 1e-7), monitor MSMARCO and XNLI performance to identify optimal rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the adiabatic tuning phenomenon generalize to other neural network architectures beyond transformer-based multilingual embeddings?
- Basis in paper: [explicit] The authors suggest adiabatic tuning as a general hypothesis that "tuning with intentionally low learning rate can preserve or improve a system's properties acquired in training, but not specifically targeted by tuning," and propose testing this across more multilingual models.
- Why unresolved: The current study only tests this hypothesis on two specific multilingual embedding models (E5 and L12), leaving open whether the phenomenon is architecture-specific or more universal.
- What evidence would resolve it: Systematic experiments applying adiabatic tuning to diverse architectures (CNNs, RNNs, vision transformers, etc.) across multiple domains and tasks would demonstrate whether the phenomenon is architecture-agnostic or specific to certain model families.

### Open Question 2
- Question: What is the precise mechanism by which freezing the output.dense.weight layer extends the adiabatic tuning regime?
- Basis in paper: [explicit] The authors observe that freezing output.dense.weight extends the adiabatic tuning range and suspect this layer is "most responsible for breaking out of the original 'minimum' region," but admit this is based on "crude measures" and call for "more detailed research."
- Why unresolved: The analysis is preliminary, identifying correlations rather than causal mechanisms, and lacks a theoretical explanation for why this particular layer has such a significant impact.
- What evidence would resolve it: Detailed ablation studies examining the effect of freezing different weight matrices individually, combined with gradient flow analysis and loss landscape visualization, would clarify the specific role of output.dense.weight in maintaining the model within the minimum region.

### Open Question 3
- Question: How does the choice of target language for tuning affect the preservation and improvement of multilingual qualities in other languages?
- Basis in paper: [explicit] The authors note that "Choosing another language for tuning would be interesting both for understanding and as a practical scenario," acknowledging this was not explored in their English-only tuning experiments.
- Why unresolved: The study only tunes on English, leaving unknown whether tuning on high-resource languages versus low-resource languages produces different patterns of multilingual quality preservation or degradation.
- What evidence would resolve it: Comparative experiments tuning the same model on different source languages (e.g., English, Chinese, Hindi) followed by evaluation on the full multilingual test set would reveal whether source language choice systematically affects cross-lingual transfer patterns.

## Limitations

- The core claims about low learning rates preserving multilingual qualities are based on internal experiments without theoretical grounding
- The "adiabatic tuning" metaphor is compelling but untested beyond the specific experimental setup
- Claims about specific layer freezing benefits are based on relative comparisons within a narrow hyperparameter range and may not generalize

## Confidence

- **High Confidence**: Claims about low learning rates preserving English performance while maintaining multilingual capabilities on XNLI
- **Medium Confidence**: The mechanism explanation (adiabatic tuning/staying in pretraining minimum) is plausible but not rigorously tested
- **Low Confidence**: Claims about specific layer freezing (output.dense.weight) providing consistent benefits across different scenarios

## Next Checks

1. **Learning Rate Sensitivity**: Systematically test learning rates from 1e-8 to 1e-6 with and without embedding block freezing to establish the true boundary of the "adiabatic" regime.

2. **Cross-Architecture Generalization**: Apply the same tuning approach to a different multilingual encoder (e.g., mBERT or XLM-R) to test whether the preservation effects are architecture-specific.

3. **Language Pair Analysis**: Conduct a detailed breakdown of XNLI performance by language pair to identify whether certain language families benefit more from the tuning approach, revealing potential biases in the preservation mechanism.