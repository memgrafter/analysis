---
ver: rpa2
title: Balancing Fairness and Accuracy in Data-Restricted Binary Classification
arxiv_id: '2403.07724'
source_url: https://arxiv.org/abs/2403.07724
tags:
- fairness
- sensitive
- each
- feature
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes fairness-accuracy trade-offs for Bayesian classifiers
  under four data-restricting scenarios (sensitive attribute available/not available;
  features decorrelated/not decorrelated from sensitive attribute). The core method
  uses vector quantization to discretize the feature space, then solves convex optimization
  problems to enforce fairness constraints while minimizing accuracy loss.
---

# Balancing Fairness and Accuracy in Data-Restricted Binary Classification

## Quick Facts
- arXiv ID: 2403.07724
- Source URL: https://arxiv.org/abs/2403.07724
- Reference count: 40
- Primary result: Framework analyzing fairness-accuracy trade-offs for Bayesian classifiers under four data-restricting scenarios using vector quantization and convex optimization

## Executive Summary
This paper presents a comprehensive framework for analyzing fairness-accuracy trade-offs in binary classification under four data-restricting scenarios: when the sensitive attribute is available or not, and when features are decorrelated or not from the sensitive attribute. The core approach uses vector quantization to discretize the feature space, then solves convex optimization problems to enforce fairness constraints while minimizing accuracy loss. The framework enables systematic exploration of how different fairness notions (group and individual) affect classifier performance under various data constraints.

## Method Summary
The method involves three main components: (1) using CT-GAN to generate 1 million samples from each dataset to approximate the population distribution, (2) applying vector quantization with 256 cells to discretize the feature space into a tractable discrete approximation, and (3) formulating and solving convex optimization problems to find fair Bayesian classifier scores. For decorrelation, an augmented Lagrangian approach minimizes correlation between features and sensitive attributes while preserving fairness constraints. The framework analyzes four data scenarios: with/without sensitive attribute awareness and with/without feature decorrelation.

## Key Results
- Group fairness constraints can typically be satisfied exactly with little accuracy loss (often <2%)
- Enforcing individual fairness significantly reduces accuracy, especially when the sensitive attribute is unavailable
- It's often possible to decorrelate features from the sensitive attribute while preserving fairness with minimal accuracy loss
- Accuracy losses increase substantially when individual fairness is enforced alongside group fairness constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The vector quantization (VQ) cell decomposition allows construction of a tractable discrete approximation of the continuous feature distribution.
- Mechanism: Lloyd's algorithm partitions the feature space into N disjoint cells, each represented by a centroid. The joint distribution statistics over these cells are estimated from densely sampled generator data, creating a finite discrete approximation.
- Core assumption: The generator produces high-quality samples that faithfully represent the true underlying distribution.
- Evidence anchors:
  - [abstract] "our framework directly analyzes the behavior of the optimal Bayesian classifier on this underlying distribution by constructing a discrete approximation it from the dataset itself"
  - [section] "we use vector quantization (VQ) to induce a cell decomposition over the feature space, with each cell representing an element on the support of our discrete approximation"
  - [corpus] Weak evidence - no direct mention of VQ in corpus neighbors
- Break condition: If the generator fails to capture the true distribution or VQ cells are too large relative to sample density, the discrete approximation becomes inaccurate.

### Mechanism 2
- Claim: Convex optimization formulations enable efficient computation of fair Bayesian classifiers under various data-restricting scenarios.
- Mechanism: The paper formulates fairness constraints as convex inequalities that can be incorporated into linear programming problems, allowing efficient optimization for both accuracy and fairness.
- Core assumption: The fairness constraints can be expressed as linear inequalities in the deviation vector m = s* - s(F).
- Evidence anchors:
  - [abstract] "This approach enables us to formulate multiple convex optimization problems, which allow us to answer the question: How is the accuracy of a Bayesian classifier affected in different data restricting scenarios when constrained to be fair?"
  - [section] "Observing that each of the constraints in this minimization problem can be made linear in m, this optimization problem is convex and can be efficiently solved using linear programming"
  - [corpus] Moderate evidence - corpus includes "Optimal Fairness under Local Differential Privacy" with optimization frameworks
- Break condition: If fairness constraints cannot be linearized or become non-convex (e.g., with complex individual fairness metrics), the optimization problem becomes intractable.

### Mechanism 3
- Claim: The decorrelation transformation matrix T preserves fairness while reducing correlation between features and sensitive attributes.
- Mechanism: The optimization problem minimizes correlation (||T(p_a - p_b)||₁) while preserving accuracy and fairness constraints through augmented Lagrangian methods.
- Core assumption: The transformation T can be found that simultaneously decorrelates features and maintains the fairness properties of the original classifier.
- Evidence anchors:
  - [abstract] "it's often possible to decorrelate features from the sensitive attribute while preserving fairness with minimal accuracy loss (<2%)"
  - [section] "This transformation comes in the form of a matrix, T, where T[i, j] represents the probability of mapping a sample from the jth to the ith VQ cell"
  - [corpus] Weak evidence - no direct mention of decorrelation in corpus neighbors
- Break condition: If the transformation cannot simultaneously satisfy decorrelation and fairness constraints, the optimization problem becomes infeasible.

## Foundational Learning

- Concept: Vector quantization and Lloyd's algorithm
  - Why needed here: Provides the mathematical foundation for discretizing the continuous feature space into manageable cells for analysis
  - Quick check question: What property of Lloyd's algorithm ensures that cells become more representative as sample density increases?

- Concept: Convex optimization and linear programming
  - Why needed here: Enables efficient computation of fair classifiers by expressing fairness constraints as convex inequalities
  - Quick check question: Why is it critical that all fairness constraints can be expressed as linear inequalities in the deviation vector?

- Concept: Augmented Lagrangian methods and method of multipliers
  - Why needed here: Provides the optimization framework for solving the complex trade-off between decorrelation, accuracy, and fairness
  - Quick check question: How does the augmented Lagrangian method handle the equality constraints that arise from preserving fairness after transformation?

## Architecture Onboarding

- Component map: Data preprocessing (VQ decomposition) -> Fairness formulation (Linear programming constraints) -> Linear programming optimization -> Decorrelation transformation (if needed) -> Evaluation and analysis

- Critical path: VQ decomposition → Fairness constraint formulation → Linear programming optimization → Decorrelation transformation (if needed) → Evaluation and analysis

- Design tradeoffs:
  - VQ cell size vs. computational complexity: More cells increase precision but require more computation
  - Granularity of individual fairness constraints vs. scalability: Finer individual fairness increases constraints exponentially
  - Decorrelation strength vs. accuracy preservation: Stronger decorrelation may reduce classifier performance

- Failure signatures:
  - High variance in accuracy across different fairness relaxations indicates unstable optimization
  - Inability to find feasible solutions for strict fairness constraints suggests conflicting requirements
  - Large discrepancy between generator-based and true distribution statistics indicates poor sample quality

- First 3 experiments:
  1. Run VQ decomposition with varying N (16, 64, 256 cells) on a small dataset to observe trade-off between precision and computation time
  2. Test linear programming formulation with only demographic parity constraint to verify basic functionality
  3. Apply decorrelation transformation with awareness of sensitive attribute to validate the T matrix optimization works as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the framework scale to datasets with more than binary sensitive attributes?
- Basis in paper: [explicit] The paper states "our formulations may be generalized to non-binary sensitive attributes through combinatorial extension."
- Why unresolved: The paper only analyzes binary sensitive attributes (A = {a, b}) and does not demonstrate the generalization to non-binary cases.
- What evidence would resolve it: Experimental results showing the framework's performance on datasets with multiple sensitive attribute categories, with analysis of how the complexity scales with the number of categories.

### Open Question 2
- Question: What is the theoretical limit on the number of fairness constraints that can be simultaneously satisfied before accuracy becomes negligible?
- Basis in paper: [inferred] The paper observes that "enforcing local individual fairness on top of any combination of three group fairness prohibits any meaningful classification result with all accuracies dropping to around 50%."
- Why unresolved: While the paper demonstrates this empirically, it doesn't provide a theoretical analysis of the fundamental limit on the number of constraints.
- What evidence would resolve it: A mathematical proof or bound showing the maximum number of fairness constraints that can be satisfied while maintaining non-trivial accuracy levels.

### Open Question 3
- Question: How does the choice of distance metric in individual fairness affect the fairness-accuracy trade-off?
- Basis in paper: [explicit] The paper notes "quantifying the similarity among individuals introduces subjectivity" and mentions adjusting the distance metric may produce different results.
- Why unresolved: The paper uses a specific distance metric (Hamming for discrete, absolute for continuous features) but doesn't systematically explore how different metrics affect outcomes.
- What evidence would resolve it: A comparative study using multiple distance metrics (e.g., Mahalanobis distance, learned metrics) to quantify their impact on both fairness metrics and classification accuracy.

## Limitations
- The framework relies heavily on synthetic data generation through CT-GAN, which may not fully capture complex dependencies in real data
- Vector quantization introduces approximation errors that depend on cell size and distribution density
- The analysis assumes perfect knowledge of the joint distribution, which may not hold in real-world scenarios with data collection constraints
- The method requires substantial computational resources for generating samples and solving optimization problems

## Confidence
- **High Confidence**: The analytical framework for convex optimization of fair Bayesian classifiers is well-established and mathematically sound. The VQ discretization approach is standard in computational statistics.
- **Medium Confidence**: The CT-GAN generation quality and its ability to faithfully represent true distributions is supported by quantitative metrics but depends on the specific characteristics of each dataset.
- **Medium Confidence**: The experimental results showing small accuracy losses (<2%) for group fairness constraints are reproducible, but the generalizability to other datasets and fairness definitions requires further validation.

## Next Checks
1. **Distribution Fidelity Validation**: Compare classifier performance using both the original training data and the CT-GAN generated samples to quantify the impact of distribution approximation errors on fairness-accuracy trade-offs.

2. **Robustness to VQ Granularity**: Systematically vary the number of VQ cells (e.g., 64, 128, 256, 512) and measure how this affects the accuracy of fairness constraint satisfaction and the computational complexity of the optimization problems.

3. **Cross-Dataset Generalization**: Apply the framework to additional datasets with different characteristics (e.g., higher dimensionality, different feature types, different sensitive attribute distributions) to test the robustness of the observed small accuracy losses under group fairness constraints.