---
ver: rpa2
title: 'GameArena: Evaluating LLM Reasoning through Live Computer Games'
arxiv_id: '2412.06394'
source_url: https://arxiv.org/abs/2412.06394
tags:
- game
- reasoning
- user
- object
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GameArena, a dynamic benchmark for evaluating
  LLM reasoning capabilities through interactive games with humans. GameArena addresses
  limitations of static benchmarks and open-ended evaluations by using three games
  (Akinator, Taboo, and Bluffing) that test deductive, inductive, abductive, and multi-hop
  reasoning.
---

# GameArena: Evaluating LLM Reasoning through Live Computer Games

## Quick Facts
- arXiv ID: 2412.06394
- Source URL: https://arxiv.org/abs/2412.06394
- Authors: Lanxiang Hu; Qiyu Li; Anze Xie; Nan Jiang; Ion Stoica; Haojian Jin; Hao Zhang
- Reference count: 40
- Key outcome: GameArena achieves 86.9% data efficiency versus 4% for Chatbot Arena while providing fine-grained reasoning metrics through interactive games

## Executive Summary
GameArena introduces a dynamic benchmark for evaluating LLM reasoning capabilities through interactive games with humans. The system addresses limitations of static benchmarks and open-ended evaluations by using three games (Akinator, Taboo, and Bluffing) that test deductive, inductive, abductive, and multi-hop reasoning. GameArena collects over 2000 game sessions and employs retrospective analysis to uncover step-by-step reasoning processes. User studies show GameArena significantly outperforms Chatbot Arena in data efficiency and user engagement while providing quantifiable metrics for LLM reasoning assessment.

## Method Summary
GameArena evaluates LLM reasoning through three interactive games requiring different reasoning types. The system collects live human-AI interaction data through game sessions, then applies retrospective analysis to extract intermediate reasoning outputs from LLMs. This process generates both outcome metrics (win rates, average rounds) and procedural metrics (recall rates, disparity ratios, consistency rates) that provide fine-grained assessment of reasoning capabilities. The benchmark compares five state-of-the-art LLMs across all games and validates results through user studies measuring engagement and data collection efficiency.

## Key Results
- GameArena achieves 86.9% data efficiency versus 4% for Chatbot Arena
- Claude 3.5 Sonnet and GPT-4o outperform other models in reasoning capabilities
- Mistral-large shows poor performance, particularly failing to make predictions in Bluffing game

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GameArena's dynamic benchmark design prevents data contamination and saturation issues present in static benchmarks
- Mechanism: By collecting live human-AI interaction data through games, GameArena generates fresh prompts that haven't been seen during training, avoiding the contamination that plagues static datasets
- Core assumption: Human players will generate sufficiently diverse prompts that LLMs haven't encountered in pretraining
- Evidence anchors:
  - [abstract] "GameArena addresses limitations of static benchmarks and open-ended evaluations by using three games...that test deductive, inductive, abductive, and multi-hop reasoning"
  - [section] "Instead of creating entirely new games, we integrate LLMs into existing games...that are fun and require complex reasoning through multi-turn conversations"
  - [corpus] Weak - corpus contains related work on game-based LLM evaluation but no direct evidence about contamination prevention
- Break condition: If human players consistently use similar prompts or if LLMs have been trained on similar game scenarios

### Mechanism 2
- Claim: Retrospective analysis enables fine-grained reasoning capability measurement that binary feedback cannot capture
- Mechanism: By prompting LLMs to generate intermediate reasoning outputs after each game round, GameArena extracts step-by-step reasoning trajectories that reveal specific reasoning capabilities
- Core assumption: LLMs can reliably reconstruct and articulate their hidden reasoning process when prompted retrospectively
- Evidence anchors:
  - [abstract] "We analyze the gaming data retrospectively to uncover the underlying reasoning processes of LLMs and measure their fine-grained reasoning capabilities"
  - [section] "To uncover the LLM's hidden reasoning process, we retrospectively analyze the gaming data by asking the LLM to reveal its intermediary thought process at each step"
  - [corpus] Moderate - corpus shows related work on LLM reasoning evaluation but limited evidence on retrospective analysis effectiveness
- Break condition: If LLMs cannot accurately reconstruct their reasoning process or if retrospective prompts introduce bias

### Mechanism 3
- Claim: Gamification significantly improves user engagement and data collection efficiency compared to traditional benchmarks
- Mechanism: Embedding evaluation tasks in entertaining games creates intrinsic motivation for human participation, leading to higher completion rates and more useful data
- Core assumption: Users find interactive games more engaging than providing binary feedback on model responses
- Evidence anchors:
  - [abstract] "Our user study with 100 participants suggests that GameArena improves user engagement compared to Chatbot Arena"
  - [section] "We conducted comparative studies between GameArena and Chatbot Arena. GameArena was found to be more effective in data collection, with over 85% of game sessions from GameArena containing useful data"
  - [corpus] Weak - corpus contains related work on game-based evaluation but limited quantitative evidence on engagement improvement
- Break condition: If game fatigue sets in or if the entertainment value diminishes over repeated sessions

## Foundational Learning

- Concept: Deductive reasoning
  - Why needed here: The Akinator game specifically tests deductive reasoning by requiring models to draw specific conclusions from chains of premises provided by yes/no answers
  - Quick check question: If a model asks "Is it bigger than a breadbox?" and gets "Yes", then "Is it alive?" and gets "No", what logical conclusion can it draw about the object?

- Concept: Abductive reasoning
  - Why needed here: The Taboo game tests abductive reasoning by requiring models to generate possible explanations from incomplete information provided through user prompts
  - Quick check question: Given vague clues like "It's something you eat for breakfast" and "It can be scrambled", what is the most probable target word?

- Concept: Multi-hop reasoning
  - Why needed here: All three games require connecting multiple pieces of information across rounds to solve the problem, testing the model's ability to synthesize information
  - Quick check question: How would you use information from three different rounds of questioning to narrow down possibilities in Akinator?

## Architecture Onboarding

- Component map: Game logic (Akinator, Taboo, Bluffing) -> LLM integration layer (handles game sessions with different models) -> Retrospective analysis engine (extracts reasoning data)
- Critical path: Human initiates game → LLM generates response → Human provides feedback → Data stored → Retrospective analysis triggers → Metrics calculated → Rankings updated
- Design tradeoffs: Real-time human interaction vs. scalability, game complexity vs. accessibility, retrospective analysis depth vs. computational cost
- Failure signatures: Low completion rates (engagement issues), inconsistent reasoning patterns (model capability issues), retrospective analysis failures (prompting issues)
- First 3 experiments:
  1. Run 50 game sessions with one LLM to validate basic game mechanics and data collection
  2. Test retrospective analysis prompts on 10 completed games to verify reasoning extraction works
  3. Compare completion rates between GameArena and Chatbot Arena with 20 participants each

## Open Questions the Paper Calls Out

None

## Limitations

- Contamination prevention effectiveness is not validated with analysis of prompt diversity and novelty
- Retrospective analysis reliability is uncertain due to lack of comparison with real-time reasoning traces
- Generalization of game-based evaluation to broader reasoning capabilities remains unproven

## Confidence

- **High Confidence**: Data collection methodology, basic game mechanics implementation, user engagement improvements over Chatbot Arena
- **Medium Confidence**: Reasoning capability measurement framework, procedural metrics validity, ranking consistency across games
- **Low Confidence**: Contamination prevention effectiveness, retrospective analysis accuracy, generalization claims

## Next Checks

1. **Prompt Diversity Analysis**: Conduct analysis of human-generated prompts to quantify their novelty and diversity compared to common pretraining corpora, measuring overlap with known datasets to validate contamination prevention claims.

2. **Retrospective vs. Real-time Comparison**: Implement a parallel study where a subset of games collect real-time reasoning traces (through chain-of-thought prompting during gameplay) and compare these with retrospective analysis outputs to assess reliability.

3. **Cross-domain Transfer Study**: Test whether models that perform well on GameArena games also demonstrate superior performance on reasoning tasks in different domains (e.g., logical reasoning benchmarks, commonsense reasoning tasks) to validate generalization claims.