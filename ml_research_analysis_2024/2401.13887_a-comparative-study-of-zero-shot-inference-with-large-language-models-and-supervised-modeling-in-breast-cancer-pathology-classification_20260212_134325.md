---
ver: rpa2
title: A comparative study of zero-shot inference with large language models and supervised
  modeling in breast cancer pathology classification
arxiv_id: '2401.13887'
source_url: https://arxiv.org/abs/2401.13887
tags:
- pathology
- should
- breast
- cancer
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared zero-shot inference with large language models
  (GPT-4 and GPT-3.5) to supervised classification models for extracting treatment-relevant
  information from breast cancer pathology reports. The zero-shot GPT-4 model achieved
  an average macro F1 score of 0.83 across 13 classification tasks, performing as
  well as or significantly better than the best supervised model (LSTM with attention,
  F1=0.75).
---

# A comparative study of zero-shot inference with large language models and supervised modeling in breast cancer pathology classification

## Quick Facts
- arXiv ID: 2401.13887
- Source URL: https://arxiv.org/abs/2401.13887
- Reference count: 40
- Primary result: Zero-shot GPT-4 achieved average macro F1 of 0.83 across 13 pathology classification tasks, outperforming supervised models

## Executive Summary
This study demonstrates that large language models can extract treatment-relevant information from breast cancer pathology reports with minimal or no labeled training data. The zero-shot GPT-4 model achieved an average macro F1 score of 0.83 across 13 classification tasks, performing as well as or significantly better than supervised models trained on manually labeled data. GPT-4 particularly excelled on tasks with high class imbalance or requiring advanced reasoning, while simpler supervised models performed comparably when large annotated datasets were available. The findings suggest LLMs can reduce the need for large-scale manual data annotations in clinical NLP tasks, though simpler supervised models remain viable alternatives when LLM use is prohibitive.

## Method Summary
The study evaluated zero-shot inference using GPT-4 and GPT-3.5 against three supervised models (random forests, LSTM with attention, and UCSF-BERT) on a manually-labeled dataset of 769 breast cancer pathology reports. The supervised models were trained on task-specific data with various hyperparameters, while the LLMs were evaluated using carefully designed prompts in JSON format. Performance was measured using macro-averaged F1 scores across 13 classification tasks, including single-label tasks (biopsy type, lymph nodes involved, ER, PR, HER2, grade, LVI, margins, DCIS margins) and multi-label tasks (histology, sites examined, sites of disease). Error analysis was conducted through confusion matrices and manual categorization of common failure modes.

## Key Results
- GPT-4 achieved an average macro F1 score of 0.83 across all 13 classification tasks
- GPT-4 significantly outperformed the best supervised model (LSTM with attention, F1=0.75) on margins and ER status classification
- GPT-4 excelled on high class imbalance and keyword-based tasks compared to supervised models
- Supervised models performed comparably to GPT-4 when large annotated datasets were available

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot GPT-4 achieves performance comparable to or better than supervised models trained on labeled pathology data by leveraging pre-trained knowledge of medical terminology and reasoning without task-specific fine-tuning. The model's pre-training on internet-scale corpora includes sufficient domain knowledge for clinical inference tasks, though performance degrades when tasks require reasoning about multiple samples or complex task designs not well-represented in pre-training data.

### Mechanism 2
LLMs excel on tasks with high class imbalance or requiring keyword matching due to pre-existing medical knowledge encoding. The pre-trained models already understand medical terminology and can match keywords effectively without needing balanced training data, though performance may suffer when medical terminology is highly specialized or rare, lacking sufficient representation in pre-training data.

### Mechanism 3
Supervised models can match LLM performance when large annotated datasets with balanced classes are available because task-specific training allows models to learn domain-specific patterns that generalize well when sufficient labeled examples exist. However, this approach requires sufficient annotation resources to represent all relevant classes and patterns, and performance suffers when annotation resources are limited or class imbalance cannot be adequately addressed through sampling.

## Foundational Learning

- **Macro F1 score vs accuracy**: Why needed here - The study uses macro F1 to evaluate performance on imbalanced datasets where accuracy would be misleading. Quick check: Why would accuracy be an inappropriate metric for evaluating classification on imbalanced datasets?

- **Zero-shot vs few-shot learning**: Why needed here - The study specifically evaluates zero-shot performance without any labeled examples. Quick check: How does zero-shot inference differ from few-shot learning in terms of model adaptation to new tasks?

- **Inter-annotator agreement**: Why needed here - The study reports Krippendorf's alpha to establish reliability of the annotated dataset. Quick check: Why was Krippendorf's alpha chosen over Cohen's kappa for this multi-label annotation task?

## Architecture Onboarding

- **Component map**: Pathology reports → Pre-processing → Model input → Random forests/LSTM/UCSF-BERT/GPT-4/GPT-3.5 → Macro F1 scoring → Error analysis

- **Critical path**: 1) Data annotation and schema development, 2) Model implementation and training (supervised models), 3) Prompt engineering for LLMs, 4) Evaluation and comparison, 5) Error analysis and interpretation

- **Design tradeoffs**: LLM approach requires no training data but needs privacy-preserving API access and may have context length limitations; supervised approach requires large annotated datasets but offers more control and can work offline; simple models (RF) work well for keyword tasks while complex models (LSTM, BERT) needed for reasoning tasks

- **Failure signatures**: GPT-4 struggles with multiple samples, complex task design, and distinguishing unknown from no findings; supervised models perform poorly on imbalanced data without careful sampling strategies; all models may exhibit demographic biases in clinical data

- **First 3 experiments**: 1) Reproduce supervised model baselines using same train/validation/test splits, 2) Test GPT-4 with simplified task prompts to isolate prompt complexity effects, 3) Compare few-shot learning performance by providing 5-10 examples in the prompt

## Open Questions the Paper Calls Out

1. **External validation**: How would performance compare on pathology reports from multiple health systems beyond UCSF? The study only evaluated performance on a dataset from one health system, limiting generalizability. Testing on diverse health systems would resolve this uncertainty.

2. **Prompt engineering optimization**: What specific prompt engineering strategies (like few-shot learning or chain-of-thought prompting) could improve GPT-4's performance on complex pathology classification tasks? The study used only a single prompt format without exploring advanced techniques, so systematic testing of various strategies would provide evidence.

3. **Cost-benefit analysis**: What is the optimal balance between LLM-based and supervised learning approaches for clinical NLP tasks when considering privacy, computational cost, and annotation effort? The study didn't explore hybrid approaches or cost-benefit analysis, so comparative studies measuring these factors would resolve this question.

## Limitations

- The dataset contains only 769 pathology reports from a single institution, limiting external validity across different reporting practices and demographic populations
- The evaluation focuses on macro F1 scores without extensive error analysis across demographic subgroups, potentially masking systematic biases
- The comparison assumes equal access to GPT-4, which may not be feasible in all healthcare settings due to cost, privacy concerns, or institutional policies

## Confidence

- **High confidence**: GPT-4 outperforms or matches supervised models on most tasks (average F1=0.83 vs 0.75)
- **Medium confidence**: LLMs excel particularly on imbalanced datasets and keyword-matching tasks
- **Low confidence**: Supervised models remain viable when large datasets are available

## Next Checks

1. **External validation**: Test the zero-shot GPT-4 approach on pathology reports from different institutions with varying reporting standards to assess generalizability across healthcare systems

2. **Cost-benefit analysis**: Compare the computational and financial costs of LLM inference versus supervised model training/inference across different deployment scales and frequency of model updates

3. **Bias assessment**: Conduct detailed demographic subgroup analysis to identify potential disparities in LLM performance across patient populations, particularly for clinically sensitive classification tasks