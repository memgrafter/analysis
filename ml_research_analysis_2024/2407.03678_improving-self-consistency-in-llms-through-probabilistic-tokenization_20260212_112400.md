---
ver: rpa2
title: Improving Self Consistency in LLMs through Probabilistic Tokenization
arxiv_id: '2407.03678'
source_url: https://arxiv.org/abs/2407.03678
tags:
- tokenization
- probabilistic
- reasoning
- tokenizations
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using multiple tokenizations of the same input
  to improve self-consistency in large language models for reasoning tasks. It presents
  a method to sample diverse tokenizations using a unigram language model, which leads
  to more logically diverse reasoning paths compared to traditional sampling methods.
---

# Improving Self Consistency in LLMs through Probabilistic Tokenization

## Quick Facts
- arXiv ID: 2407.03678
- Source URL: https://arxiv.org/abs/2407.03678
- Authors: Ashutosh Sathe; Divyanshu Aggarwal; Sunayana Sitaram
- Reference count: 32
- One-line primary result: Using multiple tokenizations of the same input improves self-consistency in LLMs for reasoning tasks, with average relative gains of +16.39% over baseline in task accuracy.

## Executive Summary
This paper proposes using multiple tokenizations of the same input to improve self-consistency in large language models for reasoning tasks. The method samples diverse tokenizations using a unigram language model, which leads to more logically diverse reasoning paths compared to traditional sampling methods. Experiments on 5 LLM families and 4 reasoning benchmarks show consistent improvements in both task accuracy and "Oracle" accuracy.

## Method Summary
The approach involves sampling multiple tokenizations of the same input string using a unigram language model to estimate tokenization probabilities. These diverse tokenizations are then fed into the LLM to generate multiple reasoning paths, with the final answer selected through majority voting. The unigram probabilities are estimated either through counting or EM algorithm, with the paper opting for a simple counting-based method for practicality.

## Key Results
- Average relative performance gains of +16.39% over baseline in task accuracy
- Higher "Oracle" accuracy, indicating at least one generated reasoning path often produces the correct answer
- Consistent improvements across 5 LLM families (OLMo-7B, Gemma-2B, Gemma-7B, Llama3-8B, Llama3-70B, Mistral-7B, and Mamba-2.8B) and 4 reasoning benchmarks (MATH, AQuA, GSM8k, and PIQA)

## Why This Works (Mechanism)

### Mechanism 1
Different tokenizations change the boundary of tokens, altering the next-token distribution during decoding. This leads to different intermediate reasoning steps, even though the final meaning is preserved.

### Mechanism 2
The unigram language model assigns higher likelihood to tokenizations that preserve frequent subword patterns, biasing the sampled paths toward linguistically plausible reasoning steps.

### Mechanism 3
Oracle accuracy improves because at least one sampled tokenization will produce a correct reasoning path, even if others fail.

## Foundational Learning

- **Concept**: Byte-Pair Encoding (BPE) tokenization and its non-uniqueness
  - Why needed here: Understanding that a single string can have multiple valid BPE tokenizations is prerequisite to grasping why probabilistic tokenization can induce diversity.
  - Quick check question: Given the string "hello world", what are two possible BPE tokenizations if "hello" and "he" are both in the vocabulary?

- **Concept**: Unigram language model for tokenization likelihood
  - Why needed here: The core of the method is sampling tokenizations proportional to their unigram likelihood; without this, you cannot control which tokenizations are chosen.
  - Quick check question: If token "hello" has unigram probability 0.01 and token "world" has 0.02, what is the likelihood of the tokenization ["hello", "world"]?

- **Concept**: Self-consistency via majority voting over multiple reasoning paths
  - Why needed here: The final answer is selected by majority vote over the predictions from each tokenization, so understanding this voting scheme is critical to interpreting results.
  - Quick check question: If 64 reasoning paths yield 40 "A" answers and 24 "B" answers, what is the final output under majority voting?

## Architecture Onboarding

- **Component map**: Input string -> Unigram probability estimator -> Tokenization sampler -> Decoder -> Voter
- **Critical path**: Input string → unigram probability estimation → l-best tokenization sampling → decoding → voting
- **Design tradeoffs**:
  - Fixed l vs l→∞ sampling: Fixed l may miss diverse tokenizations; l→∞ ensures coverage but increases computation.
  - Counting vs EM for unigram probabilities: Counting is faster but potentially less accurate; EM is slower but more theoretically grounded.
  - Greedy vs sampling decoding: Greedy is deterministic and faster; sampling may add diversity but can introduce noise.
- **Failure signatures**:
  - Oracle accuracy does not improve → model is too weak to find correct path in any tokenization.
  - Majority accuracy drops → tokenizations introduce incoherent or contradictory reasoning paths.
  - No diversity in reasoning → l is too small or unigram probabilities are too peaked.
- **First 3 experiments**:
  1. Verify that the same input string produces at least 3 distinct tokenizations under the tokenizer.
  2. Measure the unigram probability distribution and confirm that common subwords have higher probabilities.
  3. Generate reasoning paths from 2 different tokenizations and compare their intermediate steps for diversity.

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal way to estimate unigram probabilities (p(tj)) for different domains and tasks? The paper acknowledges that using a web corpus may not be optimal for domain-specific tasks and that errors in estimating p(tj) can hurt performance.

### Open Question 2
How does the choice of l (number of top tokenizations considered) affect the diversity and quality of generated reasoning paths? The paper finds that fixed l may not offer sufficient diversity but doesn't provide a comprehensive study on the relationship between l, diversity, and performance.

### Open Question 3
Can probabilistic tokenization be effectively combined with other self-consistency methods to further improve LLM reasoning? The paper focuses on probabilistic tokenization as a standalone method and doesn't investigate potential synergies with other approaches.

## Limitations

- The method's effectiveness depends on the model's capacity to generate diverse reasoning paths, which may not hold for smaller or weaker models.
- The computational overhead of generating multiple tokenizations and reasoning paths is not discussed, making practical utility unclear.
- The counting-based unigram probability estimation may sacrifice accuracy compared to the theoretically optimal EM algorithm.

## Confidence

- **High Confidence**: Multiple tokenizations produce different reasoning paths (supported by manual inspection and moderate-neighbor relevance).
- **Medium Confidence**: Majority voting improves task accuracy (supported by experimental results but lacks statistical significance testing).
- **Low Confidence**: Unigram probabilities learned from web corpus capture relevant linguistic regularities (weakly supported, no comparison to EM method).

## Next Checks

1. **Statistical Significance Testing**: Re-run experiments with 95% confidence intervals and p-values for all reported accuracy improvements.
2. **EM vs Counting Comparison**: Implement both counting-based and EM-based unigram probability estimation methods and measure performance difference on a validation set.
3. **Resource Overhead Analysis**: Measure wall-clock time and GPU memory usage for generating multiple tokenizations and reasoning paths, comparing overhead to performance gains at different model scales.