---
ver: rpa2
title: 'Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural
  Networks'
arxiv_id: '2410.22069'
source_url: https://arxiv.org/abs/2410.22069
tags:
- margin
- page
- descent
- point
- norm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the implicit bias of steepest descent algorithms
  in homogeneous neural networks, extending prior work beyond gradient descent to
  a broad class of optimization methods. The authors prove that algorithm-dependent
  geometric margins increase after perfect training accuracy, and characterize the
  asymptotic behavior by showing that limit points correspond to generalized KKT points
  of a margin maximization problem, measured via a Bregman divergence.
---

# Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks

## Quick Facts
- arXiv ID: 2410.22069
- Source URL: https://arxiv.org/abs/2410.22069
- Authors: Nikolaos Tsilivis; Eitan Gronich; Julia Kempe; Gal Vardi
- Reference count: 40
- Key outcome: This paper studies the implicit bias of steepest descent algorithms in homogeneous neural networks, extending prior work beyond gradient descent to a broad class of optimization methods.

## Executive Summary
This paper analyzes the implicit bias of steepest descent algorithms in homogeneous neural networks, showing that different optimization methods implicitly maximize algorithm-specific geometric margins. The authors prove that after perfect training accuracy is achieved, an algorithm-dependent geometric margin (ℓ2 for gradient descent, ℓ∞ for sign descent, etc.) starts increasing. They characterize the late-stage training dynamics by showing that limit points correspond to generalized KKT points of a margin maximization problem, measured via a Bregman divergence. Experiments on synthetic teacher-student setups and MNIST classification validate these theoretical findings and reveal that Adam behaves similarly to sign descent in the late training stage.

## Method Summary
The paper studies three steepest descent variants—gradient descent, coordinate descent, and sign descent—on homogeneous neural networks with exponential loss functions. The theoretical analysis assumes L-homogeneity of the network and exponentially tailed loss, proving margin increase after perfect training accuracy and convergence to generalized KKT points. Experiments use synthetic teacher-student datasets with Gaussian inputs and ReLU-activated teacher networks, as well as MNIST binary classification (digits 3 vs 6 and 2 vs 7) with 1-hidden layer networks. The authors track geometric margins (ℓ1, ℓ2, ℓ∞) throughout training to observe algorithm-specific behaviors.

## Key Results
- Different steepest descent algorithms (gradient, sign, coordinate) exhibit distinct margin-maximizing behaviors after perfect training accuracy
- Adam behaves similarly to sign descent in late-stage training, favoring ℓ₁ margin maximization
- Theoretical characterization shows that limit points of steepest descent correspond to generalized KKT points of algorithm-specific margin maximization problems
- Margin increase property is algorithm-dependent: ℓ2 for gradient descent, ℓ∞ for sign descent, ℓ1 for coordinate descent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Steepest descent algorithms increase an algorithm-dependent geometric margin after perfect training accuracy is reached.
- Mechanism: After the network achieves zero training error, the optimization dynamics shift from fitting the data to maximizing a geometric margin that is specific to the norm used by the algorithm (e.g., ℓ2 for gradient descent, ℓ∞ for sign descent).
- Core assumption: The loss function is exponentially tailed and the network is homogeneous in its parameters.
- Evidence anchors:
  - [abstract] "an algorithm-dependent geometric margin starts increasing once the networks reach perfect training accuracy"
  - [section] "Theorem 3.1 (Soft margin increases). For almost any t > t0, it holds: d log eγ/dt ≥ ... ≥ 0"
- Break condition: If the loss is not exponentially tailed or the network is not homogeneous, the margin increase property may not hold.

### Mechanism 2
- Claim: The late-stage training dynamics of steepest descent algorithms can be characterized by their convergence to generalized KKT points of a margin maximization problem.
- Mechanism: The optimization trajectory implicitly makes progress towards stationarity by decreasing a Bregman divergence between the gradient of the objective function and the gradient of the constraints of the margin maximization problem.
- Core assumption: The algorithm uses a steepest descent update rule and the network is homogeneous with an exponentially tailed loss.
- Evidence anchors:
  - [abstract] "any limit point of the training trajectory corresponds to a KKT point of the corresponding margin-maximization problem"
  - [section] "Theorem 3.8. Under assumptions (A1), (A2), (A3), consider steepest flow with respect to a norm ∥·∥ ... Then, any limit point θ̄ of {θt/∥θt∥}t≥0 is along the direction of a Deθ 1/2 ∥·∥2⋆-generalized KKT point"
- Break condition: If the Bregman divergence is not well-defined or the stationarity conditions are not properly relaxed, the convergence to generalized KKT points may not be guaranteed.

### Mechanism 3
- Claim: Different steepest descent algorithms (gradient, sign, coordinate) exhibit distinct margin-maximizing behaviors, and Adam behaves similarly to sign descent in the late stage of training.
- Mechanism: The choice of norm in the steepest descent update rule determines which geometric margin (ℓ2, ℓ∞, ℓ1) the algorithm implicitly maximizes, and Adam's adaptive nature makes it behave similarly to sign descent for a typical training run.
- Core assumption: The algorithms are implemented correctly and the network architecture and loss function are appropriate for the experiment.
- Evidence anchors:
  - [abstract] "different steepest descent variants (gradient, sign, coordinate) exhibit distinct margin-maximizing behaviors. Notably, Adam behaves similarly to sign descent in the late stage of training"
  - [section] "Figure 2 provides some experimental answers to this question, in light of Theorems 3.1, 3.8. On a pair of digits extracted from MNIST we train two-layer neural networks with GD, SD and Adam, with small initialization. We observe that, as soon as the algorithms reach 100% train accuracy, the margins start to increase ... Interestingly, Adam with the default hyperparameters ... initially, behaves similar to SD, increasing γ1, before it starts decreasing it, in order to slowly start increasing γ2!"
- Break condition: If the numerical precision constant in Adam is not set appropriately or the training setup is not suitable, the connection between Adam and sign descent may not be observed.

## Foundational Learning

- Concept: Steepest descent algorithms
  - Why needed here: The paper studies the implicit bias of a broad family of optimization methods that generalize gradient descent to different optimization geometries.
  - Quick check question: What is the update rule for steepest descent with respect to a norm ∥·∥, and how does it differ from gradient descent?

- Concept: Homogeneous neural networks
  - Why needed here: The theoretical analysis assumes that the network is L-homogeneous in its parameters, which is a key property for the margin maximization results.
  - Quick check question: What does it mean for a function to be L-homogeneous, and why is this property important for the analysis of steepest descent algorithms?

- Concept: Bregman divergence
  - Why needed here: The paper introduces a generalized notion of stationarity and measures proximity to such stationary points using a Bregman divergence induced by the dual norm of the algorithm.
  - Quick check question: How is the Bregman divergence defined, and what is its role in characterizing the implicit bias of steepest descent algorithms?

## Architecture Onboarding

- Component map: Theoretical analysis (Theorem 3.1, Theorem 3.8, Corollary 3.9) -> Experimental validation (teacher-student setup, MNIST classification)

- Critical path:
  1. Understand the setup: homogeneous neural networks, exponentially tailed loss, steepest descent algorithms
  2. Study the theoretical results: Theorem 3.1, Theorem 3.8, Corollary 3.9, and the generalized notion of stationarity
  3. Reproduce the experiments: teacher-student setup and MNIST classification
  4. Analyze the results: margin increase, convergence to KKT points, and the connection between Adam and sign descent

- Design tradeoffs:
  - The choice of norm in the steepest descent update rule determines which geometric margin the algorithm implicitly maximizes, which can affect generalization and robustness
  - The use of Bregman divergence to measure proximity to stationarity is a generalization of the usual Euclidean distance, which may provide a more appropriate measure for non-convex optimization problems

- Failure signatures:
  - If the network is not homogeneous or the loss is not exponentially tailed, the theoretical results may not hold
  - If the numerical precision constant in Adam is not set appropriately, the connection between Adam and sign descent may not be observed
  - If the training setup is not suitable (e.g., initialization scale, learning rate), the experiments may not reproduce the theoretical findings

- First 3 experiments:
  1. Implement a simple teacher-student setup with a one-hidden layer neural network and train it using gradient descent, sign descent, and coordinate descent. Observe the increase in the corresponding geometric margins after perfect training accuracy is reached.
  2. Implement the same setup but with a different pair of digits from MNIST. Train the network using gradient descent, sign descent, and Adam with different hyperparameter settings. Observe the behavior of the margins and the connection between Adam and sign descent.
  3. Vary the initialization scale and the learning rate in the previous experiments. Observe how these hyperparameters affect the margin increase and the convergence to KKT points.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the implicit bias of steepest descent algorithms be extended beyond homogeneous neural networks?
- Basis in paper: [explicit] The paper assumes L-homogeneity of the network (Assumption A2) and focuses on homogeneous models.
- Why unresolved: The analysis relies heavily on Euler's theorem for homogeneous functions and the scaling properties of homogeneous networks. Non-homogeneous networks with biases or skip connections do not satisfy these properties.
- What evidence would resolve it: Experimental or theoretical analysis showing whether steepest descent algorithms in non-homogeneous networks converge to generalized KKT points of margin maximization problems, or identifying alternative geometric structures they optimize.

### Open Question 2
- Question: Does Adam's late-stage behavior in deep networks converge to ℓ1 margin maximization or a different geometric structure?
- Basis in paper: [explicit] The paper observes that Adam behaves similarly to sign descent in the late stage of training, increasing the ℓ1 margin before potentially switching to ℓ2 margin maximization.
- Why unresolved: The experiments only show Adam's behavior for a limited number of epochs, and the switch from ℓ1 to ℓ2 margin maximization occurs when the loss becomes extremely small. It's unclear whether this behavior persists in deeper networks or with different hyperparameters.
- What evidence would resolve it: Long-term training experiments with Adam on various architectures and datasets, tracking the evolution of multiple geometric margins throughout training to determine the asymptotic behavior.

### Open Question 3
- Question: Is the relationship between early-stage dynamics and generalization stronger than the relationship between geometric margins and generalization?
- Basis in paper: [explicit] The teacher-student experiments show that while different algorithms produce different geometric margins, switching algorithms after separation does not significantly impact generalization, suggesting early-stage dynamics are important.
- Why unresolved: The experiments are limited to simple teacher-student setups with one hidden layer. It's unclear whether this finding extends to more complex architectures or real-world datasets.
- What evidence would resolve it: Systematic experiments across various architectures and datasets, comparing generalization performance when varying initialization scales, optimization algorithms, and training durations to isolate the impact of early-stage dynamics versus late-stage margin maximization.

## Limitations
- Analysis relies on homogeneous network architectures with ReLU activations, limiting generalizability
- Exponential-tailed loss assumption may not hold for all practical loss functions used in deep learning
- Convergence guarantees are asymptotic and do not provide finite-time bounds or rates of convergence to KKT points

## Confidence
- High confidence: The theoretical framework connecting steepest descent algorithms to algorithm-dependent margin maximization is mathematically rigorous
- Medium confidence: The experimental validation on MNIST and synthetic data supports the theoretical predictions, though results are limited to specific network architectures and datasets
- Low confidence: The connection between Adam and sign descent behavior in late-stage training is observational and may not generalize across different architectures and hyperparameter settings

## Next Checks
1. Test the margin maximization behavior across different activation functions (LeakyReLU, ELU) to determine if homogeneity remains the critical property
2. Implement the same experimental protocol on CIFAR-10 binary classification to assess scalability and robustness to more complex data distributions
3. Analyze the effect of batch size on the implicit bias, particularly whether stochastic variants maintain the same margin maximization properties as their deterministic counterparts