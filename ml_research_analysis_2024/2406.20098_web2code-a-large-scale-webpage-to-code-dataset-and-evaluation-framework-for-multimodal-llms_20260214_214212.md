---
ver: rpa2
title: 'Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for
  Multimodal LLMs'
arxiv_id: '2406.20098'
source_url: https://arxiv.org/abs/2406.20098
tags:
- webpage
- code
- data
- understanding
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of webpage understanding and
  HTML code generation in multimodal large language models (MLLMs). The authors introduce
  Web2Code, a large-scale instruction tuning dataset and evaluation framework for
  improving MLLM capabilities in these tasks.
---

# Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs

## Quick Facts
- **arXiv ID**: 2406.20098
- **Source URL**: https://arxiv.org/abs/2406.20098
- **Reference count**: 40
- **Key outcome**: Web2Code significantly improves multimodal LLMs' webpage understanding and HTML code generation capabilities, achieving up to 8.187 overall score on WCGB and 74.84% accuracy on WUB.

## Executive Summary
This paper addresses the challenge of webpage understanding and HTML code generation in multimodal large language models (MLLMs) by introducing Web2Code, a large-scale instruction tuning dataset and evaluation framework. Web2Code includes 1.18 million webpage-to-code pairs and 295K question-answer pairs, generated using GPT models. The dataset is evaluated on two benchmarks: Webpage Understanding Benchmark (WUB) and Webpage Code Generation Benchmark (WCGB). Results show that models trained on Web2Code significantly outperform previous datasets in both webpage understanding and code generation tasks, achieving up to 8.187 overall score on WCGB and 74.84% accuracy on WUB. The dataset also improves general visual language understanding without degrading performance.

## Method Summary
Web2Code uses a LLaVA-style architecture with CLIP ViT-L/14 visual encoder, MLP projector, and LLM backbone. The instruction tuning protocol involves pretraining with caption data for 1 epoch, then fine-tuning on Web2Code dataset for 1 epoch using AdamW optimizer with cosine decay schedule. The dataset contains 1.18M webpage-to-code pairs and 295K question-answer pairs. Evaluation uses two benchmarks: WUB for webpage understanding accuracy and WCGB for HTML code generation quality using rendered image comparison via GPT-4V.

## Key Results
- Models trained on Web2Code achieve up to 8.187 overall score on Webpage Code Generation Benchmark (WCGB)
- Webpage Understanding Benchmark (WUB) accuracy reaches 74.84% with Web2Code training
- Web2Code improves general visual language understanding without degrading performance on existing tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Web2Code improves webpage understanding by training models on structured QA pairs about webpage content.
- Mechanism: The dataset includes 295K question-answer pairs (DWU + DWUR) that force the model to learn fine-grained visual and textual comprehension of webpage elements.
- Core assumption: The QA format aligns with MLLM instruction-following capabilities and generalizes to unseen webpage structures.
- Evidence anchors:
  - [abstract]: "We further include diverse natural language QA pairs about the webpage content in the responses to enable a more comprehensive understanding of the web content."
  - [section]: "To evaluate the MLLM’s success at webpage understanding and HTML parsing, we propose the Webpage Understanding Benchmark (WUB) and Webpage Code Generation Benchmark (WCGB)."
- Break condition: If QA pairs are too generic or repetitive, models may not learn specific webpage structural understanding.

### Mechanism 2
- Claim: Web2Code improves HTML code generation by training on paired webpage images and corresponding HTML code.
- Mechanism: Models learn to map visual layout and design features to their HTML/CSS representation through large-scale image-code pairs (884.7K in DWCG + DWCGR).
- Core assumption: Visual features captured by the MLLM encoder sufficiently represent HTML structure for accurate code generation.
- Evidence anchors:
  - [abstract]: "Specifically, the inputs are webpage images and instructions, while the responses are the webpage’s HTML code."
  - [section]: "To demonstrate the utility of our dataset, we train LLaVA-style MLLMs with our dataset included in the instruction finetuning stage."
- Break condition: If visual features lack fine-grained details (e.g., spacing, color codes), generated HTML may miss critical styling.

### Mechanism 3
- Claim: Web2Code improves general multimodal reasoning without degrading existing capabilities.
- Mechanism: Instruction tuning with Web2Code data preserves and slightly enhances performance on general visual language tasks (MME, POPE, SciQA benchmarks).
- Core assumption: Webpage tasks share underlying multimodal reasoning skills with general vision-language tasks.
- Evidence anchors:
  - [abstract]: "Extensive experiments show that our proposed dataset is beneficial not only to our proposed tasks but also in the general visual domain."
  - [section]: "Table 5 summarizes the results. Specifically, we found that instruction-tuned CrystalChat-7B with the general domain data only shows poor WCGB performances while it shows on par WUB performance compared to the use of further webpage datasets."
- Break condition: If Web2Code data introduces too much domain-specific bias, performance on general tasks may degrade.

## Foundational Learning

- Concept: Multimodal instruction tuning
  - Why needed here: Web2Code uses instruction tuning to align webpage understanding and code generation with MLLM capabilities.
  - Quick check question: What are the two main stages of MLLM instruction tuning, and how do they differ?

- Concept: Vision-language mapping
  - Why needed here: The model must map visual features from webpage screenshots to text embeddings for reasoning and code generation.
  - Quick check question: How does the CLIP visual encoder contribute to the vision-language mapping in Web2Code?

- Concept: Evaluation beyond text similarity
  - Why needed here: Traditional code similarity metrics are insufficient; Web2Code uses rendered image comparison for HTML evaluation.
  - Quick check question: Why is rendering HTML back to an image and using GPT-4V more effective than BLEU or ROUGE for code generation evaluation?

## Architecture Onboarding

- Component map: CLIP ViT-L/14 visual encoder → MLP projector → LLM backbone (CrystalChat, Vicuna, etc.) → output HTML/code
- Critical path: Input webpage image → visual encoding → cross-modal reasoning → structured HTML generation
- Design tradeoffs: High-resolution images improve detail capture but increase computational cost; using pre-trained CLIP preserves visual-language alignment.
- Failure signatures: Poor HTML generation when webpage contains complex layouts; degraded general performance if dataset bias is too strong.
- First 3 experiments:
  1. Evaluate baseline MLLM on Web2Code WUB to measure webpage understanding performance.
  2. Fine-tune MLLM on DWU data only, test on WUB and general VQA benchmarks to isolate understanding improvements.
  3. Fine-tune MLLM on full Web2Code dataset, test on WCGB, WUB, and general benchmarks to measure combined impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of dataset size and diversity on the performance of MLLMs in webpage understanding and code generation tasks?
- Basis in paper: [inferred] The paper mentions the use of different dataset components (DWCG, DWU, DWCGR, DWUR) and their contributions to overall improvements, but does not provide a detailed analysis of how dataset size and diversity specifically affect performance.
- Why unresolved: While the paper demonstrates the effectiveness of the proposed dataset, it does not explore the relationship between dataset characteristics and model performance in depth.
- What evidence would resolve it: Conducting experiments with varying dataset sizes and levels of diversity, and analyzing the corresponding performance changes in MLLMs, would provide insights into the optimal dataset characteristics for webpage understanding and code generation tasks.

### Open Question 2
- Question: How do different evaluation metrics, such as code similarity and visual similarity, compare in assessing the quality of generated HTML code?
- Basis in paper: [explicit] The paper introduces a novel evaluation framework (WCGB) that focuses on visual similarity rather than traditional code similarity metrics, but does not directly compare the effectiveness of these approaches.
- Why unresolved: The paper presents a new evaluation method but does not provide a comprehensive comparison with existing code-based evaluation metrics.
- What evidence would resolve it: Conducting a comparative study using both code similarity metrics and visual similarity assessments on the same set of generated HTML codes would help determine which approach is more effective in evaluating code quality for webpage generation tasks.

### Open Question 3
- Question: What is the role of specific HTML tags and their frequency in the generated code for webpage understanding and code generation tasks?
- Basis in paper: [explicit] The paper provides a distribution of the most common HTML tags in the GPT-3.5 generated HTML data, but does not explore the relationship between tag usage and task performance.
- Why unresolved: While the paper presents tag distribution statistics, it does not investigate how the presence and frequency of specific tags impact the model's ability to understand and generate webpages.
- What evidence would resolve it: Analyzing the correlation between tag usage patterns and model performance on webpage understanding and code generation tasks would provide insights into the importance of specific tags in these tasks.

### Open Question 4
- Question: How does the inclusion of diverse natural language QA pairs about webpage content affect the model's general visual language understanding capabilities?
- Basis in paper: [explicit] The paper mentions that the inclusion of diverse natural language QA pairs (DWU and DWUR) not only improves webpage understanding but also slightly enhances general visual language understanding without degrading performance.
- Why unresolved: While the paper demonstrates the benefits of including QA pairs for webpage understanding, it does not explore the specific mechanisms by which this improves general visual language understanding.
- What evidence would resolve it: Conducting ablation studies to isolate the effects of QA pairs on general visual language understanding, and analyzing the types of QA pairs that contribute most to this improvement, would provide insights into the underlying mechanisms.

### Open Question 5
- Question: How do different LLM backbones perform on webpage understanding and code generation tasks, and what factors contribute to their relative strengths and weaknesses?
- Basis in paper: [explicit] The paper compares the performance of various LLM backbones (CrystalCoder-7B, CrystalChat-7B, Vicuna1.5-7B, LLaMA3-8B) on webpage understanding and code generation tasks, but does not provide a detailed analysis of the factors influencing their performance differences.
- Why unresolved: While the paper presents performance comparisons, it does not explore the underlying factors that contribute to the strengths and weaknesses of different LLM backbones in these tasks.
- What evidence would resolve it: Conducting a comprehensive analysis of the architectural and training differences between LLM backbones, and their correlation with performance on webpage understanding and code generation tasks, would provide insights into the factors influencing their relative effectiveness.

## Limitations

- Dataset generation relies heavily on GPT models, introducing potential biases and quality concerns without systematic error analysis.
- Evaluation framework's visual similarity approach via GPT-4V may not fully capture semantic correctness or functional aspects of generated code.
- Study focuses only on English webpages, limiting generalizability to multilingual web content.
- Experiments use relatively small model sizes (7B parameters) without exploring scaling effects or longer training durations.

## Confidence

- **High confidence**: The dataset construction methodology and benchmark design are well-specified and reproducible. The claim that Web2Code improves webpage understanding and code generation performance on the proposed benchmarks is well-supported by experimental results.
- **Medium confidence**: The claim that Web2Code improves general multimodal reasoning without degradation is supported but based on a limited set of benchmarks. The effectiveness of the dataset for real-world deployment scenarios requires further validation.
- **Low confidence**: The quality and diversity of the automatically generated dataset are difficult to assess without independent verification. The long-term impact on model capabilities beyond the tested benchmarks remains uncertain.

## Next Checks

1. **Dataset Quality Audit**: Conduct a human evaluation study to assess the quality, diversity, and potential biases in the automatically generated instruction-response pairs. Sample 100 pairs from each subset (DWU, DWUR, DWCG, DWCGR) and have independent annotators rate them on relevance, accuracy, and diversity.

2. **Cross-Domain Generalization Test**: Evaluate models trained on Web2Code on webpage datasets from different domains (e.g., e-commerce, news, educational) not represented in the training data. Measure performance degradation or improvement to assess true generalization capability.

3. **Functional Code Validation**: Implement a systematic evaluation of generated HTML code beyond visual similarity. Test whether the generated code is syntactically correct, renders properly across different browsers, and maintains responsive design properties. Use automated testing tools to verify CSS properties, JavaScript functionality, and cross-browser compatibility.