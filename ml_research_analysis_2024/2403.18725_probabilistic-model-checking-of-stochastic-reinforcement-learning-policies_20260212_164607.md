---
ver: rpa2
title: Probabilistic Model Checking of Stochastic Reinforcement Learning Policies
arxiv_id: '2403.18725'
source_url: https://arxiv.org/abs/2403.18725
tags:
- policy
- state
- checking
- policies
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method to verify stochastic reinforcement
  learning (RL) policies using probabilistic model checking. The method constructs
  an induced Markov Decision Process (MDP) and transforms it into a Discrete-Time
  Markov Chain (DTMC) to verify safety properties specified by probabilistic computation
  tree logic (PCTL) formulas.
---

# Probabilistic Model Checking of Stochastic Reinforcement Learning Policies

## Quick Facts
- arXiv ID: 2403.18725
- Source URL: https://arxiv.org/abs/2403.18725
- Reference count: 13
- Primary result: Proposed method verifies stochastic RL policies with accurate safety measurements while being faster than naive monolithic checking and more precise than deterministic estimation

## Executive Summary
This paper introduces a method to verify stochastic reinforcement learning policies using probabilistic model checking. The approach constructs an induced Markov Decision Process (MDP) from the original environment MDP and trained RL policy, then transforms it into a Discrete-Time Markov Chain (DTMC) to verify safety properties specified by PCTL formulas. The method incrementally builds the induced MDP by exploring only reachable states via actions with non-zero policy probability, which keeps the state space manageable while capturing all policy behavior. Results show the method provides more precise verification than deterministic estimation while being faster than naive monolithic model checking across benchmarks like Freeway, Crazy Climber, and Avoidance.

## Method Summary
The method integrates model checking with reinforcement learning by first constructing an induced MDP from the original MDP and trained RL policy, considering only actions with non-zero selection probability. This induced MDP is then transformed into an induced DTMC by updating transition probabilities based on both the modified MDP and the policy's action distribution. The Storm model checker is used to verify safety properties specified by PCTL formulas against this induced DTMC. The approach is compared to baseline methods including deterministic safety estimates (which only consider the highest-probability action) and naive monolithic model checking (which builds the entire MDP).

## Key Results
- Verified stochastic RL policies with accurate safety measurements (e.g., P(♦goal) = 0.7 for Freeway policy)
- Deterministic estimation was faster but less precise than the proposed method
- Naive monolithic model checking was less precise but used more states and transitions
- Method works across multiple benchmarks: Freeway, Crazy Climber, and Avoidance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Accurately verifies stochastic RL policies by incrementally building induced MDP using only actions with non-zero policy probability
- Mechanism: Starts from initial state and explores only reachable states via actions with non-zero probability, keeping state space manageable while capturing all policy behavior
- Core assumption: RL policy obeys Markov property and actions with zero probability never occur
- Evidence anchors: [abstract] leveraging MDP, trained policy, and PCTL formula; [section 4.1] iterative state exploration with non-zero probability actions
- Break condition: If policy is not memoryless or assigns very small but non-zero probabilities to many actions

### Mechanism 2
- Claim: Induced DTMC transformation correctly captures policy's probabilistic behavior by weighting transitions with action selection probabilities
- Mechanism: Transition probability to next state is multiplied by policy's probability of choosing that action, creating weighted sum over all possible actions
- Core assumption: Transition function is deterministic or already probabilistic, policy's action distribution is independent of transition dynamics
- Evidence anchors: [section 4.2] transition function equation showing weighted sum; [abstract] leveraging MDP, policy, and PCTL formula
- Break condition: If MDP's transition function already incorporates action probabilities or policy's action distribution depends on next state

### Mechanism 3
- Claim: Provides more precise verification than deterministic estimation while being faster than naive monolithic checking
- Mechanism: Considers all actions with non-zero probability to capture full stochastic behavior, more accurate than highest-probability action but avoids building entire MDP
- Evidence anchors: [section 5.2.2] precise results on Crazy Climber vs. faster deterministic estimation; [abstract] comparison to baseline methods
- Break condition: If policy is nearly deterministic, performance gain over deterministic estimation diminishes

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: Understanding state, action, and transition representation in MDPs to construct induced model
  - Quick check question: In an MDP, what determines the next state given a current state and action?

- Concept: Probabilistic Computation Tree Logic (PCTL)
  - Why needed here: PCTL formulas specify safety properties that method verifies against induced DTMC
  - Quick check question: What is the difference between state formulas and path formulas in PCTL?

- Concept: Model Checking
  - Why needed here: Method uses Storm model checker to verify safety properties of induced DTMC
  - Quick check question: What is the primary goal of model checking in the context of verifying RL policies?

## Architecture Onboarding

- Component map: RL Environment (MDP) -> Trained RL Policy (π) -> PCTL Formula (m) -> Induced MDP Constructor -> Induced DTMC Transformer -> Storm Model Checker

- Critical path:
  1. Start with MDP, policy, and PCTL formula
  2. Construct induced MDP using only reachable states/actions
  3. Transform induced MDP to induced DTMC with weighted transitions
  4. Verify induced DTMC using Storm with PCTL formula
  5. Return verification result

- Design tradeoffs:
  - Precision vs. Performance: Considering all non-zero probability actions increases accuracy but may slow verification
  - Memory vs. Completeness: Incremental construction saves memory but may miss rare but important paths
  - Generality vs. Optimality: Method works with any memoryless policy but may not be optimal for specific policy types

- Failure signatures:
  - Model checking takes too long: Induced MDP/DTMC may be too large, consider using deterministic estimation
  - Verification result seems incorrect: Check if PCTL formula is properly specified or if policy violates Markov property
  - Out of memory error: Policy may assign non-zero probabilities to too many actions, consider pruning low-probability actions

- First 3 experiments:
  1. Verify simple grid world policy with basic safety property (e.g., avoid obstacle states)
  2. Compare verification results and performance between proposed method, deterministic estimation, and naive monolithic checking on small benchmark
  3. Test method's scalability by verifying policy in larger environment (e.g., Freeway) with complex safety property (e.g., reach goal with probability > 0.5)

## Open Questions the Paper Calls Out

- Question: How does scalability of proposed method compare to other probabilistic model checking approaches for high-dimensional state spaces?
  - Basis in paper: [inferred] Primary limitations stem from increasing number of states and transitions; optimizing incremental building may increase performance
  - Why unresolved: No direct comparison of scalability with other approaches; potential for optimization not explored
  - What evidence would resolve it: Empirical results comparing scalability to other probabilistic model checking approaches in high-dimensional state spaces

- Question: Can method be extended to verify RL policies that don't strictly adhere to Markov property?
  - Basis in paper: [explicit] Method requires policy to obey Markov property; doesn't explore extending to handle memory or partial observability
  - Why unresolved: Doesn't investigate feasibility or challenges of extending to non-Markovian policies
  - What evidence would resolve it: Theoretical analysis or empirical results demonstrating extension to verify policies with memory or partial observability

- Question: How does method perform computationally when verifying policies in environments with continuous state and action spaces?
  - Basis in paper: [inferred] Method limited to discrete spaces due to discrete model checking; doesn't explore extending to continuous spaces
  - Why unresolved: Doesn't investigate computational efficiency in continuous state and action spaces
  - What evidence would resolve it: Empirical results comparing computational efficiency in discrete vs. continuous state and action spaces

## Limitations
- Method depends on Markov property of RL policy - may fail if policies violate memorylessness
- Scalability concerns for policies with many non-zero probability actions creating intractable state spaces
- Lack of detailed specification for PCTL formulas and environment abstractions creates reproducibility challenges

## Confidence
- **High confidence** in core transformation mechanism (Mechanism 1 and 2) due to clear mathematical formulation and direct implementation evidence
- **Medium confidence** in performance comparisons (Mechanism 3) as benchmark results are presented but not extensively validated across diverse scenarios
- **Low confidence** in scalability claims due to limited testing on very large state spaces

## Next Checks
1. Test method on policies that explicitly violate Markov property (e.g., using history-dependent features) to determine failure modes
2. Implement systematic study of verification time vs. policy determinism - measure how verification time scales as policies become more stochastic
3. Verify method's sensitivity to PCTL formula specification by deliberately introducing errors in formula syntax and observing verification behavior