---
ver: rpa2
title: What is it for a Machine Learning Model to Have a Capability?
arxiv_id: '2405.08989'
source_url: https://arxiv.org/abs/2405.08989
tags:
- which
- capabilities
- ability
- arxiv
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides the first systematic philosophical account\
  \ of what it means for an ML model to have a capability. Drawing on the philosophical\
  \ literature on ability modals, the authors develop the Conditional Analysis of\
  \ Model Abilities (CAMA), which states that a model has a capability to \u03C6 just\
  \ when it would reliably succeed at \u03C6ing if it \"tried.\" The authors operationalize\
  \ this framework in the context of LLMs and show how it can be used to evaluate\
  \ capabilities, distinguish between attempts and non-attempts, and perform fair\
  \ inter-model comparisons."
---

# What is it for a Machine Learning Model to Have a Capability?

## Quick Facts
- arXiv ID: 2405.08989
- Source URL: https://arxiv.org/abs/2405.08989
- Authors: Jacqueline Harding; Nathaniel Sharadin
- Reference count: 40
- Primary result: The paper provides the first systematic philosophical account of what it means for an ML model to have a capability, introducing the Conditional Analysis of Model Abilities (CAMA).

## Executive Summary
This paper addresses the fundamental question of what it means for a machine learning model to have a capability. Drawing on philosophical literature on ability modals, the authors develop the Conditional Analysis of Model Abilities (CAMA), which states that a model has a capability to φ just when it would reliably succeed at φing if it "tried." The framework provides a systematic way to evaluate model capabilities, distinguish between attempts and non-attempts, and perform fair inter-model comparisons. The authors operationalize this framework for large language models and demonstrate how it can be used to evaluate capabilities while making sense of various features of ML model evaluation practice.

## Method Summary
The paper develops CAMA as a philosophical framework for understanding model capabilities, then operationalizes it through behavioral tests. The core approach involves perturbing inputs in ways that are relevant or irrelevant to the capability in question, then testing whether the model's output changes appropriately. If outputs are sensitive to φ-relevant changes and insensitive to φ-irrelevant ones, they are explained by being directed at φcing. The framework distinguishes between performance (actual success) and competence (ability to succeed if trying), enabling fair comparisons across models by finding conditions where each model tries to perform the capability.

## Key Results
- Introduces the first systematic philosophical account of model capabilities through CAMA
- Provides a framework for distinguishing between actual capability and coincidental success
- Offers a method for fair inter-model comparisons by standardizing assessments of capabilities
- Demonstrates how to evaluate whether a model "tries" to perform a capability through counterfactual behavior testing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CAMA provides a systematic way to evaluate whether a model "tries" to perform a capability by testing counterfactual behavior.
- Mechanism: By perturbing inputs in ways that are relevant or irrelevant to the capability, we can test whether the model's output changes appropriately. If the output is sensitive to φ-relevant changes and insensitive to φ-irrelevant ones, it is explained by being directed at φcing.
- Core assumption: The explanation of an output by its being directed at φcing can be tested behaviorally through perturbation experiments.
- Evidence anchors:
  - [abstract] "Our core proposal is a conditional analysis of model abilities (CAMA): crudely, a machine learning model has a capability to X just when it would reliably succeed at doing X if it 'tried'."
  - [section 4.2] "Explaining an 'action' (where the scare quotes signal non-commitment to whether model outputs are properly conceived of as actions) by reference to the end to which it is directed is 'teleological explanation'."
  - [corpus] Weak evidence; no related papers directly discuss behavioral perturbation tests for model capabilities.
- Break condition: If perturbations cannot distinguish between φcing and other actions, or if the model's internal states are too opaque to assess explanation quality.

### Mechanism 2
- Claim: CAMA distinguishes between performance (actual success) and competence (ability to succeed if trying), enabling fair inter-model comparisons.
- Mechanism: Models may succeed or fail due to background conditions (performance) rather than their underlying capabilities (competence). CAMA accounts for this by testing whether the model tries to φ under various conditions.
- Core assumption: Background conditions affect performance but not competence; therefore, fair comparison requires finding conditions where each model tries to φ.
- Evidence anchors:
  - [section 3.2] "This distinction is regularly elided in evaluations of ML model capabilities... there are many cases where a model fails to φ that do not undermine the claim that the model is able to φ."
  - [section 5.2.3] "In making inter-model comparisons, we are interested in standardising an assessment of capabilities... background conditions don't influence competence, but that they do influence performance."
  - [corpus] No direct evidence; related papers focus on benchmarking but not on distinguishing competence vs performance.
- Break condition: If background conditions that enable trying are gerrymandered or impossible to replicate across models.

### Mechanism 3
- Claim: CAMA solves the "coincidence" problem by requiring outputs to be best explained by being directed at φcing rather than other actions.
- Mechanism: Some outputs appear to succeed at φ by coincidence (e.g., repeating a memorized answer). CAMA requires that outputs be best explained by being directed at φcing, ruling out coincidental success.
- Core assumption: Coincidence occurs when an output is better explained by some action ψ other than φcing.
- Evidence anchors:
  - [section 3.3] "Intuitively, neither interaction counts as evidence that the model is able to perform two-digit addition... the model produces the correct answer coincidentally."
  - [section 4.2] "Our view is that if a model isn't 'trying' to φc, this is true because there's some alternative action ψc that better explains the observed behaviour."
  - [corpus] No direct evidence; related papers do not address coincidence in model outputs.
- Break condition: If φcing and ψcing produce indistinguishable outputs across all perturbations, or if we cannot assess which explanation is better.

## Foundational Learning

- Concept: Ability modals and their philosophical analysis
  - Why needed here: CAMA builds on philosophical work on ability modals to define what it means for a model to have a capability.
  - Quick check question: What is the difference between a circumstantial reading and a non-circumstantial reading of ability claims?

- Concept: Operationalisation constructs and benchmarks
  - Why needed here: CAMA requires specifying operationalisation constructs for capabilities and understanding benchmarks' limitations.
  - Quick check question: How does ecological validity of an operationalisation construct affect the interpretation of model capabilities?

- Concept: Teleological explanation and counterfactual prediction
  - Why needed here: CAMA uses teleological explanation to assess whether an output is explained by being directed at φcing, which can be tested via counterfactual predictions.
  - Quick check question: How can we test whether an output is best explained by being directed at φcing using perturbations?

## Architecture Onboarding

- Component map:
  - CAMA evaluation protocol (Definition 12)
    - Input: Model, operationalisation construct c, capability φ
    - Process: Generate queries, process inputs, test explanation, reject non-trying outputs
    - Output: Assessment of whether model can φ
  - Behavioral test for explanation of outputs (Definition 11)
    - Input: Model output, perturbations to input
    - Process: Check sensitivity to φ-relevant and φ-irrelevant changes
    - Output: Whether output is explained by being directed at φcing

- Critical path:
  1. Define operationalisation construct c for capability φ
  2. Generate queries qφc
  3. For each background condition set B:
     a. Process inputs and get outputs
     b. Test whether outputs are explained by being directed at φcing
     c. Reject outputs not explained this way
  4. Assess whether model reliably succeeds on remaining outputs
  5. Conclude whether model can φ

- Design tradeoffs:
  - Behavioral vs internal state tests: Behavioral tests (Definition 11) are easier to implement but may miss some cases; internal state tests are more precise but require interpretability tools.
  - Coarse vs fine-grained perturbations: Coarse perturbations are easier to implement but may miss subtle distinctions; fine-grained perturbations are more precise but harder to assess.

- Failure signatures:
  - Model passes test but fails in deployment: Operationalisation construct lacks ecological validity
  - Model fails test but can φ: Behavioral test is too coarse or perturbations don't capture relevant distinctions
  - Model passes but for wrong reasons: Outputs are coincidentally correct rather than explained by φcing

- First 3 experiments:
  1. Test whether a model trained on arithmetic data can perform addition on unseen problems, using perturbations to check if it's adding vs. memorizing.
  2. Compare two models on sentiment classification, finding background conditions where each tries to classify sentiment and comparing success rates.
  3. Evaluate whether a model can generate hate speech, testing if outputs are explained by being directed at generating harmful content vs. other actions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the behavioral test in Definition 11 (testing for φc-relevant and φc-irrelevant perturbations) be fully automated, or will human judgment always be required in some capacity?
- Basis in paper: [explicit] The authors note that Definition 11 "is just one possible concretisation" of the explanation condition and acknowledge its limitations, including that "it may sometimes be difficult to recognise whether a perturbation is sufficiently φc-(ir)relevant."
- Why unresolved: The paper presents Definition 11 as a concrete behavioral test but doesn't fully explore its practical implementation challenges or the extent to which human judgment might be needed.
- What evidence would resolve it: Empirical studies testing automated versus human-evaluated perturbation methods on various capabilities, measuring agreement rates and identifying edge cases where automation fails.

### Open Question 2
- Question: How can we measure the reliability threshold for capability possession (as required by Definition 12) in a principled way that accounts for different types of capabilities?
- Basis in paper: [explicit] The authors note that "the evidence accumulated using the CAMA protocol will be more reliable the more queries on which the model tries" and that "we should be wary of making claims about the model's ability to φc in domains in which it only 'tries' to φc on some very narrow range of queries."
- Why unresolved: While the paper emphasizes reliability, it doesn't provide specific guidance on how to set reliability thresholds or how these might vary across different capability types.
- What evidence would resolve it: Comparative studies of capability assessments using different reliability thresholds, examining how threshold choices affect false positive/negative rates across capability categories.

### Open Question 3
- Question: Can the CAMA framework be extended to evaluate capabilities in non-text modalities (vision, robotics) while maintaining its philosophical foundations?
- Basis in paper: [inferred] The authors note that "all examples we consider involve text-only LLMs, but can be naturally extended to vision-language models" and discuss the need to identify appropriate empirical tests for "trying" in different model classes.
- Why unresolved: The paper develops the framework specifically for LLMs but acknowledges it could apply to other models without detailing how the framework would need to adapt.
- What evidence would resolve it: Case studies applying CAMA to evaluate capabilities in vision-only, robotics, or multimodal models, documenting necessary modifications to the framework.

## Limitations
- The behavioral test for explanation may be too coarse to distinguish between φcing and other actions when outputs appear similar
- The framework requires finding appropriate background conditions for fair inter-model comparisons, which may be difficult or impossible
- The paper lacks empirical validation of CAMA, relying primarily on conceptual arguments and thought experiments

## Confidence

**High confidence:** The distinction between performance and competence is well-established in philosophy of action

**Medium confidence:** The formal definitions of CAMA are coherent, but empirical validation is lacking

**Low confidence:** Whether the behavioral test for explanation can actually work in practice

## Next Checks

1. **Empirical perturbation test**: Take a model trained on arithmetic data and test whether perturbations can distinguish between addition and memorization for unseen problems. Document cases where the test succeeds vs. fails.

2. **Inter-model comparison**: Select two LLMs with different capabilities and attempt to find background conditions where each tries to perform a specific task. Compare their success rates and assess whether the comparison is fair.

3. **Coincidence detection**: Generate outputs that appear to succeed at φ by coincidence (e.g., memorized answers) and test whether CAMA correctly identifies them as not explained by being directed at φcing.