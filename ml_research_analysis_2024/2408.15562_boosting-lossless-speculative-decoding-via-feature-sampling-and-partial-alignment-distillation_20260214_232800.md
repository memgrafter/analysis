---
ver: rpa2
title: Boosting Lossless Speculative Decoding via Feature Sampling and Partial Alignment
  Distillation
arxiv_id: '2408.15562'
source_url: https://arxiv.org/abs/2408.15562
tags:
- draft
- fspad
- target
- feature
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of accelerating large language
  model (LLM) inference using lossless speculative decoding, where a lightweight draft
  model generates candidates that are verified by the target LLM. The core method,
  FSPAD, introduces two components within the EAGLE-2 framework: Feature Sampling
  and Partial Alignment Distillation.'
---

# Boosting Lossless Speculative Decoding via Feature Sampling and Partial Alignment Distillation

## Quick Facts
- arXiv ID: 2408.15562
- Source URL: https://arxiv.org/abs/2408.15562
- Authors: Lujun Gui; Bin Xiao; Lei Su; Weipeng Chen
- Reference count: 3
- Primary result: FSPAD achieves 0.28-0.48 additional tokens per step compared to EAGLE-2 across all tested tasks

## Executive Summary
This paper addresses the challenge of accelerating large language model (LLM) inference using lossless speculative decoding, where a lightweight draft model generates candidates that are verified by the target LLM. The authors introduce FSPAD, which enhances the EAGLE-2 framework with two components: Feature Sampling and Partial Alignment Distillation. Feature Sampling addresses the inherent uncertainty in target LLM features by sampling token embeddings in high-dimensional space, while Partial Alignment Distillation weakens the connection between features and logits during training to reduce conflicts between feature alignment and logit confidence. The method consistently outperforms state-of-the-art approaches across diverse tasks including multi-turn conversation, translation, summarization, question answering, mathematical reasoning, and retrieval-augmented generation.

## Method Summary
FSPAD operates within the EAGLE-2 framework by introducing a Feature Sampler that uses token embeddings to sample features of the target LLM in high-dimensional space, addressing the inherent uncertainty of features. The method also implements Partial Alignment Distillation to weaken the draft model's connection between features and logits, reducing conflicts between feature alignment and logit confidence during training. The draft model is trained using a composite loss function with a reduced logit-level loss coefficient (0.02 instead of 0.1) and AdamW optimizer with learning rate 5e-5. The approach adds 0.18B to 1.10B extra parameters for target models ranging from 7B to 70B parameters.

## Key Results
- FSPAD consistently outperforms state-of-the-art methods across all tested tasks and target LLMs
- On multi-turn conversation, translation, summarization, question answering, mathematical reasoning, and retrieval-augmented generation tasks with Vicuna and LLaMA3-Instruct models
- FSPAD achieves 0.28-0.48 additional tokens per step compared to EAGLE-2
- On the summarization task with Vicuna 33B, FSPAD demonstrates a 12.3% enhancement in average acceptance length compared to EAGLE-2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature Sampling addresses the inherent uncertainty in target LLM features by sampling token embeddings in high-dimensional space.
- Mechanism: The target LLM's features are high-dimensional and continuous, making it difficult to determine the exact token output. Feature Sampling maps the feature and token embedding sequences to a higher-dimensional space using projectors, then performs sampling in this space to obtain inputs that better represent the target LLM's behavior.
- Core assumption: The distribution of tokens within features is the primary information gain from token embeddings to features, and this distribution can be preserved through sampling in a higher-dimensional space.
- Evidence anchors:
  - [abstract] "Feature Sampling utilizes token embeddings to sample features of the target LLM in high-dimensional space before feeding them into the draft model, due to the inherent uncertainty of the features preventing the draft model from obtaining the specific token output by the target LLM."
  - [section] "In an ideal scenario, we aim to mark the tokens of the target LLMs' sampling results while preserving the token distribution within the feature. However, features are high-dimensional and continuous, and the number of token categories they need to represent is generally much greater than the dimensionality of the features themselves."
- Break condition: If the projector dimensions are not sufficiently large to separate token distributions, or if the sampling operation fails to preserve the token distribution information.

### Mechanism 2
- Claim: Partial Alignment Distillation weakens the connection between features and logits during training to reduce conflicts between feature alignment and logit confidence.
- Mechanism: The draft model outputs two feature sequences - one aligned with the target LLM features and another used for logit prediction. By separating these outputs, the model reduces the interference between feature alignment and logit confidence during training.
- Core assumption: There is a conflict between feature-level and logit-level losses during training, and weakening their connection can improve overall performance.
- Evidence anchors:
  - [abstract] "FSPAD introduces partial alignment distillation to weaken the draft model's connection between features and logits, aiming to reduce the conflict between feature alignment and logit confidence during training."
  - [section] "We first modify the coefficient w of the logit-level loss in the joint loss during the training of EAGLE. When w is reduced from 0.1 to 0.02, we observe a significant decrease in feature-level loss, but the prediction accuracy during training decreases. In Figure 3, we further illustrate the training process after weakening the feature and logit correlation using Partial Alignment Distillation in FSPAD. Weakening the connection between features and logits can reduce feature-level loss while improving prediction accuracy during training."
- Break condition: If the separation of feature and logit paths becomes too extreme, causing loss of information transfer between them.

### Mechanism 3
- Claim: The combination of Feature Sampling and Partial Alignment Distillation improves draft model performance by addressing both input representation and training objective conflicts.
- Mechanism: Feature Sampling provides better inputs to the draft model by preserving token distribution information through high-dimensional sampling, while Partial Alignment Distillation optimizes the training process by separating feature alignment and logit confidence objectives.
- Core assumption: Both the input representation problem and the training objective conflict are significant factors limiting draft model performance, and addressing both simultaneously will yield greater improvements than addressing either alone.
- Evidence anchors:
  - [abstract] "FSPAD (Feature Sampling and Partial Alignment Distillation for Lossless Speculative Decoding), which introduces two straightforward and effective components within the existing framework to boost lossless speculative decoding."
  - [section] "FSPAD introduces Feature Sampling and Partial Alignment Distillation within the framework of EAGLE-2 to boost speculative decoding."
- Break condition: If either component is removed, the combined improvement effect disappears or significantly diminishes.

## Foundational Learning

- Concept: Large Language Model inference acceleration through speculative decoding
  - Why needed here: Understanding the baseline speculative decoding framework is essential to grasp how FSPAD improves upon it
  - Quick check question: How does speculative decoding split the inference process between draft and verification phases?

- Concept: Feature-level autoregressive generation vs token-level generation
  - Why needed here: FSPAD builds upon EAGLE's approach of using feature-level autoregression, so understanding this distinction is crucial
  - Quick check question: What advantage does feature-level autoregression have over token-level autoregression in draft models?

- Concept: Knowledge distillation in language models
  - Why needed here: FSPAD uses feature alignment as a form of knowledge distillation, so understanding this concept is important
  - Quick check question: What is the purpose of using feature-level loss during training in speculative decoding?

## Architecture Onboarding

- Component map: Input features → Feature Sampler → Draft Model → Partial Alignment Distillation → Output features and logits
- Critical path: Input features → Feature Sampler → Draft Model → Partial Alignment Distillation → Output features and logits
- Design tradeoffs:
  - Higher-dimensional space sampling increases computational overhead but improves input representation
  - Separating feature and logit paths adds minimal parameters but requires careful training balance
  - Autoregressive draft model is slower than parallel approaches but achieves higher accuracy
- Failure signatures:
  - If Feature Sampling fails: Draft model performance degrades to baseline EAGLE-2 levels
  - If Partial Alignment Distillation fails: Training becomes unstable with oscillating losses
  - If both fail: No improvement over standard speculative decoding methods
- First 3 experiments:
  1. Replace Feature Sampler with simple linear combination (EAGLE-2 baseline) and measure performance drop
  2. Remove Partial Alignment Distillation by using single output feature and observe training instability
  3. Test different projector dimensions in Feature Sampler to find optimal balance between accuracy and efficiency

## Open Questions the Paper Calls Out
No explicit open questions were called out in the paper.

## Limitations
- Feature Sampler Dimensionality: The exact dimensions of projectors are not specified, making it difficult to assess computational overhead and effectiveness
- Feature-Space vs. Logit-Space Performance Trade-offs: Limited analysis of how FSPAD affects final text quality beyond acceptance length metrics
- Dataset Dependency: All experiments use SharedGPT dataset without investigation of generalization to other domains or languages

## Confidence

- High Confidence: The empirical results showing consistent improvement over EAGLE-2 across all tested tasks and model sizes
- Medium Confidence: The mechanism explanation for Feature Sampling, which relies on high-dimensional sampling preserving token distribution information
- Low Confidence: The claim that Partial Alignment Distillation specifically reduces conflict between feature alignment and logit confidence

## Next Checks

1. **Dimensionality Sensitivity Analysis**: Conduct ablation studies systematically varying the projector dimensions in the Feature Sampler to determine the optimal configuration and assess whether improvements scale with dimensionality or plateau at certain sizes.

2. **Logit-Level Quality Evaluation**: Implement human evaluation studies comparing final generated outputs from EAGLE-2, FSPAD, and direct target LLM inference on tasks requiring complex reasoning, measuring coherence, relevance, and task completion quality rather than just acceptance length.

3. **Cross-Dataset Generalization Test**: Evaluate FSPAD on multiple diverse datasets including multilingual corpora, technical documentation, and creative writing to determine whether the method's effectiveness is dataset-dependent and identify the characteristics that influence its performance.