---
ver: rpa2
title: 'Text2Traj2Text: Learning-by-Synthesis Framework for Contextual Captioning
  of Human Movement Trajectories'
arxiv_id: '2409.12670'
source_url: https://arxiv.org/abs/2409.12670
tags:
- aj70
- customer
- aj84
- aj74
- aj66
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel learning-by-synthesis framework,
  Text2Traj2Text, for contextual captioning of human movement trajectories in retail
  environments. The framework leverages large language models (LLMs) to synthesize
  realistic and diverse contextual captions paired with corresponding movement trajectories
  on store maps.
---

# Text2Traj2Text: Learning-by-Synthesis Framework for Contextual Captioning of Human Movement Trajectories

## Quick Facts
- arXiv ID: 2409.12670
- Source URL: https://arxiv.org/abs/2409.12670
- Authors: Hikaru Asano; Ryo Yonetani; Taiki Sekii; Hiroki Ouchi
- Reference count: 22
- Key outcome: Novel learning-by-synthesis framework that uses LLMs to synthesize realistic contextual captions paired with movement trajectories, then fine-tunes a T5 model that generalizes to real human trajectories and outperforms competitive approaches.

## Executive Summary
This paper introduces Text2Traj2Text, a learning-by-synthesis framework for contextual captioning of human movement trajectories in retail environments. The approach leverages large language models (LLMs) to synthesize realistic and diverse contextual captions paired with corresponding movement trajectories on store maps. The framework consists of two phases: (1) TEXT2TRAJ, where LLMs generate contextual captions and concrete trajectories via a trajectory planner, and (2) TRAJ2TEXT, where a captioning model is fine-tuned on the synthesized data. Systematic evaluation demonstrates that the captioning model generalizes well to real human trajectories and captions, outperforming competitive approaches in terms of ROUGE and BERT Score metrics.

## Method Summary
The method involves a two-phase learning-by-synthesis framework. In phase 1 (TEXT2TRAJ), LLMs generate diverse contextual captions and instantiate coarse action plans from these captions. A trajectory planner then traces these plans to generate feasible movement trajectories on a store map. In phase 2 (TRAJ2TEXT), a T5 encoder-decoder model is fine-tuned on the synthesized data. Data augmentation through paraphrasing is used to increase caption diversity. The framework is evaluated on real human trajectory data and compared against baseline approaches including GPT-3.5/4 and Llama2 using in-context learning.

## Key Results
- The captioning model trained on fully synthesized data generalizes well to real human trajectories and captions
- Achieved best performance with fewer parameters compared to GPT family and Llama2 models
- Demonstrated effectiveness on unseen store maps, showcasing practical applicability
- Monotonic improvement in metrics as number of paraphrases increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM synthesizes realistic and diverse contextual captions paired with movement trajectories via hierarchical planning
- Mechanism: LLM generates high-level action plans and contextual captions, while a classical trajectory planner instantiates concrete movement paths that align with those plans
- Core assumption: LLM reasoning about shopping behavior can produce plans that are compatible with feasible motion trajectories in store layouts
- Evidence anchors: [abstract] "synthesize a diverse and realistic collection of contextual captions as well as the corresponding movement trajectories on a store map"; [section 3.1] "an LLM first creates diverse contextual captions and instantiates coarse action plans from the captions. A trajectory planner then traces the plans to generate feasible movement trajectories on a store map."

### Mechanism 2
- Claim: Fine-tuning a T5 encoder-decoder model on synthesized trajectory-caption pairs enables generalization to real human trajectories and captions
- Mechanism: Training a sequence-to-sequence model to map trajectory representations (locations, items in contact, purchases) to captions, leveraging the diversity of synthetic data
- Core assumption: The diversity and realism of synthesized trajectories/captions are sufficient to cover the real distribution of human shopping behavior
- Evidence anchors: [abstract] "Despite learned from fully synthesized data, the captioning model can generalize well to trajectories/captions created by real human subjects"; [section 4.2] "our model achieved the best performance even with an order-of-magnitude fewer parameters compared to the GPT family and Llama2"

### Mechanism 3
- Claim: Paraphrasing synthetic captions during data augmentation increases diversity and improves model robustness
- Mechanism: For each synthetic caption, the LLM generates alternative expressions with similar meaning, and the trajectory is relabeled accordingly
- Core assumption: Paraphrased captions preserve semantic intent while introducing lexical variation, improving the model's ability to handle natural language variation
- Evidence anchors: [section 3.2] "we introduce data augmentation by paraphrasing; for each annotated trajectory, we let the LLM to produce alternative expressions of the caption with similar meanings"; [section 4.2] "A monotonic improvement in nearly all metrics as the number of paraphrases increases, indicating the effectiveness of our data augmentation strategy"

## Foundational Learning

- Concept: Large language models as reasoning engines for planning tasks
  - Why needed here: LLMs generate structured plans and contextual descriptions from abstract goals, bridging the gap between semantic intent and concrete trajectories
  - Quick check question: Can the LLM reliably produce shopping plans that respect store layout constraints and customer preferences?

- Concept: Sequence-to-sequence fine-tuning for multimodal generation
  - Why needed here: T5 is used to map structured trajectory data (text) to natural language captions, requiring the ability to handle both structured input and free-form output
  - Quick check question: Does the T5 model handle the variable length and mixed modality of trajectory input correctly?

- Concept: Data augmentation through paraphrasing
  - Why needed here: Increases caption diversity without requiring additional trajectory generation, improving robustness to language variation
  - Quick check question: Do paraphrased captions remain semantically aligned with their original captions and trajectories?

## Architecture Onboarding

- Component map: Prompt engineering → LLM (captions, action plans, item lists) → Trajectory planner (global + local) → concrete paths → T5 fine-tuning pipeline → learned captioning model → Evaluation suite (ROUGE, BERT Score)

- Critical path: 1. Generate captions via LLM 2. Convert to action plans and item lists 3. Trajectory planner produces movement paths 4. Encode trajectory + item data into T5 input format 5. Fine-tune T5 on synthetic dataset 6. Evaluate on real human trajectories

- Design tradeoffs: LLM choice (GPT-4 for plan generation vs cost/compute; GPT-3.5 for paraphrasing to save cost), Fine-tuning vs in-context learning (T5 offers lower parameter count and better scalability), Paraphrasing depth (more paraphrases increase diversity but also training cost)

- Failure signatures: LLM generates plans that violate spatial constraints → trajectory planner fails or produces unrealistic paths, Paraphrased captions drift semantically → model learns incorrect associations, Input encoding loses critical trajectory information → poor caption quality

- First 3 experiments: 1. Generate 10 synthetic trajectories with minimal paraphrasing; evaluate ROUGE/BERT Score. 2. Test model robustness to noise: perturb 5% of trajectory points and measure degradation. 3. Ablation: Train without item information and without trajectory data separately; compare performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance degrade when the trajectory data contains significant noise or errors from indoor localization systems?
- Basis in paper: [explicit] The paper mentions evaluating the model's robustness to 5% noise in trajectory data
- Why unresolved: The evaluation only tested 5% noise, which may not represent the full range of potential errors in real-world localization systems
- What evidence would resolve it: Testing the model with various levels of trajectory noise (e.g., 10%, 20%, 50%) and analyzing performance degradation would provide a clearer understanding of its robustness limits

### Open Question 2
- Question: Can the framework be extended to handle trajectories with more complex patterns, such as multiple visits to the same store or combining data from multiple stores?
- Basis in paper: [inferred] The paper focuses on single-visit trajectories in a single store map, but does not explore more complex scenarios
- Why unresolved: The current formulation and experiments are limited to simple, single-visit trajectories in one store
- What evidence would resolve it: Experiments with multi-visit trajectories and cross-store trajectory data would demonstrate the framework's ability to handle more complex shopping patterns

### Open Question 3
- Question: How does the diversity of synthesized data impact the model's ability to generalize to unseen customer behaviors and preferences?
- Basis in paper: [explicit] The paper discusses data augmentation through paraphrasing to increase caption diversity
- Why unresolved: While the paper shows that paraphrasing improves performance, it does not explore the relationship between synthesized data diversity and generalization to unseen behaviors
- What evidence would resolve it: Systematic experiments varying the diversity of synthesized data (e.g., by using different LLMs or introducing more variability in trajectory generation) and measuring generalization to unseen customer profiles would clarify this relationship

## Limitations
- Synthetic data fidelity relies heavily on LLM reasoning about shopping behavior, which may not capture real-world shopping patterns across diverse demographics
- Trajectory planner assumptions may not handle edge cases like crowded store conditions or unusual customer behaviors
- Evaluation focuses on text similarity metrics (ROUGE, BERT Score) rather than qualitative assessment of caption relevance or human evaluation of contextual accuracy

## Confidence
- High Confidence: The core mechanism of using LLM-generated synthetic data for training downstream captioning models is well-supported, with convincing empirical results showing superior performance over baseline approaches
- Medium Confidence: Generalizability to unseen store maps is demonstrated but based on limited store layouts, and paraphrasing effectiveness is shown but semantic drift risk isn't thoroughly explored
- Low Confidence: Claims about contextual accuracy and relevance of generated captions to actual human shopping intent are not directly validated through human evaluation studies or real-world deployment testing

## Next Checks
1. **Cross-Demographic Validation**: Test the framework's performance across diverse demographic groups and cultural contexts by generating synthetic data that reflects varied shopping patterns and evaluating the model's ability to capture these differences
2. **Edge Case Robustness**: Systematically evaluate the trajectory planner's ability to handle challenging scenarios such as crowded store conditions, special events, or unusual customer behaviors that might not be well-represented in the synthetic data
3. **Human Evaluation Study**: Conduct a comprehensive human evaluation study to assess not just the linguistic quality of generated captions but their contextual relevance and accuracy in explaining real human shopping behavior