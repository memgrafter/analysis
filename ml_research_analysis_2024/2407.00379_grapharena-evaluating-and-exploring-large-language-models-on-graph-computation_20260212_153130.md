---
ver: rpa2
title: 'GraphArena: Evaluating and Exploring Large Language Models on Graph Computation'
arxiv_id: '2407.00379'
source_url: https://arxiv.org/abs/2407.00379
tags:
- graph
- llms
- graphs
- tasks
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces GraphArena, a benchmark designed to evaluate\
  \ large language models (LLMs) on graph computation problems. The benchmark includes\
  \ 10 tasks\u20144 polynomial-time and 6 NP-complete\u2014using real-world graphs\
  \ from sources like DBLP, social networks, and molecular structures."
---

# GraphArena: Evaluating and Exploring Large Language Models on Graph Computation

## Quick Facts
- arXiv ID: 2407.00379
- Source URL: https://arxiv.org/abs/2407.00379
- Authors: Jianheng Tang; Qifan Zhang; Yuhan Li; Nuo Chen; Jia Li
- Reference count: 40
- Primary result: Introduces GraphArena benchmark evaluating LLMs on 10 graph computation tasks using real-world graphs, revealing significant limitations particularly on NP-complete problems

## Executive Summary
GraphArena is a comprehensive benchmark designed to evaluate large language models' capabilities in graph computation problems. The benchmark includes 10 tasks spanning both polynomial-time and NP-complete graph problems, using real-world graphs from sources like DBLP, social networks, and molecular structures. A rigorous evaluation framework categorizes LLM outputs based on solution paths, distinguishing between correct, suboptimal, hallucinatory, and missing responses. Experiments with 10 LLMs reveal that even top-performing models struggle significantly with larger, more complex graph problems, particularly NP-complete tasks, with hallucination rates increasing dramatically as graph size grows.

## Method Summary
GraphArena evaluates LLMs on 10 graph computation tasks using real-world graphs from five sources: DBLP, social networks, DBpedia, OpenFlights, and PubChemQC. The evaluation framework extracts solution paths from LLM responses using regular expressions, then verifies feasibility and optimality through classical algorithms. Performance is measured across multiple metrics including accuracy, feasibility, hallucination rates, and MRR. The benchmark tests both small and large graph variants to reveal scaling challenges, with particular focus on how hallucination rates increase with graph complexity.

## Key Results
- LLMs show significant performance degradation on NP-complete tasks compared to polynomial-time tasks
- Hallucination rates increase nearly monotonically with graph size, from 16% to over 80% for larger graphs
- Real-world graphs present substantially more challenging problems than synthetic alternatives
- Four improvement approaches (chain-of-thought, instruction tuning, code writing, scaling) show promise but have limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GraphArena reveals LLM limitations in graph reasoning by requiring path-based solution extraction rather than final answer matching
- Mechanism: The evaluation framework parses the entire solution path, checks feasibility, and verifies optimality, distinguishing between correct, suboptimal, hallucinatory, and missing responses
- Core assumption: LLM performance can be meaningfully differentiated by examining solution paths rather than just numerical outputs
- Evidence anchors: [abstract] "GraphArena features a rigorous evaluation framework that classifies LLM outputs as correct, suboptimal (feasible but not optimal), hallucinatory (properly formatted yet infeasible), or missing." [section] "Our evaluation process consists of three steps: (1) Path Extraction: We use regular expressions to extract the proposed solution path from the LLM's response... (2) Feasibility Check: We employ scripts to verify whether the extracted path meets the basic requirements... (3) Optimality Verification: For feasible paths, we calculate a path score... and compare to the optimal solution obtained through exhaustive search."

### Mechanism 2
- Claim: GraphArena's use of real-world graphs increases evaluation difficulty and diversity compared to synthetic graphs
- Mechanism: Real-world graphs from DBLP, social networks, molecular structures, and knowledge graphs preserve complex topological features that synthetic ER graphs cannot capture
- Core assumption: Real-world graph topology presents more challenging reasoning scenarios than synthetic alternatives
- Evidence anchors: [section] "GraphArena distinguishes itself from previous benchmarks by utilizing real-world graphs, offering richer diversity compared with synthetic ones. Graphs are collected from five sources: DBLP... Social Network... DBpedia... OpenFlights... and PubChemQC..." [section] "We employ a random walk with restart strategy initiating from randomly selected nodes to sample subgraphs effectively... Each problem in GraphArena features a subgraph sampled from a rich assortment of real-world graphs..."

### Mechanism 3
- Claim: GraphArena reveals scaling challenges by showing performance degradation with increasing graph size and NP-complete task complexity
- Mechanism: The benchmark includes both small and large graph variants for each task, revealing how hallucination rates increase monotonically with node count
- Core assumption: Graph size and computational complexity directly impact LLM reasoning capability and hallucination frequency
- Evidence anchors: [abstract] "Evaluation of over 10 LLMs reveals that even top-performing LLMs struggle with larger, more complex graph problems and exhibit hallucination issues." [section] "Figure 4 demonstrates a significant, nearly monotonic increase in hallucination ratio as node size grows from 5 to 30. For example, in the Diameter task, GPT-4o's hallucination probability rises dramatically from 16% at a node size of 5 to more than 80% at a node size of 30."

## Foundational Learning

- Concept: Graph theory fundamentals (nodes, edges, paths, connectivity, graph traversal algorithms)
  - Why needed here: Understanding these concepts is essential for interpreting the tasks and evaluation criteria in GraphArena
  - Quick check question: What distinguishes a connected component from a path in a graph?

- Concept: NP-completeness and polynomial-time complexity classes
  - Why needed here: The benchmark specifically contrasts polynomial-time tasks with NP-complete challenges to evaluate different reasoning capabilities
  - Quick check question: Why would an NP-complete problem like TSP be more challenging for an LLM than a polynomial-time shortest path problem?

- Concept: Regular expressions and text parsing for path extraction
  - Why needed here: The evaluation framework relies on regex to extract solution paths from LLM responses before feasibility checking
  - Quick check question: How would you design a regex pattern to extract a path like [A, B, C, D] from free text?

## Architecture Onboarding

- Component map: Graph sampling module → Problem generation template → LLM inference → Path extraction → Feasibility checker → Optimality verifier → Metrics calculator
- Critical path: Problem generation → LLM inference → Path extraction → Feasibility check → Evaluation
- Design tradeoffs: Real-world graphs provide diversity but are harder to control vs synthetic graphs that are reproducible but may lack complexity
- Failure signatures: High hallucination rates, inconsistent performance across graph sizes, poor scalability with problem complexity
- First 3 experiments:
  1. Run baseline evaluation on synthetic graphs vs real-world graphs to measure performance difference
  2. Test different graph tokenizers (edge list vs adjacency matrix) to optimize LLM input format
  3. Implement and evaluate code-writing prompting approach to reduce hallucination in complex tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to GNNs would enable them to achieve competitive performance on GraphArena tasks while maintaining their computational efficiency advantages?
- Basis in paper: [inferred] The paper mentions that GNNs were compared to LLMs but performed poorly, and that recent studies have proven limitations in GNNs' computational capabilities for graph problems
- Why unresolved: The paper only compared standard GNN architectures (GIN, GAT, GraphSAGE) without exploring task-specific modifications that could enhance their performance on graph reasoning tasks
- What evidence would resolve it: Empirical results showing that modified GNN architectures with task-specific designs can outperform both standard GNNs and LLMs on GraphArena tasks while maintaining reasonable computational efficiency

### Open Question 2
- Question: How do different graph tokenization methods (edge list, adjacency list, adjacency matrix) affect LLM performance on graph computation tasks, and is there an optimal tokenization strategy that generalizes across different graph problems?
- Basis in paper: [explicit] The paper explicitly compares three graph tokenizers (Edge List, Adjacency List, Adjacency Matrix) and shows that Edge List performs best and most stably across tasks
- Why unresolved: While the paper shows Edge List performs well, it only tested this on two tasks (Shortest Distance and TSP) and didn't explore whether this finding generalizes to all 10 GraphArena tasks or if other tokenization methods might be superior for specific problem types
- What evidence would resolve it: Comprehensive evaluation of all three tokenization methods across all 10 GraphArena tasks to identify which tokenization strategy provides optimal performance for each task type

### Open Question 3
- Question: What is the relationship between hallucination rates and graph complexity metrics (such as graph density, diameter, or node degree distribution) beyond simple node count, and how can this inform the design of more robust graph reasoning systems?
- Basis in paper: [inferred] The paper shows that hallucination rates increase with graph size, but doesn't explore other graph complexity metrics that might contribute to hallucination
- Why unresolved: The analysis only considers graph size (node count) as a factor affecting hallucination rates, without examining how other structural properties of graphs might influence LLM performance and hallucination
- What evidence would resolve it: Empirical studies correlating hallucination rates with various graph complexity metrics (density, diameter, clustering coefficient, degree distribution) to identify which structural features most strongly predict hallucination likelihood

## Limitations
- The evaluation framework relies on regular expressions for path extraction, which may miss nuanced errors or be vulnerable to ambiguous LLM outputs
- Real-world graphs from specific sources may not represent the full diversity of graph structures encountered in practice
- The evaluation approach requires complete solution paths, potentially disadvantaging LLMs that solve problems through different reasoning formats

## Confidence
- **High Confidence**: The systematic evaluation framework is well-specified and reproducible; performance degradation with graph size is clearly demonstrated; the benchmark successfully identifies LLM limitations
- **Medium Confidence**: Real-world graphs provide meaningfully more challenging problems than synthetic alternatives; the four improvement approaches have genuine potential; hallucination rates are primarily driven by graph complexity
- **Low Confidence**: The exact impact of each improvement method relative to others; whether observed limitations represent fundamental barriers or solvable challenges; the extent to which current LLM architectures can be adapted for better graph reasoning

## Next Checks
1. Test the benchmark on additional real-world graph datasets not included in the original collection to verify that performance patterns hold across different graph domains and distributions
2. Implement and compare alternative evaluation approaches that don't rely on path extraction, such as direct numerical answer comparison or end-to-end task completion metrics, to validate the robustness of the current framework
3. Systematically vary both model size and test-time compute across multiple orders of magnitude to precisely characterize the scaling relationships for different graph problem types, particularly focusing on the NP-complete tasks where performance is weakest