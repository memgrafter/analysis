---
ver: rpa2
title: 'Rs4rs: Semantically Find Recent Publications from Top Recommendation System-Related
  Venues'
arxiv_id: '2409.05570'
source_url: https://arxiv.org/abs/2409.05570
tags:
- rs4rs
- search
- papers
- recommender
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Rs4rs is a web application for semantic search of recent papers
  from top recommender systems venues. It addresses the problem of broad, low-quality
  results from existing search engines by focusing exclusively on high-tier conferences
  and journals, and using semantic search with SBERT embeddings to match queries to
  papers regardless of wording variations.
---

# Rs4rs: Semantically Find Recent Publications from Top Recommendation System-Related Venues

## Quick Facts
- arXiv ID: 2409.05570
- Source URL: https://arxiv.org/abs/2409.05570
- Reference count: 20
- Primary result: Web application for semantic search of recent papers from top recommender systems venues

## Executive Summary
Rs4rs is a web application designed to help researchers find recent publications from top recommender systems venues through semantic search. The system addresses the limitations of traditional search engines that often return broad, low-quality results by focusing exclusively on high-tier conferences and journals. By leveraging semantic search technology with SBERT embeddings, Rs4rs can match queries to relevant papers regardless of wording variations, providing researchers with quick access to high-quality, recent publications in the field.

## Method Summary
The system implements semantic search using SBERT embeddings to understand query context and match papers based on semantic similarity rather than exact keyword matching. It pre-computes paper embeddings during batch processing and ranks results online using averaged cosine similarity from two SBERT models (all-mpnet-base-v2 and msmarco-distilroberta-base-v2). The system focuses on six top-tier venues: RecSys, SIGIR, CIKM, WSDM, UMAP, and TOIS, ensuring high-quality results while maintaining computational efficiency through pre-processing.

## Key Results
- Provides semantic search capability for finding relevant papers regardless of wording variations
- Pre-computes paper embeddings to enable fast online query processing
- Offers filtering options and direct access to full-text papers through integrated search functionality

## Why This Works (Mechanism)
The system leverages semantic search technology to overcome the limitations of traditional keyword-based search engines. By using SBERT embeddings, the system can understand the semantic meaning of both queries and papers, allowing it to match related concepts even when different terminology is used. The pre-computation of embeddings during batch processing enables fast query response times, while the averaging of cosine similarities from two different SBERT models provides more robust ranking of search results.

## Foundational Learning
- **SBERT embeddings**: Why needed - to capture semantic meaning of text; Quick check - verify embeddings capture semantic similarity through cosine distance
- **Cosine similarity**: Why needed - to measure semantic similarity between query and paper embeddings; Quick check - ensure scores are in [-1,1] range and higher means more similar
- **Batch processing**: Why needed - to pre-compute expensive embeddings for fast online queries; Quick check - verify batch processing completes within acceptable time limits
- **Averaging model outputs**: Why needed - to combine strengths of different SBERT architectures; Quick check - compare results with single model baseline
- **Venue filtering**: Why needed - to ensure high-quality, relevant results; Quick check - verify venue list covers major recommender systems conferences

## Architecture Onboarding

**Component map**: User Query -> Text Processing -> Embedding Retrieval -> Similarity Calculation -> Ranking -> Results Display

**Critical path**: The system processes user queries by first cleaning and tokenizing the input text, retrieving pre-computed paper embeddings from the database, calculating cosine similarities using both SBERT models, averaging the similarity scores, and finally ranking papers by descending similarity scores before displaying the results.

**Design tradeoffs**: The system prioritizes result quality over comprehensive coverage by limiting searches to top venues, trades storage space for speed by pre-computing embeddings, and uses two SBERT models to improve robustness at the cost of additional computation during ranking.

**Failure signatures**: Poor search results may indicate issues with text preprocessing, corrupted embeddings in the database, or problems with the SBERT models. Slow query response times suggest issues with database retrieval or similarity computation. Empty result sets could indicate problems with venue filtering or paper metadata.

**First experiments**: 1) Test basic keyword search functionality with known papers, 2) Verify semantic search works by searching for related concepts with different terminology, 3) Measure query response time with varying numbers of papers in the database

## Open Questions the Paper Calls Out
None

## Limitations
- Limited venue scope excludes relevant work from other conferences and journals
- Semantic search approach may introduce inconsistency between different SBERT models
- Performance metrics and effectiveness comparison with traditional search methods not reported

## Confidence
- High confidence: The system exists and provides a web interface for searching papers from specified venues
- Medium confidence: The semantic search approach improves result relevance compared to traditional search engines
- Low confidence: The averaging of two SBERT model similarities is the optimal approach for this task

## Next Checks
1. Conduct user studies comparing Rs4rs search results with traditional search engines (Google Scholar, Semantic Scholar) for common recommender systems research queries
2. Perform ablation studies to determine if using two SBERT models provides significant improvement over single model approaches
3. Expand the venue coverage and evaluate the impact on result quality and system performance, particularly including domain-specific workshops and journals in related fields like information retrieval and machine learning