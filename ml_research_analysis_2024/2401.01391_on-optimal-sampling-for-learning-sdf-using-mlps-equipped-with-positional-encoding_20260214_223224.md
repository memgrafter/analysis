---
ver: rpa2
title: On Optimal Sampling for Learning SDF Using MLPs Equipped with Positional Encoding
arxiv_id: '2401.01391'
source_url: https://arxiv.org/abs/2401.01391
tags:
- sampling
- frequency
- network
- rate
- fitting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the cause of noisy artifacts in neural implicit
  representations learned using Multi-layer Perceptrons (MLPs) with positional encoding
  (PE). The authors observe that these artifacts arise from undersampling high-frequency
  components in the MLP's response frequency during training, following an aliasing
  effect.
---

# On Optimal Sampling for Learning SDF Using MLPs Equipped with Positional Encoding

## Quick Facts
- **arXiv ID**: 2401.01391
- **Source URL**: https://arxiv.org/abs/2401.01391
- **Reference count**: 40
- **Primary result**: Proposes optimal sampling rate estimation method for PE-equipped MLPs that suppresses aliasing artifacts and outperforms state-of-the-art SDF learning methods

## Executive Summary
This paper addresses noisy artifacts in neural implicit representations learned using Multi-layer Perceptrons (MLPs) with positional encoding (PE). The authors identify that these artifacts stem from undersampling high-frequency components in the MLP's response frequency during training, following an aliasing effect. They propose a method to estimate the intrinsic frequency spectrum of a PE-equipped MLP using randomized weights, determine a cut-off frequency from this spectrum, and use it to recommend an optimal sampling rate based on the Nyquist-Shannon sampling theorem. Experiments on signed distance field (SDF) fitting demonstrate that training with this recommended sampling rate effectively suppresses artifacts and produces high-quality SDF representations, outperforming several state-of-the-art methods.

## Method Summary
The proposed method estimates the intrinsic frequency spectrum of a PE-equipped MLP by evaluating the network's response to random inputs drawn from a uniform distribution. This process is repeated multiple times with different random weight initializations, and the frequency spectra are averaged to obtain the intrinsic spectrum. A smooth curve is fitted to this spectrum using a function of the form y(x) = a/(x^2 + b), and the cut-off frequency is determined as the point where the slope of the fitted curve drops below 6×10^-4. The recommended sampling rate is then calculated as twice the cut-off frequency, following the Nyquist-Shannon sampling theorem. During training, samples are generated at this recommended rate to ensure all significant frequency components are captured, thereby suppressing aliasing artifacts.

## Key Results
- PE-equipped MLPs exhibit intrinsic frequencies much higher than the highest frequency component in the positional encoding layer alone
- Sampling at the recommended rate (based on cut-off frequency) effectively suppresses aliasing artifacts in learned SDFs
- The method outperforms state-of-the-art SDF learning approaches in terms of both artifact suppression and representation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Noisy artifacts in PE-equipped MLPs arise from undersampling high-frequency components in the MLP's response frequency during training.
- Mechanism: When the sampling rate is insufficient relative to the highest frequency components in the MLP's response, aliasing occurs. The loss function only minimizes error on the aliased (low-frequency) components, leaving high-frequency components untrained. These untrained high-frequency components manifest as artifacts at inference time.
- Core assumption: The MLP's response frequency contains significantly higher frequencies than the positional encoding layer alone.
- Evidence anchors:
  - [abstract] "we observe that these artifacts arise from undersampling high-frequency components in the MLP's response frequency during training, following an aliasing effect"
  - [section III-B] "When an insufficient sampling rate with respect to hP E(x) is applied for training hP E(x), it will result in an aliased function of lower frequency"
- Break condition: If the MLP's architecture inherently limits its maximum frequency response below the PE layer's highest frequency, aliasing would not occur.

### Mechanism 2
- Claim: PE-equipped MLPs with the same architecture exhibit similar intrinsic frequency spectra regardless of random weight initialization.
- Mechanism: The network architecture itself determines the frequency response characteristics. Random weight initialization creates different specific weights but preserves the same frequency distribution pattern in the network's response.
- Core assumption: The nonlinear activation functions and network depth/width create consistent frequency response patterns across different random initializations.
- Evidence anchors:
  - [abstract] "We observe that a PE-equipped MLP has an intrinsic frequency much higher than the highest frequency component in the PE layer"
  - [section III-C] "When a network is assigned different sets of random weights, which are drawn from the same distribution, their frequency spectra are similar to each other"
- Break condition: If the network uses extreme random initialization schemes (very large or very small weights) that saturate activation functions, the frequency response pattern could differ.

### Mechanism 3
- Claim: The cut-off frequency determined from the intrinsic spectrum provides an optimal sampling rate that balances artifact suppression with computational efficiency.
- Mechanism: The cut-off frequency identifies where the spectral energy becomes negligible. Sampling at twice this frequency (per Nyquist-Shannon) ensures all significant frequency components are captured without oversampling.
- Core assumption: The intrinsic spectrum's tail contains negligible energy beyond the cut-off frequency, so higher sampling rates provide diminishing returns.
- Evidence anchors:
  - [abstract] "determine a cut-off frequency from this spectrum. This cut-off frequency is used to recommend an optimal sampling rate"
  - [section III-C] "We derive the cut-off frequency as follows... We use the following function to fit the spectrum... y(x) = a/(x^2 + b)"
- Break condition: If the target signal contains frequencies beyond the cut-off frequency that are significant, undersampling would still occur.

## Foundational Learning

- Concept: Nyquist-Shannon Sampling Theorem
  - Why needed here: Provides the theoretical foundation for determining the minimum sampling rate needed to avoid aliasing artifacts
  - Quick check question: What sampling rate is required to accurately capture a signal with maximum frequency F?

- Concept: Fourier Analysis of Neural Networks
  - Why needed here: Enables analysis of the frequency response characteristics of MLPs with positional encoding
  - Quick check question: How does applying a multilayer perceptron to a sinusoidal input affect the frequency spectrum?

- Concept: Positional Encoding in Neural Networks
  - Why needed here: Explains why standard MLPs struggle with high-frequency details and how PE addresses this limitation
  - Quick check question: What is the highest frequency component in a positional encoding with D=5?

## Architecture Onboarding

- Component map:
  Input (3D coordinates) -> Positional Encoding Layer -> MLP Body -> Output (Signed distance value) -> Frequency Analysis Module

- Critical path:
  1. Initialize PE-MLP with random weights
  2. Generate dense samples along coordinate axes
  3. Compute FFT of network outputs
  4. Calculate intrinsic spectrum (expectation over random weights)
  5. Fit smooth curve to spectrum
  6. Determine cut-off frequency where slope < 6×10^-4
  7. Calculate recommended sampling rate = 2 × cut-off frequency
  8. Generate training samples at recommended rate
  9. Train network using standard loss

- Design tradeoffs:
  - Higher D in positional encoding captures more high-frequency details but increases network complexity
  - Larger MLP networks can represent higher frequencies but require more training samples
  - More training samples improve accuracy but increase computational cost

- Failure signatures:
  - Wavy artifacts in learned SDF indicate undersampling
  - Over-smoothed surfaces indicate insufficient network capacity
  - Excessive training time with minimal improvement indicates oversampling

- First 3 experiments:
  1. Generate intrinsic spectrum for default architecture (8-layer MLP, D=5) and verify cut-off frequency
  2. Train SDF on simple shape using recommended sampling rate vs. insufficient sampling rate
  3. Compare SDF fitting error vs. sampling rate curve to identify convergence point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the intrinsic frequency spectrum of PE-equipped MLPs vary with different activation functions beyond softplus and sine?
- Basis in paper: [inferred] The paper focuses on softplus activation and briefly mentions sine activation in SIREN. It analyzes the intrinsic frequency spectrum but does not explore other activation functions.
- Why unresolved: The study primarily uses softplus and mentions sine activation but does not systematically compare the intrinsic frequency spectra across a range of activation functions.
- What evidence would resolve it: Conduct experiments using various activation functions (e.g., ReLU, tanh, Swish) in PE-equipped MLPs and analyze their intrinsic frequency spectra to determine if the sampling rate recommendations hold.

### Open Question 2
- Question: What is the impact of different positional encoding schemes (e.g., B-spline, Fourier features) on the intrinsic frequency spectrum and optimal sampling rate?
- Basis in paper: [inferred] The paper focuses on sinusoidal positional encoding and briefly mentions B-spline and Fourier features but does not analyze their effects on the intrinsic frequency spectrum.
- Why unresolved: While the paper mentions alternative positional encoding schemes, it does not provide a comparative analysis of their intrinsic frequency spectra or optimal sampling rates.
- What evidence would resolve it: Perform experiments using different positional encoding schemes in PE-equipped MLPs, analyze their intrinsic frequency spectra, and determine if the sampling rate recommendations are consistent across schemes.

### Open Question 3
- Question: How does the complexity of the target shape (e.g., geometric details, frequency content) influence the intrinsic frequency spectrum and optimal sampling rate of PE-equipped MLPs?
- Basis in paper: [inferred] The paper uses a variety of shapes but does not explicitly analyze the relationship between shape complexity and the intrinsic frequency spectrum of the learned MLP.
- Why unresolved: The study uses different shapes but does not provide a systematic analysis of how shape complexity affects the intrinsic frequency spectrum and optimal sampling rate.
- What evidence would resolve it: Conduct experiments using shapes with varying levels of geometric complexity and frequency content, analyze the intrinsic frequency spectra of the learned MLPs, and determine if the sampling rate recommendations are robust across different shape complexities.

## Limitations
- The method's effectiveness for implicit representations beyond SDFs (e.g., occupancy fields, radiance fields) remains untested
- The frequency analysis using randomized weights assumes architecture-dependent frequency response, but this needs validation across diverse architectures
- The optimal sampling rate may not generalize well to shapes with frequency content beyond the estimated cut-off frequency

## Confidence

- **High Confidence**: The Nyquist-Shannon sampling theorem application and the fundamental mechanism of aliasing artifacts in undersampled networks are well-established in signal processing theory.
- **Medium Confidence**: The method for estimating intrinsic frequency spectrum using randomized weights is theoretically sound but has limited empirical validation beyond the SDF fitting experiments presented.
- **Medium Confidence**: The claim that PE-equipped MLPs exhibit similar intrinsic frequency spectra across random initializations is supported by the paper's analysis but would benefit from testing on a wider range of architectures.

## Next Checks

1. **Cross-Domain Validation**: Apply the recommended sampling rate method to neural implicit representations beyond SDFs, such as neural radiance fields (NeRF) or occupancy networks, to verify the generalizability of the aliasing artifact suppression.

2. **Architecture Sensitivity Analysis**: Systematically vary the MLP architecture (depth, width, activation functions) and positional encoding parameters to quantify how these choices affect the intrinsic frequency spectrum and optimal sampling rate recommendations.

3. **Ablation Study on Weight Initialization**: Compare the intrinsic frequency spectra and artifact suppression performance when using different weight initialization schemes (e.g., He, Xavier, orthogonal) to validate the claim that architecture dominates frequency response characteristics.