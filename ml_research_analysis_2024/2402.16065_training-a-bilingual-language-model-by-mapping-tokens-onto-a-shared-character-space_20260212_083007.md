---
ver: rpa2
title: Training a Bilingual Language Model by Mapping Tokens onto a Shared Character
  Space
arxiv_id: '2402.16065'
source_url: https://arxiv.org/abs/2402.16065
tags:
- arabic
- language
- hebrew
- both
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HeArBERT, a bilingual Arabic-Hebrew language
  model where Arabic text is transliterated into the Hebrew script during pre-training.
  The authors aim to leverage the shared morphological structure and cognates between
  Arabic and Hebrew by unifying their representation in a single script, similar to
  Judeo-Arabic.
---

# Training a Bilingual Language Model by Mapping Tokens onto a Shared Character Space

## Quick Facts
- arXiv ID: 2402.16065
- Source URL: https://arxiv.org/abs/2402.16065
- Reference count: 0
- Primary result: HeArBERT achieves BLEU scores of 25.28 (Arabic→Hebrew) and 21.17 (Hebrew→Arabic) in machine translation, outperforming multilingual baselines

## Executive Summary
This paper introduces HeArBERT, a bilingual Arabic-Hebrew language model that leverages shared morphological structure and cognates between the languages by transliterating Arabic text into the Hebrew script during pre-training. The authors demonstrate that this approach improves cross-lingual knowledge transfer for machine translation tasks, achieving significant performance gains over both monolingual and multilingual baselines. The model is evaluated on the Kol Zchut parallel corpus, showing particular effectiveness for Arabic-to-Hebrew translation despite using approximately 60% less training data than comparable models.

## Method Summary
The HeArBERT model uses a BERT-base architecture with WordPiece tokenization (30,000 vocabulary size) and a limited alphabet of 100 characters. Arabic text is preprocessed using a lookup table to transliterate it into the Hebrew script before tokenization and pre-training. The model is pre-trained on the OSCAR dataset (approximately 3B Arabic words and 1B Hebrew words) and fine-tuned for machine translation on the Kol Zchut parallel corpus. The transliteration approach aims to unify cognates onto the same token space, enabling better cross-lingual representation learning.

## Key Results
- HeArBERT significantly outperforms HeArBERTNT (which keeps Arabic in original script) on translation tasks
- HeArBERT + GigaBERT combination achieves BLEU scores of 25.28 (Arabic→Hebrew) and 21.17 (Hebrew→Arabic)
- The model uses approximately 60% less training data than other language models yet delivers comparable performance
- Transliteration approach is particularly effective for Arabic-to-Hebrew translation direction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transliterating Arabic into Hebrew script aligns shared cognates onto the same token space.
- Mechanism: By mapping Arabic letters to their Hebrew equivalents, words that are cognates in both languages are represented identically in the model's vocabulary, enabling direct cross-lingual representation.
- Core assumption: The morphological and structural similarities between Arabic and Hebrew are sufficient for their shared cognates to be accurately represented in a unified script.
- Evidence anchors:
  - [abstract] The authors aim to leverage the shared morphological structure and cognates between Arabic and Hebrew by unifying their representation in a single script.
  - [section 1] Numerous lexicons have been created to record these cognates. One of those lexicons lists a total of 915 Hebrew-Arabic spelling equivalents, of which 435 have been identified as authentic cognates.
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.506, average citations=0.0.

### Mechanism 2
- Claim: Joint pre-training on transliterated texts improves cross-lingual knowledge transfer for machine translation.
- Mechanism: The model learns representations that capture the semantic and syntactic similarities between the two languages, which is beneficial for translation tasks that require understanding both languages.
- Core assumption: Pre-training on a shared script allows the model to learn more robust bilingual representations than training on separate scripts.
- Evidence anchors:
  - [abstract] The results are promising: our model outperforms a contrasting model which keeps the Arabic texts in the Arabic script, demonstrating the efficacy of the transliteration step.
  - [section 5] The results demonstrate that the second group, which utilizes our HeArBERT to initialize a Hebrew decoder, surpasses the performance of all other combinations.
  - [corpus] The average neighbor FMR is 0.506, indicating some relevance to the paper's topic but not a strong signal.

### Mechanism 3
- Claim: Using a shared script reduces the vocabulary size and improves tokenization efficiency.
- Mechanism: By limiting the accepted alphabet size to 100, the tokenizer focuses on content rather than special characters, encouraging the learning of tokens common to both languages.
- Core assumption: The shared script approach reduces the number of unique tokens needed to represent the vocabulary, leading to a more efficient model.
- Evidence anchors:
  - [section 3] We train a WordPiece tokenizer with a vocabulary size of 30,000, limiting its accepted alphabet size to 100.
  - [section 5] Comparing to the translation combination involving other language models, we see comparable results; this is encouraging given that the training data we utilized for pre-training the model is approximately 60% smaller than theirs.
  - [corpus] The corpus analysis shows some related work but does not provide strong evidence for this specific mechanism.

## Foundational Learning

- Concept: Transliteration
  - Why needed here: To map Arabic text into the Hebrew script, allowing for a unified representation of cognates.
  - Quick check question: What is the primary purpose of transliterating Arabic text into the Hebrew script in this model?

- Concept: Cognates
  - Why needed here: To identify and leverage the shared words between Arabic and Hebrew that have similar meanings and spellings.
  - Quick check question: How do cognates contribute to the effectiveness of the HeArBERT model?

- Concept: Tokenization
  - Why needed here: To break down the text into manageable units (tokens) for the language model to process.
  - Quick check question: Why is the WordPiece tokenizer chosen for this model, and how does it benefit from the shared script approach?

## Architecture Onboarding

- Component map: Arabic text → Transliteration → Tokenization → Pre-training → Fine-tuning for translation → Evaluation
- Critical path: The transliteration step is critical as it enables shared representation of cognates, which is the core innovation of the model
- Design tradeoffs: The shared script approach reduces vocabulary size and improves tokenization efficiency but may introduce transliteration ambiguities. The smaller pre-training dataset (60% smaller) may limit the model's ability to generalize.
- Failure signatures: Poor translation performance, especially in the Arabic-to-Hebrew direction, could indicate issues with transliteration accuracy or insufficient pre-training data.
- First 3 experiments:
  1. Evaluate the transliteration accuracy by comparing a sample of transliterated Arabic words to their Hebrew cognates.
  2. Test the tokenization efficiency by measuring the vocabulary size and tokenization time with and without the shared script approach.
  3. Assess the impact of the smaller pre-training dataset by comparing the model's performance to that of larger models on a benchmark translation task.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions. However, several implicit questions arise from the methodology and results.

## Limitations
- The paper relies on a simple lookup table for Arabic-to-Hebrew transliteration without systematic evaluation of transliteration accuracy.
- The model is evaluated only on machine translation tasks using the Kol Zchut corpus, limiting generalizability to other cross-lingual applications.
- The specific mechanism by which shared script representation aids learning is not directly validated through ablation studies.

## Confidence
- High Confidence: The core finding that HeArBERT outperforms HeArBERTNT (which keeps Arabic in original script) on translation tasks is well-supported by the empirical results.
- Medium Confidence: The claim that transliteration improves cross-lingual transfer is plausible but relies on several untested assumptions about cognate alignment and morphological similarity.
- Low Confidence: Claims about tokenization efficiency improvements and the specific mechanism by which shared script representation aids learning are not directly validated.

## Next Checks
1. **Transliteration Accuracy Audit**: Manually evaluate a random sample of 100 transliterated Arabic words against their Hebrew cognates to quantify transliteration accuracy and identify systematic errors that could impact model performance.

2. **Dataset Size Ablation**: Train HeArBERT variants with incrementally larger subsets of the OSCAR dataset (e.g., 25%, 50%, 75%, 100%) to empirically determine how performance scales with data size and validate the claim that the model performs well despite smaller training data.

3. **Cross-Task Generalization Test**: Evaluate HeArBERT on additional cross-lingual tasks beyond MT (e.g., cross-lingual document classification, named entity recognition) using standard benchmarks to determine whether the transliteration approach provides general cross-lingual benefits or is specific to translation.