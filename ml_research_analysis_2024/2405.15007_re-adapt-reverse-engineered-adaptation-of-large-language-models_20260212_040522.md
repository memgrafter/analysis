---
ver: rpa2
title: 'RE-Adapt: Reverse Engineered Adaptation of Large Language Models'
arxiv_id: '2405.15007'
source_url: https://arxiv.org/abs/2405.15007
tags:
- fine-tuning
- pretrained
- re-adapt
- adapters
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RE-Adapt enables fine-tuning of large language models on new domains
  without degrading instruction-following capabilities. It does so by isolating the
  differences between instruction-tuned and pretrained models into a reverse-engineered
  adapter, which can be reapplied after further fine-tuning.
---

# RE-Adapt: Reverse Engineered Adaptation of Large Language Models

## Quick Facts
- arXiv ID: 2405.15007
- Source URL: https://arxiv.org/abs/2405.15007
- Authors: William Fleshman; Benjamin Van Durme
- Reference count: 13
- Primary result: RE-Adapt enables fine-tuning on new domains without degrading instruction-following capabilities, achieving 46-54 Rouge-L on closed-book QA and 68-69 Rouge-L with retrieval-augmented generation.

## Executive Summary
RE-Adapt is a novel approach for adapting large language models to new domains while preserving their instruction-following capabilities. The method works by reverse engineering an adapter that captures what an instruction-tuned model has learned beyond its pretrained base. This adapter can be reapplied after fine-tuning the base model on new domain data, preventing catastrophic forgetting of instruction-following skills. Experiments demonstrate that RE-Adapt outperforms direct fine-tuning of pretrained or instruction-tuned models across multiple model sizes and datasets.

## Method Summary
The RE-Adapt method begins by computing the weight difference between an instruction-tuned model and its corresponding pretrained version, treating this difference as an "instruction adapter." To adapt to a new domain, only the base model is fine-tuned using DoRA (weight-decomposed low-rank adaptation), while the instruction adapter remains unchanged. After domain adaptation, the instruction adapter is reapplied to restore instruction-following capabilities. A low-rank variant, LoRE-Adapt, approximates the full instruction adapter using truncated SVD, achieving similar performance with up to 5x fewer parameters. The method also introduces partial adaptation scaling factors to control the strength of adapter combination, improving robustness to out-of-domain tasks.

## Key Results
- RE-Adapt achieves 46-54 Rouge-L on closed-book QA, outperforming direct fine-tuning of pretrained or instruction-tuned models.
- With retrieval-augmented generation, RE-Adapt reaches 68-69 Rouge-L, demonstrating strong performance on knowledge-intensive tasks.
- LoRE-Adapt maintains comparable performance to full RE-Adapt while using up to 5x fewer parameters, offering a memory-efficient alternative.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuned models can be decomposed into a pretrained base plus an "instruction adapter."
- Mechanism: The difference in weights between an instruction-tuned and corresponding pretrained model captures what was learned during instruction tuning, acting like a learned adapter.
- Core assumption: The weight difference between pretrained and instruction-tuned versions is stable enough to be reused after further fine-tuning.
- Evidence anchors:
  - [abstract] "We reverse engineer an adapter which isolates what an instruction-tuned model has learned beyond its corresponding pretrained base model."
  - [section] "We make the key observation that the difference in weights between an instruction-tuned and corresponding pretrained model is effectively an adapter."
- Break condition: If instruction tuning changes the base model in ways that are not linearly separable, the adapter will not be reusable.

### Mechanism 2
- Claim: Fine-tuning only the base model on new domain data and then reapplying the instruction adapter avoids catastrophic forgetting.
- Mechanism: By isolating instruction capabilities into a separate adapter, the base model can be updated with new knowledge without altering the instruction-following part. The adapter is reapplied after fine-tuning.
- Core assumption: The instruction adapter is independent of the base model parameters and can be added back without interference.
- Evidence anchors:
  - [abstract] "We can then fine-tune the base model on a new domain and readapt it to instruction following with the reverse engineered adapter."
  - [section] "We augment the pretrained model TΦ with a learnable adapter Ψ and fit TΦ+Ψ on a new domain by only updating the adapter weights Ψ."
- Break condition: If the new domain fine-tuning alters the base model in a way that conflicts with the instruction adapter, reapplication may fail or cause performance degradation.

### Mechanism 3
- Claim: LoRE-Adapters can approximate full RE-Adapters with fewer parameters, saving memory while maintaining performance.
- Mechanism: The singular value decomposition of the RE-Adapter reveals that most variance is captured at low rank, allowing truncation without major loss of capability.
- Core assumption: The instruction adapter is low-rank in nature, meaning most of its information can be represented with fewer parameters.
- Evidence anchors:
  - [section] "We explore the intrinsic dimensionality of RE-Adapters and their ability to be represented by low-rank approximations."
  - [section] "Figure 2 displays the cumulative explained variance plots for three layers from the RE-Adapter derived from Llama-3: we see more than half of the variance in these layers can be captured by a rank 128 approximation."
- Break condition: If the instruction adapter is not low-rank, truncation will lead to significant performance loss.

## Foundational Learning

- Concept: Linear algebra and matrix decomposition (SVD)
  - Why needed here: To analyze and construct low-rank approximations of the RE-Adapter.
  - Quick check question: If a matrix has singular values [10, 5, 2, 1], what percentage of total variance is captured by the top two singular values?
- Concept: Adapter-based fine-tuning (LoRA, DoRA)
  - Why needed here: RE-Adapt builds on adapter concepts and uses DoRA for the knowledge adapter.
  - Quick check question: In DoRA, why is it beneficial to decompose weights into magnitude and direction before applying LoRA?
- Concept: Catastrophic forgetting in continual learning
  - Why needed here: RE-Adapt aims to prevent forgetting of instruction-following capabilities during domain adaptation.
  - Quick check question: What is one technique, besides RE-Adapt, to mitigate catastrophic forgetting during fine-tuning?

## Architecture Onboarding

- Component map: Pretrained base model (TΦ) -> Instruction adapter (∆) -> Knowledge adapter (Ψ) -> Final model weights = Φ + αΨ + β∆
- Critical path:
  1. Load pretrained and instruction-tuned versions of the same model.
  2. Compute RE-Adapter by differencing weights.
  3. Train knowledge adapter on new domain data using DoRA.
  4. Combine adapters with partial adaptation scaling factors.
  5. Evaluate on target tasks.
- Design tradeoffs:
  - Full RE-Adapter vs. LoRE-Adapter: memory vs. performance.
  - Strong vs. weak partial adaptation: risk of overfitting vs. underfitting.
  - Training knowledge adapter from scratch vs. initializing from pretrained.
- Failure signatures:
  - Out-of-domain performance drops sharply → instruction adapter reapplication failed.
  - In-domain performance lags behind direct fine-tuning → knowledge adapter underfit.
  - High variance in ablation results → instability in adapter combination.
- First 3 experiments:
  1. Verify that weight differencing between pretrained and instruct models yields a usable adapter by applying it to the base model and measuring instruction-following performance.
  2. Fine-tune the base model on a small new domain dataset and test if reapplying the instruction adapter recovers full instruction-following ability.
  3. Test LoRE-Adapter truncation at different ranks to find the point where performance plateaus.

## Open Questions the Paper Calls Out
- How does the rank of RE-Adapters vary across different layers of large language models, and what implications does this have for the design of LoRE-Adapters?
- What is the impact of using RE-Adapt on tasks beyond question answering, such as summarization, translation, or code generation?
- How does the performance of RE-Adapt scale with model size, and are there diminishing returns for very large models?
- How sensitive is RE-Adapt to the choice of fine-tuning data, and does it perform equally well with structured versus unstructured data?
- What are the computational trade-offs of using LoRE-Adapters compared to full RE-Adapters, especially in terms of inference speed and memory usage?

## Limitations
- The core claim relies on the assumption that weight differences are linearly separable and independent of the base model, which may not hold for all instruction tuning mechanisms.
- Experiments focus on closed-book QA and retrieval-augmented generation, with limited validation on broader domain generalization.
- The low-rank assumption for LoRE-Adapt is supported empirically for tested layers but not explored across different model sizes or architectures.
- Prompt templates for evaluation are not provided, affecting reproducibility of exact performance numbers.

## Confidence
- High confidence in the mechanism of weight differencing to create an instruction adapter, as this is directly observable and validated by experiments.
- Medium confidence in the claim that LoRE-Adapt achieves comparable performance with fewer parameters, since results are strong but limited to specific model sizes and tasks.
- Low confidence in the generality of partial adaptation benefits, as ablation studies show it helps but the optimal scaling factors may be task-dependent.

## Next Checks
1. Test RE-Adapt on a broader set of domains and tasks (e.g., summarization, code generation) to confirm that the instruction adapter generalizes beyond QA.
2. Analyze the stability of RE-Adapter weights across different random seeds of instruction tuning to verify that the differencing approach is robust.
3. Evaluate LoRE-Adapt on larger models (e.g., Llama-3-70B) and compare the explained variance at different ranks to confirm the low-rank assumption holds at scale.