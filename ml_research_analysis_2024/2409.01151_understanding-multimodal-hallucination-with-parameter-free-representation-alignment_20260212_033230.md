---
ver: rpa2
title: Understanding Multimodal Hallucination with Parameter-Free Representation Alignment
arxiv_id: '2409.01151'
source_url: https://arxiv.org/abs/2409.01151
tags:
- image
- pfram
- llava-v1
- object
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the root cause of object hallucination
  in multimodal large language models (MLLMs) by introducing a parameter-free representation
  alignment metric (Pfram). The core idea is to measure the alignment between image
  representations from MLLMs and ground-truth object annotations without requiring
  additional training parameters.
---

# Understanding Multimodal Hallucination with Parameter-Free Representation Alignment

## Quick Facts
- arXiv ID: 2409.01151
- Source URL: https://arxiv.org/abs/2409.01151
- Reference count: 34
- Primary result: Object hallucination in MLLMs strongly correlates with object information in image representations, not model size or LLM capability

## Executive Summary
This paper introduces Pfram, a parameter-free representation alignment metric that quantifies how well image representations from multimodal large language models (MLLMs) align with ground-truth object annotations. The key finding is that object hallucination levels strongly correlate with the object information contained in image representations, as measured by Pfram. The study systematically analyzes 10 state-of-the-art MLLMs across two datasets, revealing that visual encoders contribute more to object recognition than LLMs, and that textual instructions impact object representations differently depending on whether they're used as visual encoder or LLM input.

## Method Summary
The method uses a parameter-free approach to measure alignment between image representations from MLLMs and human object annotations. It employs neighbor-based (Mutual k-NN) and ranking-based (NDCG@k) metrics to compare neural representations with ground truth annotations. The metric is calculated at both image-level and system-level, requiring no additional training parameters. Pfram scores are computed across different layers of MLLMs to identify which components contribute to object recognition, and experiments test the impact of textual instructions on representation quality.

## Key Results
- Object hallucination strongly correlates with Pfram scores measuring object information in image representations
- Visual encoder layers consistently improve object recognition while LLM layers plateau or degrade it
- Textual instructions impact object representations more significantly when used as visual encoder input versus LLM input
- Model size and projector type do not significantly affect object hallucination levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Object hallucination is strongly driven by the quality of object information in image representations rather than by model size or LLM capability.
- Mechanism: Pfram compares object-centric neural representations to human-labeled object annotations using neighbor-based or ranking-based similarity metrics, isolating the effect of visual understanding from other factors.
- Core assumption: Human object annotations perfectly preserve object information and can serve as ground truth for measuring object representation quality.
- Evidence anchors:
  - [abstract]: "object hallucination has a strong and consistent correlation with object information contained in image representations, which can be quantified by Pfram"
  - [section 4.2]: "Pfram(F ; Gobj) is an effective way to measure object information in image representations, and object information in image representations is indeed a crucial factor of object hallucinations of MLLMs"
  - [corpus]: Found 25 related papers; average neighbor FMR=0.471. Some related to alignment and hallucination mitigation, but Pfram approach appears novel.
- Break condition: If human annotations are incomplete or biased, Pfram may not accurately reflect true object information content.

### Mechanism 2
- Claim: Different modules in MLLMs contribute unequally to object recognition; visual encoders improve object recognition while LLMs plateau or degrade it.
- Mechanism: Pfram scores change systematically across layersâ€”visual encoder layers show steady improvement in object recognition, while LLM layers plateau or drop, indicating LLMs don't enhance object discrimination.
- Core assumption: Layer-wise representations can be meaningfully compared using Pfram to isolate module contributions.
- Evidence anchors:
  - [section 5.1]: "Layers in visual encoders, including QFormers, consistently improve object recognition as the Pfram metric steadily increases. Conversely, LLMs do not primarily contribute to object recognition, as indicated by the plateau in the Pfram metric."
  - [section 5.1]: "RLHF-V shows slight improvement in the higher layers of the visual encoder but exhibits significant improvement across all layers of the LLM in terms of Pfram."
  - [corpus]: Found 25 related papers; some focus on alignment and hallucination in MLLMs, but specific module contribution analysis appears novel.
- Break condition: If layer representations are not comparable or Pfram doesn't capture true object recognition capability.

### Mechanism 3
- Claim: Textual instructions impact object-level information in image representations differently depending on model architecture (QFormer vs LLM input).
- Mechanism: When instructions are used as QFormer input, they significantly alter object representation similarity (Pfram drops under different instructions, improves under same instruction). When used as LLM input, impact is weaker and less consistent.
- Core assumption: Image representations conditioned on different instructions can be meaningfully compared using Pfram to assess instruction impact.
- Evidence anchors:
  - [section 5.2]: "In InstructBLIP, where instructions are used as QFormer input, we observe a significant drop in Pfram when different instructions are applied to different images...When the same instruction is used for all images in a round of Pfram calculation, the Pfram metric improves slightly compared to when no instruction is used."
  - [section 5.2]: "In contrast, for LLaV A, where instructions are used as LLM input, the Pfram metric also decreases under setting 1 (red line), but the decline is not as significant as that in InstructBLIP."
  - [corpus]: Found 25 related papers; some on instruction tuning and MLLM alignment, but specific analysis of instruction impact on object representations appears novel.
- Break condition: If instructions don't meaningfully alter image representations or Pfram doesn't capture instruction impact.

## Foundational Learning

- Concept: Representation systems and similarity metrics
  - Why needed here: Pfram fundamentally compares different representation systems (neural vs human annotations) using similarity metrics to assess alignment.
  - Quick check question: What is the difference between an image representation and a representation system in the context of Pfram?

- Concept: Ranking-based vs neighbor-based evaluation metrics
  - Why needed here: Pfram uses both NDCG (ranking-based) and Mutual k-NN (neighbor-based) to measure system-level similarity, capturing different aspects of representation quality.
  - Quick check question: How does NDCG@k differ from Mutual k-NN in evaluating representation similarity?

- Concept: Multimodal model architecture components
  - Why needed here: Understanding how visual encoders, projectors, and LLMs interact is crucial for interpreting Pfram results across different MLLM components.
  - Quick check question: In an MLLM, which component is primarily responsible for converting image features into LLM-compatible tokens?

## Architecture Onboarding

- Component map: Image representation extraction -> Image-level similarity calculation -> System-level similarity aggregation -> Pfram score
- Critical path: 1) Extract image representations from MLLM layer, 2) Compute image-level similarity using Algorithm 1, 3) Compute system-level similarity using NDCG or Mutual k-NN, 4) Aggregate into Pfram score
- Design tradeoffs: Parameter-free design enables broad applicability but limits to ordinal comparisons; focusing on object information may miss other hallucination types; neighbor/ranking metrics require sufficient image diversity
- Failure signatures: Weak correlation between Pfram and hallucination suggests either annotation quality issues, inappropriate similarity metrics, or hallucination not driven by object representation quality
- First 3 experiments:
  1. Run Pfram on a single MLLM layer (e.g., input layer) with object annotations vs descriptions to verify strong correlation with hallucination benchmarks
  2. Compare Pfram scores across different MLLM components (ViT vs LLM layers) to identify which modules contribute to object recognition
  3. Test Pfram sensitivity to instruction presence/absence in different MLLM architectures to understand instruction impact on object representations

## Open Questions the Paper Calls Out

- Question: Does the Pfram metric's correlation with object hallucination generalize to other types of hallucinations (attributes, relations, scenes, generative)?
- Basis in paper: [inferred] The paper explicitly states in Section 6 that it focuses solely on object-level discriminative hallucinations and acknowledges other types remain uninvestigated.
- Why unresolved: The metric was only tested against object hallucination benchmarks (POPE), not other hallucination types.
- What evidence would resolve it: Testing Pfram against benchmarks for attribute/relation/scene/generative hallucinations and measuring correlation coefficients.

- Question: How does the Pfram metric perform when comparing neural representations to imperfect ground truth (noisy annotations, incomplete object labels)?
- Basis in paper: [inferred] The paper assumes human annotations perfectly preserve object information, but real-world annotations contain noise and incompleteness.
- Why unresolved: All experiments used high-quality datasets with controlled vocabularies and minimum object counts per image.
- What evidence would resolve it: Testing Pfram correlation with hallucination metrics on noisy or incomplete annotation datasets and measuring robustness.

- Question: Can the Pfram metric distinguish between hallucinations caused by visual encoder failures versus LLM context understanding failures?
- Basis in paper: [explicit] Section 5.1 shows visual encoders improve object recognition while LLMs plateau, but doesn't directly test whether Pfram can diagnose the source.
- Why unresolved: While Pfram scores correlate with overall hallucination, it's unclear if low Pfram in LLM layers specifically indicates LLM context understanding issues.
- What evidence would resolve it: Comparing Pfram layer-by-layer with ablation studies where visual encoders vs LLMs are systematically replaced or modified.

## Limitations
- The correlation analysis assumes human object annotations perfectly preserve object information, which may not hold for complex or ambiguous scenes
- The parameter-free design limits analysis to ordinal comparisons and may miss finer-grained relationships
- Focus on object hallucination may not generalize to other types of multimodal hallucinations (attribute, attribute-object, etc.)

## Confidence
- High: The correlation between Pfram scores and object hallucination levels is well-established across multiple MLLMs and datasets
- Medium: The claim that visual encoders contribute more to object recognition than LLMs is supported by layer-wise analysis but requires careful interpretation of representation comparisons
- Medium: The finding that textual instructions impact object representations differently depending on architecture is supported by the data but may be model-specific

## Next Checks
1. Test Pfram's correlation with hallucination on a different hallucination benchmark (e.g., attribute hallucination) to assess generalizability beyond object hallucination
2. Conduct ablation studies on the similarity metrics (NDCG vs Mutual k-NN) to determine which is more sensitive to representation quality changes
3. Verify Pfram results on MLLMs with different architectural designs (e.g., single-stream vs dual-stream encoders) to test the robustness of module contribution findings