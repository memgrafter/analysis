---
ver: rpa2
title: 'How JEPA Avoids Noisy Features: The Implicit Bias of Deep Linear Self Distillation
  Networks'
arxiv_id: '2407.03475'
source_url: https://arxiv.org/abs/2407.03475
tags:
- jepa
- learning
- dynamics
- features
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the implicit bias of deep linear self-distillation
  networks in Joint Embedding Predictive Architecture (JEPA) compared to Masked AutoEncoders
  (MAE). The key finding is that JEPAs prioritize learning features with high regression
  coefficients, while MAEs focus on features with high covariance.
---

# How JEPA Avoids Noisy Features: The Implicit Bias of Deep Linear Self Distillation Networks

## Quick Facts
- arXiv ID: 2407.03475
- Source URL: https://arxiv.org/abs/2407.03475
- Authors: Etai Littwin; Omid Saremi; Madhu Advani; Vimal Thilak; Preetum Nakkiran; Chen Huang; Joshua Susskind
- Reference count: 40
- Key outcome: JEPAs prioritize learning features with high regression coefficients while MAEs focus on features with high covariance

## Executive Summary
This paper analyzes the implicit bias of deep linear self-distillation networks in Joint Embedding Predictive Architecture (JEPA) versus Masked AutoEncoders (MAE). The study reveals that JEPAs learn features based on their predictive influence (regression coefficient ρ) rather than their variance, while MAEs prioritize features with high covariance regardless of predictive value. The research employs a tractable deep linear neural network model to provide analytical solutions for training dynamics, showing that deeper encoders amplify these differences by progressively suppressing noisy directions in JEPAs. These findings offer insights into why JEPAs may learn more semantic features for perception tasks compared to reconstruction-based methods.

## Method Summary
The authors analyze deep linear neural networks with JEPA and MAE objectives using a tractable theoretical framework. They study the training dynamics through gradient flow equations, focusing on how features are learned incrementally based on their importance metrics. The analysis assumes small weight initialization, orthogonal balanced initialization, and simultaneously diagonalizable data covariances. By examining the critical time at which features are learned, the authors identify the distinct implicit biases of each objective function and how encoder depth affects the learning progression.

## Key Results
- JEPAs prioritize learning features with high regression coefficients (predictive influence) before features with high variance
- MAEs learn features based primarily on their covariance magnitude, making them indifferent to predictive value
- Deeper encoders amplify the implicit bias difference, creating wider temporal gaps between learning high-ρ and low-ρ features in JEPAs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JEPA learns features with high regression coefficients before features with high variance
- Mechanism: The JEPA objective creates depth-dependent implicit bias where deeper encoders progressively suppress noisy directions based on predictive influence
- Core assumption: Balanced orthogonal initialization and simultaneously diagonalizable data covariances
- Evidence anchors:
  - [abstract]: "JEPAs prioritize learning features with high regression coefficients, while MAEs focus on features with high covariance"
  - [section 4.2]: "features with a higher ρi, rather than a high λi are learned first"
  - [corpus]: Weak evidence - no direct citations, but related works on JEPA show similar implicit bias claims
- Break condition: If data is not simultaneously diagonalizable or initialization is far from orthogonal balanced

### Mechanism 2
- Claim: MAE learns features based primarily on their covariance magnitude
- Mechanism: MAE objective's training dynamics dominated by covariance term λ, making it indifferent to regression coefficient ρ
- Core assumption: Linear deep networks with diagonal covariance matrices
- Evidence anchors:
  - [abstract]: "MAEs focus on features with high covariance"
  - [section 4.2]: "while the MAE objective retains its order of learning as in distribution 1"
  - [corpus]: Weak evidence - similar observations in [20] about reconstruction methods learning uninformative features
- Break condition: When encoder is shallow (L=1), MAE and JEPA dynamics converge

### Mechanism 3
- Claim: Depth of encoder amplifies implicit bias difference between JEPA and MAE
- Mechanism: As encoder depth L increases, critical time scaling with ρ becomes more pronounced in JEPA
- Core assumption: Small initialization regime enabling greedy learning dynamics
- Evidence anchors:
  - [section 4.2]: "deeper encoders introduce a wider temporal spread between learning features with the same λ but different ρ"
  - [section 5]: "only in the case of the JEPA objective, deeper encoders induce a more pronounced incremental learning"
  - [corpus]: Weak evidence - no direct citations, but related to depth separation results in deep linear network literature
- Break condition: If initialization is too large, greedy learning assumption breaks down

## Foundational Learning

- Concept: Implicit bias in gradient descent optimization
  - Why needed here: The paper analyzes how different objectives create different implicit biases in what features are learned first
  - Quick check question: What determines which features a neural network learns first during training - the objective function or the data distribution?

- Concept: Greedy learning dynamics in deep linear networks
  - Why needed here: The analysis shows features are learned incrementally based on their importance metrics
  - Quick check question: In a deep linear network with small initialization, do features with higher variance or higher predictive value get learned first?

- Concept: Regression coefficient and covariance decomposition
  - Why needed here: The distinction between λ (covariance) and ρ (regression coefficient) is central to understanding why JEPA and MAE prioritize different features
  - Quick check question: Given a feature with high covariance but low regression coefficient, would it be more important for MAE or JEPA?

## Architecture Onboarding

- Component map: Input x -> Encoder (L-layer MLP) -> Latent representation -> Decoder (linear layer) -> Output
- Critical path:
  1. Initialize encoder and decoder weights in balanced orthogonal configuration
  2. Sample input-target pairs (x,y) from joint distribution
  3. Forward pass through encoder and decoder
  4. Compute loss and backpropagate (with StopGrad for JEPA)
  5. Update weights using gradient flow equations
  6. Monitor feature learning progression via encoder projections

- Design tradeoffs:
  - Depth vs bias: Deeper encoders create stronger implicit bias toward high-ρ features but may reduce overall representation capacity
  - Initialization scale: Small initialization enables greedy learning dynamics; large initialization may bypass the implicit bias
  - Data structure: Simultaneous diagonalizability assumption simplifies analysis but may not hold for real data

- Failure signatures:
  - No greedy learning: If initialization is too large or learning rate too high, features may be learned simultaneously
  - Shallow encoder collapse: With L=1, JEPA and MAE dynamics converge, eliminating bias difference
  - Non-diagonalizable data: If Σxx and Σyx cannot be simultaneously diagonalized, analysis framework breaks down

- First 3 experiments:
  1. Train deep linear JEPA and MAE models on Gaussian data with varying λ and ρ values, track which features are learned first
  2. Vary encoder depth L and observe how temporal gap between high-ρ and low-ρ feature learning changes
  3. Test initialization sensitivity by training with different weight scales and measuring greedy learning behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the implicit biases of JEPA and MAE objectives generalize to non-linear models and non-simultaneously diagonalizable data distributions?
- Basis in paper: [explicit] The authors acknowledge that their theoretical results are restricted to conditions in assumption 4.1
- Why unresolved: The paper primarily focuses on deep linear networks, which may not capture the full complexity of real-world data and models
- What evidence would resolve it: Empirical studies on non-linear models and various data distributions comparing learning dynamics and feature prioritization

### Open Question 2
- Question: What are the implications of JEPA's implicit bias towards learning high-influence features for downstream task performance, particularly in scenarios with spurious or slow features?
- Basis in paper: [inferred] The paper suggests JEPA's focus on predictive yet low-variance features may explain its tendency to learn more semantic features
- Why unresolved: While the paper provides insights into learning dynamics, it does not directly investigate impact on downstream task performance
- What evidence would resolve it: Empirical studies comparing JEPA and MAE performance on downstream tasks involving datasets with known spurious or slow features

### Open Question 3
- Question: How do the depth-dependent differences in feature learning between JEPA and MAE objectives affect the efficiency and effectiveness of representation learning in practical scenarios?
- Basis in paper: [explicit] The paper demonstrates that deeper encoders introduce wider temporal spread between learning features with same λ but different ρ in JEPA
- Why unresolved: The theoretical analysis provides insights but does not directly address how these differences translate to practical representation learning scenarios
- What evidence would resolve it: Empirical studies varying depth of JEPA and MAE models, measuring representation learning efficiency and downstream task performance

## Limitations

- The analysis relies on highly idealized deep linear network model with Gaussian data distributions and simultaneous diagonalizability assumptions
- Greedy learning dynamics only hold in the small initialization regime, and real-world deep networks with nonlinear activations may exhibit qualitatively different behavior
- The distinction between λ and ρ is analytically tractable in this setting but may blur in practice due to correlations in real data

## Confidence

- High confidence in analytical framework and mathematical derivations
- Medium confidence in direct applicability to practical JEPA implementations
- Low confidence in generalization to non-linear, high-dimensional real-world scenarios

## Next Checks

1. Empirical validation on synthetic non-linear datasets where ground truth feature importance is known, comparing JEPA and MAE feature learning trajectories
2. Experiments varying initialization scale to test boundary conditions where greedy learning dynamics break down
3. Extension of analysis to convolutional architectures with natural image datasets to assess whether implicit bias persists in realistic settings