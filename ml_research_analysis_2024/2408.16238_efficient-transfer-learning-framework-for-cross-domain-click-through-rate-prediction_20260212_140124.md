---
ver: rpa2
title: Efficient Transfer Learning Framework for Cross-Domain Click-Through Rate Prediction
arxiv_id: '2408.16238'
source_url: https://arxiv.org/abs/2408.16238
tags:
- data
- domain
- user
- natural
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of cross-domain click-through
  rate (CTR) prediction in industrial recommendation systems where natural content
  and advertisements coexist but differ in data distribution. The authors propose
  an Efficient Transfer Learning Framework for Cross-Domain CTR Prediction (E-CDCTR)
  to transfer knowledge from the richer source natural content domain to the sparser
  advertising domain.
---

# Efficient Transfer Learning Framework for Cross-Domain Click-Through Rate Prediction

## Quick Facts
- arXiv ID: 2408.16238
- Source URL: https://arxiv.org/abs/2408.16238
- Authors: Qi Liu; Xingyuan Tang; Jianqiang Huang; Xiangqian Yu; Haoran Jin; Jin Chen; Yuanhao Pu; Defu Lian; Tan Qu; Zhe Wang; Jia Cheng; Jun Lei
- Reference count: 35
- Primary result: 2.9% and 2.1% relative improvements in CTR and RPM respectively in Meituan's online advertising system

## Executive Summary
This paper addresses the challenge of cross-domain click-through rate (CTR) prediction in industrial recommendation systems where natural content and advertisements coexist but differ in data distribution. The authors propose an Efficient Transfer Learning Framework for Cross-Domain CTR Prediction (E-CDCTR) that transfers knowledge from the richer source natural content domain to the sparser advertising domain. E-CDCTR employs a tri-level asynchronous framework consisting of Tiny Pre-training Model (TPM), Complete Pre-training Model (CPM), and Advertisement CTR model (A-CTR). The framework achieves significant improvements in both CTR and RPM metrics in Meituan's online advertising system.

## Method Summary
E-CDCTR is a tri-level asynchronous framework that addresses cross-domain CTR prediction challenges. TPM uses a lightweight model on long-term data to capture stable user/item embeddings, CPM uses complete features on short-term data for knowledgeable initialization, and A-CTR fine-tunes on sparse advertising data. The framework updates TPM monthly on half-year natural data, CPM weekly on one-month natural data, and A-CTR daily on one-month advertising data. This approach reduces training inefficiency while effectively alleviating catastrophic forgetting in daily-updated models.

## Key Results
- 2.9% relative improvement in CTR in Meituan's online advertising system
- 2.1% relative improvement in RPM in Meituan's online advertising system
- Demonstrated effectiveness through industrial dataset experiments
- Achieved knowledge transfer from natural content to advertising domain

## Why This Works (Mechanism)

### Mechanism 1
The tri-level asynchronous framework reduces training inefficiency by avoiding full-scale pre-training on the source domain. TPM uses a lightweight model on long-term data to capture stable user/item embeddings, CPM uses complete features on short-term data to avoid re-training large models, and A-CTR only fine-tunes on sparse advertising data. The core assumption is that historical embeddings from TPM can represent long-term personalization without needing full data access. Evidence shows TPM provides historical personalization information while CPM offers knowledgeable initialization, and A-CTR fine-tunes on advertisement data. If TPM embeddings fail to capture long-term patterns, A-CTR performance degrades and catastrophic forgetting returns.

### Mechanism 2
Multiple historical embeddings alleviate catastrophic forgetting in daily-updated models. TPM generates embeddings from three consecutive months; A-CTR concatenates and compresses them via self-attention, preserving long-term user/item representations. The core assumption is that short-term data alone cannot represent user behavior evolution adequately. Evidence shows TPM provides richer representations of user and item for both the CPM and A-CTR, effectively alleviating the forgetting problem inherent in the daily updates. If the three-month window is insufficient for the target domain's data sparsity, forgetting effects reappear.

### Mechanism 3
CPM pre-trained on short-term natural data provides better initialization for A-CTR than random initialization. CPM learns feature interactions and embeddings from rich natural content, which are transferred to A-CTR, reducing convergence time on sparse ad data. The core assumption is that natural and advertising data share underlying user preference patterns. Evidence shows CPM further enhances the advertisement model by providing knowledgeable initialization, thereby alleviating the data sparsity challenges typically encountered by advertising CTR models. If the natural and ad data distributions are too divergent, CPM initialization may hinder rather than help.

## Foundational Learning

- Concept: Catastrophic forgetting in sequential training
  - Why needed here: A-CTR is updated daily on sliding windows; without TPM embeddings, long-term personalization is lost
  - Quick check question: What happens to model performance if TPM embeddings are omitted but daily updates continue?

- Concept: Transfer learning via parameter initialization
  - Why needed here: CPM parameters initialize A-CTR to speed convergence on sparse ad data instead of training from scratch
  - Quick check question: Why does CPM use complete features while TPM uses only basic features?

- Concept: Asynchronous model updates to manage training cost
  - Why needed here: Source data is large; TPM monthly, CPM weekly, A-CTR daily balances freshness and efficiency
  - Quick check question: What would be the training cost if all three models updated daily?

## Architecture Onboarding

- Component map: TPM → CPM → A-CTR, with TPM embeddings feeding both CPM and A-CTR
- Critical path: TPM embedding generation → CPM pre-training (with embeddings) → A-CTR fine-tuning (with CPM params + TPM embeddings)
- Design tradeoffs: TPM trades model capacity for speed; CPM trades data recency for completeness; A-CTR trades initialization quality for daily adaptability
- Failure signatures: Sudden GAUC drop after TPM update suggests embedding drift; poor A-CTR convergence suggests CPM initialization is ineffective; slow training suggests CPM frequency is too high
- First 3 experiments:
  1. Compare A-CTR GAUC with/without TPM embeddings on same ad data
  2. Test CPM initialization vs random initialization on ad data
  3. Vary TPM embedding dimension and measure impact on A-CTR performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of E-CDCTR change when using different pre-training durations for CPM beyond the one-month period tested? The paper mentions CPM uses "short-term natural data" and is updated weekly, but does not explore varying pre-training durations. This remains unresolved as the paper only tests CPM with one month of data. Experiments comparing CPM performance with different pre-training durations (e.g., 2 weeks, 1 month, 2 months) on the same dataset would resolve this question.

### Open Question 2
What is the impact of different self-attention mechanisms on the aggregation of historical embeddings in TPM? The paper uses self-attention for embedding aggregation but doesn't compare it with other attention mechanisms or pooling strategies. This remains unresolved as the paper implements self-attention without comparing its effectiveness against alternatives like additive attention or mean/max pooling. Experiments replacing self-attention with alternative aggregation methods and measuring their impact on CTR prediction performance would resolve this question.

### Open Question 3
How does E-CDCTR's performance scale with different levels of data sparsity in the advertising domain? The paper addresses data sparsity challenges but doesn't test E-CDCTR across varying sparsity levels. This remains unresolved as the paper only tests on one industrial dataset with fixed sparsity levels, not exploring performance across different sparsity scenarios. Experiments applying E-CDCTR to advertising domains with different sparsity ratios (e.g., 10%, 1%, 0.1% positive samples) and comparing performance would resolve this question.

## Limitations
- The paper lacks detailed specifications of model architectures and hyperparameters, making exact reproduction challenging
- No information is provided about feature engineering approaches or embedding dimensions used across components
- The online evaluation metrics are reported without statistical significance tests or variance estimates
- The mechanism for catastrophic forgetting prevention is theoretical; empirical validation of long-term performance stability is absent

## Confidence
- **High confidence**: The tri-level asynchronous framework design is clearly articulated and logically sound for addressing the stated problem
- **Medium confidence**: The effectiveness claims are supported by reported improvements but lack statistical validation and detailed ablation studies
- **Low confidence**: The catastrophic forgetting mechanism is theoretically plausible but not empirically validated over extended time periods

## Next Checks
1. Conduct A/B tests with confidence intervals and statistical significance testing for the reported 2.9% CTR and 2.1% RPM improvements
2. Remove TPM embeddings and measure performance degradation over multiple update cycles to validate catastrophic forgetting prevention
3. Test CPM initialization effectiveness when natural and advertising data distributions are intentionally made more dissimilar to establish transfer learning boundaries