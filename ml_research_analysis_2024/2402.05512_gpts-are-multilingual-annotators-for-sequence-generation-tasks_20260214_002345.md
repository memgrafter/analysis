---
ver: rpa2
title: GPTs Are Multilingual Annotators for Sequence Generation Tasks
arxiv_id: '2402.05512'
source_url: https://arxiv.org/abs/2402.05512
tags:
- annotator
- data
- language
- dataset
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes using large language models as multilingual
  annotators for sequence generation tasks. The method generates multiple silver sentences
  from a single human-annotated gold sentence, reducing annotation costs.
---

# GPTs Are Multilingual Annotators for Sequence Generation Tasks

## Quick Facts
- arXiv ID: 2402.05512
- Source URL: https://arxiv.org/abs/2402.05512
- Reference count: 40
- Primary result: GPT annotators outperform human annotators under the same budget for sequence generation tasks across multiple languages

## Executive Summary
This paper proposes using large language models as multilingual annotators for sequence generation tasks, specifically image captioning and text style transfer. The method generates multiple silver sentences from a single human-annotated gold sentence, reducing annotation costs while maintaining quality. Experiments show GPT-4 can effectively work as a multilingual annotator without requiring human annotators fluent in target languages, achieving better performance than machine translation baselines and enabling cost-efficient construction of multilingual datasets.

## Method Summary
The method uses GPT-4 and GPT-3.5 to generate multiple silver sentences (paraphrases and translations) from single human-annotated gold sentences. For image captioning, the approach takes English captions and generates paraphrases and translations into target languages. For text style transfer, it generates sentences with different stylistic attributes. Downstream models are then trained on the augmented dataset containing both gold and silver sentences. The approach leverages few-shot learning in LLMs to learn annotation patterns from the provided examples.

## Key Results
- GPT annotators outperform human annotators under the same budget for image captioning and text style transfer tasks
- The method achieves better performance than machine translation baselines for multilingual datasets
- Effectiveness demonstrated across multiple languages including Korean, Vietnamese, Polish, Latvian, Estonian, and Finnish
- Particularly effective for low-resource languages where human annotation is expensive

## Why This Works (Mechanism)

### Mechanism 1
GPT annotators can generate diverse silver sentences that match the quality of human annotations under the same budget. By generating multiple silver sentences from a single human-annotated gold sentence, the method achieves cost efficiency while maintaining quality. Core assumption: GPT-4 can generate sentences that are as diverse and contextually appropriate as multiple human annotators. Evidence: Experiments on image captioning and text style transfer tasks show GPT annotators outperform human annotators under the same budget.

### Mechanism 2
GPT annotators can effectively work as multilingual annotators without requiring human annotators fluent in target languages. GPT-4's enhanced multilingual capabilities allow it to generate sentences in target languages from English prompts, reducing the need for language-specific human annotators. Core assumption: GPT-4's multilingual capabilities are sufficient to generate high-quality sentences in low-resource languages. Evidence: The proposed method achieves better performance than machine translation baselines and enables cost-efficient construction of multilingual datasets.

### Mechanism 3
The method is particularly effective for low-resource languages where human annotation is expensive and time-consuming. GPT-4's cost is constant regardless of language, while human annotation costs vary significantly by language, making it especially cost-effective for low-resource languages. Core assumption: The cost differential between GPT-4 and human annotation is substantial enough to justify the approach for low-resource languages. Evidence: Latvian, Estonian, and Finnish have approximately 1.5, 1.1, and 4.8 million native speakers, making them hard to hire annotators and construct datasets.

## Foundational Learning

- **Few-shot learning in large language models**: Why needed here: The method relies on GPT-4's ability to learn from few examples provided in the prompt to generate appropriate annotations. Quick check question: Can you explain how few-shot learning differs from zero-shot learning in the context of GPT-4?

- **Machine translation evaluation metrics**: Why needed here: The method needs to be compared against machine translation baselines, requiring understanding of BLEU, ROUGE, METEOR, and other evaluation metrics. Quick check question: What are the key differences between BLEU and BERTScore, and when would each be more appropriate?

- **Data augmentation techniques**: Why needed here: The method generates multiple sentences from a single annotation, similar to data augmentation strategies. Quick check question: How does the proposed method differ from traditional data augmentation techniques like back-translation?

## Architecture Onboarding

- **Component map**: Input → Prompt generation → GPT-4 API call → Output filtering → Dataset construction → Model training
- **Critical path**: The generation of high-quality silver sentences that maintain context and diversity
- **Design tradeoffs**: Quality vs. quantity (more silver sentences increase diversity but may introduce noise), cost vs. performance (GPT-4 is expensive but reduces human annotation needs)
- **Failure signatures**: Repetitive outputs, loss of context, introduction of bias, poor performance on extremely low-resource languages
- **First 3 experiments**:
  1. Validate that GPT-4 can generate diverse sentences from a single prompt by comparing output variety
  2. Test the cost-effectiveness by comparing annotation costs between human annotators and GPT-4
  3. Evaluate multilingual performance by comparing GPT-4-generated sentences against machine translation baselines

## Open Questions the Paper Calls Out

**Open Question 1**: How does the performance of GPT annotators compare to human annotators when evaluated on extremely low-resource languages like Basque and Māori? The paper includes error analysis for these languages but doesn't provide quantitative performance comparisons.

**Open Question 2**: Does the paraphrasing approach limit the diversity of information captured in generated captions compared to having multiple human annotators create completely independent captions? The paper acknowledges this limitation but doesn't quantify the information loss.

**Open Question 3**: How does the ethical bias observed in GPT-4 for low-resource languages compare to its performance on high-resource languages, and what mitigation strategies are effective? The paper identifies biases but doesn't systematically compare bias incidence rates or test mitigation techniques.

## Limitations
- Effectiveness heavily depends on prompt engineering quality, which is not fully specified
- Actual cost-effectiveness threshold compared to human annotation is not clearly established
- Evaluation relies primarily on automatic metrics without deep human evaluation across all target languages

## Confidence

**High Confidence**:
- GPT annotators can outperform human annotators under the same budget for image captioning and text style transfer tasks
- The method is effective for constructing multilingual datasets for low-resource languages
- GPT-4 can generate diverse silver sentences from single gold sentences

**Medium Confidence**:
- The approach is cost-effective compared to human annotation across all low-resource languages
- GPT-4's multilingual capabilities are sufficient for extremely low-resource languages
- The quality gap between GPT-generated and human-annotated sentences is acceptable for all target applications

**Low Confidence**:
- The method generalizes well to other sequence generation tasks beyond image captioning and text style transfer
- The approach remains effective as GPT-4's pricing model changes or newer models are released
- The quality of generated sentences remains consistent across different runs and prompt variations

## Next Checks

1. **Prompt Robustness Test**: Systematically vary the prompt templates and parameters to determine how sensitive the quality of generated silver sentences is to prompt engineering.

2. **Cost-Effectiveness Analysis**: Conduct a detailed cost analysis comparing GPT-4 API usage costs against human annotation costs across multiple language pairs and task types.

3. **Long-Term Consistency Evaluation**: Run multiple experiments over time to assess whether the quality of GPT-4-generated sentences remains consistent across different API versions and pricing models.