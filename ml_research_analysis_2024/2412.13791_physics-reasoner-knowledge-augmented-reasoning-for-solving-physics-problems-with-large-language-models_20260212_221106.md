---
ver: rpa2
title: 'Physics Reasoner: Knowledge-Augmented Reasoning for Solving Physics Problems
  with Large Language Models'
arxiv_id: '2412.13791'
source_url: https://arxiv.org/abs/2412.13791
tags:
- physics
- problem
- reasoning
- knowledge
- formula
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of solving complex physics problems
  using large language models (LLMs), which often struggle due to insufficient physics
  knowledge and incorrect knowledge application. The proposed Physics Reasoner framework
  tackles these issues by integrating a comprehensive formula set for knowledge acquisition
  and detailed checklists for effective knowledge application.
---

# Physics Reasoner: Knowledge-Augmented Reasoning for Solving Physics Problems with Large Language Models

## Quick Facts
- arXiv ID: 2412.13791
- Source URL: https://arxiv.org/abs/2412.13791
- Authors: Xinyu Pang; Ruixin Hong; Zhanke Zhou; Fangrui Lv; Xinwei Yang; Zhilong Liang; Bo Han; Changshui Zhang
- Reference count: 14
- Key outcome: Achieves state-of-the-art performance on SciBench benchmark, improving average accuracy by 5.8% and significantly reducing reasoning errors across multiple physics datasets

## Executive Summary
This paper addresses the challenge of solving complex physics problems using large language models (LLMs), which often struggle due to insufficient physics knowledge and incorrect knowledge application. The proposed Physics Reasoner framework tackles these issues by integrating a comprehensive formula set for knowledge acquisition and detailed checklists for effective knowledge application. The method follows a three-stage process: problem analysis, formula retrieval, and guided reasoning, with checklists employed to enhance LLMs' self-improvement. Evaluated on the SciBench benchmark, Physics Reasoner achieves state-of-the-art performance, improving average accuracy by 5.8% and significantly reducing reasoning errors across multiple physics datasets.

## Method Summary
Physics Reasoner addresses the challenge of solving complex physics problems using large language models (LLMs) by integrating a comprehensive formula set and detailed checklists. The method follows a three-stage process: problem analysis, formula retrieval, and guided reasoning. The formula set provides explicit physics knowledge with 122 scientifically vetted formulas, each annotated with clear variable definitions and application conditions. Checklists guide LLMs through systematic verification of both problem comprehension and calculation steps, prompting self-correction of common errors. The framework is evaluated on the SciBench benchmark using GPT-3.5-turbo, GPT-4-turbo, and Llama 3 models, demonstrating state-of-the-art performance and reduced reasoning errors.

## Key Results
- Achieves state-of-the-art performance on SciBench benchmark with 5.8% average accuracy improvement
- Significantly reduces reasoning errors across multiple physics datasets (fund, thermo, class)
- Outperforms baseline methods (System, CoT, PoT, PoT + Self-Correction, PoT + Self-Refine) on fund dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The formula set directly addresses LLMs' knowledge deficiency by providing explicit physics formulae and their contextual definitions.
- Mechanism: The formula set supplies 122 scientifically vetted formulas from standard textbooks, each annotated with clear variable definitions and application conditions. This structured knowledge is directly inserted into the problem-solving pipeline as Python code comments, ensuring LLMs have access to precise physics relationships during reasoning.
- Core assumption: LLMs can effectively leverage external, well-structured knowledge if it is provided in a format that aligns with their generation capabilities (e.g., code comments).
- Evidence anchors:
  - [abstract]: "constructs a comprehensive formula set to provide explicit physics knowledge"
  - [section 4.2]: "Each formula is accompanied by a brief description, formula content, and definitions of involved variables."
  - [corpus]: The formula set is a novel construction; no direct corpus citation provided.
- Break condition: If the formula annotations are too verbose or the variables are not clearly defined, LLMs may fail to parse or apply the formulas correctly, negating the knowledge supplementation.

### Mechanism 2
- Claim: Checklists mitigate knowledge misapplication by guiding LLMs through systematic verification of both problem comprehension and calculation steps.
- Mechanism: Two distinct checklists are employed: one for problem analysis (verifying variable extraction and unit consistency) and one for guided reasoning (ensuring correct formula application and final answer formatting). These checklists prompt the LLM to self-correct common errors identified through manual error analysis.
- Core assumption: LLMs can self-improve when provided with targeted, concise feedback that addresses specific error patterns observed in physics problem solving.
- Evidence anchors:
  - [abstract]: "utilizes checklists containing detailed instructions to guide effective knowledge application"
  - [section 4.3]: "We design checklists based on the errors observed. By testing several LLM-based reasoning methods on abundant physics problems, we identify key points that need to be carefully checked and include them in the checklists."
  - [corpus]: The error analysis leading to checklist design is detailed in section 3; no direct corpus citation provided.
- Break condition: If the checklists are too generic or fail to address the specific types of errors the LLM is making, they may not lead to meaningful self-correction.

### Mechanism 3
- Claim: The three-stage workflow (problem analysis, formula retrieval, guided reasoning) structures the problem-solving process to minimize error propagation.
- Mechanism: The workflow sequentially breaks down the problem into manageable steps, with each stage feeding into the next. Checklists are integrated at the problem analysis and guided reasoning stages to catch and correct errors before they impact subsequent steps. This structured approach prevents compounding errors.
- Core assumption: LLMs benefit from a well-defined, sequential problem-solving process with built-in verification points, rather than a free-form reasoning approach.
- Evidence anchors:
  - [abstract]: "Physics Reasoner solves it through three stages: problem analysis, formula retrieval, and guided reasoning."
  - [section 4.1]: "Physics Reasoner is divided into 3 stages: problem analysis, formula retrieval, and guided reasoning."
  - [corpus]: The three-stage process is novel to this work; no direct corpus citation provided.
- Break condition: If the stages are not clearly defined or the transitions between them are not smooth, the LLM may become confused or skip essential steps, leading to errors.

## Foundational Learning

- Concept: The structure and application of physics formulae (e.g., E = kq/r^2 for electric fields)
  - Why needed here: Physics problems require specific formulae to relate physical quantities. LLMs often lack this knowledge or misapply it.
  - Quick check question: What is the formula for the electric field due to a point charge, and what do the variables represent?

- Concept: Vector vs. scalar quantities in physics
  - Why needed here: Confusing vectors with scalars is a common error in physics problem solving. LLMs may incorrectly treat vector quantities as scalars, leading to incorrect calculations.
  - Quick check question: Is velocity a vector or a scalar quantity? What about speed?

- Concept: Unit conversion and dimensional analysis
  - Why needed here: Physics problems often involve different units, and incorrect unit handling can lead to errors. LLMs may fail to convert units properly or perform calculations with inconsistent units.
  - Quick check question: If a length is given in centimeters, what is its equivalent in meters?

## Architecture Onboarding

- Component map:
  - Formula Set (122 physics formulae with detailed annotations) -> Checklists (problem analysis and guided reasoning) -> Three-Stage Workflow (problem analysis, formula retrieval, guided reasoning) -> LLM Integration (GPT-3.5-turbo, GPT-4-turbo, Llama 3)

- Critical path:
  1. Problem Analysis: Extract known variables from the problem text and verify with the checklist.
  2. Formula Retrieval: Identify relevant physics fields and retrieve applicable formulae from the formula set.
  3. Guided Reasoning: Complete the Python code with reasoning steps, guided by the checklist, and execute to obtain the final answer.

- Design tradeoffs:
  - Formula Set Size vs. Coverage: A larger formula set provides broader coverage but may be more difficult to manage and annotate. The chosen 122 formulae balance coverage and manageability.
  - Checklist Detail vs. Conciseness: Detailed checklists provide more guidance but may be verbose and distracting. The designed checklists aim to be both instructive and concise.
  - LLM Model Choice vs. Cost: More powerful models like GPT-4 may provide better reasoning but are more expensive. The evaluation includes both open-source and closed-source models to assess performance across different cost points.

- Failure signatures:
  - Incorrect Variable Extraction: The LLM fails to identify or correctly define all relevant variables from the problem text.
  - Formula Misapplication: The LLM selects an incorrect formula or applies a correct formula with incorrect variable substitutions.
  - Unit Errors: The LLM fails to convert units properly or performs calculations with inconsistent units.
  - Checklist Ineffectiveness: The LLM ignores or fails to act on the guidance provided by the checklists.

- First 3 experiments:
  1. Baseline Comparison: Evaluate Physics Reasoner against standard baselines (System, CoT, PoT) on the SciBench fund dataset to assess the impact of the formula set and checklists.
  2. Ablation Study: Systematically remove components (formula set, checklists) to determine their individual contributions to the overall performance.
  3. Error Analysis: Manually analyze error cases for Physics Reasoner to identify remaining failure modes and potential areas for improvement.

## Open Questions the Paper Calls Out

- How does the effectiveness of Physics Reasoner vary across different physics domains (e.g., thermodynamics vs. classical dynamics)?
- Can the formula set be expanded to cover more advanced physics topics, such as quantum mechanics or relativity?
- How do the checklists perform when applied to physics problems in languages other than English?
- What is the impact of increasing the number of few-shot examples on the performance of Physics Reasoner?
- How does Physics Reasoner compare to human experts in solving complex physics problems?

## Limitations

- Formula Set Coverage: The 122-formula set may not cover all edge cases in physics problem-solving, potentially limiting effectiveness for complex interdisciplinary problems.
- Checklist Dependency: Effectiveness assumes consistent LLM interpretation of natural language instructions, which may fail if the LLM misunderstands checklist prompts.
- Model Dependency: Performance improvements are partially tied to specific LLM capabilities, suggesting the framework's effectiveness is not universally guaranteed across all models.

## Confidence

- High Confidence: The formula set directly addresses LLMs' knowledge deficiency
- Medium Confidence: Checklists effectively mitigate knowledge misapplication
- Medium Confidence: The three-stage workflow minimizes error propagation

## Next Checks

1. **Formula Coverage Expansion**: Test Physics Reasoner on physics problems requiring formulas outside the current 122-formula set to identify coverage gaps and assess performance degradation.

2. **Cross-Model Consistency**: Implement Physics Reasoner across five diverse LLMs (including smaller models) to determine whether performance improvements are model-agnostic or tied to specific architectural features.

3. **Checklist Generalization Test**: Apply Physics Reasoner to physics-adjacent domains (e.g., engineering problems) without modifying checklists to evaluate whether the guidance framework generalizes beyond its original scope.