---
ver: rpa2
title: 'HM3: Heterogeneous Multi-Class Model Merging'
arxiv_id: '2409.19173'
source_url: https://arxiv.org/abs/2409.19173
tags:
- merging
- merged
- classifier
- classifiers
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Heterogeneous Multi-Class Model Merging (HM3),
  a training-free technique for merging multi-class classifiers with different label
  spaces. HM3 expands the output layers of models to be merged with zeros, allowing
  them to share a common architecture, and then applies standard model merging strategies
  like Model Soup or TIES.
---

# HM3: Heterogeneous Multi-Class Model Merging

## Quick Facts
- arXiv ID: 2409.19173
- Source URL: https://arxiv.org/abs/2409.19173
- Reference count: 40
- Primary result: Training-free technique for merging multi-class classifiers with different label spaces using zero-padding and group-wise softmax

## Executive Summary
This paper introduces HM3 (Heterogeneous Multi-Class Model Merging), a training-free technique for merging multi-class classifiers with different label spaces. The method expands output layers with zeros to create a common architecture, then applies standard merging strategies like Model Soup or TIES. HM3 is demonstrated on BERT-based guard models for LLM moderation, showing that merged models can outperform originals in average F1-score while reducing inference time by up to 44%. The paper also introduces "self-merging" using DARE-TIES, finding that it can improve performance for poorly performing classifiers.

## Method Summary
HM3 addresses the challenge of merging models with heterogeneous output label spaces by zero-padding their output layers to match the maximum number of output classes across all models. This creates a homogeneous architecture that enables standard parameter-wise merging techniques. After merging, probabilities are computed group-wise using separate softmax functions for each original label group, preserving the semantics of each classifier. The method builds on model merging concepts like Model Soup and TIES, and introduces DARE-TIES for self-merging applications.

## Key Results
- Merged models achieved up to 44% inference time reduction while maintaining or improving performance
- Average F1-score improvements of up to 2.7% observed across four moderation tasks
- Self-merging with DARE-TIES improved performance of poorly performing hate speech classifier
- HM3 enables efficient deployment of multiple text classifiers without additional training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanding heterogeneous output layers with zeros enables direct parameter-wise averaging during model merging.
- Mechanism: Zero-padding each model's output layer to match the maximum number of output classes across all models creates a homogeneous architecture. This allows standard merging techniques like Model Soup to perform element-wise weighted averaging of parameters.
- Core assumption: Zero-padding does not introduce interference effects that would harm classification accuracy.
- Evidence anchors:
  - [abstract]: "HM3 expands the output layers of models to be merged with zeros, allowing them to share a common architecture"
  - [section]: "In order to be able to merge these models, we expand the final classifier layer of the models that we want to merge with zeros and adjust the base model accordingly"
- Break condition: If zero-padding introduces significant interference effects that degrade performance more than training-free merging benefits.

### Mechanism 2
- Claim: Group-wise softmax preserves correct probability distributions for heterogeneous label spaces after merging.
- Mechanism: After zero-padding and merging, output probabilities are computed separately for each original label group using individual softmax functions. This ensures that probabilities within each task's label space sum to one, maintaining the semantics of each original classifier.
- Core assumption: Individual softmax application per label group preserves the probabilistic interpretation of each classifier's output.
- Evidence anchors:
  - [abstract]: "Class probabilities of the merged model should be computed group-wise: softmax is applied separately on the two groups"
  - [section]: "softmax is the softmax function applied to each segment individually"
- Break condition: If cross-group interference during inference affects probability estimates despite group-wise softmax.

### Mechanism 3
- Claim: Task vector density reduction through DARE-TIES can improve model performance by reducing overfitting.
- Mechanism: DARE randomly drops task vector values and rescales remaining values, effectively reducing the influence of fine-tuning on the base model. This can help poorly performing models by preventing overfitting to their specific training data.
- Core assumption: Some fine-tuning may introduce noise or overfitting that random dropping can mitigate.
- Evidence anchors:
  - [abstract]: "We introduce self-merging to assess the impact of reduced task-vector density, finding that the more poorly performing hate speech classifier benefits from self-merging"
  - [section]: "We were asking ourselves 'Can random resetting of task-vectors without merging with a different model already improve a fine-tuned model?'"
- Break condition: If task vector dropping consistently degrades performance regardless of the base model's quality.

## Foundational Learning

- Concept: Model merging through parameter averaging
  - Why needed here: HM3 builds on the principle that fine-tuned models can be combined without retraining by averaging their parameters
  - Quick check question: What is the key difference between model merging and model ensembling?

- Concept: Task vectors as representations of fine-tuning effects
  - Why needed here: HM3 and subsequent merging strategies like TIES rely on computing and manipulating task vectors (differences between base and fine-tuned models)
  - Quick check question: How are task vectors defined mathematically in the context of model merging?

- Concept: Zero-padding for architectural alignment
  - Why needed here: HM3 specifically addresses the challenge of merging models with different output dimensions by padding with zeros
  - Quick check question: Why is zero-padding preferred over other methods for aligning heterogeneous output layers?

## Architecture Onboarding

- Component map: Input preprocessing (tokenization) -> Base model (BERT architecture) -> Output layer expansion (zero-padding) -> Group-wise softmax computation -> Merging strategy (Model Soup, TIES, or DARE-TIES)

- Critical path:
  1. Load heterogeneous models and their base model
  2. Apply HM3 transformation (zero-padding output layers)
  3. Choose merging strategy
  4. Execute merging process
  5. Apply group-wise softmax during inference
  6. Evaluate performance

- Design tradeoffs:
  - Zero-padding increases parameter count but maintains training-free advantage
  - Group-wise softmax preserves task semantics but requires careful implementation
  - DARE-TIES with low task vector density may improve some models but could harm others

- Failure signatures:
  - Degraded performance on specific tasks after merging
  - Incorrect probability distributions across label groups
  - Excessive computational overhead due to large output layers

- First 3 experiments:
  1. Merge two models with minimal label space differences (e.g., 2 vs 3 classes) using HM3 + Model Soup
  2. Test group-wise softmax implementation by feeding inputs that should trigger each label group separately
  3. Experiment with DARE-TIES task vector density on a single model (self-merging) to observe performance changes

## Open Questions the Paper Calls Out
None

## Limitations
- Zero-padding may introduce interference effects not captured in the experiments, particularly for models with vastly different label spaces
- The group-wise softmax implementation details could significantly affect practical performance
- The computational overhead of expanded output layers was not thoroughly analyzed beyond inference time savings

## Confidence

**High confidence**: The basic feasibility of training-free heterogeneous model merging using zero-padding and group-wise softmax is well-supported by the theoretical framework and implementation details provided.

**Medium confidence**: The claim that merged models can outperform original models in average F1-score requires more rigorous validation across diverse tasks and domains.

**Low confidence**: The assertion that self-merging with DARE-TIES can improve poorly performing classifiers needs more extensive validation beyond the single hate speech classifier example.

## Next Checks
1. Test HM3 on a larger and more diverse set of tasks (minimum 10 tasks) with varying label space sizes to assess generalization limits
2. Conduct ablation studies comparing HM3 with alternative architectural alignment methods (such as linear projection layers) to quantify the cost of zero-padding
3. Perform extensive sensitivity analysis on DARE-TIES parameters (task vector density) across multiple model qualities to determine when self-merging helps vs harms