---
ver: rpa2
title: 'Argument Mining in Data Scarce Settings: Cross-lingual Transfer and Few-shot
  Techniques'
arxiv_id: '2407.03748'
source_url: https://arxiv.org/abs/2407.03748
tags:
- data
- argument
- few-shot
- language
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically evaluates different strategies for addressing
  data scarcity in Argument Mining, a sequence labelling task involving long and complex
  discourse structures. Contrary to previous work on other sequence labelling tasks,
  the results show that data-transfer approaches, particularly multilingual fine-tuning
  of mBERT on projected datasets, outperform both model-transfer (zero-shot cross-lingual)
  and few-shot learning methods.
---

# Argument Mining in Data Scarce Settings: Cross-lingual Transfer and Few-shot Techniques

## Quick Facts
- arXiv ID: 2407.03748
- Source URL: https://arxiv.org/abs/2407.03748
- Authors: Anar Yeginbergen; Maite Oronoz; Rodrigo Agerri
- Reference count: 40
- Primary result: Data-transfer approaches outperform model-transfer and few-shot methods for Argument Mining

## Executive Summary
This paper empirically evaluates strategies for addressing data scarcity in Argument Mining, a sequence labeling task involving long and complex discourse structures. The study challenges previous findings from other sequence labeling tasks by demonstrating that data-transfer approaches, particularly multilingual fine-tuning of mBERT on projected datasets, consistently outperform both model-transfer (zero-shot cross-lingual) and few-shot learning methods. The optimal approach depends on task characteristics, with data-transfer excelling when domain-specific annotated data is available, while fine-tuning with 20% of the data via k-percent sampling achieves competitive results in few-shot settings.

## Method Summary
The study employs three main strategies to address data scarcity in Argument Mining: data-transfer (fine-tuning on machine-translated and projected data from other languages), model-transfer (zero-shot cross-lingual evaluation), and few-shot learning (EntLM prompting and fine-tuning with limited samples). The methodology involves translating the English AbstRCT corpus to Spanish, French, and Italian using NLLB, projecting annotations to target languages, and evaluating multiple sampling strategies including k-shot and k-percent approaches. Models tested include mBERT and mDeBERTa-v3, with performance measured using sequence-level F1-macro scores for Claim and Premise detection.

## Key Results
- Data-transfer approaches, particularly multilingual fine-tuning on projected datasets, outperform both model-transfer and few-shot methods
- Fine-tuning with k-percent sampling consistently outperforms EntLM prompting method for few-shot learning
- The 20% k-percent sampling ratio achieves competitive results comparable to full data-transfer approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Data-transfer outperforms model-transfer in Argument Mining due to the complexity and length of argument spans.
- **Mechanism**: Translating and projecting annotations to target languages provides more domain-specific context than relying on multilingual model generalization.
- **Core assumption**: Argument structures in the medical domain are sufficiently similar across languages to allow effective annotation projection.
- **Evidence anchors**:
  - [abstract]: "contrary to previous work, we show that for Argument Mining data transfer obtains better results than model-transfer"
  - [section]: "Results show that manually corrected data is crucial at least for evaluation although the post-processed version of the projections gets close enough to the gold standard"
  - [corpus]: Found 25 related papers; weak evidence for cross-lingual transfer in medical domains specifically
- **Break condition**: When domain-specific patterns differ significantly across languages or projection introduces systematic errors that cannot be corrected programmatically.

### Mechanism 2
- **Claim**: Fine-tuning with k-percent sampling outperforms EntLM prompting for sequence labelling in few-shot settings.
- **Mechanism**: Including a proportional representation of O (outside) tokens in the training data improves the model's ability to distinguish non-argument spans from argument components.
- **Core assumption**: The imbalance between B/I tokens and O tokens in Argument Mining data is crucial for learning effective span boundaries.
- **Evidence anchors**:
  - [abstract]: "fine-tuning outperforms few-shot methods"
  - [section]: "When fine-tuned with 20% and 50% of the data performance is comparable to that of data-transfer and model-transfer results"
  - [corpus]: Weak evidence; few-shot learning literature typically focuses on short spans, not long argument structures
- **Break condition**: When the sampling method fails to capture the true distribution of argument types in the target domain.

### Mechanism 3
- **Claim**: Multilingual fine-tuning improves performance by providing data augmentation effects.
- **Mechanism**: Training on multiple languages simultaneously increases the model's exposure to diverse linguistic patterns while maintaining domain consistency.
- **Core assumption**: Argument structures in the medical domain share enough common features across languages to benefit from joint training.
- **Evidence anchors**:
  - [abstract]: "multilingual data-transfer obtains the overall best results outperforming also the original English gold results"
  - [section]: "This means that data-transfer may be employed as a cost-free data-augmentation technique"
  - [corpus]: Weak evidence; most related work focuses on cross-lingual transfer without multilingual training
- **Break condition**: When linguistic divergence between languages introduces conflicting training signals.

## Foundational Learning

- **Concept**: Cross-lingual transfer learning
  - **Why needed here**: The paper compares model-transfer (cross-lingual) against data-transfer approaches for low-resource languages
  - **Quick check question**: What is the fundamental difference between model-transfer and data-transfer in cross-lingual settings?

- **Concept**: Few-shot learning sampling strategies
  - **Why needed here**: The paper demonstrates that k-percent sampling outperforms k-shot sampling for Argument Mining
  - **Quick check question**: How does k-percent sampling maintain class distribution differently from k-shot sampling?

- **Concept**: Sequence labelling with long spans
  - **Why needed here**: Argument Mining involves detecting long, complex discourse structures unlike typical named entity recognition
  - **Quick check question**: Why might standard sequence labelling techniques designed for short spans fail on long argument structures?

## Architecture Onboarding

- **Component map**: Translation (NLLB) → Annotation projection → Manual correction → Fine-tuning (mBERT/mDeBERTa) → Evaluation (sequence-level F1)
- **Critical path**: Translation → Projection → Fine-tuning → Evaluation. The annotation projection step is critical as errors propagate through the entire pipeline.
- **Design tradeoffs**: 
  - Manual correction vs automated post-processing: Manual correction provides better evaluation data but increases cost; post-processing is cheaper but may introduce residual errors.
  - k-shot vs k-percent sampling: k-shot provides equal representation but may miss distribution characteristics; k-percent maintains distribution but requires more data.
- **Failure signatures**: 
  - Poor cross-lingual performance suggests projection errors or domain divergence.
  - Suboptimal few-shot results indicate sampling issues or insufficient model capacity for long spans.
  - Multilingual degradation suggests conflicting linguistic patterns.
- **First 3 experiments**:
  1. Compare mBERT vs mDeBERTa-v3 performance on English monolingual training to establish baseline encoder effectiveness.
  2. Test k-percent sampling with varying percentages (5%, 10%, 20%, 50%) to find the minimal effective training set size.
  3. Evaluate post-processed vs manually corrected data to quantify the cost-benefit tradeoff of manual annotation correction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of data-transfer versus model-transfer generalize to other complex sequence labeling tasks beyond Argument Mining, such as coreference resolution or discourse parsing?
- Basis in paper: [inferred] The paper's results challenge previous findings on sequence labeling tasks, suggesting task-specific characteristics (like argument span length and complexity) may be crucial factors.
- Why unresolved: The paper only evaluates on Argument Mining, which has unique characteristics (long, complex discourse structures). Other sequence labeling tasks with different structural properties might yield different results.
- What evidence would resolve it: Comparative experiments applying the same transfer learning approaches to other complex sequence labeling tasks with heterogeneous span lengths and structural complexity.

### Open Question 2
- Question: What is the optimal domain alignment strategy for data-transfer when training data and target domain have minimal overlap?
- Basis in paper: [explicit] The paper notes that "the domain of the dataset used for data-transfer seems to be a deciding factor" but doesn't explore what happens when domains are misaligned or when mixed-domain data is used.
- Why unresolved: The experiments use domain-specific training data (medical abstracts) for medical text evaluation. The performance impact of using out-of-domain training data or combining multiple domains remains unexplored.
- What evidence would resolve it: Systematic experiments varying domain similarity between training and target data, including cross-domain evaluations and mixed-domain training approaches.

### Open Question 3
- Question: How does the sampling method interact with label distribution and sequence length when using few-shot learning for complex sequence labeling tasks?
- Basis in paper: [explicit] The paper demonstrates that "k-percent sampling" outperforms "k-shot sampling" and emphasizes the importance of sampling method, but doesn't fully analyze the interaction with label distribution characteristics.
- Why unresolved: While the paper shows k-percent works better, it doesn't explain why or how this interacts with the specific label distribution (high imbalance between B- and I- tokens, long argument spans) in Argument Mining.
- What evidence would resolve it: Detailed analysis of how different sampling strategies affect learning when label distributions are highly imbalanced or when sequence spans vary dramatically in length.

## Limitations

- The study's focus on a single medical domain corpus limits generalizability to other Argument Mining contexts
- Annotation projection quality depends heavily on machine translation accuracy, which may vary across language pairs
- Limited exploration of the optimal sampling percentage and its relationship to label distribution characteristics
- Comparison of few-shot methods is constrained by availability of only 50 English samples

## Confidence

**High Confidence**: Data-transfer approaches outperform model-transfer for Argument Mining in the medical domain; multilingual fine-tuning provides consistent performance improvements.

**Medium Confidence**: k-percent sampling outperforms k-shot sampling, though results vary by percentage; few-shot learning methods lag behind fine-tuning, but comparison is limited to one prompting method.

**Low Confidence**: Generalizability to non-medical domains and languages with greater linguistic divergence; optimal sampling percentage determination without systematic distribution analysis.

## Next Checks

1. **Domain Generalization Test**: Evaluate the data-transfer approach on a non-medical Argument Mining corpus (such as essays or legal documents) to assess whether the multilingual fine-tuning advantage persists across domains.

2. **Linguistic Divergence Analysis**: Test the projection method on language pairs with varying degrees of similarity (e.g., English-Spanish vs English-Japanese) to quantify the impact of linguistic distance on annotation quality and downstream performance.

3. **Sampling Distribution Investigation**: Systematically vary the k-percent sampling percentages and analyze the resulting class distributions to determine whether the 20% optimal point corresponds to specific distributional properties in the Argument Mining data.