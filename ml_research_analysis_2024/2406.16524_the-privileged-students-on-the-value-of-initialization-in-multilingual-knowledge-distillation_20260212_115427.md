---
ver: rpa2
title: 'The Privileged Students: On the Value of Initialization in Multilingual Knowledge
  Distillation'
arxiv_id: '2406.16524'
source_url: https://arxiv.org/abs/2406.16524
tags:
- teacher
- data
- multilingual
- knowledge
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the effectiveness of knowledge distillation
  (KD) in multilingual settings, focusing on the relative importance of model initialization
  versus the distillation process itself. The authors propose using direct weight
  copying from a fine-tuned teacher model to initialize the student model, as an alternative
  to pre-training.
---

# The Privileged Students: On the Value of Initialization in Multilingual Knowledge Distillation

## Quick Facts
- arXiv ID: 2406.16524
- Source URL: https://arxiv.org/abs/2406.16524
- Authors: Haryo Akbarianto Wibowo; Thamar Solorio; Alham Fikri Aji
- Reference count: 9
- Primary result: Weight copying from fine-tuned teacher models significantly improves multilingual student performance compared to distillation alone, enabling faster learning and zero-shot cross-lingual generalization.

## Executive Summary
This paper investigates the relative importance of initialization versus distillation in multilingual knowledge distillation. The authors propose a novel approach of directly copying weights from a fine-tuned teacher model to initialize the student model, as an alternative to expensive pre-training. Through extensive experiments on multiple multilingual datasets, they demonstrate that this weight-copying initialization method leads to faster learning speeds, better performance, and preserved multilingual capabilities, particularly in low-resource scenarios. The approach enables zero-shot cross-lingual generalization and outperforms traditional distillation from random initialization.

## Method Summary
The authors propose a two-step distillation process where student models are initialized by directly copying weights from fine-tuned teacher models, rather than pre-training from scratch. This method follows the TinyBERT methodology but replaces the expensive pre-training step with weight copying. The student models (6 layers) are smaller than the teacher models (12 layers), using a reduction factor of 2. Knowledge distillation training uses a combination of attention, hidden layer, embedding, and prediction losses with cross-entropy. The experiments compare three initialization strategies: from-scratch, from-base (pre-trained student), and from-teacher (weight copying), both with and without KD, across three multilingual datasets (massive, tsm, universal-ner) with evaluation on seen and unseen languages for zero-shot cross-lingual transfer.

## Key Results
- Weight copying initialization from fine-tuned teachers leads to faster learning speeds compared to from-scratch initialization across all data sizes
- The proposed method enables zero-shot cross-lingual generalization, preserving multilingual capabilities even when fine-tuning on only English data
- Weight copying significantly outperforms distillation alone, with the gap widening in low-resource scenarios
- The approach works across different model architectures (XLM-R and mDeBERTa) and maintains performance on unseen languages

## Why This Works (Mechanism)

### Mechanism 1
Copying weights from a fine-tuned teacher directly to the student model provides superior multilingual initialization compared to pre-training the student. The student inherits multilingual semantic representations encoded in the teacher's weights, bypassing the need for large-scale multilingual pre-training. Core assumption: The teacher's weights encode cross-lingual mappings that are transferable to smaller models. Evidence anchors: [abstract] "Our findings show that model initialization using copy-weight from the fine-tuned teacher contributes the most compared to the distillation process itself" and [section 2.2] "we employ a simpler and more efficient initialization approach by copying the weights from the teacher model to the student model". Break condition: If the teacher's multilingual representations are too specialized or if the student's architecture differs significantly in capacity, the copied weights may not transfer effectively.

### Mechanism 2
Direct weight copying enables zero-shot cross-lingual generalization by preserving multilingual knowledge in the student. The weight initialization process transfers the teacher's cross-lingual transfer capability to the student, allowing it to perform well on unseen languages. Core assumption: The teacher's multilingual knowledge is encoded in a way that's preserved through weight copying. Evidence anchors: [abstract] "We show that weight-copy enables zero-shot cross-lingual generalization, preserving multilingual capabilities even in low-resource scenarios" and [section 4.2] "Using English as the training data deteriorates the performance in massive's without KD setup, yet it gains considerable performance by using KD in the copy-weight initialization". Break condition: If the student architecture is too small to retain the complex multilingual representations, or if the teacher wasn't properly fine-tuned on multilingual data.

### Mechanism 3
Direct weight copying leads to faster learning speeds because the student starts closer to the optimal solution. The student begins training with weights that already encode task-relevant multilingual patterns, reducing the number of optimization steps needed. Core assumption: Starting with task-relevant weights accelerates convergence compared to random initialization. Evidence anchors: [abstract] "Our experiments reveal that the weight-copy approach leads to faster learning speeds and exhibits multilingual knowledge" and [section 5.3] "With smaller data subsets, the gap in training loss between each model is wide, with from-teacher showing the fastest convergence rate". Break condition: If the data distribution is very different from what the teacher saw, or if the student's task differs significantly from the teacher's training.

## Foundational Learning

- Concept: Knowledge Distillation fundamentals
  - Why needed here: Understanding how student models learn from teacher outputs is crucial for grasping why initialization matters
  - Quick check question: What is the difference between supervised learning and knowledge distillation in terms of what the student learns from?

- Concept: Multilingual model architectures (XLM-R, mDeBERTa)
  - Why needed here: The paper compares different multilingual model families, so understanding their architectures is essential
  - Quick check question: How does the multilingual masking strategy in XLM-R differ from monolingual BERT?

- Concept: Zero-shot cross-lingual transfer
  - Why needed here: The paper's key contribution is showing that weight copying enables zero-shot performance
  - Quick check question: What is the fundamental challenge in zero-shot cross-lingual transfer that weight copying aims to address?

## Architecture Onboarding

- Component map: Teacher model (12 layers) → Weight copying → Student model (6 layers) → KD training → Final student
- Critical path: Weight copying initialization → KD fine-tuning → Evaluation on multilingual tasks
- Design tradeoffs: Layer reduction (factor of 2) vs. hidden unit reduction; full weight copying vs. selective copying
- Failure signatures: Poor zero-shot performance indicates weight copying didn't preserve multilingual knowledge; slow convergence suggests initialization wasn't effective
- First 3 experiments:
  1. Compare from-scratch vs. from-teacher initialization on massive dataset with KD
  2. Test zero-shot cross-lingual performance using only English fine-tuning data
  3. Measure learning speed across different data subset sizes (1%, 5%, 10%, 20%)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed weight copying initialization method compare to pre-training-based initialization methods in terms of computational efficiency and final performance across different model sizes and architectures?
- Basis in paper: [explicit] The paper mentions that the proposed method avoids the expensive pre-training step used in Jiao et al., 2020; Ansell et al., 2023, and focuses on weight copying from fine-tuned teacher models.
- Why unresolved: The paper does not provide a direct comparison between weight copying and pre-training-based initialization methods in terms of computational efficiency and performance.
- What evidence would resolve it: Conducting experiments comparing the proposed method with pre-training-based initialization methods across various model sizes and architectures, measuring both computational efficiency (e.g., training time, memory usage) and final performance (e.g., F1 scores) would resolve this question.

### Open Question 2
- Question: What is the impact of different layer reduction factors (e.g., 3, 4, 5) on the performance of the student model when using the weight copying initialization method?
- Basis in paper: [explicit] The paper uses a layer reduction factor of 2 in its experiments but does not explore other reduction factors.
- Why unresolved: The paper only investigates one layer reduction factor, leaving the impact of other factors unexplored.
- What evidence would resolve it: Experimenting with different layer reduction factors (e.g., 3, 4, 5) and comparing the performance of the student models would provide insights into the optimal reduction factor for the weight copying initialization method.

### Open Question 3
- Question: How does the weight copying initialization method perform in other multilingual tasks, such as machine translation or question answering, compared to task-specific pre-training?
- Basis in paper: [explicit] The paper focuses on sequence classification and token classification tasks, leaving the performance in other multilingual tasks unexplored.
- Why unresolved: The paper's experiments are limited to sequence classification and token classification tasks, not providing insights into the method's performance in other multilingual tasks.
- What evidence would resolve it: Conducting experiments applying the weight copying initialization method to other multilingual tasks, such as machine translation or question answering, and comparing the results with task-specific pre-training would resolve this question.

### Open Question 4
- Question: What is the impact of using different teacher models (e.g., mBERT, mT5) on the performance of the student model when using the weight copying initialization method?
- Basis in paper: [explicit] The paper uses XLM-R and mDeBERTa as teacher models but does not explore the impact of using different teacher models.
- Why unresolved: The paper only investigates two teacher models, leaving the impact of using different teacher models unexplored.
- What evidence would resolve it: Experimenting with different teacher models (e.g., mBERT, mT5) and comparing the performance of the student models would provide insights into the optimal teacher model for the weight copying initialization method.

## Limitations

- The evaluation focuses primarily on text classification tasks without exploring other task types like generation or structured prediction
- Experiments use only two teacher-student architecture pairs, limiting generalizability to other architectural choices
- The paper does not investigate the effects of copying weights from teachers that weren't fine-tuned on the target task
- Zero-shot cross-lingual generalization results are evaluated on a relatively small number of languages

## Confidence

**High confidence**: The experimental demonstration that weight copying initialization leads to faster convergence compared to from-scratch training is well-supported by learning curves and loss metrics across multiple data sizes.

**Medium confidence**: The claim that weight copying preserves multilingual capabilities and enables zero-shot cross-lingual generalization is supported but limited to specific datasets and language pairs, requiring broader validation.

**Medium confidence**: The assertion that weight copying is more effective than pre-training the student model for multilingual initialization is demonstrated through comparisons but could benefit from testing against more pre-training baselines.

## Next Checks

1. **Architecture Generalization Test**: Evaluate weight copying across different teacher-student architecture pairs (varying layer reduction ratios beyond 2:1, different model families) to determine if the observed benefits generalize beyond the specific configurations tested.

2. **Task Diversity Assessment**: Apply the weight copying initialization method to non-classification tasks (e.g., sequence-to-sequence generation, structured prediction) to verify if the multilingual initialization benefits extend beyond the classification tasks examined.

3. **Teacher Quality Analysis**: Systematically vary the quality and task-specificity of the teacher models (e.g., copy from non-fine-tuned teachers, teachers fine-tuned on different tasks) to quantify how teacher preparation affects student initialization effectiveness.