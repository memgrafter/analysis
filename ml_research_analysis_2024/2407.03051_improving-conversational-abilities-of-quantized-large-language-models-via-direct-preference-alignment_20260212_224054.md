---
ver: rpa2
title: Improving Conversational Abilities of Quantized Large Language Models via Direct
  Preference Alignment
arxiv_id: '2407.03051'
source_url: https://arxiv.org/abs/2407.03051
tags:
- newline
- qdpo
- quantization
- quantized
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of conversational ability degradation
  in quantized large language models (LLMs), which occurs due to "token-flipping"
  errors during inference. The authors propose quantization-aware direct preference
  optimization (QDPO), a method that aligns quantized LLM responses with their full-precision
  counterparts by generating preference datasets and applying direct preference optimization
  during quantization-aware training.
---

# Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment

## Quick Facts
- arXiv ID: 2407.03051
- Source URL: https://arxiv.org/abs/2407.03051
- Authors: Janghwan Lee; Seongmin Park; Sukjin Hong; Minsoo Kim; Du-Seong Chang; Jungwook Choi
- Reference count: 20
- Key outcome: QDPO significantly improves conversational abilities of quantized LLMs by addressing token-flipping errors, achieving lower lose-rates and higher scores on MT-Bench, Vicuna-Eval, and FLASK while preserving task-specific performance on CSQA and MMLU

## Executive Summary
This paper addresses the critical problem of conversational ability degradation in quantized large language models (LLMs), which occurs due to "token-flipping" errors during inference. The authors propose quantization-aware direct preference optimization (QDPO), a method that aligns quantized LLM responses with their full-precision counterparts by generating preference datasets and applying direct preference optimization during quantization-aware training. Evaluated on two instruction-tuned LLMs (Vicuna and Mi:dm) across English and Korean benchmarks including MT-Bench, Vicuna-Eval, and FLASK, QDPO significantly improves conversational abilities compared to standard quantization techniques, achieving lower lose-rates and higher scores while mostly preserving task-specific performance on benchmarks like CSQA and MMLU.

## Method Summary
QDPO generates preference pairs where yw is the response from the full-precision model and yl is the response from the quantized model, then applies direct preference optimization loss to align the quantized model with the full-precision reference. The method uses automatic preference generation by running both models on the same prompts and collecting argmax responses, eliminating the need for human annotations. QDPO is implemented within a quantization-aware training framework using the Straight-Through Estimator to handle non-differentiable quantization operations, with training optimized at Î²=3e-6 learning rate.

## Key Results
- QDPO achieves significantly lower lose-rates on MT-Bench pairwise comparisons compared to standard 4-bit quantization methods (RTN and AWQ)
- QDPO improves single-answer grading scores on MT-Bench and maintains competitive performance on task-specific benchmarks (CSQA and MMLU)
- QDPO enhances skill-specific performance across diverse dimensions in the FLASK benchmark, demonstrating broad conversational improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-flipping in quantized LLMs occurs due to ambiguous token distributions that become prone to flipping when quantization errors introduce alterations.
- Mechanism: When quantization causes small probability shifts between top-1 and top-2 tokens, if the margin is narrow, quantization errors can exceed this margin and flip the selected token, leading to degraded conversational output.
- Core assumption: The top-1 and top-2 token probabilities in baseline models are often very close, creating vulnerability to quantization-induced deviations.
- Evidence anchors: Fig. 3(b) shows the average probability margin between the top-1 and top-2 tokens across each text sample, demonstrating that 4-bit quantized models have narrower margins than 16-bit baselines, indicating higher likelihood of token-flipping.

### Mechanism 2
- Claim: QDPO aligns quantized LLM responses with full-precision counterparts by generating preference datasets and applying direct preference optimization during quantization-aware training.
- Mechanism: QDPO creates preference pairs where yw is the response from the full-precision model and yl is the response from the quantized model, then optimizes the quantized model to prefer responses closer to the full-precision model using direct preference optimization loss.
- Core assumption: The full-precision model's response can serve as a reliable reference for optimal conversational quality.
- Evidence anchors: Drawing inspiration from DPO's success in aligning LLMs with human preferences, QDPO extends this application to overcome quantization challenges.

### Mechanism 3
- Claim: QDPO improves the disparity between top-1 and top-2 logits of token distribution, reducing token-flipping and fostering more relevant and consistent text output.
- Mechanism: By aligning the quantized model's responses with the full-precision model, QDPO indirectly forces the model to create clearer decision boundaries between token choices, increasing the probability gap between top-1 and top-2 tokens.
- Core assumption: Making the top-1 token probability significantly higher than top-2 probability reduces vulnerability to quantization errors.
- Evidence anchors: QDPO improves the disparity between the top-1 and top-2 logits of token distribution, reducing token-flipping, and fostering more relevant and consistent text output.

## Foundational Learning

- Concept: Token-flipping
  - Why needed here: Understanding this phenomenon is crucial because it's identified as the primary cause of conversational ability degradation in quantized LLMs.
  - Quick check question: What happens when a quantized model selects a different token at timestep t = i compared to the baseline?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: QDPO builds upon DPO by extending it to work with quantized models and automatically generated preference pairs.
  - Quick check question: How does DPO relate the optimal reward function to the optimal policy directly?

- Concept: Post-training Quantization (PTQ) vs Quantization-aware Training (QAT)
  - Why needed here: The paper compares different quantization approaches and proposes QDPO as an enhancement to existing methods.
  - Quick check question: What is the key difference between PTQ and QAT in terms of when quantization is applied?

## Architecture Onboarding

- Component map:
  Base LLM (Vicuna or Mi:dm-7B) -> Quantization module (4-bit RTN) -> Full-precision reference model -> QDPO training loop -> Preference pair generation system -> Loss calculation and gradient application

- Critical path:
  1. Generate prompts from dialogue datasets
  2. Obtain responses from full-precision and quantized models
  3. Create preference pairs (yw from full-precision, yl from quantized)
  4. Apply QDPO loss to update quantized model weights
  5. Evaluate conversational ability improvements

- Design tradeoffs:
  - Using full-precision model as reference vs human annotations (QDPO trades data quality for scalability)
  - 4-bit quantization for efficiency vs potential degradation (QDPO aims to recover lost capabilities)
  - Automatic preference generation vs potential bias (QDPO assumes full-precision model represents optimal responses)

- Failure signatures:
  - Loss not converging during QDPO training
  - Conversational ability metrics not improving despite QDPO application
  - Task-specific performance (CSQA, MMLU) degrading significantly after QDPO
  - Perplexity increasing without corresponding conversational improvement

- First 3 experiments:
  1. Run baseline 16-bit and 4-bit RTN inference on MT-Bench to confirm the conversational degradation
  2. Implement QDPO training on a quantized Vicuna model and verify loss reduction over training steps
  3. Compare conversational ability (MT-Bench, Vicuna-Eval, FLASK) between QDPO, baseline quantization methods, and 16-bit baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does QDPO perform on multilingual benchmarks beyond English and Korean?
- Basis in paper: The paper evaluates QDPO on English and Korean but does not explore other languages
- Why unresolved: The authors only tested on two languages, limiting generalizability
- What evidence would resolve it: Testing QDPO on additional languages (e.g., Spanish, French, Chinese) and comparing performance to baseline models

### Open Question 2
- Question: What is the long-term impact of QDPO on model generalization and out-of-distribution performance?
- Basis in paper: The authors note QDPO mostly preserves task-specific performance but don't explore long-term generalization
- Why unresolved: Short-term benchmarks may not capture potential degradation in generalization over time
- What evidence would resolve it: Longitudinal studies tracking QDPO performance on diverse tasks and datasets over extended training periods

### Open Question 3
- Question: How does QDPO compare to other preference optimization methods specifically designed for quantization?
- Basis in paper: The paper compares QDPO to DPO but doesn't benchmark against other quantization-aware preference alignment methods
- Why unresolved: The field is rapidly evolving with new methods emerging regularly
- What evidence would resolve it: Head-to-head comparisons of QDPO against newer quantization-aware preference optimization techniques on standard benchmarks

### Open Question 4
- Question: What is the minimum dataset size required for QDPO to effectively recover conversational abilities?
- Basis in paper: The authors use 50k prompts but don't explore sensitivity to dataset size
- Why unresolved: Dataset size could significantly impact computational efficiency and practical deployment
- What evidence would resolve it: Systematic experiments varying dataset size from minimal to maximal to determine performance thresholds

## Limitations

- The paper's core premise that full-precision model responses serve as reliable quality references for QDPO training is untested across diverse domains and may inherit biases from baseline models.
- Experimental validation is constrained to two specific 7B parameter models (Vicuna and Mi:dm) and a limited set of benchmarks, potentially limiting generalizability.
- The paper lacks sufficient detail on computational overhead introduced by QDPO during inference and does not address potential latency implications for real-time applications.

## Confidence

**High Confidence**: The identification of token-flipping as a primary degradation mechanism in quantized LLMs is well-supported by the presented evidence showing probability margin narrowing in 4-bit models compared to 16-bit baselines.

**Medium Confidence**: The effectiveness of QDPO in improving conversational abilities is supported by benchmark results, though the relatively small number of models tested and the specific benchmark selection limit generalizability.

**Low Confidence**: The claim that QDPO preserves task-specific performance while improving conversational abilities requires further validation across a broader range of tasks and model architectures to rule out overfitting to the specific evaluation suite used.

## Next Checks

1. Apply QDPO to a different LLM architecture (e.g., Llama-2 or Mistral) and evaluate whether the conversational improvements transfer beyond the Vicuna and Mi:dm models used in the original study.

2. Deploy QDPO-enhanced models in a simulated extended usage environment to test whether the conversational improvements persist over time and whether any degradation patterns emerge after prolonged inference.

3. Systematically remove or modify individual components of the QDPO approach (such as the preference generation method, the loss function formulation, or the integration with quantization-aware training) to quantify the contribution of each element to the overall performance gains.