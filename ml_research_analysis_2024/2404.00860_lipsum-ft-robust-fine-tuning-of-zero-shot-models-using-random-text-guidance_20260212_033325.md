---
ver: rpa2
title: 'Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance'
arxiv_id: '2404.00860'
source_url: https://arxiv.org/abs/2404.00860
tags:
- fine-tuning
- lipsum-ft
- distribution
- zero-shot
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Lipsum-FT, a robust fine-tuning method for
  zero-shot vision-language models. It addresses the problem of fine-tuning degrading
  robustness to distribution shifts.
---

# Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance

## Quick Facts
- arXiv ID: 2404.00860
- Source URL: https://arxiv.org/abs/2404.00860
- Reference count: 40
- Key outcome: Lipsum-FT outperforms existing robust fine-tuning methods on both reference and distribution shift data, improving accuracy and uncertainty quantification

## Executive Summary
This paper addresses the problem of fine-tuning zero-shot vision-language models degrading their robustness to distribution shifts. Lipsum-FT proposes a novel regularization approach that minimizes the energy gap between fine-tuned and zero-shot models using randomly generated text guidance. By preserving the pre-trained vision-language connection during fine-tuning, Lipsum-FT achieves superior performance on both reference data and various distribution shift benchmarks compared to existing robust fine-tuning methods.

## Method Summary
Lipsum-FT is a robust fine-tuning method that regularizes the fine-tuning process by minimizing the energy gap between fine-tuned and zero-shot models. During training, random text tokens are generated from the language model's vocabulary and used to compute the regularization term. The method combines standard cross-entropy loss for reference data with an additional term that minimizes the difference in logits (inner products) between the fine-tuned vision model and the zero-shot vision model when projected with random text features. This approach preserves the pre-trained vision-language connection, which is shown to be crucial for maintaining robustness against distribution shifts.

## Key Results
- Lipsum-FT outperforms existing robust fine-tuning methods (CAR-FT, LP-FT) on DomainNet and ImageNet benchmarks
- The method improves both accuracy and uncertainty quantification on distribution-shifted data
- Lipsum-FT achieves better calibration (lower expected calibration error) compared to standard fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lipsum-FT preserves the pre-trained vision-language connection by minimizing the energy gap between fine-tuned and zero-shot models.
- Mechanism: The regularization term minimizes the difference in logits (inner products) between the fine-tuned vision model and the zero-shot vision model when projected with random text features from the language model.
- Core assumption: The pre-trained vision-language model encodes a robust multimodal relationship that can be measured via energy values, and maintaining this relationship during fine-tuning preserves out-of-distribution generalization.
- Evidence anchors:
  - [abstract] "minimize the energy gap between fine-tuned and zero-shot models by regularizing with randomly generated text guidance"
  - [section 5.1] "Lipsum-FT minimizes the following regularization term... which can be understood as logit matching"
  - [corpus] Weak - related papers focus on robust fine-tuning but don't specifically address energy gap minimization
- Break condition: If the random text features don't adequately sample the space of possible discriminative models, or if the vision-language connection is not actually captured in the energy values.

### Mechanism 2
- Claim: Standard fine-tuning distorts the pre-trained vision-language connection, leading to reduced robustness on distribution shifts.
- Mechanism: Fine-tuning primarily optimizes for the reference data, which can change the vision model's representations in ways that weaken the connection to the language model's text embeddings.
- Core assumption: The zero-shot model's strong performance on distribution shifts comes from the pre-trained multimodal connection, and this connection can be measured and regularized.
- Evidence anchors:
  - [abstract] "fine-tuning of the zero-shot model on the reference data results in enhanced downstream performance, it compromises the model's robustness against distribution shifts"
  - [section 4.2] "we confirm this through an increase in the energy gap... the existing robust fine-tuning techniques alleviate the decline in pre-trained associations between vision and language models"
  - [corpus] Weak - related papers discuss robust fine-tuning but don't provide evidence for feature distortion theory in CLIP-ViT models
- Break condition: If the feature distortion theory doesn't apply to CLIP-ViT models, or if other factors (like optimization dynamics) are more important.

### Mechanism 3
- Claim: The MSE loss in Lipsum-FT is more effective than KLD loss for preserving the vision-language connection.
- Mechanism: MSE loss directly minimizes the difference in energy values (logits), while KLD loss with temperature scaling introduces an additional term that can hinder complete alignment.
- Core assumption: The energy values (logits) contain the information about the pre-trained vision-language connection, and minimizing their difference is more effective than minimizing distribution divergence.
- Evidence anchors:
  - [section 5.3] "MSE is a better choice than KLD from this perspective... CAR-FT MSE surpasses CAR-FT, and Lipsum-FT outperforms Lipsum-FTKLD"
  - [section 5.3] "taking a limit τ → ∞ to the KLD loss gives us... The first term aligns with our regularization... Here, the second term causes an increase in the values in vθ,ϕ and hinders complete logit matching"
  - [corpus] Weak - related papers don't discuss the comparison between MSE and KLD losses in this context
- Break condition: If the temperature scaling in KLD loss doesn't significantly impact the regularization, or if other loss functions are more effective.

## Foundational Learning

- Concept: Energy-based models and their relationship to discriminative models
  - Why needed here: Understanding how the energy function Eθ,ϕ(x, t) relates to the discriminative model's logits is crucial for grasping Lipsum-FT's regularization approach.
  - Quick check question: How does the inner product fθ,ϕ(x, t) = ⟨Fθ(x), Gϕ(t)⟩ serve as both class logits and energy function in CLIP?

- Concept: Feature distortion theory and its limitations
  - Why needed here: Recognizing why feature distortion theory doesn't fully explain CLIP-ViT fine-tuning results helps justify Lipsum-FT's alternative approach.
  - Quick check question: Why does LP-FT reduce feature distortion but not improve distribution shift performance in CLIP-ViT fine-tuning?

- Concept: Vision-language model architecture and zero-shot classification
  - Why needed here: Understanding how CLIP performs zero-shot classification using the language model's text embeddings is essential for grasping Lipsum-FT's use of random text guidance.
  - Quick check question: How are the classification head weights W computed from the language model's text embeddings in CLIP?

## Architecture Onboarding

- Component map: Image → Vision model Fθ → D-dim representation → Inner product with text → Logits
- Critical path: 1. Forward pass: Image through vision model, random text through language model
  2. Compute logits using inner products
  3. Compute regularization term using MSE loss between fine-tuned and zero-shot logits
  4. Combine with cross-entropy loss for standard fine-tuning
  5. Backpropagate and update vision model parameters
- Design tradeoffs:
  - Random vs. fixed text guidance: Random text covers all potential discriminative models but may be noisier; fixed text (like CAR-FT) targets specific aspects but may miss others
  - Length of random text (L): Longer text may capture more information but increases computation; shorter text is faster but may be less effective
  - Number of text tokens per iteration (M): More tokens provide better estimation of energy gap but increase computation; fewer tokens are faster but less accurate
- Failure signatures:
  - Poor distribution shift performance: Indicates the regularization isn't effectively preserving the vision-language connection
  - Decreased reference data performance: Suggests the regularization is too strong and hindering fine-tuning
  - High variance in results: May indicate insufficient random text sampling or hyperparameter issues
- First 3 experiments:
  1. Implement Lipsum-FT with default hyperparameters (L=8, M=80) on DomainNet-R → DomainNet-{P,C,I,S} and compare to FT baseline
  2. Vary the length of random text (L=1, 4, 8, 16, 32) to find optimal value for preserving vision-language connection
  3. Compare Lipsum-FT with MSE loss to Lipsum-FT with KLD loss to validate the effectiveness of MSE loss

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Limited ablation studies on key hyperparameters (M, L) that could impact performance
- Evaluation focused on CLIP models without testing generalization to other vision-language architectures
- Results primarily on benchmark datasets without extensive testing on real-world distribution shifts

## Confidence
**High Confidence**: The core mathematical formulation of minimizing the energy gap between fine-tuned and zero-shot models is sound and well-grounded in energy-based model theory. The experimental results showing Lipsum-FT outperforming baselines on the tested benchmarks appear reliable based on the provided metrics and statistical significance tests.

**Medium Confidence**: The interpretation that preserving the pre-trained vision-language connection explains Lipsum-FT's success is plausible but not definitively proven. Alternative explanations, such as the regularization acting as an implicit data augmentation mechanism, cannot be ruled out based on the current evidence.

**Low Confidence**: The specific claims about MSE being superior to KLD loss for this application are based on limited theoretical analysis and single experiments. The temperature limit argument for KLD is mathematically sound but may not capture practical training dynamics.

## Next Checks
1. **Ablation study on random text hyperparameters**: Systematically vary M (number of random tokens) and L (token length) to determine their impact on performance and identify optimal values across different datasets and model sizes.

2. **Comparison with alternative regularization approaches**: Test Lipsum-FT against other regularization methods that preserve pre-trained representations, such as feature-based penalties or contrastive learning objectives, to isolate the specific contribution of energy gap minimization.

3. **Generalization to new domains and models**: Apply Lipsum-FT to vision-language models beyond CLIP (e.g., ALIGN, FLAVA) and test on real-world distribution shifts not present in the standard benchmark datasets to evaluate true robustness gains.