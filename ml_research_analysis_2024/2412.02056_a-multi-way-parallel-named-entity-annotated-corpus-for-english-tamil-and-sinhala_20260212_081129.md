---
ver: rpa2
title: A Multi-way Parallel Named Entity Annotated Corpus for English, Tamil and Sinhala
arxiv_id: '2412.02056'
source_url: https://arxiv.org/abs/2412.02056
tags:
- sinhala
- entity
- language
- tamil
- named
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-way parallel English-Tamil-Sinhala
  corpus annotated with named entities (NEs) for low-resource languages. The authors
  manually annotated 3835 sentences per language using the CoNLL03 tag set and BIO
  format, establishing a new benchmark dataset for Sinhala and Tamil NER.
---

# A Multi-way Parallel Named Entity Annotated Corpus for English, Tamil and Sinhala

## Quick Facts
- arXiv ID: 2412.02056
- Source URL: https://arxiv.org/abs/2412.02056
- Authors: Surangika Ranathunga; Asanka Ranasinghea; Janaka Shamala; Ayodya Dandeniyaa; Rashmi Galappaththia; Malithi Samaraweeraa
- Reference count: 40
- Key outcome: Multi-way parallel English-Tamil-Sinhala corpus with 3835 sentences per language annotated with named entities, establishing new NER benchmarks for low-resource languages

## Executive Summary
This paper introduces a multi-way parallel corpus for English, Tamil, and Sinhala with named entity annotations, addressing the scarcity of annotated data for low-resource languages. The authors manually annotated 3835 sentences per language using the CoNLL03 tag set in BIO format, creating the first benchmark dataset for Sinhala and Tamil NER. Using pre-trained multilingual language models, they achieved state-of-the-art NER results, with XLM-R outperforming language-specific models. The study also demonstrates how NER outputs can significantly improve neural machine translation quality for low-resource language pairs.

## Method Summary
The authors created a parallel corpus by translating government documents from English to Tamil and Sinhala, then manually annotated 3835 sentences per language with named entities using the CoNLL03 tag set (PER, LOC, ORG, MISC) in BIO format. They fine-tuned several pre-trained language models including mBERT, XLM-R, SinBERT, and IndicBERT on the annotated corpus for NER tasks. The models were evaluated using macro F1 scores, and the best-performing NER system was integrated into a neural machine translation pipeline to assess its impact on translation quality. The entire dataset and trained models are made publicly available for future research.

## Key Results
- XLM-R multilingual model outperformed language-specific models (SinBERT, IndicBERT) for both Sinhala and Tamil NER tasks
- NER system integration improved NMT BLEU score from 11.9 to 21.12 for English-Sinhala translation
- Established first benchmark datasets for named entity recognition in Sinhala and Tamil languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual XLM-R model outperforms language-specific models for Sinhala and Tamil NER
- Mechanism: Cross-lingual transfer of knowledge from high-resource languages in XLM-R to low-resource languages
- Core assumption: The XLM-R model has sufficient representation of both Indo-Aryan and Dravidian language families
- Evidence anchors:
  - [abstract]: "using pre-trained multilingual Language Models (mLMs) like mBERT and XLM-R, they achieved state-of-the-art NER results, with the multilingual XLM-R model outperforming language-specific models for both Sinhala and Tamil"
  - [section 6]: "SinBERT, which was trained with just 15.7 million Sinhala sentences, significantly outperforms the corresponding Bi-LSTM CRF results... We believe the superior performance of XLM-R is due to its cross-lingual transfer abilities"
  - [corpus]: Limited direct evidence of XLM-R's internal cross-lingual mechanisms; relies on paper claims
- Break condition: If the low-resource languages lack sufficient similarity to languages represented in XLM-R's pre-training data

### Mechanism 2
- Claim: Multi-way parallel corpus serves as effective benchmark for evaluating mLMs across languages
- Mechanism: Same sentences annotated across languages allow direct comparison of model performance on identical content
- Core assumption: The parallel corpus maintains semantic equivalence across language translations
- Evidence anchors:
  - [abstract]: "multi-way parallel English-Tamil-Sinhala corpus annotated with named entities (NEs) for low-resource languages"
  - [section 3.4]: "Our final dataset consists of 3835 parallel sentences per language"
  - [corpus]: Weak evidence of semantic equivalence across languages; paper mentions translation discrepancies as a challenge
- Break condition: If translation introduces significant semantic drift between parallel sentences

### Mechanism 3
- Claim: NER system output improves neural machine translation for low-resource language pairs
- Mechanism: NER provides explicit entity identification that NMT models can leverage for better entity translation
- Core assumption: NMT models struggle with named entity translation without explicit guidance
- Evidence anchors:
  - [abstract]: "demonstrated the utility of their NER system in a low-resource neural machine translation (NMT) task, showing significant improvements in translation accuracy and BLEU scores when integrating NER outputs"
  - [section 7]: "the NMT model that incorporates our NER output shows the highest NE translation accuracy" with BLEU improvement from 11.9 to 21.12
  - [corpus]: Direct evidence from case study showing BLEU score improvement from 11.9 to 21.12
- Break condition: If the NMT model architecture cannot effectively utilize NER output features

## Foundational Learning

- Concept: BIO annotation scheme
  - Why needed here: The paper uses BIO format for named entity annotation, which is critical for training and evaluation
  - Quick check question: What do the B, I, and O tags represent in BIO format?

- Concept: Pre-trained multilingual language models (mLMs)
  - Why needed here: The paper's NER system relies on mBERT and XLM-R as foundational models
  - Quick check question: How do mLMs like XLM-R achieve cross-lingual transfer?

- Concept: Neural Machine Translation (NMT) with encoder-decoder architecture
  - Why needed here: The paper demonstrates NER utility in NMT, requiring understanding of how NMT models work
  - Quick check question: What is the role of the encoder and decoder in transformer-based NMT models?

## Architecture Onboarding

- Component map: Parallel corpus annotation pipeline -> Pre-trained LM fine-tuning layer -> Evaluation framework -> NMT integration module
- Critical path: Data annotation → Model fine-tuning → Evaluation → NMT integration → Performance measurement
- Design tradeoffs: Language-specific vs. multilingual models (accuracy vs. coverage), fine-grained vs. coarse NE tags (information richness vs. data requirements)
- Failure signatures: Poor cross-lingual transfer indicates inadequate language representation in pre-training; NER-NMT integration failures suggest feature compatibility issues
- First 3 experiments:
  1. Fine-tune XLM-R base, mBERT base, SinBERT small, and IndicBERT models on individual languages and multilingual setup (all three languages together) for NER task using 3 epochs, batch size 8, learning rate tuned via Optuna
  2. Compare XLM-R performance against language-specific models (SinBERT, IndicBERT) on individual languages
  3. Integrate NER output with baseline NMT and measure BLEU score improvement on English-Sinhala translation task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of language-specific NER models compare to multilingual models when trained on larger datasets for Sinhala and Tamil?
- Basis in paper: [inferred] The paper notes that XLM-R outperforms language-specific models like SinBERT for Sinhala and suggests multilingual models may be better when data is limited, but doesn't explore performance at scale.
- Why unresolved: The paper only tested with the current dataset size (3835 sentences) and didn't experiment with varying dataset sizes to determine when language-specific models might catch up or surpass multilingual ones.
- What evidence would resolve

## Limitations
- The annotated corpus size (3,835 sentences per language) may not capture full diversity of named entities in these languages
- Manual annotation process introduces potential inter-annotator variability that isn't quantified
- Focus on four NE categories from CoNLL03 tag set may not fully capture named entity types relevant to Sinhala and Tamil
- Translation-based approach for creating parallel corpora may introduce semantic drift between languages

## Confidence

- **High Confidence**: The experimental results showing XLM-R outperforming language-specific models are well-supported with quantitative evidence and multiple baselines. The BLEU score improvement from 11.9 to 21.12 in the NMT case study is directly measurable and reported with specific metrics.

- **Medium Confidence**: The claim that cross-lingual transfer is the primary mechanism behind XLM-R's superior performance is reasonable but not definitively proven. While the paper provides comparative results, it doesn't conduct ablation studies to isolate the contribution of cross-lingual transfer specifically.

- **Low Confidence**: The assertion that this corpus will serve as a definitive benchmark for evaluating mLMs across languages is somewhat premature given the corpus size limitations and the lack of comparison with existing benchmarks in other low-resource language pairs.

## Next Checks
1. Conduct inter-annotator agreement studies on a subset of the corpus to quantify annotation consistency and identify systematic annotation patterns or biases across the three languages.

2. Perform semantic equivalence analysis on the parallel sentences to measure translation drift and assess how well the corpus maintains semantic alignment across languages, potentially using automatic semantic similarity metrics.

3. Extend the NMT integration experiment to include multiple translation directions and diverse domains (e.g., news, social media) to evaluate the robustness and generalizability of NER-NMT integration benefits across different language pairs and content types.