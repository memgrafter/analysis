---
ver: rpa2
title: Unraveling overoptimism and publication bias in ML-driven science
arxiv_id: '2405.14422'
source_url: https://arxiv.org/abs/2405.14422
tags:
- learning
- classification
- data
- accuracy
- schizophrenia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a stochastic model for observed ML classification
  accuracy that incorporates both parametric learning curves and the effects of overfitting
  and publication bias. The model accounts for overfitting-induced bias through a
  Gaussian noise term with a mean that scales as n^(-0.5), and models publication
  bias using a selection mechanism where results below a sample-size-dependent threshold
  are censored.
---

# Unraveling overoptimism and publication bias in ML-driven science

## Quick Facts
- arXiv ID: 2405.14422
- Source URL: https://arxiv.org/abs/2405.14422
- Reference count: 40
- Primary result: Introduces truncated regression method to recover true ML learning curves from overoptimistic published results

## Executive Summary
This paper addresses the pervasive issue of overoptimism in machine learning literature by introducing a stochastic model that accounts for both overfitting and publication bias. The authors propose a truncated regression-based estimator that can recover true learning curve parameters even without knowing the publication bias threshold. Applied to meta-analyses of neurological condition classification, the method reveals realistic performance limits and identifies instances of overoptimistic published results.

## Method Summary
The approach models observed ML classification accuracy as a parametric learning curve with Gaussian noise scaled by n^(-0.5) to capture overfitting, plus truncation due to publication bias. The truncated regression method estimates parameters by matching empirical moments of published data to theoretical moments under truncation. The optimization uses NSGA-II genetic algorithm with bootstrapping for confidence intervals. Threshold estimation employs minimum order statistics and sliding window techniques.

## Key Results
- Successfully recovers true learning curve parameters from overoptimistic published accuracies in synthetic experiments
- Estimates realistic performance limits for neurological condition classification: AD (0.75-0.88), Schizophrenia (0.58-0.78), ASD (0.68-0.79), ADHD (0.56-0.75)
- Identifies systematic overoptimism in meta-analyses, with estimated true performance significantly below reported averages

## Why This Works (Mechanism)

### Mechanism 1
The truncated regression method can recover true learning curve parameters by modeling publication bias as a selection mechanism where only accuracies above sample-size-dependent thresholds are published, while accounting for overfitting through Gaussian noise with mean scaling as n^(-0.5).

### Mechanism 2
Overfitting to test sets and publication bias are primary causes of inverse sample size-accuracy relationships, as small samples are more prone to overfitting and research teams preferentially publish higher accuracies.

### Mechanism 3
Learning curve model parameters can be identified from truncated samples even with unknown thresholds, under conditions that the truncation set is a half-line and sufficient samples satisfy polynomial bounds.

## Foundational Learning

- **Parametric learning curves in ML**: Models assume power law relationships y(n) = A + αn^β between sample size and performance; needed because the paper's framework relies on this parametric form for estimation.

- **Publication bias and selection mechanisms**: Models assume only results above thresholds are published, creating truncated samples; needed because the paper explicitly models this to correct for overoptimism.

- **Truncated regression and method of moments**: Uses moment matching on truncated distributions to estimate parameters; needed because standard regression fails with truncated data and this provides identifiability.

## Architecture Onboarding

- **Component map**: Data generation -> Truncated regression -> Evaluation -> Application
- **Critical path**: Generate synthetic data → Apply truncated regression → Evaluate accuracy → Apply to real-world meta-analysis
- **Design tradeoffs**: Parametric vs. non-parametric models (parametric chosen for interpretability); known vs. unknown thresholds (unknown chosen for realism)
- **Failure signatures**: Poor agreement between estimated and true curves; inconsistent results across domains; violation of scaling assumptions
- **First 3 experiments**: 1) Recover parameters from synthetic data with known ground truth 2) Simulate ML development with overfitting/bias and evaluate recovery 3) Apply to neuroimaging meta-analysis data

## Open Questions the Paper Calls Out

- **Open Question 1**: Minimum number of observations required for reliable parameter estimation under overfitting and publication bias; unresolved due to theoretical focus on identifiability rather than sample complexity.

- **Open Question 2**: Method performance when truncation set is not a half-line (e.g., multiple intervals); unresolved because theoretical analysis assumes half-line truncation.

- **Open Question 3**: Robustness to violations of Gaussian noise assumption in the observation model; unresolved because both theory and experiments assume correct model specification.

## Limitations

- Theoretical identifiability proof requires specific sample size conditions that may not hold in real meta-analyses with limited studies per sample size
- Assumed n^(-0.5) overfitting scaling is derived asymptotically and may not reflect empirical behavior across diverse ML architectures
- Assumes all studies in a meta-analysis follow the same underlying learning curve, which may not hold for heterogeneous experimental designs

## Confidence

- **High confidence**: Core mathematical framework connecting learning curves, overfitting, and truncated regression is sound and theoretically grounded
- **Medium confidence**: Empirical demonstrations show reasonable performance but rely on synthetic data and relatively small real-world meta-analyses
- **Low confidence**: Generalizability of n^(-0.5) overfitting scaling across diverse ML domains and assumption that single learning curve describes heterogeneous published studies

## Next Checks

1. Conduct systematic sensitivity analyses varying the assumed overfitting scaling parameter (currently fixed at -0.5) across different synthetic data generation scenarios to quantify robustness to this key assumption.

2. Apply the method to additional meta-analysis domains with larger sample sizes per condition to test whether the polynomial sample size requirements for identifiability are met in practice.

3. Perform ablation studies comparing the truncated regression approach to alternative bias-correction methods (e.g., selection models, Heckman correction) on the same datasets to establish relative performance advantages.