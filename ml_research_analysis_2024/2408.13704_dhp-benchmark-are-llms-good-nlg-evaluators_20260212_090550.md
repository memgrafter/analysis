---
ver: rpa2
title: 'DHP Benchmark: Are LLMs Good NLG Evaluators?'
arxiv_id: '2408.13704'
source_url: https://arxiv.org/abs/2408.13704
tags:
- scores
- evaluation
- llms
- perturbation
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the DHP benchmark, a new framework to assess\
  \ how well large language models (LLMs) can evaluate the quality of generated text.\
  \ The core idea is to measure an LLM\u2019s \u201Cdiscernment\u201D by seeing if\
  \ it can correctly assign lower scores to perturbed (lower-quality) text compared\
  \ to the original."
---

# DHP Benchmark: Are LLMs Good NLG Evaluators?

## Quick Facts
- arXiv ID: 2408.13704
- Source URL: https://arxiv.org/abs/2408.13704
- Reference count: 40
- Primary result: Most LLMs can generally identify quality issues, but performance varies significantly by model size, task, and dataset complexity, with GPT-4 Turbo showing the most consistent high discernment scores.

## Executive Summary
This paper introduces the DHP benchmark, a new framework to assess how well large language models (LLMs) can evaluate the quality of generated text. The core idea is to measure an LLM's "discernment" by seeing if it can correctly assign lower scores to perturbed (lower-quality) text compared to the original. The benchmark uses hierarchical perturbations at character, word, and sentence levels, combined with statistical testing to derive quantitative discernment scores. Six datasets across summarization, story completion, question answering, and translation were used.

## Method Summary
The DHP framework consists of three main stages: (1) Hierarchical Perturbation - generating perturbed datasets using character/word/sentence level modifications; (2) LLM Evaluation - scoring original and perturbed texts using Auto-CoT prompts across multiple quality metrics; (3) Statistical Analysis - applying Wilcoxon tests, combining p-values via harmonic mean with expert weights, and converting to discernment scores. The perturbation methods are task-specific with 4-6 perturbation types each.

## Key Results
- Most LLMs can generally identify quality issues in generated text
- Performance varies significantly by model size, task, and dataset complexity
- GPT-4 Turbo showed the most consistent high discernment scores across tasks
- Task-specific variations in LLM performance were observed across summarization, story completion, QA, and translation

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical perturbation at multiple linguistic levels exposes different quality issues that LLMs must detect to be considered good evaluators. The benchmark systematically introduces controlled perturbations at character, word, and sentence levels using both rule-based and LLM-based methods. These perturbations are designed to degrade specific quality aspects (e.g., coherence, consistency, fluency, accuracy, relevance). LLMs are then evaluated on their ability to assign lower scores to perturbed texts compared to originals. The Wilcoxon Signed-Rank Test measures whether score distributions differ significantly, independent of absolute score values or response styles.

### Mechanism 2
Harmonic Mean p-values with expert weights provide a statistically robust method for combining multiple evaluation metrics while accounting for metric-specific relevance. For each perturbation type, multiple p-values are generated (one per metric). The Harmonic Mean p-value (HMP) method combines these p-values, emphasizing smaller values (indicating stronger discernment). Expert weights are then applied to prioritize p-values from metrics most relevant to the specific perturbation type, ensuring the combination reflects targeted quality detection rather than general score differences.

### Mechanism 3
Using statistical tests (Wilcoxon Signed-Rank Test) instead of correlation with human scores eliminates bias from LLM response styles, providing a more objective measure of evaluation capability. The Wilcoxon Signed-Rank Test compares score distributions between original and perturbed texts without assuming normality or focusing on absolute values. This approach measures whether LLMs can detect quality issues (assigning lower scores to perturbed text) rather than whether their scores align with human preferences, which may be influenced by response style biases.

## Foundational Learning

- Concept: Statistical hypothesis testing and p-value interpretation
  - Why needed here: The framework relies on Wilcoxon Signed-Rank Tests to determine if score differences between original and perturbed texts are statistically significant. Understanding how p-values reflect confidence in rejecting the null hypothesis is essential for interpreting discernment scores.
  - Quick check question: If a p-value is 0.03 for a perturbation, what does this indicate about the LLM's ability to discern quality differences?

- Concept: Hierarchical text perturbation and its effects on quality metrics
  - Why needed here: The benchmark uses perturbations at character, word, and sentence levels to create specific quality degradations. Understanding how different perturbation types affect coherence, consistency, fluency, etc., is crucial for both implementing the framework and interpreting results.
  - Quick check question: How would a grammatical error perturbation at the word level likely affect fluency versus coherence scores?

- Concept: Harmonic mean calculation and its properties for combining values
  - Why needed here: The framework uses harmonic mean p-values to combine multiple metric p-values, with expert weights applied. Understanding why harmonic mean (versus arithmetic mean) is used and how weights affect the combination is important for proper implementation.
  - Quick check question: Why might harmonic mean be preferred over arithmetic mean when combining p-values from multiple metrics?

## Architecture Onboarding

- Component map: Hierarchical Perturbation -> LLM Evaluation -> Statistical Analysis -> Discernment Score Calculation
- Critical path: The most critical sequence is: perturbation generation → LLM scoring → statistical analysis → discernment score calculation. Any failure in perturbation quality or LLM evaluation consistency will propagate through to invalid discernment scores.
- Design tradeoffs: The framework trades comprehensiveness for manageability by using 100 datapoints per dataset and 4-6 perturbation types. More perturbations would provide finer-grained assessment but increase computational cost exponentially.
- Failure signatures: Poor discernment scores (below 1) indicate either the LLM cannot detect specific quality issues or the perturbation method is not effectively targeting the intended metric. Large discrepancies between D and DEW scores suggest metric misunderstanding.
- First 3 experiments:
  1. Implement a single perturbation type (e.g., fictional named entities) on one dataset (e.g., SummEval) and verify that the LLM consistently assigns lower scores to perturbed texts than originals. Check that the Wilcoxon test produces significant p-values.
  2. Test the expert weight survey by having a small group of NLP experts complete it for one task, then compare their weight distributions to ensure they align with expected metric sensitivity to perturbations.
  3. Run the complete pipeline on a small subset (10 datapoints) of one dataset to verify the end-to-end workflow produces reasonable discernment scores and that all components (perturbation, evaluation, statistics) are functioning correctly together.

## Open Questions the Paper Calls Out

1. **Effectiveness of LLM-as-Judge Across Diverse NLG Tasks**
   - How does the effectiveness of LLM-as-judge vary across different NLG tasks, such as summarization, story completion, question answering, and translation?
   - The paper provides results for each task but does not deeply explore the reasons for task-specific performance variations.

2. **Impact of Hierarchical Perturbation Complexity on LLM Discernment**
   - How does the complexity of hierarchical perturbations (character, word, sentence level) affect the discernment scores of LLMs?
   - The paper does not fully explore how the difficulty of perturbations influences LLM performance or whether certain levels of perturbation are more challenging for LLMs to detect.

3. **Generalization of LLM Discernment Across Languages and Domains**
   - How well do LLM discernment scores generalize across different languages and domains, such as technical/scientific text versus general news?
   - The paper highlights challenges in specific datasets but does not systematically explore cross-language or cross-domain generalization.

4. **Role of Expert Weights in Mitigating LLM Metric Misunderstanding**
   - How effective are expert weights in mitigating LLM misunderstandings of evaluation metrics, and can this approach be automated?
   - The paper demonstrates the utility of expert weights but does not explore whether this process can be automated or scaled to reduce reliance on human experts.

5. **Long-Term Stability and Adaptability of LLM Evaluators**
   - How stable and adaptable are LLM evaluators over time, especially as new types of NLG tasks and quality issues emerge?
   - The paper evaluates current LLM performance but does not address how these models might perform as NLG tasks evolve or as new perturbation types are introduced.

## Limitations

- Reliance on expert-assigned weights introduces subjectivity that may affect cross-task comparability of discernment scores
- Benchmark's perturbation strategies may not capture all real-world quality issues that human evaluators consider
- Use of only 100 datapoints per dataset may not provide sufficient statistical power for some subtle perturbation effects

## Confidence

- High confidence: The statistical testing methodology using Wilcoxon Signed-Rank Tests is well-established and appropriate for paired comparisons of score distributions
- Medium confidence: The perturbation methods effectively target specific quality metrics, though actual impact needs more empirical validation
- Medium confidence: The hierarchical perturbation approach successfully differentiates between character, word, and sentence-level quality issues

## Next Checks

1. Conduct ablation studies removing individual perturbation types to quantify their relative contribution to discernment scores and verify that each perturbation method effectively targets its intended quality metric.

2. Test the framework's robustness by applying it to LLM evaluators with known response style biases to confirm that the Wilcoxon test approach successfully eliminates these biases from the discernment scores.

3. Expand the perturbation diversity beyond the current 4-6 types per task to include more nuanced quality degradations and validate whether the current perturbation set captures the full spectrum of NLG evaluation challenges.