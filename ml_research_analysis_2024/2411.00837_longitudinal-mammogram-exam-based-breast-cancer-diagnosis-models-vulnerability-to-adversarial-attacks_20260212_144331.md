---
ver: rpa2
title: 'Longitudinal Mammogram Exam-based Breast Cancer Diagnosis Models: Vulnerability
  to Adversarial Attacks'
arxiv_id: '2411.00837'
source_url: https://arxiv.org/abs/2411.00837
tags:
- adversarial
- attacks
- attack
- longitudinal
- cancer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the vulnerability of longitudinal breast
  cancer diagnosis models to adversarial attacks. The proposed method combines cross-entropy
  loss with distance metric learning, leveraging knowledge about the relationship
  between sequential mammogram exams.
---

# Longitudinal Mammogram Exam-based Breast Cancer Diagnosis Models: Vulnerability to Adversarial Attacks

## Quick Facts
- **arXiv ID**: 2411.00837
- **Source URL**: https://arxiv.org/abs/2411.00837
- **Reference count**: 26
- **Primary result**: Proposed adversarial attack method combining cross-entropy loss with distance metric learning achieves AUC of 0.205, outperforming state-of-the-art attacks on longitudinal breast cancer diagnosis models.

## Executive Summary
This study investigates the vulnerability of longitudinal breast cancer diagnosis models to adversarial attacks, specifically focusing on models that use sequential mammogram exams (Prior and Current) for diagnosis. The authors propose a novel adversarial attack method that combines cross-entropy loss with distance metric learning, leveraging knowledge about the relationship between sequential exams. Experiments on 590 breast cancer patients demonstrate that this approach successfully fools the diagnosis models, achieving significantly lower AUC scores than existing attack methods. The attack remains effective even when the target model is trained with adversarial training, highlighting a significant security vulnerability in these clinical models.

## Method Summary
The study uses a Source-Target attack framework where adversarial samples are generated using a single-input model (Source) and transferred to attack a two-input longitudinal model (Target). The proposed attack combines cross-entropy loss with knowledge-guided distance metric learning to exploit the temporal relationships between Prior and Current exams. The Target model architecture employs a Swin Transformer or VGG backbone with a cross-view module using multi-head attention mechanisms. The attack is evaluated using 5-fold cross-validation on a dataset of 590 breast cancer patients, comparing against baseline attacks including FGSM, I-FGSM, C&W, MI-FGSM, and PGD.

## Key Results
- The proposed attack method achieves an AUC of 0.205, significantly lower than baseline attacks
- Attack effectiveness remains strong even when the Target model is trained with adversarial training defenses
- The attack exploits the longitudinal model's reliance on the relationship between Prior (normal) and Current (diagnosis target) exams
- Cross-view module in the Target model is crucial for leveraging temporal relationships between sequential exams

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial attacks can exploit the relationship between sequential mammogram exams to fool longitudinal diagnosis models.
- Mechanism: The proposed method generates adversarial samples for the Current exam while leveraging knowledge of the Euclidean distance relationship between Prior and Current exams across normal and cancer cases. This exploits the longitudinal model's reliance on temporal relationships for classification.
- Core assumption: The longitudinal model's decision-making depends on the spatial relationship between Prior (normal) and Current (diagnosis target) exams.
- Evidence anchors:
  - [abstract] "Our proposed method surpasses several state-of-the-art adversarial attacks in fooling the diagnosis models to give opposite outputs."
  - [section] "In Cancer cases, we select the adversarial sample P atienti,adv_current that is closest from P atienti,prior, while in Control cases, we choose the sample that is furthest to P atienti,prior."
  - [corpus] Weak - no direct corpus evidence for this specific attack mechanism.
- Break condition: If the model no longer uses temporal relationships between sequential exams, or if the distance relationship between normal and cancer cases differs from the assumed pattern.

### Mechanism 2
- Claim: Distance metric learning can effectively guide adversarial sample generation to fool longitudinal models.
- Mechanism: The method uses a combination of cross-entropy loss (to push samples across decision boundaries) and distance metric learning (to modify the relationship between sequential imaging exams). This dual approach ensures samples both cross boundaries and maintain deceptive temporal relationships.
- Core assumption: Combining cross-entropy loss with distance metric learning creates more effective adversarial samples than either method alone.
- Evidence anchors:
  - [abstract] "Our proposed method combines cross-entropy loss with distance metric learning, leveraging knowledge about the relationship between sequential mammogram exams."
  - [section] "The proposed criteria is designed based on the knowledge of: (1) the longitudinal model relies on the relationship of Prior (always normal) and Current (could be cancer or normal) exams to make a diagnosis"
  - [corpus] Weak - no direct corpus evidence for this specific combination approach.
- Break condition: If the model's architecture changes such that temporal relationships are no longer used, or if distance metric learning becomes ineffective at guiding adversarial samples.

### Mechanism 3
- Claim: Attack transferability from single-input models to longitudinal models remains effective.
- Mechanism: The method generates adversarial samples using a Source model (single time-point input) and transfers them to attack a Target longitudinal model. This black-box approach works because the Target model's vulnerability to adversarial samples is preserved across architectures.
- Core assumption: Adversarial samples generated from a single-input model can effectively transfer to fool a longitudinal model that uses sequential inputs.
- Evidence anchors:
  - [abstract] "Experiments on 590 breast cancer patients...show that this approach outperforms several state-of-the-art adversarial attack methods"
  - [section] "Attackers craft adversarial samples using the Source model, following the scheme of 'attack transferring', to perform attacks to the Target model."
  - [corpus] Weak - no direct corpus evidence for this specific transferability approach.
- Break condition: If the Target model implements strong defenses against transferred attacks, or if the Source and Target models have fundamentally different decision boundaries.

## Foundational Learning

- **Concept: Cross-entropy loss in adversarial attacks**
  - Why needed here: Cross-entropy loss is used to guide adversarial samples across the decision boundary, ensuring they are classified incorrectly.
  - Quick check question: What is the purpose of using cross-entropy loss in adversarial sample generation?

- **Concept: Distance metric learning**
  - Why needed here: Distance metric learning modifies the relationship between sequential imaging exams, exploiting the longitudinal model's reliance on temporal relationships.
  - Quick check question: How does distance metric learning differ from standard adversarial attack methods?

- **Concept: Attack transferability**
  - Why needed here: The black-box attack approach generates samples from a single-input model and transfers them to attack a longitudinal model, avoiding the need to access the Target model's architecture.
  - Quick check question: Why is attack transferability useful in black-box scenarios?

## Architecture Onboarding

- **Component map**: Input → Feature extraction → Cross-view attention → Diagnosis prediction → Adversarial sample generation → Attack transfer
- **Critical path**: Input → Feature extraction → Cross-view attention → Diagnosis prediction → Adversarial sample generation → Attack transfer
- **Design tradeoffs**:
  - Using Swin Transformer vs. VGG for backbone architecture
  - Single-step vs. iterative adversarial sample generation
  - Distance-guided selection vs. random adversarial sample selection
- **Failure signatures**:
  - AUC values returning to normal levels (>0.5) indicates successful defense
  - Loss of attack effectiveness when model architecture changes
  - Reduced attack success when adversarial training is applied
- **First 3 experiments**:
  1. Generate adversarial samples using only cross-entropy loss and evaluate attack effectiveness
  2. Generate adversarial samples using only distance metric learning and evaluate attack effectiveness
  3. Combine both methods and evaluate if attack effectiveness improves over individual approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of adversarial attacks on longitudinal models for other medical imaging modalities (e.g., MRI, CT)?
- Basis in paper: [explicit] The study focuses on mammogram images and discusses the importance of longitudinal analysis in clinical radiology, but does not explore other modalities.
- Why unresolved: The paper only evaluates the proposed attack method on mammogram images, leaving the generalizability to other imaging modalities unexplored.
- What evidence would resolve it: Conducting experiments on longitudinal models using MRI or CT scans and comparing the effectiveness of adversarial attacks across different modalities.

### Open Question 2
- Question: How do adversarial attacks affect the interpretability of longitudinal models in clinical settings?
- Basis in paper: [inferred] The paper highlights the vulnerability of longitudinal models but does not discuss the implications for model interpretability, which is crucial for clinical trust and decision-making.
- Why unresolved: The study focuses on attack efficacy and robustness but does not address how adversarial attacks might alter the interpretability of model predictions in clinical practice.
- What evidence would resolve it: Analyzing the changes in feature importance and decision-making processes of longitudinal models under adversarial attacks to assess impacts on interpretability.

### Open Question 3
- Question: What are the long-term effects of adversarial training on the performance and robustness of longitudinal models?
- Basis in paper: [explicit] The paper mentions that adversarial training slightly decreases performance on clean data but increases robustness, yet it does not explore long-term effects.
- Why unresolved: The study evaluates short-term effects of adversarial training but does not investigate how prolonged adversarial training might impact model performance and robustness over time.
- What evidence would resolve it: Conducting longitudinal studies on models with continuous adversarial training to assess changes in performance and robustness over extended periods.

## Limitations
- Attack methodology relies on specific temporal relationship assumptions that may not generalize across different clinical datasets
- Study uses a relatively small dataset of 590 patients from a single institution, limiting external validity
- Black-box attack transferability results may not generalize to other model architectures beyond tested backbones

## Confidence
- **Medium**: The experimental results demonstrate attack effectiveness, but limitations in dataset size and architecture generalizability affect confidence

## Next Checks
1. Test the proposed attack method on an independent, multi-institutional breast cancer dataset to verify robustness across different imaging protocols and patient populations
2. Evaluate the attack transferability to additional model architectures beyond Swin Transformer and VGG, including modern vision transformers and CNNs
3. Conduct ablation studies to isolate the contribution of distance metric learning versus cross-entropy loss in achieving attack success, to validate the proposed mechanism