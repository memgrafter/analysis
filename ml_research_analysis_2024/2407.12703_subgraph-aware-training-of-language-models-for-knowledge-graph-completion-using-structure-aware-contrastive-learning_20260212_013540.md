---
ver: rpa2
title: Subgraph-Aware Training of Language Models for Knowledge Graph Completion Using
  Structure-Aware Contrastive Learning
arxiv_id: '2407.12703'
source_url: https://arxiv.org/abs/2407.12703
tags:
- triples
- satkgc
- hits
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SATKGC, a subgraph-aware training framework
  for knowledge graph completion that integrates structural inductive bias into language
  model fine-tuning. The method uses subgraph sampling with biased random walks to
  create mini-batches, proximity-aware contrastive learning that emphasizes harder
  negative triples based on shortest path distances, and frequency-aware weighting
  to address long-tailed entity distributions.
---

# Subgraph-Aware Training of Language Models for Knowledge Graph Completion Using Structure-Aware Contrastive Learning

## Quick Facts
- arXiv ID: 2407.12703
- Source URL: https://arxiv.org/abs/2407.12703
- Authors: Youmin Ko; Hyemin Yang; Taeuk Kim; Hyunjoon Kim
- Reference count: 40
- Primary result: State-of-the-art KGC performance with 5.03% Hits@1 improvement on WN18RR and 5.28% on FB15k-237

## Executive Summary
This paper introduces SATKGC, a subgraph-aware training framework for knowledge graph completion that integrates structural inductive bias into language model fine-tuning. The method uses subgraph sampling with biased random walks to create mini-batches, proximity-aware contrastive learning that emphasizes harder negative triples based on shortest path distances, and frequency-aware weighting to address long-tailed entity distributions. Extensive experiments on three benchmarks show state-of-the-art performance, with Hits@1 improvements of 5.03% on WN18RR and 5.28% on FB15k-237 compared to existing models. The approach also demonstrates strong scalability and inductive capability on large-scale Wikidata5M, outperforming retrieval-augmented and large language model-based baselines.

## Method Summary
SATKGC fine-tunes PLMs for KGC by creating subgraph-aware mini-batches using biased random walk sampling (BRWR) that preferentially visits low-degree entities. The training employs proximity-aware contrastive learning where negative triples are weighted by inverse shortest path distance between entities, making structurally closer negatives contribute more to the loss. A frequency-aware weighting mechanism reweights triples based on tail entity degree to address long-tailed entity distributions. The framework uses two MPNet encoders for (head, relation) and tail entity representations, trained with a weighted InfoNCE loss that incorporates structural hardness factors and frequency-based importance scores.

## Key Results
- Achieves state-of-the-art performance on WN18RR with 66.0% Hits@1, improving over previous best by 5.03%
- Outperforms existing models on FB15k-237 with 57.3% Hits@1, a 5.28% improvement
- Demonstrates strong scalability on Wikidata5M with 57.9% Hits@1 in transductive setting
- Shows inductive capability with 54.1% Hits@1 on Wikidata5M-Ind, surpassing RAG-based and LLM-based baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subgraph-aware mini-batching mitigates long-tailed entity frequency imbalance during training.
- Mechanism: Random-walk based subgraph sampling (BRWR) preferentially visits low-degree entities by using inverse degree probabilities in neighbor selection, creating mini-batches with more uniform entity distribution compared to random sampling.
- Core assumption: Skewed entity frequency distribution in training triples harms KGC performance, and subgraphs with inverse degree sampling can create more balanced mini-batches.
- Evidence anchors:
  - [section] "In this manner, giving more chances for an entity with a lower degree to engage in training alleviates the skewed frequency of entity occurrences throughout training, which in turn enhances the KGC performance."
  - [section] "BRWR is the least skewed whereas RANDOM shows the most skewed distribution. BRWR_P, which employs the degree-proportional distribution, extracts many duplicate entities in the subgraphs, leading to the most skewed distribution among the SaaM variants."
  - [corpus] "No direct corpus evidence found for this specific mechanism. Assumption: This is the authors' proposed mechanism based on KG topology."
- Break condition: If the KG has very uniform degree distribution or if BRWR sampling doesn't sufficiently change entity visitation patterns compared to random sampling.

### Mechanism 2
- Claim: Proximity-aware contrastive learning improves discrimination of hard negative triples based on KG structural properties.
- Mechanism: Negative triples are weighted by inverse shortest path distance between head and tail entities, making structurally closer (and thus harder) negatives contribute more to the loss function.
- Core assumption: The difficulty of distinguishing negative from positive triples correlates with the structural proximity of entities in the KG.
- Evidence anchors:
  - [abstract] "new contrastive learning to focus more on harder in-batch negative triples and harder positive triples in terms of the structural properties of the knowledge graph"
  - [section] "For each positive triple (‚Ñé, ùëü, ùë°) in a mini-batch B, we propose loss L (‚Ñé,ùëü,ùë° ) below based on our aforementioned observation, i.e., entities close to each other are more likely to be related than entities far away from each other"
  - [corpus] "No direct corpus evidence found for this specific mechanism. Assumption: This is the authors' proposed mechanism based on KG topology."
- Break condition: If shortest path distance doesn't correlate with triple difficulty in the embedding space, or if the approximate distance calculation introduces too much noise.

### Mechanism 3
- Claim: Frequency-aware mini-batch training addresses the discrepancy between KG's long-tailed entity distribution and mini-batch uniformity.
- Mechanism: Triples are reweighted during training using a factor proportional to the logarithm of tail entity degree, making triples with higher-degree tails contribute more to the loss.
- Core assumption: The long-tailed entity frequency distribution in KGs creates class imbalance that harms KGC performance, and reweighting based on entity degree can mitigate this.
- Evidence anchors:
  - [section] "we propose to reweight the importance of each sample loss via ùúì G (ùë°) above, so the frequency distribution of B becomes close to that of T"
  - [section] "the frequency of every entity that occurs in a set T of triples varies; this frequency distribution follows the power law"
  - [corpus] "No direct corpus evidence found for this specific mechanism. Assumption: This is the authors' proposed mechanism based on class imbalance literature."
- Break condition: If the relationship between entity degree and occurrence frequency doesn't hold for the specific KG, or if reweighting creates instability in training.

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: Forms the foundation for how positive and negative samples are compared in the embedding space
  - Quick check question: What is the role of temperature parameter ùúè in InfoNCE loss and how does it affect the gradient magnitude?

- Concept: Random walks and graph traversal algorithms
  - Why needed here: BRWR relies on understanding how random walkers explore graph topology to create informative subgraphs
  - Quick check question: How does restart probability ùëùùëü affect the exploration-exploitation tradeoff in BRWR subgraph sampling?

- Concept: Knowledge graph embedding and link prediction
  - Why needed here: Provides context for why KGC is formulated as a ranking problem and how embeddings are used for prediction
  - Quick check question: What's the difference between transductive and inductive settings in KGC and why does it matter for model evaluation?

## Architecture Onboarding

- Component map: Pre-training phase (BRWR-based subgraph sampling) -> Training loop (SaaM subgraph selection) -> Encoder modules (MPNet bi-encoders) -> Loss computation (proximity-aware contrastive loss + frequency-aware reweighting) -> Evaluation (ranking using cosine similarity)

- Critical path:
  1. Pre-compute shortest path distances and degrees for all entities
  2. For each training iteration:
     - Select least-visited subgraph via SaaM
     - Encode all entities in subgraph through bi-encoders
     - Compute approximate distances between heads and tails
     - Calculate weighted InfoNCE loss with structural factors
     - Backpropagate and update model parameters

- Design tradeoffs:
  - BRWR vs RWR: BRWR provides more balanced entity visitation but may miss some high-degree entity interactions
  - Approximate vs exact distances: Approximate distances using center triple distances reduce computation but may introduce approximation error
  - Batch size selection: Larger batches provide more negative samples but increase memory requirements and may reduce per-entity visitation frequency

- Failure signatures:
  - Performance degradation with high restart probability (p·µ£): Indicates insufficient subgraph exploration
  - Unstable training with large frequency weights: Suggests reweighting factor ùúì G (ùë°) is too aggressive
  - Slow convergence despite many epochs: May indicate improper temperature parameter œÑ or margin Œ≥ settings

- First 3 experiments:
  1. Compare BRWR sampling with different restart probabilities (p·µ£ = 0.01, 0.04, 0.1) on Hits@1 to find optimal exploration balance
  2. Test approximate distance calculation methods (product vs sum of distances) to validate computational efficiency vs accuracy tradeoff
  3. Evaluate different frequency weighting schemes (linear, log, polynomial) on Wikidata5M-Ind to determine optimal degree-based reweighting approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed subgraph sampling strategy generalize to knowledge graphs with different structural properties, such as those with higher clustering coefficients or different degree distributions?
- Basis in paper: [inferred] The paper mentions that the subgraph sampling strategy is designed to address the long-tailed distribution of entity occurrences, but does not explicitly test its performance on KGs with varying structural properties.
- Why unresolved: The experiments only evaluate the method on three specific datasets (WN18RR, FB15k-237, and Wikidata5M), which may not be representative of all possible KG structures.
- What evidence would resolve it: Additional experiments on a diverse set of KGs with varying structural properties would demonstrate the method's generalizability.

### Open Question 2
- Question: Can the proximity-aware contrastive learning component be further improved by incorporating more complex structural information, such as multi-hop relationships or graph motifs?
- Basis in paper: [explicit] The paper states that the proximity-aware contrastive learning focuses on the shortest path distance between entities, but does not explore more complex structural features.
- Why unresolved: The current implementation only considers the shortest path distance, which may not capture all relevant structural information for distinguishing true and false triples.
- What evidence would resolve it: Experiments comparing the current approach with variations that incorporate additional structural features would demonstrate the potential for improvement.

### Open Question 3
- Question: How does the performance of SATKGC scale with increasing KG size and complexity, particularly in terms of training time and memory requirements?
- Basis in paper: [inferred] The paper mentions that the method achieves state-of-the-art performance on Wikidata5M, a large-scale KG, but does not provide a detailed analysis of its scalability.
- Why unresolved: The experiments only evaluate the method on three specific datasets, and do not explore its performance on even larger or more complex KGs.
- What evidence would resolve it: Experiments on progressively larger KGs, with detailed analysis of training time and memory usage, would demonstrate the method's scalability.

## Limitations

- The effectiveness of individual components (BRWR sampling, proximity-aware weighting, frequency-aware reweighting) is not empirically validated through isolated ablation studies
- The approximate distance calculation using center triple distances is presented as a computational shortcut without quantifying its accuracy impact on downstream performance
- The core assumption about structural proximity correlating with triple difficulty is presented as an observation rather than empirically validated through correlation analysis

## Confidence

- **High confidence** in the overall experimental results showing state-of-the-art performance across multiple benchmarks and the reproducibility of the training framework
- **Medium confidence** in the individual mechanisms (BRWR sampling, proximity-aware weighting, frequency-aware reweighting) as their isolated contributions are not empirically validated through controlled ablations
- **Low confidence** in the theoretical claims about why structural proximity correlates with triple difficulty and why inverse degree sampling creates better mini-batches, as these are presented as observations rather than proven relationships

## Next Checks

1. **Ablation study validation**: Run controlled experiments isolating each component (BRWR vs random sampling, exact vs approximate distances, with/without frequency weighting) to quantify individual contribution to performance gains rather than relying on combined results.

2. **Distance approximation accuracy**: Measure the correlation between exact shortest path distances and the proposed approximate distances using center triple head/tail distances across different graph types to validate the computational shortcut's validity.

3. **Structural difficulty correlation**: Empirically measure the relationship between shortest path distance and triple difficulty (e.g., using embedding similarity or ranking performance) on held-out validation sets to validate the core assumption behind proximity-aware contrastive learning.