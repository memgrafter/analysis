---
ver: rpa2
title: Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval
arxiv_id: '2403.18405'
source_url: https://arxiv.org/abs/2403.18405
tags:
- legal
- relevance
- case
- facts
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of determining legal case relevance
  in legal case retrieval, which requires domain expertise and nuanced reasoning over
  lengthy texts. The authors propose an automated method leveraging a general LLM
  (GPT-3.5) to generate expert-aligned, interpretable relevance judgments.
---

# Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval

## Quick Facts
- arXiv ID: 2403.18405
- Source URL: https://arxiv.org/abs/2403.18405
- Reference count: 32
- Primary result: Automated LLM-based method achieves Cohen's Kappa up to 0.69 for Legal Facts judgments, improving legal case retrieval performance

## Executive Summary
This paper addresses the challenge of determining legal case relevance in legal case retrieval, which requires domain expertise and nuanced reasoning over lengthy texts. The authors propose an automated method leveraging a general LLM (GPT-3.5) to generate expert-aligned, interpretable relevance judgments. The approach decomposes the judgment process into staged fact extraction and annotation tasks, using adaptive demonstration matching and expert-curated examples to guide the LLM. Empirical results show high consistency with human experts and demonstrate practical utility for legal case retrieval systems.

## Method Summary
The method decomposes legal case relevance judgment into Fact Extraction (FE) and Fact Annotation (FA) stages. It uses expert-curated demonstration sets for 40 query cases covering main case types, with Adaptive Demo-Matching (ADM) via BM25 retrieval to select case-specific demonstrations. The workflow extracts Material Facts (MF) and Legal Facts (LF) sequentially, then annotates their relevance to generate synthetic data with interpretable reasoning. This synthetic data is used for knowledge distillation to train smaller 7B-scale LLMs that match or outperform GPT-3.5 in judgment accuracy.

## Key Results
- Cohen's Kappa consistency scores up to 0.69 for Legal Facts judgments with human experts
- Generated synthetic data improves retrieval model performance (NDCG@30)
- Knowledge distillation to 7B-scale LLMs achieves comparable or better judgment accuracy than GPT-3.5
- High consistency between multiple GPT-3.5 judgments validates the baseline LLM reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing the relevance judgment into fine-grained sub-processes (Fact Extraction then Fact Annotation) improves accuracy over end-to-end judgment
- Mechanism: Breaking down the complex task allows LLMs to focus on one reasoning aspect at a time, reducing cognitive load and enabling step-by-step expert reasoning
- Core assumption: Legal relevance requires sequential reasoning (Material Facts first, then Legal Facts) and cannot be effectively captured in a single holistic judgment
- Evidence anchors:
  - [abstract]: "The proposed approach decomposes the judgment process into several stages, mimicking the workflow of human annotators"
  - [section]: "Formally, for a given stage ùë° ‚àà { FE, FA} and fact type ùëì ‚àà {LF, MF}, the expert-curated demonstration sets is denoted as D ùëìùë°"
- Break condition: If the sequential reasoning order is violated or if the decomposed tasks overlap too much, the accuracy gains disappear

### Mechanism 2
- Claim: Adaptive Demo-Matching (ADM) improves judgment quality by retrieving case-specific expert demonstrations
- Mechanism: BM25-based retrieval of relevant demonstrations from expert-curated sets provides context-specific guidance that aligns LLM reasoning with expert standards
- Core assumption: Not all demonstrations are equally useful for all cases; case-specific demonstrations provide better reasoning guidance
- Evidence anchors:
  - [section]: "we use a retrieval function ùëÖ(¬∑) to adaptively retrieve case-specific demonstrations"
  - [section]: "Adaptive selection of demonstrations for few-shot in-context learning ensures better alignment between the input and expert instruction"
- Break condition: If demonstration sets are too small or too generic, ADM provides minimal benefit over random selection

### Mechanism 3
- Claim: Knowledge distillation from large LLMs to smaller models preserves judgment quality while reducing computational cost
- Mechanism: Training smaller 7B-scale models on synthetic data with both labels and interpretable reasoning transfers expertise from GPT-3.5 to more efficient models
- Core assumption: The reasoning patterns captured in synthetic data can be effectively learned by smaller models
- Evidence anchors:
  - [abstract]: "knowledge distillation transfers expertise to smaller 7B-scale LLMs, achieving comparable or better judgment accuracy"
  - [section]: "by using synthetic data with both labels and interpretable reasoning generated during the judgment process, we trained two 7B-scale LLMs that match or even outperform GPT-3.5"
- Break condition: If the synthetic data quality is poor or the reasoning traces are insufficient, smaller models cannot effectively learn the expertise

## Foundational Learning

- Concept: Legal case relevance judgment criteria (Material Facts vs Legal Facts)
  - Why needed here: The paper's approach explicitly separates these two fact types with different relevance scoring weights (1 for MF, 2 for LF)
  - Quick check question: What are the key differences between Material Facts and Legal Facts in legal case retrieval, and why does the paper weight Legal Facts more heavily?

- Concept: Few-shot learning with in-context demonstrations
  - Why needed here: The method relies on adaptive retrieval of demonstrations to guide LLM reasoning without extensive fine-tuning
  - Quick check question: How does adaptive demo-matching differ from traditional few-shot prompting, and what retrieval method is used to select demonstrations?

- Concept: Knowledge distillation and synthetic data generation
  - Why needed here: The approach transfers expertise from GPT-3.5 to smaller models using synthetic data with explanations
  - Quick check question: What are the advantages of using synthetic data with reasoning traces versus just labels for knowledge distillation?

## Architecture Onboarding

- Component map:
  - Expert-curated demonstration sets (4 sets: MF/FE, LF/FE, MF/FA, LF/FA)
  - Adaptive Demo-Matching module (BM25-based retrieval)
  - Fact Extraction pipeline (Material Facts ‚Üí Legal Facts)
  - Fact Annotation pipeline (relevance scoring based on extracted facts)
  - Knowledge distillation module (synthetic data generation and smaller model training)

- Critical path:
  1. Retrieve case-specific demonstrations via ADM
  2. Extract Material Facts using FE with demonstrations
  3. Extract Legal Facts using FE with MF as input
  4. Annotate MF relevance using FA with demonstrations
  5. Annotate LF relevance using FA with demonstrations
  6. Combine scores (MF score √ó 1 + LF score √ó 2)

- Design tradeoffs:
  - Using general LLMs vs domain-specific models: General LLMs have stronger reasoning but require more prompting structure
  - Fact Extraction depth vs speed: More detailed extraction improves accuracy but increases computational cost
  - Demonstration set size vs quality: Larger sets provide more coverage but may include less relevant examples

- Failure signatures:
  - Poor kappa scores indicate breakdown in expert alignment
  - Inconsistent MF/LF extraction suggests issues with demonstration quality or prompt structure
  - Knowledge distillation failure manifests as smaller models underperforming on both MF and LF tasks

- First 3 experiments:
  1. Test kappa consistency between multiple GPT-3.5 judgments at different temperatures to establish baseline reliability
  2. Compare kappa scores with and without Adaptive Demo-Matching to measure its impact
  3. Evaluate smaller LLM performance before and after knowledge distillation training to quantify transfer effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method scale with different model sizes of LLMs, particularly when using smaller models with similar instruction-tuning approaches?
- Basis in paper: [explicit] The paper mentions that the method transfers expertise to smaller 7B-scale LLMs and compares performance with GPT-3.5, but doesn't explore the full range of model sizes or optimal scaling strategies.
- Why unresolved: The experiments focus on specific 7B models (Llama-2 and Qwen-2) without systematic exploration of smaller or larger model variants or comprehensive ablation studies on model capacity.
- What evidence would resolve it: Systematic experiments comparing different model sizes (e.g., 1B, 3B, 7B, 13B) using the same workflow, analyzing performance trends and identifying optimal model sizes for different stages of the pipeline.

### Open Question 2
- Question: What is the optimal strategy for demonstration selection and retrieval in the Adaptive Demo-Matching component, beyond the current BM25 approach?
- Basis in paper: [explicit] The paper uses BM25 for demonstration retrieval but notes that "adaptive selection of demonstrations for few-shot in-context learning ensures better alignment" without exploring alternative retrieval strategies.
- Why unresolved: The current implementation uses a simple BM25 approach without exploring more sophisticated retrieval methods like semantic similarity, learned retrievers, or hybrid approaches that might improve demonstration relevance.
- What evidence would resolve it: Comparative experiments testing different retrieval methods (semantic search, learned retrievers, hybrid approaches) against BM25, measuring their impact on final judgment quality and identifying optimal demonstration selection strategies.

### Open Question 3
- Question: How robust is the method to variations in legal systems and languages, and what are the minimum requirements for adapting it to new legal domains?
- Basis in paper: [explicit] The paper states it focuses on Chinese law due to expert availability and mentions potential generalization to other countries' legal cases, but only provides one additional experiment on CAIL2019 without systematic cross-jurisdictional evaluation.
- Why unresolved: The current experiments are limited to Chinese legal cases, with only one brief exploration of civil cases, leaving questions about adaptability to common law systems, different languages, and varying legal document structures.
- What evidence would resolve it: Systematic evaluation across multiple legal systems (e.g., US, UK, Germany), languages, and case types, documenting the adaptation process, required expertise, and performance variations to establish generalizability bounds and adaptation guidelines.

## Limitations
- Method's effectiveness highly dependent on quality and coverage of expert-curated demonstrations
- Moderate kappa scores (up to 0.69) indicate only substantial rather than near-perfect expert alignment
- Current experiments limited to Chinese legal cases with only brief exploration of civil cases

## Confidence
- **High confidence**: The decomposition mechanism and knowledge distillation process are well-established techniques with clear implementation paths
- **Medium confidence**: The Adaptive Demo-Matching improvement is supported by the paper but would benefit from ablation studies comparing against random demonstration selection
- **Medium confidence**: The retrieval performance gains (NDCG@30) are demonstrated but could be affected by dataset-specific characteristics

## Next Checks
1. Conduct an ablation study testing kappa consistency with and without Adaptive Demo-Matching across different case types to quantify its contribution
2. Test the knowledge distillation transfer by evaluating smaller models on cases from completely different legal domains than those in the demonstration sets
3. Implement temperature sensitivity analysis for GPT-3.5 judgments to establish the reliability and consistency of the baseline LLM judgments before demonstration matching