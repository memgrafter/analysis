---
ver: rpa2
title: Can Self Supervision Rejuvenate Similarity-Based Link Prediction?
arxiv_id: '2410.19183'
source_url: https://arxiv.org/abs/2410.19183
tags:
- node
- graph
- learning
- similarity-based
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the challenge of link prediction in unsupervised
  scenarios where traditional similarity-based methods struggle due to limited node
  feature information. The authors propose a novel approach, 3SLP, that integrates
  self-supervised graph learning techniques into similarity-based link prediction.
---

# Can Self Supervision Rejuvenate Similarity-Based Link Prediction?

## Quick Facts
- arXiv ID: 2410.19183
- Source URL: https://arxiv.org/abs/2410.19183
- Authors: Chenhan Zhang; Weiqi Wang; Zhiyi Tian; James Jianqiao Yu; Mohamed Ali Kaafar; An Liu; Shui Yu
- Reference count: 40
- Key outcome: 3SLP achieves up to 21.2% improvement in AUC over traditional similarity-based methods by integrating self-supervised graph learning into link prediction.

## Executive Summary
This paper addresses the challenge of link prediction in unsupervised scenarios where traditional similarity-based methods struggle due to limited node feature information. The authors propose 3SLP, a novel approach that integrates self-supervised graph learning techniques into similarity-based link prediction. The core innovation is a dual-view contrastive node representation learning (DCNRL) method that develops more informative node representations through crafted data augmentation and cross-scale contrasting, significantly outperforming traditional methods on benchmark datasets.

## Method Summary
3SLP operates on edgeless attributed graphs by first initializing a graph structure from node attributes using cosine similarity and top-k nearest neighbors. Edge diffusion with different teleportation probabilities creates two augmented views of the graph. Two GNN encoders process these views to generate node-level and graph-level representations, which are contrasted across views and scales to learn comprehensive node embeddings. These learned representations replace raw node attributes in a pairwise similarity-based clustering (PSC) backbone for link prediction. The method is trained using a contrastive objective with negative samples generated through attribute shuffling.

## Key Results
- 3SLP achieves up to 21.2% improvement in AUC over traditional similarity-based methods on benchmark datasets
- The method shows significant performance gains in cold-start scenarios where traditional methods fail
- 3SLP demonstrates promising results in knowledge transfer tasks, highlighting its practical utility
- Performance is positively correlated with graph attribute homophily, with better results on homophilic graphs like Citeseer

## Why This Works (Mechanism)

### Mechanism 1
Self-supervised learning with dual-view contrastive learning generates node representations that outperform raw node attributes in similarity-based link prediction. By initializing a graph structure from node attributes and applying edge diffusion with different teleportation probabilities, the method creates two augmented views. A GNN encoder processes each view to generate node-level and graph-level representations, which are contrasted across views and scales. This contrastive objective encourages the model to learn features capturing both local and global graph structure, leading to more informative node embeddings than raw attributes.

### Mechanism 2
Cross-scale contrasting between node-level and graph-level representations enhances the quality of learned node embeddings for link prediction. After generating node-level representations for each view, graph-level representations are created by pooling node embeddings. The contrastive objective then maximizes mutual information between node-level representations from one view and graph-level representations from the other view, and vice versa. This forces the model to learn representations that are globally consistent while preserving local node-specific information.

### Mechanism 3
The method's performance is correlated with graph attribute homophily, making it particularly effective for homophilic graphs. The initialization method uses cosine similarity between node attributes to create edges, which inherently assumes that similar nodes are more likely to be connected. This homophily assumption is reinforced throughout the learning process, leading to better performance on graphs where attribute similarity correlates with link formation.

## Foundational Learning

- Concept: Self-supervised learning (SSL) on graphs
  - Why needed here: The method operates in unsupervised scenarios without link labels, requiring a way to learn useful representations from node attributes alone.
  - Quick check question: What are the key components of a typical self-supervised learning framework on graphs?

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: The method uses GNN encoders to process the augmented graph views and generate node representations, relying on message passing to aggregate neighborhood information.
  - Quick check question: How does a single-layer GCN aggregate information from a node's neighbors?

- Concept: Contrastive learning objectives
  - Why needed here: The method uses contrastive learning to maximize mutual information between positive views while minimizing it between positive and negative views, requiring understanding of how these objectives work.
  - Quick check question: What is the difference between instance-level and view-level contrastive learning?

## Architecture Onboarding

- Component map: Graph initialization -> Edge diffusion -> GNN encoding -> Cross-scale contrasting -> Final node representations -> PSC clustering

- Critical path: Graph initialization → Edge diffusion → GNN encoding → Cross-scale contrasting → Final node representations → PSC clustering

- Design tradeoffs:
  - Using two separate GNN encoders vs. one encoder with different inputs
  - Different teleportation probabilities for views vs. same probability
  - Cross-scale contrasting vs. only node-level contrasting

- Failure signatures:
  - Poor performance on heterophilic graphs
  - Sensitive to initialization parameter k
  - May underperform on very small graphs

- First 3 experiments:
  1. Compare 3SLP with baseline PSC-NA on a homophilic dataset (e.g., Cora) to verify the core improvement claim
  2. Test different values of k in the graph initialization to find optimal parameter setting
  3. Compare performance with different teleportation probabilities (α1, α2) to understand their impact

## Open Questions the Paper Calls Out

### Open Question 1
How can the proposed method be extended to handle heterophilic graphs effectively? The paper identifies this as a limitation, noting that the method follows similarity-based assumptions that may not hold for heterophilic graphs, but does not provide concrete solutions.

### Open Question 2
What is the optimal teleportation probability (α) for edge diffusion in the data augmentation phase? While the paper shows that performance improves with larger differences between α1 and α2, it does not specify the best values for these parameters.

### Open Question 3
How does the proposed method perform on graphs with varying levels of homophily? The paper only compares performance on a few datasets with different homophily levels but lacks comprehensive analysis across a wide range of homophily levels.

## Limitations
- The method's performance is significantly limited on heterophilic graphs due to its reliance on attribute homophily assumptions
- Key hyperparameters (k, α1, α2) are fixed without comprehensive sensitivity analysis to determine optimal settings
- Claims about cold-start performance and knowledge transfer are mentioned but not rigorously evaluated with appropriate experimental designs

## Confidence

- **High Confidence**: The core mechanism of using self-supervised learning to generate node representations that outperform raw attributes is well-supported by experimental results, particularly the 21.2% AUC improvement over traditional PSC methods.
- **Medium Confidence**: The cross-scale contrasting mechanism's contribution is plausible but not conclusively isolated from other factors like the quality of the initial representations or the specific choice of diffusion parameters.
- **Low Confidence**: Claims about cold-start performance and knowledge transfer are mentioned but not rigorously evaluated with appropriate experimental designs that would isolate these scenarios.

## Next Checks

1. **Heterophily Stress Test**: Evaluate 3SLP on a clearly heterophilic dataset (e.g., with low AAC/DAC) to determine if performance degrades as expected and by how much.

2. **Parameter Sensitivity Analysis**: Systematically vary k, α1, and α2 across reasonable ranges to identify the most robust parameter settings and understand their impact on performance.

3. **Ablation on Contrasting Mechanism**: Remove cross-scale contrasting while keeping other components constant to quantify its specific contribution to the overall performance improvement.