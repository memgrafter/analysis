---
ver: rpa2
title: Learning from Noisy Labels via Self-Taught On-the-Fly Meta Loss Rescaling
arxiv_id: '2412.12955'
source_url: https://arxiv.org/abs/2412.12955
tags:
- learning
- noisy
- data
- loss
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of training machine learning models
  on noisy labeled data, which is common in real-world datasets and particularly challenging
  in tasks like dialogue modeling. The authors propose STORM (Self-Taught On-the-fly
  Rescaling via Meta loss), a novel meta learning approach that dynamically reweights
  training samples without requiring clean validation data.
---

# Learning from Noisy Labels via Self-Taught On-the-Fly Meta Loss Rescaling

## Quick Facts
- arXiv ID: 2412.12955
- Source URL: https://arxiv.org/abs/2412.12955
- Reference count: 26
- Primary result: STORM improves dialogue modeling performance by over 2% absolute in joint goal accuracy without clean validation data

## Executive Summary
The paper introduces STORM, a meta-learning approach for training models on noisy labeled data without requiring clean validation sets. STORM dynamically reweights training samples using a self-taught rescaling function that leverages model-generated features like losses and prediction probabilities. Extensive experiments demonstrate that STORM consistently outperforms standard training on noisy data, particularly excelling in dialogue state tracking tasks with over 2% improvement in joint goal accuracy. The method is robust to class imbalance and different noise types while preventing overfitting through careful calibration.

## Method Summary
STORM implements a meta-learning framework that continuously updates a rescaling function during training to dynamically weight samples. The method uses features computed from the model's own outputs (losses and prediction probabilities) rather than requiring clean validation data. A neural network learns to rescale sample importance based on these features, with the rescaling function updated through an outer loop that minimizes validation loss computed from noisy training samples. The approach maintains minimal computational overhead while achieving complementary optimization between model training (inner loop) and rescaling function updates (outer loop).

## Key Results
- STORM achieves significant improvements over noisy training baselines, particularly in dialogue modeling (over 2% absolute JGA improvement)
- The method demonstrates robustness across different noise levels (10-40%) and various NLP tasks including classification and dialogue state tracking
- STORM effectively prevents overfitting while maintaining good calibration for clean samples
- Performance gains are consistent across datasets with both synthetic and real noise patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: STORM dynamically reweights training samples based on self-taught meta loss rescaling without clean validation data.
- Mechanism: Uses model's own signals (losses and prediction probabilities) to compute features that feed into a learned rescaling function that adjusts sample importance in real-time.
- Core assumption: Model's internal signals can reliably distinguish noisy from clean samples as training progresses.
- Evidence anchors:
  - [abstract]: "We rely only on features provided by the model being trained, to learn a rescaling function in real time without knowledge of the true clean data distribution."
  - [section]: "We use features based on sample losses and prediction probabilities instead of sample gradients, reducing computational complexity."

### Mechanism 2
- Claim: STORM prevents overfitting by maintaining good calibration for clean samples while downscaling noisy ones.
- Mechanism: Continuously updates rescaling function based on validation samples drawn from noisy training set, ensuring clean samples retain higher weights while noisy samples are progressively downweighted.
- Core assumption: Noisy validation data, when properly rescaled, provides useful signal for distinguishing between clean and noisy samples.
- Evidence anchors:
  - [abstract]: "Our strategy is robust in the face of noisy and clean data, handles class imbalance, and prevents overfitting to noisy labels."
  - [section]: "As training progresses, the impact of correctly labeled data is scaled up, while the impact of wrongly labeled data is suppressed."

### Mechanism 3
- Claim: STORM achieves complementary goals through inner and outer loop updates in meta learning.
- Mechanism: Inner loop trains model to focus on more relevant samples within mini-batch, while outer loop updates rescaling function to minimize validation loss, creating feedback loop that refines weight predictions.
- Core assumption: Joint optimization of model and rescaling function leads to improved performance by balancing needs of both loops.
- Evidence anchors:
  - [section]: "The inner and outer loop updates pursue complementary goals, creating a feedback loop that refines and improves weight predictions and model performance."

## Foundational Learning

- Concept: Meta Learning
  - Why needed here: Enables continuous updating of rescaling function during training without clean validation data, allowing on-the-fly adaptation to noisy labels.
  - Quick check question: How does meta learning differ from traditional learning in terms of handling noisy labels?

- Concept: Sample Reweighting
  - Why needed here: Dynamically adjusting sample weights during training reduces negative impact of noisy labels without permanently filtering potentially useful data.
  - Quick check question: What are advantages of reweighting samples compared to filtering them out entirely?

- Concept: Calibration
  - Why needed here: Maintaining good calibration for clean samples while downscaling noisy ones is crucial for preventing overfitting and ensuring reliable model performance.
  - Quick check question: How does calibration help in distinguishing between clean and noisy samples during training?

## Architecture Onboarding

- Component map:
  - Model Θ -> Rescaling Function Ω -> Feature Computation Pipeline -> Loss Computation -> Backward Pass
  - Model provides losses and prediction probabilities as features
  - Rescaling function applies learned weights to samples
  - Features include sample losses, prediction probabilities, KL divergence, overlap coefficient

- Critical path: Model forward pass → Feature computation → Rescaling function application → Loss computation → Backward pass for both model and rescaling function

- Design tradeoffs:
  - Using model signals for feature computation reduces computational overhead but may introduce bias if initial predictions are unreliable
  - Sampling validation data from noisy training set eliminates need for clean data but may complicate distinction between clean and noisy samples

- Failure signatures:
  - Rescaling function consistently assigns near-zero weights to all samples indicates model cannot differentiate clean from noisy
  - No performance improvement despite rescaling suggests insufficient feature set or unbalanced meta learning setup

- First 3 experiments:
  1. Implement STORM on synthetic dataset with known noise patterns to verify rescaling function correctly identifies and downweights noisy samples
  2. Compare STORM performance against baseline using clean validation data to assess impact of sampling from noisy training set
  3. Conduct ablation study to determine contribution of each feature type (losses, probabilities, etc.) to rescaling function effectiveness

## Open Questions the Paper Calls Out

- Question: How does STORM's performance scale with different noise patterns beyond random label noise, such as systematic or structured noise?
  - Basis in paper: [inferred] Paper tests on uniform noise and mentions "different types of noise" but doesn't extensively explore structured noise patterns.
  - Why unresolved: Experiments focus on random noise and real noise from dialogue datasets, not systematic noise structures.
  - What evidence would resolve it: Experiments testing STORM on datasets with known systematic noise patterns (label flipping based on semantic similarity, hierarchical noise structures).

- Question: What is theoretical limit of STORM's effectiveness when clean data is extremely rare (e.g., <5% of training data)?
  - Basis in paper: [inferred] Paper states STORM is "robust in the face of noisy and clean data" but doesn't explore extreme imbalance scenarios.
  - Why unresolved: Experiments use datasets with 10-40% noise, not scenarios where clean data is tiny minority.
  - What evidence would resolve it: Experiments with progressively decreasing amounts of clean data (1%, 2%, 5%, etc.) to identify breaking point.

- Question: How does STORM's computational overhead scale with model size and dataset dimensionality?
  - Basis in paper: [explicit] Paper mentions "minimal computational overhead" and discusses complexity but doesn't provide scaling analysis.
  - Why unresolved: Paper only provides general complexity analysis without empirical scaling studies.
  - What evidence would resolve it: Systematic experiments varying model size, feature dimensionality, and dataset size to measure runtime and memory scaling.

## Limitations

- STORM's effectiveness heavily depends on initial model's ability to partially separate clean from noisy data, which may not hold for datasets with severe noise or complex label distributions
- The method's theoretical guarantees for when self-taught approach will succeed remain unclear
- Performance in extreme class imbalance scenarios (clean data <5% of training data) has not been thoroughly explored

## Confidence

- High confidence: STORM's ability to improve over baseline noisy training (proven across multiple datasets)
- Medium confidence: The mechanism of using model features for rescaling (empirically validated but theoretically underspecified)
- Medium confidence: Claims about preventing overfitting (supported by experiments but limited ablation studies)

## Next Checks

1. **Noise Threshold Analysis**: Systematically vary noise levels (10%-50%) to identify the point where STORM's performance degrades, establishing the method's operational boundaries.

2. **Feature Ablation Stress Test**: Remove individual features (losses, probabilities, KL divergence) to quantify their marginal contribution to noise detection, particularly in datasets with different noise characteristics.

3. **Generalization Benchmark**: Apply STORM to a held-out NLP dataset with known noise characteristics (e.g., crowd-sourced labels with systematic biases) to test whether the method generalizes beyond the evaluation datasets.