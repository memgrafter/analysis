---
ver: rpa2
title: 'The Cognitive Revolution in Interpretability: From Explaining Behavior to
  Interpreting Representations and Algorithms'
arxiv_id: '2408.05859'
source_url: https://arxiv.org/abs/2408.05859
tags:
- learning
- arxiv
- representations
- interpretation
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a taxonomy for mechanistic interpretability
  research, distinguishing between semantic interpretation (what latent representations
  are learned) and algorithmic interpretation (what operations are performed over
  representations). The authors argue that current interpretability methods are ripe
  to facilitate a transition in deep learning interpretation echoing the "cognitive
  revolution" in 20th-century psychology.
---

# The Cognitive Revolution in Interpretability: From Explaining Behavior to Interpreting Representations and Algorithms

## Quick Facts
- arXiv ID: 2408.05859
- Source URL: https://arxiv.org/abs/2408.05859
- Authors: Adam Davies; Ashkan Khakzar
- Reference count: 40
- One-line primary result: Proposes a taxonomy distinguishing between semantic interpretation (what latent representations are learned) and algorithmic interpretation (what operations are performed over representations)

## Executive Summary
This paper proposes a novel taxonomy for mechanistic interpretability research that distinguishes between semantic interpretation (understanding what latent representations are learned) and algorithmic interpretation (understanding what operations are performed over representations). The authors argue that current interpretability methods are positioned to facilitate a transition in deep learning interpretation that echoes the "cognitive revolution" in 20th-century psychology. By grounding this work in parallels from cognitive science history, particularly Marr's levels of analysis, the paper provides a framework for understanding how mechanistic interpretability fits into broader scientific analysis of intelligent systems.

## Method Summary
The paper employs theoretical analysis and literature review to develop its taxonomy framework, drawing parallels between cognitive science history and current interpretability challenges. Rather than using specific datasets or models, the method involves analyzing existing interpretability approaches through the lens of the proposed semantic-algorithmic distinction, identifying key challenges in each category, and discussing how these approaches relate to Marr's computational, algorithmic, and implementation levels. The authors systematically outline challenges including the "needle in a haystack" problem for neuron-level analysis, correlation vs. causation issues in probing, and the Rashomon effect in circuit discovery.

## Key Results
- Establishes a taxonomy distinguishing semantic interpretation (latent representations) from algorithmic interpretation (operations/algorithms)
- Identifies three major challenges in mechanistic interpretability: needle-in-haystack problem, correlation vs. causation, and Rashomon effect
- Proposes that current methods are positioned to enable a cognitive revolution-style transition in deep learning interpretation

## Why This Works (Mechanism)

### Mechanism 1
The paper's categorization into semantic and algorithmic interpretation provides a framework that mirrors Marr's levels of analysis, enabling researchers to systematically address different aspects of model interpretability. By distinguishing between what latent representations are learned (semantic interpretation) and what operations are performed over representations (algorithmic interpretation), the framework creates a structured approach to mechanistic interpretability that parallels established cognitive science methodologies.

### Mechanism 2
The paper's grounding in cognitive science history provides valuable context and lessons that can inform current mechanistic interpretability research. By drawing parallels between the behaviorist-to-cognitive revolution in psychology and current shifts in deep learning interpretation paradigms, the paper offers historical perspective that helps researchers understand the significance and trajectory of their work.

### Mechanism 3
The paper's identification of key challenges in both semantic and algorithmic interpretation provides a roadmap for addressing fundamental obstacles in mechanistic interpretability research. By systematically outlining challenges like the "needle in a haystack" problem for neuron-level analysis, the correlation vs. causation problem in probing, and the Rashomon effect in circuit discovery, the paper helps researchers understand what needs to be solved.

## Foundational Learning

- Concept: Marr's levels of analysis (computational theory, representations/algorithms, implementation)
  - Why needed here: The paper uses Marr's framework as a foundation for understanding how mechanistic interpretability fits into broader scientific analysis of intelligent systems.
  - Quick check question: Can you explain how Marr's levels of analysis map to the semantic and algorithmic interpretation categories proposed in this paper?

- Concept: The behaviorist-to-cognitive revolution in psychology
  - Why needed here: The paper draws parallels between this historical shift and current changes in deep learning interpretation paradigms.
  - Quick check question: What were the key limitations of behaviorism that led to the cognitive revolution, and how might these parallel current limitations in deep learning interpretability?

- Concept: The Rashomon effect in interpretability
  - Why needed here: The paper identifies this as a key challenge where multiple equally valid explanations may exist for the same model behavior.
  - Quick check question: How does the Rashomon effect complicate efforts to determine which circuit or feature interpretation is "correct" when multiple explanations have equal causal efficacy?

## Architecture Onboarding

- Component map: Taxonomy of interpretability: Semantic interpretation (latent representations) vs. Algorithmic interpretation (operations/algorithms) -> Historical grounding: Cognitive science parallels and Marr's levels of analysis -> Challenge analysis: Systematic identification of obstacles in each interpretability category -> Research roadmap: Suggestions for unifying semantic and algorithmic approaches
- Critical path: Understanding the taxonomy → Appreciating historical context → Recognizing challenges → Exploring research directions
- Design tradeoffs: The paper prioritizes conceptual clarity and historical grounding over technical depth, which makes it accessible but may leave some readers wanting more concrete implementation details.
- Failure signatures: The framework might fail if researchers find the semantic-algorithmic distinction artificial, if historical parallels prove superficial, or if identified challenges prove intractable.
- First 3 experiments:
  1. Apply the taxonomy to categorize existing interpretability papers and assess whether the framework captures meaningful distinctions.
  2. Identify a specific interpretability challenge (e.g., Rashomon effect) and design an experiment to test potential mitigation strategies.
  3. Attempt to apply Marr's levels of analysis to a specific mechanistic interpretability study and evaluate whether the framework provides useful insights.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the appropriate level of representation (neurons, attention heads, MLPs, etc.) for algorithmic interpretation in large language models?
- Basis in paper: [explicit] The paper discusses how circuit discovery work typically focuses on attention heads but acknowledges this choice is arbitrary and may miss important sub-circuits in other parts of the model architecture.
- Why unresolved: The paper highlights that different architectural components may implement different operations and algorithms, but there is no principled framework for determining which level is most appropriate for different types of interpretability questions.
- What evidence would resolve it: Empirical studies comparing circuit discovery results across different architectural levels, demonstrating which levels capture the most meaningful algorithmic operations for explaining model behavior.

### Open Question 2
- Question: How can we automatically evaluate the quality and correctness of feature interpretations discovered through dictionary learning methods?
- Basis in paper: [explicit] The paper discusses the challenge of interpreting millions of features discovered by sparse autoencoders and the need for automated evaluation, noting that current approaches rely on causal interventions but cannot determine if these are complete or selective.
- Why unresolved: While the paper mentions using causal interventions to test feature interpretations, it acknowledges this only shows whether a feature contributes to behavior, not whether other features contribute equally or whether interventions are comprehensive.
- What evidence would resolve it: Development of quantitative metrics for feature interpretation quality that go beyond causal intervention tests, potentially incorporating redundancy analysis, completeness measures, or alternative validation approaches.

### Open Question 3
- Question: Can we develop a unified framework that simultaneously accounts for both semantic and algorithmic interpretation while scaling to real-world foundation models?
- Basis in paper: [explicit] The paper discusses recent efforts to unify these perspectives through causal abstraction frameworks but notes these approaches either only work on small toy models or only intervene in single layers, limiting their ability to describe multi-layer operations.
- Why unresolved: Current theoretical frameworks lack empirical validation on large-scale models, and existing empirical approaches cannot capture the full complexity of how representations are transformed across multiple layers through various operations.
- What evidence would resolve it: A demonstrated causal abstraction framework that successfully explains complex behaviors in multi-billion parameter models by capturing both the representations at each layer and the operations that transform them, validated through intervention experiments.

## Limitations

- The proposed semantic-algorithmic distinction may not capture all important dimensions of interpretability research, potentially oversimplifying a complex landscape.
- Historical parallels drawn from cognitive science may not translate perfectly to modern deep learning contexts where fundamental differences in system architecture exist.
- The taxonomy appears primarily conceptual without extensive empirical validation showing how well it classifies existing research or guides new investigations.

## Confidence

**High Confidence:**
- Identification of specific challenges in interpretability research (needle-in-haystack problem, correlation vs. causation issues, Rashomon effect) is well-grounded in existing literature.

**Medium Confidence:**
- The semantic-algorithmic distinction provides a useful organizing framework for understanding different approaches to mechanistic interpretability.
- Historical parallels to cognitive science revolutions offer valuable context for understanding current shifts in interpretability research.

**Low Confidence:**
- The claim that current methods are "ripe" to facilitate a transition analogous to the cognitive revolution may be overstated.
- The assertion that the proposed framework provides a clear path forward for deep learning interpretation is more aspirational than demonstrated.

## Next Checks

1. **Taxonomy Validation Study**: Systematically apply the semantic-algorithmic taxonomy to a large sample of existing interpretability papers (minimum 50) to assess classification consistency and whether the framework captures meaningful distinctions that researchers find useful for organizing their work.

2. **Historical Parallel Stress Test**: Design a comparison study that explicitly tests which aspects of cognitive science history (e.g., Marr's levels, behaviorist-to-cognitive shift) actually provide actionable insights for current interpretability research, versus which parallels are superficial.

3. **Challenge Mitigation Experiment**: Select one identified challenge (e.g., the Rashomon effect) and implement multiple proposed mitigation strategies on a concrete model (such as GPT-2 small), then evaluate which approaches successfully narrow down to the most causally relevant explanations.