---
ver: rpa2
title: 'KMMLU: Measuring Massive Multitask Language Understanding in Korean'
arxiv_id: '2402.11548'
source_url: https://arxiv.org/abs/2402.11548
tags:
- korean
- arxiv
- language
- engineering
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KMMLU, a Korean benchmark with 35,030 expert-level
  multiple-choice questions across 45 subjects. Unlike previous benchmarks translated
  from English, KMMLU is collected from original Korean exams, capturing linguistic
  and cultural nuances.
---

# KMMLU: Measuring Massive Multitask Language Understanding in Korean

## Quick Facts
- arXiv ID: 2402.11548
- Source URL: https://arxiv.org/abs/2402.11548
- Reference count: 40
- Best public model score: 50.5%

## Executive Summary
KMMLU is a novel Korean benchmark consisting of 35,030 expert-level multiple-choice questions across 45 subjects, designed to evaluate massive multitask language understanding in Korean. Unlike previous benchmarks that relied on translated English questions, KMMLU is constructed from original Korean exams, capturing linguistic and cultural nuances specific to Korean education and professional contexts. The benchmark is evaluated on 27 public and proprietary large language models, revealing that even the most capable models struggle with expert-level Korean questions, with GPT-4 and HyperCLOVA X scoring below 60%.

## Method Summary
The KMMLU benchmark is constructed from original Korean exam questions collected from various educational and professional sources, rather than translating English benchmarks. Questions span 45 subjects including language arts, mathematics, social studies, and technical fields. Each question is a multiple-choice format with four options. The benchmark undergoes quality control through manual review and validation. Evaluation is conducted using the EleutherAI Language Model Evaluation Harness, testing both public and proprietary models across the full question set.

## Key Results
- Best public model performance: 50.5% accuracy
- Top proprietary models (GPT-4, HyperCLOVA X): below 60% accuracy
- 27 models evaluated across 35,030 questions
- Benchmark covers 45 distinct subject areas

## Why This Works (Mechanism)
KMMLU captures genuine Korean language understanding by using original exam questions rather than translations, preserving cultural context and linguistic nuances. The expert-level questions test deep domain knowledge across diverse subjects, making it difficult for models to rely solely on surface-level language patterns or statistical correlations.

## Foundational Learning
- Korean linguistic structures and grammar: Essential for understanding question nuances and cultural context. Quick check: Verify question translations maintain original meaning.
- Expert domain knowledge across 45 subjects: Required to evaluate deep understanding beyond general language capabilities. Quick check: Cross-validate subject coverage with Korean curriculum standards.
- Multiple-choice question design principles: Important for ensuring question quality and avoiding ambiguity. Quick check: Review distractor quality and question clarity.

## Architecture Onboarding
Component map: Question Collection -> Quality Control -> Model Evaluation -> Results Analysis

Critical path: The benchmark construction process (question collection and quality control) directly impacts the validity of model evaluations. Any biases or quality issues in the question set will propagate to evaluation results.

Design tradeoffs: Original Korean questions vs. translated English questions - original questions capture cultural nuances but may have limited availability; multiple-choice format vs. open-ended - multiple-choice enables consistent evaluation but may not fully capture reasoning depth.

Failure signatures: Models scoring above 70% on translated benchmarks but below 60% on KMMLU indicate inability to handle authentic Korean linguistic and cultural contexts. Consistent performance gaps across subject areas suggest fundamental limitations in Korean language understanding.

First experiments:
1. Evaluate a subset of questions with human experts to establish ground truth baseline
2. Test model performance on questions from different subject categories to identify domain-specific weaknesses
3. Analyze error patterns to determine whether failures stem from language understanding or domain knowledge gaps

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark covers only a subset of possible Korean expert knowledge domains
- Quality control process may not eliminate all potential ambiguities
- Multiple-choice format may not fully capture language understanding depth
- No systematic validation of claimed cultural and linguistic uniqueness

## Confidence
High: Benchmark construction methodology is clearly presented; performance gaps between public and proprietary models are consistent with field trends
Medium: Claims about capturing unique linguistic/cultural nuances need more systematic validation; assertion about room for improvement is somewhat subjective
Low: No detailed analysis of potential biases in question selection or overlap with pretraining data

## Next Checks
1. Conduct inter-rater reliability tests on a subset of questions to quantify consistency in difficulty and quality assessments
2. Perform statistical analysis to check for potential overlap between benchmark questions and pretraining data of evaluated models
3. Expand the benchmark to include open-ended question formats to complement the multiple-choice format and provide a more comprehensive evaluation of language understanding capabilities