---
ver: rpa2
title: 'Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation
  Puzzles'
arxiv_id: '2410.06733'
source_url: https://arxiv.org/abs/2410.06733
tags:
- judge
- llms
- thinking
- answer
- puzzles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPLAT, a benchmark designed to evaluate the
  lateral thinking capabilities of Large Language Models (LLMs) using situation puzzles.
  The benchmark comprises 975 puzzles categorized into three difficulty levels and
  employs a multi-turn player-judge framework to assess model performance without
  requiring a stronger evaluation model.
---

# Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles

## Quick Facts
- **arXiv ID:** 2410.06733
- **Source URL:** https://arxiv.org/abs/2410.06733
- **Reference count:** 40
- **Primary result:** Introduces SPLAT benchmark to evaluate lateral thinking in LLMs using situation puzzles with 975 puzzles across three difficulty levels

## Executive Summary
This paper introduces SPLAT, a benchmark designed to evaluate the lateral thinking capabilities of Large Language Models (LLMs) using situation puzzles. The benchmark employs a multi-turn player-judge framework to assess model performance without requiring a stronger evaluation model. The results show that the evaluation model WizardLM-2 achieves over 80% agreement with human judgments across all difficulty levels, and applying SPLAT data to other lateral thinking benchmarks leads to performance improvements.

## Method Summary
The paper proposes a multi-turn player-judge framework where LLMs (as players) ask yes/no questions to an evaluation model (as judge) about incomplete stories to infer full scenarios. The SPLAT benchmark contains 975 situation puzzles categorized into three difficulty levels based on solving time, answer length, and complexity. The judge model evaluates whether player predictions align semantically with reference answers, reducing reliance on stronger evaluation models. The framework tracks accuracy and average rounds needed to solve puzzles.

## Key Results
- WizardLM-2 achieves over 80% agreement with human judgments across all difficulty levels
- LLMs show performance degradation on harder puzzles, with accuracy dropping significantly for complex scenarios
- Fine-tuning models on SPLAT data improves performance on other lateral thinking benchmarks (RiddleSense, BrainTeaser)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The multi-turn player-judge framework reduces reliance on a stronger evaluation model by delegating the evaluation task to a model that only needs to judge semantic alignment rather than absolute correctness.
- **Mechanism:** Instead of requiring the evaluation model to be more capable than the tested model, the framework uses a semantic alignment check. The judge model only needs to verify whether the player's predicted answer semantically matches the reference scenario, which is a simpler task than generating the answer.
- **Core assumption:** Semantic alignment checking is easier than answer generation and can be reliably performed by a model that is not necessarily stronger than the tested model.
- **Evidence anchors:**
  - [abstract] "This framework simulates an interactive game where the model (player) asks the evaluation model (judge) questions about an incomplete story to infer the full scenario."
  - [section] "Our approach only necessitates that the judge model determines if the predicted answer is semantically aligned with the reference answer."
- **Break condition:** If semantic alignment checking becomes too complex for the judge model or if the judge model has inherent biases toward certain answer formats, the framework's effectiveness would be compromised.

### Mechanism 2
- **Claim:** The interactive questioning process helps elicit lateral thinking by forcing models to engage in multi-step reasoning and creative problem-solving rather than relying on pattern matching.
- **Mechanism:** By requiring models to ask yes/no questions and iteratively refine their understanding, the framework encourages models to explore multiple angles and think creatively about the problem, rather than attempting to directly solve the puzzle in one step.
- **Core assumption:** Multi-turn interaction with constrained question types promotes deeper reasoning and prevents models from using shortcut solutions.
- **Evidence anchors:**
  - [abstract] "The player poses a set of questions Q = {q1, q2, . . . , qi, . . .} gradually for the judge to gather information about the unknown and detailed scenario/answer."
  - [section] "The player poses a set of questions Q = {q1, q2, . . . , qi, . . .} gradually for the judge to gather information about the unknown and detailed scenario/answer."
- **Break condition:** If models learn to game the system by asking strategically optimized questions rather than genuinely exploring the problem space, or if the question constraints limit creative exploration.

### Mechanism 3
- **Claim:** The difficulty-graded puzzle structure enables systematic evaluation of lateral thinking capabilities across different complexity levels.
- **Mechanism:** By categorizing puzzles into Easy, Medium, and Hard levels based on time to solve, length of reference answer, and subjective complexity, the framework allows for nuanced assessment of how lateral thinking capabilities scale with problem difficulty.
- **Core assumption:** Difficulty levels can be reliably categorized and that performance differences across these levels reflect genuine differences in lateral thinking ability rather than other factors.
- **Evidence anchors:**
  - [section] "Inspired by the organisation of GAIA [26], we categorise our collected situation puzzles into various distinct levels of difficulty, ranging from 1 (easiest) to 9 (hardest)."
  - [section] "We categorise our collected situation puzzles into various distinct levels of difficulty, ranging from 1 (easiest) to 9 (hardest)."
- **Break condition:** If the difficulty categorization is inconsistent or if models perform differently across difficulty levels due to factors unrelated to lateral thinking ability (such as memorization or pattern recognition).

## Foundational Learning

- **Concept:** Semantic alignment checking
  - Why needed here: The framework relies on judge models to verify whether player responses semantically match reference answers without requiring exact matches, which is crucial for evaluating open-ended creative responses.
  - Quick check question: Can you explain how semantic alignment differs from exact match evaluation and why it's more suitable for lateral thinking tasks?

- **Concept:** Multi-turn dialogue systems
  - Why needed here: The framework's effectiveness depends on understanding how models can engage in iterative questioning and reasoning over multiple turns, which is different from single-turn question answering.
  - Quick check question: What are the key differences between single-turn and multi-turn dialogue systems in terms of state tracking and context management?

- **Concept:** Lateral thinking principles
  - Why needed here: Understanding the distinction between vertical (logical) and lateral (creative) thinking is essential for designing appropriate evaluation metrics and interpreting results.
  - Quick check question: How would you differentiate between a puzzle that requires vertical thinking versus one that requires lateral thinking?

## Architecture Onboarding

- **Component map:** Player module -> Judge module -> Puzzle database -> Evaluation framework -> Annotation pipeline
- **Critical path:** 1. Initialize player and judge models with character guidelines 2. Present incomplete story to player 3. Player asks yes/no questions (up to max rounds) 4. Judge responds based on reference scenario 5. Player attempts final answer 6. Judge evaluates semantic alignment with reference 7. Record accuracy and round count 8. Aggregate results across puzzles and difficulty levels
- **Design tradeoffs:** Open-ended vs. multiple-choice: Open-ended allows for creative solutions but requires semantic evaluation; Number of rounds: More rounds allow deeper exploration but increase computational cost; Judge model selection: Stronger judges provide more reliable evaluation but may introduce bias; Difficulty categorization: Granular categories provide better analysis but require more annotation effort
- **Failure signatures:** Low accuracy across all difficulty levels: Indicates fundamental limitations in lateral thinking capabilities; High accuracy on easy but low on hard: Suggests inability to scale reasoning complexity; Consistently low rounds but low accuracy: Indicates poor exploration strategy; High variance in performance: May indicate instability or sensitivity to question phrasing
- **First 3 experiments:** 1. Baseline evaluation: Run all LLMs on SPLAT with default settings (R=1, max rounds=15) to establish performance baselines 2. Round sensitivity: Test with different max round limits (5, 10, 15, 20) to understand the impact of interaction depth 3. Judge model comparison: Evaluate the same player models using different judge models (WizardLM-2, Llama3-70B, human judges) to assess judge reliability and potential bias

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the multi-turn player-judge framework's performance scale with increasingly complex puzzles that require multiple layers of lateral thinking?
- **Basis in paper:** [explicit] The paper mentions that harder puzzles require a significant amount of time to unravel, with lengthy and complex scenarios that may include extensive details or embedded subtleties.
- **Why unresolved:** The paper only tests up to a max round of 15 and doesn't explore scenarios requiring significantly more rounds or layers of deduction.
- **What evidence would resolve it:** Testing the framework with puzzles requiring 30+ rounds and analyzing if the judge model maintains accuracy and if the player model can still derive correct answers.

### Open Question 2
- **Question:** Can the framework effectively evaluate models that generate highly creative but semantically divergent answers that still satisfy the puzzle constraints?
- **Basis in paper:** [inferred] The paper uses semantic alignment for evaluation but doesn't explicitly test if highly creative answers that satisfy constraints but differ from the reference are correctly identified.
- **Why unresolved:** The current evaluation only checks for semantic alignment with reference answers, not whether the framework can recognize valid alternative solutions.
- **What evidence would resolve it:** Creating puzzles with multiple valid solutions and testing if the framework recognizes all correct answers as valid, not just the reference one.

### Open Question 3
- **Question:** What is the impact of using different judge models with varying reasoning capabilities on the framework's ability to evaluate increasingly advanced LLMs?
- **Basis in paper:** [explicit] The paper uses WizardLM-2 as judge and shows it aligns well with human judgments, but doesn't explore the framework's limits with different judge model capabilities.
- **Why unresolved:** The experiments only use one relatively strong judge model and don't test the framework's robustness when judge and player models have similar or varying capabilities.
- **What evidence would resolve it:** Systematically testing the framework with judge models of different capabilities (weaker, comparable, and stronger than the player models) and analyzing how evaluation accuracy changes.

## Limitations

- Judge model reliability may vary across different puzzle types, potentially affecting evaluation fairness
- Difficulty categorization criteria may not be consistently applied across all puzzles
- Transferability of SPLAT data improvements to other benchmarks needs independent verification

## Confidence

- **High confidence:** Core framework design and basic evaluation methodology are well-specified and reproducible
- **Medium confidence:** Claim about reducing reliance on stronger evaluation models is supported but needs further validation
- **Low confidence:** Transferability claims regarding performance improvements on other benchmarks need independent verification

## Next Checks

1. **Judge model robustness test:** Evaluate the same player models using multiple judge models (including human judges) on a subset of SPLAT puzzles to quantify inter-judge agreement rates and identify systematic biases in evaluation.

2. **Difficulty boundary analysis:** Conduct controlled experiments where human experts independently categorize puzzles by difficulty and compare results with the original categorization to assess consistency and identify potential misclassifications.

3. **Transfer learning validation:** Test whether fine-tuning models on SPLAT data improves performance on completely different lateral thinking benchmarks not used in the original study, to verify that improvements reflect genuine capability enhancement rather than overfitting.