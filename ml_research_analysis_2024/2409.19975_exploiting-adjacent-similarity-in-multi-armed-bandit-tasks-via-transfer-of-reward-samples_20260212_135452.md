---
ver: rpa2
title: Exploiting Adjacent Similarity in Multi-Armed Bandit Tasks via Transfer of
  Reward Samples
arxiv_id: '2409.19975'
source_url: https://arxiv.org/abs/2409.19975
tags:
- tasks
- task
- reward
- algorithm
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies sequential multi-task stochastic multi-armed
  bandit problems where tasks are adjacently similar, meaning the difference in mean
  rewards between consecutive tasks is bounded. The authors propose two Upper Confidence
  Bound (UCB)-based algorithms to transfer reward samples from previous tasks to improve
  performance in the current task.
---

# Exploiting Adjacent Similarity in Multi-Armed Bandit Tasks via Transfer of Reward Samples

## Quick Facts
- arXiv ID: 2409.19975
- Source URL: https://arxiv.org/abs/2409.19975
- Reference count: 18
- Primary result: Two UCB-based algorithms transfer reward samples between adjacent-similar multi-armed bandit tasks, achieving lower regret than non-transfer baseline

## Executive Summary
This paper addresses sequential multi-task stochastic multi-armed bandit problems where consecutive tasks have similar reward distributions. The authors propose two algorithms that transfer reward samples from previous tasks to improve performance in the current task. Tr-UCB assumes the similarity parameter is known, while Tr-UCB2 estimates it from data. Both algorithms guarantee no negative transfer - performance never falls below the non-transfer baseline. Theoretical analysis proves regret reduction through transfer, and empirical results demonstrate significant improvements over baseline methods.

## Method Summary
The paper studies sequential multi-task stochastic multi-armed bandit problems with K arms across J tasks. Each task j has arm means μ_j^k bounded in [0,1], and tasks are adjacent-similar with |μ_j^k - μ_{j+1}^k| ≤ ε_k. Two algorithms are proposed: Tr-UCB transfers up to B_k = η^{-4ε_k^2/4ε_k^2} samples from the previous task using a conservative min-combination of UCB estimates, while Tr-UCB2 estimates ε_k through uniform arm sampling in an initial phase before applying similar transfer logic. Both guarantee no negative transfer by construction.

## Key Results
- Tr-UCB and Tr-UCB2 achieve significantly lower regret than NT-UCB and Naive-Transfer baselines
- Regret bounds explicitly capture performance improvement through transfer
- No-negative-transfer guarantee ensures algorithms never perform worse than non-transfer baseline
- Empirical results show greatest benefit when tasks are highly similar (small ε_k values)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Transferring bounded reward samples from preceding task reduces regret when tasks are adjacent-similar
- **Mechanism**: Uses auxiliary estimate (ˆµj_2k) from previous task rewards, combined with current-task UCB estimate via min operator to form conservative confidence bound
- **Core assumption**: Bounded similarity assumption (|µj_k - µj+1_k| ≤ εk) holds; controls transferred sample count via Bk
- **Evidence anchors**: [abstract] "transferring samples reduces the regret"; [section III-B] "samples from previous tasks contain information"
- **Break condition**: If εk is large (tasks dissimilar), Bk shrinks to near zero and transfer benefit vanishes

### Mechanism 2
- **Claim**: Two-phase approach estimates εk when unknown and applies conservative transfer logic
- **Mechanism**: Phase I uniform sampling collects sufficient samples; estimate ˆεj_k computed as maximum observed mean-difference bound; Phase II proceeds like Tr-UCB with dynamic Bk
- **Core assumption**: True εk is bounded and estimate ˆεj_k is conservative (biased high) due to max operator
- **Evidence anchors**: [section III-C] Definition of ˆεj_k and its computation; "This method is pessimistic (biased towards higher values of ˆεk)"
- **Break condition**: If δ too large or l too small, ˆεj_k may be underestimated, causing overly optimistic transfer

### Mechanism 3
- **Claim**: Conservative min-combination of UCB and auxiliary estimates ensures no negative transfer
- **Mechanism**: Taking min{ˆµj_1k + qj_1k, ˆµj_2k + qj_2k} never over-estimates true mean reward, reducing exploration bonus in proportion to similarity
- **Core assumption**: True mean reward lies within both confidence intervals; min operation is safe
- **Evidence anchors**: [section III-B] "Taking the minimum of the upper confidence bounds... leads to increased exploitation"; [theorem 1 proof] Shows regret bound never exceeds NT-UCB baseline
- **Break condition**: If auxiliary estimate's confidence width q2_k computed incorrectly, min could overly suppress exploration

## Foundational Learning

- **Concept**: Multi-armed bandit regret bounds (UCB analysis)
  - **Why needed here**: Algorithms extend standard UCB; understanding O(log n / ∆) exploration term essential to see how transfer changes ∆-related counts
  - **Quick check question**: In UCB, how does pulling a suboptimal arm k N_k times contribute to regret? (Answer: ∆k * N_k)

- **Concept**: Concentration inequalities (Hoeffding bounds)
  - **Why needed here**: Algorithm uses Hoeffding to bound differences between empirical means across tasks and compute confidence widths q1_k, q2_k
  - **Quick check question**: What does a one-sided Hoeffding bound tell you about |sample_mean - true_mean| for bounded rewards? (Answer: It bounds the deviation with high probability given sample size)

- **Concept**: Sequential task similarity and bounded difference assumptions
  - **Why needed here**: Adjacent-similarity assumption (|µj_k - µj+1_k| ≤ εk) justifies why past task samples inform current task
  - **Quick check question**: If εk = 0, what does assumption imply about tasks? (Answer: They are identical; all past samples are valid)

## Architecture Onboarding

- **Component map**: NT-UCB -> Tr-UCB -> Tr-UCB2 (wraps Tr-UCB with Phase I uniform sampling and εk estimation)
- **Critical path**: For each arm pull: compute both UCB and auxiliary estimates → compute widths → take min → select arm
- **Design tradeoffs**: Larger Bk (smaller εk) ⇒ more transfer samples ⇒ potentially more bias but more information; uniform sampling in Phase I increases ˆεj_k confidence but costs regret
- **Failure signatures**: Regret growth > NT-UCB baseline ⇒ likely εk mis-estimated high or Bk too large; persistent suboptimal arm pulls ⇒ auxiliary estimate too pessimistic
- **First 3 experiments**: 1) Run Tr-UCB with known εk on synthetic data where εk = 0 (identical tasks); 2) Run Tr-UCB2 with unknown εk, small δ, moderate l; 3) Compare Tr-UCB vs Naive-Transfer for varying εk values

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does performance of Tr-UCB2 compare to Tr-UCB in terms of regret bounds and empirical results when number of tasks J is very large?
- **Basis in paper**: [explicit] Paper states Tr-UCB2 performs at least as well as NT-UCB but doesn't fully capture transfer benefit, while Tr-UCB explicitly shows performance improvement
- **Why unresolved**: Paper mentions gap between performance but doesn't provide detailed comparison of regret bounds or empirical results for large J
- **What evidence would resolve it**: Detailed regret analysis and empirical results comparing Tr-UCB and Tr-UCB2 for varying numbers of tasks, especially when J is very large

### Open Question 2
- **Question**: What is the lower bound on the regret for sequential multi-task stochastic multi-armed bandit problem with adjacent similarity?
- **Basis in paper**: [explicit] Paper discusses regret upper bounds for proposed algorithms but doesn't provide lower bound on regret
- **Why unresolved**: Paper focuses on upper bounds and transfer benefit, doesn't address fundamental limit of problem
- **What evidence would resolve it**: Derivation and proof of lower bound on regret for sequential multi-task stochastic multi-armed bandit problem with adjacent similarity

### Open Question 3
- **Question**: How does performance of proposed algorithms change when similarity parameter εk varies over time or is non-stationary?
- **Basis in paper**: [inferred] Paper assumes similarity parameter εk is constant and doesn't address scenarios where it might change over time or be non-stationary
- **Why unresolved**: Paper's analysis based on fixed εk, doesn't explore impact of varying or non-stationary similarity parameters on algorithm performance
- **What evidence would resolve it**: Empirical and theoretical analysis of proposed algorithms' performance under varying or non-stationary similarity parameters

## Limitations
- Strong adjacent-similarity assumption may not hold in many real-world scenarios
- Tr-UCB2 requires costly uniform sampling phase that could be prohibitive for large K
- Transfer bound Bk depends on known/estimated εk, and misestimation could lead to insufficient or excessive transfer
- Analysis assumes independent tasks with only adjacent similarity, not leveraging information from multiple previous tasks

## Confidence

- **High confidence**: No-negative-transfer guarantee and regret reduction when tasks are highly similar and εk is known
- **Medium confidence**: Estimation procedure for unknown εk and resulting regret bounds
- **Medium confidence**: Empirical results showing performance gains over NT-UCB and Naive-Transfer

## Next Checks

1. Test Tr-UCB2 on synthetic data where εk varies over time (e.g., starts small and grows large) to verify it gracefully handles similarity changes
2. Measure the Phase I overhead cost empirically across different K values to quantify the trade-off between estimation accuracy and regret cost
3. Implement a variant that transfers samples from multiple previous tasks (not just immediately preceding one) to evaluate if performance improves when tasks exhibit longer-range similarity