---
ver: rpa2
title: 'Self-Discover: Large Language Models Self-Compose Reasoning Structures'
arxiv_id: '2402.03620'
source_url: https://arxiv.org/abs/2402.03620
tags:
- reasoning
- self
- structures
- structure
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SELF-DISCOVER is a framework that enables LLMs to automatically
  compose reasoning structures from atomic reasoning modules for solving complex reasoning
  tasks. The approach involves a two-stage process where the LLM first selects and
  adapts relevant reasoning modules to the task, then implements them into an explicit
  reasoning structure.
---

# Self-Discover: Large Language Models Self-Compose Reasoning Structures

## Quick Facts
- arXiv ID: 2402.03620
- Source URL: https://arxiv.org/abs/2402.03620
- Authors: Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V. Le, Ed H. Chi, Denny Zhou, Swaroop Mishra, Huaixiu Steven Zheng
- Reference count: 40
- One-line primary result: SELF-DISCOVER significantly improves reasoning performance on challenging benchmarks like BBH, T4D, and MATH by up to 32% compared to Chain-of-Thought.

## Executive Summary
SELF-DISCOVER is a framework that enables LLMs to automatically compose reasoning structures from atomic reasoning modules for solving complex reasoning tasks. The approach involves a two-stage process where the LLM first selects and adapts relevant reasoning modules to the task, then implements them into an explicit reasoning structure. SELF-DISCOVER significantly improves reasoning performance on challenging benchmarks like BBH, T4D, and MATH by up to 32% compared to Chain-of-Thought, while requiring 10-40x fewer inference calls than methods like self-consistency.

## Method Summary
SELF-DISCOVER is a two-stage framework where LLMs first self-discover task-specific reasoning structures, then use them to solve instances of the task. In Stage 1, the model selects relevant atomic reasoning modules, adapts them to the specific task, and implements them into a structured JSON-like format. In Stage 2, the model uses the discovered structure to solve task instances by filling in values. The framework improves reasoning performance while being computationally efficient compared to inference-intensive ensemble methods.

## Key Results
- SELF-DISCOVER improves reasoning performance on challenging benchmarks (BBH, T4D, MATH) by up to 32% compared to Chain-of-Thought
- SELF-DISCOVER requires 10-40x fewer inference calls than methods like CoT-Self-Consistency
- Self-discovered reasoning structures are transferable across model families (PaLM 2-L to GPT-4, and GPT-4 to Llama2)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-discovery of reasoning structures improves performance by grounding problem-solving in atomic reasoning modules that are task-specific.
- Mechanism: The framework first selects relevant atomic reasoning modules from a predefined set, then adapts them to the specific task, and finally implements them into a structured JSON-like format. This process allows the model to leverage multiple reasoning approaches tailored to the task rather than relying on a single generic method.
- Core assumption: The task has an intrinsic reasoning structure that can be discovered by meta-reasoning, and this structure is more effective than applying a single a priori method like Chain-of-Thought.
- Evidence anchors:
  - [abstract] "Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure"
  - [section 2] "Stage 1 aims to uncover the intrinsic reasoning structure for solving this task via meta-reasoning... The structure of the meta-prompts and full prompts are shown in Appendix."
  - [corpus] "Recent advancements in prompt engineering strategies, such as Chain-of-Thought (CoT) and Self-Discover, have demonstrated significant potential in improving the reasoning abilities of Large Language Models (LLMs)." (weak corpus evidence, but supportive)

### Mechanism 2
- Claim: Self-discovered reasoning structures are transferable across model families, demonstrating their universality.
- Mechanism: The reasoning structures discovered by one model (e.g., PaLM 2-L) can be applied to another model (e.g., GPT-4) during decoding, leveraging the universal nature of the underlying reasoning patterns.
- Core assumption: The reasoning structures capture fundamental problem-solving approaches that are not model-specific but rather task-specific.
- Evidence anchors:
  - [abstract] "Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2"
  - [section 5.2] "We first use a PaLM 2-L model to discover the reasoning structures of 4 reasoning tasks. Then, we apply the resulting reasoning structures to the decoding of GPT-4 as grounding."
  - [corpus] "Effects of structure on reasoning in instance-level Self-Discover" (direct but not yet available)

### Mechanism 3
- Claim: Self-discovery is computationally efficient compared to inference-heavy ensemble methods.
- Mechanism: The framework requires only 3 additional inference steps at the task level (SELECT, ADAPT, IMPLEMENT) and then one inference call per instance, while ensemble methods like CoT-Self-Consistency require multiple samples per instance.
- Core assumption: The cost of the meta-reasoning stage is amortized over all instances of the task, making it efficient for large-scale deployment.
- Evidence anchors:
  - [abstract] "Furthermore, SELF -DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute."
  - [section 4.3] "Figure 5 shows average accuracy and number of inference calls required per instance for each method using GPT-4... SELF -DISCOVER only requires one call per instance and three more inference calls on the task-level"
  - [corpus] Weak evidence; no direct corpus support for the efficiency claim.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Understanding CoT is crucial as SELF-DISCOVER is compared against it and aims to improve upon its limitations by composing multiple reasoning modules.
  - Quick check question: What is the main limitation of Chain-of-Thought prompting that SELF-DISCOVER aims to address?

- Concept: Meta-reasoning
  - Why needed here: SELF-DISCOVER uses meta-reasoning to discover task-specific reasoning structures, which is a key mechanism of the framework.
  - Quick check question: How does meta-reasoning differ from regular reasoning in the context of SELF-DISCOVER?

- Concept: Atomic reasoning modules
  - Why needed here: The framework is built upon a set of atomic reasoning modules that are selected, adapted, and composed into a reasoning structure.
  - Quick check question: Why is it beneficial to compose multiple atomic reasoning modules rather than using a single one?

## Architecture Onboarding

- Component map:
  - SELECT -> ADAPT -> IMPLEMENT -> Execution

- Critical path:
  1. Input: Task, reasoning module descriptions, task examples
  2. SELECT: Model selects relevant modules
  3. ADAPT: Model adapts selected modules to the task
  4. IMPLEMENT: Model creates structured reasoning plan
  5. Execution: Model solves instances using the plan
  6. Output: Final answers

- Design tradeoffs:
  - Flexibility vs. Complexity: Allowing many atomic modules provides flexibility but increases complexity in selection and adaptation.
  - Structure vs. Expressiveness: JSON-like structure aids interpretability but may constrain the model's natural reasoning flow.
  - Transferability vs. Task-specificity: Universal structures are more transferable but may be less optimal for specific tasks.

- Failure signatures:
  - Incorrect module selection: If SELECT chooses irrelevant modules, the reasoning structure will be ineffective.
  - Poor adaptation: If ADAPT fails to make modules task-specific, the structure may not capture the task's nuances.
  - Structural issues: If IMPLEMENT creates an unclear or incorrect structure, the model may not follow it properly during execution.

- First 3 experiments:
  1. Test SELECT, ADAPT, and IMPLEMENT stages separately on a simple task to ensure each stage works as expected.
  2. Run SELF-DISCOVER on a small set of BBH tasks and compare performance with CoT to verify improvements.
  3. Test transferability by using a structure discovered by one model (e.g., PaLM 2-L) on another model (e.g., GPT-4) on a simple task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SELF-DISCOVER's self-discovered reasoning structures be applied to real-world, open-ended problems beyond structured benchmarks?
- Basis in paper: [inferred] The paper demonstrates SELF-DISCOVER on various reasoning benchmarks, but doesn't explore its applicability to more complex, real-world scenarios.
- Why unresolved: The current experiments focus on controlled benchmark tasks. Real-world problems often involve ambiguity, incomplete information, and multi-modal inputs that weren't tested.
- What evidence would resolve it: Testing SELF-DISCOVER on real-world problem-solving tasks like medical diagnosis, legal reasoning, or complex decision-making scenarios would provide evidence of its broader applicability.

### Open Question 2
- Question: How does SELF-DISCOVER's performance scale with increasingly complex reasoning tasks that require multiple levels of abstraction?
- Basis in paper: [inferred] The paper shows improvements on challenging benchmarks, but doesn't explore how the method performs as task complexity increases significantly.
- Why unresolved: Current benchmarks may not fully capture the complexity of real-world reasoning. Tasks requiring deep hierarchical reasoning or multiple nested abstractions remain untested.
- What evidence would resolve it: Creating and testing SELF-DISCOVER on a benchmark with tasks requiring multiple levels of abstraction (e.g., multi-step planning with subgoals) would show its limits and scaling behavior.

### Open Question 3
- Question: Can SELF-DISCOVER's discovered structures be used to improve model interpretability beyond just task-solving?
- Basis in paper: [explicit] The paper mentions that self-discovered structures convey "LLMs' insights about the task in a more interpretable way than optimized prompts."
- Why unresolved: While interpretability is mentioned, the paper doesn't deeply explore how these structures can be used for understanding model behavior, debugging, or explaining decisions.
- What evidence would resolve it: Conducting user studies where human experts analyze and interpret self-discovered structures to understand model reasoning would demonstrate practical interpretability benefits.

## Limitations
- The transferability claims rely heavily on empirical demonstration rather than theoretical grounding, with unclear reasons for universal applicability.
- The efficiency comparison assumes a large number of task instances to amortize overhead, which may not hold for tasks with few instances.
- The universality of atomic reasoning modules across different domains and task complexities is not fully established.

## Confidence
**High confidence**: The SELF-DISCOVER framework is technically sound and the two-stage process (SELECT, ADAPT, IMPLEMENT) is clearly defined. The empirical improvements over CoT on the tested benchmarks are well-documented.

**Medium confidence**: The transferability of reasoning structures across model families is demonstrated but not theoretically explained. The efficiency claims are supported by data but may not generalize to all use cases.

**Low confidence**: The universality of atomic reasoning modules across domains and the long-term effectiveness of discovered structures are not established. The paper does not address potential catastrophic forgetting or degradation of discovered structures over time.

## Next Checks
1. **Transferability Stress Test**: Test the transferability of discovered structures across more diverse model architectures (e.g., Mistral, Claude) and sizes (e.g., GPT-3.5, LLaMA-13B) to determine the boundaries of universal applicability.

2. **Efficiency Analysis Under Varying Task Volume**: Measure the actual computational cost of the meta-reasoning stage (SELECT, ADAPT, IMPLEMENT) and determine the break-even point where SELF-DISCOVER becomes more efficient than direct methods like CoT for tasks with varying numbers of instances.

3. **Module Universality Investigation**: Apply SELF-DISCOVER to tasks from entirely different domains (e.g., medical diagnosis, legal reasoning, code generation) to test whether the same set of atomic reasoning modules is effective or if domain-specific modules are needed.