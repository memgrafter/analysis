---
ver: rpa2
title: 'FM-Fusion: Instance-aware Semantic Mapping Boosted by Vision-Language Foundation
  Models'
arxiv_id: '2402.04555'
source_url: https://arxiv.org/abs/2402.04555
tags:
- semantic
- label
- instance
- object
- instances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of semantic mapping in robotics,
  where traditional methods using supervised object detectors struggle with generalization
  across different environments. To overcome this limitation, the authors propose
  FM-Fusion, a method that leverages vision-language foundation models to detect objects
  in open-set labels and high-quality masks.
---

# FM-Fusion: Instance-aware Semantic Mapping Boosted by Vision-Language Foundation Models

## Quick Facts
- arXiv ID: 2402.04555
- Source URL: https://arxiv.org/abs/2402.04555
- Reference count: 25
- Primary result: Achieves 40.3 mAP on ScanNet semantic instance segmentation task

## Executive Summary
This paper addresses the challenge of semantic mapping in robotics, where traditional methods using supervised object detectors struggle with generalization across different environments. To overcome this limitation, the authors propose FM-Fusion, a method that leverages vision-language foundation models to detect objects in open-set labels and high-quality masks. The key idea is to fuse object detection from foundation models into an instance-aware semantic map using a probabilistic label fusion method and an instance refinement module. The method incrementally reconstructs an instance-aware semantic map by reading a sequence of RGB-D input. The authors evaluate the zero-shot performance of their method in ScanNet and SceneNN datasets, demonstrating significant improvements over traditional semantic mapping methods.

## Method Summary
The FM-Fusion method integrates pre-trained vision-language foundation models (RAM, GroundingDINO, SAM) into a semantic mapping pipeline without fine-tuning. It processes RGB-D frames to incrementally build an instance-aware semantic map through four key components: data association matches detections with existing instances, probabilistic label fusion predicts semantic classes using Bayesian inference, instance refinement merges over-segmentation, and instance-geometry fusion filters voxel outliers. The method achieves zero-shot generalization by leveraging foundation models trained on large-scale diverse data, and addresses over-segmentation through spatial overlap-based merging of instances.

## Key Results
- Achieves 40.3 mAP on ScanNet semantic instance segmentation task
- Significantly outperforms traditional semantic mapping methods
- Demonstrates robustness in other image distributions beyond training data

## Why This Works (Mechanism)

### Mechanism 1
Foundation models provide zero-shot generalization across different image distributions, enabling semantic mapping in new environments without fine-tuning. Vision-language foundation models (RAM, GroundingDINO, SAM) detect objects with open-set labels and high-quality masks, replacing supervised object detectors that are sensitive to image distribution shifts. The core assumption is that foundation models have been trained on large-scale diverse data that covers the target environment's characteristics. This works because the models can detect objects without requiring environment-specific training. Break condition: Foundation models fail to generalize if the target environment contains objects or contexts significantly different from their training data, or if domain shift is too large.

### Mechanism 2
Probabilistic label fusion can predict close-set semantic classes from open-set label measurements using Bayesian inference. A Bayes filter algorithm fuses multiple open-set label measurements across frames, using a statistic-summarized likelihood matrix to predict the most probable close-set semantic class for each instance. The core assumption is that the statistic-summarized likelihood matrix accurately represents the relationship between open-set measurements and close-set semantic classes. This works because Bayesian updating can combine uncertain measurements over time to improve predictions. Break condition: If the statistic-summarized likelihood matrix doesn't capture the true relationship between open-set and close-set labels, or if measurements are too noisy or contradictory.

### Mechanism 3
Instance refinement can merge over-segmented instances caused by inconsistent segmentation masks across different viewpoints. Spatial overlap information (semantic similarity and 3D IoU) is used to merge instances that represent the same physical object but were segmented differently at different viewpoints. The core assumption is that instances representing the same physical object will have high semantic similarity and spatial overlap across different viewpoints. This works because physical objects should have consistent semantic predictions and spatial relationships across different observations. Break condition: If instances representing different physical objects have similar semantic predictions and spatial overlap, or if viewpoint changes cause significant appearance differences that prevent correct merging.

## Foundational Learning

- **Concept: Bayes filter algorithm**
  - Why needed here: To fuse multiple uncertain measurements over time and predict the most probable semantic class for each instance
  - Quick check question: How does the Bayes filter update the probability distribution when a new measurement arrives?

- **Concept: Data association**
  - Why needed here: To correctly match detection results with existing instance volumes across frames
  - Quick check question: What metric is used to determine if a detection should be associated with an existing instance?

- **Concept: Voxel grid map fusion**
  - Why needed here: To incrementally integrate detected instances into the global semantic map
  - Quick check question: How are voxel weights updated when integrating new observations?

## Architecture Onboarding

- **Component map:** RGB-D frames -> SLAM (camera pose, global TSDF) -> Foundation Models (RAM, GroundingDINO, SAM) -> FM-Fusion (data association, probabilistic label fusion, instance refinement, instance-geometry fusion) -> Instance-aware semantic map

- **Critical path:**
  1. Foundation models detect objects and generate masks
  2. Data association matches detections with existing instances
  3. Probabilistic label fusion predicts semantic classes
  4. Instance refinement merges over-segmentation
  5. Instance-geometry fusion filters voxel outliers

- **Design tradeoffs:**
  - Separate global TSDF map from instance volumes for robustness to detection errors
  - Use statistic-summarized likelihood vs manually assigned for label fusion
  - Maintain instance volumes separately vs clustering in global map

- **Failure signatures:**
  - High false positive rate: Incorrect data association or noisy foundation model outputs
  - Over-segmentation: Inconsistent instance masks not properly merged
  - Wrong semantic predictions: Poor label likelihood matrix or insufficient measurements

- **First 3 experiments:**
  1. Run with only SAM segmentation (no foundation models) to establish baseline
  2. Run with foundation models but no label fusion to test detection quality
  3. Run with label fusion but no instance refinement to test semantic prediction accuracy

## Open Questions the Paper Calls Out
- How does the performance of the probabilistic label fusion method vary when using different semantic class hierarchies or label set structures beyond NYUv2?
- How does the text prompt augmentation strategy affect the detection performance of GroundingDINO in environments with highly dynamic or occluded objects?
- How does the choice of instance segmentation model (e.g., SAM vs. Mask R-CNN) impact the overall performance and computational efficiency of the semantic mapping system?

## Limitations
- The method relies heavily on the generalization ability of pre-trained foundation models, which may not perform well in environments significantly different from their training data
- The probabilistic label fusion requires a statistic-summarized likelihood matrix that may not capture all possible relationships between open-set and close-set labels
- The instance refinement module depends on spatial overlap thresholds that may not generalize well across different object types and environments

## Confidence

- **High confidence** in the core mechanism of using foundation models for zero-shot detection in semantic mapping
- **Medium confidence** in the probabilistic label fusion approach, as the likelihood matrix derivation is not fully validated
- **Medium confidence** in instance refinement effectiveness, as threshold selection may be dataset-specific

## Next Checks

1. Evaluate foundation model performance on a dataset with significant domain shift from training data to test generalization limits
2. Conduct ablation studies varying the semantic similarity and 3D IoU thresholds in instance refinement across different object categories
3. Test the label fusion module with manually constructed likelihood matrices versus statistic-summarized ones to quantify the impact of the approximation