---
ver: rpa2
title: Identity Preserving 3D Head Stylization with Multiview Score Distillation
arxiv_id: '2411.13536'
source_url: https://arxiv.org/abs/2411.13536
tags:
- stylization
- image
- vision
- conference
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of 3D head stylization, where
  existing methods struggle to preserve unique identities while transforming facial
  features into artistic representations, particularly when generating 360-degree
  views. The authors propose a novel framework using PanoHead for 360-degree synthesis
  and employ negative log-likelihood distillation (LD) to enhance identity preservation.
---

# Identity Preserving 3D Head Stylization with Multiview Score Distillation

## Quick Facts
- **arXiv ID**: 2411.13536
- **Source URL**: https://arxiv.org/abs/2411.13536
- **Reference count**: 40
- **Primary result**: Achieves FID scores of 77.6-144.5 across multiple stylization tasks while significantly improving identity preservation

## Executive Summary
This paper introduces a novel approach to 3D head stylization that preserves unique facial identities while transforming realistic features into artistic representations across 360-degree views. The method leverages PanoHead for multiview synthesis and employs negative log-likelihood distillation with several key innovations: rank-weighted score tensors for latent channels, mirror gradient techniques for symmetric poses, and multi-view grid distillation. These additions effectively regularize the distillation gradients and improve both stylization quality and identity preservation.

The proposed framework demonstrates substantial quantitative improvements over existing methods, achieving FID scores ranging from 67.7 to 144.5 across different stylization prompts (Pixar, Joker, Werewolf, Sketch, Statue). Identity preservation metrics show similar gains, with ID similarity scores reaching 0.69-0.75. The method successfully maintains facial features and accessories while generating high-quality artistic stylizations from multiple viewpoints, addressing a critical limitation in current 3D head stylization approaches.

## Method Summary
The approach combines PanoHead's 360-degree face synthesis capabilities with negative log-likelihood distillation guided by a pre-trained text-conditioned diffusion model (RV v5.1). Key innovations include rank-weighted score tensors that apply different weights to singular values during SVD decomposition, mirror gradient techniques that enforce consistency across symmetric poses, and multi-view grid distillation that captures spatial relationships before super-resolution layers. The method is trained for 10k iterations using Adam optimizer with learning rate 1e-4, employing ControlNet guidance and CFG weight of 7.5.

## Key Results
- Achieves FID scores of 77.6 (Pixar), 67.7 (Joker), 99.7 (Werewolf), 91.6 (Sketch), and 144.5 (Statue)
- Improves identity preservation with ID similarity scores of 0.69-0.75 across stylization prompts
- Outperforms competing methods with FID scores ranging from 81.1 to 189.3
- Successfully maintains facial features and accessories while achieving artistic stylization across multiple viewpoints

## Why This Works (Mechanism)
The method addresses the fundamental challenge of balancing artistic stylization with identity preservation by introducing three key regularization techniques. Rank-weighted score tensors allow the model to prioritize important latent features during distillation, while mirror gradients enforce consistency across symmetric poses to prevent identity drift. Multi-view grid distillation captures spatial relationships and geometric consistency before super-resolution, ensuring that stylization remains coherent across the full 360-degree view. These techniques work together to guide the generator toward solutions that satisfy both the stylization objective and identity preservation constraints.

## Foundational Learning

**Negative Log-Likelihood Distillation**: A training technique where gradients from a pre-trained diffusion model guide a generator to produce images matching desired style distributions. Needed to transfer artistic styles while maintaining control over the generation process. Quick check: Verify gradient flow from diffusion model to generator is correctly implemented.

**Rank-Weighted SVD**: Decomposes score tensors and applies different weights to singular values to prioritize important latent features. Required to prevent low-rank artifacts and maintain high-frequency details during stylization. Quick check: Monitor singular value distributions during training for stability.

**Mirror Gradients**: Technique that enforces consistency across symmetric poses by averaging gradients from pose pairs. Essential for preventing identity distortion when rotating views. Quick check: Compare outputs for symmetric poses to ensure visual consistency.

**Multi-View Grid Distillation**: Captures spatial relationships and geometric consistency by distilling across multiple viewpoints before super-resolution. Needed to maintain 3D coherence across the full 360-degree view. Quick check: Verify geometric consistency metrics (∆D) remain stable during training.

## Architecture Onboarding

**Component Map**: Text prompt → Diffusion model (RV v5.1) → Score distillation → PanoHead generator → 360° head output

**Critical Path**: The distillation pipeline from diffusion model scores through rank-weighted SVD, mirror gradients, and grid distillation to the PanoHead generator represents the critical path. Each component must function correctly to maintain identity preservation while achieving stylization.

**Design Tradeoffs**: The method trades computational complexity (multiple gradient calculations for mirror and grid distillation) for improved identity preservation and geometric consistency. The rank weighting adds regularization but requires careful tuning of coefficients.

**Failure Signatures**: Blurry outputs or identity loss indicate issues with CFG weight settings or gradient flow. Artifacts around hair/ears suggest problems with rank weighting implementation or insufficient grid distillation before SR layers.

**3 First Experiments**:
1. Test basic LD with PanoHead using standard CFG weight to establish baseline performance
2. Implement rank-weighted SVD with coefficients W = diag(1, 0.75, 0.5, 0.25) and compare identity preservation
3. Add mirror gradients for symmetric pose pairs and measure improvement in pose consistency

## Open Questions the Paper Calls Out
None

## Limitations
- The method relies heavily on pre-trained models (PanoHead, diffusion model) which may limit generalization to different domains
- Computational overhead from multiple gradient calculations (mirror and grid distillation) increases training time
- Rank weighting coefficients and pose selection strategies require careful tuning that may not transfer well across different stylization tasks

## Confidence

**High confidence**: Core methodology of LD with rank-weighted tensors and mirror gradients; quantitative results showing FID and ID similarity improvements

**Medium confidence**: Effectiveness of multi-view grid distillation integration; relative performance gains compared to baselines

**Low confidence**: Exact implementation details for rank weighting and pose sampling strategies; sensitivity to pre-trained model versions

## Next Checks

1. Implement and test the rank-weighted SVD decomposition with the specified coefficients (W = diag(1, 0.75, 0.5, 0.25)) to verify its impact on identity preservation

2. Conduct ablation studies comparing mirror gradients vs. standard gradients across symmetric pose pairs to quantify the contribution of this technique

3. Evaluate the sensitivity of results to different pre-trained PanoHead and diffusion model versions to establish robustness boundaries