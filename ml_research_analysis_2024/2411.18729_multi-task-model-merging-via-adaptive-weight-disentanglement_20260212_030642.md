---
ver: rpa2
title: Multi-Task Model Merging via Adaptive Weight Disentanglement
arxiv_id: '2411.18729'
source_url: https://arxiv.org/abs/2411.18729
tags:
- task
- vectors
- merging
- performance
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adaptive Weight Disentanglement (AWD), a
  method to improve multi-task model merging by enhancing orthogonality among task
  vectors while preserving individual task performance. The approach decomposes task
  vectors into redundant and disentangled components, optimizing for orthogonality
  and invariance using an automatic differentiation framework.
---

# Multi-Task Model Merging via Adaptive Weight Disentanglement

## Quick Facts
- arXiv ID: 2411.18729
- Source URL: https://arxiv.org/abs/2411.18729
- Reference count: 40
- Primary result: AWD improves multi-task merging by enhancing orthogonality among task vectors while preserving individual task performance

## Executive Summary
This paper introduces Adaptive Weight Disentanglement (AWD), a method to improve multi-task model merging by enhancing orthogonality among task vectors while preserving individual task performance. The approach decomposes task vectors into redundant and disentangled components, optimizing for orthogonality and invariance using an automatic differentiation framework. Experiments across vision and language tasks show AWD consistently outperforms state-of-the-art merging methods, achieving absolute accuracy improvements of 2.8% on ViT-B/32 and 1.5% on ViT-L/14 over prior approaches.

## Method Summary
AWD works by first computing task vectors as differences between fine-tuned and pre-trained models. It then identifies redundant components across these vectors and decomposes each task vector into a redundant part and a disentangled part. The method uses automatic differentiation to optimize a redundant vector δ that, when subtracted from each task vector, maximizes orthogonality among the resulting disentangled vectors while minimizing the norm of δ to preserve task-specific performance. This optimized decomposition is then used in standard model merging approaches like Task Arithmetic or AdaMerging to produce the final merged model.

## Key Results
- AWD achieves 2.8% absolute accuracy improvement on ViT-B/32 and 1.5% on ViT-L/14 over prior approaches
- The method consistently outperforms state-of-the-art merging methods across 8 vision and 8 language tasks
- AWD shows robustness to varying numbers of tasks and different scaling coefficients
- The approach generalizes effectively from vision to language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonal task vectors reduce interference between tasks during model merging.
- Mechanism: By decomposing task vectors into redundant and disentangled components, the method enhances orthogonality among the disentangled vectors, minimizing overlap in parameter space that causes interference.
- Core assumption: Task interference in model merging is primarily caused by non-orthogonal (overlapping) task vectors in parameter space.
- Evidence anchors:
  - [abstract] "task vectors employed in model merging should be orthogonal to minimize interference among tasks"
  - [section] "when all task vectors are orthogonal to each other, Eq. 4 will be satisfied: ∀ i ̸= j, cos (τi, τj) = 0 ⇒ ∀i, G i = 0"
  - [corpus] "Task Arithmetic (TA) defines task vectors by subtracting the pre-trained... Model merging aims to integrate task-specific abilities from individually fine-tuned models into a single model without extra training"

### Mechanism 2
- Claim: Removing redundant vector components preserves task-specific performance while improving orthogonality.
- Mechanism: The redundant vector δ captures shared or unnecessary components across task vectors. Subtracting δ from each task vector yields disentangled vectors that maintain task-specific information while reducing redundancy.
- Core assumption: Task vectors contain redundant components that, when removed, do not significantly impact task-specific performance but improve orthogonality.
- Evidence anchors:
  - [abstract] "decomposes task vectors into redundant and disentangled components"
  - [section] "task vectors exhibit significant redundancy... By eliminating redundant components, some conflicts along the parameter directions can be avoided"
  - [corpus] "purifying task vectors... existing methods attempt to alleviate task conflicts by sparsifying task vectors or promoting orthogonality among them"

### Mechanism 3
- Claim: Norm constraint on the redundant vector preserves task-specific model performance.
- Mechanism: By minimizing the norm of the redundant vector δ, the method ensures that only non-essential components are removed, preserving the performance of individual task-specific models.
- Core assumption: Task-specific performance is primarily determined by the essential components of task vectors, and removing only non-essential components (small norm δ) will not degrade performance.
- Evidence anchors:
  - [abstract] "impose an norm constraint on the redundant vectors to preserve the performance of the task-specific models"
  - [section] "without proper constraints on δ, the disentangled task vector τi − δ may experience a significant drop in performance... we introduce an norm constraint on δ"
  - [corpus] "Task Vector Quantization for Memory-Efficient Model Merging... quantizing task vector... existing methods attempt to alleviate task conflicts"

## Foundational Learning

- Concept: Task Arithmetic Property
  - Why needed here: Provides the theoretical foundation for combining task vectors through arithmetic operations to transfer capabilities between tasks.
  - Quick check question: What is the Task Arithmetic Property and how does it relate to model merging?

- Concept: Taylor Expansion Approximation
  - Why needed here: Used to simplify the analysis of merging gap by approximating higher-order terms as negligible when task vectors are small.
  - Quick check question: Why can we approximate the second-order terms as zero in the Taylor expansion of the merging gap?

- Concept: Cosine Similarity
  - Why needed here: Measures the angle between task vectors, with orthogonality corresponding to zero cosine similarity, which reduces interference.
  - Quick check question: How does cosine similarity between task vectors relate to task interference in model merging?

## Architecture Onboarding

- Component map:
  - Pre-trained model Θ (fixed)
  - Task-specific models {Θ⋆i} (fine-tuned from Θ)
  - Task vectors {τi = Θ⋆i - Θ}
  - Redundant vector δ (trainable, initialized to zero)
  - Disentangled task vectors {bτi = τi - δ}
  - Merged model bΘ = Θ + Σ λi bτi (for Task Arithmetic)

- Critical path:
  1. Initialize redundant vector δ to zero
  2. Compute disentangled task vectors bτi = τi - δ
  3. Calculate orthogonality loss (cosine similarity between bτi vectors)
  4. Calculate invariance loss (norm of δ)
  5. Update δ using gradient descent
  6. Use disentangled task vectors for model merging

- Design tradeoffs:
  - Orthogonality vs. invariance: Balancing the hyperparameter α controls the trade-off between making task vectors more orthogonal and preserving their task-specific performance.
  - Computational cost: Additional optimization step for finding δ, but minimal compared to full fine-tuning.
  - Model size sensitivity: Performance improvements are more pronounced for smaller models (ViT-B/32) compared to larger models (ViT-L/14).

- Failure signatures:
  - Performance degradation on individual tasks: Indicates that the norm constraint on δ is too restrictive or that δ is removing essential components.
  - Minimal improvement over baseline methods: Suggests that task vectors already have good orthogonality or that interference is not the primary bottleneck.
  - Increased computational time without performance gain: May indicate issues with the optimization process or hyperparameter settings.

- First 3 experiments:
  1. Verify that subtracting δ from task vectors reduces cosine similarity between them while maintaining task-specific performance on a validation set.
  2. Test the sensitivity of the merged model's performance to the hyperparameter α by varying it over a range (e.g., {1e-2, 1e-3, 1e-4, 1e-5, 1e-6}).
  3. Compare the performance of the merged model using disentangled task vectors against using original task vectors on a held-out test set with the same task coefficients λi.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AWD scale when merging more than 8 tasks, and what is the theoretical limit of task interference reduction?
- Basis in paper: [explicit] The paper discusses robustness analysis on varying task numbers but only tests up to 8 tasks, showing decreasing performance as task numbers increase.
- Why unresolved: The experiments only cover up to 8 tasks, leaving open questions about scalability to larger multi-task scenarios.
- What evidence would resolve it: Testing AWD on benchmarks with 10+ tasks and analyzing the relationship between task count and performance degradation.

### Open Question 2
- Question: Can the adaptive weight disentanglement approach be extended to work with non-linear task vector representations or more complex model architectures?
- Basis in paper: [inferred] The paper focuses on linear task vectors in transformer-based models but mentions potential limitations with high-dimensional embedding layers.
- Why unresolved: The current implementation is limited to linear weight layers, and the paper doesn't explore non-linear or more complex representations.
- What evidence would resolve it: Experiments applying AWD to non-linear task representations or testing on architectures beyond standard transformers.

### Open Question 3
- Question: What is the impact of task vector orthogonality on the generalization ability of merged models to completely unseen tasks?
- Basis in paper: [explicit] The paper shows AWD improves performance on unseen tasks but doesn't deeply analyze the relationship between orthogonality and generalization.
- Why unresolved: While performance improvements are shown, the underlying mechanism connecting orthogonality to generalization capability remains unexplored.
- What evidence would resolve it: Systematic studies comparing generalization performance across different levels of task vector orthogonality.

## Limitations

- The paper assumes task vectors exhibit significant redundancy, but does not provide quantitative analysis of how common this property is across different model architectures or task combinations
- The hyperparameter α requires tuning for optimal performance, and the paper does not provide guidance on how to select this parameter for new tasks or models
- While AWD shows improvements on ViT and RoBERTa architectures, generalization to other architectures (CNNs, smaller language models, or different modalities) remains untested

## Confidence

- **High Confidence**: The orthogonality mechanism (Mechanism 1) and the norm constraint for preserving performance (Mechanism 3) are well-supported by both theoretical analysis and experimental results
- **Medium Confidence**: The redundancy decomposition approach (Mechanism 2) is theoretically sound, but the extent of redundancy in task vectors across different domains needs further validation
- **Medium Confidence**: Generalization claims to language models are supported by experiments on RoBERTa variants, but broader architecture coverage would strengthen this

## Next Checks

1. Analyze the redundancy distribution: Measure the magnitude of the redundant vector δ relative to task vectors across different task combinations to quantify how much redundancy exists in practice
2. Test hyperparameter sensitivity: Systematically evaluate the merged model performance across a wider range of α values and task coefficient λ combinations to establish robustness to hyperparameter choices
3. Cross-architecture validation: Apply AWD to a CNN-based vision model (e.g., ResNet) and a smaller language model (e.g., BERT-Base) to verify the method's effectiveness across different architectures and scales