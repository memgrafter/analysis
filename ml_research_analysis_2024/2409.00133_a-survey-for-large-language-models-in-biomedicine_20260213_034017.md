---
ver: rpa2
title: A Survey for Large Language Models in Biomedicine
arxiv_id: '2409.00133'
source_url: https://arxiv.org/abs/2409.00133
tags:
- llms
- biomedical
- medical
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews the applications, adaptation
  strategies, and challenges of large language models (LLMs) in biomedicine based
  on an analysis of 484 publications. The study examines zero-shot performance across
  137 biomedical tasks, finding that while general-purpose LLMs like GPT-4 show promise
  in diagnostic assistance and literature analysis, their performance varies significantly
  across different medical specialties and expertise levels.
---

# A Survey for Large Language Models in Biomedicine

## Quick Facts
- arXiv ID: 2409.00133
- Source URL: https://arxiv.org/abs/2409.00133
- Reference count: 40
- Primary result: Comprehensive review of 484 publications analyzing LLM applications, adaptation strategies, and challenges in biomedicine

## Executive Summary
This survey provides a comprehensive analysis of large language models in biomedical applications, examining zero-shot performance across 137 biomedical tasks and evaluating various adaptation strategies. The study finds that while general-purpose LLMs like GPT-4 show promise in diagnostic assistance and literature analysis, their performance varies significantly across different medical specialties and expertise levels. The review details adaptation approaches including full-parameter fine-tuning, instruction tuning, and parameter-efficient methods to enhance LLM performance in specialized biomedical contexts.

## Method Summary
The survey analyzed 484 publications from PubMed, Web of Science, and arXiv to examine LLM applications in biomedicine. The methodology included evaluating zero-shot performance of general-purpose LLMs across 137 biomedical tasks, assessing adaptation strategies such as full-parameter fine-tuning, instruction tuning, and parameter-efficient fine-tuning methods, and analyzing multimodal integration techniques. Performance metrics included F1 scores, accuracy, BLEU-1, ROUGE-L, and specialized metrics for professionalism, fluency, and safety.

## Key Results
- Zero-shot performance of general-purpose LLMs varies significantly across different medical specialties and expertise levels
- Fine-tuned models like GatorTron achieved F1 scores of 93.01% in medical question answering, surpassing previous benchmarks by 7.77%
- Multimodal LLMs demonstrated superior performance in radiology report generation with METEOR scores of 0.534

## Why This Works (Mechanism)

### Mechanism 1: Fine-tuning for Domain Specificity
- Fine-tuning enables LLMs to capture biomedical domain specificity by adjusting all parameters to specialized terminology and contexts
- Core assumption: Domain-specific datasets contain sufficient high-quality examples to meaningfully adjust all model parameters
- Break Condition: Performance plateaus despite continued fine-tuning

### Mechanism 2: Parameter-Efficient Fine-Tuning
- PEFT techniques like LoRA and QLoRA add small trainable matrices to the model for task-specific adaptations
- Core assumption: Base LLM already encodes general knowledge that can be adapted through small parameter modifications
- Break Condition: Performance degrades significantly when fine-tuning very small percentage of parameters

### Mechanism 3: Multimodal Integration
- Multimodal LLMs combine text, images, gene sequences, and protein structures through attention mechanisms
- Core assumption: Different data modalities contain complementary information that can be meaningfully integrated
- Break Condition: Model performance does not improve or degrades when adding additional modalities

## Foundational Learning

- Concept: Transformer Architecture
  - Why needed here: Understanding base architecture is crucial for grasping how different fine-tuning strategies modify the model
  - Quick check question: What are the three main types of transformer architectures used in LLMs, and what are their primary use cases?

- Concept: Fine-tuning Techniques
  - Why needed here: Different fine-tuning approaches have distinct computational and performance characteristics
  - Quick check question: What is the primary difference between full-parameter fine-tuning and parameter-efficient fine-tuning in terms of model modification?

- Concept: Biomedical Domain Specificity
  - Why needed here: Biomedical applications require specialized knowledge that general LLMs lack
  - Quick check question: Why do general-purpose LLMs often struggle with biomedical tasks in zero-shot settings?

## Architecture Onboarding

- Component map: Data pipeline (collection → preprocessing → cleaning → standardization → augmentation → splitting) → Model training (base model selection → pre-training → fine-tuning strategy selection → training/validation/test phases) → Evaluation (task-specific metrics → benchmark comparison → human evaluation) → Deployment considerations (privacy preservation → interpretability requirements → computational resource constraints)
- Critical path: Data quality → appropriate fine-tuning strategy → model performance → clinical validation → deployment
- Design tradeoffs:
  - Performance vs. computational efficiency (full-parameter vs. PEFT)
  - Model complexity vs. interpretability
  - Data privacy vs. model performance (federated learning considerations)
  - Generalizability vs. specialization
- Failure signatures:
  - Overfitting on training data with poor generalization
  - Catastrophic forgetting of base model knowledge
  - Performance degradation with additional modalities in multimodal models
  - Privacy leaks in federated learning scenarios
- First 3 experiments:
  1. Compare zero-shot performance of general LLMs vs. domain-specific fine-tuned models on a simple biomedical task (e.g., named entity recognition)
  2. Evaluate different fine-tuning strategies (full-parameter, LoRA, QLoRA) on the same biomedical dataset to measure performance-efficiency tradeoffs
  3. Test multimodal integration by combining text and image data on a radiology report generation task to assess information fusion effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be effectively adapted to handle specialized terminology and complex concepts in different biomedical fields while maintaining accuracy and reliability?
- Basis in paper: [explicit] The paper discusses challenges of applying general-purpose LLMs to biomedicine due to specialized vocabulary and complex relationships between biological entities
- Why unresolved: The effectiveness of adaptation strategies across different biomedical disciplines and tasks remains to be fully evaluated
- What evidence would resolve it: Comparative studies evaluating performance of LLMs adapted using different strategies across wide range of biomedical tasks and specialties

### Open Question 2
- Question: What are the most effective ways to address challenges of data privacy, model interpretability, and fairness in biomedical LLM applications?
- Basis in paper: [explicit] The paper identifies data privacy concerns, limited model interpretability, and fairness issues as key challenges
- Why unresolved: While potential solutions are suggested, comprehensive evaluation of their effectiveness in real-world biomedical applications is lacking
- What evidence would resolve it: Empirical studies demonstrating effectiveness of federated learning and explainable AI techniques in improving data privacy, model interpretability, and fairness

### Open Question 3
- Question: How can LLMs be effectively integrated into different levels of healthcare delivery while ensuring appropriate customization and continuous updating?
- Basis in paper: [explicit] The paper discusses varying roles and impacts of LLMs across different levels of healthcare delivery
- Why unresolved: The paper highlights potential but does not provide specific guidelines for effective integration and implementation at each level
- What evidence would resolve it: Case studies and pilot implementations of LLMs in different healthcare settings, evaluating impact on clinical outcomes and workflow efficiency

## Limitations

- Analysis of 484 publications may miss emerging adaptation techniques and recent model developments
- Zero-shot performance metrics show significant variability depending on medical specialty and expertise level
- Computational requirements for different fine-tuning strategies are not fully quantified

## Confidence

- High confidence: General observations about LLM performance variation across medical specialties and necessity of adaptation strategies
- Medium confidence: Specific performance metrics and F1 scores reported for individual models
- Low confidence: Long-term clinical impact assessments and privacy preservation claims

## Next Checks

1. Replicate zero-shot performance evaluation using multiple general-purpose LLMs on standardized biomedical benchmarks to verify consistency across different task types and medical domains
2. Conduct head-to-head comparison of full-parameter fine-tuning versus parameter-efficient methods on identical biomedical datasets to quantify performance-efficiency tradeoffs
3. Implement federated learning approach for a biomedical LLM application to empirically test privacy preservation claims while measuring performance degradation