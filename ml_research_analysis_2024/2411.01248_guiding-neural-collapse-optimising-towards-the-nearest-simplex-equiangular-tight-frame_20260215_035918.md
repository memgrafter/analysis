---
ver: rpa2
title: 'Guiding Neural Collapse: Optimising Towards the Nearest Simplex Equiangular
  Tight Frame'
arxiv_id: '2411.01248'
source_url: https://arxiv.org/abs/2411.01248
tags:
- simplex
- neural
- training
- where
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for accelerating neural network
  convergence by dynamically aligning classifier weights to the nearest Simplex Equiangular
  Tight Frame (ETF) geometry of the penultimate layer feature means. The approach
  formulates this alignment as a Riemannian optimization problem over the Stiefel
  manifold, solved at each training iteration using a trust-region method.
---

# Guiding Neural Collapse: Optimising Towards the Nearest Simplex Equiangular Tight Frame

## Quick Facts
- arXiv ID: 2411.01248
- Source URL: https://arxiv.org/abs/2411.01248
- Reference count: 40
- One-line primary result: Dynamic alignment of classifier weights to nearest simplex ETF geometry accelerates neural network convergence

## Executive Summary
This paper introduces a method for accelerating neural network convergence by dynamically aligning classifier weights to the nearest Simplex Equiangular Tight Frame (ETF) geometry of the penultimate layer feature means. The approach formulates this alignment as a Riemannian optimization problem over the Stiefel manifold, solved at each training iteration using a trust-region method. By encapsulating the optimization within a declarative node, gradients propagate through the process for end-to-end learning. Experiments on synthetic Unconstrained Feature Models (UFMs) and real datasets (CIFAR10/100, STL10, ImageNet) with ResNet and VGG architectures show faster convergence and improved training stability compared to both fixed ETF and standard learned classifier approaches.

## Method Summary
The method computes the nearest simplex ETF to the current feature means by solving a Riemannian optimization problem over the Stiefel manifold at each training iteration. This is implemented as a deep declarative node (DDN) that enables end-to-end learning through implicit differentiation. A proximal regularization term ensures solution uniqueness and training stability. The approach dynamically adjusts classifier weights based on the current feature distribution rather than using fixed or purely learned weights.

## Key Results
- Faster convergence to neural collapse metrics compared to standard and fixed ETF baselines
- Improved training stability across multiple random seeds and dataset configurations
- Competitive final accuracy while achieving neural collapse more rapidly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamically aligning classifier weights to the nearest simplex ETF geometry accelerates convergence
- Mechanism: At each training iteration, the classifier weights are implicitly set to the solution of a Riemannian optimization problem that finds the nearest simplex ETF to the current penultimate layer feature means. This provides a more optimal starting point than learning from scratch or using a fixed ETF.
- Core assumption: The optimal classifier structure follows a simplex ETF geometry, and the closest such structure to current features can be efficiently computed via Riemannian optimization on the Stiefel manifold.
- Evidence anchors:
  - [abstract]: "the final classifier layer converges to a Simplex Equiangular Tight Frame (ETF), which maximally separates the weights corresponding to each class"
  - [section]: "Specifically, the nearest simplex ETF is determined by solving a Riemannian optimisation problem"
  - [corpus]: Weak - most related papers discuss neural collapse and ETFs but don't provide evidence for the specific mechanism of dynamic nearest-ETF computation
- Break condition: If the Riemannian optimization becomes computationally prohibitive for large-scale problems, or if the assumption about optimal ETF structure breaks down in non-standard training scenarios

### Mechanism 2
- Claim: The deep declarative node enables end-to-end learning by allowing gradients to propagate through the Riemannian optimization
- Mechanism: By encapsulating the Riemannian optimization problem within a declarative node, gradients can flow through the optimization process using implicit differentiation, allowing the network to learn features that better align with the optimal ETF structure
- Core assumption: The solution to the Riemannian optimization problem is differentiable with respect to the input features, and the implicit function theorem can be applied to compute these gradients efficiently
- Evidence anchors:
  - [abstract]: "by constructing this inner-optimisation problem as a deep declarative node [23], we allow gradients to propagate through the Riemannian optimisation facilitating end-to-end learning"
  - [section]: "we employ techniques described in Gould et al. [23] utilising the implicit function theorem to compute the gradients"
  - [corpus]: Moderate - Gould et al. [23] establishes the theoretical foundation for deep declarative nodes, though specific evidence for this ETF application is limited
- Break condition: If the optimization problem becomes ill-conditioned or if the implicit differentiation becomes numerically unstable for certain feature configurations

### Mechanism 3
- Claim: The proximal term ensures solution uniqueness and training stability
- Mechanism: Adding a proximal regularization term to the optimization objective guarantees a unique solution by penalizing deviations from a reference simplex ETF direction, preventing oscillation between multiple valid ETF configurations
- Core assumption: The original optimization problem without the proximal term has multiple solutions due to the rank-deficient nature of simplex ETFs, and the proximal term effectively regularizes this non-uniqueness
- Evidence anchors:
  - [section]: "We address this issue by introducing a proximal term to the problem's objective function. This guarantees the uniqueness of the solution and stabilises the training process"
  - [abstract]: No direct mention of proximal term mechanism
  - [corpus]: Weak - limited discussion of proximal regularization in related neural collapse literature
- Break condition: If the proximal coefficient is set too high, causing the solution to converge to the proximal direction rather than the optimal ETF for current features

## Foundational Learning

- Riemannian Optimization on Stiefel Manifolds:
  - Why needed here: The problem requires finding orthogonal matrices that maximize separation between class weights, which naturally forms a constrained optimization problem on the Stiefel manifold
  - Quick check question: What is the difference between a Stiefel manifold and a Grassmann manifold in the context of this problem?

- Implicit Differentiation and Deep Declarative Networks:
  - Why needed here: To enable backpropagation through the optimization problem that defines the classifier weights at each iteration
  - Quick check question: How does the implicit function theorem enable gradient computation through optimization problems?

- Neural Collapse Phenomenon:
  - Why needed here: Understanding why simplex ETF geometry emerges as optimal provides the theoretical foundation for why this approach should work
  - Quick check question: What are the four key properties of neural collapse observed at the terminal phase of training?

## Architecture Onboarding

- Component map: Feature extraction backbone (CNN/ResNet/VGG) -> Exponential moving average computation for feature means -> Riemannian optimization module (Trust-Region method on Stiefel manifold) -> Deep declarative node wrapper for gradient propagation -> Standard classification loss computation

- Critical path: Forward pass through backbone → Feature mean computation → Riemannian optimization → Classifier weight assignment → Loss computation → Backward pass through DDN

- Design tradeoffs: Computational cost of Riemannian optimization vs. convergence speed improvement; memory requirements for DDN gradient computation vs. training stability

- Failure signatures: Training instability when batch size is too small (insufficient class representation); poor convergence when proximal coefficient is mis-tuned; numerical instability in DDN gradient computation for large feature dimensions

- First 3 experiments:
  1. Verify neural collapse metrics (NC1, NC2, NC3) converge faster than baseline methods on CIFAR10 with ResNet18
  2. Test training stability across different random seeds and measure variance in final accuracy
  3. Benchmark computational cost (forward/backward time per epoch) against standard and fixed ETF approaches on CIFAR100

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the implicit ETF method's performance scale with increasing dataset size and model complexity, particularly for very large-scale datasets like ImageNet?
- Basis in paper: [explicit] The paper notes computational challenges for large-scale datasets like ImageNet, particularly regarding DDN gradient computation, and mentions plans to explore ways to expedite the DDN forward and backward pass in future work.
- Why unresolved: The current implementation faces memory inefficiency issues when computing DDN gradients for large d and C, as seen in the ImageNet experiments where DDN gradients were omitted due to computational constraints.
- What evidence would resolve it: Empirical results comparing the implicit ETF method with and without DDN gradients on ImageNet, along with benchmark timings and memory usage, would clarify the scalability limitations and potential optimizations needed.

### Open Question 2
- Question: What is the exact contribution of the DDN gradient to feature updates during training, and under what conditions is it most critical?
- Basis in paper: [explicit] The paper notes that empirical observations on small-scale datasets indicate that even without the back-propagation through the DDN layer, performance remains comparable, though there is a strong impact on the atomic feature level when the DDN gradient is included.
- Why unresolved: While the paper shows some impact on feature-level metrics, it doesn't definitively establish when the DDN gradient is essential versus optional, particularly across different dataset scales and architectures.
- What evidence would resolve it: Controlled experiments systematically varying when DDN gradients are included/excluded across different dataset sizes, architectures, and training stages would quantify the conditions under which the DDN gradient is most beneficial.

### Open Question 3
- Question: How sensitive is the method to the choice of initialization schemes for Uinit and Uprox, and are there more optimal strategies than those currently explored?
- Basis in paper: [explicit] The paper explores several initialization methods but concludes that solving the original problem without the proximal term and using the resulting U* to initialize both Uinit and Uprox yielded the most stable results.
- Why unresolved: While the paper identifies a stable initialization approach, it doesn't explore the full space of possible initialization strategies or analyze sensitivity to different hyperparameter choices.
- What evidence would resolve it: Systematic ablation studies testing various initialization strategies (including random orthogonal matrices, canonical directions, and learned initializations) across different problem scales would reveal the optimal initialization approach and its sensitivity to hyperparameters.

## Limitations
- Computational complexity of trust-region Riemannian optimization may become prohibitive for very large-scale problems
- Reliance on implicit function theorem for gradient computation introduces potential numerical stability concerns
- Proximal regularization term requires careful tuning that may not generalize across different architectures

## Confidence
- **High Confidence**: The theoretical foundation linking neural collapse to simplex ETF geometry and the basic formulation of the Riemannian optimization problem
- **Medium Confidence**: The practical implementation details of the trust-region solver and the DDN gradient propagation mechanism
- **Low Confidence**: The robustness of the approach across diverse architectural choices and the sensitivity to proximal coefficient tuning

## Next Checks
1. **Scale Sensitivity Test**: Evaluate the method's performance and computational efficiency on ImageNet-scale problems with 1000+ classes to verify practical scalability
2. **Architectural Robustness**: Test the approach across diverse architectures (Transformer-based, DenseNet, EfficientNet) to assess generalizability beyond standard CNNs
3. **Ablation Study**: Systematically remove or modify the proximal regularization term and trust-region solver parameters to quantify their individual contributions to convergence acceleration