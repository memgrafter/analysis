---
ver: rpa2
title: Entropy-Regularized Process Reward Model
arxiv_id: '2412.11006'
source_url: https://arxiv.org/abs/2412.11006
tags:
- reward
- process
- policy
- reasoning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an entropy-regularized process reward model
  (ER-PRM) for mathematical reasoning in LLMs. It addresses the limitation of standard
  RL methods that use either outcome rewards or process rewards without entropy regularization,
  which can lead to reward hacking and unstable training.
---

# Entropy-Regularized Process Reward Model

## Quick Facts
- **arXiv ID:** 2412.11006
- **Source URL:** https://arxiv.org/abs/2412.11006
- **Reference count:** 18
- **Primary result:** ER-PRM achieves 1% improvement on GSM8K and 2-3% on MATH benchmarks in best-of-N settings, with over 1% improvement in RLHF settings

## Executive Summary
This paper introduces an entropy-regularized process reward model (ER-PRM) that addresses the limitations of standard reinforcement learning methods in mathematical reasoning tasks. The core innovation is a novel reward construction formula derived from KL-regularized MDP theory that enables process rewards to be computed using only initial policy sampling, eliminating the need for simultaneous policy and reward learning. ER-PRM is evaluated on GSM8K and MATH benchmarks using both best-of-N sampling and RLHF, demonstrating consistent improvements over baseline methods. The approach balances policy optimization with entropy regularization to prevent reward hacking and unstable training.

## Method Summary
ER-PRM integrates KL-regularized Markov Decision Processes to create a process reward model that scores each intermediate reasoning step rather than just the final outcome. The key innovation is a reward calculation formula that uses the initial policy π0 to generate completions, allowing the process reward to be computed without learning the optimal policy simultaneously. The method employs entropy regularization through a KL divergence term that prevents the policy from shifting too far from its initial pretrained distribution. ER-PRM is trained using an auto-regressive strategy on approximately 260K process reward data points collected from Mistral-MetaMath-7B and DeepSeek-math-7B-instruct generators, with Llama-3.1-8B serving as the reward model base.

## Key Results
- ER-PRM achieves 1% improvement on GSM8K benchmark in best-of-N settings
- ER-PRM shows 2-3% improvement on MATH500 benchmark in best-of-N settings
- ER-PRM demonstrates over 1% improvement in RLHF settings compared to baselines
- The method maintains consistent performance across different model scales (Llama-3.1-8B and Llama-3.2-1B)
- Performance improvements are observed for both Mistral-MetaMath-7B and DeepSeek-math-7B-instruct base models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entropy regularization stabilizes policy updates by preventing the policy from shifting too far from the initial pretrained model distribution
- Mechanism: The KL divergence term in the loss function creates a soft constraint that keeps the optimized policy close to the initial policy π0, preventing reward hacking and overfitting
- Core assumption: The initial policy π0 is already a strong baseline model whose distribution should not be deviated from excessively
- Evidence anchors:
  - [abstract]: "integrates KL-regularized Markov Decision Processes (MDP) to balance policy optimization with the need to prevent the policy from shifting too far from its initial distribution"
  - [section 3.1]: "We want to find LLM, denoted by a policy π*(a|x), that optimizes π∈Π with the KL-regularized loss function L(π) = −E_x E_a∼π(·|x) [r(a,x) − 1/η ln π(a|x)/π0(a|x)]"
  - [corpus]: Weak - the corpus mentions entropy regularization but not specifically the KL-regularized MDP framework

### Mechanism 2
- Claim: Process rewards provide more granular feedback than outcome rewards, leading to better learning of multi-step reasoning
- Mechanism: By scoring each intermediate step rather than just the final answer, the model receives step-by-step guidance on which reasoning paths are correct
- Core assumption: Mathematical reasoning involves multiple steps where individual errors can be identified and corrected
- Evidence anchors:
  - [abstract]: "particularly those focusing on process rewards, which score each intermediate step rather than solely evaluating the final outcome"
  - [section 1]: "Accordingly, there are generally two types of reward models in the mathematical domain: the outcome reward model (ORM) and the process reward model (PRM). ORM provides an overall score for the quality of the final answer, while PRM assigns a score to each step in the reasoning process"
  - [corpus]: Weak - the corpus mentions process rewards but doesn't provide specific evidence about step-by-step feedback benefits

### Mechanism 3
- Claim: The entropy-regularized process reward can be computed using only the initial policy sampling, eliminating the need to learn policy and reward simultaneously
- Mechanism: The reward calculation formula allows the process reward to be derived from completions generated by the reference policy π0, rather than requiring the optimal policy π*
- Core assumption: The initial policy π0 can generate sufficiently diverse and representative completions to estimate the process reward
- Evidence anchors:
  - [section 3.2]: "The partial reward according to this formula is our definition of entropy regularized process reward. We can optimize partial reasoning step π(a[l]|x) using this process reward"
  - [section 3.2]: "An important property of this formulation is that our process reward model can be computed by using the reference policy π0 to generate the completion"
  - [corpus]: Weak - the corpus doesn't mention this specific computational advantage

## Foundational Learning

- Concept: KL-regularized Markov Decision Processes
  - Why needed here: Provides the theoretical foundation for entropy regularization in the reward formulation
  - Quick check question: What does the KL divergence term in the loss function prevent from happening during policy optimization?

- Concept: Process supervision vs outcome supervision
  - Why needed here: Understanding the difference is crucial for appreciating why step-by-step feedback is more effective for mathematical reasoning
  - Quick check question: How does process reward differ from outcome reward in terms of the granularity of feedback provided?

- Concept: Soft-max vs soft-min aggregation
  - Why needed here: The dual formulation perspective (soft-max with initial policy vs soft-min with optimal policy) is key to understanding the method's flexibility
  - Quick check question: What is the difference between the soft-max and soft-min perspectives in terms of how they evaluate possible future paths?

## Architecture Onboarding

- Component map:
  Initial policy π0 (Mistral-MetaMath-7B or DeepSeek-math-7B-instruct) -> Generator -> Reasoning chains -> Intermediate steps -> Completer -> Possible continuations -> Process reward model (ER-PRM) -> Reward scores -> RLHF pipeline

- Critical path:
  1. Generate reasoning chains using the generator
  2. For each intermediate step, use the completer to generate possible continuations
  3. Calculate process rewards using the entropy-regularized formula
  4. Train the reward model to predict these process rewards
  5. Use the trained reward model to guide policy optimization via RLHF

- Design tradeoffs:
  - Larger reward models provide better accuracy but increase computational cost
  - Higher η values provide more optimistic reward estimation but may amplify noise
  - More completions per step provide better reward estimation but increase computation time
  - Using initial policy vs optimal policy for reward calculation affects the stability and bias of the reward signal

- Failure signatures:
  - Reward hacking: reward model overfits to certain patterns and assigns high scores to incorrect reasoning
  - Mode collapse: policy converges to a narrow set of reasoning patterns due to overly restrictive entropy regularization
  - Slow convergence: if η is too large, the entropy regularization term dominates and slows learning
  - Inaccurate rewards: if the initial policy π0 cannot generate diverse enough completions, the estimated rewards will be poor

- First 3 experiments:
  1. Implement the basic process reward calculation using equation 2 and verify it produces reasonable scores on a small dataset
  2. Train a simple reward model using the generated process rewards and evaluate its performance on a held-out validation set
  3. Compare best-of-N accuracy with and without the entropy regularization term to verify the benefit of the regularization approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of η affect the performance of ER-PRM across different mathematical reasoning datasets?
- Basis in paper: [explicit] The paper discusses the impact of η on entropy regularization, noting that different values of η yield different performance outcomes on datasets like GSM8K and MATH.
- Why unresolved: While the paper explores the impact of η on performance, it does not provide a comprehensive analysis of how η affects ER-PRM across a wider variety of mathematical reasoning tasks or datasets.
- What evidence would resolve it: Empirical studies comparing ER-PRM performance with various η values across diverse mathematical reasoning datasets would provide insights into optimal η settings.

### Open Question 2
- Question: Can ER-PRM be effectively generalized to non-mathematical reasoning tasks?
- Basis in paper: [inferred] The paper focuses on mathematical reasoning, and while it mentions the potential for generalization, it does not provide experimental results or theoretical analysis for non-mathematical domains.
- Why unresolved: The current focus is solely on mathematical reasoning, and the paper does not explore or validate the effectiveness of ER-PRM in other reasoning tasks or domains.
- What evidence would resolve it: Experiments applying ER-PRM to tasks such as logical reasoning, code generation, or general language understanding would demonstrate its generalizability.

### Open Question 3
- Question: What are the computational trade-offs of using ER-PRM compared to traditional RL methods in terms of scalability and resource requirements?
- Basis in paper: [explicit] The paper mentions that ER-PRM has high computational costs and may hinder scalability, but it does not provide a detailed comparison with traditional RL methods.
- Why unresolved: While the paper acknowledges computational challenges, it lacks a detailed analysis of how ER-PRM's resource requirements compare to those of traditional RL approaches.
- What evidence would resolve it: Comparative studies measuring the computational resources, training time, and scalability of ER-PRM versus traditional RL methods would clarify the trade-offs.

### Open Question 4
- Question: How does ER-PRM handle the reward hacking problem, and what mechanisms are in place to prevent overfitting?
- Basis in paper: [explicit] The paper identifies reward hacking as a potential issue, especially with larger N, but it does not provide detailed mechanisms or solutions to prevent it.
- Why unresolved: The paper acknowledges the problem but does not explore specific strategies or mechanisms to mitigate reward hacking or overfitting in ER-PRM.
- What evidence would resolve it: Research into additional regularization techniques or architectural modifications to ER-PRM that specifically address reward hacking and overfitting would provide solutions.

## Limitations
- The computational efficiency claims lack empirical validation, with no comparative analysis of wall-clock training time or memory usage against traditional RL methods
- The absolute performance improvements are modest (1-3%) and the paper doesn't adequately address potential reward hacking or mode collapse issues
- The method's generalizability to non-mathematical reasoning tasks remains untested, limiting the scope of the claimed benefits

## Confidence

- **High confidence:** The theoretical foundation using KL-regularized MDPs is well-established in the literature and the mathematical derivations appear correct
- **Medium confidence:** The experimental results showing consistent improvements over baselines are reproducible, but the modest absolute gains and lack of comparison to state-of-the-art methods limit generalizability
- **Low confidence:** Claims about computational efficiency and the practical advantages of the single-policy approach lack empirical validation in the paper

## Next Checks

1. **Computational efficiency validation:** Measure and compare wall-clock training time and memory usage between ER-PRM and standard process reward models to empirically verify the claimed computational advantages of the single-policy approach

2. **Reward hacking stress test:** Systematically evaluate model performance at very large N values (e.g., N=128, N=256) to test for reward hacking behavior where the model might select incorrect answers with high reward scores

3. **Ablation study on η parameter:** Conduct a more comprehensive ablation study across multiple η values (e.g., 0.5, 2, 10, 50) to understand the sensitivity of performance to the entropy regularization strength and identify optimal settings for different task types