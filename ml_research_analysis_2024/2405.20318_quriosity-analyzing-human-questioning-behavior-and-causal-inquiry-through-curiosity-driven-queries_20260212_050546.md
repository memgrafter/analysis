---
ver: rpa2
title: 'Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry through
  Curiosity-Driven Queries'
arxiv_id: '2405.20318'
source_url: https://arxiv.org/abs/2405.20318
tags:
- questions
- causal
- question
- answer
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Quriosity, a dataset of 13,500 naturally\
  \ occurring human questions spanning three interaction channels: search engines,\
  \ human-to-human forums, and LLM conversations. It analyzes these questions' linguistic\
  \ properties, cognitive complexity, and user intent, revealing that curiosity-driven\
  \ queries are 38% more open-ended than curated test datasets and more evenly distributed\
  \ across Bloom\u2019s Taxonomy cognitive levels."
---

# Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry through Curiosity-Driven Queries

## Quick Facts
- arXiv ID: 2405.20318
- Source URL: https://arxiv.org/abs/2405.20318
- Reference count: 40
- Primary result: Quriosity dataset reveals 42% of naturally occurring human questions are causal, with H-to-H interactions containing highest density of causal queries

## Executive Summary
This paper introduces Quriosity, a dataset of 13,500 naturally occurring human questions collected from search engines, human-to-human forums, and LLM conversations. The study analyzes these questions' linguistic properties, cognitive complexity, and user intent, revealing that curiosity-driven queries are significantly more open-ended than curated test datasets and more evenly distributed across Bloom's Taxonomy cognitive levels. The research identifies a substantial proportion (42%) of causal questions and develops an iterative prompt improvement framework for their classification. The work lays the foundation for specialized causal reasoning systems and demonstrates that smaller efficient models can effectively classify causal questions for routing purposes.

## Method Summary
The study collected questions from three sources (H-to-SE, H-to-H, H-to-LLM) and used human annotation to validate causal question classification. An iterative prompt improvement process refined LLM-based classification through error analysis and in-context learning. Seven different models including FLAN-T5 variants and Phi-1.5 were fine-tuned for causal question classification, with evaluation across multiple performance metrics to identify efficiency-accuracy tradeoffs.

## Key Results
- 68% of curiosity-driven questions are open-ended, allowing semantically different answers
- 42% of questions in Quriosity dataset are causal in nature
- H-to-H interactions contain the highest density of causal questions
- Smaller models like FLAN-T5-Small provide good compromise between accuracy and computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Iterative prompt improvement significantly improves LLM classification performance for causal question identification
- Starting with basic prompts, iterative refinement based on human-annotated validation sets improves accuracy through techniques like in-context learning and chain-of-thought prompting
- Core assumption: LLMs can improve classification accuracy through targeted prompt engineering when given feedback on error patterns

### Mechanism 2
- Multi-source question collection captures more authentic curiosity-driven behavior than curated test datasets
- Sampling from three distinct channels captures genuine information needs rather than evaluation design, revealing patterns in cognitive complexity and open-endedness
- Core assumption: Questions asked in natural settings reflect authentic human curiosity patterns that differ systematically from test questions

### Mechanism 3
- Smaller efficient models can effectively classify causal questions with reasonable accuracy for routing purposes
- Fine-tuning compact models on labeled Quriosity dataset enables causal question classification that balances accuracy with computational efficiency
- Core assumption: Causal question classification can be performed effectively without requiring the largest LLM architectures

## Foundational Learning

- **Bloom's Taxonomy of Cognitive Complexity**: Six levels from lowest to highest cognitive complexity (Remember, Understand, Apply, Analyze, Evaluate, Create); used to analyze and compare cognitive complexity across different question sources and between causal vs non-causal questions.

- **Causal inference fundamentals**: Understanding causality is central to identifying causal questions and developing causal reasoning systems; distinguishes causal questions from correlational ones through requirement for understanding cause-effect relationships.

- **Prompt engineering techniques**: Iterative prompt improvement method relies on techniques like in-context learning and chain-of-thought prompting; chain-of-thought prompting helps improve LLM classification performance by showing reasoning steps for similar problems.

## Architecture Onboarding

- **Component map**: Data collection pipeline (aggregates questions from three sources) -> Annotation and classification pipeline (human annotation and iterative LLM prompting) -> Analysis and routing components (examines linguistic properties and deploys classification models)

- **Critical path**: Question → Source aggregation → Human annotation (validation set) → Iterative prompt improvement → Scaled classification → Analysis → Routing

- **Design tradeoffs**: System trades off between annotation accuracy (human labeling) and scalability (LLM-based scaling), and between model accuracy (larger models) and efficiency (smaller models)

- **Failure signatures**: Poor inter-annotator agreement suggests ambiguous causal definitions; low LLM classification accuracy indicates inadequate prompt engineering; unexpected distribution patterns may suggest source bias

- **First 3 experiments**:
  1. Test iterative prompt improvement on small validation set to measure performance gains per iteration
  2. Compare classification accuracy across different model sizes to identify efficiency-accuracy tradeoffs
  3. Analyze distribution of causal questions across three sources to verify H-to-H dominance finding

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can Quriosity be improved to reduce selection bias from its diverse data sources?
- **Basis in paper**: Explicit
- **Why unresolved**: Paper acknowledges significant selection bias from data sources but doesn't provide concrete mitigation methods
- **What evidence would resolve it**: Empirical studies comparing Quriosity's distribution to representative samples of human curiosity across different demographics and platforms

### Open Question 2
- **Question**: What is the optimal balance between model size and accuracy for causal question classification in practical applications?
- **Basis in paper**: Explicit
- **Why unresolved**: Paper presents seven different models but doesn't provide systematic analysis of computational cost vs accuracy tradeoff
- **What evidence would resolve it**: Comprehensive benchmarking studies measuring inference time, memory usage, and accuracy across different deployment scenarios

### Open Question 3
- **Question**: How do causal reasoning capabilities of large language models compare to human performance on the same questions?
- **Basis in paper**: Inferred
- **Why unresolved**: Paper evaluates GPT-4o performance but doesn't compare to human responses on same questions
- **What evidence would resolve it**: Head-to-head comparison studies where human experts answer same causal questions from Quriosity

### Open Question 4
- **Question**: What are the linguistic and semantic features that most reliably distinguish causal from non-causal questions across different languages?
- **Basis in paper**: Explicit
- **Why unresolved**: Paper analyzes English questions but acknowledges need for deeper semantic understanding and notes dataset only includes English queries
- **What evidence would resolve it**: Comparative linguistic analysis of causal question patterns across multiple languages

### Open Question 5
- **Question**: How does distribution of causal questions vary across different domains of human knowledge and inquiry?
- **Basis in paper**: Explicit
- **Why unresolved**: Paper identifies 42% causal questions but doesn't systematically analyze how prevalence varies by knowledge domain
- **What evidence would resolve it**: Domain-specific analysis of causal question density and types across all knowledge categories in Quriosity

## Limitations

- Iterative prompt improvement process lacks detailed quantitative metrics showing performance gains at each iteration step
- Sampling methodology for each data source isn't fully specified, raising questions about potential source bias
- Inter-annotator agreement rates not reported for full dataset, which could indicate potential subjectivity in causal question identification

## Confidence

**High Confidence**: Findings regarding multi-source question collection capturing authentic curiosity patterns (68% open-endedness, Bloom's taxonomy distribution) are well-supported by systematic comparisons with curated datasets and multiple validation metrics. Performance evaluation of efficient models for causal classification is robust with comprehensive testing across seven model architectures.

**Medium Confidence**: Identification of H-to-H interactions as having highest density of causal questions is supported by data but could benefit from deeper analysis of why this pattern emerges and whether consistent across different topic domains.

**Low Confidence**: Specific thresholds and decision boundaries used in iterative prompt improvement process are not fully detailed, making it difficult to reproduce or validate exact methodology. Generalizability of causal classification models to questions outside Quriosity dataset domain is not extensively tested.

## Next Checks

1. **Inter-annotator Reliability Test**: Conduct new round of human annotation with multiple annotators on held-out subset to measure inter-annotator agreement rates specifically for causal vs non-causal classification, and compare these rates to LLM classification performance.

2. **Source Sampling Validation**: Reconstruct sampling methodology for each of three interaction channels to verify question distributions aren't skewed by over-representation of certain topics, timeframes, or user demographics, and test whether causal question density varies across different topical domains.

3. **Cross-dataset Generalization Test**: Evaluate best-performing causal classification models from Quriosity on at least two other established question-answering datasets to assess whether patterns identified in curiosity-driven queries hold true for more general question collections.