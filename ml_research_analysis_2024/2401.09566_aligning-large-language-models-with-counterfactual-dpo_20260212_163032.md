---
ver: rpa2
title: Aligning Large Language Models with Counterfactual DPO
arxiv_id: '2401.09566'
source_url: https://arxiv.org/abs/2401.09566
tags:
- style
- prompt
- human
- counterfactual
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to align Large Language
  Models (LLMs) using counterfactual prompts within the Direct Preference Optimization
  (DPO) framework. The method effectively instills desirable behaviors, mitigates
  undesirable ones, and encourages models to disregard inappropriate instructions
  without requiring human intervention.
---

# Aligning Large Language Models with Counterfactual DPO

## Quick Facts
- arXiv ID: 2401.09566
- Source URL: https://arxiv.org/abs/2401.09566
- Reference count: 5
- One-line primary result: Novel method using counterfactual prompts within DPO effectively aligns LLMs to reduce bias and hallucinations without human feedback.

## Executive Summary
This paper introduces Counterfactual Direct Preference Optimization (DPO), a method to align large language models using styled counterfactual prompts instead of human preference labels. By fine-tuning models to prefer responses generated from a styled prompt over unstyled ones, the approach can instill desirable behaviors, mitigate biases, and encourage models to disregard inappropriate instructions. Experiments on Mistral-7B-Instruct-v0.2 demonstrate significant improvements in reducing bias and hallucinations compared to standard prompting and supervised fine-tuning, with the Contrastive DPO variant showing the most robust performance.

## Method Summary
Counterfactual DPO extends the DPO framework by using counterfactual prompts to create preference pairs without human labels. The method generates control and treatment prompts where the treatment prompt includes a style instruction (e.g., "write without bias" or "redact all entities"). During training, DPO is optimized to prefer the styled response, effectively shifting the model's default behavior toward the desired style. Contrastive DPO combines CounterfactualENC (encouraging desired behavior) and CounterfactualDIS (discouraging undesired behavior) for robust behavior modification. The approach requires only 1,000 samples and eliminates the need for human preference annotation.

## Key Results
- Mistral-7B model fine-tuned with Contrastive DPO outperformed base model on bias reduction, maintaining robust performance even when Counterfactual DPO showed decline
- On Vectara hallucination leaderboard, Contrastive DPO fine-tuned model beat Google Gemini Pro, demonstrating superior factual consistency
- Entity redaction task showed significant reduction in named entities when using CounterfactualENC DPO compared to baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual DPO can steer model behavior without human feedback by using styled prompts.
- Mechanism: The model is fine-tuned to treat a styled response as the preferred outcome relative to an unstyled one, thereby shifting the model's default behavior toward the desired style.
- Core assumption: The styled prompt reliably produces the desired behavior, so the unstyled response can serve as a valid "control."
- Evidence anchors:
  - [abstract] "This method effectively instils desirable behaviour, mitigates undesirable ones, and encourages the model to disregard inappropriate instructions."
  - [section] "We effectively fool DPO into optimising assuming the treatment generation was created with the reference prompt x, nudging the default style of an LLM towards the desired style."
- Break condition: If the model cannot reliably follow the styled instruction, the counterfactual assumption fails and the method loses effectiveness.

### Mechanism 2
- Claim: Contrastive DPO combines positive and negative style signals to robustly shift model behavior.
- Mechanism: By training on both desired and undesired styled responses, the model learns both to increase the likelihood of the desired style and to suppress the undesired style.
- Core assumption: The model can generate both the desired and undesired styles reliably so that contrastive examples are meaningful.
- Evidence anchors:
  - [section] "Contrastive DPO is the combination of CounterfactualENC and CounterfactualDIS DPO... This is useful in the case where we both want to encourage certain behaviour and discourage others."
  - [section] "Notably, in our bias reduction experiment, Contrastive DPO demonstrated resilience, maintaining robust performance even when Counterfactual DPO (ENC) showed a decline."
- Break condition: If either styled response is unreliable, the contrastive signal becomes noisy and performance degrades.

### Mechanism 3
- Claim: Instruction negation can train models to ignore harmful or unwanted instructions.
- Mechanism: The control prompt is unstyled, but the treatment prompt contains the unwanted instruction. DPO is trained to prefer the control output, teaching the model to disregard the harmful instruction.
- Core assumption: The model's default behavior without the harmful instruction is acceptable and should be reinforced.
- Evidence anchors:
  - [section] "This technique aims to orient the model's outputs towards those that would emerge in the absence of the negative instruction."
  - [section] "In this case DPO is trained to assume the responses have been generated from the styled xt prompt."
- Break condition: If the model's default behavior without the harmful instruction is also undesirable, the method will not produce the intended safe behavior.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: This paper builds directly on DPO; understanding the loss function and how it replaces RLHF is essential.
  - Quick check question: What is the DPO loss term M(x, yw, yl) in terms of the model and reference policies?

- Concept: Counterfactual prompting
  - Why needed here: The core novelty is using counterfactual prompts within DPO; understanding what a counterfactual prompt is and how it differs from normal prompts is key.
  - Quick check question: In the counterfactual setup, which response is considered "preferred" when using CounterfactualDIS DPO?

- Concept: Supervised fine-tuning vs DPO vs RLHF
  - Why needed here: The paper compares its approach to SFT and RLHF; knowing the differences in objectives and data requirements is important for understanding the method's advantages.
  - Quick check question: What is the main difference between how DPO and RLHF use human preference data?

## Architecture Onboarding

- Component map:
  Base LLM -> Prompt generator -> DPO fine-tuning loop -> Evaluation harness

- Critical path:
  1. Load base model and tokenizer.
  2. Generate prompt-response pairs for control, treatment, and negative styles.
  3. Construct DPO training dataset with paired responses.
  4. Fine-tune base model using DPO loss for N epochs.
  5. Evaluate on target benchmarks.

- Design tradeoffs:
  - Prompt styling vs human annotation: Styling removes human effort but relies on the model's ability to follow the style instruction.
  - Contrastive vs single-sided: Contrastive is more robust but requires generating two styled responses; single-sided is simpler but less stable.
  - Dataset size: The paper uses 1,000 samples; larger datasets may improve stability but increase compute cost.

- Failure signatures:
  - Poor style adherence: Model outputs do not match the desired style; check prompt generation and model capability.
  - Overfitting to style: Model becomes too rigid and loses general capability; monitor performance on held-out reasoning tasks like Hellaswag.
  - Degraded instruction-following: Model ignores instructions entirely; check balance of control vs styled prompts in training data.

- First 3 experiments:
  1. Entity redaction test: Use CNN/Dailymail dataset, train with CounterfactualENC DPO to suppress names in summaries, evaluate entity count reduction.
  2. Bias reduction test: Use OpenOrca questions with BBQ benchmark, train with Contrastive DPO to reduce biased responses, evaluate bias score change.
  3. Hallucination reduction test: Use OpenOrca questions with Vectara benchmark, train with Contrastive DPO to ground responses to context, evaluate factual consistency rate.

## Open Questions the Paper Calls Out
None

## Limitations

- Style adherence reliability: The method assumes the LLM can reliably follow styled instructions; if the model fails to adhere to the style, the counterfactual DPO assumption breaks down.
- Evaluation scope and metrics: Bias and hallucination evaluations use specific benchmarks but don't test broader generalization across complex scenarios.
- Scalability and dataset dependence: The approach uses only 1,000 training samples, raising questions about performance at scale and dependence on prompt generator quality.

## Confidence

**High confidence**: The mathematical formulation of Counterfactual DPO is sound and extends DPO in a logically consistent way. The experimental results showing improvements over the base model on specific benchmarks are reproducible.

**Medium confidence**: Claims about outperforming Gemini Pro on the Vectara hallucination leaderboard are supported by reported metrics but compared against a single proprietary model rather than broader baselines.

**Low confidence**: Generalizability of the approach to completely different styles remains untested, and long-term stability of fine-tuned behaviors and potential catastrophic forgetting is not evaluated.

## Next Checks

1. **Cross-style generalization test**: Apply Counterfactual DPO to three new styles (e.g., formal tone, simplified language for children, and technical jargon) using the same Mistral-7B model, then evaluate style adherence and general capability retention on HELM benchmarks.

2. **Ablation on dataset size**: Train models with Counterfactual DPO using 100, 1,000, and 10,000 samples on the entity redaction task, measuring both redaction performance and HELM score degradation to establish the relationship between dataset size and effectiveness.

3. **Long-term behavior stability**: After fine-tuning with Contrastive DPO for bias reduction, evaluate the model's bias scores immediately after training, after 1 week of regular use (simulated by continued pretraining on general data), and after instruction tuning, to measure retention of aligned behaviors.