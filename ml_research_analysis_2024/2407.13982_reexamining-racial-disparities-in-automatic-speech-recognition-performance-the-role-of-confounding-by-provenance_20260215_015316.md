---
ver: rpa2
title: 'Reexamining Racial Disparities in Automatic Speech Recognition Performance:
  The Role of Confounding by Provenance'
arxiv_id: '2407.13982'
source_url: https://arxiv.org/abs/2407.13982
tags:
- coraal
- speech
- performance
- subsets
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study reexamines racial disparities in ASR performance, focusing
  on the role of confounding by provenance. The authors hypothesize that differences
  in audio recording quality within the CORAAL dataset may impact ASR accuracy, leading
  to disparities in performance.
---

# Reexamining Racial Disparities in Automatic Speech Recognition Performance: The Role of Confounding by Provenance

## Quick Facts
- arXiv ID: 2407.13982
- Source URL: https://arxiv.org/abs/2407.13982
- Authors: Changye Li; Trevor Cohen; Serguei Pakhomov
- Reference count: 10
- Primary result: Audio recording quality differences act as a confounding variable in ASR performance disparity studies

## Executive Summary
This study reexamines racial disparities in ASR performance by investigating the role of audio recording quality as a potential confounding factor. The authors evaluate the Whisper ASR model on the CORAAL dataset and compare its performance across different geographical subsets, while also analyzing the impact of recording quality by comparing with documentary film interviews. Their findings reveal significant dialectal variation across neighboring AAE communities and confirm that recording quality differences substantially impact ASR accuracy, creating a "confounding by provenance" effect that may explain apparent racial disparities in ASR performance.

## Method Summary
The study evaluates the Whisper ASR model on the CORAAL 2021 dataset, comparing WER across geographical subsets and testing fine-tuning on CORAAL subsets. Researchers train character-level language models on each subset to measure dialectal variation through perplexity scores. They also compare ASR performance between CORAAL data and documentary film interviews from the same community to isolate recording quality effects. The study employs both standard WER metrics and cosine similarity measures to assess semantic accuracy versus orthographic differences.

## Key Results
- Audio recording quality differences significantly impact ASR accuracy, creating spurious correlations between race/dialect and ASR performance
- Significant dialectal variation exists even across neighboring AAE communities, affecting ASR performance
- WER alone may underestimate ASR capabilities as utterances with high WER can still have moderate to high semantic similarity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Audio recording quality differences act as a confounding variable in ASR performance disparity studies.
- Mechanism: Lower recording quality increases word error rate (WER) independently of dialectal variation, creating spurious correlations between race/dialect and ASR accuracy.
- Core assumption: CORAAL's heterogeneous recording practices introduce measurable quality differences that affect ASR accuracy.
- Evidence anchors:
  - [abstract] "differences in audio recording practices within the dataset have a significant impact on ASR accuracy resulting in a 'confounding by provenance' effect"
  - [section] "variability in audio quality resulting from variable recording protocols and equipment may act as a confounding variable"
  - [corpus] Weak - corpus only shows related papers on ASR bias but not specifically on recording quality as confounder

### Mechanism 2
- Claim: Dialectal variation between neighboring AAE communities contributes to ASR performance disparities.
- Mechanism: ASR models trained on general data perform worse on speech patterns that differ from their training distribution, even when recording quality is controlled.
- Core assumption: African American English exhibits substantial linguistic diversity across geographically proximate communities.
- Evidence anchors:
  - [abstract] "significant dialectal variation even across neighboring communities, and worse ASR performance on AAE"
  - [section] "character-level LMs trained on CORAAL subsets collected in urban areas... generally produced lower PPLs relative to subsets obtained from rural areas"
  - [corpus] Moderate - corpus shows papers on regional dialect bias in ASR but limited evidence on AAE dialect diversity specifically

### Mechanism 3
- Claim: Standard WER metric underestimates ASR semantic accuracy, especially for disfluent speech.
- Mechanism: WER penalizes orthographic differences and disfluencies that don't affect semantic meaning, while cosine similarity better captures semantic equivalence.
- Core assumption: Semantic similarity correlates with user comprehension and downstream task performance better than WER for some applications.
- Evidence anchors:
  - [abstract] "over-reliance on WER alone may present an overly pessimistic perspective on ASR capabilities"
  - [section] "we observed a strong negative correlation (Spearman's Ï = -0.88) between WERs and cosine similarity scores"
  - [corpus] Weak - corpus shows general ASR evaluation papers but limited evidence on semantic vs. orthographic accuracy tradeoffs

## Foundational Learning

- Concept: Confounding variables in observational studies
  - Why needed here: To understand how recording quality creates spurious associations between dialect and ASR performance
  - Quick check question: What three conditions must be met for a variable to be considered a confounder?

- Concept: Dialectal variation and language modeling
  - Why needed here: To grasp why different AAE communities show varying ASR performance and how LMs capture linguistic diversity
  - Quick check question: How does perplexity relate to the similarity between training and test language patterns?

- Concept: ASR evaluation metrics (WER vs. semantic similarity)
  - Why needed here: To understand the limitations of WER and why semantic similarity provides complementary information
  - Quick check question: Why might two utterances with different WER values have similar semantic similarity scores?

## Architecture Onboarding

- Component map: CORAAL dataset loading -> text preprocessing -> audio preprocessing -> train/test split -> Whisper inference -> WER calculation -> cosine similarity computation -> character-level LM training -> statistical analysis

- Critical path:
  1. Load and preprocess CORAAL data (text and audio)
  2. Train/test split and subset creation
  3. Run Whisper inference on test sets
  4. Calculate WER and cosine similarity
  5. Train character-level LMs for linguistic analysis
  6. Analyze results for confounding effects and dialectal variation

- Design tradeoffs:
  - Using long-form vs. utterance-level evaluation: Long-form may capture context but increases computational cost
  - Disabling Whisper's internal text normalization: Provides consistency but may miss useful normalization for evaluation
  - Character-level vs. word-level LMs: Character-level handles OOV better but may miss higher-level linguistic patterns

- Failure signatures:
  - High WER with high cosine similarity: Indicates orthographic/structural differences rather than semantic errors
  - Inconsistent WER across subsets with similar PPL: Suggests recording quality issues rather than linguistic factors
  - Fine-tuning improving WER on training subset but not generalizing: Indicates overfitting to specific linguistic patterns

- First 3 experiments:
  1. Compare WER on analog vs. digital recordings within same dialect region to isolate recording quality effects
  2. Train Whisper on documentary data and test on CORAAL PRV subset to measure transferability
  3. Compute correlation between audio quality metrics (SNR, etc.) and WER across all subsets to quantify confounding effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between audio recording quality and ASR performance, and how can this relationship be quantified and controlled for in future studies?
- Basis in paper: [explicit] The authors identify audio recording quality as a significant confounding factor affecting ASR performance on CORAAL data and call for further investigation to disentangle its effects from linguistic diversity.
- Why unresolved: The study provides preliminary evidence of the impact of recording quality but does not establish a quantitative model or method for adjusting ASR results based on recording quality differences.
- What evidence would resolve it: Experimental studies that systematically vary recording quality while controlling for dialect and language features, potentially using a within-subjects design where the same speakers are recorded under different quality conditions.

### Open Question 2
- Question: How generalizable are the observed dialectal variations within AAE to other regional variants, and what are the implications for ASR model training and evaluation?
- Basis in paper: [explicit] The authors observe significant dialectal variation even across neighboring AAE communities and note that fine-tuning on CORAAL subsets did not generalize well to other subsets, suggesting limited generalizability of dialect-specific adaptations.
- Why unresolved: The study focuses on a specific subset of AAE variants and does not explore the full range of dialectal diversity within AAE or test generalization across a broader range of regional variants.
- What evidence would resolve it: Comprehensive studies examining ASR performance across a wider range of AAE variants, potentially using geographically diverse datasets and evaluating model performance on unseen dialects.

### Open Question 3
- Question: Can alternative evaluation metrics beyond WER better capture the semantic accuracy of ASR systems, particularly for applications where meaning comprehension is more important than verbatim transcription?
- Basis in paper: [explicit] The authors compare WER with cosine similarity between reference and hypothesis utterance embeddings, finding that utterances with high WER can still have moderate to high semantic similarity, suggesting WER may underestimate ASR accuracy in capturing meaning.
- Why unresolved: While the study provides preliminary evidence of the limitations of WER, it does not propose or validate a comprehensive alternative evaluation framework that incorporates both form and meaning-based metrics.
- What evidence would resolve it: Development and validation of multi-faceted evaluation metrics that combine WER with semantic similarity measures, tested across various ASR applications and domains to assess their practical utility and effectiveness.

## Limitations

- The heterogeneous recording quality within CORAAL introduces substantial noise that may not generalize to other dialect datasets
- The Whisper model's large size and pre-training on diverse data create additional complexity when interpreting fine-tuning results
- The semantic similarity metric, while innovative, has not been validated against human judgments of ASR quality in real-world applications

## Confidence

- **Confounding by recording quality**: Medium - The evidence is strong within the CORAAL dataset, but the generalizability to other ASR bias studies remains uncertain. The documentary comparison provides useful but limited evidence.
- **Dialectal variation across neighboring communities**: High - The language modeling results showing distinct PPL patterns across geographical subsets are robust and well-supported by the data.
- **Semantic similarity as complementary metric**: Medium - While the strong correlation with WER is promising, the practical significance for real-world ASR deployment needs further validation.

## Next Checks

1. **Quantify audio quality impact**: Measure objective audio quality metrics (SNR, dynamic range, background noise levels) across CORAAL subsets and compute their correlation with WER. This would provide direct evidence of recording quality's confounding effect.

2. **Cross-dialect transferability test**: Train Whisper on one AAE subset (e.g., urban) and test on another (e.g., rural), then repeat with documentary data as training set. This would isolate dialectal variation from recording quality effects.

3. **Human evaluation validation**: Conduct human transcription and comprehension studies on CORAAL utterances with varying WER and semantic similarity scores to validate whether semantic similarity better predicts human understanding than WER alone.