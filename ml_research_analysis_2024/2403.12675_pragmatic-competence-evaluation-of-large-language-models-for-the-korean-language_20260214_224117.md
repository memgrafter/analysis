---
ver: rpa2
title: Pragmatic Competence Evaluation of Large Language Models for the Korean Language
arxiv_id: '2403.12675'
source_url: https://arxiv.org/abs/2403.12675
tags:
- llms
- language
- arxiv
- pragmatic
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates how well large language models (LLMs) grasp\
  \ context-dependent expressions in Korean. The authors test five LLMs\u2014GPT-4,\
  \ HyperCLOVA X, Gemini-Pro, GPT-3.5, and LDCC-Solar\u2014using both multiple-choice\
  \ questions (MCQs) and open-ended questions (OEQs) aligned with Gricean maxims of\
  \ conversation."
---

# Pragmatic Competence Evaluation of Large Language Models for the Korean Language
## Quick Facts
- arXiv ID: 2403.12675
- Source URL: https://arxiv.org/abs/2403.12675
- Reference count: 40
- Key outcome: GPT-4 achieved highest scores (81.11% MCQs, 85.69% OEQs) in Korean pragmatic competence evaluation

## Executive Summary
This paper evaluates how well large language models (LLMs) understand context-dependent expressions in Korean through the lens of Gricean maxims of conversation. The authors test five LLMs—GPT-4, HyperCLOVA X, Gemini-Pro, GPT-3.5, and LDCC-Solar—using both multiple-choice questions (MCQs) and open-ended questions (OEQs) designed to assess pragmatic competence. GPT-4 emerged as the top performer with 81.11% accuracy in MCQs and 85.69% in OEQs, followed closely by HyperCLOVA X. The study demonstrates that few-shot learning improves performance, while Chain-of-Thought prompting often leads to overly literal interpretations that limit pragmatic inference. The findings underscore the need for LLMs to understand and generate language that aligns with human communicative norms in Korean.

## Method Summary
The authors created a benchmark dataset of Korean pragmatic scenarios aligned with Gricean maxims of conversation, including quality (truthfulness), quantity (appropriateness), relation (relevance), and manner (clarity). They evaluated five LLMs—GPT-4, HyperCLOVA X, Gemini-Pro, GPT-3.5, and LDCC-Solar—using both multiple-choice questions and open-ended questions. The evaluation tested the impact of different prompting strategies, including few-shot learning and Chain-of-Thought prompting. Performance was measured by comparing model responses against expert-annotated answer keys, with separate scoring for MCQs (exact match) and OEQs (qualitative assessment of pragmatic appropriateness).

## Key Results
- GPT-4 achieved the highest scores: 81.11% accuracy in MCQs and 85.69% in OEQs
- Few-shot learning consistently improved LLM performance across all models
- Chain-of-Thought prompting often led to overly literal interpretations, reducing pragmatic inference accuracy
- HyperCLOVA X performed second-best, demonstrating strong Korean language capabilities

## Why This Works (Mechanism)
The evaluation framework leverages Gricean maxims as a principled approach to assess whether LLMs can understand context-dependent meanings beyond literal interpretations. By testing both MCQs and OEQs, the study captures different dimensions of pragmatic competence—recognition of appropriate responses versus generation of contextually appropriate utterances. The use of few-shot learning provides LLMs with relevant examples to adapt their responses to pragmatic contexts, while the comparison with Chain-of-Thought prompting reveals that step-by-step reasoning may actually hinder the intuitive understanding required for pragmatic inference.

## Foundational Learning
**Gricean Maxims**: Conversational rules (quality, quantity, relation, manner) that govern cooperative communication; needed to create principled test scenarios that assess whether models understand human communication norms; quick check: can the model distinguish between literal and implied meanings in context?
**Pragmatic Competence**: Ability to use language appropriately in context, including understanding implicatures and indirect speech acts; needed to evaluate real-world language understanding beyond grammar and semantics; quick check: does the model generate contextually appropriate responses in ambiguous situations?
**Few-shot Learning**: Prompting technique where models are given examples before the target task; needed to help models adapt to specific pragmatic contexts without extensive retraining; quick check: does providing 3-5 examples improve performance compared to zero-shot prompting?
**Chain-of-Thought Prompting**: Method encouraging step-by-step reasoning; needed to test whether explicit reasoning helps or hinders pragmatic inference; quick check: does detailed reasoning lead to more accurate or more literal interpretations?

## Architecture Onboarding
**Component Map**: Test Data Creation -> LLM Evaluation -> Performance Analysis -> Comparative Assessment
**Critical Path**: The evaluation pipeline follows: (1) Design Korean pragmatic scenarios based on Gricean maxims, (2) Create MCQs and OEQs with answer keys, (3) Apply different prompting strategies (few-shot, CoT), (4) Score model responses, (5) Analyze performance differences across models and prompt types
**Design Tradeoffs**: MCQs provide objective scoring but may miss nuanced understanding; OEQs capture richer pragmatic competence but require subjective evaluation; few-shot learning improves performance but may not reflect general capabilities; CoT prompting aids reasoning but can lead to over-literal interpretations
**Failure Signatures**: Poor performance on scenarios requiring indirect speech act interpretation; inability to distinguish between literal and implied meanings; over-reliance on surface-level semantic matching; failure to maintain conversational coherence across exchanges
**3 First Experiments**: 1) Test zero-shot vs few-shot prompting on the same models to quantify the learning effect, 2) Compare performance on quality vs quantity maxims to identify specific pragmatic weaknesses, 3) Evaluate model responses with native Korean speakers to validate automated scoring

## Open Questions the Paper Calls Out
None

## Limitations
- Limited diversity of Korean pragmatic scenarios, focusing primarily on Gricean maxims while potentially overlooking politeness strategies and cultural context-specific norms
- Manual creation of test items introduces subjectivity in question design and answer key validation
- Only five LLMs tested, leaving gaps in understanding how smaller or specialized models perform on similar tasks
- Performance differences may be influenced by training data composition, tokenization differences, and instruction tuning approaches not fully controlled across models

## Confidence
High confidence: GPT-4's superior performance across both MCQ and OEQ formats, and the general observation that few-shot learning improves pragmatic competence.
Medium confidence: The claim that Chain-of-Thought prompting leads to overly literal interpretations, as this effect may vary depending on prompt engineering and specific pragmatic contexts tested.
Medium confidence: The assertion that pragmatic competence must align with "human communicative norms," as this normative claim depends on which specific norms are prioritized and how they're operationalized.

## Next Checks
1. Replicate the evaluation using a larger, more diverse set of Korean pragmatic scenarios including politeness levels, honorifics usage, and culturally-specific contextual cues to test generalization beyond Gricean maxims
2. Test additional models including open-source Korean-specific LLMs and smaller parameter models to establish performance baselines and identify whether observed patterns hold across the broader LLM landscape
3. Conduct human evaluation studies where native Korean speakers rate model responses for pragmatic appropriateness in context, comparing these ratings against automated scoring to validate the evaluation methodology's alignment with human judgment