---
ver: rpa2
title: On Instabilities of Unsupervised Denoising Diffusion Models in Magnetic Resonance
  Imaging Reconstruction
arxiv_id: '2406.16983'
source_url: https://arxiv.org/abs/2406.16983
tags:
- reconstruction
- diffusion
- worst-case
- perturbations
- perturbation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the vulnerability of denoising diffusion
  models for MRI reconstruction to adversarial perturbations. The authors found that
  even tiny worst-case perturbations transferred from surrogate models can cause these
  models to generate fake tissue structures that may mislead clinicians.
---

# On Instabilities of Unsupervised Denoising Diffusion Models in Magnetic Resonance Imaging Reconstruction

## Quick Facts
- arXiv ID: 2406.16983
- Source URL: https://arxiv.org/abs/2406.16983
- Reference count: 17
- Primary result: Adversarial perturbations transfer between MRI reconstruction models, with diffusion models producing Gaussian noise-like artifacts distinct from anatomical distortions in supervised models

## Executive Summary
This study investigates the vulnerability of denoising diffusion models to adversarial perturbations in MRI reconstruction tasks. The authors demonstrate that both supervised models (ResUnet++ and i-RIM) and unsupervised diffusion models are susceptible to adversarial attacks in both white-box and black-box settings. They find that adversarial perturbations can effectively transfer between independently trained models, similar to transferability observed in classification tasks. The study reveals that diffusion models exhibit Gaussian noise-like artifacts when attacked, which are more challenging to detect compared to the anatomical structure distortions seen in supervised models. These findings highlight the need for improved robustness in diffusion-based reconstruction models for clinical applications.

## Method Summary
The study trains supervised reconstruction models (ResUnet++ and i-RIM) and an unsupervised diffusion model on MRI data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Adversarial perturbations are generated using projected gradient descent (PGD) attacks in the k-space domain. The authors evaluate model robustness using Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR) metrics, comparing artifact types between supervised and unsupervised models. They investigate both white-box attacks (where the attacker has full knowledge of the target model) and black-box attacks (where perturbations are generated against a surrogate model and transferred to the target).

## Key Results
- Adversarial perturbations transfer between independently trained MRI reconstruction models, affecting both supervised and unsupervised models
- Diffusion models produce Gaussian noise-like artifacts under adversarial attacks, distinct from anatomical distortions in supervised models
- Even minimal perturbations (ϵ = 0.01) crafted from surrogate models can significantly distort tissue structures in diffusion model reconstructions
- Black-box attacks demonstrate that perturbations generated against one model can effectively fool other models, highlighting shared vulnerabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models trained like denoisers exhibit unique adversarial noise artifacts distinct from supervised models
- Mechanism: When diffusion models are adversarially perturbed, they generate Gaussian noise-like artifacts because they're trained to denoise rather than to preserve anatomical structure
- Core assumption: The diffusion model's denoising training objective makes it fundamentally different in how it responds to adversarial perturbations compared to supervised reconstruction models
- Evidence anchors:
  - [abstract]: "at larger perturbation strengths, diffusion models exhibit Gaussian noise-like artifacts that are distinct from those observed in supervised models and are more challenging to detect"
  - [section]: "the unsupervised diffusion model is also susceptible to worst-case distribution shifts in the form of adversarial perturbations. Even with a minimal adversarial perturbation of ϵ = 0.01 crafted from ResUnet++ parameters, the diffusion model distorts the gray matter structure in its reconstruction"
  - [corpus]: Weak evidence - corpus doesn't directly address diffusion model noise artifacts, only mentions related diffusion model applications

### Mechanism 2
- Claim: Adversarial perturbations can transfer between independently trained reconstruction models
- Mechanism: The same k-space perturbations that fool one model can fool other models because they exploit shared vulnerabilities in how different architectures handle undersampled k-space data
- Core assumption: Different reconstruction models share common weaknesses in how they interpret corrupted k-space data, allowing perturbation transferability
- Evidence anchors:
  - [abstract]: "worst-case perturbations can effectively transfer between independently trained regression models, similar to the transferability observed in classification tasks"
  - [section]: "Empirical evidence for the transferability of adversarial examples has been investigated in classification applications, but rarely demonstrated in regression tasks such as image reconstruction. In Fig. 2a, we explore a black-box scenario where adversarial perturbations are generated against a surrogate model, such as an i-RIM, rather than the actual models used for reconstruction, such as diffusion"
  - [corpus]: Weak evidence - corpus neighbors focus on diffusion applications but don't discuss perturbation transferability

### Mechanism 3
- Claim: Tiny perturbations in k-space can create large reconstruction errors in image space
- Mechanism: The inverse problem of MRI reconstruction is ill-posed, so small errors in k-space measurements can be amplified into significant structural distortions in the reconstructed image
- Core assumption: The relationship between k-space perturbations and image reconstruction errors is nonlinear and can be exploited by carefully crafted adversarial perturbations
- Evidence anchors:
  - [section]: "our worst-case noise is designed to be small in k-space but can induce a significant mismatch in the slice recovered by the undersampled version of the perturbed ground truth"
  - [section]: "Finding an adversarial direction δ ∈ Rm in the measurement domain can be viewed as solving an optimization with the following objective: max δ:∥δ∥≤ϵ Ey h ∥f (y; θ) − f (y + δ; θ)∥2 2 i"
  - [corpus]: Weak evidence - corpus doesn't address k-space perturbation effects on reconstruction

## Foundational Learning

- Concept: MRI reconstruction as an inverse problem
  - Why needed here: Understanding that MRI reconstruction from undersampled k-space data is inherently ill-posed explains why small perturbations can have large effects
  - Quick check question: Why does undersampling k-space data make MRI reconstruction challenging from a mathematical perspective?

- Concept: Denoising diffusion models and score matching
  - Why needed here: The study compares diffusion models trained like denoisers against supervised models, so understanding diffusion model training is crucial
  - Quick check question: How does the training objective of a denoising diffusion model differ from that of a supervised reconstruction model?

- Concept: Adversarial machine learning and perturbation transferability
  - Why needed here: The paper's core contribution is demonstrating that adversarial perturbations transfer between models, which requires understanding of black-box vs white-box attacks
  - Quick check question: What makes adversarial perturbations "transferable" between independently trained models?

## Architecture Onboarding

- Component map: ADNI dataset -> k-space undersampling -> reconstruction models (ResUnet++, i-RIM, Diffusion) -> PGD-based perturbation generation -> evaluation (SSIM/PSNR, visual inspection)

- Critical path: Train reconstruction models on ADNI dataset -> Generate adversarial perturbations using PGD on one model -> Apply perturbations to test data -> Evaluate reconstruction quality across all models -> Compare artifact types between supervised and unsupervised models

- Design tradeoffs: Acceleration factor (8x) vs reconstruction quality -> Model complexity (ResUnet++ vs i-RIM vs diffusion) vs robustness -> White-box attack capability vs real-world black-box vulnerability -> Detection difficulty of Gaussian noise artifacts vs anatomical distortions

- Failure signatures: Supervised models: Anatomical structure distortions, loss of tissue boundaries -> Diffusion models: Gaussian noise-like artifacts, loss of anatomical coherence -> All models: SSIM/PSNR degradation proportional to perturbation strength

- First 3 experiments: 1) Train all three reconstruction models on the same ADNI dataset and compare baseline SSIM/PSNR -> 2) Generate white-box perturbations using i-RIM and test on all three models -> 3) Generate black-box perturbations using ResUnet++ and test on diffusion and i-RIM models

## Open Questions the Paper Calls Out

- Question: How do the Gaussian noise-like artifacts produced by adversarial perturbations in diffusion models affect clinical diagnostic accuracy and reliability?
  - Basis in paper: [explicit] The paper states that diffusion models exhibit Gaussian noise-like artifacts at larger perturbation strengths that are distinct from those observed in supervised models and are more challenging to detect.
  - Why unresolved: The paper demonstrates the existence of these artifacts but does not explore their impact on clinical decision-making or diagnostic accuracy.
  - What evidence would resolve it: A clinical study comparing diagnostic accuracy and reliability when clinicians interpret images reconstructed by diffusion models with and without adversarial perturbations.

- Question: Can classical regularization techniques like total variation regularization improve the robustness of diffusion models against adversarial perturbations?
  - Basis in paper: [explicit] The paper hypothesizes that classical regularization techniques might offer better robustness but does not test this hypothesis.
  - Why unresolved: The paper only presents the hypothesis without experimental validation of the effectiveness of these techniques.
  - What evidence would resolve it: Experiments comparing the robustness of diffusion models with and without classical regularization techniques against adversarial perturbations.

- Question: Are there specific architectural modifications or training strategies that can make diffusion models more resistant to adversarial perturbations?
  - Basis in paper: [explicit] The paper highlights the vulnerability of diffusion models but does not explore potential architectural or training modifications to enhance robustness.
  - Why unresolved: The paper identifies the problem but does not investigate solutions or modifications to address the vulnerability.
  - What evidence would resolve it: Experiments testing various architectural modifications or training strategies to improve the robustness of diffusion models against adversarial perturbations.

## Limitations

- Limited generalizability to other MRI datasets and acquisition protocols beyond the ADNI dataset
- No exploration of potential defenses or mitigation strategies against adversarial attacks
- White-box attack scenarios may overestimate real-world vulnerability compared to constrained black-box scenarios

## Confidence

The findings regarding diffusion model vulnerability to adversarial perturbations are **High confidence** based on systematic experimentation across multiple attack scenarios. The transferability of perturbations between independently trained models is supported by both theoretical analysis and empirical validation, though this represents **Medium confidence** due to the limited exploration of different model architectures and training strategies. The characterization of Gaussian noise-like artifacts in diffusion models versus anatomical distortions in supervised models is **High confidence** based on visual inspection and quantitative metrics, but the clinical significance of these artifacts requires further investigation.

## Next Checks

1. Test transferability of adversarial perturbations across different MRI datasets and acquisition protocols to assess generalizability
2. Evaluate the clinical relevance of diffusion model artifacts by conducting radiologist studies to determine detection difficulty and diagnostic impact
3. Investigate potential defenses such as adversarial training, input preprocessing, or architectural modifications to improve model robustness against perturbations