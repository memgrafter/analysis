---
ver: rpa2
title: 'Asking Again and Again: Exploring LLM Robustness to Repeated Questions'
arxiv_id: '2412.07923'
source_url: https://arxiv.org/abs/2412.07923
tags:
- question
- repetition
- questions
- prompt
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the impact of question repetition within\
  \ prompts on the performance of large language models (LLMs). We evaluate five recent\
  \ LLMs\u2014including GPT-4o-mini, DeepSeek-V3, and smaller open-source models\u2014\
  on three reading comprehension datasets under different prompt settings, varying\
  \ question repetition levels (1, 3, or 5 times per prompt)."
---

# Asking Again and Again: Exploring LLM Robustness to Repeated Questions

## Quick Facts
- arXiv ID: 2412.07923
- Source URL: https://arxiv.org/abs/2412.07923
- Reference count: 14
- Models tested showed no significant performance difference when questions were repeated 1, 3, or 5 times

## Executive Summary
This study investigates whether repeating questions within prompts affects large language model performance on reading comprehension tasks. Testing five recent LLMs across three datasets and four prompt configurations, the researchers found that question repetition can increase accuracy by up to 6%, but these differences are not statistically significant. The results demonstrate that LLMs are robust to repeated input structures, suggesting that repetition alone does not meaningfully impact output quality or encourage models to focus more on repeated information.

## Method Summary
The study evaluates five LLMs (GPT-4o-mini, DeepSeek-V3, Llama-3.1, Mistral 7B, Phi-4) on three reading comprehension datasets (SQuAD, HotPotQA, Natural Questions) using 500 sampled questions each. Four prompt configurations are tested: Open-Book (context + question), Closed-Book (question only), Question-Context-Question (Q-C-Q), and Paraphrasing. Question repetition levels of 1, 3, and 5 times are applied within each prompt. Accuracy is measured through substring matching, checking whether gold answers appear in model outputs.

## Key Results
- Question repetition can increase accuracy by up to 6% but differences are not statistically significant
- Results hold across all tested models, settings, and datasets
- LLMs show robustness to repeated input structures regardless of prompt configuration
- Statistical tests (Friedman test) confirm no significant differences between repetition levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Repeating a question does not increase model focus on key elements.
- Mechanism: The model's internal attention and representation mechanisms are not influenced by redundant phrasing in the input prompt.
- Core assumption: LLMs encode semantic meaning of the question independently of how many times it appears.
- Evidence anchors:
  - [abstract] "These findings provide insights into prompt design and LLM behavior, suggesting that repetition alone does not significantly impact output quality."
  - [section 3.1] "Our results show that repeating questions have no significant effect on LLM performance across different settings."
- Break condition: If the model architecture changed to include explicit repetition-aware attention, or if prompting strategy explicitly required restating the question.

### Mechanism 2
- Claim: Statistical significance of performance differences across repetition levels is absent.
- Mechanism: Variance in accuracy across repetitions is within the margin of error, so the observed differences are not meaningful.
- Core assumption: Measurement noise and model stochasticity dominate any small signal from repetition.
- Evidence anchors:
  - [section 3.1] "Statistical tests... confirm that the differences between repeating questions 1, 3, or 5 times were not significant."
- Break condition: If sample size increased dramatically or measurement precision improved enough to detect small effects.

### Mechanism 3
- Claim: Models are robust to redundant input structures.
- Mechanism: The underlying transformer architecture does not allocate additional computational resources or attention to repeated text.
- Core assumption: Attention heads treat repeated tokens as duplicates rather than emphasizing them.
- Evidence anchors:
  - [abstract] "This finding suggests that repetition alone does not significantly impact output quality, highlighting the robustness of LLMs to repeated input structures."
- Break condition: If the model were fine-tuned to be repetition-sensitive or if prompt engineering explicitly asked for restatement.

## Foundational Learning

- Concept: Substring matching accuracy metric
  - Why needed here: The study uses substring matching to evaluate if gold answers appear in the model output; understanding this metric is essential to interpret results.
  - Quick check question: If the gold answer is "Paris" and the model outputs "The capital is Paris, France," does substring matching count this as correct?

- Concept: Statistical significance testing (Friedman test)
  - Why needed here: The study uses non-parametric Friedman test because data is not normally distributed; knowing this ensures correct interpretation of "no significant effect."
  - Quick check question: What does a p-value of 0.70 from the Friedman test imply about differences across repetition levels?

- Concept: Prompt configuration variants (open-book vs closed-book vs QCQ vs paraphrasing)
  - Why needed here: The experiments test four different ways of presenting questions and context; understanding these setups is key to grasp the experimental design.
  - Quick check question: In the QCQ setup, where is the question placed relative to the context?

## Architecture Onboarding

- Component map: Input encoder -> attention layers -> output decoder
- Critical path: Prompt construction -> model inference -> substring matching evaluation
- Design tradeoffs: Repeating questions increases prompt length (cost) but does not improve accuracy; closed-book is cheaper but less accurate
- Failure signatures: If repetition caused accuracy drop, it would suggest over-saturation or noise; if it caused increase, it would suggest attention bias
- First 3 experiments:
  1. Run the same prompt with 1, 3, and 5 repetitions and measure accuracy; confirm no significant difference
  2. Compare open-book vs closed-book accuracy to establish baseline effect of context
  3. Test paraphrasing variant to see if rephrasing adds noise versus repetition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does question repetition affect LLM performance in reasoning-heavy or creative domains compared to reading comprehension tasks?
- Basis in paper: [inferred] from the "Limitations" section, which states "repetition effects might differ in reasoning-heavy or creative domains."
- Why unresolved: The study focuses exclusively on reading comprehension tasks, leaving the impact of repetition in other domains unexplored.
- What evidence would resolve it: Experiments evaluating the same repetition patterns across diverse domains like mathematical reasoning, code generation, or creative writing would clarify if effects generalize beyond reading comprehension.

### Open Question 2
- Question: What is the effect of question repetition on masked language models (MLMs) compared to causal LLMs?
- Basis in paper: [explicit] from the "Limitations" section, which states "the effect of question repetition remains unexplored in masked LMs."
- Why unresolved: The study focuses on causal LLMs, which dominate much of the field, but MLMs represent a distinct architectural paradigm.
- What evidence would resolve it: Systematic testing of the same repetition patterns across masked LMs like BERT, RoBERTa, and their variants would reveal whether architectural differences influence repetition sensitivity.

### Open Question 3
- Question: How does question repetition interact with prompt structure and interpretability in LLM outputs?
- Basis in paper: [inferred] from the conclusion suggesting "a detailed investigation into the interplay between prompt structure, repetition, and interpretability."
- Why unresolved: The study demonstrates robustness to repetition but doesn't examine how repetition affects the interpretability or explainability of LLM reasoning processes.
- What evidence would resolve it: Analysis of attention patterns, reasoning traces, or interpretability metrics across repetition levels would reveal whether repetition influences how models process and present their reasoning.

## Limitations
- Study focuses exclusively on reading comprehension tasks, leaving domain-specific effects unexplored
- Effect size may be too small to detect with current sample size (500 questions per dataset)
- Does not investigate internal attention mechanisms to confirm whether repeated tokens are truly treated as duplicates

## Confidence

- **High Confidence**: The finding that question repetition does not significantly impact output quality across all tested models and configurations
- **Medium Confidence**: The claim that LLMs are robust to repeated input structures
- **Low Confidence**: The specific mechanism that attention heads treat repeated tokens as duplicates rather than emphasizing them

## Next Checks
1. **Attention Analysis**: Analyze the attention weight distributions in the transformer layers when processing repeated questions versus single instances to verify whether repeated tokens receive the same attention weights as non-repeated ones.

2. **Sample Size Sensitivity**: Conduct power analysis to determine the minimum sample size required to detect a 6% accuracy improvement with statistical significance, then run experiments at that scale to verify whether the effect is real but undetected due to insufficient power.

3. **Domain-Specific Effects**: Test the repetition effect across different question types (fact-based, reasoning, multi-hop) and domains to identify whether certain categories of questions show sensitivity to repetition that was masked in the aggregate analysis.