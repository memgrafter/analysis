---
ver: rpa2
title: 'ProMQA: Question Answering Dataset for Multimodal Procedural Activity Understanding'
arxiv_id: '2410.22211'
source_url: https://arxiv.org/abs/2410.22211
tags:
- answers
- question
- questions
- each
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ProMQA, a novel multimodal question-answering
  dataset designed to evaluate systems' capabilities in understanding procedural activities
  like cooking. The dataset contains 401 QA pairs constructed using a cost-effective
  human-LLM collaborative approach, where LLMs generate candidate QA pairs that are
  then verified and augmented by human annotators.
---

# ProMQA: Question Answering Dataset for Multimodal Procedural Activity Understanding

## Quick Facts
- arXiv ID: 2410.22211
- Source URL: https://arxiv.org/abs/2410.22211
- Reference count: 27
- Primary result: ProMQA dataset reveals substantial performance gap between humans and multimodal models on procedural activity understanding tasks

## Executive Summary
ProMQA is a novel multimodal question-answering dataset designed to evaluate systems' capabilities in understanding procedural activities like cooking. The dataset contains 401 QA pairs constructed using a cost-effective human-LLM collaborative approach, where LLMs generate candidate QA pairs that are then verified and augmented by human annotators. This approach significantly reduces annotation costs compared to full-human annotation. The dataset uniquely combines recipes (text instructions) with user recordings (video) to enable evaluation of multimodal reasoning in procedural contexts. Benchmark experiments reveal a substantial performance gap between humans and both open and proprietary multimodal models, including strong proprietary models like GPT-4o and Claude 3.5 Sonnet.

## Method Summary
The ProMQA dataset was constructed using a human-LLM collaborative annotation approach. LLMs generated candidate QA pairs from cooking videos paired with corresponding recipes, achieving approximately 80% approval rate after human verification. Human annotators either approved the generated pairs or added their own answers to improve quality. The dataset contains 401 QA pairs covering various question types including missing steps, next actions, order verification, measurement checks, preparation steps, techniques, temperature, and timing questions. Each question requires both recipe text and video input for correct answering, creating a true multimodal reasoning challenge.

## Key Results
- ProMQA achieves 401 high-quality QA pairs through cost-effective human-LLM collaboration
- Significant performance gap exists between humans and all tested multimodal models
- Proprietary models (GPT-4o, Claude 3.5 Sonnet) outperform open models but still lag human performance
- Dataset successfully captures multimodal procedural reasoning that single-modality approaches cannot solve

## Why This Works (Mechanism)

### Mechanism 1
Cost-effective annotation through LLM-assisted QA generation: LLMs generate candidate QA pairs which are then verified by human annotators, reducing manual annotation time while maintaining quality. Core assumption: LLMs can generate valid QA pairs that capture multimodal procedural reasoning when given appropriate prompts. Evidence anchors: 80% retention rate of generated QA pairs, explicit cost comparison to full-human annotation. Break condition: If LLM-generated questions fail to capture procedural aspects or require extensive rewriting, the cost savings diminish significantly.

### Mechanism 2
Multimodal reasoning assessment through combined text and video inputs: QA pairs require both recipe instructions and video recordings to answer, preventing text-only or vision-only models from succeeding. Core assumption: Procedural activities inherently require cross-modal understanding that cannot be reduced to single-modality tasks. Evidence anchors: Dataset construction explicitly pairs recipes with videos, question types require cross-modal reasoning. Break condition: If questions can be answered from single modality alone, the multimodal constraint becomes ineffective.

### Mechanism 3
LLM-as-judge provides reliable evaluation for diverse answer formats: Ternary scoring (match/partial-match/unmatch) with step descriptions context enables consistent evaluation across varied correct answers. Core assumption: LLMs can reliably judge answer quality when given appropriate context and scoring rubrics. Evidence anchors: Correlation studies showing highest correlation with human judgments using ternary scoring with step context. Break condition: If LLM evaluator shows self-preference bias or poor correlation with human judgments, evaluation reliability decreases.

## Foundational Learning

- Concept: Cross-modal reasoning - Why needed here: The dataset requires understanding how textual instructions map to visual actions in procedural activities. Quick check question: Can you identify which steps in a recipe have been completed by watching a cooking video?
- Concept: Procedural task analysis - Why needed here: Understanding the distinction between process-level questions (multiple steps) and step-specific questions (individual actions). Quick check question: What's the difference between asking "What should I do next?" versus "Did I measure correctly?"
- Concept: Error detection in sequential tasks - Why needed here: The dataset includes questions about timing errors, missing steps, and incorrect techniques that require temporal reasoning. Quick check question: How would you determine if someone skipped a step in a recipe by watching their cooking video?

## Architecture Onboarding

- Component map: LLM generator → Human verifier → QA storage → Benchmark models → LLM evaluator → Performance metrics
- Critical path: LLM generation → Human verification → Benchmark evaluation → Performance analysis
- Design tradeoffs: Annotation cost vs. quality (LLM generation with human verification vs. full human annotation), model complexity vs. performance (unimodal vs. multimodal approaches)
- Failure signatures: Low approval rate for generated questions indicates prompt/template issues; poor benchmark performance suggests model limitations in cross-modal reasoning
- First 3 experiments: 1) Test LLM generator with different prompt templates to maximize approval rate, 2) Compare different LLM evaluators for correlation with human judgments, 3) Benchmark unimodal vs. multimodal models on clean vs. noisy examples

## Open Questions the Paper Calls Out
1. How does the performance of multimodal models vary when the number of recipe steps increases or decreases?
2. Would expanding the dataset to include procedural activities beyond cooking improve model performance and generalizability?
3. How does the human-LLM collaborative annotation approach compare to fully human annotation in terms of question diversity and coverage of edge cases?
4. What is the impact of different frame sampling rates and resolutions on model performance, and is there an optimal configuration?

## Limitations
- Dataset restricted to cooking activities, limiting generalizability to other procedural domains
- Proprietary models (GPT-4o, Claude 3.5 Sonnet) create reproducibility challenges for benchmark comparisons
- Quality of error descriptions in source datasets may affect validity of generated questions

## Confidence
- **High Confidence**: The cost-effectiveness of the human-LLM collaborative annotation approach
- **Medium Confidence**: The multimodal reasoning assessment capability
- **Medium Confidence**: The LLM-as-judge evaluation framework

## Next Checks
1. Conduct ablation studies where benchmark models are tested on text-only, vision-only, and multimodal versions of the same questions to verify the cross-modal requirement is essential for task completion
2. Perform inter-annotator agreement studies on a subset of LLM-generated questions to quantify human verification reliability and identify systematic rejection patterns
3. Test the LLM evaluator's correlation with human judgments across different question types and difficulty levels to identify potential bias in specific categories