---
ver: rpa2
title: 'Evaluation of Table Representations to Answer Questions from Tables in Documents
  : A Case Study using 3GPP Specifications'
arxiv_id: '2408.17008'
source_url: https://arxiv.org/abs/2408.17008
tags:
- table
- tables
- text
- retrieval
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates different representations of tables interspersed
  with text for question answering using 3GPP technical specifications. The researchers
  created a dataset of 278 expert-curated questions across four types: extraction,
  aggregation, multi-row/column, and inferential.'
---

# Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications

## Quick Facts
- arXiv ID: 2408.17008
- Source URL: https://arxiv.org/abs/2408.17008
- Reference count: 27
- Key outcome: Row-level table representations with repeated header information improve retrieval accuracy in 3GPP technical specifications

## Executive Summary
This study evaluates different table representations for question answering in documents containing interspersed text, using 3GPP technical specifications as the corpus. The researchers created a dataset of 278 expert-curated questions across four types (extraction, aggregation, multi-row/column, and inferential) and systematically tested 16 table representation configurations using five pre-trained embedding models. The results demonstrate that row-level representations with repeated header information significantly improve retrieval accuracy, while the choice of column separator (pipe vs space) shows minimal but consistent impact. The study concludes that leveraging structural table information through header repetition and row-level chunking is crucial for effective retrieval, even with publicly available embedding models.

## Method Summary
The researchers parsed 392 3GPP Release 18 documents containing 21,824 tables and 948,616 sentences of free-flowing text. They created table representations with variations in chunk granularity (table vs row level), header inclusion, and column separators (pipe vs space). Five pre-trained embedding models (MPNET, MiniLM, bge-large-en, llm-embedder, bge-m3) were used to generate embeddings for both questions and table representations. Cosine similarity was computed between question embeddings and corpus embeddings to retrieve top-k relevant table chunks. The study evaluated 16 different table representation configurations across the five embedding models to determine the optimal approach for table-based question answering.

## Key Results
- Row-level representations with repeated header information improve retrieval accuracy by providing explicit column-context information at each row
- Pipe separators show slightly better performance than space separators due to clearer column boundaries
- Including interspersed text reduces retrieval accuracy by introducing noise that interferes with table-relevant context
- Publicly available pre-trained embedding models perform competitively for table-based retrieval tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Row-level representations with repeated header information improve retrieval accuracy because the model can associate cell values with their column context at each row.
- Mechanism: When headers are repeated in every cell, the embedding model receives explicit column-context information at the row granularity, making it easier to distinguish between semantically similar values in different columns.
- Core assumption: The embedding model can effectively utilize repeated structural information without suffering from token dilution or noise.
- Evidence anchors:
  - [abstract]: "row level representations with corresponding table header information being included in every cell improves the performance of the retrieval"
  - [section]: "We observe that it is better to consider embeddings of rows of tables rather than a single embedding of an entire table. Also, representation introducing table header information along with the tabular content for each cell, prior to the embedding process, improves performance."
  - [corpus]: Weak - no direct neighbor study explicitly confirms this mechanism.

### Mechanism 2
- Claim: Separating columns with pipe characters slightly improves retrieval because the separator is less likely to appear within table data and may be less ambiguous to embedding models.
- Mechanism: The pipe character provides a clear column boundary that the embedding model can use to distinguish columnar data, reducing ambiguity in parsing cell content.
- Core assumption: Pre-trained embedding models have seen pipe characters in their training data and can use them to infer structure.
- Evidence anchors:
  - [section]: "We find publicly available pre-trained embedding models to be competitive in terms of retrieval accuracies in an interspersed text and table setting and the table header being introduced in the cell information."
  - [corpus]: No direct evidence in neighbors - this is an inference from the paper's observation.

### Mechanism 3
- Claim: Removing interspersed text from the corpus improves retrieval accuracy because it reduces noise and prevents the model from confusing table-relevant context with general text.
- Mechanism: When text and tables are separated, the embedding model can focus on table-specific patterns without interference from surrounding prose.
- Core assumption: The embedding model treats interspersed text as noise rather than useful context for table-based questions.
- Evidence anchors:
  - [section]: "we find that retrieval accuracies reduce with introduction of tables interspersed with text" and "Comparing retrieval accuracies of 'Include Text: No' and 'Include Text: Yes' groups across all the 4 sub-plots of Figure 3, we find that retrieval accuracies reduce with introduction of tables interspersed with text."
  - [corpus]: No direct neighbor evidence - this is a novel observation from the paper.

## Foundational Learning

- Concept: Embedding model selection and pre-training characteristics
  - Why needed here: The choice of embedding model (MPNET, MiniLM, bge-large-en, llm-embedder, bge-m3) directly affects retrieval performance, and understanding their training data and architecture helps explain results.
  - Quick check question: Which embedding models were used in this study, and what are their key architectural differences?

- Concept: Chunking strategies for document retrieval
  - Why needed here: The decision between table-level vs row-level chunking is central to the paper's findings and requires understanding how chunk granularity affects embedding quality and retrieval relevance.
  - Quick check question: What are the two chunking levels tested in this study, and which one showed better performance?

- Concept: Table parsing and representation formats
  - Why needed here: Understanding how tables are parsed from DOCX to JSON and then represented with different separators and header inclusion is crucial for replicating or extending this work.
  - Quick check question: What are the three main table representation variations tested in this study?

## Architecture Onboarding

- Component map:
  Document parser (DOCX → JSON) → Table representation formatter → Embedding model → Cosine similarity retriever → Top-k accuracy evaluator
  Question embedding generator → Corpus embedding store → Retrieval engine → Evaluation pipeline

- Critical path:
  Question → Embedding → Corpus search → Retrieve relevant table chunks → Check if correct table is in top-k → Calculate accuracy

- Design tradeoffs:
  Row-level vs table-level: More chunks but better granularity vs fewer chunks but potential loss of context
  Header repetition: Better context but increased token usage
  Separator choice: Clarity vs potential token issues

- Failure signatures:
  Low accuracy across all configurations suggests embedding model inadequacy
  High accuracy without text but low with text suggests noise interference
  Row-level worse than table-level suggests chunk size too small

- First 3 experiments:
  1. Run the baseline configuration (table-level chunks, no headers, space separator) to establish initial performance
  2. Test row-level chunking with no headers to isolate the effect of chunk granularity
  3. Add header repetition to the best-performing configuration to verify the structural benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of interspersed text affect retrieval performance for different types of questions (extraction, aggregation, multi-row/column, and inferential)?
- Basis in paper: [explicit] The paper states "Comparing retrieval accuracies of 'Include Text: No' and 'Include Text: Yes' groups across all the 4 sub-plots of Figure 3, we find that retrieval accuracies reduce with introduction of tables interspersed with text."
- Why unresolved: While the paper identifies a general decrease in performance, it doesn't provide a detailed breakdown of how this effect varies across different question types. The paper mentions that the drop is higher for aggregation and multi-row/column questions but doesn't offer a comprehensive analysis.
- What evidence would resolve it: A detailed breakdown of retrieval accuracy for each question type (E, A, M, I) with and without interspersed text, along with statistical significance tests to confirm the differences.

### Open Question 2
- Question: Does the choice of column separator (pipe vs. space) have a consistent impact on retrieval accuracy across different embedding models?
- Basis in paper: [explicit] The paper states "In terms of separator, we find that our results do not indicate consistent change in accuracy, though we obtain the highest performance when pipe is used as a separator."
- Why unresolved: The paper acknowledges the lack of consistent impact but doesn't explore the reasons behind this observation or provide a deeper analysis of how the separator choice interacts with different embedding models.
- What evidence would resolve it: A comprehensive analysis of retrieval accuracy for each embedding model with both pipe and space separators, including statistical tests to determine if the differences are significant. Additionally, an exploration of why certain models might be more sensitive to the separator choice.

### Open Question 3
- Question: How would domain adaptation techniques affect the retrieval performance when using the best table representation (row-level chunking with repeated headers)?
- Basis in paper: [inferred] The paper mentions "Although we limit our research to publicly available embeddings, as this is the starting point and often constraints most industrial applications, we expect that the importance of representations would carry through even under domain adaptation techniques."
- Why unresolved: The paper doesn't explore domain adaptation techniques, leaving open the question of how much improvement could be achieved by fine-tuning embeddings on the specific 3GPP domain.
- What evidence would resolve it: An experiment comparing retrieval performance using publicly available embeddings versus domain-adapted embeddings (fine-tuned on the 3GPP corpus) with the best table representation identified in the study. This would quantify the potential improvement from domain adaptation.

## Limitations
- The study is limited to 3GPP technical specifications in DOCX format, which may not generalize to other technical domains or document types
- The analysis focuses solely on retrieval accuracy without evaluating complete question answering performance
- The 278 question dataset may not fully represent the diversity of real-world table-based question answering scenarios
- The study assumes publicly available pre-trained embedding models are sufficient without exploring fine-tuning possibilities

## Confidence

**High Confidence**: The finding that row-level representations with header repetition improve retrieval accuracy is strongly supported by the experimental data across all embedding models tested. The consistent pattern across multiple configurations provides robust evidence for this claim.

**Medium Confidence**: The observation that pipe separators slightly outperform space separators is supported by the data but shows smaller effect sizes that may not be practically significant. The mechanism behind this difference remains speculative.

**Medium Confidence**: The conclusion that interspersed text reduces retrieval accuracy is well-supported, though the study does not explore whether this is due to noise or lack of useful contextual information.

## Next Checks

1. **Domain Transfer Validation**: Test the same table representation configurations on a different technical domain (e.g., medical literature or financial reports) to assess generalizability beyond 3GPP specifications.

2. **End-to-End QA Evaluation**: Implement a complete question answering pipeline that uses the retrieved table chunks to actually answer questions, comparing performance against retrieval-only accuracy to identify potential gaps.

3. **Fine-tuning Experiment**: Fine-tune one of the pre-trained embedding models on a subset of the 3GPP corpus to determine if performance can be improved beyond what's achievable with off-the-shelf models, particularly for the interspersed text scenario.