---
ver: rpa2
title: 'Instance Selection for Dynamic Algorithm Configuration with Reinforcement
  Learning: Improving Generalization'
arxiv_id: '2407.13513'
source_url: https://arxiv.org/abs/2407.13513
tags:
- instance
- agent
- instances
- training
- selector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the limited generalization of Reinforcement
  Learning (RL) agents in Dynamic Algorithm Configuration (DAC), where agents fail
  to perform well on instances not seen during training. The core method involves
  using SELECTOR to select a representative subset of training instances based on
  time-series features extracted from the RL agent's rollout trajectories, and then
  retraining the agent on this subset to improve generalization.
---

# Instance Selection for Dynamic Algorithm Configuration with Reinforcement Learning: Improving Generalization

## Quick Facts
- arXiv ID: 2407.13513
- Source URL: https://arxiv.org/abs/2407.13513
- Reference count: 23
- Key outcome: Instance selection using time-series features from RL agent trajectories improves generalization in DAC by up to 14 percentage points on benchmark problems

## Executive Summary
This work addresses the critical challenge of limited generalization in Reinforcement Learning (RL) agents for Dynamic Algorithm Configuration (DAC), where agents trained on specific instances fail to perform well on unseen ones. The authors propose using SELECTOR to identify representative training instances based on time-series features extracted from agent rollout trajectories, then retraining the agent on this subset. Through empirical evaluations on the Sigmoid and CMA-ES benchmarks from DACBench, the method demonstrates significant improvements in generalization performance compared to training on the full instance set.

## Method Summary
The method involves training a PPO RL agent on the full training instance set, then generating rollout trajectories to extract time-series features using Catch22. SELECTOR (using Dominating Sets or Maximal Independent Set algorithms) identifies a representative subset of instances based on these features. The RL agent is then retrained on this subset with the same total training budget as the initial training. The key innovation is using trajectory-derived time-series features rather than raw instance features, capturing the dynamic nature of the RL agent's behavior during training.

## Key Results
- On CMA-ES benchmark, SELECTOR with threshold 0.8 achieved mean performance of 0.75 vs 0.66 for full set training
- Instance selection improved generalization by up to 14 percentage points compared to baseline RL training
- Catch22 time-series features consistently outperformed raw action/reward features across benchmarks
- SELECTOR with higher similarity thresholds (0.8-0.95) generally produced better generalization results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using time-series features from RL agent trajectories enables more informative instance selection than raw actions or rewards alone
- Mechanism: Catch22 extracts 24 time-series features capturing temporal autocorrelation, fluctuation scaling, and distribution properties that reveal underlying dynamics not evident from raw data
- Core assumption: Time-series features meaningfully capture DAC problem dynamics and correlate with generalization performance
- Evidence anchors: [abstract] mentions accounting for dynamic nature of RL agent by computing time series features; [section] describes Catch22 features capturing distribution, autocorrelation, and scaling properties
- Break condition: If time-series features fail to capture meaningful differences or introduce noise obscuring instance relationships

### Mechanism 2
- Claim: SELECTOR subselects representative instances that capture diversity while reducing redundancy
- Mechanism: SELECTOR builds similarity graph from meta-features and selects maximally dissimilar instances representing full instance space
- Core assumption: Diverse subset provides sufficient coverage for training robust DAC policy
- Evidence anchors: [abstract] discusses overcoming overrepresentation through representative subset selection; [section] notes instances capture essential dynamics for better generalization
- Break condition: If selected subset fails to cover critical regions or similarity measure misaligns with generalization

### Mechanism 3
- Claim: Retraining on subselected instances improves generalization by reducing overfitting to redundant instances
- Mechanism: Training on representative subset forces RL agent to learn robust policy for diverse instance space rather than overfitting to specific patterns
- Core assumption: Training budget can be redistributed to fewer representative instances without performance loss
- Evidence anchors: [abstract] highlights efficacy of instance selection in refining DAC policies; [section] describes equal training budget allocation before and after subselection
- Break condition: If subselected instances too few for adequate learning signals or reduced diversity causes missing important patterns

## Foundational Learning

- Concept: Reinforcement Learning in Dynamic Algorithm Configuration
  - Why needed here: Builds on RL to learn policies for dynamically setting algorithm hyperparameters across diverse instances
  - Quick check question: What is the key difference between static algorithm configuration and dynamic algorithm configuration?

- Concept: Instance Selection and Meta-features
  - Why needed here: Core contribution involves selecting representative training instances using meta-features from agent trajectories
  - Quick check question: How do time-series features from agent trajectories differ from traditional instance meta-features?

- Concept: Benchmark Libraries for DAC (DACBench)
  - Why needed here: Experiments use DACBench which provides standardized benchmarks for evaluating DAC approaches
  - Quick check question: What are the two benchmarks used in this study and what do they evaluate?

## Architecture Onboarding

- Component map: PPO training -> Rollout trajectory generation -> Catch22 feature extraction -> SELECTOR instance selection -> Retraining on subset -> Test set evaluation

- Critical path: 1. Train RL agent on full training set 2. Generate rollout trajectories 3. Extract Catch22 features 4. Apply SELECTOR to identify representative subset 5. Retrain RL agent on selected subset 6. Evaluate on test set

- Design tradeoffs: Training RL agent twice increases computational cost but improves generalization; Catch22 features provide richer representation but add complexity; SELECTOR's similarity threshold affects subset size and diversity

- Failure signatures: Poor generalization despite instance selection (poor feature representation or inadequate subset diversity); selected instances fail to cover critical regions (similarity measure misalignment); computational overhead outweighs benefits (need for early stopping or more efficient selection)

- First 3 experiments: 1. Run baseline RL agent training on full Sigmoid instance set, evaluate on test set 2. Generate rollout trajectories, extract Catch22 features, apply SELECTOR with different thresholds, evaluate selected subset diversity 3. Retrain RL agent on SELECTOR-selected subset, compare generalization performance to baseline

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions but several remain unexplored

## Limitations
- Computational cost of training RL agent twice (full set then subset) may not be practical for expensive DAC problems
- Catch22 feature extraction adds complexity and could introduce noise or fail to capture meaningful instance differences
- SELECTOR's performance heavily depends on similarity threshold choice, which appears somewhat arbitrary

## Confidence

- **High**: The core mechanism of using time-series features from agent trajectories is well-grounded and empirical results show clear improvements in generalization
- **Medium**: Effectiveness of SELECTOR for instance selection is demonstrated but could be more robust with additional threshold tuning or alternative selection criteria
- **Low**: Practical applicability to more complex DAC problems with higher computational costs remains uncertain

## Next Checks

1. Conduct ablation studies removing Catch22 features to quantify their contribution to generalization improvements
2. Test the method on additional DACBench benchmarks (e.g., MaxCut, LeadingOnes) to assess broader applicability
3. Implement early stopping criteria during initial training phase to reduce computational overhead while maintaining performance