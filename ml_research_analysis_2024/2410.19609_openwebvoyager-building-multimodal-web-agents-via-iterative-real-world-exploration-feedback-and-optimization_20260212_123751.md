---
ver: rpa2
title: 'OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration,
  Feedback and Optimization'
arxiv_id: '2410.19609'
source_url: https://arxiv.org/abs/2410.19609
tags:
- agent
- trajectories
- openwebv
- learning
- oyager
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OpenWebVoyager is an open-source framework that enables multimodal
  web agents to autonomously improve through iterative real-world exploration, feedback,
  and optimization. It begins with imitation learning using expert trajectories from
  GPT-4o, then continuously explores new tasks, collecting feedback via GPT-4o to
  refine the policy.
---

# OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization

## Quick Facts
- arXiv ID: 2410.19609
- Source URL: https://arxiv.org/abs/2410.19609
- Reference count: 24
- One-line primary result: Web agent improves task success rate from 19.9% to 25.8% on WebVoyager through iterative self-improvement

## Executive Summary
OpenWebVoyager presents an open-source framework for building multimodal web agents that can autonomously improve through iterative exploration, feedback, and optimization. The system starts with imitation learning from expert trajectories collected using GPT-4o, then continuously explores new tasks while collecting feedback from GPT-4o to refine its policy. Built on idefics2-8b-instruct, the agent processes both visual screenshots and accessibility trees to navigate web pages. Through multiple iterations of exploration and fine-tuning, the agent demonstrates measurable performance improvement on web navigation tasks.

## Method Summary
The method employs a three-stage approach: (1) Imitation Learning on expert trajectories collected by GPT-4o/WebVoyager-4o across 48 websites, (2) Iterative Exploration where the agent attempts new tasks with temperature 1.2 sampling, and (3) Feedback Optimization where GPT-4o evaluates trajectories and successful ones are used for fine-tuning. The process repeats for multiple cycles, with each iteration collecting 480 new queries and retraining the idefics2-8b-instruct model. The system uses Selenium-based browser automation with both screenshots and accessibility trees as observations, focusing on basic web actions (clicks, typing, scrolling).

## Key Results
- Task Success Rate improves from 19.9% to 25.8% on WebVoyager test set after 3 iterations
- Performance on Mind2Web cross-task/cross-web datasets improves from 6.3% to 19.6%
- Agent shows generalization capability to unseen websites despite being trained on 48 specific sites
- Self-improvement occurs through autonomous exploration without additional expert intervention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative exploration-feedback-optimization cycles improve agent performance over time.
- Mechanism: Agent explores web tasks, GPT-4o evaluates trajectory success, successful trajectories are used to retrain the model, and this cycle repeats.
- Core assumption: GPT-4o's evaluation of trajectory correctness is reliable and consistent.
- Evidence anchors: [abstract] "Experimental results show that our web agent successfully improves itself after each iteration"; [section] "we employ trajectory-level rejection sampling via GPT-4o to ensure quality trajectories"
- Break condition: GPT-4o evaluation becomes unreliable, or exploration fails to find successful trajectories.

### Mechanism 2
- Claim: Multimodal perception (screenshots + accessibility trees) provides richer information than text-only approaches.
- Mechanism: Agent receives both visual screenshots and structured accessibility trees as observations, allowing it to ground actions in visual context.
- Core assumption: Visual information captures web page layout and design that text-only representations miss.
- Evidence anchors: [abstract] "real-world webpages are designed based on human visual preference, ignoring the visual inputs can cause significant information loss"; [section] "We adopt the Selenium-based online web navigation environment...we modify the observation of the web page to include the accessibility tree and its corresponding unmarked screenshot"
- Break condition: Visual grounding becomes too difficult for the model, or accessibility trees alone become sufficient.

### Mechanism 3
- Claim: Imitation learning on expert trajectories provides a strong initial policy that can be refined through self-exploration.
- Mechanism: Agent first learns from WebVoyager-4o's successful trajectories, then explores new tasks and improves based on its own successful attempts.
- Core assumption: WebVoyager-4o's trajectories represent high-quality examples that teach effective navigation strategies.
- Evidence anchors: [section] "We utilize GPT-4o along with the WebVoyager paradigm to generate web navigation trajectories...In this stage, to better distill knowledge from GPT-4o, we filter out unfinished trajectories"; [section] "Through Imitation Learning, the agent has already learned the basic operation logic and response format"
- Break condition: Expert trajectories contain flawed strategies, or self-exploration fails to discover new successful approaches.

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: Web navigation is modeled as a POMDP where the agent doesn't have full state information
  - Quick check question: What are the four components of a POMDP tuple (S, O, A, T, R)?

- Concept: Reinforcement Learning with Sparse Rewards
  - Why needed here: Web navigation tasks provide sparse binary rewards (success/failure) making learning challenging
  - Quick check question: How does the agent distinguish between good and bad trajectories when rewards are only 0 or 1?

- Concept: Rejection Sampling for Trajectory Filtering
  - Why needed here: Only successful trajectories should be used for training to avoid reinforcing failed behaviors
  - Quick check question: What role does GPT-4o play in the rejection sampling process?

## Architecture Onboarding

- Component map:
  Web environment (Selenium-based browser automation) -> Observation processor (screenshots + accessibility trees) -> Large Multimodal Model (Idefics2-8b-instruct) -> GPT-4o evaluator (for trajectory feedback) -> Training pipeline (imitation learning + iterative fine-tuning)

- Critical path:
  1. Generate task queries
  2. Collect expert trajectories (IL phase)
  3. Train base model on expert data
  4. Explore new tasks with base model
  5. GPT-4o evaluates exploration results
  6. Retrain model on successful trajectories
  7. Repeat steps 4-6 for multiple iterations

- Design tradeoffs:
  - Visual vs text-only observations: richer information but increased computational cost
  - Expert initialization vs training from scratch: faster convergence but potentially limited exploration
  - GPT-4o evaluation vs other feedback methods: high quality but expensive and dependent on OpenAI API

- Failure signatures:
  - Agent gets stuck in loops (same actions repeated)
  - Agent fails to finish tasks despite having correct information
  - Performance degrades over iterations (catastrophic forgetting)
  - Exploration becomes too random (low success@K metrics)

- First 3 experiments:
  1. Verify IL phase works: train on WebVoyager-4o trajectories and test on WebVoyager test set
  2. Test single exploration cycle: run base model on new tasks, collect feedback, retrain, and measure improvement
  3. Test iteration stability: run multiple exploration cycles and verify performance doesn't degrade

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the agent's performance improvement generalize to more complex web actions beyond basic clicks, typing, and scrolling?
- Basis in paper: [inferred] The paper acknowledges that only basic web actions (clicks, typing, scrolling) were considered, explicitly stating "We only consider the most common executable web actions in the simulated environment, including clicking, typing, and scrolling, without more advanced actions such as dragging and zooming."
- Why unresolved: The study deliberately limited the action space to basic operations, leaving the agent's capability with advanced interactions unexplored. The current framework's ability to handle dragging, zooming, or other complex interactions remains untested.
- What evidence would resolve it: Testing the OpenWebVoyager agent on websites requiring advanced interactions like image zooming, file dragging, or complex form manipulations would demonstrate whether the self-improvement approach generalizes to more sophisticated web actions.

### Open Question 2
- Question: How would increasing the model size beyond 8B parameters affect the agent's performance and exploration efficiency?
- Basis in paper: [explicit] The paper states "Our approach is based on a relatively small LMM Idefics2 with 8B parameters, which may limit the agent's ability to effectively navigate websites of unseen domains and respond to complex user queries."
- Why unresolved: The current implementation uses Idefics2-8b-instruct as the backbone model, but the paper acknowledges this might be limiting. The trade-off between model size, computational cost, and performance gains remains unexplored.
- What evidence would resolve it: Training and evaluating OpenWebVoyager using larger models (e.g., 34B or 70B parameter variants) while measuring task success rates, exploration efficiency, and computational requirements would clarify whether increased model size provides meaningful performance benefits.

### Open Question 3
- Question: Can the hallucination problem be mitigated through architectural changes or additional training objectives?
- Basis in paper: [explicit] The paper identifies hallucination as a significant limitation: "We find that agents often directly hallucinate answers that do not appear during the navigation process. The decrease in trajectory length might have increased the frequency of this issue."
- Why unresolved: While the paper recognizes hallucination as a problem, it doesn't propose or test solutions beyond noting the correlation with trajectory length. The root causes and potential remedies remain unexplored.
- What evidence would resolve it: Implementing and comparing different approaches such as confidence scoring mechanisms, fact-checking modules, or contrastive training objectives designed to reduce hallucination would demonstrate whether the issue can be effectively addressed.

## Limitations
- Evaluation Reliability: The entire improvement loop depends on GPT-4o's ability to accurately evaluate trajectory success without validation against human judgment.
- Generalization Concerns: Performance on unseen websites remains relatively low (19.6% Task Success Rate), suggesting potential overfitting to the 48 training websites.
- Ablation Gaps: The paper lacks crucial comparisons such as visual vs text-only performance and expert initialization vs training from scratch.

## Confidence

**High Confidence:** The basic framework architecture and methodology are clearly described. The iterative exploration-feedback-optimization cycle is well-defined and reproducible.

**Medium Confidence:** The reported performance improvements appear plausible given the methodology, but the dependence on GPT-4o evaluation introduces uncertainty. The moderate improvement from 19.9% to 25.8% on WebVoyager is believable but not transformative.

**Low Confidence:** Claims about generalization to unseen websites are weak due to limited absolute performance. The assumption that GPT-4o evaluation quality is sufficient for reliable self-improvement lacks direct validation.

## Next Checks
1. **Human Evaluation Validation:** Conduct human evaluation of a sample of GPT-4o's trajectory assessments to verify the reliability of the feedback mechanism. Compare GPT-4o judgments against human judgments on success/failure classification.

2. **Ablation Studies:** Run controlled experiments removing visual inputs (screenshots) to measure the impact on performance. Also compare performance when training from scratch versus using expert initialization to quantify the benefit of the imitation learning phase.

3. **Exploration Efficiency Analysis:** Analyze the trajectory length and hallucination frequency during exploration phases. Measure whether exploration becomes more efficient over iterations or if the agent still struggles with basic navigation tasks despite overall performance improvements.