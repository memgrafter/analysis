---
ver: rpa2
title: 'SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity Mixture'
arxiv_id: '2410.07739'
source_url: https://arxiv.org/abs/2410.07739
tags:
- lora
- adapters
- downstream
- forgetting
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting in parameter-efficient
  fine-tuning (PEFT) of LLMs. The proposed SLIM method combines low-rank adapters
  (LoRA) with identity layers in a mixture-of-experts architecture, using dynamic
  routing enhanced by weight yielding with sliding clustering to distinguish in-domain
  from out-of-domain samples.
---

# SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity Mixture

## Quick Facts
- **arXiv ID:** 2410.07739
- **Source URL:** https://arxiv.org/abs/2410.07739
- **Reference count:** 10
- **Primary result:** Reduces catastrophic forgetting in LLMs via identity mixtures and dynamic merging while maintaining PEFT efficiency

## Executive Summary
SLIM introduces a parameter-efficient fine-tuning method that mitigates catastrophic forgetting in LLMs by combining low-rank adapters with identity layers in a mixture-of-experts architecture. The approach uses dynamic routing enhanced by weight yielding with sliding clustering to distinguish in-domain from out-of-domain samples. A novel fast dynamic merging mechanism converts the mixture into a model merging formulation, preserving general capabilities without requiring data replay. Experiments on OpenChat-8B demonstrate that SLIM achieves comparable or superior downstream performance to state-of-the-art PEFT methods while significantly reducing forgetting on general tasks after fine-tuning on domain-specific datasets.

## Method Summary
SLIM addresses catastrophic forgetting in parameter-efficient fine-tuning by integrating identity layers into a mixture-of-experts architecture alongside LoRA adapters. The method employs dynamic routing with weight yielding and sliding clustering to classify samples as in-domain or out-of-domain, enabling adaptive routing decisions. A key innovation is the fast dynamic merging of LoRA adapters into a model merging formulation, which preserves general capabilities during fine-tuning. The approach eliminates the need for data replay while maintaining PEFT efficiency, with ablation studies confirming the effectiveness of identity layers, weight yielding, and dynamic merging in reducing forgetting and improving downstream performance.

## Key Results
- SLIM achieves comparable or superior downstream performance to LoRA, LoRAMoE, and MixLoRA on OpenChat-8B
- Significantly reduces forgetting on general tasks (MMLU, GSM8K) after fine-tuning on downstream datasets
- Eliminates need for data replay while preserving general capabilities through dynamic merging

## Why This Works (Mechanism)
The mechanism prevents catastrophic forgetting by maintaining a dual-path architecture where identity layers preserve baseline capabilities while LoRA adapters learn domain-specific features. Weight yielding with sliding clustering dynamically routes samples based on their similarity to training distributions, allowing the model to distinguish between in-domain and out-of-domain data. The fast dynamic merging converts the mixture into a model merging formulation, effectively freezing general knowledge while adapting to new tasks. This approach creates a balance between plasticity (learning new tasks) and stability (preserving old knowledge) without the computational overhead of full fine-tuning or data replay.

## Foundational Learning
- **Catastrophic forgetting:** When models overwrite old knowledge during new training, leading to degraded performance on previously learned tasks. This is critical because most PEFT methods suffer from this limitation, making them unsuitable for continual learning scenarios.
- **Mixture-of-experts architecture:** Distributes computation across multiple specialized sub-networks, allowing selective activation based on input characteristics. Quick check: Verify that only relevant experts activate for specific input types.
- **Model merging:** Combines separately trained models into a single unified model, preserving knowledge from both sources. Quick check: Test merged model performance against individual component models.
- **Sliding clustering:** An online clustering method that adapts to data distribution changes over time. Quick check: Monitor cluster stability and adaptation speed across training epochs.
- **Weight yielding:** A mechanism for transferring learned weights between model components based on routing decisions. Quick check: Validate weight transfer improves performance on both old and new tasks.

## Architecture Onboarding

**Component map:** Input -> Dynamic Routing -> Weight Yielding/Sliding Clustering -> Identity Layer + LoRA Adapters -> Fast Dynamic Merging -> Output

**Critical path:** Input → Dynamic Routing → Weight Yielding → Identity Layer/LoRA Adapters → Fast Dynamic Merging → Output

**Design tradeoffs:** Balances between computational efficiency (PEFT) and knowledge preservation (model merging), with dynamic routing adding overhead but enabling adaptive behavior. The identity layers increase parameter count but prevent forgetting.

**Failure signatures:** Poor routing decisions lead to over/under-activation of experts; ineffective merging causes loss of general capabilities; clustering instability results in erratic behavior across data distributions.

**3 first experiments:**
1. Test routing accuracy on synthetic in-domain/out-of-domain classification tasks
2. Evaluate forgetting rates on sequential task learning with and without identity layers
3. Benchmark inference latency overhead from mixture-of-experts architecture

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single base model (OpenChat-8B), raising generalizability concerns across different LLM architectures
- Computational overhead from mixture-of-experts and dynamic routing not thoroughly quantified, particularly inference latency
- Sliding clustering approach lacks detailed sensitivity analysis for hyperparameters and behavior in highly heterogeneous data distributions

## Confidence
- **High** confidence in comparative performance results on downstream tasks
- **Medium** confidence in forgetting reduction claims due to limited benchmark scope
- **Low** confidence in scalability and robustness across diverse LLM architectures and data distributions

## Next Checks
1. Evaluate SLIM on multiple base models (Llama, Mistral) and varying model sizes to assess generalizability
2. Quantify computational overhead (inference latency, memory usage) from mixture-of-experts and dynamic routing
3. Test method's robustness to highly heterogeneous or noisy datasets to validate sliding clustering effectiveness