---
ver: rpa2
title: 'RevRIR: Joint Reverberant Speech and Room Impulse Response Embedding using
  Contrastive Learning with Application to Room Shape Classification'
arxiv_id: '2406.03120'
source_url: https://arxiv.org/abs/2406.03120
tags:
- room
- speech
- classification
- encoder
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RevRIR introduces a dual-encoder architecture using contrastive
  learning to embed reverberant speech and room impulse responses (RIRs) jointly,
  enabling room shape classification from speech alone. One encoder processes reverberant
  speech via an Audio Spectrogram Transformer (AST), while the other encodes RIRs
  using feed-forward layers.
---

# RevRIR: Joint Reverberant Speech and Room Impulse Response Embedding using Contrastive Learning with Application to Room Shape Classification

## Quick Facts
- arXiv ID: 2406.03120
- Source URL: https://arxiv.org/abs/2406.03120
- Reference count: 0
- Primary result: 40% Top-1 accuracy for room shape classification from speech alone using contrastive learning embeddings

## Executive Summary
RevRIR introduces a dual-encoder architecture that jointly embeds reverberant speech and room impulse responses (RIRs) using contrastive learning. The model enables room shape classification from speech alone without requiring RIRs at inference time. One encoder processes reverberant speech via an Audio Spectrogram Transformer, while the other encodes RIRs using feed-forward layers. The contrastive loss aligns embeddings from the same room class, allowing the speech encoder to capture room acoustic identity independent of speaker or content.

## Method Summary
The RevRIR model consists of two stages: pre-training and fine-tuning. During pre-training, a dual-encoder architecture learns to map reverberant speech and RIRs into a shared embedding space using contrastive loss. The speech encoder is an Audio Spectrogram Transformer (AST) that processes log-spectrogram features, while the RIR encoder uses feed-forward layers. The model is trained to align embeddings from the same room class while separating different rooms. In the fine-tuning stage, the pre-trained encoder (frozen or unfrozen) is paired with a linear classification head to predict room classes from the embeddings.

## Key Results
- 40% Top-1 accuracy for 110-room classification from reverberant speech alone
- 83% Top-1 accuracy for RIR-based classification (upper bound)
- 95.4% Top-1 accuracy when collapsing to 3 room types from speech
- 99.6% Top-1 accuracy for 3 room types from RIRs
- Outperforms baseline RIR-feature-based method (85.8% accuracy) by 9.6%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint embeddings capture room acoustic identity independent of speaker or content
- Mechanism: Contrastive learning aligns reverberant speech and RIR embeddings from the same room class, forcing the model to ignore speaker and content differences
- Core assumption: RIR and reverberant speech from the same room share invariant acoustic characteristics
- Evidence anchors: [abstract] and [section] statements about contrastive loss encouraging acoustic feature capture
- Break condition: If room acoustics are not invariant across speakers, embeddings collapse to speaker-specific features

### Mechanism 2
- Claim: Pre-training enables zero-RIR inference for room classification
- Mechanism: Learning to embed speech and RIRs into the same space allows speech-only inference
- Core assumption: Shared embedding space carries enough room information for classification
- Evidence anchors: [abstract] and [section] claims about RIR-free inference capability
- Break condition: If shared space is not discriminative enough, speech-only accuracy drops

### Mechanism 3
- Claim: Fine-tuning on frozen encoder preserves pre-trained acoustic embeddings
- Mechanism: Freezing encoder keeps acoustic feature extraction intact while only training classification head
- Core assumption: Pre-trained encoder already captures discriminative room features
- Evidence anchors: [section] results showing 40% vs 37% accuracy for frozen vs unfrozen encoders
- Break condition: If pre-trained embeddings aren't discriminative enough, freezing limits adaptation

## Foundational Learning

- Concept: Contrastive learning loss and temperature scaling
  - Why needed here: Aligns embeddings from same room class while pushing apart different classes
  - Quick check question: What happens to similarity distribution if τ is set too high vs too low?

- Concept: Convolution of speech with RIR to generate reverberant speech
  - Why needed here: Realistic simulation of reverberant speech for matched speech-RIR pairs
  - Quick check question: In Eq. (1), what is effect of padding/truncation on convolution output length?

- Concept: Audio spectrogram transformer (AST) architecture
  - Why needed here: Captures temporal-spectral patterns in reverberant speech
  - Quick check question: How does AST tokenization handle variable-length audio inputs?

## Architecture Onboarding

- Component map:
  - Reverberant speech -> AST encoder -> Speech embedding
  - RIR -> Feed-forward encoder -> RIR embedding
  - Both embeddings -> Contrastive loss -> Shared embedding space
  - During inference: Speech embedding -> Classification head -> Room class

- Critical path:
  - Generate reverberant speech (convolve clean speech with RIR)
  - Compute speech and RIR embeddings
  - Apply contrastive loss to align embeddings
  - During inference, only speech encoder + classification head used

- Design tradeoffs:
  - Dual encoders increase model size but allow RIR-free inference
  - Freezing encoder simplifies deployment but may limit fine-tuning gains
  - AST provides strong speech feature extraction but is heavier than CNN-based encoders

- Failure signatures:
  - Low contrastive loss but poor classification accuracy → embeddings not discriminative enough
  - High training accuracy but low validation accuracy → overfitting, consider stronger regularization
  - Low accuracy for certain room types → imbalanced data or insufficient coverage

- First 3 experiments:
  1. Verify contrastive loss decreases during pre-training and embeddings cluster by room class in t-SNE
  2. Compare fine-tuning with frozen vs unfrozen encoders on validation set to quantify accuracy trade-off
  3. Test inference accuracy using only speech encoder on held-out test set to confirm RIR-free classification

## Open Questions the Paper Calls Out

- Can RevRIR's joint embedding space effectively generalize to real-world room recordings with varying shapes, structures, and acoustic properties?
- How does the choice of RIR encoder architecture impact the effectiveness of contrastive learning in capturing acoustic features?
- Can RevRIR's embeddings be leveraged for tasks beyond room shape classification, such as speaker localization or speech dereverberation?

## Limitations
- Synthetic data may not capture real-world acoustic complexity and variability
- Embedding space alignment evidence is primarily indirect through downstream accuracy
- Encoder freezing trade-off analysis is limited; 3% difference may not be statistically significant

## Confidence
- High confidence: Architectural design using dual encoders with contrastive learning is sound and reproducible
- Medium confidence: Core claim of 40% Top-1 accuracy from speech alone is supported but absolute performance suggests room for improvement
- Low confidence: Assumption that embeddings capture room acoustic identity independent of speaker/content is plausible but not directly validated

## Next Checks
1. Generate t-SNE/UMAP visualizations of pre-trained embeddings to verify clustering by room class and alignment of speech-RIR pairs
2. Systematically compare frozen vs unfrozen encoder fine-tuning across multiple random seeds to assess statistical significance of 3% difference
3. Evaluate trained model on real room recordings to quantify domain gap between simulated and real acoustic environments