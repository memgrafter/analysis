---
ver: rpa2
title: Information Leakage from Embedding in Large Language Models
arxiv_id: '2405.11916'
source_url: https://arxiv.org/abs/2405.11916
tags:
- embed
- language
- hidden
- llama2-7b
- parrot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerability of large language models
  (LLMs) to input reconstruction attacks, where a malicious model provider could potentially
  recover user inputs from embeddings. The authors propose Embed Parrot, a Transformer-based
  method, to reconstruct original inputs from embeddings in deep layers of LLMs like
  ChatGLM-6B and Llama2-7B.
---

# Information Leakage from Embedding in Large Language Models

## Quick Facts
- arXiv ID: 2405.11916
- Source URL: https://arxiv.org/abs/2405.11916
- Authors: Zhipeng Wan; Anda Cheng; Yinggui Wang; Lei Wang
- Reference count: 18
- One-line primary result: Embed Parrot reconstructs original text from deep-layer LLM embeddings with stable performance across token lengths and distributions.

## Executive Summary
This paper investigates the vulnerability of large language models to input reconstruction attacks, where malicious model providers could recover user inputs from embeddings. The authors propose Embed Parrot, a Transformer-based method, to reconstruct original inputs from embeddings in deep layers of LLMs like ChatGLM-6B and Llama2-7B. Their experiments demonstrate that Embed Parrot effectively reconstructs original inputs with stable performance across various token lengths and data distributions. The authors also introduce a defense mechanism to deter exploitation of the embedding reconstruction process. Their findings emphasize the importance of safeguarding user privacy in distributed learning systems and contribute valuable insights to enhance the security protocols within such environments.

## Method Summary
The paper proposes Embed Parrot, a Transformer-based method for input reconstruction attacks on LLMs by exploiting latent representations in deep layers. The approach involves training a model to map deep-layer hidden states to early-layer embeddings, then decoding to recover original tokens. The method uses cosine similarity optimization and can incorporate perplexity as a loss term. The authors also introduce a DCT-based defense mechanism that transforms embeddings to protect privacy while preserving model inference capability. Experiments are conducted on Llama2-7B and ChatGLM-6B using FinGPT and Wikitext datasets, evaluating reconstruction quality with ROUGE scores, F1, and semantic similarity metrics.

## Key Results
- Embed Parrot successfully reconstructs original inputs from deep-layer embeddings with stable performance across token lengths.
- The method outperforms baseline approaches (BEI and HEI) on reconstruction tasks for both ChatGLM-6B and Llama2-7B models.
- DCT-based defense mechanism effectively reduces reconstruction capability while maintaining acceptable model performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hidden states from deeper layers contain semantic representations that can be mapped back to original tokens using a trained decoder model.
- Mechanism: The model first transforms embeddings into a space where they resemble early-layer hidden states. Then it applies a decoding function (e.g., argmax after softmax) to recover tokens.
- Core assumption: There exists a stable, learnable mapping from deep-layer hidden states to earlier-layer hidden states that preserves token-level information.
- Evidence anchors:
  - [abstract]: "We present Embed Parrot, an innovative approach developed to execute input reconstruction attacks on systems by exploiting their latent representations."
  - [section 4.3]: "We train Embed Parrot for ChatGLM-6B layer 28 and Llama2-7B layer 30... using cosine similarity of H i y and H 0 x to optimize the loss function."
  - [corpus]: Weak. No neighboring papers provide direct empirical support for the mapping claim.
- Break condition: If the mapping from deep to shallow embeddings is non-invertible or highly model-dependent, the reconstruction will fail.

### Mechanism 2
- Claim: Cosine similarity in embedding space can identify vocabulary words whose embeddings best match the target hidden state.
- Mechanism: For each hidden state vector, compute cosine similarity against all vocabulary embeddings; pick the highest-scoring word as the reconstruction.
- Core assumption: The embedding space preserves semantic similarity such that the closest embedding corresponds to the original token.
- Evidence anchors:
  - [section 4.2]: "HEI method is predicated on the maximization of cosine similarity within the embedding space of a pre-trained language model."
  - [section 4.2]: "The HEI method calculates the pairwise cosine similarities between the target embedding vector and all embedding vectors in the model's vocabulary."
  - [corpus]: Weak. Corpus papers focus on inversion attacks but do not detail cosine-based token recovery.
- Break condition: If the embedding space is highly compressed or the hidden state is an abstract combination of multiple tokens, cosine similarity will not recover the original token.

### Mechanism 3
- Claim: Applying a transformation to hidden states before decoding can disrupt reconstruction while preserving model inference capability.
- Mechanism: Use DCT to move to frequency domain, apply a binary overlap mask, then inverse DCT to get obfuscated embeddings.
- Core assumption: The frequency-domain representation disperses sensitive information, and selective masking retains enough structure for forward passes but not inversion.
- Evidence anchors:
  - [section 5.3.1]: "We introduce a novel data transformation technique that aims to protect the privacy of input embeddings... utilizing an overlap matrix in conjunction with the Discrete Cosine Transform (DCT) and its inverse (IDCT)."
  - [section 5.3.2]: "For ChatGLM-6B, applying the transformation... to the hidden states output at the 28th layer is more effective at diminishing the reconstructive capability of Embed Parrot."
  - [corpus]: Weak. No direct experimental results in corpus papers supporting this exact masking approach.
- Break condition: If an attacker can invert the DCT and overlap mask, or if the mask leaks enough structure, the defense fails.

## Foundational Learning

- Concept: Transformer layer outputs and hidden state evolution.
  - Why needed here: The attack relies on understanding how hidden states change across layers and how they can be manipulated.
  - Quick check question: How does the self-attention mechanism in a Transformer layer transform hidden states relative to the previous layer?

- Concept: Embedding inversion and token recovery methods.
  - Why needed here: The core of the attack is reconstructing tokens from embeddings; knowing the inversion landscape is essential.
  - Quick check question: What are the main differences between direct decoding (argmax after softmax) and cosine-similarity-based embedding inversion?

- Concept: Privacy threat modeling in federated learning.
  - Why needed here: The attack is framed in the context of malicious model providers; understanding the threat model clarifies attack goals.
  - Quick check question: In federated learning, why might a model provider have access to intermediate hidden states but not raw text?

## Architecture Onboarding

- Component map: Input-Adapter -> Decode Model (GPT-2 XL) -> Output-Adapter; defense = DCT -> overlap mask -> IDCT
- Critical path: (1) Capture hidden state from target layer. (2) Apply Embed Parrot to map to early-layer embedding. (3) Decode to text. (4) If defending, transform embedding before step 3.
- Design tradeoffs: Using a decoder-only model (GPT-2 XL) simplifies training but may limit the mapping's expressiveness; simpler defense uses DCT but is less robust than adversarial training.
- Failure signatures: Low ROUGE or F1 scores indicate mapping or decoding failure; high perplexity after defense indicates model performance degradation.
- First 3 experiments:
  1. Train Embed Parrot on a small dataset (e.g., 1k samples) and measure reconstruction accuracy on a validation set.
  2. Compare BEI, HEI, and Embed Parrot on a deep layer to confirm that Embed Parrot improves reconstruction.
  3. Apply the DCT-based defense to hidden states and measure both reconstruction failure and model perplexity change.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Embed Parrot's performance vary when applied to shallower layers of LLMs, as opposed to the deeper layers tested in this study?
- Basis in paper: [inferred] The paper mentions that Embed Parrot is designed to address the decreased effectiveness of other methods in deeper layers, but does not test its performance on shallower layers.
- Why unresolved: The paper focuses on the performance of Embed Parrot on deeper layers, leaving its effectiveness on shallower layers untested.
- What evidence would resolve it: Experimental results comparing Embed Parrot's performance on both shallow and deep layers of LLMs would provide clarity on its versatility and effectiveness across different layers.

### Open Question 2
- Question: Can the generalizability of Embed Parrot be improved by introducing more parameters, incorporating regularization techniques, or exploring alternative network architectures?
- Basis in paper: [explicit] The paper suggests that the current network architecture used for Embed Parrot is relatively simple, which may raise concerns regarding its generalizability.
- Why unresolved: The study does not explore alternative architectures or enhancements to the current model that could potentially improve its generalizability.
- What evidence would resolve it: Comparative studies of Embed Parrot's performance using different network architectures, parameter counts, and regularization techniques would indicate the potential for improving its generalizability.

### Open Question 3
- Question: How does the incorporation of Perplexity (PPL) as a loss function term affect Embed Parrot's performance on datasets with longer token sequences?
- Basis in paper: [explicit] The paper notes that while PPL did not optimize Embed Parrot's performance in most cases, it showed potential benefits for datasets with token sequences exceeding 512 tokens.
- Why unresolved: The study did not extensively explore the impact of PPL on datasets with varying token lengths, particularly longer sequences.
- What evidence would resolve it: Detailed experimental results analyzing Embed Parrot's performance with and without PPL across datasets with different token lengths would clarify the role of PPL in enhancing reconstruction quality for longer sequences.

## Limitations

- Missing training hyperparameters for Embed Parrot (learning rate, batch size, epochs) and DCT defense details (overlap matrix specifics, DCT/IDCT parameters) create significant reproduction barriers.
- Results are limited to Llama2-7B and ChatGLM-6B models with FinGPT and Wikitext datasets, raising questions about generalizability across different architectures and data distributions.
- The DCT-based defense mechanism's long-term effectiveness and resistance to adaptive attacks are not rigorously tested; claims about privacy guarantees are provisional.

## Confidence

- **High Confidence**: The feasibility of input reconstruction attacks on LLM embeddings is well-established in the literature; the paper's contribution is in demonstrating this specifically for deep layers of popular models.
- **Medium Confidence**: The Embed Parrot architecture and its relative performance gains over baseline methods (BEI, HEI) are supported by reported metrics, but exact replication is hindered by missing training details.
- **Low Confidence**: The DCT-based defense mechanism's long-term effectiveness and resistance to adaptive attacks are not rigorously tested; claims about its privacy guarantees are provisional.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Reproduce Embed Parrot training with a systematic sweep of learning rates, batch sizes, and loss function weights (cosine similarity vs. perplexity) to identify the impact on reconstruction accuracy and determine optimal configurations.

2. **Cross-Model Generalization Test**: Apply Embed Parrot to a different LLM family (e.g., GPT-3, BLOOM) and dataset (e.g., OpenWebText) to assess whether the attack's effectiveness holds beyond the specific models and data used in the paper.

3. **Adaptive Attack Resistance Evaluation**: Design an attacker who has partial knowledge of the DCT defense (e.g., knows the mask structure) and attempt to invert the transformation. Measure the success rate to quantify the defense's robustness against informed adversaries.