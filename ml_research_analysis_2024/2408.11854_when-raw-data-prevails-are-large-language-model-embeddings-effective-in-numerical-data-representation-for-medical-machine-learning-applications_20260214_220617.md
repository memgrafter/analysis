---
ver: rpa2
title: 'When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical
  Data Representation for Medical Machine Learning Applications?'
arxiv_id: '2408.11854'
source_url: https://arxiv.org/abs/2408.11854
tags: []
core_contribution: This study compares the effectiveness of Large Language Model (LLM)
  embeddings versus raw numerical EHR data for medical machine learning tasks. Researchers
  tested zero-shot LLM embeddings as feature inputs for traditional classifiers like
  XGBoost on clinical prediction tasks, including diagnosis, mortality, and length-of-stay
  prediction.
---

# When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?

## Quick Facts
- arXiv ID: 2408.11854
- Source URL: https://arxiv.org/abs/2408.11854
- Reference count: 19
- Large language model embeddings generally underperform raw numerical EHR data for medical predictions

## Executive Summary
This study systematically compares large language model (LLM) embeddings versus raw numerical electronic health record (EHR) data for clinical prediction tasks. Researchers tested zero-shot LLM embeddings as feature inputs for traditional classifiers like XGBoost on diagnosis, mortality, and length-of-stay prediction tasks. While LLMs demonstrated knowledge of normal medical values, raw data features consistently outperformed LLM embeddings across most tasks. The performance gaps suggest that current LLM embedding approaches need methodological improvements to match the effectiveness of raw numerical data for clinical predictions.

## Method Summary
The researchers converted tabular EHR data to text using four methods (NARRATIVES, JSON, HTML, MARKDOWN), then generated LLM embeddings using zero-shot, few-shot, and prompt engineering approaches. They trained ML classifiers (XGBoost, Random Forest, Logistic Regression) on both raw data features and LLM embeddings, comparing performance using AUROC with 95% confidence intervals. The study used two datasets: 660 patients for diagnosis prediction and MIMIC-III Extract with 23,884 patients for mortality and length-of-stay prediction tasks.

## Key Results
- Raw numerical features generally outperformed LLM embeddings across most clinical prediction tasks
- Zero-shot LLM embeddings achieved competitive results in some cases, suggesting promising avenues for future research
- Different table-to-text conversion methods significantly impacted embedding quality, with NARRATIVES and JSON generally outperforming HTML and MARKDOWN formats

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM embeddings can outperform randomly initialized embeddings when paired with traditional ML classifiers like XGBoost.
- Mechanism: The pre-trained knowledge embedded in LLM weights captures medical terminology and relationships, which provides more informative features than random noise for downstream classification tasks.
- Core assumption: The LLM's pre-training includes sufficient medical knowledge to be useful for clinical feature extraction, even without fine-tuning.
- Evidence anchors:
  - [abstract] "zero-shot LLM embeddings demonstrate competitive results, suggesting a promising avenue for future research in medical applications."
  - [section 5.1] "For LLM embeddings with zero-shot setting, we observed performance gain over a randomly initialized embedding approach into XGB with substantial gains in all decoder LLMs."
- Break condition: If the LLM's pre-training corpus lacks medical domain coverage, the embeddings would not capture relevant clinical knowledge, reducing to random performance.

### Mechanism 2
- Claim: Table-to-text conversion methods affect embedding quality and downstream ML performance.
- Mechanism: Different serialization formats (NARRATIVE, JSON, HTML, MARKDOWN) provide varying levels of contextual information and structural clarity to the LLM, influencing how well it can encode the numerical features.
- Core assumption: The LLM can parse and interpret structured data representations, and the chosen format preserves clinically relevant relationships.
- Evidence anchors:
  - [section 4.1] "We employed four different methods to convert EHR tables into input formats for LLMs: NARRATIVES, JSON, HTML, and MARKDOWN."
  - [section 5.3] "When encoding data with different formats, Mistral shows preference for NARRATIVES, JSON, and HTML. The MARKDOWN format generally yielded the lowest performance across the models."
- Break condition: If the LLM cannot meaningfully interpret the serialization format, all formats would yield similar (poor) performance.

### Mechanism 3
- Claim: Prompt engineering and system instructions can significantly impact the quality of LLM embeddings for clinical tasks.
- Mechanism: Different personas, thinking styles, and question types guide the LLM to focus on different aspects of the input data, affecting which features are emphasized in the embeddings.
- Core assumption: Instruction-tuned LLMs are sensitive to prompt framing and will adjust their internal representations based on the provided context.
- Evidence anchors:
  - [section 4.4] "Given that instruction-tuned LLMs are known to be sensitive to system instructions, we designed four system instructions that vary by persona, tasks, thinking style, and question type."
  - [section 5.4] "Mistral, under 0-shot with a system instruction with persona of medical professional and the task of assessing patient condition (prompt 1 in Table 8), achieved an AUROC of 71.35 on Sepsis prediction, the highest of all models."
- Break condition: If the LLM's instruction-tuning makes it resistant to prompt variations, different prompts would yield similar performance.

## Foundational Learning

- Concept: Vector representation and embedding extraction methods (Max Pooling, Mean Pooling, Last Token, First Token)
  - Why needed here: Understanding how different pooling strategies aggregate token-level information into fixed-size feature vectors is crucial for interpreting embedding quality and selecting appropriate methods.
  - Quick check question: If a sequence has token embeddings [v1, v2, v3] and you apply Max Pooling, what would the resulting embedding be?

- Concept: Electronic Health Record (EHR) data structure and clinical feature types
  - Why needed here: The study uses specific clinical features (vital signs, labs, demographics) that have medical significance; understanding their nature helps interpret why certain embeddings work better.
  - Quick check question: What type of clinical data would be most likely to have clear reference ranges that an LLM might encode?

- Concept: Machine learning classifier evaluation metrics (AUROC, confidence intervals)
  - Why needed here: The study compares different feature representations using statistical measures; understanding these metrics is essential for interpreting performance differences.
  - Quick check question: If a classifier has AUROC of 0.7 for a binary task, what does this indicate about its discriminative ability?

## Architecture Onboarding

- Component map: Raw EHR data (tabular format) → Table-to-text conversion → LLM input text → LLM last hidden states → Embedding extraction (pooling) → ML classifier (XGBoost, Logistic Regression, Random Forest) → Prediction output
- Critical path: Data conversion → LLM selection and zero-shot performance → Embedding extraction method → ML classifier choice
- Design tradeoffs:
  - LLM size vs. computational efficiency: Larger models may capture more medical knowledge but require more GPU memory and inference time
  - Conversion method complexity vs. interpretability: NARRATIVES are human-readable but may introduce noise; JSON is structured but less contextual
  - Embedding dimensionality vs. classifier performance: Higher dimensions capture more information but may require more training data to avoid overfitting
- Failure signatures:
  - Random embeddings perform as well as LLM embeddings → LLM lacks relevant medical knowledge or conversion method fails
  - All formats perform similarly poorly → LLM cannot interpret structured data effectively
  - Performance drops with larger training sets for embeddings but improves for raw data → Embeddings not capturing task-specific patterns that raw data contains
- First 3 experiments:
  1. Compare embedding extraction methods (Max Pooling, Mean Pooling, Last Token) on a single task with fixed conversion method to identify best pooling strategy
  2. Test different table-to-text conversion formats (NARRATIVE vs JSON vs HTML vs MARKDOWN) with the best embedding extraction from experiment 1
  3. Evaluate prompt engineering variations (different personas, thinking styles) using the best format and extraction method from experiments 1-2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM embeddings be optimized to match or exceed raw data performance in imbalanced medical prediction tasks?
- Basis in paper: [inferred] The paper shows LLM embeddings underperform raw data in imbalanced tasks like mortality prediction, suggesting methodological improvements are needed.
- Why unresolved: Current embeddings lack task-specific adaptation for rare event prediction, and the paper only explores limited fine-tuning approaches.
- What evidence would resolve it: Systematic comparison of different fine-tuning strategies, distillation methods, or embedding architectures on highly imbalanced medical datasets showing competitive or superior performance to raw data baselines.

### Open Question 2
- Question: Which embedding extraction method (max pooling, mean pooling, last token) is optimal for different types of clinical prediction tasks?
- Basis in paper: [explicit] The paper shows different embedding methods yield varying performance across models, with max pooling best for Mistral/Meditron and mean pooling for Llama3-8b.
- Why unresolved: The paper doesn't provide theoretical justification for why different models perform better with different pooling methods, nor does it explore the impact on task-specific performance.
- What evidence would resolve it: Task-specific ablation studies demonstrating consistent performance patterns across embedding methods, along with analysis of what clinical features each method captures.

### Open Question 3
- Question: How does table-to-text conversion format impact LLM embedding quality for numerical clinical data?
- Basis in paper: [explicit] The paper tests NARRATIVES, JSON, HTML, and MARKDOWN formats, finding Mistral prefers NARRATIVES/JSON/HTML while Llama3-8b shows high variability across formats.
- Why unresolved: The paper doesn't explain why certain formats work better for specific models or how format choice interacts with clinical data characteristics.
- What evidence would resolve it: Systematic evaluation of format performance across diverse clinical datasets and models, plus qualitative analysis of what clinical information each format preserves or loses.

## Limitations

- LLM embeddings generally underperform raw numerical data, suggesting fundamental limitations in how LLMs encode structured clinical information
- Table-to-text conversion methods significantly impact performance, but optimal approaches are highly task and model-dependent
- Results are sensitive to prompt engineering, raising concerns about reproducibility and generalizability across different clinical domains

## Confidence

**High Confidence Claims**:
- LLM embeddings can outperform randomly initialized embeddings when paired with traditional ML classifiers
- Raw numerical features generally outperform LLM embeddings for clinical prediction tasks
- Different table-to-text conversion methods significantly impact embedding quality

**Medium Confidence Claims**:
- Zero-shot LLM embeddings demonstrate competitive results in some cases
- Encoder LLMs with instruction tuning perform better than decoder LLMs
- Larger LLM sizes consistently improve performance

**Low Confidence Claims**:
- Few-shot learning with synthetic data generation consistently improves performance
- Specific prompt personas are optimal for clinical tasks
- MARKDOWN format consistently yields the lowest performance

## Next Checks

1. **Temporal Pattern Encoding Validation**: Design an experiment that specifically tests whether LLM embeddings capture temporal dependencies in time-varying clinical features by comparing performance on tasks with different temporal characteristics.

2. **Conversion Method Optimization**: Systematically explore hybrid conversion approaches that combine the strengths of different formats and evaluate whether these can close the performance gap with raw data.

3. **Prompt Engineering Systematic Study**: Conduct a comprehensive grid search over prompt variations across multiple LLM architectures to identify optimal prompt templates and assess the reproducibility of prompt-dependent performance differences.