---
ver: rpa2
title: Optimizing Novelty of Top-k Recommendations using Large Language Models and
  Reinforcement Learning
arxiv_id: '2406.14169'
source_url: https://arxiv.org/abs/2406.14169
tags:
- items
- novelty
- policy
- query
- top-k
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the challenge of optimizing novelty in top-k
  recommendations, where novelty is difficult to optimize due to the non-differentiable
  sorting operation and lack of user feedback data for novel items. The authors propose
  using large language models (LLMs) to provide relevance feedback for novel items,
  combined with a reinforcement learning (RL) approach to optimize novelty.
---

# Optimizing Novelty of Top-k Recommendations using Large Language Models and Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.14169
- Source URL: https://arxiv.org/abs/2406.14169
- Reference count: 40
- One-line primary result: PG-Ret achieves 2X-5X gains in novel keywords in top-50 and over 2X improvement in novelty for top-20 items with minimal loss in recall and precision

## Executive Summary
This paper addresses the challenge of optimizing novelty in top-k recommendations, where novelty is difficult to optimize due to the non-differentiable sorting operation and lack of user feedback data for novel items. The authors propose using large language models (LLMs) to provide relevance feedback for novel items, combined with a reinforcement learning (RL) approach to optimize novelty. To address the large action space problem in RL, they reformulate the state space to consist of <query, item> tuples and reduce the action space to a binary decision. This reformulation leads to a significantly lower complexity when the number of items is large.

## Method Summary
The paper introduces PG-Ret, a reinforcement learning algorithm that optimizes novelty in top-k recommendations by reformulating the problem as binary decisions for each (query, item) pair. The algorithm uses LLMs (GPT-3.5/GPT-4) as reward models to provide relevance feedback for novel items, enabling optimization without requiring user click data. The RL policy is initialized with a supervised model trained using InfoNCE loss on user feedback data. During training, the algorithm samples items from three sources: current policy, exploration model, and training data, then uses LLM feedback to compute rewards based on novelty and relevance.

## Key Results
- PG-Ret (Conservative) obtains 2X-5X gains in novel keywords in top-50 recommendations
- Over 2X improvement in novelty for the top-20 items in the ORCAS dataset
- Minimal loss in recall and precision compared to supervised finetuning
- Significant novelty gains achieved with a substantially reduced sample complexity due to binary-action reformulation

## Why This Works (Mechanism)

### Mechanism 1
Large language models (LLMs) can provide relevance feedback for novel items, enabling optimization of novelty without requiring user click data. The paper leverages LLMs like GPT-3.5 to act as reward models that can assess semantic relevance between queries and items, even when those items have never been shown to users before.

### Mechanism 2
Reformulating the RL problem from selecting top-k items to binary decisions for each (query, item) pair dramatically reduces sample complexity. Instead of treating the entire top-k list as the action (with combinatorial explosion), the problem is reformulated so that the state is a (query, item) pair and the action is whether to include that item in the recommendation.

### Mechanism 3
Initializing the RL policy with a supervised model trained on user feedback data provides a better starting point than random initialization, leading to faster convergence. The paper uses a supervised model trained with InfoNCE loss as the initial policy for RL finetuning, ensuring the initial policy has learned useful representations from real user data before applying novelty optimization.

## Foundational Learning

- **Concept:** Reinforcement Learning with Large Action Spaces
  - Why needed here: The recommendation problem involves selecting from millions of candidate items, creating a massive action space that standard RL algorithms struggle with.
  - Quick check question: What is the theoretical relationship between action space size and sample complexity in policy gradient algorithms?

- **Concept:** Contrastive Learning for Recommendation Systems
  - Why needed here: The supervised initialization relies on contrastive learning objectives like InfoNCE to learn meaningful item representations from user feedback data.
  - Quick check question: How does the InfoNCE loss encourage similar representations for clicked query-item pairs compared to negative pairs?

- **Concept:** Novelty Metrics in Recommender Systems
  - Why needed here: The paper defines novelty relative to a base model's recommendations, requiring understanding of how to measure and optimize for this specific notion of novelty.
  - Quick check question: How is novelty@k defined in terms of the base model's top-L predictions?

## Architecture Onboarding

- **Component map:** Query encoder (trainable) -> Item encoder (fixed) -> LLM reward model (GPT-3.5/GPT-4) -> Base model (for novelty comparison) -> Supervised initialization model

- **Critical path:** 1. Sample query from training data, 2. Compute similarity scores between query and all items, 3. Sample items using mixture of policy, exploration model, and training data, 4. Get LLM relevance feedback for each (query, item) pair, 5. Compute rewards based on novelty and relevance, 6. Calculate policy gradient loss, 7. Update query encoder parameters

- **Design tradeoffs:** Fix item encoder to avoid recomputing embeddings vs. potentially better performance with end-to-end training; Use contrastive losses for efficiency vs. exact softmax computation; Aggressive novelty optimization (higher novelty, lower accuracy) vs. conservative approach

- **Failure signatures:** Reward hacking: Policy learns to exploit LLM reward function rather than optimize true novelty; Catastrophic forgetting: Policy loses ability to recommend relevant items while optimizing for novelty; Slow convergence: Poor initialization or inadequate exploration prevents finding good policies

- **First 3 experiments:** 1. Train supervised model with InfoNCE loss on available training data and verify it outperforms random initialization on relevance metrics, 2. Implement PG-Ret with conservative novelty rewards and verify it achieves higher novelty than supervised model while maintaining reasonable recall, 3. Test different sampling strategies for (query, item) pairs to find optimal balance between exploration and exploitation

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of LLM (GPT-3.5 vs GPT-4) impact the accuracy and computational cost of relevance feedback for novel items? The paper uses GPT-3.5 for training and GPT-4 for evaluation, noting that GPT-4 provides more reliable feedback, but doesn't compare the performance of different LLMs in terms of accuracy, sample efficiency, or cost during training.

### Open Question 2
What is the optimal exploration-exploitation balance (α, β parameters) for different recommendation domains and dataset sizes? The paper mentions using three sources for sampling (current policy, exploration model, and training data) but doesn't provide systematic analysis of optimal parameter settings across different domains.

### Open Question 3
How does the binary-action reformulation affect convergence when the initial supervised model has poor novelty coverage? The paper assumes the initial model has higher than random probability for the optimal action, but doesn't analyze failure cases where the initial model has systematically poor novelty performance.

## Limitations

- The additive decomposition of novelty rewards across individual items is a critical assumption that may not hold for combinatorial notions of novelty where the novelty of a set depends on specific item combinations
- The reliability of LLM-based relevance feedback specifically for novel items in recommendation systems remains an open question, with potential for systematic bias
- The evaluation is conducted on three datasets from relatively similar domains, limiting knowledge about generalization to substantially different recommendation scenarios

## Confidence

**High Confidence**: The effectiveness of the binary-action reformulation for reducing sample complexity is well-supported by theoretical analysis and empirical results.

**Medium Confidence**: The use of LLM-based rewards for optimizing novelty is promising but depends on the reliability of LLM relevance assessment, which is supported by evidence but remains an assumption that could fail.

**Low Confidence**: The assumption that supervised model initialization provides significant benefits for RL convergence is plausible but not thoroughly validated through ablation studies.

## Next Checks

1. **Ablation Study on Action Space Formulation**: Implement a version of PG-Ret that does not assume additive reward decomposition and compare its performance and sample efficiency against the binary-action formulation to validate whether the reformulation is truly beneficial.

2. **LLM Reward Model Robustness Test**: Conduct a systematic evaluation of GPT-3.5's relevance assessment for novel items by comparing its judgments against human evaluations on a held-out test set to quantify the reliability of LLM rewards.

3. **Cross-Domain Generalization Study**: Apply PG-Ret to a fourth dataset from a substantially different domain (e.g., music or video recommendations) and compare its performance characteristics to test whether the algorithm's effectiveness generalizes beyond the studied domains.