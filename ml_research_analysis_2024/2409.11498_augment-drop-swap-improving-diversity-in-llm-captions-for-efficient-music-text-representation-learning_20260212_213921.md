---
ver: rpa2
title: 'Augment, Drop & Swap: Improving Diversity in LLM Captions for Efficient Music-Text
  Representation Learning'
arxiv_id: '2409.11498'
source_url: https://arxiv.org/abs/2409.11498
tags:
- training
- data
- text
- learning
- music
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning effective music-text
  representations under resource-constrained scenarios with limited data and compute
  budgets. The authors systematically study the impact of encoder choices and data
  curation on contrastive learning performance, finding that data curation quality
  outweighs dataset scale.
---

# Augment, Drop & Swap: Improving Diversity in LLM Captions for Efficient Music-Text Representation Learning

## Quick Facts
- arXiv ID: 2409.11498
- Source URL: https://arxiv.org/abs/2409.11498
- Reference count: 0
- Key outcome: Introduces Augment, Drop & Swap pipeline with tag-to-caption augmentation, Augmented View Dropout, and TextSwap to improve music-text contrastive learning performance, achieving state-of-the-art results on multiple benchmarks

## Executive Summary
This paper addresses the challenge of learning effective music-text representations under resource-constrained scenarios with limited data and compute budgets. The authors systematically study the impact of encoder choices and data curation on contrastive learning performance, finding that data curation quality outweighs dataset scale. To improve model diversity and robustness, they propose three novel text augmentation techniques: (1) tag-to-caption augmentation using LLMs, (2) Augmented View Dropout for generating diverse partial views of tags, and (3) TextSwap for creating hard negative examples by swapping descriptor keywords. Their proposed Augment, Drop & Swap pipeline consistently improves performance across different model configurations, pre-training regimes, and downstream datasets, establishing new state-of-the-art results on three benchmark datasets. Notably, their parameter-efficient approach achieves competitive results with less than 1% of total parameters trained, demonstrating effective alignment of locked text representations to audio through music-text contrastive learning.

## Method Summary
The paper proposes a training recipe called Augment, Drop & Swap to improve music-text representation learning through three novel augmentation techniques. The method uses dual-encoder multimodal contrastive learning with frozen pre-trained encoders (HTS-AT for audio, CLIP-T or mT5 for text) and a learnable two-head, two-layer Transformer projection module. The augmentation pipeline includes: (1) tag-to-caption augmentation using BLOOM-176B LLM to convert sparse tags into natural language captions, (2) Augmented View Dropout to create diverse partial views of tags by randomly masking descriptor words, and (3) TextSwap to generate hard negative examples by stochastically swapping keywords like genres and moods. The model is trained using InfoNCE loss with cosine similarity between l2-normalized projection embeddings, with TextSwap probability gradually increasing from 0 to 15% over 20 epochs.

## Key Results
- Achieves state-of-the-art results on MusicCaps, YT8M-MTC, and Song Describer datasets
- Data curation quality has greater impact on contrastive learning performance than dataset scale in resource-constrained scenarios
- Parameter-efficient approach achieves competitive results with less than 1% of total parameters trained
- Tag-to-caption augmentation improves performance, but using only synthetic captions degrades results on some datasets

## Why This Works (Mechanism)

### Mechanism 1
Data curation quality has greater impact on contrastive learning performance than dataset scale in resource-constrained scenarios. Curated datasets with high-quality, descriptive captions provide stronger semantic grounding and reduce noise in contrastive learning objectives, leading to better alignment in the embedding space. Descriptive captions contain sufficient information to capture semantic relationships between music and text. If captions become too generic or hallucinated, the benefit of curation diminishes.

### Mechanism 2
Tag-to-caption augmentation via LLMs enriches training data without requiring additional paired audio-text examples. LLMs transform sparse categorical tags into natural language captions, providing more context and specificity for contrastive learning while maintaining semantic alignment to the original audio content. LLMs can generate captions that preserve the semantic content of original tags while adding descriptive detail. If LLM-generated captions drift from audio content or become too generic, the augmentation becomes counterproductive.

### Mechanism 3
Introducing hard negative examples through TextSwap improves model robustness by increasing the complexity of contrastive learning. By stochastically swapping descriptor keywords in captions, TextSwap creates partially perturbed versions that are semantically closer to positives than random negatives, forcing the model to learn finer-grained distinctions. Hard negatives are more effective for learning discriminative features than easy negatives, especially in later stages of training. If TextSwap swaps keywords too frequently or creates implausible captions, it may introduce noise rather than useful contrastive examples.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The paper relies on contrastive learning framework to align music and text representations in a shared embedding space
  - Quick check question: How does the InfoNCE loss function encourage similar items to be closer in embedding space while pushing dissimilar items apart?

- Concept: Multimodal representation learning
  - Why needed here: Understanding how dual-encoder architectures process and align different modalities (audio and text) is crucial for grasping the proposed improvements
  - Quick check question: What are the key differences between single-modality and multimodal contrastive learning approaches?

- Concept: Data augmentation strategies in contrastive learning
  - Why needed here: The paper introduces novel augmentation techniques specifically designed for improving contrastive learning in music-text scenarios
  - Quick check question: How do data augmentation techniques create effective contrastive views while maintaining semantic alignment?

## Architecture Onboarding

- Component map: Audio encoder (MERT or HTS-AT) -> Text encoder (CLIP-T or mT5) -> Projection module (two-head Transformer) -> Augmentation pipeline -> InfoNCE loss

- Critical path: Input audio and text pairs -> Process through modality-specific encoders -> Apply projection module to obtain embeddings -> Apply augmentation techniques to create contrastive views -> Compute cosine similarity and InfoNCE loss -> Backpropagate and update projection parameters

- Design tradeoffs: Freezing encoders vs. fine-tuning reduces computational cost but may limit adaptation to music domain; tag vs. caption inputs balance efficiency and descriptiveness; hard vs. easy negatives improve discrimination but may introduce noise

- Failure signatures: Poor retrieval performance across all datasets suggests issues with encoder alignment or projection module; inconsistent performance across datasets indicates distribution shift or overfitting; degradation when using captions instead of tags suggests LLM hallucinations

- First 3 experiments: (1) Compare frozen MERT + frozen CLIP-T vs. frozen MERT + frozen RoBERTa on MusicCaps to validate CLIP-T advantage; (2) Test varying pcap values (0, 0.5, 1) on MusicTextHQ to measure impact of tag-to-caption augmentation; (3) Apply only Augmented View Dropout to DuET-MC configuration to isolate its effect from TextSwap

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of synthetic captions from LLMs affect downstream music retrieval performance compared to human-written captions? The paper shows that while tag-to-caption augmentation improves performance, using only synthetic captions degrades performance on some datasets compared to using tags or mixed inputs, suggesting issues with caption quality. The paper notes that pseudo-captions generated from sparse labels can be non-descriptive or inaccurate due to hallucinations, but doesn't systematically analyze the relationship between caption quality metrics and retrieval performance.

### Open Question 2
What is the optimal strategy for balancing text augmentation techniques across different stages of contrastive learning training? The paper applies TextSwap gradually (0-15% over 20 epochs) based on the hypothesis that hard negatives are more beneficial in later training stages, but doesn't systematically explore different scheduling strategies or optimal timing. The paper only tests one linear scheduling approach for TextSwap and doesn't compare it against other strategies like adaptive scheduling based on training progress.

### Open Question 3
How do distribution shifts in evaluation datasets affect the reliability of automatic metrics for music-text retrieval? The paper observes significant performance differences across datasets and conducts a human evaluation study revealing that metrics may be inflated due to in-distribution bias. While the paper identifies this issue through human evaluation, it doesn't quantify the relationship between distribution shift and metric reliability or propose methods to calibrate metrics for cross-dataset evaluation.

## Limitations
- Limited ablation of individual augmentation effects - the contribution of each technique to overall performance is not fully isolated
- Dependence on LLM quality for tag-to-caption augmentation - effectiveness relies heavily on quality and consistency of BLOOM-176B
- Evaluation on limited downstream tasks - focuses primarily on text-based music retrieval rather than broader applications

## Confidence
- High confidence: Core finding that data curation quality is more important than dataset scale is well-supported by systematic experiments
- Medium confidence: Claim of establishing new state-of-the-art results is supported but contribution of each component could be more thoroughly analyzed
- Low confidence: Parameter-efficient claim is somewhat misleading as it refers to projection module parameters while frozen encoders still contain millions of parameters

## Next Checks
1. **Comprehensive ablation study**: Conduct systematic ablation isolating effects of each augmentation technique and their interactions across different training stages and dataset compositions

2. **LLM robustness analysis**: Evaluate impact of different LLM choices on tag-to-caption generation quality and downstream performance, analyzing how caption quality variations affect retrieval results

3. **Extended downstream evaluation**: Test learned representations on additional tasks beyond text-based music retrieval, such as music classification, genre prediction, or cross-modal generation tasks