---
ver: rpa2
title: 'WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code
  and Interacting with the Environment'
arxiv_id: '2402.12275'
source_url: https://arxiv.org/abs/2402.12275
tags:
- wall
- state
- agent
- name
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents WorldCoder, a model-based LLM agent that learns
  world models by writing code and interacting with the environment. The agent builds
  a Python program representing its knowledge of the world, explaining its interactions
  while being optimistic about achievable rewards.
---

# WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment

## Quick Facts
- arXiv ID: 2402.12275
- Source URL: https://arxiv.org/abs/2402.12275
- Authors: Hao Tang; Darren Key; Kevin Ellis
- Reference count: 40
- Primary result: WorldCoder learns world models by writing Python code, showing sample-efficiency advantages over deep RL and compute-efficiency over ReAct-style agents while enabling transfer learning across environments

## Executive Summary
WorldCoder presents a novel model-based LLM agent that learns world models through iterative code synthesis and refinement. The agent writes Python programs to represent transition and reward functions, using logical constraints to ensure data consistency and optimism under uncertainty. Tested on gridworlds and task planning, WorldCoder demonstrates superior sample efficiency compared to deep RL and better computational efficiency than ReAct-style agents, while uniquely enabling knowledge transfer across environments through code editing.

## Method Summary
WorldCoder is a model-based LLM agent that learns world models by writing and refining Python code to explain observed state transitions and predict future states. The agent collects experience tuples and uses an LLM to generate candidate programs, which are checked against data consistency (ϕ1) and goal achievability (ϕ2) constraints. When constraints are violated, the agent refines its code iteratively using the REx algorithm. For planning, WorldCoder uses Monte Carlo Tree Search or value iteration depending on environment complexity. The approach enables transfer learning by editing existing world models when moving to new environments rather than building from scratch.

## Key Results
- Demonstrated more sample-efficiency compared to deep RL baselines on Sokoban and Minigrid environments
- Achieved more compute-efficiency than ReAct-style agents while maintaining similar task completion rates
- Successfully transferred knowledge across environments by editing existing code models, reducing learning time in new tasks

## Why This Works (Mechanism)

### Mechanism 1: Iterative Code Refinement for World Modeling
- **Claim:** WorldCoder learns world models by iteratively refining Python code to fit observed state transitions and achieve goals
- **Mechanism:** The agent collects experience tuples (state, action, reward, next_state, goal) and uses them to generate candidate programs. It then checks these programs against the collected data for consistency (ϕ1) and goal achievability (ϕ2). If inconsistencies are found, the LLM is prompted to debug and refine the code, creating an iterative loop until the program satisfies both constraints
- **Core assumption:** LLMs can generate and debug Python code effectively based on structured feedback about data inconsistencies and goal reachability
- **Evidence anchors:** [abstract] "The agent builds a Python program representing its knowledge of the world based on its interactions with the environment"; [section 2.5] "One approach to program synthesis is to have an LLM iteratively improve and debug its initial outputs [7, 47], which we call refinement"
- **Break condition:** If the LLM cannot generate programs that satisfy both data consistency and goal achievability, the refinement loop will fail to converge to a usable world model

### Mechanism 2: Optimism-Driven Exploration
- **Claim:** Optimism under uncertainty (ϕ2) drives goal-directed exploration in sparse reward environments
- **Mechanism:** When the agent has no experience with a goal, ϕ2 forces it to imagine a reward function that it believes it could achieve but hasn't yet. This creates a temporary optimistic reward model that guides exploration toward potentially rewarding states. As the agent explores, it collects data that either confirms or refutes this optimism, leading to model updates
- **Core assumption:** Imagining achievable rewards when uncertain creates more efficient exploration than random exploration
- **Evidence anchors:** [abstract] "The world model tries to explain its interactions, while also being optimistic about what reward it can achieve"; [section 2.2] "One less obvious learning objective is that the world model should suffice to plan to the goal. Given two world models, both consistent with the observed data, the agent should prefer a model which implies it is possible to get positive reward, effectively being optimistic in the face of uncertainty about world dynamics"
- **Break condition:** If the agent's optimism consistently fails to match reality, it may waste exploration time pursuing impossible goals

### Mechanism 3: Code-Based Transfer Learning
- **Claim:** Code-based world models enable efficient transfer learning across environments
- **Mechanism:** When moving to a new environment, WorldCoder can start with a world model from a previous environment and refine it rather than building from scratch. This works because the code structure (entities, actions, transitions) can be partially reused, with only new dynamics requiring additional synthesis
- **Core assumption:** World dynamics across environments share enough structural similarity that code components can be meaningfully reused
- **Evidence anchors:** [abstract] "that it can transfer its knowledge across environments by editing its code"; [section 3] "To better understand the transfer learning and instruction following aspects of our approach, we next study Minigrid [9, 8], a suite of grid games designed for language-conditioned RL"
- **Break condition:** If new environments have completely different dynamics or entity types, transfer may provide minimal benefit or require extensive rewriting

## Foundational Learning

- **Concept:** Model-based reinforcement learning (MBRL)
  - Why needed here: WorldCoder fundamentally operates as an MBRL agent, learning a transition function to plan rather than learning a policy directly
  - Quick check question: What distinguishes model-based from model-free RL in terms of how they use experience?

- **Concept:** Program synthesis from examples
  - Why needed here: WorldCoder uses program synthesis to generate Python code that explains observed state transitions and predicts future states
  - Quick check question: How does program synthesis differ from traditional function approximation in machine learning?

- **Concept:** Logical constraints for learning objectives
  - Why needed here: WorldCoder uses logical constraints (ϕ1 for data consistency, ϕ2 for optimism) to guide program synthesis, creating a formal learning framework
  - Quick check question: How do logical constraints provide more interpretable learning objectives compared to loss functions?

## Architecture Onboarding

- **Component map:** State observation -> Action selection via planner -> Environment step -> Store experience -> Trigger synthesis if constraints violated -> LLM refines code -> Update world model -> Repeat
- **Critical path:** State observation → Action selection via planner → Environment step → Store experience → Trigger synthesis if constraints violated → LLM refines code → Update world model → Repeat
- **Design tradeoffs:**
  - Code vs neural models: More interpretable and transferable but requires symbolic state representation
  - Refinement vs from-scratch synthesis: More efficient but may get stuck in local optima
  - Optimism level: Too optimistic wastes exploration; too conservative slows learning
- **Failure signatures:**
  - Code won't compile or run: LLM generation issues
  - Programs fail consistency checks: Insufficient data or overly complex dynamics
  - Agent doesn't explore effectively: Optimism objective too weak or incorrectly specified
  - Transfer fails: Environments too dissimilar or code structure incompatible
- **First 3 experiments:**
  1. Gridworld with simple deterministic dynamics to verify basic synthesis works
  2. Gridworld with sparse rewards to test optimism-driven exploration
  3. Environment transfer test: solve environment A, then refine model for environment B

## Open Questions the Paper Calls Out

- **Open Question 1:** How does WorldCoder's approach scale to continuous state and action spaces compared to its current discrete and symbolic setup?
  - Basis in paper: [inferred] from the limitations section which discusses the need for bridging the gap between perception and symbol processing
  - Why unresolved: The paper demonstrates success in discrete, symbolic environments but acknowledges this is a limitation for real-world applications
  - What evidence would resolve it: Empirical results showing WorldCoder's performance on continuous environments or a theoretical framework for extending the approach to continuous spaces

- **Open Question 2:** What is the impact of using different planners (MCTS vs Value Iteration) on WorldCoder's performance across various environment complexities?
  - Basis in paper: [explicit] The paper mentions using different planners but doesn't provide a systematic comparison of their effectiveness
  - Why unresolved: While the paper mentions using MCTS for complex tasks and value iteration for simple domains, it doesn't analyze the trade-offs between these approaches
  - What evidence would resolve it: A comprehensive ablation study comparing performance, sample efficiency, and computational costs across different planners and environment complexities

- **Open Question 3:** How does WorldCoder's performance change when incorporating probabilistic programs instead of deterministic ones?
  - Basis in paper: [inferred] from the limitations section which mentions the assumption of deterministic dynamics as a current limitation
  - Why unresolved: The paper acknowledges this as a limitation but doesn't explore the potential benefits or challenges of probabilistic programming
  - What evidence would resolve it: Empirical results showing WorldCoder's performance with probabilistic programs on environments with stochastic dynamics

## Limitations

- The reliance on GPT-4 for program synthesis creates substantial computational overhead and cost barriers that may limit real-world applicability
- The approach requires symbolic state representations, which restricts its applicability to environments with well-defined entities and relationships
- The evaluation focuses primarily on relatively simple gridworld environments and task planning, with limited testing in complex, high-dimensional real-world scenarios

## Confidence

- **High confidence:** The core mechanism of using LLM-generated Python code as world models is technically sound and well-implemented
- **Medium confidence:** Sample efficiency improvements over deep RL and transfer learning capabilities are demonstrated but may not generalize to more complex environments
- **Medium confidence:** The optimism-driven exploration mechanism shows promise but requires more extensive validation in diverse environments
- **Low confidence:** Claims about compute efficiency relative to ReAct-style agents need more rigorous benchmarking

## Next Checks

1. **Robustness test:** Evaluate WorldCoder's performance when 50% of state observations are noisy or incomplete to test the robustness of the code-based world model to imperfect data
2. **Scaling experiment:** Test the approach on environments with significantly larger state and action spaces (e.g., Montezuma's Revenge or complex robotics tasks) to assess scalability limitations
3. **Cost analysis:** Perform a detailed computation cost comparison between WorldCoder and baseline methods across different model sizes and training durations to validate efficiency claims beyond theoretical arguments