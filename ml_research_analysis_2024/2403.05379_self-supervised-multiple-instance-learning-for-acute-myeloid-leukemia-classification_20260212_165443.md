---
ver: rpa2
title: Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia Classification
arxiv_id: '2403.05379'
source_url: https://arxiv.org/abs/2403.05379
tags:
- learning
- encoder
- supervised
- pre-training
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of self-supervised learning (SSL)
  methods for pre-training encoders in a multiple instance learning (MIL) framework
  for classifying genetic subtypes of Acute Myeloid Leukemia (AML) from blood smear
  images. The authors compare three SSL methods - SimCLR, SwAV, and DINO - against
  a fully supervised encoder on a dataset of 218 patients with 5 AML subtypes.
---

# Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia Classification

## Quick Facts
- arXiv ID: 2403.05379
- Source URL: https://arxiv.org/abs/2403.05379
- Reference count: 26
- Key outcome: SSL-pretrained encoders achieve comparable performance to supervised methods in MIL-based AML classification, with SimCLR performing best

## Executive Summary
This paper investigates self-supervised learning (SSL) methods for pre-training encoders in a multiple instance learning (MIL) framework to classify genetic subtypes of Acute Myeloid Leukemia (AML) from blood smear images. The authors compare three SSL methods—SimCLR, SwAV, and DINO—against a fully supervised encoder on a dataset of 218 patients with 5 AML subtypes. They find that SSL-pretrained encoders achieve comparable performance to the supervised encoder, with SimCLR performing best. The attention mechanism in the MIL framework successfully focuses on malignant cells, with SSL-pretrained encoders showing more pronounced attention differences between malignant and healthy cells. UMAP visualizations reveal that SSL-pretrained encoders can distinguish cell types without labels, even for ambiguous cells.

## Method Summary
The method employs SSL pre-training on ResNet18 using SimCLR, SwAV, or DINO, followed by MIL attention-based classification. The SSL phase learns representations from unlabeled single-cell images, then the MIL network fine-tunes these encoders to classify patient-level AML subtypes. The attention module identifies which individual cells contribute most to predictions, providing interpretability. Training uses 5-fold cross-validation with class balancing and mixed precision optimization.

## Key Results
- SSL-pretrained encoders achieve comparable macro F1 scores to fully supervised pre-training
- SimCLR outperforms SwAV and DINO across most AML subtypes
- Attention maps show SSL-pretrained models better distinguish malignant from healthy cells
- UMAP visualizations demonstrate label-free cell type separation with SSL features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised pre-training improves MIL feature quality by learning cell-level representations without labels, enabling better attention-based malignant cell identification.
- Mechanism: SSL methods (SimCLR, SwAV, DINO) learn embeddings that preserve cell type structure in latent space. These representations are then used by the MIL encoder to extract meaningful features, allowing the attention module to focus on cells most relevant to disease subtype.
- Core assumption: The structure of cell embeddings learned via SSL correlates with biological cell types and disease relevance, even without label supervision.
- Evidence anchors:
  - [abstract] "SSL-pretrained encoders achieve comparable performance, showcasing the potential of SSL in MIL"
  - [section] "The attention module integrated into the MIL classifier demonstrates its efficacy in focusing on malignant cells. These cells consistently receive higher attention values, enhancing the model's interpretability."
  - [corpus] Weak evidence - no direct SSL-MIL integration studies found in corpus.
- Break Condition: If SSL pretraining fails to preserve cell-level discriminative features, attention mechanism will not effectively identify malignant cells, degrading classification performance.

### Mechanism 2
- Claim: SSL pretraining reduces reliance on expensive single-cell annotations while maintaining classification accuracy.
- Mechanism: SSL methods leverage unlabeled cell images to train encoders, bypassing the need for costly expert annotations. The learned features generalize well enough for downstream MIL classification.
- Core assumption: Unlabeled cell images contain sufficient structural and visual information for SSL to learn meaningful representations.
- Evidence anchors:
  - [abstract] "removing the need for labeled data during encoder training"
  - [section] "Unlike supervised pre-training methods that rely on costly annotations, SSL techniques enable the encoder to learn informative representations from abundant unlabeled data without explicit annotations."
  - [corpus] Weak evidence - limited studies on SSL for medical image classification without annotations.
- Break Condition: If the unlabeled dataset is too small or lacks diversity, SSL pretraining will fail to capture relevant features, forcing reliance on labeled data anyway.

### Mechanism 3
- Claim: Different SSL methods (SimCLR, SwAV, DINO) produce varying levels of class separation in the latent space, affecting downstream MIL performance.
- Mechanism: Each SSL method uses a different strategy (contrastive, clustering, distillation) to organize embeddings. These organizational differences impact how well the MIL classifier can separate cell types and focus attention.
- Core assumption: The quality of latent space organization directly impacts the ability of MIL to identify relevant cells.
- Evidence anchors:
  - [section] "Different pre-training methods exhibit varying classification performances across different classes... SimCLR demonstrated superior performance in predicting CBF B :: M Y H11 fusion"
  - [section] "Examining the attention patterns of classifiers trained with different pre-trained SSL encoders... SimCLR encoder exhibits a larger gap in average attention between malignant and healthy cells"
  - [corpus] Weak evidence - no direct comparison of SSL methods for MIL in medical imaging.
- Break Condition: If SSL method choice has negligible impact on latent space quality, differences in MIL performance will be minimal.

## Foundational Learning

- Concept: Multiple Instance Learning (MIL)
  - Why needed here: MIL handles weakly labeled data where only bag-level (patient) labels are available, not individual cell labels. This matches the AML dataset structure.
  - Quick check question: In MIL, if a bag contains at least one positive instance, what is the bag label? (Answer: Positive)

- Concept: Self-Supervised Learning (SSL)
  - Why needed here: SSL learns useful representations from unlabeled data, addressing the scarcity of annotated single-cell images in medical domains.
  - Quick check question: What is the key difference between contrastive and clustering-based SSL methods? (Answer: Contrastive maximizes similarity between augmentations of same image while clustering groups similar instances)

- Concept: Attention Mechanisms
  - Why needed here: Attention allows the MIL model to identify which individual cells within a bag are most relevant for classification, providing interpretability.
  - Quick check question: How does attention help explain MIL predictions? (Answer: By ranking cells based on their contribution to the bag-level prediction)

## Architecture Onboarding

- Component map: Input images → SSL pretraining (SimCLR/SwAV/DINO) → Encoder (ResNet18) → Feature extraction → MIL attention network → Classification head → Output (AML subtype)
- Critical path: SSL pretraining → Encoder → Attention mechanism → Classification
- Design tradeoffs: SSL pretraining trades computational cost and complexity for reduced labeling requirements. Choice of SSL method affects performance but adds implementation complexity.
- Failure signatures: Poor attention focus (uniform attention across cells), degraded classification performance compared to supervised baseline, failure to generalize to unseen patients.
- First 3 experiments:
  1. Verify SSL pretraining works: Train SimCLR on unlabeled data, visualize embedding space with UMAP, check for clustering.
  2. Test attention mechanism: Run trained MIL model, visualize attention maps, verify malignant cells receive higher attention.
  3. Compare SSL methods: Train MIL with each SSL-pretrained encoder, measure F1 scores, identify best-performing method.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the different SSL methods (SimCLR, SwAV, DINO) perform when scaling to larger datasets with more diverse cell types?
- Basis in paper: [explicit] The paper mentions that SSL methods can leverage unlabeled data in medical environments and discusses the potential of these methods, but does not explore performance on larger or more diverse datasets.
- Why unresolved: The study focuses on a specific AML dataset with limited diversity in cell types, and does not investigate the scalability of SSL methods to larger or more varied datasets.
- What evidence would resolve it: Comparative studies on larger, more diverse medical image datasets would provide insights into the scalability and generalizability of different SSL methods.

### Open Question 2
- Question: What are the specific mechanisms by which SSL-pretrained encoders improve the attention module's ability to focus on malignant cells?
- Basis in paper: [inferred] The paper notes that SSL-pretrained encoders show more pronounced attention differences between malignant and healthy cells, but does not delve into the underlying mechanisms.
- Why unresolved: The paper demonstrates improved attention performance but does not explore the internal mechanisms or feature representations that lead to this improvement.
- What evidence would resolve it: Detailed analysis of the feature representations and attention weights in SSL-pretrained vs. supervised models could elucidate the mechanisms behind improved attention.

### Open Question 3
- Question: How does the performance of SSL-pretrained encoders compare to other pre-training strategies, such as transfer learning from related domains or multi-task learning?
- Basis in paper: [explicit] The paper compares SSL methods to fully supervised pre-training but does not explore other pre-training strategies.
- Why unresolved: The study focuses on comparing SSL methods to supervised pre-training, leaving a gap in understanding how SSL compares to other potential pre-training approaches.
- What evidence would resolve it: Direct comparisons of SSL-pretrained encoders with models pre-trained using transfer learning or multi-task learning on the same datasets would provide a comprehensive evaluation of different pre-training strategies.

## Limitations
- Dataset size (218 patients) is relatively small for drawing broad conclusions about SSL effectiveness
- SSL pretraining still requires substantial unlabeled data and computational resources
- Performance differences between SSL methods lack statistical significance testing

## Confidence
**High confidence**: SSL pretraining can achieve comparable performance to supervised methods in this MIL framework. The attention mechanism successfully identifies malignant cells.
**Medium confidence**: SimCLR is definitively superior to other SSL methods for this task.
**Low confidence**: SSL pretraining meaningfully reduces labeling requirements or generalizes to larger, more diverse datasets.

## Next Checks
1. Conduct statistical significance tests (paired t-tests or McNemar's test) comparing SSL vs supervised performance to verify observed differences are not due to chance.
2. Test the SSL pretraining approach on a larger, independent AML dataset to assess generalizability and true labeling efficiency gains.
3. Perform ablation studies removing SSL pretraining to quantify the exact performance contribution and determine if the computational overhead is justified.