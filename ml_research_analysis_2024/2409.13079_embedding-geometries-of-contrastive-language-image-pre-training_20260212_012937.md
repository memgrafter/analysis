---
ver: rpa2
title: Embedding Geometries of Contrastive Language-Image Pre-Training
arxiv_id: '2409.13079'
source_url: https://arxiv.org/abs/2409.13079
tags:
- clip
- embedding
- entailment
- loss
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates alternative embedding geometries for contrastive
  language-image pre-training (CLIP), challenging the original design choices of L2
  normalization and cosine similarity. The authors systematically compare Euclidean,
  elliptic, and hyperbolic geometries with various softmax logits, including distance
  and distance-squared options.
---

# Embedding Geometries of Contrastive Language-Image Pre-Training

## Quick Facts
- arXiv ID: 2409.13079
- Source URL: https://arxiv.org/abs/2409.13079
- Authors: Jason Chuan-Chih Chou; Nahid Alam
- Reference count: 40
- Primary result: Euclidean CLIP (EuCLIP) matches or exceeds original CLIP and MERU performance on ImageNet with 35.17% accuracy

## Executive Summary
This study investigates alternative embedding geometries for contrastive language-image pre-training (CLIP), challenging the original design choices of L2 normalization and cosine similarity. The authors systematically compare Euclidean, elliptic, and hyperbolic geometries with various softmax logits, including distance and distance-squared options. Their primary finding is that Euclidean CLIP (EuCLIP) - using Euclidean geometry with distance-squared logit and no final LayerNorm - matches or exceeds the performance of both the original CLIP and the more complex MERU model on ImageNet and other zero-shot benchmarks.

## Method Summary
The authors evaluate three geometric embeddings (Euclidean, elliptic, hyperbolic) with four softmax logits (distance, distance-squared, cosine similarity, dot product) and two normalization schemes (with and without final LayerNorm). They train CLIP models with these configurations on 1.2M random ImageNet samples and compare performance on zero-shot ImageNet classification and hierarchical image retrieval. The study also investigates the necessity of entailment loss for separating text and image embeddings.

## Key Results
- EuCLIP achieves 35.17% accuracy on ImageNet zero-shot classification, exceeding CLIP's 34.73% and MERU's 33.84% for ViT-B/16 models
- Final LayerNorm severely impacts performance when output norm carries information, reducing degrees of freedom in embedding space
- Entailment loss is necessary to separate text and image embeddings, as this separation does not occur spontaneously
- Euclidean geometry with distance-squared logit supports hierarchical relationships as well as hyperbolic alternatives

## Why This Works (Mechanism)
The study reveals that the original CLIP's final LayerNorm, while beneficial for most transformer tasks, becomes detrimental when the embedding norm itself carries meaningful information about similarity. By removing this normalization and using Euclidean distance-squared as the similarity metric, the model can leverage the full embedding space for representing both semantic and geometric relationships. The distance-squared logit provides better gradient signals for optimization compared to raw distance or cosine similarity.

## Foundational Learning
- **Contrastive Learning**: Learning to map similar inputs (text-image pairs) closer in embedding space while pushing dissimilar pairs apart - needed to understand CLIP's fundamental objective
- **Geodesic Distance**: Distance between points on curved manifolds (hyperbolic/elliptic) - needed to understand how different geometries affect similarity measurements
- **Softmax Logits**: The choice of similarity function (dot product, cosine, distance, distance-squared) directly impacts how gradients flow during training - needed to understand the logit-ablation experiments
- **Layer Normalization**: A final LayerNorm after the transformer can eliminate useful information in the embedding norm - needed to understand why removing it helps
- **Entailment Loss**: Additional loss term that explicitly separates text and image embeddings - needed to understand why text-image separation doesn't occur spontaneously

## Architecture Onboarding

**Component Map**
- Image Encoder (ViT-B/16) -> Joint Embedding Space -> Softmax Logit -> Contrastive Loss
- Text Encoder (Transformer) -> Joint Embedding Space -> Softmax Logit -> Contrastive Loss
- Joint Embedding Space -> L2 Normalization (optional) -> Softmax Logit

**Critical Path**
Image/Text Encoder → Joint Embedding → Softmax Logit → Contrastive Loss → Parameter Updates

**Design Tradeoffs**
- Euclidean geometry offers simplicity and strong performance but may struggle with hierarchical data compared to hyperbolic alternatives
- Distance-squared logit provides better optimization signals but may amplify outliers compared to distance
- Removing final LayerNorm preserves embedding norm information but may affect training stability

**Failure Signatures**
- Performance drops when output norm carries information but final LayerNorm is present
- Inability to separate text and image embeddings without entailment loss
- Suboptimal hierarchical retrieval with distance logit compared to distance-squared

**3 First Experiments**
1. Train Euclidean CLIP with distance-squared logit without final LayerNorm on a small dataset
2. Compare performance of cosine similarity vs distance-squared logits with identical initialization
3. Train with and without entailment loss to observe text-image embedding separation

## Open Questions the Paper Calls Out
None

## Limitations
- Limited exploration of other architectures beyond ViT-B/16, such as ConvNets or larger ViT variants
- Computational constraints prevented testing on the full ImageNet-21k training set, using only 1.2M random samples
- Ablation studies don't exhaustively explore all combinations of geometry, logit, and normalization choices

## Confidence
- High: The empirical finding that final LayerNorm harms performance when output norm carries information
- Medium: The claim that Euclidean geometry with distance-squared logit matches or exceeds hyperbolic alternatives
- Medium: The necessity of entailment loss for separating text and image embeddings
- Low: Generalization of findings to other model architectures and datasets beyond ImageNet

## Next Checks
1. Test EuCLIP performance across different backbone architectures (ConvNets, larger ViT variants) to verify architectural robustness
2. Evaluate the proposed embedding geometries on additional zero-shot classification benchmarks beyond ImageNet to assess generalizability
3. Conduct ablation studies on the interaction between embedding dimension size and geometry choice to identify optimal configurations