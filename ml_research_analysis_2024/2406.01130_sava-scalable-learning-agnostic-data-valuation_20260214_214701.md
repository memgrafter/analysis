---
ver: rpa2
title: 'SAVA: Scalable Learning-Agnostic Data Valuation'
arxiv_id: '2406.01130'
source_url: https://arxiv.org/abs/2406.01130
tags:
- data
- training
- points
- dataset
- batches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SAVA introduces a scalable variant of LAVA for data valuation by
  leveraging hierarchical optimal transport (OT) on batches of data points instead
  of the entire dataset. This approach overcomes the memory bottleneck of LAVA, which
  has quadratic complexity O(N^2) with dataset size N, limiting its application to
  small datasets.
---

# SAVA: Scalable Learning-Agnostic Data Valuation

## Quick Facts
- arXiv ID: 2406.01130
- Source URL: https://arxiv.org/abs/2406.01130
- Authors: Samuel Kessler; Tam Le; Vu Nguyen
- Reference count: 40
- SAVA enables data valuation on large-scale datasets with millions of points while maintaining comparable performance to LAVA

## Executive Summary
SAVA introduces a scalable variant of LAVA for data valuation by leveraging hierarchical optimal transport (OT) on batches of data points instead of the entire dataset. This approach overcomes the memory bottleneck of LAVA, which has quadratic complexity O(N²) with dataset size N, limiting its application to small datasets. SAVA performs OT computations on smaller batches, enabling data valuation on large-scale datasets with millions of data points.

The method maintains comparable data valuation performance to LAVA while achieving significant scalability improvements. On benchmark problems, SAVA achieves similar corruption detection rates and improved data selection performance on large web-scraped datasets like Clothing1M, outperforming other methods in accuracy while overcoming memory limitations.

## Method Summary
SAVA extends LAVA by decomposing the optimal transport problem into smaller batch-level computations. Instead of computing OT between entire training and validation sets, SAVA partitions both datasets into batches and computes OT between each batch pair. The final data valuations are obtained by aggregating batch-level gradients weighted by the optimal transport plan between batches. This hierarchical approach reduces computational complexity while maintaining valuation quality through careful calibration of entropic regularization effects.

## Key Results
- Scales LAVA to datasets with millions of data points by using hierarchical optimal transport on batches
- Maintains comparable corruption detection rates to LAVA while overcoming memory limitations
- Achieves improved data selection performance on large web-scraped datasets like Clothing1M
- Outperforms other data valuation methods in accuracy on large-scale problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical optimal transport (HOT) enables scalable data valuation by decomposing a large OT problem into multiple smaller OT problems on batches.
- Mechanism: Instead of solving one expensive OT problem on the full dataset (quadratic memory O(N²)), SAVA partitions the training and validation sets into batches and solves OT problems between each pair of batches. The final valuation scores are computed by aggregating batch-level gradients weighted by the optimal transport plan between batches.
- Core assumption: The hierarchical OT approximates the full OT well enough for data valuation purposes, and the approximation error is bounded and acceptable.
- Evidence anchors:
  - [abstract] "SAVA performs OT computations on smaller batches, enabling data valuation on large-scale datasets with millions of data points"
  - [section 3] "SAVA follows the same scheme as LAVA which leverages the hierarchically defined OT for data valuation. However, while LAVA processes the whole dataset, SAVA divides the dataset into batches of data points, and carries out the OT problem computation on those batches"
  - [corpus] Weak - no direct citations about hierarchical OT in related papers
- Break condition: If batch size is too small (many batches) or too large (approaches full dataset), the approximation error increases or memory complexity becomes prohibitive.

### Mechanism 2
- Claim: The gradient of the OT distance with respect to probability mass provides a meaningful measure of data point contribution to the overall distribution alignment.
- Mechanism: The calibrated gradient (Eq. 6) measures how the OT distance changes when probability mass is shifted to a given data point. Points with large positive gradients contribute more to the OT distance, while those with large negative gradients help align the distributions.
- Core assumption: The OT distance is sensitive to data point contributions and can serve as a proxy for data quality/relevance without relying on model performance.
- Evidence anchors:
  - [section 2.2] "the gradient of the OT distance w.r.t. the probability mass associated with each point can be leveraged as a surrogate to measure the contribution of that point"
  - [abstract] "The gradient of the OT distance w.r.t. the probability mass of data points in the two datasets can be expressed"
  - [corpus] Missing - no related work directly discusses OT gradient for data valuation
- Break condition: If the OT distance is not sensitive enough to distinguish between clean and corrupted data, the gradient-based valuation fails.

### Mechanism 3
- Claim: Entropic regularization of OT problems provides a computationally efficient approximation while maintaining sufficient accuracy for data valuation.
- Mechanism: Instead of solving the exact OT problem (super-cubic complexity), SAVA uses entropic regularization (quadratic complexity) via the Sinkhorn algorithm, making the computation tractable for large datasets.
- Core assumption: The entropic regularized OT provides a good approximation of the true OT distance, and the calibration terms account for the deviation.
- Evidence anchors:
  - [section 2.1] "To deal with high computational complexity, i.e., super cubic w.r.t. the number of supports of input measures, an efficient approach utilizes entropic regularization (Cuturi, 2013) to reduce its complexity into quadratic"
  - [section 3] "As the common practice in computing the OT via its entropic regularization, using the Sinkhorn algorithm (Cuturi, 2013), we quantify the deviation in the calibrated gradients caused by the entropy regularizer"
  - [corpus] Weak - only mentions related work on data valuation but not specifically about entropic regularization
- Break condition: If the regularization parameter is too large, the approximation deviates significantly from the true OT, affecting valuation quality.

## Foundational Learning

- Concept: Optimal Transport (OT) and Wasserstein distance
  - Why needed here: OT provides a principled way to measure the distance between probability distributions, which is the foundation for comparing training and validation datasets
  - Quick check question: What is the computational complexity of exact OT between two datasets of size N and N', and why is this problematic for large datasets?

- Concept: Dual formulation and Sinkhorn algorithm
  - Why needed here: The dual formulation enables efficient computation of entropic regularized OT via the Sinkhorn algorithm, which is crucial for scalability
  - Quick check question: How does the Sinkhorn algorithm reduce the computational complexity of OT, and what is the role of the dual variables?

- Concept: Hierarchical decomposition and batching strategies
  - Why needed here: Hierarchical decomposition allows breaking down a large problem into smaller, manageable subproblems, which is the key to SAVA's scalability
  - Quick check question: How does the hierarchical OT (HOT) formulation relate to the standard OT formulation, and what are the trade-offs in approximation accuracy?

## Architecture Onboarding

- Component map:
  Data batching module -> Pairwise OT solver -> Batch-level aggregation -> Gradient computation module -> Caching layer

- Critical path:
  1. Partition datasets into batches
  2. For each batch pair, compute pairwise OT and store dual solutions
  3. Solve OT between batch distributions to get optimal plan
  4. Compute data point valuations using weighted aggregation of batch gradients
  5. Return sorted valuations for data selection/pruning

- Design tradeoffs:
  - Batch size vs. approximation accuracy: larger batches give better approximation but higher memory usage
  - Exact vs. entropic OT: exact OT is more accurate but computationally prohibitive; entropic OT is faster but introduces regularization bias
  - Caching vs. recomputation: caching label-to-label costs speeds up runtime but increases memory usage

- Failure signatures:
  - Out-of-memory errors: batch size too large or too many batch pairs
  - Poor detection rates: batch size too small (insufficient clean points for comparison) or regularization too strong
  - Slow runtime: insufficient caching or suboptimal batch partitioning

- First 3 experiments:
  1. Run SAVA on CIFAR10 with varying batch sizes (128, 1024, 4096) and measure detection rates for noisy label corruption
  2. Compare SAVA with and without label-to-label caching on a 10k dataset to measure runtime improvement
  3. Test SAVA's scalability by progressively increasing dataset size from 5k to 50k points and measuring memory usage and detection rates

## Open Questions the Paper Calls Out

None identified in the provided content.

## Limitations

- Theoretical analysis of hierarchical OT approximation error is limited, with no quantitative bounds on how batch size affects valuation accuracy
- Dependency on batch size selection creates practical challenges, as too small batches degrade performance while too large batches eliminate memory benefits
- Entropic regularization trade-offs are acknowledged but not quantified with error bounds on downstream data valuation quality

## Confidence

- Scalability mechanism (hierarchical OT): Medium - Supported by empirical results but lacking theoretical approximation bounds
- Gradient-based valuation effectiveness: Medium - Works well in benchmarks but no ablation on alternative gradient formulations
- Entropic regularization trade-offs: Medium - Acknowledged but not quantified with error bounds
- Comparison to LAVA: High - Direct empirical comparisons show similar performance with better scalability

## Next Checks

1. **Approximation Error Analysis**: Systematically vary batch sizes from 64 to 4096 on CIFAR10 and CIFAR100 with different noise levels, measuring both OT approximation error and data valuation accuracy to establish the relationship between batch size and performance.

2. **Runtime Scalability Benchmark**: Measure wall-clock time and memory usage of SAVA versus LAVA on progressively larger datasets (5k, 10k, 25k, 50k, 100k points) to empirically validate the claimed quadratic-to-linear complexity improvement.

3. **Cross-Dataset Generalization**: Test SAVA on datasets with different characteristics (e.g., CIFAR10, ImageNet subsets, and tabular data) to verify that the hierarchical OT approximation maintains effectiveness across domains with varying feature dimensionalities and data distributions.