---
ver: rpa2
title: Revisiting Word Embeddings in the LLM Era
arxiv_id: '2402.11094'
source_url: https://arxiv.org/abs/2402.11094
tags:
- word
- anchor
- embeddings
- sentence
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper compares word and sentence embeddings from 13 models\u2014\
  7 large language models (LLMs) and 6 classical models\u2014in both decontextualized\
  \ and contextualized settings. In decontextualized evaluation, embeddings were generated\
  \ for ~80,000 words and tested on word-pair similarity and analogy tasks."
---

# Revisiting Word Embeddings in the LLM Era

## Quick Facts
- **arXiv ID**: 2402.11094
- **Source URL**: https://arxiv.org/abs/2402.11094
- **Reference count**: 40
- **Primary result**: LLMs cluster related words tightly but classical models excel at fine-grained semantic tasks, showing complementary strengths.

## Executive Summary
This paper systematically compares word and sentence embeddings from 13 models—7 LLMs and 6 classical models—across decontextualized and contextualized settings. The evaluation uses ~80,000 words from WordNet for word-pair similarity and analogy tasks, and ~1,200 anchor words for nine contextualized linguistic variation tasks. LLMs like PaLM and ADA achieve the best analogy task performance, while classical models such as SBERT and GloVe remain competitive on fine-grained semantic tasks. In contextualized evaluation, LLaMA2 excels at contextual token-level deviation, while classical models show lower sentence-level variance and better polysemy handling. The results demonstrate that LLM embeddings cluster related words tightly but classical models retain advantages for tasks requiring semantic precision.

## Method Summary
The paper evaluates embeddings through two main approaches: decontextualized and contextualized. For decontextualized evaluation, embeddings are generated for ~80,000 WordNet words and tested on word-pair similarity distributions and the BATS analogy dataset using five different analogy methods (3CosAdd, 3CosAvg, 3CosMul, LRCos, PairDistance). For contextualized evaluation, ~1,200 anchor words are used to generate synthetic sentences for nine linguistic variation tasks (synonym, antonym, negation, etc.) via Claude-Sonnet 3.5, and three cosine-based similarity measures are computed: Anchor Inter-Contextual Variance, Anchor Contextual Deviation, and Sentence Meaning Variance. The analysis reveals complementary strengths between LLMs and classical models depending on the task type and evaluation metric.

## Key Results
- LLMs like PaLM and ADA achieved the highest accuracy on analogy tasks, though classical models like SBERT and GloVe were also highly competitive
- LLaMA2 showed superior performance in contextual token-level deviation, clustering related words tightly
- Classical models (SimCSE, SBERT) achieved lower sentence-level variance and better polysemy handling in contextualized settings
- The findings demonstrate that LLMs and classical models have complementary strengths depending on the specific use case

## Why This Works (Mechanism)
None provided

## Foundational Learning
- **WordNet lexical database**: A comprehensive lexical database of English words organized by semantic relationships, used as the source for evaluation words and anchor words
- **Cosine similarity for embedding comparison**: Measures the angle between embedding vectors to assess semantic similarity, forming the basis for all evaluation metrics in the paper
- **BATS analogy dataset**: A benchmark dataset containing word analogy problems organized into morphological, lexicographic, encyclopedic, and semantic categories
- **Tokenization and subword handling**: Different models use different tokenization strategies, requiring consistent averaging of subword embeddings for fair comparison
- **Spearman's ρ and Kendall's τ correlation**: Statistical measures used to evaluate the relationship between similarity distributions across different models

## Architecture Onboarding
**Component map**: WordNet words → Model tokenizers → Embeddings → Similarity calculations → Evaluation metrics
**Critical path**: Word selection → Embedding generation → Similarity computation → Metric aggregation → Result analysis
**Design tradeoffs**: Computational cost vs. sampling accuracy (full pairwise computation vs. random sampling); model coverage vs. vocabulary completeness (filtering WordNet for limited-vocabulary models)
**Failure signatures**: High computational cost for full pairwise similarity computation; vocabulary limitations with classical models; inconsistent tokenization strategies
**First experiments**:
1. Generate embeddings for 100 randomly selected words from each model to verify basic functionality
2. Compute pairwise similarities for 1,000 word pairs to validate cosine similarity implementation
3. Test analogy task evaluation on a small subset of BATS examples to ensure correct methodology

## Open Questions the Paper Calls Out
**Open Question 1**: Do larger LLMs with more parameters consistently outperform smaller models in contextualized embedding tasks across all linguistic variations? The paper compares models of varying sizes but doesn't establish a clear trend showing that larger models always perform better across all contextualized tasks.

**Open Question 2**: How do LLM-induced embeddings compare to classical models when evaluated on linguistic tasks beyond those tested in this paper? The paper acknowledges that the nine tested tasks represent only a subset of possible linguistic evaluations.

**Open Question 3**: What is the root cause of the observed differences between LLM and classical model performance in contextualized settings? The authors present only two hypotheses about over-generalization and training objectives without empirical validation.

**Open Question 4**: How do these embedding models perform on languages other than English? All evaluations focus solely on English, leaving open whether findings generalize to other languages.

**Open Question 5**: Do LLM embeddings show consistent behavior across different prompt formulations for contextualized tasks? The paper uses a single prompt design for each task without testing sensitivity to prompt variations.

## Limitations
- Computational scaling issues due to full pairwise similarity computation generating billions of pairs
- Missing exact prompts and generation templates for contextualized task creation
- Lack of specific model versions and parameter settings for classical models
- Vocabulary coverage limitations with Word2Vec and GloVe requiring WordNet filtering

## Confidence
**High Confidence**: The overall finding that LLMs cluster related words tightly while classical models maintain advantages for fine-grained semantic tasks
**Medium Confidence**: The specific ranking of models within each category (LLMs vs classical) 
**Low Confidence**: The exact numerical values reported for contextualized metrics due to missing generation specifications

## Next Checks
**Validation Check 1**: Replicate decontextualized evaluation using random sampling of 100,000 word pairs to verify similarity distribution patterns and analogy task performance rankings across all 13 models.

**Validation Check 2**: Generate a smaller subset of contextualized tasks (e.g., 100 anchor words × 2 tasks) using publicly available LLMs with documented prompts to test sensitivity of the three contextualized metrics to task generation variations.

**Validation Check 3**: Perform ablation studies by comparing results using different sampling rates (10%, 50%, 100%) for correlation analysis to quantify impact of computational approximations on model ranking stability.