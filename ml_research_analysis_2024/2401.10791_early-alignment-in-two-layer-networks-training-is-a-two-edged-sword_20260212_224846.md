---
ver: rpa2
title: Early alignment in two-layer networks training is a two-edged sword
arxiv_id: '2401.10791'
source_url: https://arxiv.org/abs/2401.10791
tags:
- lemma
- neurons
- alignment
- then
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the early alignment phenomenon in training two-layer
  ReLU networks with small initialization. It provides a rigorous, quantitative characterization
  of how neurons align toward a small number of extremal directions during the initial
  phase of gradient flow, demonstrating that this alignment leads to sparse representations
  but can also hinder convergence to global minima.
---

# Early alignment in two-layer networks training is a two-edged sword

## Quick Facts
- arXiv ID: 2401.10791
- Source URL: https://arxiv.org/abs/2401.10791
- Reference count: 40
- This paper studies the early alignment phenomenon in training two-layer ReLU networks with small initialization, demonstrating how neurons align toward extremal directions and the implications for convergence.

## Executive Summary
This paper provides a rigorous, quantitative characterization of early alignment in two-layer ReLU networks with small initialization. The authors demonstrate that neurons rapidly align toward a small number of extremal directions during the initial training phase, creating sparse representations but also potentially hindering convergence to global minima. Under certain data conditions, gradient flow can converge to spurious stationary points even with infinite width, revealing a fundamental tradeoff between implicit bias toward low-rank solutions and the risk of non-convergence.

## Method Summary
The authors analyze gradient flow dynamics in two-layer ReLU networks with small initialization, using Clarke subdifferential for non-differentiable optimization. They characterize neuron alignment through differential inclusions on the unit sphere, proving that neurons align toward extremal vectors of a piecewise linear gradient alignment function G. The analysis establishes conditions under which balanced initialization preserves omnidirectionality and proves convergence to spurious stationary points when this property is lost. The framework holds in the mean-field limit and provides precise bounds on alignment rates and norm growth phases.

## Key Results
- Early alignment causes neurons to concentrate around a few key directions (extremal vectors), leading to sparse representations
- Loss of weight omnidirectionality during early alignment can cause convergence to spurious stationary points even with infinite network width
- Small initialization creates a tradeoff: it promotes implicit bias toward low-rank solutions but risks non-convergence to global minima

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Early alignment induces sparsity in network representation by concentrating neurons toward extremal directions.
- **Mechanism:** During the initial training phase with small initialization, neurons align toward extremal vectors of the gradient alignment function G. This alignment occurs rapidly compared to norm growth, causing neurons to cluster around a few key directions. The alignment is governed by projected gradient dynamics on the unit sphere, where neurons move toward critical points of G.
- **Core assumption:** The function G has no saddle points and the initialization satisfies the balancedness condition |a₀ⱼ| ≥ ‖w₀ⱼ‖.
- **Evidence anchors:** [abstract] "early alignment phase... leads to an alignment of the neurons towards key directions"; [section] "Theorem 1 precisely quantifies this phenomenon... neurons end up aligned towards a few key directions, given by extremal vectors"; [corpus] Weak - related works discuss alignment but not this specific sparsity-inducing mechanism
- **Break condition:** If G has saddle points, neurons can evolve arbitrarily slowly near these points, preventing the clean quantization of directions.

### Mechanism 2
- **Claim:** Loss of weight omnidirectionality during early alignment can cause convergence to spurious stationary points.
- **Mechanism:** Early alignment concentrates positive neurons around a single extremal vector, creating a cone-like structure in weight space. This breaks omnidirectionality, meaning the weights no longer cover all possible directions. Without omnidirectionality, there's no guarantee of reaching global minima even in the infinite width limit.
- **Core assumption:** The dataset has certain correlation properties (e.g., Assumption 3) that make additional neuron directions harmful to training loss.
- **Evidence anchors:** [abstract] "under certain data conditions, gradient flow can converge to spurious stationary points even with infinite width"; [section] "early alignment phenomenon can be the cause of largely overparameterised neural networks converging to spurious stationary points"; [corpus] Missing - corpus doesn't discuss convergence to spurious points
- **Break condition:** If initialization is perfectly balanced (‖w₀ⱼ‖ = |a₀ⱼ|), omnidirectionality may be preserved despite early alignment.

### Mechanism 3
- **Claim:** Small initialization creates a tradeoff between implicit bias toward low-rank solutions and risk of non-convergence.
- **Mechanism:** Small initialization promotes the mean-field regime where features are learned, inducing implicit bias toward simple solutions. However, this same small scale triggers early alignment that can lead to spurious convergence. Large initialization avoids early alignment (NTK regime) but sacrifices feature learning and generalization.
- **Core assumption:** The initialization scale λ doesn't depend on network width m, allowing results to hold in the mean-field limit.
- **Evidence anchors:** [abstract] "small initialisation promotes implicit bias toward low-rank solutions but risks non-convergence"; [section] "Theorem 2 even holds in the limit m → ∞" and "This discrepancy with previous results is discussed further in Section 6"; [corpus] Weak - corpus mentions initialization but not this specific tradeoff
- **Break condition:** If initialization scale λ depends on width m, the mean-field limit arguments break down.

## Foundational Learning

- **Concept:** Clarke subdifferential for non-differentiable optimization
  - **Why needed here:** The loss function with ReLU activations is not continuously differentiable, requiring generalized gradient concepts for rigorous analysis of gradient flow dynamics.
  - **Quick check question:** Can you explain why standard gradients don't exist at ReLU activation boundaries and how the Clarke subdifferential addresses this?

- **Concept:** Balanced initialization and its implications
  - **Why needed here:** Balanced initialization (|a₀ⱼ| ≥ ‖w₀ⱼ‖) ensures that neuron output weights maintain consistent signs during training, which is crucial for controlling alignment dynamics and proving convergence properties.
  - **Quick check question:** What happens to the alignment dynamics if we use unbalanced initialization where some neurons have |a₀ⱼ| < ‖w₀ⱼ‖?

- **Concept:** Extremal vectors and critical points of piecewise linear functions
  - **Why needed here:** The early alignment phenomenon is governed by the extremal vectors of the gradient alignment function G, which correspond to critical points of this piecewise linear function. Understanding these vectors is essential for characterizing the quantization of represented directions.
  - **Quick check question:** How many extremal vectors can exist for a dataset with n data points in d dimensions, and what determines their number?

## Architecture Onboarding

- **Component map:** Two-layer ReLU network with gradient flow dynamics → neuron alignment toward extremal vectors → norm growth phase → convergence (global minimum or spurious point)
- **Critical path:** Initialization → Early alignment phase → Norm growth phase → Convergence (global minimum or spurious point). The critical decision point is whether early alignment breaks omnidirectionality.
- **Design tradeoffs:** Small initialization promotes feature learning and implicit bias but risks spurious convergence; large initialization ensures convergence but sacrifices feature learning and generalization.
- **Failure signatures:** Convergence to a single-neuron equivalent network despite having many neurons available; loss plateauing at a non-zero value; network weights clustering in a narrow cone of directions.
- **First 3 experiments:**
  1. Train a two-layer ReLU network on a 3-point dataset with small initialization and observe whether it converges to a single-neuron solution.
  2. Vary the initialization scale λ and measure the degree of neuron alignment and final training loss to quantify the initialization tradeoff.
  3. Test with perfectly balanced initialization (‖w₀ⱼ‖ = |a₀ⱼ|) versus unbalanced initialization to observe the effect on omnidirectionality preservation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the threshold initialization scale λ* depend on the number of training samples n and data dimension d for typical data models?
- Basis in paper: [inferred] The paper notes that while the threshold λ* is independent of network width m, it still depends on the data. The authors state it is unclear how this threshold scales with quantities like n and d, and believe such a description is feasible for typical data models.
- Why unresolved: The paper only considers a general data model (Assumption 2) to derive the threshold, which does not allow for a more precise description of its dependence on n and d. Analyzing specific data models would be required.
- What evidence would resolve it: Deriving the exact dependence of λ* on n and d for specific data models (e.g., linear data with Gaussian inputs) through theoretical analysis or empirical studies.

### Open Question 2
- Question: Does the failure to converge towards global minima due to early alignment also occur in classification tasks, or is it specific to regression tasks?
- Basis in paper: [explicit] The paper states "Theorem 2 focuses on a regression task. Stewart et al. (2023) suggest that finding global minima is easier for classification than regression tasks... Whether a similar failure of training is possible with classification tasks is left open for future work."
- Why unresolved: The paper only proves the convergence to spurious stationary points for a regression task with ReLU activation. It conjectures that a similar failure could happen in classification tasks due to loss of weight omnidirectionality, but does not provide a proof or empirical evidence.
- What evidence would resolve it: Proving or disproving the conjecture through theoretical analysis or experiments on classification tasks with different activation functions.

### Open Question 3
- Question: How does the choice of initialization (balanced vs. unbalanced) affect the preservation of weight omnidirectionality and the ability to find global minima in the presence of early alignment?
- Basis in paper: [explicit] The paper discusses that with balanced initialization, the convergence point can be a saddle point or local minimum depending on whether it includes zero neurons. With unbalanced initializations, omnidirectionality is preserved in the 3-point example but the paper conjectures it might not be generally preserved, leading to failure in finding global minima.
- Why unresolved: The paper provides theoretical and empirical evidence for the 3-point example but states that theoretically studying examples where unbalanced initializations fail to find global minima is more complex and left open for future work.
- What evidence would resolve it: Theoretical analysis or empirical studies on the effect of balanced vs. unbalanced initialization on weight omnidirectionality and convergence to global minima for different data models and activation functions.

## Limitations

- The analysis focuses on gradient flow rather than practical optimization algorithms like SGD, which may limit direct applicability to real-world training
- The convergence to spurious stationary points is proven only for a specific 3-point regression example, and generalizability to more complex datasets remains uncertain
- The exact conditions under which saddle points emerge in the alignment function G are not fully characterized

## Confidence

- Characterization of early alignment dynamics and direction quantization: High confidence
- Early alignment inducing sparsity in representations: Medium confidence
- Loss of omnidirectionality causing convergence to spurious stationary points: High confidence
- Generalizability of Assumption 3 to broader datasets: Low confidence

## Next Checks

1. **Empirical validation of alignment dynamics**: Implement the two-layer ReLU network and track neuron alignment toward extremal vectors on synthetic datasets with known extremal structures to verify the theoretical predictions about alignment rates and direction quantization.

2. **Convergence testing across initialization scales**: Systematically vary initialization scale λ and measure the final training loss and representation sparsity to empirically quantify the tradeoff between implicit bias and convergence reliability across different dataset types.

3. **Generalization to complex datasets**: Test the convergence behavior on larger, more complex datasets (e.g., CIFAR-10 subsets) to assess whether the spurious stationary point phenomenon persists beyond the 3-point example and how initialization scale affects practical training outcomes.