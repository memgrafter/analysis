---
ver: rpa2
title: 'Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging'
arxiv_id: '2406.15479'
source_url: https://arxiv.org/abs/2406.15479
tags:
- merging
- knowledge
- tasks
- performance
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Twin-Merging addresses the challenge of combining multiple task-specific
  models into a single multitask model without extra training, while mitigating interference
  between models and handling heterogeneous test data. The core idea is to modularize
  knowledge into shared and exclusive components, compress the exclusive knowledge
  using SVD, and dynamically merge them based on the input using a small router network.
---

# Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging

## Quick Facts
- arXiv ID: 2406.15479
- Source URL: https://arxiv.org/abs/2406.15479
- Reference count: 40
- Key outcome: Twin-Merging improves average performance by 28.34% in absolute normalized score for discriminative tasks and surpasses the fine-tuned upper bound on generative tasks.

## Executive Summary
Twin-Merging addresses the challenge of combining multiple task-specific models into a single multitask model without extra training, while mitigating interference between models and handling heterogeneous test data. The core idea is to modularize knowledge into shared and exclusive components, compress the exclusive knowledge using SVD, and dynamically merge them based on the input using a small router network. Experiments on 20 datasets covering both language and vision tasks show that Twin-Merging improves average performance by 28.34% in absolute normalized score for discriminative tasks and even surpasses the fine-tuned upper bound on generative tasks. The method also scales well with model size, achieves significant parameter efficiency (maintaining 86% performance with 0.1% of parameters), and is compatible with other merging methods.

## Method Summary
Twin-Merging involves two principal stages: (1) Modularizing knowledge into shared and exclusive components, with compression using SVD to reduce redundancy and enhance efficiency; (2) Dynamically merging shared and task-specific knowledge based on the input using a small router network. The method aims to combine multiple task-specific models into a single multitask model without extra training, while mitigating interference between models and handling heterogeneous test data.

## Key Results
- Improves average performance by 28.34% in absolute normalized score for discriminative tasks
- Surpasses the fine-tuned upper bound on generative tasks
- Maintains 86% performance even at a 99.8% compression rate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge modularization into shared and exclusive components reduces interference during model merging.
- Mechanism: By decomposing task-specific models into a shared expert capturing common knowledge and exclusive task-specific vectors, Twin-Merging isolates conflicting knowledge and prevents parameter redundancy from disrupting integration.
- Core assumption: Task-specific models contain both shared and exclusive knowledge, and mixing them directly causes performance degradation.
- Evidence anchors:
  - [abstract]: "We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance."
  - [section 3.2]: "Single-task models often contain both types, complicating the merging process and leading to interference."
  - [corpus]: Weak. No direct corpus mention of modular decomposition reducing interference; inference based on experimental results.
- Break condition: If shared knowledge is minimal or exclusive knowledge is not truly task-specific, modularization may not yield gains.

### Mechanism 2
- Claim: Dynamic merging based on input routing allows adaptive task-specific knowledge injection.
- Mechanism: A small router network computes weights for combining shared and exclusive knowledge vectors conditioned on each input, enabling the merged model to adapt to heterogeneous test data.
- Core assumption: The test data distribution is unpredictable, so a static merged model cannot optimally handle all inputs.
- Evidence anchors:
  - [abstract]: "dynamically merging shared and task-specific knowledge based on the input."
  - [section 3.3]: "we introduce a router to dynamically merge shared and exclusive knowledge based on the test inputs."
  - [corpus]: Weak. Corpus neighbors focus on other merging methods but do not explicitly validate dynamic routing for heterogeneous data.
- Break condition: If the router is too small or poorly trained, it may fail to select appropriate task-specific knowledge, reducing performance.

### Mechanism 3
- Claim: SVD compression of exclusive knowledge vectors preserves task-specific information while reducing storage cost.
- Mechanism: Singular value decomposition is applied to the exclusive knowledge vectors to retain the most informative components, allowing high compression without significant performance loss.
- Core assumption: Most of the task-specific knowledge can be captured in a low-rank subspace.
- Evidence anchors:
  - [section 3.3]: "we apply singular value decomposition (SVD) to further compress the above exclusive knowledge into vectors for each task."
  - [section 4.6]: "Twin-Merging maintains 86.4% performance even at a 99.8% compression rate."
  - [corpus]: Weak. No corpus mention of SVD compression in merging; inference based on internal results.
- Break condition: If the rank is reduced too much, essential task-specific information may be lost, harming performance.

## Foundational Learning

- Concept: Parameter space interpolation and linear mode connectivity.
  - Why needed here: Understanding how fine-tuned models relate in parameter space is crucial for designing merging methods that avoid interference.
  - Quick check question: Why does simple weight averaging often fail to match fine-tuned model performance?

- Concept: Mixture-of-Experts (MoE) routing.
  - Why needed here: Twin-Merging borrows the idea of conditional expert selection from MoE to dynamically merge knowledge based on inputs.
  - Quick check question: How does the router in Twin-Merging differ from a traditional MoE gating network?

- Concept: Singular value decomposition (SVD) for dimensionality reduction.
  - Why needed here: SVD is used to compress exclusive knowledge vectors while retaining the most informative components.
  - Quick check question: What happens to the information content when the rank of SVD is reduced below the intrinsic dimensionality of the data?

## Architecture Onboarding

- Component map:
  - Shared expert (Î¸s): A merged model capturing common knowledge across tasks
  - Exclusive knowledge vectors (vt): Task-specific components obtained by subtracting the shared expert from each task model
  - Router (R): A small network that computes fusion weights based on input embeddings
  - SVD compressor: Reduces dimensionality of exclusive vectors for storage efficiency

- Critical path:
  1. Compute shared expert by merging task models (e.g., Task Arithmetic)
  2. Extract exclusive vectors as differences between task models and shared expert
  3. Compress exclusive vectors with SVD
  4. During inference, embed input, route through router to get weights
  5. Dynamically merge shared expert and weighted exclusive vectors

- Design tradeoffs:
  - Router size vs. adaptability: A larger router may better capture input patterns but increases computation
  - SVD rank vs. compression: Higher rank preserves more information but reduces storage gains
  - Preprocessing time vs. inference speed: SVD and router training add upfront cost but enable faster, more efficient inference

- Failure signatures:
  - Poor router training: Router assigns near-uniform weights, reducing dynamic adaptation benefits
  - Over-compression: SVD rank too low, exclusive knowledge vectors lose critical task-specific information
  - Shared expert misalignment: If shared expert does not capture true common knowledge, exclusive vectors become noisy

- First 3 experiments:
  1. Validate modularization: Merge two task models using simple averaging vs. shared+exclusive decomposition; compare performance
  2. Test dynamic routing: Implement router on a small dataset; check if routing weights correlate with input task similarity
  3. Evaluate compression: Vary SVD rank; measure performance vs. storage trade-off to find optimal compression level

## Open Questions the Paper Calls Out

- Question: How does the presence of "evil knowledge" (knowledge detrimental to all tasks) impact the performance of Twin-Merging, and can it be effectively isolated and mitigated during the modularization stage?
- Basis in paper: [inferred] The authors mention "evil knowledge" as a potential type of knowledge that may exist in fine-tuned models, which is useless for any task and can distract the model, obscuring critical knowledge during merging.
- Why unresolved: The paper acknowledges the existence of evil knowledge but does not provide a method for isolating or mitigating its impact during the Twin-Merging process.
- What evidence would resolve it: Experimental results comparing Twin-Merging performance with and without a mechanism to identify and exclude evil knowledge, or a theoretical analysis of how evil knowledge affects the merging process and its impact on the final model's performance.

- Question: How does Twin-Merging perform when merging models trained on tasks with significantly different data distributions, and what is the impact on the shared knowledge representation?
- Basis in paper: [explicit] The paper discusses the challenge of heterogeneous test data and the importance of shared knowledge, but does not explicitly explore the impact of merging models trained on tasks with vastly different data distributions.
- Why unresolved: The experiments primarily focus on merging models trained on related tasks or tasks within the same domain (e.g., GLUE benchmark, generative tasks). The paper does not investigate the performance of Twin-Merging when merging models trained on tasks with significantly different data distributions.
- What evidence would resolve it: Experiments comparing Twin-Merging performance when merging models trained on tasks with similar vs. significantly different data distributions, and an analysis of how the shared knowledge representation is affected in each case.

- Question: Can the Twin-Merging approach be extended to handle model merging across different architectures, and what are the challenges and potential solutions?
- Basis in paper: [explicit] The authors mention that current merging is limited to models with the same architecture and that it may be difficult to find a suitable fine-tuned model with specific capacities.
- Why unresolved: The paper does not explore the possibility of extending Twin-Merging to handle model merging across different architectures, nor does it discuss the challenges and potential solutions for this extension.
- What evidence would resolve it: A theoretical framework or experimental results demonstrating the feasibility of extending Twin-Merging to handle model merging across different architectures, along with a discussion of the challenges and potential solutions.

## Limitations
- Limited exploration of merging models trained on tasks with significantly different data distributions
- Potential challenges in extending the approach to handle model merging across different architectures
- Dependence on SVD rank selection for optimal compression, which is not fully explored

## Confidence
- Performance claims (28.34% improvement): Medium confidence
- Parameter efficiency claims (86% at 0.1% parameters): Medium confidence
- Claims about surpassing fine-tuned upper bound: Low confidence
- Router effectiveness in heterogeneous scenarios: Medium confidence

## Next Checks
1. Stress-test the router on deliberately mixed or adversarial input distributions to verify adaptive performance degradation bounds
2. Conduct ablation studies on SVD rank vs. performance across different task types to identify optimal compression parameters
3. Scale experiments to larger foundation models (e.g., 7B+ parameters) to validate claims about model-size independence and parameter efficiency