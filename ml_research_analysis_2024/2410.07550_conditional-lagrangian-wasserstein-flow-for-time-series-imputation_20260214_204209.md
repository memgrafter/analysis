---
ver: rpa2
title: Conditional Lagrangian Wasserstein Flow for Time Series Imputation
arxiv_id: '2410.07550'
source_url: https://arxiv.org/abs/2410.07550
tags:
- time
- data
- imputation
- series
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new method called Conditional Lagrangian
  Wasserstein Flow (CLWF) for time series imputation. The authors frame the problem
  as a conditional optimal transport problem, where the goal is to move random noise
  to the missing data distribution using observed data as conditions.
---

# Conditional Lagrangian Wasserstein Flow for Time Series Imputation

## Quick Facts
- arXiv ID: 2410.07550
- Source URL: https://arxiv.org/abs/2410.07550
- Reference count: 17
- Primary result: CLWF achieves RMSE of 18.1 and MAE of 9.7 on PM 2.5 dataset, outperforming CSDI, CSBI, and DSPD-GP

## Executive Summary
This paper proposes Conditional Lagrangian Wasserstein Flow (CLWF), a novel method for time series imputation that frames the problem as a conditional optimal transport task. The method uses Lagrangian mechanics to learn a velocity field that moves random noise to the missing data distribution while conditioning on observed data. By leveraging optimal transport theory, CLWF claims to achieve faster convergence than traditional diffusion-based approaches while incorporating prior knowledge through a task-specific potential function estimated by a variational autoencoder.

## Method Summary
CLWF treats time series imputation as a conditional optimal transport problem where the goal is to transform random noise into the missing data distribution using observed data as conditions. The method employs Lagrangian mechanics to learn the velocity field by minimizing kinetic energy, with a task-specific potential function incorporated through a pre-trained VAE. The approach uses ODE sampling for inference and can optionally apply Rao-Blackwellization for variance reduction. The model is trained to minimize the kinetic energy between intermediate samples and the learned velocity field.

## Key Results
- On PM 2.5 dataset: RMSE of 18.1 and MAE of 9.7, outperforming baseline methods CSDI, CSBI, and DSPD-GP
- On PhysioNet dataset: Demonstrates competitive performance with 80% missing rate
- Shows faster convergence than diffusion-based approaches due to simulation-free learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed method achieves faster convergence by using optimal transport theory to straighten the probability flow between source (noise) and target (missing data) distributions.
- Mechanism: By projecting interpolations into Wasserstein space via mini-batch optimal transport maps before interpolation, the method ensures the intermediate samples follow geodesic paths, reducing the number of simulation steps needed.
- Core assumption: The shortest path in Wasserstein space between distributions corresponds to the most efficient data generation trajectory.
- Evidence anchors:
  - [abstract]: "The proposed method leverages the (conditional) optimal transport theory to learn the probability flow in a simulation-free manner"
  - [section 3.2]: "To solve Eq. 7, we need to sample the intermediate variable Xt in the Wasserstein space first... we need to project the interpolations in the Wasserstein space before interpolating to strengthen the probability flow"
  - [corpus]: Weak - no direct evidence in neighbors about OT theory application

### Mechanism 2
- Claim: Incorporating a task-specific potential function through a pre-trained VAE improves data generation performance by reducing sampling variance.
- Mechanism: The potential function ∇xU(Xt) = (Xt - VAE(Xt))/σ²p provides gradient information that guides the flow toward plausible reconstructions, acting as a prior that regularizes the generation process.
- Core assumption: The reconstruction discrepancies from a VAE trained on observed data capture meaningful structure that can guide imputation of missing values.
- Evidence anchors:
  - [abstract]: "Moreover, to incorporate more prior information into the model, we parameterize the derivative of a task-specific potential function via a variational autoencoder"
  - [section 3.4]: "we can incorporate the prior knowledge learned from the accessible training data into the sampling process formulated by Eq. 14 to enhance the data generation performance"
  - [corpus]: Weak - neighbors don't discuss VAE-based potential functions

### Mechanism 3
- Claim: The Rao-Blackwellized sampler constructed from the ODE base sampler and VAE potential function provides lower-variance estimates than the base sampler alone.
- Mechanism: By conditioning the base estimator on the sufficient statistic from the VAE, the Rao-Blackwell theorem guarantees variance reduction, leading to more accurate imputation results.
- Core assumption: The VAE reconstruction serves as a sufficient statistic for the target distribution, even if not perfectly accurate.
- Evidence anchors:
  - [section 3.5]: "According to the Rao-Blackwell theorem Casella and Robert [1996], we have E[S* - Xt+1]² ≤ E[S - Xt+1]²"
  - [section 3.5]: "we can construct a more powerful sampler with smaller errors than the base ODE sampler S using Rao-Blackwellization"
  - [corpus]: Weak - no neighbors discuss Rao-Blackwellization in this context

## Foundational Learning

- Concept: Optimal Transport Theory
  - Why needed here: Provides the mathematical framework for finding the most efficient transformation between the noise distribution and the missing data distribution
  - Quick check question: How does the Kantorovich formulation of optimal transport differ from the Monge formulation in terms of existence and uniqueness of solutions?

- Concept: Lagrangian Mechanics
  - Why needed here: Supplies the principle of least action to derive the velocity field that moves probability mass from source to target distribution with minimal "effort"
  - Quick check question: What is the relationship between the kinetic energy term in the Lagrangian and the velocity matching loss in the flow matching formulation?

- Concept: Diffusion Models and Score Matching
  - Why needed here: Provides context for why the proposed method offers advantages over traditional diffusion approaches in terms of convergence speed and computational efficiency
  - Quick check question: Why do diffusion models typically require many more sampling steps than the proposed method?

## Architecture Onboarding

- Component map: Input → OT projection → Interpolation → Velocity learning → VAE potential (optional) → Rao-Blackwellization → Output
- Critical path: Input → OT projection → Interpolation → Velocity learning → VAE potential (optional) → Rao-Blackwellization → Output
- Design tradeoffs:
  - Speed vs. accuracy: Using ODE sampler instead of SDE sampler reduces computation but may limit diversity
  - Memory vs. fidelity: Mini-batch OT maps require less memory than full OT but may be less accurate
  - Complexity vs. performance: Adding VAE and Rao-Blackwellization improves results but increases implementation complexity

- Failure signatures:
  - Slow convergence: Indicates OT projection is not effectively straightening the flow
  - High variance in results: Suggests VAE potential function is not well-aligned with data structure
  - Mode collapse: May indicate insufficient exploration in the sampling process

- First 3 experiments:
  1. Implement the basic flow matching without VAE or Rao-Blackwellization on a small synthetic dataset to verify the core OT-based interpolation works
  2. Add the VAE component and test on a dataset with sufficient observed data to train a meaningful VAE
  3. Implement Rao-Blackwellization and compare variance reduction on a validation set against the base sampler

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the potential function U(Xt) impact the performance and convergence speed of CLWF compared to other parameterizations?
- Basis in paper: [explicit] The paper discusses using a task-specific potential function estimated by a VAE, but notes that other parameterizations are possible and suggests exploring better task-specific potential functions for sparse multivariate time series data.
- Why unresolved: The paper only explores one specific parameterization of the potential function (using a VAE) and doesn't compare it to other possible parameterizations or analyze its impact on performance and convergence speed.
- What evidence would resolve it: Experimental results comparing CLWF with different potential function parameterizations (e.g., neural networks, physical models) on various datasets, measuring both imputation accuracy and convergence speed.

### Open Question 2
- Question: What is the theoretical justification for using the derivative of the VAE reconstruction error as an estimate of the task-specific potential function gradient?
- Basis in paper: [explicit] The paper proposes using the VAE reconstruction error to estimate the derivative of the potential function, but doesn't provide theoretical justification for this choice.
- Why unresolved: The paper presents this as a practical implementation choice without theoretical analysis of why this would be an appropriate estimate for the potential function gradient.
- What evidence would resolve it: Theoretical analysis connecting VAE reconstruction error to optimal potential functions, or empirical studies showing the effectiveness of this approach compared to alternative estimation methods.

### Open Question 3
- Question: How does CLWF perform on irregularly sampled time series data compared to methods specifically designed for this setting?
- Basis in paper: [inferred] The paper mentions DSPD-GP as a baseline for irregular sampling, but doesn't compare CLWF's performance on irregularly sampled data.
- Why unresolved: While the paper evaluates CLWF on regularly sampled datasets, it doesn't explore its performance on the more challenging irregularly sampled setting where the baseline methods have shown advantages.
- What evidence would resolve it: Experiments comparing CLWF to state-of-the-art irregular sampling methods on datasets with varying levels of irregularity, measuring imputation accuracy and computational efficiency.

## Limitations
- The method's performance depends heavily on the quality of OT projections and VAE-based potential functions
- Limited implementation details about OT map sampling algorithm and VAE architecture make exact reproduction challenging
- Performance gains may be dataset-dependent and sensitive to hyperparameter choices

## Confidence
- High confidence: The core theoretical framework combining OT theory with Lagrangian mechanics is sound and well-established
- Medium confidence: The specific implementation details and architectural choices are likely correct but not fully verifiable from the paper
- Medium confidence: The empirical performance improvements are likely real but may be dataset-dependent

## Next Checks
1. Implement a minimal version of CLWF on synthetic time series data with known missing patterns to verify the core OT-based interpolation mechanism before adding complexity
2. Conduct ablation studies comparing performance with and without the VAE potential function and Rao-Blackwellization to isolate their contributions
3. Test the method on datasets with varying missingness patterns (MCAR, MAR, MNAR) to assess robustness across different data scenarios