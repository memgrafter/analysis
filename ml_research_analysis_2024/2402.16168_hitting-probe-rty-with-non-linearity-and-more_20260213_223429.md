---
ver: rpa2
title: Hitting "Probe"rty with Non-Linearity, and More
arxiv_id: '2402.16168'
source_url: https://arxiv.org/abs/2402.16168
tags:
- probe
- probes
- bert
- dependency
- uuas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes non-linear structural probes to analyze how
  dependency trees are encoded in the hidden states of language models. The authors
  reformulate existing non-linear probe designs, making them simpler yet effective.
---

# Hitting "Probe"rty with Non-Linearity, and More

## Quick Facts
- **arXiv ID**: 2402.16168
- **Source URL**: https://arxiv.org/abs/2402.16168
- **Authors**: Avik Pal; Madhura Pawar
- **Reference count**: 10
- **Primary result**: RBF probe outperforms linear probes in capturing syntactic structure in BERT models

## Executive Summary
This paper investigates how dependency trees are encoded in language model hidden states by introducing non-linear structural probes. The authors reformulate existing non-linear probe designs to be simpler while maintaining effectiveness, focusing on polynomial, radial basis function (RBF), and sigmoid variants. Through experiments on BERT and BERT LARGE, they demonstrate that the RBF probe significantly outperforms linear probes in capturing syntactic structure as measured by unlabeled undirected attachment score (UUAS). The study also introduces a visualization framework to qualitatively assess the strength of predicted dependencies between words.

## Method Summary
The authors reformulate non-linear structural probes by introducing three variants: polynomial, radial basis function (RBF), and sigmoid probes. These probes transform contextualized embeddings from BERT/BERT LARGE using a linear transformation matrix B followed by a non-linear function. The transformed embeddings are then used to compute pairwise distances that predict dependency trees. Probes are trained for 200 epochs with early stopping and evaluated using UUAS. A strength metric (distance from probe / distance in gold tree) is introduced for qualitative visualization of predicted dependencies across different layers.

## Key Results
- RBF probe achieves higher UUAS scores than linear probes on BERT and BERT LARGE
- RBF probes predict fewer but more accurate dependencies with higher strength compared to linear probes
- Middle layers of BERT capture the most syntactic information according to layer-wise probing analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-linear probes like RBF can capture more complex dependency tree structures than linear probes.
- Mechanism: The RBF probe transforms embeddings using a radial basis function that measures similarity in a non-linear space, allowing it to better model the non-linear relationships inherent in syntactic dependency trees.
- Core assumption: Dependency tree structures are inherently non-linear and cannot be fully captured by linear transformations.
- Evidence anchors:
  - [abstract] "We find that the radial basis function (RBF) is an effective non-linear probe for the BERT model than the linear probe."
  - [section 3.2] "We design our probe in such a way so that it has a simple design while factoring in the effect of applying non-linearity on the embeddings."
  - [corpus] Weak evidence - corpus papers discuss non-linear probing but don't specifically validate RBF effectiveness.
- Break condition: If the RBF probe's performance degrades significantly on larger, more diverse datasets, or if linear probes catch up with sufficient parameter tuning.

### Mechanism 2
- Claim: The strength metric (dB/dT) provides a more nuanced evaluation of probe performance than UUAS alone.
- Mechanism: By computing the ratio of predicted distance to true syntactic distance, the strength metric reveals how confidently and accurately a probe predicts dependencies, capturing both precision and confidence.
- Core assumption: Not all correctly predicted dependencies are equally meaningful; strength reflects both correctness and confidence.
- Evidence anchors:
  - [section 3.3] "This metric can help us visualize qualitatively how different probes encode the dependency trees from the contextual representations."
  - [section 5] "We see in Figure 2 that linear probes predict more edge dependencies which are (1) 'false negatives' ... (2) 'true positives' but with a lower strength."
  - [corpus] No direct evidence in corpus papers for strength-based evaluation metrics.
- Break condition: If the strength metric shows no correlation with downstream task performance, or if it becomes computationally prohibitive for large-scale analysis.

### Mechanism 3
- Claim: BERT layers encode syntactic information gradually, with middle layers capturing the most syntactic details.
- Mechanism: As BERT processes information through its layers, it builds up syntactic representations from basic word order in lower layers to complex dependency structures in middle layers, before specializing in task-specific features in later layers.
- Core assumption: Language model layers learn linguistic features in a hierarchical manner, from surface to deep structures.
- Evidence anchors:
  - [section 5] "We see words from the first sub-part of the sentence ... connecting with words from the second sub-part ... which then is getting connected with the last sub-part of the sentence."
  - [section 5] "Figure 3b shows the dependency tree of mid-layer 6 where many edges with high strength are predicted. The mid layers convey the most syntactic information as per (Rogers et al., 2020)."
  - [corpus] Weak evidence - corpus papers mention layer-wise probing but don't specifically validate gradual syntax encoding.
- Break condition: If layer-wise probing reveals no clear progression of syntactic information, or if the trend reverses in larger models.

## Foundational Learning

- Concept: Dependency trees and syntactic structure
  - Why needed here: Understanding how syntactic relationships between words are represented is fundamental to evaluating how well probes capture linguistic structure.
  - Quick check question: Can you explain the difference between a dependency tree and a phrase structure tree?

- Concept: Probing methodology and evaluation metrics
  - Why needed here: The paper relies on specific probing techniques (structural probes) and evaluation metrics (UUAS, strength) to assess linguistic knowledge in language models.
  - Quick check question: How does UUAS differ from directed attachment score, and what does it tell us about probe performance?

- Concept: Non-linear transformations and kernel methods
  - Why needed here: The RBF probe uses a radial basis function to transform embeddings non-linearly, which is key to understanding its effectiveness.
  - Quick check question: What is the effect of the σ parameter in the RBF function, and how does it influence the probe's behavior?

## Architecture Onboarding

- Component map: Contextualized embeddings -> Linear transformation B -> Non-linear function (RBF/polynomial/sigmoid) -> Pairwise distances -> Predicted dependency tree -> UUAS evaluation + strength visualization

- Critical path:
  1. Extract contextualized embeddings for each word
  2. Apply linear transformation B to each embedding
  3. Apply non-linear function (e.g., RBF) to transformed embeddings
  4. Compute pairwise distances between transformed embeddings
  5. Construct predicted dependency tree based on distances
  6. Evaluate using UUAS and strength metrics

- Design tradeoffs:
  - Linear vs. non-linear probes: Linear probes are simpler and faster but may miss complex patterns; non-linear probes capture more structure but are more computationally expensive.
  - Choice of non-linear function: RBF is effective but sensitive to σ parameter; polynomial is simpler but may overfit; sigmoid is bounded but may lose information.
  - Matrix B rank: Higher rank allows more expressive transformations but increases computation and risk of overfitting.

- Failure signatures:
  - Poor UUAS scores across all layers indicate the probe is not capturing syntactic structure effectively.
  - Inconsistent performance between BERT and BERTLARGE suggests model-specific issues.
  - High strength for incorrect dependencies indicates the probe is overconfident in its predictions.

- First 3 experiments:
  1. Reproduce linear probe results on BERT to establish baseline UUAS scores.
  2. Implement RBF probe and compare UUAS scores to linear probe on BERT.
  3. Visualize dependency trees predicted by linear and RBF probes for sample sentences to qualitatively assess performance differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does incorporating the strength of predicted edges and false positives into UUAS calculation affect the robustness of syntactic probing?
- Basis in paper: [explicit] The authors observe that UUAS alone may not be sufficient to assess probe quality, noting that linear probes predict many weak edges while RBF probes predict fewer but stronger and more accurate edges. They suggest UUAS could be made more robust by penalizing false positives and incorporating edge strength.
- Why unresolved: The paper only qualitatively analyzes edge strength through visualization but does not implement quantitative metrics that account for edge strength or false positives in UUAS.
- What evidence would resolve it: Experiments comparing standard UUAS with modified versions that incorporate edge strength (e.g., weighted UUAS) and penalize false positives would show whether these modifications improve the metric's ability to distinguish between probe quality.

### Open Question 2
- Question: Do non-linear probes exhibit gradual layer-wise learning of syntactic structure in BERT LARGE similar to BERT?
- Basis in paper: [inferred] The authors qualitatively analyze three layers of BERT showing gradual learning from lower (linear order) to middle (syntactic details) to upper (task-specific) layers. For BERT LARGE, they only visualize three layers (3, 16, 24) and note they couldn't analyze all 24 layers due to time constraints, observing no clear gradual pattern in these specific layers.
- Why unresolved: The analysis was limited to three layers of BERT LARGE rather than examining all layers systematically, preventing conclusions about whether the gradual learning pattern observed in BERT extends to BERT LARGE.
- What evidence would resolve it: Systematic visualization and analysis of all 24 layers of BERT LARGE using the same methodology as BERT would reveal whether non-linear probes capture gradual syntactic learning in both models.

### Open Question 3
- Question: What explains the different behavior of non-linear probes (RBF and sigmoid) on GPT-2's non-contextualized representations compared to BERT?
- Basis in paper: [explicit] The authors find that GPT-2's UUAS scores for non-contextualized representations show arbitrary patterns with RBF and sigmoid probes performing erratically, unlike BERT where performance degrades consistently. They hypothesize this may be due to GPT-2's autoregressive pre-training providing only limited context compared to BERT's bidirectional context.
- Why unresolved: The authors could not provide a clear explanation for why RBF and sigmoid probes specifically fail to work well on GPT-2's non-contextualized representations, leaving the differential probe behavior unexplained.
- What evidence would resolve it: Controlled experiments comparing probe performance on GPT-2's contextualized vs. non-contextualized representations, along with analysis of how the probe functions differ in handling the different embedding spaces, would clarify the cause of this discrepancy.

## Limitations
- The exact implementation details of the original non-linear probe design by White et al. (2021) that led to the reformulation are unknown.
- The paper lacks direct evidence from corpus papers for the effectiveness of the strength-based evaluation metric and gradual encoding of syntactic information across BERT layers.
- Analysis was limited to three layers of BERT LARGE rather than examining all layers systematically.

## Confidence
- **High**: The RBF probe outperforms linear probes in capturing syntactic structure (UUAS scores)
- **Medium**: The strength metric provides a more nuanced evaluation of probe performance
- **Medium**: BERT layers encode syntactic information gradually, with middle layers capturing the most details

## Next Checks
1. Implement the exact distance computation from White et al. (2021) and compare UUAS scores to the reformulated probe to quantify the impact of the simplification.
2. Incorporate the strength measure into the UUAS calculation and evaluate if it leads to better correlation with downstream task performance.
3. Conduct layer-wise probing on a larger, more diverse dataset (e.g., Universal Dependencies treebanks for multiple languages) to validate the trend of gradual syntactic encoding and identify any language-specific patterns.