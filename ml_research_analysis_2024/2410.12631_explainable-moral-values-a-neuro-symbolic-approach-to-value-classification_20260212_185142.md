---
ver: rpa2
title: 'Explainable Moral Values: a neuro-symbolic approach to value classification'
arxiv_id: '2410.12631'
source_url: https://arxiv.org/abs/2410.12631
tags:
- value
- values
- moral
- sentence
- roles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work combines ontology-based reasoning with ML for explainable
  moral value classification. It uses the sandra neuro-symbolic reasoner with the
  DnS Ontology Design Pattern to infer values from sentences, formalized using the
  Moral Foundations Theory in ValueNet.
---

# Explainable Moral Values: a neuro-symbolic approach to value classification

## Quick Facts
- arXiv ID: 2410.12631
- Source URL: https://arxiv.org/abs/2410.12631
- Reference count: 37
- This work combines ontology-based reasoning with ML for explainable moral value classification

## Executive Summary
This paper presents a neuro-symbolic approach for classifying moral values in text that achieves high accuracy while maintaining explainability. The method combines the sandra neuro-symbolic reasoner with the DnS Ontology Design Pattern to infer moral values from sentences, formalized using the Moral Foundations Theory in ValueNet. Synthetic sentences and their structured representations are generated using an open-source LLM. The approach demonstrates that combining ontological reasoning with traditional ML methods significantly outperforms both pure distributional approaches and LLM-based methods.

## Method Summary
The approach generates synthetic moral value sentences using Mistral-7B LLM with few-shot, cloze, and role-playing prompting techniques. These sentences are structured according to the ValueNet ontology, which formalizes moral foundations theory. The sandra neuro-symbolic reasoner infers description satisfaction probabilities for each sentence, which are then combined with traditional ML features (TF-IDF, sentence embeddings) to train classification models. Multiple classifiers are evaluated, including logistic regression, SVM, random forest, LightGBM, and others.

## Key Results
- Sandra inferences alone provide comparable accuracy to complex approaches
- Combining sandra inferences with TF-IDF or sentence embeddings significantly outperforms all baselines
- The approach maintains explainability by allowing interpretation of classification decisions
- A visualization tool is publicly available for interactive exploration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neuro-symbolic reasoning outperforms pure distributional semantics in explainable value classification
- Mechanism: Sandra combines ontological descriptions with geometric vector spaces, allowing role-based inference that complements statistical text features
- Core assumption: The ValueNet ontology accurately captures moral value semantics and their inter-relationships
- Evidence anchors: "combining the reasoner's inferences with distributional semantics methods largely outperforms all the baselines"
- Break condition: If the ontological formalization poorly represents moral value relationships, inference quality degrades

### Mechanism 2
- Claim: Synthetic data generation using few-shot prompting creates high-quality moral value sentences
- Mechanism: Mistral-7B generates sentences structured according to ValueNet roles without explicit value mention, forcing semantic extrapolation
- Core assumption: LLM's pre-training corpus contains sufficient moral value semantics to enable coherent sentence generation
- Evidence anchors: "We extract all the roles used in every description. This allows the LLM to generate sentences whose roles are not confined to a specific value"
- Break condition: If the LLM's training data lacks diverse moral contexts, generated sentences become biased or semantically incoherent

### Mechanism 3
- Claim: Hybrid models combining sandra inferences with traditional ML achieve superior performance while maintaining explainability
- Mechanism: Feature vectors concatenate probability distributions from sandra with TF-IDF or sentence embeddings, providing both semantic structure and statistical patterns
- Core assumption: sandra's role satisfaction probabilities capture complementary information to distributional features
- Evidence anchors: "TF-IDF + sandra" achieves highest accuracy across multiple classifiers
- Break condition: If sandra inferences become computationally prohibitive or the concatenated feature space becomes too sparse, model performance degrades

## Foundational Learning

- Concept: Description and Situation (DnS) ontology design pattern
  - Why needed here: Provides formal framework for representing moral values as n-ary relations between semantic roles
  - Quick check question: How does a "situation" differ from a "description" in DnS?

- Concept: Frame semantics and semantic role labeling
  - Why needed here: Enables structured representation of sentences where individuals are classified by semantic roles
  - Quick check question: What is the relationship between ValueNet's roles and Framester's semantic frames?

- Concept: Gradient boosting decision trees (LightGBM)
  - Why needed here: State-of-the-art method for combining multiple weak learners to achieve high classification accuracy
  - Quick check question: Why does LightGBM typically outperform single decision trees in tabular data classification?

## Architecture Onboarding

- Component map:
  ValueNet ontology -> sandra neuro-symbolic reasoner -> Mistral-7B LLM -> Feature extraction pipeline -> Classification models -> Visualization dashboard

- Critical path:
  1. Generate synthetic sentences with structured role annotations
  2. Infer description satisfaction probabilities using sandra
  3. Extract text features (TF-IDF, embeddings)
  4. Concatenate sandra inferences with text features
  5. Train classification model
  6. Deploy with visualization interface

- Design tradeoffs:
  - Ontology granularity vs. computational efficiency: Finer-grained roles improve accuracy but increase reasoning complexity
  - Synthetic vs. real data: Synthetic data enables controlled experiments but may not capture real-world value expressions
  - Explainability vs. performance: Simpler models are more interpretable but may sacrifice accuracy

- Failure signatures:
  - Low correlation between sandra inferences and classification results indicates ontology misalignment
  - High hallucination rate in LLM generation suggests prompting issues
  - Overfitting on synthetic data indicates poor generalization to real-world scenarios

- First 3 experiments:
  1. Evaluate sandra-only classification performance vs. baseline models
  2. Test feature concatenation impact by comparing hybrid vs. pure distributional approaches
  3. Validate synthetic data quality by analyzing emotion polarity correlations with value labels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ValueNet's granularity be improved to better capture the semantic relations among values and reduce the overlap in role satisfaction across different values?
- Basis in paper: The paper notes that ValueNet's definitions may lack granularity, as evidenced by the overlap in roles across different values and the correlations between values that may not intuitively align
- Why unresolved: The paper suggests that ValueNet might benefit from further refinement in terms of granularity, but does not provide specific methods or evidence for how this refinement could be achieved or validated
- What evidence would resolve it: Empirical testing of refined ValueNet definitions on a benchmark dataset, demonstrating improved classification accuracy and reduced role overlap

### Open Question 2
- Question: How does the performance of the neuro-symbolic approach compare to traditional ML methods when applied to real-world datasets validated by human annotators?
- Basis in paper: The paper mentions that future work includes testing the methods on well-established datasets validated by humans, but does not provide current comparative results
- Why unresolved: The current evaluation is based on a synthetic dataset, and the paper does not include performance comparisons on real-world datasets
- What evidence would resolve it: Conducting experiments on real-world datasets with human validation and comparing the results with traditional ML methods

### Open Question 3
- Question: How can the bias introduced by the LLM in generating synthetic sentences be mitigated to ensure the dataset is representative of real-world sentences expressing human values?
- Basis in paper: The paper acknowledges that the synthetic dataset suffers from the bias of the LLM used to generate sentences and that it is not possible to assume the dataset is representative of real-world sentences
- Why unresolved: The paper does not propose specific strategies to mitigate this bias or validate the representativeness of the generated dataset
- What evidence would resolve it: Developing and applying bias mitigation techniques to the LLM-generated dataset and validating the representativeness through comparison with real-world data

## Limitations

- Reliance on synthetic data may not capture real-world moral value expression complexity
- Computational cost of sandra inference for large-scale applications is not addressed
- Performance gap between synthetic and real-world data validation is not evaluated

## Confidence

- High confidence: The hybrid approach combining sandra inferences with traditional ML methods demonstrably improves classification accuracy over pure distributional approaches
- Medium confidence: The synthetic data generation pipeline produces semantically coherent moral value sentences, though real-world validation is needed
- Medium confidence: The explainability claims are valid for the synthetic dataset but require further testing on real-world data

## Next Checks

1. Conduct human evaluation studies to validate the quality and moral coherence of synthetic sentences generated using the few-shot prompting approach
2. Test the hybrid model's performance on a real-world dataset of human-annotated moral value statements to assess generalization
3. Measure the computational overhead of sandra inference in production settings and evaluate potential optimizations for scalability