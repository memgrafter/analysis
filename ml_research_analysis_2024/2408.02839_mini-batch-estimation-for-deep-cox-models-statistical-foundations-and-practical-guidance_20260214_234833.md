---
ver: rpa2
title: 'Mini-batch Estimation for Deep Cox Models: Statistical Foundations and Practical
  Guidance'
arxiv_id: '2408.02839'
source_url: https://arxiv.org/abs/2408.02839
tags:
- batch
- size
- data
- where
- cox-nn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies stochastic gradient descent (SGD) for optimizing
  Cox proportional hazards models and Cox-based neural networks (Cox-NNs). Unlike
  standard SGD, which minimizes the average of individual losses, SGD for Cox models
  optimizes a batch-size-dependent function, requiring new theoretical foundations.
---

# Mini-batch Estimation for Deep Cox Models: Statistical Foundations and Practical Guidance

## Quick Facts
- arXiv ID: 2408.02839
- Source URL: https://arxiv.org/abs/2408.02839
- Reference count: 13
- Key outcome: This paper studies stochastic gradient descent (SGD) for optimizing Cox proportional hazards models and Cox-based neural networks (Cox-NNs), establishing consistency, convergence rates, and asymptotic efficiency improvements with batch size.

## Executive Summary
This paper addresses the theoretical foundations of mini-batch stochastic gradient descent (SGD) for Cox proportional hazards models and Cox-based neural networks (Cox-NNs). Unlike standard SGD that minimizes the average of individual losses, SGD for Cox models optimizes a batch-size-dependent partial likelihood function. The authors establish that the SGD estimator for Cox-NN is consistent and achieves minimax optimal convergence rates up to polylogarithmic factors. For linear Cox regression, they prove √n-consistency and asymptotic normality with variance depending on batch size, and demonstrate that doubling batch size improves statistical efficiency. These theoretical results are validated through simulations and a real-world application predicting eye disease progression using fundus images.

## Method Summary
The paper develops a framework for mini-batch SGD optimization of Cox proportional hazards models and Cox-NNs. For Cox-NNs, SGD minimizes a batch-size-dependent population loss rather than the full-sample partial likelihood, requiring new theoretical foundations. The authors establish consistency and optimal convergence rates by showing the batch-size-dependent loss remains convex around the true function. For linear Cox regression, they prove the SGD estimator is √n-consistent and asymptotically normal, with variance decreasing as batch size increases. The method uses projected SGD for Cox regression and employs the linear scaling rule for learning rate to batch size ratio. Experiments include synthetic data generation, simulation studies comparing different batch sampling strategies, and application to real AREDS data with fundus images.

## Key Results
- The SGD estimator for Cox-NN is consistent and achieves the minimax optimal convergence rate up to a polylogarithmic factor.
- For Cox regression, the SGD estimator is √n-consistent and asymptotically normal, with variance depending on batch size.
- Doubling the batch size improves the asymptotic efficiency of SGD estimators for Cox regression, unlike typical SGD optimizations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mini-batch SGD for Cox models optimizes a batch-size-dependent partial likelihood function rather than the full-sample likelihood.
- Mechanism: When a mini-batch is sampled without replacement, the expected gradient over all possible batches equals the gradient of a batch-size-dependent population loss. This loss function changes with batch size because the at-risk set is defined only within the mini-batch.
- Core assumption: The gradient of the batch-size-dependent population loss at the true parameter equals s times the covariance of the batch gradient.
- Evidence anchors:
  - [abstract] "SGD aims to optimize the average of mini-batch partial-likelihood, which is different from the standard partial-likelihood."
  - [section 2.2] "The target function solved by SGD differs from the one solved by GD... the theoretical results for the GD estimator can not be applied to the SGD estimator."
  - [corpus] Weak. No direct neighbor papers discuss batch-size-dependent objectives for Cox models.
- Break condition: If the sampling is with replacement, the equivalence between batch gradient covariance and Hessian fails.

### Mechanism 2
- Claim: The SGD estimator for Cox-NN is consistent and achieves the minimax optimal convergence rate up to a polylogarithmic factor.
- Mechanism: The batch-size-dependent population loss remains convex around the true function, and its minimizer is the true function. Neural network approximation error and estimation error balance at the optimal rate due to sparsity and depth constraints.
- Core assumption: The true function f0 belongs to a composite smoothness class and can be approximated by a neural network within a ball of radius c.
- Evidence anchors:
  - [abstract] "mb-MPLE for Cox-NN is consistent and achieves the optimal minimax convergence rate up to a polylogarithmic factor."
  - [section 3.1] "The convergence rate is determined by the smoothness and the intrinsic dimension of the function f0, rather than the dimension d."
  - [corpus] Weak. No neighbor papers provide supporting evidence for consistency in this specific mini-batch setting.
- Break condition: If the neural network is too small or the true function is not in the smoothness class, approximation error dominates.

### Mechanism 3
- Claim: For Cox regression, doubling the batch size improves the asymptotic efficiency of the SGD estimator, unlike standard SGD where efficiency is batch-size independent.
- Mechanism: The Hessian of the batch-size-dependent population loss at the true parameter equals s times the gradient covariance, and this Hessian increases with batch size, reducing the asymptotic variance.
- Evidence anchors:
  - [abstract] "for Cox regression, we demonstrate the improvement in statistical efficiency when batch size doubles."
  - [section 4.1] "Equality (4.6) indicates that the asymptotic variance of˜θF B(s) n is sH −1 s Σs(H −1 s )T = H −1 s. Then by (4.7), we have H −1 2s ⪯ H −1 s."
  - [corpus] Weak. No neighbor papers discuss batch-size effects on asymptotic efficiency in Cox regression.
- Break condition: If the batch gradient cannot be expressed as a sum of i.i.d. terms, the efficiency gain may not hold for all batch sizes.

## Foundational Learning

- Concept: Partial likelihood and at-risk sets in survival analysis
  - Why needed here: Understanding how the partial likelihood is computed and how the at-risk set is defined is crucial for grasping why mini-batch SGD cannot directly use individual losses.
  - Quick check question: In a dataset of 100 patients, if patient 10 dies at time t and 20 patients are still at risk, how many terms contribute to the partial likelihood for patient 10?

- Concept: U-statistics and their asymptotic properties
  - Why needed here: The mini-batch partial likelihood is a U-statistic, and its asymptotic properties determine the consistency and normality of the SGD estimator.
  - Quick check question: What is the variance of a U-statistic estimator in terms of the kernel's variance and the number of terms?

- Concept: Convexity and strong convexity in non-convex optimization
  - Why needed here: The target function for Cox regression is not strongly convex globally, but is strongly convex in a neighborhood of the true parameter, which is key for proving convergence of projected SGD.
  - Quick check question: If a function has a positive definite Hessian at a point, is it necessarily strongly convex in a neighborhood of that point?

## Architecture Onboarding

- Component map:
  - Data module: Generates survival data with covariates, event times, and censoring indicators.
  - Mini-batch sampler: Samples mini-batches without replacement for SGD.
  - Cox-NN model: Neural network approximating the log-hazard function.
  - Cox regression model: Linear model for the log-hazard function.
  - SGD optimizer: Updates parameters using mini-batch gradients.
  - Evaluation module: Computes loss, C-index, and parameter estimation error.

- Critical path:
  1. Generate survival data.
  2. Sample mini-batch.
  3. Compute mini-batch partial likelihood and gradient.
  4. Update parameters using SGD.
  5. Evaluate convergence and statistical properties.

- Design tradeoffs:
  - Mini-batch size vs. statistical efficiency: Larger batch sizes improve efficiency but increase memory usage and per-iteration computation time.
  - Neural network depth and width vs. approximation error: Deeper and wider networks can approximate more complex functions but increase estimation error.
  - Learning rate schedule vs. convergence speed: Larger initial learning rates speed up convergence but may overshoot the optimum.

- Failure signatures:
  - If mini-batch size is too small, the estimator may be biased and inefficient.
  - If the neural network is too small, it may underfit the true function.
  - If the learning rate is too large, the algorithm may diverge.

- First 3 experiments:
  1. Verify that the mini-batch partial likelihood is a U-statistic and compute its expected value.
  2. Test the consistency of the SGD estimator for Cox-NN on simulated data with known ground truth.
  3. Compare the asymptotic efficiency of SGD estimators for Cox regression with different mini-batch sizes on simulated data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the linear scaling rule (keeping the ratio of learning rate to batch size constant) hold for all activation functions in Cox-NN, or is it specific to ReLU?
- Basis in paper: [explicit] The paper demonstrates the linear scaling rule for Cox-NN with ReLU activation in Section 5.2, but doesn't explore other activation functions.
- Why unresolved: The authors only tested the linear scaling rule with ReLU activation. Different activation functions might have different curvature properties in their loss landscapes, potentially affecting the scaling relationship.
- What evidence would resolve it: Experimental results comparing training dynamics of Cox-NN with various activation functions (ReLU, sigmoid, tanh, etc.) while keeping the learning rate to batch size ratio constant.

### Open Question 2
- Question: How does the choice of mini-batch sampling strategy (stochastic vs. fixed) affect the generalization performance of Cox-NN models in real-world applications?
- Basis in paper: [explicit] The paper mentions both stochastic batch (SB) and fixed batch (FB) strategies in Section 4.1 but doesn't empirically compare their generalization performance.
- Why unresolved: While the paper establishes statistical properties of both strategies, it doesn't investigate how these properties translate to actual prediction performance on unseen data in practical scenarios.
- What evidence would resolve it: Comparative studies on Cox-NN models using SB and FB strategies across multiple real-world datasets, measuring generalization metrics like C-index on held-out test sets.

### Open Question 3
- Question: What is the relationship between batch size and the sharpness of the loss landscape minima found by SGD in Cox-NN, and how does this affect generalization?
- Basis in paper: [inferred] The paper mentions that sharpness-aware optimization has been proposed for general neural networks (Section 6), but doesn't investigate this relationship for Cox-NN specifically.
- Why unresolved: The authors note that the sharpness of the loss function around minima is related to generalization ability, but they don't explore how batch size influences this sharpness in the context of Cox-NN.
- What evidence would resolve it: Empirical analysis of the loss landscape sharpness (e.g., using Hessian-based measures) at the minima found by SGD with different batch sizes in Cox-NN, correlated with generalization performance on test data.

## Limitations
- The theoretical analysis relies heavily on the assumption that the mini-batch partial likelihood forms a proper U-statistic with i.i.d. components, which may not hold for small batch sizes or specific sampling schemes.
- The consistency and convergence rate results for Cox-NN depend on the true function being in a specific smoothness class, which may not be verifiable in practice.
- The improvement in asymptotic efficiency with larger batch sizes is shown theoretically but may have practical limits due to memory constraints and diminishing returns.

## Confidence
- **High confidence**: SGD for Cox regression achieves √n-consistency and asymptotic normality with batch-size-dependent variance (Section 4).
- **Medium confidence**: Mini-batch SGD for Cox-NN is consistent and achieves optimal convergence rates up to polylogarithmic factors (Section 3).
- **Medium confidence**: Doubling batch size improves asymptotic efficiency of SGD estimators for Cox regression (Section 4.1).

## Next Checks
1. **Empirical validation of batch-size efficiency**: Conduct extensive simulations comparing the mean squared error of SGD estimators for Cox regression across different batch sizes (e.g., 2, 4, 8, 16) to verify the theoretical efficiency gains.
2. **Approximation error analysis**: For Cox-NN, systematically vary neural network architecture (depth, width) and measure the trade-off between approximation error and estimation error on synthetic data with known ground truth.
3. **Robustness to sampling schemes**: Test whether the consistency and convergence properties hold when mini-batches are sampled with replacement or using stratified sampling, as opposed to the assumed without-replacement sampling.