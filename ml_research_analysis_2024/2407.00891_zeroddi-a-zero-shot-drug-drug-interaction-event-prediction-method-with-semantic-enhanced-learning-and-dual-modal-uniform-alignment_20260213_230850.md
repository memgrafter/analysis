---
ver: rpa2
title: 'ZeroDDI: A Zero-Shot Drug-Drug Interaction Event Prediction Method with Semantic
  Enhanced Learning and Dual-Modal Uniform Alignment'
arxiv_id: '2407.00891'
source_url: https://arxiv.org/abs/2407.00891
tags:
- ddie
- drug
- which
- ddies
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ZeroDDI, a zero-shot learning approach for
  predicting unseen drug-drug interaction events (DDIEs). The method addresses two
  main challenges: obtaining suitable DDIE representations and handling class imbalance.'
---

# ZeroDDI: A Zero-Shot Drug-Drug Interaction Event Prediction Method with Semantic Enhanced Learning and Dual-Modal Uniform Alignment

## Quick Facts
- arXiv ID: 2407.00891
- Source URL: https://arxiv.org/abs/2407.00891
- Authors: Ziyan Wang; Zhankun Xiong; Feng Huang; Xuan Liu; Wen Zhang
- Reference count: 25
- Zero-shot DDIE prediction with superior performance over baselines

## Executive Summary
This paper introduces ZeroDDI, a zero-shot learning approach for predicting unseen drug-drug interaction events (DDIEs). The method addresses two main challenges: obtaining suitable DDIE representations and handling class imbalance. ZeroDDI employs a biological semantic enhanced representation learning module to emphasize key biological semantics and distill discriminative molecular substructure-related semantics. It also uses a dual-modal uniform alignment strategy to distribute drug pair and DDIE representations uniformly in a unit sphere, mitigating class imbalance. Experiments show that ZeroDDI outperforms baselines, achieving superior performance in zero-shot DDIE prediction, demonstrating its potential as a practical tool for detecting unseen DDIEs.

## Method Summary
ZeroDDI is a zero-shot learning method for DDIE prediction that uses a compatibility framework with three main components. First, the Drug Pair Encoder extracts drug pair representations and molecular substructure embeddings using GNNs and transformer-like learners. Second, the Biological Semantic Enhanced DDIE Representation Learning (BRL) module extracts and fuses class-level and attribute-level semantics from DDIE descriptions using BioBERT and cross-modal attention. Third, the Dual-Modal Uniform Alignment (DUA) strategy aligns drug pair and DDIE representations in a unit sphere using contrastive loss and uniformity constraints. The model is trained using a combined loss function and predicts unseen DDIEs using dot-product similarity.

## Key Results
- ZeroDDI achieves superior performance in zero-shot DDIE prediction compared to baseline methods
- The biological semantic enhanced learning module improves representation quality by emphasizing key biological semantics
- The dual-modal uniform alignment strategy effectively mitigates class imbalance issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Biological semantic enhanced DDIE representation learning (BRL) improves zero-shot performance by emphasizing key biological semantics and distilling discriminative molecular substructure-related semantics.
- Mechanism: BRL extracts both class-level and attribute-level semantics from DDIE textual descriptions and attributes, then fuses them using molecular substructure embeddings through cross-modality attention. This allows the model to capture fine-grained interactions between substructures and semantic tokens, preserving discriminative information for better knowledge transferability.
- Core assumption: The effect attribute contains key biological semantics that should be emphasized, and molecular substructures play an important role in drug properties.
- Evidence anchors:
  - [abstract]: "we design a biological semantic enhanced DDIE representation learning module, which emphasizes the key biological semantics and distills discriminative molecular substructure-related semantics for DDIE representation learning."
  - [section]: "BRL first extracts attribute-level semantics of Effect and together with class-level semantics of DDIE textual descriptions. Then, considering that molecular substructure plays an important role in drug properties [Jin et al., 2023], BRL establishes the fine-gained interaction between substructures and semantic tokens of text to guide the fusion of bi-level semantics, which can distill discriminative semantics for learning DDIE representations."
  - [corpus]: Weak - No direct evidence in corpus neighbors, but related to molecular substructure learning which is a common theme in drug-drug interaction prediction.
- Break condition: If the attribute-level semantics do not contain relevant biological information or the molecular substructure embeddings fail to capture meaningful drug properties, the BRL module would lose its effectiveness.

### Mechanism 2
- Claim: Dual-modal uniform alignment (DUA) strategy mitigates class imbalance by distributing drug pair and DDIE representations uniformly in a unit sphere and aligning matched ones.
- Mechanism: DUA uses a contrastive loss to enforce drug pairs to have the highest similarity with their matched DDIE representations, and applies dual-modal uniformity loss to pull both modal representations toward uniform distribution on a unit sphere centered at the mean of all DDIE representations. This creates clearer decision boundaries and improves discriminative ability.
- Core assumption: Class imbalance leads to unclear decision boundaries and reduced discriminative ability, which can be mitigated by uniform distribution of representations.
- Evidence anchors:
  - [abstract]: "we propose a dual-modal uniform alignment strategy to distribute drug pair representations and DDIE semantic representations uniformly in a unit sphere and align the matched ones, which can mitigate the issue of class imbalance."
  - [section]: "To realize the uniform distribution, i.e., maximize the representation distance between classes in unit sphere space, inspired by [Lu et al., 2022], we take the center of all the DDIE representations as the center of the sphere. Then we utilize the class uniformity loss Lcla to pull every DDIE representation to unit sphere."
  - [corpus]: Weak - No direct evidence in corpus neighbors, but uniform distribution is a common technique in few-shot learning to handle class imbalance.
- Break condition: If the class imbalance is not severe enough to impact decision boundaries, or if the uniform distribution creates artificial separation that harms performance, DUA would not provide benefits.

### Mechanism 3
- Claim: Compatibility framework with cross-modal contrastive learning enables effective knowledge transfer from seen to unseen DDIE classes.
- Mechanism: The model learns drug pair representations and DDIE representations in a common semantic space, then uses dot-product similarity and contrastive loss to align matched pairs while pushing apart mismatched ones. This framework allows the model to classify drug pairs into unseen DDIE classes based on semantic similarity.
- Core assumption: A common semantic space can be constructed where representations of drug pairs and DDIE classes have meaningful similarity relationships, enabling zero-shot classification.
- Evidence anchors:
  - [abstract]: "Based on the compatibility framework, we first obtain the drug pair representations and molecular substructure embeddings of drug pairs from a Drug Pair Encoder... Then we get the DDIE representations by our designed BRL. Thereafter, DUA is designed to align the drug pair representations and their matched DDIE representations in unit sphere space and train the model."
  - [section]: "We introduce a DUA as the compatibility function, which includes an alignment function to align the drug pair representations and its matched DDIE representations for classification, and a dual-modal uniformity loss to constraint both modal representations uniformly distributed in unit sphere."
  - [corpus]: Moderate - Related work "DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning" suggests compatibility frameworks are being explored for DDIE prediction.
- Break condition: If the semantic space cannot capture meaningful relationships between drug pairs and DDIE classes, or if the contrastive learning fails to create proper alignment, the compatibility framework would fail.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The task requires classifying drug pairs into DDIE classes that have no labeled examples in training, which is the definition of zero-shot learning.
  - Quick check question: What is the key difference between zero-shot learning and few-shot learning in terms of labeled data availability?

- Concept: Graph neural networks for molecular representation
  - Why needed here: Drug pairs are represented using molecular graphs, and GNNs are used to extract meaningful substructure embeddings from these graphs.
  - Quick check question: How do graph neural networks capture structural information in molecular graphs differently from traditional feature extraction methods?

- Concept: Cross-modal attention mechanisms
  - Why needed here: The model needs to fuse information from molecular substructures and textual semantics, which requires attention mechanisms to establish fine-grained interactions between different modalities.
  - Quick check question: What is the advantage of using cross-modal attention over simple concatenation or averaging when fusing information from different modalities?

## Architecture Onboarding

- Component map: Drug Pair Encoder -> BRL -> DUA -> Training -> Prediction
- Critical path: Drug Pair Encoder → BRL → DUA → Training → Prediction
- Design tradeoffs:
  - Using BioBERT vs simpler text encoders: BioBERT provides better biomedical semantic understanding but increases computational cost
  - Uniform distribution vs other class imbalance solutions: Uniform distribution creates clearer decision boundaries but may not always be optimal
  - Cross-modal attention vs simpler fusion: Attention provides better fusion but increases model complexity

- Failure signatures:
  - Poor performance on unseen classes: Indicates BRL or DUA may not be working effectively
  - Overfitting to seen classes: Suggests the model is not generalizing properly to the zero-shot setting
  - Slow convergence: May indicate inappropriate hyperparameter settings

- First 3 experiments:
  1. Train with only Drug Pair Encoder and BRL (without DUA) to verify basic representation learning works
  2. Train with Drug Pair Encoder and DUA but using random DDIE representations to verify alignment mechanism works
  3. Train full model with small dataset to verify end-to-end functionality before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ZeroDDI change when applied to real-world drug interaction data from multiple sources beyond DrugBank?
- Basis in paper: [explicit] The paper mentions evaluating on DrugBank v5.1.9 and v5.1.11 datasets but doesn't discuss performance on other real-world data sources.
- Why unresolved: The paper only validates the method on a single dataset, limiting understanding of its generalizability to different data sources and real-world scenarios.
- What evidence would resolve it: Testing ZeroDDI on diverse drug interaction databases and clinical datasets, comparing performance metrics and identifying any dataset-specific challenges or advantages.

### Open Question 2
- Question: What is the impact of different PLM models (beyond BioBERT, SCIBERT, and PubMedBERT) on the performance of the BRL module in ZeroDDI?
- Basis in paper: [explicit] The paper compares BioBERT, SCIBERT, and PubMedBERT but doesn't explore other potential PLM models for the BRL module.
- Why unresolved: The choice of PLM can significantly affect the quality of learned representations, and other models might offer better performance or domain-specific advantages.
- What evidence would resolve it: Systematic evaluation of ZeroDDI using various PLM models (e.g., RoBERTa, ELECTRA, domain-specific biomedical models) and comparing their impact on zero-shot DDIE prediction accuracy.

### Open Question 3
- Question: How does the class imbalance ratio affect the performance of ZeroDDI in scenarios with extreme imbalance (e.g., 1:100000)?
- Basis in paper: [inferred] The paper discusses class imbalance and tests with a 1:100 ratio, but doesn't explore more extreme imbalance scenarios.
- Why unresolved: Understanding the method's robustness to extreme class imbalance is crucial for real-world applications where some DDIEs might be extremely rare.
- What evidence would resolve it: Testing ZeroDDI on datasets with progressively more extreme class imbalance ratios (e.g., 1:1000, 1:10000, 1:100000) and analyzing performance degradation or improvement strategies.

## Limitations
- Performance gains are primarily demonstrated through ablation studies on a single dataset (DrugBank v5.1.9), which may not generalize to other sources or real-world scenarios.
- The uniform alignment strategy's effectiveness in handling class imbalance is theoretical - the paper doesn't show empirical evidence that class imbalance was actually a problem in their experiments.
- The model's complexity (combining BioBERT, GNNs, cross-modal attention, and dual-modal uniformity loss) raises concerns about practical deployment and computational efficiency.

## Confidence
- High confidence: The basic compatibility framework architecture and the use of BioBERT for semantic extraction are well-established approaches with strong supporting evidence in the literature.
- Medium confidence: The effectiveness of the biological semantic enhanced learning module is supported by the ablation study results, but the specific design choices (e.g., cross-modal attention with molecular substructures) lack extensive validation or comparison with simpler alternatives.
- Low confidence: The claim that dual-modal uniform alignment significantly mitigates class imbalance is based on theoretical reasoning rather than empirical validation - the paper doesn't demonstrate that class imbalance was actually problematic in their experiments.

## Next Checks
1. Conduct experiments removing the uniform alignment component on datasets with varying levels of class imbalance to empirically verify its effectiveness in addressing this specific issue.
2. Test ZeroDDI on additional DDIE datasets beyond DrugBank to assess generalization performance and identify potential dataset-specific biases or overfitting.
3. Measure training and inference times, memory usage, and compare with baseline methods to evaluate the practical deployment feasibility of the complex model architecture.