---
ver: rpa2
title: Residual Random Neural Networks
arxiv_id: '2410.19987'
source_url: https://arxiv.org/abs/2410.19987
tags:
- data
- random
- neural
- accuracy
- also
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Residual Random Neural Networks (RRNNs), a
  training method for single-layer feedforward neural networks with random weights
  that achieves high classification accuracy with relatively few hidden neurons. The
  key insight is that in high-dimensional spaces, randomly drawn vectors are almost
  orthogonal, leading to better sample separation.
---

# Residual Random Neural Networks

## Quick Facts
- arXiv ID: 2410.19987
- Source URL: https://arxiv.org/abs/2410.19987
- Authors: M. Andrecut
- Reference count: 18
- Primary result: Achieves up to 99.12% accuracy on MNIST with only 15 iterations and random weights

## Executive Summary
This paper introduces Residual Random Neural Networks (RRNNs), a novel training method for single-layer feedforward neural networks with random weights that achieves high classification accuracy with relatively few hidden neurons. The key insight leverages the property that in high-dimensional spaces, randomly drawn vectors are almost orthogonal, leading to better sample separation. The method uses an iterative residual training algorithm that incrementally improves classification by minimizing the residual error between predictions and targets. The paper demonstrates this approach on MNIST and Fashion-MNIST datasets, achieving state-of-the-art accuracy with a simple architecture. Additionally, the paper presents a security approach using orthonormal random matrices to encrypt both training data and models, enabling secure cloud deployment.

## Method Summary
The method employs an iterative residual training algorithm where each iteration generates random projections, computes hidden layer outputs using hyperbolic tangent activation, and solves a ridge regression problem to minimize the residual error between predictions and targets. The process repeats for T iterations, with each layer trained on the residual from previous layers. The method is extended to a least-squares kernel version (RRKN) that achieves similar performance with a simpler architecture. Data augmentation through FFT is used to increase effective dimensionality. The approach challenges conventional wisdom that random neural networks require many more hidden neurons than input dimensions, showing effective performance when these numbers are of the same order of magnitude in reasonably high-dimensional spaces.

## Key Results
- Achieves 99.12% accuracy on MNIST test set with just 15 iterations
- Achieves 91.41% accuracy on Fashion-MNIST with same methodology
- Demonstrates that random neural networks can achieve high accuracy when J ≈ M (projections ≈ input dimension) rather than requiring J >> M
- Shows effective performance even with relatively few hidden neurons compared to traditional approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-dimensional random vectors are almost orthogonal, improving sample separation
- Mechanism: In high-dimensional spaces (M large), randomly drawn vectors from normal distributions become nearly orthogonal, creating a "blessing of dimensionality" that enhances sample separation for classification
- Core assumption: The dimensionality M must be "reasonably high" for the almost-orthogonality effect to manifest
- Evidence anchors:
  - [abstract]: "in high-dimensional spaces, randomly drawn vectors are almost orthogonal, leading to better sample separation"
  - [section]: "Thus, any two random vectorsu, v ∈ RM with the coordinates sampled from the normal distribution N (0, 1) are almost orthogonal, if the dimensionalityM of the space is large"
  - [corpus]: Weak evidence - no direct citations found in corpus
- Break condition: If M is too small, the orthogonality assumption fails and classification accuracy degrades

### Mechanism 2
- Claim: Residual error correction through iterative training improves classification
- Mechanism: The algorithm iteratively minimizes the residual between predictions and targets by training new random projection layers on the remaining error, creating an error-correcting neural network
- Core assumption: Each residual layer can capture orthogonal information not captured by previous layers
- Evidence anchors:
  - [abstract]: "The method uses an iterative residual training algorithm that incrementally improves classification by minimizing the residual error between predictions and targets"
  - [section]: "Therefore, once we compute the predicted values ˜Y, we can subtract these values from the target matrixY, and we can try to apply the method again but this time on the residualR = Y − ˜Y"
  - [corpus]: Weak evidence - no direct citations found in corpus
- Break condition: When residual norm falls below threshold ε or maximum iterations T is reached

### Mechanism 3
- Claim: Random projections with same order of magnitude as input dimension can achieve high accuracy
- Mechanism: By increasing dimensionality through data augmentation (like FFT) rather than increasing projection count, the method achieves high accuracy with J ≈ M rather than J >> M
- Core assumption: Adding meaningful information (not just padding) to increase dimensionality preserves the almost-orthogonality benefit
- Evidence anchors:
  - [abstract]: "showing effective performance when these numbers are of the same order of magnitude in reasonably high-dimensional spaces"
  - [section]: "one can obtain good classification results even if the number of hidden neurons has the same order of magnitude as the dimensionality of the data samples, if this dimensionality is reasonably high"
  - [corpus]: Weak evidence - no direct citations found in corpus
- Break condition: If added information is not meaningful or orthogonal to original features

## Foundational Learning

- Concept: Ridge regression
  - Why needed here: The method reduces to solving a ridge regression problem for each residual layer, determining output weights W from hidden layer outputs
  - Quick check question: What is the closed-form solution for ridge regression weights when minimizing ||HW - Y||² + λ||W||²?

- Concept: Almost orthogonality in high dimensions
  - Why needed here: Understanding why randomly drawn vectors in high dimensions are nearly orthogonal is crucial for grasping why this method works
  - Quick check question: What is the probability that two random vectors in M-dimensional space are within δ of being orthogonal?

- Concept: Kernel methods and random Fourier features
  - Why needed here: The RRKN extension uses randomized kernel methods, requiring understanding of how random sampling of training points approximates kernel matrices
  - Quick check question: How does the randomized kernel method approximate the full kernel matrix using only J randomly selected samples?

## Architecture Onboarding

- Component map:
  Input data → Random projection generation → Hidden layer computation → Ridge regression → Residual calculation → Final prediction

- Critical path:
  1. Data preprocessing and normalization
  2. Iterative loop: generate random projection → compute hidden outputs → solve ridge regression → update residual
  3. Final prediction using accumulated residuals

- Design tradeoffs:
  - Number of iterations T vs. computational cost vs. accuracy gain
  - Dimensionality M vs. number of projections J (challenging the J >> M assumption)
  - Regularization parameter λ (affects overfitting and stability)
  - Data augmentation strategy (FFT vs. resizing vs. other methods)

- Failure signatures:
  - Accuracy plateaus early (may indicate insufficient iterations or poor random projections)
  - High variance in results (may indicate need for more stable random seed or averaging)
  - Computational cost becomes prohibitive (may indicate need to reduce T or J)

- First 3 experiments:
  1. Baseline test: MNIST with M=784, J=784, T=10, λ=0 - verify ~99% accuracy claim
  2. Dimensionality test: MNIST with M=784, J=200, T=20, λ=0 - test if high T compensates for low J
  3. Data augmentation test: MNIST with original M=784 + FFT, J=784, T=15, λ=0 - verify FFT augmentation benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio J/M that maximizes classification performance for random-SLFNNs in different dimensionality regimes?
- Basis in paper: [explicit] "rises the following fundamental question: is there an optimal ratioJ/M, suchthattheclassificationperformanceismaximized?"
- Why unresolved: The paper shows good performance when J and M are of the same order of magnitude, but doesn't provide a theoretical framework for finding the optimal ratio across different data dimensionalities and distributions.
- What evidence would resolve it: Systematic experiments varying both J and M across multiple datasets with different dimensionalities, combined with theoretical analysis of how random projections' orthogonality properties affect classification accuracy.

### Open Question 2
- Question: How does the RRNN/RRKN approach scale to ultra-high dimensional data (M >> 10,000) and what modifications might be needed?
- Basis in paper: [inferred] The paper demonstrates effectiveness up to reasonable dimensionalities but doesn't explore extremely high-dimensional spaces or address potential computational/memory constraints.
- Why unresolved: The paper focuses on MNIST (784 dimensions) and Fashion-MNIST datasets, leaving open questions about performance and practicality at much higher dimensions.
- What evidence would resolve it: Experiments with high-dimensional datasets (e.g., hyperspectral imagery, genomics data) and analysis of computational complexity and memory requirements as M increases.

### Open Question 3
- Question: What are the theoretical limits of the encryption/obfuscation approach for protecting trained models, particularly regarding security guarantees and vulnerability to attacks?
- Basis in paper: [explicit] "we also describe an encryption (obfuscation) method which can be used to protect both the data and the resulted network model" with specific description of the orthonormal matrix approach.
- Why unresolved: The paper presents a practical encryption scheme but doesn't provide formal security analysis or explore potential vulnerabilities to cryptanalytic attacks.
- What evidence would resolve it: Formal security proofs, analysis of attack resistance (e.g., brute force, differential analysis), and comparison with established cryptographic methods for model protection.

## Limitations

- Limited empirical validation scope: Results primarily demonstrated on MNIST and Fashion-MNIST, which are relatively simple compared to modern image classification challenges
- Theoretical grounding gaps: While orthogonality properties are proven, the connection between this property and classification performance improvement is primarily empirical
- Scalability concerns: Security extension using orthonormal random matrices introduces significant computational overhead without performance benchmarks

## Confidence

**High Confidence Claims**:
- The mathematical proof that random vectors in high dimensions are almost orthogonal
- The ridge regression solution for output weight computation
- The general framework of residual error correction through iterative training

**Medium Confidence Claims**:
- The claimed accuracy improvements on MNIST and Fashion-MNIST
- The assertion that J ≈ M can achieve comparable results to J >> M
- The security benefits of the orthonormal random matrix approach

**Low-Medium Confidence Claims**:
- The generalizability to more complex datasets beyond MNIST
- The claimed efficiency improvements over traditional random neural networks
- The practical security guarantees of the encryption approach

## Next Checks

1. **Cross-dataset validation**: Test the RRNN method on CIFAR-10/100 and ImageNet datasets to verify if the claimed benefits extend to more challenging classification tasks with higher input dimensionality and complexity.

2. **Ablation study on dimensionality**: Systematically vary the ratio of J/M (projections to input dimension) and T (iterations) to quantify the trade-offs between computational cost and accuracy, particularly testing the claim that J ≈ M can achieve high accuracy.

3. **Security performance benchmarking**: Implement the encryption approach and measure the computational overhead compared to standard random neural network training, including timing benchmarks and memory usage analysis for practical deployment scenarios.