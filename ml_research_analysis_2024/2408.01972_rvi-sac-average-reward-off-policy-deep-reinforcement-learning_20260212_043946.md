---
ver: rpa2
title: 'RVI-SAC: Average Reward Off-Policy Deep Reinforcement Learning'
arxiv_id: '2408.01972'
source_url: https://arxiv.org/abs/2408.01972
tags:
- reset
- reward
- average
- rvi-sac
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes RVI-SAC, an off-policy deep reinforcement
  learning method using the average reward criterion. It extends SAC to continuing
  tasks by introducing: (1) RVI Q-learning with delayed f(Q) updates for critic training,
  (2) average reward soft policy improvement theorem for actor updates, and (3) automatic
  reset cost adjustment for handling task termination.'
---

# RVI-SAC: Average Reward Off-Policy Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2408.01972
- Source URL: https://arxiv.org/abs/2408.01972
- Reference count: 40
- This paper proposes RVI-SAC, an off-policy deep RL method using the average reward criterion that achieves competitive or better performance than SAC with various discount rates and ARO-DDPG on Mujoco benchmarks.

## Executive Summary
This paper introduces RVI-SAC, a novel off-policy deep reinforcement learning algorithm that extends SAC to continuing tasks using the average reward criterion. The method addresses three key challenges: stabilizing Q-learning updates through delayed f(Q) estimation, providing theoretical grounding for soft policy improvement under average reward, and automatically adjusting reset costs for environments with task termination. The algorithm demonstrates strong empirical performance across six Mujoco environments, matching or exceeding SAC variants with optimally tuned discount factors while eliminating the need for discount factor tuning.

## Method Summary
RVI-SAC implements off-policy deep RL for continuing tasks by combining RVI Q-learning with delayed f(Q) updates, average reward soft policy improvement, and automatic reset cost adjustment. The algorithm uses double Q-networks updated via a variant of RVI Q-learning where f(Q) is estimated from mini-batches and smoothed through exponential moving average. Policy updates follow from a new average reward soft policy improvement theorem, and a separate Q-network estimates reset frequency to automatically adjust the reset penalty via constrained optimization. The method is evaluated on Mujoco tasks with 10 random seeds per environment, comparing against SAC with various discount rates and ARO-DDPG.

## Key Results
- RVI-SAC achieves competitive or better performance than SAC with optimally tuned discount rates across all tested Mujoco environments
- The delayed f(Q) update technique demonstrates reduced variance in target value estimation compared to direct mini-batch estimation
- Automatic reset cost adjustment outperforms manual tuning, showing particular benefits in Ant environment with termination
- Theoretical analysis proves asymptotic convergence of the proposed RVI Q-learning variant under standard assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The delayed f(Q) update reduces variance in target value estimation, stabilizing learning.
- Mechanism: Instead of directly using f(Qϕ′; B) for the target, the algorithm tracks a smoothed estimate ξt updated via exponential moving average: ξt+1 = ξt + βt(f(Qϕ′; B) − ξt). This dampens high-variance updates.
- Core assumption: The variance of f(Qϕ′; B) across mini-batches is significant enough to destabilize learning if used directly.
- Evidence anchors:
  - [abstract] Introduces "Delayed f(Q) Update technique" to mitigate variance issues.
  - [section 3.1] States "we propose the Delayed f(Q) Update technique" and explains its role in reducing variance.
  - [corpus] No direct evidence from neighbors; assumed from description.
- Break condition: If βt → 0 too fast, ξt becomes stale and loses its variance-reduction benefit; if too slow, adaptation to Q changes lags.

### Mechanism 2
- Claim: Using the average reward criterion avoids the need to tune discount factors, especially in tasks like Swimmer where high γ improves performance.
- Mechanism: Average reward maximizes the long-run time-average return directly, so there is no dependence on a discount factor γ that might need tuning per task.
- Core assumption: In continuing tasks, the optimal policy under average reward differs from the optimal under any finite γ, and SAC with a suboptimal γ underperforms.
- Evidence anchors:
  - [abstract] States that discounted reward can cause "discrepancy between training objective and performance metrics in continuing tasks".
  - [section 4.1] Shows RVI-SAC matches or beats SAC with the best γ across environments, and notes γ = 0.999 was needed for Swimmer but hurts other tasks.
  - [corpus] No neighbor evidence; inferred from ablation.
- Break condition: If the environment is episodic with well-defined termination, average reward may be less natural than discounting.

### Mechanism 3
- Claim: Automatic reset cost adjustment removes the need for hand-tuned reset penalties, enabling stable learning in tasks with termination.
- Mechanism: The algorithm treats the reset frequency as a constraint and adjusts rcost via a Lagrangian dual method so that ρπ
reset ≤ ϵreset, aligning reset frequency with the reward scale automatically.
- Core assumption: Manual tuning of rcost is brittle and environment-dependent; a principled automatic scheme improves robustness.
- Evidence anchors:
  - [section 3.3] Describes "automatic adjustment of the Reset Cost" via constrained optimization and dual variable λ.
  - [section 4.2] Shows RVI-SAC outperforms RVI-SAC with fixed rcost values (0, 10, 100, 250, 500) in Ant.
  - [corpus] No neighbor evidence; stated in paper.
- Break condition: If the constraint ϵreset is set too tight, the agent may never explore risky states; too loose, resets become too frequent and hurt learning.

## Foundational Learning

- Concept: Ergodic MDPs and existence of stationary state distributions.
  - Why needed here: Average reward theory assumes the MDP is ergodic so that a unique stationary distribution dπ(s) exists; otherwise ρπ and Qπ are not well-defined.
  - Quick check question: In an ergodic MDP, does every policy π induce a unique stationary state distribution dπ(s)? (Yes.)

- Concept: RVI Q-learning and the role of the function f.
  - Why needed here: RVI Q-learning updates Q(s,a) using r − f(Q) + maxa′Q(s′,a′); f(Q) estimates the average reward. The convergence proof requires f to satisfy specific Lipschitz and linearity properties.
  - Quick check question: If f(Q) = Q(S,A) for a fixed (S,A), what assumption must hold about visitation frequency for unbiasedness? (The pair must be visited infinitely often under the behavior policy.)

- Concept: Soft policy improvement under average reward.
  - Why needed here: The paper extends SAC's entropy-regularized policy improvement to the average reward setting via a new theorem ensuring ρπnew ≥ ρπold.
  - Quick check question: In the average reward soft Q function, what term replaces the discount factor γt in the cumulative sum? (None; the sum is over infinite horizon without γ.)

## Architecture Onboarding

- Component map:
  - Q-networks (double) -> RVI Q-learning with delayed f(Q) update
  - Policy network -> soft policy update using average reward soft policy improvement theorem
  - Reset Q-network -> estimates reset frequency for automatic rcost adjustment
  - Temperature α and target entropy -> entropy regularization (same as SAC-v2)
  - Replay buffer -> off-policy data storage

- Critical path:
  1. Collect transition (s,a,r,s′,reset_flag)
  2. Store in replay buffer
  3. Sample mini-batch B
  4. Update Q-ϕ1,ϕ2 via RVI loss with delayed f(Q)
  5. Update policy θ via soft policy objective
  6. Update temperature α
  7. If reset_flag: update reset Q and ξreset, adjust rcost
  8. Soft update target networks

- Design tradeoffs:
  - Delayed f(Q) vs. reference-state f(Q): delayed f(Q) avoids dependence on a specific (s,a) but adds a moving-average parameter ξ; reference-state is simpler but brittle
  - Automatic reset cost vs. fixed: automatic removes tuning burden but adds an extra Q-network and constraint handling overhead
  - Average reward vs. discounted: no discount tuning but loses the natural episodic structure unless resets are handled

- Failure signatures:
  - High variance in Q updates -> unstable learning; check ξ adaptation speed
  - Policy collapse to deterministic -> temperature α may be too low; monitor entropy
  - Frequent resets with zero reward -> rcost too high or ϵreset too tight; inspect reset frequency vs. target
  - Slow convergence -> step-sizes a(k), b(k) too small; verify Robbins-Monro conditions

- First 3 experiments:
  1. Run RVI-SAC on Swimmer with SAC (γ=0.99) as baseline; verify performance parity or improvement
  2. Disable delayed f(Q) update (use f(Qϕ′; B) directly); confirm performance drop and higher variance
  3. Fix rcost=0 and compare to automatic adjustment; show tuning sensitivity and performance gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal function f(Q) for RVI Q-learning with delayed updates in deep neural network settings?
- Basis in paper: [explicit] The paper discusses choosing f(Q) = (1/|B|)ΣQ(s',a') as an unbiased estimator but doesn't determine the optimal choice.
- Why unresolved: Different choices of f(Q) could lead to different convergence properties and performance characteristics that haven't been theoretically or empirically compared.
- What evidence would resolve it: A systematic comparison of different f(Q) choices (reference state, sum over states, other estimators) on benchmark tasks showing which provides best convergence speed and final performance.

### Open Question 2
- Question: How does the automatic reset cost adjustment perform in environments with varying reward scales and termination frequencies?
- Basis in paper: [inferred] The paper demonstrates automatic reset cost adjustment on Mujoco tasks but doesn't explore its behavior across diverse environments with different reward structures.
- Why unresolved: The effectiveness of the constrained optimization approach for reset cost adjustment may vary significantly depending on the reward scale and termination dynamics of different environments.
- What evidence would resolve it: Experiments testing the algorithm on environments with widely varying reward scales, termination conditions, and frequency targets to determine robustness and limitations.

### Open Question 3
- Question: What are the finite-time convergence guarantees for RVI-SAC with neural network function approximation?
- Basis in paper: [explicit] The paper provides asymptotic convergence analysis in tabular settings but notes that finite-time analysis with linear function approximation is future work.
- Why unresolved: Current theoretical analysis only covers asymptotic convergence in tabular settings, leaving a gap in understanding practical convergence behavior with function approximation.
- What evidence would resolve it: A theoretical analysis providing finite-time regret bounds or convergence rates for RVI-SAC with neural network function approximation, or extensive empirical studies measuring convergence speed across diverse environments.

## Limitations
- The theoretical convergence proof relies on assumptions about step-size sequences and function class properties that may not hold with neural networks
- Ablation studies don't exhaustively test individual component contributions across all environments
- The automatic reset cost adjustment is only validated on the Ant environment, leaving its general applicability uncertain

## Confidence

High: RVI-SAC can match or exceed SAC performance in continuing tasks when tuned appropriately, given strong experimental results across six Mujoco environments.

Medium: The necessity and sufficiency of the delayed f(Q) update for variance reduction, as the paper demonstrates its benefit but doesn't directly compare to alternative variance-reduction techniques.

Low: The general applicability of the automatic reset cost adjustment, as it is only validated on the Ant environment and its performance in purely continuing tasks without termination is not reported.

## Next Checks

1. **Ablation of Delayed Update**: Run RVI-SAC with direct f(Qϕ′; B) targets (no delayed ξ) on HalfCheetah and Walker2d to quantify the variance reduction and performance impact claimed in Mechanism 1.

2. **Reset Cost Sensitivity**: Sweep the constraint parameter ϵreset over a wider range (e.g., 0.01 to 0.5) in the Ant environment to map the performance landscape and identify if the automatic adjustment is robust to its choice.

3. **Continuing Task Test**: Evaluate RVI-SAC on a known continuing task without termination (e.g., a modified Mujoco environment with no done signal) to verify that the algorithm degrades gracefully and still provides benefits over discounted SAC without requiring the reset mechanism.