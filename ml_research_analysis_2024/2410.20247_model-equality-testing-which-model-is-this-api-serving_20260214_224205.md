---
ver: rpa2
title: 'Model Equality Testing: Which Model Is This API Serving?'
arxiv_id: '2410.20247'
source_url: https://arxiv.org/abs/2410.20247
tags:
- llama-3
- samples
- power
- numbers
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Model Equality Testing, a statistical framework
  to detect when black-box language model APIs serve distributions that differ from
  reference weights due to quantization, watermarking, finetuning, or other modifications.
  The core method uses Maximum Mean Discrepancy (MMD) with a Hamming kernel to compare
  prompt-completion samples from the API against a reference distribution, achieving
  a median power of 77.4% in detecting distortions using only ~10 samples per prompt.
---

# Model Equality Testing: Which Model Is This API Serving?

## Quick Facts
- **arXiv ID**: 2410.20247
- **Source URL**: https://arxiv.org/abs/2410.20247
- **Reference count**: 40
- **Primary result**: MMD with Hamming kernel achieves 77.4% median power in detecting API distribution distortions using ~10 samples per prompt

## Executive Summary
This paper introduces Model Equality Testing, a statistical framework to detect when black-box language model APIs serve distributions that differ from reference weights due to quantization, watermarking, finetuning, or other modifications. The core method uses Maximum Mean Discrepancy (MMD) with a Hamming kernel to compare prompt-completion samples from the API against a reference distribution, achieving a median power of 77.4% in detecting distortions using only ~10 samples per prompt. When applied to 31 commercial Llama model endpoints, 11 were flagged as statistically different from reference weights. The framework also enables estimation of pairwise distances between model distributions, revealing that models within the same family are more similar than those within the same size range.

## Method Summary
The framework uses Maximum Mean Discrepancy (MMD) with a Hamming kernel to test whether black-box API distributions match reference model weights. The test samples completions from both reference (fp32/fp16) and API distributions, computes Hamming MMD, and uses permutation to estimate p-values. A composite null hypothesis combines fp32 and fp16 references to control false positives. The same machinery estimates pairwise distances between distributions, enabling both hypothesis testing and effect-size quantification.

## Key Results
- MMD with Hamming kernel achieves 77.4% median power against various distortions using ~10 samples per prompt
- 11 out of 31 commercial Llama endpoints were statistically different from reference weights
- Pairwise distance estimates reveal models within the same family are more similar than those within the same size range
- The framework successfully detects quantization, watermarking, and finetuning modifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MMD with a Hamming kernel is sample-efficient for high-dimensional text comparison.
- Mechanism: The Hamming kernel counts position-wise character matches, reducing the effective dimensionality compared to a full one-hot encoding over all possible strings. This lets the test capture distributional differences with far fewer samples than needed for a naive string-space comparison.
- Core assumption: Language model outputs are not uniformly distributed over all possible strings, so most tokens/characters contribute little to the MMD signal.
- Evidence anchors:
  - [abstract] "a test built on a simple string kernel achieves a median of 77.4% power against a range of distortions, using an average of just 10 samples per prompt."
  - [section 3] "we investigate a fast kernel related to the Hamming distance between completions"
  - [corpus] Weak; related works focus on API testing rather than MMD-kernel efficiency.

### Mechanism 2
- Claim: MMD estimates a true statistical distance, enabling both hypothesis testing and effect-size quantification.
- Mechanism: The test statistic MMD(DQ, DP) is an unbiased estimator of the squared distance between the two distributions' kernel embeddings. This dual use allows one to reject H0 and also rank how far apart two APIs are.
- Core assumption: The chosen kernel is characteristic (or at least pseudo-metric) so that MMD = 0 iff the distributions are identical under the test's semantics.
- Evidence anchors:
  - [abstract] "Because our test statistic is an estimator of a distance, we also explore how this same machinery can be used to quantify statistical distances between black-box endpoints."
  - [section 3] "the test statistic is an estimator of a distance. As a result, we can reuse the machinery to quantify the degree to which two models differ"
  - [corpus] Weak; neighbors discuss API testing but not MMD distance estimation.

### Mechanism 3
- Claim: Composite null testing with multiple reference precisions (fp32 vs. fp16) controls false positives while remaining sensitive to most API distortions.
- Mechanism: By requiring rejection against both high- and low-precision nulls, the test guards against false alarms from floating-point rounding while still flagging meaningful changes like quantization to int8 or watermarking.
- Core assumption: Most user-facing distortions change the distribution more than the difference between fp32 and fp16.
- Evidence anchors:
  - [section 5] "We consider two possible null distributions, both derived from model weights released by Meta on Hugging Face: P1 is the full-precision model weights, and P2 is the fp16-precision model weights."
  - [corpus] Weak; related works focus on API stability but not composite null construction.

## Foundational Learning

- Concept: Two-sample kernel test and Maximum Mean Discrepancy
  - Why needed here: Provides a distribution-free, non-parametric way to compare high-dimensional LLM completions without needing explicit probability access.
  - Quick check question: In MMD, what does a large empirical estimate imply about the two samples?

- Concept: Hamming distance as a string kernel
  - Why needed here: Enables efficient comparison of text completions by focusing on per-position character matches rather than exact string equality.
  - Quick check question: How does the Hamming kernel differ from a one-hot kernel over the full string space?

- Concept: Type I error control and composite null hypothesis
  - Why needed here: Ensures that rejections are reliable and guards against false positives from minor implementation differences like precision changes.
  - Quick check question: How does combining two one-sided tests (fp32 vs. fp16) into a composite rule control the overall false positive rate?

## Architecture Onboarding

- Component map:
  - Data collection: prompt distributions (Wikipedia, HumanEval, UltraChat), local model sampling (fp32/fp16/int8/nf4/watermark), API sampling (via provider packages or HTTP)
  - Test engine: MMD estimator with Hamming kernel, permutation or bootstrap p-value simulation, composite null aggregation
  - Analysis: power estimation via Monte Carlo, pairwise distance clustering, audit result aggregation

- Critical path:
  1. Sample from reference distribution(s) and API
  2. Compute Hamming MMD and estimate p-value (permutation or bootstrap)
  3. For composite null, repeat step 2 for each reference and apply logical AND
  4. Report rejections and estimated distances

- Design tradeoffs:
  - Hamming kernel vs. all-substrings: faster but not universal; may miss some subtle differences
  - Permutation vs. parametric simulation: permutation needs no null sampling but is slower; parametric is fast but requires cached null stats
  - Sample size: 10 m samples per prompt gives good power for many distortions but may miss very small effect sizes

- Failure signatures:
  - Low power against fp16: suggests kernel is insensitive to small floating-point changes
  - High variance in MMD estimates: indicates insufficient samples or highly variable completions
  - False positives across providers: may indicate shared preprocessing or decoding quirks

- First 3 experiments:
  1. Run Hamming MMD on fp32 vs. fp16 for a small model (should not reject) to confirm Type I error control
  2. Test fp32 vs. int8 on a larger model (should reject with high power) to validate sensitivity
  3. Compare two different API endpoints for the same model (e.g., Amazon vs. Azure) and compute pairwise MMDs to check clustering behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal sample size N for balancing statistical power and cost efficiency in Model Equality Testing across different model families and prompt distributions?
- Basis in paper: [explicit] The paper shows that power increases with sample size N, but the relationship varies across models and tasks. They use N=10m (average 10 samples per prompt) as a default but note that larger m values increase power.
- Why unresolved: The paper doesn't systematically explore how the optimal N varies across different model families (Llama vs GPT vs others), prompt types (Wikipedia vs coding vs instruction-following), or specific distortions (quantization vs finetuning vs watermarking). The cost trade-offs for different API providers are also not quantified.
- What evidence would resolve it: A comprehensive study testing multiple N values (e.g., N=5m, 10m, 25m, 50m) across all model families and prompt types, measuring both statistical power and actual monetary costs, would identify optimal sample sizes for different scenarios.

### Open Question 2
- Question: How does the performance of the Hamming MMD test degrade when applied to multimodal language models that output both text and structured data (e.g., code with execution results, tables, or images)?
- Basis in paper: [inferred] The paper focuses exclusively on text-only language model outputs and uses a string kernel based on Hamming distance. The framework assumes a fixed vocabulary V and completion length L, which may not apply to multimodal outputs.
- Why unresolved: The paper doesn't explore how the test would handle structured outputs, different data types, or variable-length responses. The assumption of a fixed vocabulary becomes problematic when models can generate arbitrary tokens, code, or structured data.
- What evidence would resolve it: Testing the Hamming MMD and alternative kernels on multimodal model outputs, comparing performance to text-only models, and developing adaptations for handling structured data would establish the test's limitations and potential extensions.

### Open Question 3
- Question: What specific modifications to API implementations (quantization levels, caching strategies, temperature settings) produce statistically detectable differences that are also practically significant for end users?
- Basis in paper: [explicit] The paper identifies that 11 out of 31 API endpoints serve different distributions than reference weights, and estimates MMD distances between providers. They find some deviations are comparable to substituting entirely different models.
- Why unresolved: The paper doesn't establish a clear threshold for what constitutes a practically significant deviation, nor does it systematically vary implementation parameters to determine which modifications are most detectable. The correlation between MMD distance and task performance (like HumanEval accuracy) is only moderate.
- What evidence would resolve it: Controlled experiments varying specific implementation parameters (e.g., different quantization schemes, cache sizes, temperature values) while measuring both statistical detectability via MMD and practical impact on downstream task performance would establish meaningful thresholds.

## Limitations
- Power variability across different model families and prompt distributions is not fully characterized
- The Hamming kernel may miss distributional changes that preserve per-character statistics
- Without ground truth about intentional vs. unintentional API differences, audit results are difficult to interpret
- Composite null testing may sacrifice sensitivity for conservative false positive control

## Confidence
- **High confidence**: The fundamental mechanism of using MMD with a Hamming kernel for comparing text distributions is mathematically sound and the implementation details are sufficiently specified for reproduction.
- **Medium confidence**: The empirical power estimates and audit results are methodologically valid but may not generalize across all model families, distortion types, or API implementations without additional validation.
- **Low confidence**: The interpretation of audit results (which flagged differences are meaningful for users) and the characterization of kernel sensitivity boundaries require further investigation with ground truth data.

## Next Checks
1. **Power characterization across distortion spectrum**: Systematically test MMD detection thresholds by applying controlled perturbations to reference distributions (ranging from fp16 quantization to subtle fine-tuning) and mapping the resulting power curve to establish clear detection boundaries.

2. **Ground truth audit validation**: For a subset of the 31 audited endpoints, obtain direct confirmation from providers about whether detected differences are intentional (e.g., safety fine-tuning) or unintentional (e.g., implementation artifacts), then validate the framework's practical utility.

3. **Kernel sensitivity analysis**: Design and test synthetic modifications that preserve per-character marginal distributions but shift higher-order n-gram statistics to determine whether Hamming MMD systematically misses certain classes of distributional changes.