---
ver: rpa2
title: 'Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual
  Similarity'
arxiv_id: '2406.09790'
source_url: https://arxiv.org/abs/2406.09790
tags:
- pcc-tuning
- learning
- contrastive
- arxiv
- spearman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the performance ceiling of contrastive learning
  methods in semantic textual similarity (STS) tasks, demonstrating that the theoretical
  upper limit for Spearman's correlation scores is 87.5 due to the binary classification
  nature of contrastive learning. To break through this ceiling, the authors propose
  Pcc-tuning, a two-stage training approach that first uses contrastive learning on
  NLI data, then fine-tunes with a small amount of fine-grained annotated data using
  Pearson correlation as the loss function.
---

# Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity

## Quick Facts
- arXiv ID: 2406.09790
- Source URL: https://arxiv.org/abs/2406.09790
- Authors: Bowen Zhang; Chunping Li
- Reference count: 15
- One-line primary result: Achieves average Spearman's correlation scores above 90 across seven STS benchmarks with 7B-scale PLMs

## Executive Summary
This paper identifies a fundamental limitation in contrastive learning methods for semantic textual similarity (STS) tasks, demonstrating that the binary classification nature of these approaches caps achievable Spearman correlation scores at 87.5. To overcome this ceiling, the authors propose Pcc-tuning, a two-stage training approach that combines contrastive learning on NLI data with fine-tuning using Pearson correlation loss on a small annotated dataset. Experiments show Pcc-tuning significantly outperforms previous state-of-the-art methods, achieving scores above 90 across seven benchmarks with various 7B-scale PLMs while requiring only 3.7% of the original training data in the second stage.

## Method Summary
Pcc-tuning is a two-stage training approach designed to surpass the performance ceiling of contrastive learning methods in STS tasks. The first stage employs contrastive learning using InfoNCE loss on NLI datasets to establish a good semantic space. The second stage fine-tunes the model using Pearson correlation as the loss function on a small amount of fine-grained annotated data from STS-B and SICK-R datasets. This approach leverages the strengths of contrastive learning for semantic space establishment while using Pearson correlation to capture fine-grained ordinal relationships between text pairs that binary classification cannot model.

## Key Results
- Achieves average Spearman's correlation scores above 90 across seven STS benchmarks
- Surpasses the theoretical ceiling of 87.5 for contrastive learning methods
- Requires only 3.7% of the original training data in the second stage
- Demonstrates robustness across different prompts and batch sizes
- Outperforms previous state-of-the-art methods with various 7B-scale PLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning's binary classification structure inherently limits Spearman correlation scores to 87.5
- Mechanism: Contrastive learning methods like InfoNCE Loss only distinguish between similar and dissimilar text pairs, treating all dissimilar pairs as equivalent. This binary classification approach prevents modeling fine-grained ordinal relationships between text pairs.
- Core assumption: The performance ceiling is determined by the binary classifier's optimal behavior rather than the quality of PLMs or training data
- Evidence anchors:
  - [section] "Specifically, contrastive learning only distinguishes between two categories: similar and dissimilar, in determining the semantic relationships between text pairs. This binary classification strategy restricts its maximum achievable Spearman's correlation score to 87.5, even under optimal conditions."
  - [abstract] "we propose Pcc-tuning, a two-stage training approach that first uses contrastive learning on NLI data, then fine-tunes with a small amount of fine-grained annotated data using Pearson correlation as the loss function"
- Break condition: When the method can model more than two categories of semantic similarity (fine-grained ordinal relationships)

### Mechanism 2
- Claim: Pcc-tuning breaks the ceiling by adding Pearson correlation fine-tuning on a small annotated dataset
- Mechanism: After contrastive learning pre-training, Pcc-tuning introduces Pearson correlation as a loss function during fine-tuning. This allows the model to leverage fine-grained ordinal relationships between text pairs that contrastive learning cannot capture.
- Core assumption: Pearson correlation can effectively model ordinal relationships between text pairs and provide differentiable gradients for training
- Evidence anchors:
  - [abstract] "Experimental results demonstrate Pcc-tuning significantly outperforms previous state-of-the-art methods, achieving average Spearman's correlation scores above 90 across seven STS benchmarks with various 7B-scale PLMs"
  - [section] "Employing Pearson coefficient as the loss function enables effective utilization of fine-grained annotation information and supports diverse combinations with a small volume of data"
- Break condition: When Pearson correlation cannot effectively model the ordinal relationships in the data, or when the small annotated dataset is insufficient

### Mechanism 3
- Claim: The two-stage training approach allows contrastive learning to optimize semantic space while Pearson fine-tuning captures fine-grained relationships
- Mechanism: Stage 1 uses contrastive learning to establish a good semantic space with uniform embedding distribution. Stage 2 uses Pearson correlation to refine the embeddings based on fine-grained ordinal relationships, achieving performance beyond the contrastive learning ceiling.
- Core assumption: The contrastive learning stage provides a good foundation for the Pearson fine-tuning stage, and the two stages complement each other
- Evidence anchors:
  - [section] "Pcc-tuning employs a two-stage training pipeline and is designed to surpass the 87.5 performance upper bound of contrastive learning methods"
  - [abstract] "Experiments show Pcc-tuning significantly outperforms previous state-of-the-art methods across different PLMs and prompts"
- Break condition: When the contrastive learning stage fails to establish a good semantic space, or when the two-stage approach introduces optimization difficulties

## Foundational Learning

- Concept: Spearman correlation coefficient
  - Why needed here: The paper's core claim revolves around the theoretical upper bound of Spearman correlation scores achievable by contrastive learning methods
  - Quick check question: What is the maximum Spearman correlation score achievable by binary classifiers in STS tasks, and how is this derived?

- Concept: Pearson correlation coefficient
  - Why needed here: Pcc-tuning uses Pearson correlation as the loss function in the second stage to leverage fine-grained ordinal relationships
  - Quick check question: Why is Pearson correlation used instead of Spearman correlation in the fine-tuning stage, given that Spearman correlation is the evaluation metric?

- Concept: Contrastive learning and InfoNCE Loss
  - Why needed here: Understanding how contrastive learning works and its limitations is crucial to understanding why Pcc-tuning is needed
  - Quick check question: How does InfoNCE Loss function as a binary classifier, and why does this limit its performance in STS tasks?

## Architecture Onboarding

- Component map: NLI dataset -> Stage 1 (Contrastive learning with InfoNCE Loss) -> Mixed STS-B + SICK-R dataset -> Stage 2 (Pearson correlation fine-tuning) -> Evaluation on STS benchmarks

- Critical path:
  1. Load PLM checkpoint
  2. Stage 1: Fine-tune with contrastive learning on NLI dataset
  3. Stage 2: Fine-tune with Pearson correlation on STS-B + SICK-R
  4. Evaluate on STS benchmarks

- Design tradeoffs:
  - Using a small annotated dataset (3.7% of original) vs. larger dataset
  - Two-stage training (complexity) vs. single-stage (ceiling limitation)
  - Pearson correlation (differentiable) vs. Spearman correlation (non-differentiable)

- Failure signatures:
  - Stage 1 fails to establish good semantic space: Poor performance even before Stage 2
  - Stage 2 overfitting: Performance degradation on validation sets
  - Batch size too small: Model collapse or unstable training

- First 3 experiments:
  1. Reproduce baseline contrastive learning results to verify the 87.5 ceiling
  2. Implement Stage 1 (contrastive learning) and verify performance improvement over baseline
  3. Implement full Pcc-tuning and verify performance exceeding the ceiling on STS benchmarks

## Open Questions the Paper Calls Out
- [No open questions explicitly called out in the provided content]

## Limitations
- Limited evaluation to English STS benchmarks only
- Performance on domain-specific or cross-lingual STS tasks untested
- No systematic exploration of minimum fine-tuning dataset size required
- Evaluation limited to 7B-scale PLMs without testing scaling patterns

## Confidence
- **Medium** on the theoretical derivation of the 87.5 ceiling
- **Low** on the generalizability of Pcc-tuning across different domains
- **Medium** on the claimed robustness to prompts and batch sizes

## Next Checks
1. **Mathematical validation of the 87.5 ceiling**: Derive the theoretical upper bound under various assumptions about the underlying data distribution and classifier behavior. Compare with empirical observations across different PLMs and datasets to identify discrepancies between theory and practice.

2. **Cross-domain generalization test**: Apply Pcc-tuning to domain-specific STS datasets (e.g., clinical text similarity, legal document similarity) that differ significantly from the general-domain NLI and STS data used in the original experiments. Measure performance degradation and identify which components of the method are most sensitive to domain shifts.

3. **Ablation study on data efficiency**: Systematically vary the size of the fine-tuning dataset in Stage 2 (beyond the 3.7% mentioned) to quantify the trade-off between annotation cost and performance gains. Test whether Pcc-tuning maintains its advantage over contrastive learning when using only 1% or 0.5% of the original training data.