---
ver: rpa2
title: 'Attention Mechanisms Don''t Learn Additive Models: Rethinking Feature Importance
  for Transformers'
arxiv_id: '2405.13536'
source_url: https://arxiv.org/abs/2405.13536
tags:
- slalom
- token
- scores
- linear
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical and empirical analysis of the
  limitations of transformer models in representing additive functions, which are
  commonly used for feature attribution in explainable AI (XAI). The authors formally
  prove that transformer architectures are structurally incapable of representing
  linear or additive surrogate models used for feature attribution, undermining the
  grounding of these conventional explanation methodologies.
---

# Attention Mechanisms Don't Learn Additive Models: Rethinking Feature Importance for Transformers

## Quick Facts
- arXiv ID: 2405.13536
- Source URL: https://arxiv.org/abs/2405.13536
- Reference count: 40
- Primary result: Transformers are structurally incapable of representing linear/additive surrogate models used for feature attribution, requiring new explanation approaches like SLALOM

## Executive Summary
This paper presents a theoretical and empirical analysis of the limitations of transformer models in representing additive functions, which are commonly used for feature attribution in explainable AI (XAI). The authors formally prove that transformer architectures are structurally incapable of representing linear or additive surrogate models used for feature attribution, undermining the grounding of these conventional explanation methodologies. To address this discrepancy, they introduce the Softmax-Linked Additive Log Odds Model (SLALOM), a novel surrogate model specifically designed to align with the transformer framework.

## Method Summary
The paper combines theoretical analysis with empirical validation to examine the mismatch between transformer architectures and additive surrogate models. The authors prove mathematically that transformers cannot represent additive functions due to their attention mechanism structure. They then introduce SLALOM, a new surrogate model that incorporates the softmax nonlinearity inherent to transformers. SLALOM is evaluated on both synthetic and real-world datasets, demonstrating its ability to provide efficient and accurate feature importance explanations that better align with how transformers actually process information.

## Key Results
- Transformers are structurally incapable of representing linear or additive surrogate models used for feature attribution
- SLALOM achieves substantially higher fidelity than competing surrogate models or provides comparable quality at much lower computational cost
- Experiments confirm a mismatch between surrogate models and predictive models, with two scores covering different angles of interpretability

## Why This Works (Mechanism)
SLALOM works by explicitly modeling the softmax nonlinearity that is fundamental to transformer attention mechanisms. Traditional additive models assume linear relationships between inputs and outputs, but transformers apply softmax operations that create non-linear interactions between features. SLALOM incorporates this softmax structure directly into its mathematical formulation, allowing it to better approximate how transformers actually process and weigh different input features during prediction.

## Foundational Learning
- Transformer attention mechanisms: Essential for understanding why traditional additive models fail; quick check: verify understanding of how softmax operates in attention weights
- Feature attribution methods: Needed to grasp the importance of additive models in XAI; quick check: explain what feature attribution aims to accomplish
- Surrogate modeling: Critical for understanding how explanations approximate black-box model behavior; quick check: describe the purpose of using surrogate models for interpretability
- Mathematical proofs of model limitations: Important for validating the theoretical claims; quick check: understand the basic structure of the proof showing transformers cannot represent additive functions
- Efficiency-quality trade-offs: Key for evaluating SLALOM's practical advantages; quick check: explain what makes an explanation method both efficient and high-fidelity

## Architecture Onboarding

Component map: Input text -> Token embedding -> Transformer layers (self-attention + feedforward) -> Output logits -> SLALOM explanation extraction

Critical path: The core insight is that the softmax operation in attention creates non-linear interactions that prevent transformers from learning additive relationships between features and predictions.

Design tradeoffs: The paper trades traditional interpretability assumptions (linearity) for architectural alignment (softmax-incorporation), accepting that explanations must match the non-linear nature of the model being explained.

Failure signatures: If a surrogate model assumes linearity when the target model uses softmax-based attention, the explanations will systematically misrepresent feature importance.

First experiments to run:
1. Verify the mathematical proof showing transformers cannot represent additive models on a simple synthetic dataset
2. Compare SLALOM explanations against traditional additive methods on a small text classification task
3. Test the efficiency claims by measuring computation time for explanations across different model sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation relies heavily on synthetic datasets that may not capture real-world complexity
- Performance advantages need careful interpretation as evaluations compare against mismatched baseline methods
- Findings may not generalize to all transformer variants and tasks without further investigation

## Confidence
- High confidence: Transformer structural limitations for additive models (theoretical proof)
- Medium confidence: SLALOM's practical advantages in real-world scenarios (based on current empirical evidence)
- Low confidence: Claims about universal efficiency gains across all transformer variants

## Next Checks
1. Test SLALOM's performance on a broader range of transformer architectures (BERT, RoBERTa, GPT variants) across multiple tasks beyond text classification
2. Validate findings with larger-scale real-world datasets and compare against state-of-the-art XAI methods in production settings
3. Investigate whether architectural modifications to attention mechanisms could enable better additive model representation without requiring new surrogate models