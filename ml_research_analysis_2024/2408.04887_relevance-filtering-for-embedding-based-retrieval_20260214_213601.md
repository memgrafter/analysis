---
ver: rpa2
title: Relevance Filtering for Embedding-based Retrieval
arxiv_id: '2408.04887'
source_url: https://arxiv.org/abs/2408.04887
tags:
- retrieval
- cosine
- relevance
- filtering
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of low precision in embedding-based
  retrieval systems, particularly prominent in product search where the number of
  relevant products is often small. The authors propose a novel relevance filtering
  component called "Cosine Adapter" that transforms raw cosine similarity scores into
  interpretable relevance scores using a query-dependent mapping function.
---

# Relevance Filtering for Embedding-based Retrieval

## Quick Facts
- arXiv ID: 2408.04887
- Source URL: https://arxiv.org/abs/2408.04887
- Authors: Nicholas Rossi, Juexin Lin, Feng Liu, Zhen Yang, Tony Lee, Alessandro Magnani, Ciya Liao
- Reference count: 40
- Primary result: Improves precision of embedding-based retrieval by 0.139-0.207 PR AUC on MS MARCO and 0.7074-0.8619 PR AUC on Walmart product search data

## Executive Summary
This paper addresses a critical limitation in embedding-based retrieval systems: low precision when the number of relevant items is small, particularly in product search scenarios. The authors propose a novel relevance filtering component called the Cosine Adapter that transforms raw cosine similarity scores into interpretable relevance scores using a query-dependent mapping function. By applying a global threshold on these mapped scores, the system can effectively filter out irrelevant results while maintaining high recall. The approach demonstrates significant improvements in precision metrics across both public benchmark datasets and large-scale e-commerce applications, with successful validation through online A/B testing.

## Method Summary
The paper introduces a two-stage approach to embedding-based retrieval. First, a dual-encoder model generates query and item embeddings, producing cosine similarity scores. The Cosine Adapter then transforms these raw similarity scores into calibrated relevance scores through a learned mapping function. This adapter consists of feedforward layers with residual connections that map the cosine similarity (scaled to [0,1]) to a calibrated score. The system applies a global threshold to these calibrated scores to filter out irrelevant items, with the threshold calibrated to maintain high recall (99% for Walmart, 95% for other experiments). The adapter is trained on manually annotated relevance data using mean squared error loss, while the dual encoders are trained on engagement data. The entire pipeline operates efficiently, with the adapter adding only O(d²) complexity for feedforward layers and O(K) for computing calibrated scores.

## Key Results
- Cosine Adapter achieves 0.139-0.207 PR AUC improvement over baselines on MS MARCO dataset
- On Walmart product search data, the approach improves PR AUC by 0.7074-0.8619 compared to baselines
- Online A/B testing on Walmart site validates practical effectiveness of the approach
- The method maintains high recall (>95%) while significantly improving precision
- Global threshold strategy works effectively across diverse query types without query-specific tuning

## Why This Works (Mechanism)
The Cosine Adapter works by learning a query-dependent mapping from raw cosine similarity scores to calibrated relevance scores. This addresses the fundamental issue that cosine similarity in high-dimensional embedding space does not directly correlate with relevance - two items with similar cosine scores may have vastly different actual relevance to a query. By learning this mapping from labeled relevance data, the system can better distinguish truly relevant items from those that merely have high similarity scores. The query-dependent nature allows the adapter to account for query-specific patterns in how similarity relates to relevance, while the global threshold provides a simple yet effective filtering mechanism that maintains high recall.

## Foundational Learning

1. **Embedding-based retrieval fundamentals**: Dense vector representations enable efficient similarity search but suffer from precision issues. Why needed: Understanding this limitation motivates the relevance filtering approach. Quick check: Can you explain why dense retrieval might retrieve semantically similar but topically irrelevant items?

2. **Cosine similarity limitations**: Raw cosine similarity scores don't directly correspond to relevance judgments. Why needed: This gap is the core problem the Cosine Adapter addresses. Quick check: Can you identify scenarios where high cosine similarity doesn't indicate high relevance?

3. **Query-dependent vs. global transformations**: The adapter learns query-specific mappings while using a universal threshold. Why needed: Balancing personalization with simplicity is key to the approach's effectiveness. Quick check: How would query-dependent thresholds differ from the global threshold approach?

4. **Two-stage retrieval architecture**: ANN search followed by relevance filtering. Why needed: This design enables both efficiency and precision improvements. Quick check: What are the computational implications of adding the filtering stage?

5. **Calibration techniques**: Transforming raw scores to calibrated relevance scores. Why needed: Calibration enables interpretable thresholding and better precision-recall trade-offs. Quick check: How does calibration differ from simple score scaling?

6. **Mean squared error for relevance training**: Using MSE loss to train the adapter on relevance judgments. Why needed: This loss function directly optimizes for accurate relevance score prediction. Quick check: What alternatives to MSE could be used for this training objective?

## Architecture Onboarding

**Component Map**: Dual Encoder -> Cosine Similarity -> Cosine Adapter -> Threshold Filter -> Retrieved Set

**Critical Path**: Query -> Dual Encoder -> Cosine Similarity Computation -> Cosine Adapter Mapping -> Threshold Application -> Final Results

**Design Tradeoffs**: The global threshold approach trades some potential precision gains from query-specific tuning for simplicity and robustness. The adapter adds minimal computational overhead (O(d²) + O(K)) compared to the base retrieval system, making it practical for large-scale deployment.

**Failure Signatures**: 
- Low precision despite high recall suggests threshold is too permissive
- Very low recall indicates threshold is too strict
- Inconsistent performance across query types may indicate the adapter isn't learning query-dependent patterns effectively
- Poor performance on certain product categories might indicate insufficient training data for those domains

**First 3 Experiments**:
1. Verify that raw cosine similarity scores don't correlate well with relevance judgments on your dataset
2. Train the Cosine Adapter on a small manually labeled dataset and evaluate calibration quality
3. Test different threshold values to find the optimal precision-recall trade-off for your specific use case

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Heavy reliance on proprietary Walmart data limits reproducibility and generalizability
- Global threshold approach may not be optimal for all query types or domains
- The claim about small number of relevant items in product search is asserted but not empirically validated across categories
- No comprehensive analysis of computational efficiency at scale

## Confidence

**High Confidence Claims**:
- The Cosine Adapter architecture is technically sound and mathematically correct
- PR AUC improvements on both MS MARCO and Walmart datasets are verifiable
- The two-stage retrieval approach is a valid engineering solution

**Medium Confidence Claims**:
- The approach "significantly increases precision" - practical significance depends on unmeasured business metrics
- The calibrated scores are "interpretable" - interpretability demonstrated through thresholding but not validated through user studies
- The global threshold approach is effective across diverse query types

**Low Confidence Claims**:
- Generalizability beyond the e-commerce domain tested
- Scalability claims without comprehensive performance benchmarks
- Superiority over all existing precision-improvement methods without broader comparative analysis

## Next Checks

1. **Cross-Domain Validation**: Test the Cosine Adapter approach on non-e-commerce datasets (e.g., academic paper search, news recommendation) to assess domain generalizability and identify any domain-specific limitations.

2. **Threshold Sensitivity Analysis**: Conduct a comprehensive study varying the global threshold across different query distributions and product categories to quantify the robustness of the "universal threshold" approach and identify edge cases.

3. **Comparative Analysis with Alternative Methods**: Implement and compare against alternative precision-improvement techniques such as query-specific re-ranking, dynamic thresholding, or probabilistic relevance models to establish the relative effectiveness of the Cosine Adapter approach.