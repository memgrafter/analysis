---
ver: rpa2
title: A Theory of Interpretable Approximations
arxiv_id: '2406.10529'
source_url: https://arxiv.org/abs/2406.10529
tags:
- depth
- interpretable
- decision
- tree
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces the concept of "interpretable approximations"
  to address the challenge of approximating complex machine learning models (like
  deep neural networks) using simpler, more interpretable models (like decision trees).
  The authors define three degrees of interpretability based on the depth of approximating
  decision trees: non-approximability, approximability without interpretability, and
  uniform interpretability at constant depth.'
---

# A Theory of Interpretable Approximations

## Quick Facts
- arXiv ID: 2406.10529
- Source URL: https://arxiv.org/abs/2406.10529
- Reference count: 40
- The paper establishes a trichotomy theorem showing that for any hypothesis class H and concept c, exactly one of three interpretability cases holds.

## Executive Summary
This paper introduces the concept of "interpretable approximations" in machine learning, addressing how complex models can be approximated using simpler, interpretable models like decision trees. The authors define three degrees of interpretability based on tree depth and prove a trichotomy theorem that precisely characterizes when concepts are approximable and interpretable. For VC classes, the theorem shows that if a concept is interpretable at any rate, it is uniformly interpretable at constant depth. The work provides algebraic characterizations of approximability and interpretability, and extends results to general complexity measures beyond tree depth.

## Method Summary
The paper uses theoretical techniques from boosting theory, VC theory, and measure theory to analyze interpretable approximations. The main approach involves defining closure operations on hypothesis classes and characterizing approximability and interpretability through algebraic structures. The authors prove a trichotomy theorem that shows for any concept and hypothesis class, exactly one of three cases holds: non-approximability, approximability without interpretability, or uniform interpretability at constant depth. The proofs rely on properties of VC dimension, closure under distributions, and subadditivity of complexity measures.

## Key Results
- For VC classes H, if c is interpretable at any rate, then it is uniformly interpretable at constant depth
- Algebraic characterizations show approximability is equivalent to membership in the closure of the σ-algebra generated by H
- For non-VC classes, interpretability collapses to uniform interpretability at logarithmic depth
- The results extend to a broader class of complexity measures beyond tree depth

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A concept is uniformly interpretable at constant depth if and only if it lies in the algebra generated by the closure of the hypothesis class.
- Mechanism: The algebra structure allows any concept in the closure to be represented as a finite Boolean combination of approximating hypotheses, so depth can be bounded by a constant independent of accuracy.
- Core assumption: The hypothesis class H is VC (finite VC dimension).
- Evidence anchors:
  - [abstract]: "For VC classes H, the theorem states that if c is interpretable at any rate, then it is uniformly interpretable at constant depth."
  - [section]: "Theorem 8 (Characterization of uniform interpretability for VC classes) Let X be any domain and let H be a VC hypothesis class over X. A concept c is uniformly interpretable (at a constant depth) if and only if c ∈ ⋃∞ d=1 clos(Algd(H))."
- Break condition: If H is not VC (infinite VC dimension), the characterization may not collapse to constant depth; logarithmic depth bounds still apply.

### Mechanism 2
- Claim: Approximability by H is equivalent to membership in the closure of the σ-algebra generated by H.
- Mechanism: Closure under distributions means any concept approximable to arbitrary accuracy under all distributions must be expressible as a limit of finite unions/intersections/complements of H elements.
- Core assumption: The domain may be uncountable; closure is defined via measure-theoretic limits.
- Evidence anchors:
  - [abstract]: "We show that, in the case of interpretable approximations, even a slightly nontrivial a-priori guarantee on the complexity of approximations implies approximations with constant (distribution-free and accuracy-free) complexity."
  - [section]: "Theorem 7 (Algebraic characterization of approximability) Let X be any domain and H any hypothesis class over X. A concept c ⊆ X is approximable by H if and only if c ∈ clos(σ (H))."
- Break condition: On countable domains, closure reduces to pointwise convergence, so the characterization becomes σ(H) rather than clos(σ(H)).

### Mechanism 3
- Claim: Graded complexity measures satisfying certain properties collapse interpretability to constant or logarithmic rates depending on VC dimension.
- Mechanism: The properties of the complexity measure ensure that depth bounds from boosting arguments translate directly into complexity bounds, with VC dimension controlling the rate of growth.
- Core assumption: Γ is a graded complexity measure with subadditivity over unions/intersections/complements.
- Evidence anchors:
  - [abstract]: "We extend our trichotomy to classes H of unbounded VC dimension and give characterizations of interpretability based on the algebra generated by H."
  - [section]: "Theorem 11 (Interpretability trichotomy for general representations) Let X be any domain and let Γ be any graded complexity measure. Then, for every concept c and every VC hypothesis class H over X exactly one of the following cases holds: (1) c is not approximable by H. (2) c is approximable by H but not interpretable by H. (3) c is uniformly interpretable by H at constant Γ-complexity rate."
- Break condition: If Γ is not graded (e.g., not subadditive), the trichotomy may not hold; interpretability could require unbounded complexity even for VC classes.

## Foundational Learning

- Concept: VC dimension and its role in uniform convergence.
  - Why needed here: VC dimension bounds the complexity of the hypothesis class, enabling uniform convergence results that guarantee constant-depth approximations.
  - Quick check question: Given a hypothesis class H with VC(X, H) = 3, can you bound the uniform convergence rate for any distribution?

- Concept: Closure under distributions and algebras generated by hypothesis classes.
  - Why needed here: Closure characterizes which concepts can be approximated arbitrarily well by H, and the algebra structure captures all finite Boolean combinations.
  - Quick check question: If H = {{1}, {2}, {3}} on X = {1,2,3}, list all elements of Alg(H).

- Concept: Boosting and weak learning assumptions.
  - Why needed here: The proof of constant-depth interpretability uses boosting arguments to combine weak learners into a strong one with bounded depth.
  - Quick check question: In a weak learning scenario with error 1/2 - γ, what is the maximum depth of the boosted decision tree after m rounds?

## Architecture Onboarding

- Component map: Hypothesis class H -> Closure operator clos(·) -> Algebra Alg(H) -> Complexity measure Γ -> Distribution P -> Concept c

- Critical path:
  1. Verify H is VC or not.
  2. Compute clos(σ(H)) or clos(Alg(H)) as needed.
  3. Check if c ∈ clos(σ(H)) (approximability).
  4. If approximable, check if c ∈ Alg(clos(H)) (uniform interpretability at constant depth for VC).
  5. Apply graded complexity measure results if Γ is provided.

- Design tradeoffs:
  - Using richer hypothesis classes H increases closure size but may increase computational cost.
  - Constant-depth interpretability is stronger but only guaranteed for VC classes.
  - Allowing non-VC classes yields logarithmic depth bounds but weaker guarantees.

- Failure signatures:
  - c not in clos(σ(H)) → non-approximability.
  - c in clos(σ(H)) but not in Alg(clos(H)) → approximable but not interpretable at constant depth.
  - Non-VC H with c in Alg(clos(H)) → only logarithmic depth interpretability.

- First 3 experiments:
  1. Implement closure computation for simple H (e.g., decision stumps) and test on synthetic concepts.
  2. Verify uniform interpretability at constant depth for VC H by constructing constant-depth trees.
  3. Apply graded complexity measure Γ and test interpretability bounds for both VC and non-VC H.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does interpretability collapse to uniform interpretability at constant depth for non-VC classes?
- Basis in paper: [explicit] Theorem 3 states case (3') for non-VC classes, but authors explicitly note they don't know if this collapses to case (3).
- Why unresolved: The proof techniques used for VC classes (uniform convergence, minimax theorem) may not directly extend to non-VC classes.
- What evidence would resolve it: A proof showing either that constant depth interpretability always holds for non-VC classes, or a counterexample demonstrating that logarithmic depth is unavoidable.

### Open Question 2
- Question: Can the O(1/ε^d) bound in Theorem 11 be improved to O(log(1/ε)) for more general graded complexity measures?
- Basis in paper: [explicit] Section 7 mentions that the O(1/ε^d) bound is due to the generality of Γ, and Appendix C discusses conditions under which O(log(1/ε)) can be recovered.
- Why unresolved: The general graded complexity measure definition allows for very broad behaviors that may not admit efficient interpretations.
- What evidence would resolve it: Either a proof that O(log(1/ε)) is achievable under weaker conditions than those stated in Appendix C, or a counterexample showing that some graded complexity measures inherently require polynomial depth.

### Open Question 3
- Question: Is there a single H-based decision tree that provides uniform interpretability guarantees for all accuracy levels simultaneously?
- Basis in paper: [explicit] The authors note in Section 5 that while Theorem 3 shows constant depth interpretability, this doesn't necessarily imply a single tree works for all ε > 0.
- Why unresolved: The proof constructs different trees for different accuracy levels, and the relationship between these trees is not explored.
- What evidence would resolve it: Either a construction of a single tree that works for all accuracy levels, or a proof that such a universal tree cannot exist for some hypothesis classes.

## Limitations
- The work is purely theoretical with no empirical validation provided
- Results depend heavily on specific technical assumptions about hypothesis classes and complexity measures
- Computational complexity of constructing interpretable approximations is not addressed

## Confidence
- High: The trichotomy theorem and its basic formulation for VC classes
- Medium: The algebraic characterizations and their proofs
- Low: Practical implications and computational feasibility of the theoretical results

## Next Checks
1. Implement the closure computation for simple hypothesis classes (e.g., decision stumps) and verify the algebraic characterizations on synthetic concepts
2. Construct explicit constant-depth decision trees for VC classes to empirically validate the uniform interpretability claim
3. Test the graded complexity measure results by applying different complexity measures to both VC and non-VC hypothesis classes and comparing interpretability bounds