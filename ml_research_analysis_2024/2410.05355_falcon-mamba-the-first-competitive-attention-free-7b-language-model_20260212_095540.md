---
ver: rpa2
title: 'Falcon Mamba: The First Competitive Attention-free 7B Language Model'
arxiv_id: '2410.05355'
source_url: https://arxiv.org/abs/2410.05355
tags:
- arxiv
- mamba
- data
- preprint
- falcon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Falcon Mamba 7B, a 7B parameter language model
  built on the Mamba state-space architecture that outperforms leading transformer-based
  models including Llama 3.1 8B, Mistral 7B, and Falcon2 11B on the Open LLM Leaderboard
  benchmarks. The model achieves an average score of 64.09 across various tasks, surpassing
  models with hybrid Mamba-Transformer designs.
---

# Falcon Mamba: The First Competitive Attention-free 7B Language Model

## Quick Facts
- **arXiv ID**: 2410.05355
- **Source URL**: https://arxiv.org/abs/2410.05355
- **Reference count**: 4
- **Primary result**: Pure Mamba 7B model outperforms transformer baselines on Open LLM Leaderboard benchmarks

## Executive Summary
This paper presents Falcon Mamba 7B, a 7B parameter language model built on the Mamba state-space architecture that outperforms leading transformer-based models including Llama 3.1 8B, Mistral 7B, and Falcon2 11B on the Open LLM Leaderboard benchmarks. The model achieves an average score of 64.09 across various tasks, surpassing models with hybrid Mamba-Transformer designs. A key innovation is demonstrating that a pure Mamba architecture can match or exceed transformer performance at scale, challenging the notion that hybrid designs are necessary for competitive results. The model offers significant advantages in long-context generation, maintaining constant memory and throughput regardless of sequence length.

## Method Summary
Falcon Mamba 7B uses a pure Mamba state-space architecture with 4096-dimensional hidden states and 64 layers. The training employed RMSNorm layers for stability, a high learning rate schedule, and a diverse pretraining corpus of 5.8 trillion tokens. The model achieves its performance through careful architectural design and training methodology, demonstrating that pure Mamba architectures can compete with transformer-based models at scale. The weights are publicly available under a permissive license.

## Key Results
- Achieves average score of 64.09 on Open LLM Leaderboard, outperforming Llama 3.1 8B, Mistral 7B, and Falcon2 11B
- Demonstrates constant memory and throughput regardless of sequence length for long-context generation
- First pure Mamba architecture to match or exceed transformer performance at the 7B parameter scale

## Why This Works (Mechanism)
The pure Mamba architecture achieves competitive performance through its state-space modeling approach, which provides efficient sequence processing with linear complexity. The model leverages 4096-dimensional hidden states and 64 layers to capture complex patterns in the data. The training methodology, including RMSNorm for stability and diverse pretraining data of 5.8 trillion tokens, enables the model to learn robust representations. The constant memory and throughput characteristics for long-context generation stem from the architecture's design, which avoids the quadratic complexity inherent in attention mechanisms.

## Foundational Learning
The model's training approach builds on established techniques for large language model development, including the use of diverse pretraining corpora and careful hyperparameter tuning. The 5.8 trillion token training corpus provides broad coverage of language patterns, enabling the model to develop general-purpose capabilities. The training methodology demonstrates that state-space models can effectively learn from large-scale data, achieving performance comparable to transformer-based approaches through different architectural mechanisms.

## Architecture Onboarding
The Mamba state-space architecture differs fundamentally from transformers by using selective state spaces rather than attention mechanisms. The 4096-dimensional hidden states and 64 layers create a deep network capable of capturing complex relationships. The RMSNorm layers throughout the architecture provide training stability, while the state-space design enables efficient processing of long sequences without the memory overhead of attention matrices. The architecture represents a departure from traditional transformer designs while maintaining competitive performance.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- Evaluation relies heavily on Open LLM Leaderboard rather than comprehensive task coverage
- Performance on specific domains like mathematical reasoning or code generation is not thoroughly characterized
- Lacks ablation studies to isolate impact of architectural choices versus training scale and data quality
- The paper does not provide detailed analysis of failure modes or limitations of the Mamba architecture

## Confidence
- High confidence in the claim that Falcon Mamba 7B outperforms specified transformer baselines on Open LLM Leaderboard metrics
- Medium confidence in the assertion that pure Mamba architectures can match hybrid designs at scale
- Medium confidence in the long-context generation advantages
- Low confidence in the characterization of model behavior in specialized domains

## Next Checks
1. Conduct targeted evaluations on mathematical reasoning and code generation tasks to assess domain-specific capabilities beyond the Open LLM Leaderboard
2. Perform architectural ablation studies varying hidden dimensions and layer counts to determine sensitivity and optimal configurations
3. Compare generation quality and coherence in long-context scenarios (10K+ tokens) against transformer baselines using human evaluation or specialized long-context benchmarks
4. Investigate the model's performance on multilingual tasks to understand its cross-lingual capabilities
5. Examine the model's behavior on edge cases and failure modes to better understand limitations of the Mamba architecture