---
ver: rpa2
title: Distinguishing the Knowable from the Unknowable with Language Models
arxiv_id: '2402.03563'
source_url: https://arxiv.org/abs/2402.03563
tags:
- llama
- small
- entropy
- linear
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the feasibility of distinguishing epistemic uncertainty
  from aleatoric uncertainty in the outputs of large language models over free-form
  text. Since ground-truth probabilities are unavailable, the authors propose using
  significantly larger models as proxies for the ground truth to approximately disentangle
  a given LLM's uncertainty.
---

# Distinguishing the Knowable from the Unknowable with Language Models

## Quick Facts
- arXiv ID: 2402.03563
- Source URL: https://arxiv.org/abs/2402.03563
- Reference count: 40
- Primary result: Linear probes trained on small model embeddings accurately predict when larger models will be more confident at the token level

## Executive Summary
This work studies whether large language models contain internal representations of epistemic uncertainty (lack of knowledge) versus aleatoric uncertainty (inherent randomness). Since ground-truth probabilities are unavailable, the authors use significantly larger models as proxies to disentangle a given LLM's uncertainty. They demonstrate that small linear probes trained on embeddings of frozen, pretrained models can accurately predict when larger models will be more confident at the token level. The method generalizes across text domains and includes a fully unsupervised approach inspired by in-context learning.

## Method Summary
The authors train linear probes on embeddings from smaller language models to predict when a larger model will have lower entropy on individual tokens. They use the larger model's entropy as a proxy for ground truth, training on Wikipedia data and evaluating on code, multilingual, and Q&A datasets. They also propose an unsupervised In-Context Learning Test (ICLT) method that measures how suggestible the model is to provided context, under the hypothesis that models with epistemic uncertainty will update their predictions more readily. The approach relies on filtering tokens based on entropy bands and balancing class distributions to avoid trivial baselines.

## Key Results
- Linear probes achieve AUC > 0.9 for distinguishing epistemic from aleatoric uncertainty
- Probes trained on Wikipedia generalize to code, multilingual, and Q&A-style evaluations
- Unsupervised ICLT method achieves non-trivial accuracy on the same task
- Results suggest LLMs naturally contain internal representations of different uncertainty types

## Why This Works (Mechanism)

### Mechanism 1
Small and large models share similar internal representations of uncertainty that linear probes can extract from embedding layers. The embeddings from the small model contain sufficient information to predict large model confidence without needing the large model's outputs.

### Mechanism 2
When models have epistemic uncertainty, they are more likely to update their predictions based on context, while aleatoric uncertainty leads to less change. The model's willingness to update based on context indicates the type of uncertainty it has.

### Mechanism 3
Uncertainty representations are domain-agnostic rather than relying on domain-specific heuristics. Internal representations of uncertainty are learned features rather than memorized correlations specific to training data.

## Foundational Learning

- **Concept**: Calibration and uncertainty types in language models
  - Why needed here: Understanding the difference between epistemic and aleatoric uncertainty is fundamental to the classification task
  - Quick check question: What's the key difference between epistemic and aleatoric uncertainty in the context of language model predictions?

- **Concept**: Linear probe training and evaluation
  - Why needed here: The core method uses linear probes trained on embeddings to predict uncertainty types
  - Quick check question: How do linear probes work as a method for extracting information from frozen model embeddings?

- **Concept**: In-context learning and its relationship to uncertainty
  - Why needed here: The unsupervised method relies on different in-context learning behaviors based on uncertainty type
  - Quick check question: How might a model's willingness to update based on context indicate the type of uncertainty it has?

## Architecture Onboarding

- **Component map**: Small model (S) -> Linear probe -> Large model (L) -> Evaluation
- **Critical path**: 1. Generate small and large model predictions on text corpus, 2. Label tokens based on entropy differences between models, 3. Train linear probes on small model embeddings to predict labels, 4. Evaluate probe performance on held-out data, 5. (Optional) Apply ICLT method for unsupervised classification
- **Design tradeoffs**: Using larger models as ground truth proxies vs. having true labels, Linear vs. non-linear probes (simplicity vs. potential accuracy), Supervised vs. unsupervised methods (data requirements vs. applicability), Token-level vs. sequence-level uncertainty classification
- **Failure signatures**: High correlation between small and large model entropies, Poor generalization to out-of-domain data, ICLT method failing on certain model families, Probes learning domain-specific heuristics rather than general uncertainty representations
- **First 3 experiments**: 1. Train and evaluate linear probe on Wikipedia data using 7B/65B LLaMA pairing, measure AUC, 2. Test same probe on code data from Pile to measure out-of-domain generalization, 3. Apply ICLT method on Wikipedia data and compare performance to SME baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of epistemic uncertainty detection scale with increasing size of the "large" model? The authors note that it is unclear how increasing the size of the "large" model affects the difficulty of the task and mention that testing with even larger models like ChatGPT would be valuable.

### Open Question 2
Can the In-Context Learning Test (ICLT) method be improved to work reliably on Pythia models? The authors observe that ICLT fails on Pythia models due to their tendency to repeat context verbatim, and they suspect this is related to differences in document separator tokens used during pre-training.

### Open Question 3
Can a synthetic dataset be created to generate gold-standard labels for epistemic and aleatoric uncertainty? The authors attempted to create such a dataset using Wikidata but were unsuccessful due to issues with prompt diversity and LLM behavior.

## Limitations
- Fundamental reliance on larger models as ground-truth proxies introduces uncertainty about whether we're measuring what we intend to measure
- Performance degrades when the model pair correlation is high, limiting applicability in certain model families
- Current method requires access to multiple models of different scales, which may not be available in all settings

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Linear probe results | High |
| ICLT method performance | Medium |
| Domain-agnostic uncertainty representations | Medium |

## Next Checks

1. **Cross-model-family validation**: Test the linear probe approach across diverse model families (e.g., LLaMA, Pythia, BLOOM) to verify that the uncertainty representations generalize beyond specific architectural choices.

2. **Out-of-distribution robustness**: Evaluate probe performance on deliberately challenging out-of-domain data (e.g., legal documents, medical text, code with heavy domain-specific terminology) to test the claimed domain-agnostic properties.

3. **Temporal validation**: Assess whether the uncertainty representations remain stable across model updates and training iterations of the same architecture, as this would impact long-term reliability in production settings.