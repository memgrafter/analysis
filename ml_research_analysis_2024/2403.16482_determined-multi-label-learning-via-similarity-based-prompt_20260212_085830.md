---
ver: rpa2
title: Determined Multi-Label Learning via Similarity-Based Prompt
arxiv_id: '2403.16482'
source_url: https://arxiv.org/abs/2403.16482
tags:
- multi-label
- learning
- label
- labels
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Determined Multi-Label Learning (DMLL), a novel
  weakly supervised multi-label learning setting where each instance is labeled with
  only one randomly selected class label, significantly reducing annotation costs.
  The authors derive a risk-consistent loss function for learning from these determined
  labels and introduce a similarity-based prompt learning method to enhance semantic
  information.
---

# Determined Multi-Label Learning via Similarity-Based Prompt

## Quick Facts
- arXiv ID: 2403.16482
- Source URL: https://arxiv.org/abs/2403.16482
- Reference count: 40
- Key outcome: Novel weakly supervised multi-label learning setting (DMLL) where each instance has only one randomly selected class label, achieving MAP improvements up to 5.9% over state-of-the-art methods

## Executive Summary
This paper introduces Determined Multi-Label Learning (DMLL), a novel weakly supervised setting where each instance is labeled with only one randomly selected class label, significantly reducing annotation costs. The authors theoretically derive a risk-consistent estimator to learn from these determined labels and propose a similarity-based prompt learning method that leverages large-scale vision-language models to enhance semantic information. Experiments on four standard multi-label benchmarks demonstrate that DMLL outperforms existing weakly supervised methods while requiring minimal supervision.

## Method Summary
The method consists of two key components: (1) a risk-consistent loss function that accurately recovers multi-label classification risk from single-determined-label annotations by decomposing the full label set and weighting contributions by inverse probability of observing each label, and (2) a similarity-based prompt learning method that enhances semantic information by finding semantically similar labels from a large label space using CLIP text encoder similarity and embedding them into prompt templates. The approach leverages pre-trained vision-language models (RAM and CLIP) to provide initial feature representations that are then refined through the determined label supervision and semantic prompt enrichment.

## Key Results
- Achieves MAP improvements up to 5.9% over existing state-of-the-art weakly supervised methods
- Demonstrates effectiveness across four standard multi-label benchmarks: VOC, COCO, NUS, and CUB
- Shows both risk-consistent loss and similarity-based prompt contribute to performance improvements through ablation study
- Successfully leverages large-scale vision-language models while requiring minimal annotation effort

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The risk-consistent estimator accurately recovers multi-label classification risk from single-determined-label annotations
- Mechanism: Risk is decomposed into contributions from instances where determined label is positive/negative, weighted by inverse probability of observing that label
- Core assumption: Determined label is sampled uniformly at random from full label set
- Evidence anchors: Theoretical derivation in section 3.3 with equations (2) through (7)
- Break condition: Non-uniform sampling of determined labels breaks the weighting scheme

### Mechanism 2
- Claim: Similarity-based prompt enhances semantic information by leveraging large label space
- Mechanism: Finds top-σ similar labels using CLIP text encoder similarity, embeds into prompts like "a photo of a {target}, similar to {similar1}, {similar2}, ..."
- Core assumption: CLIP text embeddings capture semantic relationships between labels
- Evidence anchors: Section 3.4 describes selection and embedding process
- Break condition: Poor CLIP embeddings or irrelevant labels in large space introduce noise

### Mechanism 3
- Claim: Combination of risk-consistent loss and similarity-based prompt enables effective learning from extreme weak supervision
- Mechanism: Risk-consistent loss corrects RAM model output using determined labels, similarity-based prompt provides semantic context
- Core assumption: RAM provides reasonable initial estimates that can be corrected and semantically enriched
- Evidence anchors: Section 4.3 ablation study shows both components contribute to improvements
- Break condition: Poor initial RAM estimates or misaligned semantic enrichment fails to recover multi-label information

## Foundational Learning

- Concept: Multi-label classification risk decomposition
  - Why needed here: Essential for deriving risk-consistent estimator that handles single-determined-label supervision
  - Quick check question: Can you explain why risk decomposition in equation (2) separates instances based on whether determined label is present or absent?

- Concept: Prompt engineering with vision-language models
  - Why needed here: Similarity-based prompt relies on CLIP's text encoder for finding semantically similar labels
  - Quick check question: How does similarity calculation in equation (8) use CLIP's text encoder to find similar labels?

- Concept: Weak supervision learning theory
  - Why needed here: Paper extends theory of learning from incomplete/noisy labels to determined multi-label setting
  - Quick check question: What makes determined multi-label learning more extreme than single-positive multi-label learning?

## Architecture Onboarding

- Component map: RAM image encoder -> CLIP text encoder -> Similarity calculation -> Prompt generation -> Risk-consistent loss calculation -> Multi-label classifier
- Critical path: Image → RAM encoder → Text → CLIP encoder → Similarity calculation → Prompt generation → Risk-consistent loss calculation → Model update
- Design tradeoffs: Trades annotation cost for model complexity, requiring sophisticated prompt learning and risk-consistent estimation rather than simple label assignment
- Failure signatures: Poor performance on rare labels, degradation when determined label sampling is non-uniform, instability in prompt optimization, struggles when CLIP embeddings don't capture label semantics
- First 3 experiments:
  1. Run risk-consistent loss component alone on small dataset to verify reasonable label probability recovery
  2. Test SBP component alone with fixed prompts to ensure semantic similarity finding works
  3. Combine both components on validation set to verify integration before full training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DMLL perform on datasets with extremely large label spaces beyond tested benchmarks?
- Basis in paper: Tested on VOC (20 classes), COCO (80 classes), NUS (81 classes), and CUB (312 classes), but scalability to larger spaces unexplored
- Why unresolved: No theoretical analysis or empirical evidence of performance characteristics as label space grows
- What evidence would resolve it: Experiments on datasets with thousands/tens of thousands of labels or theoretical analysis of computational complexity

### Open Question 2
- Question: Impact of similarity-based prompt on models without access to large-scale vision-language models?
- Basis in paper: Similarity-based prompt relies on RAM and CLIP features, but effectiveness without these models unexplored
- Why unresolved: Focus on scenarios with large-scale vision-language models available, not addressing method's effectiveness without them
- What evidence would resolve it: Comparisons with/without similarity-based prompt using smaller models or training from scratch

### Open Question 3
- Question: How does risk-consistent loss perform under different types of label noise in determined labels?
- Basis in paper: Assumes accurate determined labels, but robustness to noise unexplored
- Why unresolved: Theoretical derivation assumes perfect determined labels, real-world scenarios may involve noisy labels
- What evidence would resolve it: Experiments with various levels/types of label noise and measuring impact on DMLL performance

## Limitations

- The risk-consistent estimator relies heavily on uniform sampling assumption for determined labels - any systematic bias in label selection breaks theoretical guarantees
- The similarity-based prompt method depends on CLIP text embeddings capturing true semantic relationships, which may not hold across all label domains
- Evaluation focuses on MAP improvements without uncertainty estimates or statistical significance testing, making it difficult to assess reliability of reported gains
- Paper doesn't address computational costs, which could be substantial given need to compute similarities between all label pairs

## Confidence

**High confidence**: Theoretical derivation of risk-consistent estimator is mathematically sound given uniform sampling assumption; experimental methodology and dataset choices are appropriate

**Medium confidence**: Effectiveness of similarity-based prompt depends on CLIP's semantic understanding which varies by domain; reported performance improvements are substantial but lack statistical validation

**Low confidence**: Paper doesn't adequately address performance on datasets with rare/imbalanced labels or when determined label sampling deviates from uniformity in practice

## Next Checks

1. **Sensitivity analysis to label sampling bias**: Test method when determined labels are sampled with slight non-uniform probabilities (e.g., 60/40 instead of 50/50) to verify robustness to deviations from theoretical assumption

2. **Ablation on label space size**: Evaluate performance using different sizes of label spaces for similarity calculation (e.g., 500 vs 4585 labels) to quantify impact of large label space assumption and computational overhead

3. **Statistical significance testing**: Re-run experiments with multiple random seeds and apply paired statistical tests (e.g., Wilcoxon signed-rank) to determine if MAP improvements over baselines are statistically significant rather than due to random variation