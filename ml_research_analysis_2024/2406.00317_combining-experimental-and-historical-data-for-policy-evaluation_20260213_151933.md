---
ver: rpa2
title: Combining Experimental and Historical Data for Policy Evaluation
arxiv_id: '2406.00317'
source_url: https://arxiv.org/abs/2406.00317
tags:
- data
- estimator
- policy
- historical
- experimental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating experimental
  and historical data for policy evaluation, particularly when the historical data
  comes from a single control arm. The authors propose weighted estimators that linearly
  combine base estimators from experimental and historical data, with weights optimized
  to minimize mean squared error (MSE).
---

# Combining Experimental and Historical Data for Policy Evaluation

## Quick Facts
- arXiv ID: 2406.00317
- Source URL: https://arxiv.org/abs/2406.00317
- Reference count: 40
- Authors: Ting Li; Chengchun Shi; Qianglin Wen; Yang Sui; Yongli Qin; Chunbo Lai; Hongtu Zhu

## Executive Summary
This paper addresses the challenge of integrating experimental and historical data for policy evaluation, particularly when historical data comes from a single control arm. The authors propose weighted estimators that linearly combine base estimators from experimental and historical data, with weights optimized to minimize mean squared error (MSE). They develop both non-pessimistic and pessimistic approaches, with the pessimistic estimator incorporating uncertainty to improve robustness. Theoretical analysis establishes non-asymptotic error bounds and proves oracle, efficiency, and robustness properties across various reward shift scenarios. Numerical experiments on ridesharing data demonstrate superior performance compared to existing methods.

## Method Summary
The paper proposes novel data integration methods that linearly combine base policy value estimators constructed from experimental and historical data. The key innovation is optimizing weights to minimize mean square error (MSE) while accounting for potential reward shifts between datasets. Two estimators are developed: a non-pessimistic estimator that directly minimizes estimated MSE, and a pessimistic estimator that incorporates uncertainty quantification to handle cases with moderately large reward shifts. The method relies on doubly robust estimators for both datasets and uses sample splitting to avoid overfitting. Theoretical guarantees include non-asymptotic error bounds and oracle properties across different reward shift scenarios.

## Key Results
- Proposed estimators achieve lower MSE compared to existing methods across various reward shift scenarios
- Pessimistic estimator shows greater robustness for moderate reward shifts where non-pessimistic estimator underperforms
- Theoretical analysis establishes non-asymptotic error bounds and proves oracle, efficiency, and robustness properties
- Numerical experiments on ridesharing data demonstrate practical effectiveness of the approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed estimators linearly combine base estimators from experimental and historical data, with weights optimized to minimize mean squared error (MSE).
- Mechanism: By using weighted combinations of doubly robust estimators from both datasets, the method balances bias and variance. The non-pessimistic estimator chooses weights to minimize estimated MSE, while the pessimistic estimator accounts for uncertainty in reward shift estimation.
- Core assumption: The doubly robust property holds for both base estimators, meaning either the reward function or propensity score (or density ratio) is correctly specified.
- Evidence anchors:
  - [abstract]: "We propose novel data integration methods that linearly integrate base policy value estimators constructed based on the experimental and historical data, with weights optimized to minimize the mean square error (MSE)"
  - [section 3]: "Both the proposed non-pessimistic and pessimistic estimators are formulated as linear combinations of the two base estimatorsbτe andbτh"
  - [corpus]: Weak evidence - corpus focuses on general OPE methods without specific discussion of linear combination approaches
- Break condition: If both reward functions and propensity scores are misspecified, the doubly robust property fails and the estimators become biased.

### Mechanism 2
- Claim: The pessimistic estimator achieves greater robustness, particularly in situations with moderately large reward shifts.
- Mechanism: Instead of selecting weights based solely on estimated MSE, the pessimistic estimator incorporates uncertainty quantification. It uses (|bbh| + U)² as a conservative estimate of the squared reward shift, where U represents high-confidence bounds on the estimation error.
- Core assumption: The uncertainty quantifier U can be constructed such that P(|bbh − bh| ≤ U) ≤ 1 − α for a given significance level α.
- Evidence anchors:
  - [section 3]: "The pessimistic estimator addresses this limitation by incorporating the uncertainty of cost estimation. Instead of selecting the greedy arm with the lowest estimated cost, it selects the arm based on a more pessimistic cost estimate"
  - [section 5.2]: "The pessimistic estimator effectively mitigates the aforementioned limitation of the non-pessimistic estimator by incorporating the estimation error ofbbh into weight selection"
  - [corpus]: Weak evidence - corpus contains general OPE methods but lacks specific discussion of pessimistic principles in weighted estimator contexts
- Break condition: If the uncertainty quantification U is poorly estimated (e.g., too small), the pessimistic protection fails and the estimator may underperform.

### Mechanism 3
- Claim: The proposed methods achieve oracle properties across different reward shift scenarios, with theoretical guarantees for small, moderate, and large shifts.
- Mechanism: Through non-asymptotic error bounds and analysis of different regimes, the paper establishes conditions under which each estimator performs optimally. For small shifts, the method approaches efficiency bounds; for large shifts, it recovers oracle performance.
- Core assumption: The distributional shifts between datasets can be characterized by the mean reward shift parameter bh, and the variance/covariance terms can be consistently estimated.
- Evidence anchors:
  - [abstract]: "Theoretically, we establish non-asymptotic error bounds for the MSEs of our proposed estimators, and derive their oracle, efficiency and robustness properties across a broad spectrum of reward shift scenarios"
  - [section 5.1]: "The non-pessimistic estimator tends to be effective in scenarios where the reward shift is minimal or substantial. In contrast, the pessimistic estimator demonstrates greater robustness, particularly in situations with moderately large reward shifts"
  - [corpus]: Weak evidence - corpus lacks theoretical analysis of oracle properties in multi-source data integration
- Break condition: If the reward shift bh falls into the "moderate" regime but the uncertainty quantification is inadequate, the estimator may suffer from suboptimal performance.

## Foundational Learning

- Concept: Doubly robust estimation
  - Why needed here: The base estimators rely on doubly robust properties to ensure unbiasedness when either the reward function or propensity score is correctly specified
  - Quick check question: If both the reward function and propensity score are misspecified, will the doubly robust estimator still be unbiased?

- Concept: Importance sampling and density ratio estimation
  - Why needed here: The method requires estimating density ratios between experimental and historical data distributions to correct for covariate shifts
  - Quick check question: What happens to the estimator if the estimated density ratio is severely biased?

- Concept: Sample splitting and cross-fitting
  - Why needed here: The theoretical analysis uses sample splitting to avoid overfitting when selecting weights based on the same data used for estimation
  - Quick check question: How would the theoretical guarantees change if we didn't use sample splitting?

## Architecture Onboarding

- Component map: Data preprocessing -> Base estimator construction -> Weight optimization -> Uncertainty quantification -> Final estimator -> Inference
- Critical path: Data → Base Estimators → Weight Optimization → Final Estimator → Inference
- Design tradeoffs:
  - Non-pessimistic vs pessimistic: Speed vs robustness
  - Sample splitting vs full data: Variance vs bias
  - Linear combination vs non-linear: Simplicity vs flexibility
- Failure signatures:
  - Large bias in final estimator: Likely both reward functions and propensity scores are misspecified
  - Unstable weights: Variance estimation may be unreliable or sample size too small
  - Confidence intervals with poor coverage: Uncertainty quantification may be inadequate
- First 3 experiments:
  1. Test with known reward shift (synthetic data) to verify oracle properties in different regimes
  2. Vary sample sizes to assess performance degradation and identify minimum viable sample sizes
  3. Introduce misspecification in either reward function or propensity score to test doubly robust property

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed estimators change when the reward shift is exactly zero versus very close to zero?
- Basis in paper: [explicit] The authors discuss scenarios with zero, small, moderate, and large reward shifts, noting that the SPE estimator performs best when the reward shift is zero.
- Why unresolved: The paper does not provide a precise characterization of the transition point where the performance of the estimators changes as the reward shift approaches zero.
- What evidence would resolve it: Empirical studies showing the performance of the estimators as the reward shift parameter varies continuously from zero to small values.

### Open Question 2
- Question: What is the impact of the choice of tuning parameter in the Lasso method on its performance, and how does it compare to the proposed methods?
- Basis in paper: [explicit] The authors mention that the Lasso method is sensitive to the choice of the tuning parameter and provide numerical results showing this sensitivity.
- Why unresolved: The paper does not provide a detailed analysis of how different tuning parameters affect the Lasso method's performance or a comparison with the proposed methods under varying tuning parameters.
- What evidence would resolve it: A comprehensive sensitivity analysis of the Lasso method's performance across a wide range of tuning parameters and a direct comparison with the proposed methods.

### Open Question 3
- Question: How does the hybrid procedure perform in practice when prior knowledge about the reward shift is unavailable?
- Basis in paper: [explicit] The authors propose a hybrid procedure that chooses different methods based on the magnitude of the bias but note that estimating the regime to which the data belongs introduces additional variability.
- Why unresolved: The paper does not provide empirical evidence on the performance of the hybrid procedure when the reward shift is unknown and must be estimated.
- What evidence would resolve it: Experimental results comparing the hybrid procedure with other methods when the reward shift is estimated rather than known, along with an analysis of the estimation error's impact on performance.

## Limitations
- Theoretical guarantees rely heavily on doubly robust property, which may fail if both reward functions and propensity scores are misspecified
- Effectiveness of pessimistic estimator depends critically on accurate uncertainty quantification, which may be challenging with limited sample sizes
- The "moderate" reward shift regime remains less well-characterized, and estimator performance in this region depends on uncertainty estimation quality

## Confidence

- High confidence: The mechanism of linearly combining base estimators with MSE-optimized weights is well-established and theoretically sound
- Medium confidence: The pessimistic approach's robustness claims are supported by both theory and experiments, but the uncertainty quantification assumptions require further validation
- Medium confidence: The oracle properties across different reward shift regimes are theoretically proven, but the practical implications depend on accurate estimation of distributional shifts

## Next Checks

1. Test the estimators under severe misspecification where both reward functions and propensity scores are wrong to assess robustness limits
2. Conduct sensitivity analysis on the uncertainty quantification parameter U to determine its impact on pessimistic estimator performance
3. Validate the theoretical bounds empirically across different reward shift magnitudes using synthetic data with known ground truth