---
ver: rpa2
title: Behaviour Distillation
arxiv_id: '2406.15042'
source_url: https://arxiv.org/abs/2406.15042
tags:
- dataset
- distillation
- learning
- policy
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces behaviour distillation, a new setting for
  distilling expert policies into synthetic datasets without access to expert data.
  The proposed method, HaDES, uses an evolutionary outer loop to optimize synthetic
  datasets, combined with supervised learning in the inner loop to train policies.
---

# Behaviour Distillation

## Quick Facts
- arXiv ID: 2406.15042
- Source URL: https://arxiv.org/abs/2406.15042
- Reference count: 40
- One-line primary result: HaDES discovers small synthetic datasets (as few as 4 state-action pairs) that train policies to competitive performance in continuous control tasks without expert data.

## Executive Summary
This paper introduces behaviour distillation, a novel setting for distilling expert policies into synthetic datasets without access to expert data. The proposed method, HaDES, uses evolutionary strategies to optimize synthetic datasets, which are then used to train policies through supervised learning. HaDES can discover extremely compact datasets that, when used for supervised learning, train agents to competitive performance levels in continuous control tasks. The method demonstrates strong generalization across different policy architectures and hyperparameters, achieving state-of-the-art results on standard dataset distillation benchmarks.

## Method Summary
HaDES employs a bi-level optimization framework where evolutionary strategies (ES) optimize synthetic datasets in the outer loop, and supervised learning trains policies on these datasets in the inner loop. The method represents expert behavior as a fixed-size set of state-action pairs, optimized through ES based on the performance of policies trained on the synthetic data. Two variants are proposed: HaDES-F uses fixed policy initialization, while HaDES-R uses randomized initialization to improve generalization. The approach achieves behavior distillation by iteratively refining synthetic datasets to capture essential expert behavior patterns that can be effectively learned through supervised training.

## Key Results
- HaDES discovers datasets as small as 4 state-action pairs that train policies to competitive performance in continuous control tasks
- The method generalizes to train policies with various architectures and hyperparameters different from those used during dataset optimization
- HaDES achieves state-of-the-art results on standard supervised dataset distillation tasks while providing significant improvements in neuroevolution for RL
- Visualizations of synthetic datasets provide human-interpretable insights into task requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HaDES uses evolutionary strategies to optimize synthetic datasets without requiring meta-gradients, enabling scalable behavior distillation.
- Mechanism: The method employs a bi-level optimization structure where evolutionary strategies (ES) update the synthetic dataset in the outer loop, and supervised learning (behavior cloning) updates the policy in the inner loop. The fitness function for ES is the performance of the policy trained on the synthetic dataset.
- Core assumption: The synthetic dataset can be represented as a fixed-size set of state-action pairs that, when used for supervised learning, can train policies to competitive performance levels.
- Evidence anchors:
  - [abstract] "HaDES, a method for behaviour distillation that can discover datasets of just four state-action pairs which, under supervised learning, train agents to competitive performance levels"
  - [section] "HaDES optimizes the inner loop objective to obtain θ* using gradient descent, and optimizes the outer loop objective (the return of πθ*) using ES"

### Mechanism 2
- Claim: HaDES-F (fixed initialization) and HaDES-R (randomized initialization) variants provide different inductive biases for dataset optimization.
- Mechanism: HaDES-F uses a single fixed policy initialization, optimizing the dataset specifically for that initialization. HaDES-R uses multiple random initializations, optimizing the dataset to generalize across different starting points.
- Core assumption: The choice of policy initialization strategy affects how well the synthetic dataset generalizes to different architectures and hyperparameters.
- Evidence anchors:
  - [section] "The first variant is HaDES with fixed policy initialization, or HaDES-F... The second variant is HaDES with randomized policy initialization, or HaDES-R"
  - [section] "HaDES-F has a stronger inductive bias... HaDES-R has a weaker inductive bias"

### Mechanism 3
- Claim: The synthetic datasets generated by HaDES can generalize to train policies with vastly different architectures and hyperparameters than those used during dataset optimization.
- Mechanism: The evolutionary optimization process discovers compact representations of expert behavior that are robust to changes in policy architecture, learning rates, and training epochs.
- Core assumption: The synthetic dataset captures fundamental behavior patterns rather than memorization of specific policy characteristics.
- Evidence anchors:
  - [section] "We show that these datasets generalize out of distribution to training policies with a wide range of architectures and hyperparameters"
  - [section] "HaDES-R dataset was trained with randomized (π1_0, ..., πk_0)i and generalizes much better across all architectures and training parameters"

## Foundational Learning

- Concept: Reinforcement Learning Basics (MDPs, policies, reward maximization)
  - Why needed here: Understanding the core RL problem is essential to grasp why behavior distillation is challenging and valuable
  - Quick check question: What is the difference between a policy and a value function in RL?

- Concept: Evolution Strategies and Bi-level Optimization
  - Why needed here: HaDES relies on ES for outer loop optimization and supervised learning for inner loop training
  - Quick check question: How does evolution strategies differ from traditional gradient-based RL methods?

- Concept: Dataset Distillation in Supervised Learning
  - Why needed here: Behavior distillation extends the concept of dataset distillation from supervised to reinforcement learning
- Quick check question: What is the main goal of dataset distillation in supervised learning?

## Architecture Onboarding

- Component map:
  Environment interface (Brax/MinAtar) -> Synthetic dataset representation (fixed-size state-action pairs) -> Evolution strategy optimizer (ES) -> Supervised learning trainer (behavior cloning) -> Policy evaluation module -> Multi-task agent trainer

- Critical path: ES optimization → Synthetic dataset generation → Supervised training → Policy evaluation → Fitness computation → ES update

- Design tradeoffs:
  - Dataset size vs. expressivity: Smaller datasets are more interpretable but may lack information
  - HaDES-F vs. HaDES-R: Fixed initialization may achieve higher performance but less generalization
  - Population size in ES: Larger populations provide better exploration but increase computational cost

- Failure signatures:
  - Poor policy performance despite successful ES optimization: May indicate dataset size is too small or representation is insufficient
  - High variance in policy returns: Could suggest insufficient population size or poor fitness signal
  - Inability to generalize to new architectures: May require switching from HaDES-F to HaDES-R

- First 3 experiments:
  1. Run HaDES on a simple continuous control environment (e.g., Cartpole) with dataset size 4 to verify basic functionality
  2. Compare HaDES-F vs HaDES-R on the same environment to observe generalization differences
  3. Test dataset transfer by training policies with different architectures using a distilled dataset from experiment 1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between dataset size and policy performance in behavior distillation?
- Basis in paper: [explicit] The paper discusses the impact of dataset size on policy performance in Figure 6, showing that both too small and too large dataset sizes can degrade performance.
- Why unresolved: The paper does not provide a definitive answer on how to determine the optimal dataset size for different environments and tasks. It only shows that there is a trade-off.
- What evidence would resolve it: Systematic experiments across a wider range of environments and tasks, testing various dataset sizes to find the optimal size that balances performance and computational efficiency.

### Open Question 2
- Question: How does the choice of inner loop parameters (e.g., learning rate, number of epochs) affect the quality of the distilled dataset?
- Basis in paper: [explicit] The paper mentions that both the outer loop ES parameters and inner loop supervised learning parameters need to be tuned, and that anecdotal evidence suggests ES can adapt to inner loop parameters.
- Why unresolved: The paper does not provide a detailed analysis of how specific inner loop parameter choices impact the final distilled dataset quality.
- What evidence would resolve it: Ablation studies varying inner loop parameters while keeping outer loop parameters fixed, and measuring the impact on final policy performance and dataset generalization.

### Open Question 3
- Question: Can behavior distillation be extended to more complex environments, such as those with pixel-based observations or long-horizon tasks?
- Basis in paper: [inferred] The paper mentions that the number of parameters in HaDES grows linearly with the number of datapoints, especially in pixel-based environments, and that scaling to CIFAR-10 was not possible.
- Why unresolved: The paper only demonstrates behavior distillation on relatively simple continuous control and discrete action tasks, and does not explore more complex environments.
- What evidence would resolve it: Successful application of HaDES to pixel-based environments like Atari games or long-horizon tasks like Montezuma's Revenge, showing that the method can scale to more complex settings.

## Limitations
- Computational expense of evolutionary optimization, particularly for HaDES-R with multiple initializations
- Performance sensitivity to dataset size and representation choices, with no theoretical guarantees for optimality
- Generalization limits when policy architectures deviate significantly from those used during optimization
- Potential information loss when using extremely compact synthetic datasets for complex behaviors

## Confidence

- High confidence in the basic mechanism of bi-level optimization with ES and supervised learning
- Medium confidence in the generalization claims across architectures, as the tested range may not cover all possible variations
- Medium confidence in the interpretability claims, as human interpretation of small datasets may vary

## Next Checks

1. Test HaDES on more diverse environment types (e.g., sparse reward settings, partially observable environments) to assess robustness beyond standard continuous control tasks.

2. Conduct ablation studies on ES hyperparameters (population size, mutation rate) to determine their impact on both performance and computational efficiency.

3. Evaluate the transferability of distilled datasets across fundamentally different state representations (e.g., raw pixels vs. state vectors) to test the limits of dataset generalization.