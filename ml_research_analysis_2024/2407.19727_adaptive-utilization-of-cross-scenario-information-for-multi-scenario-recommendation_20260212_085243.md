---
ver: rpa2
title: Adaptive Utilization of Cross-scenario Information for Multi-scenario Recommendation
arxiv_id: '2407.19727'
source_url: https://arxiv.org/abs/2407.19727
tags:
- scenarios
- scenario
- information
- data
- csii
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of improving ranking performance
  in multi-scenario recommendation systems, where recommender systems serve multiple
  business scenarios with skewed and sparse data distributions. The proposed method,
  Cross-Scenario Information Interaction (CSII), introduces two key modules: Transferable
  Feature Extraction (TFE) to select highly transferable features across scenarios
  and avoid negative transfer, and Cross-Scenario Aggregation (CSA) to adaptively
  extract relevant knowledge from cross-scenario representations using multi-level
  attention mechanisms.'
---

# Adaptive Utilization of Cross-scenario Information for Multi-scenario Recommendation

## Quick Facts
- arXiv ID: 2407.19727
- Source URL: https://arxiv.org/abs/2407.19727
- Reference count: 20
- Key outcome: Achieves 1.0% average GMV improvement across all scenarios with significant offline AUC gains

## Executive Summary
This paper addresses the challenge of improving ranking performance in multi-scenario recommendation systems where recommender systems serve multiple business scenarios with skewed and sparse data distributions. The proposed Cross-Scenario Information Interaction (CSII) model introduces two key modules: Transferable Feature Extraction (TFE) to select highly transferable features across scenarios and avoid negative transfer, and Cross-Scenario Aggregation (CSA) to adaptively extract relevant knowledge from cross-scenario representations using multi-level attention mechanisms. Experiments on a large-scale production dataset from Meituan Waimai demonstrate that CSII achieves significant improvements in both offline and online settings, effectively handling data skew and sparsity, particularly in minor scenarios.

## Method Summary
The CSII model employs a scenario-dominated experts paradigm with one expert per scenario plus a shared expert to leverage multi-scenario information. The Transferable Feature Extraction (TFE) module measures feature transferability across scenarios using distance-based scoring to select features that contribute positively to cross-scenario knowledge transfer while avoiding negative transfer. The Cross-Scenario Aggregation (CSA) module uses multi-level attention mechanisms, including intra-scenario attention aggregators that capture complex interactions within each scenario and inter-scenario attention aggregators that adaptively extract relevant knowledge from cross-scenario representations. The model is trained using Adam optimizer with learning rate 0.001 and standard cross-entropy loss, mixing samples from different scenarios during training.

## Key Results
- Achieves 1.0% average improvement in Gross Merchandise Value (GMV) across all scenarios in online A/B testing
- Shows significant offline AUC improvements for both CTR and CTCVR prediction tasks across all 7 business scenarios
- Particularly effective in minor scenarios with less than 1% data share, demonstrating superior handling of data sparsity and skew

## Why This Works (Mechanism)
The CSII model works by intelligently managing the trade-off between scenario-specific learning and cross-scenario knowledge transfer. The TFE module ensures that only features with high transferability across scenarios are used for cross-scenario aggregation, preventing negative transfer that could harm model performance. The CSA module then uses attention mechanisms to dynamically weight the importance of cross-scenario information based on the current scenario's needs, allowing the model to adaptively extract relevant knowledge while preserving scenario-specific characteristics. This selective and adaptive approach to cross-scenario information utilization enables the model to effectively leverage the rich information available across scenarios while maintaining strong performance in individual scenarios, especially those with limited data.

## Foundational Learning
- **Multi-gate Mixture-of-Experts (MMoE)**: Why needed - to handle multiple tasks with shared and task-specific parameters; Quick check - verify gating network properly weights expert outputs
- **Feature transferability measurement**: Why needed - to identify which features can safely transfer knowledge across scenarios; Quick check - validate transferability scores correlate with cross-scenario performance gains
- **Attention-based aggregation**: Why needed - to dynamically weight cross-scenario information relevance; Quick check - inspect attention score distributions for meaningful patterns
- **Scenario-dominated experts**: Why needed - to balance scenario-specific learning with cross-scenario knowledge sharing; Quick check - compare performance with shared-expert-only baseline
- **Multi-task learning framework**: Why needed - to jointly optimize multiple objectives (CTR/CTCVR) across scenarios; Quick check - verify task-specific losses contribute to overall optimization

## Architecture Onboarding

**Component Map:**
Input Features -> TFE Module -> Scenario Experts (7 dominated + 1 shared) -> CSA Module -> Output Heads

**Critical Path:**
Feature extraction → Transferability scoring → Expert processing → Cross-scenario aggregation → Prediction

**Design Tradeoffs:**
- Number of experts per scenario: One per scenario provides clear separation but may limit flexibility for highly sparse scenarios
- Distance metric in TFE: Euclidean distance is simple but cosine similarity might better capture feature relationships
- Attention mechanism depth: Deeper attention allows more complex interactions but increases computational cost

**Failure Signatures:**
- Poor performance in minor scenarios: Indicates negative transfer or insufficient cross-scenario knowledge extraction
- Inconsistent attention patterns: Suggests improper calibration of inter-scenario relevance weighting
- Feature transferability scores near zero: May indicate poor feature engineering or inappropriate distance metric

**First Experiments:**
1. Train baseline MTL model without cross-scenario aggregation to establish performance floor
2. Implement TFE module alone with single scenario to verify feature selection effectiveness
3. Test CSA module with synthetic cross-scenario data to validate attention mechanism behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Transferable Feature Extraction (TFE) module's performance vary when using different distance metrics (e.g., cosine similarity vs. Euclidean distance) in equation 1?
- Basis in paper: [explicit] The paper mentions that "some other metrics like cosine similarity can also be considered" but only evaluates Euclidean distance.
- Why unresolved: The paper does not conduct ablation studies comparing different distance metrics, leaving uncertainty about which metric is optimal for feature transferability measurement.
- What evidence would resolve it: Comparative experiments showing CTR/CTCVR performance using different distance metrics (Euclidean, cosine, etc.) across multiple scenarios and datasets.

### Open Question 2
- Question: What is the impact of varying the number of experts in the scenario-dominated paradigm on model performance, particularly for scenarios with extreme data sparsity?
- Basis in paper: [inferred] The paper uses one expert per scenario but mentions that "when the amount of data among scenarios is skewed or the data in some scenarios is extremely sparse, it is difficult to learn scenario-specific parameters well."
- Why unresolved: The paper only reports results using one expert per scenario without exploring how performance changes with different numbers of experts, especially for minor scenarios with <1% data share.
- What evidence would resolve it: Experiments varying the number of experts per scenario (e.g., 1, 3, 5 experts) and measuring performance impact on both major and minor scenarios.

### Open Question 3
- Question: How does the Cross-Scenario Aggregation (CSA) module handle scenarios with completely disjoint user/item sets, and what is its performance degradation in such cases?
- Basis in paper: [explicit] The paper states "If users and items are fully overlapped in various scenarios" as a condition for cross-scenario knowledge transfer, implying uncertainty about non-overlapping scenarios.
- Why unresolved: The paper does not evaluate scenarios where user/item overlap is minimal or absent, leaving questions about the method's robustness to such cases.
- What evidence would resolve it: Experiments on datasets with varying degrees of user/item overlap between scenarios, measuring performance degradation as overlap decreases.

### Open Question 4
- Question: What is the computational overhead of the CSII model compared to simpler baselines like MTL Base during both training and inference, particularly at scale?
- Basis in paper: [inferred] While the paper mentions "massive size of our system" and online deployment, it does not provide quantitative comparisons of computational efficiency between CSII and simpler models.
- Why unresolved: The paper focuses on accuracy improvements but does not discuss the trade-off between performance gains and increased computational costs, which is critical for production systems.
- What evidence would resolve it: Benchmarking results showing training time, inference latency, and memory usage of CSII compared to baselines like MTL Base and MMoE under production-scale workloads.

## Limitations
- Experiments conducted on proprietary production dataset limiting external validation and reproducibility
- Limited baseline comparisons may miss other relevant approaches in multi-scenario recommendation literature
- Online evaluation results lack statistical significance testing and confidence intervals for claimed improvements

## Confidence
- High confidence: Technical feasibility of CSII architecture building on established components
- Medium confidence: Effectiveness of feature transferability selection mechanism (implementation details partially unspecified)
- Medium confidence: Reported performance improvements (based on single proprietary dataset without independent validation)

## Next Checks
1. Conduct ablation studies to quantify individual contributions of TFE and CSA modules to overall performance
2. Implement model on publicly available multi-scenario recommendation datasets or create synthetic datasets with controlled skew and sparsity
3. Perform statistical significance testing on both offline AUC metrics and online GMV improvements with confidence intervals