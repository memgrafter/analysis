---
ver: rpa2
title: Compatible Gradient Approximations for Actor-Critic Algorithms
arxiv_id: '2409.01477'
source_url: https://arxiv.org/abs/2409.01477
tags:
- gradient
- policy
- learning
- reward
- approximation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Compatible Policy Gradient (CPG), a model-free
  actor-critic algorithm that addresses compatibility issues in deterministic policy
  gradients by using a zeroth-order approximation of action-value gradients through
  two-point stochastic gradient estimation in action space. This approach eliminates
  the need for explicit action-value gradient computation while maintaining provable
  compatibility with the true policy gradient.
---

# Compatible Gradient Approximations for Actor-Critic Algorithms

## Quick Facts
- arXiv ID: 2409.01477
- Source URL: https://arxiv.org/abs/2409.01477
- Reference count: 40
- Primary result: CPG achieves state-of-the-art performance on continuous control tasks while addressing compatibility issues in deterministic policy gradients

## Executive Summary
This paper introduces Compatible Policy Gradient (CPG), a model-free actor-critic algorithm that addresses fundamental compatibility issues in deterministic policy gradients. The method uses a zeroth-order approximation of action-value gradients through two-point stochastic gradient estimation in action space, eliminating the need for explicit gradient computation while maintaining provable compatibility with the true policy gradient. Experiments on OpenAI Gym continuous control tasks demonstrate that CPG either matches or significantly outperforms state-of-the-art methods like TD3 and SAC in terms of convergence speed and final reward levels across five of six tested environments.

## Method Summary
CPG addresses compatibility issues in deterministic policy gradients by using a zeroth-order approximation of action-value gradients. The algorithm evaluates critics at low-dimensional random perturbations within the action space and uses two-point stochastic gradient estimation to approximate the gradient of the Q-function. This approach eliminates the need for explicit action-value gradient computation while maintaining provable compatibility with the true policy gradient. The method is implemented in an off-policy setting with clipped double Q-learning, delayed policy updates, and target networks following the TD3 framework.

## Key Results
- CPG either matches or significantly outperforms TD3 and SAC in 5 of 6 tested MuJoCo environments
- The method demonstrates robust performance under both perfect and imperfect environmental conditions
- Statistical tests confirm consistent improvements across multiple runs
- CPG addresses compatibility issues inherent in deterministic policy gradient schemes

## Why This Works (Mechanism)

### Mechanism 1
The zeroth-order approximation directly estimates action-value gradients using only value estimates, avoiding the need for explicit gradient computation. The algorithm evaluates the critic at low-dimensional random perturbations in action space, then computes the gradient approximation using two-point stochastic estimation. This works because the smoothed Q-function's gradient can be accurately estimated using finite samples from the critic's value estimates. The approach breaks down if the critic's value estimates are highly inaccurate or the perturbation noise overwhelms the signal.

### Mechanism 2
The algorithm maintains compatibility with the true policy gradient through controlled error bounds. The smoothing parameter μ creates a trade-off between exploration and gradient approximation accuracy, with error bounds scaling as O(√p/μ + Gμ). This works because the Q-function is sufficiently smooth and the critic can be trained to minimize perturbation representation error. The approach fails if μ is chosen too small relative to the critic's approximation error, or too large causing excessive exploration noise.

### Mechanism 3
The two-point evaluation provides finite-variance gradient estimates that improve stability. Including the Qψ(s,a)u term in the gradient approximation serves as a baseline, ensuring finite variance of the stochastic gradient estimate. This works because the baseline term has zero mean and can be computed alongside the main gradient estimate. The approach breaks down if the baseline term becomes correlated with the main gradient estimate or if the perturbation noise is too large.

## Foundational Learning

- Concept: Zeroth-order optimization
  - Why needed here: The algorithm replaces gradient computation with function evaluations, requiring understanding of how to approximate gradients without explicit derivatives
  - Quick check question: How does two-point estimation work to approximate gradients using only function values?

- Concept: Function approximation error and its impact on gradients
  - Why needed here: The method relies on understanding why standard DPG methods fail due to gradient incompatibility, even when value approximation is good
  - Quick check question: Why can two functions have identical value approximation error but very different gradients?

- Concept: Compatibility conditions in deterministic policy gradients
  - Why needed here: The algorithm specifically addresses compatibility issues that arise when using neural networks for function approximation
  - Quick check question: What are the classical compatibility requirements and why do they break with deep networks?

## Architecture Onboarding

- Component map: Actor network -> Two Q-networks -> Target networks -> Experience replay buffer -> Perturbation generator

- Critical path: 1. Sample action with exploration noise: a = πθ(s) + μu 2. Execute action and store transition 3. Sample minibatch and compute target Q-values 4. Update Q-networks to minimize MSE 5. Compute compatible policy gradient using two-point evaluation 6. Update actor network using CPG 7. Update target networks

- Design tradeoffs: Perturbation magnitude μ (larger values improve exploration but increase gradient approximation error), network capacity (larger networks can better approximate gradients but increase computational cost), update frequency (more frequent updates can improve learning speed but may reduce stability)

- Failure signatures: High variance in gradient estimates (indicates μ too large or Q-network not accurate enough), policy collapse (indicates exploration insufficient or learning rate too high), slow convergence (indicates Q-network not learning fast enough or μ poorly tuned)

- First 3 experiments: 1. Verify gradient approximation accuracy: Compare CPG gradient estimates against analytical gradients on a simple function 2. Test smoothing parameter sensitivity: Run algorithm with different μ values on a simple environment 3. Validate compatibility: Measure the gap between CPG and true gradient on a known policy gradient problem

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but the methodology raises several important research directions. The approach's effectiveness in high-dimensional action spaces, its behavior under non-stationary dynamics, and the theoretical convergence properties in different settings remain unexplored areas that could significantly impact the method's applicability and understanding.

## Limitations

- The compatibility analysis relies on smoothness assumptions about the Q-function that may not hold in practice
- The theoretical error bounds create a fundamental trade-off between exploration and gradient accuracy requiring careful tuning
- Sample efficiency gains relative to TD3 and SAC are not quantified, making it difficult to assess computational cost implications

## Confidence

- **High confidence**: The zeroth-order gradient approximation mechanism and its implementation details
- **Medium confidence**: The compatibility claims and theoretical error bounds, as these depend on assumptions about function smoothness
- **Medium confidence**: The experimental results showing performance improvements, though sample efficiency analysis is lacking

## Next Checks

1. **Gradient approximation validation**: Compare CPG gradient estimates against analytical gradients on a suite of benchmark functions with known derivatives to verify the accuracy of the two-point stochastic estimation

2. **Perturbation sensitivity analysis**: Systematically vary the smoothing parameter μ across multiple orders of magnitude to characterize the exploration-accuracy trade-off and identify optimal ranges for different environment complexities

3. **Sample efficiency benchmarking**: Measure wall-clock time and total environment interactions required to reach performance thresholds, comparing CPG against TD3 and SAC to quantify computational overhead from the additional Q-function evaluations