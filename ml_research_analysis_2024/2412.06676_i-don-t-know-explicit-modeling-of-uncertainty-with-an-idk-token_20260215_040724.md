---
ver: rpa2
title: 'I Don''t Know: Explicit Modeling of Uncertainty with an [IDK] Token'
arxiv_id: '2412.06676'
source_url: https://arxiv.org/abs/2412.06676
tags:
- language
- token
- should
- answer
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel calibration method to combat hallucinations
  in large language models (LLMs). The core idea is to add a special [IDK] ("I don't
  know") token to the model's vocabulary and modify the training objective so that
  when the model makes incorrect predictions, probability mass is shifted towards
  the [IDK] token based on an uncertainty factor.
---

# I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token

## Quick Facts
- arXiv ID: 2412.06676
- Source URL: https://arxiv.org/abs/2412.06676
- Reference count: 40
- Primary result: Novel calibration method using [IDK] token improves precision up to 78.1% vs 23.0% baseline on factual tasks

## Executive Summary
This paper introduces a novel calibration method to combat hallucinations in large language models (LLMs) by adding a special [IDK] ("I don't know") token to the model's vocabulary and modifying the training objective. The approach shifts probability mass toward the [IDK] token for incorrect predictions based on an uncertainty factor, allowing models to explicitly express uncertainty rather than generating incorrect text. The method is evaluated across multiple model architectures and factual downstream tasks, showing significant improvements in precision with only small decreases in recall.

## Method Summary
The method adds an [IDK] token to the model's vocabulary and modifies the cross-entropy objective to shift probability mass to this token when the model makes incorrect predictions. The uncertainty factor λ determines how much probability mass is shifted, calculated as Π × (1 - P(gold)/max(P(all tokens))). An anti-false positive regularization term (LFP-reg) prevents the model from always predicting [IDK] when it knows the answer. The approach is applied during continued pretraining on factual data, teaching models to express uncertainty explicitly.

## Key Results
- Precision improves from 23.0% to 78.1% on factual tasks
- Performance scales log-linearly with model size, most effective on larger models
- Only small decreases in recall (3.7-8.2 percentage points) despite significant precision gains
- Extensive ablation studies validate individual components of the method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shifting probability mass toward the [IDK] token during pretraining teaches the model to express uncertainty.
- Mechanism: The modified cross-entropy objective LIDK redirects target probability mass from the gold token to [IDK] when the model's prediction is incorrect, scaled by an uncertainty factor λ.
- Core assumption: The pretraining objective's uncertainty signal (e.g., low gold token probability) correlates with uncertainty on downstream factual tasks.
- Evidence anchors:
  - [abstract]: "We add a special [IDK] token... and introduce an objective function that shifts probability mass to the [IDK] token for incorrect predictions."
  - [section]: "During a continued pretraining phase, we modify the conventional cross-entropy objective to express uncertainty... We leverage the uncertainty captured by the pretraining objective on its pretraining data."
- Break condition: If the pretraining data's uncertainty signal does not generalize to factual downstream tasks, the method fails.

### Mechanism 2
- Claim: The adaptive uncertainty factor λ balances precision and recall by modulating how much probability mass is shifted to [IDK].
- Mechanism: λ = Π × (1 - P(gold) / max_i P(i)) ensures that probability mass is shifted more aggressively when the gold token's probability is low relative to the highest predicted token.
- Core assumption: The ratio of gold token probability to max token probability is a reliable indicator of prediction confidence.
- Evidence anchors:
  - [section]: "We define λ as one minus the probability mass on the gold token divided by the maximum probability mass put on any token."
  - [section]: "This allows the model to express uncertainty in its output explicitly."
- Break condition: If the gold token probability is not a reliable confidence signal, the adaptive λ fails to balance precision and recall.

### Mechanism 3
- Claim: The LFP-reg regularization term prevents the model from collapsing into always predicting [IDK].
- Mechanism: LFP-reg = -log(1 - P(IDK)) is applied only when the model's prediction is correct, discouraging unnecessary use of [IDK].
- Core assumption: Without regularization, the model will over-optimize the IDK objective by predicting [IDK] too often.
- Evidence anchors:
  - [section]: "We add the following anti-false positive regularization to our objective... This aims to minimize the [IDK] token's probability mass the model learns to predict in cases it knows the answer."
  - [section]: "We perform an ablation of LFP-reg in Section 4.2."
- Break condition: If the regularization strength is insufficient, the model may still collapse to always predicting [IDK].

## Foundational Learning

- Concept: Cross-entropy loss and its role in language model training
  - Why needed here: The IDK objective is a modified cross-entropy loss, so understanding the base concept is essential.
  - Quick check question: What is the difference between cross-entropy loss and the modified LIDK loss?

- Concept: Uncertainty quantification in machine learning
  - Why needed here: The method relies on estimating model uncertainty to decide when to shift probability mass to [IDK].
  - Quick check question: How does the uncertainty factor λ relate to model confidence?

- Concept: Calibration of probabilistic predictions
  - Why needed here: The goal is to improve model calibration so that predicted probabilities better reflect true uncertainty.
  - Quick check question: What is the relationship between model calibration and hallucination reduction?

## Architecture Onboarding

- Component map: Input text sequence -> Pretrained transformer (BERT, Mistral-7B, Pythia) -> Modified loss (standard cross-entropy + LIDK + LFP-reg) -> Probability distribution over vocabulary including [IDK]

- Critical path:
  1. Load pretrained model and add [IDK] token to vocabulary
  2. Initialize [IDK] token embedding
  3. During training, compute LIDK loss for each token prediction
  4. Apply LFP-reg only when prediction is correct
  5. Update model parameters using combined loss

- Design tradeoffs:
  - Precision vs. recall: Increasing probability mass on [IDK] improves precision but reduces recall
  - Regularization strength: Too weak allows collapse to [IDK], too strong prevents effective uncertainty expression
  - Upper bound Π: Controls maximum probability mass that can be shifted to [IDK]

- Failure signatures:
  - Model always predicts [IDK]: Likely insufficient LFP-reg or too high Π
  - No improvement in precision: Possibly ineffective λ calculation or poor generalization of uncertainty signal
  - Training instability: May occur with very small models due to initial [IDK] token probability being too low

- First 3 experiments:
  1. Verify that [IDK] token is added to vocabulary and embedding is initialized
  2. Check that LIDK loss reduces to standard cross-entropy when prediction is correct
  3. Confirm that LFP-reg is only applied when prediction is correct

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which smaller models (pythia-70m and pythia-160m) experience training divergence when using the IDK objective?
- Basis in paper: [explicit] The paper explicitly states that these models show NaN errors in predicted logits and training collapse, but the underlying cause is not fully understood.
- Why unresolved: The paper observes the divergence but only conjectures about potential causes (initialization of the [IDK] token, bias towards [IDK] at the beginning of training) without definitive experimental validation.
- What evidence would resolve it: Controlled experiments testing different initialization strategies for the [IDK] token embedding, and systematic investigation of how probability mass on [IDK] affects gradient norms during early training steps.

### Open Question 2
- Question: How does the IDK objective affect the model's ability to handle open-ended generation tasks beyond factual question answering, such as creative writing or dialogue generation?
- Basis in paper: [inferred] The paper only evaluates text summarization as a general language modeling task, leaving other creative or open-ended tasks unexplored.
- Why unresolved: The evaluation focuses on factual tasks and summarization, which may not fully capture the impact on more creative or context-dependent generation scenarios.
- What evidence would resolve it: Systematic evaluation of IDK-tuned models on diverse open-ended generation benchmarks including story generation, dialogue systems, and creative writing tasks.

### Open Question 3
- Question: What is the optimal trade-off between precision and recall when using the IDK objective, and how should this be tuned for different application domains?
- Basis in paper: [explicit] The paper discusses the precision-recall tradeoff but does not provide guidance on how to optimize this balance for specific use cases.
- Why unresolved: While the paper shows that higher Π values increase IDK recall at the cost of IDK error rate, it does not explore how to dynamically adjust this based on application requirements or user preferences.
- What evidence would resolve it: User studies across different domains (medical diagnosis, legal reasoning, customer service) to determine acceptable precision-recall tradeoffs, and development of methods to dynamically adjust the IDK objective parameters based on task-specific requirements.

## Limitations

- Limited generalizability to creative or open-ended generation tasks beyond factual question-answering
- Permanent vocabulary overhead from dedicated [IDK] token may be prohibitive for smaller models
- Diminishing returns for extremely large models due to log-linear scaling relationship

## Confidence

- High confidence: The core technical contribution is clearly specified and reproducible with strong ablation evidence
- Medium confidence: Substantial precision improvements are well-supported but may overstate real-world impact
- Low confidence: Claims about not harming general language modeling abilities are based on limited evaluation

## Next Checks

- Validation Check 1: Replicate core results on an additional factual dataset not used in the paper to verify generalizability
- Validation Check 2: Test method's impact on creative generation tasks to empirically verify claims about general language modeling abilities
- Validation Check 3: Conduct user study comparing human perceptions of model factuality between baseline and IDK-trained models