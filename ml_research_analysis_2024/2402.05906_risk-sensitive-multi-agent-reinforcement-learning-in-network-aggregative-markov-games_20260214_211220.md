---
ver: rpa2
title: Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov
  Games
arxiv_id: '2402.05906'
source_url: https://arxiv.org/abs/2402.05906
tags:
- risk
- risk-sensitive
- agents
- agent
- markov
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of risk-sensitive multi-agent
  reinforcement learning (MARL) in network aggregative Markov games (NAMGs) using
  cumulative prospect theory (CPT), a non-convex risk measure that generalizes coherent
  risk measures. The authors propose a distributed actor-critic algorithm called Distributed
  Nested CPT-AC that incorporates CPT risk into the MARL objective.
---

# Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games

## Quick Facts
- arXiv ID: 2402.05906
- Source URL: https://arxiv.org/abs/2402.05906
- Reference count: 40
- Primary result: Distributed nested CPT actor-critic algorithm converges to subjective Markov perfect Nash equilibrium in network aggregative Markov games

## Executive Summary
This paper introduces a risk-sensitive multi-agent reinforcement learning framework for network aggregative Markov games using cumulative prospect theory (CPT). The authors propose a distributed actor-critic algorithm that incorporates CPT risk measures, enabling agents to exhibit risk-sensitive behavior including loss aversion and probability weighting. Under specific assumptions, they prove convergence to a subjective Markov perfect Nash equilibrium. The framework allows agents to develop different risk attitudes while preserving privacy, as it doesn't require knowledge of other agents' policies or reward functions.

## Method Summary
The method employs a distributed nested CPT actor-critic algorithm for network aggregative Markov games. Each agent uses policy gradient theorem adapted for CPT risk measures, with gradient estimation based on a subjective steady-state distribution. The critic estimates state values using CPT operator with bootstrapping from experience replay. The algorithm operates under assumptions of convexity, bounded gradients, strong monotonicity, and Lipschitz continuity of the pseudo-gradient. Value function estimation uses a CPT-specific algorithm that leverages the nested structure to estimate values from samples.

## Key Results
- Subjective CPT policies differ from risk-neutral ones due to non-linear probability weighting
- Agents with higher loss aversion tend to socially isolate themselves more in NAMGs
- Algorithm converges to subjective Markov perfect Nash equilibrium under Assumptions 4 and 5

## Why This Works (Mechanism)

### Mechanism 1
CPT risk-sensitive MARL objective yields subjective policies that differ from risk-neutral ones due to non-linear probability weighting. The CPT operator replaces expectation with non-linear transformation that overweights small probabilities and underweights large ones, changing effective visitation probabilities and leading to different optimal policies. This mechanism collapses to standard risk-neutral MARL if CPT weighting functions are linear or agents are risk-neutral.

### Mechanism 2
Agents with higher loss aversion tend to socially isolate themselves more in an NAMG. Higher loss aversion (larger λ in CPT utility) makes perceived cost of losses more severe, leading agents to prefer safe actions more frequently and reduce interaction with community. This mechanism fails if reward structure doesn't create meaningful risk or if agents don't have individual utility parameters.

### Mechanism 3
Distributed nested CPT actor-critic algorithm converges to subjective Markov perfect Nash equilibrium under certain assumptions. The algorithm uses policy gradient theorem derived for CPT risk measure with gradient estimation based on subjective steady-state distribution. Under Assumptions 4 and 5 (convexity, bounded gradients, strong monotonicity, Lipschitz continuity), actor updates converge to Nash policy, yielding full algorithm convergence to subjective MPNE.

## Foundational Learning

- Concept: Cumulative Prospect Theory (CPT)
  - Why needed here: Provides risk measure that makes agents risk-sensitive rather than risk-neutral, enabling modeling of loss aversion and probability weighting
  - Quick check question: What are the two key components of CPT that differ from expected utility theory?
    Answer: Probability weighting functions (ω+ and ω-) and non-linear utility functions (u+ and u-)

- Concept: Network Aggregative Markov Games (NAMGs)
  - Why needed here: Provides multi-agent framework where agents' rewards depend on their own actions and aggregative function of neighbors' actions, enabling study of social isolation effects
  - Quick check question: How is the aggregative term σi(a−i) defined in an NAMG?
    Answer: σi(a−i) = Σ(j∈N\i) ωij aj, where ω are edge weights of communication graph

- Concept: Markov Perfect Nash Equilibrium (MPNE)
  - Why needed here: Solution concept that algorithm aims to converge to, representing set of policies where no agent can benefit by unilaterally deviating
  - Quick check question: What property must hold at a Markov Perfect Nash Equilibrium?
    Answer: Given other agents' policies, each agent's policy must be a best response

## Architecture Onboarding

- Component map:
  Simulator/Environment -> Experience Replay -> CPT Value Estimator -> Critic Network -> Gradient Calculator -> Actor Networks

- Critical path:
  1. Agent samples action from current policy
  2. Execute action, observe reward and next state
  3. Store experience in replay
  4. Sample experiences to estimate CPT value of next state
  5. Compute TD error and update critic
  6. Estimate gradient of CPT value at initial state
  7. Update actor parameters

- Design tradeoffs:
  - Model-based vs model-free: Algorithm requires transition probabilities but not reward functions or other agents' policies (privacy-preserving)
  - Function approximation: Limited to tabular representation due to CPT operator properties; linear approximation not guaranteed to converge
  - Sample efficiency: Requires sampling from simulator or large experience replay for accurate CPT estimation

- Failure signatures:
  - Critic divergence: Value estimates grow unbounded or oscillate
  - Actor instability: Policy parameters change erratically or diverge
  - Slow convergence: Learning rates too small or gradient estimates too noisy
  - Suboptimal policies: Algorithm converges to local rather than global optimum (if Assumptions 4,5 fail)

- First 3 experiments:
  1. Verify CPT value estimation: Implement Algorithm 1 and test on known distributions to confirm asymptotic consistency
  2. Test critic convergence: Run critic updates on simple MDP with known CPT values to verify convergence
  3. Validate gradient estimation: Compare analytical gradients from Theorem 1 with finite-difference approximations on simple NAMG

## Open Questions the Paper Calls Out

### Open Question 1
Which formulation of CPT (nested or non-nested) better represents human risk behavior in multi-agent settings? The paper discusses both formulations and their respective advantages/disadvantages, noting that no cognitive research has been conducted to ascertain which best represents dynamic risk behavior exhibited by humans. This remains unresolved as the paper doesn't provide experimental or theoretical evidence to determine which formulation is more cognitively plausible for human agents.

### Open Question 2
Can function approximation (linear or neural network) be used with proposed CPT risk-sensitive algorithm while maintaining theoretical convergence guarantees? The paper states mathematical properties of CPT-sensitive MDPs don't allow them to belong to family of robust MDPs, and scaling proposed algorithm to larger environments and continuous control is not compatible with theoretical convergence guarantees. This limitation remains unresolved as the paper explicitly notes this constraint.

### Open Question 3
How does loss aversion affect social dynamics and cooperation in network aggregative Markov games? While the paper demonstrates correlation between loss aversion and social isolation in specific experimental setup, it doesn't provide comprehensive analysis of how varying levels of loss aversion across different network structures and game dynamics affect cooperation and social outcomes. This requires systematic experiments varying loss aversion parameters across different network topologies.

## Limitations

- Theoretical analysis relies heavily on Assumptions 4 and 5 (convexity, bounded gradients, strong monotonicity, Lipschitz continuity of pseudo-gradient) which are not empirically verified
- Algorithm's performance in non-tabular function approximation settings remains unexplored
- Convergence proof only guarantees convergence to subjective MPNE under specific learning rate conditions, with no guarantees on convergence speed or sample complexity

## Confidence

- Mechanism 1 (CPT vs risk-neutral policy differences): Low - theoretical claim with limited empirical validation
- Mechanism 2 (loss aversion leading to social isolation): Low - hypothesis-driven with only illustrative example provided
- Mechanism 3 (convergence to subjective MPNE): Medium - convergence proof exists but relies on strong assumptions

## Next Checks

1. Implement the CPT value estimation algorithm (Algorithm 1) and verify its asymptotic consistency on a simple MDP with known CPT values across varying sample sizes.
2. Test the critic convergence properties by running the algorithm on a small NAMG with known transition dynamics and comparing estimated CPT values against analytical solutions.
3. Validate the policy gradient calculations by comparing the analytical gradients from Theorem 1 with finite-difference approximations across different policy parameterizations and CPT settings.