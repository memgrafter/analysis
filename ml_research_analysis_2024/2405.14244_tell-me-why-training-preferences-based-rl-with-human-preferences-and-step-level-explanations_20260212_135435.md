---
ver: rpa2
title: 'Tell me why: Training preferences-based RL with human preferences and step-level
  explanations'
arxiv_id: '2405.14244'
source_url: https://arxiv.org/abs/2405.14244
tags:
- learning
- human
- reward
- explanations
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends preference-based reinforcement learning by allowing
  humans to provide step-level explanations alongside binary preferences over trajectory
  segments. The approach generates saliency-based explanations from the reward model
  and compares them to human annotations, adding auxiliary losses to align the model's
  explanations with human reasoning.
---

# Tell me why: Training preferences-based RL with human preferences and step-level explanations

## Quick Facts
- arXiv ID: 2405.14244
- Source URL: https://arxiv.org/abs/2405.14244
- Reference count: 7
- Introduces a method to incorporate step-level human explanations in preference-based RL

## Executive Summary
This paper presents a novel approach to preference-based reinforcement learning that incorporates human-provided step-level explanations alongside binary trajectory preferences. The method extends existing preference-based RL frameworks by adding auxiliary losses that align the model's saliency-based explanations with human reasoning. The approach aims to accelerate reward learning by capturing not just what humans prefer, but why they prefer certain trajectory segments. Experiments across three MuJoCo locomotion tasks demonstrate improved learning speed compared to baseline PEBBLE, particularly in scenarios with simulated human irrationality.

## Method Summary
The approach extends preference-based RL by allowing humans to provide step-level explanations alongside binary preferences over trajectory segments. A reward model is trained using both the preference comparisons and the human explanations, with auxiliary losses added to align the model's saliency-based explanations with human reasoning. The saliency maps are generated from the reward model and compared to human annotations using alignment metrics. This multi-modal feedback is integrated into the training process without requiring changes to the underlying RL algorithm.

## Key Results
- Improved learning speed compared to baseline PEBBLE in Walker, HalfCheetah, and Hopper environments
- Statistically significant performance gains in several human irrationality scenarios
- Demonstrates that richer human feedback can accelerate reward learning in preference-based RL
- Shows reasonable correlation between model-generated and human annotations in explanation alignment

## Why This Works (Mechanism)
The method works by incorporating richer human feedback into the reward learning process. By capturing both what humans prefer and why they prefer it through step-level explanations, the model can better align its learned reward function with human values. The auxiliary losses ensure that the model's internal reasoning (as captured by saliency maps) matches human reasoning, creating a more interpretable and aligned reward model. This multi-modal approach helps the model learn more efficiently by focusing on the features that humans actually care about rather than just mimicking preference patterns.

## Foundational Learning
- **Preference-based RL**: Why needed - provides a framework for learning from human feedback without requiring explicit reward functions. Quick check - understands binary preference comparisons over trajectory segments.
- **Saliency-based explanations**: Why needed - provides interpretable insights into model decision-making. Quick check - can generate and interpret saliency maps from neural networks.
- **Auxiliary losses**: Why needed - allows incorporation of additional training signals beyond primary objective. Quick check - understands how to add and optimize multiple loss terms simultaneously.
- **Human-in-the-loop learning**: Why needed - enables interactive refinement of models based on human feedback. Quick check - understands iterative feedback loops between human and model.

## Architecture Onboarding

**Component Map:**
Human -> Preference Comparator -> Reward Model <- Saliency Generator
                     |                             |
                     v                             v
                 Auxiliary Losses <- Explanation Comparator

**Critical Path:**
1. Human provides trajectory preferences and step-level explanations
2. Preference comparator generates training signals from binary preferences
3. Reward model processes trajectories and generates predictions
4. Saliency generator creates explanations from reward model
5. Explanation comparator measures alignment with human explanations
6. Auxiliary losses integrate explanation alignment into training

**Design Tradeoffs:**
- Richer human feedback vs. increased annotation burden
- Explanation quality vs. computational overhead for saliency generation
- Alignment strength vs. risk of overfitting to potentially noisy human explanations

**Failure Signatures:**
- Poor alignment between model and human explanations despite good preference learning
- Increased training instability due to conflicting primary and auxiliary objectives
- Degraded performance when human explanations are inconsistent or low-quality

**3 First Experiments:**
1. Compare learning curves with and without step-level explanations on simple preference tasks
2. Evaluate explanation alignment quality using correlation metrics
3. Test robustness to varying levels of human explanation noise

## Open Questions the Paper Calls Out
None

## Limitations
- Relies entirely on simulated human feedback rather than real human participants
- Method's scalability to more complex environments beyond simple MuJoCo tasks is unclear
- Quality of saliency-based explanations may vary significantly across different task domains

## Confidence

**High confidence:**
- Core methodology of incorporating step-level explanations with auxiliary losses is sound and well-implemented

**Medium confidence:**
- Learning speed improvements over PEBBLE in simulated environments, though actual performance gains vary by scenario

**Low confidence:**
- Generalization to real human feedback and more complex environments beyond the tested MuJoCo tasks

## Next Checks
1. Conduct user studies with real human participants to validate that simulated human irrationality models accurately capture actual human feedback patterns
2. Test the method on more complex environments with longer time horizons and multi-task scenarios to evaluate scalability
3. Compare the quality of generated explanations against ground-truth human reasoning using established interpretability metrics and conduct ablation studies on the auxiliary losses