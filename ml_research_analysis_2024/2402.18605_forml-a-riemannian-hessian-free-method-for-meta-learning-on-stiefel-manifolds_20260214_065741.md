---
ver: rpa2
title: 'FORML: A Riemannian Hessian-free Method for Meta-learning on Stiefel Manifolds'
arxiv_id: '2402.18605'
source_url: https://arxiv.org/abs/2402.18605
tags:
- riemannian
- optimization
- learning
- forml
- meta-learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FORML, a first-order Hessian-free method
  for meta-learning on Stiefel manifolds. The method addresses the computational complexity
  of Riemannian meta-learning, which requires second-order derivatives and involves
  expensive Riemannian operations.
---

# FORML: A Riemannian Hessian-free Method for Meta-learning on Stiefel Manifolds

## Quick Facts
- arXiv ID: 2402.18605
- Source URL: https://arxiv.org/abs/2402.18605
- Reference count: 40
- Key outcome: FORML achieves competitive few-shot classification accuracy with 3-20x computational speedup over second-order Riemannian methods

## Executive Summary
FORM: introduces a first-order Hessian-free method for meta-learning on Stiefel manifolds, specifically targeting the last classification layer of neural networks. By approximating second-order Riemannian derivatives with first-order Kronecker sum operations, FORML significantly reduces computational complexity while maintaining competitive accuracy. The method enforces orthogonality constraints on classification layer weights, effectively computing cosine similarities and enhancing representation reuse across tasks. Experimental results on mini-ImageNet, CUB, and FC100 datasets demonstrate FORML outperforms its Euclidean counterpart MAML and achieves substantial speedups over the non-approximation-based RMAML while maintaining state-of-the-art accuracy.

## Method Summary
FORM: applies Riemannian meta-learning to the last fully-connected layer of neural networks by constraining it to the Stiefel manifold, ensuring orthogonality of weight matrices. The method uses first-order approximation to avoid expensive second-order Hessian computations, employing Kronecker sum operations to approximate the differential of the retraction operator. FORML alternates between inner-loop task-specific optimization using Riemannian gradient descent with retraction on the Stiefel layer, and outer-loop meta-parameter updates using the first-order approximation. The approach is evaluated on few-shot classification tasks using various backbone networks including Conv-4, Conv-6, ResNet-10, and ResNet-18.

## Key Results
- FORML outperforms MAML on few-shot classification accuracy while maintaining computational efficiency
- Achieves 3-20x speedup over non-approximation-based RMAML in epoch and inner-loop optimization time
- Demonstrates competitive performance against state-of-the-art methods on mini-ImageNet, CUB, and FC100 datasets
- Maintains orthogonality constraints while achieving high classification accuracy across different backbone architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FORML replaces expensive second-order Riemannian backpropagation with a first-order approximation that avoids Hessian computations while preserving gradient direction accuracy.
- Mechanism: The method approximates the differential of the retraction operator on the Stiefel manifold using Kronecker sums of matrices, enabling first-order updates without nested differentiation through retraction and projection steps.
- Core assumption: The retraction operator on the Stiefel manifold can be locally approximated by a first-order Taylor expansion that preserves orthogonality constraints while maintaining convergence properties.
- Evidence anchors:
  - [abstract] "introduces a Hessian-free approach that uses a first-order approximation of derivatives on the Stiefel manifold"
  - [section] "By ignoring those terms and using vec(·) operator, the derivatives with respect to Θ which is an orthogonal matrix (a point on Stiefel manifold) can be written as: vec(d(Θ(t)))−0.5α vec(d(Θ(t))(∇Θ(t) Li)T Θ(t))−0.5α vec(Θ(t)(∇Θ(t) Li)T d(Θ(t)))"
  - [corpus] Weak corpus evidence - no direct citations found, indicating novelty
- Break condition: If the first-order approximation significantly deviates from the true Riemannian gradient direction, convergence to optimal solutions may fail, particularly on highly curved manifold regions.

### Mechanism 2
- Claim: Enforcing orthogonality constraints on the classification layer parameters improves representation reuse and acts as regularization to prevent overfitting.
- Mechanism: By placing the last fully-connected layer on the Stiefel manifold, the weight matrix remains orthonormal, effectively computing cosine similarities between inputs and class prototypes, which enhances transfer of learned representations across tasks.
- Core assumption: Orthonormal weight matrices provide better class separation and generalization than unconstrained Euclidean counterparts, particularly in few-shot scenarios where data is limited.
- Evidence anchors:
  - [abstract] "using a Stiefel fully-connected layer that enforces orthogonality constraint on the parameters of the last classification layer as the head of the backbone network, strengthens the representation reuse of the gradient-based meta-learning methods"
  - [section] "normalizing the input of the head leads to computing the cosine similarities between the input and the output classes, which results in increasing the representation reuse phenomena"
  - [corpus] Weak corpus evidence - limited direct citations for this specific combination
- Break condition: If the orthogonality constraint becomes too restrictive for the specific classification task, it may limit the model's ability to learn optimal decision boundaries, potentially degrading performance on complex datasets.

### Mechanism 3
- Claim: First-order Riemannian meta-learning achieves competitive performance with significantly reduced computational complexity and memory footprint compared to second-order approaches.
- Mechanism: By avoiding second-order Hessian computations and using efficient Kronecker sum approximations, FORML reduces both the computational load per iteration and the memory required to store intermediate gradient computations.
- Core assumption: The computational savings from avoiding second-order operations outweigh any potential accuracy loss from first-order approximation, particularly when combined with the regularization benefits of orthogonality constraints.
- Evidence anchors:
  - [abstract] "significantly reduces the computational load and memory footprint" and "Our experimental results across various few-shot learning datasets, demonstrate the superiority of our proposed method compared to the state-of-the-art methods"
  - [section] "FORM shows a significant improvement over its non-Riemannian counterpart (i.e MAML) in terms of classification accuracy" and "FORM demonstrates a significant improve over its non-approximation-based version, RMAML, by performing an epoch and an inner-level optimization loop more than 3 times and 20 times faster, respectively"
  - [corpus] Moderate corpus evidence with 5 related papers averaging 0.444 FMR score
- Break condition: If the computational savings are insufficient to offset the overhead of Riemannian operations, or if the first-order approximation becomes too coarse for complex optimization landscapes.

## Foundational Learning

- Concept: Riemannian manifolds and optimization on manifolds
  - Why needed here: Understanding the geometric structure where parameters reside and how standard gradient descent must be adapted to respect manifold constraints is fundamental to implementing FORML
  - Quick check question: What are the three key Riemannian operations (orthogonal projection, retraction, parallel transport) and how do they differ from Euclidean gradient descent?

- Concept: Stiefel manifolds and orthogonality constraints
  - Why needed here: The Stiefel manifold specifically enforces orthonormal matrices, which is crucial for understanding why placing the classification layer here computes cosine similarities and how this affects representation reuse
  - Quick check question: How does the constraint P^T P = I on the Stiefel manifold relate to computing cosine similarity between input vectors and class prototypes?

- Concept: First-order approximation of second-order derivatives
  - Why needed here: Understanding how FORML avoids Hessian computations through Kronecker sum approximations is essential for both implementation and debugging
  - Quick check question: How does the Kronecker sum (A ⊕ B) = A ⊗ I + I ⊗ B simplify the computation of the first-order approximation compared to explicit second-order differentiation?

## Architecture Onboarding

- Component map: Backbone network (Conv-4, Conv-6, ResNet variants) with Euclidean parameters → Stiefel manifold-constrained classification layer → Riemannian gradient descent with retraction → First-order approximation using Kronecker sums
- Critical path: Forward pass through backbone → Normalize input to Stiefel layer → Compute cosine similarity using orthogonal weights → Loss computation → Inner-loop Riemannian gradient descent with retraction → Outer-loop first-order approximation update using Kronecker sums → Repeat
- Design tradeoffs: Stiefel constraints provide regularization and representation reuse benefits but may limit flexibility in learning optimal decision boundaries. First-order approximation reduces computational cost but may sacrifice some accuracy compared to second-order methods. Choice of which layer(s) to place on manifold affects both performance and computational requirements.
- Failure signatures: Non-orthonormal weight matrices indicate manifold constraint violations. Poor classification accuracy suggests inadequate first-order approximation. Divergence indicates incorrect Riemannian operation implementation.
- First 3 experiments:
  1. Verify orthogonality constraint: Implement a test that checks P^T P = I for the Stiefel layer weights after each update to ensure manifold constraints are maintained
  2. Compare gradient directions: Compute both exact Riemannian gradients and first-order approximations for a simple task to verify the approximation quality and identify when it breaks down
  3. Measure computational overhead: Profile the runtime of Riemannian operations (retraction, projection) versus their Euclidean counterparts to quantify the computational savings from the first-order approximation

## Open Questions the Paper Calls Out
1. How does FORML's performance compare to other state-of-the-art meta-learning methods on even more challenging few-shot learning benchmarks beyond FC100?
2. Can FORML be effectively extended to multi-modal or multi-task Riemannian meta-learning scenarios?
3. How does FORML's performance scale with the size of the model (e.g., deeper networks or larger datasets)?

## Limitations
- First-order approximation may break down on highly curved manifold regions where second-order information is critical
- Orthogonality constraints might be overly restrictive for certain classification tasks
- Method is currently limited to the classification layer, missing opportunities for broader manifold optimization

## Confidence
- High confidence in computational efficiency claims based on clear theoretical justification and reported 3-20x speedups
- Medium confidence in orthogonality constraint benefits due to limited ablation studies and theoretical analysis
- Low confidence in absolute performance levels due to potential implementation variations across studies

## Next Checks
1. Test FORML on datasets with different geometric properties (e.g., CIFAR-100) to assess robustness of the first-order approximation across varying optimization landscapes
2. Perform ablation studies comparing FORML with and without orthogonality constraints on the classification layer to quantify the specific contribution of the Stiefel manifold constraint
3. Implement and evaluate the exact second-order RMAML method on the same hardware to verify the reported 20x speedup for inner-loop optimization is achievable in practice